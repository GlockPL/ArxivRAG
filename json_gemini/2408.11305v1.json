{"title": "UniFashion: A Unified Vision-Language Model for Multimodal Fashion Retrieval and Generation", "authors": ["Xiangyu Zhao", "Yuehan Zhang", "Wenlong Zhang", "Xiao-Ming Wu"], "abstract": "The fashion domain encompasses a variety of real-world multimodal tasks, including multimodal retrieval and multimodal generation. The rapid advancements in artificial intelligence generated content, particularly in technologies like large language models for text generation and diffusion models for visual generation, have sparked widespread research interest in applying these multimodal models in the fashion domain. However, tasks involving embeddings, such as image-to-text or text-to-image retrieval, have been largely overlooked from this perspective due to the diverse nature of the multimodal fashion domain. And current research on multi-task single models lack focus on image generation. In this work, we present UniFashion, a unified framework that simultaneously tackles the challenges of multimodal generation and retrieval tasks within the fashion domain, integrating image generation with retrieval tasks and text generation tasks. UniFashion unifies embedding and generative tasks by integrating a diffusion model and LLM, enabling controllable and high-fidelity generation. Our model significantly outperforms previous single-task state-of-the-art models across diverse fashion tasks, and can be readily adapted to manage complex vision-language tasks. This work demonstrates the potential learning synergy between multimodal generation and retrieval, offering a promising direction for future research in the fashion domain. The source code is available at https://github.com/xiangyu-mm/UniFashion.", "sections": [{"title": "1 Introduction", "content": "The fashion domain presents a range of real-world multimodal tasks, encompassing multimodal retrieval (Gao et al., 2020; Wu et al., 2021; Bai et al., 2023) and multimodal generation (Yang et al., 2020) tasks. Such tasks have been utilized in diverse e-commerce scenarios to enhance product discoverability, seller-buyer interaction, and customer conversion rates after catalog browsing (Han et al., 2023; Zhuge et al., 2021). The remarkable progress in the field of artificial intelligence generated content (AIGC), particularly in technologies like large language models (LLMs) (Chiang et al., 2023; Touvron et al., 2023; Brown et al., 2020) for text generation and diffusion models (Rombach et al., 2022; Nichol et al., 2022; Saharia et al., 2022) for visual generation, has sparked widespread research interest in applying these multimodal models in the fashion domain.\nMultimodal large language models (Liu et al., 2023a; Dai et al., 2023; Dong et al., 2023) (MLLMs) seem to emerge as a promising direction for a single multi-task model. However, due to the heterogeneous nature of the multimodal fashion tasks (Han et al., 2023), existing MLLMs lack the capability to be directly applied to the fashion domain, such as embedding ability. For example, in the fashion domain, retrieval tasks that rely on embedding ability, like image-to-text or text-to-image retrieval, have been largely neglected from this aspect. Furthermore, MLLMs lack the ability to solve composed image retrieval (CIR) (Liu et al., 2021; Baldrati et al., 2022) task, which composes the reference image and related caption into a joint embedding to calculate similarities with the candidate images and is particularly relevant in fashion recommendation systems (Han et al., 2017).\nDrawing inspiration from GRIT (Muennighoff et al., 2024), which successfully combined embedding and generative tasks into a unified model for text-centric applications and showed improved embedding performance through the addition of a generative objective, it becomes clear that investigating task correlations and integrating embedding with generative models in the fashion domain is is both necessary and promising.\nWhile previous works (Han et al., 2023; Zhuge et al., 2021) in the fashion domain have also proposed using a single model for solving multiple tasks, they ignore the image generation tasks. Besides, for fashion tasks such as try-on (Choi et al., 2021) and fashion design (Baldrati et al., 2023b), it is generally required to generate target images based on multimodal input. However, previous works (Baldrati et al., 2023b) in fashion image generation typically adopt the CLIP text encoder to encode text information, which may not be capable of effectively understanding the textual context due to their weaker text encoder, as noted in Saharia et al. (2022). Hence, we posit that current studies have not fully exploited the potential in learning synergy between generation and retrieval.\nIn this work, we propose UniFashion, which unifies retrieval and generation tasks by integrating LLMs and diffusion models, as illustrated in Figure 2. UniFashion consists of three parts: The Q-Former is crucial for amalgamating text and image input, creating multimodal learnable queries. These queries, once refined through task-specific adapters, enable the LLM module to utilize them as soft prompts for generating captions for target images. Simultaneously, the diffusion module utilizes the learnable queries as conditions to guide the latent diffusion model in image synthesis and editing tasks. To enable controllable and high-fidelity generation, we propose a two-phase training strategy. In the first phase, we perform multimodal representation learning on image-text pairs datasets. We freeze Q-Former and fine-tune the LLM and diffusion modules, ensuring they develop the capability to comprehend the multimodal representations provided by Q-Former. Subsequently, in the second phase, we proceed to fine-tune UniFashion on datasets with multimodal inputs, such as Fashion-IQ, where we freeze the LLM and diffusion modules, only tuning Q-Former. This strategy ensures that Q-Former is adept at crafting multimodal representations that effectively integrate both reference images and text inputs.\nUniFashion holds three significant advantages that address the challenges in multimodal fashion retrieval and generation:\nFor the first time, we conduct an in-depth study of the synergistic modeling of multimodal retrieval and generation tasks within the fashion domain, thoroughly exploiting the inter-task relatedness. Further, we introduce UniFashion, a versatile, unified model that can handle all fashion tasks.\nSecondly, our model enhances performance via mutual task reinforcement. Specifically, the caption generation module aids the CIR task, while jointly training the generation and retrieval tasks improves the multimodal encoder for the diffusion module.\nThird, extensive experiments on diverse fashion tasks\u2014including cross-modal retrieval, composed image retrieval, and multimodal generation\u2014demonstrate that our unified model significantly surpasses previous state-of-the-art methods."}, {"title": "2 Preliminaries and Related Works", "content": "Fashion tasks encompass a range of image and language manipulations, including cross-modal retrieval, composed image retrieval, fashion image captioning and generation, etc. The representative tasks can be briefly divided into the following two groups:\nFashion Retrieval generally consists of Cross-Modal Retrieval (CMR) (Ma et al., 2022; Rostamzadeh et al., 2018) and composed image retrieval (CIR) tasks (Baldrati et al., 2023a; Bai et al., 2023). CMR requests to efficiently retrieve the most matched image/sentence from a large candidate pool \\(D\\) given a text/image query. CIR is a special type of image retrieval with a multimodal query (a combination of a reference image and a modifying text) matched against a set of images. It retrieves a target image from a vast image database based on a reference image and a text description detailing changes to be applied to the reference image. In this scenario, a query pair \\(p = {I_R,t}\\) is provided, where \\(I_R\\) is the reference image and \\(t\\) is the text describing the desired modifications. The challenge for this task is to accurately identify the target image \\(I_T\\) that best matches the query among all potential candidates in the image corpus \\(D\\).\nFashion Generation consists of Fashion Image Captioning (FIC) and Fashion Image Generation (FIG). FIC (Yang et al., 2020) aims to generate a descriptive caption for a product based on the visual and/or textual information provided in the input. FIG aims to generate images based on the multimodal input, such as try-on (Choi et al., 2021; Gou et al., 2023) and fashion design (Baldrati et al., 2023b)."}, {"title": "2.2 Multimodal Language Models", "content": "Recent research has witnessed a surge of interest in multimodal LLMs, including collaborative models (Wu et al., 2023; Yang et al., 2023b; Shen et al., 2023) and end-to-end methods (Alayrac et al., 2022; Zhao et al., 2023; Li et al., 2022; Bao et al., 2021; Wang et al., 2022b,a,a). More recently, some works also explore training LLMs with parameter-efficient tuning (Li et al., 2023b; Zhang et al., 2023b) and instruction tuning (Dai et al., 2023; Liu et al., 2023a; Ye et al., 2023; Zhu et al., 2023a; Li et al., 2023a). They only focus on generation tasks, while UniFashion is built upon a unified framework that enables both retrieval and generation tasks."}, {"title": "2.3 Diffusion Models", "content": "Diffusion generative models (Rombach et al., 2022; Ramesh et al., 2021; Nichol et al., 2022; Ruiz et al., 2023) have achieved strong results in text conditioned image generation works. Among contemporary works that aim to condition pretrained latent diffusion models, ControlNet (Zhang et al., 2023a) proposes to extend the Stable Diffusion model with an additional trainable copy part for conditioning input. In this work, we focus on the fashion domain and propose a unified framework that can leverage latent diffusion models that directly exploit the conditioning of textual sentences and other modalities such as human body poses and garment sketches."}, {"title": "2.4 Problem Formulation", "content": "Existing fashion image retrieval and generation methods are typically designed for specific tasks, which inherently restricts their applicability to the various task forms and input/output forms in the fashion domain. To train a unified model that can handle multiple fashion tasks, our approach introduces a versatile framework capable of handling multiple fashion tasks by aligning the multimodal representation into the LLM and the diffusion model. This innovative strategy enhances the model's adaptability, and it can be represented as:\n\\[I_{out}, T_{out} = F_{TRet}, T_{Gen} (I_{in}, T_{in}; \\Theta),\\] (1)\nwhere \\(F_T\\) represents the unified model parameterized by \\(\\Theta\\), it consists of retrieval module \\(T_{Ret}\\) and generative module \\(T_{Gen}\\)."}, {"title": "3 Proposed Model: UniFashion", "content": "In this section, we introduce the UniFashion to unify the fashion retrieval and generation tasks into a single model. By combining retrieval and generative modules, the proposed UniFashion employs a two-stage training strategy to capture relatedness between image and language information. Consequently, it can seamlessly switch between two operational modes for cross-modal tasks and composed modal tasks."}, {"title": "3.1 Phase 1: Cross-modal Pre-training", "content": "In the first stage, we conduct pre-training on the retrieval and generation modules to equip the Large Language Model (LLM) and diffusion model with strong cross-modal fashion representation capabilities for the next phase."}, {"title": "3.1.1 Cross-modal Retrieval", "content": "For cross-modal retrieval tasks, given a batch of image caption pairs \\(p = {I, C'}\\), we first calculate their unimodal representations using an independent method. In particular, we adopt a lightweight Querying Transformer, i.e., Q-Former in BLIP-2 (Li et al., 2023b), to encode the multimodal inputs, as it is effective in bridging the modality gap. To avoid information leaks, we employ a unimodal self-attention mask (Li et al., 2023b), where the queries and text are not allowed to see each other:\n\\[Z_I = Q-Former(I, q),\\]\n\\[Z_C = Q-Former(C).\\] (2)\nwhere the output sequence \\(Z_I\\) is the encoding result of an initialized learnable query \\(q\\) with the input image and \\(Z_C\\) is the encoded caption, which contains the embedding of the output of the [CLS] token \\(e_{cls}\\), which is a representation of the input caption text. Since \\(Z_I\\) contains multiple output embeddings (one from each query), we first compute the pairwise similarity between each query output and \\(e_{cls}\\), and then select the highest one as the image-text similarity. In our experiments, we employ 32 queries in \\(q\\), with each query having a dimension of 768, which is the same as the hidden dimension of the Q-Former. For cross-modal learning objective, we leverage the Image-Text Contrastive Learning (ITC) and Image-Text Matching (ITM) method. The first loss term is image-text contrastive loss, which has been widely adopted in existing text-to-image retrieval models. Specifically, the image-text contrastive loss is defined as:\n\\[\\mathcal{L}_{ITC}(X, Y) = - \\frac{1}{B} \\sum_{i=1}^{B} \\log \\frac{\\exp[X(\\mathbf{x}_i^T \\mathbf{y}_i)]}{\\sum_{j=1}^{B} \\exp[X(\\mathbf{x}_i^T \\mathbf{y}_j)]},\\] (3)\nwhere \\(X\\) is a learnable temperature parameter. ITM aims to learn fine-grained alignment between image and text representation. It is a binary classification task where the model is asked to predict whether an image-text pair is positive (matched) or negative (unmatched), it is defined as,\n\\[\\mathcal{L}_{ITM}(X, Y) = - \\frac{1}{B} \\sum_{i=1}^{B} \\log \\frac{\\exp[f_\\Theta(\\mathbf{x}_i, \\mathbf{y}_i)]}{\\sum_{j=1}^{B} \\exp[f_\\Theta(\\mathbf{x}_i, \\mathbf{y}_j)]}.\\] (4)\nThen, we maximize their similarities via symmetrical contrastive loss:\n\\[\\mathcal{L}_{cross} = \\mathcal{L}_{ITC}(t_c, Z_I) + \\mathcal{L}_{ITM}(Z_C, Z_I),\\] (5)"}, {"title": "3.1.2 Cross-modal Generation", "content": "As depicted in Fig. 2, after the learnable queries \\(q\\) pass through the multimodal encoder, they are capable of integrating the visual information with textual guidance. However, in Section 3.1.1, we did not specify a learning target for \\(q\\). Empirically, the \\(q\\) that has been merged with the reference image and edited text information should be equivalent to the encoding of the target image. This implies that we should be able to reconstruct the target image and its caption based on \\(q\\). In this section, we will employ generative objectives to improve the representation of augmented \\(q\\).\nIn the first stage, we connect the Q-Former (equipped with a frozen image encoder) to a Large Language Model (LLM) to harness the LLM's prowess in language generation, and to a diffusion model to exploit its image generation capabilities. Notably, we exclusively train the model using image-text pairs throughout this process. As depicted in Figure 2, we employ a Task Specific Adapter (TSA) layer to linearly project the output query embeddings \\(q\\) to match the dimensionality of the embeddings used by the LLM and diffusion model. In this stage, we freeze the parameters of the Q-Former and fine-tune only the adapter layers, connecting LLM and diffusion models. This approach allows us to develop a discriminative model that can evaluate whether queries \\(q\\) can generate the target image and its corresponding caption.\nTarget caption generation. The adapter layer is placed before the LLM to map the output of Q-Former to the text embedding space of the LLM. To synchronize the space of Q-Former with that of the LLM, we propose to use the image-grounded text generation (ITG) objective to drive the model to generate texts based on the input image by computing the auto-regressive loss:\n\\[\\mathcal{L}_{ITG} = - \\frac{1}{L} \\sum_{l=1}^{L} \\log p_\\phi(w_l|w_{<l}, f_\\Theta(q)),\\] (6)\nwhere \\(w_g = (w_1, ..., w_L)\\) represents the ground-truth caption of image \\(I\\) with length \\(L\\), \\(q = Q-Former(I, q)\\), \\(\\phi\\) denotes the LLM's parameters, and \\(\\Theta\\) denotes the text adapter layers' parameters.\nTarget image generation. In the first stage, our task also aims to reconstruct the image \\(I_T\\) from \\(q\\). As in standard latent diffusion models, given an encoded input \\(x\\), the proposed denoising network is trained to predict the noise stochastically added to \\(x\\). The corresponding objective function can be specified as:\n\\[\\mathcal{L}_{q2I} = \\mathbb{E}_{e \\nu, x_0} [||\\epsilon - \\epsilon_\\eta(x_t x, f_\\phi(q), t_x)||^2],\\] (7)\nwhere \\(\\eta\\) denotes the u-net models' parameters and \\(\\phi\\) denotes the image adapter layers' parameters. The overall loss in the first stage can be expressed:\n\\[\\mathcal{L}_{ph1} = \\mathcal{L}_{cross} + \\mathcal{L}_{ITG} + \\mathcal{L}_{q2I}.\\] (8)\nAfter the first training stage, we can leverage the LLM and diffusion model as discriminators to guide the generation of composed queries."}, {"title": "3.2 Phase 2: Composed Multimodal Fine-tuning", "content": "In this phase, the inputs are reference image and guidance text, and we fine-tune the model for composed multimodal retrieval and generation tasks."}, {"title": "3.2.1 Composed Image Retrieval", "content": "For CIR task, the target image \\(I_T\\) generally encompasses the removal of objects and the modification of attributes in the reference image. To solve this problem, as depicted in Fig. 2, the multimodal encoder is utilized to extract features from the reference image and the guide text. It joint embeds the given pair \\(p = {I_R, t}\\) in a sequential output. Specifically, a set of learnable queries \\(q\\) concatenated with text guidance \\(t\\) is introduced to interact with the features of the reference image. Finally, the output of Q-Former is the multimodal synthetic prompt \\(Z_R\\). We use a bi-directional self-attention mask, similar to the one used in BLIP2 (Li et al., 2023b), where all queries and texts can attend to each other. The output query embeddings \\(Z_R\\) thus capture multimodal information:\n\\[Z_R = Q-Former(I_R, t, q_R),\\]\n\\[Z_T = Q-Former(I_T, q_T).\\] (9)\nNoting that the output sequence \\(Z_R\\) consists of learnable queries \\(q\\) and encoded text guidance \\(t\\), which includes \\(e_{cls}\\), the embedding of the output of the [CLS] token. On the other hand, the target image's output sequence \\(Z_T\\) consists only of learnable queries. Therefore, we can use \\(Z_R\\) as a representation that incorporates information from the reference image and the guidance text and align it with the features of the target image \\(Z_T\\). Moreover, as UniFashion acquires the ability to generate captions for images from Sec. 3.1.2, we can generate captions for the candidate images and use \\(e_{cls}\\) to retrieve the caption \\(Z_C\\) of the target image. Then, the final contrastive loss for the CIR task is:\n\\[\\mathcal{L}_{cir} = \\mathcal{L}_{ITC}(e_{cls}, Z_T) + \\mathcal{L}_{ITC}(e_{cls}, Z_C)\\]\n\\[+ \\mathcal{L}_{ITM}(t, Z_T),\\] (10)"}, {"title": "3.2.2 Composed Multimodal Generation", "content": "For these generation tasks, we freeze the LLM parameters and tune the parameters of the task-specific adapters, the diffusion model, and the Q-Former. The loss function for the target image's caption generation is formulated in a way that is similar to Eq. 6:\n\\[\\mathcal{L}_{ITG} = - \\frac{1}{L} \\sum_{l=1}^{L} \\log p_\\phi(w_l|w_{<l}, f_\\Theta(q_R)),\\] (11)\nThe loss function for the target image generation is formulated in a way that is similar to Eq. 7:\n\\[\\mathcal{L}_{q2I} = \\mathbb{E}_{e \\nu, x_0} [||\\epsilon^r - \\epsilon_\\eta(x_t x, f_\\phi(q_R), t^x)||^2],\\] (12)\nThe overall loss in the second stage can be expressed as:\n\\[\\mathcal{L}_{stage2} = \\mathcal{L}_{cir} + \\mathcal{L}_{ITG} + \\mathcal{L}_{q2I}.\\] (13)"}, {"title": "4 Experiments", "content": "We initialize the multimodal encoder from BLIP2's Q-Former and MLLM from LLaVA-1.5. As for the diffusion module, following, StableVITON, we inherit the autoencoder and the denoising U-Net of the Stable Diffusion v1.4. We initialize the weights of the U-Net from the Paint-by-Example and for more refined person texture, we utilized a VAE fine-tuned on the VITONHD dataset from Stable-VITON. The statistics of the two-stage datasets can be found in Table 6. For cross-modal retrieval, we evaluated UniFashion on FashionGen validation set. For the image captioning task, UniFashion is evaluated in the FashionGen dataset. For the composed image retrieval task, we evaluated the Fashion-IQ validation set. To maintain consistency with previous work, for the composed image generation task, we fine-tuned UniFashion and evaluated it on the VITON-HD and MGD datasets. More details can be found in Appendix C.\nPhase 1: For multimodal representation learning, we follow BLIP2 and pretrain the Q-Former on fashion image-text pairs. To adapt the model for multimodal generation, we freeze the parameters of Q-Former and fine-tune the MLLM and diffusion model with their task specific adapters separately. Due to the different styles of captions in different fashion datasets, we adopt the approach of instruction tuning to train the LLM so that it can generate captions of different styles. More details can be found in Appendix D.\nPhase 2: In order to make UniFashion have the composed retrieval and generation abilities, we freeze the parameters of LLM and diffusion model, only fine-tune the multimodal encoder."}, {"title": "4.2 Evaluation Methods", "content": "We compare our models with previous state-of-the-art methods on each task. For extensive and fair comparisons, all prior competitors are based on large-scale pre-trained models.\nCross-modal retrieval evaluation: We consider both image-to-text retrieval and text-to-image retrieval with random 100 protocols used by previous methods. 100 candidates are randomly sampled from the same category to construct a retrieval database. The goal is to locate the positive match depicting the same garment instance from these 100 same-category negative matches. We utilize Recall@K as the evaluation metric, which reflects the percentage of queries whose true target ranked within the top K candidates.\nFashion image captioning evaluation: For evaluating the performance of caption generation, we utilize BLEU-4, METEOR, ROUGE-L, and CIDEr as metrics.\nComposed fashion image retrieval evaluation: We compare our UniFashion with CIR methods and the FAME-ViL model of V + L that is oriented towards fashion in the original protocol used by Fashion-IQ. For this task, we also utilize Recall@K as the evaluation metric.\nComposed fashion image generation evaluation: We compare our UniFashion with try-on methods on VITON-HD dataset and fashion design works on MGD dataset. To evaluate the quality of image generation, we use the Frechet Inception Distance (FID) score to measure the divergence between two multivariate normal distributions and employ the CLIP Score (CLIP-S) provided in the TorchMetrics library to assess the adherence of the image to the textual conditioning input (for fashion design task)."}, {"title": "4.3 Comparative Analysis of Baselines and Our Method", "content": "UniFashion performs better compared to baselines in all data sets. Tab. 1 presents the evaluation results for each baseline and our models in Fashion-Gen data sets for cross-modal retrieval. UniFashion outperforms most of the baseline models on both the text-to-image and image-to-text tasks. Following FAME-ViL, we also adopt a more challenging and practical protocol that conducts retrieval on the entire product set, which is in line with actual product retrieval scenarios. In Tab. 2, we performed a comparison between our UniFashion and other baselines on the FashionGen dataset for the image captioning task. By integrating the powerful generative ability of the LLM, our model performed significantly better than the traditional multimodal models in this task. In Tab. 4, we conducted a comparison between our UniFashion and CIR-specialist methods. Our findings are in line with those of Tab. 1.\nAfter fine-tuning UniFashion on different modal input composed image generation/editing tasks, it also demonstrates excellent performance. Tab. 3 evaluates the quality of the generated image of UniFashion in the VITON-HD unpaired setting. In order to verify that our model can achieve good results in a variety of modal inputs, we have conducted tests respectively on the traditional try-on task and the fashion design task proposed in MGD. For a fair evaluation with baselines, all the models are trained at a 512 \u00d7 384 resolution. To confirm the efficacy of our approach, we assess the realism using FID and KID score on all the tasks and using CLIP-S score for fashion design task. As can be seen, the proposed UniFashion model consistently outperforms competitors in terms of realism (i.e., FID and KID) and coherence with input modalities (i.e., CLIP-S), indicating that our method can better encode multimodal information. Meanwhile, although our model is slightly lower than StableVITON on the try-on task, this is because we froze the parameters of the diffusion model on the try-on task and only fine-tuned the Q-former part, but it can still achieve top2 results."}, {"title": "4.4 Ablation Study", "content": "Our model completes the multimodal composed tasks in more aspects. In Tab. 4, we also carry out ablation studies on different retrieval methods. Since UniFashion is capable of generating captions, for the CIR task, we initially utilize UniFashion to generate the captions of candidate images and then conduct the image retrieval task (denoted as UniFashion w/o cap) and the caption retrieval task (denoted as UniFashion w/o img). We find that our single-task variant has already achieved superior performance in the relevant field. Furthermore, due to the generative ability of our model, the pre-generated candidate library optimizes the model's performance in this task. For specific implementation details, please refer to Appendix B.\nWe researched the impact of different modules in UniFashion on various fashion tasks. In Tab. 5, we perform an ablation study on the proposed model architecture, with a focus on LLM and diffusion models. For comparison on the cross-modal retrieval task (CMR), we design the base model as directly fine-tuning BLIP2 without any new modules. The results indicate that the base model performs relatively well on this task and that the introduction of other modules does not lead to significant improvements. However, in the CIR task, the introduction of LLM and diffusion models as supervision can lead to significant improvements, especially when utilizing the captions pre-generated by the UniFashion to assist in retrieval, resulting in greater benefits. At the same time, we note that, after introducing the diffusion model, it may have some negative impact on the model's image captioning ability, possibly due to the inherent alignment differences between LLM and the diffusion model."}, {"title": "5 Conclusion", "content": "We have introduced UniFashion, a unified framework that addresses the challenges in multimodal generation and retrieval tasks within the fashion domain. By unifying embedding and generative tasks with a diffusion model and LLM, UniFashion enables controllable and high-fidelity generation, significantly outperforming previous single-task state-of-the-art models across diverse fashion tasks. Our model's ability to readily adapt to manage complex vision-language tasks demonstrates its potential for enhancing various e-commerce scenarios and fashion-related applications. The findings of this study highlight the importance of exploring the potential learning synergy between multimodal generation and retrieval, and provide a promising direction for future research in the fashion domain."}, {"title": "6 Limitations", "content": "This section aims to highlight the limitations of our work and provide further insight into the research in this area. Our model relies on diffusion for multimodal interaction, which means that the composed image generation processes may take longer. In our experiments, we tested the performance of our model on one A100 (80G) GPU. During inference, using 1000 examples from VITON-HD dataset, UniFashion took approximately 3.15 seconds for each image generation. We believe it would be beneficial to explore more efficient sampling methods, such as DPM-Solver++ (Lu et al., 2022), to improve the overall efficiency of UniFashion."}]}