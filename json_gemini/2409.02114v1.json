{"title": "TINY-TOXIC-DETECTOR: A COMPACT TRANSFORMER-BASED MODEL FOR TOXIC CONTENT DETECTION", "authors": ["Michiel Kamphuis"], "abstract": "This paper introduces Tiny-toxic-detector, a compact transformer-based model for toxic content detection. Despite having only 2.1 million parameters, the model achieves competitive performance on benchmark datasets, with 90.97% accuracy on ToxiGen and 86.98% accuracy on the Jigsaw dataset, competing with models over 50 times its size. This efficiency makes it suitable for deployment in resource-constrained environments, addressing the need for effective content moderation tools that balance performance with computational requirements. The model's architecture consists of 4 transformer encoder layers with 2 attention heads each, an embedding dimension of 64, and a feedforward dimension of 128. Trained on a combination of public and private datasets, Tiny-toxic-detector demonstrates the potential of task-specific, efficient models in tackling online toxicity. The paper discusses the model's architecture, training process, performance benchmarks, and limitations, highlighting its applicability in various scenarios such as social media monitoring and content moderation. By achieving comparable results to much larger models while significantly reducing computational requirements, Tiny-toxic-detector represents a step towards more sustainable and scalable AI-powered content moderation solutions.", "sections": [{"title": "Introduction", "content": "The proliferation of user-generated content on the internet has led to a need for effective moderation tools to detect and mitigate toxic language. While large-scale transformer models have achieved state-of-the-art performance on toxic content detection tasks, their massive size and computational requirements make them impractical for deployment in many real-world applications. For instance, many social media platforms, online forums, and content moderation services operate on resource-constrained devices or have limited budgets for computing infrastructure. In such scenarios, the computational overhead of large models can lead to significant latency, increased costs, and reduced scalability.\nFurthermore, the environmental impact of training and deploying large models is becoming increasingly concerning. The carbon footprint of training a single large language model can be equivalent to the emissions of several cars over their entire lifetimes. As the demand for AI-powered content moderation continues to grow, it is essential to develop models that are not only accurate but also efficient, scalable, and environmentally sustainable.\nIn this paper, we introduce Tiny-toxic-detector, a compact transformer-based model that achieves competitive performance on toxic content detection tasks while requiring only 2.1 million parameters. This represents a significant reduction in size compared to state-of-the-art models, which often have over 100 million parameters. Despite its small size, Tiny-toxic-detector achieves an accuracy of 90.97% on the ToxiGen dataset and 86.98% on the Jigsaw dataset, demonstrating its potential for efficient deployment in real-world applications and making it competitive with models over 50 times its size. Our work highlights the importance of developing compact and efficient models for toxic content detection, which can help mitigate the risks associated with online harassment and hate speech while also reducing the environmental impact of AI-powered content moderation."}, {"title": "Related Works", "content": "The field of toxic content detection has evolved significantly in recent years, driven by the increasing need to moderate online discussions and mitigate the spread of harmful content. This area of research has seen a progression from traditional machine learning approaches to advanced deep learning techniques, particularly transformer-based models.\nEarly work in toxic content detection often relied on traditional machine learning methods such as Support Vector Machines (SVMs) and Naive Bayes classifiers. These approaches typically used hand-crafted features like lexicon-based attributes, syntactic features, and semantic features to identify potentially toxic content. While effective to some degree, these methods often struggled with context-dependent toxicity and more subtle forms of harmful language. The advent of deep learning brought significant advancements to the field. Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs), particularly Long Short-Term Memory (LSTM) networks, demonstrated improved performance in capturing complex linguistic patterns associated with toxic content. These models could automatically learn relevant features from raw text data, reducing the need for manual feature engineering.\nA major breakthrough came with the introduction of transformer-based models, starting with BERT and followed by variants like RoBERTa. These models, pre-trained on large corpora of text, showed remarkable ability to understand context and nuance in language, leading to significant improvements in toxic content detection. For instance, the ROBERTa-based toxicity classifier achieved high accuracy on benchmark datasets, showcasing the potential of these models in real-world applications.\nTo facilitate research and enable fair comparisons between different approaches, several benchmark datasets have been introduced. The Jigsaw Toxic Comment Classification Challenge dataset has become a standard for evaluating toxicity detection models. More recently, the ToxiGen dataset was introduced to address the limited samples available.\nDespite the impressive performance of large transformer-based models, their computational requirements pose challenges for deployment in resource-constrained environments. This has led to increased interest in developing more efficient models that can maintain high accuracy while reducing computational overhead. The development of such models represents an important direction for making toxic content detection more accessible and scalable across various platforms and devices."}, {"title": "Model Architecture and Training", "content": "The Tiny-toxic-detector is designed as a compact transformer-based model optimized for toxicity detection. Its architecture is characterized by a balance between efficiency and performance, making it suitable for deployment in resource-constrained environments."}, {"title": "Overview", "content": "The model comprises 4 transformer encoder layers, each equipped with 2 attention heads. The embedding dimension is set to 64, and the feedforward layer has a dimension of 128. These parameters contribute to the model's overall efficiency while retaining the ability to capture complex patterns in text data."}, {"title": "Components", "content": "The architecture consists of the following main components:\n\u2022 Embedding Layer: The model begins with an embedding layer that converts input tokens into dense vectors of size 64. This is followed by a positional encoding that adds information about the position of each token in the sequence to help the model understand token order.\n\u2022 Transformer Encoder: The core of the model is built upon a stack of 4 transformer encoder layers. Each layer includes multi-head self-attention mechanisms with 2 attention heads, allowing the model to focus on different parts of the input sequence. The feedforward dimension of 128 helps in transforming the representations learned by the attention heads.\n\u2022 Dropout and Linear Layers: To prevent overfitting and improve generalization, dropout is applied to the embeddings and the output of the transformer layers. Finally, a linear layer reduces the dimensionality of the transformed embeddings to a single output value, which is then passed through a sigmoid activation function to produce the probability of the input text being toxic or not."}, {"title": "Training Data", "content": "What really sets the Tiny-toxic-detector model apart is the fact that the model has no generic pre-training. It was trained using a combination of multiple datasets: a publicly available dataset such as the 'Jigsaw toxic classification' [5] dataset and a closed-source dataset with synthetic labels. Finally, we overfitted the model on a closed-source dataset with human-generated labels that established better generalization. As all samples have been labeled, there is no pre-training. To ensure the quality of our training data, we compared all used datasets against established benchmarking datasets such as the test-set of the 'Jigsaw toxic classification challenge' [5] to check for data contamination. No direct contamination was detected.\nThe training process was iterative. For each version of the model, we conducted benchmarking to identify situations where additional training was needed. This iterative approach helped us address specific issues such as unintended biases and improve the model's generalization by overfitting on certain situations.\nA notable example is that there is a lot more leeway for comments talking about their own sexuality as opposed to the sexuality of someone else. The former is often factual and unfortunately the latter is often meant as an insult, which other attempts at smaller-parameter toxicity detection models such as the 'toxic-comment-model' based on DistilBert do not understand and as a result mostly classify either situation as toxic.\nDue to the constraint size of the model, carefully training the model was of vital importance to ensure strong generalization."}, {"title": "Operational Details", "content": "During the forward pass, the input tokens are first embedded and combined with positional encodings. This combined representation is processed through the transformer encoder layers, which apply self-attention and feedforward operations. The final output from the transformer layers undergoes global average pooling to aggregate information across the sequence, followed by dropout and a linear transformation to produce the final prediction.\nThis simplistic and streamlined architecture, with its compact size of 2.1 million parameters, achieves competitive performance on toxicity detection benchmarks while maintaining computational efficiency."}, {"title": "CO2 Emission Related to Experiments", "content": "Experiments were conducted using a private infrastructure, which has a carbon efficiency of 0.479 kgCO2eq/kWh. A cumulative 12 hours of computation was performed on hardware of type RTX 3090 (TDP of 350W).\nTotal emissions are estimated to be 2.01 kgCO2eq of which 20 percents were directly offset.\nEstimations were conducted using the MachineLearning Impact calculator.\nThis reiterates the potential carbon emission savings when training small task-specific models such as Tiny-toxic-detector compared to relatively larger pre-trained models such as the well-known distilgpt2 model which contains 88 million parameters that emitted 149.2 kg eq. CO2 during training."}, {"title": "Usage and Limitations", "content": "Below, we outline the intended usage, potential applications, and limitations of the Tiny-toxic-detector."}, {"title": "Intended Usage", "content": "The Tiny-toxic-detector is designed to classify comments for toxicity. It is particularly useful in scenarios where minimal resource usage and rapid inference are essential. Key features include:\n\u2022 Low Resource Consumption: With a requirement of only 10MB of RAM and 8MB of VRAM [1], this model is well-suited for environments with limited hardware resources.\n\u2022 Fast Inference: The model provides high-speed inference, as demonstrated in Table 2. The Tiny-toxic-detector significantly outperforms larger models on CPU-based systems. Due to the overhead of using GPU inference, small models with a relatively small number of input tokens are often faster on CPU. This includes the Tiny-toxic-detector."}, {"title": "Potential Use-Cases", "content": "The Tiny-toxic-detector can be effectively applied in various contexts, including:\n\u2022 Social Media Monitoring: Automatically flag or filter toxic comments on platforms like forums, social media, or review sites to improve user experience and maintain community standards.\n\u2022 Content Moderation: Assist content moderators by pre-screening user-generated content, highlighting potentially toxic comments for further review.\n\u2022 Customer Support: Analyze customer interactions in support systems to identify and address toxic or abusive language, improving the quality of customer service.\n\u2022 Educational Platforms: Monitor student interactions in online educational environments to prevent and manage toxic behavior.\n\u2022 Gaming Communities: Enhance player experiences by detecting and managing toxic language in online gaming forums and chats."}, {"title": "Limitations", "content": ""}, {"title": "Training Data", "content": "The Tiny-toxic-detector has been trained exclusively on English-language data, limiting its ability to classify toxicity in other languages."}, {"title": "Maximum Context Length", "content": "The model can handle up to 512 input tokens. Comments exceeding this length are not in the scope of this model. While extending the context length is possible, such modifications have not been trained for or validated. Early tests with a 4096-token context resulted in a performance drop of over 10% on the Toxigen benchmark[4]."}, {"title": "Language Ambiguity", "content": "The Tiny-toxic-detector may struggle with ambiguous or nuanced language as any other model would. Even though benchmarks like Toxigen[4] also evaluate the model's performance with ambiguous language, it may still misclassify comments where toxicity is not clearly defined."}, {"title": "Conclusion", "content": "In this work, we presented Tiny-toxic-detector, a highly efficient transformer-based model specifically designed for detecting toxic content. Despite its compact size of just 2.1 million parameters, the model demonstrated robust performance, achieving competitive accuracy on widely recognized datasets such as ToxiGen and the Jigsaw Toxic Comment Classification Challenge. This performance underscores the potential of Tiny-toxic-detector for deployment in environments where computational resources are limited.\nThe success of this model highlights the importance of developing task-specific, resource-efficient solutions for content moderation. By striking a balance between performance and efficiency, Tiny-toxic-detector addresses the growing need for scalable, sustainable AI systems in the fight against online toxicity.\nFuture research could focus on further optimizing the model through techniques such as quantization and knowledge distillation to minimize overreliance on specific words. Additionally, exploring the model's adaptability to other languages and longer contexts could extend its applicability, ensuring it remains a versatile tool for toxic content detection across diverse platforms and environments."}]}