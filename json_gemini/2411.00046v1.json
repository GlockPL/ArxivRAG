{"title": "CurateGPT: A flexible language-model assisted biocuration tool", "authors": ["J Harry Caufield", "Carlo Kroll", "Shawn T O'Neil", "Justin T Reese", "Marcin P Joachimiak", "Harshad Hegde", "Nomi L Harris", "Madan Krishnamurthy", "James A McLaughlin", "Damian Smedley", "Melissa A Haendel", "Peter N Robinson", "Christopher J Mungall"], "abstract": "Effective data-driven biomedical discovery requires data curation: a time-consuming process of finding, organizing, distilling, integrating, interpreting, annotating, and validating diverse information into a structured form suitable for databases and knowledge bases. Accurate and efficient curation of these digital assets is critical to ensuring that they are FAIR, trustworthy, and sustainable. Unfortunately, expert curators face significant time and resource constraints. The rapid pace of new information being published daily is exceeding their capacity for curation.\nGenerative Al, exemplified by instruction-tuned large language models (LLMs), has opened up new possibilities for assisting human-driven curation. The design philosophy of agents combines the emerging abilities of generative Al with more precise methods. A curator's tasks can be aided by agents for performing reasoning, searching ontologies, and integrating knowledge across external sources, all efforts otherwise requiring extensive manual effort.\nOur LLM-driven annotation tool, CurateGPT, melds the power of generative Al together with trusted knowledge bases and literature sources. CurateGPT streamlines the curation process, enhancing collaboration and efficiency in common workflows. Compared to direct interaction with an LLM, CurateGPT's agents enable access to information beyond that in the LLM's training data and they provide direct links to the data supporting each claim. This helps curators, researchers, and engineers scale up curation efforts to keep pace with the ever-increasing volume of scientific data.", "sections": [{"title": "Introduction", "content": "Databases and knowledge bases require high-quality curation to ensure integrity, validity and FAIRness. Curation is the task of finding and contextualizing diverse information into a structured form suitable for populating these knowledge resources. Curation is a time-consuming, painstaking process, so there is interest in applying machine learning (ML) and natural language processing (NLP) techniques to assist with the labor. However, the efficacy of these approaches is limited by the unavailability of domain-specific training data and an inability to incorporate domain knowledge (1). ML approaches have yet to be integrated into production curation systems, save for PubTator's biomedical literature annotation (2) and for certain circumscribed tasks such as literature triage (3\u20135). These automated approaches can help to address the ever growing volume of scientific data and publications: PubMed alone adds roughly one million new citations to its index each year (as per the MEDLINE Citation Counts as of January 2023; https://www.nlm.nih.gov/bsd/medline_cit_counts_yr_pub.html).\nDependability is crucial for curated data resources, so ML-based curation support approaches without demonstrable reliability are of limited value. Curation workflows based around ML-driven tools pose definite risks: they may miss relevant details, introduce factual inaccuracies, or even generate fully unsupported data relationships, all without sufficient provenance details to identify the source of error. To identify these issues, curators must sift through the automated predictions - often a more time-intensive task than curating directly from source. The current consensus is that, on their own, traditional NLP approaches are not a good substitute for manual curation, particularly in the biomedical domain (6,7). NLP methods have a substantial history of assisting with curation (8\u201310). The emergence of large language models (LLMs), such as GPT-4, Claude, Gemini, and Llama3, has introduced potential solutions for the limitations of NLP. LLMs have highly general information processing capabilities and do not require retraining for each task or use case. The advantages LLMs offer over more traditional NLP approaches, including their flexibility and rapid adaptability to varied tasks (11), may render them more feasible for literature curation applications. Researchers developing new sets of annotated biomedical text have found LLMs to be particularly efficient (12), as have medical informatics researchers in applying the models to clinical decision support (13). Even so, evaluating LLM output continues to be a job best performed by human curators.\nBiomedical curation is often performed using customized user interfaces with few avenues for integrating Al assistance. We hypothesized that the creation of an Al-powered integrated development environment (IDE) could transform the practice of biocuration. We have developed the prototype of such an IDE, called CurateGPT\n(https://github.com/monarch-initiative/curate-gpt). This system supports curation of an extensive variety of data types, ranging from tabular data to nested schemas common to biomedical knowledge bases (e.g., UniProt (14) or the Alliance of Genome Resources (15)) and ontologies (e.g., Gene Ontology (16) or Cell Ontology (17)). CurateGPT is based around a flexible document store and vector database with dynamic access to online resources, including Wikipedia, PubMed, and other NCBI databases. It integrates several generative Al engineering techniques: Retrieval Augmented Generation (RAG), which allows for the most relevant"}, {"title": "Methods", "content": "The CurateGPT architecture centers around a collection of independent agents that can be invoked in three ways: via APIs; from the command line; and via a prototype graphical interface. Each agent is the combination of a function and an LLM (or more specifically, the interface to an LLM). These agents are described below. Further technical detail is provided in the System Architecture section."}, {"title": "CurateGPT Agents", "content": "CurateGPT provides agents to enable the following tasks:\n\u2022 Search - find entries within an ontology or document collection relating to a text query.\n\u2022 Chat - query a knowledge base with natural language.\n\u2022 Curate - generate a new ontology class or an object fitting a provided data model.\n\u2022 Extract transform unstructured text into structured knowledge.\n\u2022 CiteSeek - find citations for a claim, based on the contents of a knowledge base.\n\u2022 Match - identify potential mappings for a term within an ontology.\n\u2022 Bootstrap create a seed schema and knowledge base from a description.\nCombinations of these agents can power a curation workflow. For example, the process of isolating specific claims from new literature may start with the Curate command and then use the CiteSeek command to obtain additional sources for context.\nAll CurateGPT interfaces offer a variety of settings, including:\n\u2022 Which LLM to use. CurateGPT agents are largely model architecture-agnostic and will work with a variety of LLM interfaces and local implementations, including open models such as Llama3 (21) and its successors.\n\u2022 Which collection of input knowledge to operate on and source examples from. This may be an ontology, a controlled vocabulary, or some other structured data resource.\n\u2022 What collection to use for background sources, such as PubMed or Wikipedia."}, {"title": "Search", "content": "CurateGPT's Search agent allows curators to find values within their data matching a text query. Search results can be displayed as a table or using a reduced dimensionality visualization such as PCA, t-SNE, or UMAP (in the graphical interface, this is titled Cluster Search). Though search is not the primary function of CurateGPT, search underpins many operations in the curation process. Either pre-loaded static collections can be searched, or dynamic search can be performed on wrapped remote resources accessed via API, such as PubMed or Wikipedia. The results of searches can be selected to be placed in a cart, for use either as (a) curation/refinement or (b) background knowledge in other curation tasks."}, {"title": "Chat", "content": "The CurateGPT Chat agent allows curators to ask natural language questions about one or more data collections. The Chat agent will find the most relevant objects relating to the question (specifically, those with the least difference between embeddings of the query and the collection data), then include these as background context. The model will be prompted to cite these objects in the response. The response is then parsed, and matched back to the original input objects. The answers are provided with inline citations, hyperlinked to specific sources further down on the page (see example in Table 1). Compared to direct interaction with an LLM through its chat interface, the Chat agent provides two major advantages: (a) it derives information from custom input documents, including documents not present in the LLM training data; and (b) it provides identifiers for relevant data, giving the user additional opportunities to verify the LLM response. As part of the chat prompt, CurateGPT provides the LLM with relevant information from the custom input, so answers will pertain to those additional documents. Further technical details are discussed in the System Architecture section below."}, {"title": "Curate", "content": "The Curate agent supports CurateGPT's ability to generate new ontology entries and structured data elements directly from text and the parameterized knowledge within an LLM. The Curate agent generates an object to add to a collection given some quantity of seed information. The seed information is provided as separate, distinct values for a single entry, but may be quite incomplete: a single label or description alone is enough to start building a more extensively described entry. In an ontology such as the Human Phenotype Ontology (HPO) (22), for example, providing the Curate agent with a class label not already in the ontology will yield a new class object that includes an identifier in the expected format, a definition similar in structure to others in the ontology, and relationships connecting the new class to others."}, {"title": "Extract", "content": "The Extract agent provides example-based extraction (also known as structured object autocomplete) from raw text. Unlike the Curate agent, which creates a new ontology object given an incomplete set of initial values, the Extract agent requires no separation of the input text into distinct components. The input is parsed directly into a structured form corresponding with the target collection, with fields populated using background knowledge wherever possible. Additional rules may also be passed to the LLM to guide generation of the extracted object. An example is shown in Table 3."}, {"title": "CiteSeek", "content": "The CiteSeek agent is a central component of CurateGPT. It supports an essential task in curation: retrieving citations providing evidence for assertions. Identifying supporting literature is of particular value to building and curating relationships within ontologies. The CiteSeek agent is particularly powerful because it retrieves supporting information from external sources including PubMed and Wikipedia rather than the LLM alone (24,25). On their own, LLMs may frequently provide incomplete support for provided claims. The resulting citations are therefore much less likely to be incomplete or purely generated results and are much more likely to correspond to extant publications. The agent provides human-readable context for each, limiting the time curators must spend on hunting for pertinent text in each reference.\nRetrieving citations with the CiteSeek agent works well when retrieving records through the PubMed API. For example, if we use CiteSeek with the PubMed API (see Table 4A), the agent first uses the LLM to identify specific search terms, then constructs a query to retrieve relevant literature records through the API (this is a crucial preprocessing step; without it, the example shown in the table yields no results). It then assembles a prompt instructing the LLM to examine the records and produce structured representations of their support. This may include instances where a statement from the literature supports, partially supports, refutes, finds no evidence for, or disagrees with the details of a statement. Alternatively, the input may be provided as a structured relationship (Table 4). When complete, the agent returns a textual summary describing each citation along with bibliographic details for the source supporting this claim."}, {"title": "Match", "content": "Mapping terms between ontologies and controlled vocabularies is a frequently necessary task when curating and applying these resources. It is very time-consuming for curators, as they must link identifiers that denote entities or concepts with varying degrees of abstraction and similarity. We have previously outlined some of these challenges in our paper on Simple Standards for Sharing Ontology Mappings (SSSOM) (26). CurateGPT supports mapping curation through the Match agent, which takes the label of a concept to match as input, queries the data collection for the best matches by vector distance, then queries the LLM regarding the best options out of the set of potential matches. For example, searching for the best matches for the text \u201cround red fruit with many seeds in it\u201d across the Food Ontology (FOODON) yields ten results by default, including \u201csweet red bell pepper\"\n(FOODON:00003485), \u201cred currant\u201d (FOODON:00003766), and \u201cred raspberry\u201d\n(FOODON:00003729). The Match agent selects \u201cred raspberry\u201d as the best match. This works well for cross-lingual matches as well: instructing the Match agent to search for matches for the Polish word \u201cw\u0105troba\u201d in OBI yields the class for the corresponding concept with its English label, \u201cliver\u201d (imported from the Uberon anatomy ontology, UBERON:0002107)."}, {"title": "Bootstrap", "content": "The Bootstrap agent facilitates generation of the initial version of a knowledge base. Starting with a brief description, the agent uses the LLM to produce a corresponding LinkML schema."}, {"title": "System Architecture", "content": "The high-level architecture of CurateGPT is shown in Figure 1. In brief, CurateGPT provides direct access to LLM-driven tools for curating structured data from collections of more loosely-structured data and text. The following sections detail how CurateGPT indexes knowledge and supports curation-related tasks."}, {"title": "Integration with an LLM tooling ecosystem", "content": "CurateGPT is part of a thriving and evolving ecosystem of LLMs-powered software tools for supporting data curation tasks. Much of CurateGPT's ability to generate ontology entries (e.g., by building entries based on content from one or more research manuscripts) is built upon our previously described method, Dynamic Retrieval Augmented Generation of Ontologies using Al (DRAGON-AI; (27)). Recognizing the need to associate LLM-extracted information to a consistent set of identifiers, we also constructed the Structured Prompt Interrogation and Recursive Extraction of Semantics (SPIRES) method (18) for performing zero to few-shot information extraction coupled with ontology-based grounding. SPIRES may be used through our OntoGPT software (https://github.com/monarch-initiative/ontogpt) or as part of TALISMAN, a pipeline for gene set summary generation (28). CurateGPT acts as infrastructure for integrating the above methods. We have also found that LLMs are effective tools for querying the structured data relationships within knowledge graphs: our Phenomics Assistant allows biocurators and researchers to use natural language to query a multifaceted biomolecular knowledge graph (29).\nThe above tools, including CurateGPT itself, do not rely on any specific LLM architecture or infrastructure. They are generalizable to models beyond the popular OpenAI GPT models, including Llama3 and other large open-source models, as well as their corresponding embeddings. These models may be accessed and used with CurateGPT through the Ilm (https://github.com/simonw/llm) and litellm (https://www.litellm.ai/) frameworks; the former provides direct access to OpenAI API and a series of plugins for interfacing with other models (e.g., GPT4All models) the latter provides a proxy interface (see Fig. 1A) to connect to a variety of local and remote LLM endpoints using the same parameters as the OpenAl API uses."}, {"title": "Retrieval augmented generation using structured objects", "content": "LLMs are highly flexible in their abilities, including complex question-answering, and the capability to shape responses into appropriate data structures. However, without specific tuning or retraining, LLMs will not include knowledge from recently published or domain-specific literature. LLM accuracy may suffer regarding topics the model received limited exposure to during training (30).\nA common approach to address these concerns is Retrieval Augmented Generation (RAG) (31). By including the most relevant pieces of information as in-context examples when asking a question of an LLM, RAG can noticeably improve accuracy and relevance of results while reducing the likelihood of LLM hallucination. RAG can be thought of as a kind of dynamic \"on-the-fly training\u201d of otherwise fixed models, in which a custom source of documents is indexed in advance, and then queried using semantic similarity to retrieve the most relevant documents for a given query, which are then presented to the model as part of background context.\nCurateGPT implements specialized forms of RAG in which combined collections of structured knowledge, unstructured text, and external data may be used to supplement LLM-driven"}, {"title": "Generative-Al-friendly object store", "content": "At the core of CurateGPT is a data storage system capable of managing a variety of object types. Here, an \u201cobject\u201d is a tree-like structure, equivalent to a JSON object. This allows for flexibility in adapting to different use cases and data, whether structured as tables of measurements, sophisticated nested associations (e.g., the GO-CAM structure (32) used by the Gene Ontology), or documents representing ontology terms. CurateGPT can interface directly with databases as described below. Future work will integrate CurateGPT with the LinkML-Store framework (https://github.com/linkml/linkml-store) for using a broader variety of file systems and data structures.\nCurateGPT assists in the generation of meaningful, structured data by searching for relevant text and other objects in background data. In CurateGPT, all objects are stored in a database supporting vector search; the framework currently supports ChromaDB and the DuckDB Vector Similarity Search (VSS) extension. A vector database creates embeddings (i.e., low-dimensional statistical representations) of each object stored within it, allowing for semantic search of the store. This is achieved by first creating an embedding of the query (using the same method as is used for the index), and then calculating the nearest N objects using vector-based similarity. CurateGPT uses cosine similarity by default but vector search backends support other distance metrics such as Euclidean distance or negative inner product. Queries may be simple terms, complex questions, long narrative text, or even serialized JSON objects. Search results are re-ranked using a Maximal Marginal Relevance (MMR) approach (33). Our goal is to ensure that a diverse set of terms relevant to the search are included, particularly when multiple interrelated ontologies are involved. Further details on this approach are provided in our description of the DRAGON-Al method.\nCurateGPT is integrated with a broader data ecosystem. It is capable of retrieving ontologies directly from the Ontology Access Kit (OAK) (https://github.com/INCATools/ontology-access-kit), a feature providing rapid access to current versions of all resources in the OBO Foundry collection (34). To facilitate easy exchange of embeddings, CurateGPT supports a workflow for sharing embeddings and associated metadata. Metadata for embeddings are represented in Vector Embedding Named Object Model indeX (venomx) format, a LinkML-based representation of metadata features such as embedding model, dataset that was embedded, and date of creation (https://github.com/cmungall/venomx). CurateGPT supports a command to upload embeddings and metadata to HuggingFace to enable their sharing and re-use."}, {"title": "Integrating diverse resources", "content": "The CurateGPT backend supports retrieval of structured data from external resources. This includes both loading of local data through an Extract-Transform-Load (ETL) pipeline and retrieval through external APIs. CurateGPT includes a loader capable of loading most ontologies; even large ontologies like ChEBI (35) can easily be ingested in their entirety as part of an initialization pipeline. Ingesting larger resources such as PubMed or Wikipedia in their entirety is more resource intensive (and in this case, may require numerous calls to retrieve embeddings from an LLM) and is instead accomplished through wrappers that call their respective APIs. In these cases, CurateGPT performs relevancy-ranked searches using the resource's public API, then stores the results in its local vector store cache, before performing a more refined embedding-based semantic search. The CurateGPT data retrieval framework includes a novel LLM-powered enhancement: search queries for external APIs are passed to an LLM with a prompt instructing the model to decompose the query into a set of search terms. These search terms are then parsed and fed to the API. See Table 6 for further examples of resources accessible by CurateGPT and usable with its curation support agents."}, {"title": "Results", "content": null}, {"title": "Example workflow: curation and literature retrieval", "content": "Translating observations reported in scientific literature directly into structured, searchable knowledge yields indispensable collections of knowledge, but it remains one of the most laborious tasks performed by biocurators (6,36). CurateGPT can rapidly accelerate this process. For example, a biocurator seeking to add new terms about killifish (a large group of small, freshwater fish, some species of which are used as models of aging and neurodegeneration (37)) to Vertebrate Breed Ontology (VBO) (38) may use the following workflow to find and interpret source literature. To set up the necessary resources, the curator first sets their OpenAl API key, then obtains and indexes a recent version of VBO with the command \u201cmake ont_vbo\". To save time, this process downloads a pre-built sqlite representation of the ontology.\nOnce the new database is created, the curator makes it accessible to the CurateGPT interface (\"cp -r stagedb/* db/\") and opens the Streamlit app (\u201cmake app\"). The graphical interface is similar to that shown in Figure 2.\nThere is not yet an entry in VBO with the label \u201ckillifish\u201d, but the curator is curious about whether similar types of fish may be present in the ontology, so they begin with the Search command. They set their collection to the newly created vbo_new and specify their search string as \u201ckillifish\u201d. The results indicate no exact results, but general entries for other types of fish are present in VBO (e.g., \u201cRainbow trout breed\"). The curator then uses the Curate agent to create a new entry for \u201cKillifish breed\u201d in accordance with the VBO structure and patterns. They enter \"Killifish breed\u201d as the label value and do not generate background, though they do include the additional instruction to \u201cInclude a definition field with a brief description.\u201d (At this point, they may also use another resource such as the NCBI Taxonomy for background, which can help to provide appropriate external identifiers.) The resulting object generated by CurateGPT is shown in Table 7."}, {"title": "CurateGPT can use design pattern documents to refine suggested terms", "content": "Ontologies and knowledge bases frequently include documentation for curators to help guide them in making editorial choices that are consistent with the patterns in the knowledge base as a whole. This documentation might be heavily natural language oriented; or it may be structured according to a computationally-readable design pattern data model, such as is the case for Dead Simple Ontology Design Patterns (DOSDPs) (39). CurateGPT can load either natural language or structured documentation as background knowledge to generate ontology entries better aligned with expectations.\nAs an experiment, we selected an existing term from the Mondo disease ontology (40) to determine if CurateGPT could generate similar versions of the term when provided or not provided with editor documentation. This term, \u201cbladder urachal squamous cell carcinoma\u201d, lacked a definition but otherwise had several relationships within Mondo (e.g., it is a subclass of \"bladder squamous cell carcinoma\"). Results are shown in Table 8. The editor documentation for Mondo includes highly structured DOSDP patterns. We found it was necessary to to pass in additional instructions on how to use the highly technical structures of DOSDPs: \u201cuse design pattern documents to ensure definition is populated. Use the \"def\" pattern for definitions\". These instructions may be provided to CurateGPT through the Additional Instructions input in the Extract agent, which is analogous to instructing a human curator in how to make use of these patterns. With this additional instruction, CurateGPT was able to generate the missing definition according to the pattern. However, the presence of the design patterns also confused the model and caused it to generate structures that were not consistent with the overall data model loaded in the store. This is likely because the DOSDPs follow an OWL functional style syntax and data model, whereas we loaded in a more OBO-like model for Mondo. It is likely that if the underlying data model used to load Mondo were more OWL-like then these would be more effective. More research is needed here to determine the best ways to use design patterns and editor documentation \u2013 this may simply involve providing a textual description of the data model mapping, or using CurateGPT to infer these mappings."}, {"title": "Discussion", "content": "CurateGPT is not designed or intended to replace human curators. Its current agents can supplant some of the more laborious tasks in a curation workflow, such as extracting specific types of information from scholarly texts and identifying sources of supporting details. These tasks are currently supported by a massive array of well-established tools: curators use the search interfaces for structured resources such as PubMed, OMIM, or UniProt to accomplish both of the above tasks and more. CurateGPT's value is in bypassing some of the more frequent questions asked in the course of accomplishing these tasks (e.g., \u201cAre there already"}, {"title": "Ensuring a human-centric curation philosophy", "content": "We do not believe that Al can or should replace manual curation. We have a deeply human-centric view of curation, informed by our own professional work and experience in curating knowledge resources, ontologies, and repositories, as well as developing frameworks, interfaces, and algorithms to support these activities. Al can be a powerful assistant, one that can be personalized and interacted with while efficiently supporting the expertise and intuition of curators. The technical capabilities of Al extend well beyond curation support systems that simply present tables of scored predictions. We are encouraged by studies that demonstrate the ability of LLMs to not only enhance productivity, but to help level the playing field in knowledge-centric fields (41,42). Similarly, LLM-driven systems such as GitHub Copilot have seen concrete success in production systems as coding assistants for software developers. These tools offer a crucial feature in that developers can quickly and easily decide whether to accept or reject generated suggestions. Evaluating knowledge is generally more complicated than evaluating code, but both processes may benefit from an effortless interplay between human evaluator and generative process. We have found that making evaluation of generated text as easy as possible is of particular value when combined with RAG approaches, as the final product is a result of at least three sources of knowledge: the human curator, the LLM, and structured external data."}]}