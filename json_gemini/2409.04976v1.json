{"title": "HYDRA: Hybrid Data Multiplexing and Run-time Layer Configurable DNN Accelerator", "authors": ["Sonu Kumar", "Komal Gupta", "Gopal Raut", "Mukul Lokhande", "Santosh Kumar Vishvakarma"], "abstract": "Deep neural networks (DNNs) offer plenty of challenges in executing efficient computation at edge nodes, primarily due to the huge hardware resource demands. The article proposes HYDRA, hybrid data multiplexing, and runtime layer configurable DNN accelerators to overcome the drawbacks. The work proposes a layer-multiplexed approach, which further reuses a single activation function within the execution of a single layer with improved Fused-Multiply-Accumulate (FMA). The proposed approach works in iterative mode to reuse the same hardware and execute different layers in a configurable fashion. The proposed architectures achieve over 90% of power consumption and resource utilisation improvements of state-of-the-art works, with 35.21 TOPSW. The proposed architecture reduces the area-overhead (N-1) times required in bandwidth, AF and layer architecture. This work shows HYDRA architecture supports optimal DNN computations while improving performance on resource-constrained edge devices.", "sections": [{"title": "I. INTRODUCTION", "content": "Deep neural networks, or DNNs, have gained popularity in various fields, including science, medicine, and agriculture, thanks to advances in computer technology. DNNs are highly effective for tasks involving cognition, learning, and pattern recognition, but substantial hardware and power requirements challenge their deployment at the edge. The core components of DNNs are resource-intensive, such as the fused multiply-accumulate (FMA) units that each neuron applies weighted input sums and non-linear activation functions. Traditionally, DNN computation is performed in the cloud, which introduces latency due to transmitting results back to edge devices. Deploying DNNs directly on edge devices can reduce latency, save bandwidth, and improve data security; however, limited battery power, storage, and computing resources pose significant challenges for this approach. The increasing requirement for data processing in DNNs, composed of multiple layers of interconnected neurons, exacerbates these challenges. The extensive computation must be handled by edge devices that increase the complexity in hardware and power consumption, which makes it very challenging to integrate DNNs at the edge. The edge devices enable the DNN operations at the edge with constraints like response time requirements, privacy concerns, and data transmission costs, which are very crucial for the AI applications. The design of a 1D array architecture with fused multiply-accumulate (FMA) units followed by an activation function , fused multiply-accumulate is a promising solution to these problems. Existing methods for DNN implementation on such architectures either involve implementing all layers simultaneously, which is energy-intensive and increases area, or implementing layers individually, which conserves energy but can introduce higher latency.\n\nThe article proposes a novel architecture of a DNN accelerator that executes neural networks in a layer fashion within resource-constrained environments at Edge AI. The key contributions of this work are:\nLayer-Multiplexed DNN accelerator: The proposed DNN accelerator allows configuration for any depth by defining the top design parameters. The architecture uses a single layer to compute DNN sequentially by reusing the same underlined hardware. The benchmark configuration of 64:32:32:10 is evaluated and validated on the FPGA board Virtex VC-707 and with software analysis that interprets the inference accuracy results.\nRuntime Configurable Layer Architecture: An area-optimized, layer multiplexed FC layer is presented, which reuses a single AF with the function of PISO that parallelism the data flow and multiplexed it. The impact of saving hardware resources by (n-1) AF in each layer of the layer reused DNN configuration without the throughput loss and performance-improved Fused-Multiply-Accumulate is analysed.\n\nThe rest of this paper is organised as follows: Section 2 gives Related Work and Motivation. Section 3 describes the proposed design for the hardware-reuse technique and layer reuse for the neural network. The experimental setup is described in Section 4. Section 5 explains the performance analysis of the proposed architecture and corresponding neural network. Section 6 concludes in brief about the above work and interprets the future directions."}, {"title": "II. RELATED WORK AND MOTIVATION", "content": "Traditional DNN accelerators suffer from underutilising hardware underlined. Though the two-dimensional array accelerators are more fruitful for convolution and matrix multiplication, in practice, the hardware suffers from inefficient mapping techniques for ANN/RNN/fully connected layers. Thus, creating a 1-D layer based on FMA units is necessary to fit these layers more efficiently. However, it can also be noted that FMA is utilised for N*N clock cycles for N*N kernel computation, while AF is utilized only once. Hence, the infrequently used hardware can be reduced while providing an area-throughput tradeoff. Further, it could be noted that the execution of these layers is more sequential. Hence, it is more beneficial to have layer-reused architecture than fully parallel architecture, considering the resource-constrained edge environment. The proposed architecture addresses the issue by mapping different layers onto the same hardware, thus enhancing hardware efficiency and configurable architecture through data reuse. AF multiplexing further enhances the layer hardware, reducing static power consumption. Researchers have also looked at different DNN mapping methods, sparsity exploitation, and data reuse, which talk about the pros and cons of area, latency, and power. The smaller architectures can utilize fewer resources, though unsuitable considering edge devices' quick response time. Thus, instead of tiling the network over a small FMA architecture, we suggest providing at least the minimum hardware to execute a single layer simultaneously, reducing intra-dependencies in data movement. This approach further reduces the complexity of the control engine and produces a simpler, more energy-efficient architecture."}, {"title": "III. PROPOSED WORK", "content": "We designed a 1D array of 64 FMA units to implement feed-forward neural networks efficiently. The current implementation targets a 64:32:32:10 network configuration comprising four fully connected layers. The architecture includes an input layer with 64 FMA and two hidden layers (each utilising 32 FMAs) and concludes with a 10-class output layer. The accelerator will be fed 196 pixels, i.e., 14x14 (half-folded version of MNIST images.) The edge-AI hardware consists of 90% of multiplication and addition operations, making FMA the central block. The FMA computes the product of filter maps with activation inputs and is added with bias values. A clock cycle could be saved here by preloading the bias value into the accumulator."}, {"title": "A. Layer Reuse architecture", "content": "Deploying deeper neural networks with fully parallel architectures is often impractical due to substantial hardware resource demands. This research introduces a hardware-multiplexed, design-parameterised architecture for resource-efficient DNN implementation. Utilizing an enhanced layer-multiplexed architecture, our method allows for realising any neural network size by reusing a single layer. The architecture improves overall system throughput by employing multiplexing to select a necessary configuration. The design, written in Verilog, is implemented on FPGAs and synthesised for ASICS, offering modularity and scalability for various DNN sizes. Control units that handle design parameters based on configuration settings manage the reuse of a single layer. This control unit is critical, as it manages the data flow between layers, from input to intermediate stages, and coordinates the start and finish signals for each layer. It also assigns weights and inputs from previous layers. After processing every layer, the control unit signals the device that the final result is ready for transmission by sending it an \"ANN done\" signal."}, {"title": "B. Activation Function Reuse within Layer of Optimised FMA", "content": "Examining a feed-forward DNN with the model number 64:32:32:10 will help you calculate the computation time with the suggested DNN architecture. To address this inefficiency, we have removed the AF from each neuron and connected it to a parallel-in-serial-out (PISO) configuration, which reduces hardware overhead per neuron and the overall area at the expense of some latency. With 64 neurons in the first layer and 196 inputs per neuron, the FMA units need 196 clock cycles to calculate the weighted sum. N FMAs are connected to one reconfigurable AF through PISO in the proposed layer. The proposed AF reuse approach leads to hardware resource savings of (N-1) AF. There is a viable tradeoff between throughput and chip area. The possible challenges may arise in control complexity with the same approach. When applied to a layer-sized network, this method can reduce hardware usage by approximately ten times. While the PISO effectively reduces data throughput, reusing a single AF increases area gain; this small increase in complexity yields significant area savings.\n\nTypically, the data is fed into input and weight banks using serial multiplexing. After calculating FMAs in parallel, two additional clock cycles are required to process output from the first FMA through the PISO and AF. The first feature map's convolution output is available at the 198th clock cycle. Layer one would take an extra 64 clock cycles to get each output serial and would be stored, thus reducing the bandwidth at the output. The next hidden layer will process the outputs with 32 FMAs for 64 inputs and 66 clock cycles. The layer output would be available after 32 clock cycles in serial manner. Ten clock cycles are needed for the FMA units of the output layer, and additional cycles are needed for the softmax in the output layer, which consists of four neurons with ten inputs. The FMA outputs must be identified before the execution of the layer and once the previous layer has been executed. The approach would help control the engine with easier reconfiguration and power gating for FMAs that are not in use. The mathematical modelling for timing analysis for computation clock cycles required for Fully Parallel architecture (Tp) and Layer Reuse (TR) is shown in equations 1 and 2. Here, L is the number of layers in the DNN model, and n(l) is the number of FMAs in the 1th layer.\n\n$T_p = \\sum_{l=1}^{L-1} n(l) + L - 1$                                                            (1)\n$T_R = \\sum_{l=1}^L n(l) + 2L \u2013 3$                                                               (2)"}, {"title": "IV. EXPERIMENTAL SETUP & PERFORMANCE EVALUATION", "content": "Neural networks are widely utilised in everyday applications, and several parameters determine the efficiency of the accelerator. As demonstrated in Table II and Fig. 4a, which offer a comparative analysis with alternative designs, the resource utilization of the FPGA is assessed using the VC707. The resource utilization on the FPGA was evaluated using the VC707, as shown in Table II and Fig. 4a, which provide a comparative analysis with alternative designs. The layer-reuse architecture and activation function (AF) multiplexing, which greatly reduces hardware requirements, are largely responsible for these improvements. Our design demonstrates a reduction in LUTs, slices, and power consumption by 15.4 times, 38.7 times, and 20 times, respectively, compared with the AF-reuse approach , which reuses a common AF across fully connected layers. Several other cutting-edge designs, such as Layer multiplexed, AF reused, and hardware-reused architectures , show that LUTs, slices, and power consumption can be reduced by roughly 10 times, 20 times, and 7 times, respectively.\n\nThe additional analysis to discuss the impact of Power-Area-Delay parameters varying with signed bit FMAs is presented in table III. For 5, 8, 16, and 32-bit FMAs, the reduction in Slice LUTs are 13%, 33%, 24%, and 15%, while that of Slice Reg./FF. are by 28%, 34%, 31%, and 30%, when compared with Xilinx IP. The PDP is improved up to 86 % at 8-bit precision, with a negligible impact on accuracy, as shown in Table 4b. The improved resource efficiency is linearly translated at the architectural level when the proposed custom ANN is applied to MNIST/CIFAR-10. Table II reports the resource comparison with SOTA architectures. Though the SOTA works report resource utilisation for models like VGG-16, MobileNetV2, and ResNet-50, it is also possible in our architecture to implement the same hardware; however, for simplistic implementation, Custom ANN is reported. Our proposed design achieves a noteworthy power efficiency of 35.21 GOPS per watt when operating at 100 MHz. The design highlights the synthesizable nature as no DSP blocks are utilised within. Overall, evaluating the suggested architecture using datasets such as MNIST and CIFAR-10 across various FMAs and bit precisions shows its scalability and flexibility."}, {"title": "V. CONCLUSION", "content": "This work presents HYDRA architecture, a hybrid data multiplexing and runtime layer-configurable DNN accelerator to enhance the performance within hardware constraints of executing DNNs at edge nodes. By utilizing an innovative layer-multiplexed approach and optimizing the FMA unit, substantial resource consumption is reduced, leading to hardware area reductions of up to 15 times, enhanced power efficiency, and minimal latency. The proposed architectures achieve over 90% of power consumption and resource utilisation improvements of state-of-the-art works, with 35.21 TOPSW. The outcomes highlight how well HYDRA supports resource-constrained edge devices' efficient DNN computations."}]}