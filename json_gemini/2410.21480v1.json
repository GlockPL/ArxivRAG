{"title": "AISCIVISION: A FRAMEWORK FOR SPECIALIZING LARGE MULTIMODAL MODELS IN SCIENTIFIC IMAGE CLASSIFICATION", "authors": ["Brendan Hogan", "Anmol Kabra", "Felipe Siqueira Pacheco", "Laura Greenstreet", "Joshua Fan", "Aaron Ferber", "Marta Ummus", "Alecsander Brito", "Olivia Graham", "Lillian Aoki", "Drew Harvell", "Alex Flecker", "Carla Gomes"], "abstract": "Trust and interpretability are crucial for the use of Artificial Intelligence (AI) in scientific research, but current models often operate as black boxes offering limited transparency and justifications for their outputs. Motivated by this problem, we introduce AISciVision, a framework that specializes Large Multimodal Models (LMMs) into interactive research partners and classification models for image classification tasks in niche scientific domains. Our framework uses two key components: (1) Visual Retrieval-Augmented Generation (VisRAG) and (2) domain-specific tools utilized in an agentic workflow. To classify a target image, AISciVision first retrieves the most similar positive and negative labeled images as context for the LMM. Then the LMM agent actively selects and applies tools to manipulate and inspect the target image over multiple rounds, refining its analysis before making a final prediction. These VisRAG and tooling components are designed to mirror the processes of domain experts, as humans often compare new data to similar examples and use specialized tools to manipulate and inspect images before arriving at a conclusion. Each inference produces both a prediction and a natural language transcript detailing the reasoning and tool usage that led to the prediction. We evaluate AISciVision on three real-world scientific image classification datasets: detecting the presence of aquaculture ponds, diseased eelgrass, and solar panels. Across these datasets, our method outperforms fully supervised models in low and full-labeled data settings. AISciVision is actively deployed in real-world use, specifically for aquaculture research, through a dedicated web application that displays and allows the expert users to converse with the transcripts. This work represents a crucial step toward AI systems that are both interpretable and effective, advancing their use in scientific research and scientific discovery.", "sections": [{"title": "1 INTRODUCTION", "content": "Until recently, meaningful interactions with Al models were largely restricted to researchers and prac- titioners, who accessed these models through technical interfaces, often for niche applications. But the emergence of Large Multimodal Models (LMMs) such as OpenAI's GPT (Achiam et al., 2023), Google's Gemini (Gemini-Team, 2024), and Meta's Llama (Touvron et al., 2023) has dramatically transformed the landscape. Now, both experts and the general public can converse meaningfully with AI, making these inter- actions a part of everyday life. This change highlights not only the rapid advancements in model capabilities but also introduces new standards for how we think about and interact with AI, as accessible, versatile, and personal assistants.\nHowever, while this transformation has enabled AI to serve as a general-purpose assistant across a wide range of topics (Kiros et al., 2014; Vinyals et al., 2015; Ramesh et al., 2021; Abdelhamed et al., 2024), it raises a critical question: can these models provide the same depth of expertise in highly specialized and impactful domains? In areas such as medicine, law, scientific research and scientific discovery, the need goes beyond general conversations, these fields demand models capable of deep, domain-specific reasoning (Lu et al., 2022; Mall et al., 2024). The general knowledge embedded in LMMs falls short of the nuanced expertise required for these specialized tasks, limiting their effectiveness where it matters most.\nFortunately, the large context windows of LMMs allow for flexible specialization via in-context learning. By providing rich prompts and context relevant to a particular task, LMMs can adapt to domain-specific requirements, a strategy that is driving exciting research in Retrieval-Augmented Generation (RAG) (Lewis et al., 2020; Khandelwal et al., 2020). RAG techniques enhance LMM predictions by retrieving task-specific examples, effectively refining the model's responses based on context and thereby specializing it for the task at hand. This approach is particularly valuable because the sheer scale and cost of training LMMs are often only feasible for large organizations. For most researchers, leveraging context is a practical way to harness the general knowledge of LMMs while also enabling them to excel at unique, specialized tasks (Soong et al., 2024; Zakka et al., 2024). One critical area for specialization is scientific image classification. The ability to adapt LMMs to these domains would be transformative, as it has the potential to democratize access to advanced analysis previously limited by the scarcity of domain expertise. By enabling AI to provide nuanced, explainable insights in specialized scientific tasks, we can significantly accelerate research and discovery, bridging the gap between powerful, general-purpose AI models and the pressing, domain-specific challenges faced by experts.\nTo address these challenges, we introduce AISciVision, a framework that adapts general-purpose LMMs to accurately classify scientific images while generating transparent, context-specific reasoning transcripts. AISciVision operates in a model-agnostic way, combining two key components: a Visual Retrieval- Augmented Generation system (VisRAG) and an interactive tooling AI agent that classifies images through dialogue. Users start by providing labeled training data, which is embedded into a feature space, where the system organizes positive and negative class examples separately. They also specify classification tools, ranging from basic image adjustments (e.g., contrast) to domain-specific operations like zooming into satel- lite imagery. During inference, AISciVision retrieves similar positive and negative image examples from the training set based on cosine similarity in the embedding space and uses them as context for the LMM's anal- ysis. The LMM then engages in multiple rounds of interactive analysis, using the specified tools to refine its understanding of the target image, before arriving at a final prediction.\nWe evaluate our method on three real-world scientific image classification datasets: detecting aquaculture ponds in satellite images (Greenstreet et al., 2023), diseased eelgrass (Rappazzo et al., 2021), and solar panels in satellite images (Lacoste et al., 2023). Our AISciVision framework outperforms both fully su- pervised and zero-shot methods while producing transcripts detailing the agent's reasoning. We deploy our framework as a web application for real-time scientific monitoring, where users can interact with the infer-"}, {"title": "2 RELATED WORKS", "content": "Our work builds on recent research in multimodal models, Retrieval-Augmented Generation, and interactive AI agents that leverage tools. We discuss and compare related work in this section, and distinguish our work as integrating these domains into a unified approach.\nMultimodal models in low-labeled data regimes Large Multimodal Models (LMMs) like CLIP (Rad- ford et al., 2021), GPT-4 (Achiam et al., 2023), LLaVa (Liu et al., 2023), and PaLM-E (Driess et al., 2023) have been demonstrated to understand and generate content across multiple modalities like text, visual, and audio. By building rich and general-purpose representations of inputs from large and diverse datasets, these LMMs demonstrate competitive zero- and few-shot capabilities on a wide range of tasks (Brown et al., 2020), such as natural language understanding (Kiros et al., 2014), image captioning (Vinyals et al., 2015), text-to-image generation (Ramesh et al., 2021), and image classification (Guillaumin et al., 2010; Abdel- hamed et al., 2024). Acosta et al. (2022); Moor et al. (2023); Wang et al. (2023) demonstrate that these capabilities are key for advancing research in domains where obtaining labeled data is costly and tedious, for instance, in scientific research. On the other hand, Lu et al. (2022); Mall et al. (2024) find that zero- and few-shot capabilities of general-purpose LMMs like CLIP do not suffice in scientific applications, where uti- lizing domain-specific information becomes important. Lu et al. (2022) find that Chain-of-Thought prompt- ing (Wei et al., 2022) improves question-answering performance, whereas Mall et al. (2024) demonstrate that a \"Vision Language Model\" explicitly trained on satellite images outperforms CLIP. Our framework AISciVision extends general-purpose LMMs to classify images in low-labeled data regimes like scientific applications. We incorporate Retrieval-Augmented Generation and domain-specific tool use to ground out- puts in such specialized scientific applications. AISciVision allows an LMM to predict after multiple rounds of tool use, thus going beyond classical Chain-of-Thought prompting.\nRetrieval-Augmented Generation (RAG) Since Large Language Models (LLMs) suffer from halluci- nations in generations and static memory about the world, there is much work in Retrieval-Augmented Genereation (RAG) to retrieve relevant context from external knowledge sources to ground generations in reality (Khandelwal et al., 2020; Lewis et al., 2020). RAG has proven useful for language generation and question answering in scientific applications such as biomedical research and medicine (Soong et al., 2024; Zakka et al., 2024; Bae et al., 2024). Recent work has also demonstrated the effectiveness of RAG in multi- modal settings by retrieving relevant images and documents to enrich the model's prompt, in applications of visual question answering and image captioning (Chen et al., 2022; Yasunaga et al., 2023; Lin et al., 2023). Our framework AISciVision leverages RAG for scientific image classification, by enriching the LMM's context with domain-relevant examples during inference."}, {"title": "Interactive AI Agents and tool-use", "content": "In recent years, Large Language Models have enabled users to en- gage in multi-turn natural language conversations to perform a wide range of tasks, from brainstorming and writing to writing code and solving math equations (Achiam et al., 2023; Driess et al., 2023; Touvron et al., 2023; Gemini-Team, 2024). There is growing interest in enabling such models to act as an agent on their generations, by deploying them in environments (Fan et al., 2022; Wang et al., 2024) and attaching tools to interact with the web (Nakano et al., 2022; Patil et al., 2023; Schick et al., 2023). Yao et al. (2023) accomplish this with ReAct, by prompting the model to invoke calls to specified tools in natural language format, whereas Schick et al. (2023) introduce ToolFormer which is a model finetuned on a list of available tools. When environments do not return natural language feedback, e.g. when the feedback is scalar or binary, Shinn et al. (2023) find that training a helper model to generate natural language descriptions of the feedback improves the agent's performance-termed as Reflexion. Lu et al. (2024) introduce the \"AI Scien- tist\" that imitates the research process in the Machine Learning community. They utilize Chain-of-Thought, Reflexion, and tool use to imitate the research process: brainstorm research ideas, execute experiments, and write a paper.\nFor scientific research in physical, life, and climate sciences, not only is it important to obtain accurate pre- dictions but also to get insight into the underlying justifications that lead to these predictions (Wang et al., 2023; Chen et al., 2021; Society, 2024; Kong et al., 2022). With the ReAct approach of interacting with ex- ternal tools and knowledge sources in a multi-turn conversation, Yao et al. (2023) shows that the agent leaves a 'paper trail' or a 'transcript' of the decision-making process. Our framework AISciVision accomplishes exactly this feat, and is one of the first attempts at developing an interactive AI agent for scientific image classification. Following the extensive research in tool use, we develop domain-specific tools in the AISciVision framework, for instance, zoom/pan for satellite image datasets and image enhancement tools. Our agent interacts with these tools in a multi-turn fashion and leaves a conversation transcript, enhancing interpretability, transparency, and trust-all crucial for scientific research."}, {"title": "3 METHODOLOGY", "content": "Our proposed framework, AISciVision, integrates a Visual Retrieval-Augmented Generation (VisRAG) pro- cedure with domain-specific tools, which an LMM uses to classify images in the scientific domain. In this section, we describe these two components and discuss how AISciVision uses them during inference."}, {"title": "3.1 RETRIEVING RELEVANT IMAGES WITH VISUAL RAG (VISRAG)", "content": "To specialize a general-purpose LMM for scientific image classification during inference, we enrich the model's prompt with images relevant to the given test image. We first encode all available training images into a shared embedding space. Let $D = \\{(x, y)\\}$ be a training set of binary labeled images, where $x_i \\in \\mathbb{R}^{H\\times W\\times C}$ is an image and $y_i \\in \\{\\pm 1\\}$ is its label. We map each image $x_i$ to an embedding $e_i = \\phi(x_i) \\in \\mathbb{R}^d$ using an embedding model $\\phi : X \\rightarrow \\mathbb{R}^d$ on the set of images $X$. This embedding model could be a pre-trained image embedding model, for instance, CLIP (Radford et al., 2021), that ensures that similar visual content has similar embeddings. Second, we separate the embeddings into two sets: positive examples $E^+ = \\{e_i | y_i = 1\\}$ and negative examples $E^- = \\{e_i | y_i = -1\\}$. This allows us to enrich the model's prompt with structured context, described below.\nOn inference, we embed an input test image $x_{\\text{test}}$ to get embedding $e_{\\text{test}} = \\phi(x_{\\text{test}})$. We then retrieve relevant images to enrich the LMM's context by computing the cosine similarity of the test image embedding $X_{\\text{test}}$ with all positive embeddings $E^+$, and with all negative embeddings $E^-$. By ranking all embeddings according to the cosine similarity, we obtain the most similar positive example $e_{\\text{sim}}^+$ and negative example"}, {"title": "", "content": "$e_{\\text{sim}}^-$ as follows:\n$e_{\\text{sim}}^+ := \\underset{e_i \\in E^+}{\\text{arg max}} \\cos(e_{\\text{test}}, e_i)$ and $e_{\\text{sim}}^- := \\underset{e_i \\in E^-}{\\text{arg max}} \\cos(e_{\\text{test}}, e_i)$, where $\\cos(e_{\\text{test}}, e_i) = \\frac{e_{\\text{test}} \\cdot e_i}{\\|e_{\\text{test}}\\| \\|e_i\\|}$ \nWe then provide the images $x_{\\text{sim}}^+$ and $x_{\\text{sim}}^-$ of the respective embeddings $e_{\\text{sim}}^+$ and $e_{\\text{sim}}^-$ to the LMM in its prompt. This enables the model to evaluate images it might not have been trained on, in our case, scientific images. Adding both positive and negative examples provides relevant visual features that characterize the domain-specific classification task, effectively helping to ground the model's reasoning. Hence, this VisRAG approach facilitates more accurate and context-aware inference, leveraging the structure inherent in the classification task."}, {"title": "3.2 DOMAIN-SPECIFIC INTERACTIVE TOOLS", "content": "We leverage expert-designed tools for each classification task, empowering the LMM to refine its predictions by interacting with these tools. These tools mimic transformations and \u201cexpert advice\" that a human would use to manipulate, inspect, and analyze images, before attempting to classify an image. Therefore, by interacting with these tools in the AISciVision framework, the LMM acts akin to an interactive AI agent making informed and interpretable decisions.\nGenerally, we define a tool $T$ as a function on images $X$, with outputs as images $X$ or a real-valued scalar in $\\mathbb{R}$. That is, such a tool either transforms an image $x \\in X$ to another image $T(x) \\in X$ or returns a numeric output $T(x) \\in \\mathbb{R}$ such as a confidence score from an external model. For example, a tool $T_{ML}$ might use an externally-trained Machine Learning model with parameters $\\theta$ to predict the probability of a label given the image: $T_{ML} = Pr[y = 1 | x; \\theta]$. Other tools $T_{br}$ or $T_{co}$ might adjust brightness or contrast by some value $\\alpha$ such that $T_{br} = AdjustBrightness(x, \\alpha)$ or $T_{co} = IncreaseConstrast(x, \\alpha)$.\nFor each image classification task, we define a set of tools $T = \\{T_1, . . ., T_K \\}$ and provide their descriptions in natural language as a prompt to the LMM. At any turn $i$ in the conversation, the model submits a request for a tool $T_i \\in T$, which can either transform the image $x' = T_i(x)$ or return a numeric value. The AISciVision parses this request and returns the tool's result as a prompt, with the transformed image or the numeric value described in a sentence. In essence, this is similar to ReAct (Yao et al., 2023) in that we use a hardcoded prompt template for the response. Iterative use of domain-specific tools enables the agent in our AISciVision framework to refine predictions in a context-aware manner, not only improving accuracy (see Section 4) but also producing a transcript of the agent's reasoning. Such a transcript provides interpretable insight into our framework's reasoning, which is crucial for applications in scientific discovery."}, {"title": "3.3 INFERENCE PROCESS IN THE AISCIVISION FRAMEWORK", "content": "Given an input test image, AISciVision enriches the model's prompt with VisRAG and descriptions of the tools, which the model calls in subsequent turns of the conversation. Finally, the model outputs a classifica- tion label with a probability score indicating its confidence.\nWe design the initial system prompt to reflect the specified domain (see Appendix C for example transcripts). First, we use the VisRAG approach to retrieve the most similar positive and negative examples from the training set, $x_{\\text{sim}}^+$ and $x_{\\text{sim}}^-$ respectively. We then describe the set $T$ of available domain-specific tools, and encourage the model to use them to obtain more context during inference. After a few conversation turns, the LMM responds with a binary prediction and a confidence score. The transcript of interactions represents a record of justifications at the inference stage, which a domain expert can review after the fact. In this way, AISciVision presents an interactive AI agent that uses domain-specific knowledge to not only classify scientific images but also justify its underlying reasoning in natural language."}, {"title": "4 EXPERIMENTS", "content": "We extensively evaluate our AISciVision framework on three image datasets from scientific applications: detecting the presence of aquaculture ponds (Greenstreet et al., 2023), diseased eelgrass (Rappazzo et al., 2021), and solar panels (Lacoste et al., 2023). We compare against natural baselines and conduct ablation studies on components of AISciVision. We discuss our experimental results in this section.\nDatasets and Experimental Setup We provide a brief overview of the three datasets and their significance to environmental research, along with a summary of the domain-specific tools used in the AISciVision framework (see Appendix A for a full list of tools).\nAquaculture Pond Detection. Aquaculture, vital for the global food supply, requires careful mon- itoring from satellite imagery. This poses challenges due to the varied appearance of water bodies. The Aquaculture dataset contains 799 images (640\u00d7640) from Rond\u00f4nia, Brazil (Greenstreet et al., 2023), with \u2248 20% containing aquaculture ponds. Since the metadata includes geospatial location data, in AISciVision we define tools like zoom and pan, utilizing real-time Google Maps API.\nEelgrass Wasting Disease Detection. Eelgrass (Zostera marina), essential for coastal ecosystems, faces threats from Eelgrass Wasting Disease (EWD) (Groner et al., 2016). The Eelgrass dataset contains 9887 images (128 \u00d7 128) from Washington state, with \u2248 45% showing diseased eel- grass (Rappazzo et al., 2021). We incorporate tools like contrast and sharpening adjustments.\nSolar Panel Detection. The open-source Solar dataset tracks solar panel adoption with satellite images (Lacoste et al., 2023). It contains 11814 images (320 \u00d7 320), with \u2248 15% containing solar panels. We use similar image enhancement tools as the Eelgrass dataset, since geospatial metadata is absent.\nWe test on 100 randomly subsampled examples from each dataset's test set for consistent evaluation across all methods. To balance the cost constraints of LMM experimentation, we use this small test set for robust experiments and ablation studies. All methods are evaluated in low-labeled (20%) and full-labeled (100%) data settings, on Accuracy, F1-score, and Area Under Curve (AUC) metrics."}, {"title": "AISciVision method", "content": "We use the GPT-4o as the framework's LMM, and attach the two components Vis- RAG and domain-specific tools for each dataset. We provide text descriptions of the available tools as prompts to the LMM, and instruct how to request tool use. We encourage the LMM to use at least 3 tools during inference, and instruct it to submit a classification decision with a confidence value in 4 conversation turns. All embeddings for VisRAG are computed via CLIP, and the cosine similarity is used to gauge the similarity between images. We include example transcripts and our prompts for all datasets in Appendix C."}, {"title": "Baselines", "content": "A key component of AISciVision is VisRAG, which retrieves the most similar positive and negative examples during inference before prompting the LMM. Hence, a natural baseline is na\u00efve k-Nearest Neighbor (k-NN with k = 3) using CLIP embeddings. We also include CLIP-ZeroShot as a baseline: we classify a test image by comparing the cosine similarities of CLIP text embeddings of the two labels with the CLIP image embedding. As CLIP-ZeroShot does not rely on image features specific to the scientific domain, we evaluate another baseline CLIP+MLP for binary classification, where we train a 2-layer Multi- Layer Perceptron (MLP) on top of frozen CLIP image embeddings. The MLP has a hidden layer with 256 units and ReLU activation function, and is trained for 10 epochs using the Adam optimizer (learning rate 0.01 and batch-size 32). We find that AISciVision consistently outperforms all baselines on all three datasets, in both low- and full-labeled data regimes (Table 1)."}, {"title": "Ablation studies and tool efficacy", "content": "We conduct ablation studies on the two components of AISciVision: VisRAG and domain-specific tools. We compare the following variants with AISciVision (GPT40 + VisRAG + Tools): (1) GPT40-ZeroShot that predicts a label and confidence score after the initial prompt\u00b2, (2) GPT40 + VisRAG that uses only VisRAG to retrieve examples relevant to the test image, (3) GPT40 + Tools that uses only our domain-specific tools. We report these ablation experiment results in Table 2. These ablation experiments allow us to isolate and evaluate the effects of retrieval through VisRAG and usage of domain-specific tools."}, {"title": "5 DEPLOYED APPLICATION", "content": "AISciVision is not just a conceptual framework, we have deployed it as a fully functional web application to detect aquaculture ponds, enabling real-time and scalable use by ecologists. When ecologists upload a test image to the web application, the AISciVision framework prompts the LMM to detect if the image contains aquaculture ponds. Importantly, AISciVision provides the detailed transcript that outlines the LMM's reasoning and tool use."}, {"title": "6 DISCUSSION", "content": "Our AISciVision framework combines robust prediction capabilities, transparency, and adaptability, offering a practical approach for AI use in scientific contexts. By delivering accurate and context-aware predictions along with a full reasoning transcript, AISciVision serves as a valuable research partner across different applications. Transcripts enhance the accountability and traceability of model outputs, enabling researchers to validate decision-making processes. This is particularly important in scientific research, for instance in conservation efforts in complex ecosystems like the Amazon Basin, where accurate spatial data is crucial for sustainability. The combination of predictions and interpretable transcripts can help advance scientific discovery, support regulatory compliance by providing a clear record of inferences, and aid education by giving concrete examples of classification processes.\nLimitations and Future Work While AISciVision offers significant benefits in providing transparent rea- soning and the potential for enhanced accuracy, it comes with a trade-off: using off-the-shelf LMMs for inference is financially expensive, compared to traditional machine learning methods. For our future work, we aim to actively develop our web application to continue to collect expert feedback on the LMM agent's reasoning through a ChatGPT-style interface. Experts can provide real-time feedback and corrections, which will be stored within the VisRAG component to improve the LMM agent's performance with use. We en- vision the system continuing to learn as experts interact with the agent, and provide rich natural language feedback. Beyond refining our approach for image data, we also plan to test and extend our method to other modalities, such as sound or any tokenizable input that can be incorporated into an LMM."}]}