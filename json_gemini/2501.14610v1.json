{"title": "Leveraging Spatial Cues from Cochlear Implant Microphones to Efficiently Enhance Speech Separation in Real-World Listening Scenes", "authors": ["Feyisayo Olalere", "Kiki van der Heijden", "Christiaan H Stronks", "Jeroen Briaire", "Johan HM Frijns", "Marcel van Gerven"], "abstract": "Speech separation approaches for single-channel, dry speech mixtures have improved greatly. However, speech separation in real-world, spatial and reverberant acoustic environments remains challenging. This limits the efficiency of existing approaches for real-world speech separation applications in assistive hearing devices such as cochlear implants (CIs). To address this issue, we quantify the impact of real-world acoustic scenes on speech separation and investigate to what extent spatial cues from such real-world scenes can improve separation quality in an efficient manner. Crucially, we characterize speech separation performance as a function of implicit spatial cues (i.e., cues inherently present in the acoustic input that can be learned by the model), as well as of explicit spatial cues (i.e., manually calculated spatial features added as auxiliary input to the model). Our findings show that spatial cues (both implicit and explicit) improve separation performance for mixtures with spatially separated talkers, but also for mixtures with nearby talkers. Further, we demonstrate that spatial cues enhance speech separation in particular when spectral cues for separation are ambiguous, that is, when voices are similar. Finally, we show that the addition of explicit, auxiliary spatial cues is particularly beneficial when implicit spatial cues are weak. For example, microphone recordings from a single CI contain weaker implicit spatial cues than when microphone recordings from two, bilateral CIs are combined. These findings emphasize the importance of training models on real-world data to improve generalisability to everyday listening situations and contribute to the development of more efficient speech separation approaches for CIs or other assistive hearing devices in such real-world listening situations.", "sections": [{"title": "I. INTRODUCTION", "content": "HEARING impairments affect more than 5% of the global population and result in significant communication difficulties, even with the use of assistive hearing devices such as cochlear implants (CIs) [1], [2]. CIs are implanted neuroprosthetic devices which directly stimulate the auditory nerve to restore hearing for individuals with severe- to-profound sensorineural hearing loss [3]. While CI users experience tremendous improvements in speech perception in quiet environments, communication difficulties persist in noisy, everyday listening scenes such as the classroom, the office, or at social gatherings [4]\u2013[7]. These difficulties are a consequence of peripheral processing deficits, which hamper a CI user's ability so selectively attend to a talker in noisy listening scenes [7].\nImplementing a speech denoising algorithm as a front- end processing step in a CI or other assistive hearing device significantly improves speech perception for hearing impaired listeners, because this provides the user with clean speech signals. However, everyday listening environments often con- sist of multiple, simultaneous talkers in addition to various background noise sources [8]. As voices have similar acoustic characteristics, such multi-talker scenes are particularly chal- lenging for speech denoising algorithms. Therefore, automatic speech separation also plays a crucial role in front-end speech processing [9].\nCurrently, deep neural network (DNN) based approaches are the state of the art for speech separation. Time-frequency based DNN speech separation methods operate on spectro- gram representations of multi-talker mixtures [10]\u2013[12]. Such approaches either extract masks of each talker in the mixture [13], [14] or directly reconstruct the spectrogram of each talker [15], [16]. However, time-frequency speech separation approaches have several drawbacks. Most notably, as phase information is discarded, only an approximated phase [17] or the phase of the mixed audio [18] can be used to reconstruct the sound wave for each talker. This introduces additional noise and limits the quality of the reconstructed, clean talkers [19].\nMore recent speech separation approaches therefore focus on separation in the time-domain, i.e., operating directly on the audio mixture's waveform [20]\u2013[22]. These approaches avoid the phase inversion problem and therefore introduce fewer artifacts in the reconstructed speech waveforms [22]. Moreover, several time-domain approaches were shown to be highly computationally efficient [22]\u2013[24], making them more suitable for small devices such as CIs and other assistive hearing technology.\nHowever, few approaches (either in the time or in the time- frequency domain) have been applied to real-world acoustic"}, {"title": "II. SPEECH SEPARATION IN REAL-WORLD SCENES", "content": "In this study, the task of speech separation consists of estimat- ing the waveform of talker 81 and 82 from the discrete wave- form of the speech mixture $y \\in R^{C \\times T}$. Here, C represents the number of channels (i.e., microphones) and T denotes time.\nWe simulated a dataset consisting of two-talker mixtures in spatial, real-world listening scenes including reverberation usingthe WSJ0-2mix dataset [35] and a custom sound spatial- ization pipeline. The WSJ0-2 dataset is extensively employed for speech separation tasks (e.g., [20]\u2013[22]) and comprises 20,000 training, 5,000 validation and 3,000 test samples of two-talker mixtures.\nThe spatialization pipeline consisted of four components (Fig. 1): (1) Simulation of room impulse responses (RIRs), (2) CI-HRTFs, (3) Generation of binaural room impulse responses (BRIRs); (4) Mixing real-world two-talker mixtures.\n1) Room Impulse Response (RIR): A RIR describes room- specific acoustic properties including reverberation, reflection, and echo. Here, we used the Pyroomacoustics Python package [36], which employs an image source model to efficiently simulate RIRs by simulating sound wave propagation from a source to a receiver within a shoebox room. In total, we simulated 500 shoebox rooms with different dimensions and reverberation properties (Fig. 1 A). Room dimensions were randomly selected from a range of 4 \u00d7 4 \u00d7 2.5 m to 10 \u00d7 10 \u00d7 5 m (length \u00d7 width \u00d7 height), encompassing common sizes of classrooms, meeting rooms, and restaurants [37].\nFor each Room Impulse Response (RIR), we sampled reverberation time (T60) from a range of 0.2 s to 0.7 s (stepsize = 0.01 s). The reverberation time reflects the strength of reverberation in a room and is dependent both on the size of the room and the materials of which the room consists [38], [39]. To introduce a naturalistic relation between room size and T60, we restricted the range of T60 to sample from based on room size. That is, we scaled all room sizes used"}, {"title": "III. EXPERIMENTAL PROCEDURES", "content": "We trained the SuDoRM-RF model on various input configu- rations to quantify the impact of implicit and explicit spatial cues on speech separation performance in real-world listening scenes. As stated previously, we define implicit spatial cues as cues that are inherently present in the data and can be learned by the model, but which are not manually extracted as an auxiliary input feature. Examples of such implicit spatial cues are inter-channel level differences (ILDs) and inter-channel phase differences (IPDs), which can be extracted from the comparison between two or more microphone channels.\nThree of the input configurations included in the present ex- periment (Fig. 3B) exclusively contained such implicit spatial cues. Specifically, input samples consisting of a one-channel waveform contain implicit spatial cues in the form of level differences between the two talkers within the single channel. However, the implicit spatial cues in this input configuration are weak because no inter-channel comparison is possible.\nBy contrast, both input samples consisting of two channels from a single CI (that is, unilateral, either from left or right ear) and input samples consisting of two channels of two different CIs (that is, bilateral, one on left and one on right ear) contain implicit spatial cues which can be extracted from an inter-channel comparison. However, implicit spatial cues are stronger in the two-channel, bilateral samples than in the two-channel, unilateral samples due to the position of the head in between the two channels.\nFurther, in the present paper we refer to the IPDs added as auxilliary feature as explicit spatial cues. To extract IPDs, we first converted waveforms to the time-frequency domain using the Short-Time Fourier Transform (STFT; hop size = 8 s, number of frequency bins = 512, and window length = 512 samples), and we matched the parameters with the frequency features as encoded by the encoders. We then calculated IPD similar to [28] as\n$IPD(t, f) := \\angle \\frac{Y_{c1}(t, f)}{Y_{c2}(t, f)}$ (1)\nwhere y denotes the spectrogram representation of the two-"}, {"title": "IV. RESULTS", "content": "The present work aimed to quantify the impact of implicit and explicit spatial cues captured by CI microphones on speech separation in real-world listening scenes. To this end, we assessed and compared speech performance for seven different input configurations, which vary in the presence and strength of implicit and explicit spatial cues (Fig. 3B and Table I).\nTo establish a baseline, we first trained and evaluated the SudoRM-RF model on one-channel, dry and non-spatial two- talker mixtures (i.e., the original WSJ0-2mix dataset [35]).\nAs outlined in Table I (row 1), the model obtained a SI- SDRi of 12.96 dB for this dataset, indicating that the model accurately separated two concurrent speech streams. Although other, larger speech separation models outperform the current SuDoRM-RF implementation on the WSJ0-2mix dataset [22], [59], it should be noted that we purposefully selected a small and efficient model that can potentially be deployed on a CI. Moreover, we did not perform any type of data pre-processing (for example, silence removal) to ensure that sound scenes maintain their natural characteristics.\nWe quantified to what extent speech separation performance of the SuDORM-RF model deteriorated when the model was trained on one-channel, real-world speech mixtures (Table I, row 1 and 2). Results show that the overall quality of the resulting separated speech waveforms was substantially lower when the model was trained on one-channel, real-world two-talker mixtures (SI-SDR = 2.66, STOI = 0.70, PESQ = 1.81) than when the model was trained on one-channel, non- spatial, dry two-talker mixtures (SI-SDR = 12.96, STOI = 0.89 and PESQ = 2.97; Table I). However, the SI-SDRi score was substantially higher for the model trained on real-world scenes (27.01 dB) than for the model trained on non-spatial, dry scenes (12.96 dB). This discrepancy between SI-SDRi and SI-SDR can be explained by the difference in baseline SI-SDR of the two input configurations: the real-world two- talker mixtures had a baseline SI-SDR of -24.26 dB, while the non-spatial, dry two-talker mixtures had a SI-SDR baseline"}, {"title": "V. DISCUSSION", "content": "This study investigated the efficacy of utilising spatial cues derived from cochlear implant microphones to improve speech separation in real-world, spatial and reverberant scenes in an efficient manner using the time-domain speech separation model SudoRM-RF [24].\nSimilar to other studies performing speech separation on spatial and reverberant speech mixtures [26], [27], [32], we observed a notable decline in the model's performance when trained on real-world acoustic scenes in comparison to when trained on non-spatial, dry scenes. These findings confirm that a model developed for speech separation of non-spatial, dry two-talker mixtures does not generalize robustly to spatial, reverberant two-talker mixtures. This highlights the impor- tance of employing ecologically valid data for the development and optimization of speech separation models for real-world applications.\nUnlike existing approaches which aim to improve speech separation by adding spatial cues as an auxilliary feature and thereby unavoidably reduce model efficiency (e.g. [25]), our approach demonstrates that a time-domain model can learn to extract and utilise the naturally available spatial cues that are"}, {"title": "VI. CONCLUSION", "content": "This study explored the potential of leveraging spatial cues de- rived from cochlear implant microphones for efficient speech separation in real-world acoustic scenes. Our results highlight that ecologically valid data is crucial for the development"}]}