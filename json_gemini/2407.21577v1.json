{"title": "Multi-Site Class-Incremental Learning with Weighted Experts in Echocardiography", "authors": ["Kit M. Bransby", "Woo-Jin Cho Kim", "Jorge Oliveira", "Alex Thorley", "Arian Beqiri", "Alberto Gomez", "Agisilaos Chartsias"], "abstract": "Building an echocardiography view classifier that maintains performance in real-life cases requires diverse multi-site data, and frequent updates with newly available data to mitigate model drift. Simply fine-tuning on new datasets results in \"catastrophic forgetting\", and cannot adapt to variations of view labels between sites. Alternatively, collecting all data on a single server and re-training may not be feasible as data sharing agreements may restrict image transfer, or datasets may only become available at different times. Furthermore, time and cost associated with re-training grows with every new dataset. We propose a class-incremental learning method which learns an expert network for each dataset, and combines all expert networks with a score fusion model. The influence of \"unqualified experts\" is minimised by weighting each contribution with a learnt in-distribution score. These weights promote transparency as the contribution of each expert is known during inference. Instead of using the original images, we use learned features from each dataset, which are easier to share and raise fewer licensing and privacy concerns. We validate our work on six datasets from multiple sites, demonstrating significant reductions in training time while improving view classification performance.", "sections": [{"title": "1 Introduction", "content": "Echocardiography (echo) view classification is often a necessary first step in automated image interpretation and analysis, as different tasks may require different views as input [19]. Several deep learning methods have been proposed to this end [19,22,10,14] demonstrating excellent performance. To generalise and maintain performance in real-life cases, view classifiers need to be trained on a diverse multi-site dataset to accommodate varying acquisition, demographic, and clinical parameters. However, these parameters may change over time, e.g. with machine upgrades, modification of protocols, or changes in demographics.\nSuch changes require updating the classifier with newly available data to mitigate model drift [17]. Naive approaches like fine-tuning on a new dataset may result in \"catastrophic forgetting\" [15], in which previous knowledge is forgotten at the expense of the new information. Retaining previous knowledge while learning on new data is the goal of incremental-learning [20].\nFurthermore, newly available data (e.g. from sites with different protocols) may have different sets of view labels. Labels can be characterised as \"base\", \"novel\", or \"overlapping\", depending on whether they are present in the original data, the new data, or both. The goal of class-incremental learning (CIL) [20] is to learn to classify the increasing label set over time. A straightforward solution is to retrain using all data combined. However, this is not always feasible as data sharing agreements may limit access or transfer outside the acquisition site; or different datasets may be available at different times. In addition, retraining a classifier when a new dataset becomes available is inefficient as the total training time and cost grows considerably with the number and size of datasets.\nA common approach for CIL is to use a single model, and update parts or all of the weights using knowledge distillation [16,12,4] or weight regularisation [9,1]. Others learn additional parameters with dynamic architecture changes [18,24,21], or by duplicating the model for novel data and pruning [25]. Wu et al. [23] expand on this by duplicating and fine-tuning a base model for each new dataset, resulting in set of expert model branches, each specialised on a single dataset. They combine the output logits of the independently trained branches using a learnt score fusion (SF) network that enables knowledge transfer between base and novel classes. This leverages shared information between representations in different branches, but is sensitive to one branch contributing ineffective information to another branch. We term this the \"unqualified expert\" problem and may happen when there are differences in data distributions, caused for instance by different label sets, patient demographics or scanner manufacturers. As a result view predictions for out-of-distribution (0.0.d.) images can be incorrect, but also highly confident.\nIn this paper we address the problem of building an echo view classifier model with multiple datasets from different sites (see Fig. 1) that are not simultaneously available and have different, overlapping sets of labels. To this end, we build upon [23], and mitigate the \"unqualified expert\" problem by weighing the con-"}, {"title": "2 Method", "content": "2.1 Problem Setting\nGiven a dataset $D = \\{(x_i, y_i)\\}_{i=1}^N$, where $x_i$ represents an ultrasound image and $y_i$ denotes the corresponding ground truth view label, the objective of view classification is to learn an image encoder $\\phi$ and linear classifier $W \\in R^{k\\times |\\mathcal{Y}|}$, where $\\mathcal{Y}$ is the set of labels in $D$ and $k$ is the size of the image features. The view label is learnt by minimising the cross entropy loss between $y$ and the label prediction $\\hat{y} = \\sigma(\\phi(x)W)$ where $\\sigma(\\cdot)$ is a softmax function.\nWe first train a base model $\\mathcal{M}_b = \\{\\phi_b, W_b\\}$ on a large dataset $D_b$ with label set $\\mathcal{Y}_b$ of base classes. We consider $t$ incremental steps $n \\in \\{n_1, n_2, .., n_t\\}$ each with a dataset $D_n$ and label set $\\mathcal{Y}_n$. There can be varying degrees of intersection between base classes $\\mathcal{Y}_b$ and the new label set $\\mathcal{Y}_n$. For instance, there may be novel classes not present in the base classes $(\\mathcal{Y}_n \\cap \\mathcal{Y}_b = \\emptyset)$ or overlapping classes $(\\mathcal{Y}_b \\cap \\mathcal{Y}_n \\neq \\emptyset)$. Our goal is to accurately predict all classes, regardless of whether they are introduced in an incremental step, are novel, base, or overlapping.\nSimply combining base and incremental datasets into a single set and retraining $\\mathcal{M}_b$ from scratch is not ideal due to cost, time, and restrictive data licensing as motivated in Section 1. Similarly, naively fine-tuning $\\mathcal{M}_b$ on successive incremental datasets $D_{n1}, D_{n2}, \u2026, D_{nt}$ is difficult due to the changing size of the label set, and ultimately results in catastrophic forgetting of the base classes $\\mathcal{Y}_b$.\n2.2 Score Fusion\nWe follow Wu et al. [23] where $\\mathcal{M}_b$ is cloned and fine-tuned for each incremental dataset $D_n$, yielding $t + 1$ expert models $\\{\\mathcal{M}_b, \\mathcal{M}_{n1}, \u2026, \\mathcal{M}_{nt}\\}$. This approach"}, {"title": "2.3 Weighted Score Fusion", "content": "Given an input image, we compute an i.d. score to weigh each expert's logits such that the influence of unqualified 0.0.d experts is minimised and the influence of i.d. experts is maximised. We do so by learning an attention-like weighting vector $A \\in R^{|d|}$ using a feed-forward network $\\delta_a$ consisting of 3 linear layers, the first two with ReLU activation and the last with a softmax."}, {"title": "2.4 Multi-Site Training", "content": "Assuming that the incremental datasets are acquired at different medical sites, which often impose data sharing restrictions, we propose a multi-site training paradigm, which does not require transferring the image data. For every step, the view classifier is cloned, transferred into the remote server, and fine-tuned on dataset $D_n$. We then make five forward passes of the fine-tuned classifier $\\mathcal{M}_n$ on randomly augmented data from $D_n$ to generate five different $g$ and $h$ vectors for each example. We then transfer these vectors back to the local server, which are used as input to train the weighted score fusion network."}, {"title": "3 Experiments & Results", "content": "3.1 Datasets\nAs base dataset $D_b$, we use WASE-Normals (WASE) [2], a large multi-site proprietary dataset of 2,009 healthy volunteers acquired at 18 sites from 15 countries."}, {"title": "3.2 Implementation & Training", "content": "View classifiers use ResNet18 [5] architecture and were trained for 200 epochs on a NVIDIA GeForce RTX 2080Ti with cross-entropy loss, Adam optimiser, batch size of 64, and learning rate of 1e-3. The score fusion networks were trained for 50 epochs with the same settings. Weights from the epoch with the highest validation accuracy were saved. Hyperparameters were tuned on a held-out validation set. We evaluate classification performance using accuracy and F1-score metrics. Our method is implemented in PyTorch, and the code is available here:\nhttps://github.com/kitbransby/class-incremental-learning-echo"}, {"title": "3.3 Comparison to Existing Methods & Ablation Study", "content": "We validate our method on an internal test set (WASE, CAMUS, Medstar, StG) to measure performance on incremental datasets seen during training, and an external set (MAHI, UoC) of data not seen during training to measure generalisability. Results are presented in Tables 2 and 3, respectively.\nWe demonstrate that naively fine-tuning a classifier on successive datasets results in \"catastrophic forgetting\". We test two configurations, one where the classifier head is replaced at every step, and a second that expands to accommodate novel classes [13]. These do not require data transfers and perform well on the final dataset (StG) but forget learning from previous data. We also explore other methods that do not require data transfer such as using a single expert and find they perform well on i.d. data but do not generalise well on o.o.d data.\nOur primary comparison is to the training oracle where the model is retrained with all data combined at each incremental step. This method is preferred given"}, {"title": "4 Conclusion", "content": "Echo view classifiers require training on diverse multi-site data, and frequent updates with newly acquired data to avoid model drift. We address the problem of updating such a model using multiple datasets with partly overlapping label sets that are from different sites and may not be simultaneously available. We propose attn-wSF and nmd-wSF, two CIL methods that train and combine predictions from multiple experts models with a learnt i.d. weighting which minimises the influence of o.o.d predictions from \"unqualified experts\". Our results demonstrate improved view classification with significantly reduced cumulative"}]}