{"title": "Multi-Site Class-Incremental Learning with Weighted Experts in Echocardiography", "authors": ["Kit M. Bransby", "Woo-Jin Cho Kim", "Jorge Oliveira", "Alex Thorley", "Arian Beqiri", "Alberto Gomez", "Agisilaos Chartsias"], "abstract": "Building an echocardiography view classifier that maintains performance in real-life cases requires diverse multi-site data, and frequent updates with newly available data to mitigate model drift. Simply fine-tuning on new datasets results in \"catastrophic forgetting\", and cannot adapt to variations of view labels between sites. Alternatively, collecting all data on a single server and re-training may not be feasible as data sharing agreements may restrict image transfer, or datasets may only become available at different times. Furthermore, time and cost associated with re-training grows with every new dataset. We propose a class-incremental learning method which learns an expert network for each dataset, and combines all expert networks with a score fusion model. The influence of \"unqualified experts\" is minimised by weighting each contribution with a learnt in-distribution score. These weights promote transparency as the contribution of each expert is known during inference. Instead of using the original images, we use learned features from each dataset, which are easier to share and raise fewer licensing and privacy concerns. We validate our work on six datasets from multiple sites, demonstrating significant reductions in training time while improving view classification performance.", "sections": [{"title": "1 Introduction", "content": "Echocardiography (echo) view classification is often a necessary first step in automated image interpretation and analysis, as different tasks may require different views as input [19]. Several deep learning methods have been proposed to this end [19,22,10,14] demonstrating excellent performance. To generalise and maintain performance in real-life cases, view classifiers need to be trained on a diverse multi-site dataset to accommodate varying acquisition, demographic, and clinical parameters. However, these parameters may change over time, e.g. with machine upgrades, modification of protocols, or changes in demographics."}, {"title": "2 Method", "content": ""}, {"title": "2.1 Problem Setting", "content": "Given a dataset \\(D = \\{(x_i, y_i)\\}_{i=1}^N\\), where \\(x_i\\) represents an ultrasound image and \\(y_i\\) denotes the corresponding ground truth view label, the objective of view classification is to learn an image encoder \\(\\phi\\) and linear classifier \\(W \\in \\mathbb{R}^{k \\times |\\mathcal{Y}|}\\), where \\(\\mathcal{Y}\\) is the set of labels in \\(D\\) and \\(k\\) is the size of the image features. The view label is learnt by minimising the cross entropy loss between \\(y\\) and the label prediction \\(\\hat{y} = \\sigma(\\phi(x)W)\\) where \\(\\sigma(\\cdot)\\) is a softmax function.\nWe first train a base model \\(\\mathcal{M}_b = \\{\\phi_b, W_b\\}\\) on a large dataset \\(D_1\\) with label set \\(\\mathcal{Y}_b\\) of base classes. We consider \\(t\\) incremental steps \\(n \\in \\{n_1, n_2, .., n_t\\}\\) each with a dataset \\(D_n\\) and label set \\(\\mathcal{Y}_n\\). There can be varying degrees of intersection between base classes \\(\\mathcal{Y}_b\\) and the new label set \\(\\mathcal{Y}_n\\). For instance, there may be novel classes not present in the base classes (\\(\\mathcal{Y}_n \\cap \\mathcal{Y}_b = \\emptyset\\)) or overlapping classes (\\(\\mathcal{Y}_b \\cap \\mathcal{Y}_n \\neq \\emptyset\\)). Our goal is to accurately predict all classes, regardless of whether they are introduced in an incremental step, are novel, base, or overlapping.\nSimply combining base and incremental datasets into a single set and retraining \\(\\mathcal{M}_1\\) from scratch is not ideal due to cost, time, and restrictive data licensing as motivated in Section 1. Similarly, naively fine-tuning \\(\\mathcal{M}_1\\) on successive incremental datasets \\(D_{n_1}, D_{n_2}, \u2026, D_{n_t}\\) is difficult due to the changing size of the label set, and ultimately results in catastrophic forgetting of the base classes \\(\\mathcal{Y}_b\\)."}, {"title": "2.2 Score Fusion", "content": "We follow Wu et al. [23] where \\(\\mathcal{M}_1\\) is cloned and fine-tuned for each incremental dataset \\(D_n\\), yielding \\(t + 1\\) expert models \\(\\{\\mathcal{M}_b, \\mathcal{M}_{n_1}, \u2026, \\mathcal{M}_{n_t}\\}\\)."}, {"title": "2.3 Weighted Score Fusion", "content": "Given an input image, we compute an i.d. score to weigh each expert's logits such that the influence of unqualified o.o.d experts is minimised and the influence of i.d. experts is maximised. We do so by learning an attention-like weighting vector \\(A \\in \\mathbb{R}^{|d|}\\) using a feed-forward network \\(\\delta_a\\) consisting of 3 linear layers, the first two with ReLU activation and the last with a softmax."}, {"title": "2.4 Multi-Site Training", "content": "Assuming that the incremental datasets are acquired at different medical sites, which often impose data sharing restrictions, we propose a multi-site training paradigm, which does not require transferring the image data. For every step, the view classifier is cloned, transferred into the remote server, and fine-tuned on dataset \\(D_n\\). We then make five forward passes of the fine-tuned classifier \\(\\mathcal{M}_n\\) on randomly augmented data from \\(D_n\\) to generate five different \\(g\\) and \\(h\\) vectors for each example. We then transfer these vectors back to the local server, which are used as input to train the weighted score fusion network."}, {"title": "3 Experiments & Results", "content": ""}, {"title": "3.1 Datasets", "content": "As base dataset \\(D_b\\), we use WASE-Normals (WASE) [2], a large multi-site proprietary dataset of 2,009 healthy volunteers acquired at 18 sites from 15 countries."}, {"title": "3.2 Implementation & Training", "content": "View classifiers use ResNet18 [5] architecture and were trained for 200 epochs on a NVIDIA GeForce RTX 2080Ti with cross-entropy loss, Adam optimiser, batch size of 64, and learning rate of 1e-3. The score fusion networks were trained for 50 epochs with the same settings. Weights from the epoch with the highest validation accuracy were saved. Hyperparameters were tuned on a held-out validation set. We evaluate classification performance using accuracy and F1-score metrics. Our method is implemented in PyTorch, and the code is available here:\nhttps://github.com/kitbransby/class-incremental-learning-echo"}, {"title": "3.3 Comparison to Existing Methods & Ablation Study", "content": "We validate our method on an internal test set (WASE, CAMUS, Medstar, StG) to measure performance on incremental datasets seen during training, and an external set (MAHI, UoC) of data not seen during training to measure generalisability. Results are presented in Tables 2 and 3, respectively.\nWe demonstrate that naively fine-tuning a classifier on successive datasets results in \"catastrophic forgetting\". We test two configurations, one where the classifier head is replaced at every step, and a second that expands to accommodate novel classes [13]. These do not require data transfers and perform well on the final dataset (StG) but forget learning from previous data. We also explore other methods that do not require data transfer such as using a single expert, and find they perform well on i.d. data but do not generalise well on o.o.d data. Our primary comparison is to the training oracle where the model is retrained with all data combined at each incremental step. This method is preferred given"}, {"title": "4 Conclusion", "content": "Echo view classifiers require training on diverse multi-site data, and frequent updates with newly acquired data to avoid model drift. We address the problem of updating such a model using multiple datasets with partly overlapping label sets that are from different sites and may not be simultaneously available. We propose attn-wSF and nmd-wSF, two CIL methods that train and combine predictions from multiple experts models with a learnt i.d. weighting which minimises the influence of o.o.d predictions from \"unqualified experts\". Our results demonstrate improved view classification with significantly reduced cumulative"}]}