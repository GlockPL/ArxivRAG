{"title": "FlipConcept: Tuning-Free Multi-Concept Personalization for Text-to-Image Generation", "authors": ["Young-Beom Woo", "Sun-Eung Kim"], "abstract": "Recently, methods that integrate multiple personalized concepts into a single image have garnered significant attention in the field of text-to-image (T2I) generation. However, existing methods experience performance degradation in complex scenes with multiple objects due to distortions in non-personalized regions. To address this issue, we propose FlipConcept, a novel approach that seamlessly integrates multiple personalized concepts into a single image without requiring additional tuning. We introduce guided appearance attention to accurately mimic the appearance of a personalized concept as intended. Additionally, we introduce mask-guided noise mixing to protect non-personalized regions during editing. Lastly, we apply background dilution to minimize attribute leakage, which is the undesired blending of personalized concept attributes with other objects in the image. In our experiments, we demonstrate that the proposed method, despite not requiring tuning, outperforms existing models in both single and multiple personalized concept inference.", "sections": [{"title": "I. INTRODUCTION", "content": "Text-to-image (T2I) generation has made significant progress in creating images based on user-provided personalized concepts. At the same time, combining multiple concepts into a single image remains challenging, as it requires harmoniously arranging multiple concepts in line with the input text's intent while preserving their unique quality and identity. Custom Diffusion [12], a representative approach in personalization, fine-tunes the cross-attention layers of a pre-trained diffusion model using multiple personalized images to effectively adapt to new concepts. Nevertheless, such tuning-based methods [5], [6], [10], [12]-[14], [22], [24], [25] are prone to overfitting and face significant challenges in generalizing when tasked with combining multiple concepts into a single image. To resolve these problems, recent research has increasingly focused on exploring tuning-free approaches [2], [3], [16].\nHowever, despite their remarkable achievements, as shown in Figure 1, existing methods still frequently fail to manage interactions among multiple objects effectively. In particular, in complex scenarios, such as balancing multiple objects within an image, these methods often fail to maintain the relationships between concepts or handle non-personalized regions appropriately, leading to unintended mixing, distortion, or modification.\nTo address these challenges, we propose a novel method called FlipConcept, which consistently generates images containing multiple personalized concepts using just a single reference image per concept, without requiring additional fine-tuning. Our framework operates in two distinct stages.\nIn the first stage, we prepare the input data for the model. Specifically, we generate a background image and extract masks from it. Then, we perform Edit-Friendly DDPM inversion [9] on the prepared background image and the personalized concept images. This process yields edit-friendly latent representations and masks with accurately defined regions.\nIn the second stage, we leverage the latents and masks obtained in the previous step to generate an image that seamlessly integrates the personalized concepts with the background. To accomplish this effectively, we introduce three core techniques. During the image generation process, Guided Appearance Attention reconstructs the keys and values of personalized concept images by referencing those from other images, thereby amplifying the influence of personalized concepts on overall appearance. Mask-Guided Noise Mixing then selectively integrates noise predictions based on extracted masks, ensuring that only regions containing the personalized concepts are modified while non-personalized areas remain unaffected. Finally, Background Dilution applies an extended object mask to reduce the influence of regions outside the designated areas, mitigating concept leakage during self-attention operations and maintaining high image quality by minimizing interference with non-concept regions.\nFlipConcept provides an efficient and modular approach for multi-concept personalization in diffusion models, enabling flexible addition or modification of personalized concepts without requiring additional tuning. It addresses the overfitting issues commonly observed in tuning-based approaches and avoids the computational overhead of retraining. Comprehensive experimental findings verify that our method effectively generates personalized images in complex scenarios while preserving non-edited areas. For example, in scenarios involving multiple characters and objects, our method ensures the coherence of each concept and maintains the integrity of the background. Furthermore, the generated images achieve high"}, {"title": "II. RELATED WORKS", "content": "A. Diffusion-based Text-to-Image Generation.\nDenoising diffusion models [7], [23] and their various extensions [17], [20] have made remarkable progress in the field of text-to-image (T2I) generation. By leveraging powerful text encoders such as CLIP [18], it has become possible to synthesize high-quality images conditioned on textual prompts. Recently, methods specialized for high-resolution generation [20], classifier-free guidance for enhanced controllability [8], and improved models adopting large-scale diffusion backbones [17] have also been proposed.\nB. Image Editing with Diffusion Models.\nIn an effort to harness the outstanding image-generation capabilities of diffusion models, various editing techniques have been proposed. For example, MasaCtrl [3] demonstrates that freezing the keys and values in the self-attention layers of the denoising network [21] can more faithfully preserve the original appearance of an image. Subsequently, methods that manipulate self-attention layers to better maintain the shape of a concept [2], [3], [16] have also been introduced. Such approaches enable localized editing and global style transfer without the need to retrain the entire model, but their performance on complex scenes with multiple objects has yet to be sufficiently verified.\nC. Multi-Concept Personalization.\nGenerating images that incorporate multiple user-specific concepts is an important yet challenging task. Textual Inversion [5] introduced a technique that optimizes text embeddings to represent novel concepts, and subsequent research has investigated approaches such as full [22] or partially fine-tuning the model parameters [12] or inserting rank-one edits [24]. Although training multiple concepts simultaneously may lead to concept blending or partial loss, recent studies aim to mitigate these issues via weight merging [6]. Concept Weaver [13] and DreamMatcher [16] offer a training-free approach that combines multiple concepts during sampling, though certain methods may still require inversion or fine-tuning during inference. By proposing a novel approach that minimizes additional optimization steps while flexibly generating high-quality images of multiple concepts, our work extends the context of prior research."}, {"title": "III. PRELIMINARY", "content": "A. Latent Diffusion Models\nIn this study, we implement a text-to-image (T2I) generation model based on the Latent Diffusion Model (LDM) [20]. It operates in a compressed latent space instead of using high-dimensional images, thereby achieving both computational efficiency and high-resolution image generation. An input image is converted into a latent vector via an autoencoder and is subsequently reconstructed into the original image through a decoder."}, {"title": "IV. METHOD", "content": "In this section, we propose an overall image generation framework called FlipConcept, which can naturally integrate multiple user-defined personal concepts into a single image using a single model, without any additional tuning process. This framework organically combines various personal concepts into a single scene to ultimately produce a high-quality outcome that reflects the user's intention. Through this approach, it is possible to go beyond the limitations of object editing in existing methods and reflect more complex and rich scenarios or contexts in the image, thereby expanding the scope of the model's applicability by simultaneously handling multiple concepts with a single model.\nInspired by a step-by-step image generation approach [13], this framework goes through two main processes in total. In the first stage, we create latent representations and masks for image generation. Initially, we obtain a background image that aligns with the intended purpose. This image is then fed into a text-segmentation model to extract both an object mask, which defines the target object's region, and a background region mask. Next, we transform the prepared background image and the personal concept images into latent representations and then replicate the background latent representation. In the second stage, we feed all the latent representations and masks obtained from the previous steps into the diffusion model to generate the background image $I_{out}$ that naturally reflects the personal concepts. Fig.2 illustrates the overall structure of the proposed framework, and the subsequent sections will provide more detailed explanations of these key steps.\nA. Input Preparation\n1) Background Image and Mask Generation: Most existing approaches [5], [6], [12], [24] tend to randomly generate the background regions, excluding those defined by the personal concepts. In contrast, our approach produces candidate background images and then allows the user to directly choose among them. To achieve this, we utilize Stable Diffusion XL [17] model to generate a variety of background images, from which the user selects the desired scene. At this stage, the chosen background image must clearly include the specific object that will be modified during the subsequent editing process.\nOnce the background image is prepared, masks for the regions of the target objects and the background excluding those objects are extracted. In this process, we use the Grounded SAM package [19], which combines Grounding DINO [15] for open-set detection and Segment Anything Model (SAM) [11] for promptable segmentation. Specifically, Grounding DINO generates bounding boxes for the objects desired by the user based on the text prompt, which are then used as conditions to guide SAM for fine-grained object segmentation. The generated masks represent the location and extent of specific entities mentioned in the text prompt, precisely defining the object regions.\nAfter generating the masks for each object, the background mask is defined as the region outside the combined object"}, {"title": "V. EXPERIMENTS", "content": "In this section, we present the experimental results that verify the performance of the proposed method from various perspectives. By conducting a comprehensive evaluation based on both quantitative and qualitative metrics, comparisons with representative models, and an ablation study, we demonstrate that our approach outperforms existing methods. For this purpose, we utilized the Stable Diffusion V2.1 base model [20], and all images were generated at a resolution of 512 \u00d7 512 using two NVIDIA A40 GPUs. Nevertheless, for some recently introduced models such as Mix-of-Show [6], we found that their proposed approaches could not be straightforwardly adapted to Stable Diffusion V2.1, leading us to rely on the official source code and configurations provided in their respective papers.\nA. Experimental Settings\nWe utilize the Custom Concept 101 dataset [12] as well as images generated via Stable Diffusion [20]. The Custom Concept 101 dataset comprises multiple scenarios built from a variety of conceptual categories such as animals, humans, natural landscapes, and objects, and we prepared five sets of"}, {"title": "VI. CONCLUSION", "content": "We presented FlipConcept, a novel approach that integrates multiple personalized concepts into a single image without additional fine-tuning. By leveraging a step-by-step process with our three core techniques guided appearance attention, mask-guided noise mixing, and background dilution, we achieve a more robust representation of both image structure and personalized concepts. Consequently, our method excels at text-guided multi-concept editing, preserving each concept's integrity while avoiding undesired alterations to other image regions. FlipConcept demonstrates superior performance over existing methods in combining multiple personal concepts into a single scene, minimizing concept leakage and background distortion. Moreover, FlipConcept is broadly applicable to AR/VR, e-commerce, and entertainment. For instance, AR/VR environments benefit from real-time integration of personalized concepts for immersive experiences; in e-commerce, FlipConcept flexibly merges products with various backgrounds for richer visual representations; and in the entertainment domain, it enables swift and precise generation or editing of multiple characters and objects in complex scenes. As future work, we plan to extend FlipConcept by validating and improving its performance in high-resolution image generation and complex multi-concept scenarios (e.g., scenes with overlapping multiple objects or multiple persons). These efforts will not only demonstrate FlipConcept's potential to surpass existing multi-concept personalized image generation methods but also establish a new benchmark for advanced text-to-image systems."}]}