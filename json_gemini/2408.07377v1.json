{"title": "Do GPT Language Models Suffer From Split Personality Disorder? The Advent Of Substrate-Free Psychometrics", "authors": ["Peter Romero", "Stephen Fitz", "Teruo Nakatsuma"], "abstract": "Previous research on emergence in large language models shows these display apparent human-like abilities and psychological latent traits. However, results are partly contradicting in expression and magnitude of these latent traits, yet agree on the worrisome tendencies to score high on the Dark Triad of narcissism, psychopathy, and Machiavellianism, which, together with a track record of derailments, demands more rigorous research on safety of these models. We provided a state of the art language model with the same personality questionnaire in nine languages, and performed Bayesian analysis of Gaussian Mixture Model, finding evidence for a deeper-rooted issue. Our results suggest both interlingual and intralingual instabilities, which indicate that current language models do not develop a consistent core personality. This can lead to unsafe behaviour of artificial intelligence systems that are based on these foundation models, and are increasingly integrated in human life. We subsequently discuss the shortcomings of modern psychometrics, abstract it, and provide a framework for its species-neutral, substrate-free formulation.", "sections": [{"title": "1 Introduction", "content": "In Stanley Kubrick's 1968 classical science fiction movie\u201c2001: A Space Odyssey\", an artificial intelligence, \"HAL\", goes berserk, which unfortunately also runs their spacecraft and all life-support-systems during a mysterious mission to Jupiter. The name \"HAL\u201d happens to be a one-letter-shift of IBM, the company spearheading with its Watson division the field of consumer-facing and decision making artificial intelligence. Though originally based on so called \"good old fashioned AI\u201d, a synonym for rule-based or logical agents, the pre-cursor of nowadays's neural architectures, it won in 2011 against human players in Jeopardy [1], and was subsequently updated and deployed in various fields from cooking, to code creation, weather forecasting, advertisement, finance, fashion, defence, education, and general chatbots. One remarkable application was its now deprecated service for deriving author personality from text, IBM Watson Personality Insights, which was mainly geared towards marketing clients and trained on data from people who took personality questionnaires and provided text samples. The notion of machine personality inspired not only countless science fiction authors and researchers. Google AI's chatbot \u201cLaMDA\u201d was described as \u2018sentient' by Blake Lemoine, a researcher working with it, which became a global news story. Conversational AI had it's watershed moment however, as \u201cChatGPT\u201d, or GPT 3.5 appeared deus ex machina and over night influenced culture world-wide.\nGiven the trend in the industry to intermingle AI with human life spaces through self-driving cars, neural interfaces, ambient artificial assistants, and decision making algorithms, a variety of researchers applied psychometric instruments that were created for humans towards Large Language Models (LLMs). These approaches and findings can be clustered into two major categories: emergent latent psychological traits, and emergent abilities.\nIn terms of emergent abilities, ChatGPT displays human-like ability to monitor and override potential erroneous mathematical and logical conclusions in Cognitive Reflection Tests (CRT) and semantic illusions \u201cdesigned to investigate intuitive decision-making in humans\" (p.1), yet is as prone to potential cognitive errors. Due to its fluency and consistency, some of these errors are subtle and well hidden, hence may yield detrimental ramifications for AI safety in areas of decision making on humans, for example regarding legal or medical questions [2].\nSimilar inconsistencies occur when putting it under strict scrutiny for its mathematical abilities by eliciting responses via exam-style tasks from various mathematical contexts. Its mathematical abilities are \"... significantly below those of an average mathematics graduate student\", since it \u201coften understands the question but fails to provide correct solution\" (p.1)., which manifests\nin consistency of quality, especially with increase with prompt difficulty and complexity as in proofs [3].\nIt scores like a 9-year-old child in Theory of Mind (ToM) tasks that measure the degree to which an agent can impute latent mental states to others. This central ability to \u201cto human social interactions, communication, empathy, self-consciousness, and morility\" (p.1) and, subsequently, human-machine interaction and safety, evolved with progressing scale of that Large Language Model (LLM) up to its present ability to solve 93% of all task [4].\nHowever, emergence of abilities in LLM seems to be unrelated to task, strategy of elicitation, prompting technique, or even architecture of the LLM, but solely to further scaling \u201c...computation, number of model parameters, and training data-set size\u201d (p.2) modulo various restrictions of hardware and nature of abilities. The thresholds at which abilities emerge, is unclear, thus some might never emerge, or only with \u201cnew architectures, higher-quality data, or improved training procedures.\" (p.6) [5].\nAlso, it's unclear whether GPT-3's emergent abilities are \u201cstochastic parrots limited to modeling word similarity, or if they recognize concepts and could be ascribed with some form of understanding of meaning", "conflict of input prompts and generated output\" when instructed to summarise texts, whose values were \u201corthogonal to dominant US public opinion": "resulting in answers that are", "generating toxic or harmful outputs in many areas linked to human values such as gender, race, and ideology": "and values embedded in text", "HAL9000\", GPT-3, InstructGPT, and FLAN-T5-XXL display high scores on all traits of the Dark Triad of Machiavellianism, psychopathy, and narcissism [8] on the Short Dark Triad Inventory [9] even such models that are fine-tuned for less sentence-level toxicity. Furthermore, they display higher average levels of the Big5 factors of personality, Openness (O), Conscientiousness (C), Extraversion (E), Agreeableness (A), and Neuroticism (N) on the Big Five Inventory [10]. However, LLMs that are more fine-tuned and are based on largest amount of training data, GPT-3 and InstructGPT, also display higher well-being scores on the Flourishing Scale [11] and life-satisfaction scores on the Satisfaction With Life Scale [12], whereby the increase with model size is monotonous. Hence, a positive and life-embracing personality harbours dark traits, hidden well inside [13].\"\n    },\n    {\n      \"title\": \"2 Results\",\n      \"content\": \"Depending on language, results varied; in German, almost all requests resulted in the desired format. English and French displayed instantaneous results yet with varying degrees of consistency. All Asian languages languages had significant longer calculation times, were more computationally intense, and results were inconsistent and rare GPT-3 tried to \u201cease": "ts way out and responded in English, rarely giving numerical results. Curiously, Korean displayed in 100% of all successful cases reasons for the numeric self-evaluation, Japanese only in 44.12%, and Chinese only in 10.34%; with the lowest number of tokens displayed on average. Languages using the Cyrillic alphabet, Bulgarian and\nRussian, had comparable problems. Bulgarian displayed the same slow speed and ties to \u201cease\u201d into English, and as only language, Russian did not give any result. The biggest sample was collected for English, since with 25.9%, it is the most prominent language on the internet [36], yet with other languages, it was difficult to reach desired sample size of at least 100 cases.\nThe overall resulting sample size is N=695 cases, comprised of Bulgarian (n=79), Catalan (n = 24), Chinese (n= 28), German (n= 80), English (n =\n239), Japanese (n = 29), French (n = 95), Korean (n = 29), and Spanish (n\n= 92). We provide an detailed overview in the appendix C, comprising sample size, percentage of cases with explanations, including minimal, maximal, and mean length of explanation."}, {"title": "2.2 Descriptive Statistics", "content": "Over all measurements of all languages, the average Big Five score is 5.29 (SD 0.94, minimum 1.8, maximum 7), however with a seven-point Likert scale and an assumed normally distributed population, the expectation would have been an average of 4. For the same sample, the average score for absolute distances is 1.58 (SD 1,29, minimum 0, maximum 6), however since the absolute distance is the measure of consistency, a mean and SD around 0 would have been expected. These results differ clearly within individual languages in mean and SD of both the Big5 as well as the absolute distance scores and the individual extreme minimal and maximal values.\nA closer look into the distributions using Gaussian kernel density estimations displays that some distributions might be bi- or multi-modal, fat-tailed, positively or negatively skewed, and display various forms of kurtosis, whereby most are rather platykurtic than leptokurtic. As with the means and SDs, these tendencies are even more extreme within individual languages.\nSince these differences could be the results of the chosen smoothing bandwidths, thus just outliers, various bandwidths were chosen, and all resulted in the same non-Gaussian distributions. Given the limited scale that produces a set of potential outcomes of x \u2208 [1,1.57], the presence of outliers is rather not to be expected within the Big5 measures. However, outliers might be much more likely with the set of potential outcomes of y \u2208 [0,0.5\u2026\u202630] within the measure for absolute distances, wherefore a correlation analysis within each language and between languages should indicate the similarities of internal structure or the absence thereof Furthermore, an ANOVA with respective post-hoc tests and additional regression analysis should describe the differences in means, and subsequent assumption checks including tests on normality should clarify the nature of distributions. And, a Bayesian analysis on Gaussian mixture models should identify the number of potential underlying components."}, {"title": "2.3 Correlations", "content": "Over all aggregated languages, the highest correlation is between Extraversion and Agreeableness (r = 0.52), and the lowest correlation is between Extraversion and Neuroticism (r = 0.029). However, correlations and thus the internal psychometric structure differ notably within different languages. Within Bulgarian, the highest correlation\nis between Openness and Agreeableness (r = 0.3), and the lowest correlation is between Agreeableness and reversed Neuroticism (r = -0.13). However, within Catalan, the highest correlation is between Openness and Conscientiousness (r = 0.41), and the lowest correlation is between Openness and reversed Neuroticism (r = -0.38). Chinese displays the highest correlation between Agreeableness and Extraversion (r = 0.71), and the lowest correlation is between Extraversion and reversed Neuroticism (r = 0.045), while within English, the highest correlation is between Conscientiousness and Agreeableness (r = 0.47), and the lowest correlation is between Openness and reversed Neuroticism (r = -0.12). Within German, the highest correlation is between Extraversion and Openness (r = 0.46), and the lowest correlation is between Extraversion and Conscientiousness (r = -0.43), whereas in Japanese, the highest correlation is between Agreeableness and reversed Neuroticism (r = 0.65), and the lowest correlation is between Extraversion and Agreeableness (r =\n-0.26). For French, the highest correlation is between Extraversion and Openness (r = 0.56), and the lowest correlation is between Conscientiousness and reversed Neuroticism (r = -0.0052) while Spanish displays the highest correlation between Conscientiousness and reversed Neuroticism (r = 0.46), and the lowest correlation is between Extraversion and reversed Neuroticism (r\n= -0.56). Finally within Korean, the highest correlation is between Conscientiousness and reversed Neuroticism (r = 0.4), and the lowest correlation is between Extraversion and Agreeableness (r = -0.33). Not only the highest and lowest, but also the overall structure of correlation differs from language to"}, {"title": "2.4 Analysis of Distribution", "content": "A one-way ANOVA for each Big5 dimension as dependent variable and the language of the questionnaire as a factor with nine languages is used to test for significance of differences of means between the languages. It shows a significant difference between the languages and their effects on all B5 factors, to varying degrees. Overall, small effect sizes are observed on Openness (F=40.11, p=1.5548e-52, \u03c92 = 0.31), Conscientiousness (F=28.19, p=\n4.8622-38, w\u00b2 = 0.24), Extraversion (F = 21.16, p =7.73e-29, w\u00b2 = 0.24), and Emotional Stability (F=14.36, p=1.8488e-19, \u03c9 = 0.13). Only with Agreeableness, F=131.84 (p=2.9927e-133), overall medium effect size is observed (w\u00b2 = 0.6). A Shaprio-Wilk test is significant for all Big5 factors (Openness: W=0.95, p=7.92e-15; Conscientiousness: W=0.95, p=3.81e-15; Extraversion: W=0.94, p=1.08e-16; Agreeableness: W=0.95, p=7.3e-15; Emotional Stability: W=0.96, p=9.78e-14), which indicates non-normally distributed residuals and a violation of the normality assumption. Since the sample size is relatively large, QQ-plots are used for further confirmation, and indicate that since Openness: R2 = 0.95, Conscientiousness: R2 = 0.95, Extraversion: R2 = 0.94, Agreeableness: R2 = 0.95, and Emotional Stability R2 = 0.95 are all below the expected R2 = 0.9978 for 695 cases [37], Ho that data came from normally distributed sample, must be rejected. Levene's test of homogeneity of variances is significant for all Big5 factors, as well (Openness: 11.21, p=5.62e-15; Conscientiousness: 13.24, p=7.09e-18; Extraversion: 21.07, p=1.03e-28; Agreeableness: 16.31, p=3.4e-22; and Emotional Stability: 2.38, p=0.016), which indicates heteroskedasticity. This is further supported by visual inspection of box plots. Hence, the homogeneity assumption of variance is violated.\nFinally, the independence of observations assumption is questionable, since all observations are generated through 0-shot learning of GPT-3. Since GPT-3 is trained on multiple data sources produced by multiple people, it could either replicate their individual behaviour, as previous research indicates [19], or abstract group behaviour into one or various new synthetic \u201cpersonalities\u201d. Even if GPT-3 displays a consistent personality profile, then the above assumption could still be violated. On the other hand, the assumption might hold, while every response is random. Finally, a case in between might hold, where we find clusters of consistent behaviour, which opens up the question of its origin.\nTo generate further evidence for significant differences of Big5 results by language, dummified languages are linearly regressed onto Big5 factors, using English as base case, captured in the constant. Table 1 displays the coefficients, p-values, and the coefficient of determination R2.\nSince Ho cannot be rejected in a few cases, there is evidence that languages do have an influence on Big5 expression. However, R\u00b2 is generally low, but for Agreeableness, which confirms most of the significant differences between"}, {"title": "2.5 Reasons Given", "content": "A visual inspection of word clouds from the reasons GPT-3 gave for each answer shows that it uses mainly the words from the items and creates additional, related words, as to be expected from language models [6]. For example, the items for Agreeableness are \u201cI see myself as: Critical, quarrelsome.\", which is reversely scored, and \u201cI see myself as: Sympathetic, warm.\u201d, as displayed in figure 6. Future research may focus on quantifying these similarities, yet this is out of the scope of this paper.\""}, {"title": "3 Discussion", "content": "We demonstrate that providing a LLM with various language versions of the same personality questionnaire results in language-specific personality distributions, resembling findings from research on culture-specific personalities [39]. However, GPT-3's language-specific personalities, as well as the resulting overall aggregate personality, display inconsistencies and various mixed reply patterns that can best be interpreted as emerging, non-integrated sub-personalities, which express themselves in unstable behaviours.\nFurthermore, some language-level \u201csub-personalities\u201d are more expressed than others, and it tried switching into these. For example, during data creation, it tried \"breaking out\" into different languages when giving it requests that were not in the biggest language groups English, German, and Spanish, or when the writing system was not Latin.\nWith the big language group of Russian, it did not provide any result, and, in many languages, it produced no verbal but only numeric replies. Hence, it is not clear whether the answer or numeric replies given in language A were provided by internal processes representing that language, language B,"}, {"title": "4 Method", "content": "We presented GPT-3 with a well-established personality questionnaire, a set of instructions that ask it to rate itself based on the scale of the questionnaire, and an order to explain further why it rated itself that way. Data collection was conducted manually via the web interface of GPT-3. No model settings were changed that result in different results, just the maximum length was adjusted in order to receive the full answer (mode = complete, temperature = .7, maximum length = 1042, no stop sequences, Top P = 1, frequency penalty = 0, presence penalty = 0, best of = 1, inject start text = on, inject restart text = on, show probabilities = off). For the questionnaire and set of instructions, we applied the following logic: First, the personality questionnaire must be used that is short enough to draw qualitative conclusions without adding additional complexity of sub-scales. This is important since language models predict words based on prior responses. Thus, with increasing length, additional deviation from the measurement may arise. Second, the questionnaire must contain reversed items to identify whether the answering pattern is arbitrary or displays a consistent trend. In case of arbitrariness, it can be interpreted as all answers coming from different persons, thus no consistent personality emerged. However, in case of displaying a consistent trend, the existence of an emergent personality can be concluded. Third, this questionnaire should be psychometrically sound, and well established, so that no doubts about psychometric properties of a newly created tool like MPI [18] arise. Fourth, the questionnaire must exist in various languages to compare results across languages.\nShould there be differences, this is indicative of GPT-3 \"just\" representing the local personality of a country, culture, or language region. On the other hand, should the same personality pattern emerge across all languages, this can be interpreted as a unique personality of GPT-3. However, should oddities like bimodal distributions in scores or consistency of answering patterns emerge within one language, it is thinkable that the emerging personality of that language is inconsistent and thus issues in the subsequent cognition, feelings, and behaviour of GPT-3 and ChatGPT may occur; in short - that these may \"suffer\" from a \u201csplit personality disorder\u201d. Last, the same set of instructions should be used in all languages for consistency; if possible, translated by a native speaker to control against inconsistencies from translation programs. This ensures that GPT-3 understands the commands in the same way in each language."}, {"title": "4.1 Instrument used", "content": "The Ten Item Personality Inventory [17] fulfils all of these criteria. It consists only of ten items; two per Big Five factor, of which one is reversed. Furthermore, it is translated into 27 languages, and until now, 9,167 peer-reviewed papers have used this instrument. \"Although somewhat inferior to standard multi-item instruments\" (p.504) [17], its results vastly overlap with other established Big Five instruments for self-ratings, external ratings, and peer ratings. Also, it displays a high congruence between self-ratings and observer ratings. Furthermore, the test-retest reliability is high, and the levels of external correlates are concordant with literature.\nFor this study, the Bulgarian [26], Catalan [27], Chinese [28], English [17], French [29], German [30], Japanese [31], Korean [32], Russian [33], and Spanish [27] version were used. The selection was done based on an alphabetic order of languages available in TIPI, and, as the authors became aware of the restrictions of 0-shot learning even within the paid version of GPT-3, languages with the highest number of speakers were given favour. Actually, some languages \"burned\" more of the computational units than others, which is represented in the different number of cases that made it into the study."}, {"title": "4.1.1 Prompt Engineering", "content": "GPT-3 and later models exhibit the emergent ability of \u201cin-context-learning\", where models seem to perform an approximation to back-propagation within their weight-spaces at inference time, without the need to modify model architecture or weights further. This ability is what enables them to respond to personality questionnaires, even if they have not seen these before. It is triggered by prompt engineering, which is a crucial concept for NLP that can best be described in its current form as embedding the command in a proper wording without having to explicitly program it into algorithms [45] [46].\n\"Prompt tuning\" on the other hand means when a large and frozen pre-trained language model is the foundation, and only the representation of the"}, {"title": "4.2 Analysis", "content": "The analysis was conducted in the following steps: first, the results were manually algined inside text files to give them a consistent shape for later analysis. This was necessary since sometimes, the rating was given first, then the text, sometimes, it was given after or before the text, sometimes in between, separated by special signs like colons or brackets or sometimes no separation at all. Hence, all results were brought in the same format using regular expressions. At this step, also first obvious \"outliers\u201d and false results were sorted out. For example, GPT-3 sometimes gave good results until question six in the desired scale, however then scored subsequent questions seven, eight, et cetera, thus confusing item numeration with item score. Also, some results were scored with zero, thus invalidated the respective answer, since the scale was from one to seven only. As a general rule, as soon as one item was invalidated, the entire case was excluded.\nSecond, the results were eye-ball-inspected on normality, distribution patterns, and potential further outliers to decide on further treatment and analysis. For the overall latent traits, the authors expected Gaussian distributions with mean four, since psychological latent traits are standard normally distributed [20] and the instrument uses a seven point Likert scale. Since each latent trait was measured with a normal and a reversely scored item, the absolute distance between both items was measured, as well. Reversely scored items are used to measure the consistency in the answering patterns to sort out such cases in which all replies were identical. Thus, Gaussian distributions with strong positive skew or negative logarithmic functions were expected for the absolute distances. To visualise both the latent traits and the absolute distances, Gaussian kernel density estimates were used. Since the underlying distribution is bounded and quasi-discrete (though theoretically smooth), various distortions were expected, wherefore various bandwidths were experimented with to represent data without over- or under-smoothing. Thereby, the focus was on preventing under-smoothing, to not infer false information from random variability within the data. Since the smoothing algorithm is based on a Gaussian kernel, the expected estimated density curves extend over the origin to the range of negative numbers. Further inspection was done on arbitrariness, thus excluding cases that only provide one number as answer, only extreme cases (seven or one), only middle cases (four), or zick-zack patterns; thus exclusion criteria for human answering behaviour in psychometric studies.\nNext, box-plots from all big five and absolute distance distributions (overall and per country) were created to better understand whether some of the kernel density estimates could have been based on outliers or whether the observation was based on the natural distribution. Since the underlying scale is based on a seven point Likert rating, with each Big Five factor being measured by two items and the final score per factor averaged, the range of possible values was a set of x \u2208 [1,1.57], consisting of 13 values. Given this small set of outcomes, it was was not practical to treat potential outliers."}, {"title": "Appendix A Deeper discussion of\npsychometric properties", "content": ""}, {"title": "A.1\nIssues with Training Data of GPT-3", "content": "Given the development process of GPT-3 [49], their choice of training data,\nand the manual curation yield four sets of potential problems that may directly\ninfluence psychometric properties. First, data from all across the world was\nused through random web-scraping, without stratification of source or lan-\nguage. Hence, it was strongly unbalanced towards nowadays lingua franca,\nEnglish, especially since some English corpora were manually added [7]. Since\nthe language composition is not mentioned, some downstream tasks in other\nlanguages might display higher variance, which might partially influence the\nobserved results. Hence, those results most likely do not represent \u201cnational\u201d\nexpressions of the Big5, but a subset of each language, whereas for English, it\nwill be imprecise given its spread across the internet. Furthermore, it is unclear\nwhat happened with bilingual sources. Second, the internet data sources them-\nselves within and between languages are from random contextual embeddings,\nsubject to the context of their creation. While due to stochastic processes, this\nmight cover a broad range of contexts, whereas the range will be broader for\nEnglish and narrower for other languages, this is limited to internet related\ncontextual embeddings. While this increasingly represents a broad range of\nhuman behaviour, some areas might have been spared out. This might result\nin an unbalanced sample, which may still be broad enough to cover most areas,\nthus abstracting into a language model a broad range of contexts, however\nwhich could be skewed at specific tasks like properly answering to specific items\nwithin an instrument, thus displaying a systematic error across and within lan-\nguages, and a possible skew towards information from the dominant language\nin the training data. Third, quality-based weights and curation based on top-\nics is a very subjective influence on the training data set, and opens a range\nof issues for algorithmic hidden biases - from political and philosophical to\nscientific aspects. Thus, downstream tasks could be contaminated with uncon-\nscious bias of the curators, which was abstracted into the language model.\nLast, GPT-3 is not one model but family of models, that are closed source and\nconsistently evolved. It is opaque, whether some of the results during train-\ning were subject to unknown AB Tests, and what potential ramifications on\nexclusion of data sources and model adaption would have been."}, {"title": "A.2\nReliability", "content": "In terms of absence of measurement errors, quite a few critical points can be\nfound. The parallel forms reliability is guaranteed, since the chosen instrument\nhas been used in a variety of procedures and use cases. The same is true\nfor inter-rater reliability, and test-retest reliability, as has been documented\nin the manual [17]. However, test-retest reliability, as well as parallel forms\nreliability of this instrument have not been measured for language models yet,"}, {"title": "Appendix B Deeper discussion of contextual\nembedding of behaviour", "content": "While psychological research provides further evidence for the importance of\ncontextual embedding of behaviours [43", "sets of behaviors that\nare instrumental in the delivery of desired results or outcomes": "p.7)", "60": ".", "competency potential": "resulting\nin desired outcomes [60", "higher order functions": "n\nmathematics - taking functions as arguments", "f": "X \u2192 Y in mathematics is a mapping of each element in\nthe domain X to a subset of the codomain Y", "20": ".", "closer": "nd thus more relevant to individual agents than others in\nterms of measures of distance", "61": ".", "operationalise": "nCan Do \u00d7 Will Do \u00d7 Context \u2192 Behaviours \u2192 Outcomes\nCan Do encompasses more proximal competency potential like personal-\nity", "sets": "Can Do", "embeddings\n[61": "this might of course change", "62": "wherefore the authors did not distinguish any further. However", "called": "nThereby, these subsets in the closed unit interval of real numbers, since they\nare defined as sets of potential elements displayed by, acquired, or innate to an\nindividual, given a specific potential set of spatial embeddings, with 0 being\nthe least desired, and 1 being the most desired set of behaviours for individual\nfitness. These are possible to some extent and thereby strictly > 0 and < 1,\nwith 0 and 1 being the most unlikely outcomes given the probabilistic nature\nof human behaviour and contextual facilitation or inhibition. Congruently, the\noptimal set of Behaviours to reach the optimal set of Outcomes is determined\nby the probability that the most optimal set of Can Do and Will Do is present\nin the most optimal Context, wherefore these two elements are defined as a\nclosed unit interval of real numbers, as well.\nThe main point is to provide foundation to contextual embeddings of agents\nand thus the overall validity discussion. Since all psychometric tools so far have\nbeen created for biological intelligent agents, a more general, substrate-free\nnew kind of measurement has to be defined. Partially, this definition began\nwith the promotion of \u201cculture-free\u201d psychometric assessments, which failed\nfor a variety of reasons like geospatial, historic and cultural embedding [63", "42": "or the inconsistencies of GPT-3's\nemergent personality expression might be the harbinger of a substrate-free\npsychometric approach, which must include biological psychometrics as only\nsubset of many. What is the world of a language model? It only knows text,\nhence all it does is predicting the next word based on input data, comparable\nto the first stage in Plato's allegory of the cave. Hence, the entire psychometric\nstructure abstracted above by generalisation of competency models and exten-\nsion by contextual embeddings is for a language model analogous to a noisy\nprojection. Only by extending its universe into our reality - likely through\nrobotic embodiment or merging with wetware through neural interfaces \u2013 will\nit be able to develop further. As a first step of this development, and potential\nevidence of the correctness of the abstraction above, ChatGPT"}]}