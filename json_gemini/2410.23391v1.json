{"title": "Understanding Representation of Deep Equilibrium Models from Neural Collapse Perspective", "authors": ["Haixiang Sun", "Ye Shi"], "abstract": "Deep Equilibrium Model (DEQ), which serves as a typical implicit neural network, emphasizes their memory efficiency and competitive performance compared to explicit neural networks. However, there has been relatively limited theoretical analysis on the representation of DEQ. In this paper, we utilize the Neural Collapse (NC) as a tool to systematically analyze the representation of DEQ under both balanced and imbalanced conditions. NC is an interesting phenomenon in the neural network training process that characterizes the geometry of class features and classifier weights. While extensively studied in traditional explicit neural networks, the NC phenomenon has not received substantial attention in the context of implicit neural networks. We theoretically show that NC exists in DEQ under balanced conditions. Moreover, in imbalanced settings, despite the presence of minority collapse, DEQ demonstrated advantages over explicit neural networks. These advantages include the convergence of extracted features to the vertices of a simplex equiangular tight frame and self-duality properties under mild conditions, highlighting DEQ's superiority in handling imbalanced datasets. Finally, we validate our theoretical analyses through experiments in both balanced and imbalanced scenarios.", "sections": [{"title": "1 Introduction", "content": "Recently, there has been significant research on implicitly-defined layers in neural networks [1, 2, 3, 6, 9, 20, 21, 49], where the output is implicitly mapped from the input under certain conditions. These layers embed interpretability and introduce inductive bias [26] into black-box neural networks, demonstrating superior performance compared to existing explicit layers.\nAmong these implicit networks, the Deep Equilibrium Model (DEQ) is a memory-efficient architecture that represents all hidden layers as the equilibrium point of a nonlinear fixed-point equation. Due to the absence of a closed-form solution in its forward process, DEQ can be viewed as having an infinite number of layers during iteration as long as the threshold is set low enough, enhancing its ability to fit input data. Consequently, its representational capacity is relatively stronger compared to a single-layer network structure. This phenomenon explains why DEQ has achieved state-of-the-art results in classification tasks compared to existing architectures like ResNet. For instance, it has been successfully applied to language tasks and image classification tasks, reaching state-of-the-art performance. Additionally, DEQ can be applied in various domains and integrated with numerous other models, including inverse problems [19], Neural ODEs [11], diffusion models [24, 43], Gaussian processes [17], and more.\nHowever, recent research reveals a phenomenon called Neural Collapse (NC) concerning the learned deep representations across datasets in image classification tasks [42]. Under the NC regime, the last-"}, {"title": "2 Background and related works", "content": "We consider a classification task with $K$ classes. Let $n_k$ denote the number of training samples in each class $k$, and $N = \\sum_{k=1}^K n_k$ represent the total number of training samples. A traditional neural network can be expressed as a mapping:\n$\\psi(x) = W\\phi(x) + b,$\nwhere $\\phi(x) : \\mathbb{R}^{in \\times N} \\rightarrow \\mathbb{R}^{D \\times N}$ is the feature extraction, $W \\in \\mathbb{R}^{K \\times D}$ and $b \\in \\mathbb{R}^{K}$ are the classifiers and bias in the last layer, respectively. For simplicity, we consider the bias-free case and omit the term $b$. Besides, we will denote $H = \\phi(x)$ in later sections."}, {"title": "2.1 Deep Equilibrium Models", "content": "There have been numerous neural network architectures designed for various practical tasks from different perspectives [17, 32, 38, 39, 48]. DEQ, a typical implicit network [13, 52], incorporates unrolling methods [12, 41], which are devised for training arbitrarily deep networks by integrating all the network layers into one [3, 4, 5, 35, 36, 58]."}, {"title": "2.2 Neural Collapse NC", "content": "The phenomenon of NC was initially uncovered by [42], which is considered an intriguing regularity in neural network training with many elegant geometric properties [50, 61, 66]. When the model is at the terminal phase of training (TPT), or more precisely, achieves zero training error, the within-class means of features and the classifier vectors converge to the vertices of a simplex Equiangular Tight Frame (ETF) on a balanced dataset.\nDefinition 2.1. (Simplex Equiangular Tight Frame) A collection of points $s_i \\in \\mathbb{R}^{D}, i = 1, 2, \\dots, K$, is said to be a simplex equiangular tight frame if\n$S = \\sqrt{\\frac{K}{K-1}} P (I_K - \\frac{1}{K} 1_K 1_K^T),$\nwhere $\\alpha$ is a non-zero scalar, $S = [s_1,\\dots,s_K] \\in \\mathbb{R}^{D \\times K}$, $I_K \\in \\mathbb{R}^{K \\times K}$ is the identity matrix, $1_K$ is the ones vector, and $P \\in \\mathbb{R}^{D \\times K} (D > K)$ is a partial orthogonal matrix such that $P^T P = I_K$.\nNC incorporates the following four properties of the last-layer features and classifiers in deep learning training on balanced datasets:\nNC1: Variability collapse: The feature within-class converges to a unique vector, i.e., for any sample $i$ in the same class $k$, its feature $h_{k,i}$ satisfies $||h_{k,i} - h_k|| \\rightarrow 0, k \\in [k]$, with the training procedure.\nNC2: Convergence to simplex ETF: The mean value $h^*$ of optimal features for each class collapses to the vertices of the simplex ETF.\nNC3: Convergence to self-duality: The class means and the classifier weights mutually converge: $W^* \\propto H^{*T} \\propto W H^*$\nNC4: Nearest Neighbor: The classifier determines the class based on the Euclidean distances among the feature vector and the classifier weights."}, {"title": "2.3 Layer-peeled model under balanced and imbalanced conditions", "content": "Current studies often focus on the case where only the last-layer features and classifier are learnable without considering the layers in the backbone network under the assumption of Unconstrained Features Mode (UFM) [66], which can also be referred to as the Layer-peeled Model [14, 28]. First, we define the feasible set of parameters:\n$\\mathcal{C} = \\left\\{ W, H \\mid \\frac{1}{K} \\sum_{k=1}^K ||w_k||^2 < E_W, \\frac{1}{K} \\sum_{k=1}^K \\frac{1}{n_k} \\sum_{i=1}^{n_k} ||h_{k,i}||^2 < E_H \\right\\}.$\nDefinition 2.2. (Layer-peeled Model) When $H$ and $W$ are the last layer classifier and weights respectively, then the optimization process of the neural network can be reformulated as:\n$\\min_{W, H} \\frac{1}{N} \\sum_{k=1}^K \\sum_{i=1}^{n_k} L(W h_{k,i}, y_k) \\quad \\text{s.t.} \\quad w_k, h_{k,i} \\in \\mathcal{C},$\nwhere $E_H$ and $E_W$ are two predefined values, $N$ refers to the total number of samples."}, {"title": "3 Comparison under balanced setting", "content": "In this section, we first analyze the NC phenomenon in DEQ under balanced settings. As illustrated in Figure 1, after completing the initial feature extraction, we further examine the feature $H$ obtained respectively by explicit neural networks and DEQ to reveal the NC phenomenon."}, {"title": "3.1 NC in Explicit Neural Networks", "content": "Building upon (6), we analyze NC in explicit neural networks by considering the following constrained optimization problem during training:\n$\\min_{W, W_{EX}, H_0} \\frac{1}{N} \\sum_{k=1}^K \\sum_{i=1}^{n_k} L(W W_{EX} h_{k,i}^0, y_k)$ (7)\ns.t. $|W_{EX}|_F < E_H;w_k, h_{k,i} \\in \\mathcal{C}$,"}, {"title": "3.2 NC in Deep Equilibrium models", "content": "Building upon recent investigations into the NC phenomenon, we embrace the layer-peeled model, where the last-layer features $h = \\phi(x)$ (equilibrium points in DEQ $z^*$) as unconstrained optimization variables. Accordingly, we add the following constraints to enforce NC in DEQ:\n$\\mathcal{C}_{DEQ} = \\{z^*, W_{DEQ}| z^* = f(H^0; W_{DEQ}), ||W_{DEQ}||_F \\le E_H\\}.$\nCompared to explicit layers, the active parameter in Deep Equilibrium models is $W_{DEQ}$, hence imposing restrictions on it to align with the same feasible space. Then the formulation of DEQ with NC becomes:\n$\\min_{W, W_{DEQ}, Z, H_0} \\frac{1}{N} \\sum_{k=1}^{n_k} \\sum_{i=1}^{n_k} L(Wz^*, y_k)$ (9)\ns.t. $w_k, h_{k,i} \\in \\mathcal{C}; z^*, W_{DEQ} \\in \\mathcal{C}_{DEQ}.$\nNo matter whether under DEQ or explicit neural networks, these constraints must be imposed. This is because when these constraints are satisfied and the loss function reaches its lower bound, the NC phenomenon is guaranteed. In our theoretical analysis, we assume that the DEQ is linear, that is, $z^* = fixed\\_point(f_{\\theta}(x), z) = \\sum_{i=0}^\\infty W_{DEQ}x$. Detailed analysis incorporating these constraints is provided in Appendix B.\nThe following theorem elucidates the specific scenarios in which the NC phenomenon occurs. For a fair comparison, we assume that the extracted features $H^0$ of the image encoder are the same in the derivation.\nTheorem 3.1. (Feature collapse of explicit fully connected layers and implicit deep equilibrium models under balanced setting) Suppose (7) and (9) reaches its minimal, then\nNC1: For $\\forall k = 1, 2,\\dots, K$ and $\\forall i = 1, 2,...,n$:\nW_E_Xh^0_{ki} = W_E_Xh^0_{k},\nwhere $h^0_{k} = \\sum_{i \\in \\tau(k)} h^0_{k,i}$. Similarly, if the model is DEQ, then\nf(h^0_{ki}; W_{DEQ}) = f(h^0_{k}; W_{DEQ}).\nNC2: The classifier aligns to the Simplex ETF, regardless of whether explicit neural network and DEQ are applied:\n$W W^T \\propto \\sqrt{E_W / E_H} W W_{EX} H^0 \\propto \\sqrt{E_W / E_H} W f(H^0; W_{DEQ}) \\propto \\frac{KE_W}{\\sqrt{K-1}}(I_K - \\frac{1}{K} 1_K 1_K^T).\nNC3: For $\\forall k = 1, 2,\\dots, K$, the feature aligns to the weights:\nW_E_Xh^0_{k} \\propto W_k.\nIn DEQ cases:\nf(h^0_{k}; W_{DEQ}) \\propto W_k.\nThe theorem demonstrates that when the network training reaches its limit, i.e., when the loss function reaches its minimum, the NC phenomenon emerges regardless of whether the chosen network is DEQ or explicit neural network. Besides, in certain scenarios, the lower bound of the loss function for DEQ is relatively smaller compared to explicit neural networks. More detailed proofs are in Appendix Section B."}, {"title": "4 Comparison under imbalanced setting", "content": "In this section, we analyze the performance differences between DEQ and explicit neural network on imbalanced datasets. We observe that, unlike in balanced scenarios, as long as certain conditions are met, the advantages of DEQ over explicit neural network become more pronounced on imbalanced datasets. And we provide theoretical evidence to support this phenomenon.\nSuppose the total number of classes is $K$, with $K_A$ being the number of majority classes and $K_B = K - K_A$ being the number of minority classes. Each majority class has $n_A$ samples, and each minority class has $n_B$ samples. The total number of samples is given by $N = K_A n_A + K_B n_B$. Note that $n_A > n_B$ with no requirement for $K_A$ to be greater than $K_B$. We first start with the loss function, which can be partitioned into two components as follows:\n$\\min_{W, W, H_0} \\frac{K_A n_A}{N} \\sum_{k=1}^{K_A} \\sum_{i=1}^{n_A} L(W W H_{ki}^0, y_k) + \\frac{K_B n_B}{N} \\sum_{k=K_A+1}^{K_B} \\sum_{i=1}^{n_B} L(W W H_{ki}^0, y_k),$\ns.t. $W \\in \\{C_{ex} or C_{DEQ}\\}, w_k, h_{k,i} \\in C,$\nwhere $W$ represents the weights of Deep Equilibrium Models $W_{DEQ}$ and explicit neural network $W_{EX}$. To analyze the NC phenomenon, we present the results in the following theorem:\nTheorem 4.1. (Neural Collapse under imbalanced settings on explicit neural networks and deep equilibrium models)\nWhen the loss function reaches the minimum, then\nNC1: For $\\forall k = 1, 2,\\dots, K$ and $\\forall i = 1, 2,\\dots,n$:\nW_E_Xh^0_{k_i} = W_E_Xh^0_{k},\nwhere $h^0_{k} = \\sum_{i \\in \\tau(i)} h^0_{ki}$. Similarly, if the model is DEQ, then\nf(h^0_{ki}; W_{DEQ}) = f(h^0_{k}); W_{DEQ}).\nNC2: Not exists, but the results of explicit neural network and DEQ can be compared:\nHere we denote $(h^0_{k})^T h^0_{k'} = m_{k,k'}$ and $S$ is a K-Simplex ETF, if\n$\\frac{1}{E_H} < 2S_{ij} - m_{ij} < \\frac{1}{1 - E_H}$\nis satisfied, the following inequality\n$|| (W_{EX}H^0)^T (W_{EX}H^0) - S||_F > || f^T (H^0; W_{DEQ}) f (H^0; W_{DEQ}) - S||_F$\nholds.\nNC3: Similarly as NC2, though it does not exist, the results can still be compared, when\n$\\frac{E_H}{E_W + \\frac{E_H}{E_W} + E_H(1 - E_H)} < 2$\nis satisfied, then the cosine distance satisfies:\n$\\cos(f(h^0_{k}; W_{DEQ}), w_k) / \\cos(W_{Ex}h^0_{k}, w_k) > 1.$\nThe detailed proof is in Appendix Section C.\nBesides, the conclusion regarding the loss function is quite similar to that of Theorem B.3 under balanced settings. As analyzed in (43) and (44) in the Appendix, the lower bound of the loss function in DEQ is still lower than that in explicit neural network, where the performance of learned features is more evident in Figure 2, where we use t-SNE [53] and Gram matrix of features to describe the performance of two models. Although the phenomenon of NC2 and NC3 does not exist, we have discovered in Theorem 4.1 that under mild conditions, DEQ is superior in terms of NC compared to"}, {"title": "5 Experiments", "content": "In this section, we empirically conducted experiments to validate the correctness of the proposed theorems. Initially, we implemented DEQ on a balanced dataset and compared its NC performance"}, {"title": "5.1 Experiment setup", "content": "Without loss of generality, since any traditional neural network can be formulated as a DEQ, we use ResNet18 [22] as the backbone architecture here. As discussed earlier, to utilize the fixed point $z^*$ learned by DEQ as the extracted feature, we formulate the last ResNet block into a DEQ format, while maintaining the remaining structure identical to ResNet. As mentioned in [5], training with DEQ can lead to instability issues. This is especially noticeable as training progresses, where some samples struggle to converge to a fixed point. To address this, in accordance with their setting, we implement the solver with a threshold $\\epsilon$ set to $10^{-3}$ and introduce an early stopping mechanism. If convergence is not achieved within $T > 20$ iterations, we terminate the fixed-point iteration. Additionally, when facing problematic samples during fixed-point solving, we skip them to ensure training stability. During training, we set the learning rate to $1 \\times 10^{-4}$ and utilize stochastic gradient descent with a momentum of 0.9 and weight decay of $5 \\times 10^{-4}$. Both $E_W$ and $E_H$ are set to 0.01. The training phase for each network consists of 100 epochs, with a batch size of 128. In this context, accuracy is assessed by averaging the results from the last 10 epochs and computing their standard deviation."}, {"title": "5.2 Performance under balanced conditions", "content": "By using the settings in (7) and (9), we compared the performance of DEQ and Explicit NN on Cifar-10 [30] and Cifar-100 [29] for validation, as shown in Figure 3(a). Their NC performances remain comparable, i.e., DEQ achieves results similar to Explicit NN, corroborating the findings of Theorem 3.1. As for accuracy, from the results in the first column of Table 1, it can be observed that DEQ's accuracy is higher than that of the explicit layer, which aligns with Theorem B.3. However, the increase is only marginal due to the fact that the coefficients $E_H$ and $E_W$ act as scaling factors. Therefore, compared to explicit neural network, DEQ finds it challenging to achieve a significantly lower loss and, consequently, a substantial improvement. Moreover, Explicit NN performs well in fitting balanced datasets, so the accuracy of DEQ does not experience a significant boost in this context."}, {"title": "5.3 Performance under imbalanced conditions", "content": "We conducted experiments with varying configurations with different numbers of majority and minority classes and imbalance degrees. Assume the numbers of majority and minority classes are $(K_A, K_B)$ with corresponding sample sizes $(n_A, n_B)$, the imbalance degree is denoted as $R = n_A / n_B$. We considered different setups for majority and minority class quantities, such as (3, 7), (5, 5), and (7, 3). Additionally, we varied the ratio of sample quantities R between majority and minority classes with values of 10, 50 and 100. We also tested the phenomenon of NC and accuracy on the Cifar-10 and Cifar-100 datasets, which own a total of 5000 images for each class. Specifically, when R = 100 and $(K_A, K_B) = (3, 7)$ for Cifar-10, the number of samples for all classes is (5000, 5000, 5000, 50, 50, 50, 50, 50, 50, 50).\nThe results for $(K_A, K_B) = (3, 7)$ are shown in Table 2, where the test dataset owns the same distribution as the training dataset. We use \u201coverall\u201d, \u201cmajority\u201d, and \u201cminority\u201d to represent the results across all categories, the majority class, and the minority class, respectively. We contrasted the difference in the training outcomes between the Explicit Neural Network and DEQ, and the superior performance of DEQ compared to Explicit Neural Network confirms DEQ's higher learning potential. This suggests that DEQ can achieve a lower bound on its loss function. The experimental results indicate that DEQ consistently outperforms explicit neural network in accuracy during imbalanced training, aligning with Theorem 4.1. Specifically, we present the outcomes for $(K_A, K_B) = (5, 5)$ with R = 100 are depicted in Figure 3(b). The results strongly corroborate Theorem 4.1, affirming DEQ exhibits the same NC1 phenomenon as an explicit neural network under these conditions. However, DEQ outperforms the explicit neural network in terms of NC2 and NC3. Additional experimental results with different parameters are detailed in Appendix Section D.\nIn addition to the stability considerations discussed in Section 5.2, we refrain from training for an extensive number of epochs due to the imbalance in the samples of the training set. This is because excessive learning rounds might cause the network parameters to predominantly capture information from the majority class, resulting in overfitting its features. This, in turn, diminishes the generalization of learning features from other classes, leading to marginal improvements in accuracy on the test set. As depicted in Figure 3(b), the model has already converged at this point. Moreover, limiting the number of training epochs helps to avoid the gradual instability in the learning process of DEQ."}, {"title": "6 Conclusion", "content": "In this study, we have systematically analyzed the representation of Deep Equilibrium Models (DEQ) and explicit neural networks under both balanced and imbalanced conditions using the phenomenon of Neural Collapse (NC). Our theoretical analysis demonstrated that NC is present in DEQ under"}, {"title": "Appendix Contents", "content": "A Evaluation metrics of NC\nB Proof under balanced setting\nB.1 Problem definition\nB.2 NC analysis\nB.2.1 NC proof in Explicit neural networks\nB.2.2 NC proof in DEQ\nC Proof under imbalanced learning\nC.1 Lower bound of the loss function\nC.2 NC Analysis\nD More experiments"}, {"title": "A Evaluation metrics of NC", "content": "Followed by the settings of [51] and [10], the measurement of NC are set as follow:\nLet $h_{k} \\triangleq \\frac{1}{n_k} \\sum_{i=1}^{n_k} h_{k,i}$ represent the average of all features within class $k$ and these $K$ classes collectively constitute the average matrix $H = [h_1,\\dots, h_K]$. Besides, The global average is defined as $h_G \\triangleq \\frac{1}{K} \\sum_{k=1}^K h_k$. Subsequently, the within-class and between-class covariances can be calculated as:\n$\\Sigma_W \\triangleq \\frac{1}{N} \\sum_{k=1}^K \\sum_{i=1}^{n_k} (h_{k,i} - h_k) (h_{k,i} - h_k)^T,$\n$\\Sigma_B \\triangleq \\frac{1}{K} \\sum_{k=1}^K (h_k - h_G) (h_k - h_G)^T.$\nNC1 measures the variation of features with-in the same class:\n$NC1 \\triangleq \\frac{1}{K} Tr (\\Sigma_B \\Sigma_W^{\\dagger}),$\nwhere $\\Sigma_W^{\\dagger}$ denotes the pseudo-inverse of $\\Sigma_B$.\nNC2 measures similarity between the mean of learned last-layer features $H$ and the structure of Simplex ETF:\n$NC2 \\triangleq \\left|| \\frac{H^T H}{||H^T H||_F} - \\sqrt{\\frac{K}{K-1}}(I_K - \\frac{1}{K} 1_K 1_K^T)\\right||_F.$\nNC3 measures similarity of the last-layer feature $H$ and weights of classifier $W$:\n$NC3 = \\frac{||W \\frac{H}{||H||_F}||}{||W||_F}.$\nAdditionally, it is worth noting that all above NC criteria are exclusively based on the training set. This is because our focus is solely on analyzing learning performance on imbalanced datasets, and generalization is not a primary concern."}, {"title": "B Proof under balanced setting", "content": "B.1 Problem definition\nAs different layers in the neural network introduce complexity, the optimization problem is non-convex, and KKT conditions do not guarantee global optimality. Therefore, we consider applying inequality relaxation to the joint optimization problem, obtaining a lower bound for the loss function. By determining the conditions under which the equality holds, we can derive the requirements for the NC phenomenon. This analysis assumes a balanced setting, where all $\\#\\tau(k) = n_1 = n_2 = \\dots = n_K = N/K$.\nWe considered the fully connected layers (explicit) and Deep Equilibrium Models (implicit) under the balanced settings respectively, and then derived the detailed proof.\n(Fully Connected Layers)\n(Deep Equilibrium Models)\n$\\min_{W, W_{EX}, H} \\frac{1}{N} \\sum_{k=1}^K \\sum_{i=1}^{n_k} L(W W_{EX} h_{k,i}^0, y_k)$\n$\\min_{W, W_{DEQ}, Z} \\frac{1}{N} \\sum_{k=1}^K \\sum_{i=1}^{n_k} L(W z^*, y_k)$\ns.t. $h_{k,i} = W_{EX} h_{i}^0,$\ns.t. $z^* = f(h_{k,i}^0; W_{DEQ}),$\n$||W_{EX}||_F \\le E_H,$\n$||W_{DEQ}||_F \\le E_H,$\n$\\frac{1}{K} \\sum_{k=1}^K ||w_k||^2 \\le E_W,$\n$\\frac{1}{K} \\sum_{k=1}^K ||w_k||^2 \\le E_W,$\n$\\frac{1}{K} \\sum_{k=1}^K \\frac{1}{n_k} \\sum_{i=1}^{n_k} ||h_{k}||^2 < E_H,$\n$\\frac{1}{K} \\sum_{k=1}^K \\frac{1}{n_k} \\sum_{i=1}^{n_k} ||z_i^*||^2 < E_H,$\nNote that here $n_1 = n_2 = \\dots = n_K = n$, and f represents the form of Linear DEQ, where we will use $f(x; W_{DEQ}) = \\sum_{i=1}^\\infty W_{DEQ}x$ for representation in the following proofs.\nIn a classification task, cross-entropy loss $L(W h_{k,i}, y_k)$ is regarded as the final loss function. Drawing inspiration from [14], our initial efforts revolve around organizing and simplifying the log function to distinguish the logit in class k from other classes.\nFirst consider the following lemma:\nLemma B.1. Let there be $K$ variables $\\delta_1, \\delta_2, \\dots, \\delta_K$, and the logit of each variable $\\delta_i$ satisfies the inequality:\n$\\log \\left( \\frac{\\delta_k}{\\sum_{i=1}^K \\delta_i} \\right) \\le M_1 \\left(\\log \\delta_k - \\frac{1}{K-1} \\sum_{k'\\neq k} \\log \\delta_{k'}\\right) + M_2,$\nwhere $M_1$ and $M_2$ are predefined constants.\nProof. Split the sum in the denominator and sequentially introduce weights for each term. Here, define $K$ coefficients such that their sum is 1. Therefore, we have:\n$\\frac{C_1}{C_1+C_2} + \\frac{C_3}{(K-1)} + \\dots + \\frac{C_3}{(K-1)} = 1,$"}, {"title": "B.2 NC analysis", "content": "We separately discuss the representation of NC in the cases of Explicit NN and DEQ, and compare the lower bounds of the loss function."}, {"title": "B.2.1 NC proof in Explicit neural networks", "content": "For convenience, we assume there is only one layer in the feature extractor, that is, $h = W_{Ex} h^0$, then the first term in C becomes:\n$\\frac{1}{2} \\sum_{k=1}^K \\sum_{i=1}^n ||h_i - h_{k,i}||^2 = \\frac{1}{2} \\sum_{k=1}^K \\sum_{i=1}^n ||W_{EX}(h^0_i - h^0_{k,i})||^2 = \\frac{K}{2} \\sum_{i=1}^n (||W_{EX}|| + ||h^0_i - h^0_{k,i}||^2)$.\nSubstituting them into the loss function (19), we can observe that:\n$\\mathcal{L} \\ge \\frac{NK}{4C_5} ||W_{EX}||_F + \\frac{1}{4C_5} \\frac{K}{K^2} \\frac{n^K}{n} \\sum_{k=1} |h^0_{k,i}|^2 \\ge \\frac{C_5NK}{2} E_W$\n$\\ge \\frac{K_N}{4C_5} E_H - \\frac{C_5K_N}{2} E_W + \\frac{C_5KN}{4C_5} K^2 \\frac{n}{n_k} \\sum_{i=1} |h^0_{k,i}|^2 \\ge \\frac{NK}{4C_5} E_H.$\nTo acquire the lower bound of the loss function, we assign the value $C_5 = \\sqrt{E_H / E_W}$, the lower bound becomes:\n$\\inf L_{EX} = - \\frac{C_1 K}{(C_1 + C_2)(K - 1)} \\sqrt{E_H E_W} + C_4.$\nFurthermore, the condition $|h^0_{i}|^2 = 0$ should also be satisfied, indicating that the average of the features for the i-th sample, $\\frac{1}{K} \\sum_{k=1} h^0_{k_i}$, is equal to zero.\nThe satisfaction conditions for the inequalities include the following:\n\u2022 In Eq. (19): The first inequality becomes equality when\n$\\frac{(C_1 + C_2) h^T_i w_k}{C_1} = \\frac{h^T_i w_{k'}}{C_3},$\nthat is,\n$h_i^T w_k = h_i^T w_{k'} + \\frac{C_1(K-1)}{C_2} \\log \\frac{C_3}{C_1(K-1)}.$\nThe second inequality is reduced to equality when $h_i - h_{k,i} = -C_5 w_k$.\n\u2022 In Eq. (20): $||W_{EX}||^2 = \\sum_{k=1} \\sum_{i=1} ||h^0_i - h^0_{k,i}||^2$."}, {"title": "B.2.2 NC proof in DEQ", "content": "In the blocks for feature extraction, DEQ can be referred as a mapping from the features by backbone to the output $h^0 \\rightarrow h^*$, which can be directly solved using the implicit equation:\n$h^* = f(W_{DEQ}; h^0) = \\sum_{i=1}^\\infty W^i_{DEQ} h^0.$\nSimilar as the explicit case, start with the term:\n$\\frac{K}{2} \\sum_{k=1} \\sum_{i=1} ||h_i - h_{k,i}||^2 = \\frac{K}{2} \\sum_{k=1} \\sum_{i=1} ||\\sum_{j=0}^\\infty W^i_{DEQ} (h^0 - h^0_{k,i})||^2$.\nSince the Neumann series can be regarded as a recursive procedure, denote $G_j = \\sum_{i=0}^{j-1} W_{DEQ}h^0_i,$ $ G^0_{k,i}= (h^0 - h^0_{k,i}) (j = 0,1,\\dots, \\infty)$, therefore $G^0_{k,i} \\propto W_{DEQ} G^1_k + (h^0_i - h^0_{k,i})$.\n$- \\frac{K}{2} \\sum_{k=1} \\sum_{i=1} ||G^0_k||^2 = - \\frac{K}{2} \\sum_{k=1} \\sum_{i=1} ||W_{DEQ} G^1_k + (h^0_i - h^0_{k,i}) ||$\n$\\ge - \\frac{K}{2} \\sum_{k=1} \\sum_{i=1} || W_{DEQ} G^1_k||^2 - \\frac{K}{2} \\sum_{k=1} \\sum_{i=1} || h^0_i - h^0_{k,i} ||^2$.\nContinuing the recursion, we can obtain:\n$\\frac{1}{2} ||G^j_k||^2 \\ge (>-1)$"}, {"title": "C Proof under imbalanced learning", "content": "C.1 Lower bound of the loss function\nConsider the loss function:\n$\\mathcal{L} = - \\frac{K_A n_A}{N} \\sum_{k=1}^{K_A} \\sum_{i=1}^{n_A} L(W h_i, y_k) + \\frac{K_B n_B}{N} \\sum"}]}