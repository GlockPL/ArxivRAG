{"title": "KIMAS: A Configurable Knowledge Integrated Multi-Agent System", "authors": ["Zitao Li", "Fei Wei", "Yuexiang Xie", "Dawei Gao", "Weirui Kuang", "Zhijian Ma", "Bingchen Qian", "Yaliang Li", "Bolin Ding"], "abstract": "Knowledge-intensive conversations supported by large language models (LLMs) have become one of the\nmost popular and helpful applications that can assist people in different aspects. Many current knowledge-\nintensive applications are centered on retrieval-augmented generation (RAG) techniques. While many\nopen-source RAG frameworks facilitate the development of RAG-based applications, they often fall short\nin handling practical scenarios complicated by heterogeneous data in topics and formats, conversational\ncontext management, and the requirement of low-latency response times. This technical report presents a\nconfigurable knowledge integrated multi-agent system, KIMAS, to address these challenges. KIMAs features\na flexible and configurable system for integrating diverse knowledge sources with 1) context management\nand query rewrite mechanisms to improve retrieval accuracy and multi-turn conversational coherency, 2)\nefficient knowledge routing and retrieval, 3) simple but effective filter and reference generation mechanisms,\nand 4) optimized parallelizable multi-agent pipeline execution. Our work provides a scalable framework for\nadvancing the deployment of LLMs in real-world settings. To show how KIMAS can help developers build\nknowledge-intensive applications with different scales and emphases, we demonstrate how we configure\nthe system to three applications already running in practice with reliable performance.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) have had a profound impact on various aspects of people's lives, particularly\nas the foundational technology behind conversational applications such as chatbots. These models have\nbecome indispensable as virtual assistants, offering powerful capabilities for various tasks, including addressing\ncommon-sense queries, generating summaries for academic papers [16], and solving programming challenges and\ntasks [11]. Despite their impressive functionality, LLMs are of some limitations. Issues such as hallucinations\nand the inability to provide the most up-to-date information or private knowledge hinder their reliability in\ndirectly serving for knowledge-intensive applications. These shortcomings can be mitigated by integrating\nLLMs with external information in the input context [20, 28]. One notable approach is retrieval-augmented\ngeneration (RAG) techniques [1, 10], which enhances LLMs by equipping them with retrieval capabilities,\nallows LLMs to address questions that exceed the scope of their pre-trained internal knowledge. RAG has\nproven highly effective in improving performance on question-answering (QA) tasks emphasizing faithfulness\nto truths, showcasing its potential to bridge the gap between static pre-trained knowledge and dynamic,\ncontext-specific information.\nWhile many real-world applications have adopted RAG techniques [13, 22], open-source frameworks have\nalso emerged to facilitate the adaptation of RAG to a wide range of tasks [14, 18] for the public to hold RAG\napplication services themselves with local data. While these open-source RAG frameworks provide convenient\nstarting points for building RAG-based applications, there remain significant opportunities for improvement,\nespecially in more practical and complicated scenarios, e.g., efficient multi-source knowledge retrieval, which\nprovides primary motivations for this paper.\nChallenge 1. While developing a basic chatbot using LLM APIs is relatively straightforward, the complexity\nincreases significantly when the conversation requires intensive external knowledge. A user's question may"}, {"title": "Preliminary", "content": ""}, {"title": "LLM-based Multi-Agent System", "content": "Agent. In this paper, \"agent\", abbreviated from \"LLM-based agent\", is usually characterized as a paradigm\nof LLM-centric applications that employ LLM to mimic human brain [31]. With the reasoning capability\nof LLMs [17, 19, 29, 35], an agent can decompose a complex task into subtasks that can be solved more\nreliably; meanwhile, LLM can make decisions so that an agent can dynamically decide when and how to use\ntools (i.e., APIs of different external functionalities, such as query portal of current weather of a city) given a\ntask [21, 24, 36]; finally, with a memory for maintaining context and related knowledge [8, 26], agents can\ngenerate appropriate answers in long conversations or utilize external knowledge. In this paper, we relax the\ndefinition of an agent so that it may contain only a subset of these three elements.\nMulti-agent and pipeline. At the current stage, an agent's performance is not satisfying given complicated\ntasks. For example, end-to-end code generation [9] or have a large number of tools for selection. Multi-agent\nsystems represent an emerging paradigm, where multiple agents, each specialized in some specific tasks,\ncollaborate to solve complex tasks. Some of the multi-agent frameworks are conversational [15, 33], where\nmultiple agents collaborate and communicate with each other by generating messages in natural language\ngenerated by LLM. Some other multi-agent frameworks [9] emphasize the diverse capabilities of LLMs to\nbreak down tasks into modular components, allowing individual agents to specialize in specific roles such as\ninformation retrieval, reasoning, or decision-making. By communicating and exchanging intermediate outputs,\nagents collectively achieve goals that exceed the capabilities of a single LLM operating in isolation. There\nare also many multi-agent systems that are designed for specific tasks, including medical [27], coding [34],\nand evaluation [2]. Although the above work may have implementation differences, the main idea is still to\ndecompose complex tasks into simple subtasks and let each agent work as an information processing node in\na pipeline."}, {"title": "Knowledge-intensive QA", "content": "RAG is one of the most popular techniques coupled with LLMs, designed to address knowledge-intensive tasks,\nparticularly those requiring high-confidence answers or involving private knowledge unavailable during the\nmodel's training phase. In the early stages, when language models lacked strong in-context learning capabilities,\nRAG techniques primarily relied on training or fine-tuning models to integrate retrieved knowledge [1, 10].\nHowever, with the rapid advancement of language models and their demonstrated ability to perform in-context\nlearning, a more efficient and cost-effective approach has emerged. This strategy involves appending retrieved\ntext chunks as additional prompts to the LLM input, avoiding the need for task-specific fine-tuning [23]. In\nparallel, significant research has been dedicated to improving retrieval mechanisms to ensure higher-quality\ninputs, such as dense retrieval methods that enhance the relevance of retrieved text [12]. These developments\nhighlight the ongoing evolution of RAG techniques and their critical role in extending the capabilities of\nLLMs for real-world applications."}, {"title": "Knowledge-oriented QA System Designs in KIMAS", "content": "KIMAs focuses on the scenario where application users expect to obtain accurate answers based on knowledge\nfrom multiple homogeneous or heterogeneous sources. We use the following hypothetical use case to\ndemonstrate the challenges of building knowledge-oriented QA applications in practice. The real use cases\nare presented in the following Section 4.\nHypothetical use case. If a developer of a GitHub repository wants to build an LLM-based QA plugin based\non his repository, he may need to consider the following potential questions:"}, {"title": "Modules and System Structure Overview", "content": "Agentive Modularization. In our design, three modules serve as cores in KIMAs: conversation context\nmanagement, information retrieval, and final answer generation. As in Figure 1, we design a specialized agent\nclass for each module: context manager, retrieval agent, and summarizer.\n\u2022 Context manager: In everyday communication, conversations often rely heavily on contextual information.\nThe same is true in knowledge-intensive conversational QA tasks. In general, the context manager is\ndesigned to enrich the query by extracting missing information from the conversation context, ensuring\nits completeness. For instance, when KIMAs is used to provide QA services for the AgentScope [5]\nGitHub repository, a user might query whether there is any multi-agent games application in the"}, {"title": "Context-based Query Enhancing", "content": "Knowledge-intensive QA applications usually have two kinds of \"contexts\" as essential factors to answer a\nquestion: the context of conversation and the context of knowledge sources. Correspondingly, two types of\nagents in our design can rewrite the user query to enrich its semantic meaning with different information\nsources: context manager and retrieval agent. In general, the context manager, designed to help the retrieval\nagent and summarize digesting conversation in advance, ensures that the query contains necessary information\nfrom the conversation context; on the other hand, a retrieval agent is supposed to rewrite the query better\nto fit the retrieval mechanism in its knowledge source context. While the rewrite mechanisms of context\nmanager are fixed, the ones for retrieval agents are designed to be more flexible. The following are details\nabout context-based query enhancement.\nContext manager: conversation-contextual query rewrite for retrieval agents. As discussed in\nSection 3.1, understanding the user's real intention behind a query heavily depends on the context of the\nconversation. Without some keywords in the conversation context, knowledge retrieval can be pointless.\nThus, the goal of the query rewrite mechanism at this stage is to enrich the query with precise conversation\ncontext information. 1) The first point to consider is that some key information is lost in the query but can\nbe obtained from the conversation history (e.g., the example mentioned in Section 3.1). The context manager\nchecks whether the query itself is ambiguous, including containing pronouns referring to terms appearing\nin the previous conversation. 2) The second consideration is that some users may rephrase their previous\nquestions when they find that the answer provided by KIMAs is not satisfactory. Therefore, the context\nmanager also needs to revise the query with reflection. 3) Besides, this step is expected not to consume too\nmuch time as one of the intermediate steps in the pipeline. Therefore, these two goals are integrated into\nthe LLM prompt together with the conversation history and the LLM is expected to properly rewrite the\nquestion with necessary details filled and emphasized. Considering the time constraint, we also recommend\nusing a lightweight LLM to handle this task.\nContext manager: conversation-contextual query rewrite for summarizer. The final step in\ngenerating the answer is another step that is heavily based on the context information. In this step, the\nquality of the generated answer depends directly on the degree to which the LLM understands the relevant\ncontext. Our observations indicate that while many LLMs perform well for requests with simple requirements,"}, {"title": "Efficient Multi-source Information Retrieval", "content": ""}, {"title": "Knowledge retrieval mechanisms", "content": "The retrieval agents play key roles in KIMAs because they have access to knowledge sources. Their responsibility\nis to retrieve and provide relative knowledge to user queries. The following built-in support for the different\nknowledge sources can be easily assigned to retrieval agents via configuration."}, {"title": "Embedding clustering routing", "content": "We expect each retrieval agent to take charge of one or a few knowledge sources. The key guidance is that\none can group similar knowledge sources to the same retrieval agent so that the knowledge context query\nrewrite can benefit the retrieval of all similar knowledge sources. On the other hand, knowledge sources with\nvery different topics or contents are supposed to be assigned to different retrieval agents.\nHowever, a challenge is correctly selecting the best-fit retrieval agent(s) with the most relevant knowledge\nsource. Most existing multi-agent routing mechanisms rely on 1) manually created descriptions for the\n(functionalities of) agents and 2) using LLMs as decision-makers to decide which agents should be activated\nto provide knowledge. However, such a combination is not suitable for routing between knowledge retrieval\nagents. As a knowledge source may contain a large volume of knowledge, it can easily exceed the context\nlength of any LLMs; nevertheless, it is challenging for human beings to summarize all the knowledge from a\nsource comprehensively. Meanwhile, using LLMs to select the knowledge source may not be a good idea in\npractice because 1) its output can be non-negligible randomness, which can make it hard to ensure consistency;\n2) LLMs inference can be slow and restricted by the context length (i.e., not suitable for high-efficiency\napplications or context with long conversation)."}, {"title": "Summarization", "content": "Reranking for filtering. A common strategy to avoid the false-negative cases (i.e., high-relevance\nknowledge is not retrieved) is to retrieve slightly overwhelming information that is larger than the context\nlength of the LLMs. In KIMAS, since multiple retrieval agents can provide different knowledge sources,\nthe summarizer can receive overwhelming information that cannot fit into the effective context window of\nLLMs. However, although embedding similarity matching mechanisms can efficiently retrieve a lot of relevant\ninformation, they are not metrics to rank or filter information due to the following important issues. 1) Some"}, {"title": "Optimizing Pipeline for Efficient Execution", "content": "Many LLM-based applications seek low response latency, which poses a challenge for multi-stage applications\nlike KIMAs. More specifically, since multiple sub-tasks exist, LLM will inevitably be used to generate multiple\ntimes and introduce some intermediate result waiting time. Therefore, efficiently arranging the execution\nrequires parallelizing as much LLM usage as possible. In order to optimize efficiency, the execution of KIMAS\ncan be executed in parallel with requests for asynchronous model APIs as follows.\n\u2022 Parallelization 1: Query ingest. At this stage, two functions are executed in parallel. One is\nthe context manager conversation context query rewrite for retrieval agents, which fills in missing key\ninformation for the current query question based on the whole conversation history so that the following\nretrieval stage can use more accurate information. The other is query routing, deciding which retrieval\nagents are the correct ones to execute.\n\u2022 Parallelization 2: Knowledge retrieval and context analysis. At this stage, each retrieval\nagent obtains the query enriched with the necessary context information of the conversation. At\nthis stage, each agent can rewrite a knowledge-context query to match the knowledge context and\nretrieve knowledge. Meanwhile, the context manager analyzes the context of the previous conversation"}, {"title": "Use Cases", "content": "Sytem implementation. KIMAs is implemented based on a multi-agent framework, AgentScope [5]. At the\nagent level, the agents inherits from the built-in agents in AgentScope but with extra features specialized\nfor KIMAs, such as the methods supporting the efficient routing mechanism and some external knowledge\nmanagement and retrieval functions. At the pipeline level, the agents receive input and pass process\ninformation via AgentScope's message objects; some of the parallel execution stages in the pipeline are\ntailored specifically in KIMAs for easy management and code management.\nIn the following, we demonstrate how we configure KIMAs to build different QA applications for three\ndifferent tasks."}, {"title": "Small scale application: AgentScope QA", "content": "As a proof-of-concept example, we first present an example with offline knowledge sources with different data\nformats and topics as a starting point to demonstrate how KIMAs works.\nGoals. In this use case, we adapt KIMAs to help answer questions about\nAgentScope's GitHub repository. The expectation for this application is to\nserve as a chatbot in a Q&A group for developers building their multi-agent\napplication with AgentScope framework, providing an accurate response in time\nto the raised questions. It is observed that the most common questions fall in\nthe following categories:\n\u2022 Preliminary project questions. As the Q&A group is open for everyone,\nsome potential users of AgentScope are also presented in the group. Their\nquestions are usually about the feature of AgentScope, its advantages\ncompared with other similar open-source frameworks, and the feasibility\nof using AgentScope for their task. Once some of them decide to initiate\ntheir project with AgentScope, they may be looking for whether there are\nalready demonstration examples for tasks similar to theirs provided in the\nGitHub repository for reference.\n\u2022 Coding or debugging related questions. Another majority of questions in the\nQ&A group appear to be developers using AgentScope but encounter issues\nin their development process. For example, they may seek clarifications\nof two different agent classes or help to solve the execution errors in their\nAgentScope-based applications."}, {"title": "Larger scale: ModelScope QA", "content": "Goal. While serving as a Q&A chatbot similar to the AgentScope use case, the spectrum of questions to\nbe handled is significantly larger than the ones in Agentscope. Modelscope community is a platform of\nopen-sourced machine learning models, datasets, training/finetuning libraries and applications built with\nLLMs and other models. It is expected a chatbot to provide an accurate initial answers potentially based on\nall these kinds of the knowledges from different sources.\nApplication Configuration. To helpfully serve the users in the community, we highlight the specialties in\nconfigure as the following.\n\u2022 Knowledge sources configuration. The knowledge sources used in these applications can be categorized\ninto two types, online and offline.\nThe online knowledge sources include model and dataset information, official articles about the latest\nopen-source community technical progress. These knowledge sources are achieved by Bing search API\nwith different constraints, i.e., restricted to domain of available models/datasets/articles on the website.\nIt is configured to use a commercial search engine instead of in-site search because the results of Bing\ncan also be used to provide related knowledge that ranked by more sophisticated mechanisms beyond\ntext similarity, such as recommending the most popular Text-to-Image models.\nThe offline knowledge sources include the tutorial documents and eight different GitHub repositories\naffiliated with ModelScope. The tutorial covers knowledge about how to use models, datasets and\ncomputation resources available on modelscope.cn, and the repositories contain code files and repository-\nlevel instructions. Compared with the online knowledge sources, these knowledge sources can be hosted\nand retrieved locally because the retrieval standard is more\n\u2022 Pipeline configuration. Generally speaking, the pipeline configuration is similar to the AgentScope use\ncase, involving all three kinds of agents. But key difference is in the routine mechanism configuration.\nWe provide some manual description mix-in for the routing mechanism to bias some selections manually.\nFor example, both the tutorial and model knowledge source contains the information of some models\n(or models with similar names); however, the information from model knowledge source is more up-\nto-date than the one from the tutorial. Therefore, we add some manual mix-in to bias the \"model"}, {"title": "Turbo Scale: Olympic Bot on Weibo", "content": "Goal. During the Paris 2024 Olympic Games, KIMAs serves as the core of the back-end algorithm to generate\nauto-comments for the posts related to the Olympics\u00b2. The posts and comments of this Weibo bot are\nsupposed to focus only on Olympic Games, including the news and historic Olympic events. However, there\nare many other bots performing on Weibo, but there is a restriction that no more than two bots can reply to\nthe same Weibo post. Therefore, there is a race condition and the response generation efficiency becomes\na key point in this scenario. To encounter such challenge, our configuration of KIMAS needs to make the\nfollowing changes.\nApplication Configuration. In order to fulfill the requirements, KIMAs is configured as the following.\n\u2022 Knowledge sources configuration. We configure a retrieval agent to use the specialized search API for\nOlympic related events. The APIs perform searches similar to public search engines using keywords to\nmatch related information.\n\u2022 Pipeline configuration. To extremely reduce response latency, the pipeline configuration becomes simple.\nThe context manager is deactivated, and the only retrieval agent will perform a keyword query rewrite\nonly. The full conversation history and the retrieved knowledge will be directly fed to the summarizer\nto generate the final answer.\nWith such knowledge and pipeline configuration, the end-to-end latency is reduced to less than 10 seconds\nper post or comment, while the responses are still very informative and popular."}, {"title": "Conclusion", "content": "In this technical report, we introduce KIMAs, our configurable knowledge-integrated multi-agent system for\ndevelopers to build their knowledge-intensive QA system. We present three use cases built on our system with\ndifferent emphases to demonstrate that KIMAs can be configured to various applications. While the current\nversion of KIMAs focuses on knowledge-intensive QA tasks, future development can expand its capabilities to\naddress a broader range of challenges, including code generation based on some specific local code base and\ninteractive recommendation system for e-business."}]}