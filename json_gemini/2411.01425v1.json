{"title": "Learning Hidden Subgoals under Temporal Ordering Constraints in Reinforcement Learning", "authors": ["Duo Xu", "Faramarz Fekri"], "abstract": "In real-world applications, the success of completing a task\nis often determined by multiple key steps which are distant\nin time steps and have to be achieved in a fixed time order.\nFor example, the key steps listed on the cooking recipe should\nbe achieved one-by-one in the right time order. These key\nsteps can be regarded as subgoals of the task and their time\norderings are described as temporal ordering constraints. How-\never, in many real-world problems, subgoals or key states are\noften hidden in the state space and their temporal ordering\nconstraints are also unknown, which make it challenging for\nprevious RL algorithms to solve this kind of tasks. In order to\naddress this issue, in this work we propose a novel RL algo-\nrithm for learning hidden subgoals under temporal ordering\nconstraints (LSTOC). We propose a new contrastive learning\nobjective which can effectively learn hidden subgoals (key\nstates) and their temporal orderings at the same time, based on\nfirst-occupancy representation and temporal geometric sam-\npling. In addition, we propose a sample-efficient learning strat-\negy to discover subgoals one-by-one following their temporal\norder constraints by building a subgoal tree to represent dis-\ncovered subgoals and their temporal ordering relationships.\nSpecifically, this tree can be used to improve the sample ef-\nficiency of trajectory collection, fasten the task solving and\ngeneralize to unseen tasks. The LSTOC framework is evalu-\nated on several environments with image-based observations,\nshowing its significant improvement over baseline methods.", "sections": [{"title": "1 Introduction", "content": "In real life, successfully completing a task often involves mul-\ntiple temporally extended key steps, where these key steps\nhave to be achieved in specified time orders. For instance, in\nthe process of making chemicals, different operations have\nto be strictly performed in the right time order, e.g., sulfuric\nacid must be added after water. Otherwise, the right chemical\nreaction can never occur or even the safety will be threatened.\nThese key steps are necessary for the success of completing\nthe given task and skipping any of them or doing them in the\nwrong time order will lead to failure of the task. In this work,\nthese key steps are regarded as subgoals. Tasks consisting\nof multiple subgoals with temporal ordering constraints are\ncommon in many real-world applications, such as the tem-\nporal logic tasks in control systems and robotics (Baier and\nKatoen 2008). Since these tasks may have long-time hori-\nzon and sparse reward, the knowledge of subgoals and their\ntemporal orderings are necessary for modern RL algorithms\nto solve these tasks efficiently. However, these subgoals can\nbe hidden and unknown in many real-world scenarios. For\ninstance, due to the user's lack of knowledge, these subgoals\nmay be missing when specifying the task. Alternatively, due\nto the partial observability of environment, the agent does\nnot know subgoals and their temporal orderings in advance.\nMotivating example. For example, consider a service robot\ntasked to collect the diamond in limited time steps, as shown\nin Figure 1(a). Due to the limited power and the blockage of\nriver, the agent has to first go to the charger to get charged,\nthen pick up wheel or board to go across the river, and finally\nget the diamond. If the agent first picks up the wheel or board\nand then goes to the charger, the task cannot be finished in\nthe required time steps. The temporal dependencies of these\nsubgoals can be described by the finite state machine (FSM)\nin Figure 1(b). The temporal logic language for describing\nthese dependencies is c;(b/w);d. However, since the agent\ncan only observe things around him, it is not aware of the\nriver and does not know that charger, wheel and board are\nsubgoals, i.e., subgoals are hidden to the agent.\nWhen solving tasks with hidden subgoals, the binary label\nat the end of the episode, indicating the task is accomplished\nsuccessfully or not, is the only reward information for the\nagent to leverage to solve the task. This existing situation can\nbe challenging for modern RL algorithms which use Bellman\nequation to propagate value estimates back to the earlier key\nsteps (Sutton and Barto 2018). These algorithms suffer from\nslow convergence and expensive learning complexity, which\nare going to be verified by empirical experiments. Therefore,"}, {"title": "2 Related Works", "content": "Recently linear temporal logic (LTL) formulas have been\nwidely used in Reinforcement Learning (RL) to specify tem-\nporal logic tasks (Littman et al. 2017). Some papers develop\nRL algorithms to solve tasks in the LTL specification (Ca-\nmacho et al. 2019; De Giacomo et al. 2019; Bozkurt et al.\n2020). In some other papers, authors focus on learning the\ntask machine from traces of symbolic observations based on\nbinary labels received from the environment (Gaon and Braf-\nman 2020; Xu et al. 2021; Ronca et al. 2022). However, all\nthese papers assume the access to a labeling function which\nmaps raw states into propositional symbols, working in the\nlabeled MDP (Hasanbeig et al. 2019).\nThere are some papers assuming to have an imperfect\nlabeling function, where the predicted symbols can be erro-\nneous or uncertain (Li et al. 2022; Hatanaka, Yamashina, and\nMatsubara 2023). But these papers do not address the prob-\nlem of learning subgoals. A recent paper studies the problem\nof grounding LTL in finite traces (LTLf) formulas in im-\nage sequences (Umili, Capobianco, and De Giacomo 2023).\nHowever, their method is only applicable to offline cases with\nstatic dataset and does not consider the online exploration\nin the environment. In addition, authors in (Luo et al. 2023)\npropose an algorithm for learning rational subgoals based on\ndynamic programming. However, Their approach requires\nthe availability of the state transition model, which is not\nfeasible in general real-world applications.\nContrastive learning was used to detect subgoals (key\nstates) in previous RL papers (Zhang and Kashima 2023;\nSun et al. 2023; Casper et al. 2023; Park et al. 2022; Liang\net al. 2022). However, these methods sample clips of pos-\nitive and negative trajectories to formulate the contrastive\nobjective. It can make the temporal distances of states not dis-\ntinguishable and cannot learn temporal distances of detected\nsubgoals. Therefore, these methods cannot be used to learn\nsubgoals under temporal ordering constraints."}, {"title": "3 Preliminaries", "content": "3.1 Reinforcement Learning\nReinforcement learning (RL) is a framework for learning the\nstrategy of selecting actions in an environment in order to\nmaximize the collected rewards over time (Sutton and Barto\n2018). The problems addressed by RL can be formalized as\nMarkov decision processes (MDP), defined as a tuple M =\n(S, A, T, R, \u03b3, So), where S is a finite set of environment\nstates, A is a finite set of agent actions, $T : S \\times A \\times S \\rightarrow$\n[0, 1] is a probabilistic transition function, R : S \u00d7 A \u2192\n[Rmin, Rmax] is a reward function with Rmin, Rmax \u2208 R and\n\u03b3\u2208 [0,1) is a discount factor. Note that So is the set of\ninitial states where the agent starts in every episode, and\nSo: so~So is a distribution of initial states.\nIn addition, corresponding to the MDP, we assume that\nthere is a set of semantic symbols G representing subgoals.\nWe also define a labeling function L : S \u2192 GU {0} that\nmaps an environmental state to a subgoal which is a key step\nto accomplish the task. Furthermore, the key state is defined\nas the environmental state whose output of function L is a\nsubgoal in G. Most states of the environment are not key\nstates where the outputs of L are empty (\u00d8). In this work, the\nstates corresponding to subgoals and the labeling function are\nall unknown to the agent initially. The agent can only leverage\ncollected trajectories and their labels of task accomplishing\nresults to solve the task.\n3.2 Temporal Logic Language\nWe assume that the temporal dependencies (orderings) of\nsubgoals considered in this work can be described by a formal\nlanguage TL composed by three operators. Syntactically, all\nsubgoals in G are in TL, and \u220041,42 \u20ac TL, the expressions\n(91;42), (41 \u222842) and (41 ^62) are all in TL, representing\n\"41 then 42\", \"41 or 42\" and \"41 and 42\", respectively.\nFormally, a trajectory of states T = ($1,..., Sn) satisfies a\ntask description 4, written as \u03c4 |= 4, whenever one of the\nfollowing holds:\n\u2022 If is a single subgoal g\u2208 G, then the first state of T\nmust not satisfy g, and instead the last state must satisfy\ng, which implies that has at least 2 states\n\u2022 If\n = (41;42), then \u22030 < j < n such that\n(81,...,8j) = 41 and (sj,..., Sn) = 42, i.e., task 41\nshould be finished before 42\n\u2022 If y = (41 V 42), then T = 41 or t = 42, i.e., the agent\nshould either finish 41 or 42\n\u2022 If y = (41 \u039b 42), then T = (41;42) or t = (42;41),\ni.e., the agent should finish both 41 and 42 in any order\nNote that the language TL for specifying temporal depen-\ndencies is expressive enough and covers LTLf (De Giacomo\nand Vardi 2013) which is a finite fragment of LTL without\nusing \"always\" operator.\"\nEvery task specification \u03c6 \u2208 TL can be represented\nby a non-deterministic finite-state machine (FSM) (Luo\net al. 2023), representing the temporal orderings and\nbranching structures. Each FSM M of task is a tuple\n(V\u03c6, \u0395\u03c6, \u0399\u03c6, F) which denote subgoal nodes, edges, the set\nof initial nodes and the set of accepting (terminal) nodes,\nrespectively.\n3.3 First-occupancy Representation\nWe use first-occupancy representation (FR) for learning sub-\ngoals. FR measures the duration in which a policy is expected\nto reach a state for the first time, which emphasizes the first\noccupancy.\nDefinition 1.(Moskovitz, Wilson, and Sahani 2021) For an\nMDP with finite S, the first-occupancy representation (FR)\nfor a policy \u03c0F\u3160 \u2208 [0,1]|S|\u00d7|S| is given by\n$F^{\\pi}(s, s') := E_{\\pi} [\\sum_{k=0}^{\\infty} \\gamma^k 1(S_{t+k} = s', s' \\notin \\{S_{t:t+k}\\} | S_t = s]$ (1)\nwhere {St:t+k} = {St, St+1,...,St+k-1} and {St:t+0} =\n\u00d8. The above indicator function 1 equals 1 only when s'\nfirst occurs at time t + k since time t. So $F^{\\pi}(s, s')$ gives\nthe expected discount at the time the policy first reaches s'\nstarting from s.\n3.4 Contrastive Learning in RL\nContrastive learning was used in previous RL algorithms to\ndetect important states (Sun et al. 2023; Zhang and Kashima"}, {"title": "4 Methodology", "content": "In this work, we propose the LSTOC framework for learning\nhidden subgoals and their temporal ordering constraints by\nleveraging trajectories and results of task accomplishment\n(positive or negative labels) collected from the environment.\nBased on constructed subgoal tree, the agent can solve the\ntask faster and generalize to other unseen tasks involving\nsame subgoals.\n4.1 General Context\nThe diagram of LSTOC is shown in Figure 4. In learning\nsubgoal, the agent iteratively learns hidden subgoals one-by-\none in a depth first manner and builds a subgoal tree To to\nguide the trajectory collection and represent learned subgoals\nand temporal orderings. For example, as the problem shown\nin Figure 1, the agent will first learn subgoal \"c\", then \"w\",\nthen \"d\", and then \"b\", finally \"d\", building the subgoal tree\nin Figure 3.\nIn every iteration, the agent focuses on learning subgoals\nnext to the current working node by using a contrastive learn-\ning method, and expands To by adding a new leaf node\nlabeled with the newly learned subgoal, which is used as\nthe working node in the next iteration. This iteration will be\nrepeated until the success of every trajectory on task com-\npletion can be explained by the learned subgoals and their\ntemporal relationships on Tp. Then, the agent will proceed to\nthe labeling component. When collecting a trajectory Tk, by\nusing an exploration policy exp trained with reward shaping\nmethod, the agent is guided by To to reach the working node\n(an unexplored node of T). A binary label lk indicating the\nresult of task completion is received at the end of Tk. Positive\n(lk = 1) and negative (lk = 0) trajectories are stored into\npositive (BP) and negative buffers (BN), respectively.\nOnce the FSM M which describes subgoal temporal\ndependencies by semantic symbols in G becomes available,\nthe labeling component of LSTOC can determine the map-\nping from discovered key states to subgoal symbols in G by\nsolving an integer linear programming (ILP) problem, lever-\naging To to impose the semantic meaning onto every learned\nsubgoal.\nSubgoal Notation. We define the set \u015cK as an ordered set of\ndiscovered key states which form a subset of the state space\nof the environment, i.e., SK C S. Every node of the subgoal\ntree is labeled by a state in \u015cK. For the k-th state in \u015ck, i.e.,\n\u015dk, k is the index for indicating detected subgoal and \u015dk is\nthe key state corresponding to the detected subgoal k. Only\nnewly discovered key state not included in SK will be added\nto SK, creating an index indicating a newly detected subgoal.\nRemark. Although subgoals are hidden, the result of the\ntask completion can still be returned by the environment to\nthe agent for every trajectory. This is common in robotic\napplications. For example, in service robot, when the task\nspecification misses key steps, the robot can learn to discover\nthese missing steps based on its trajectory data and feedbacks\nabout task completion from users or other external sources\n(Gonzalez-Aguirre et al. 2021).\n4.2 Learning Subgoals\nSince the task is assumed to have multiple temporally ex-\ntended hidden subgoals, it is impractical to collect sufficient\ninformative trajectories for an offline supervised learning\napproach to detect all the subgoals at once. Therefore, we\npropose a new learning method to discover subgoals one-by-one iteratively. In every iteration, in addition to trajectory\ncollection, the agent conduct operations including discover-\ning next subgoal, expanding the subgoal tree and training the\nexploration policy, which will be introduced as following.\nExpand Subgoal Tree Tree Definition. As discussed in\nSection 3.2, the temporal ordering constraints of discovered\nsubgoals can be expressed by the subgoal tree T\u03c6. In T\u1ef5,\nexcept the root, each node un is labeled by a key state \u015dkum\nof the kvm -th learned subgoal in \u015cK. Specifically, in Tp, the\nchildren of a node labeled with subgoal p contain key states\nof next subgoals to achieve after p is visited in the FSM\nof subgoal temporal dependencies. Since children of every\nnode are only labeled by the temporally nearest subgoals\nfor task completion, the temporal orderings of subgoals can\nbe well represented in Tp. Whenever To is well established,\nevery path from the root to a leaf node in To corresponds\nto a satisfying sequence of the task (concept introduced in\nSection 3.2).\nTree Expansion. Based on discovered key states, To is ex-\npanded iteratively in a depth-first manner. Initially, To only\nhas the root node vo. For a node vi, we define the path of vi\n(denoted as \u03be\u03b9) as the sequence of key states along the path\nfrom vo to vr in To, as an example shown in Figure 6. We\nfirst define nodes whose paths has not lead to task completion\nyet as unexplored nodes. In each iteration of tree expansion,\nthe agent first selects an unexplored node vw from To as the\nworking node, and then focuses on discovering next subgoals\nto achieve after visiting vw in T. Examples of building the\nsubgoal tree are shown in Figure 5, where the dashed nodes\nare unexplored nodes.\nExpansion at a Working Node. For expansion at vw, the"}, {"title": "5 Experiments", "content": "The experiments aim at answering the following questions:\n1. How well the proposed contrastive learning method would\ndetect key states for subgoals and learn their temporal\ndistances?\n2. Can we solve the task and learn hidden subgoals more\nefficiently with the help of building the subgoal tree?"}, {"title": "6 Correctness and Limitation", "content": "Empirically, as long as Nr is large enough and randomness\nof exploration policy Texp is sufficient, the contrastive learn-\ning in (3) can always discover the correct key state of next\nsubgoal at every working node. Then, the correct subgoal tree\ncan be built and labeling function can be correctly obtained\nby solving the ILP problem. Specifically, the randomness of\nTexp can be guaranteed by using e-greedy into action selec-\ntion, where \u20ac = 0.5 is enough for all the environments.\nHowever, LSTOC framework still has limitations. In some\ncases, the labeling component can not distinguish environ-\nmental bottleneck states from hidden subgoals. In some\nother cases, the labeling component cannot tell the differ-\nences of symmetric branches in the given FSM. Furthermore,\nthe trajectory collection can be problematic in some hard-\nexploration environments. The details of correctness and\nlimitation are presented in Appendix G."}, {"title": "7 Conclusion", "content": "In this work, we propose a framework for learning hidden\nsubgoals under temporal ordering constraints, including a\nnew contrastive learning method and a sample-efficient learn-\ning strategy for temporally extended hidden subgoals. In\nthe future, we will resolve the issues of extending this work\ninto hard-exploration environments, improving its sample\nefficiency in environments with large state space."}, {"title": "A Labeling Component of LSTOC", "content": "The FSM M is the FSM specification of subgoal temporal dependencies given by the user, where each node is described by a\nsubgoal semantic symbol in G. Denote the state space of My as U which has U states. The transition function of M is defined\nas d\u2084 : U \u00d7 G \u00d7 U \u2192 {0,1}, and the transition from state u to u' conditioned on symbol g is expressed as d\u2084(u, g, u') = 1,\nwhere g is the index of symbol in G.\nAssume that the set of discovered satisfying sequences Ps has P sequences, and the m-th sequence has lm elements. Note that\nPs consists of all the paths in the subgoal tree To which is built by the subgoal learning component of LSTOC framework. The\nset SK contains the discovered key states of subgoals. The target of labeling component of LSTOC is to determine the mapping\nfrom SK to G, making every sequence in Ps lead to an accepting state in M, and hence yielding the labeling function Lv.\nNow we start formulating the integer linear programming (ILP) problem for learning the labeling function. The binary variables\nof this ILP problem are composed by state transition variables Um,n,i,j and mapping variables \u03c5\u03ba,\u03b9, where Um,n,i,j = 1 denotes\nthat the n-th element of m-th sequence of Ps makes the agent transit from state i to j over FSM M\u03c6, and vk,\u03b9 = 1 denotes\nthat k-th state in \u015cK is mapped to l-th symbol in G. Based on their definitions, we can first have 5 constraints on these binary\nvariables:\n$\\sum_{i,j=1}^{U} U_{m,n,i,j} = 1, \\forall m = 1,...,|P_s|, n = 1,..., l_n$ (4)\n$\\sum_{j=1}^{U} U_{m,1,1,j} = 1, \\forall m = 1,...,|P_s|$ (5)\n$\\sum_{i=1}^{U} U_{m,n,i,j} = \\sum_{i=1}^{U} U_{m,n+1,j,i}, \\forall m = 1,..., |P_s|, n = 1,..., l_n - 1, j = 1, ..., U$ (6)\n$\\sum_{l=1}^{|\\mathcal{G}|} \\upsilon_{k,l} \\le 1, \\forall k = 1,...,|\\hat{S}_K|$ (7)\n$\\sum_{k=1}^{|\\hat{S}_K|} \\upsilon_{k,l} = 1, \\forall l = 1,..., |\\mathcal{G}|$ (8)\nwhere these constraints mean: 1) every element of every sequence in Ps makes a transition, including staying at the same state\nof FSM M; 2) the first element of every sequence is in the first state of M4; 3) for any pair of consecutive elements of every\nsequence, the out-going state of the previous element is the same as the in-coming state of the other one; 4) every state in \u0160K\nis mapped to at most one semantic symbol in G, since some discovered key states may be redundant; 5) every symbol in G is\nassociated with one discovered key state in \u0160K.\nSince the transition variables Um,n,i,j and mapping variables \u03c5\u03ba,\u03b9 must be consistent with the state transitions of M\u03c6 (\u03b4\u03c6), we\nhave another set of constraints:\n$U_{m,n,i,j} \\le \\delta_{\\phi}(i, l, j) \\cdot \\upsilon_{k,l}$ (9)\nwhere m, n, i, j and k, I have the same range of values as above constraints.\nFinally, we have another set of constraints which make sure that the last element of every sequence in Ps makes the agent stay\nin any accepting state of FSM M4. Then, we have\n$\\sum_{j \\in U_F} U_{m,l_m,i,j} = 1$ (10)\nwhere UF denotes the set of accepting states of M4. In order to ignore the states in SK not associated with any subgoals during\nmapping, such as bottleneck state in the environmental layout, we use the sum of mapping variables as the objective:\n$\\sum_{k=1}^{|\\hat{S}_K|} \\sum_{l=1}^{|\\mathcal{G}|} \\upsilon_{k,l}$ (11)\nThe formulated ILP problem has the objective (11) and constraints (4)-(10). We solve it by Gurobi solver (GurobiOptimization\n2023)."}, {"title": "B Algorithms", "content": "We present the algorithm tables of the proposed framework in this section. Since the agent is not familiar with the environment in\nadvance and the optimal selection of NT is not clear, we design a main loop to try to call LSTOC (described in Algorithm 3)\nwith different values of NT increasing incrementally. In implementation, we set NT = 80 and HT = 20 for every domain.\nThe subgoal discovery process at a working node is described in Algorithm 2. It is same as the process described in Section\n4.2. The inputs Dp and DN only contain trajectories conditioned on the current working node. In line 3, the discriminative\nstate representation is first pre-trained based on data in BP U BN. For line 5 and 6, In the process of FR, trajectories in D are\nprocessed to only keep the part after achieving the working node (the red part in Figure 6 as an example), then the FR of every\nprocessed trajectory is first computed by removing repetitive states. Then, from line 7 to 13, the importance function fo is\niteratively trained by the objective formulated in (3). In implementation, the number of iterations (L) is set to be 700 for every\ndomain. The batch size of negative samples (B) is chosen to be 64. Due to the simple architecture of fw, the learning rate of each\niteration is set to be 0.01. In line 14, the state with highest value of fw (highest temporal ordering) is selected as the discovered\nkey state of next subgoal and returned to LSTOC in line 15.\nThe LSTOC is described in Algorithm 3. In line 5, the agent first collects sufficient number of trajectories from the environment.\nThe loop starting at line 6 is the iterating process of building subgoal tree To which will terminate when 1) every positive\ntrajectory is explained by Tp (line 8) and ILP problem for symbol grounding is successfully solved (line 9); or 2) To is wrongly\nbuilt (line 37) with condition (***) introduced in the following paragraph. The loop starting at line 7 is the loop of collecting\nsufficient number of positive trajectory for subgoal discovery at the current working node vw. From line 14 to 16, trajectories\nconditioned on vw are selected from BP and BN with explained trajectories discarded, and stored as Dp and DN. In line 17, if\nevery trajectory in Dp which follows the path of vw can lead to the accomplishment of the task (getting a positive label), the path\npw will be a newly discovered satisfying path and tree expansion will be moved back to the parent node of vw, initiating another\niteration. In line 21, if the condition (**) of being fully explored is met, the tree expansion will be moved to the parent node of vw\nand starts another iteration of expansion. In line 24, if the condition of subgoal discovery at vw is not met, more trajectories will\nbe collected by Explore process and exploration policy will be further trained to make more trajectories conditioned on vw to be\ncollected. Otherwise, the subgoal discovery process will be called in line 32. After new key state of subgoal is discovered, from\nline 33 to 39, Tp and set of key states SK will be expanded by adding a new node Vnew. Then, in line 40 and 41, the working\nnode is moved to new.\nThe condition (*) in Algorithm 3 states that pw is a newly discovered satisfying sequence (concept defined in Section 3.2)\nand no further expansion at the current working node is needed. The condition (**) in Algorithm 3 states that every positive\ntrajectory conditioned on vw can be explained and no further expansion at the current working node is needed. The condition\n(***) in Algorithm 3 refers to the situations where To is wrong built. The condition (***) becomes true when the longest path in\nTo is longer than that in M or the largest degree of nodes in To is larger than that in M.\nC Environments\nEnvironment domains adopted in the experiments include both grid-based and pixel-based observations. Note that in all the\nenvironment domains, there is no labelling function which maps from agent's observation into any letter or items, so letters or\nitems are all hidden to the agent.\nLetter. The first environment domain is the Letter. As shown in Figure 10(a), there are multiple letters allocated in the map. In\nthis domain, the observation of the agent can be the full or partial map, which is an image-based tensor without any specific\ninformation on locations of letters. If the map has the size of m \u00d7 n with k letters, the observation is a m \u00d7 n \u00d7 (k + 1) binary\ntensor. The agent's actions include movements in four cardinal directions. Only a subset of letters in the map is used as task\nsubgoals, making the problem more challenging.\nOffice. The second environment domain is the Office. It is a variant of office game widely used in previous papers (Icarte et al.\n2018). As shown in Figure 10(b), in this domain, the agent only as partial observation of the environment, which is a 5 \u00d7 5 grid"}, {"title": "F Neural Architecture", "content": "We build neural network architectures for state representation function of and importance function fw and the exploration policy\nTexp. The function of is used to extract a d-dimensional vector as state representation for an input raw state or observation. In\nLetter and Office domain, d is 128 and of is realized by a two-layer MLP with 128 neurons in each layer. In Crafter domain, \u043e\u04e9\nis realized by a convolutional neural network (CNN) module. This CNN is the same as the classical CNN for deep RL proposed\nin (Mnih et al. 2015), where the first convolutional layer has 32 channels with kernel size of 8 and stride of 4, the second layer\nhas 64 channels with the kernel size of 4 and stride of 2 and the third layer has 64 channels with the kernel size of 3 and stride of\n1. The CNN module produces an embedding vector with the size of d = 512.\nThe importance function fw is realized by MLP in all three domains, which is a two-layer MLP with 128 neurons in Letter\nand Office domains and 256 neurons in Crafter domain.\nThe exploration policy Texp is a GRU-based policy. In Texp, the hidden dimension of gated recurrent unit (GRU) module is\n128 for Letter/Office domains and 256 for Crafter domain. The outputs of \u03c0exp consist of action and predicted value, which are\nconditioned on both the hidden state and the embedding vector of input observation."}, {"title": "G Correctness and Limitation", "content": "G.1 Correctness\nEmpirically, as long as N\u012b is large enough and randomness of exploration policy #exp is sufficient, the contrastive learning in (3)\ncan always discover the correct key state of next subgoal at every working node. This is because sufficient trajectory data and\nrandomness in data collection can make most parts of state space covered by both positive and negative trajectories. Then, the\ncorrect subgoal tree can be built and labeling function can be correctly obtained by solving the ILP problem. Specifically, the\nrandomness of exp can be guaranteed by using e-greedy into action selection, where e = 0.5 is enough for all the environments.\nIn other words, whenever the state space is covered sufficiently enough by collected trajectory data, no hidden subgoal in the\nenvironment can be missed. Then, whenever every collected positive trajectory can be explained by some path in To, all the\nhidden subgoals and their temporal orderings are learned, showing the correctness of the termination condition of LSTOC.\nG.2 Limitation\nHowever, LSTOC framework still has limitations. First, its labeling component cannot tell the difference between the bottleneck\nstate in the environment and the real key states of subgoals, even though these are all key states. For example, as shown in Figure\n15, a navigation environment has two rooms.\nRoom 1 has a red ball and room 2 has a blue ball and a green ball. However, there is an open corridor connecting room 1\nand 2. This corridor becomes a bottleneck state of connecting room 1 and 2. Assume that the given task has hidden subgoals of\nfirst picking up red ball, then blue ball, and finally green ball, i.e., r; b; g in temporal logic language. In this case, four subgoals\nwill be discovered by LSTOC, which are corridor and states of red, blue and green balls. Then, since the FSM does not have\ncorridor and the agent does not know the location of blue ball, the the corridor cannot be distinguished from the state of blue\nball. Therefore, the ILP problem in the labeling component does not have a unique solution and the labeling function cannot be\nobtained. However, the learning subgoal part of LSTOC can still detect every meaningful key state.\nSecond, the labeling component cannot tell the differences of symmetric branches of FSM. For example, the given FSM is\n(a; b; c) V (d; e; f) and two branches are symmetric. The learning subgoal component of LSTOC will discover 6 key states of\nsubgoals, but the mapping from discovered key states to semantic symbols (a,b,c,d,e, and f) cannot be determined.\nIt is important to note that the limitations outlined above cannot be resolved. In the problem formulation, the only\nfeedback available to the agent for anchoring subgoal symbols is a binary label indicating task completion at the end of\neach trajectory. The agent learns the labeling function (i.e., the mapping from learned subgoals to subgoal symbols in the\ngiven FSM), by utilizing these binary labels and structural information of subgoal symbols in the given FSM. As a result,\nwhen there are hidden bottlenecks in the environment or the task involves symmetric branches of subgoals, the given\nFSM has ambiguity and hence the agent lacks sufficient information to accurately identify the mapping from learned\nsubgoals to subgoal symbols in the given FSM. Consequently, in these situations, it is impossible for the agent to correctly\nlearn the labeling function, but the agent can still accurately estimate the hidden subgoals in the environment.\nThird, the trajectory collection could not cover some important states in hard-exploration environments. In environments like\nMontezuma-revenge, some key states need thousands of actions to reach and the exploration policy trained by task completion\nsignal only cannot collect trajectories covering these key states. Then, some hidden subgoals cannot be discovered and the\nlabeling component cannot produce correct result."}]}