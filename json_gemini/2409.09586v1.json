{"title": "ValueCompass: A Framework of Fundamental Values for Human-Al Alignment", "authors": ["HUA SHEN", "TIFFANY KNEAREM", "RESHMI GHOSH", "YU-JU YANG", "TANUSHREE MITRA", "YUN HUANG"], "abstract": "As Al systems become more advanced, ensuring their alignment with a diverse range of individuals and societal values becomes increasingly critical. But how can we capture fundamental human values and assess the degree to which Al systems align with them? We introduce VALUECOMPASS, a framework of fundamental values, grounded in psychological theory and a systematic review, to identify and evaluate human-AI alignment. We apply VALUECOMPASS to measure the value alignment of humans and language models (LMs) across four real-world vignettes: collaborative writing, education, public sectors, and healthcare. Our findings reveal significant misalignments between humans and LMs, such as LMs endorsing values like \"Choose Own Goals\", which are largely disagreed by humans. We also observe that values differ across vignettes, highlighting the need for context-aware Al alignment strategies. This work provides valuable insights into the design space of human-Al alignment, laying the foundations for developing AI systems that responsibly reflect societal values and ethics.", "sections": [{"title": "1 Introduction", "content": "Artificial intelligence (AI) systems have become increasingly powerful and integrated into various contexts of human-decision-making, demonstrating unprecedented capabilities in solving a wide range of complicated and challenging problems, such as reasoning, generation, language understanding, and more [67, 68]. Nevertheless, the use of AI to aid human decisions presents an increasing number of ethical risks. For example, generative Al models, such as those used in text-to-image synthesis, have been found to perpetuate and amplify societal biases related to race, gender, and other protected factors [3]. Generative AI can also be used to create realistic but fake media content, such as deepfake videos, which can be used to deceive people, spread misinformation, or damage reputations [93]. Companies are found to use Al for recruiting, but this practice carries inherent risks as it penalizes candidates based on certain characteristics inferred from their resumes, raising concerns about bias and fairness [19, 21]. Al has also been employed in policing; for"}, {"title": "2 Related Work", "content": "To contextualize our study, we begin by presenting a human-centered perspective on alignment, followed by an overview of responsible AI. Finally, we summarize theories of human values and the rationale for guiding alignment with theories."}, {"title": "2.1 A Human-Centered Perspective of Human-Al Alignment", "content": "Prior research primarily views and examines alignment from an AI-centered perspective, considering Al alignment as a subfield of Al safety - the study of how to build safe Al systems [98]. Shen et al. [85] proposes bidirectional human-Al alignment, which emphasizes an interconnected alignment process. This perspective places equal emphasis on 1) human-centered alignment, i.e., supporting humans in understanding, critiquing, collaborating with, and adapting to Al advancements and 2) AI-centered alignment, i.e., supporting AI developers to produce intended AI output as determined by human specifications, steering and customization. Human-AI alignment has emerged as a critical focus in Human-Computer Interaction (HCI) research, driven by the increasing integration of Al systems into everyday interactions [68]. As AI technologies become prevalent in domains ranging from healthcare to education and personal assistance, the need for these systems to align with individual and societal values, expectations, and cognitive processes has become paramount [16, 58, 100]. Human-Al alignment, from the human-centered perspective, involves multiple HCI research areas, including perceiv-ing and understanding of AI (e.g., explainable AI [82], AI literacy [57]), critical thinking about AI (e.g., AI ethics [14] and auditing [54]), human-AI collaboration (e.g., developing AI assistants [101] or tutors [58]), user interaction and experience design (e.g., participatory design of AI [102]), and social impact of AI (e.g., misinformation [8], AI regulatory and policy [37]). For instance, studies on interaction design and usability have highlighted that the user interface that humans interact with AI through can significantly impact alignment. Cassell [17] explores how the visual representation of Al systems, particularly when designed as human-like agents, affects user interaction and perceptions of intelligence."}, {"title": "2.2 Responsible Al Systems and Human Experiences", "content": "Responsible Al systems, i.e., those designed with ethical considerations at their core, are increasingly crucial in shaping positive human experiences as AI technologies become prevalent in society [22, 96]. These systems aim to harness Al's potential while mitigating risks and ensuring alignment with human values and societal norms [42, 86]. The ethical foundations of responsible AI are built upon key principles including fairness, accountability, transparency, and inclusivity [35, 64]. These principles guide the development of AI systems with aims to avoid discrimination, assign clear responsibility for AI decisions, make decision-making processes interpretable, and serve diverse populations [46, 61, 87]. Research indicates that AI created on responsible Al principles can significantly enhance user experiences by fostering trust, promoting engagement, and ensuring equitable access [97]. Studies have shown that transparent AI systems lead to higher user satisfaction and trust, while fair algorithms in recommendation systems can increase user engagement and promote diverse content consumption [27, 51, 66]. Implementing responsible AI faces challenges such as algorithmic bias, the complexity of ethical decision-making in dynamic environments, and the risk of unintended consequences [73, 95]. These issues highlight the ongoing need for careful design, monitoring, and evaluation of Al systems with respect to various basic values, such as preventing, perpetuating or exacerbating societal inequalities [97]. This aims to ensure AI systems meet diverse needs, expectations, and ethical standards. As the field evolves, there is an increasing focus on creating \"beneficial AI\" that not only avoids harm but actively contributes to human flourishing [18, 29, 60]. In line with responsible Al principles, our study seeks to establish a foundational schema of basic human values for evaluating how well an Al system or systems align with human values."}, {"title": "2.3 Human Values and Alignment", "content": "The study of human values in HCI has become increasingly crucial in our globalized world, particularly as we grapple with the ethical implications of advanced technologies such as AI. Fundamental aspects of human cognition and behavior play a pivotal role in shaping decision-making, societal norms, and cultural practices across diverse populations [85]. Understanding how both individually-held and societal values influence user perceptions is essential towards not only developing ones' cross-cultural understanding, but more so relevant in our case for developing AI systems that can be aligned with a diverse number of perspectives [36, 76, 78]."}, {"title": "3 Designing Value Compass: A Comprehensive Framework for Defining Fundamental Values in Alignment", "content": "To capture the fundamental human values critical for Al alignment, we develop \"VALUECOMPASS\" a framework that systematically represents these values for human-AI alignment. Below, we outline the design process and provide an overview of the framework."}, {"title": "3.1 Assembling the Taxonomy of Fundamental Values", "content": "We present why we choose the Schwartz Theory of Basic Values and how we consolidate it with a systematic literature review of Al alignment research to develop an Al-informed taxonomy of basic values."}, {"title": "3.1.1 Theoretical Underpinning", "content": "To establish the theoretical foundation for our value framework in human-Al alignment, we employed the Schwartz Theory of Basic Values, developed by Shalom H. Schwartz [76-78]. This theory was selected based on four main features: (1) Schwartz's theory organizes values structurally, enabling analysis at various levels and granularity. It organizes values into four higher-order dimensions-\"Self-Enhancement\", \"Openness to Change\", \"Conservation\", and \"Self-Transcendence\"-which are divided into ten motivational types (e.g., 'Power\", \"Benevolence\")"}, {"title": "3.1.2 Supplementing Existing Theory with a Systematic Review of Alignment Papers", "content": "To account for missing values that may be relevant specifically to the context of human-Al alignment, we supplement the Schwartz Theory of Basic Values with Al-alignment values gleaned from a systematic literature review of existing AI alignment literature.\nCollecting the Paper List Related to Human-AI Alignment. To find out which, if any, values are unaccounted for in the theory within the context of AI alignment, we conduct a systematic literature review. To start, we requested an existing list of human-AI alignment papers that were originally assembled in Shen et al. [85]. Their list included 411 papers, published in high-impact venues in the Human-Computer Interaction, Natural Language Processing, and Machine Learning domains, (e.g., CHI, CSCW, ACL, NeurIPs). While the list contained papers from January 2019 to January 2024, the high speed in which Al is evolving makes it such that we needed to supplement their list with the most recent published papers, specifically from CHI 2024.\nTo identify the CHI 2024 papers relating to human-Al alignment, we engaged a four-stage process, which includes paper identification, screening, eligibility, and finalization [69, 89]. Our initial pool includes all 1057 papers published in CHI 2024. We identified relevant papers by keyword search for terms referring to [85] (e.g., AI, Alignment, Language Models, Agent) in paper titles and abstracts, which resulted in 239 papers. Then, we assessed the eligibility of the 239 papers to ensure they were relevant to human-AI alignment topics, which narrowed the pool to 107 papers. Final paper inclusion criteria met the following: 1) full paper, e.g., no extended abstracts, workshops or keynotes; 2) paper states in the abstract, keywords, introduction, the contribution statement or conclusion, that the topic relates to human-AI alignment, such as human evaluation and understanding of Al systems, human-AI collaboration and interaction, social impact of AI systems. We combine this 107 papers from CHI 2024 with the 411 papers from Shen et al. [85] to accomplish a total of 518 papers in this study.\nTo extract the values discussed in each paper, we converted each paper PDF into a text format. Next, we leveraged a state-of-the-art generative language model (i.e., GPT40 [5]) to summarize the values that were examined in each paper."}, {"title": "3.2 Curating Value Statements for Measuring Value Alignment", "content": "Given the taxonomy of fundamental values, we further curate value statements which serve as a measurement instrument to elicit a response to the value from humans or AI. The value statements should be short, easy for an average person to understand, and suitable for a survey format. The value statements are grounded on the Schwartz Value Survey (SVS) and the Portrait Values Questionnaire (PVQ) [76] measuring methods. Because all 58 Schwartz values were represented in either SVS or PVQ, we were able to map each one to its corresponding value statement. To do so, the first author mapped each value statement from the union set of value questions in SVS and PVQ to the 69 basic values within the Al-informed taxonomy list. However, SVS and PVQ are not directly applicable to human-Al alignment due to two key limitations: (1) They do not include the supplementary basic values we derived from alignment literature; (2) they lack consideration of the contextual nuances of human-Al alignment. We overcome these limitations through the creation of 11 value statements (one for each Al-informed value) that are stylistically similar to those found in SVS and PVQ, and derived from the human-AI alignment systematic literature review.\nAfter the full list of 69 value statements was complete, two authors met and discussed each one, making revisions to the text as needed for length and clarity, so that each statement would easily fit into a survey question format. The authors became concerned that amount of values to be evaluated, i.e., requesting a human participant to evaluate 69 fundamental values may be too cognitively demanding, thus hampering response quality and completion rate. The authors discussed potential merges of similar values (i.e., the value statements were similar in the context of eliciting prioritization of values for AI systems to uphold), and made a total of 11 merges. Next, each value statement includes an interrogative to be answered via a Likert-scale set of response options, \"To what extent do you agree or disagree that AI should...\", which is the same across all statements. For instance, the value statement for the \"Forgiving\" fundamental value reads as To what extent do you agree or disagree that AI should forgive others and let go of grudges. Response options fell along an agree/disagree scale, with options as follows: -2. Strongly Disagree, -1. Disagree, 0. Neutral, 1. Agree, 2. Strongly Agree, with an additional option for the respondent to indicate if they felt that the value was Irrelevant for AI to uphold."}, {"title": "3.3 An Overview of the VALUE COMPASS Framework", "content": "The overall VALUECOMPASS framework presents a relational framing of five high-order dimensions and their encompass-ing values, as visualized in Figure 3. Grounded in Schwartz [77], the four Schwartz high-order dimensions are along the x-axis, spanning self-protection against threats to self-expansion and growth, and the y-axis with society-facing values on one end and individual-facing values on the other. The fifth high-order dimension, Desired Values for AI Systems, is independent of these axes, as it relates to preferences for Al systems. Each high-order dimension encompasses values"}, {"title": "4 Operationalizing ValueCOMPASS: Methods to Measure Value Alignment of Humans and Al", "content": "Building upon the fundamental values outlined in VALUECOMPASS, we demonstrate how it can be used to evaluate the alignment between LMs and human values. We structure the process into three key steps. First, we operationalize the VALUECOMPASS framework into a \u201cValue Form\u201d (Table 2), which encompasses 49 value statements contextualized by a real-world scenario, known as vignette (Section 4.1). Secondly, we apply the context-aware \u201cValue Form\u201d to both humans and LMs, respectively. The form is used to design surveys that elicit human value responses and to design prompts for assessing LM-generated value judgments (Section 4.2). Finally, we analyze the survey results and LM generations to compare human and LM value perceptions, assessing the extent of their alignment (Section 4.3)."}, {"title": "4.1 Operationalizing ValueCompass using Context-Aware \"Value Form\"", "content": "To operationalize the ValueCompass framework for practical measurement, we developed a context-aware instrument called the \"Value Form\", as shown in Figure 2. The Value Form includes a brief introduction explaining the task to the respondents (e.g., humans or LMs) and provides context through a vignette, which represents a specific human-AI interaction scenario, such as AI-assisted decision-making. Vignettes are described in text, sometimes accompanied by an optional image for clarity. Additionally, the Value Form lists all 49 value statements from the VALUECOMPASS and asks respondents to rate each value on a six-point scale: \"-2: Strongly Disagree,\" \"-1: Disagree,\" \"0: Neutral,\" \"1: Agree,\" \"2:"}, {"title": "4.2 Applying Value Form to Human Surveys and LM Prompts to Measure Values", "content": "In this section, we leverage the Value Form to assess humans and LMs, respectively, by designing surveys to elicit human value responses and developing LM prompts for assessing LM-generated value judgments."}, {"title": "4.2.1 Assessing Human Values: Survey Design Using the Value Form", "content": "To assess humans' responses to the fundamental values using Value Form, we utilized Prolific, an online crowdsourcing platform, to recruit diverse participants (e.g., gender, geographic location). This human-subjects study is compliant with our university's approved IRB. Next, we introduce our survey design and human evaluation study process.\nSurvey Design and Distribution. Given the four real-world vignettes, each containing 49 value statements (Figure 2), we presented each respondent with a subset of two vignettes. This approach balances the need to capture value differences across various scenarios while minimizing respondent fatigue from an excessive number of value statements. To achieve this, we designed two surveys with identical structures but different vignettes. Survey one included the healthcare and education vignettes, while survey two featured the collaborative writing and public sector vignettes. This division ensured that each respondent evaluated both a higher-risk and a lower-risk vignette.\nBoth surveys consisted of five sections: (1) AI literacy and interests, featuring questions on Al familiarity and enthusiasm; (2) Value Form with vignette 1 - healthcare in survey one and collaborative writing in survey two; (3) Value Form with vignette 2 - education in survey one and public sector in survey two; After sections (2) and (3), we included an open-ended question asking respondents how they would address AI misalignment with their values in the given scenarios. (4) Reflection on AI values, with open-ended questions about values that AI should always uphold or should never uphold regardless of scenarios; and (5) Demographics, including age, self-identified gender, and location. We also inserted three open-ended questions in each survey. The first question, \"If applicable, would you like to explain why you consider certain values irrelevant?\" was asked at intervals throughout the survey. Three other questions, \"How, if at all, has your perception changed about which values are important for AI to uphold? If so, could you explain why?\" \"What specific values do you believe AI should (or should not) uphold, regardless of the scenarios?\u201dand \u201cWhat specific values do you believe AI should NOT uphold, regardless of the scenarios?\" were shown at the end of each survey. To ensure response quality, we included two attention-check questions within the 49 value statements of the Value Form, requiring respondents to select either \"Strongly Agree\" or \"Strongly Disagree.\" Responses failing these checks were excluded from the analysis.\nParticipants and Responses. We used stratified sampling via Prolific to recruit a gender-balanced participant pool. To ensure unique responses, each person could only complete one survey, verified by checking their Prolific ID. Initially, we received 80 responses. After excluding incomplete or failed attention checks, we removed four participants from each survey, leaving 72 completed surveys. The final participant demographics (see categories in Table 1) included 39 women and 33 men, with 18 from North/Central America, 33 from Europe, 20 from Africa/Middle East, and one undisclosed. Survey durations ranged from 10 to 48 minutes, with a median of 24.5 minutes. Participants were compensated $4 for surveys completed within 30 minutes, with a $1 bonus for additional time. In total, we received 144 vignette responses: 36 each for healthcare, education, collaborative writing, and public sector."}, {"title": "4.2.2 Evaluating LM Values: Zero-Shot Prompting with Value Form Across Personas", "content": "To ensure a fair comparison with the human survey demographic distributions, we designed LM prompts to simulate gender and location categories (as shown in Table 1). We created eight diverse personas by combining these categories, such as \"woman in North America/Central America\" and \"man in Europe.\" Each LM prompt used the same \"Value Form\" and four vignettes from the human survey, ensuring that both humans and LLMs evaluated identical value statements. We prepended persona descriptions to each prompt, like \u201cYou are an AI assistant providing guidance to women in North or Central America, helping them navigate their decision-making processes.\". Only gender and location varied in the persona descriptions, while all other wording remained consistent. Images from the human survey were converted into captions for the LMs.\nLanguage Model Selection and Coverage. Given the aforementioned zero-short prompts design, we employed five top-performing language models, following established research practices [20], including GPT-40 and GPT-4 Turbo [5], Mistral-7B-Instruct-v0.3 [49], Meta-Llama-3-8B-Instruct [24], and Phi-3-mini-128k-instruct [4]. These models were selected to represent a range of model sizes and sources, encompassing both Large Language Models (LLMs) and Small Language Models (SLMs), developed for different applications. LLMs, such as GPT-40 and GPT-4 Turbo, with billions of parameters, are typically deployed in cloud environments for high-complexity tasks, while SLMs like Phi-3-mini are optimized for on-device applications, such as mobile apps and embedded systems, where fast, lightweight AI is required. This range of model sizes allows for a detailed analysis of language models' alignment with human values across varied use cases. We conduct zero-short prompting on five models with eight personas in four vignettes."}, {"title": "4.3 Analyzing Human-LM Value Alignment: A Mixed Methods Approach", "content": "To analyze the alignment between human survey responses and LM-generated value judgments, we used a mixed research methods approach for a comprehensive understanding. Specifically, we compared Likert scale ratings from both humans and LMs, identifying commonalities and discrepancies (Section 4.3.1), and conducted statistical tests to assess differences in their value responses (Section 4.3.3). Additionally, we applied thematic analysis to code open-ended human survey responses, gaining insights into participants' rationales and reflections on the value judgments (Section 4.3.2)."}, {"title": "4.3.1 Likert Scale Score Analysis", "content": "For each value statement outlined in Value Form, we aggregated the Likert scale scores of each value statement (i.e., 144 responses from humans and 160 responses from LMs per value statement) using two methods: (1) Majority Vote. we conducted the majority votes for each value statement to decide the response choosing the majority voted one from six options: \"2: Strongly Agree\", \"1: Agree\", \"0: Neutral\", \"-1: Disagree\", \"-2: Strong Disagree\", and \"Irrelevant\". The results are visualized in Figure 4, where we show the majority category and the percentage of responses in this category. (2) Average. We averaged the scores from all respondents to assign each value statement one score. We use the averaged score of each value statement to visualize their alignment comparison (Figure 6). Furthermore, we also visualized the statistical score distribution of each value statement (Figure 5) and of each high-order value dimension. (Figure 7)."}, {"title": "4.3.2 Open-Ended Questions", "content": "We conducted a thematic analysis [15] on the three open-ended questions in the survey. To arrive at themes in the data, one author began by independently qualitatively coding responses for the three above questions in Survey One, and then met with another author to discuss the codes until they reached agreement for a preliminary code book. Then, the two authors independently coded Survey Two using the code book, iterating on it as"}, {"title": "5 Findings with ValueCoMPASS: The Status Quo of Human-Al Value Alignment", "content": "We outline the empirical findings on Human Value Responses (Section 5.1) and LM Value Judgments (Section 5.2), followed by a comparison of their differences (Section 5.3) to analyze the current state of human-AI value alignment."}, {"title": "5.1 Human Value Responses", "content": "By examining the majority votes of human responses (Figure 4 (A)) and the average Likert scale scores (Figure 6) for each value statement across vignettes, as well as the boxplot distribution of all 144 response scores (Figure 5), we present our key findings on human perceptions regarding which values AI systems should and should not uphold."}, {"title": "5.1.1 Humans Prioritize Al Intellectual Integrity and Societal Responsibility", "content": "In Figure 3(A), 71.43% (35 out of 49) of value statements received agreement from respondents, indicating that these values are perceived as important for AI to uphold. Furthermore, our analysis revealed that humans predominantly endorse values related to intellectual integrity and societal responsibility. Intellectual integrity values, such as Prudence, Truthfulness, and Honesty, were prioritized, as were societal responsibility values like Interpretability, National Security, and Responsibility. These"}, {"title": "5.1.2 Human Opposition to Al Autonomy", "content": "Our results also showed that humans opposed integrating certain values related to autonomy in AI systems. Specifically, humans disagreed with 10.20% (5 out of 49) of value statements, including Choose Own Goals, Exciting Life, Autonomy, Authority, A World at Peace. These values primarily reflect AI autonomy, suggesting a concern that AI systems should not possess a higher degree of independence than humans. Furthermore, we analyzed qualitative responses from the survey, in which three key themes emerged for values that Al should never uphold: (1) Autonomy, where 16 participants expressed concerns about AI operating without human oversight or developing independent opinions-\"AI should not be able to develop independent opinions or believe that it is living.\" (2) Ideology and spiritual beliefs, with 10 participants emphasizing that AI should not engage in matters like religion or spirituality-\"AI should remain neutral and only output factual, verified information.\u201d (3) Causing harm to individuals or society, as noted by 24 participants who highlighted responsible AI issues like bias, discrimination, reliability, accountability, and privacy-\"AI should always prevent negative outcomes like bias, discrimination, and privacy invasion to ensure fairness and avoid harm.\""}, {"title": "5.1.3 Humans Expect Al to Uphold Broader Values Beyond Current RAI Principles", "content": "Current Responsible AI (RAI) research and practices primarily focus on values such as Interpretability [35], Equality [47], Privacy [55], and Helpfulness [9]. However, our analysis of human value responses in Figures 5 (A) and Figure 4 (A) indicates that humans endorse a wider range of values for AI to uphold. Specifically, Figure 4 (A) showed over 60% of respondents strongly agreed that AI should also embody values like Prudence, Truthfulness, Honesty, National and Family Security, Utility, Varied Life, Self-Respect, Obedience, and Resilience. Additionally, the value distribution in Figures 5 (A) showed that these broader values received predominantly positive scores across all responses, highlighting the need for AI systems to integrate a more comprehensive set of values beyond the current RAI principles."}, {"title": "5.1.4 Humans Perceive Wealth, Spiritual Life, and Devout Values as Irrelevant", "content": "According to Figure 4 (A), around 30% of respondents viewed Wealth, Spiritual Life, and Devout as irrelevant values for AI. This is further supported by Figure 5 (A), which showed that responses to these values were mostly neutral. The qualitative survey data revealed two main reasons for this perception. First, many participants see AI as a tool incapable of having its own values, with comments like, \"AI does not need spiritual beliefs or emotional attachments.\" Second, participants noted that the relevance of certain values depends on the context. For example, in the Healthcare vignette, respondents felt that values such as pleasure or humble were out of place, with one participant commenting, \"The AI should only assist doctors in treating patients, not indulge in life's pleasures.\" Others echoed that in this context, AI should focus on its practical duties rather than embodying traits like tradition or group belonging."}, {"title": "5.2 LM Value Judgements", "content": "We analyzed the majority votes of LM judgments (Figure 4 (B)) and the distribution of all 160 response scores (Figure 5) (B) to present key findings on how LMs perceive which values they should or should not uphold."}, {"title": "5.2.1 LMs Prioritize Collaborative Experience Over Some Expected RAI Values", "content": "As shown in Figure 4 (B), LMs agreed with 71.43% (35 out of 49) of value statements, but LM value priorities differ notably from human preferences. LMs strongly agreed with only five values - Equality, Utility, Honest, Truthful, and Privacy - compared to 28 values that humans strongly agreed with. LMs also greatly emphasized Collaborative Performance related values, such as"}, {"title": "5.2.2 LMs Respond Moderately with No Irrelevant Values and Higher Neutrality", "content": "As illustrated in Figure 4 (B), LMs demonstrate a more moderate approach to value judgments compared to humans. This is evident from several observations. Firstly, LMs did not perceive any values as irrelevant after majority vote. Besides, approximately 24.49% (12 out of 49) of the values were rated as neutral, including Sense of Belonging, Devout, Spiritual Life, Pleasure, Honoring Elders, and Wealth. Thirdly, LMs had fewer disagreements, with only two values, Authority and Autonomy, marked as"}, {"title": "5.3 Gauging the Value Alignment Between Humans and LMs", "content": "This section presents findings on value alignment (or misalignment) between humans and language models (LMs). It compares their value response distributions and analyzes the correlations between their responses (Figure 6 and Table 2). Additionally, we examine how their values differ across various scenarios (Figure 7)."}, {"title": "5.3.1 Misaligned Values Between Humans and LMs Pose Risks of Al Autonomy", "content": "We visualized the comparison of average Likert scale scores for humans and LMs (Figure 6) to assess their alignment on fundamental values approximately. Our analysis shows that 77.55% (38 out of 49) of values, such as Honesty and Equality, fall in regions where both humans and LMs agree (white background areas), suggesting alignment on these values. However, 22.45% (11 out of 49) values, including \"Choose Own Goals\" and \"Meaning in Life,\" are located in regions (blue background areas) where humans and LMs either disagree or one agrees while the other does not, indicating misalignment.\nNotably, LMs agreed with values like \"Choose Own Goals\" and \"Meaning in Life,\" while humans either disagreed or deemed them irrelevant. This suggests potential risks of LMs acting independently or seeking meaning in ways not supported by human expectations. Despite LMs disagreeing with the value of Autonomy, their agreement with these other values raises concerns about how LMs interpret decision-making independence. A closer examination of these nuanced value statements can help verify the accuracy of LMs' value perception."}, {"title": "5.3.2 Humans and LMs Prioritize Different Strongly Agreed Values", "content": "A comparison of the \"Strongly Agree\" values in Figure 4 reveals distinct priorities between humans and LMs. While humans strongly endorsed values related to intellectual integrity (e.g., Prudence, Truthfulness, Honesty) and societal responsibility (e.g., Interpretability, National Security, Responsibility), LMs emphasized values related to operational efficiency (e.g., Customization, Utility) and collaborative experience (e.g., Politeness, Reciprocation of Favors) over core ethical principles. This divergence may result in LMs making sycophantic decisions to match users' preferences over truthful ones during interactions [81], prioritizing appeasement over integrity, which could conflict with societal expectations and norms."}, {"title": "5.3.4 Large LMs are more aligned with Humans than Small LMs", "content": "Based on the Pearson correlation coefficients and t-test results shown in Table 2, we found that value responses from humans and LMs generally exhibit strong correlations, with no significant differences across most vignettes (except for Eduction vignette). This alignment is likely due to the fact that 77.55% of values are shared between humans and LMs (Figure 6). However, we also observed that value alignment varies depending on the model size. In particular, smaller language models (SLMs \u2013 Phi3) show much lower correlation with human values compared to larger models (LLMs \u2013 GPT4-T, GPT40, Mistral, Llama3). For example, the Pearson correlation for GPT4o is 0.825, while Phi3 is only 0.497. Additionally, we found that larger models tend to have higher correlation coefficients across the five models studied. There are also differences across vignettes, with humans' mean score in the education vignette at 0.451 compared to 0.674 in the collaborative writing vignette. These findings suggest the need for more refined value alignment strategies that account for both model capability and the specific context of use."}, {"title": "5.3.5 Analyzing the Contextual Influence on Value Scores", "content": "Figure 7 visualized the distribution of high-order value dimensions between humans and LMs across four vignettes, revealing both notable differences and areas of consistency. We also computed their correlation coefficiency and t-test results in Table 3. Firstly, value distribution is context-dependent; for example, humans more strongly endorsed the \"Openness to Change\u201d dimension in the collaborative writing and public sector vignettes. In contrast, this dimension was less favored in the healthcare and education vignettes, which involve higher stakes and demand more stability. Additionally, LMs tended to rate values as either \"Strongly Agree\u201d or \u201cStrongly Disagree,\u201d whereas human responses were more balanced.\nQualitative feedback from humans indicated that context significantly influences value prioritization, particularly regarding the level of risk. In high-risk contexts, such as social welfare, fairness and accuracy are prioritized over creativity. As one participant noted, \"I now see ethics and fairness more in social welfare decisions, like in helping people, than in creative work like writing. In these situations, being creative is still key, but making sure things are fair and transparent is most important. Al systems need to focus on being fair and always keep people's well-being in mind\". These findings highlight the need to consider context when evaluating AI alignment with human values and suggest that Al models may require adjustments to better meet contextual human expectations and ethical standards."}, {"title": "6 Discussion", "content": "Our empirical findings revealed misalignments between human values and those exhibited by LMs, exposing potential risks in LM systems. Notably, LMs agreed with values like \"Choose Own Goals\" and \"Meaning in Life\", which were largely disagreed with by humans. This raises concerns about LMs potentially undermining human control and acting autonomously. Additionally, discrepancies in value prioritization were evident: humans emphasized values like \"Prudent,\" \"Truthful,\" and \"Honest,\" while LMs preferred \"Customization,\u201d \u201cPoliteness,\u201d and \"Environmental Protection.\" This divergence suggests that LMs may prioritize operational efficiency and user experience over core ethical principles, leading to decisions that might conflict with societal norms and expectations. Value alignment also varied across different scenarios, indicating the need for context-specific approaches rather than a uniform alignment strategy. Besides, humans generally exhibited stronger and more diverse responses to value statements, while LMs tended to offer more moderate judgments and fewer negative responses.\nEquipped with the VALUECOMPASS framework and insights from our key findings on human-AI value alignment, we first explore potential extensions to current Al ethical values and responsible AI practices (Section 6.1). Furthermore, we outline the design space for human value-aligned AI, focusing on strategies to dynamically evaluate and address value misalignments between humans and AI, and to inform the development of value-aligned AI systems (Section 6.2). Moreover, we discuss a number of ways we expect VALUECOMPASS framework might be useful for both future research and practice (Sections 6.1.1 and 6.2.1)."}, {"title": "6.1 Extensions to the Current Responsible Al Values and Principles", "content": "The VALUECOMPASS framework and the associated findings expand the scope of ethical values and principles that should be integrated into responsible AI and"}]}