{"title": "State-of-the-Art Approaches to Enhancing Privacy Preservation of Machine Learning Datasets: A Survey", "authors": ["Chaoyu Zhang"], "abstract": "This paper examines the evolving landscape of machine learning (ML) and its profound impact across various sectors, with a special focus on the emerging field of Privacy-preserving Machine Learning (PPML). As ML applications become increasingly integral to industries like telecommunications, financial technology, and surveillance, they raise significant privacy concerns, necessitating the development of PPML strategies. The paper highlights the unique challenges in safeguarding privacy within ML frameworks, which stem from the diverse capabilities of potential adversaries, including their ability to infer sensitive information from model outputs or training data. We delve into the spectrum of threat models that characterize adversarial intentions, ranging from membership and attribute inference to data reconstruction. The paper emphasizes the importance of maintaining the confidentiality and integrity of training data, outlining current research efforts that focus on refining training data to minimize privacy-sensitive information and enhancing data processing techniques to uphold privacy. Through a comprehensive analysis of privacy leakage risks and countermeasures in both centralized and collaborative learning settings, this paper aims to provide a thorough understanding of effective strategies for protecting ML training data against privacy intrusions. It explores the balance between data privacy and model utility, shedding light on privacy-preserving techniques that leverage cryptographic methods, Differential Privacy, and Trusted Execution Environments. The discussion extends to the application of these techniques in sensitive domains, underscoring the critical role of PPML in ensuring the privacy and security of ML systems.", "sections": [{"title": "Introduction", "content": "Recent developments in the field of machine learning, especially in the domain of deep learning, have markedly impacted numerous sectors, such as next-g network[69], financial technology, and surveillance systems[13, 12], signaling a transformative phase in these industries. Simultaneously, the surge in machine learning-powered artificial intelligence has brought privacy issues to the forefront. Hence, the notion of Privacy-preserving Machine Learning (PPML) has arisen as a prominent area of interest in both the industrial and academic communities.\nIt is essential to acknowledge that the task of maintaining privacy in machine learning presents distinct challenges, which vary based on the capabilities of potential adversaries. The growing emphasis on understanding and mitigating information leakage from training data involves considering a spectrum of threat models. These models reflect various levels of adversarial power, including the ability to view model outputs, gain access to model parameters, or use certain iterative optimization techniques. The objectives of these adversaries can vary, such as membership inference, attribute inference, property inference, and data reconstruction [10, 46]. Each of these objectives, as identified in multiple studies, introduces specific difficulties in protecting the integrity and confidentiality of training data within machine learning frameworks.\nIn the field of PPML, a primary focus is on ensuring that the implemented ML models prevent the escape of confidential information from the training data beyond the trusted boundaries of the data sources. Specifically, during training phase, the main issues of privacy leakage are centered around the handling of data and its computation. Current research addresses these challenges through two key approaches: (i) determining methods to refine or filter the training data with the aim of either minimizing or entirely eradicating any information sensitive to privacy; and (ii) developing techniques for processing the training data in a way that upholds privacy."}, {"title": "A Taxonomy of Adversary Goals", "content": "To compromise data privacy in machine learning systems, adversaries primarily target the exposure of sensitive information embedded in the training dataset. Their goal is to exploit vulnerabilities in machine learning models to extract, reconstruct, or infer private data. This includes conducting Membership Inference Attacks (MIAs) to ascertain if certain data points were used in training, thereby potentially revealing personal or confidential information. Data Reconstruction Attacks take this a step further, attempting to regenerate specific data points or entire datasets from model outputs, thus breaching individual privacy. Property Inference Attacks aim to deduce general properties or characteristics of the training dataset, such as demographic information, without necessarily pinpointing individual data records. Moreover, Model Inversion Attacks specifically attempt to reverse-engineer the model to extract sensitive features of the input data, further endangering data privacy. These attacks underscore the significant risks posed to data privacy by inadequately secured machine learning models, highlighting the necessity for robust privacy-preserving techniques in model training and deployment.\nIn this paper, We categorize and summarize four different kinds of attacks that are pertinent to the privacy risk associated with the training dataset [51]:"}, {"title": "Membership Inference Attack:", "content": "The adversary's goal is to determine if a particular record was included in the training dataset of the target model [59].\nDefinition: Given a target machine learning model $f(x;\\theta)$ and a data record $x$, the goal of MIA is to determine whether $x$ was part of the training dataset $D_{train} = \\{(x_i, Y_i)\\}_{i=1}^n$. This can be defined as:\n$MIA(f(x; \\theta)) = \\begin{cases}\n1 & \\text{if } x \\in D_{train},\n\\\\ 0 & \\text{otherwise}.\n\\\\ \\end{cases}$\nHere, 1 indicates that x is a member of the training set, while 0 indicates non-membership. MIAS can be categorized based on the attacker's knowledge and access level into black-box and white-box attacks:\nBlack-Box Attack: In this scenario, the attacker has limited knowledge. They only have access to the model's predictions (output probabilities or classes) for given inputs. The attacker uses this output information to infer whether a particular data point was part of the training dataset. This type of attack is common in real-world scenarios where the internal workings of the model are not exposed [75, 67, 7].\nWhite-Box Attack: Here, the attacker has complete knowledge about and access to the target model. This includes the model's architecture, parameters, and training data. White-box attacks are more powerful as they can leverage the inner workings of the model for more accurate inferences about the training data[29].\nBoth types of attacks pose significant threats to data privacy, each requiring different defense strategies due to their varied levels of access and knowledge."}, {"title": "Data Reconstruction:", "content": "Reconstructing samples from the target model's training dataset is the adversary's goal. In this case, a successful attack can result in the training dataset being partially reconstructed [4].\nDefinition: Given the training set $D_{train} = \\{(x_i, Y_i)\\}_{i=1}^n$, where $x_i$ is the image and $y_i \\in \\{+1, -1\\}$ is the label, and a neural network $f(x; \\theta)$, we can reconstruct a large subset of the training data by exploiting the implicit biases of neural networks. The neural network aims to solve the max-margin problem:\n$\\arg \\min_{\\theta'} \\frac{1}{2}||\\theta'||^2 \\quad s.t. \\quad \\forall i \\in [n], Y_i f_{\\theta'}(x_i) \\geq 1$\nThis shows that by optimizing images (and dual parameters) to satisfy the Karush-Kuhn-Tucker (KKT) conditions of the max-margin problem using a trained neural network, training data can be reconstructed. This represents a potential attack leading to the leakage of training data. A more efficient version of the dataset reconstruction attack, which can provably recover the entire training set in the infinite width regime, is presented in [36, 55]. In the paper [53], the authors demonstrate that the changes in the output of a black-box ML model, observed before and after an update, can leak information about the dataset used for the update, specifically the updating set."}, {"title": "Property Inference:", "content": "The adversary seeks to uncover sensitive statistical characteristics of the training distribution of the target model [20].\nIn the context of property inference attacks, we utilize specific notations and definitions to frame our analysis. Sets are denoted by calligraphic letters, such as $\\mathcal{T}$, while distributions are represented by capital letters, like D. The joint distribution of two random variables, for example, the distribution of labeled instances, is indicated by (X, Y). Equivalence between two distributions is expressed as $D_1 = D_2$. Sampling from a distribution X is denoted by $x \\leftarrow X$, and the probability of such sampling is indicated by $Pr_{x \\leftarrow X}$. The support set of a distribution X is represented by Supp(X). We also use the notation $p \\cdot D_1 + (1 \u2212 p) \\cdot D_2$ to describe the weighted mixture of two distributions $D_1$ and $D_2$ with respective weights p and (1 \u2013 \u0440).\nIn studying property inference, we adopt the model introduced in prior research. This involves a learning algorithm $\\mathcal{L}: (\\mathcal{X} \\times \\mathcal{Y})^* \\rightarrow H$ that maps datasets in $\\mathcal{T} \\in (\\mathcal{X} \\times \\mathcal{Y})^*$ to a hypothesis class H. We consider a Boolean property $f : \\mathcal{X} \\rightarrow \\{0,1\\}$ and focus on adversaries who seek to ascertain statistical information about the property f over the training dataset $\\mathcal{T}$. Specifically, the adversary's objective is to discern whether the fraction of data entries in $\\mathcal{T}$ that exhibit the property f is $f(\\mathcal{T}) = t_0$ or $f(\\mathcal{T}) = t_1$ for some $t_0 < t_1$ within the range [0, 1]. This is achieved in a black-box setting where the adversary can only query the trained model for output labels without access to confidence values or model parameters. This approach aligns with recent explorations in the field of membership-inference attacks.\nTo formalize property inference attacks, we distinguish between two underlying distributions D, $D_+$ for dataset instances where f(x) = 0 and f(x) = 1, respectively. We then consider two mixed distributions $D_t = t \\cdot D_+ + (1 \u2212 t) \\cdot D$, where $D_t$ represents the distribution with a t fraction of points having f(x) = 1. The adversary's goal is to differentiate between $D_{t_0}$ and $D_{t_1}$ for certain values of $t_0$ and $t_1$, by querying a model M trained on one of these distributions in a black-box manner. This approach follows methodologies used in previous research and assumes that the adversary can sample from both D and $D_+[39, 20]."}, {"title": "Model Inversion and Attribute Inference:", "content": "An adversary having partial knowledge of specific training records and access to a model trained on those records attempts to deduce sensitive information about the records. [68, 40].\nOptimization-based Gradient Inversion and Closed-form Gradient Inversion are two distinct methods used to analyze privacy risks in federated learning, where participants collaboratively train a model by sharing local model gradients or updates.\nIn Optimization-based Gradient Inversion, an attacker can revert the gradient $g_i$ uploaded by individual clients $i \\in \\{1,2,\\dots,n\\}$ to their respective local datasets $D_i$ by solving an optimization problem:\n$\\arg \\min_{\\tilde{D_i}} [d(\\nabla D_i - \\nabla \\tilde{D_i}) + r(\\tilde{D_i})]$\nwhere $D_i$ refers to randomly initialized dummy samples, d(\u00b7) is a distance function, and r(\u00b7) is a regulation function. [76] identified this issue and proposed the deep leakage from gradient (DLG) attack, which uses the second norm as the distance function and the L-BFGS optimizer to solve the optimization problem.\nAlternatively, Closed-form Gradient Inversion offers a less computationally demanding method. This technique aims to reconstruct inputs using a direct closed-form derivation, as opposed to an iterative optimization process. The research conducted by Aono et al. [2] examines privacy breaches in linear models, revealing that inputs to linear layers can be perfectly reconstructed from their gradients, a situation described as linear leakage. This approach is especially potent in contexts involving linear models, where the relationship between gradients and inputs is directly discernible. This direct method facilitates the recovery of original inputs from gradients in a more straightforward and efficient manner compared to iterative techniques.\nAttribute Inference [25, 21] in machine learning and data privacy involves an attacker inferring sensitive attributes (y) of individuals from non-sensitive data (x and z). Consider a dataset D with instances as tuples (x, y, z), where x and z are non-sensitive and sensitive attributes, respectively. The attacker leverages a prediction model f: X \u00d7 Z \u2192 Y trained on D, to construct an inference function g: X X Z \u2192 Y that estimates y. The goal is to maximize the accuracy of g in predicting y, formulated as maximize Accuracy (g(x, z), y). Various statistical methods, such as logistic regression or neural networks, can be used to build g. This approach highlights privacy risks, especially when y represents personal data like health or financial information."}, {"title": "Privacy-Preserving Techniques in PPML", "content": "Regarding training data privacy in privacy-preserving machine learning , current approaches primarily focus on several strategies:\n\u2022 Utilization of traditional anonymization mechanisms, such as k-anonymity [19], which involves removing identifying information from the training data prior to its use in training processes. This method aims to ensure that individual data points cannot be traced back to specific individuals.\n\u2022 Representation of the original dataset through a surrogate dataset. This is achieved either by grouping anonymized data or abstracting the dataset using sketch techniques [32]. These methods aim to retain the utility of the data for analysis and training while reducing the risk of privacy breaches.\n\u2022 Implementation of differential privacy (DP) mechanisms [1, 14]. This involves integrating a privacy budget, typically in the form of added noise, into the dataset. The objective is to prevent the leakage of private information by ensuring that the output of the analysis or training does not significantly change when an individual's data is altered or removed, thereby preserving their privacy.\n\u2022 Training ML models on encrypted training data is facilitated by the use of cryptographic techniques, notably homomorphic encryption and secure multiparty computation (MPC) [42, 34].\nFrom the perspective of computation, current PPML techniques can be categorized into two distinct approaches:\n\u2022 When the training data is processed using traditional anonymization or differential privacy mechanisms, the computational process during training is akin to that in standard, or 'vanilla', model training. In these cases, while the data may be modified or perturbed for privacy reasons, the computational methods and algorithms used for training the model remain largely unchanged from those used in conventional ML training.\n\u2022 In scenarios where the training data is safeguarded through cryptographic methods, the computa-tion involved in privacy-preserving (crypto-based) training becomes more intricate than in regular model training. Cryptographic techniques like homomorphic encryption or secure multiparty"}, {"title": "Differential Privacy", "content": "In centralized machine learning settings, where a single entity is responsible for training the model, the technique introduced by Abadi et al. [1], i.e., DP-SGD, offers enhanced differential privacy protections. DP itself is a key framework that provides a structured way to evaluate the privacy protections of algorithms. Differential Privacy (DP) is a technique used to enhance privacy in data processing by adding random noise to training data or model parameters. Informally, it requires that alterations to a single training sample should only have a statistically negligible impact on the output. This use of random noise ensures that the original dataset cannot be reliably deduced from the computation results.\nTo understand differential privacy more clearly, it's essential to grasp the concept of 'neighboring datasets.' Two datasets, D and D', are considered neighbors if they differ by at most one sample. A randomized algorithm A is said to satisfy (\u03b5, \u03b4)-differential privacy if, for any two neighboring datasets D and D' and for all possible outputs S within the range of A, the following condition is met:\n$Pr[A(D) \\in S] < e^{\\epsilon} \\cdot Pr [A (D') \\in S]$\nIn this formula, \u03b5 is a small positive number representing the privacy loss, while \u03b4 is a parameter that gives the probability of this privacy guarantee being broken. The smaller these values, the stronger the privacy guarantee. This inequality ensures that the algorithm's output is similarly likely whether the input dataset is D or D', thus preserving the privacy of individual data points within the dataset. Several well-known mechanisms exist for implementing differential privacy, including the exponential mechanism, the Laplacian mechanism, and the Gaussian mechanism [15]. Each of these mechanisms offers a different way to achieve differential privacy, and the choice between them depends on the specific requirements of the task, such as the type of data and the desired balance between privacy and accuracy.\nDP plays a pivotal role in ensuring that the output of an algorithm remains statistically consistent, even if the input data varies by a single record. This concept is particularly crucial in machine learning, where a \"record\" represents an individual data point in the training set, and the \"algorithm\" is the machine learning model. DP thus safeguards the privacy of individuals in the dataset by guaranteeing that the inclusion or exclusion of any single data point does not markedly influence the model's overall output. DP-SGD is a specific application of DP in the context of machine learning. It focuses on the manipulation of gradients within the learning algorithm. This involves introducing random perturbations to the gradients calculated during the training process, before they are applied to update the model's parameters. Such perturbations help in maintaining the indistinguishability of individual data points, thereby enhancing privacy.\nThe general principle of DP-based protocols in PPML involves adding random noise to various elements needing protection, such as training samples, gradients, and model parameters. Applying DP in machine learning ensures that models are resistant to specific types of attacks, such as membership inference attacks (where an attacker tries to determine if a particular data point was used in training) and model inversion attacks (aiming to recreate input data from model outputs). While differential privacy offers substantial privacy benefits and can be efficient, it often comes with a trade-off in terms of the usability or accuracy of the data. This is due to the noise added for privacy preservation.\nIn exploring DP-based PPML protocols, it is essential to consider two key aspects: Model Training and Model Prediction."}, {"title": "DP in Model Training", "content": "In this phase, DP can be implemented to ensure that the training process does not reveal sensitive information. This is typically achieved by adding noise to the gradients during the learning process, thus preserving the privacy of individual data points.\nShokri and Shmatikov's work [58] was a significant contribution to the field of privacy-preserving machine learning, particularly in the context of distributed deep networks. They introduced a framework"}, {"title": "DP in Model Prediction", "content": "When it comes to model prediction or inference, DP helps ensure that the model's predictions do not compromise individual privacy. This can involve adding noise to the outputs of the model or to the queries made to the model, preventing potential privacy breaches based on the model's responses.\nDifferential privacy may be used not only to training data, but also to properly safeguard model parameters, thereby preventing any leakage of the model, as mentioned in the study by Carlini et al. [8]. Differential privacy guarantees that the acquired parameters of a model do not disclose any sensitive details regarding the training data.\nPathak et al. [47] developed a technique particularly designed for combining classifiers in a distributed environment, where each classifier is trained on datasets held by parties that do not trust each other. Their methodology combined these separately trained classifiers while ensuring a guarantee of differentiated privacy. A noteworthy feature of their approach was that the level of noise introduced for privacy purposes decreased as the size of the smallest dataset included increased. Consequently, when the dataset size decreased, a greater amount of noise was necessary to preserve anonymity, potentially compromising the effectiveness of the combined model.\nExpanding on this, Jayaraman et al. introduced an output perturbation technique, as described in their paper cited as [24]. This method significantly enhanced the effectiveness of the methodology mentioned in [47]. Their approach consisted of applying Laplace noise directly to the global model after the training process. Unlike Pathak et al.'s method, the noise in Jayaraman et al.'s approach was roughly inversely proportional to the size of the entire dataset rather than the smallest dataset. This adjustment meant that the noise level was more manageable and less likely to significantly degrade the utility of the model, especially in scenarios involving larger datasets.\nThese studies highlight the ongoing evolution and refinement of differential privacy techniques in distributed learning environments, balancing the twin objectives of maintaining robust privacy protections while ensuring the practical utility of the resulting models.\nPapernot et al. [45] introduced a novel approach known as Private Aggregation of Teacher Ensembles (PATE), which marked a significant advancement in privacy-preserving machine learning. The PATE framework involves dividing sensitive data into multiple disjoint subsets, with each subset used to train a separate \"teacher\" model. For making predictions, noise is added to the aggregated results of these teacher models. This noise addition is a crucial step in ensuring differential privacy. The labels generated by the teacher models are then used to train a \"student\" model, which is ultimately deployed for predictions. This approach effectively leverages the strengths of multiple models while maintaining privacy through noise addition and aggregation.\nBuilding on the PATE framework, adaptations have been made for generative models, leading to the development of PATE-GAN [26] and G-PATE [35]. These adaptations apply the principles of PATE to generative adversarial networks (GANs), allowing for the generation of synthetic data that retains the statistical properties of real data while preserving privacy.\nHowever, a challenge encountered with G-PATE was the high privacy cost associated with managing high-dimensional gradients. This issue arises because the complexity and dimensionality of the data"}, {"title": "Cryptographic Techniques", "content": "Training ML models on encrypted data is gaining recognition as an effective strategy for safeguarding the privacy of training data. This approach offers a significant advantage over traditional anonymization and DP mechanisms, which are still vulnerable to inference or de-anonymization attacks, particularly when an adversary possesses additional background knowledge. Encryption-based methods provide more robust privacy guarantees by protecting either the training data or the transferred model parameters using cryptosystems. These methods enable computations to be carried out beyond the trusted scope of the data sources, while still maintaining the confidentiality of the data[70, 17, 56]. This growing interest in encryption-based approaches is a response to their potential to offer stronger and more reliable privacy protections in the field of machine learning [66, 65, 64, 57].\nCryptographic methods, like Homomorphic Encryption (HE), functional encryption schemes, and Secure Multi-Party Computation (MPC) [6, 49], allow for secure operations on encrypted data, ensuring the data remains confidential while being processed; DP, conversely, offers robust privacy guarantees by injecting controlled noise into the data, and ensures that individual data points are protected, even when the aggregated data is utilized for ML model training. The synergy between the cryptographic techniques and DP provides a comprehensive approach to safeguarding privacy in ML, balancing the need for data utility with the imperatives of privacy and security. Homomorphic encryption allows for computations to be performed directly on encrypted data, yielding encrypted results that, when decrypted, match the results of operations performed on the plaintext. This means that data can remain encrypted throughout the entire training process, ensuring its confidentiality. Secure multiparty computation, on the other hand, enables multiple parties to jointly compute a function over their inputs while keeping those inputs private.\nSMPC allows for collaborative computation without revealing the underlying data to any of the participating parties, the main primitives of SMPC systems include Garbled circuits(GC), Oblivious Transfer (OT), and Secret Sharing. Both of these cryptographic methods are crucial for enhancing privacy in ML, enabling training on sensitive data without exposing it.\nGiven the large-scale datasets typical in machine learning, directly applying generic SMPC protocols for training is often not feasible due to computational inefficiencies. Consequently, researchers are faced with the challenge of developing specialized and efficient protocols for PPML. These protocols must be capable of supporting both the training and prediction phases of machine learning models, even when dealing with large-scale datasets. This task requires a careful balance between maintaining data privacy and ensuring the computational feasibility of processing extensive data volumes.\nAdditionally, specialized SMPC primitives like Private Set Intersection (PSI) occupy a unique position in the context of machine learning. PSI extends beyond being merely an application of SMPC; it also serves as a fundamental component for other more advanced applications, such as Federated Learning. In this capacity, PSI plays a critical role in enabling collaborative computation and data analysis while preserving the privacy of individual data sets, particularly vital in scenarios where multiple entities need to work together without fully disclosing their respective data. PSI allows parties to jointly compute common samples from their respective datasets without revealing the original datasets. This capability is vital in data preprocessing, as it ensures that privacy is maintained while enabling collaborative computation or learning from multiple data sources. The development of efficient and specialized SMPC protocols continues to be a significant area of research, aiming to balance the privacy needs with the computational challenges in large-scale machine learning applications.\nIn applications of HE-based machine learning, the typical process involves data owners encrypting their data before transmitting it to a server. When it comes to tasks like making predictions, the required computations are performed directly on the encrypted data, or ciphertexts. This method allows for data processing and analysis while maintaining the confidentiality of the original data, as the"}, {"title": "Model Training", "content": "Demmler et al. [11] developed ABY, a mixed-protocol framework that stood out for its efficient conversion between three types of secret sharing: Arithmetic sharing, Boolean sharing, and Yao sharing. Each of these sharing types is suited for different kinds of computations (arithmetic, logical, and oblivious transfer-based operations, respectively). ABY's ability to seamlessly convert between these sharing types made it a versatile tool for secure computation.\nHowever, one of the challenges in ABY was that the conversions between different sharing types necessitated a significant number of Oblivious Transfers (OTs), which could diminish the computational efficiency during the online phase of the protocol.\nIn response to this challenge, an improved version named ABY2.0 [48] was proposed. ABY2.0 enhanced the performance of the original ABY framework by shifting the bulk of OTs to the preprocessing phase. This meant that the time-consuming process of executing OTs did not hinder the efficiency of the actual computation phase. Furthermore, ABY2.0 supported multi-input multiplication operations without increasing the online communication overhead, thereby making it more efficient for complex computations.\nSecureML [42] used a distinct strategy, emphasizing a configuration that involves two servers that do not collude with each other. The study presented a technique for ensuring the security of neural network training and inference in a scenario where the adversary is semi-honest, meaning they adhere to the protocol but attempt to get extra information. The system developed by SecureML facilitated cooperation among clients while effectively preventing collusion between servers and clients. The objective of this design was to guarantee the integrity of the computation process while safeguarding the data from any internal attacks within the system.\nAdopting heterogeneous computing architectures can make defending against adversarial attacks in complex systems more viable and cost-effective[31]. By integrating various computing resources such as CPUs, GPUs, and FPGAs, security protocols and cryptographic algorithms can be expedited, improving defense strategies' efficiency and practicality [74]. This method optimizes the allocation of computational tasks to the most appropriate processors, enhancing both performance and energy efficiency [17, 70]. Consequently, heterogeneous computing is crucial for implementing advanced, affordable security measures in safeguarding against adversarial threats [72, 30].\nThese developments in secure computation frameworks like ABY, ABY2.0, and SecureML highlight the ongoing efforts to balance computational efficiency with robust security measures in the field of privacy-preserving machine learning and secure multiparty computation."}, {"title": "Model Prediction", "content": "Liu et al. [33] introduced MiniONN, a framework designed for privacy-preserving neural network (NN) predictions. MiniONN's approach involves transforming an existing neural network into an oblivious version, enabling secure predictions. This transformation allows the NN to operate on encrypted data, ensuring privacy while maintaining the functionality of the network.\nDeepsecure [52], another significant work in this field, also focuses on the prediction phase of oblivious neural networks. It is particularly noted for its scalability and provable security within the semi-honest adversary model, where participants are assumed to follow the protocol but may attempt to glean additional information. Deepsecure, however, relies entirely on Garbled Circuits, a cryptographic technique for secure function evaluation. While GC-based frameworks like Deepsecure are highly secure, they also tend to have high communication costs due to the nature of GC operations."}, {"title": "Hardware-protected Method", "content": "Trusted Execution Environments (TEEs) like Intel SGX are engineered to enable the secure execution of programs within hardware-protected enclaves. Intel SGX specifically forms secure memory regions within the address space, segregating these areas from all other system codes. This technique facilitates both authentication and encapsulation, allowing external entities to verify that communications originate from an authentic enclave. Within the framework of Privacy-Preserving Machine Learning (PPML) protocols that utilize TEEs, there is an essential process whereby involved parties must transfer confidential data or models into an enclave through a protected channel. Following the authentication of the software operating within the enclave, it can securely process and return encrypted results. These TEE-based methods are relevant and useful in various aspects of machine learning, encompassing both the training phase and the prediction stage of models, as indicated in [22, 23, 43]. The use of TEEs in PPML ensures that the computation of sensitive data or models is performed in a secure and isolated environment, thus enhancing privacy and security.\nExample. [23] introduces Chiron, a system enabling users to train machine learning models on their data while keeping it concealed from the service provider, typified as ML-as-a-Service. Chiron employs hardware-protected SGX enclaves to segregate the service provider's code from the platform, thereby mitigating the risk of data leakage. Additionally, it utilizes a sandbox to restrict the service provider's code, limiting its interaction with the machine learning toolchain and the parameter server."}, {"title": "Defenses in Collaborative/Federated Learning", "content": "While Federated Learning (FL) contributes to privacy, it does not entirely prevent data exposure. Studies have shown that inversion attacks might reveal detailed images from models' weights or gradients [18, 62, 61, 73]. Additionally, when models are involved in MLaaS, they are vulnerable to misuse or theft by untrustworthy entities. To overcome these shortcomings, FL needs to be enhanced with additional privacy-preserving methods. Implementing FL with secure aggregation (SecAgg) or integrating differential privacy (DP) can impede attacks aimed at dataset reconstruction. Furthermore, applying secure multi-party computation (SMPC) protocols can safeguard the models during the inference process."}, {"title": "DP in Federated Learning", "content": "In the realm of Federated Learning (FL), a prominent method for defining functions resistant to adversarial inferences is DP, which effectively limits the potential privacy loss of individual data subjects through the introduction of noise. Within FL's framework, there are two main variants of DP: Local Differential Privacy (LDP) and Central Differential Privacy (CDP). LDP involves each participant adding noise to their updates prior to transmission to the server, whereas CDP involves the server implementing a DP-based aggregation algorithm to ensure data privacy [44]."}, {"title": "Cryptographic Techniques", "content": "Besides, numerous techniques focused on privacy enhancement have been developed to enable various parties to collaboratively train machine learning models while ensuring their private data is not disclosed in its original form. The aforementioned cryptographic techniques have also consistently been integrated into FL to preserve data privacy, e.g., HE and MPC. The HE ensures that the server can decrypt only the combined or aggregated data, i.e., secure aggregation (SecAgg) [5, 27]. SMPC enables multiple entities to collaboratively compute a function over their inputs without revealing those inputs to each other.\nPrivate Set Intersection (PSI) is a cryptographic technique that allows two parties, P1 and P2, each holding a private set (X and Y, respectively), to determine the intersection of these sets (X\u2229Y) without revealing any additional information about the individual sets. The only information disclosed through the PSI computation is the intersection itself, ensuring the privacy of the elements outside the intersection. PSI is particularly relevant in the context of machine learning, especially in vertical federated learning scenarios. Vertical federated learning involves multiple parties collaborating to train a model, where each party holds different features of the same set of samples. PSI is used here to identify the common samples across the parties' datasets. This process ensures that the model training is conducted only on the overlapping data points, thereby maintaining the privacy of each party's unique data.\nLu and Ding [37] extended the application of PSI to multi-party scenarios within vertical federated learning. Their notable contribution was to accommodate the possibility of participants dropping out during the protocol execution, a practical consideration in real-world applications. This flexibility makes their approach more robust and adaptable to various collaborative environments.\nFurther advancements in the field have seen the integration of PSI with privacy-preserving biometric searches. For instance, recent developments [60] have focused on a specialized form of PSI known as fuzzy-labeled PSI, which has been applied to facial search applications. In this context, fuzzy-labeled PSI allows for the matching of biometric data (like facial images) while preserving the privacy of the individuals in the dataset. This approach is particularly useful for sensitive applications where biometric data needs to be matched or searched without compromising individual privacy.\nThe role of PSI in the field of machine learning, underscores its importance as a tool for safeguarding privacy in collaborative tasks that involve sensitive data processing. This highlights PSI's critical function in balancing the need for data utility with the imperative of maintaining confidentiality, particularly in contexts where multiple parties collaborate on datasets with overlapping but distinct data points."}, {"title": "Hybrid Methods: DP + Crypto", "content": "[27] proposed an FL framework of privacy-preserving medical image processing that incorporates both DP and SecAgg techniques to prevent from model inversion attacks. This design enhances the FL framework's ability to protect sensitive medical data, ensuring that individual records remain confidential while allowing for the collaborative development of robust medical image processing models.\nPriMIA integrates DP at each FL node, i.e., LDP, to offer assurances of privacy at the patient level. It manages the privacy budget for each node through the use of a R\u00e9nyi Differential Privacy (RDP) Accountant [41]. This method allows for precise tracking and control of privacy loss incurred during the learning process, ensuring that the privacy guarantees remain within acceptable bounds for each participating node. In the context of training, SMPC is applied to SecAgg the updates of network weights. This is accomplished using additive secret sharing, a technique which is part of the SPDZ protocol [28], ensuring that each party's contribution to the model update remains confidential while allowing for the collective update to be computed."}, {"title": "Measurement and Evaluation of Privacy-Preserving Techniques", "content": "There are many other emerging techniques relevant to preserving training data privacy, e.g., quantification techniques in measuring privacy leakage.\nIn recent years, the ML privacy field has witnessed a growing trend in the formalization of threat models and adversary capabilities through the use of security and privacy games [54, 38, 63]. This approach, which has been extensively utilized in cryptographic research, involves setting up a security experiment as a probability space. These games set up a security experiment as a probability space, where the effectiveness of an adversary is evaluated based on the probability of their attack succeeding or failing. This approach allows for a more structured and measurable assessment of privacy risks.\nIn these security and privacy games, various scenarios are constructed to simulate potential attack vectors. The adversary's capabilities are defined within the scope of these scenarios, providing a clear framework for understanding and evaluating the level of threat they pose. By quantifying the likelihood of an adversary's success or failure in these controlled environments, researchers and practitioners can gain a more nuanced and empirical understanding of the vulnerabilities in ML systems.\nThis methodology not only aids in identifying and measuring privacy risks but also guides the development of more robust and secure ML models. By understanding the probable effectiveness of different types of attacks, ML privacy research can better focus on mitigating those risks that pose the greatest threat to data privacy. This shift towards a more formalized and empirical approach in assessing ML privacy is a crucial step in enhancing the overall security posture of ML systems and protecting sensitive training data from potential breaches."}, {"title": "Comparison and Summary", "content": "To summarize", "Performance": "Network latency can affect SMPC", "Privacy": "Trust among participants is essential for SMPC to function correctly.\n(c) Application: It is typically"}]}