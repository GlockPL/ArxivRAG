{"title": "Joint Localization and Activation Editing for Low-Resource Fine-Tuning", "authors": ["Wen Lai", "Alexander Fraser", "Ivan Titov"], "abstract": "Parameter-efficient fine-tuning (PEFT) methods, such as LoRA, are commonly used to adapt LLMs. However, the effectiveness of standard PEFT methods is limited in low-resource scenarios with only a few hundred examples. Recent advances in interpretability research have inspired the emergence of activation editing techniques, which modify the activations of specific model components. These methods, due to their extremely small parameter counts, show promise for small datasets. However, their performance is highly dependent on identifying the correct modules to edit and often lacks stability across different datasets. In this paper, we propose Joint Localization and Activation Editing (JOLA), a method that jointly learns (1) which heads in the Transformer to edit (2) whether the intervention should be additive, multiplicative, or both and (3) the intervention parameters themselves - the vectors applied as additive offsets or multiplicative scalings to the head output. Through evaluations on three benchmarks spanning commonsense reasoning, natural language understanding, and natural language generation, we demonstrate that JOLA consistently outperforms existing methods.", "sections": [{"title": "1. Introduction", "content": "Parameter-efficient fine-tuning (PEFT; Han et al., 2024) methods are widely used to adapt large language models (LLMs). However, popular PEFT approaches like LoRA (Hu et al., 2021) often struggle in low-resource settings with only a few hundred examples. Inspired by advances in interpretability research (Vig et al., 2020; Zhang & Nanda, 2023), activation editing techniques (Wu et al., 2024a; Yin et al.,"}, {"title": "2. Background", "content": "Activation editing in LLMs modifies intermediate activation outputs to steer model behavior. We categorize existing approaches into three types based on the transformation function applied to activations. Given an activation output $z^{(l,i)} \\in \\mathbb{R}^{d_l}$ for $i$th component at layer $l$, the general transformation is:\n$z^{(l,i)'} = f(z^{(l,i)}),$ (1)\nwhere $f(\\cdot)$ determines the intervention type:\n*   Additive methods apply a bias vector $a^{(l,i)} \\in \\mathbb{R}^{d_l}$ :\n    $z^{(l,i)'} = z^{(l,i)} + a^{(l,i)}.$\n*   Multiplicative methods scale activations as $z^{(l,i)'} = m^{(l,i)} \\odot z^{(l,i)}$ or $z^{(l,i)'} = (1 + m^{(l,i)}) \\odot z^{(l,i)}$, where $m^{(l,i)} \\in \\mathbb{R}^{d_l}$ and $\\odot$ is an element-wise product.\n*   Hybrid methods combine both transformations:"}, {"title": "3. Method", "content": "In this section, we introduce JOLA, a novel approach for fine-tuning LLMs in low-resource settings. We first identify two key challenges in existing approaches and present an analysis to better motivate our method (Section 3.1). We then propose a gated dynamic attention head selection mechanism to address these limitations (Section 3.2). Figure 1 illustrates the comparison of previous activation editing approaches and JOLA."}, {"title": "3.1. Motivation", "content": "Activation editing methods have demonstrated success in modifying Transformer components such as bias terms (Ben Zaken et al., 2022), MLP layers (Wu et al., 2024a), low-rank hidden state subspaces (Wu et al., 2024b), and specific attention heads (Yin et al., 2024). However, two critical questions remain underexplored: Q1: Which Transformer components are most crucial for effective activation editing? Q2: What combination of multiplicative and additive operations yields the best performance for intervention? Existing approaches predefine the components to edit and rely on fixed intervention strategies, such as simple multiplicative scaling, which limits adaptability and can lead to inconsistent performance across tasks, especially in low-resource scenarios. To address these questions, we conduct controlled experiments to compare the effectiveness of editing different Transformer components and analyze the relative contributions of multiplicative and additive operations."}, {"title": "3.2. Joint Localization and Editing", "content": "Based on our insights from Section 3.1, JOLA focuses on adaptive attention head interventions to maximize activation editing effectiveness. Existing methods like LoFIT (Yin et al., 2024) require manual hyperparameter tuning to select the number of attention heads and cannot adjust the chosen heads during training. Moreover, their head selection criteria do not necessarily align with interventions. For instance, LoFIT employs multiplicative variables to determine head selection before restarting training with additive-only interventions. To address these limitations, we propose a method that jointly learns which heads to modify while optimizing intervention parameters (i.e., vectors $m^{(l,i)}$ and $a^{(l,i)}$).\nWe extend the hybrid intervention method from Section 2 by introducing two scalar gates, $g_a^{(l,i)}$, and $g_m^{(l,i)}$, both in [0, 1]. This results in the transformation:\n$z^{(l,i)'} = (1 + g_m^{(l,i)} \\cdot m^{(l,i)}) \\odot z^{(l,i)} + g_a^{(l,i)} \\cdot a^{(l,i)},$ (2)\nwhere 1 $\\in \\mathbb{R}^{d_l}$ is a vector of ones. The transformation is designed so that when both gates are closed ($g_a^{(l,i)} = g_m^{(l,i)} = 0$), it reduces to the identity map, effectively disabling the intervention for that head. By using separate gates, the model can learn to apply additive and multiplicative modifications independently.\nSince our goal is to apply activation editing to a small, adaptively chosen subset of heads, we encourage the gates to be exactly zero where intervention is unnecessary. To achieve this, we use expected-Lo regularization, a technique originally introduced by Louizos et al. (2018) for pruning neural network weights. This approach has since been successfully applied to tasks such as head pruning (Voita et al., 2019) and extracting reasoning paths in graph neural networks (Schlichtkrull et al., 2021).\nDuring training, each gate is modeled as a stochastic variable drawn from a Hard-Concrete distribution (Louizos et al., 2018),\n$g_a^{(l,i)} \\sim P(g_a^{(l,i)} | \\phi_a^{(l,i)}), g_m^{(l,i)} \\sim P(g_m^{(l,i)} | \\phi_m^{(l,i)})$ (3)\nThe Hard-Concrete distribution is a mixed discrete-continuous distribution over [0, 1], with point masses at 0 and 1 and a continuous density over (0, 1). The closed-form probability of a gate being non-zero (e.g., $1-P(g_a^{(l,i)} = 0 | \\phi_a^{(l,i)}))$, is used to define a sparsity-inducing regularizer:\n$L_c(\\phi) = \\sum_{l,i} (1 - P(g_a^{(l,i)} = 0 | \\phi_a^{(l,i)})) \n + (1 - P(g_m^{(l,i)} = 0 | \\phi_m^{(l,i)})).$ (5)\nThe overall training objective balances task-specific performance with sparsity:\n$L(m, a, \\phi) = L_{xent}(m, a) + \\lambda L_c(\\phi),$ (6)\nwhere $L_{xent}$ is the standard cross-entropy loss, and $\\lambda$ controls the trade-off between performance and sparsity. Optimization is performed over all intervention parameters: $\\phi$, m and a."}, {"title": "4. Experiments", "content": "We evaluate JOLA on three diverse tasks: commonsense reasoning, natural language understanding, and natural language generation. Additional details regarding the datasets can be found in Appendix B."}, {"title": "4.1. Datasets and Tasks", "content": "Commonsense Reasoning. For commonsense reasoning, we utilize a widely adopted benchmark (Hu et al., 2023; Wu et al., 2024b) containing 8 datasets: ARC-c and ARC-e (Clark et al., 2018), BoolQ (Clark et al., 2019), HellaSwag (Zellers et al., 2019), OBQA (Mihaylov et al., 2018), PIQA (Bisk et al., 2020), SIQA (Sap et al., 2019), and WinoGrande (Sakaguchi et al., 2021). These datasets consist of multiple-choice questions, where the model must directly generate the correct option without providing explanations.\nNatural Language Understanding. We evaluate on the MMLU-Pro benchmark (Wang et al., 2024b), covering 14 domains: Biology, Business, Chemistry, Computer Science, Economics, Engineering, Health, History, Law, Math, Philosophy, Physics, Psychology, and Others. Each task requires selecting the correct answer from ten options, testing the model's broad knowledge and reasoning capabilities.\nNatural Language Generation. For generation tasks, we select 4 datasets from GEM benchmark (Gehrmann et al., 2022), including CommonGen (Lin et al., 2020) for concept-to-sentence generation, E2E (Novikova et al., 2017) and WebNLG (Gardent et al., 2017) for data-to-text generation, and XSum (Narayan et al., 2018) for abstractive summarization of long documents. This selection ensures a diverse evaluation of generation tasks, including coherence, informativeness, and abstraction."}, {"title": "4.2. Baselines", "content": "We compare JOLA against a range of state-of-the-art baselines: (1) Zero-Shot: Direct evaluation of pre-trained large language models (LLMs) without fine-tuning, including LLaMA-3 (Dubey et al., 2024) and Qwen-2.5 (Yang et al.,"}, {"title": "4.3. Implementation", "content": "We conduct experiments using the Llama-3.1-8B-Instruct (8B) and Qwen2.5-7B-Instruct (7B) models as the primary base models. Both are publicly available via the Huggingface repository. To study the impact of model size, we also experiment with smaller (Llama-3.2-1B-Instruct, Llama-3.2-3B-Instruct) and larger (Llama-3.1-70B-Instruct) model variants. For all datasets, we sample 200 examples to simulate low-resource scenarios, with further analysis of data size effects provided in Section 6. The prompt templates used in our method are also included in the Appendix C. In all baseline experiments, we observe that the choice of hyperparameters significantly affected performance across different tasks. To address this, we conduct a hyperparameter search for each method, selecting five hyperparameters and averaging the results. The final outcomes are presented in Table 1 and Figure 4. More details on the training setup, computational resources, and hyperparameter selection process are provided in Appendix D."}, {"title": "4.4. Evaluation Metrics", "content": "We employ exact match accuracy as the evaluation metric for commonsense reasoning and natural language understanding tasks. For natural language generation, we use BLEU (Papineni et al., 2002), ROUGE-L (Lin, 2004) and BERTScore (Zhang et al., 2019) scores as implemented in the GEM benchmark (Gehrmann et al., 2022)."}, {"title": "5. Results", "content": "This section evaluates the performance of our proposed method, JOLA, in comparison with various baselines. Table 1 presents the average performance for all methods across the three tasks, while Figure 4 illustrates the results for individual subtasks. More detailed numerical results can be found in Appendix E.\nPerformance of Activation-Based Baselines. Activation editing baselines exhibit varying levels of success across tasks, but their sensitivity to hyperparameter selection and layer intervention limits their consistency. For example, BitFit (Ben Zaken et al., 2022) demonstrates notable sensitivity to the placement of bias terms within the model. Adjusting bias terms in dropout layers or attention mechanisms results in performance fluctuations, particularly in low-data scenarios. Similarly, RED (Wu et al., 2024a) depends on the specific positions where scaling and bias vectors are introduced, leading to inconsistent results. REPE (Zou et al., 2023) is highly sensitive to the quality of activation representations across tasks, making it challenging to generalize its performance. ReFT (Wu et al., 2024b) achieves moderate success by intervening on selected layers but faces challenges in determining the optimal number and choice of layers. LoFIT (Yin et al., 2024), while effective in leveraging task-relevant attention heads, struggles to maintain consistency across tasks.\nPerformance of LoRA. LoRA achieves noticeable improvements over zero-shot baselines and, somewhat surprisingly, outperforms previous activation editing methods across all tasks when its rank hyperparameter is appropriately tuned. In tasks such as natural language generation, LoRA achieves higher BLEU and ROUGE-L scores, highlighting its ability to generate coherent outputs.\nPerformance of JOLA Our proposed method, JOLA, consistently outperforms all baselines across the three tasks by a significant margin. This can be attributed to JOLA's dynamic gated attention mechanism, which allows for adaptive activation of attention heads. Unlike LoFIT (Yin et al., 2024), which requires manual selection of attention heads, JOLA's mechanism enables less relevant heads to gradually \"die off\" during training, improving robustness and adaptability. In commonsense reasoning, JOLA achieves an average improvement of 3.97% over the best-performing baseline (LoRA) in LLaMA-3, as shown in Table 1. For natural language understanding, JOLA demonstrates consistent performance across diverse domains in the MMLU-Pro benchmark (Wang et al., 2024b) across all 14 subtasks as illustrated in Figure 4. In natural language generation tasks, JOLA achieves higher BLEU, ROUGE-L and BERTScore scores compared to activation-based baselines and LoRA."}, {"title": "6. Analysis", "content": "In this section, we present a detailed analysis of JOLA through ablation studies (Section 6.1, Section 6.2, and Section 6.3), an exploration of gate status during training (Section 6.4), and evaluations across varying data and model sizes (Section 6.5). Unless otherwise specified, the analyses in this section are conducted on selected tasks, including SIQA, WinoGrande, Law, Physics, E2E NLG, and"}, {"title": "6.1. Ablation 1: Gate Mechanism", "content": "Dynamic gating attention head selection is central to the performance of JOLA, as detailed in Section 3.2. To evaluate its necessity, we compare models with and without the gating mechanism. As illustrated in Figure 5, the gating mechanism provides a significant improvement to task performance. We speculate that this improvement arises because certain attention heads can be modified more effectively to achieve the desired behavior in a generalizable"}, {"title": "6.2. Ablation 2: Number of Gates", "content": "In Equation 2, we employ separate gating units for the scaling vector and the bias vector. To investigate the impact of this design, we compare configurations where each vector has its own gate with configurations where both vectors share a single gate. As illustrated in Figure 6, although the shared gating configuration achieves a performance improvement over the zero-shot baseline, it underperforms compared to the configuration with separate gates. This suggests that the choice of intervention should depend on what role the head plays in a given task. Using independent gating units enables fine-grained control over each vector's contribution, facilitating more precise task-specific adjustments and preventing over-modification of the activation outputs."}, {"title": "6.3. Ablation 3: Different Head Selection Strategies", "content": "Head selection is a critical component of JOLA's design. To evaluate whether alternative selection strategies could achieve similar outcomes, we compare JOLA with three"}, {"title": "6.4. Gate Status During Traning", "content": "To further investigate the dynamic gating mechanism, we analyzed the probability of the $g_m$ and $g_a$ gates being \"closed\" (i.e., having a value of 0) during training on the OBQA dataset (Mihaylov et al., 2018). As shown in Figure 8, both gates start in the \"open\" state (at batch=0), allowing all attention heads to contribute to the learning process. The probability of the gates being closed increases during training, which reflects the gradual closing of redundant attention heads. This behavior is consistent with our expectations, i.e., attention heads that encode task-relevant information remain active early in training, while less important heads are pruned as their representations converge."}, {"title": "6.5. Further Analysis", "content": "Different Data Size To evaluate JOLA's robustness across varying data scales, we conduct experiments using 100 to 1000 training examples sampled from the SIQA and WinoGrande datasets. Figure 9 shows that JOLA consistently outperforms baselines even with only 100 examples, demonstrating its efficacy in extreme low-resource settings. Performance improves steadily with increasing data, highlighting the method's adaptability to diverse levels of data availability. This scalability across data sizes underscores the practicality of JOLA for real-world applications where labeled data is often limited.\nDifferent Model Size To evaluate the scalability of JOLA with respect to model size, we test three variants: Llama-3.2-1B-Instruct, Llama-3.2-3B-Instruct, and Llama-3.1-70B-Instruct. As shown in Table 2, JOLA consistently delivers significant performance improvements across all model sizes. Notably, larger models benefit more substantially from JOLA's dynamic selection mechanism, as they inherently possess greater redundancy in attention heads. This finding highlights JOLA's scalability and effectiveness in optimizing large-scale models while maintaining robust performance in low-data scenarios."}, {"title": "7. Related Work", "content": "Low-Resource Fine-tuning. Recent advancements in LLMs have transformed a wide range of NLP tasks (Zhao et al., 2023). However, efficiently adapting these models to diverse applications remains challenging, especially in low-resource settings. Parameter-efficient fine-tuning (PEFT) methods (Hu et al., 2021; Dettmers et al., 2024), which update a small subset of parameters or integrate new modules (Houlsby et al., 2019), achieve performance comparable to full fine-tuning across various tasks (Wang et al., 2024a). Yet, mitigating overfitting in low-resource scenarios remains a key challenge. Activation editing techniques (Ben Zaken et al., 2022; Wu et al., 2024a;b; Yin et al., 2024) offer a lightweight approach to model adaptation, often argued to be more data-efficient than standard PEFT methods.\nPruning Neural network pruning (Cheng et al., 2024) aims to reduce model complexity and computational demands by removing less important or redundant components. Our approach builds on pruning techniques, specifically expected-Lo regularization (Louizos et al., 2018). However, rather than pruning heads, our goal is to modify a selected subset of heads while keeping the rest intact. Subnetwork pruning techniques (e.g., Frantar & Alistarh, 2023;Sun et al., 2023) seek to identify an effective subnetwork, often tailored to a specific task, domain, or language. However, their primary objective is typically to match the performance of the full model rather than to specialize it for a particular task.\nSparse Fine-tuning. Sparse finetuning (Dao et al., 2022; Thangarasa et al., 2023) is a technique for adapting LLMs to specific tasks or datasets while only updating a small subset of the model's parameters. Our approach shares similarities with sparse fine-tuning, a technique commonly used in multilingual modeling (Nooralahzadeh & Sennrich, 2023; Choenni et al., 2024), where languages are typically assumed to be encoded modularly. Sparse fine-tuning identifies specific components (e.g., heads), fine-tunes all their parameters, and discards the others. In contrast, JOLA adjusts the activations of selected components while keeping the rest intact. While the goal of sparse fine-tuning is often to match the performance of the full model using a smaller version, our aim is not only to reduce model size but to enhance performance over the full model."}, {"title": "8. Conclusion", "content": "In this paper, we introduce JOLA, a novel approach for low-resource fine-tuning that jointly learns to dynamically localize the attention heads to be targeted for intervention and the best practice for editing the activation outputs with multiplicative scaling vectors and/or additive bias vectors. We observe that attention heads are more effective than other model components in activation editing, offering a novel perspective for future research in this area. Extensive experiments and ablation studies demonstrate the robustness of our method in low-data scenarios and across varying model sizes, highlighting the importance of joint component selection and activation editing."}, {"title": "Impact Statement", "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here."}, {"title": "B. Datasets", "content": "We conduct experiments across three tasks: commonsense reasoning (Hu et al., 2023), natural language understanding (Wang et al., 2024b), and natural language generation (Gehrmann et al., 2022). Table 4 provides a brief overview of the sub-datasets or sub-tasks within the three benchmarks evaluated. The commonsense reasoning task is framed as a multiple-choice problem, where the correct answer is selected from 2 to 4 possible options. The natural language understanding task also follows a multiple-choice format, but with ten options. The natural language generation task, on the other hand, is an end-to-end text generation task, where unstructured data (such as commonsense concepts or data) is converted into coherent text. In the training phase, we simulate a low-resource scenario by using 200 examples. Section 6.5 further explores experiments with varying numbers of samples. To ensure consistency across experiments, we used the same random seed (seed= 42) for data sampling, ensuring identical training samples in all runs."}, {"title": "C. Prompt Setting", "content": "Recent studies (He et al., 2024; Lai et al., 2024) have highlighted the substantial impact of prompt design on model performance. In our experiments, we adopt the same prompt configurations as Hu et al. (2023) for the commonsense reasoning benchmark, and used the prompts from the original paper for the MMLU-Pro benchmark (Wang et al., 2024b)."}, {"title": "D. Experiment Configurations", "content": "We conduct all experiments using the HuggingFace Transformers' library and fine-tuned the models with the TRL toolkit. The AdamW optimizer (Loshchilov, 2017) was used for fine-tuning, with e = 1e - 6 and one epoch of warm-up. In addition, we employ an exponentially decaying (Li & Arora, 2019) learning rate schedule, defined by the following formula:\n$lr(t) = lr_0 \\cdot e^{-\\lambda \\cdot decay \\cdot t}$ (7)\nwhere $lr_0$ is the initial learning rate $lr_0 set to 5 \\times 10^{-4}$, $\\lambda$ is 0.1, and the decay rate is 0.01. For the gating units, we used a temperature of 0.33 in the Gumbel Softmax (Jang et al., 2017). Fine-tuning was performed in full precision for the 7B, 8B, 1B, and 3B models, while for the 70B model, we applied 4-bit quantization to enable single-precision fine-tuning."}, {"title": "D.1. Training Setup", "content": "D.2. Computational Resources\nAll experiments for the 1B, 3B, 8B, and 13B models were conducted on a single NVIDIA A100 80GB GPU server. The 70B model, described in Section 6.5, was evaluated on an NVIDIA H100 94GB GPU server. As an example, with the 8B LLaMA-3 model, JOLA converged within 2 GPU hours on most tasks in the low-resource setting, using only 200 training samples."}, {"title": "D.3. Hyperparameter Search for Baselines", "content": "As discussed in Section 1 and Section 4, the performance of baseline methods in low-resource settings is highly sensitive to hyperparameters across different tasks. However, it is impractical to conduct hyperparameter searches for each task individually, given that we evaluate 26 tasks in total, and performing a separate search for each would be time-consuming. To mitigate this, we use five different sets of hyperparameters during the baseline experiments. The average results of these five configurations are reported in Table 1 and Figure 4. The specific hyperparameters used for each method are detailed in Table 6."}, {"title": "E. Full Results Across all Tasks", "content": "Due to page limitations, we present the average performance across the 26 tasks in Table 1 and Figure 4. In this section, we provide detailed performance metrics for each individual task. Specifically, Table 7 reports the accuracy of LLaMA-3 on the commonsense reasoning task, while Table 8 presents the accuracy of Qwen-2.5 on the same task. Table 9 shows the accuracy of LLaMA-3 on the natural language understanding task, and Table 10 shows the corresponding accuracy for Qwen-2.5. Finally, Table 11 presents the BLEU, ROUGE-L, and BERTScore for LLaMA-3 on the natural language generation task, with Table 12 displaying the corresponding metrics for Qwen-2.5."}, {"title": "F. Case Study", "content": "To provide an intuitive evaluation of the advantages of our method, we select one representative case from each of the tasks: commonsense reasoning, natural language understanding, and natural language generation. The results generated by the baseline and our approach are presented below."}]}