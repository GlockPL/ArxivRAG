{"title": "DiM-Gesture: Co-Speech Gesture Generation with Adaptive Layer Normalization Mamba-2 framework", "authors": ["Fan Zhang", "Naye Ji", "Fuxing Gao", "Bozuo Zhao", "Jingmei Wu", "Yanbing Jiang", "Hui Du", "Zhenqing Ye", "Leyao Yan", "Jiayang Zhu", "WeiFan Zhong", "Xiaomeng Ma"], "abstract": "Speech-driven gesture generation is an emerging domain within virtual human creation, where current methods predominantly utilize Transformer-based architectures that necessitate extensive memory and are characterized by slow inference speeds. In response to these limitations, we propose DiM-Gestures, a novel end-to-end generative model crafted to create highly personalized 3D full-body gestures solely from raw speech audio, employing Mamba-based architectures. This model integrates a Mamba-based fuzzy feature extractor with a non-autoregressive Adaptive Layer Normalization (AdaLN) Mamba-2 diffusion architecture. The extractor, leveraging a Mamba framework and a WavLM pre-trained model, autonomously derives implicit, continuous fuzzy features, which are then unified into a singular latent feature. This feature is processed by the AdaLN Mamba-2, which implements a uniform conditional mechanism across all tokens to robustly model the interplay between the fuzzy features and the resultant gesture sequence. This innovative approach guarantees high fidelity in gesture-speech synchronization while maintaining the naturalness of the gestures. Employing a diffusion model for training and inference, our framework has undergone extensive subjective and objective evaluations on the ZEGGS and BEAT datasets. These assessments substantiate our model's enhanced performance relative to contemporary state-of-the-art methods, demonstrating competitive outcomes with the DiTs architecture (Persona-Gestors) while optimizing memory usage and accelerating inference speed.", "sections": [{"title": "I. INTRODUCTION", "content": "RECENT advancements have significantly broadened the scope of 3D virtual human technology, with its applications permeating diverse fields such as animation, gaming, human-computer interaction, and digital reception. Central to this research is the development of credible, personalized co-speech gestures. Speech-driven gesture generation, facilitated by deep learning, offers an efficient alternative to traditional motion capture systems, which typically require extensive manual input.\nHowever, a fundamental challenge within this domain is the identification and integration of numerous input conditions essential for effective gesture synthesis. This complexity stems from a variety of factors, such as acoustic nuances, semantic content, emotional expressions, personality traits, and demographic elements like gender and age. These components collectively influence the dynamics of co-speech gestures, necessitating sophisticated models that can interpret and synthesize naturalistic human movements based on verbal communication.\nPrevious approaches [1]-[7] have employed manual labels and diverse feature inputs to facilitate the synthesis of personalized gestures. However, these methods rely heavily on varied unstructured feature inputs and necessitate complex multimodal processing, posing significant barriers to the practical application and broader adoption of virtual human technologies.\nThe fuzzy inference strategy, derived from the concept of fuzzy logic [8], has proven particularly useful in fields requiring the management of uncertain or imprecise information. This approach is noted for its effectiveness in applications such as speech-emotion recognition [9] and audio classification [10]. Unlike methods that rely on explicit classification outputs, fuzzy inference provides a spectrum of feature information, transitioning from a limited explicit discrete space to an expansive implicit continuous fuzzy space. This fuzzy space aligns more closely with real-world scenarios. Psychological research underscores the importance of various factors in speech [11]\u2013[14], which, when considered as fuzzy features, are intricately linked to co-speech gestures. Zhang et al. [15] pioneered the successful integration of fuzzy inference strategies for generating personalized co-speech gestures.\nAchieving high gesture-speech synchronization while maintaining naturalness poses a significant challenge in the field of speech-driven gesture generation. Recent advancements have pivoted towards employing Transformer and Diffusion-based models, enhancing the efficiency and flexibility of gesture-generation technologies. Notable innovations in this domain include Diffuse Style Gesture [2], Diffuse Style Gesture+ [3], GestureDiffuClip [16], and LDA [4]. However, these methods sometimes struggle with maintaining an optimal correlation between gesture and speech, which can detract from the naturalness of the gestures produced.\nThe emergence of Diffusion Transformers (DiTs) in fields such as text-to-image generation [17] and text-to-video tasks"}, {"title": "II. RELATED WORK", "content": "The discussion presented provides a concise overview of Transformer-based and diffusion-based generative models within the realm of speech-driven gesture generation.\nDiffMotion [20] marks an innovative use of diffusion models in gesture synthesis, integrating an LSTM to generate diverse gestures. Alexanderson et al. [4] have refined DiffWave by replacing dilated convolutions to optimize the potential of transformer architectures. Conformers [21] employ classifier-free guidance to enhance style expression. GestureDiffuCLIP [16] utilizes transformers and AdaIN layers to integrate style guidance within the diffusion process. DiffuseStyleGesture (DSG) [2] and its extension DSG+ [3] incorporate cross-local attention and layer normalization within their transformer models. However, these methodologies often face challenges in striking an optimal balance between gesture and speech synchronization, potentially leading to gestures that seem either underrepresented or excessively matched to the speech.\nPersona-Gestor [15] introduces a fuzzy feature extractor that leverages a 1D convolution to process raw speech audio for feature extraction, combined with an Adaptive Layer Normalization (AdaLN) transformer [17] to model the intricate correlation between these features and the resulting gesture sequence. While this method achieves superior motion quality, the significant memory requirements and slower inference speeds of convolutional and transformer architectures remain a challenge.\nIn this study, we implement a fuzzy feature inference strategy to implicitly capture nuanced features within speech audio. We synthesize natural, personalized co-speech gestures exclusively based on raw speech audio, leveraging the Mamba architecture. Furthermore, we employ the AdaLN Mamba-2 architecture in place of traditional AdaLN transformers. This adaptation preserves the model's ability to capture the complex interplay between speech and gestures while ensuring the generation of high-quality actions. Notably, this approach significantly reduces memory requirements and enhances inference speed, offering a more efficient alternative for gesture synthesis in virtual human interactions."}, {"title": "III. SYSTEM OVERVIEW", "content": "DiM-Gesture, an end-to-end Mamba and diffusion-based architecture, processes raw speech audio as its sole input to synthesize personalized gestures. This model adeptly balances naturalness with synchronized alignment to the corresponding speech, ensuring the gestures it generates are both realistic and timely, enhancing the interaction experience in virtual environments."}, {"title": "A. Problem Formulation", "content": "In this study, we frame the problem of co-speech gesture generation as a sequence-to-sequence translation challenge, where the objective is to map a sequence of speech audio features, denoted as $a = a_{1:T} = [a_1,...,a_t,...,a_T] \\in \\mathbb{R}^T$, to a corresponding sequence of full-body gesture features, represented as $g^o = g^o_{1:T} = [g^o_1, \\dots, g^o_t, \\dots, g^o_T] \\in \\mathbb{R}^{T\\times(D+6)}$.\nHere, each $g^o_t \\in \\mathbb{R}^{(D+6)}$ comprises 3D joint angles alongside root positional and rotational velocities at frame t, with D indicating the number of joint channels.\nWe define the probability density function (PDF), $p_{\\theta}(\\cdot)$, to approximate the actual gesture data distribution $p(\\cdot)$, facilitating efficient sampling. Our aim is to generate a non-autoregressive whole pose sequence $(g^o)$ from its conditional probability distribution given the audio sequence (a) as a covariate:\n$g^o \\sim p_{\\theta}(g^o | a) \\approx p(\\cdot) := p(g^o | a)$\nThis formulation underscores the use of a denoising diffusion model trained to approximate the true conditional distribution of gestures given speech, illustrating the direct relationship between the audio inputs and gesture outputs, thus setting the stage for our model to learn this complex mapping effectively."}, {"title": "B. Model Architecture", "content": "The architecture of DiM-Gesture, illustrated in Figure 1, is composed of four main components designed to streamline the synthesis of personalized gestures from speech audio. These components include:\n1) Mamba-based Style Fuzzy Feature Extractor, 2) AdaLN Mamba-2 Architecture, 3) Gesture Encoder and Decoder, and 4) Diffusion Network.\nTogether, these components form a cohesive system that not only captures the complexity of human gestures relative to speech but also enhances the quality and personalization of the generated gestures.\n1) Mamba-based Fuzzy Feature Extractor: This module employs a fuzzy inference strategy, which means it does not rely on explicit classification outputs. Instead, it provides implicit, continuous, and fuzzy feature information, automatically learning and inferring the global style and specific details directly from raw speech audio. Illustrated in Figure 1b and Figure 2, this module is a dual-component extractor comprising both global and local extractors. The local extractor utilizes the WavLM large-scale pre-trained model [22] to process the audio sequence into discrete tokens. We selected WavLM for its proficiency in capturing the complex attributes of speech audio, which allows it to effectively represent universal audio latent features, denoted as $z_a$. This sophisticated approach ensures a nuanced translation of audio features into gesture-relevant data, enhancing the fidelity and personalization of the generated gestures.\nWe implement a global style extractor within our system, utilizing the Mamba module [18] to process $z_a$, the universal audio latent representations. This global extractor is adept at automatically capturing and embedding global fuzzy style information from $z_a$, producing an output token $z_{last} \\in \\mathbb{R}^{1\\times D'}$, which is the last output of the Selective State Space Model(SSM). This token is subsequently broadcasted and amalgamated with $z_a \\in \\mathbb{R}^{T'\\times D'}$ to forge a unified latent representation $z_l \\in \\mathbb{R}^{T\\times D''}$. By merging both local and global insights, our architecture enhances the representational fidelity of the entire sequence, tailored for co-speech gesture generation. This unified latent representation is then channeled to the downsampling module for further refinement.\nThe downsampling module, crucial for aligning each latent representation with its corresponding sequence of encoded gestures, is seamlessly integrated into the condition extractor. Initially, we explored linear alignment strategies, similar to those employed in DSG [2] and DSG+ [3]. However, these methods often led to issues such as foot-skating. To address this, we have implemented a Conv1D layer with a kernel size of 201 within our architecture. This configuration enables the mapping of every 201-length target token output from the WavLM to a single gesture frame, thereby enhancing the precision of gesture synchronization. The output of this module, $c_1$, serves as a unified latent representation that encapsulates both encoded audio features and the diffusion time step n, ensuring a coherent and accurate gesture generation process.\n2) AdaLN Mamba-2: The AdaLN's fundamental purpose is to incorporate a conditional mechanism that uniformly applies a specific function across all tokens, thereby significantly improving the model's capacity for representing both conditional and output features with enhanced efficiency. It offers a more sophisticated and nuanced approach to modeling, enabling the system to capture and articulate the complex dynamics between various input conditions and their corresponding outputs. Consequently, this leads to an improvement in the model's predictive accuracy and ability to generate outputs more aligned with the given conditions.\nDiffusion Transformers (DiTs) exemplify a sophisticated advancement in diffusion model architectures, incorporating an AdaLN-infused transformer framework primarily for text-to-image synthesis. This methodological enhancement has substantially lowered Fr\u00e9chet Inception Distance (FID) scores, demonstrating improved image quality. The utility of DiTs has recently expanded to include text-conditional video generation, illustrating their versatility. Furthermore, DiTs have shown potential in co-speech gesture generation [15], marking a significant step in applying these models to sequence-based tasks. However, the inherent quadratic space complexity associated with Transformers results in substantial memory consumption and slower inference speeds, presenting practical challenges in real-time applications.\nIn contrast to traditional Diffusion Transformers, our approach integrates the Mamba-2 architecture [19] as a replacement for the conventional transformer module. This strategic adaptation leverages the minimal memory footprint and enhanced processing efficiency of Mamba-2, significantly accelerating inference speeds without sacrificing output quality. This novel substitution is pivotal in addressing the challenges associated with real-time, speech-driven gesture synthesis, ensuring efficient and high-quality performance.\nThe module involves regressing the dimension-wise scale and shift parameters (\u03b3 and \u03b2), derived from the fuzzy feature extractor output $C_{1:T}$, rather than directly learning these parameters, as depicted in Figure 1c. In each AdaLN Mamba-2 stack, a latent feature $z_{1:T,m}$ is generated, combining condition information and gesture using AdaLN and the Mamba-2 architecture. The index m ranges from 1 to M, where M represents the total number of AdaLN Mamba-2 stacks. Furthermore, as illustrated in Figure 1b, the final layer utilizes the same fuzzy features, supplemented by a scale and shift operation to fine-tune the gesture synthesis.\nThis method enables the creation of detailed gesture sequences directly from speech audio, obviating the need for discrete style labels or additional inputs. It significantly enhances the model's ability to generate personalized and contextually aligned gestures, providing a refined and context-sensitive gesture synthesis capability. Moreover, compared with the AdaLN transformer, the Mamba-2 architecture reduces the memory footprint and improves inference speed, while still ensuring high generation quality.\n3) Gesture Encoder and Decoder: The architecture of the gesture encoder and decoder is designed to process the gesture sequence, as illustrated in Fig. la and Fig. 1b. The gesture encoder employs a Convolution1D layer with a kernel size of 3 to encode the initial sequence of gestures $g$ into a hidden state $h\\in \\mathbb{R}^{T\\times D''}$. Our experimental results indicate that a kernel size of 1 tends to produce animation jitter. In contrast, a kernel size of 3 effectively mitigates this issue by capturing the spatial-temporal relationships inherent in gesture sequences. The gesture decoder then reduces the feature dimension of the transformer's output D'' back to the original dimension D, which corresponds to the number of channels representing skeleton joints, resulting in the output of the predicted noise $(\\epsilon_{\\theta})$. Employing a 1D kernel of size 1 in the input sequence allows the model to effectively extract meaningful features and relationships between adjacent joint channels, enhancing the quality and coherence of the generated gestures."}, {"title": "C. Training and Inferencing with DDPM", "content": "The diffusion process in our architecture is crucial for reconstructing the conditional probability distribution between gestures and fuzzy features. This involves employing a systematic sampling approach from this reconstructed distribution, facilitating the generation of diverse gestures.\nFollowing our previous work, Persona-Gestor [15], incorporating the Denoising Diffusion Probabilistic Model (DDPM) into our approach.\nThe model operates through two principal processes: the diffusion process and the generation process. In the diffusion process during training, the model gradually transforms the original gesture data $(g^o)$ into white noise $(g_N)$ by optimizing a variational bound on the data likelihood. This transformation is characterized by progressively adding noise to the data in a controlled manner.\nDuring inference, the generation process endeavors to reverse this transformation. It recovers the original data from the noise by reversing the noising process through a Markov chain, employing Langevin sampling [23]. This technique ensures the effective and accurate reconstruction of the gesture data from its noised state. The Markov chains utilized in both the diffusion and generation processes ensure a coherent and systematic transition between stages, thereby maintaining the integrity and quality of the generated gestures. This dual-process framework allows the model to efficiently handle and synthesize complex gesture data, reflecting the dynamic nature of human movement. The Markov chains in the diffusion process and the generation process are:\n$p (g^n|g^o) = N (g^n; \\sqrt{\\bar{\\alpha}^n}g^o, (1-\\bar{\\alpha}^n) I)$ and\n$p_{\\theta} (g^{n-1}|g^n, g^o) = N (g^{n-1}; \\mu_{\\theta} (g^n, g^o), \\tilde{\\beta}^nI)$,\nwhere $\\alpha_n := 1 - \\beta_n$ and $\\bar{\\alpha}^n := \\prod_{i=1}^n \\alpha^i$. As shown by [24], $\\beta_n$ is a increasing variance schedule $\\beta_1,..., \\beta_N$ with $\\beta_n \\in (0,1)$, and $\\bar{\\beta}_n := 1-\\beta_n$.\nThe training objective centers on optimizing the parameters $\\theta$ by minimizing the Negative Log-Likelihood (NLL). This is achieved via the Mean Squared Error (MSE) loss, which quantifies the discrepancy between the true noise, represented by $\\epsilon \\sim N(0, I)$, and the predicted noise $\\epsilon_{\\theta}$:\n$\\mathbb{E}_{g^o, \\epsilon} [\\Vert \\epsilon - \\epsilon_{\\theta} (\\sqrt{\\bar{\\alpha}^n}g^o + \\sqrt{1-\\bar{\\alpha}^n}\\epsilon, a_{1:T}, n) \\Vert^2]$,\nHere $\\epsilon_{\\theta}$ is a neural network (see figure la), which uses input $g^n, a_{t-1}$ and n that to predict the $\\epsilon$, and contains the similar architecture employed in [25]. The complete training procedure is outlined in Algorithm 1.\nAfter the training phase, we employ variational inference to generate a complete sequence of new gestures that match the original data distribution $(g^o \\sim p_{\\theta}(g^o | a))$. This is facilitated by the sampling procedure detailed in Algorithm 2. During this process, we sample the entire sequence $g^o$ from the learned distributions.\nThe term $\\sigma_{\\theta}$ represents the standard deviation of the conditional distribution $p_{\\theta}(g^{n-1} | g^n)$, which is crucial for accurately capturing the variability and intricacies of the transition between different diffusion stages. For our model, we set $\\sigma_{\\theta} := \\beta_n$, where $\\beta_n$ is a predetermined scaling factor that adjusts the noise level during each diffusion step, allowing for a controlled smoothing and detail enhancement in the generation process."}, {"title": "IV. EXPERIMENTS", "content": "To validate our approach, we utilized two co-speech gesture datasets: ZEGGS [7] and BEAT [26]. Our experiments focused on producing full 3D body gestures, including finger motions and locomotion.\n1) Datasets: The ZEGGS dataset features a comprehensive collection of emotional expressions, enabling the exploration of gesture generation across a spectrum of emotions. Conversely, the BEAT dataset is distinguished by its focus on personalized movements, capturing the unique gesture styles of various individuals. This diversity in datasets allows for robust testing and enhancement of models aimed at generating nuanced and contextually appropriate co-speech gestures.\n2) Speech Audio Data Process: Due to the WavLM large model being pre-trained on speech audio sampled at 16 kHz, we uniformly resampled all audio from the ZEGGS and BEAT datasets down from 44.1 kHz to match this frequency, ensuring compatibility and optimal performance.\n3) Gesture Data Process: We concentrate exclusively on full-body gestures, employing the data processing techniques detailed by Alexanderson et al. [27]. This includes capturing translational and rotational velocities to accurately delineate the root's trajectory and orientation. The datasets are uniformly downsampled to a frame rate of 20 fps. To ensure precise and continuous representation of joint angles, we utilize the exponential map technique [28]. All data are segmented into 20-second clips for training and validation purposes. For user evaluation, the generated gesture sequences are divided into 10-second clips to enhance evaluation efficiency."}, {"title": "B. Model Settings", "content": "Our experiments utilize three AdaLN Mamba-2 blocks, with each Mamba-2 configured with a 256 SSM state expansion factor, a local convolution width of 4, and a block expansion factor of 2. This encoding process transforms each frame of the gesture sequence into hidden states $h \\in \\mathbb{R}^{1280}$. We employ the pre-trained WavLM Large model\u00b2 for audio processing.\nThe diffusion model uses a quaternary variance schedule, starting from $\\beta_1 = 1 \\times 10^{-4}$ to $\\beta_N = 8 \\times 10^{-2}$ with a linear beat schedule, and a total of N = 1000 diffusion steps. The training batch size is set to 32 per GPU.\nOur model was tested on an Intel i9 processor with a 4090 GPU, in contrast to the A100 GPU used by Persona-Gestor. The training times were approximately 6 hours for ZEGGS and 22 hours for BEAT."}, {"title": "C. Visualization Results", "content": "Our system excels in generating personalized gestures that are contextually aligned with speech. By leveraging the Mamba-based fuzzy inference strategy, it autonomously derives fuzzy features directly from speech audio. The results demonstrate that our AdaLN Mamba-2 framework produces gesture actions comparable to those generated by Persona-Gestor, which uses convolution global fuzzy feature extractor and AdaLN transformers.\nFigure 3 illustrates the visual outcomes of gestures aligned with the emotional valence conveyed by the audio. For example, the system generates gestures of joy in response to happy audio cues (see Figure 3a) and gestures of sadness for sorrowful audio (as shown in Figure 3b). Additionally, the system can infer age-related characteristics or other nuanced states from the speech audio (as depicted in 3e and 3f). These visual comparisons demonstrate that DiM-Gesture can produce co-speech gestures of comparable quality to those generated by Persona-Gestor."}, {"title": "D. Subjective and Objective Evaluation", "content": "In line with established practices in gesture generation research, we conducted a series of subjective and objective evaluations to assess the co-speech gestures generated by our proposed DiM-Gesture (DiM) model.\nWe adopted slightly varied baselines for different datasets. For the ZEGGS dataset, we employed LDA [4], and DiffuseStyleGesture (DSG) [2]. For the BEAT dataset, we utilized the same baseline models as in ZEGGS but replaced DSG with DSG+ [3] and introduced GestureDiffuCLIP (GDC) [16] as an additional baseline model. All baseline models employed are based on transformer and diffusion architectures.\nIn our experiments with the ZEGGS and BEAT datasets, we extended the original LDA, DSG, and DSG+ models to encompass all styles within these datasets. Efforts to adapt LDA to include finger motions encountered significant challenges, resulting in unsatisfactory gesture-generation outcomes. Consequently, we utilized gestures generated by the LDA model, excluding finger movements, for our analysis.\n1) Subjective Evaluation: For comprehensive subjective evaluations, we employ three metrics: human-likeness, appropriateness, and style-appropriateness. Human-likeness assesses the naturalness and resemblance of gestures to authentic human movements, independent of speech. Appropriateness evaluates the temporal alignment of gestures with speech rhythm, intonation, and semantics, ensuring a natural flow. Style-appropriateness measures the similarity between generated gestures and their original human counterparts.\nWe conducted a user study employing pairwise comparisons, as recommended by [29]. In each trial, participants were presented with two 10-second video clips side by side, generated by different models, including the Ground Truth (GT), for direct comparison. Participants were instructed to select the clip they preferred based on specified evaluation criteria. Preferences were quantified on a scale from 0 to 2, with the non-selected clip in each pair receiving an inverse score (e.g., a -2 score for the non-chosen clip if the chosen one received a score of 2). A score of zero indicated a neutral preference.\nGiven the extensive range of styles in the ZEGGS (19) and BEAT (30) datasets, evaluating each style individually was deemed impractical. Therefore, we employed a random selection method, assigning each participant a subset of five styles from ZEGGS and six characters from BEAT for evaluation. Importantly, none of the selected audio clips were included in the training or validation sets, ensuring the integrity of the assessments.\nThirty volunteer participants\u201414 males and 16 females aged between 18 and 35\u2014were recruited for this study. All participants demonstrated a high level of English proficiency, essential for accurately interpreting and responding to the tasks involved.\nOne-way ANOVA and posthoc Tukey, multiple comparison tests, were conducted to determine if there were statistically significant differences among the models' scores across the three evaluation aspects. The results are presented in Table I, offering detailed insights into the performance variances observed among different models regarding human-likeness, appropriateness, and style-appropriateness.\nThe results from the ZEGGS and BEAT datasets show that the Ground Truth (GT) achieves the highest scores (0.93\u00b11.22 for ZEGGS and 0.65\u00b11.16 for BEAT), exhibiting statistically significant differences (p < 0.001) in human-likeness evaluations compared to model-generated gestures. The GT features a diverse yet limited selection of gestures, each characterized by distinct traits that enhance the realism of movements. However, these specific gestures fall into the long-tail distribution of the datasets, presenting substantial challenges to the learning capabilities of the models. Furthermore, the uniqueness of these gestures significantly influences the appropriateness and style-appropriateness scores.\nThe evaluations of the ZEGGS and BEAT datasets showed no statistically significant differences (p > 0.05) between our DiM model and Persona-Gestor (PG) across all three metrics. However, PG scored slightly higher in Human-likeness on the ZEGGS dataset, whereas DiM attained marginally higher scores in Appropriateness and Style-appropriateness. In contrast, on the BEAT dataset, DiM scored slightly higher across all subjective evaluation metrics. These outcomes demonstrate that DiM is capable of generating co-speech gestures of comparable quality to those produced by PG, affirming the robustness and effectiveness of our approach in synthesizing natural and well-aligned gestures across diverse datasets.\n2) Objective Evaluation: We employ three objective evaluation metrics to assess the quality and synchronization of generated gestures: Fr\u00e9chet Gesture Distance (FGD) in both feature and raw data spaces [30], and BeatAlign [31]. Inspired by the Fr\u00e9chet Inception Distance (FID) [32], FGD evaluates the quality of generated gestures and has shown moderate correlation with human-likeness ratings, surpassing other objective metrics [33]. BeatAlign, on the other hand, assesses gesture-audio synchrony by calculating the Chamfer Distance between audio beats and gesture beats, thus providing insights into the temporal alignment of gestures with speech rhythms.\nTable I presents our results, underscoring the state-of-the-art performance of our method in objective evaluations using the Fr\u00e9chet Gesture Distance (FGD) metrics. Our model achieves superior performance (28.16 for ZEGGS) compared to other architectures, generating gestures that closely align with the Ground Truth (GT). It also matches the BeatAlign scores (0.67 for ZEGGS) of other models, except for GestureDiffuClip (GDC) which scores slightly higher (0.69 for BEAT), highlighting its efficacy in producing co-speech gestures that synchronize accurately with speech rhythms. Despite GDC's high BeatAlign score, corroborating user feedback indicates its overemphasis on prosodic cues results in frequent high-frequency gestures that, while technically accurate, reduce the naturalness of the gestures. On the BEAT dataset, our model's FGD scores (276.32) are marginally lower than Persona-Gestor (276.25), reflecting competitive performance in gesture quality."}, {"title": "E. Ablation Studies", "content": "Ablation studies were conducted to assess the influence of crucial components within our model, particularly focusing on the Mamba-based fuzzy feature extractor and the AdaLN Mamba-2 architecture. These studies aimed to elucidate how each component contributes to the overall performance and effectiveness of the gesture synthesis process.\n1) Ablation of Mamba-based Style Fuzzy Feature Extractor: For the Mamba-based style fuzzy feature extractor, we conducted an investigation into the implications of replacing the Mamba architecture with a convolutional style extractor (similar to the approach in PG), which we termed DiM-ConvSE. This experiment aimed to assess the differential impacts of the Mamba and convolutional methodologies on the effectiveness and fidelity of the synthesized gestures.\nThe results, as detailed in Table I, reveal no statistically significant differences (p > 0.05) in the style-appropriateness metrics between DiM and DiM-ConvSE for both the ZEGGS and BEAT dataset experiments. These findings underscore the equivalence of the Mamba-based style fuzzy feature extractor in performance with the convolution-based extractor, highlighting its efficacy in gesture synthesis. Additionally, as indicated in Table II, the Mamba architecture demonstrates less parameter count and lower memory consumption than DiM-ConvSE.\n2) Ablation of AdaLN Mamba-2: In our ablation study, we explored the impact of different versions of the Mamba architecture by replacing the AdaLN Mamba-2 with Mamba-1 [18]. This experiment was designed to assess the efficacy of the AdaLN Mamba-2 in handling the intricate dynamics of gesture generation compared to its predecessor, Mamba-1. The main focus was to determine whether the advancements in Mamba-2, particularly the integration of adaptive layer normalization (AdaLN), provide substantial improvements in gesture synthesis quality, efficiency, and model responsiveness. The results of this comparison are detailed in Table I and Table II, illustrating differences in gesture quality, computational efficiency, and performance metrics between the two Mamba versions.\nIn our ablation studies on the AdaLN Mamba-2, substituting the Mamba-2 module with Mamba-1 led to a significant decline in performance across all metrics. This deterioration can be attributed to the earlier architecture's inability to precisely synchronize speech rhythm and capture stylistic nuances like its successor. These findings highlight the integrated enhancements in Mamba-2, particularly its advanced capability to effectively handle the intricate dynamics of speech-driven gesture generation. However, it is worth noting that DiM-AdaLN-Mamba-1 exhibits a faster inference time."}, {"title": "V. DISSCUSTION AND CONCLUSION", "content": "In this study, we present DiM-Gesture, an innovative network architecture designed to generate personality-specific gestures directly from raw speech audio using a Mamba-based architecture. DiM-Gesture incorporates a Mamba-based style fuzzy feature extractor and an AdaLN Mamba-2 diffusion architecture, facilitating the seamless synthesis of nuanced, personality-driven gestures. Our approach achieves a quality of action generation comparable to that of the AdaLN Transformer architecture (Persona-Gestor) while requiring significantly less memory and substantially enhancing generation speed than Persona-Gestor. This demonstrates DiM-Gesture's capability to provide efficient and high-quality gesture synthesis.\nThe Mamba-based fuzzy feature extractor employs a fuzzy feature inference strategy within its dual-component module to autonomously infer both fuzzy stylistic features and specific audio details. These elements are merged into a unified latent representation, enabling the generation of speaker-aware personalized 3D full-body gestures. This approach integrates a pivotal innovation in synthesizing personality-driven gestures by leveraging automatically inferred fuzzy features, thereby eliminating the need for explicit style labels or additional features. Such advancements facilitate an end-to-end gesture generation process that authentically reflects the speaker's unique characteristics directly from raw speech audio. The integration of fuzzy feature inference streamlines the creation process, enhancing both generalization capabilities and user accessibility.\nThe AdaLN Mamba-based mechanism, a conditional architecture, uniformly applies a specific function across all sequence tokens, significantly enhancing the model's ability to capture and represent both conditional dependencies and output characteristics efficiently. Like AdaLN transformers, AdaLN Mamba-2 enhances the understanding and processing of the complex interactions between continuous fuzzy features as conditional inputs and resultant gesture synthesis. This leads to improved model performance and output fidelity. Ultimately, DiM-Gesture employs a diffusion mechanism to produce a diverse spectrum of gesture outputs, showcasing its capability to handle varied and nuanced gesture synthesis effectively.\nOur study highlights key areas for enhancement: There remains a noticeable disparity between the current batch generation form and the desired real-time generation capability. Bridging this gap is crucial for applications requiring immediate gesture synthesis, such as live interactions or performances."}]}