{"title": "AlignMamba: Enhancing Multimodal Mamba with Local and Global Cross-modal Alignment", "authors": ["Yan Li", "Yifei Xing", "Xiangyuan Lan", "Xin Li", "Haifeng Chen", "Dongmei Jiang"], "abstract": "Cross-modal alignment is crucial for multimodal representation fusion due to the inherent heterogeneity between modalities. While Transformer-based methods have shown promising results in modeling inter-modal relationships, their quadratic computational complexity limits their applicability to long-sequence or large-scale data. Although recent Mamba-based approaches achieve linear complexity, their sequential scanning mechanism poses fundamental challenges in comprehensively modeling cross-modal relationships. To address this limitation, we propose Align-Mamba, an efficient and effective method for multimodal fusion. Specifically, grounded in Optimal Transport, we introduce a local cross-modal alignment module that explicitly learns token-level correspondences between different modalities. Moreover, we propose a global cross-modal alignment loss based on Maximum Mean Discrepancy to implicitly enforce the consistency between different modal distributions. Finally, the unimodal representations after local and global alignment are passed to the Mamba backbone for further cross-modal interaction and multimodal fusion. Extensive experiments on complete and incomplete multimodal fusion tasks demonstrate the effectiveness and efficiency of the proposed method. For instance, on the CMU-MOSI dataset, AlignMamba improves classification accuracy by 0.9%, reduces GPU memory usage by 20.3%, and decreases inference time by 83.3%.", "sections": [{"title": "1. Introduction", "content": "In recent years, multimodal representation fusion has emerged as a critical technology for integrating and understanding information across different modalities (e.g., audio, video, language). This capability is fundamental to a wide range of applications such as visual-language understanding [41] and audio-visual analysis [13, 40]. However, due to the inherent heterogeneity between modalities - each with its distinct statistical properties and feature distributions - achieving effective cross-modal alignment and fusion remains a significant challenge.\nTraditional approaches to this challenge have primarily relied on Transformer-based [31] architectures, which can be broadly categorized into two paradigms. Single-stream methods (e.g., VisualBERT [15], ViLT [11], LLaVA [21]) concatenate features from different modalities into a unified sequence and process them through a shared Transformer layer. In contrast, multi-stream approaches (e.g., LXMERT [29], ViLBERT [23], MulT [30], CMA [44]) employ separate encoders for each modality with cross-modal Transformers to facilitate information exchange. While these methods have demonstrated promising results in capturing dynamic cross-modal interactions, they suffer from a fundamental limitation: the quadratic computational complexity of attention mechanisms makes them inefficient for processing long-sequence or large-scale data common in real-world multimodal applications.\nRecent advances in sequence modeling have introduced the Mamba [3] architecture, based on State Space Models (SSMs) [4, 5], which achieves linear computational complexity while maintaining strong modeling capabilities. By incorporating selection mechanisms and hardware-aware parallel algorithms into SSMs, Mamba effectively captures long-range dependencies without the computational burden of attention mechanisms. This breakthrough has sparked considerable interest in adapting Mamba for multimodal fusion tasks, with approaches ranging from direct feature concatenation (e.g., VL-Mamba [26], Cobra [42], RoboMamba [22]) to multi-stream architectures (e.g., Pan-Mamba [8], Fusion-Mamba [2], MambaDFuse [17]). However, our analysis reveals a critical limitation. As shown in Fig. 1, Mamba's sequential scanning mechanism, while computationally efficient, struggles to capture comprehensive cross-modal relationships, particularly with unscanned tokens. This inherent limitation leads to suboptimal alignment between modalities and consequently affects the quality of learned multimodal fusion representations."}, {"title": "2. Related work", "content": null}, {"title": "2.1. Transformer-based Multimodal Fusion", "content": "Transformer [31], with its powerful modeling capabilities, has become the cornerstone architecture in modern neural networks. Existing multimodal fusion methods mainly rely on Transformers to model relationships between different modalities and learn multimodal fusion representations. These approaches can be categorized into two main types: multi-stream and single-stream methods.\nMulti-stream methods employ cross-modal Transformers to model interactions between any two modalities. For vision-language pre-training tasks, models like ViL-BERT [23] and LXMERT [29] utilize two co-attention Transformer layers to model bidirectional relationships between visual and textual modalities. For audio-visual-textual trimodal fusion tasks, MulT [30] leverages cross-modal Transformers to model pairwise modal interactions, and then concatenate all bimodal fusion representations to obtain trimodal fusion representations. Similarly, CMA [30], based on cross-modal attention mechanisms, was proposed to fuse features from three modalities. More recently, BLIP-2 [14] introduced Q-Former, a lightweight querying Transformer architecture, to align vision-language modalities and learn multimodal fusion representations.\nSingle-stream methods adopt a more straightforward strategy by concatenating features from different modalities and feeding them into a Transformer encoder for cross-modal interaction and multimodal fusion. For instance, in vision-language pre-training tasks, VisualBERT [15] extracts features from key regions using object detectors and concatenates these region feature sequences with text token embeddings before feeding them into a Transformer. In contrast, ViLT [11] replaces region feature sequences with image patch embedding sequences, discarding the object detection backbone and improving efficiency. Recent multimodal pre-training models, such as LLaVA [21], have adopted similar approaches to model cross-modal correspondences and learn multimodal fusion representations for downstream tasks.\nExisting methods achieve cross-modal interaction and fusion through cross-attention or self-attention mechanisms, learning comprehensive and effective multimodal fusion representations. However, the quadratic time complexity of Transformers limits their efficiency when processing large-scale or long-sequence data. This limitation necessitates the development of novel multimodal fusion methods that balance effectiveness and efficiency."}, {"title": "2.2. Mamba-based Multimodal Fusion", "content": "As a novel architectural paradigm, Mamba [3] incorporates selection mechanisms and hardware-aware parallel algorithms into SSMs [4, 5], achieving efficient and effective sequence modeling in the language domain. Inspired by its success, recent studies have explored adapting Mamba for multimodal fusion tasks. For instance, Pan-mamba [8] and Fusion-mamba [2] incorporate features from other modalities as inputs to unimodal Mamba to enable cross-modal interaction and fusion. Similarly, MambaDFuse [17] and MTMamba [19] utilize multimodal representations as inputs to unimodal Mamba for cross-modal interaction and fusion. In contrast, some approaches adopt a simpler strategy: VL-Mamba [26] and Cobra [42], for example, concatenate visual and textual representation sequences before feeding them into Mamba for sequence modeling and multimodal fusion.\nWhile these Mamba-based approaches demonstrate significant computational advantages compared to Transformer-based multimodal fusion methods, they face inherent limitations due to Mamba's sequential scanning mechanism. This mechanism makes it challenging to effectively learn cross-modal correspondences, particularly with unscanned tokens. The resulting loss in cross-modal alignment information may constrain the effectiveness of learned multimodal fusion representations. Therefore, how to effectively leverage cross-modal relationships within the Mamba framework to learn more comprehensive multimodal fusion representations remains an open research challenge."}, {"title": "3. Method", "content": null}, {"title": "3.1. Overview", "content": "Fig. 2 presents the framework of our proposed Align-Mamba. Using audio-visual-language trimodal data as a case study, the framework first processes raw signals from each modality through modality-specific encoders to generate corresponding unimodal embedding sequences Xa, Xv, and Xl. The framework then employs two complementary alignment mechanisms: an OT-based local alignment module that captures token-level correspondences, and an MMD-based global alignment loss that ensures distribution-level consistency. These mechanisms yield aligned embedding sequences X\u0303a and X\u0303v (illustrated here by aligning audio and visual modalities to the language modality as the anchor). The aligned unimodal embeddings, which now incorporate cross-modal correspondence information, are subsequently processed by the Mamba backbone for multimodal fusion. The following sections provide a detailed description of each component."}, {"title": "3.2. OT-based Local Cross-modal Alignment", "content": "Optimal Transport provides a principled framework for comparing and aligning probability distributions by finding the optimal way to transform one distribution into another while minimizing the transportation cost [32]. In our multimodal alignment context, OT offers a natural way to establish token-level correspondences between different modalities by treating feature sequences as discrete distributions. Given the unimodal feature sequences Xa \u2208 \\mathbb{R}^{T_a \\times d}, Xv \u2208 \\mathbb{R}^{T_v \\times d}, and Xl \u2208 \\mathbb{R}^{T_l \\times d} from audio, video, and language modalities respectively, where Ta, Tv, and Tl denote the sequence lengths of different modalities and d is the feature dimension, we aim to learn the transport matrix M that capture fine-grained correspondences between different modalities. Take video-to-language alignment as an example, the classical optimal transport problem can be formulated as follows:\n$$\\min_{M_{v2l}} \\sum_{i=1}^{T_v} \\sum_{j=1}^{T_l} M_{v2l}(i, j)C_{v2l}(i, j).$$ (1)\nThe optimization is constrained by:\n$$\\begin{cases}\n\\sum_{j=1}^{T_l} M_{v2l}(i, j) = \\frac{1}{T_v}, & \\forall i \\in [1, T_v] \\\\\n\\sum_{i=1}^{T_v} M_{v2l}(i, j) = \\frac{1}{T_l}, & \\forall j \\in [1, T_l] \\\\\nM_{v2l}(i, j) \\ge 0, & \\forall i, j\n\\end{cases}$$(2)\nwhere Cv2l \u2208 \\mathbb{R}^{T_v \\times T_l} is the cost matrix. Given that the cosine distance emphasizes angular relationships between feature vectors while providing numerical stability through its bounded range, we use cosine distance as the cost matrix:\n$$C_{v2l}(i, j) = 1 - \\frac{X_i \\cdot X_j}{||X_i||_2 ||X_j||_2}$$(3)\nHowever, solving this OT problem is extremely computationally expensive. Following [12], we adopt a relaxed version by removing the incoming sum constraint:\n$$\\begin{cases}\n\\sum_{j=1}^{T_l} M_{v2l}(i, j) = \\frac{1}{T_v}, & \\forall i \\in [1, T_v] \\\\\nM_{v2l}(i, j) \\ge 0, & \\forall i, j\n\\end{cases}$$(4)\nThis relaxed formulation allows each textual feature to be matched with multiple video features without constraining the total incoming flow, significantly reducing the computational complexity while maintaining the ability to capture meaningful cross-modal correspondences. The corresponding solution is defined as:\n$$M_{v2l}(i, j) = \\begin{cases}1, & \\text{arg} \\min_j C_{v2l}(i, j) \\\\\n0, & j \\ne \\text{arg} \\min_j C_{v2l}(i, j). \\end{cases}$$(5)\nSimilarly, we compute the transport matrix Ma2l for audio-to-language alignment. Finally, the aligned video and audio"}, {"title": "3.3. MMD-based Global Cross-modal Alignment", "content": "To ensure distribution-level consistency across modalities, we employ Maximum Mean Discrepancy as the global alignment metric. MMD measures the statistical discrepancy between different modalities in a high-dimensional Reproducing Kernel Hilbert Space (RKHS) by comparing all orders of their statistics. For two feature sequences X and Y, the squared MMD distance is defined as:\n$$\\text{MMD}^2(X, Y) = ||\\frac{1}{T} \\sum_{i=1}^T \\phi(x_i) - \\frac{1}{T} \\sum_{j=1}^T \\phi(y_j)||_H^2,$$(7)\nwhere \u03d5(\u00b7) is a feature mapping to a RKHS H. Using the kernel trick, this can be computed as:\n$$\\text{MMD}^2(X, Y) = \\frac{1}{T^2} \\sum_{i=1}^T \\sum_{i'=1}^T k(x_i, x_{i'}) + \\frac{1}{T^2} \\sum_{j=1}^T \\sum_{j'=1}^T k(y_j, y_{j'}) - \\frac{2}{T^2} \\sum_{i=1}^T \\sum_{j=1}^T k(x_i, y_j),$$ (8)\nwhere k(\u00b7, \u00b7) is a positive definite kernel function. In our implementation, we adopt the Gaussian kernel:\n$$k(x, y) = \\exp(-\\frac{||x - y||^2}{2\\sigma^2}),$$(9)"}, {"title": "3.4. Mamba-based Fusion and Optimization", "content": "Mamba-based Multimodal Fusion. Following the local and global alignment processes, we employ Mamba to facilitate efficient multimodal fusion while maintaining its inherent linear computational complexity. Unlike traditional Transformer-based methods that process all tokens simultaneously through self-attention mechanisms, our approach implements a time-priority scanning strategy that preserves Mamba's sequential nature while enabling effective cross-modal interactions. Given the aligned audio features X\u0303a, the aligned video features X\u0303v, and the language features X\u0303l, we construct a unified multimodal feature sequence Xmm by interleaving features from different modalities at each timestep:\n$$X_{mm} = [\\tilde{X}_{a}^1, \\tilde{X}_{v}^1, \\tilde{X}_{l}^1, \\tilde{X}_{a}^2, \\tilde{X}_{v}^2, \\tilde{X}_{l}^2, ..., \\tilde{X}_{a}^{T_l}, \\tilde{X}_{v}^{T_l}, \\tilde{X}_{l}^{T_l}],$$(11)\nwhere the superscript denotes the temporal index. This temporal-priority organization ensures that features from different modalities at the same timestep are processed sequentially, allowing the selective scan mechanism of Mamba to effectively capture both intra- and inter-modal dependencies. The fused representations are obtained by processing the constructed sequence through multiple Mamba layers.\nTraining Objective. The framework is optimized end-to-end using a composite loss function that combines the task-specific objective with the alignment constraints:\n$$\\mathcal{L} = \\mathcal{L}_{task} + \\lambda \\mathcal{L}_{align},$$(12)\nwhere Ltask is determined by the downstream task (e.g., cross-entropy loss for classification or mean squared error for regression), Lalign is the MMD-based alignment loss, and \u03bb is a hyperparameter that balances the two objectives. During training, minimizing Ltask drives the model to learn task-relevant multimodal representations, while Lalign ensures consistent feature distributions across modalities."}, {"title": "4. Experiment", "content": "We evaluate our proposed method on two distinct multimodal fusion scenarios: complete multimodal fusion and incomplete multimodal fusion. In the complete fusion setting, all modalities are available during both training and inference, which tests the model's ability to effectively integrate complementary information across modalities. The incomplete fusion scenario, where certain modalities may be missing during inference, presents a more challenging yet practical setting that evaluates the model's robustness and adaptability to partial observations. Through extensive experiments on these two scenarios, we demonstrate the effectiveness of our approach in both ideal conditions and more challenging practical situations."}, {"title": "4.1. Datasets and Evaluation Metrics", "content": "We conduct experiments on two multimodal representation fusion benchmarks: CMU-MOSI [39] and CMU-MOSEI [40]. Both datasets consist of video segments collected from online platforms, containing visual (facial expressions), acoustic (voice), and textual (transcribed speech) modalities. Compared to CMU-MOSI, CMU-MOSEI exhibits greater diversity in terms of speakers, topics, and recording conditions. Each segment in both datasets is annotated with a sentiment score ranging from -3 (highly negative) to +3 (highly positive). These scores are binarized into positive and negative sentiments for classification. To evaluate the effectiveness of our method, we adopt the following metrics based on previous works [18, 34]: binary accuracy and binary F1 score."}, {"title": "4.2. Comparison with SoTA methods", "content": null}, {"title": "4.2.1. Results on Complete Multimodal Fusion Tasks", "content": "Table 2 presents a comprehensive comparison between our approach and various state-of-the-art methods on the complete multimodal representation fusion task, which can be categorized into three main groups: (1) LSTM methods, including ICCN [28], MISA [7], and MMIM [6]; (2) Cross-modal Transformer methods: MulT [30], Self-MM [38], and DMD [16]; (3) Contrastive learning methods: HyCon [24], Confede [36], and MTMD [20].\nOn one hand, AlignMamba additionally incorporates token-level alignment to enhance multimodal fusion compared to contrastive learning approaches. On the other hand, AlignMamba's advantage over cross-modal Transformer methods lies in its consideration of distributional alignment relationships. Consequently, AlignMamba attains the best performance on all metrics in both datasets. For example, on the CMU-MOSI dataset, AlignMamba achieves a binary classification accuracy of 86.9%, representing a 0.9% improvement over previous methods. These results can be ascribed to AlignMamba's capacity to conduct extensive cross-modal alignment by leveraging its local alignment module and global alignment loss, thereby adeptly exploiting cross-modal correlations across different granularities and enabling the learning of more effective multimodal fusion representations."}, {"title": "4.2.2. Results on Incomplete Multimodal Fusion Tasks", "content": "Table 1 presents experimental results on incomplete multimodal representation fusion tasks. We compare Align-Mamba with various state-of-the-art methods, which can be categorized into two main groups: (1) modality recovery approaches, including MCTN [25], MMIN [43], GCNet [18], and IMDer [34], which attempt to reconstruct missing modalities from available ones; and (2) non-recovery approaches, such as DCCA [1] and DCCAE [33], which directly learn from available modalities.\nThe results demonstrate that AlignMamba consistently outperforms existing methods across different missing rates, achieving an average accuracy of 79.9% on the CMU-MOSI dataset, a 1.2% improvement over previous methods. More importantly, AlignMamba demonstrates stronger robustness to increasing modality missing rates. For instance, on the CMU-MOSI dataset, while MMIN and IMDer experience significant performance degradation with accuracy drops of 19.0% and 13.0% respectively, AlignMamba shows better resilience with only an 11.9% decrease in binary classification accuracy.\nIn conclusion, these improvements on both complete and incomplete multimodal fusion tasks can be attributed to the proposed dual alignment strategy: the local token-level alignment and global distribution-level alignment mechanisms work together to capture comprehensive cross-modal correspondences. This dual alignment strategy, combined with Mamba's efficient sequence modeling capabilities, not only enables learning more comprehensive and accurate multimodal fusion representations in complete multimodal scenarios, but also improves the robustness of learned representations in incomplete multimodal settings."}, {"title": "4.3. Efficiency Analysis", "content": "We conduct comprehensive efficiency analysis for Align-Mamba and compare them against both single-stream and multi-stream Transformer methods. Our evaluation metrics consist of GPU memory usage, inference time, and computational complexity. For a fair comparison, we specifically focus on the cross-modal interaction and fusion components, excluding the computational costs of unimodal encoders. All experiments are performed under identical conditions."}, {"title": "4.3.1. GPU Memory Usage", "content": "First, we report the GPU memory usage of each method with respect to varying input sequence lengths in Fig. 3. We exclude multi-stream Transformers on the 12.8k-token setting as they encounter an out-of-memory error. Align-Mamba consistently achieves the best trade-off between sequence length and memory usage across all settings, surpassing other Transformer-based approaches by a non-trivial margin. For instance, when processing 6.4k tokens, AlignMamba requires only 8.53 GB of memory, achieving 20.3% and 58.0% memory reduction compared to single-stream (10.7 GB) and multi-stream (20.3 GB) Transformers, respectively. This significant advantage in memory consumption is particularly valuable for processing longer sequences and deploying models on resource-constrained devices."}, {"title": "4.3.2. Inference Time", "content": "Next, we report the inference time of each method with respect to varying input sequence lengths in Fig. 4. To ensure fairness, we aggregate the running time of 50 inference passes for each model. AlignMamba again demonstrates consistent and substantial speed advantages over Transformer-based approaches across all settings. For instance, when processing 6.4k tokens, AlignMamba takes only 6.05 seconds, achieving 83.3% and 87.6% reduction in inference time compared to single-stream (36.13s) and multi-stream (48.61s) Transformers, respectively."}, {"title": "4.3.3. Computational Complexity", "content": "Finally, we analyze the FLOPs required by each method to quantify their computational efficiency. Without loss of generality, we fix the input sequence length to 1024 for each model. AlignMamba demonstrates superior efficiency with only 46.7G FLOPs, compared to 101.6G FLOPs for single-stream Transformer and 203.2G FLOPs for multi-stream Transformer. This represents a reduction of more than 54% compared to single-stream and 77% compared to multi-stream approaches, highlighting AlignMamba's computational advantages in cross-modal alignment and multimodal fusion tasks. This also justifies the lower memory consumption and swift inference speed as presented in the previous sections."}, {"title": "4.4. Ablation study", "content": "We conduct comprehensive ablation studies from three aspects to evaluate our proposed method, as shown in Table 3.\nComponent analysis. First, we evaluate the effectiveness of the OT-based local alignment module and MMD-based global alignment loss. Removing either component led to performance degradation across both datasets. For instance, on the CMU-MOSI dataset, accuracy dropped by 2.3% and 1.1% respectively. Notably, the OT-based alignment module demonstrated superior performance compared to the MMD-based alignment loss, likely because OT-based alignment provides explicit alignment plans while MMD-based alignment only imposes implicit alignment constraints.\nMamba-based fusion. Furthermore, we ablate Align-Mamba with regular single-stream [42] and multi-stream Mamba-based fusion methods [2] to show the effectiveness of our method in terms of multimodal fusion. Results demonstrate reduced performances in these two Mamba-based methods, suggesting their lack of explicit consideration of inter-modal correspondences, which makes it difficult to learn comprehensive cross-modal relationships. This shows that naive Mamba architecture alone does not suffice in effective multimodal fusion and highlights both the limitations of Mamba's original scanning mechanism and the necessity of our proposed cross-modal alignment.\nModality ablations. Lastly, we conduct modality ablation experiments by removing one modality at a time. When the text modality is removed, we only align the audio modality with the video modality. This results in significant performance degradation, likely due to the strong correlation between language and emotions. In contrast, removing the audio modality results in a smaller performance drop, possibly due to the sizable presence of irrelevant information in the audio modality such as background noise, reducing its impact on the overall performance."}, {"title": "4.5. Further Analysis", "content": null}, {"title": "4.5.1. Cross-modal Alignment", "content": "To quantitatively assess our dual alignment strategy, we measure the A-distance between modality pairs in Table 4. The A-distance \u2208 [0,2] is a common metric for domain discrepancy, with higher values indicating greater modality differences. Aal and Avl represents the audio-language and video-language distances respectively. The results reveal significant and consistent reductions in inter-modal distances through our dual alignment strategy in both CMU-MOSI and CMU-MOSEI. These improvements demonstrate the effectiveness of our strategy in bridging the modality gap by learning meaningful cross-modal correlations, leading to more robust multimodal fusion representations."}, {"title": "4.5.2. Optimal Transport Plan", "content": "Here, we qualitatively present the learned optimal transport plan. Figure 5 illustrates an example from the CMU-MOSI dataset. Notice that modalities exhibit temporal misalignment: the sentiment correspondence between different modalities may appear at different timesteps, which poses a challenge for multimodal representation fusion. For instance, the visual modality exhibits negative expressions at the beginning, while the textual modality introduces negative words toward the end. The original Mamba model struggles to explicitly learn these correspondences due to its sequential scanning mechanisms. In contrast, our proposed method leverages optimal transport to explicitly transform and align features in different temporal stages across modalities, reducing the modality gap and improving the effectiveness of multimodal fusion."}, {"title": "5. Conclusion", "content": "In this paper, we proposed AlignMamba, an efficient and effective method for multimodal representation fusion. By integrating OT-based local alignment and MMD-based global alignment, our method captures comprehensive cross-modal relationships while maintaining lower computational complexity. Extensive experiments on both complete and incomplete multimodal fusion tasks demonstrate that AlignMamba achieves state-of-the-art performance with significantly reduced computational costs."}]}