{"title": "MTPChat: A Multimodal Time-Aware Persona Dataset for Conversational Agents", "authors": ["Wanqi Yang", "Yanda Li", "Meng Fang", "Ling Chen"], "abstract": "Understanding temporal dynamics is critical for conversational agents, enabling effective content analysis and informed decision-making. However, time-aware datasets, particularly for persona-grounded conversations, are still limited, which narrows their scope and diminishes their complexity. To address this gap, we introduce MTPChat, a multimodal, time-aware persona dialogue dataset that integrates linguistic, visual, and temporal elements within dialogue and persona memory. Leveraging MTPChat, we propose two time-sensitive tasks: Temporal Next Response Prediction (TNRP) and Temporal Grounding Memory Prediction (TGMP), both designed to assess a model's ability to understand implicit temporal cues and dynamic interactions. Additionally, we present an innovative framework featuring an adaptive temporal module to effectively integrate multimodal streams and capture temporal dependencies. Experimental results validate the challenges posed by MTPChat and demonstrate the effectiveness of our framework in multimodal time-sensitive scenarios.", "sections": [{"title": "1 Introduction", "content": "Temporal awareness has garnered significant attention in AI research, particularly following Min et al.'s work (Min et al., 2020), which highlighted the inherent temporal dynamics in question-answering systems. Understanding time-sensitive information is crucial across various domains, including financial decision-making, event prediction, multimedia content analysis, and conversational AI. To explore temporal reasoning in large language models (LLMs), multiple time-sensitive datasets have been developed. TimeQA (Chen et al., 2021) and SituatedQA (Zhang and Choi, 2021) provide temporally grounded questions with free-text contexts extracted from WikiData (Vrande\u010di\u0107 and Kr\u00f6tzsch, 2014). Similarly, TEMPLAMA (Dhingra et al., 2022) builds on temporal knowledge bases, while StreamingQA (Liska et al., 2022) compiles time-sensitive question-answering (QA) data from English news articles spanning 2007 to 2020.\nHowever, these datasets primarily focus on text-based QA tasks, limiting their applicability to real-world conversational AI. They lack the multimodal components (e.g., images) that are essential for capturing rich temporal contexts and do not account for persona-grounded dialogues, where responses evolve based on a user's dynamic memory and past interactions. Although TimeIT (Ren et al., 2023) introduces time-sensitive multimodal tasks for long-video understanding, it has several limitations: (1) its focus on QA tasks restricts broader conversational applications, (2) the use of explicit temporal markers in videos reduces the challenge of reasoning over implicit temporal cues, and (3) its rigid response format (e.g., \u201c<timestamp_start> to <timestamp_end> seconds: <event_description>\") simplifies the task, minimizing complex temporal reasoning. Similarly, MPChat (Ahn et al., 2023) provides persona-grounded dialogues with multimodal memory, but it lacks an explicit temporal dimension.\nTo overcome these limitations, we introduce MT-"}, {"title": "2 Comparison with Existing Datasets", "content": "We start with a brief comparison of existing datasets, emphasizing multi-modal and time-aware strategies (see Table 1 for an overview).\nTime-Sensitive QA Datasets Time-Sensitive Question Answering (TSQA) involves interpreting and responding to questions that are dependent on specific time points or intervals. We analyse a set of TSQA datasets (Dhingra et al., 2022; Chen et al., 2021; Liska et al., 2022; Tan et al., 2023), as shown in the upper part of Table 1. Currently, TSQA datasets typically use free-text form or knowledge graphs (KGs) and are structured as QA tasks. However, our work introduces the first multimodal time-aware dataset based on conversation. Similar to TSQA, we modify the time of dialogues, which affects the responses and the related grounding memory, thereby testing the model's ability to understand time.\nMultiModal Dialogue Datasets Multimodal dialogue datasets generally comprise one or more images and multi-turn textual dialogues. As depicted"}, {"title": "3 MTPChat Dataset", "content": "Our dataset is built on the basis of MPChat (Ahn et al., 2023), a comprehensive multimodal persona-grounded dialogue dataset that includes both linguistic and visual components derived from episodic-memory-based personas. MPChat gathered from the social media platform Reddit, consists of memory image-sentence pairs and dialogue instances grounded on the speakers' multimodal memories.\nA significant challenge is the ingenious integration of time information and multimodal dialogue, aiming to establish a multimodal time-aware dataset. Based on MPChat dataset, we develop a novel methodology that involves three primary steps: 1) Time annotations, 2) Constructing time-aware conversations, and 3) Memory annotations. These efforts achieve the creation of a pioneering multimodal time-aware dialogue dataset. MTPChat breaks away from the limitations of current time-sensitive datasets confined to QA tasks, free-text formats and relying on explicit time information.\nWe believe that our work fosters the development of more diverse time-sensitive datasets and advancing research toward achieving human-level temporal understanding in models."}, {"title": "3.1 Time Annotations", "content": "We converted the UTC strings in MPChat dataset into date format \u201cyyyy/mm/dd\" and incorporated this feature into both the dialogue and memory components. The dialogue in our dataset is structured as a triplet consisting of (dialogue context, dialogue image, dialogue time), while each memory of the speaker is similarly organized as a triplet (memory description context, memory image, memory time)."}, {"title": "3.2 Time-Aware Conversations", "content": "In real-world scenarios, conversations can vary significantly based on the time they occur, even with similar contexts. For instance, as a high school student asked, \"What is machine learning?\u201d, you might respond with no knowledge on the subject. However, after three years of studying machine learning at university, your response to the same conversation would be more detailed, potentially including discussions about deep learning and related topics.\nInspired by how the temporal order of conversation and memories influences human responses, we constructed conversational data with temporal orders:\n\u2022 Later Stage Conversations: We used the original memories and conversations from the MPChat dataset, adding time annotations as described in Section 3.1. For instance, if you are a university student with three years of study in machine learning and are asked, \"What is machine learning?\", your response might include topics like deep learning.\n\u2022 Early Stage Conversations: To simulate conversations from earlier times, we assumed there was no prior memory of the discussion topic. We used the context of the original conversations but removed the original responses. We then add new, earlier time annotations and responses. The newly created responses differ from the original ones and contain minimal information about the discussion topic due to the lack of relevant memory. For example, if\""}, {"title": "3.3 Memory Annotations", "content": "To gain a more precise understanding of the model's capabilities in temporal awareness, we align conversations with memory. For the memory component, we add time annotations as outlined in Section 3.1. Since the memories of the speakers are sourced from real users on Reddit, we avoid creating fabricated memories to preserve data authenticity. Additionally, we incorporate a \"No Memory\" category into the speaker's memory set. Structured similarly to existing memory triplets (memory description context, memory image, memory time), the \u201cNo Memory\" category is assigned as the description context, indicating that there is no memory to align with the response. This memory category is used to align early-stage conversations. We then synchronize the memory time with the conversation's time information."}, {"title": "3.4 Dataset Statistics", "content": "MTPChat comprises 18,973 conversations and 25,877 users. We divided MTPChat into training, validation, and test sets with 15,056, 1,994, and 1,923 conversations respectively. We analyzed the proportion of later stage conversations and early stage conversations, finding a ratio of 3:1. As well as later stage conversations with grounding memories (some later stage conversations lack grounding memory) and early stage conversations with \u201cNo Memory\", resulting in a ratio of 2:1. Furthermore, to gain deeper insight into the time information"}, {"title": "4 Task Definition", "content": "The MTPChat datasets consist of N examples D = {(dn, rn, Mn)}n=1, where \u2200n \u2208 {1, ..., N} and each example contains a dialogue dn, the speaker's response rn to the dialogue dn and a memory set Mn from the speaker. Each dialogue dn = (cdn, idn, tdn) encompasses the context cdn (context utterances), an associated image idn and the time marking tdn (formatted as yyyy/mm/dd) when the dialogue occurred. The memory set for the speaker consists of m distinct memories Mn = {Mn1,..., Mnm}, where each memory Mnm = (cMnm, iMnm, tMnm) characterized by a description context cMnm (context utterances), an image \u00bfMnm and the time marking tMnm (formatted as yyyy/mm/dd) when the memory occurred."}, {"title": "4.1 Temporal Next Response Prediction", "content": "As illustrated in the Fig 3, Temporal Next Response Prediction (TNRP) is a retrieval task aimed at predicting the next responser from a set Re containing C response candidates based on the dialogue d = (cd, id, td) and the speaker's memories M = {M\u2081 = (cM1, \u00bfM1, tM1), ..., Mm}. The response candidate set Re comprises one ground truth and C 1 distractor responses. It is essential to emphasize that, 1) Identical dialogue content and speaker memories can lead to vastly different responses depending on the time of the dialogue. 2) To intensify the task's complexity and underline the temporal factor's significance, our response candidate set includes responses from later-stage dialogue and early-stage dialogue. The remainder of the response candidates are randomly selected from other dialogues."}, {"title": "4.2 Temporal Grounding Memory Prediction", "content": "Temporal Grounding Memory Prediction (TGMP) task is also a retrieval task that requires predicting the most likely memory element from a set Mc containing C memory candidates based on a given dialogue d = (cd, id, td) and a remainder memory set (except grounding memory) before producing a response. The memory candidate set Me comprises one grounding memory, one \u201cNo Memory\u201d option and C - 2 distractor memories randomly selected from other speakers. As shown in Fig 3, time variations within the dialogue substantially influence the choice of the grounding memory. Specifically, when the time of the dialogue is later than the time of the grounding memory, suggesting the availability of memory related to the dialogue for supporting the speaker's response, the model is capable of predicting the grounding memory. Conversely, if the time of the dialogue is earlier than that of the grounding memory, indicating an absence of relevant dialogue memory, the model must predict a \"No Memory\u201d outcome.\nIn TGMP task, we deliberately exclude the speaker's response from the input. This decision is based on the consideration that potential responses of early-stage dialogue can vary significantly-ranging from disinterest in the dialogue topic to expressing a desire to learn. These different but reasonable responses could potentially confuse the model to predict grounding memory. The principal objective of the TGMP task is making model recognize the critical temporal order between dialogue and memory. By focusing on whether the"}, {"title": "5 Framework", "content": "In this section, we present a framework to perform above retrieval tasks based on dialogue and memory. The inputs include dialogue dn, the speaker's response rn to the dialogue and a memory set Mn. We define various encoders to process different modalities of data, fuse the extracted features, and achieve both the temporal next response prediction task and the temporal grounding memory prediction task. The architecture of our framework is shown in Fig 4."}, {"title": "Text Encoder", "content": "In this study, we employ the text encoder to process textual components within tasks, specifically extracting representations of text and date information from dialogues, memories, and responses. For both dialogue and speaker memories, which may contain multiple entries, we first concatenate the text and date information for each entry. These concatenated strings are then further combined using a delimiter, forming unified representations. This method ensures comprehensive feature extraction by the text encoder, facilitating a more robust analysis of the textual data involved."}, {"title": "Vision Encoder", "content": "Similarly, our vision encoder to extract features from images embedded in dialogues and memories. In datasets featuring speaker memories with multiple images, each image is processed by this vision encoder. The extracted features are then aggregated via mean-pool operation to create a consolidated visual representation. This methodology ensures a coherent integration of visual data, significantly enhancing the model's capacity to process multi-image features effectively."}, {"title": "Adaptive Temporal Module", "content": "Following the extraction of textual and visual representations, it is essential to effectively integrate these features. As the inclusion of date information into textual representations can impact the correspondence between the text and vision features extracted by text encoder and vision encoder, we propose a method to dynamically balance these modalities to maintain the alignment of text and visual information within the same set of memories and dialogues. We introduce a module called the Adaptive Temporal Module (ATM), which is designed to be both simple and effective.\nFirst, we concatenate the corresponding text and vision features and map them through a linear layer. Subsequently, a sigmoid layer is used to derive the weights for both text and vision features. These weights are then employed to merge the features based on their relevance, ensuring better alignment and integration. This approach allows for a more coherent and contextually appropriate fusion of multimodal features, enhancing the overall interpretative capability of the model."}, {"title": "6 Experiments", "content": ""}, {"title": "6.1 Experimental Setup", "content": "Baselines We consider the following baselines:\n\u2022 SBERT+CLIP: We adopt a Transformer (Vaswani et al., 2017) initialized weights of SBERT (Reimers and Gurevych, 2019) and CLIP-ViT-B/32 vision model (Radford et al., 2021) as text encoder and vision encoder to represent text and image respectively. SBERT enhances the original BERT model (Devlin et al., 2018) to better handle similarity comparisons of dialogue and memory textual information. CLIP-ViT-B/32 vision model utilizes a Vision Transformer (ViT) (Dosovitskiy et al., 2020) with 32 attention heads, which enables it to capture more visual features.\n\u2022 CLIP+CLIP: We utilize the CLIP-ViT-B/32 model (Radford et al., 2021) as text encoder (CLIP-ViT-B/32 text model) and vision encoder (CLIP-ViT-B/32 vision model). CLIP-ViT-B/32 text model employs a Transformer similar to GPT (Radford et al., 2018), designed specifically for processing textual input, making it ideally suited for textual analysis requirements.\nTraining We train both baselines and our framework for 5 epochs with a batch size of 8 on a NVIDIA Tesla V100 GPU. The model is optimized using Adam (Kingma and Ba, 2014) with a learning rate of 3e-6. For our framework, we incorporated the Adaptive Temporal Module (ATM) into two baselines to validate the effectiveness of framework. We set the number of speaker's memories is m = 20 and the number of candidates is C = 100.\nEvaluation Metrics We assess the performance of the model on two tasks using Recall@1 and Mean Reciprocal Rank (MRR), which is the standard evaluation metrics on dialogue task (Lee et al., 2021; Feng et al., 2022; Ahn et al., 2023). Recall@1 quantifies the model's accuracy in retrieving the most relevant result as the top result for each query, effectively capturing the model's ability to return the most relevant result as the first item. MRR evaluates the average inverse ranking of the first relevant result across queries, providing insight into the model's overall retrieval quality."}, {"title": "6.2 Experimental Results", "content": "We conduct experiments of two baselines with and without our framework on time-sensitive tasks in MTPChat. Besides, we define two input settings: one limited to dialogue, and the other encompassing both dialogue and speaker's memories. The findings, as depicted in Table 2, reveal several insights: 1) MTPChat poses challenges in terms of"}, {"title": "6.3 Ablation Study", "content": "Zero-Shot Setting We explore the performance of the CLIP+CLIP model with a zero-shot setting on time-sensitive tasks. As shown in Table 4, the model demonstrates poor performance on MTPChat time-sensitive tasks, showing the challenges inherent in MTPChat and highlighting the urgent need for research into multimodal temporal awareness."}, {"title": "The Importance of Temporal Awareness", "content": "This study highlights the critical role of temporal awareness in models. Utilizing the CLIP+CLIP model, we trained on datasets both with and without temporal data of dialogue and memories. These models were then evaluated on the Temporal Grounding Memory Prediction (TGMP) task. Our findings (see Table 5) reveal a marked difference in performance: models without temporal awareness demonstrated substantial difficulties in time-sensitive tasks. Conversely, models incorporating temporal awareness significantly excelled, achieving a 12.7% increase in recall@1 and a 20.8% improvement in MRR."}, {"title": "7 Related Work", "content": "Time-Sensitive Datasets In recent years, time-sensitive datasets have predominantly been designed for question answering tasks and primarily consisting of textual data (Zhang and Choi, 2021; Chen et al., 2021; Tan et al., 2023; Liska et al., 2022; Wei et al., 2023; Yang et al., 2024b). Among these, the SituatedQA dataset (Zhang and Choi, 2021) represents a significant contribution by emphasizing open-domain, time-sensitive question answering. It reannotates questions from the Natural Questions (NQ) (Kwiatkowski et al., 2019) and Wikidata (Vrande\u010di\u0107 and Kr\u00f6tzsch, 2014) to capture contextual dependencies and temporal variations in answers. Similarly, TimeQA (Chen et al., 2021) comprises 20,000 questions, including a challenging variant that requires models to infer answers from implicit temporal cues in text passages. Additionally, the TempReason dataset (Tan et al., 2023) offers a comprehensive framework for evaluating various facets of temporal understanding. In these Open Book Question Answering (OBQA) settings, models leverage external textual resources to deduce correct answers (Izacard and Grave, 2020; Zaheer et al., 2020; Wei et al., 2021; Ouyang et al., 2022; Yang et al., 2024a).\nTime-sensitive datasets have also been developed for Closed Book Question Answering (CBQA), where models must generate answers relying solely on the information contained in the question (F\u00e9vry et al., 2020; Roberts et al., 2020; Dhingra et al., 2022). Furthermore, datasets built on knowledge graphs-such as TEQUILA (Jia et al., 2018), TimeQuestions (Jia et al., 2021), and CronQuestions (Saxena et al., 2021)\u2014pose more complex natural language queries, requiring mod-"}, {"title": "Multimodal Dialogue Datasets", "content": "Multimodal dialogue research has gained traction with the emergence of datasets that integrate images with multi-turn textual dialogues. Such datasets aim to jointly model visual and linguistic information to either answer questions (Antol et al., 2015; Das et al., 2017; Seo et al., 2017; Kottur et al., 2019; Li et al., 2023) or generate coherent responses (Meng et al., 2020; Zheng et al., 2021; Wang et al., 2021; Zang et al., 2021; Feng et al., 2022). For example, Mostafazadeh et al. (Mostafazadeh et al., 2017) introduced the IGC dataset, which comprises 4,000 dialogues centered around an image accompanied by a textual description and related questions and responses. Building on this, Shuster et al. (Shuster et al., 2018) released the ImageChat dataset, a substantially larger collection that captures more diverse conversational scenarios. Recent efforts have incorporated persona information to foster more personalized interactions. Datasets such as Fo-Cusd (Jang et al., 2022), MPChat (Ahn et al., 2023), DuLeMon (Xu et al., 2022), and MSPD (Kwon et al., 2023) augment dialogues with persona details-ranging from purely textual to multimodal attributes-enabling models to extract relevant personal context and enhance the naturalness of generated responses."}, {"title": "8 Conclusion", "content": "In this work, we addressed the underexplored challenge of temporal awareness in multimodal, persona-grounded dialogues by introducing MTPChat, a multimodal, time-aware persona dialogue dataset, along with an adaptive temporal framework. MTPChat presents new challenges by requiring conversational agents to comprehend implicit temporal dynamics in evolving dialogues and persona memories, thereby expanding the scope of temporal reasoning beyond traditional QA tasks. Additionally, our proposed adaptive temporal module has demonstrated significant improvements in model performance, underscoring its effectiveness in integrating multimodal streams and capturing dynamic temporal dependencies. Our findings highlight the importance of temporal reasoning in conversational AI, and we anticipate that MTPChat will serve as a valuable resource for future research in multimodal, time-aware AI systems."}, {"title": "9 Limitations", "content": "Despite its comprehensive structure and innovative tasks, the MTPChat dataset and our framework present certain limitations and need attention for future development. For MTPChat dataset, while the dataset significantly enhances the challenge of temporal reasoning by incorporating implicit temporal cues, it may still not fully capture the subtleties of real-world temporal dynamics, such as those influenced by cultural, historical, or personal contexts that affect human interactions. For our framework, future research should focus on refining this framework and exploring its scalability and adaptability across different domains and temporal challenges, aiming to further our understanding of time's impact on cognitive and decision-making processes."}, {"title": "10 Ethics Statement", "content": "In the development of the MTPChat dataset, we have placed a high priority on privacy and adherence to ethical standards. We ensured that the images in the dataset do not contain identifiable features such as faces, license plates, or email addresses, and the text is free from offensive language. We urge users of the dataset to be aware of these inherent risks. Additionally, commercial use of our data is strictly limited to ensure compliance with the Reddit API Terms and to protect user privacy. The MTPChat dataset is exclusively permitted for academic research purposes."}, {"title": "Appendix", "content": ""}, {"title": "A Detailed Prompt of GPT-4", "content": ""}, {"title": "B Detailed Parameters", "content": ""}]}