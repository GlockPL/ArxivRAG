{"title": "Adversarial Reasoning at Jailbreaking Time", "authors": ["Mahdi Sabbaghi", "Paul Kassianik", "George Pappas", "Yaron Singer", "Amin Karbasi", "Hamed Hassani"], "abstract": "As large language models (LLMs) are becoming more capable and widespread, the study of their failure cases is becoming increasingly important. Recent advances in standardizing, measuring, and scaling test-time compute suggest new methodologies for optimizing models to achieve high performance on hard tasks. In this paper, we apply these advances to the task of \u201cmodel jailbreaking\u201d: eliciting harmful responses from aligned LLMs. We develop an adversarial reasoning approach to automatic jailbreaking via test-time computation that achieves SOTA attack success rates (ASR) against many aligned LLMs, even the ones that aim to trade inference-time compute for adversarial robustness. Our approach introduces a new paradigm in understanding LLM vulnerabilities, laying the foundation for the development of more robust and trustworthy AI systems. Code is available at https://github.com/Helloworld10011/Adversarial-Reasoning.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) are increasingly deployed with various safety techniques to ensure alignment with human values. Common strategies include RLHF [CLB+23, OWJ+22], DPO [RSM+24], and the usage of dedicated guardrail models [IUC+23, RDS+23]. In nominal use cases, alignment methods typically refuse to generate objectionable content, but adversarially designed prompts can bypass these guardrails. A challenge known as jailbreaking consists of finding prompts that circumvent safety measures and elicit undesirable behaviors.\nCurrent jailbreaking methods fall into two categories: token-space and prompt-space attacks. Token-space attacks [SRI+20, WJK+23, ZWC+23, HBC+24, ACF24] focus on token-level modifications of the input to minimize some loss value, often using gradient-based heuristics [ZWC+23] or random searches [ACF24]. Such methods view jailbreaking as an optimization problem over sequences of tokens and use the loss information to inform their navigation through the optimization landscape. As a consequence, token-level methods produce semantically meaningless input prompts that can be mitigated by perplexity-based or smoothing-based filters [AK23, RWHP24].\nIn contrast, prompt-space attacks generate semantically coherent adversarial prompts using techniques like multi-round LLM interactions or fine-tuning for crafted outputs [CRD+24, GZH+23, LXCX24, MZK+24, ZLZ+24, SRL+24, LLS+24b]. A notable subset deploys chain-of-thought reasoning [WWS+23, NAGA+21] to guide the interaction with the target LLM and better navigate the prompt search space [CRD+24, MZK+24]. These methods are designed to exploit binary feedback from the target LLM of the form \"has the current"}, {"title": "2 Related Work", "content": "Token-space Jailbreaking. Token-space attacks [SRI+20, WJK+23, ZWC+23, HBC+24, ACF24] modify the input at the token level to decrease some loss value. For example, the GCG algorithm [ZWC+23], one of the first transferrable token-level attacks to achieve significant success rates on aligned models, uses the gradient of the loss to guide the greedy search. Subsequent work has refined this approach to obtain lower computational cost and improved effectiveness [LS24, JPD+24], including token-level modifications by other heuristics and without a gradient [HBC+24] and random searches over cleverly chosen initial prompts [ACF24]. We adopt the use of a loss function from these methods as a signal to inform how to navigate the prompt-space for better jailbreaks while remaining gradient-free.\nPrompt-space Jailbreaking. These methods often rely on a \"red-teaming\" LLM to generate adversarial prompts [PHS+22, WHS23, SSS+24, CRD+24, LXCX24, MZK+24, ZLZ+24, SRL+24, LLS+24b]. Methods such as PAIR [CRD+24] deploy a separate LLM, called the \"attacker\", which uses a crafted system prompt to interact with the target LLM over multiple rounds and generate semantic jailbreaks; they operate in a black-box manner, requiring only the target's outputs, and are highly transferrable [CRD+24]. Some other methods fine-tune a model to generate the attacking prompts [PHS+22, GZH+23, ZLZ+24, PZG+24, BCW+24], though this demands substantial computational resources. Rather than fine-tuning, we rely on increased test-time computation [SLXK24], while others start from expert-crafted prompts (e.g., DAN [Cha24]) and refine them via genetic algorithms [LXCX24, SRL+24, LLS24a]. Like these methods, our approach generates semantically meaningful jailbreaks by using another LLM as the attacker, however, our approach is significantly different from the prior work as we develop reasoning modules based on the loss values to better navigate the prompt space.\nComparison with PAIR and TAP. The closest methods to our framework are PAIR [CRD+24] and TAP-T [MZK+24]. Our verifier-driven method outperforms PAIR and TAP, whose prompt-based CoT remains static and does not leverage a loss. That said, while TAP-T creates a tree of attacks based on the attacker's CoT, it prunes only prompts that do not request the same content as the original intent, and does not utilize any reasoning methodologies.\nChain-of-Thought (CoT). CoT prompting [WWS+23] and scratch-padding [NAGA+21] demonstrate how prompting the model to produce a step-by-step solution improves the LLM's performance. Recent work [ZLC+24, WZW+24, XSG+25] suggests constructing the CoT through several modules rather than relying only on the language model's CoT capabilities. Notably, [XSG+25] explicitly constructs the CoT to ensure that it follows a particular path. Likewise, we use three modules for explicitly constructing the reasoning steps, aiming to reduce the loss function with each step.\nReasoning. Recent advances in reasoning have enhanced LLMs' capabilities in solving complex problems by scaling test-time computation mechanisms [HBK+21, RPBN+23, AVL+24, SWZ+24, RHS+23, Ope24b]. \u201cBest-of-N\u201d sampling [CKB+21, YGW24], which runs N parallel streams and verifies the final answers through outcome-based reward models (ORMs), is a straightforward test-time scaling approach. However, it might fail to uncover solutions that require incremental improvements, limiting its effectiveness compared to other test-time methods. To address this limitation, recent work utilizes process-based reward models (PRMs) [UKK+22, LKB+23, WLS+24] to optimally scale the test-time computation, thereby improving the reasoning performance [SLXK24, XGZ+24, GLG+24, Ope24a]. PRMs provide step-by-step verification that facilitates a look-ahead signal at each step, often necessary for a searching algorithm [XGZ+24]. Similarly, the use of a continuous loss function as a step-wise verifier allows us to run a tree search. This framework fits well into the \u201cProposer and Verifier\" perspective [SLXK24] of test-time computation, where the proposer proposes a distribution of solutions and a verifier assigns rewards to the proposed distributions. Robust verifier models are essential for accurate guidance [ZKL+24, ZHB+24, SVK24], but they require intermediate annotations from human annotators or heuristics [UKK+22, LKB+23, WLS+24]. In our work, we use the loss values from a surrogate LLM as a verifier, eliminating the need for training a verifier model.\nReasoning vs safety. The reasoning framework for exploiting test-time compute can also be used to improve alignment. OpenAI uses \u201cdeliberative alignment\u201d [GJW+24] to incorporate human-generated and adversarial data to improve the alignment of the o1 model family [Ope24b, Ope24a]. These models"}, {"title": "3 Preliminaries", "content": "The objective of jailbreaking is to elicit a target LLM T to generate objectionable content corresponding to a malicious intent I (i.e., \u201cTell me how to build a bomb\"). This will be obtained by designing a prompt P such that the target LLM's response T(P) corresponds to I. A judge function, Judge (Target(P), I) \u2192 {0,1}, is then used to decide whether the response satisfies this condition. Therefore, successful jailbreaking amounts to finding a prompt P such that:\nJudge (T(P), I) = 1.\nWe reinterpret this problem as a reasoning problem. Rather than directly optimizing the prompt P as token-level methods do, we construct P by applying an attacker LLM A to a reasoning string S, i.e., P = A(S). This allows us to update the attacker's output distribution according to the \u201cProposer and Verifier\" framework [SLXK24] by iteratively refining S. Thus, the challenge is to find a string S such that, when passed to the attacker as shown in Figure 1, it satisfies the following objective:\nJudge (T(A(S)), I) = 1.\nThis formulation allows us to view jailbreaking methods through the lens of an iterative refinement of S. Note that many existing reasoning algorithms can be framed into this formulation. For instance, chain-of-thought prompting can be realized by repeatedly generating partial thoughts from an LLM and appending them to S. The final answer is generated by passing the updated S to the same LLM. Similarly, in the jailbreaking literature, methods such as PAIR [CRD+24] aim to solve (3.1) by updating S at each iteration, appending the generated CoT from the attacker and the responses from the target. The string S encapsulates all partial instructions and the intermediate steps executed during the attacking process.\nPrior prompt-space jailbreaking methods have often used prompted or fine-tuned LLMs [CRD+24, MZK+24, MPY+24] as judges to evaluate whether a jailbreak is successful. These judges are the simplest \u201cverifiers\u201d: once the refinement of S is over, the judge will evaluate if A(S) is a successful jailbreak. However, the judge only provides a binary signal: whether or not jailbreaking has taken place. This makes binary verifiers unsuitable for estimating intermediate rewards. To alleviate this, we use a continuous and more informative loss function. A loss function will provide more granular feedback by assigning smaller loss values to prompts that are semantically closer to eliciting the desired behavior from T. Following prior work [ZWC+23, HBC+24], we use the cross-entropy loss of a particular affirmative string for each intent (e.g., \u201cSure, here is the step-by-step instructions for building a bomb\"), measuring how likely the target model is to begin with that string. Showing this desired string as \\(y_1 = \\{y_1, y_2,\\ldots, y_l\\}\\), our goal is to optimize the following next-word prediction loss:\nLT(P,y1) = - \\log (PT(y1,\\ldots, yl|P))\n= - \\sum_{i=1}^{l} \\log (PT(y_i|[P, y_{1:i-1}]))\nThis function can be calculated by reading the log-prob vectors of the target model. Utilizing this loss function as our process reward model, we must refine the reasoning string S such that:\n\\min_{S} L_T(A(S), y_1).\""}, {"title": "4 Algorithm", "content": "Optimization over the reasoning string. We must find a reasoning string S for an attacker LLM A such that the resulting prompt P := A(S) jailbreaks the target LLM. Our algorithm iteratively refines S, aiming to minimize the loss given in Equation (3.3). Starting from a root node S(0)\u2014a predefined template in Appendix C\u2014we iteratively construct a reasoning tree with nodes representing candidate strings (see Figure 1). At iteration t, a node in the tree with the best score is selected. This node will expand into m children \\(S^{(t+1)}_1,\\ldots, S^{(t+1)}_m\\). The tree will be further pruned at each iteration to retain a buffer of the best nodes. We now explain this process in detail, beginning with a description of an individual reasoning step in our method.\nFeedback according to the target's loss. Let S(t) be the reasoning string at time t. We generate n prompts P1, P2,\u2026, Pn sampled independently from the distribution of A with S(t) as input (see Figure 2). Let l1,..., ln be the loss values obtained from Equation (3.2) for P\u2081,\u00b7\u00b7\u00b7, Pn, respectively. For simplicity, we assume that the prompts are ordered in a way that \\(l_1 \\le l_2 < \\cdots \\le l_n\\), i.e., prompt P\u2081 incurs the lowest loss and Pn the highest loss on the target LLM. We use Feedback LLM IF with a crafted system prompt (given in Appendix C) that takes the ordered set of prompts as input and generates a feedback string F as a textual analysis, semantically explaining why P\u2081 is a better attacking prompt than P2, why P2 is better than P3, etc., and identifies any apparent patterns across the prompts:\nF := F([P\u2081,\uff65\uff65\uff65,Pn]).\nExamples of feedback strings F are illustrated in Figure 2, where P\u2081 with a role-play scenario has a lower loss, so Feedback LLM highlights this observation for use as an extra instruction in the next iteration.\nApplying the feedback via a refiner. Once the feedback is obtained, the reasoning string must be refined and updated into its children. One way to do this is to append the feedback to the reasoning string at each time. This quickly becomes intractable\u2014not only does the string length grow with each iteration, but also the set of different feedbacks can contradict each other. Instead, we deploy a Refiner LLM IR inspired by the idea of \"textual gradients\u201d introduced in [YBB+24]. Taking S(t) and the feedback string F as its arguments, R generates a new reasoning string S(t+1) that refines S(t) based on the feedback:\nS(t+1) = IR(S(t), F).\nReplicating the above process (Feedback+Refine) m times in parallel, we produce m new reasoning strings \\(\\{S^{(t+1)}_1,\\ldots, S^{(t+1)}_m\\}\\) as the children of S(t). Figure 2 shows a single iteration of our method and illustrates how the updated reasoning string incorporates the key components of the feedback. Note that rather than relying on the attacker's CoT process\u2014which lacks any information about the loss function\u2014we explicitly engineer the reasoning steps aiming to decrease the loss function. This setup parallels recent efforts that align a model's intermediate steps with predefined strategies [WZW+24, XSG+25].\nVerifier. Next, to quantify the quality of each reasoning string, we assign a score using the loss function in Equation (3.3). For a given reasoning string S, we define the verifier's score as:\nV(S) := \\min_{\\{P_1,\\ldots, P_n\\}\\sim A(S)} L_T(P_i, y_1),"}, {"title": "5 Experiments", "content": "Baselines and Evaluations. We compare our algorithm with state-of-the-art methods for jailbreaking in both the token-space and the prompt-space. Specifically, we include GCG [ZWC+23], PAIR [CRD+24], TAP-T [MZK+24], and AutoDAN-turbo [LLS+24b] which is an extension of AutoDAN [LXCX24]. Additionally, we incorporate results from [ACF24], even though some of their methods go beyond just modifying the target LLM's input and employ pre-filling attacks or alter the target model's system prompt. As we limit our comparison to methods that interact with the target LLM through its input, we use only their crafted template along with random search, which we refer to as \u201cPrompt + Random Search\". We use Attack Success Rate (ASR) as the main metric for comparison. We execute our algorithm against some of the safest LLMs according to [Swa] leaderboard, including both open-source (white-box) and proprietary (black-box) models. The HarmBench judge [MPY+24] is deployed to evaluate the target LLM's responses due to its high alignment with human evaluations [SLB+24]. We test our algorithm on 50 uniformly sampled tasks selected from standard behaviors in the Harmbench dataset [MPY+24]. We manually verify all proposed jailbreaks to avoid false positives.\nAttacker models As for the attacker model, we use LLMs without any moderation mechanism to ensure compliance. Specifically, we use \u201cVicuna-13b-v1.5\" (Vicuna) [CLL+23] and \u201cMixtral-8x7B-v0.1"}, {"title": "5.1 Attack Success Rate", "content": "In this section, we present our results on white-box target LLMs that permit direct access to log-prob vectors\u2014essential for calculating our loss function given in (3.2). Results for black-box models are given in Section 5.2.\nFor all methods that rely on an attacker LLM to generate the attacking prompts\u2014except for AutoDAN-Turbo\u00b9 we have deployed Mixtral2. The main results are presented in Table 1. Except for Llama-3-8B, our method achieves the best ASR among both the token-space and the prompt-space attempts. This is significant because token-space algorithms operate without constraints to be semantically meaningful, making it easier to find a jailbreaking prompt. However, as shown in Table 1, for target models that have been adversarially trained against token-level jailbreaking such as Llama-3-8B-RR [ZPW+24] and R2D2 [MPY+24], these algorithms largely fail since as the rely on eliciting only a handful of tokens. In Appendix D, we provide jailbreak examples and details of how our algorithm works w.r.t. those examples in Figures 10 and 11."}, {"title": "5.2 Multi-shot transfer attacks", "content": "Given the infeasibility of obtaining the log-prob vectors in black-box models, we evaluate the success of our algorithm using two transfer methods. We perform the transfer by optimizing the loss function on a surrogate white-box model and then applying the derived adversarial prompt to the target black-box model. A common approach is to transfer the prompt that jailbreaks or yields the lowest loss on the surrogate model [ZWC+23]\u2014we call this a \u201cone-shot\u201d transfer attack. However, this does not always result in an effective attack as the loss function serves only as a heuristic in the transfer. We improve effectiveness by using a scheme that queries the target model with all the attacking prompts collected from executing the algorithm (n prompts per iteration). We call this a \u201cmulti-shot\u201d transfer. We show that the transfer success significantly increases with this scheme. We use the loss values from three white-box models: Llama-2, Llama-3-RR, and"}, {"title": "5.3 Ablation studies", "content": "We investigate whether our method fulfills its objectives in (i) minimizing the loss function; and (ii) generating the feedback based on the ordered attacking prompts. Besides, we demonstrate that our method finds more jailbreaks in later iterations compared to previous work.\nEffective loss minimization. Our algorithm aims to optimize a loss function defined over a string. To assess its effectiveness, we can probe V(S) in Equation (4.3) over iterations. Section 5.3 illustrates this loss progression for Llama-2-7b, Llama-3-8b, Llama-3-8b-RR, and R2D2, with Mixtral as the attacker and averaged over 50 tasks. The results showcases a quantitative decrease in the minimum loss for all target models until approximately the 10th iteration, after which the loss exhibits slight oscillations. Notably, for R2D2, the loss converges to zero despite the safety measures. As depicted in Section 5.3, the loss curve for Llama-3-8b-RR shows a significant gap compared to what its fine-tuned from, Llama-3-8b, and despite this gap, our algorithm achieves 44% of ASR. To investigate this, we have plotted the loss separately for successful and failed jailbreaking attempts in Section 5.3. This figure shows that, on average, successful attempts start and end with lower loss values than failures. This demonstrates the utility of Equation (3.2) as a heuristic; Although the absolute loss value may remain high, its relative value serves as an informative metric. Consequently, comparing the losses of multiple attacking prompts provides feedback that guides the algorithm toward more effective attempts. Unsurprisingly, the jailbreaks do not begin with their desired output string y\u012b for Llama-3-8b-RR. However, in Appendix D, we show that there are other possibilities such as initially refusing followed by compliance. This helps us establish our work as an attack in the prompt space that rigorously optimizes a loss function, similar to algorithms in the token-space, but with significantly fewer number of iterations. In contrast, the number of iterations for the token-space algorithms can be as high as 104 [ACF24].\nFeedback consistency. We conduct an experiment to demonstrate how feedback shifts the attacker's output distribution toward more effective attacking prompts. We show that applying the feedback increases the generation probability of attacking prompts with lower losses in subsequent iterations. We consider a feedback F as consistent if the following constraint holds for it:\n\\frac{P_A (P_a|R(S, F))}{P_A(P_a|S)} \\geq \\frac{P_A (P_b|R(S, F))}{P_A(P_b|S)} if L_T(y_1, P_a) \\leq L_T(y_1, P_b) and a,b \\in \\{1,\\ldots, n\\}\nHere, IPA (PIS) denotes the probability of the attacker LLM generating prompt P conditioned on the reasoning string S. I.e., IPA (P|S) = \\Pi_{i=1}^{l}P_A(p_i|[S, P_{1:i-1}]) where P = [p1,\u2026, p\u0131]. To evaluate this condition, we"}, {"title": "6 Acknowledgment", "content": "This work of HH and GP was supported by the NSF Institute for CORE Emerging Methods in Data Science (EnCORE). SM was supported by both EnCORE and RobustIntelligence. The authors wish to thank Alexander Robey for helpful discussions."}, {"title": "7 Conclusion", "content": "The core problem we address in this paper is the role of reasoning in AI safety. While there have been recent efforts arguing that replacing reasoning with increased compute can lead to better defense mechanisms, these approaches contain a fundamental oversight. They fail to consider that attackers may also leverage reasoning to bypass guardrails. This paper defines adversarial reasoning, demonstrates a practical implementation, and provides state-of-the-art results on attack success rate.\nOur work points to new directions for understanding and improving language model security. By bridging reasoning frameworks with adversarial attacks, we have demonstrated how structured exploration of the prompt space can reveal vulnerabilities even in heavily defended models. This suggests that future work on model alignment may need to consider not just individual prompts but entire reasoning paths when developing robust defenses. The success of our transfer attack methodology also highlights the importance of considering multiple surrogate models when evaluating model security. Looking ahead, our findings point to several promising research directions, including developing more sophisticated reasoning-guided search strategies, exploring hybrid approaches that combine token-level and semantic-level optimization, and investigating how process-based rewards could be incorporated into defensive training. Finally, while our study has focused on textual LLMs, our framework can potentially be relevant to the broader class of LLM-driven agents [ASD+24]. In particular, our methods can be naturally extended to LLM-controlled robots [LHX+23, KNC+23, VBBK24], web-based agents [WSK+24], and AI-powered search engines [RBC+24]. Recent work [RRK+24] underscores this connection by demonstrating that vulnerabilities identified in textual models can be transferred to real-world scenarios."}, {"title": "A Experiments setting", "content": "For running Algorithm 1 in Section 5.1, we loaded the target models on our local GPUs to read to the\nlog-probs, but used TogetherAI [Tog] for collecting the full responses. We also used TogetherAI for the\nattacker, Feedback LLM, and Refiner LLM in all the sections. We used one NVIDIA A100 for our experiments.\nTransfer method We chose to put GPT4o and Llama-3.1-405B in the black-box category. Despite the\nprevious attempt to extract the entire log-prob vector based on the top-5 log-probs for GPT-4 [HBC+24],\nthere is no guarantee that OpenAI will preserve this feature in later releases, so we have included this model\nas one of the black-box models. For Llama-3.1-405B, we used TogetherAI for querying since the model would\nnot fit to our GPUs, and TogetherAI does not give access to the entire log-prob vector.\nAs mentioned, OpenAI o1 and Gemini-pro come with a content moderation filter that blocks the genera-\ntion when activated. However, there are two main reasons that cause content moderation to stay random for\nthese models. First, at the time of doing our experiments, it was not possible to set the temperature to 0 for\nOpenAI 01, resulting in non-deterministic generation along with its moderation. Secondly, in our experi-\nments with Gemini-pro, we observed that content moderation remains random is spite of a zero temperature\nsetting, and can be bypassed through repeated attempts for the same attacking prompt. Therefore, for both\nmodels, we repeat the query 3 times in case of a generation block before accepting the refusal as a response."}, {"title": "B Additional experiments", "content": "LLM vulnerabilities For some of the most common LLMs, we illustrate their vulnerabilities in different\ncategories of Harmbench [MPY+24]. Figure 6 shows that Claude has a stronger performance (lower ASR)"}, {"title": "C System Prompts", "content": "Judge We get the judge provided by Harmbench [MPY+24] to give a verdict for the jailbreaking attempts.\nThis model is fine-tuned based on Llama-2-13B [AI@23] with a knowledge distillation process from GPT-4,\nand a meticulously crafted prompt for querying the models. Below we have attached the prompt used for\nthis manner, which is a slightly modified version of Harmbench's original prompt. This prompt emphasizes"}, {"title": "D Examples", "content": "Figures 10 and 11 show the details of producing the jailbreaking prompt, including the feedback strings.\nIn Figure 10, the algorithm runs for 6 iterations and expands multiple strings (only the expanded ones are\nshown here), whereas in Figure 11, it finds a jailbreaking prompt with only one iteration. Figure 12 shows\nthat PAIR sometime deviates from the original intent. Figures 13 and 14 show that our algorithm finds\njailbreaks that the target LLM begins with refusal but then complies."}]}