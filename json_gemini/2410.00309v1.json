{"title": "Ask, Pose, Unite: Scaling Data Acquisition for Close Interactions with Vision Language Models", "authors": ["Laura Bravo-S\u00e1nchez", "Jaewoo Heo", "Zhenzhen Weng", "Kuan-Chieh Wang", "Serena Yeung-Levy"], "abstract": "Social dynamics in close human interactions pose significant challenges for Human\nMesh Estimation (HME), particularly due to the complexity of physical contacts\nand the scarcity of training data. Addressing these challenges, we introduce a novel\ndata generation method that utilizes Large Vision Language Models (LVLMs)\nto annotate contact maps which guide test-time optimization to produce paired\nimage and pseudo-ground truth meshes. This methodology not only alleviates the\nannotation burden but also enables the assembly of a comprehensive dataset specif-\nically tailored for close interactions in HME. Our Ask Pose Unite (APU) dataset,\ncomprising over 6.2k human mesh pairs in contact covering diverse interaction\ntypes, is curated from images depicting naturalistic person-to-person scenes. We", "sections": [{"title": "1 Introduction", "content": "Understanding human behavior is fundamental for many fields, such as socially aware robotics,\npatient-caregiver interactions in healthcare, and parent-child interactions in psychology. Central to\nthis pursuit is the study of close interactions between individuals, which are crucial for deciphering\nthe complexities of human dynamics. The field of Human Mesh Estimation (HME) has emerged as\na promising approach to study these dynamics, leveraging parametric models to interpret intricate\nscenes involving multiple people. However, despite significant advancements in multi-person HME,\ncurrent methods struggle when faced with close interactions. This is because accurately reasoning\nabout contacts requires a deep understanding of their 3D nature-how people touch and interact in\nthree-dimensional space. Capturing this type of training data is particularly challenging due to the\nscarcity of ground truth 3D meshes, which are essential for precise estimation and analysis. The lack\nof detailed 3D data leaves gaps in the ability to model the subtleties of human contact, resulting in\nless accurate and reliable HME outcomes. Addressing this data scarcity is crucial for advancing the\nfield and improving the handling of complex interaction scenarios.\nRecent efforts [13, 65, 16] have successfully acquired ground truth data for closely interacting\nscenes using motion capture (mocap) systems. Although effective, these systems are costly and limit\nthe dataset's scope. Typically, these datasets feature only two subjects at a time, are confined to\nindoor lab environments, and cover a restricted set of predefined actions. Other approaches [13, 36]\nhave proposed using weak supervision to avoid the need for mocap by formulating contact as a\nmatching problem between surface body regions of the SMPL model [31] in the form of binary\ncontact matrices. While promising, this approach requires manual annotation of each contact region,\nlimiting the scalability and integration into existing HME pipelines.\nMore recently, M\u00fcller et al. [36] successfully generated pseudo-ground truth meshes from manually\nannotated image-contact matrix pairs from the FlickrCI3D dataset [13], which they used to train a\ndiffusion-based contact prior for HME. The key insight for this approach was that the additional\ndata enabled the contact prior to learn more meaningful contacts, significantly enriching the training\nset. However, despite these advancements, the contact prior still faces challenges with complex or\nout-of-distribution interaction scenarios. Problem compounded by the high cost of sourcing relevant\nimages and manually annotating people interacting with their contact matrices. This highlights the\nongoing need for solutions that can reflect the wide range of interactions found in in-the-wild scenes.\nFurther progress in HME will be facilitated by datasets that accurately mirror natural interactions,\ncapturing the full spectrum of human dynamics in diverse and unstructured environments.\nIn this work, we introduce a novel dataset and data generation method to increase the diversity of\nposed meshes interacting closely. We develop an innovative approach Ask, Pose, Unite that leverages\nLarge Vision Language Models (LVLMs) to automatically create paired pseudo-ground truth meshes\nfor scenes with closely interacting individuals (see Figure. 1). Our method enables the generation\nof accurate and detailed 3D representations of interactions without the need for costly and labor-\nintensive motion capture systems. To support this, we curate a comprehensive dataset, the Ask Pose\nUnite (APU) dataset, which features a large diversity of natural interaction types. We source these\ntypes from in-the-wild images depicting people involved in close contact, capturing the complexity\nand variability of real-world human dynamics. By including a wide range of interaction scenarios,\nour dataset provides a robust foundation for training Human Mesh Estimation (HME) models for\ninteracting humans. We validate the effectiveness of our data generation method and dataset through a\ncase study, demonstrating that retraining a contact prior for HME with our data significantly improves\nprediction accuracy in new settings with uncommon actions. This validation shows that our approach\nnot only enhances the quality of HME models but also ensures their adaptability to diverse interaction\nscenarios."}, {"title": "2 Related Work", "content": "Data Diversity in Multi-person HME. Monocular Human Mesh Estimation is an underspecified\nproblem, particularly challenging due to the difficulty of capturing paired 2D to 3D ground truth,\nespecially for multiple people. To address this challenge, various datasets have adopted alternate\nsupervision strategies (see Table 1). Some datasets, [37, 4] use synthetic data to bypass the difficulties\nof capturing real-world 3D ground truth, providing a large number of images and subjects. Others\n[68, 34] restrict their settings to lab environments, capturing high-quality 3D data but a trade-off\non diversity. Since the work by [5], single person HME methods have relied on weak supervision\nto overcome the scarcity of paired 2D to 3D ground truth, using body part segmentations [23], 2D\nkeypoints [27, 58], and priors based on mocap data [39, 26, 56, 32]. Our work extends this line of\nresearch by introducing a data generation method for paired 2D to 3D pseudo-ground truth.\nMulti-person HME methods often process individuals independently, which can yield accurate\npredictions for isolated figures but fails to correctly position them relative to one another in world\nspace. Recent advancements have addressed these issues by using a unified spatial framework for\nentire images [29], jointly modeling scene and camera dynamics [64], tracking people across time\n[66, 42, 48, 15], harnessing all available data [6], and managing occlusions [10, 24, 69]. In contrast,\nsingle-stage approaches [47, 22, 49, 2, 46, 57], which predict all subjects simultaneously, have\ndemonstrated superior performance in terms of spatial accuracy and scale consistency. Despite their"}, {"title": "3 Ask Pose Unite Dataset", "content": "We hypothesize that existing datasets that feature closely interacting humans often suffer from a lack\nof diversity and imbalance in their interaction types. We perform an analysis of the interactions in\nHME close interaction datasets using the representation space of CLIP [41] text embeddings as a\nproxy for analyzing the variety of interaction types across all datasets. For each dataset, including our\nAPU dataset, we curate a list of all unique interaction types as well as their respective frequencies.\nThen, we extract the CLIP text embeddings for all unique interaction types and visualize the principal\ncomponents after PCA. Because FlickrCI3D lacks explicit classes we obtain per image descriptions\nwith the BLIP-2 [28] captioning model and group similar actions by pattern matching on the action\nphrases. For APU we use the interaction predicted by the LVLM. Figure 2 shows our method's ability\nto increase data collection on interaction types that are typically under-represented in prior datasets.\nOur APU dataset extends beyond the clusters formed by the other datasets, indicating that it includes\nnovel interaction classes. We highlight some example interactions where the increase in diversity or\npoints are noteworthy, such as \"assisting\", \"hug\", \"arm-in-arm\", among others. This straightforward\nexperiment confirms our hypothesis that both our APU dataset and data generation method are viable\nsolutions for responding to the data scarcity problem in close interactions for HME."}, {"title": "3.2 Dataset statistics", "content": "We compile our APU dataset by building on a key insights of prior works [13, 36]: using 2D images\nwith weak labels to target interaction diversity. We have gathered more than 6,000 meshes paired\nwith images, contact annotations, and natural language descriptions of the interactions from both\nlaboratory and in-the-wild scenes, encompassing a variety of ages, subjects, and interactions (see\nTable 1). Figure 3 shows examples of the images and mesh pairs from our dataset."}, {"title": "3.3 Data generation method", "content": "Problem formulation. We aim to curate images depicting pairs of people closely interacting with\nwell-reconstructed pseudo-ground truth meshes from any set of in-the-wild images. To achieve this,\nwe propose a data generation method (Figure 4 outlines the main steps of our approach). Specifically,\nour goal is to locate pairs of closely interacting people within any set of images and produce mesh\nestimates for each pair. Since we only rely on weak supervision in the form of predicted contact\nmaps, 2D keypoints, and interaction labels, we also aim to automatically select the well-reconstructed\nmeshes.\nIn the context of a single image capturing a scene of close interactions between individuals, our\nobjective is to fit a SMPL-X [39] parametric 3D human mesh model for each individual p to recover\ntheir pose \\(\\theta_p \\in \\mathbb{R}^{21 \\times 3}\\) and shape \\(\\beta_p \\in \\mathbb{R}^{10}\\) parameters. We position each mesh in world coordinates\nby also estimating the root translation \\(Y_p \\in \\mathbb{R}^{3}\\) and global body rotation \\(\\varphi_p \\in \\mathbb{R}^{3}\\). Following previous\nwork [37, 49], we support the prediction of multiple ages including children with the SMPL-XA\nmodel which adds an interpolation parameter \\(\\sigma_p\\) between the shape space of SMPL-X and SMIL [20].\nIn practice \\(\\sigma_p\\) is concatenated to the shape parameters such that \\(\\beta_p \\in \\mathbb{R}^{11}\\).\nGiven an unannotated image I of two people closely interacting we aim to recover their meshes \\(M_a\\)\nand \\(M_b\\) by following an optimization of the parameters \\(\\{\\theta_p, \\beta_p, Y_p, \\Phi_\\rho, \\sigma_p\\}_{p=a,b}\\) under the constraint\nof a contact map C. Where \\(C \\in \\{0, 1\\}^{R \\times R}\\) is defined as a guidance of which body surface regions\nare in contact. In particular, \\(C_{i,j} = 1\\) indicates that region \\(r_i\\) of \\(M_a\\) is in contact with region \\(r_j\\) of\n\\(M_b\\).\nCandidate proposal. For a set of images we obtain 2D keypoints and initial mesh predictions from\nfrom off-the shelf estimators. We propose as candidates all pairs of people with k valid keypoints\nwithin a distance d of each other and with mesh predictions aligned with the keypoints.\nLVLM contact map querying. We employ a LVLM to automatically generate C. The inherent\nchallenge of using LVLMs for this task lies in their low performance when grounding complex spatial\nrelationships depicted in 2D images [53]. Naively querying the LVLM often results in hallucinated\nor missing contacts leading to degenerate mesh predictions. We tackle this limitation by in-context\nprompting and denoising the contact maps (explained in the following section).\nTo query the LVLM for pairs of body parts that are touching in I we first regroup the 75 body regions\nintroduced in [13] into coarser semantically meaningful sets. These sets correspond to the body parts:\nhand, arm, leg, thigh, chest, stomach, back, neck, face, head, foot, shoulder, elbow, knee, forearm,\nupper arm, and waist. In practice we re-map a body part \\(B_i\\) to a list of corresponding body regions\nsuch that \\(B_i = \\{r_1, r_2, ..., r_n\\}\\). When available, we ground the LVLM by incorporating a low-cost\nsoft label A, which indicates the type of interaction depicted in the image. A is a fundamental element\nof existing close interaction datasets and serves in this setting as a contextual prior.\nContact map denoising. We observe that even with contextual clues, LVLMs are unreliable when\npredicting the laterality of the body parts. We hypothesize that this problem arises from the model"}, {"title": "Constrained optimization", "content": "Following prior work [36, 5], we obtain pseudo-ground truth meshes \\(M_a\\)\nand \\(M_b\\) for a pair of people in contact using a two-stage optimization which takes as input estimated\n2D keypoints, an initial estimate of the parameters \\(\\{\\theta_p, \\beta_p, \\rho_p\\}_{p=a,b}\\), and C. In the first stage we\noptimize \\(\\{\\theta_p, \\beta_p, Y_p\\}_{p=a,b}\\) given a contact loss \\(L_c\\) and other priors as guidance. We propose a\nsoft version of the contact loss from [36] to account for the uncertainty in the predicted contacts.\n\\(L_c = \\sum_{i,j} W_{ij} C_{ij} \\min_{v \\in r_i, u \\in r_j} ||v - u||^2\\), where u and v are vertices, and \\(W_{ij} = \\frac{1}{Z}\\) is the normalized\nkeypoint distance from the denoising step scaled by the LVLM's confidence for the contact.\nAs additional guidance for the optimization we use a pose prior based on a Gaussian Mixture Model\n\\(L_{GMM}\\) [5], an \\(L_2\\) shape prior \\(L_{\\beta}\\) that penalizes deviation from the SMPL-X mean shape, \\(L_{\\Theta}\\) an \\(L_2\\)\nloss that penalizes deviation from the initial pose \\(\\tilde{\\Theta}_p\\), and a 2D keypoint reprojection loss \\(L_j\\). In the\nsecond stage we fix \\(\\beta_p\\) and add \\(L_p\\) to resolve interpenetration between meshes [36].\nThe complete loss for the constrained optimization with values that re-weigh each term is: \\(L_{optim} =\\\\\n\\lambda_J L_J + \\lambda_{GMM} L_{GMM} + \\lambda_{\\beta} L_{\\beta} + \\lambda_P L_P\\)\nAutomatic filtering. As a last step we implement a filtering strategy to remove incorrect mesh\nproducts from the optimization by thresholding the 2D keypoint reprojection loss of \\(M^a\\) and \\(M^b\\).\nWe keep all instances with error less than 20 for both subjects."}, {"title": "4 Case Study: Using APU to improve estimation for novel interactions", "content": "The main advantage of our dataset and data acquisition method is to introduce training data from\na larger variety of interactions for downstream HME models. We study the effect of enhancing the\nrepresentation space one such model, a contact interaction prior from [36], with our dataset. Below\nwe detail the contact prior model, training process and results."}, {"title": "4.1 Close interaction NTU RGB+D 120 test set", "content": "For every sequence in the dataset's original test set we label the contact frames with a combination\nof the distance between the annotated 3D joints, 2D keypoints from an off-the shelf estimator, and\nmanual frame-level annotation. Then, we ensure the quality of the 3D joints both visually and by\ncalculating the error between the 2D keypoints and reprojected joints. The final test set comprises\n309 frames across 8 classes with a mean of 38.6 (SD: 16.3) frames per class."}, {"title": "4.2 Enhancing a contact prior with the APU dataset", "content": "Diffusion model. As a contact prior we adapt BUDDI [36], a diffusion model conditioned\non initial mesh estimates. During training the diffusion model gradually noises data samples\nto a point of randomness and then learns to reverse this process by denoising samples step-\nby-step until reaching a coherent structure. In particular, at each time step the noise level t\nis uniformly sampled with \\(\\epsilon_t \\sim N(0,I)\\) to obtain from a ground-truth sample \\(x_0\\) the noisy\nsample \\(x_t = \\sqrt{\\sigma_t} x_0 + \\sqrt{1 - \\sigma_t} \\epsilon_t\\) with \\(\\sigma_t = \\prod_{i=1}^{t}(1 - \\alpha_t)\\). BUDDI is trained to minimize\n\\(\\mathbb{E}_{x_0 \\sim P_{data}} \\mathbb{E}_{t \\sim U\\{0,T\\}, x_t \\sim q(\\cdot|x_0)} ||BUDDI (x_t; t, \\O) - x_0||^2\\). Specifically, an input sample \\(x_0\\) corresponds\nto the input SMPL-XA parameters \\(\\varphi_p, \\theta_p, \\beta_p, Y_p\\) for each person p. The loss to train the contact prior\nis \\(L_{prior} = \\lambda_{\\Theta} L_{\\Theta} + \\lambda_{\\beta} L_{\\beta} + \\lambda_Y L_Y + \\lambda_{v2v} L_{v2v}\\), where all terms are \\(L_2\\) losses w.r.t the parameters\nand \\(L_{v2v}\\) is a squared \\(L_2\\) loss on the vertices.\nInference with the contact prior. At test time, we perform a two-stage optimization process to\nobtain the mesh estimates \\(M^a\\) and \\(M^b\\) for the pair of people in an image similarly to section 3.3.\nHowever, we replace the contact map guidance with the trained contact prior. At each iteration we\ndiffuse and denoise the current estimate \\(x_0\\) with a noise level at t = 10. The denoised estimate \\(x_0\\)\nregularizes the current estimate \\(x_0\\) with an \\(L_2\\) loss \\(L_{diffusion} = ||x_0 - \\tilde{x_0}||\\). In practice, the decoded\nparameters are penalized directly by \\(L_{diffusion} = \\lambda_{\\Theta} ||\\Theta_0 - \\tilde{\\Theta}|| + \\lambda_{\\theta} ||\\theta_0 - \\tilde{\\theta}|| + \\lambda_{\\beta} ||\\beta_0 - \\tilde{\\beta}|| + \\lambda_Y ||Y_0 - \\tilde{Y}||\\).\nThe contact prior offers enough guidance that the GMM pose prior is not needed. Thus, the complete\nloss function for the optimization is \\(L_{optim} = \\lambda_J L_J + \\lambda_{\\Theta} L_{\\Theta} + \\lambda_P L_P + L_{diffusion}\\). Where \\(L_{\\tilde{\\Theta}}\\) is a\nprior to encourage the solution to be close to the denoised initialization."}, {"title": "5 Conclusion", "content": "In this paper, we address a key challenge in HME: data scarcity for new domains. We introduce a\nnovel data generation method for close interactions, leveraging automatic annotations to produce\npseudo-ground truth meshes from in-the-wild images. We curated APU, a diverse dataset of paired\nimages and pseudo-ground truth meshes, covering a wide range of close interaction types. We\ndemostrate that our data can be used to improve HME methods for close interactions, particularly for\nless common interaction scenarios, on a case study of the NTU RGB+D 120 dataset."}, {"title": "Limitations & Ethical Concerns", "content": "Close interactions in HME is an ongoing line of research. Our\nautomatic data generation method filters out many close interaction images even if they appear\nsuitable, yet if the 2D keypoints, initial mesh estimation, or automatic contact maps are not all\naccurate, the images can be excluded. As improvements in the models that produce each of these\ncomponents are made, the diversity of interactions will increase.\nDue to budget constraints, we query a single LVLM, GPT-4V [1], once per image to generate the\ncontacts for our dataset. This approach may replicate the existing biases of this model in our dataset.\nProducing multiple outputs per image and querying multiple LVLMs could provide a measure of\nuncertainty to the predicted contacts, which could be integrated into our soft contact maps for\nimproved robustness. We do not foresee significant risks of security threats or human rights violations\nin our work. However, the advancements in close interactions HME could be misused for creating\nmisleading visual content, leading to potential harm or deception."}, {"title": "Supplementary Material for Ask, Pose, Unite: Scaling Data Acquisition for Close Interactions with Vision Language Models", "content": "This supplementary material provides detailed information about the APU dataset, which is derived\nfrom our novel data generation method. Unlike traditional datasets compiled through direct data\ncollection or benchmarking, our dataset uses automatic annotations and pseudo-ground truth meshes\nposed from in-the-wild images. This document includes the dataset datasheet, prompting strategy\nand additional analysis, highlighting the unique aspects and methodology behind our work. We have\nincluded a code.zip file with our GitHub repository containing the code and instructions for accessing\nthe APU dataset."}, {"title": "B Dataset Datasheet", "content": "What data does each instance consist of? The raw data are the images from each data source.\nIs there a label or target associated with each instance? For each pair of people we provide the\nreconstructed meshes from our method."}, {"title": "C LVLM Prompts", "content": "Figure S5: In-context example from the TV Interactions dataset provided with the prompt.\nTo query the images in our dataset we use the prompt below."}]}