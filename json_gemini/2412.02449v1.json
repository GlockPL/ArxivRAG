{"title": "BYE: Build Your Encoder with One Sequence of Exploration Data for Long-Term Dynamic Scene Understanding", "authors": ["Chenguang Huang", "Shengchao Yan", "Wolfram Burgard"], "abstract": "Dynamic scene understanding remains a persistent challenge in robotic applications. Early dynamic mapping methods focused on mitigating the negative influence of short-term dynamic objects on camera motion estimation by masking or tracking specific categories, which often fall short in adapting to long-term scene changes. Recent efforts address object association in long-term dynamic environments using neural networks trained on synthetic datasets, but they still rely on predefined object shapes and categories. Other methods incorporate visual, geometric, or semantic heuristics for the association but often lack robustness. In this work, we introduce BYE, a class-agnostic, per-scene point cloud encoder that removes the need for predefined categories, shape priors, or extensive association datasets. Trained on only a single sequence of exploration data, BYE can efficiently perform object association in dynamically changing scenes. We further propose an ensembling scheme combining the semantic strengths of Vision Language Models (VLMs) with the scene-specific expertise of BYE, achieving a 7% improvement and a 95% success rate in object association tasks. Code and dataset are available at https://byencoder.github.io.", "sections": [{"title": "I. INTRODUCTION", "content": "INTERACTING with the physical environment is inherently dynamic. On one hand, numerous objects in our daily lives\u2014such as people, animals, vehicles, and machines\u2014are constantly in motion, requiring us to reason, react, avoid, maneuver, and operate while processing information instantly. On the other hand, beyond our immediate sight, the environment itself continues to evolve: objects are moved, drawers opened, screens turned off, and containers refilled. As we revisit or explore, we continuously associate new observations with past experiences, updating our knowledge base. This raises the question: can a robot similarly associate new observations with its existing knowledge base?\nDespite the recent spike of robot generalist policy research [1] which aims at training a reactive robot policy that handles dynamics through learning from a plethora of data, the majority of the robotic systems still rely on a knowledge base of the environment to operate within a certain scope. Such a knowledge base is usually a scene representation, namely, a map. Previous dynamic mapping techniques focused on"}, {"title": "II. RELATED WORK", "content": "Open-Vocabulary Semantic Mapping In recent years, the advancement of vision language models and their fine-tuned counterparts [13]\u2013[15] have substantially innovated the mapping techniques in different robotic applications such as navigation [16]\u2013[20], manipulation [21], and 3D semantic scene understanding [10], [22]. By integrating visual-language features into sparse topological nodes [16], [18], [20], dense 3D voxels or 2D pixels [17], [23], discrete 3D locations [24], or implicit neural representations [10], [22], [25], [26], those created maps can be used to retrieve concepts with natural language descriptions, extending closed-set semantics retrieval [5], [27] to open-vocabulary level and enabling more flexible and efficient human-robot interaction in the downstream tasks. However, most of the open-vocabulary semantic mapping approaches assumes a static environment, struggling to readjust the contents to scene changes in a long-term dynamic environment. In this work, we propose training a scene-wise point cloud encoder to extract class-agnostic, instance-level features stored in an object memory bank to manage future observations of dynamic scene changes.\nDynamic Scene Understanding\nUnderstanding dynamic environments remains a challenge due to constant scene motion, complicating tracking and mapping. Early methods used semantic segmentation masks to filter objects during SLAM optimization [2]\u2013[4], but these rely on static/dynamic category assumptions, which are often invalid (e.g., parked vs. moving cars). Other approaches leveraged optical flow for object motion tracking [28] or non-rigid tracking [29]. Recent advances in neural implicit representations, like Gaussian Splatting [11], enable rendering dynamic scenes over time [30] or simulating dynamics [31], but focus on limited spatial ranges.\nLong-term scene changes, central to this work, involve tasks like change detection and association, namely, tracking object displacement, addition, or removal across sequences. Prior methods registered and compared reconstructed scenes using visual, geometric, or semantic clues [6], [7], [32], or trained networks for object association and reconstruction [8], [33]. Others employed scene graphs [34] or probabilistic pipelines [35]. These methods often relied on large synthetic datasets or predefined categories. In contrast, our approach trains an encoder on observational data sequences, avoiding category priors and synthetic data, to effectively tackle long-term change associations in dynamic scenes.\nShape Representation Learning A key technique to enhance the understanding of 3D scenes or objects is 3D shape representation learning. Early approaches focusing on 3D semantic understanding learned global or point-wise representations of point clouds [36], [37], which nowadays serve as strong backbones for advanced methods. Other works learned implicit functions resembling traditional 3D shape representations like SDFs and occupancy grids with neural networks [38]. Stemming from the techniques above, other methods explore learning the neural descriptor fields that map spatial locations relative to a shape to latent features for shape completion [39], registration [40], manipulation [41], and object-level SLAM [42]. While the methods in those applications above have shown promising results, most of them require a curated 3D shape dataset containing a variety of complete shapes spanning different categories. In this work, our method only needs the observation data during the exploration of a scene to train a scene-wise encoder which can be used for object association in a long-term dynamic environment. By following the strategy introduced in SimCLR [43], we train an efficient encoder that attracts partial point cloud observations of the same object while repelling those of different instances."}, {"title": "III. PROBLEM DEFINITION", "content": "To formalize the problem, we define a long-term dynamic environment where object locations can change between exploration trials, but each object remains static during a single trial. At each time step of a trial, we assume access to RGB images $I_t$, depth images $D_t$, 2D instance masks $M_t = \\{M_{tk}\\}_{k=1}^K$, and camera poses $T_t$, from which we construct an instance-level map representing objects as independent point clouds $\\{P_i\\}_1^M$, where $P_i$ is the point cloud of the i-th object.\nThe problem of long-term object change detection and association is as follows: given two sets of point clouds, $\\{P_i^{ref}\\}_{i=1}^{M'}$ from a reference trial and $\\{P_i^{new}\\}_{i=1}^M$ from a new trial after object changes, find a bijective mapping $f : \\{1, ..., M'\\} \\rightarrow \\{1,..., M\\}$. If $f(j) = i$, $P_j^{new}$ and $P_i^{ref}$ correspond to the same object. Here, $M = M'$ after excluding added or removed objects."}, {"title": "IV. METHOD", "content": "This work aims to train a per-scene point cloud encoder that extracts latent features of partial object point cloud observations. With contrastive learning, we ensure that observations from the same object have similar embeddings while those from different objects are distinct from one another. By using the encoder to generate latent embeddings of all partial point cloud observations of all instances in the reference trial of the exploration, we can build a memory bank for the scene objects. Later, during the new trial of exploration after object relocations have happened, we can use the same encoder to get the latent embeddings of new point cloud observations, find nearest neighbors in the memory bank, and thus associate to an instance in the reference trial.\nThe idea of training a \"per-scene\" encoder is inspired by LangSplat [10] that trained an autoencoder mapping high dimensional CLIP features [13] for images collected in a single scene to low dimensional ones, which accelerates the optimization of CLIP-enriched Gaussian Splatting. In our work, we follow the idea of training a network only for data collected in a single scene during one trial of exploration which facilitates object change detection and association in long-term dynamic environments.\nIn the following, we introduce (i) the construction of an instance-level map which is the prerequisite of generating training data (Sec. IV-A), (ii) the generation of partial point cloud observation dataset (Sec. IV-B), (iii) the training method of a per-scene object point cloud encoder with contrastive learning (Sec. IV-C), (iv) the generation of memory bank (Sec. IV-D), and lastly (v) object association with the memory bank and the ensembling technique to further boost the object association performance (Sec. IV-E). The overview of the pipeline is shown in Fig 2."}, {"title": "A. Instance-Level Map Construction", "content": "We construct the instance-level map with the reference trial of exploration before object relocation. Given the instance segmentation masks $M_t = \\{M_{tk}\\}_{k=1,...,K}$, depth image $D_t$, and camera pose $T_t$ of each frame t in a trial of exploration, we can easily back-project the instance masks into 3D through the depth image, transform them to the global coordinate frame, and either fuse them with existing global instance point clouds with the instance IDs of those masks or initialize new global instances. After iterating the process in each frame, we can obtain a list of point clouds $\\{P_i\\}_{i=1,...,}$ with instance IDs $i = 1,..., M$ in this trial of exploration. For simplicity, we assume known instance masks and odometry as input to emphasize the effectiveness of our trained encoder and mitigate the impact of the quality of segmentation and odometry results. However, this can be easily extended with any existing instance-level mapping techniques such as ConceptGraphs [18], HOV-SG [20], and so on."}, {"title": "B. Partial Point Cloud Observation Data Generation", "content": "After obtaining the instance-level map $\\{P_i\\}_{i=1,...,M}$, for each instance point cloud $P_i$, we find all the instance masks used to generate it and back-project them into camera coordinate frame to get a list of partial point cloud observations of the instance $\\{P_i^{cam}\\}_{r=1,...,R_i}$ where $R_i$ is the total number of masks for object i during this trial of exploration. Each $P_i^{cam}$ contains a list of 6-dimensional vectors each storing the 3D position and the RGB values of a point. We then subtract each point cloud's 3D coordinate with its mean to obtain a zero-center point cloud $P_i^{ir}$ where $x_p^{ir} = x_p - \\frac{1}{|P|}\\sum_{x_p \\in P} x_p$ where $x_p \\in R^3$ and $\\overline{x_p} \\in R^3$. For each zero-center point cloud $P_i^{ir}$, we take its instance id i as label, forming one data sample as a tuple $(P_i^{ir}, i)$. For simplicity, we denote all point clouds and their object ID labels as $\\{(P_k,Y_k)\\}_{k=1,...,L}$ where $y_k$ is the object ID, and $L = \\sum_i^M R_i$ is the total masks number of all objects."}, {"title": "C. Training Scene-wise Object Point Cloud Encoder", "content": "The main goal of the encoder is to extract a latent representation for a point cloud in a scene so that the point clouds belonging to the same object have similar embeddings while the point clouds from different objects are far away from one"}, {"title": "D. Object Memory Bank Generation", "content": "After training the encoder for the scene with the reference trial of exploration data, you can encode all the partial point cloud observations $\\{P_l\\}_{l=1,...,L}$ in your dataset in Sec. IV-B into the latent embeddings $\\{h(P_l)\\}_{l=1,...,L}$ and associate those embeddings with their corresponding instance ID labels $\\{y_l\\}_{l=1,...,L}$, forming embedding-ID pairs $\\{(h_l^{ref}, y_l^{ref})\\}_{l=1,...,L}$ (for simplicity, we write $h(P_l)$ as $h_l^{ref}$). Since the embeddings and labels are for the reference exploration trial, we add a superscript of ref to their symbols. We treat these embeddings and labels as the object memory bank of the scene. Since the object embeddings are translation-invariant, we can easily look up the memory bank when new observations come after object location changes."}, {"title": "E. Object Association in Dynamic Environment", "content": "Retrieval from Object Memory Bank Now we switch to the new trial of exploration data, after a random number of object locations changes without adding new objects or removing old ones. Given the instance segmentation masks $M_t = \\{M_{tk}\\}_{k=1,...,K}$, depth image $D_t$, and camera pose $T_t$ of each frame t in a new trial of exploration, we can build a new instance-level map as in Sec. IV-A. In addition, for each mask observation $M_{tk}$, we can back-project them into the camera coordinate, center them at their means, and apply voxel-downsampling to a resolution of 0.01 meter to obtain partial point cloud observations $P_t^{new}$ as in Sec. IV-B. We can use the trained encoder (see Sec. IV-C) to generate a latent embedding $h(P_t^{new})$ (we write it as $h_t^{new}$ for sim-plicity) for each partial point cloud observation $P_t^{new}$. In the following step, we can find the distance of $h_t^{new}$ to all the embeddings $\\{h_l^{ref} \\}_{l=1,...,L}$ in the object memory bank created in Sec. IV-D. We take the 10 nearest embeddings and store their object ID labels in the reference trial. These reference labels are associated with the global object ID in the new trial in a dictionary $\\{\\text{new object ID} : \\text{reference object IDs}\\}$. Whenever there are new observations for the same new object ID later, the reference object IDs list will be extended with new labels. During this process, we can use frequency to approximate the probability of association such that $P(f(j) = i) = \\frac{\\frac{\\#(y^{ref}=i)}{\\sum_{t=1:n}}}{\\sum_{m=1}^{M} \\frac{\\#(y^{ref}=m)}{\\sum_{t=1:n}} }$ where f(\u00b7) is the mapping from new trial object ID j to reference trial object ID i, zt denotes the observations at timestep t, #(yref = i) represents the count of reference labels i associated with new object j, and M denotes the total number of objects in reference trial. To determine the final association for a new object ID, we simply retrieve the reference object ID with the highest probability.\nVLM Ensembling To further improve the object association success rate, we propose an ensembling scheme to benefit from both the semantic reasoning capability of Vision Language Models as well as the scene-specific expertise of BYE encoder. With the association dictionary $\\{\\text{new object ID} : \\text{reference object IDs}\\}$, we can construct a matrix storing normalized scores between each new object and reference object $A^{BYE}$ where $a_{ij}^{BYE}$ indicates the score of new object j associating with reference object i, which is $P(f(j) = i\\frac{\\frac{\\#(y^{ref}=i)}{\\sum_{t=1:n}}}{\\sum_{m=1}^{M} \\frac{\\#(y^{ref}=m)}{\\sum_{t=1:n}} })$ introduced above. To exploit the power of VLMs, we can use HOV-SG [20] to construct instance-level open-vocabulary maps for both the reference and the modified scenes. In general, HOV-SG first constructs a voxel map where each voxel stores the back-projected open-vocabulary visual features generated by Vision Language Models such as CLIP [13]. Then it creates a segment-level geometric map where each object is represented by an independent point cloud. To obtain an open-vocabulary feature for each object, we simply collect the nearest voxel features in the voxel map for all points in each object's point cloud and apply DBSCAN to select the major feature cluster. Finally, we take the feature closest to the cluster mean as the object's feature. By computing the cosine similarity between the features of new and old objects, we can also obtain a score matrix $A^{VLM}$. We ensemble their results by taking the sum of the two matrices $A = A^{BYE} + A^{VLM}$ and applying the Hungarian algorithm to get the association results."}, {"title": "V. EXPERIMENTAL RESULTS", "content": "In this section, we want to answer the following three ques-tions: (i) how powerful BYE is compared to the foundation-model-based encoder in object association tasks in long-term dynamic environments (Sec. V-A), (ii) how the object associ-ation ability of BYE generalizes to real-world environments across various scenes (Sec. V-B), and (iii) how efficient BYE is for potential robotics applications (Sec. V-C). We will address all these questions in the following subsections."}, {"title": "A. Object Association in Long-Term Dynamic Environments", "content": "Experiment Setup: To evaluate the association effective-ness of BYE, we collected 10 reference scenes (FloorPlan 1, 2, 4, 5, 6, which are kitchens, and FloorPlan 301-305, which are bedrooms) and 10 corresponding change scenes in AI2THOR [12] where only object locations are changed in the same scene. A robot was manually controlled to gather exploration data, including RGB images, depth images, instance masks, and camera poses. For the change scenes, we initialized them as reference scenes, randomly moved some objects, and then collected exploration data after the changes. For each reference scene, we built an instance-level map, generated a dataset of partial instance point clouds with labels, trained the encoder, and used the checkpoint with the best validation loss to create an object memory bank as in Sec. IV. During the association process, we iterate through all observations of the new exploration, extracting masked point clouds, back-projecting, centering them, and generating latent embeddings using the same checkpoint. We then found the 10 nearest neighbors in the object memory bank for each new observation, maintaining a count for each associated reference instance ID. This created a mapping from new instance IDs to reference IDs which evolves over time when more observations come. Finally, for pure BYE methods, namely BYE (DGCNN) and BYE (PointNet), we used majority voting to assign the most likely reference instance ID to each new instance. For ensemble methods BYE (DGCNN+CLIP) and BYE (PointNet+CLIP), we follow the VLM ensembling trick introduced in Sec. IV-E and the Hungarian algorithm to assign a reference instance ID to each object in the new exploration.\nBaselines: The goal of the baseline methods is to integrate semantic-rich visual language features into the segment-level map, namely, associate one feature with each instance. We exploit the open-vocabulary mapping scheme introduced in HOV-SG [20] as is introduced in the VLM ensembling part in Sec. IV-E. First, we build a voxel feature map. This requires a visual encoder that can generate dense pixel-level visual features. Here we use LSeg [47], OVSeg [14], DINOv2 [46] (ViT B/14), and CLIP [13] (ViT B/32) as the encoders in our baselines. We use DINOv2's last intermediate layer's patch-wise embeddings and upsample it to the image resolution with nearest-neighbor interpolation. CLIP originally only generates global image features but we can obtain small regions of a single image based on the instance masks and assign each"}, {"title": "B. Real-World Experiments", "content": "Experiment Setup: We evaluated BYE's object association performance in two real-world scenarios: a tabletop setting and a furniture setting, reflecting manipulation and navigation tasks. In the tabletop setting, 10 objects were arranged in four random layouts on a table. In the furniture setting, eight chairs were positioned across three rooms (meeting room, office, hallway) in five layouts, with variations such as objects placed on seat pads (e.g., paper, notebook, or empty). This setup, shown in Fig. 5, is particularly challenging due to the chairs' similar appearances. Using an Azure Kinect sensor, we recorded nine exploration trials, generating instance masks with SAM2 [49] and estimating camera poses via DROID-SLAM [50]. One trial per setting was used to train the scene-specific encoder, with the rest for evaluation. The total association task number is 62."}, {"title": "C. Runtime Analysis", "content": "Experiment Setup: We use a machine with an AMD Ryzen 9 PRO 7945 12-core CPU and an NVIDIA GeForce RTX 4060Ti GPU with 16 GB VRAM. We load the DGCNN and PointNet encoders in evaluation mode, and load the data with different batch sizes and a single CPU thread. Then we iterate through a dataset of each scene (41267 partial point cloud samples in total) and let the encoder generate latent embeddings in inference mode. We count the time for preprocessing (downsampling) and generating all embeddings with partial point cloud observations in the scene and divide the time by the total partial observations number to get the average runtime of the encoder."}, {"title": "VI. CONCLUSION", "content": "In this work, we presented a novel approach called BYE to the problem of object association in long-term dynamic scenes. By combining a per-scene encoder trained with scene-specific exploration data with foundation models, we achieve a near-perfect association success rate (95% in simulation and 100% in real-world scenarios) across dynamic scenes with high efficiency. We believe that our work highlights a promising pathway toward lifelong learning, namely customiz-ing foundation models by combining them with a lightweight scene-specific expert model. However, BYE faces limitations, including challenges in detecting newly added or removed objects in long-term dynamic environments and reliance on instance masks as supervision for training the per-scene en-coder. Future research will focus on integrating this encoder with instance-level open-vocabulary mapping techniques and deploying it in real-world robotic navigation systems. This integration has the potential to advance spatio-temporal open-vocabulary navigation and object search capabilities. Addi-tionally, extending the proposed approach to training scene-specific models using exploration data to other application domains presents a promising avenue for further investigation."}]}