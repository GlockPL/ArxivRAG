{"title": "UNPACKING SDXL TURBO: INTERPRETING TEXT-TO-IMAGE MODELS WITH SPARSE AUTOENCODERS", "authors": ["Viacheslav Surkov", "Chris Wendler", "Mikhail Terekhov", "Justin Deschenaux", "Robert West", "Caglar Gulcehre"], "abstract": "Sparse autoencoders (SAEs) have become a core ingredient in the reverse engineering of large-language models (LLMs). For LLMs, they have been shown to decompose intermediate representations that often are not interpretable directly into sparse sums of interpretable features, facilitating better control and subsequent analysis. However, similar analyses and approaches have been lacking for text-to-image models. We investigated the possibility of using SAEs to learn interpretable features for a few-step text-to-image diffusion models, such as SDXL Turbo. To this end, we train SAEs on the updates performed by transformer blocks within SDXL Turbo's denoising U-net. We find that their learned features are interpretable, causally influence the generation process, and reveal specialization among the blocks. In particular, we find one block that deals mainly with image composition, one that is mainly responsible for adding local details, and one for color, illumination, and style. Therefore, our work is an important first step towards better understanding the internals of generative text-to-image models like SDXL Turbo and showcases the potential of features learned by SAEs for the visual domain.", "sections": [{"title": "1 INTRODUCTION", "content": "Text-to-image generation is a rapidly evolving field. The DALL-E model first captured public interest (Ramesh et al., 2021), combining learned visual vocabularies with sequence modeling to produce high-quality images based on user input prompts. Today's best text-to-image models are largely based on text-conditioned diffusion models (Rombach et al., 2022; Saharia et al., 2022b; Podell et al., 2023; Sauer et al., 2023b; Betker et al., 2023; Pernias et al., 2023). This can be partially attributed to the stable training dynamics of diffusion models, which makes them easier to scale than previous approaches such as generative adversarial neural networks (Dhariwal & Nichol, 2021). As a result, they can be trained on internet scale image-text datasets like LAION-5B (Schuhmann et al., 2022a) and learn to generate photorealistic images from text.\nHowever, the underlying logic of the neural networks that enable the text-to-image pipelines we have today, due to their black-box nature, is not well understood. Unfortunately, this lack of interpretability is typical in the deep learning field. For example, advances in image recognition (Krizhevsky et al., 2012) and language modeling (Devlin, 2018; Brown, 2020) come mainly from scaling models (Hoffmann et al., 2022), rather than from an improved understanding of their internals. Recently, the emerging field of mechanistic interpretability has sought to alleviate this limitation by reverse engineering visual models (Olah et al., 2020) and transformer-based LLMs (Rai et al., 2024). At the same time, diffusion models have remained under-explored."}, {"title": "2 BACKGROUND", "content": null}, {"title": "2.1 SPARSE AUTOENCODERS", "content": "Let h(x) \u2208 Rd be an intermediate result during a forward pass of a neural network on input x. In a fully connected neural network, h(x) could correspond to a vector of neuron activations. In transformers, which are neural network architectures that combine attentions with fully connected layers and residual connections, h(x) could either refer to the content of the residual stream after a layer, an update to the residual stream by a layer, or a vector of neuron activations within a fully connected block."}, {"title": null, "content": "It has been shown (Yun et al., 2021; Cunningham et al., 2023; Bricken et al., 2023) that in many neural networks, especially LLMs, intermediate representations can be well approximated by sparse sums of nf \u2208 N learned feature vectors, i.e.,\n$$h(x) = \\sum_{p=1}^{n_f} s_p(x)f_\\rho,$$\nwhere sp(x) are the input-dependent coefficients, most of which are equal to zero and f\u2081,..., fnf \u2208 Rd is a learned dictionary of feature vectors. Importantly, the features are usually interpretable.\nSparse autoencoders. To implement the sparse decomposition from equation 1, the vector s containing the nf coefficients of the sparse sum, is parameterized by a single linear layer followed by ReLU activations, called the encoder,\n$$s = ENC(h) = \\sigma(W^{ENC}(h \u2013 b_{pre}) + b_{act}),$$\nin which h\u2208 Rd is the latent that we aim to decompose, \u03c3(\u00b7) = max(0,\u00b7), WENC \u2208 Rnf\u00d7d is a learnable weight matrix and bpre and bact are learnable bias terms. We omitted the dependencies h = h(x) and s = s(h), which are clear from the context.\nSimilarly, the learnable features are parametrized by a single linear layer called decoder,\n$$h' = DEC(s) = W^{DEC}s + b_{pre},$$\nin which WDEC (f1|\u00b7\u00b7\u00b7|fnf) \u2208 Rd\u00d7nf is a learnable matrix. Its columns take the role of learnable features and bpre is a learnable bias term."}, {"title": "2.2 FEW STEP DIFFUSION MODELS: SDXL TURBO", "content": "Diffusion models. Diffusion models (Sohl-Dickstein et al., 2015; Ramesh et al., 2022; Rombach et al., 2022; Saharia et al., 2022a) sample from an unknown distribution p by learning to iteratively denoise corrupted samples, starting from pure noise. The corruption process is defined on training samples from p. Mathematically, the images are corrupted with Gaussian noise and are distributed according to\n$$q_t(x_t|x_0) := \\mathcal{N}( \\sqrt{a_t} x_0, \\sigma^2_t I ),$$\nwhere xo corresponds to a real image from p, 0 \u2264 t \u2264 T, \u03b1\u2081, \u03c3\u03b5 are positive real-valued scalars such that the signal-to-noise ratio SNR := is monotonically decreasing, and, I is the identity matrix. Additionally, the coefficients \u03b1\u03c4\u22121,07\u22121 are typically chosen such that x\u2020 ~ N(0, I).\nThe denoising process is implemented through a learned distribution Po(Xt\u22121|Xt). The simplest way to generate samples using po (Xt\u22121|xt) is to first generate a sample of pure noise xf ~ N(0,I),\nfollowed by T iterative applications of po, which yields a sequence XT, XT-1, ..., X1, X0, where xo\napproximates a sample from p. The vector @ represents the parameters of the neural network that defines po(xt-1|xt). The denoising distribution po(xXt\u22121|xt) is parameterized to be Gaussian and a neural network is trained to optimize the parameters of this distribution.\nLatent diffusion. Originally, diffusion models operated directly on pixels (Ho et al., 2020; Song &\nErmon, 2020). However, training a denoising network in pixel space is computationally expensive (Hoogeboom et al., 2023). Thus, Rombach et al. (2022) use a pre-trained variational autoencoder to first compress images into latent representations and define a diffusion process in the latent space of the variational autoencoder instead. To make this difference clear, they write po(Zt\u22121|2t), in which now zt refers to a noisy latent instead of a noisy image.\nSDXL Turbo. To speed-up inference of latent diffusion models, Sauer et al. (2023b) distill a pre-trained model called Stable Diffusion XL (SDXL) (Podell et al., 2023). The distilled model is referred to as SDXL Turbo because it allows high-quality sampling in as little as 1-4 steps. In comparison, the original SDXL model is trained with a noise schedule of 1000 steps, but in practice, sampling with 20 to 50 steps still generates high-quality images."}, {"title": "Neural network architecture.", "content": "The denoising network of SDXL Turbo estimating po(Zt-1|2t) is implemented using a U-net similar to Rombach et al. (2022). The U-net is composed of a down-sampling path, a bottleneck, and an up-sampling path. Both the down-sampling and up-sampling paths are composed of 3 individual blocks. The individual block structure differs slightly, but both down- and up-sampling blocks consist of residual layers, with some blocks including cross-attention transformer layers while others do not. Finally, the bottleneck layer is also composed of attention and residual layers. Importantly, the text conditioning is achieved via cross-attention to text embeddings performed by 11 transformer blocks embedded in the down-, up-sampling path, and bottleneck. An architecture diagram displaying the relevant blocks can be found in App. G Fig. 4."}, {"title": "3 SPARSE AUTOENCODERS FOR SDXL TURBO", "content": "With the necessary definitions at hand, in this section we show a way to apply SAEs to SDXL Turbo. In the following, we assume that all SDXL Turbo generations are done using a 1-step process.\nWhere to apply the SAEs. We apply SAEs to updates performed within the cross-attention transformer blocks responsible for incorporating the text prompt (depicted in App. A Fig. 4). Each of these blocks consists of multiple transformer layers, which attend to all spatial locations (self-attention) and to the text prompt embeddings (cross-attention).\nFormally, the lth cross-attention transformer block updates its inputs in the following way\n$$D^{[l]}_{out} = D^{[l]}_{in} + TRANSFORMER^{[l]}(D^{[l]}_{in}, c)_{ij},$$,\nin which D[l]in, D[l]out \u2208 Rh\u00d7w\u00d7d denote the residual stream before and after application of the l-th cross-attention transformer block respectively. The transformer block itself calculates the function TRANSFORMER[l] : [Rhxwxd \u2192 Rhxwxd. Note that we omitted the dependence on input noise zt and text embedding c for both D[l]in (zt, c) and D[l]out (zt, c).\nWe train SAEs on the residual updates TRANSFORMER[l](D[l]in, c)ij \u2208 Rd denoted by\n$$\\Delta D^{[l]}_{ij} := TRANSFORMER^{[l]}(D^{[l]}_{in}, c)_{ij} = D^{[l]}_{out} \u2013 D^{[l]}_{in}.$$,\nThat is, we jointly train one encoder ENC[l] and decoder DEC[l] pair per transformer block l and share it over all spatial locations i, j. For notational convenience we omit block indices from now. We do this for the 4 (out of 11) transformer blocks (App. G Fig. 4) that we found have the highest impact on the generation (see App. A), namely, down.2.1,mid.0,up.0.0 and up.0.1.\nFeature maps. We refer to \u2206D \u2208 Rh\u00d7w\u00d7d as dense feature map and applying ENC to all image locations results in the sparse feature map S \u2208 [Rh\u00d7w\u00d7nf_with entries\n$$S_{ij} = ENC(\\Delta D_{ij}).$$,\nWe refer to the feature map of the pth learned feature using Sp \u2208 Rh\u00d7w. This feature map Sp\ncontains the spatial activations of the pth learned feature. Its associated feature vector f, \u2208 Rd is a column in the decoder matrix WDEC = (f1|\u06f0\u06f0\u06f0 |fnf) \u2208 Rdxnf. Using this notation, we can represent each element of the dense feature map as a sparse sum\n$$\\Delta D_{ij} \\sim \\sum_{\\rho=1}^{n_f} S_{ij}^{\\rho} f_{\\rho}, \\text{ with } S_{ij}^{\\rho} \\geq 0 \\text{ for most } \\rho \\in \\{1, ..., n_f\\}.$$,\nTraining. In order to train an SAE for a transformer block, we collected dense feature maps ADij from SDXL Turbo one-step generations on 1.5M prompts from the LAION-COCO (Schuhmann et al., 2022b). Each feature map has dimensions of 16 \u00d7 16, resulting in a training dataset of 384M dense feature vectors per transformer block. For the SAE training process, we followed the methodology described in (Gao et al., 2024), using the TopK activation function and an auxiliary loss to handle dead features. For more details on the SAE training, see App. F and for training metrics see App. B."}, {"title": "4 QUALITATIVE ANALYSIS OF THE TRANSFORMER BLOCKS", "content": "We perform a visual qualitative analysis to gain deeper insight into the behavior and characteristics of the learned features across transformer blocks. First, we introduce feature visualization techniques and then use them to conduct two case studies. For the sake of simplicity in the notation, we omit the transformer block index l."}, {"title": "4.1 FEATURE VISUALIZATION TECHNIQUES", "content": "We start by introducing necessary notation and formally describing the methods used for feature visualization.\nSpatial activations. We visualize a sparse feature map Sp \u2208 Rh\u00d7w containing activations of a feature p across the spatial locations by up-scaling it to the size of the generated images and overlaying it as a heatmap over the generated images. In the heatmap, red indicates the highest activation of features, and blue represents the lowest non-zero activation.\nTop dataset examples. For a given feature p, we sort dataset examples according to their average spatial activation\n$$\\alpha_{\\rho} = \\frac{1}{h \\cdot w} \\sum_{i=1}^{h} \\sum_{j=1}^{w} S_{ij}^{\\rho} \\in \\mathbb{R}.$$,\nWe use equation 9 to define the top dataset examples and to sample from the top 5% quantile of the activating examples (ap > 0). We will refer to them as top 5% images for a feature p.\nNote that S always depends on an embedding of the input prompt c and input noise 21, via Sij (C, 21) = ENC(\u2206Dij (c, z1)), which we usually omit for ease of notation. As a result, ap also depends on c and 21. When we refer to the top dataset examples, we mean our (c, z\u2081) pairs with the largest values for ap(c, 21).\nActivation modulation. We design interventions that allow us to modulate the strength of the oth feature. Specifically, we achieve this by adding or subtracting a multiple of the feature p on all of the spatial locations i, j proportional to its original activation S\n$$\\Delta D'_{ij} = \\Delta D_{ij} + \\beta S_{ij}^{\\rho} f_{\\rho},$$\nin which \u2206Dij is the update performed by the transformer block before and AD; after the intervention, \u03b2\u2208 R is a modulation factor, and f, is the pth learned feature vector. In the following, we will refer to this intervention as activation modulation intervention.\nActivation on empty context. Another way of visualizing the causal effect of features is to activate them while doing a forward pass on the empty prompt c(\"\"). To do so, we turn off all other features at the transformer block l of intervention and turn on the target feature p. Formally, we modify the forward pass by setting\n$$D^{out'}_{ij} = D^{in}_{ij} + \\gamma \\kappa_{\\rho} f_{\\rho},$$,\nin which Dout' replaces residual stream plus transformer block update, Din is the input to the block, f is the pth learned feature vector, \u03b3\u2208 R is a hyperparameter to adjust the intervention strength, and up is a feature-dependent multiplier obtained by taking the average activation across positive activations of p (collected over a subset of 50.000 dataset examples). Multiplying it by k aims to recover the coefficients lost by setting the other features to zero. Further in the text, we will refer to this intervention as empty-prompt intervention, and the images generated using this method with y set to 1, as empty-prompt intervention images.\nNote that we directly added/subtracted feature vectors to the dense vectors for both intervention types instead of encoding, manipulating sparse features, and decoding. This approach helps mitigate side effects caused due to reconstruction loss (see App. B)."}, {"title": "4.2 CASE STUDY I: MOST ACTIVE FEATURES ON A PROMPT", "content": "Combining the feature visualization techniques in Fig. 1, we depict the features with the highest average activation when processing the prompt: \u201cA cinematic shot of a professor sloth wearing a tuxedo at a BBQ party\". We present analysis of the transformer blocks in order of decreasing interpretability. An extended version of this case study demonstrating top 9 features per transfromer block instead of 5, is available in App. C Fig. 5.\nDown.2.1. The down.2.1 transformer block appears to contribute to the image composition. Several features relate to the prompt: 4539 \"sloth\", 4751 \u201ca tuxedo\", 2881 \"party\".\nActivation modulation interventions with negative \u1e9e (A. -6.0 columns) result in removing or changing scene objects in ways that align with the heatmap (hmap column) and the top examples (C columns): 1674 removes the light chains in the back, 4608 the umbrellas/tents, 4539 the 3D animation-like sloth face, and, 4751 changes the type of suit. Similarly, enhancing the same features (A. 6.0 column) makes the corresponding elements more visible and distinct.\nNotably, activating the features on the empty prompt often creates meaningful images with related elements (B. column). For reference, with the fixed random seed we use, the empty prompt generation without interventions resembles a painting of a piece of nature with a lot of green and brown tones."}, {"title": "Up.0.1.", "content": "Based, on our observations, the features of up. 0.1 appear to contribute to the style.\nInterestingly, turning on the up.0.1 features on the empty prompt (B. column) results in texture-like images. Furthermore, when activating them locally (A. columns), their contribution to the output is highly localized, and most of inactive image area remains unchanged. For the up. 0.1 we find it remarkable that often the features' ablations and amplifications are counterparts: 500 (light, shadow), 2727 (shadow, light), 3936 (blue, orange)."}, {"title": "Up.0.0.", "content": "For the third transformer block, up. 0.0, we observe that most top dataset examples and their activations (C columns) are quite interpretable: 3603 corresponds to party decorations, 5005 to the upper part of a tent, 775 to buttons on a suit, 153 to the lower animal jaw, 1550 to collars. All the features exhibit an expected causal effect on the generation when ablated or enhanced (A. columns).\nThe activation regions of the features often are very concentrated. Similarly to up. 0.1, activation modulation interventions leave inactive image regions mostly unaffected. For the empty prompt, activating these features produces abstract-looking images that are hard to relate to the other columns. Thus, we excluded this visualization technique and instead added one more dataset example. In summary, the learned features of this transformer block primarily add local details to the generation and, importantly, they are effective only within a suitable context."}, {"title": "Mid.0.", "content": "The specific role of the fourth block (mid.0) is not well understood. We find it more difficult to interpret because most interventions in the mid. 0 block have subtle effects. We did not include empty-prompt intervention results because they barely affect the generation.\nDespite these subtle effects, dataset examples (C. columns) and heatmaps (hmap column) mostly agree with each other and are specific enough to be interpretable: 4755 activates on bottom right part of faces, 4235 on left part of (animal) faces, 1388 on people in the background, and, 5102 on outlines the left border of the main object in the scene. We hypothesize that mid.0's features are more abstract, potentially encoding spatial location and relations between objects."}, {"title": "4.3 CASE STUDY II: RANDOM FEATURES", "content": "In this case study, we explore the learned features independently of any specific prompt. In App. D Fig. 6 and Fig. 7, we demonstrate the first 5 and last 5 learned features for each transformer block. In addition, we provide similar visualisations for the first 100 features of each layer in the supplementary material. As SAEs are randomly initialized before the training process, these sets can be considered as random samples of features. Each feature visualization consists of 3 images of top 5% images for this feature, and their perturbations with activation modulation interventions. For down.2.1 and up. 0. 1, we also include the empty-prompt intervention images. Additionally, we provide visualizations of several selected features in App. D Fig. 8 and demonstrate the effects of their forced activation on unrelated prompts in App. D Fig. 9.\nOverall, our insights gained from Case Study I (Sec. 4.2) appear to generalize on random feature samples. In particular, this suggests that a significant portion of the learned features are interpretable. Additionally, when studying features in isolation, it becomes apparent that distinctions between the blocks are blurred. For example, some down.2.1 features correspond to elements of style, e.g., a anime style and a cartoon style feature in App. D Fig. 8. Likewise, some up. 0.1 features don't only change style but also add and remove elements of the scene, e.g., dog eyes (App. D Fig. 6 up. 0.1 feature 3)."}, {"title": "5 QUANTITATIVE EVALUATION OF THE LEARNED FEATURES", "content": "In this section, we follow up on qualitative insights by collecting quantitative evidence."}, {"title": "5.1 ANNOTATION PIPELINE", "content": "Feature annotation with an LLM followed by further evaluation is a common way to assess feature properties such as specificity, sensitivity, and causality (Caden et al., 2024). We found it applicable to the features learned by the down.2.1 transformer block, which have a strong effect on the generation. Thus, they are amendable to automatic annotation using visual language models (VLMs) such as GPT-40 (OpenAI, 2024). In contrast, for the features of other blocks with more subtle effects, we found VLM-generated captions to be unsatisfactory. In order to caption the features of down.2.1, we prompt GPT-40 with a sequence of 14 images. The first five images are irrelevant to the feature (i.e., the feature was inactive during the generation of the images), followed by a progression of 4 images with increasing average activation values, and finished by five images with the highest average activation values. The last nine images are provided alongside their so-called \u201ccoldmaps\": a version of an image with weakly active and inactive regions being faded and concealed. The prompt template and examples of the captions can be found in App. E.\""}, {"title": "5.2 EXPERIMENTAL DETAILS", "content": "We perform a series of experiments in order to get statistical insights into the features learned. We will report the majority of the experimental scores in the format M(S). When the score is reported in the context of a SDXL Turbo transformer block, it means that we computed the score for each feature of the block and set M and S to mean and standard deviation across the feature scores. For the baselines, we calculate the mean and standard deviation across the scores of a 100-element sample."}, {"title": "Interpretability.", "content": "Features are usually considered interpretable if they are sufficiently specific, i.e., images exhibiting the feature share some commonality. In order to measure this property, we compute the similarity between images on which the feature is active. High similarity between these images is a proxy for high specificity. For each feature, we collect 10 random images among top 5% images for this feature and calculate their average pairwise CLIP similarity (Radford et al., 2021; Cherti et al., 2023). This value reflects how semantically similar the contexts are in which the feature is most active. We display the results in the first column of Table 1 (a), which shows that the CLIP similarity between images with the feature active is significantly higher then the random baseline (CLIP similarity between random images) for all transformer blocks. This suggests that the generated images share similarities when a feature is active.\nFor down.2.1 we compute an additional interpretability score by comparing how well the generated annotations align with the top 5% images. The resulting CLIP similarity score is 0.21 (0.03) and significantly higher then the random baseline (average CLIP similarity with random images) 0.12 (0.02). To obtain an upper bound on this score we also compute the CLIP similarity to an image generated from the feature annotation, which is 0.25 (0.03).\nCausality. We can use the feature annotations to measure a feature's causal strength by comparing the empty prompt intervention images with the caption. The CLIP similarity between intervention images and feature caption is 0.19 (0.04) and almost matches the annotation-based interpretability score of 0.21 (0.03). This suggests that feature annotations effectively describe to the corresponding"}, {"title": "Sensitivity.", "content": "A feature is considered sensitive when activated in its relevant context. As a proxy for the context, we have chosen the feature annotations obtained with the auto-annotation pipeline. For each learned feature, we collected the 100 prompts from a 1.5M sample of LAION-COCO with the highest sentence similarity based on sentence transformer embeddings of all-MiniLM-L6-v2 (Reimers & Gurevych, 2019). Next, we run SDXL Turbo on these prompts and count the proportion of generated images in which the feature is active on more than 0%, 10%, 30% of the image area, resulting in 0.60 (0.32), 0.40 (0.34), 0.27 (0.30) respectively, which is much higher than the random baseline, which is at 0.06 (0.09), 0.003 (0.006), 0.001 (0.003). However, the average scores are <1 and thus not perfect. This may be caused by incorrect or imprecise annotations for subtle features and, therefore, hard to annotate with a VLM and SDXL Turbo failing to comply with some prompts.\nRelatedness to texture. In Fig. 1 and App. D Fig. 6 the empty prompt interventions of the up.0.1 features resulted in texture-like pictures. To quantify whether this consistently happens, we design a simple texture score by computing CLIP similarity between an image and the word \"texture\". Using this score, we compare empty-prompt interventions of the different transformer blocks with each other and real-world texture images. The results are in the second column of Table 1 (a) and suggest that empty-prompt intervention images of up.0.1 and up. 0.0 resemble textures and some of the down.2.1 images look like textures as well. For up. 0.0, we did not observe any connection of these images to the top activating images. Interestingly, the score of up. 0.1 is higher than the one of the real-world textures dataset (Cimpoi et al. (2014)).\nColor sensitivity. In our qualitative analysis, we suggested that the features learned on up.0.1 relate to texture and color. If this holds, the image regions that activate a feature should not differ significantly in color on average. To test that, we calculate the \"average\" color for each feature: this is a weighted average of pixel colors with the feature activation values as weights. To determine the average color of a each feature we compute it over a sample of 10 images of the feature's top 5% images. Then, we calculate Manhattan distances between the colors of the pixels and the \"average\" color on the same images (the highest possible distance is 3.255 = 765). Finally, we take a weighted average of the Manhattan distances using the same weights. We report these distances for different transformer blocks and for the images generated on random prompts from LAION-COCO. We present the results in the third column of Table 1 (a). The average distance for the up.0.1 transformer block is, in fact, the lowest.\nIntervention locality. We suggested that the features learned on up.0.0 and up. 0.1 influence intervened generations locally. We estimate how the top 5% images change inside and outside the active regions to quantitatively assess this claim. To exclude weak activation regions from consideration, we say that a pixel is inside the active area if the corresponding 32x32 patch has an activation value larger than 50% of the image patches, and it is outside the active area if the corresponding 32x32 patch has activation value of zero. In Table 1 (b), we report Manhattan distances between the original images and the intervened images outside and inside the active areas for activation modulation intervention strengths -10, -5, 5, 10. The features for up.0.0 and up.0.1 have a higher effect inside the active area than outside, in contrast to down.2.1 for which this difference is smaller."}, {"title": "6 RELATED WORK", "content": "Image editing with diffusion models. Numerous studies have analyzed diffusion models' attribute editing capabilities. Yue et al. (2024) learn a disentangled representation on human faces (Liu et al., 2015; Karras et al., 2019) and bedrooms (Yu et al., 2015). The models of Yue et al. (2024) exhibit interpolation abilities between the attributes of two reference images. Similarly, Wang & Golland (2023) interpolate between pairs of images by fine-tuning a latent diffusion model. Through experiments on synthetic and real datasets, Deschenaux et al. (2024) demonstrate that even if a diffusion model is trained on an incomplete subset of the data distribution, it can still generate samples from the full distribution. Kim et al. (2022) show that one can guide a pre-trained diffusion model using text instructions leveraging CLIP (Radford et al., 2021). Meng et al. (2022) edit images using an off-the-shelf diffusion model that was not explicitly trained for image editing using the stochastic differential equation formalism of diffusion models. For example, the method of Meng et al. (2022)"}, {"title": "Analyzing the latent space of diffusion models.", "content": "Kwon et al. (2023) show that diffusion models naturally have a semantically meaningful latent space. Park et al. (2023) analyze the latent space of diffusion models using Riemannian geometry. Li et al. (2024) and Dalva & Yanardag (2024) present self-supervised methods for finding semantic directions in the latent space. Similarly, Gandikota et al. (2023) show that the attribute variations lie in a low-rank space by learning LoRA adapters (Hu et al., 2021) on top of pre-trained diffusion models. Brack et al. (2023) and Wang et al. (2023) demonstrate effective semantic vector algebraic operations in the latent space of DMs, as observed by Mikolov et al. (2013). However, none of those works explicitly train SAEs to interpret and control the latent space."}, {"title": "Mechanistic interpretability using SAEs.", "content": "Sparse autoencoders have recently been popularized by Bricken et al. (2023), in which they show that it is possible to learn interpretable features by decomposing neuron activations in MLPs in 2-layer transformer language models. At the same time, a parallel work decomposed the elements of the residual stream (Cunningham et al., 2023), which followed up on (Sharkey et al., 2022). To our knowledge, the first work that applied sparse autoencoders to transformer-based LLM was (Yun et al., 2021), which learned a joint dictionary for features of all layers. Recently, sparse autoencoders have gained much traction, and many have been trained even on state-of-the-art LLMs (Gao et al., 2024; Templeton & et al., 2024; Lieberum et al., 2024). In addition, great tools are available for inspection (Lin & Bloom, 2023) and automatic interpretation (Caden et al., 2024) of learned features. Marks et al. (2024) have shown how to use SAE features to facilitate automatic circuit discovery.\nThe studies most closely related to our work are (Bau et al., 2019), (Ismail et al., 2023) and (Daujotas, 2024). Ismail et al. (2023) apply concept bottleneck methods (Koh et al., 2020) that decompose latent concepts into vectors of interpretable concepts to generative image models, including diffusion models. Unlike the SAEs that we train, this method requires labeled concept data. Daujotas (2024) decomposes CLIP (Radford et al., 2021; Cherti et al., 2023) vision embeddings using SAEs and use them for conditional image generation with a diffusion model called Kandinsky (Razzhigaev et al., 2023). Importantly, using SAE features, they are able to manipulate the image generation process in interpretable ways. In contrast, in our work, we train SAEs on intermediate representations of the forward pass of SDXL Turbo. Consequently, we can interpret and manipulate SDXL Turbo's forward pass on a finer granularity, e.g., by intervening on specific transformer blocks and spatial positions. Another closely related work to ours is (Bau et al., 2019), in which neurons in generative adversarial neural networks are interpreted and manipulated. The interventions in (Bau et al., 2019) are similar to ours, but on neurons instead of sparse features. In order to identify neurons corresponding to a semantic concept, Bau et al. (2019) require semantic image segmentation maps."}, {"title": "7 CONCLUSION AND DISCUSSION", "content": "We trained SAEs on SDXL Turbo's opaque intermediate representations. This study is the first in the academic literature to mechanistically interpret the intermediate representations of a modern text-to-image model. Our findings demonstrate that SAEs can extract interpretable features and have a significant causal effect on the generated images. Importantly, the learned features provide insights into SDXL Turbo's forward pass, revealing that transformer blocks fulfill specific and varying roles in the generation process. In particular, our results clarify the functions of down.2.1,up.0.0, and up. 0.1. However, the role of mid.0 remains less defined; it"}]}