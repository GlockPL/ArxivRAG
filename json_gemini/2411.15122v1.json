{"title": "ReXrank: A Public Leaderboard for AI-Powered Radiology Report Generation", "authors": ["Xiaoman Zhang", "Hong-Yu Zhou", "Xiaoli Yang", "Oishi Banerjee", "Juli\u00e1n N. Acosta", "Josh Miller", "Ouwen Huang", "Pranav Rajpurkar"], "abstract": "AI-driven models have demonstrated significant potential in automating radiology report generation for chest X-rays. However, there is no standardized benchmark for objectively evaluating their performance. To address this, we present ReXrank (https://rexrank.ai), a public leaderboard and challenge for assessing AI-powered radiology report generation. Our framework incorporates ReXGradient, the largest test dataset consisting of 10,000 studies, and three public datasets (MIMIC-CXR, IU-Xray, CheXpert Plus) for report generation assessment. ReXrank employs 8 evaluation metrics and separately assesses models capable of generating only findings sections and those providing both findings and impressions sections. By providing this standardized evaluation framework, ReXrank enables meaningful comparisons of model performance and offers crucial insights into their robustness across diverse clinical settings. Beyond its current focus on chest X-rays, ReXrank's framework sets the stage for comprehensive evaluation of automated reporting across the full spectrum of medical imaging.", "sections": [{"title": "1 Introduction", "content": "Writing accurate radiology reports from medical images is a critical but complex task, requiring both deep expertise in medical imaging and the ability to accurately interpret and articulate intricate findings. The demand for such reports has surged with the rapid advancements in imaging technologies, leading to increased workloads for radiologists, risks of information loss, and longer report turnaround times [3].\nAI-driven solutions have emerged as a potential answer to these challenges, serving as assistive tools to enhance reporting efficiency and ensure access to high-quality, specialty-level interpretations. Medical visual-language models have shown promise in automating the generation of radiology reports from chest X-ray images [7, 19, 5, 14, 30]. However, as the field of AI-assisted medical reporting rapidly evolves, there is a growing need for standardized benchmarks to objectively assess and compare the performance of these models. Existing datasets for chest X-ray report generation, such as MIMIC-CXR [12], are valuable but exhibit limitations that hinder their effectiveness for benchmarking. These datasets frequently suffer from inconsistent data splits and a lack of standardized metrics during evaluation, which impedes reliable comparative analysis across different model architectures. Furthermore, the data distribution in MIMIC-CXR, commonly used in model training, fails to adequately test the models' ability to generalize to new, unseen distributions. To fill this gap, we introduce ReXrank (https://rexrank.ai), a public leaderboard and challenge specifically designed for evaluating AI-powered radiology report generation from chest X-ray images.\nReXrank offers a comprehensive evaluation framework that sets a standardized benchmark for assessing the effectiveness of different radiology report generation models. To ensure robust and clinically relevant evaluations, it integrates diverse datasets, including MIMIC-CXR [12], IU-Xray [8], CheXpert Plus [4], and ReXGradient, a large-scale private dataset of 10,000 studies. This broad dataset spectrum allows us to evaluate model performance on data with varying distributions, providing deeper insights into the models' generalization capabilities. Furthermore, ReXrank implements various report evaluation metrics, including BLEU-2 [18], BERTScore [28], SembScore [20], RadGraph-F1 [26], RadCliQ [26], RaTEScore [29], GREEN [17], FineRadScore [10], etc., to offer a detailed view of each model's strengths and weaknesses. This comprehensive approach enables a more nuanced understanding of model performance and facilitates meaningful comparisons between different AI-powered radiology report generation systems."}, {"title": "2 Overview", "content": "Datasets. ReXrank leverages three public datasets and one comprehensive private dataset for report generation assessment. The private ReXGradient dataset comprises 10,000 studies from 67 U.S. medical sites, making it one of the largest and most geographically diverse evaluation sets. For public datasets, we utilize the official test splits of MIMIC-CXR (2,347 studies) and IU-Xray (590 studies), along with CheXpert Plus's validation set (200 studies) as no test split is available.\nModels. ReXrank currently includes 16 report generation models from 10 different institutions, including BiomedGPT_IU [27], CheXagent [7], CheXpertPlus_CheX [4], CheXpertPlus_CheX_MIMIC [4], CheXpertPlus_MIMIC [4], Cvt2distilgpt2_IU [16], Cvt2distilgpt2_MIMIC [16], GPT4V [25], LLM-CXR [14], MAIRA-2 [2], MedVersa [30], RadFM [22], RaDialog [19], RGRG [21], VLCI_IU [5] and VLCI_MIMIC [5]. These models were trained on different medical datasets, primarily MIMIC-CXR, CheXpert Plus, and IU-Xray, with some models capable of handling multiple tasks beyond just report generation.\nMetrics. ReXrank employs 8 different metrics to comprehensively assess the quality of generated radiology reports, including traditional text generation metrics like BLEU-2 [18] and BERTScore [28], as well as domain-specific metrics designed for radiology report evaluation such as SembScore [20], RadGraph-F1 [26], RadCliQ-v1 [26] and RaTEScore [29]. The framework also incorporates recently developed LLM-based metrics including GREEN [17], and FineRadScore [10], which focus on identifying clinically significant errors. Each metric evaluates different aspects of the generated reports, from textual similarity to clinical accuracy, providing a thorough assessment of model performance. We default use RadCliQ-v1 as the primary metric.\nResults. MedVersa emerges as one of the top-performing models (Figure 2), with best 1/RadCliQ-v1 scores of 0.98 \u00b1 0.05 on ReXGradient and 0.92 \u00b1 0.02 on MIMIC-CXR. However, its performance on the CheXpert Plus dataset is comparatively lower, ranking fourth with a 1/RadCliQ-v1 score of 0.72 \u00b1 0.10 on the Findings. MedVersa consistently outperforms GPT4V, the state-of-the-art generalist vision-language model, across multiple metrics and datasets. We further analyze the distribution of evaluation metrics across datasets. Among the four datasets, IU X-ray stands out as the least challenging, consistently yielding high performance"}, {"title": "3 Method", "content": "3.1 Datasets\nOur evaluation leverages four distinct datasets: ReXGradient, MIMIC-CXR, IU-Xray, and CheXpert Plus. These datasets provide diverse testing distributions across different medical institutions and patient populations.\nReXGradient. This private test set is provided by Gradient Health, which consists of 10,000 studies collected from 7,004 patients across 67 medical sites in the United States.\nMIMIC-CXR [12]. This is a large, publicly accessible dataset comprising 377,110 chest X-rays (CXRs) corresponding to 227,835 radiographic studies performed at the Beth Israel Deaconess Medical Center in Boston, MA. For this dataset, we extracted sections of indication, comparison, findings, and impression via keyword matching. In our experiments, we follow MIMIC-CXR's official split and report scores on the test set, which consists of 2,347 studies.\nIU-Xray [8]. This is a publicly accessible dataset containing 7,470 pairs of CXRs and radiology reports. Each study in this dataset includes one frontal and one lateral CXR, associated with a single radiology report. We follow the split provided by R2Gen [6] and report scores on the test set, which comprises 590 studies.\nCheXpert Plus [4]. This is a large, publicly accessible consisting of 223,462 unique pairs of radiology reports and chest X-rays. These correspond to 187,711 radiographic studies from 64,725 patients. We follow the official split of CheXpert Plus and report scores on the validation set, which contains 200 studies.\n3.2 Data format\nFor each study in our test datasets, the data is organized in a structured format.\n\u2022 id: Unique identifier for the study\n\u2022 image_path: List of paths to all relevant chest X-ray images\n\u2022 frontal_lateral: List indicating the view type of each image\n\u2022 key_image_path: Path to the primary image (typically frontal view)\n\u2022 context: Patient information and clinical context\n\u2022 report: Radiologist's findings and impressions\n3.3 Metrics\nBLEU-2 [18]. BLEU (Bilingual Evaluation Understudy) is a widely used metric in machine translation and text generation tasks. It evaluates the quality of generated text by comparing n-gram precision between the candidate and reference texts, with scores ranging from 0 to 1. In this work, we specifically use BLEU-2, which focuses on bigram precision to assess the quality of the generated text.\nBERTScore [28]. BERTScore is a neural metric that uses pre-trained BERT models [13] to evaluate text similarity. It computes cosine similarity between the BERT embeddings of model-generated and groundtruth radiology reports.\nSembScore [20]. SembScore (CheXbert labeler vector similarity) is a domain-specific metric for radiology report evaluation. It computes the cosine similarity between the indicator vectors of 14 pathologies that the CheXbert automatic labeler extracts from model-generated and groundtruth radiology reports.\nRadGraph-F1 [26]. RadGraph-F1 is a metric for radiology report evaluation. It computes the overlap in clinical entities and relations that RadGraph [11] extracts from candidate and reference reports.\n1/RadCliQ-v1 [26]. RadCliQ is a composite metric designed for evaluating radiology report generation, combining BLEU, BERTScore, SembScore, and RadGraph-F1 to provide a comprehensive assessment of generated reports. For our evaluation, we utilized the official implementation\u00b9, which also includes BLEU, BERTScore, SembScore, RadGraph-F1. While the original RadCliQ metric is designed as lower-is-better, we first calculate the average RadCliQ-v1 score for each model across the dataset, then take its reciprocal (1/RadCliQ-v1) to maintain consistency with other metrics where higher values indicate better performance.\nRaTEScore [29]. RaTEScore is an entity-aware metric for radiology report evaluation. It emphasizes crucial medical entities like diagnostic outcomes and anatomical details and is robust against complex medical synonyms and sensitive to negation expressions. For our evaluation, we utilized the official implementation\u00b2.\nGREEN [17]. GREEN (Generative Radiology Report Evaluation and Error Notation) is an LLM-based metric for evaluating radiology report generation. It leverages language models to identify and explain clinically significant errors in quantitative and qualitative candidate reports. We utilized the official implementation\u00b3"}, {"title": "3.4 Confidence Intervals", "content": "In our analysis, we generate confidence intervals (CIs) by assuming a normal distribution of data. This statistical method calculates the mean and standard deviation of our data and then uses the standard error of the mean to estimate variability. For a 95% confidence level, a Z-score of approximately 1.96 is used to determine the interval. This Z-score indicates that the true mean is likely within 1.96 standard errors of the sample mean. By multiplying the Z-score by the standard error, we obtain the CI, providing a range that encapsulates the true average value with 95% certainty."}, {"title": "3.5 Participating Models", "content": "BiomedGPT__IU [27]. BiomedGPT is a lightweight, open-source vision-language model designed for diverse biomedical tasks across modalities. The model was fine-tuned for VQA and image captioning tasks using multiple datasets, including radiology and pathology data. BiomedGPT_IU is fine-tuned on the IU X-ray dataset for image captioning tasks. In our evaluation, we used the publicly available checkpoints trained on the IU-Xray dataset\u2075.\nCheXagent [7]. CheXagent is an instruction-tuned foundation model specifically designed for chest X-ray interpretation. The model consists of a vision encoder for representing CXR images, and a network to bridge the vision and language modalities. This model is trained on CheXinstruct, a large-scale instruction-tuning dataset curated from 28 publicly-available datasets. For our evaluation, we utilized the publicly available 8 billion parameter checkpoint from Hugging Face\u2076.\nCheXpertPlus_CheX [4]. CheXpertPlus_CheX, introduced in the CheXpert Plus paper, utilizes a Swinv2 [15] architecture with a two-layer BERT decoder [13] for medical report generation. CheXpert-Plus_CheX is trained exclusively on the CheXpert Plus dataset. In our evaluation, we utilized the publicly available Findings Checkpoint and Impression Checkpoints. Our evaluation employs these models sequentially, generating the findings and impression sections separately, and then combining them with appropriate headers to form the complete report.\nCheXpertPlus_CheX_MIMIC [4]. CheXpertPlus_CheX_MIMIC shares the same architectural design as CheXpertPlus_MIMIC, employing the Swinv2 architecture with a two-layer BERT decoder. CheXpert-Plus_CheX is trained exclusively on the combination of MIMIC-CXR and CheXpert Plus dataset. In our evaluation, we utilized the publicly available Findings Checkpoint and Impression Checkpoint\u00b9\u2070. Our evaluation employs these models sequentially, generating the findings and impression sections separately, and then combining them with appropriate headers to form the complete report.\nCheXpertPlus_MIMIC [4]. CheXpertPlus_MIMIC shares the same architectural design as CheXpert-Plus_CheX, employing the Swinv2 architecture with a two-layer BERT decoder. CheXpertPlus_MIMIC comprises two distinct models trained on MIMIC-CXR: one for findings and another for impressions. In our evaluation, we utilized the publicly available Findings Checkpoint\u00b9\u00b9 and Impression Checkpoint \u00b9\u00b2. Our evaluation employs these models sequentially, generating the findings and impression sections separately, and then combining them with appropriate headers to form the complete report.\nCvT2DistilGPT2_IU [16] CvT2DistilGPT2_IU employs a hybrid architecture combining a Convolutional vision Transformer (CVT) encoder [23] pre-trained on ImageNet-21K [9] with a DistilGPT2 [1] decoder for chest X-ray report generation. This model leverages the CvT's efficient hierarchical design for image feature extraction and DistilGPT2's natural language generation capabilities. In our evaluation, we utilized the publicly available checkpoint from Github\u00b9\u00b3 and followed the official evaluation guidelines.\nCvT2DistilGPT2_MIMIC [16] Cvt2distilgpt2_MIMIC applies the same Cvt2distilgpt2 architecture but is trained on the MIMIC-CXR dataset. Our evaluation utilized the publicly available MIMIC-CXR-trained checkpoints.\nRGRG [21]. RGRG (Region-Guided Radiology Report Generation) employs object detection to extract localized visual features from 29 anatomical regions in chest X-rays. It uses binary classifiers to select salient features and encode abnormalities, followed by a language model generating sentences for each selected region. RGRG was trained on the Chest ImaGenome dataset [24]. In our evaluation, we utilized the publicly available checkpoint from Github\u00b9\u2074 and followed the official evaluation guidelines.\nRaDialog [19]. RaDialog is a large vision-language model for radiology report generation and interactive dialogue. It integrates visual image features and structured pathology findings with a large language model (LLM), adapted to radiology using parameter-efficient fine-tuning. RaDialog was trained on the MIMIC-CXR for radiology report generation tasks. In our evaluation, we utilized the publicly available LLaVA version checkpoint from Huggingface\u00b9\u2075 and followed the official evaluation guidelines.\nGPT-4V [25]. GPT-4V (GPT-4 with vision) is a multimodal large language model released by OpenAI, which enables users to instruct GPT-4 to analyze image inputs provided by the user. In our evaluation, we used the API of model \"gpt4o05132024\" and followed the official evaluation protocols to assess its performance. The prompt we used is \"You are a helpful assistant. Please generate a report for the given images, including both findings and impressions. Return the report in the following format: Findings: {} Impression: {}. \".\nLLM-CXR [14]. LLM-CXR is a multimodal large language model that utilizes VQ-GAN to tokenize images, integrating both image and text tokens as input to its base LLM architecture. This model enables CXR-to-report generation, report-to-CXR generation, and CXR-related visual question answering (VQA). For our evaluation, we used the publicly available checkpoints\u00b9\u2076 and followed the official evaluation guidelines.\nMAIRA-2 [2]. MAIRA-2 is a multimodal large language model that combines a radiology-specific image encoder with a Large Language Model (LLM), trained for grounded report generation from chest X-rays. For input, the model accepts X-ray images along with indication, comparison, and technique information. For our evaluation, we used the publicly available checkpoints\u00b9\u2077 and followed the official evaluation guidelines. For studies containing both frontal and lateral views, we input the technique that \"PA and lateral views of the chest were obtained.\". For studies with only frontal views, we use \"PA view of the chest was obtained.\".\nMedVersa [30]. MedVersa is a versatile medical AI system that can coordinate multiple models and tools to perform various tasks and generate multimodal outputs. MedVersa was trained on the MIMIC-CXR training and validation dataset for medical report generation tasks. In our evaluation, we utilized the publicly available checkpoint from Huggingface\u00b9\u2078 and followed the official evaluation guidelines. The standard prompt structure we employed for report generation was \"Can you provide a report of input_image_token} with findings and impression?\", where {input_image_token} represents the placeholder for the input image.\nRadFM [22]. RadFM is a radiology foundation model trained on large-scale multi-modal datasets. It supports both 2D and 3D scans, multi-image input, and visual-language interleaving cases. The model's training included the MIMIC-CXR dataset. For our evaluation, we utilized the publicly available checkpoint from Huggingface\u00b9\u2079 and followed the official evaluation guidelines. The prompt we employed for report generation was \"Can you provide a radiology report for this medical image?\".\nVLCI_IU [5]. VLCI (Visual-Linguistic Causal Intervention) is a vision-language model using a multiway transformer for cross-modal alignment with Visual-linguistic causal intervention, integrating a pre-trained transformer and Visual and linguistic de-confounding Modules to mitigate cross-modal bias through local and global visual sampling and linguistic estimation using a vocabulary dictionary and visual features. In our evaluation, we used the publicly available checkpoints trained on the IU-Xray dataset \u00b2\u2070.\nVLCI_MIMIC [5]. VLCI_MIMIC applies the same VLCI architecture but is trained on the MIMIC-CXR dataset. Our evaluation utilized the publicly available MIMIC-CXR-trained checkpoints."}, {"title": "4 Results", "content": "Table 1, 2, 3 and 4 summarize the performance of various medical report generation models across four different datasets: ReXGradient, MIMIC-CXR, IU X-ray and CheXpert Plus. Among these, MedVersa demonstrates superior performance, achieving the best 1/RadCliQ-v1 scores on ReXGradient (1.01 \u00b1 0.01), MIMIC-CXR (1.10 \u00b1 0.02), and IU X-ray (1.46 \u00b1 0.03) on the findings section. This consistent top performance across different datasets indicates MedVersa's robust generalization capabilities and effectiveness in medical report generation.\nMoreover, the results displayed in the tables show that ReXGradient serves as a dataset where models consistently exhibit minimal confidence intervals of 0.01 for most models on the RadCliQ-v1 metric, thereby supporting its utility as a reliable benchmark for medical report generation models. Figure 3 illustrates the distribution of evaluation metrics. The IU X-ray dataset, while always showing high-performance scores (the best model achieving a 1/RadCliQ-v1 score of 1.46 \u00b1 0.03 on the findings sections), suggests that it may be too simplistic or lacking in complexity necessary for rigorous model differentiation. In contrast, CheXpert Plus shows lower overall performance (the best model obtaining a 1/RadCliQ-v1 score of 0.81 \u00b1 0.12 on the findings sections) with higher variance, potentially indicating dataset distribution shifts or noise.\nModels trained on multiple datasets (e.g., CheXpertPlus_CheX_MIMIC) tend to outperform those trained on individual datasets, suggesting that a multi-dataset training approach helps bridge the distributional gap and enhances generalization. Models perform better when evaluated on the same distribution seen in training, for instance, VLCI_IU achieves superior performance on IU X-ray (RadCliQ-v1: 1.38 \u00b1 0.04) compared to VLCI_MIMIC (0.91 \u00b1 0.04), while VLCI_MIMIC performs better on MIMIC-CXR (0.68 \u00b1 0.02 vs 0.60 \u00b1 0.02 for VLCI_IU).\nThe comparison between findings-only and findings + impression tasks reveals interesting model behaviors. On ReXGradient, MedVersa shows slight performance degradation when generating both findings and impressions (1/RadCliQ-v1 decreasing from 1.01 \u00b1 0.01 to 0.98 \u00b1 0.05), while CheXpertPlus_CheX_MIMIC shows improvement (from 0.83 \u00b1 0.01 to 0.85 \u00b1 0.01). This may be due to CheXpertPlus_CheX_MIMIC using separate models for findings and impression generation, while MedVersa only uses a single model architecture. The separate models may allow CheXpertPlus_CheX_MIMIC to better specialize in each subtask."}, {"title": "Disclosures", "content": "O.H and J.M are founders and hold equity in Gradient Health, a private company focused on health data accessibility and availability for commercial research. Gradient Health provided the Private Dataset used in this work and did not provide funding for this research and had no role in its design, execution, or publication. The Private Dataset's 4x downsampled version is available under the Gradient Health Public License."}]}