{"title": "META-REWARDING LANGUAGE MODELS:\nSelf-Improving Alignment with LLM-as-a-Meta-Judge", "authors": ["Tianhao Wu", "Weizhe Yuan", "Olga Golovneva", "Jing Xu", "Yuandong Tian", "Jiantao Jiao", "Jason Weston", "Sainbayar Sukhbaatar"], "abstract": "Large Language Models (LLMs) are rapidly surpassing human knowledge in\nmany domains. While improving these models traditionally relies on costly hu-\nman data, recent self-rewarding mechanisms (Yuan et al., 2024c) have shown that\nLLMs can improve by judging their own responses instead of relying on human\nlabelers. However, existing methods have primarily focused on improving model\nresponses rather than judgment capabilities, resulting in rapid saturation during\niterative training. To address this issue, we introduce a novel Meta-Rewarding\nstep to the self-improvement process, where the model judges its own judgements\nand uses that feedback to refine its judgment skills. Surprisingly, this unsupervised\napproach improves the model's ability to judge and follow instructions, as demon-\nstrated by a win rate improvement of Llama-3-8B-Instruct from 22.9% to 39.4%\non AlpacaEval 2, and 20.6% to 29.1% on Arena-Hard. These results strongly\nsuggest the potential for self-improving models without human supervision.", "sections": [{"title": "INTRODUCTION", "content": "Large Language Models (LLMs) are advancing significantly in their ability to follow instructions\nand respond to user queries (OpenAI, 2023; Touvron et al., 2023). An important phase in training\nthese models is instruction tuning (Ouyang et al., 2022), which typically involves training LLMs on\ndatasets curated by humans, either via supervised finetuning or preference optimization. Neverthe-\nless, the acquisition of human-generated data is both costly and time-consuming. Furthermore, the\nquality of such data is inherently constrained by the limitations of human capabilities. The so-called\n'Super Alignment' challenge (Burns et al., 2023) aims to find a solution to steering or controlling\npotentially super-intelligent AIs when their actions are inherently beyond human abilities to judge.\nAmong the potential solutions to this challenge, self-judging by the AI emerges as a particularly\npromising approach. Yuan et al. (2024c) introduces an iterative Self-Rewarding mechanism that\nenables an LLM to improve autonomously. The process involves a single model that takes on two\ndistinct roles, as an actor and as a judge. As an actor, the model produces responses that are aimed to\nfulfill specific instructions. As a judge (a special kind of acting), the model evaluates these responses\nvia LLM-as-a-Judge prompting (Zheng et al., 2024) and assigns rewards. The objective of the actor\nduring this self-play is to maximize its reward, thereby improving its ability to follow instructions.\nWe hypothesize that a major limitation of this previous work is that its learning objective enhances\nthe model's ability as an actor to generate better responses, while overlooking improving the model's\nability as a judge. If the ability to judge does not improve then training the actor over iterations can\nquickly saturate \u2013 or worse could overfit the reward signal, a.k.a. reward hacking. Consequently, it\nis imperative to also improve the model's capabilities as a judge in addition to its ability to act.\nIn this paper, we propose a novel method called Meta-Rewarding which assigns rewards to its own\njudgements to train the model's ability to judge. The key idea is to introduce a third role of meta-\njudge, whose task is to evaluate the model's own judgements. While the judge evaluates the actor's\nresponses, the meta-judge evaluates the judge's judgments (including rewards that it assigns) using a\nmechanism similar to LLM-as-a-Judge, which we term LLM-as-a-Meta-Judge. The meta-judge en-\nables us to build training data containing preference pairs of judgements, in addition to the standard\npreferences between actor responses derived from the standard judge. Our Meta-Rewarding method"}, {"title": "META-REWARDING", "content": "In our method, we assume a setup where we only have an initial seed model, an instruction-tuned\nLLM, and no further human supervised training data. The idea is to generate training data from the\nmodel itself through an iterative self-play process. In this process, the model assumes three main\nroles: as an actor, it generates responses to given prompts; as a judge, it evaluates and scores its own\nresponses; and as a meta-judge, it compares the quality of its own judgments.\nWhile training the actor to generate better responses to user queries is the final objective, this train-\ning's efficacy relies on the accuracy of the judge. As the judge's accuracy increases, it will provide\nhigher quality feedback for training the actor, ultimately leading to a better actor. Therefore, the goal\nof Meta-Rewarding is to improve the model's capability both as actor and judge during training. The\nrole of the meta-judge is to provide feedback necessary for training the judge.\nAt a high level, as depicted in Figure 1, our method is an iterative training scheme that starts from a\ngiven seed LLM, which assumes all three roles. An iteration starts with the actor generating multiple\nresponse variations for each prompt. This is followed by the judge evaluating each response using an\nLLM-as-a-Judge prompt and generating a judgement that contains a score. This score then allows\nus to build preference pairs of responses for training the actor. For training the judge, we pick a\nsingle response and let the meta-judge compare two of its judgement variations generated by the"}, {"title": "ACTOR PREFERENCE DATASET CREATION", "content": "Our approach to create the actor preference dataset on a given iteration is built upon the pipeline\nintroduced by Yuan et al. (2024c), with a crucial modification to incorporate a length-control mech-\nanism. As we see later in Section 3.5, this change proves to be essential in preventing the responses\nfrom lengthening and improving the length-controlled win rate. The dataset creation process con-\nsists of three main steps:\nSample Responses from Actor. We assume we have a given set of prompts. For each prompt x, we\ngenerate K different responses {y_1, ..., y_K} by sampling from the current model M_t at iteration t.\nAggregate Multiple Judgments. For each response $y_k$, we generate N different judgments\n$\\{j^1_k,...,j^N_k\\}$ from $M_t$ using an LLM-as-a-Judge prompt (shown in Section A.1). The prompt in-\nstructs the model to evaluate the given response $y_k$ for prompt x according to a fixed rubric and"}, {"title": "JUDGE PREFERENCE DATASET CREATION", "content": "Unlike the judge that provides score-based judgements, we design the meta-judge to operate in a\npairwise mode by comparing two given judgements. Thereby, we adopt the following three steps for\ngenerating and selecting chosen and rejected pairs, while carefully controlling for positional bias:\nResponse Selection: To prepare effective training data for the judge, we focus on responses where\nthe judge is the least certain, as measured by the variance of the scores it has given. To be more\nspecific, we first compute the score variance given by the N different judgments for every response\n$y_k$. We then pick the response y with the highest score variance for each prompt x to be used in the\njudge training. If multiple responses have the same variance, we break ties randomly.\nPairwise Meta-Judge Evaluations: For each selected response y, we have up to N corresponding\njudgments, denoted as $\\{j_1, ..., j_N\\}$. We then evaluate each pair of different judgments $(j_m, j_n)$\nusing a meta-judge prompt shown in Figure 2. This LLM-as-a-Meta-Judge prompt includes the\noriginal prompt x, response y, and its two judgements $(j_m, j_n)$ as well as the rubric used by the\njudge. Then the model is asked to generate chain-of-thought reasoning followed by its choice of the\nbetter judgement. Again this uses the same LLM model, but acting as a meta-judge this time.\nTo mitigate positional bias (where the meta-judge might e.g. tend to prefer the judgment that appears\nfirst), we prompt the model twice by changing the ordering of the two judgements. In addition, we\nalso introduce weighted scoring for winning in the first vs second positions. We define $win_{1st}$ and\n$win_{2nd}$ as the total wins in the first and second positions respectively, and calculate the weights as:\n$w_1 = \\frac{win_{2nd}}{win_{1st} + win_{2nd}}, \\qquad w_2 = \\frac{win_{1st}}{win_{1st} + win_{2nd}}$\nThe result of a single battle between judgments $(j^m, j^n)$ is defined as:\n$r_{mn} =\\begin{cases}\n1 & \\text{If the meta-judge prefers m wins}\n-1 & \\text{If the meta-judge prefers n wins}\n0 & \\text{If tie or parse error.}\n\\end{cases}$\nWe then construct a battle matrix B as the weighted sum of the battle results:\n$B_{mn} = w_1 \\mathbb{1}[r_{mn} = 1] + w_2 \\mathbb{1}[r_{nm} = -1]$\nElo Score and Pairs Selection: The next step is to convert the battle matrix into rewards (meta-\nrewards) corresponding to each judgement. Inspired by Zheng et al. (2024), we determine the Elo\nscore $E_m$ for each judgment $j_m$ by solving the following maximum likelihood estimation problem:\n$\\arg \\max_{E_m} \\sum_{m,n} B_{mn} \\log \\left( \\frac{e^{E_m}}{1 + e^{E_m - E_n}} \\right)$"}, {"title": "EXPERIMENTAL SETUP", "content": "We use instruction-finetuned Llama-3-8B-Instruct as a seed model, and otherwise closely follow the\nexperimental setup of Yuan et al. (2024c). Before our Meta-Rewarding training, we first perform\nsupervised finetuning (SFT) of the seed model on the Evaluation Fine-Tuning (EFT) dataset from\nYuan et al. (2024c). This dataset is built from Open Assistant (K\u00f6pf et al., 2024) and provides initial\nLLM-as-a-Judge training data of ranked human responses, thus aiding the model to act as a judge.\nSince the seed model is already instruction finetuned, we skip training directly on human responses\nfor the actor. We refer to this model as SFT on EFT, or simply SFT for short.\nFor Meta-Rewarding iterations, we utilize 20,000 prompts from Yuan et al. (2024c) that were gener-\nated by Llama-2-70B-Chat using an 8-shot prompt. We provide a visualization of their distribution\nin Appendix Figure 6. For each iteration, we sample 5,000 prompts from this seed set and conduct\nfour iterations in total. The iterative process is formally defined as follows:\nIter 1 Obtain $M_1$ by training using DPO (initialized from the SFT model) on both actor and judge\npreference pairs generated by the SFT model.\nIter 2 Obtain $M_2$ by training $M_1$ using DPO on actor and judge preference pairs generated by $M_1$.\nIter 3 Obtain $M_3$ by training $M_2$ using DPO exclusively on actor preference pairs generated by $M_2$.\nIter 4 Obtain $M_4$ by training $M_3$ using DPO exclusively on actor preference pairs generated by $M_3$.\nWe provide a detailed recipe for training in Section A.3. In each iteration, we generate K = 7\nresponse variations per prompt using temperature 0.8 and top-p 0.95. This results in a total of\n35,000 responses per iteration. We then filter out identical responses, typically removing no more"}, {"title": "EVALUATION METHODS", "content": "As Meta-Rewarding aims to improve the model both as an actor and a judge, we evaluate its per-\nformance in both of these roles. In addition, we also compare it against a Self-Rewarding baseline\n(Yuan et al., 2024c) in the same setup, equipped with the same length-control mechanism. This\nallows us to measure the gains brought by the judge training data generated via meta-rewarding.\nActor's Instruction Following We make use of three well-established auto-evaluation benchmarks\nbased on GPT4-as-a-Judge: AlpacaEval 2 (Dubois et al., 2024a), Arena-Hard (Li et al., 2024) and\nMT-Bench (Zheng et al., 2024). These benchmarks focus on different aspects of the model. For\ninstance, AlpacaEval mainly focuses on chat scenarios, where the prompt sets cover a diverse range\nof daily questions. In comparison, Arena-Hard consist of more complex or challenging questions,\nwhere they satisfy more criteria in the predefined 7 aspects (creativity, complexity, problem-solving,\netc). Notably, Arena-Hard has the highest correlation with Chatbot-Arena among popular open-\nended LLM benchmarks (Li et al., 2024). MT-Bench has 8 different question categories and evalu-\nates the multi-turn conversation ability of the model.\nJudge's Reward Modeling To evaluate the reward modeling capability of the judge, we measure\nthe correlation of our judge scores with human preferences, as well as a strong AI judge when hu-\nman labeling is not available. We quantitatively calculate the Spearman correlation and agreement\nbetween the model-generated ranking with the human-labeled preferences provided in the Open As-\nsistant dataset. We use a held-out split of 190 samples, with each sample consisting of a prompt and\nseveral human ranked responses, totalling 580 different responses. Additionally, we also measure\nthe judge's performance on ranking responses generated by the seed model, which is considered to\nbe more in-distribution compared to human or other model generated responses. This is because the\njudge is mainly trained and applied on samples that are self-generated. However, in this case, we do\nnot have ground-truth human preference labels, so we adopt the strong judge gpt-4-1106-preview as\na proxy."}, {"title": "INSTRUCTION FOLLOWING EVALUATION", "content": "Meta-Rewarding iterations significantly improves the win rate. In Figure 3, we show the length-\ncontrolled (LC) win rate of our method over its training iterations on the AlpacaEval benchmark."}, {"title": "ABLATIONS AND ANALYSIS", "content": "Length-Control Mechanism: Our length-control mechanism is essential in maintaining a balance\nbetween comprehensiveness and conciseness of the model responses. We compare the last training\niteration with different length-control parameter choices $\\rho$ and present the results in Table 4. Us-\ning $\\rho = 0$ is equivalent to not performing any length-control in the preference data selection. As\nexpected, training this way makes the model excessively verbose for both models, and negatively\naffects the LC win rate as shown for Self-Rewarding LLMs.\nTraining with an External Reward Model: Meta-Rewarding employs an LLM-as-a-Judge prompt\nto judge its own responses. Instead, we experiment with using a strong external reward model\nStarling-RM-34B (Zhu et al., 2023) to select actor preference pairs. However, we find that Starling-\nRM-34B failed to increase the LC win rate of AlpacaEval in the first iteration (24.63% vs 27.85%),\nperhaps due to its length bias.\nMeta-Judge Biases: After the first iteration of Meta-Rewarding training, the meta-judge becomes\nmore likely to prefer a higher score judgment nearly all the time, as shown in Table 5. This score-\nbias, in turn, significantly shifts the scoring distribution of the judge towards the full score of 5. For"}, {"title": "LIMITATIONS", "content": "A deficiency in our experimental setup is the 5-point judging system that we chose, following Yuan\net al. (2024b). We discovered that this scoring method often results in ties due to minimal quality\ndifferences between responses, necessitating careful averaging of multiple judgments to differentiate\nbetween them. Moreover, as training progressed, responses increasingly approached the maximum\nscore, making further improvements difficult to detect. A more nuanced scoring system that covers\ndiverse aspects (Wang et al., 2024) or a comparison-based approach might address these issues.\nAnother significant limitation lies in the judge training process. Despite our efforts to mitigate po-\nsitional bias of our meta-judge, this issue persists and hindered further improvements in Iteration 3.\nThe judge also demonstrated a tendency to assign higher scores, which accelerated score saturation\nand reduced its ability to discriminate between responses. Furthermore, the judge showed limited\nimprovement in evaluating non-self-generated responses in our evaluations. We believe there is sub-\nstantial room for improvement if these issues can be effectively addressed, which could significantly\nboost the overall effectiveness of our approach."}, {"title": "CONCLUSION", "content": "In this work, we propose a novel mechanism for improving the judging skill of models by using a\nmeta-judge that assigns meta-rewards to select chosen and rejected judgments for preference opti-\nmization. This addresses a major limitation of the Self-Rewarding framework (Yuan et al., 2024c),\nspecifically the lack of training the judge. To make Meta-Rewarding training work, we additionally\nintroduce a new length-control technique to mitigate the issue of length explosion when training with\nAI feedback. The effectiveness of our method is demonstrated through auto-evaluation benchmarks\nAlpacaEval, Arena-Hard, and MT-Bench. Remarkably, even without additional human feedback,\nour approach significantly improves upon Llama-3-8B-Instruct and surpasses both Self-Rewarding\nand SPPO (Wu et al., 2024), a strong baseline that relies heavily on human feedback. Furthermore,\nwhen we evaluate our model's judging ability, it shows significant improvement in correlation with\nboth human judges and strong AI judges like gpt-4-1106-preview. Overall, our findings provide\nstrong evidence that self-improving the model without any human feedback is a promising direction\nfor achieving super alignment."}, {"title": "APPENDIX", "content": null}, {"title": "JUDGE PROMPT", "content": "We adopt the same judge prompt as in Yuan et al. (2024c)."}, {"title": "GPT4 JUDGE PROMPT", "content": "We adopt this prompt from AlpacaEval, which is proved to have high correlation with human judges."}, {"title": "TRAINING DETAILS", "content": "For the SFT model, we train for a total of 10 epochs using a learning rate $5 \\times 10^{-8}$ and global batch\nsize of 32. We employed cosine learning rate scheduling and saved a checkpoint after every epoch.\nWe selected checkpoint from epoch 5 as the final model.\nFor all DPO training, we also trained for 10 epochs, with a learning rate of $5 \\times 10^{-6}$, $\\beta = 0.1$ and\nglobal batch size of 32. We adopted cosine learning rate scheduling.\nFor Self-Rewarding training, during Iteration 1 we set $\\rho = 0$ for actor data creation and applied a\nfilter to exclude pairs where the chosen response length exceeded 2500 characters. We selected the\ncheckpoint from epoch 5 for this iteration. In both Iteration 2 & 3 we continue with $\\rho = 0$ and chose\ncheckpoints from epoch 1 and epoch 2 respectively. For Iteration 4, we adjust $\\rho$ to 0.1 and selected\nthe checkpoint from epoch 2.\nFor Meta-Rewarding training in Iteration 1 we set $\\rho = 0$ for actor data creation, and we filtered\nout pairs with chosen response length exceeding 2500 characters. Additionally, for the judge data\ncreation, we filtered out pairs if the chosen judgment length exceeded 1100. We selected checkpoint"}]}