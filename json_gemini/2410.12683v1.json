{"title": "Generative Neural Reparameterization for Differentiable PDE-Constrained Optimization", "authors": ["Archis S. Joglekar"], "abstract": "Partial-differential-equation (PDE)-constrained optimization is a well-worn technique for acquiring optimal parameters of systems governed by PDEs. However, this approach is limited to providing a single set of optimal parameters per optimization. Given a differentiable PDE solver, if the free parameters are reparameterized as the output of a neural network, that neural network can be trained to learn a map from a probability distribution to the distribution of optimal parameters. This proves useful in the case where there are many well performing local minima for the PDE. We apply this technique to train a neural network that generates optimal parameters that minimize laser-plasma instabilities relevant to laser fusion and show that the neural network generates many well performing and diverse minima.", "sections": [{"title": "1 Introduction", "content": "While Automatic Differentiation (AD) is not new [1] and scientific programmers have exploited implementations of it for some time now, AD tooling has become much more accessible as a byproduct of the deep learning revolution in machine learning. The maturation of AD tooling has led to the paradigm of differentiable programming, where general purpose programs can be written using an AD-capable framework and then benefit from natively supporting machine learning and gradient-descent. At this point, this paradigm has been exploited by many disciplines and it would be disingenuous to attempt to enumerate them here.\nPartial-differential-equation (PDE) constrained optimization is pertinent to many scientific and engineering disciplines [2, 3]. A key component of PDE-constrained optimization is the method by which the gradient is computed. When a numerical program that solves PDEs is written using an AD-capable framework, it immediately lends itself to PDE constrained optimization.\nIn this work, we will show that when PDE constrained optimization is performed using an AD-capable framework i.e. when the PDE solver is made differentiable, the benefit is not only access to a fast and accurate gradient with respect to the optimization parameters, but also the ability to learn distributions of optimal parameters via highly-parameterized neural networks. When the neural network is made \u2018generative', i.e. when it maps from a random vector, it can help capture degeneracy in PDE-constrained optimization problems with many local minima."}, {"title": "2 Generative PDE-constrained optimization", "content": "In this section, we explain our method, how it is an extension of previous work, and the advantages it offers in comparison to direct PDE constrained optimization. In the discussion that follows, Y (T, x) is the solution to a PDE parameterized by parameter set, Op. M is the function that computes the scalar quantity that is to be minimized. The minimization strategy is first-order gradient descent."}, {"title": "2.1 Direct method", "content": "Here, direct optimization refers to solving the optimization problem posed by\n\\begin{equation}\nmin C [Y (T, 7; 0p)]\n\\end{equation}\n\\begin{equation}\ns.t. \\frac{d\u0141Y}{dt} = f(t, x, Y (t, x);0p).\n\\end{equation}\nThis is a typical PDE constrained optimization problem. While effective, performing gradient descent to solve eq. 1 to obtain a distribution of optimal parameters can require many iterations and suffer from the curse of dimensionality because one optimization produces one set of optimal parameters."}, {"title": "2.2 Neural Reparameterization (NR)", "content": "In ref. [4], it was proposed that the parameter set be reparameterized using a neural network. For that particular application, the reparameterization enabled the optimization algorithm to uncover better performing minima. Formally, they solved the optimization problem posed by\n\\begin{equation}\nmin C [Y (T, 7; N(\u1ef9; 0\u2116))]\n\\end{equation}\n\\begin{equation}\ns.t. \\frac{d\u0141Y}{dt} = f(t, x, Y (t, x); N(g; \u03b8\u2116)),\n\\end{equation}\nwhere p = N(\u1ef9; 0n) is the neural reparameterization (NR) of @p. Crucially, in that work, the input vector, \u1ef9, to the neural network is a learned quantity in addition to the weights of the neural network ON. Like with Direct method above, NR is also unable to obtain multiple local minima without suffering from the same problems."}, {"title": "2.3 Generative Neural Reparameterization (GNR)", "content": "Here, we propose that \u011f is no longer learned, but is instead a random variable. This results in the neural network being responsible for learning a mapping from a random variable to an optimal set of parameters. Figure 1 provides a conceptual illustration of this method. Formally, the problem is stated by\n\\begin{equation}\nmin L [Y (T, 7; N(G; 0N))]\n\\end{equation}\n\\begin{equation}\ns.t. \\frac{d\u0141Y}{dt} = f(t, x, Y (t, x); N(G;0\u2116)),\n\\end{equation}"}, {"title": "3 Application - Laser Plasma Instability Modeling", "content": "where G is now a random vector from an D-dimensional unit normal vector where D is another hyperparameter for the neural network. Here, we choose D = 6. We find the training process to be relatively insensitive to this parameter within 4 < D < 16. This addition to the NR method enables learning distributions of optimal parameters. The neural network is responsible for transforming the unit normal probability distribution to the probability distribution of optimal spectra.\nIf the optimization problem has many satisfactory local minima and many parameters where gradient-descent is necessary for the problem to be tractable, each instantiation of the direct method and NR will converge to a single minimum from the set of satisfactory minima. On the other hand, GNR learns distributions of optimal parameters. In addition to obtaining a generative function for inverse design, learning a distribution of optimal solutions can help with downstream analyses of the behavior of the solution at the optimal point.\nIn the following section, we apply the GNR method to a PDE-constrained minimization problem where local minima abound."}, {"title": "3.1 Background", "content": "Laser-driven inertial fusion has demonstrated promising recent results in the laboratory [5, 6] and efforts are underway to scale these experiments into those that can deliver enough power to serve as part of a fusion power plant. One of the persistent challenges with scaling the experiments and delivering large amounts of laser energy to the fusion target is the presence of laser plasma interactions. Plasmas are electromagnetic media which can and do interact with lasers. While laser plasma interactions have been used to make implosions more symmetric by leveraging plasma's ability to transfer energy between laser beams [7], they are typically detrimental in other aspects of the laser plasma physics that occurs during such experiments. For this reason, it is important to model the behavior of, and minimize the impact of, these laser plasma instabilities that plague laser-driven inertial fusion.\nRecent advances in laser technology have enabled the development of broadband high energy lasers. Adding bandwidth, i.e. where the laser is not monochromatic but has high energy and intensity light at many wavelengths, helps mitigate laser-plasma instabilities [8, 9, 10]. For this reason, future laser fusion experiments and facilities will be designed with bandwidth. It will be important to determine exactly what kind of bandwidth spectra best serve the purpose of transferring laser energy to the target through the plasma. To do so, we will need bandwidth that helps minimize the interaction between the laser and the plasma. In the following section, we detail the PDE and the ensuing laser plasma instability minimization problem."}, {"title": "3.2 Equations", "content": "We model one such laser-plasma instability, called Two Plasmon Decay, using the PDE that describes electron plasma waves, also given in ref. [9], given by\n\\begin{equation}\n\\nabla \\left[\\left(\\frac{\\partial}{\\partial t} + v_{e} \\nabla + \\frac{i 2 \\delta \\omega}{2 \\omega_{p e 0}}\\right)^{2} - \\frac{3 \\omega_{p e 0}^{2}}{2 n_{0}} \\nabla \\nabla \\right] E_{h} = S_{T P D} (E_{0}) + S_{h}\n\\end{equation}\n\\begin{equation}\nS_{T P D} = \\frac{\\rho_{0}(x)}{S_{L N}(w_{0}, w_{p 0})} \\nabla \\cdot \\left[\\nabla (E_{0} E_{0}) - E_{0} E_{0}\\right] e^{-i(w_{0} - 2 w_{p 0}) t}\n\\end{equation}\nwhere the source term, $S_{TPD}$, contains $E_{0}$, the laser field that can be optimized. The equations are described in more detail in ref. [9] and the appendix. It is the laser field that contains the free parameters that can be optimized to minimize laser-plasma instability activity. The laser field is a sum of $N_{c}$ complex fields and is given by\n\\begin{equation}\nE_{0}(t, x) = \\sum_{j}^{N_{c}} a_{j} \\exp (i k_{j}(x) x-i w_{j} t+\\phi_{j}).\n\\end{equation}\nFormally, the optimization problem of interest is given by\n\\begin{equation}\n\\min_{a_{j}, \\phi_{j}} \\int_{15 p s}^{20 p s} \\int \\int |\\frac{\\partial E_{h}(t, x, y ; a_{j}, a_{j}, \\phi_{j}, N_{c} = 8)}{\\partial t}|^{2} d x d y d t,\n\\end{equation}"}, {"title": "3.3 Results", "content": "where we are interested in minimizing the growth rate of the instability.\nPerforming GNR on eq. 9 gives\n\\begin{equation}\n\\min _{\\theta_{N}} \\int_{15 p s}^{20 p s} \\int \\int |\\frac{\\partial E_{h}(t, x, y ; ., y ; a_{j}, \\phi_{j}, N_{c} = 8)}{\\partial t}|^{2} d x d y d t\n\\end{equation}\n\\begin{equation}\ns.t. \\quad a_{j} = \\varsigma(N(G) ; \\theta_{N}) \\text { and } \\phi_{j} = \\varrho(N(G) ; \\theta_{N}).\n\\end{equation}\nIn this section, we solve eq. 11\u00b9 and show comparisons of the results of the optimization for $N_{c}=8$ using GNR to the baseline case of a uniform amplitude, $a_{j}=0.125$, spectrum with random phase $\\phi_{j}=U[0,2 \\pi]$, as in ref. [9]. An example amplitude and phase pair from the uniform case, along with six instantiations of the generated spectra, are shown in fig. 2.\nFigure 3 shows the comparison of growth rates of the absolute instability from simulations with generated spectra and from simulations using the baseline. The figure indicates that the generated distribution generally performs better than the baseline case in task of minimizing the growth rate. The analysis of the results and the underlying physics is beyond the scope of this workshop paper. The aim here is to demonstrate that the GNR method can capture many well performing local minima in PDE-constrained optimization."}, {"title": "4 Conclusions", "content": "PDE-constrained optimization is a common technique that can benefit from Automatic Differentiation and Differentiable Programming. In addition to the fast calculation of gradients, the ability to train neural networks in-line opens many possibilities. We highlight one such possibility here where a neural network can be trained to map from a random vector to a set of optimal parameters. This enables machine learning distributions of optimal parameters in the context of PDE-constrained optimization with many desirable local minima."}]}