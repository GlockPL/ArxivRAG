{"title": "Analytical Uncertainty-Based Loss Weighting in Multi-Task Learning", "authors": ["Lukas Kirchdorfer", "Cathrin Elich", "Simon Kutsche", "Heiner Stuckenschmidt", "Lukas Schott", "Jan M. K\u00f6hler"], "abstract": "With the rise of neural networks in various domains, multi-task learning (MTL) gained significant relevance. A key challenge in MTL is balancing individual task losses during neural network training to improve performance and efficiency through knowledge sharing across tasks. To address these challenges, we propose a novel task-weighting method by building on the most prevalent approach of Uncertainty Weighting and computing analytically optimal uncertainty-based weights, normalized by a softmax function with tunable temperature. Our approach yields comparable results to the combinatorially prohibitive, brute-force approach of Scalarization while offering a more cost-effective yet high-performing alternative. We conduct an extensive benchmark on various datasets and architectures. Our method consistently outperforms six other common weighting methods. Furthermore, we report noteworthy experimental findings for the practical application of MTL. For example, larger networks diminish the influence of weighting methods, and tuning the weight decay has a low impact compared to the learning rate.", "sections": [{"title": "1 Introduction", "content": "Multi-task learning (MTL) aims at solving multiple tasks simultaneously in a mutually beneficial manner. Intuitively, related tasks should share their knowledge and unrelated tasks should be processed more isolated to efficiently use the available data and compute resources.\nOne of the major challenges is to find the right (implicit or explicit) balance between individual tasks to gain a good performance across tasks. Recent approaches in deep learning tackle this problem from various angles, such as adapting the network architecture [19,17] or resolving conflicts between task-specific gradients during optimization [30,48]. A methodologically simple yet effective approach is based on providing task-specific weights during optimization, usually termed as loss weighting.\nNot explicitly weighting tasks, also termed Equal Weighting (EW), has potential pitfalls. Different tasks could be measured with different loss functions,"}, {"title": "2 Related Work", "content": "MTL considers training of tasks simultaneously by efficiently distributing resources and sharing knowledge between them [4,39,44]. There are typically two orthogonal research directions found in the recent literature: One line of work considers multi-task architectures which focus on how features can be shared between tasks [12,47,36,32,46,34]. In this work, we make use of the basic hard-parameter sharing network structure, which consists of a fully-shared backbone and task-specific heads. We expect our method to be combinable with any other MTL network architecture. In contrast, MTO methods aim to balance the tasks to tackle the negative transfer that might occur between them. These methods can further be distinguished as gradient-based and loss weighting methods:\nLoss weighting methods address the challenge of weighting task-specific losses appropriately. Most relevant for our work, Uncertainty Weighting (UW) [22] weights different losses by learning the respective task-specific homoscedastic uncertainty. We adapt this by computing task weights based on the analytically optimal solution of UW and normalizing the results through a softmax function (Section 3.2). Lin et al. [2] argue that random sampling of loss weights (RLW) should be considered a relevant baseline. Alternatively to the weighted sum of losses, the geometric loss strategy (GLS) [9] computes the geometric mean. While this method does not require any additional hyperparameters, it is numerically sensitive to a large number of tasks. Dynamic weight averaging (DWA) [32] assigns a higher weight to tasks whose respective loss shows a slower decrease compared to other task losses. Impartial Multi-Task Learning (IMTL-L) [31] learns the scaling factor of the losses via gradient descent such that scaled losses would become constant for all tasks. The brute force method Scalarization [45] which searches all possible combinations of fixed loss weights has shown competitive performance compared to current automated MTO methods. Other loss weighting methods are proposed in [24,26,14,16].\nGradient-based methods make direct use of task-specific gradients to either determine individual scaling factors which are applied on the task-wise gradients directly [7,40,31,38,35,41] or perform manipulations on the gradients to resolve potential alignment conflicts between them [31,8,21,42,29]. They mostly differ in the type of strategy used to handle these conflicts, such as projecting conflicting gradients onto the normal plane [48], or considering trade-offs between average and worst-case losses [30]. A disadvantage of these methods is that computing task-wise gradients is computationally expensive. In this paper, we do not consider gradient-based optimization methods as they have been shown to not outperform the simple loss weighting Scalarization approach [45], and Kurin et al. [23] report that loss weighting methods commonly have significantly shorter training times [8] which is relevant in practice."}, {"title": "3 Background and Method", "content": "In MTL, we aim to resolve $K$ tasks for some input data point $x \\in \\mathcal{X}$. For this, $x$ is mapped to labels $\\{y_k \\in \\mathcal{Y}_k\\}_{k\\in [1,K]}$ simultaneously using specific mappings $\\{f_k: \\mathcal{X} \\rightarrow \\mathcal{Y}_k\\}$. We assume hard task-shared parameters $\\theta$ in a hydra-like neural network architecture. This means all tasks receive the same intermediate feature $z = f(x;\\theta)$ from the shared backbone and each task head yields output $f_k(x) = f(z; \\theta_k)$ with task-specific parameters $\\theta_k$ [39].\nThe network is trained by considering all tasks' losses $\\mathcal{L}_k$. Naively summing up these losses (the equal weighting method) typically leads to imbalanced learning as tasks with high loss magnitude might dominate the training. The goal is thus to find optimal (dynamic) loss weights $w_k$ for all tasks to optimize the loss $\\mathcal{L} = \\sum_k w_k \\mathcal{L}_k$ in a way that tasks benefit w.r.t. their final performance metrics."}, {"title": "3.1 Weaknesses of Uncertainty Weighting and Scalarization", "content": "UW [22] is one famous MTO approach with over 3.2k citations (May'24) and yields competitive performance (see Section 4.2) besides its simplicity. However, UW also shows some drawbacks: First, we observe that UW can be affected by bad initialization / inertia. As uncertainty weights are usually initialized equally for all tasks, it can slow down their progress toward reaching the best values for each task and epoch using gradient descent, especially as task weights often differ in orders of magnitude. We refer to this phenomenon as update inertia. To demonstrate this phenomenon empirically, we focus on the development of task weights $w_t$ of the NYUv2 dataset with two different initializations of $w_t$ (Figure 1). In the first initialization setting (blue line, UW S1), we consider the usual initialization of $w_k = 0.8$ [28]. In the other setting (orange line, UW S2), we initialize the task weights higher and choose the initialization values equal to the final $w_t$ of a previous run. We observe that the learned task weights develop differently due to the different initializations. It takes roughly 100 epochs (1/4 of the whole training) to recover, i.e., the blue and the orange line then behave similar. As both experiments receive the same non-weighted losses from each task at the beginning we would expect that its weightings $w_k$ adapt quickly to the task loss ignoring the wrong initialization value. Thus, a non-optimal initialization has a direct impact on the training dynamics due to the update inertia of the task weights. Another example for update inertia is discussed in section A3.2 for the CelebA dataset.\nSecond, we observe that UW is prone to overfitting - a detailed discussion is presented after the benchmark results in Section 4.2.\nLastly, we give a first hint with a toy example in Section A3.2 that UW does not only depend on task-wise aleatoric homoscedastic uncertainty but also shows a model complexity dependence.\nScalarization is demonstrated [45] to yield competitive performance on a range of MTL problems, but there are shortcomings: As mentioned by the authors, manually tuning loss weights by performing an extensive grid search is computationally expensive. Besides, it can only be applied to problems with a"}, {"title": "3.2 Our contribution: Soft Optimal Uncertainty Weighting", "content": "AS UW [22] shows inertia with task uncertainty $\\sigma_\\epsilon$ being updated gradually through gradient descent, we are determining which $\\sigma_\\epsilon$ values would analytically minimize the total Loss $\\mathcal{L}$ in a given batch. These values are then normalized using a softmax function with temperature.\nUW-O: Minimizing the total loss in UW The approach UW [22] weights losses based on their task-specific homoscedastic aleatoric uncertainty. The exact weighting formulae depend on the type of loss. For instance, for tasks with an $\\mathcal{L}_1$ loss it can be derived by assuming a Laplace posterior distribution and identifying $\\sigma_k$ with the uncertainty of each task $k \\in \\mathcal{K}$, treating them as learnable parameters that are input independent\n$\\mathcal{L} = \\sum_{k \\in \\mathcal{K}} \\frac{1}{\\sigma_k} \\mathcal{L}_k + \\log \\sigma_k$   (1)\nHere, $\\mathcal{L}_k$ is the task-specific loss, e.g., a mean absolute error. Intuitively, the $\\sigma_k$ in the first term allows to down-weight difficult tasks by increasing the uncertainty"}, {"title": "4 Experiments and Results", "content": "In this Section, we provide details about the used datasets, network architectures, metrics, and training proceeding. Further details can be found in Section A2.\nDatasets We use three common computer vision MTL datasets: two datasets for scene understanding NYUv2 [37] and Cityscapes [10] and a binary attribute dataset CelebA [33]. NYUv2 and Cityscapes comprise the tasks of semantic segmentation and depth estimation. The third task for NYUv2 is surface normals estimation. CelebA constitutes a 40-class binary classification problem.\nArchitectures For NYUv2, we use a SegNet [1], ImageNet pretrained ResNet-50 / ResNet-101 with a DeepLabHead, and the MTAN on top of the SegNet [32]. For Cityscapes, we use a SegNet, a DeepLabV3+ [6] network with pre-trained ResNet-50 / ResNet-101 backbones, and again the MTAN/SegNet. All Single-task learning (STL) baselines are trained with the SegNet. For CelebA, we use a ResNet-18, also for STL.\nMetrics To compare models, we use task-specific metrics and the established $\\Delta_m$-metric [34]. It measures the average relative performance gain of the multi-task model $\\mathcal{M}_m$ w.r.t. a single-task baseline $\\mathcal{M}_b$: $\\Delta_m = \\frac{1}{K} \\sum_{k=1}^{K} (\\frac{\\mathcal{M}_{m,k} - \\mathcal{M}_{b,k}}{\\mathcal{M}_{b,k}})$, where $l_k$ is 1/0 if a higher / lower value is better for criterion $M_k$.\nTwo Evaluation Setups Several papers (e.g., [30,32,48,38]) have used a fixed training protocol for NYUv2 and Cityscapes, with no hyperparameter tuning, averaging results over the last 10 test epochs. In contrast, other studies (e.g., [45,23,40]) advocate for method-specific hyperparameter tuning, which is more relevant for practitioners. Following [45], we perform a thorough hyperparameter search for all methods, selecting the best combination based on the $\\Delta_m$ score and using early stopping on the validation set. For final evaluation, we train on 5 random seeds and report the mean test performance. However, acknowledging other works, we also provide results using the fixed protocol with MTAN/SegNet on NYUv2 and Cityscapes.\nAll models are trained with the Adam optimizer which has been shown to perform advantageous on MTL setups [13]. Compared to [30], we increase the"}, {"title": "4.2 Common loss weighting methods benchmark", "content": "We compare our methods to the most common loss weighting approaches. Overall, UW-SO consistently performs best or second-best across all datasets and architectures w.r.t. the $\\Delta_m$ metric. This holds for the hyperparameter-tuned experiments as well as for those with the fixed training protocol. Occasionally, our method gets beaten by the computationally expensive Scalarization approach, especially when using the SegNet architecture. Noticeably, performance differences between MTO algorithms decrease for larger networks.\nCityscapes On Cityscapes, UW-SO achieves the best $\\Delta_m$ score when trained on both ResNet architectures as well as on the MTAN/SegNet with the fixed hyperparameters, and the second-best behind Scalarization when trained on the SegNet (see Table 1). In contrast to NYUv2 (see Table 2), the performance of Scalarization decreases for larger networks due to weak results on the difficult and highly sensitive relative depth error. While an even more fine-grained search of task weights might yield better results, we argue that our weight search with"}, {"title": "4.3 Ablation Studies", "content": "In the following, we present some ablation studies about our method UW-SO. Further ablation studies can be found in sections A3.5 till A3.9.\nInfluence of softmax To demonstrate the influence of the softmax function on the inverse loss weights, we compare the $\\Delta_m$ scores using UW-O and UW-SO across all datasets and architectures in Table 4. UW-SO outperforms UW-O in all experiments, indicating the performance gain provided by the tempered softmax function. However, we want to emphasize that this is not due to a significantly worse performance of UW-O compared to other MTO methods. Therefore, we also present the results using IMTL-L, which, like UW-O, aims to scale each weighted task loss to 1, but unlike UW-O, it learns rather than computing the weights. Interestingly, none of the two methods can outperform the other one, indicating that despite UW-O's simplicity, it still provides reasonable results compared to existing methods. Furthermore, in a toy example, we find that UW-O is particularly strong in dealing with extreme loss magnitude differences, at which most of the previous methods fail (Section A3.4). However, this is of"}, {"title": "5 Discussion and Conclusion", "content": "In this paper, we introduce UW-SO, a new method for weighting losses in Multi-Task Learning (MTL). Derived from the analytical solution of Uncertainty Weighting, UW-SO applies the tempered softmax function to the inverse of the losses to effectively weight tasks. In an extensive benchmark with 3 datasets, up to 4 architectures per dataset, and 8 different loss weighting methods (focussing on pure loss-weighting and not gradient-based methods), we demonstrate that UW-SO achieves superior results. Only the brute-force Scalarization approach could occasionally challenge UW-SO, though, Scalarization is not feasible in practice due to the immense tuning demand for a large number of tasks.\nFurthermore, our evaluation reveals insights into the training behavior of existing weighting methods, indicating that larger networks lead to less pronounced differences among MTL methods, and that learning rate tuning for each weighting approach is essential while weight decay tuning seems less influential.\nWe hope that our benchmark lays the ground for fruitful future discussion on MTL and gives guidance for practitioners. Future investigations should focus on further reducing the computational demands of weighting methods while preserving performance, e.g., heuristics to determine a good value for T in UW-SO presents a promising avenue for research. Furthermore, it remains open whether there is a threshold for the \"strength\" of a network at which MTL weighting methods no longer significantly influence performance."}, {"title": "A1 Derivation of UW-SO", "content": "In this section, we provide more details on the analytically optimal derivation of the uncertainty-based task weights. Note that the analytical solution to UW, which we call UW-O, varies (just as UW) for different loss criteria (e.g., $\\mathcal{L}_2$ and Cross-Entropy Loss yield a different constant in the denominator compared to $\\mathcal{L}_1$). However, we simplify this by taking a unified formula (see Eq. 3) not depending on the kind of task as also done in [30,28] for UW. This reduces the implementation overhead. Empirically, we further saw small improvements in the results in first experiments due to the unification.\nNevertheless, we show the detailed derivation of UW-O for $\\mathcal{L}_1$, $\\mathcal{L}_2$, and Cross Entropy Loss in the following paragraphs.\n$\\mathcal{L}_1$ loss In the case of regression tasks evaluated by the $\\mathcal{L}_1$ loss we define our likelihood as a Laplace distribution, thus, the objective is given as [22]\n$\\min_{\\sigma_k} \\frac{1}{\\sigma_k} \\mathcal{L}_k + \\log \\sigma_k$,   (5)\nwhere we minimize the UW loss function with respect to $\\sigma_k$. Taking the derivative and solving for $\\sigma_k$ results in an analytically optimal solution:\n$\\frac{\\partial}{\\partial \\sigma_k} \\frac{1}{\\sigma_k} \\mathcal{L}_k + \\log \\sigma_k = -\\frac{1}{\\sigma_k^2} \\mathcal{L}_k + \\frac{1}{\\sigma_k} = 0$  (6)\n$\\frac{1}{\\sigma_k} = \\frac{1}{\\mathcal{L}_k}$  (7)\n$\\sigma_k = \\mathcal{L}_k$  (8)\nWe assume $\\sigma_k$ to be positive and therefore only allow for positive losses. Note that this limitation comes from UW which only works for losses that are positive and are based on a location scale distribution.\nReplacing $\\sigma_k$ with its analytical solution $\\mathcal{L}_k$ in the total loss function gives the following loss:\n$\\mathcal{L} = \\frac{1}{sg[\\mathcal{L}_k]} \\mathcal{L} + \\log sg[\\mathcal{L}]$,   (9)\nwhere we denote sg as the stopgradient operator to avoid zero gradients of the network updates. Since we do not compute any gradient of the second part of the loss, we can simplify the term, such that\n$\\mathcal{L} = \\frac{1}{sg[\\mathcal{L}_k]} \\mathcal{L}$.  (10)"}, {"title": "A2 Implementation details", "content": "In this Section, we provide details about the used datasets, network architectures, metrics, and training proceeding. Further details can be found in Section A2.\nDatasets We use three common computer vision MTL datasets: two datasets for scene understanding NYUv2 [37] and Cityscapes [10] and a binary attribute dataset CelebA [33]. NYUv2 and Cityscapes comprise the tasks of semantic segmentation and depth estimation. The third task for NYUv2 is surface normals estimation. CelebA constitutes a 40-class binary classification problem.\nArchitectures For NYUv2, we use a SegNet [1], ImageNet pretrained ResNet-50 / ResNet-101 with a DeepLabHead, and the MTAN on top of the SegNet [32]. For Cityscapes, we use a SegNet, a DeepLabV3+ [6] network with pre-trained ResNet-50 / ResNet-101 backbones, and again the MTAN/SegNet. All Single-task learning (STL) baselines are trained with the SegNet. For CelebA, we use a ResNet-18, also for STL.\nMetrics To compare models, we use task-specific metrics and the established $\\Delta_m$-metric [34]. It measures the average relative performance gain of the multi-task model $\\mathcal{M}_m$ w.r.t. a single-task baseline $\\mathcal{M}_b$: $\\Delta_m = \\frac{1}{K} \\sum_{k=1}^{K} (\\frac{\\mathcal{M}_{m,k} - \\mathcal{M}_{b,k}}{\\mathcal{M}_{b,k}})$, where $l_k$ is 1/0 if a higher / lower value is better for criterion $M_k$.\nTwo Evaluation Setups Several papers (e.g., [30,32,48,38]) have used a fixed training protocol for NYUv2 and Cityscapes, with no hyperparameter tuning, averaging results over the last 10 test epochs. In contrast, other studies (e.g., [45,23,40]) advocate for method-specific hyperparameter tuning, which is more relevant for practitioners. Following [45], we perform a thorough hyperparameter search for all methods, selecting the best combination based on the $\\Delta_m$ score and using early stopping on the validation set. For final evaluation, we train on 5 random seeds and report the mean test performance. However, acknowledging other works, we also provide results using the fixed protocol with MTAN/SegNet on NYUv2 and Cityscapes.\nAll models are trained with the Adam optimizer which has been shown to perform advantageous on MTL setups [13]. Compared to [30], we increase the"}, {"title": "A2.3 Hyperparameter optimization", "content": "To avoid a huge computational overhead due to the large amount of hyperparameters involved in the MTO problem, we perform a line search to tune hyperparameters. From a practitioner's point of view, this is much more feasible than a grid search (as employed by Xin et al. [45]) or other techniques. Thus, we perform an extensive and fair comparison of weighting methods in MTL and still keep computational costs as low as possible by using the following approach.\nFirst of all, we start with a fixed weight decay value for each combination of architecture and dataset. To get a reasonable initial weight decay value, we set the hyperparameter based on previous work. For example, we use $\\lambda = 10^{-4}$ for all weighting methods trained with the ResNet-18 on the CelebA dataset, following Liu et al. [31]. Similarly, we set $\\lambda = 10^{-5}$ for the NYUv2 and Cityscapes experiments, following Lin et al. [27]. Assuming the selected weight decay, we perform an extensive grid search of learning rate and weighting method-specific hyperparameters. Latter includes the scalars from the Scalarization approach and the softmax temperature $T$ from UW-SO. Doing so, we ensure to find the best combination of learning rate and task weights, which is crucial to achieving competitive performance (see Figure A1 for the intermediate test results after tuning the learning rate and the method-specific hyperparameters with a fixed weight decay). After finding optimal values for learning rate $\\gamma$ and task weights $w$, we finally tune the weight decay $\\lambda$ using a line search. This ensures that each weighting method is equipped with the right amount of regularization to achieve the best possible results. Figure A2 empirically underlines that our line search approach does not suffer in performance compared to a grid search. This is because adapting the weight decay (if reasonably small, i.e., < 0.0001) results in only a very low variation in performance. In fact, for Scalarization, GLS, and"}, {"title": "A2.4 Discussion on Am metric", "content": "Some MTO approaches improve the result of one task while lowering the performance of others. The $\\Delta_m$ metric shows a balanced result of relative improvement among all tasks. It can thus be seen as an indication which MTO approach yields the best results if all single task metrics are equally important. However, as some tasks are easier to improve on a relative scale, MTO methods focusing on those tasks achieve a better $\\Delta_m$ score. Moreover, tasks with more metrics (e.g. the surface normal for NYUv2) have a higher influence on the $\\Delta_m$ score, which is therefore biased towards these tasks (see Table 2, the best performing methods on the normal task also reach the highest $\\Delta_m$ score).\nAs $\\Delta_m$ is currently the standard evaluation criteria and we have no specified prioritization of any task in our setting we nevertheless report it as our main decision criteria."}, {"title": "A2.5 Two evaluation setups", "content": "Several papers from 2018 to 2022 (e.g., [30,32,48,38]) have adopted a strict training protocol for NYUv2 and Cityscapes to compare MTO methods, which trains with fixed hyperparameters (200 epochs, no tuning of LR, WD=0), without a validation set and report results as the average of the last 10 test epochs. On the contrary, another recent direction in the MTO domain (e.g., [45,23,40]) advocates strongly for method-specific hyperparameter tuning, which is of more relevance for the practitioner. Our decision to follow the latter group is based on the findings of [45] that MTO methods are sensitive to LR and WD and not tuning those \"can create a false perception of performance improvement\" [45, p.2]. We confirm these results in Fig. A1 and A2. Following [45], we perform a thorough hyperparameter search (LR and WD) for all methods choosing the best combination w.r.t. the best $\\Delta_m$ score using early stopping on the validation set on one seed. For the final evaluation, we train on 5 random seeds and report mean test set performance, as done in [45,23,38]. However, acknowledging [30,32,48,38], we also show results using their training protocol with an MTAN/SegNet on NYUv2 and Cityscapes.\nTo the best of our knowledge, this is the first loss weighting benchmark that includes extensive method-specific LR and WD tuning. To give an intuition for the extensiveness, we evaluated 253 experiments only for NYUv2 with the SegNet (including the grid search of LR and MTO hyperparameters, WD search, and 5 seeds). However, acknowledging [30,32,48,38], we also show results using their training protocol with an MTAN/SegNet on NYUv2 and Cityscapes."}, {"title": "A3 Additional Results", "content": "In general, since there is no predefined training and validation protocol in MTL, it is difficult to compare results from different publications. Aspects such as the"}, {"title": "A3.3 Weaknesses of Scalarization", "content": "To demonstrate the high computational cost of the brute force Scalarization approach, we show the resulting experiments required to perform the grid search of learning rate and scalar weights for the NYUv2 dataset in Figure A2. Note that this is part of our line search hyperparameter optimization approach, where we perform a grid search over learning rate and scalars using a fixed weight decay. Thus, the results are slightly worse than those reported in Table 2, as the weight decay is not yet optimized. Also, note that we only show the results for 3 different learning rates, including the best one. When looking at 3 learning rates in combination with 3 different scalars $w_i$ (one for each task), where $\\sum_i w_i = 1$, we have to perform 108 different runs (36 weight combinations per learning rate), which is computationally highly expensive compared to weighting methods that do not include hyperparameters, such as UW. Also, only 3 out of 108 experiments result in a $\\Delta_m$ score below 0. Thus, finding the optimum using Scalarization requires careful tuning of task weights, which becomes computationally intractable and practically not feasible for a large number of tasks.\nFurthermore, the choice of step size is non-trivial and depends on the dataset. While a step size of 0.1 achieves already good results with NYUv2, it is no fine enough for Cityscapes. Thus, we had to tune the weights in Cityscapes with a step size of 0.02 to get proper results."}]}