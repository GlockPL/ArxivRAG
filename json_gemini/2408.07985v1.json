{"title": "Analytical Uncertainty-Based Loss Weighting in Multi-Task Learning", "authors": ["Lukas Kirchdorfer", "Cathrin Elich", "Simon Kutsche", "Heiner Stuckenschmidt", "Lukas Schott", "Jan M. K\u00f6hler"], "abstract": "With the rise of neural networks in various domains, multi-task learning (MTL) gained significant relevance. A key challenge in MTL is balancing individual task losses during neural network training to improve performance and efficiency through knowledge sharing across tasks. To address these challenges, we propose a novel task-weighting method by building on the most prevalent approach of Uncertainty Weighting and computing analytically optimal uncertainty-based weights, normalized by a softmax function with tunable temperature. Our approach yields comparable results to the combinatorially prohibitive, brute-force approach of Scalarization while offering a more cost-effective yet high-performing alternative. We conduct an extensive benchmark on various datasets and architectures. Our method consistently outperforms six other common weighting methods. Furthermore, we report noteworthy experimental findings for the practical application of MTL. For example, larger networks diminish the influence of weighting methods, and tuning the weight decay has a low impact compared to the learning rate.", "sections": [{"title": "1 Introduction", "content": "Multi-task learning (MTL) aims at solving multiple tasks simultaneously in a mutually beneficial manner. Intuitively, related tasks should share their knowledge and unrelated tasks should be processed more isolated to efficiently use the available data and compute resources.\nOne of the major challenges is to find the right (implicit or explicit) balance between individual tasks to gain a good performance across tasks. Recent approaches in deep learning tackle this problem from various angles, such as adapting the network architecture [19,17] or resolving conflicts between task-specific gradients during optimization [30,48]. A methodologically simple yet effective approach is based on providing task-specific weights during optimization, usually termed as loss weighting.\nNot explicitly weighting tasks, also termed Equal Weighting (EW), has potential pitfalls. Different tasks could be measured with different loss functions,"}, {"title": "2 Related Work", "content": "MTL considers training of tasks simultaneously by efficiently distributing resources and sharing knowledge between them [4,39,44]. There are typically two orthogonal research directions found in the recent literature: One line of work considers multi-task architectures which focus on how features can be shared between tasks [12,47,36,32,46,34]. In this work, we make use of the basic hard-parameter sharing network structure, which consists of a fully-shared backbone and task-specific heads. We expect our method to be combinable with any other MTL network architecture. In contrast, MTO methods aim to balance the tasks to tackle the negative transfer that might occur between them. These methods can further be distinguished as gradient-based and loss weighting methods:\nLoss weighting methods address the challenge of weighting task-specific losses appropriately. Most relevant for our work, Uncertainty Weighting (UW) [22] weights different losses by learning the respective task-specific homoscedastic uncertainty. We adapt this by computing task weights based on the analytically optimal solution of UW and normalizing the results through a softmax function (Section 3.2). Lin et al. [2] argue that random sampling of loss weights (RLW) should be considered a relevant baseline. Alternatively to the weighted sum of losses, the geometric loss strategy (GLS) [9] computes the geometric mean. While this method does not require any additional hyperparameters, it is numerically sensitive to a large number of tasks. Dynamic weight averaging (DWA) [32] assigns a higher weight to tasks whose respective loss shows a slower decrease compared to other task losses. Impartial Multi-Task Learning (IMTL-L) [31] learns the scaling factor of the losses via gradient descent such that scaled losses would become constant for all tasks. The brute force method Scalarization [45] which searches all possible combinations of fixed loss weights has shown competitive performance compared to current automated MTO methods. Other loss weighting methods are proposed in [24,26,14,16].\nGradient-based methods make direct use of task-specific gradients to either determine individual scaling factors which are applied on the task-wise gradients directly [7,40,31,38,35,41] or perform manipulations on the gradients to resolve potential alignment conflicts between them [31,8,21,42,29]. They mostly differ in the type of strategy used to handle these conflicts, such as projecting conflicting gradients onto the normal plane [48], or considering trade-offs between average and worst-case losses [30]. A disadvantage of these methods is that computing task-wise gradients is computationally expensive. In this paper, we do not consider gradient-based optimization methods as they have been shown to not outperform the simple loss weighting Scalarization approach [45], and Kurin et al. [23] report that loss weighting methods commonly have significantly shorter training times [8] which is relevant in practice."}, {"title": "3 Background and Method", "content": "In MTL, we aim to resolve K tasks for some input data point $x \\in \\mathcal{X}$. For this, $x$ is mapped to labels $\\{y_k \\in \\mathcal{Y}_k\\}_{k \\in [1,K]}$ simultaneously using specific mappings $\\{f_k: \\mathcal{X} \\rightarrow \\mathcal{Y}_k\\}$. We assume hard task-shared parameters $\\theta$ in a hydra-like neural network architecture. This means all tasks receive the same intermediate feature $z = f(x;\\theta)$ from the shared backbone and each task head yields output $f_k(x) = f(z; \\theta_k)$ with task-specific parameters $\\theta_k$ [39].\nThe network is trained by considering all tasks' losses $L_k$. Naively summing up these losses (the equal weighting method) typically leads to imbalanced learning as tasks with high loss magnitude might dominate the training. The goal is thus to find optimal (dynamic) loss weights $w_k$ for all tasks to optimize the loss $L = \\sum_k w_k L_k$ in a way that tasks benefit w.r.t. their final performance metrics."}, {"title": "3.1 Weaknesses of Uncertainty Weighting and Scalarization", "content": "UW [22] is one famous MTO approach with over 3.2k citations (May'24) and yields competitive performance (see Section 4.2) besides its simplicity. However, UW also shows some drawbacks: First, we observe that UW can be affected by bad initialization / inertia. As uncertainty weights are usually initialized equally for all tasks, it can slow down their progress toward reaching the best values for each task and epoch using gradient descent, especially as task weights often differ in orders of magnitude. We refer to this phenomenon as update inertia. To demonstrate this phenomenon empirically, we focus on the development of task weights $w_t$ of the NYUv2 dataset with two different initializations of $w_t$ (Figure 1). In the first initialization setting (blue line, UW S1), we consider the usual initialization of $w_k = 0.8$ [28]. In the other setting (orange line, UW S2), we initialize the task weights higher and choose the initialization values equal to the final $w_t$ of a previous run. We observe that the learned task weights develop differently due to the different initializations. It takes roughly 100 epochs (1/4 of the whole training) to recover, i.e., the blue and the orange line then behave similar. As both experiments receive the same non-weighted losses from each task at the beginning we would expect that its weightings $w_k$ adapt quickly to the task loss ignoring the wrong initialization value. Thus, a non-optimal initialization has a direct impact on the training dynamics due to the update inertia of the task weights. Another example for update inertia is discussed in section A3.2 for the CelebA dataset.\nSecond, we observe that UW is prone to overfitting - a detailed discussion is presented after the benchmark results in Section 4.2.\nLastly, we give a first hint with a toy example in Section A3.2 that UW does not only depend on task-wise aleatoric homoscedastic uncertainty but also shows a model complexity dependence.\nScalarization is demonstrated [45] to yield competitive performance on a range of MTL problems, but there are shortcomings: As mentioned by the authors, manually tuning loss weights by performing an extensive grid search is computationally expensive. Besides, it can only be applied to problems with a"}, {"title": "3.2 Our contribution: Soft Optimal Uncertainty Weighting", "content": "AS UW [22] shows inertia with task uncertainty $\\sigma_t$ being updated gradually through gradient descent, we are determining which $\\sigma_t$ values would analytically minimize the total Loss $L$ in a given batch. These values are then normalized using a softmax function with temperature.\nUW-O: Minimizing the total loss in UW The approach UW [22] weights losses based on their task-specific homoscedastic aleatoric uncertainty. The exact weighting formulae depend on the type of loss. For instance, for tasks with an $L_1$ loss it can be derived by assuming a Laplace posterior distribution and identifying $\\sigma_k$ with the uncertainty of each task $k \\in K$, treating them as learnable parameters that are input independent\n$L = \\sum_{k \\in K} \\frac{1}{\\sigma_k} L_k + \\log \\sigma_k \\qquad (1)$\nHere, $L_k$ is the task-specific loss, e.g., a mean absolute error. Intuitively, the $\\sigma_k$ in the first term allows to down-weight difficult tasks by increasing the uncertainty"}, {"title": "4 Experiments and Results", "content": "In this Section, we provide details about the used datasets, network architectures, metrics, and training proceeding. Further details can be found in Section A2.\nDatasets We use three common computer vision MTL datasets: two datasets for scene understanding NYUv2 [37] and Cityscapes [10] and a binary attribute dataset CelebA [33]. NYUv2 and Cityscapes comprise the tasks of semantic segmentation and depth estimation. The third task for NYUv2 is surface normals estimation. CelebA constitutes a 40-class binary classification problem.\nArchitectures For NYUv2, we use a SegNet [1], ImageNet pretrained ResNet-50 / ResNet-101 with a DeepLabHead, and the MTAN on top of the SegNet [32]. For Cityscapes, we use a SegNet, a DeepLabV3+ [6] network with pre-trained ResNet-50 / ResNet-101 backbones, and again the MTAN/SegNet. All Single-task learning (STL) baselines are trained with the SegNet. For CelebA, we use a ResNet-18, also for STL.\nMetrics To compare models, we use task-specific metrics and the established $\\Delta_m$-metric [34]. It measures the average relative performance gain of the multi-task model $M_m$ w.r.t. a single-task baseline $M_b$: $\\Delta_m = \\frac{1}{K} \\sum_{k=1}^{K} (\\frac{M_{m,k} - M_{b,k}}{M_{b,k}})$, where $l_k$ is 1/0 if a higher / lower value is better for criterion $M_k$.\nTwo Evaluation Setups Several papers (e.g., [30,32,48,38]) have used a fixed training protocol for NYUv2 and Cityscapes, with no hyperparameter tuning, averaging results over the last 10 test epochs. In contrast, other studies (e.g., [45,23,40]) advocate for method-specific hyperparameter tuning, which is more relevant for practitioners. Following [45], we perform a thorough hyperparameter search for all methods, selecting the best combination based on the $\\Delta_m$ score and using early stopping on the validation set. For final evaluation, we train on 5 random seeds and report the mean test performance. However, acknowledging other works, we also provide results using the fixed protocol with MTAN/SegNet on NYUv2 and Cityscapes.\nAll models are trained with the Adam optimizer which has been shown to perform advantageous on MTL setups [13]. Compared to [30], we increase the"}, {"title": "4.1 Experimental setup", "content": "Analytical Uncertainty-Based Loss Weighting in Multi-Task Learning\nFor later reference, we name this intermediate result Optimal Uncertainty Weighting (UW-O), where optimal refers to the analytical loss minimum. Interestingly, we found that there exist three approaches that lead to a similar solution: 1) IMTL-L [31] aims to have each weighted loss $w_k L_k$ scaled to 1, though, they learn $w_k$ using gradient descent. 2) Dual-balancing [26] transforms the loss to $\\log L$ to normalize over different scales. Taking the gradient of $\\log L$ is equivalent to taking the gradient of $\\frac{\\text{sg} [L]}{L_k} = 1$ for all tasks $k$; for the $L = || \\cdot ||_1$ loss it is equivalent to equation 3. Thus, in dual-balancing the gradient is scaled whereas we scale the loss $L$. 3) EMA [24] scale the loss by the Exponential Moving Average which is identical to the Inverse Loss when the hyperparameter is $\\beta = 1$."}, {"title": "4.2 Common loss weighting methods benchmark", "content": "We compare our methods to the most common loss weighting approaches. Overall, UW-SO consistently performs best or second-best across all datasets and architectures w.r.t. the $\\Delta_m$ metric. This holds for the hyperparameter-tuned experiments as well as for those with the fixed training protocol. Occasionally, our method gets beaten by the computationally expensive Scalarization approach, especially when using the SegNet architecture. Noticeably, performance differences between MTO algorithms decrease for larger networks.\nCityscapes On Cityscapes, UW-SO achieves the best $\\Delta_m$ score when trained on both ResNet architectures as well as on the MTAN/SegNet with the fixed hyperparameters, and the second-best behind Scalarization when trained on the SegNet (see Table 1). In contrast to NYUv2 (see Table 2), the performance of Scalarization decreases for larger networks due to weak results on the difficult and highly sensitive relative depth error. While an even more fine-grained search of task weights might yield better results, we argue that our weight search with"}, {"title": "4.3 Ablation Studies", "content": "In the following, we present some ablation studies about our method UW-SO. Further ablation studies can be found in sections A3.5 till A3.9.\nInfluence of softmax To demonstrate the influence of the softmax function on the inverse loss weights, we compare the $\\Delta_m$ scores using UW-O and UW-SO across all datasets and architectures in Table 4. UW-SO outperforms UW-O in all experiments, indicating the performance gain provided by the tempered softmax function. However, we want to emphasize that this is not due to a significantly worse performance of UW-O compared to other MTO methods. Therefore, we also present the results using IMTL-L, which, like UW-O, aims to scale each weighted task loss to 1, but unlike UW-O, it learns rather than computing the weights. Interestingly, none of the two methods can outperform the other one, indicating that despite UW-O's simplicity, it still provides reasonable results compared to existing methods. Furthermore, in a toy example, we find that UW-O is particularly strong in dealing with extreme loss magnitude differences, at which most of the previous methods fail (Section A3.4). However, this is of"}, {"title": "5 Discussion and Conclusion", "content": "In this paper, we introduce UW-SO, a new method for weighting losses in Multi-Task Learning (MTL). Derived from the analytical solution of Uncertainty Weighting, UW-SO applies the tempered softmax function to the inverse of the losses to effectively weight tasks. In an extensive benchmark with 3 datasets, up to 4 architectures per dataset, and 8 different loss weighting methods (focussing on pure loss-weighting and not gradient-based methods), we demonstrate that UW-SO achieves superior results. Only the brute-force Scalarization approach could occasionally challenge UW-SO, though, Scalarization is not feasible in practice due to the immense tuning demand for a large number of tasks.\nFurthermore, our evaluation reveals insights into the training behavior of existing weighting methods, indicating that larger networks lead to less pronounced differences among MTL methods, and that learning rate tuning for each weighting approach is essential while weight decay tuning seems less influential.\nWe hope that our benchmark lays the ground for fruitful future discussion on MTL and gives guidance for practitioners. Future investigations should focus on further reducing the computational demands of weighting methods while preserving performance, e.g., heuristics to determine a good value for T in UW-SO presents a promising avenue for research. Furthermore, it remains open whether there is a threshold for the \"strength\" of a network at which MTL weighting methods no longer significantly influence performance."}]}