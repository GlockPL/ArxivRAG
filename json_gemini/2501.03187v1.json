{"title": "Turn-based Multi-Agent Reinforcement Learning Model Checking", "authors": ["Dennis Gross"], "abstract": "In this paper, we propose a novel approach for verifying the compliance of turn-based multi-agent reinforcement learning (TMARL) agents with complex requirements in stochastic multiplayer games. Our method overcomes the limitations of existing verification approaches, which are inadequate for dealing with TMARL agents and not scalable to large games with multiple agents. Our approach relies on tight integration of TMARL and a verification technique referred to as model checking. We demonstrate the effectiveness and scalability of our technique through experiments in different types of environments. Our experiments show that our method is suited to verify TMARL agents and scales better than naive monolithic model checking.", "sections": [{"title": "1 INTRODUCTION", "content": "AI technology has revolutionized the game industry (Berner et al., 2019), enabling the creation of agents that can outperform human players using turn-based multi-agent reinforcement learning (TMARL) (Silver et al., 2016). TMARL consists of multiple agents, where each one learns a near-optimal policy based on its own objective by making observations and gaining rewards through turn-based interactions with the environment (Wong et al., 2022).\nThe strength of these agents can also be a problem, limiting the gameplay experience and hindering the design of high-quality games with non-player characters (NPCs) (Svelch, 2020; Nam et al., 2022). Game developers want to ensure that their TMARL agents behave as intended, and tracking their rewards can allow them to fine-tune their performance. However, rewards are not expressive enough to encode more complex requirements for TMARL agents, such as ensuring that a specific sequence of events occurs in a particular order (Littman et al., 2017; Hahn et al., 2019; Hasanbeig et al., 2020; Vamplew et al., 2022).\nThis paper addresses the challenge of verifying the compliance of TMARL agents with complex requirements by combining TMARL with rigorous model checking (Baier and Katoen, 2008). Rigorous model checking is a formal verification technique that uses mathematical models to verify the correctness of a system with respect to a given property. It is called \"rigorous\" because it provides guarantees of correctness based on rigorous mathematical reasoning and logical deductions. In the context of this paper, rigorous model checking is used to verify TMARL agents. The system being verified is the TMARL system, which is modeled as a Markov decision process (MDP) treating the collection of agents as a joint agent, and the property is the set of requirements that the agents must satisfy. Our proposed method\u00b9 supports a broad range of properties that can be expressed by probabilistic computation tree logic (PCTL; Hansson and Jonsson, 1994). We evaluate our method on different TMARL benchmarks and show that it outperforms naive monolithic model checking\u00b2.\nTo summarize, the main contributions of this paper are:\n1. rigorous model checking of TMARL agents,\n2. a method that outperforms naive monolithic model checking on different benchmarks.\nThe paper is structured in the following way. First, we summarize the related work and position our paper in it. Second, we explain the fundamentals of our technique. Then, we present the TMARL model checking method and describe its functionalities and limitations. After that, we evaluate"}, {"title": "2 RELATED WORK", "content": "PRISM (Kwiatkowska et al.,\n2011) and\nStorm (Hensel et al., 2022) are tools for formal modeling and analysis of systems that exhibit uncertain behavior. PRISM is also a language for modeling discrete-time Markov chains (DTMCs) and MDPs. We use PRISM to model the TMARL environments as MDPs. Until now, PRISM and Storm do not allow verifying TMARL agents. PRISM-games (Kwiatkowska et al., 2018) is an extension of PRISM to verify stochastic multi-player games (including turn-based stochastic multi-player games). Various works about turn-based stochastic game model checking have been published (Kwiatkowska et al.,\n2022, 2019; Li et al., 2020; Hansen et al., 2013; Kucera, 2011). None of them focus on TMARL systems. TMARL has been applied to multiple turn-based games (Wender and Watson,\n2008; Pagalyte et al., 2020; Silver et al., 2016; Videga\u00edn and Garc\u00eda-S\u00e1nchez, 2021; Pagalyte et al.,\n2020). The major work about model checking for RL agents focuses on single RL agents (Wang et al.,\n2020; Hasanbeig et al., 2020; Hahn et al., 2019; Hasanbeig et al.,\n2019;\nFulton and Platzer,\n2019; Sadigh et al., 2014; Bouton et al., 2019; Chatterjee et al., 2017). However, model checking work exists for cooperative MARL (Riley et al.,\n2021a; Khan et al., 2019; Riley et al., 2021b), but no work for TMARL. Therefore, with our research, we try to close the gap between TMARL and model checking."}, {"title": "3 BACKGROUND", "content": "In this section, we introduce the fundamentals of our work. We begin by summarizing the modeling and analysis of probabilistic systems, which forms the basis of our approach to check TMARL agents. We then describe TMARL in more detail."}, {"title": "3.1 Probabilistic Systems", "content": "A probability distribution over a set X is a function \u03bc : X \u2192 [0,1] with $E_{x \\in x}\u03bc(x) = 1$. The set of all distributions on X is denoted Distr(X).\nDefinition 3.1 (Markov Decision Process). A Markov decision process (MDP) is a tuple M = (S,so,A,T, rew) where S is a finite, nonempty set of states; so \u2208 S is an initial state; A is a finite set of actions; T: S\u00d7A \u2192 Distr(S) is a probability transition function; rew: SXAR is a reward function.\nWe employ a factored state representation where each state s is a vector of features (f1, f2, ..., fn) where each feature $f_i \\in Z$ for 1 \u2264 i \u2264 n (n is the dimension of the state). The available actions in s \u2208 S are A(s) = {a \u2208 A | T(s,a) \u2260 1}. An MDP with only one action per state (\u2200s \u2208 S : |A(s)| = 1) is a DTMC. A path of an MDP M is an (in)finite sequence t = $s_0 \\xrightarrow{a_0, r_0} s_1 \\xrightarrow{a_1, r_1} ...$, where si \u2208 S, ai \u2208 A(si),\nri := rew(si,ai), and T(si,ai)(si+1) \u2260 0. A state s'\nis reachable from state s if there exists a path t from state s to state s'. We say a state s is reachable if s is reachable from so.\nDefinition 3.2 (Policy). A memoryless deterministic policy for an MDP M is a function \u03c0: S \u2192 A that maps a state s \u2208 S to an action a \u2208 A(s).\nApplying a policy \u03c0 to an MDP M yields an induced DTMC, denoted as D, where all non-determinism is resolved. A state s is reachable by a policy n if s is reachable in the DTMC induced by \u03c0. We specify the properties of a DTMC via the specification language PCTL (Wang et al., 2020).\nDefinition 3.3 (PCTL Syntax). Let AP be a set of atomic propositions. The following grammar defines a state formula: \u03a6 ::= true | a | \u04241 \u039b\n\u04242 | \u00ac\u0424 | $P_{\\Delta p}$($) | $P_{max \\Delta p}$($) | $P_{min \\Delta p}$($) where \u03b1 \u0395 \u0391\u03a1,\u0394\u0395\n{<,>,\u2264,\u2265}, p\u2208 [0,1] is a threshold, and o is a\npath formula which is formed according to the following grammar \u00a2 ::= \u0425\u0424 | \u03a61 U \u03a62 | \u03a61 $U_{\\sigma}$ \u03a62 | G\u0424\nwith \u03c3 = {<,\u2264}.\nFor MDPS, PCTL formulae are interpreted over the states of the induced DTMC of an MDP and a policy. In a slight abuse of notation, we use PCTL state formulas to denote probability values. That is, we sometimes write Pop($) where we omit the threshold p. For instance, in this paper, P(F collision) denotes the reachability probability of eventually running into a collision. There exist a variety of model checking algorithms for verifying PCTL properties (Courcoubetis and Yannakakis, 1988, 1995). PRISM (Kwiatkowska et al., 2011) and Storm (Hensel et al., 2022) offer efficient and"}, {"title": "3.2 Turn-based Multi-Agent\nReinforcement Learning (TMARL)", "content": "We now introduce TMARL. The standard learning goal for RL is to find a policy \u03c0 in an MDP such that \u03c0maximizes the expected discounted reward, that is, \u0395[\u03a3\u2080\u2191L \u03b3\u2191tR\u2191t], where \u03b3 with 0 \u2264 y \u2264 1 is the discount factor, R\u2081 is the reward at time t, and L is the total number of steps (Kaelbling et al., 1996).\nTMARL extends the RL idea to find near-optimal agent policies \u03c0; in a TSG setting (compare Figure 1 with Figure 2). Each policy \u03c0\u2081 is represented by a neural network. A neural network is a function parameterized by weights \u03b8i. The neural network policy \u03c0\u2081 can be trained by minimizing a sequence of loss functions J(\u03b8i, s, ai) (Mnih et al., 2013)."}, {"title": "4 Model Checking of TMARL\nagents", "content": "We now describe how to verify trained TMARL agents. Recall, the joint policy \u03c0 induced by the set of all agent policies {\u03c0i}i\u2208l is a single policy \u03c0. The tool COOL-MC (Gross et al., 2022) allows model checking of a single RL policy against a user-provided PCTL property P($) and MDP M. Thereby, it builds the induced DTMC D incrementally (Cassez et al., 2005).\nTo verify a TMARL system, we model it as a normal MDP. We have to extend the MDP with an additional feature called turn that controls which agent's turn it is. To support joint policies \u03c0(s), and therefore multiple TMARL agents, we created a joint policy wrapper that queries the corresponding TMARL agent policy at every turn (see Figure 3). With the joint policy wrapper, we build the induced DTMC the following way. For every state s that is reachable via the joint policy \u03c0, we query for an action a = \u03c0(s). In the underlying MDP M, only states s' that may be reached via that action a \u2208 A(s) are expanded. The resulting DTMC induced by M and is fully deterministic, as no action choices are left open and ready for efficient model checking.\nLimitations. Our method allows the model checking of probabilistic policies by always choosing the action with the highest probability at each state. We"}, {"title": "5 EXPERIMENTS", "content": "We now evaluate our proposed model checking method in multiple environments."}, {"title": "5.1 Setup", "content": "In this section, we provide an overview of the experimental setup. We first introduce the environments, followed by the trained TMARL agents. Next, we describe the model checking properties that we used, and finally, we provide details about the technical setup.\nEnvironments. Pokemon is an environment from the game franchise Pokemon that was developed by Game Freak and published by Nintendo in 1996 (Freak, 1996). It is used in the Showdown AI competition (Lee and Togelius, 2017). In a Pokemon battle, two agents fight one another until the Pokemon of one agent is knocked out (see Figure 4). The impact of randomness in Pokemon is significant, and much of the game's competitive strategy comes from accurately estimating stochastic events. The damage calculation formula for attacks includes a random multiplier between the values of 0.85 and 1.0. Each Pokemon has four different attack actions (tackle, punch, poison, sleep) and can use items (for example, heal pots to recover its hit points (HP)). The attacks"}, {"title": "5.2 Analysis", "content": "In this section, we address the following research questions:\n1. Does our proposed method scale better than naive monolithic model checking?\n2. How many TMARL agents can our method handle?\n3. Do the TMARL agents perform specific game moves?\nWe will provide detailed answers to these questions and discuss the implications of our findings.\nDoes our proposed method scale better than naive monolithic model checking? In this experiment,\nwe compare our method with a naive monolithic model checking in the Pokemon environment. For a whole Pokemon battle (starting with HP=100, unlimited tackle attacks, 5 punch attacks, 2 sleep attacks, 2 poison attacks, and 3 heal pots), naive monolithic model checking runs out of memory. On the other hand, our method runs out of time (time out after 24 hours). However, we can train TMARL agents in the Pokemon environment and can, for example, analyze the end game. For instance, our method allows the model checking of environments with HP of 20 and 1 heal pot left, and we can quantify the probability that Pokemon 1 uses the heal pot with 0.65 (see useHeal20P1 in Table 1). On the other hand, for naive monolithic model checking, it is impossible to extract this probability because it runs out of memory with a model that contains 31,502,736 states and 51,6547,296 transitions. However, at some point, our model checking method is also limited by the size of the induced DTMC and runs out of memory (Gross et al., 2022).\nHow many TMARL agents can our method handle? We perform this experiment in the MABP environment with multiple agents because, in this environment, it is straightforward to show how our"}, {"title": "6 CONCLUSION", "content": "In this work, we presented an analytical method for model checking TMARL agents. Our method is based on constructing an induced DTMC from the TMARL system and using probabilistic model checking techniques to verify the behavior of the agents. We applied our method to multiple environments and found that it is able to accurately verify the behavior of the TMARL agents. Our method can handle scenarios that can not be verified using naive monolithic model checking methods. However, at some point, our technique is limited by the size of the induced DTMC and the number of TMARL agents in the system.\nIn future work, we plan to extend our method to incorporate safe TMARL approaches. This has been previously done in the single agent RL domain (Jin et al., 2022; Jothimurugan et al., 2022), and we believe it can also be applied to TMARL systems. We also plan to combine our proposed method with interpretable RL techniques (Davoodi and Komeili, 2021) to better understand the trained TMARL agents. This could provide valuable insights into the behavior of the agents."}]}