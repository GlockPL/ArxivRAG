{"title": "Robust Multivariate Time Series Forecasting against Intra- and Inter-Series Transitional Shift", "authors": ["Hui He", "Qi Zhang", "Kun Yi", "Xiaojun Xue", "Shoujin Wang", "Liang Hu", "Longbing Cao"], "abstract": "The non-stationary nature of real-world Multivariate Time Series (MTS) data presents forecasting models with a formidable challenge of the time-variant distribution of time series, referred to as distribution shift. Existing studies on the distribution shift mostly adhere to adaptive normalization techniques for alleviating temporal mean and covariance shifts or time-variant modeling for capturing temporal shifts. Despite improving model generalization, these normalization-based methods often assume a time-invariant transition between outputs and inputs but disregard specific intra-/inter-series correlations, while time-variant models overlook the intrinsic causes of the distribution shift. This limits model expressiveness and interpretability of tackling the distribution shift for MTS forecasting. To mitigate such a dilemma, we present a unified Probabilistic Graphical Model to Jointly capturing intra-/inter-series correlations and modeling the time-variant transitional distribution, and instantiate a neural framework called JointPGM for non-stationary MTS forecasting. Specifically, JointPGM first employs multiple Fourier basis functions to learn dynamic time factors and designs two distinct learners: intra-series and inter-series learners. The intra-series learner effectively captures temporal dynamics by utilizing temporal gates, while the inter-series learner explicitly models spatial dynamics through multi-hop propagation, incorporating Gumbel-softmax sampling. These two types of series dynamics are subsequently fused into a latent variable, which is inversely employed to infer time factors, generate final prediction, and perform reconstruction. We validate the effectiveness and efficiency of JointPGM through extensive experiments on six highly non-stationary MTS datasets, achieving state-of-the-art forecasting performance of MTS forecasting.", "sections": [{"title": "I. INTRODUCTION", "content": "Multivariate Time Series (MTS) forecasting has been playing an increasingly ubiquitous role in real-world applications, such as weather condition estimation [1], stock trend analysis [2], [3], electricity consumption planning [4], and traffic flow and speed prediction [5], [6]. Impressively, various deep learning-based approaches have emerged and led to a surge in deep MTS forecasting models. These approaches elaborately capture complex temporal variations by Temporal Convolution Networks (TCNs) [7]\u2013[9], Recurrent Neural Networks (RNNs) [10], [11], and Transformers [12]-[15], or explore specific variable-wise dependencies by Graph Neural Networks (GNNs) [16]\u2013[20]. Despite the remarkable performance, they fall short in adapting to real-world scenarios where the distributions (a.k.a., statistical properties) of time series change over time due to dynamic generation mechanisms. This phenomenon, known as distribution shift [21]\u2013[23], exposes time series' highly dynamic and non-stationary nature. It poses significant challenges for forecasting models in effective generalization to varying distributions. Such vulnerability to rapid distributional changes [24] ultimately results in a dramatic decline in forecasting accuracy over time [25].\nResearchers have explored two primary categories of approaches to tackle the distribution shift in MTS forecasting. The first category involves normalization techniques to align time series instances based on the Gaussian assumption. They normalize the input and denormalize the output using adaptively learned statistics (e.g., mean and variance) [21], [22], [26]\u2013[28] to alleviate the temporal mean and covariance shift among instances or between inputs and outputs. Most of these methods, however, assume a time-invariant transitional distribution between output predictions and input observations, i.e.,  P(x_{u:u+H}|X_{u-L:u}) = P(X_{v:v+H}|X_{v-L:v}) at any two steps u \u2260 v. This assumption severely simplifies the non-stationarity of time series and is not consistent with the practical distribution shift [29], [30]. Give an easily understandable example: In stock prediction, the financial factors P(x_t) naturally change due to market fluctuations. Meanwhile, economic laws P(x_{t:t+h}|x_{t-L:t}) are also vulnerable to abrupt policies, such as government price controls. Additionally, these methods focus on exploring the variable-wise data distribution, overlooking specific intra-/inter-series correlations [31], [32]. As a result, they struggle to effectively address the counterpart non-stationarity, especially the distribution shift along with inter-series dynamics.\nThe second category aims to model time-variant transitional distribution, i.e., P_t(x_{t:t+H}|X_{t-L:t}) adapted to any time step t, to improve models' temporal generalization. Many advanced models within this category have integrated time information to enhance forecasting performance, indicating that time information enables the models to effectively capture the time-variant characteristics of time series, thereby alleviating the issue of non-stationarity [12], [13], [15]. Herein, some models incorporate temporal meta-knowledge to correct the bias caused by the distribution shift within a discriminative meta-"}, {"title": "II. RELATED WORK", "content": "Multivariate time series (MTS) forecasting is a longstanding research topic [39], [40]. Initially, traditional statistical models such as Gaussian process (GP) [41] have been proposed for their appealing simplicity and interpretability. Recently, with the bloom of deep learning, many deep models with elaboratively designed architectures have made great breakthroughs in capturing intra- and inter-series correlations for MTS forecasting. On one hand, the RNN- [10], [11] and TCN-based [7]\u2013[9] models have shown competitiveness in modeling complex temporal relationships. However, due to their recurrent structures or the locality property of one-dimensional convolutional kernels, they are limited in handling long-term dependencies. Soon afterward, Transformer and its variants [12]\u2013[15], [42] have achieved superior performance on MTS forecasting, particularly notable in scenarios with long prediction lengths. They focus on renovating the canonical structure and designing a novel attention mechanism to reduce the quadratic complexity while automatically learning the correlations between elements in a series. Despite the complicated design of Transformer-based models, recent MLP-based models [43]\u2013[46] with simple structure and low complexity can surpass previous models across various common benchmarks for MTS forecasting. Another crucial aspect of MTS forecasting involves capturing the correlations among multiple time series. Current models highly depend on GNNs [4], [16]\u2013[20], [47] or ordered tree [48] due to their remarkable\nA. Deep Models for Multivariate Time Series Forecasting"}, {"title": "B. Improving Robustness against Distribution Shifts", "content": "Despite many remarkable deep models, MTS forecasting still suffers from severe distribution shifts [51] considering the distribution of real-world series changes temporally. To improve robustness over varying distributions, one category of widely-explored methods [21], [22], [26]-[28], [52] stationarize deep model inputs by the normalization techniques. For example, RevIN [27] proposes a reversible instance normalization technique to reduce temporal distribution shift. Based on RevIN, Dish-TS [21] designs a dual coefficient network to learn two sets of distribution coefficients and captures the distribution shift between inputs and outputs. Stationary [22] adopts de-stationary attention to handle the over-stationarization issue which may damage the model's capability of modeling specific temporal dependency. SAN [28] utilizes slice-level adaptive normalization to mitigate non-stationarity. However, these methods typically assume a time-invariant transitional distribution and overlook the distribution shift caused by inter-series dynamics. Another category [2], [33], [34] learns to model time-variant transitional distribution by incorporating temporal meta-knowledge to correct the bias caused by distribution shift in a discriminative meta-learning framework, which is generally designed for bridging the gap between the training and test data. More recently, Koopman predictors Koopa [35] and KNF [53] employ Koopman operators as linear portraits of implicit transitions to capture time-invariant and time-variant dynamics. While these models model the time-variant transitional distributions, such coarse-grained modeling fails to reveal the intrinsic causes of the transitional distribution, limiting the models' interpretability and expressiveness. In this paper, we propose JointPGM to model the practical transitional distribution and decompose it based on the prevalent approach of learning intra-/inter-series correlations."}, {"title": "III. PROBLEM FORMULATIONS", "content": "In this section, we start with the formulations of MTS forecasting and define the concepts central to distribution shift. Detailed notations are summarized in Table V in Appendix A.\nLet a regularly sampled time series dataset with a total of N distinct time series and T time steps be denoted as [\u00e6^{(1)}, ..., x^{(i)}, ..., x^{(N)}] \u2208 R^{N\u00d7T}, where x^{(i)} \u2208 R^T denotes the sequence values of time series i at T time steps. Given a lookback window of length-L and a horizon window of length-H, the multivariate time series forecasting involves utilizing historical multivariate observations X_{t\u2212L:t} = {x_{t\u2212L:t}^{(i)}} to predict their future multivariate values X_{t:t+H} = {x_{t:t+H}^{(i)}} at time step t. The forecasting process can be formulated as:\nX_{t:t+H} = F_\u0398(X_{t-L:t}) = F_\u0398({x_{t-L:t}^{(i)}})\nwhere the function map F_\u0398 : R^{N\u00d7L} \u2192 R^{N\u00d7H} can be regarded as a forecasting model parameterized by \u0398.\nDistribution Shift in Time Series. Recall the intuitive financial example mentioned in Section I, where the economic laws are vulnerable to abrupt policy changes. Therefore, we propose to involve the more rational and practical transitional shift assumption and further decompose the integrated transitional shift in time series into two types at a finer granularity, namely, intra-series transitional shift and inter-series transitional shift, with their definitions provided below.\nGiven the ith time series x^{(i)}, which can be split into several lookback windows {x_{t-L:t}^{(i)}}_{t=L}^{T-H} and their corresponding horizon windows {x_{t:t+H}^{(i)}}_{t=L}^{T-H}. Intra-series Transitional Shift is referred to the case that the transitional distribution P(x_{u+H}^{(i)}/x_{u-L:u}^{(i)}) \u2260 P(x_{v+H}^{(i)}/x_{v-L:v}^{(i)}) for any two time steps u and v with L\u2264u\u2260v\u2264T\u2013 H.\nGiven the ith time series x^{(i)} with its complementary set x^{(i)}. Similarly, x^{(i)} can be split into several lookback windows {x_{t-L:t}^{(i)}}_{t=L}^{T-H} and their corresponding horizon windows {x_{t:t+H}^{(i)}}_{t=L}^{T-H}. Inter-series Transitional Shift is referred to the case that the transitional distribution P(x_{u:u+H}^{(i)}/X_{u-L:u}^{(-i)}) \u2260 P(x_{v:v+H}^{(i)}/X_{v-L:v}^{(-i)}) for any two time steps u and v with L<u\u2260v\u2264T \u2013 H.\nThe combination of these two definitions fully describes the complex distribution shifts encountered in reality. The former indicates the variations in transitional distribution for each series, while the latter reflects the variations in transitional distribution among different series. Since characterizing the local relationship between pairwise series in Definition 2 is overly complex, we describe the relationship between each series and its complementary set from a global perspective.\nMultivariate Time Series Forecasting.\nDefinition 1 (Intra-series Transitional Shift).\nDefinition 2 (Inter-series Transitional Shift).\nRemark 1."}, {"title": "IV. METHODOLOGY", "content": "In this section, we first present our tailored PGM and formally analyze the distinctions between normalization-based methods, time-variant models, and ours in Section IV-A. In Section IV-B, we introduce the corresponding instantiated dual-encoder architecture. Finally, we decompose the learning objective based on the PGM and our purpose in Section IV-C.\nRecall the notations in non-stationary MTS forecasting task, i.e., X_{t\u2212L:t}, X_{t:t+H}, and t, they correspond to the latent variable Z_{t-L:t}, Z_{t:t+H}, and Z_t respectively. We construct the probabilistic graphical model for normalization-based methods, time-variant models, and our proposed JointPGM. The corresponding graphical representations of their overall computational paths are shown in Figure 1.\nNormalization-based methods utilize adaptively learned mean \u03bc_t and variance \u03c3_t to normalize the input observations X_{t-L:t} and encode them into their latent variable Z_{t-L:t}, which is subsequently decoded into output predictions X_{t:t+H} through de-normalization, alleviating the temporal mean and covariance shift between inputs and outputs. As Figure 1a shows, this process assumes that the dependency between X_{t-L:t} and X_{t:t+H} (i.e., transitional distribution) remain fixed over time t. As shown in Figure 1b, time-variant models model the dependency between X_{t\u2212L:t} and X_{t:t+H} at each time step t (i.e., time-variant transitional distribution). However, this coarse-grained process mixes the transitional patterns occurring within each series and among different series, failing to reveal the intrinsic causes of the distribution shift.\nIn contrast, JointPGM segments X_{t-L:t} along the variable dimension, obtaining N distinct series as input, and then encodes each series x_{t-L:t}^{(i)} separately into its latent variable 2_{t-L:t}^{(i)}, as shown in Figure 1c. The time factors t are additionally introduced to dynamically regulate the mapping processes both within each series (x_{t-L:t}^{(i)} \u2192 h_{t-L:t}^{(i)}) and among different series (2_{t-L:t}^{(i)} \u2192 A_t). Thus, the intra-/inter-series correlations are captured, collectively forming the final latent variable Z_{t\u2212L:t}. In alignment with this process, t\u2192 Z_t denotes encoding time factors into the corresponding latent variable. Afterward, Z_{t\u2212L:t} \u2192 Z_t means using a variational distribution P(Z_t|Z_{t-L:t}) to approximate the distribution P(Z_t|t). Herein, this relationship is designed to reversely infer time factors in latent space. As the latent variable Z_{t-L:t} is exploited to generate X_{t:t+H}, the time-variant transitional distribution is naturally decomposed into intra-/inter-series transitional distributions at a finer granularity.\nJointPGM focuses on a probabilistic manner to account for the underlying causes of distribution shift in MTS forecasting. As Figure 2 shows, JointPGM is organized with a dual-encoder architecture, which mainly involves four main components: 1) Time factor encoder (TFE) takes temporal order set {t - L + 1, ...,t + H} as input to learn the dynamic time factors M^{(1)} and their latent variable Z_t, which can reflect the clues of environmental changes; 2) Independence-based series encoder (ISE) captures series correlations by two distinct learners. While intra-series learner (left part of ISE in Figure 2) focuses on capturing temporal dynamics within each series with temporal gate G^{(i)} adjusted by M^{(1)}, inter-series learner (right part of ISE in Figure 2) is to explicitly model the spatial dynamics with multi-hop propagation incorporating Gumbel-softmax sampling; 3) Dynamic Inference (DI) uses latent variable Z_{t-L:t} to dynamically infer time factors and align with Z_t; 4) Decoder transforms Z_{t-L:t}, formed by these two dynamics, into the final prediction and reconstruction.\nLearning time factor representation that can accurately reflect irregular environmental changes is crucial for modeling distribution shifts. Transformer-based methods [12]\u2013[14], [49] obtain learnable additive position encoding by heuristic sinusoidal mapping to distinguish the temporal order of tokens or patches. However, this design only monitors the temporal order of the lookback window, neglecting the association with its corresponding horizon window and thereby compromising predictive performance. In this regard, we propose to use temporal orders that span across both windows t = {\\frac{i+L}{L+H}}_{i = -L,-L + 1, ..., H -1}, i.e., a [0,1]-normalized temporal order set. It is noteworthy that timestamp features (e.g., Minute-of-Hour, Day-of-Week, etc.) are also informative and can contribute to learning time factors. We opt for order features due to their more compact representations compared to timestamps. Additionally, embedding timestamp features with MLPs may have limitations in learning high-frequency patterns, commonly known as 'spectral bias' [54], [55].\nTo obtain the high-quality representation of conditional information, we concatenate multiple Fourier basis functions with diverse scale parameters as suggested by [55], and then learn the deep features and align the dimensions using a feedforward neural network:\nM^{(0)} = [sin(2\u03c0B_1t)|cos(2\u03c0B_1t)|...| sin(2\u03c0B_st)|cos(2\u03c0B_st)]\nM^{(1)} = FeedForward(M^{(0)}),\nwhere elements in B_i \u2208 R are sampled from N(0, \u03c3_e^2) with b denotes the Fourier feature size. M^{(0)} \u2208 R^{(L+H)\u00d7b} and M^{(1)} \u2208 R^{L\u00d7d} with d denotes the latent dimension size. \u03c3_e \u2208 {0.01, 0.1, 1, 5, 10, 20, 50, 100} denotes the scale hyperparameter and s is its corresponding index starting from 1... represents the concatenation operation. FeedForward : R^{(L+H)\u00d7b} \u2192 ]R^{L\u00d7d} is implemented by two linear layers with intermediate ReLU non-linearity. As shown in Figure 2, taking the Fourier basis function cos(\u00b7) as an example, its output has two main properties that could aid JointPGM in distinguishing\nKnown for effectively representing complex distribution and statistical relationships among variables in a principled and interpretable manner, PGM is an underexplored but promising framework for robust MTS forecasting against distribution shifts. We organize JointPGM as a dual-encoder architecture, which includes a time factor encoder (TFE) and an independence-based series encoder (ISE). Technically, in TFE, JointPGM employs multiple Fourier basis functions to capture dynamic time factors and introduces linear projections to learn the mean and variance of Gaussian sampling. Corresponding to the intra-/inter-series transitional shifts, JointPGM sequentially employs two distinct learners in ISE: intra-series and inter-series learners. The intra-series learner focuses on capturing temporal dynamics within each series and utilizes a temporal gate adjusted by learned time factors to control the message passing of temporal features, making them sensitive to non-stationary environments. The inter-series learner explicitly models spatial dynamics through multi-hop propagation incorporating Gumbel-softmax sampling. These two types of series dynamics are then fused and transformed into the latent variable, which is inversely used to infer time factors, generate final prediction, and perform reconstruction. We incorporate various constraints on all the above sub-processes based on a tailored PGM framework with theoretical guarantees, ensuring a clear understanding of the role played by each sub-process in forecasting MTS with non-stationarity.\n1) Time Factor Encoder (TFE):\nA. Probabilistic Decomposition for Transitional Shift\nB. Dual-Encoder Architecture"}, {"title": "V. EXPERIMENTS", "content": "We conduct extensive experiments on various datasets to evaluate the performance and efficiency of JointPGM. We include six well-acknowledged benchmarks used in previous non-stationary time series forecasting works [21]-[23], [27], [28], [35]: Exchange\u00b9, ETT\u00b2 (ETTh1 and ETTm2), Electricity\u00b3, METR-LA\u2074 and ILI\u2075 datasets. The overall statistics of these datasets are summarized in Table I. To show the non-stationarity of the six datasets, we especially choose the Augmented Dick-Fuller (ADF) test statistic used in [22], [28], [35] as the metric to quantitatively measure the degree of distribution shift. A larger ADF test statistic means a higher level of non-stationarity, i.e., more severe distribution shifts. Based on the ADF test results in Table I, the MTS datasets adopted in our experiments show a high degree of distribution shift. Notably, since the ADF statistical test of Weather (-26.661 in [35]) is much smaller than other datasets, indicating relative stationarity, it is excluded from our evaluation benchmarks. Additionally, we use the more non-stationary METR-LA from the same transportation domain\nWe compare JointPGM with the following state-of-the-art models for time series forecasting, including MLP-based models: Koopa [35] and DLinear [44]; Transformer-based models: Stationary [22], PatchTST [14], FEDformer [15], Autoformer [13], iTransformer [50] and Crossformer [49], and GNN-based model: WaveForM [19]. Notably, Koopa and Stationary are specifically designed to tackle non-stationary forecasting challenges in time series, while iTransformer, Crossformer, and WaveForM are tailored for MTS forecasting. Besides, we further compare JointPGM\nAll the experiments are implemented with Pytorch on an NVIDIA RTX 4090 24GB GPU. In our experiments, all mean functions(\u00b7), f_\u03bc^{(1)}(\u00b7), f_\u03bc^{(2)}(\u00b7), f_\u03bc^{(t)}(\u00b7) and variance functions f_\u03c3^2(\u00b7), f_\u03c3^2{(1)}(\u00b7), f_\u03c3^2{(2)}(\u00b7), f_\u03c3^2{(t)}(\u00b7) are instantiated as single linear layer. The depth of propagation K in the Multi-hop Propagation is set to 2 which is consistent\nwith three model-agnostic normalization-based methods, including SAN [28], Dish-TS [21], and RevIN [27], which respectively use Autoformer and FEDformer as backbones for non-stationary forecasting. More baseline details are provided in Appendix D-B. Regarding the evaluation metrics, we evaluate MTS forecasting performance using mean absolute error (MAE) and mean squared error (MSE). A lower MAE/MSE indicates better forecasting performance. Each experiment is repeated three times with different seeds for each model on each dataset, and the mean of the test results is reported.\n1) Datasets:\n3) Implementation Details:\nA. Experimental Setup"}, {"title": "B. Overall Performance", "content": "Table II showcases the forecasting results of JointPGM compared to nine representative baselines with the best in bold and the second underlined. From the table, we can observe JointPGM achieves state-of-the-art performance in nearly 80% forecasting results with various prediction lengths. Concretely, JointPGM outperforms all general deep forecasting models across all time series datasets, with particularly notable improvements observed on datasets characterized by high non-stationarity: compared to their state-of-the-art results, we achieve 13.2% MSE reduction (0.086 \u2192 0.076) on Exchange (ADF: -1.902) and 4.8% (4.000 \u2192 3.818) on ILI (ADF: - 5.334) under the horizon window of 96 and 24 respectively, which indicates that the potential of deep forecasting models is still constrained on non-stationary data. Also, JointPGM outperforms almost all deep models specifically designed to address distribution shifts. Notably, JointPGM surpasses Stationary, the non-stationary version of Transformer, by a large margin, indicating that the traditional covariate shift assumption may not be consistent with the true distribution shift. This highlights the challenges posed by the diverse transitional shift patterns underlying the time series for model capacity. Besides, different from Koopa disentangling series dynamics into time-variant dynamics and time-invariant dynamics using Koopman operators, JointPGM achieves an average reduction of 2.7% in MAE and 9.3% in MSE by innovatively rethinking time-variant dynamics from both intra- and inter-series perspectives at a finer granularity."}, {"title": "C. Comparison with Normalization-based Methods", "content": "We further compare our performance with the advanced normalization-based methods including SAN, Dish-TS, and RevIN for addressing distribution shift. Table III has presented a performance comparison in MTS forecasting using Autoformer and FEDformer as backbones. From the results, we can observe that JointPGM achieves the best performance in nearly 92% forecasting results compared to the existing normalization-based methods. We attribute this superiority to the fine-grained capture of time-variant dynamics through explicit modeling of the time-variant transitional distribution between observations and predictions, while simultaneously considering the intra- and inter-series relationships inherent in the observations. This is demonstrated by the performance on two typical non-stationary datasets Exchange and ILI. Specifically, compared to the second-best FEDformer+SAN, JointPGM achieves an MSE reduction of 5.3% on the Exchange dataset and 6.6% on the ILI dataset. Notably, as shown in Table II and Table III, JointPGM exhibits slightly worse MAE than other compared models. A potential explanation is that most compared models use a single MSE as an objective function while JointPGM employs two MSE losses for prediction and reconstruction. Therefore, JointPGM tends to prioritize improvements in MSE, consistently ranking first among all compared models except when L/H = 24/24."}, {"title": "D. Model Analysis", "content": "To explore the impact of lookback length L, horizon length H, trade-off parameter \u03b1, and the depth of propagation K, we conduct the following experiments for the sensitivity of these hyperparameters.\nFirst, we investigate the impact of lookback length L on the performance of the top-8 forecasting models on the ETTh1 dataset. In principle, extending the lookback window increases historical information availability, which will potentially improve forecasting performance. However, Figure 3a demonstrates that Stationary and FEDformer, with Transformer-based architectures, have not benefited from a longer lookback window, aligning with the analysis in [12]. Conversely, the remaining models consistently decrease MAE scores as the lookback\n1) Hyperparameter Analysis:"}, {"title": "E. Visualization Analysis", "content": "To showcase the rationale behind studying fine-grained transitional shift from both intra- and inter-series perspectives, we visualize the feature distribution of series representation H_{t-L:t} using t-SNE for JointPGM and its three variants w/o ISE (\u0391), w/o ISE (F) and w/o IL. For reliability, we randomly choose a batch of the Electricity test set and repeatedly run each experiment three times with different seeds. The overall results are depicted in Figure 5. The figure shows that: 1) The series representations learned from JointPGM and the variant w/o IL exhibit a distinct clustering structure, indicating a robust and differentiated representation space. In contrast, those learned from the variants w/o ISE (A) and w/o ISE (F) reveal a more spread-out and less clustered pattern, suggesting that without the fine-grained decomposition, the series representations are comparatively less informative and distinguishable; 2) While the series representations learned from the variant w/o IL exhibit a clear clustering pattern by focusing only on intra-series transitional shift, they also show information loss (marked by the red box), possibly caused by misclustering due to spurious inter-series correlations. In contrast, those learned from JointPGM are more evenly distributed across the entire 2D space. We further validate this rationale by showing heatmap visualizations of inter-series correlations in Appendix E-C, Figure 8. Based on the insights from Figures 5 and 8, JointPGM can learn superior series representations to improve robustness against intricate distribution shifts and offer enhanced interpretability.\nWe present a case study on real-world time series (METR-LA) in Figure 6. We select one weekday (period 1) and one weekend (period 2) as the representative horizon windows. Firstly, we compare the predictions of series #17 achieved by Koopa and our JointPGM during these two periods at the data and distribution levels. We easily observe significant changes in series trend (could be regarded as significant changes in intra-series correlation, the black arrows), but Koopa cannot acquire accurate predictions. In contrast, our JointPGM can perform precise predictions of future values and their distributions. The intuitive reason is when addressing the distribution shift, compared to coarse-grained Koopa, JointPGM has jointly handled the distribution shift and modeled intra-/inter-series correlations, thereby boosting the performance. Furthermore, we visualize the A1 and A2 learned by our JointPGM during period 1 and 2. It can be observed that the learned correlation between series #17 and other series also changes in different periods (the blue arrows), e.g., series #17 exhibits a high correlation with series #12 on weekday but not on weekends. This is reasonable since series #12 is located near a school as indicated by the Google Map. We present more forecasting showcases of JointPGM and the three baselines: Koopa, Dish-TS, and RevIN, in Figures 9, 10, and 11 in Appendix E-D, respectively.\n1) t-SNE Visualization of Series Representation:\n2) Case Study of Forecasting:"}, {"title": "VI. CONCLUSION", "content": "This study aims to address the distribution shift problem to enhance the robustness of MTS forecasting by proposing a novel probabilistic graphical model and instantiating a neural framework, JointPGM. Unlike previous normalization-based methods and time-variant models, JointPGM deeply exploits the intrinsic causes of the distribution shift, boosting desirable model interpretability and the potential to enhance forecasting performance by jointly handling the distribution shift and modeling intra-/inter-series correlations. Experimentally, our model shows competitive performance on six real-world benchmarks with remarkable efficiency. Future works will explore time-variant dynamics on higher-dimensional MTS data and further improve efficiency."}, {"title": "APPENDIX A NOTATIONS", "content": "All symbols in our paper are carefully defined based on rules and we have provided detailed and clear explanations for the meanings of all symbols in Table V.\nKL[P\u03c8(XH, ZL, Zt|XL)||P\u03c6(XH, ZL, Zt|XL, t)]\n\u222b \u222b \u222b P\u03c8(XH, ZL, Zt|XL) log P\u03c6(XH, ZL, Zt|XL,t) dXH dZLdZt\n+ \u222b \u222b \u222b P\u03c8(XH|ZL)P\u03c8(ZL, Zt|XL) log dXH dZLdZt\n= \u222b \u222b\u222b P\u03c8(XH|ZL)P\u03c8(ZL, Zt|XL) log dXH dZLdZt\n+ \u222b \u222b\u222b P\u03c8(XH ZL)P\u03c8(ZL, Zt|XL) log dXH dZLdZt\n= \u222b \u222b\u222b P\u03c8(XH|ZL)P\u03c8(ZL, Zt|XL) log dXH dZLdZt\n+ \u222b \u222b\u222b P\u03c8(ZL, Zt|XL) \u222b P\u03c8(XH|ZL) log P\u03c6(XH|XL,t)  dZL dZt\n= \u222b\u222b P\u03c8(ZL, Zt|XL) KL[P\u03c8(XH|ZL)||P\u03c6(XH|XL,t)] dZL dZt\nKL[P\u03c8(ZL, Zt|XL)||P\u03c6(ZL, Zt|XL, t)]\n\u222b\u222b P\u03c8(ZL, Zt|XL) log\nKL[P\u03c8(ZL, Zt|XL)||P\u03c6(ZL, Zt|XL, t)]\nP\u03c8(ZL, Zt|XL)\nP\u03c6(ZL, Zt|XL,t)\n\u222b\u222b P\u03c8(ZL|XL)P\u03c6(Zt|ZL)\nP\u03c8(ZL|XL)P\u03c6(Zt|ZL) log dZL dZt\n\u222b P\u03c8(ZL|XL)dZt dZL\nP\u03c6(ZL|XL,t)P\u03c6(Zt|t)\n\u222b\u222b P\u03c8(ZL|XL)P\u03c6(Zt|ZL) log dZL dZt\n+\u222b P\u03c8(ZL|XL)P\u03c6(Zt|ZL) log dZL dZt\n= \u222b P\u03c8(ZL|XL)KL[P\u03c8(Zt|ZL)||P\u03c6(Zt|t)] dZL"}, {"title": "APPENDIX B THE DETAILS OF THEORETICAL ANALYSES", "content": "Since our objective is for the learned time series and their corresponding time factors to exhibit strong discriminative characteristics and closely align with each other, we perform the following variational approximation with the Kullback\u2013\nFor term (a), i.e., KL[P\u03c8(ZL|XL)||P\u03c6(ZL|XL, t)], it aims to keep the inference using the variational distribution and the inference using the posterior is close, which also guarantees the reliable and high-quality sampling.\n\u222b\u222b P\u03c6(ZL XL) log dZt\nP\u03c6(ZL|XL,t)\n + EZL P\u03c8(ZL|XL) dZL\n+EZL P\u03c8(ZL|XL) dZL\n+EZL\nP\u03c8(ZL|XL) P\u03c6(Zt|ZL)\nP\u03c6(Zt|ZL)\n+\nKL[P\u03c8(ZL|XL)||P\u03c6(ZL|XL, t)]"}, {"title": "APPENDIX C DEEP DISCUSSIONS OF OUR METHOD JointPGM", "content": "As stated in the introduction, we argue there are two primary categories of approaches to address the non-stationary time series forecasting issue.\nOur JointPGM belongs to the second category. Different from Koopa [35] disentangling the series into time-variant and time-invariant dynamics, we rethink the time-variant dynamics from the intra- and inter-series perspective from a finer granularity. Since non-stationarity is intrinsically attributed to distributional shift, learning time-variant dynamics essentially stems from modeling the time-variant transitional distribution between inputs and outputs. Hence, we decompose such time-variant transitional distribution into intra- and inter-series parts.\nWhy can our proposed JointPGM address the non-stationary MTS forecasting problem?"}, {"title": "APPENDIX D MORE EXPERIMENTAL DETAILS", "content": "We adopt six real-world benchmarks in the experiments to evaluate the non-stationary MTS forecasting task. The overall statistics of these datasets are summarized in Table I. The experimental results (see Table II) on these datasets are better than those of baselines, which sufficiently proves that our proposed method is exactly superior and effective in handling the distribution shift problem in MTS forecasting.\nAs in Section E-C"}]}