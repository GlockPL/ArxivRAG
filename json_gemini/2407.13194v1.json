{"title": "Robust Multivariate Time Series Forecasting against Intra- and Inter-Series Transitional Shift", "authors": ["Hui He", "Qi Zhang", "Kun Yi", "Xiaojun Xue", "Shoujin Wang", "Liang Hu", "Longbing Cao"], "abstract": "The non-stationary nature of real-world Multivariate Time Series (MTS) data presents forecasting models with a formidable challenge of the time-variant distribution of time series, referred to as distribution shift. Existing studies on the distribution shift mostly adhere to adaptive normalization techniques for alleviating temporal mean and covariance shifts or time-variant modeling for capturing temporal shifts. Despite improving model generalization, these normalization-based methods often assume a time-invariant transition between outputs and inputs but disregard specific intra-/inter-series correlations, while time-variant models overlook the intrinsic causes of the distribution shift. This limits model expressiveness and interpretability of tackling the distribution shift for MTS forecasting. To mitigate such a dilemma, we present a unified Probabilistic Graphical Model to Jointly capturing intra-/inter-series correlations and modeling the time-variant transitional distribution, and instantiate a neural framework called JointPGM for non-stationary MTS forecasting. Specifically, JointPGM first employs multiple Fourier basis functions to learn dynamic time factors and designs two distinct learners: intra-series and inter-series learners. The intra-series learner effectively captures temporal dynamics by utilizing temporal gates, while the inter-series learner explicitly models spatial dynamics through multi-hop propagation, incorporating Gumbel-softmax sampling. These two types of series dynamics are subsequently fused into a latent variable, which is inversely employed to infer time factors, generate final prediction, and perform reconstruction. We validate the effectiveness and efficiency of JointPGM through extensive experiments on six highly non-stationary MTS datasets, achieving state-of-the-art forecasting performance of MTS forecasting.", "sections": [{"title": "I. INTRODUCTION", "content": "Multivariate Time Series (MTS) forecasting has been playing an increasingly ubiquitous role in real-world applications, such as weather condition estimation [1], stock trend analysis [2], [3], electricity consumption planning [4], and traffic flow and speed prediction [5], [6]. Impressively, various deep learning-based approaches have emerged and led to a surge in deep MTS forecasting models. These approaches elaborately capture complex temporal variations by Temporal Convolution Networks (TCNs) [7]\u2013[9], Recurrent Neural Networks (RNNs) [10], [11], and Transformers [12]\u2013[15], or explore specific variable-wise dependencies by Graph Neural Networks (GNNs) [16]\u2013[20]. Despite the remarkable performance, they fall short in adapting to real-world scenarios where the distributions (a.k.a., statistical properties) of time series change over time due to dynamic generation mechanisms. This phenomenon, known as distribution shift [21]\u2013[23], exposes time series' highly dynamic and non-stationary nature. It poses significant challenges for forecasting models in effective generalization to varying distributions. Such vulnerability to rapid distributional changes [24] ultimately results in a dramatic decline in forecasting accuracy over time [25].\nResearchers have explored two primary categories of approaches to tackle the distribution shift in MTS forecasting. The first category involves normalization techniques to align time series instances based on the Gaussian assumption. They normalize the input and denormalize the output using adaptively learned statistics (e.g., mean and variance) [21], [22], [26]\u2013[28] to alleviate the temporal mean and covariance shift among instances or between inputs and outputs. Most of these methods, however, assume a time-invariant transitional distribution between output predictions and input observations, i.e., $P(x_{u:u+H}|X_{u-L:u}) = P(X_{v:v+H}|X_{v-L:v})$ at any two steps $u \\neq v$. This assumption severely simplifies the non-stationarity of time series and is not consistent with the practical distribution shift [29], [30]. Give an easily understandable example: In stock prediction, the financial factors $P(x_t)$ naturally change due to market fluctuations. Meanwhile, economic laws $P(x_{t:t+h}|x_{t-L:t})$ are also vulnerable to abrupt policies, such as government price controls. Additionally, these methods focus on exploring the variable-wise data distribution, overlooking specific intra-/inter-series correlations [31], [32]. As a result, they struggle to effectively address the counterpart non-stationarity, especially the distribution shift along with inter-series dynamics.\nThe second category aims to model time-variant transitional distribution, i.e., $P_t(x_{t:t+H}|X_{t-L:t})$ adapted to any time step $t$, to improve models' temporal generalization. Many advanced models within this category have integrated time information to enhance forecasting performance, indicating that time information enables the models to effectively capture the time-variant characteristics of time series, thereby alleviating the issue of non-stationarity [12], [13], [15]. Herein, some models incorporate temporal meta-knowledge to correct the bias caused by the distribution shift within a discriminative meta-"}, {"title": "II. RELATED WORK", "content": "Multivariate time series (MTS) forecasting is a longstanding research topic [39], [40]. Initially, traditional statistical models such as Gaussian process (GP) [41] have been proposed for their appealing simplicity and interpretability. Recently, with the bloom of deep learning, many deep models with elaboratively designed architectures have made great breakthroughs in capturing intra- and inter-series correlations for MTS forecasting. On one hand, the RNN- [10], [11] and TCN-based [7]\u2013[9] models have shown competitiveness in modeling complex temporal relationships. However, due to their recurrent structures or the locality property of one-dimensional convolutional kernels, they are limited in handling long-term dependencies. Soon afterward, Transformer and its variants [12]\u2013[15], [42] have achieved superior performance on MTS forecasting, particularly notable in scenarios with long prediction lengths. They focus on renovating the canonical structure and designing a novel attention mechanism to reduce the quadratic complexity while automatically learning the correlations between elements in a series. Despite the complicated design of Transformer-based models, recent MLP-based models [43]\u2013[46] with simple structure and low complexity can surpass previous models across various common benchmarks for MTS forecasting. Another crucial aspect of MTS forecasting involves capturing the correlations among multiple time series. Current models highly depend on GNNs [4], [16]\u2013[20], [47] or ordered tree [48] due to their remarkable capability in modeling structural dependencies. Most of them can automatically learn the topological structure of inter-series correlations by leveraging node similarity [17]\u2013[19] or self-attention mechanism [4]. More recently, Crossformer [49] and iTransformer [50] have been specifically proposed to explicitly capture the mutual interactions among multiple variables by refurbishing the architecture and components such as the attention module of Transformer. Different from previous works focusing on better modeling temporal relationships within and among time series, we analyze the MTS forecasting task from a more fundamental review of the non-stationary nature, which constitutes an indispensable property of MTS data."}, {"title": "B. Improving Robustness against Distribution Shifts", "content": "Despite many remarkable deep models, MTS forecasting still suffers from severe distribution shifts [51] considering the distribution of real-world series changes temporally. To improve robustness over varying distributions, one category of widely-explored methods [21], [22], [26]\u2013[28], [52] stationarize deep model inputs by the normalization techniques. For example, RevIN [27] proposes a reversible instance normalization technique to reduce temporal distribution shift. Based on RevIN, Dish-TS [21] designs a dual coefficient network to learn two sets of distribution coefficients and captures the distribution shift between inputs and outputs. Stationary [22] adopts de-stationary attention to handle the over-stationarization issue which may damage the model's capability of modeling specific temporal dependency. SAN [28] utilizes slice-level adaptive normalization to mitigate non-stationarity. However, these methods typically assume a time-invariant transitional distribution and overlook the distribution shift caused by inter-series dynamics. Another category [2], [33], [34] learns to model time-variant transitional distribution by incorporating temporal meta-knowledge to correct the bias caused by distribution shift in a discriminative meta-learning framework, which is generally designed for bridging the gap between the training and test data. More recently, Koopman predictors Koopa [35] and KNF [53] employ Koopman operators as linear portraits of implicit transitions to capture time-invariant and time-variant dynamics. While these models model the time-variant transitional distributions, such coarse-grained modeling fails to reveal the intrinsic causes of the transitional distribution, limiting the models' interpretability and expressiveness. In this paper, we propose JointPGM to model the practical transitional distribution and decompose it based on the prevalent approach of learning intra-/inter-series correlations."}, {"title": "III. PROBLEM FORMULATIONS", "content": "In this section, we start with the formulations of MTS forecasting and define the concepts central to distribution shift. Multivariate Time Series Forecasting. Let a regularly sampled time series dataset with a total of N distinct time series and T time steps be denoted as $[x^{(1)}, ..., x^{(i)}, ..., x^{(N)}] \\in R^{N \\times T}$, where $x^{(i)} \\in R^T$ denotes the sequence values of time series i at T time steps. Given a lookback window of length-L and a horizon window of length-H, the multivariate time series forecasting involves utilizing historical multivariate observations $X_{t-L:t} = \\{x_{t-L:t}^{(i)}\\}_{i=1}^{N}$ to predict their future multivariate values $X_{t:t+H} = \\{x_{t:t+H}^{(i)}\\}_{i=1}^{N}$ at time step t. The forecasting process can be formulated as:\n$X_{t:t+H} = F_\\Theta(X_{t-L:t}) = F_\\Theta(\\left\\{x_{t-L:t}^{(i)}\\right\\}_{i=1}^{N})$   (1)\nwhere the function map $F_\\Theta : R^{N \\times L} \\rightarrow R^{N \\times H}$ can be regarded as a forecasting model parameterized by $\\Theta$.\nDistribution Shift in Time Series. Recall the intuitive financial example mentioned in Section I, where the economic laws are vulnerable to abrupt policy changes. Therefore, we propose to involve the more rational and practical transitional shift assumption and further decompose the integrated transitional shift in time series into two types at a finer granularity, namely, intra-series transitional shift and inter-series transitional shift, with their definitions provided below.\nDefinition 1 (Intra-series Transitional Shift). Given the ith time series $x^{(i)}$, which can be split into several lookback windows $\\{x_{t-L:t}^{(i)}\\}_{t=L}^{T-H}$ and their corresponding horizon windows $\\{x_{t:t+H}^{(i)}\\}_{t=L}^{T-H}$. Intra-series Transitional Shift is referred to the case that the transitional distribution $P(x_{u+H}^{(i)}/X_{u-L:u}^{(i)}) \\neq P(x_{v+H}^{(i)}/X_{v-L:v}^{(i)})$ for any two time steps u and v with $L\\leq u \\neq v \\leq T\u2013 H$.\nDefinition 2 (Inter-series Transitional Shift). Given the ith time series $x^{(i)}$ with its complementary set $x^{(i)'}$. Similar to $x^{(i)}$, $x^{(i)'}$ can be split into several lookback windows $\\{X_{t-L:t}^{(i)'}\\}_{t=L}^{T-H}$ and their corresponding horizon windows $\\{X_{t:t+H}^{(i)'}\\}_{t=H}^{T-H}$. Inter-series Transitional Shift is referred to the case that the transitional distribution $P(x_{u:u+H}^{(i)}/X_{u-L:u}^{(i)'}) \\neq P(x_{v:v+H}^{(i)}/X_{v-L:v}^{(i)'})$ for any two time steps u and v with $L<u \\neq v\\leqT \u2013 H$.\nRemark 1. The combination of these two definitions fully describes the complex distribution shifts encountered in reality. The former indicates the variations in transitional distribution for each series, while the latter reflects the variations in transitional distribution among different series. Since characterizing the local relationship between pairwise series in Definition 2 is overly complex, we describe the relationship between each series and its complementary set from a global perspective."}, {"title": "IV. METHODOLOGY", "content": "In this section, we first present our tailored PGM and formally analyze the distinctions between normalization-based methods, time-variant models, and ours in Section IV-A. In Section IV-B, we introduce the corresponding instantiated dual-encoder architecture. Finally, we decompose the learning objective based on the PGM and our purpose in Section IV-C.\nA. Probabilistic Decomposition for Transitional Shift\nRecall the notations in non-stationary MTS forecasting task, i.e., $X_{t\u2212L:t}, X_{t:t+H}$, and t, they correspond to the latent variable $Z_{t-L:t}, Z_{t:t+H}$, and $Z_t$ respectively. We construct the probabilistic graphical model for normalization-based methods, time-variant models, and our proposed JointPGM. The corresponding graphical representations of their overall computational paths are shown in Figure 1.\nNormalization-based methods utilize adaptively learned mean $u_t$ and variance $\\sigma_t$ to normalize the input observations $X_{t-L:t}$ and encode them into their latent variable $Z_{t-L:t}$, which is subsequently decoded into output predictions $X_{t:t+H}$ through de-normalization, alleviating the temporal mean and covariance shift between inputs and outputs. As Figure 1a shows, this process assumes that the dependency between $X_{t-L:t}$ and $X_{t:t+H}$ (i.e., transitional distribution) remain fixed over time t. As shown in Figure 1b, time-variant models model the dependency between $X_{t\u2212L:t}$ and $X_{t:t+H}$ at each time step t (i.e., time-variant transitional distribution). However, this coarse-grained process mixes the transitional patterns occurring within each series and among different series, failing to reveal the intrinsic causes of the distribution shift.\nIn contrast, JointPGM segments $X_{t-L:t}$ along the variable dimension, obtaining N distinct series as input, and then encodes each series $x_{t-L:t}^{(i)}$ separately into its latent variable $h_{t-L:t}^{(i)}$, as shown in Figure 1c. The time factors t are additionally introduced to dynamically regulate the mapping processes both within each series $(x_{t-L:t}^{(i)} \\rightarrow h_{t-L:t}^{(i)})$ and among different series $(X_{t-L:t}^{(i)} \\rightarrow A_t)$. Thus, the intra-/inter-series correlations are captured, collectively forming the final latent variable $Z_{t\u2212L:t}$. In alignment with this process, t\u2192 $Z_t$ denotes encoding time factors into the corresponding latent variable. Afterward, $Z_{t\u2212L:t} \\rightarrow Z_t$ means using a variational distribution $P(Z_t|Z_{t-L:t})$ to approximate the distribution $P(Z_t|t)$. Herein, this relationship is designed to reversely infer time factors in latent space. As the latent variable $Z_{t-L:t}$ is exploited to generate $X_{t:t+H}$, the time-variant transitional distribution is naturally decomposed into intra-/inter-series transitional distributions at a finer granularity."}, {"title": "B. Dual-Encoder Architecture", "content": "JointPGM focuses on a probabilistic manner to account for the underlying causes of distribution shift in MTS forecasting. As Figure 2 shows, JointPGM is organized with a dual-encoder architecture, which mainly involves four main components: 1) Time factor encoder (TFE) takes temporal order set $\\{t - L + 1, ...,t + H\\}$ as input to learn the dynamic time factors $M^{(1)}$ and their latent variable $Z_t$, which can reflect the clues of environmental changes; 2) Independence-based series encoder (ISE) captures series correlations by two distinct learners. While intra-series learner (left part of ISE in Figure 2) focuses on capturing temporal dynamics within each series with temporal gate $G^{(i)}$ adjusted by $M^{(1)}$, inter-series learner (right part of ISE in Figure 2) is to explicitly model the spatial dynamics with multi-hop propagation incorporating Gumbel-softmax sampling; 3) Dynamic Inference (DI) uses latent variable $Z_{t-L:t}$ to dynamically infer time factors and align with $Z_t$; 4) Decoder transforms $Z_{t-L:t}$, formed by these two dynamics, into the final prediction and reconstruction.\n1) Time Factor Encoder (TFE): Learning time factor representation that can accurately reflect irregular environmental changes is crucial for modeling distribution shifts. Transformer-based methods [12]\u2013[14], [49] obtain learnable additive position encoding by heuristic sinusoidal mapping to distinguish the temporal order of tokens or patches. However, this design only monitors the temporal order of the lookback window, neglecting the association with its corresponding horizon window and thereby compromising predictive performance. In this regard, we propose to use temporal orders that span across both windows $t = \\{0,..., \\frac{i+L}{i+L+H}, .., 1\\}$ for i = -L,-L + 1, ..., H 1, i.e., a [0,1]-normalized temporal order set. It is noteworthy that timestamp features (e.g., Minute-of-Hour, Day-of-Week, etc.) are also informative and can contribute to learning time factors. We opt for order features due to their more compact representations compared to timestamps. Additionally, embedding timestamp features with MLPs may have limitations in learning high-frequency patterns, commonly known as 'spectral bias' [54], [55].\nTo obtain the high-quality representation of conditional information, we concatenate multiple Fourier basis functions with diverse scale parameters as suggested by [55], and then learn the deep features and align the dimensions using a feedforward neural network:\n$M^{(0)} = | sin(2\\pi B_1t)| | cos(2\\pi B_1t)|...| sin(2\\pi B_st)|cos(2\\pi B_st)|$,   (2)\n$M^{(1)} = FeedForward(M^{(0)})$,   (3)\nwhere elements in $B_i \\in R$ are sampled from $N(0, \\frac{i}{\\sigma^2})$ with b denotes the Fourier feature size. $M^{(0)} \\in R^{(L+H) \\times b}$ and $M^{(1)} \\in R^{L \\times d}$ with d denotes the latent dimension size. $\\sigma_i \\in \\{0.01, 0.1, 1, 5, 10, 20, 50, 100\\}$ denotes the scale hyperparameter and s is its corresponding index starting from 1...| represents the concatenation operation. $FeedForward : R^{(L+H) \\times b} \\rightarrow ]R^{L \\times d}$ is implemented by two linear layers with intermediate ReLU non-linearity. As shown in Figure 2, taking the Fourier basis function $cos(\\cdot)$ as an example, its output has two main properties that could aid JointPGM in distinguishing different temporal orders: similar temporal orders yield similar representations (e.g., the plot of t, t + 1) and the larger the temporal order the earlier the values in representations oscillate between -1 and +1 (e.g., the plot of t, t + H).\nThen, we model $P(Z_t|t)$ by stochastically sample $Z_t$ from the Gaussian distribution using the reparameterization trick:\n$\\mu_t = f^{(1)}(M^{(1)})$,   (4)\n$\\sigma_t = f^{(2)}(M^{(1)})$,   (5)\n$P(Z_t|t) = N(\\mu_\\tau, \\sigma_\\tau I)$,   (6)\nwhere two multivariate functions $f^{(1)}(\\cdot)$ and $f^{(2)}(\\cdot)$ map the input $M^{(1)}$ to the mean and variance vectors of size N \u00d7d and N \u00d7 d. In practice, $f^{(1)}(\\cdot)$ and $f^{(2)}(\\cdot)$ are instantiated as a single linear layer.\n2) Independence-based Series Encoder (ISE): Series independence mechanism refers to the case of taking only one individual series as model input at each instance and mapping it into a latent space, rather than simultaneously incorporating all time series to mix information. This mechanism allows the model to only focus on learning information along the time axis and has shown effectiveness in working with linear models [43], [44] and Transformer-based models [14] in time series forecasting tasks. Therefore, we apply the series independence mechanism to sequentially explore two distinct types of correlations: intra- and inter-series correlations, thereby benefiting the modeling of time-variant transitional distribution within each series (i.e., $P(x_{t:t+H}^{(i)}|X_{t-L:t}^{(i)}, t)$) and among different series (i.e., $P(x_{t:t+H}^{(i)}| X_{t- L:t}^{(i)'}, t)$) respectively. Note that such sequential style is beneficial and widely adopted by [43], [45], with intra-series learner providing a solid representation foundation for inter-series learner. ISE mainly consists of a intra-series learner and an inter-series learner, which are introduced as follows:\nIntra-series Learner. The intra-series learner takes $X_{t\u2212L:t} \u2208 R^{N\u00d7L}$ as input, which can be split into N series. To illustrate the modeling process of intra-series transitional distribution, we draw inspiration from the structure of [44] and take the ith series $x_{t-L:t}^{(i)} \u2208 R^{L}$ as an example. Concretely, $x_{t-L:t}^{(2)}$ is fed into a linear layer according to our series-independent setting, then the linear layer will provide mapping results accordingly:\n$h_{t-L:t}^{(i)} = Linear(x_{t-L:t}^{(i)})$,   (7)\nwhere $Linear: R^{L} \\rightarrow R^{d}$ is implemented by a single linear layer, and $h_{t-L:t}^{(i)} \u2208 R^{d}$. All series share the weights along the time dimension.\nTo render the modeled transitional distribution change over time, we design a temporal gate that is capable of distilling discriminative historical signals [56] sensitive to non-stationary environments based on dynamic time factors. Specifically, we utilize a linear layer with a Sigmoid activation function to learn the temporal gate $G \u2208 R^{N\u00d7d}$. Subsequently, the ith gate $G^{(i)}$ is applied to the representation $h_{t-L:t}^{(i)}$ of the ith series:\n$G = Sigmoid(Linear(M^{(1)}))$,   (8)\n$h_{t-L:t}^{(i)} = G^{(i)} \\odot h_{t-L:t}^{(i)}$,   (9)\nwhere $\\odot$ is element-wise multiplication, and all sub-gates $G^{(i)}, i \u2208 \\{1, ..., N\\}$ share the weights along time dimension. To capture the transitional distribution for each series, we explicitly model the distribution $P(2_{t\u2212L:t}^{(2)}|x_{t-L:t}^{(i)}, t)$ by stochastically sampling each latent variable $z_{t\u2212L:t}^{(2)(i)}$ from a Gaussian distribution:\n$\\mu_t^{(2)} = f^{(1)}(h_{t-L:t}^{(i)})$,   (10)\n$\\sigma_t^{(2)} = f^{(2)}(h_{t-L:t}^{(i)})$,   (11)\n$P(z_{t\u2212L:t}^{(2)(i)}|x_{t-L:t}^{(i)}, t) = N(\\mu_t, \\sigma_t I)$,   (12)\nwhere $z_{t\u2212L:t}^{(2)(i)} \u2208 R^d$. $f^{(1)}(\\cdot)$ and $f^{(2)}(\\cdot)$ are instantiated as a single linear layer and share weights between the latent states of N series. Finally, we ensemble $h_{t-L:t}^{(i)}$ and $z_{t\u2212L:t}^{(2)(i)}$ of N time series into a whole respectively, yielding respective outputs $H_{t-L:t} \u2208 R^{N\u00d7d}$ and $2_{t-L:t}^{(2)} \u2208 R^{N\u00d7d}$. Considering the previously implemented series-independent processes, where each series $x_{t-L:t}^{(i)}$ is independent of each other series belonging to $x_{t-L:t}^{(i)'}$, we can compose the posterior distribution $P(Z_{t-L:t}|X_{t\u2212L:t}, t)$ from multiple sub-distributions $P(z_{t\u2212L:t}^{(2)(i)}|x_{t-L:t}^{(i)}, t)$, i \u2208 {1, ..., N}.\nInter-series Learner. Most methods [17]-[19] randomly initialize node embeddings for all nodes and infer the dependencies between each pair of nodes by multiplication operations. The adjacency matrix derived in this way is essentially input-unconditioned, making it challenging to effectively handle abrupt changes in non-stationary time series. Hence, we propose to calculate the relationships between nodes by the self-attention mechanism [4]:\n$Q_t = 2_{t-L:t}^{(2)}W_t^Q$,   (13)\n$K_t = 2_{t-L:t}^{(2)}W_t^K$,   (14)\n$W_t = Softmax(\\frac{Q_t K_t^T}{\\sqrt{d}})$,   (15)\nwhere $Q_t$ and $K_t$ indicate the representation for query and key at time step t, which can be calculated by linear projections with learnable parameters $W_t^Q$ and $W_t^K$ respectively. Here, $W_t$ is the continuous version of the adjacency matrix (a.k.a., probability matrix) then $W_{ij,t} \u2208 W_t$ denotes the probability to preserve the edge of series i to j at time step t. However, such soft weights are incapable of decisively choosing between retaining or discarding edges, thereby hindering the explicit modeling of how each series is influenced by its relevant series during the distribution shift. Therefore, inspired by [57], [58], we apply the Gumbel reparameterization trick:\n$a_{ij,t} = Sigmoid((log(W_{ij,t}/(1 \u2013 W_{ij,t}))) + (g_{ij,t} \u2013 g'_{ij,t})/\\tau)$,   (16)\ns.t. $g_{ij,t}, g'_{ij,t} ~ Gumbel(0, 1)$, where $\\tau\u2208 (0,\u221e)$ is a temperature parameter. When $\\tau\u2192 0, a_{ij,t} = 1 \u2208 A_t$ with probability $W_{ij,t}$ and 0 with remaining probability.\nAfterward, we utilize multi-hop propagation, which is a simplified version of mix-hop propagation proposed by [18], [19], to aggregate information from immediate neighbors. Given the input $\u0124_{t\u2212L:t}$ and adjacency matrix $A_t$, the process of K-layer propagation can be formulated as follows:\n$H_{t-L:t}^{(K)} = \\sum_{k=1}^{K} Linear^{(k)}(A_t H_{t-L:t}^{(k-1)})$,   (17)\nwhere K is the depth of propagation, $H_{t\u2212L:t} \u2208 R^{N\u00d7d}$ denotes the output representation of the current layer, $H_{t\u2212L:t}^{(0)} = H_{t-L:t}$. Such simplification provides an important insight: under the wild non-stationarity, mixing the original representation $H_{t-L:t}$ in each hop can easily introduce noise into the inter-series correlation learning, as well as into the subsequent inter-series transitional distribution modeling.\nHere we explicate the distribution modeling from a holistic perspective. We denote the latent state of $X_{t-L:t}$ that are inferred from $H_{t-L:t}$ by $Z_{t\u2212L:t}$, which is distinguished from $2_{t\u2212L:t}^{(2)}$ achieved by the temporal gate. The distribution $P(Z_{t-L:t}| X_{t-L:t}, t)$ is stochastically sampled $\u017d_{t-L:t}$ from the Gaussian distribution:\n$\\mu_{2}^{(2)} = f^{(1)}(H_{t-L:t})$,   (19)\n$\\sigma_{2}^{(2)} = f^{(2)}(H_{t-L:t})$,   (20)\n$P(\u017d_{t-L:t}| X_{t-L:t},t) = N(\\mu_\u03b1, \\sigma_\u03b1 I)$,   (21)\nwhere $\u017d_{t-L:t} \u2208 R^{N\u00d7d}, f^{(1)}(\\cdot)$ and $f^{(2)}(\\cdot)$ are also instantiated as a single linear layer. For each series $zi_{t-L:t}^{(2)} \u2208 Z_{t-L:t}$, the posterior approximation can be successfully represented by a product of two sub-distributions: $P(z_{t-L:t}^{(2)}|x_{t-L:t}^{(2)}, t) = P(z_{t-L:t}^{(2)(i)}|x_{t-L:t}^{(2)(i)})P(z_{t-L:t}^{(2)}| x_{t-L:t}^{(i)'}, t)$. Accordingly, the process of Gumbel-softmax sampling explicitly selects correlated series $x_{t-L:t}^{(i)}$ for each series $x_{t-L:t}^{(2)(i)}, x_i \u2208 \\{1, ..., N\\}$, which is part of the former, while the latter involves the independent modeling of each correlated series.\nThen, the latent variable output from intra-series learner, $2_{t-L:t}^{(2)}$, joints the latent variable output from inter-series learner, $Z_{t-L:t}$, to form $Z_{t\u2212L:t} ~ P(Z_{t\u2212L:t}|X_{t\u2212L:t}, t)$, with their proportions regulated by trade-off parameter \u03b1:\n$Z_{t-L:t} = \u03b1\u017d_{t-L:t} + (1 \u2013 \u03b1)2_{t-L:t}^{(2)}$,   (22)\n3) Dynamic Inference (DI): As illustrated in Section IV-A, we aim to use variational distribution $P(\u017b_t|Z_{t-L:t})$ to estimate the distribution $P(Z_t|t)$ achieved by time factor encoder. Accordingly, we derive the latent variable \u017dt through a linear layer. After that, we use two linear functions ft(\u00b7) and fo\u00b2 () to map the latent state Zt to the mean and variance vectors, as formulated follows:\n$2_t = Linear(Z_{t-L:t})$,   (23)\n$\\mu_t^{(1)} = f(2_t)$,   (24)\n$\\sigma_t^{(2)} = f^{(2)}(2_t)$,   (25)\n4) Decoder: We utilize the learned latent variable $Z_{t-L:t}$ to perform reconstruction and prediction with one forward step which can avoid error accumulation, as formulated below:\n$X_{t-L:t} = FeedForward_{rec}(Z_{t-L:t})$,   (26)\n$X_{t:t+H} = FeedForward_{pre}(Z_{t\u2212L:t})$,   (27)\nwhere $FeedForward_{rec} : R^{d} \\rightarrow R^{L}$ and $FeedForward_{pre} : R^{d} \\rightarrow R^{H}$ are both implemented using two linear layers with intermediate LeakyReLU non-linearity."}, {"title": "C. Objective Decomposition", "content": "For simplicity", "X_{t\u2212L": "t"}, "as $X_L$, $Z_{t-L:t}$ as $Z_L$, $X_{t:t+H}$ as $X_H$, and omit intermediate variables $2_{tL:t}^{(2)}$ and $A_t$ when there is no confusion. To tackle the distribution shift in MTS forecasting, our objective is to explicitly model the time-variant transitional distribution between output predictions and input observations. It requires the learned latent variables of time series to be informative and discriminative, while also exhibiting a high sensitivity to dynamic time factors that can reflect non-stationary environments. Therefore, based on our tailored PGM, we conduct the following variational inference using the Kullback-Leibler (KL) divergence:\n$L = KL[P_\u03c8(X_H, Z_L, Z_t|X_L)||P_\u03c6(X_H, Z_L, Z_t|X_L,t)"], "as": "n$L = KL [P_\u03c8(X_H", "have": "n$L_a = -ELBO$\\\n$= -E_{Z_L} [log P(X_L|Z_L)] + KL[P_\u03c8(Z_L|X_L)||P(Z_L)]$\\\n$= l(X_L \u2013 X_L) + (- log \u03c3_z + \\frac{1}{2} \\frac{\\mu_z^2}{\\sigma^2}+\\frac{1}{2} \\frac{\\mu_z^2}{\\sigma^2} - \\frac{1}{2})$,   (30)\nwhere I denotes a distance metric for which we use the MSE loss, $\u03bc_z$ and $\u03c3_z$ are the mean and variance vectors of $Z_L$.\nFor term (b), i.e., $E_{Z_L\u223cP_\u03c8(Z_L/X_L)}KL[P_\u03c8(Z_t|Z_L)||P_\u03c6(Z_t|t)]$, to make time factors more sensitive to non-stationary environments, we use a variational distribution to approximate posterior distribution"}