{"title": "Polarized Patterns of Language Toxicity and Sentiment of Debunking Posts on Social Media", "authors": ["Wentao Xu", "Wenlu Fan", "Shiqian Lu", "Tenghao Li", "Bin Wang"], "abstract": "The rise of misinformation and fake news in online political discourse poses significant challenges to democratic processes and public engagement. While debunking efforts aim to counteract misinformation and foster fact-based dialogue, these discussions are often marred by language toxicity and emotional polarization. In this study, we examined over 86 million debunking tweets and more than 4 million Reddit debunking comments to investigate the relationship between language toxicity, pessimism, and social polarization in debunking efforts. Focusing on discussions of the 2016 and 2020 U.S. presidential elections and the QAnon conspiracy theory, our analysis reveals three key findings: (1) peripheral participants (1-degree users) play a disproportionate role in shaping toxic discourse, driven by lower community accountability and emotional expression; (2) platform mechanisms significantly influence polarization, with Twitter amplifying partisan differences and Reddit fostering higher overall toxicity due to its structured, community-driven interactions; and (3) a surprising negative correlation exists between language toxicity and pessimism, with increased interaction (e.g., replies) reducing toxicity, especially on Reddit. We further show that platform architecture affects the informational complexity of user interactions, with Twitter promoting concentrated, uniform discourse and Reddit encouraging diverse, complex communication. By incorporating an unprecedented scale of data from two distinct platforms, our findings highlight the importance of user engagement patterns, platform dynamics, and emotional expressions in shaping polarization in debunking discourse. This study offers actionable insights for policymakers and platform designers to mitigate harmful effects and promote healthier, more constructive online discussions, with broader implications for understanding the dynamics of misinformation, hate speech, and political polarization in digital environments.", "sections": [{"title": "Introduction", "content": "In today's digital age, the spread of fake news and misinformation has become a major concern, particularly in online political discourse\u00b9. The proliferation of such misinformation has led to challenges in distinguishing fact from fiction, especially as social media platforms amplify polarized political discussions\u00b2. These platforms, while providing access to diverse viewpoints, also contribute to echo chambers and ideological polarization. In this context, debunking false information is critical not only for restoring truth but also for promoting informed public engagement and protecting democratic processes\u00b3. By systematically identifying and correcting misleading claims, debunking efforts can help foster more rational, fact-based political dialogue, counteracting the negative effects of misinformation and polarization\u2074.\nAs a defining characteristic of this polarized environment, language toxicity not only amplifies misinformation but also manifests in various forms, both online and offline\u2075,\u2076 Such toxicity often takes the form of personal attacks, insults, and hateful rhetoric, exacerbated by the anonymity afforded by the online world emboldens users to shed social inhibitions, fostering a sense of detachment from the consequences of their words\u2077,\u2078. Additionally, the lack of nonverbal cues, such as facial expressions or tone of voice, makes it easy to misinterpret communication, leading to misunderstandings and further escalations. Civil discourse has become a casualty, being replaced by a hostile environment where respectful disagreement is a rarity\u00b9\u2070, \u00b9\u00b9\nOften prevalent in online interactions or social media platforms, language toxicity can contribute to a hostile environment, amplifying polarization, and undermining respectful discourse. Toxic speech can inflict harm even in the absence of slurs or epithets, with mechanisms that vary depending on the context and the target, creating a negative impact on the individuals involved \u00b9\u00b2. Additionally, the relatively low barriers to participation and the anonymity of online platforms can exacerbate this language toxicity, as individuals are more likely to express harmful views without fear of accountability\u00b9\u00b3. It can have significant psychological and social consequences, particularly when such language is directed toward individuals or groups based on their identity, beliefs, or behaviors\u00b9\u2074. For instance, the stigma created by harmful language-especially in contexts like mental health or addiction\u2014can significantly worsen psychological distress and hinder social integration\u00b9\u2074. As polarization intensifies, the prevalence of toxic language on online platforms has led to growing pessimism regarding the future of public discourse\u00b9\u2075. The proliferation of toxic language in digital spaces naturally breeds and amplifies collective pessimism \u00b9\u2076.\nOnline pessimism, particularly when expressed through toxic language, thrives in digital spaces where anonymity and distance obscure accountability. This pessimism feeds on negativity, cultivating a sense of disillusionment, frustration, and hopelessness surrounding societal, political, or personal issues. Toxic language exacerbates these emotions, drowning out constructive discourse and fostering an environment where vitriol reigns supreme\u00b9\u2077. Platforms like Twitter and Reddit, where emotional tones run high, illustrate this dynamic, with pessimism standing out as an emotion that is deeply influenced by social-psychological pressure\u00b9\u2078. Unlike individuals with positive dispositions, those with pessimistic tendencies exhibit negative cognitive processing of uncertain information\u00b9\u2079, develop more negative thoughts that reduce their ability to handle stress and adjust mentally\u00b2\u2070, and demonstrate adverse online behavior\u00b2\u00b9. This online manifestation of pessimism aligns with broader psychological patterns in individual behavior.\nWhile these toxic interactions and pessimistic expressions are often viewed through the lens of political divisions, our study reveals that polarization manifests across multiple dimensions in online discourse. Beyond partisan differences, we observe polarization in user engagement (degree patterns), platform dynamics (between different social media architectures)\u00b2\u00b2, linguistic expression (in language toxicity and sentiment), information complexity (through entropy variations)\u00b2\u00b3, and replying behaviors\u00b2\u2074. Understanding these various dimensions of polarization is crucial for comprehending how online discussions develop and fragment, extending beyond traditional political divides to encompass structural, behavioral, and communicative aspects of social media interaction.\nPrevious research has extensively explored the concepts of language toxicity and pessimism, but little research has focused on their role in the polarization of debunking efforts within social media environments. Existing literature has established that misinformation spreads rapidly on social media\u00b2\u2075\u2013\u00b2\u2077, and that social media exhibit distinct patterns of engagement while debunking misinformation\u00b2\u2078,\u00b2\u2079. However, it remains unclear how debunking contents vary in terms of language toxicity and sentiment, and how these variations influence broader social polarization. Understanding this dynamic is crucial for gaining insights into the development of hate speech, negative sentiment, misinformation, fake news, and the formation of echo chambers. To address this gap, we investigated the polarized patterns of language toxicity and pessimism by analyzing discussions of the 2016 and 2020 U.S. presidential elections, and the QAnon conspiracy theory on Twitter and Reddit. This study sheds light on how toxicity and pessimism manifest within partisan communities, providing a more comprehensive understanding of the underlying dynamics.\nBuilding on these observations of complex polarization patterns, the central objectives of this research are twofold. First, we aim to map the polarized patterns of language toxicity and sentiment both within and between partisan groups-specifically Democrats and Republicans-on platforms like Twitter and Reddit. Second, we seek to explore how these linguistic features are linked to user engagement and behavior, with a particular emphasis on understanding the relationship between language toxicity and pessimism in debunking discourse. To guide our investigation, we pose the following research questions (RQs):\nRQ1 How do less-engaged users contribute to language toxicity and sentiment? RQ2 Whether the messaging mechanism of Twitter and Reddit contribute to the polarization? RQ3 How are replying behaviors correlated with language toxicity and pessimism? By addressing the polarized patterns of language toxicity and sentiment in debunking discourse, this study offers valuable contributions to the ongoing conversation about the role of social media in shaping political and social polarization. Our findings have broad implications for both academic researchers and policymakers concerned with mitigating the harmful effects of toxic online behavior and fostering healthier, more constructive digital discourse."}, {"title": "Methods", "content": "Data collection\nTo investigate these research questions empirically, we collected data from two major social media platforms. We used Twarc, a command line tool for collecting X data via the X API, to collect social data for our study. Over a 78-month period between October 1, 2016 and March 31, 2023, we used the Twitter Search API to gather tweets (now called \"posts\") by querying debunking-related keywords: \u201cfact check,\u201d \u201cfact-checking,\" \"fact checker,\" \"fact checkers,\u201d \u201cfake news,\u201d \u201cmisinformation,\" \"disinformation,\u201d \u201cdebunkers,\u201d \u201cdebunker,\u201d \u201cdebunking,\u201d and \u201cdebunk.\u201d. This dataset was used to extract tweets for the 2016, 2020 U.S. presidential elections and QAnon. For Reddit, we used data maintained by Pushshift\u00b9 from June 2005 to March 2023. The Pushshift Reddit dataset consists of two sets of files: submissions and comments\u00b3\u2070. Each line in a submission file contains a submission in JSON object format. The comments are a collection of NDJSON files, with each file containing one month of data. Each line in a comment file corresponds to a comment in JSON object format."}, {"title": "Sentiment calculation and pessimism detection", "content": "We utilized NLTK2, a well-recognized Python library for NLP, to calculate sentiment. The core idea for this calculation relies on the Valence Aware Dictionary and Sentiment Reasoner (VADER)\u00b3\u00b9. The algorithm splits the text into individual words and assigns a score to each word to determine whether it is positive, negative, or neutral. Based on these word-level scores, VADER computes an overall sentiment score for the entire text. For sentiment analysis, we analyzed the aggregated text of each user. Specifically, for reply-to users, we aggregated the texts that reply-to users received. The sentiment, quantified by the compound score, ranges from -1 (extremely negative) to 1 (extremely positive), indicating the overall polarity of the text. For pessimism user identification, we employed a RoBERTa-based model\u00b3\u00b2, specifically the \u201ccardiffnlp/twitter-roberta- base-emotion-multilabel-latest\u201d\u00b3 checkpoint available on Huggingface. This model is one of the most widely adopted deep learning-based models for sentiment classification. It is a fine-tuned RoBERTa\u00b3\u00b3 model trained with Affect Tweets\u00b3\u2074 for emotion detection. To obtain the proper compound score and classification of a user, we aggregate the user's text and then remove the duplicated texts."}, {"title": "Language toxicity measurement", "content": "To define language toxicity in our analysis, we leverage the Perspective API\u00b3\u2075, a leading tool for the automated detection of toxic language. Perspective API defines language toxicity as \"rude, disrespectful, or unreasonable comments likely to drive someone away from a discussion\u201d\u00b3\u2076,\u00b3\u2077. The Perspective API\u2074 uses a probability score to indicate how likely it is that a reader would perceive the comment provided in the request as containing a given attribute. The score is a value between 0 and 1, with a higher score indicating a greater probability that a reader would perceive the comment as containing the attribute. For example, if a comment receives a probability score of 0.7 for attribute TOXICITY, indicating that 7 out of 10 people would perceive that comment as toxic. To obtain the language toxicity value of a user, we aggregate the user's text and then remove the duplicated text. For the replying scenario, each replied-to user's received texts were considered. However, it is important to point out that such automated tools, like Perspective API, have limitations and potential biases, and people have to treat them considerably\u00b3\u2078."}, {"title": "Identification of 1-degree users", "content": "To determine how 1-degree users contributed to sentiment and language toxicity, we identified these users in the discussions of the 2016 and 2020 U.S. presidential elections. We constructed retweet network for Twitter, and reply network for Reddit for each topic. In the retweet network, each node represented a user and directed edges between nodes represented retweets, and each node represented a user and directed edges between nodes represented replies for Reddit. Then, we used NetworkX\u00b3\u2079 to construct a 2-core network. Finally, 1-degree users are defined as the set difference between the total user population and the set of 2-core users."}, {"title": "Identification of Republican and Democratic users and verification of user classes", "content": "As previous research\u2074\u2070 showed, the retweet network of Twitter can be used for user classification in QAnon conspiracy theory scenarios. We adopted this method, expecting to identify a characteristic retweet network where Republican and Democratic users are segregated. We constructed retweet networks using datasets from the 2016 and 2020 U.S. presidential elections and QAnon discussions. We applied k-core decomposition (k = 2)\u2074\u00b9, where each node represented a user and directed edges between nodes represented retweets. As expected, this resulted in a retweet network with two major clusters for each topic. We determined which cluster corresponded to Republican and Democratic groups by manually examining high-indegree users (those who were frequently retweeted) in each cluster, analyzing their tweets and profile descriptions. However, due to the nature of the reply-based mechanism, it was not possible to build a retweet network for the Reddit dataset. To complement the network-based classification approach, we employed PoliticalBiasBERT\u2074\u00b2 to classify users as Republican or Democratic. PoliticalBiasBERT is a BERT-based model fine-tuned with articles and their political ideology annotations from Allsides\u2076. To confirm whether the classification of Republican and Democratic users was reliable enough, we conducted a manual verification as follows. We conducted the manual verification by dividing all users into two classes. Two coders participated in this task. On Twitter, 20 Republican users and 20 Democratic users were randomly selected from each topic of 2016, 2020 U.S. presidential elections and QAnon. As a result, 120 users were obtained. On Reddit, we aggregated the users of all the topics and then randomly selected 25 Republican users and 25 Democratic users, for a total of 50 users. Then, we checked the consistency of their classifications by computing Cohen's kappa. For Twitter, the resulting Kappa values were 0.9264, 0.9116, and 0.6482 for the 2016, 2020 U.S. presidential elections, and QAnon, respectively. For Reddit, the resulting Kappa values was 0.7248. Note that Cohen's kappa value is interpreted as follows: 0.0\u20130.2 for slight agreement; 0.2-0.4 for fair agreement; 0.4-0.6 for moderate agreement; 0.6\u20130.8 for substantial agreement; and 0.8\u20131.0 for near-perfect agreement\u2074\u00b3. The Kappa values indicated substantial agreement for QAnon, and near-perfect agreement for the 2016, 2020 U.S. presidential elections of Twitter, and substantial agreement for Reddit. The manual verification certified our user classification result as statistically reliable."}, {"title": "Identification of minimal entropy interval", "content": "The Shannon entropy\u2074\u2074 was used to calculate the text entropy for each user:\n$H = -\\sum p(x)logp(x)$\n, where H represents the Shannon entropy, p(x) represents the word probability of the text of a user. To quantify the platform-specific differences in entropy, we measured the entropy difference between Twitter and Reddit. We proposed the Minimal Entropy Interval Identification algorithm (Algorithm 1). The algorithm aims to identify text information patterns in user engagement on Twitter and Reddit by finding the minimal entropy interval containing the majority of users. This methodology provides a quantitative approach to understanding user text information concentration through entropy distribution analysis. Given a distribution of entropy values from user interactions, we seek to find the smallest possible interval [a,b] such that: 1. The interval contains 50% (or just over 50% due to calculation reasons) of all users 2. The interval length l = b \u2212 a is minimized 3. The granularity of measurement is fixed at 0.1.\nMinimal Entropy Interval Identification methodology was developed to quantitatively characterize and compare user behavior patterns across digital platforms through the systematic analysis of entropy distributions. The algorithm implements a three-tiered computational approach to identify the minimal interval containing the majority (> 50%) of users within an entropy distribution. The primary computational framework consists of three interconnected functions operating at different analytical levels. The base function, ComputeProp, calculates the proportion of users within specified entropy bounds by determining the ratio of users within a given interval to the total user population. Building upon this, the FindIntervals function employs a sliding window analysis technique with a granularity of 0.1 to systematically identify intervals exceeding the 50% threshold criterion. This function iteratively evaluates potential intervals across the entire entropy range, recording qualifying intervals along with their associated metrics including start point, end point, length, and contained proportion. The highest-level function, FindMinimumInterval, orchestrates the overall search process by implementing an incremental expansion strategy, beginning with the minimum interval length of 0.1 and systematically increasing until a solution is found. The hierarchical computational approach ensures the identification of the smallest possible qualifying interval, thereby providing a quantitative metric for behavioral concentration. This analytical framework enables quantitative cross-platform comparisons through the interpretation of interval lengths: smaller intervals indicate more concentrated user behavior patterns (clustered entropy values), while larger intervals suggest more dispersed behavioral patterns. The methodology provides a robust foundation for comparative analysis of user engagement patterns across different platforms and topics, offering insights into platform-specific behavioral dynamics."}, {"title": "Bubble plot for entropy visualization", "content": "The entropy magnitude for each platform is derived by computing the median entropy values of Republican and Democratic users for the 2016 and 2020 U.S. presidential elections and QAnon discussions on Twitter and Reddit. Bubble size is a visualization size indicating the amount of entropy, here, we set q = 500, and a larger q produces a larger size for normalization transformation. The bubble size is calculated using:\n$Size_{Bubble} = q \\times \\frac{2^{H}}{2^{min_{p,t}H}}$\n, where a visualization parameter q is used to a generate distinctive bubble size indicating the amount of entropy (Here, we set q = 500, and a larger q produces a larger size for normalization transformation.), $min_{p,t}H$ represents the minimum entropy of a user category of a political affiliation for a topic (t) on a platform (p). The difference of two entropies (H1, H2) can be calculated using:\n$\\Delta H = H_1 \u2013 H_2 = lg\\frac{Bubble_{Size1}}{Bubble_{Size2}}$"}, {"title": "Results", "content": "Polarization in 1-Degree and 2-Core Users\nFigure 1 shows the retweet network constructed from the 2016, 2020, U.S. presidential elections and QAnon dataset, revealing that Republican and Democratic users were segregated. Based on the retweet network analysis, we then identified and classified Republican and Democratic users on both Twitter and Reddit platforms (cf. Methods). The demographics of the two classes of users were described in Table 1.\nWe found that the majority of participants in discussions of the 2016, 2020 U.S. presidential elections, as well as QAnon, on both Twitter and Reddit, were 1-degree users. While 1-degree users constituted the numerical majority, our analysis revealed that 2-core users exhibited more pronounced polarization in both language toxicity and pessimism. It is important to note that Twitter's degree is based on retweet networks, whereas Reddit's is derived from reply networks. As Table 1 shows, on Twitter, 340,234 (67.38%), 566,813 (56.28%), and 56,208 (80.68%) of participants in discussions of the 2016, 2020 U.S. presidential elections and QAnon, respectively, were 1-degree users, compared to 164,728 (32.62%), 440,393 (43.72%), and 3,463 (19.32%) 2-core users. A similar trend was observed on Reddit, where 61,376 (92.88%), 164,505 (95.15%), and 10,436 (99.24%) of participants were 1-degree users, while 4,707 (7.12%), 8,390 (4.85%), and 80 (0.76%) were 2-core users for the same topics, respectively."}, {"title": "Discussion", "content": "Our longitudinal analysis of social media debunking efforts reveals important insights into the manifestation of language toxicity and pessimism. Although our study focused specifically on Twitter and Reddit discussions of the 2016 and 2020 U.S. presidential elections and QAnon conspiracy theory, prior research on polarization patterns across social media platforms\u00b2\u2076 suggests broader applicability of our findings. This broader applicability is supported by research showing consistent patterns of polarization across different social media environments and various controversial topics\u00b2\u2076. Building on these observations, our analysis demonstrates that polarization in debunking discourse operates through multiple interconnected mechanisms, extending beyond traditional partisan divisions to encompass user engagement patterns, platform dynamics, and emotional expression.\nRegarding RQ1 about the role of less engaged users In addressing RQ1 regarding how less engaged users contribute to language toxicity, our analysis revealed that peripheral participants (1-degree users) play a previously underappreciated role in shaping the polarized landscape of debunking discussions. While research has traditionally focused on echo chambers and highly engaged users as primary vectors for spreading misinformation, our findings indicate that peripheral participants play a crucial role in shaping language toxicity of debunking discourse. This phenomenon can be understood through the lens of deindividuation theory from social psychology\u2074\u2077: when individuals feel less accountable due to limited community investment, they may express more extreme views when attempting to correct misinformation\u2074\u2078. However, our binary classification of user engagement (1-degree versus 2-core) may oversimplify the spectrum of participation patterns in debunking activities, as users might shift between categories over time or display different behaviors across topics.\nOur RQ2 examined whether the messaging mechanisms of Twitter and Reddit contribute to polarization. We found that platform architecture significantly influences polarization patterns, with Reddit's discussion structure facilitating higher overall toxicity levels while Twitter's messaging mechanism amplifies partisan differences. This aligns with research showing how platform features shape information exposure\u00b2. The negative correlation between language toxicity and pessimism was particularly pronounced on Reddit, suggesting that its community-driven structure may intensify emotional expressions in debunking efforts. These platform-specific patterns may generalize to emerging social media platforms with similar architectural features.\nIn examining RQ3, we found a significant negative correlation between reply frequency and toxicity levels, with this effect being particularly pronounced on Reddit. This suggests that sustained interaction in debunking efforts might activate social calibration mechanisms\u2074\u2079, similar to how intergroup contact reduces prejudice in offline settings\u2075\u2070. Yet Yarchi et al. (2021)\u2075\u00b9 found that increased interaction doesn't always reduce polarization in fact-checking contexts, highlighting the complexity of these dynamics.\nBuilding on Brady et al.'s (2017) \u2075\u00b2 framework of emotional content's role in political information diffusion, our study reveals three critical mechanisms that drive polarization in debunking discourse: the outsized influence of peripheral users in generating toxic debunking content, the platform-dependent relationship between information complexity and polarized debunking discourse, and the distinct patterns of emotional expression across partisan lines when challenging misinformation. Although our analysis centered on U.S. presidential elections and the QAnon conspiracy theory across Twitter and Reddit, the fundamental and platform-independent nature of these three mechanisms suggests broader applicability: they reflect basic patterns of human behavior in online information sharing rather than platform-specific phenomena The consistent patterns we observed suggest these findings could be applicable to emerging platforms like TikTok, Instagram, or YouTube, where similar dynamics of user engagement and information sharing exist. Additionally, the mechanisms we identified - particularly regarding user engagement levels and emotional expression - likely manifest in other controversial topics such as climate change\u2075\u00b3, abolitionism\u2075\u2074, and #Gamergate\u2075\u2075. Based on these three mechanisms, we propose that effective intervention strategies should address: (1) early engagement of peripheral users to prevent toxic content generation, (2) platform-specific architectural adjustments to manage information complexity, and (3) partisan-aware moderation approaches that account for different styles of emotional expression. Platform designers and policymakers should consider these dynamics when developing strategies to reduce polarization in debunking efforts, particularly by creating features that accommodate different partisan styles of emotional expression in debunking discussions."}]}