{"title": "Graph Neural Networks for O-RAN Mobility Management: A Link Prediction Approach", "authors": ["Ana Gonzalez Bermudez", "Miquel Farreras", "Milan Groshev", "Jos\u00e9 Antonio Trujillo", "Isabel de la Bandera", "Raquel Barco"], "abstract": "Mobility performance has been a key focus in cellular networks up to 5G. To enhance handover (HO) performance, 3GPP introduced Conditional Handover (CHO) and Layer 1/Layer 2 Triggered Mobility (LTM) mechanisms in 5G. While these reactive HO strategies address the trade-off between HO failures (HOF) and ping-pong effects, they often result in inefficient radio resource utilization due to additional HO preparations. To overcome these challenges, this article proposes a proactive HO framework for mobility management in O-RAN, leveraging user-cell link predictions to identify the optimal target cell for HO. We explore various categories of Graph Neural Networks (GNNs) for link prediction and analyze the complexity of applying them to the mobility management domain. Two GNN models are compared using a real-world dataset, with experimental results demonstrating their ability to capture the dynamic and graph-structured nature of cellular networks. Finally, we present key insights from our study and outline future steps to enable the integration of GNN-based link prediction for mobility management in 6G networks.", "sections": [{"title": "I. INTRODUCTION", "content": "Mobility management is a fundamental feature of cellular networks, ensuring seamless service continuity during user mobility by minimizing call drops, Radio Link Failures (RLFs) and unnecessary handovers (HOs), known as ping-ponging [1]. Traditionally, it follows a reactive approach, where HO decisions rely on predefined network parameters. However, determining optimal mobility parameters remains complex due to the trade-off between HO failures (HOF) and ping-pong effects. Additionally, the dynamic communication environment often limits reactive techniques. These challenges are further exacerbated in 5G networks, where densification, higher frequencies, and stringent Ultra-Reliable Low-Latency Communication (URLLC) requirements demand more sophisticated mobility management solutions [2].\nThe advent of Open Radio Access Network (O-RAN) has revolutionized the RAN by leveraging Artificial Intelligence (AI) and Machine Learning (ML) to enhance network performance and flexibility. These models, deployed at the edge and centralized cloud, enable intelligent decision-making and automation in managing modern cellular networks. Mobility management is a key O-RAN use case, and recent studies have explored proactive HO mechanisms based on user trajectory [3], measurement [4], and RLF prediction [5]. ML models such as sequence-to-sequence (Seq2Seq) and Long Short-Term Memory (LSTM) show promise in capturing long-term dependencies between User Equipment (UE) and cell metrics. However, these models rely on sequential data processing, which, while effective for temporal dependencies, fails to fully exploit the inherently graph-based structure of communication networks.\nTo better capture evolving network topologies, Graph Neural Networks (GNNs) have gained momentum in academia and industry [6], [7]. GNNs process graph-structured data by iteratively aggregating information from neighboring nodes for tasks like node/edge regression or link prediction. Existing studies have explored GNNs for predicting key UE/cell metrics such as signal strength for mobility management [8]. However, while GNNs have been used for regression tasks, to the best of our knowledge, no study has applied their link prediction capabilities to forecast the UE-cell connections.\nIn this article, we study the capabilities of GNN link prediction for mobility management and how to overcome the technical challenges when adapting these ML models as proactive HO mechanism. We propose next-cell prediction, the first proactive HO optimization solution that leverages a GNN-based link prediction model to forecast the upcoming cell connection for each user. We provide an in-depth analysis and experimental comparison of two categories of GNN link prediction models and analyze the complexity of applying them to the mobility management problem. Sec. II reviews the O-RAN framework, current HO optimization techniques and the role of GNNs, while also summarizing recent advancements in proactive HO optimization. Sec. III outlines the problem formulation and proposes an O-RAN framework for next-cell prediction. In Sec. IV, an in-depth analysis of different GNN categories for link prediction is presented. Sec. V compares the performance of prediction accuracy of subgraph-based and autoencoder-based GNN link prediction models, trained on real-world datasets. Finally, Sec. VI discusses the key findings and insights on the applicability of GNN-based link prediction for mobility management, concluding the article with practical implications and future directions."}, {"title": "II. BACKGROUND", "content": "This section presents a background on O-RAN, HO optimization, and GNNs in the context of cellular networks."}, {"title": "A. O-RAN", "content": "The O-RAN Alliance is a collaboration aimed to transform the mobile industry to be intelligent, open, multi-vendor, interoperable and autonomous. A key innovation is the introduction of Radio Intelligent Controllers (RICs): the non-Real-Time (non-RT) RIC for long-term optimization and planning, and the near-Real-Time (near-RT) RIC for dynamic network adjustments. These RICs facilitate the AI/ML integration through rApps and xApps. The major distinction between these applications lies in their hosting RIC, which establishes their response time restrictions: rApps in non-RT RIC with a response time higher than 1s, and xApps in near-RT RIC with a response time below 10ms."}, {"title": "B. Handover optimization in cellular networks", "content": "The baseline HO procedure in cellular networks, shown in Fig. 1, uses Layer 3 (L3) signaling. The UE remains connected to the source gNB while measuring metrics from the neighboring cells. These measurements, such as Signal-to-Noise Ratio (SNR), are reported to the source gNB (1), which initiates a HO if the source metrics weakens and a neighboring metrics are stronger. The source gNB selects a target cell, sends a HO request (2), and receives a configuration from the target gNB (3). It then instructs the UE to switch to the target cell (4), where the UE completes the process with a HO complete message (5).\nThroughout the years, network-triggered HO procedures like conditional HO and L1/L2 Triggered Mobility (LTM) [1] have been proposed to enhance HO process. In conditional HO, the UE executes a pre-stored HO command when a monitored condition is met, bypassing the need for measurement reports and additional signaling. In LTM, the network uses L1/L2 measurements to identify candidate cells and enables the UE to synchronize with the target cell via dual connectivity while maintaining the source cell connection.\nAI solutions have great potential to improve the mobility management mechanisms by introducing a proactive approach. In [3], a Seq2Seq model optimizes intra-frequency HOs by using UE trajectory data for training. [4] employs a LSTM model that integrates mobility details to next-cell prediction. These methods, while innovative, often rely on sequential data-processing models like Recurrent Neural Networks (RNNs) or Reinforcement Learning (RL) techniques (e.g., Deep Q-Networks, Deep-RL) that lack the capacity to fully exploit the inherently graph-based structure of cellular networks [9]."}, {"title": "C. Graph Neural Networks", "content": "GNNs are designed to process graph-structured data by iteratively aggregating information from neighboring nodes. This enables GNNs to capture both local and global graph properties for node/edge regression and link prediction. Cellular networks naturally form graphs of interdependent nodes with features, making GNNs suitable for solving network problems. They have been applied to radio resource management [10] [2], 5G signal inference [11] and traffic prediction [6].\nRecently, studies used GNNs for proactive mobility management. The authors in [8] formulated connection management as a graph optimization problem combined with DRL approach. However, this work relies on computationally expensive subgraph-based methods and evaluates feasibility using a small synthetic dataset, limiting its scalability to real-world networks. In the industry, Amazon presented a solution for next-cell prediction using GNNs [7]. While this highlights the potential of GNNs link prediction for cellular networks, the specific technologies and methods remain undisclosed. Finally, [5] proposes a GNN-based framework for predicting RLFs by aggregating weather effects and leveraging time-series transformers. However, this work focuses primarily on radio link reliability rather than mobility management.\nDespite these advancements, link prediction for proactive HO optimization in O-RAN systems remains underexplored. This gap presents significant opportunities for innovative GNN-based solutions tailored to dynamic, graph-structured cellular networks."}, {"title": "III. NEXT-CELL PREDICTION FOR HANDOVER \n\u039f\u03a1\u03a4\u0399\u039c\u0399\u0396ATION", "content": "The baseline HO procedure can lead to signaling overhead and delays, due to necessary re-transmissions which cause the HO to be triggered too late. In addition, the HO interruption time is typically between 50-90 ms, often too long for URLLC services. LTM, although designed to improve this interruption time, can lead to ping-ponging. Moreover, triggers based on low-level metrics may ignore the overall network quality or congestion levels, potentially leading to suboptimal HO. \u0391\u0399 predictive approaches, although promising, may struggle with adaptability and generalization in dynamic environments [9]. To address these challenges, we propose a GNN-based link prediction solution to the mobility management problem in cellular networks. Unlike traditional reactive and proactive methods, our approach enables accurate next-cell prediction for UEs, reducing signaling overhead and minimizing the risk of late HO triggers. By considering both low-level metrics and broader network conditions, the solution avoids ping-ponging and optimizes HO timing. To the best of our knowledge, we are the first to propose such a solution, positioning it as a foundational tool for network operators to optimize HO performance and enhance user experience."}, {"title": "IV. GRAPH NEURAL NETWORKS FOR LINK PREDICTION", "content": "To address the O-RAN next-cell prediction problem, this section explores available GNN models for link prediction, categorizing them into autoencoder-based and subgraph-based approaches. Additionally, it discusses the challenges and solutions for adapting these models to the mobility management problem. Table I provides a summary, outlining their inputs, outputs, benefits, and limitations."}, {"title": "A. Autoencoder-based models", "content": "Autoencoders are unsupervised models for link prediction by learning graph embeddings that capture structural and feature information. Using an encoder-decoder structure, the encoder generates a compressed graph representation, while the decoder predicts links based on these embeddings. This structure offers flexibility in the model design, allowing their adaptation to O-RAN specific challenges.\nTwo main variants exist: Graph Autoencoders (GAE) and Variational Graph Autoencoders (VGAE). GAE employs a deterministic approach, producing fixed latent representations, making it simpler and more efficient. VGAE, on the other hand, models a latent distribution, capturing uncertainty and variability, enhancing its adaptability. This probabilistic nature allows VGAE to represent nodes as distributions rather than fixed points, making it ideal for complex and evolving data."}, {"title": "B. Subgraph-based models", "content": "Subgraph-based models perform link prediction by extracting local subgraphs around target links, capturing patterns often missed in global representations. They are especially effective in dense, dynamic networks. SEAL and ComplEx use different approaches: SEAL implements supervised learning on enclosing subgraphs with a GNN to identify short-range dependencies, while ComplEx extends matrix factorization to multi-relational graphs, embedding nodes and edges in complex-valued vectors. ComplEx excels in asymmetric relationships like directed graphs, but may overlook broader structural properties."}, {"title": "C. Challenges of applying existing GNN models for mobility management", "content": "1) Data heterogeneity: The heterogeneous nature of cellular networks is characterized by diverse node types (e.g., gNB, cells, UE), interconnected by distinct edges (e.g., radio-link, interfaces), resulting in a heterogeneous graph that needs to be handled by the models. However, the vanilla versions of the GNN models for link prediction are only compatible with homogeneous graphs, impeding the effective integration of heterogeneous network structures. Two possible solutions are available for overcoming this challenge: the adaptation of the structure of the models to facilitate the use of heterogeneous graphs, or the transformation of the graph to a homogeneous one.\n2) Data imbalance: The graph structure in cellular networks exhibits an imbalanced node distribution, with a significantly higher number of UEs than cells. This imbalance can affect model performance, particularly for approaches relying on message passing, as low-degree nodes may not receive enough information, potentially limiting their predictive capabilities. GATs address this issue by employing a weighted attention mechanism that prioritizes significant neighbors, enhancing the focus on relevant connections during node embedding.\nThe node imbalance poses another challenge to obtain the next-cell prediction. Since the presented models generate probabilistic scores instead of binary outputs, it is necessary to apply a threshold to obtain the final predictions. This process also allows to obtain additional metrics such as recall or F1-score. However, the usual default threshold value of 0.5 is often unsuitable for imbalanced scenarios [12] where an optimal decision boundary is required. To address this, the optimal threshold for each model should be determined through multiple executions, selecting values that balance True Positives and True Negatives, enhancing robustness and accuracy under imbalanced data.\n3) Edge features: Edge features are crucial in next-cell prediction, as they directly influence O-RAN HO decisions. However, standard GNN models typically exclude edge features from the input graph, as they rely in a convolutional layer for message passing, only aggregating information from neighboring nodes to generate embeddings. This limitation can be addressed by incorporating an advanced convolutional layer capable of processing edge features (e.g., GAT, GINE), or by modifying the input graph structure to represent edge features as additional nodes."}, {"title": "V. EXPERIMENTAL EVALUATION", "content": "This section evaluates two of the above mentioned GNN models: VGAE based on GAT and SEAL. VGAE captures global embeddings, while SEAL focuses on localized sub-graphs, providing a complementary comparison for HO optimization. The following subsections outline the dataset, model creation, training methodology, and experimental setup."}, {"title": "A. Dataset description", "content": "We use an open-source dataset by Huawei Technologies [13] that comprises of approximately 35 million edges and 169,000"}, {"title": "B. Model creation", "content": "The particularities of the created graph and the solution objective require the implementation of multiple adaptations in the models. One general adaptation is derived for the nature of the dataset, which comprises two nodes types connected through a single hop. Consequently, the model design is constrained to employ a single layer for data transmission between neighbours. The incorporation of additional layers would result in the transmission of redundant information between users"}, {"title": "C. Model training", "content": "Once the models are created, a systematic training process is applied to optimize their performance. Firstly, the dataset is divided into training, validation, and test sets with an 80/10/10 split. A custom split function is created to obtain consistent partitions, preserving the structure in each split. This split generates, for each set, two types of edges: positive edge (existing edges) and negative edge (non-existing edge, required for contrastive supervision during training). We prevent potential issues associated with the use of automatic split generation functions (e.g., RandomLinkSplit from PyTorch Geometric), such as the inclusion of existing edges as negative edges, and the duplication of the same negative edge in different sets, which impact negatively on the models performance [14] [15].\nVGAE and SEAL use the labeled data to perform supervised training with Kullback-Leibler (KL) divergence and cross-entropy loss, respectively. We use the Adam optimizer due to its efficiency in handling large and sparse datasets. Fine tuning is also applied in both models, on parameters such as weights, learning rate, latent representation size. During training, model performance was evaluated using Area Under the ROC Curve (AUC), Average Precision (AP), Precision, Recall, F1-score, Accuracy, and Matthews Correlation Coefficient (MCC). \u03a4\u03bf address potential overfitting, we also apply early stopping in both models and L2 regularization in SEAL."}, {"title": "D. Performance comparison", "content": "Table II shows the mean metrics values obtained from 10 executions of each model. The first two rows correspond to SEAL and VGAE performances respectively, using the graph of 985 cells and 10,000 UEs. VGAE outperforms SEAL in terms of execution speed, achieving a 98.84% reduction in training time thanks to its direct graph processing, instead of the SEAL's subgraph extraction. Although VGAE also reduces inference time, both models times are below 100 ms, complying with the O-RAN rApp requirements. VGAE also demonstrates faster convergence during training, achieving satisfactory results within less than 50 epochs. This rapid convergence, coupled with its simpler architecture, enables efficient training, even on larger datasets.\nIn terms of predictive quality, both models exhibit comparable predictive performance. VGAE improves SEAL's AP and Recall in 6.21% and 11.23% respectively. Meanwhile SEAL achieves AUC, Precision, F1-score, Accuracy and MCC improvements over VGAE of 4.5%, 12.3%, 10.09%, 0.61% and 2.81% respectively. VGAE effectively identifies edges within a broader context, albeit with more False Positives (FP), thus it outperforms SEAL in metrics not dependent on FP such as Recall. However, due to the higher number of FPs in VGAE, SEAL outperforms VGAE in all other metrics. Also, the use of localized subgraph information enhances SEAL's precision in predictions with smaller datasets.\nThe following two rows of Table II examine the scalability of VGAE, increasing the graph size to 10,000 cells and 20,000 cells, respectively, both with the 100,000 UEs. SEAL was unable to run on larger datasets due to the computational cost associated with subgraph creation. The VGAE's training time increases linearly with the size of the graph. An overall improvement is achieved with the increase of the dataset size, from VGAE-985 to VGAE-10k. The metric with the highest growth is Recall with 4.91% followed by AUC and Accuracy and F1-score with 3%, 2.2% and 2.18% respectively. This indicates an increase in correctly predicted edges, resulting from better generalization by exposing the model to a wider range of subgraph structures and interactions. As a result, during training, the model is able to obtain more accurate node attention. However, the performance of the VGAE-20k model deteriorates. In this case, the used dataset maintains the same number of users (100,000) but duplicates the cells, from 10,000 to 20,000. This implies a drastic growth of the nodes with higher density, which may affect the attention mechanism and increase the FP, being the main limitation of the model.\nThe findings of this study demonstrate the efficacy of graph-based methodologies for link prediction in large-scale networks, even without features or temporal information, just using graph structure for learning. While no prior studies have applied GNN models for link prediction, works such as [5] and [3] use GNN-based regression models to estimate HOs, predicting radio link failure or user mobility respectively. [5] achieves 79% F1-score for urban scenarios. Therefore, over a similar scenario, our proposal achieves a 5% higher F1-score using only graph structure information. Also, [3] reports that its model can achieve 85% accuracy with nodes features processing and can be improved by up to 90% by including user's historical mobility data. Therefore, this suggests that the inclusion of temporality and features would enhance our model performance, which already demonstrates a high efficiency by using only the graph structure for training."}, {"title": "VI. DISCUSSION", "content": "This section discusses future research directions for O-RAN link prediction management."}, {"title": "A. GNN-link prediction and computational trade-offs", "content": "The SEAL model outperforms the VGAE model in reducing FPs, making it more selective in edge identification-critical for minimizing unnecessary HOs and system costs in O-RAN networks. However, VGAE is computationally more efficient and better suited for large-scale networks, excelling in larger graphs by leveraging its attention mechanism to capture node relationships. Both models have limitations: SEAL's high computational cost hinders scalability, while VGAE struggles in highly dynamic and complex environments, as shown in its declining performance in the final test scenario. Future research should address these limitations by exploring Spatio-Temporal GNNs like Temporal Graph Networks (TGNs) or GraphMixer, which incorporate temporal data such as user movement patterns and network dynamics to improve accuracy. These models also support heterogeneous graphs, enhancing attention mechanisms and adaptability. Including temporal information in datasets is essential to leverage these models, enabling more precise and timely HO decisions in evolving O-RAN networks."}, {"title": "B. Scalability", "content": "Scalability remains a challenge in large-scale O-RAN deployments, as existing models face limitations in dynamic and complex network environments. SEAL's high computational complexity restricts its use in massive networks, while VGAE shows performance degradation as dataset size increases. The usage of GraphSAGE could improve scalability by generalizing to unseen nodes, but its fixed neighbor sampling might result in information loss. To overcome these challenges, future research should explore Spatio-Temporal GNNs, which incorporate temporal information such as historical user movement and evolving network conditions, enabling more accurate and adaptive link prediction in O-RAN."}, {"title": "C. Lack of available datasets", "content": "A key limitation observed in both models is their reliance on the graph structure over node and edge features. The dataset used in this study has uniformly distributed, uncorrelated features, which makes it hard for the models to learn meaningful relationships. In real O-RAN networks, user movement, signal quality, and other measurements will provide more correlated information for the GNN model. Moreover, this dataset doesn't provide timestamps, which would not allow the use of Spatio-Temporal GNNs. There is a lack of publicly available 5G radio datasets, difficulting the testing and validation of GNN-link prediction models. Initially, emulators such as Keysight EXata and RICtest or VIAVI TeraVM RIC Test that support user mobility can be used to create complex cellular networks and generate temporal datasets with UE and cell features. Finally, in order to bring this kind of models close to real products it is crucial that telco operators make datasets publicly available."}, {"title": "D. Low connectivity and node density", "content": "Low connectivity is common in telecommunications datasets, as users are often linked to specific cells at a given moment. Sparse graphs have a significant impact on GNN models, especially those based on VGAE and GAE because of its dependence on the neighboring embeddings, decreasing overall link prediction accuracy, as shown in [15]. In cellular networks, this issue arises from isolated subgraphs of users connected to individual cells, as seen in our findings. However, the issue could be addressed by incorporating higher-level nodes, such as CU or DU nodes, to interconnect cells, increase density, and enable additional message passing layers, improving the generation of embeddings."}, {"title": "E. Negative edges creation", "content": "The authors in [14], [15] stress out the importance of the negative edges creation process. By using random sampling functions, positive edges of the original graph can be wrongly introduced as negative edges. Depending on the set in which wrong negative edges are introduced, this can lead to incorrect training (training set) or miscalculation of the error metrics due to reverse edge types (validation or test sets). Also, sampling functions duplicates negative edges in training/validation/test sets. The inclusion of test edges in the training set causes overfitting and distribution shift. Therefore, it can be observed that the creation of negative edges plays a significant role in autoencoder-based models. This process can be improved by leveraging the bipartite nature of the cellular network, facilitating the attention mechanism over cell nodes and reducing the degradation of VGAE for large-scale networks."}, {"title": "VII. CONCLUSIONS", "content": "In this article we proposed a GNN-based link prediction solution for proactive mobility management in O-RAN. Using real-world cellular data, we compared two GNN models: an autoencoder-based model and a subgraph-based model where results show the autoencoder model offers superior speed and training efficiency, especially for larger datasets, achieving competitive predictive performance with high recall and average precision. While the subgraph model excelled in precision, the autoencoder model demonstrated better generalization and recall with larger datasets, though its performance decreased with high node density. Given the ability of the autoencoder-based model to deliver accurate next-cell predictions efficiently and its potential to address challenges in mobility management, we advocate for further exploration of this approach. Future research could refine and extend these findings, paving"}]}