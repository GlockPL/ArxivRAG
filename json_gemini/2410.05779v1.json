{"title": "LIGHTRAG: SIMPLE AND FAST\nRETRIEVAL-AUGMENTED GENERATION", "authors": ["Zirui Guo", "Lianghao Xia", "Yanhua Yu", "Tu Ao", "Chao Huang"], "abstract": "Retrieval-Augmented Generation (RAG) systems enhance large language models\n(LLMs) by integrating external knowledge sources, enabling more accurate and\ncontextually relevant responses tailored to user needs. However, existing RAG\nsystems have significant limitations, including reliance on flat data representa-\ntions and inadequate contextual awareness, which can lead to fragmented answers\nthat fail to capture complex inter-dependencies. To address these challenges, we\npropose LightRAG, which incorporates graph structures into text indexing and\nretrieval processes. This innovative framework employs a dual-level retrieval sys-\ntem that enhances comprehensive information retrieval from both low-level and\nhigh-level knowledge discovery. Additionally, the integration of graph structures\nwith vector representations facilitates efficient retrieval of related entities and their\nrelationships, significantly improving response times while maintaining contextual\nrelevance. This capability is further enhanced by an incremental update algorithm\nthat ensures the timely integration of new data, allowing the system to remain\neffective and responsive in rapidly changing data environments. Extensive experi-\nmental validation demonstrates considerable improvements in retrieval accuracy\nand efficiency compared to existing approaches. We have made our LightRAG\nopen-source and available at the link: https://github.com/HKUDS/LightRAG.", "sections": [{"title": "1 INTRODUCTION", "content": "Retrieval-Augmented Generation (RAG) systems have been developed to enhance large language\nmodels (LLMs) by integrating external knowledge sources Sudhi et al. (2024); Es et al. (2024);\nSalemi & Zamani (2024). This innovative integration allows LLMs to generate more accurate and\ncontextually relevant responses, significantly improving their utility in real-world applications. By\nadapting to specific domain knowledge Tu et al. (2024), RAG systems ensure that the information\nprovided is not only pertinent but also tailored to the user's needs. Furthermore, they offer access to\nup-to-date information Zhao et al. (2024), which is crucial in rapidly evolving fields. Chunking plays\na vital role in facilitating the retrieval-augmented generation process Lyu et al. (2024). By breaking\ndown a large external text corpus into smaller, more manageable segments, chunking significantly\nenhances the accuracy of information retrieval. This approach allows for more targeted similarity\nsearches, ensuring that the retrieved content is directly relevant to user queries.\nHowever, existing RAG systems have key limitations that hinder their performance. First, many\nmethods rely on flat data representations, restricting their ability to understand and retrieve information\nbased on intricate relationships between entities. Second, these systems often lack the contextual\nawareness needed to maintain coherence across various entities and their interrelations, resulting\nin responses that may not fully address user queries. For example, consider a user asking, \u201cHow\ndoes the rise of electric vehicles influence urban air quality and public transportation infrastructure?\u201d\nExisting RAG methods might retrieve separate documents on electric vehicles, air pollution, and\npublic transportation challenges but struggle to synthesize this information into a cohesive response.\nThey may fail to explain how the adoption of electric vehicles can improve air quality, which in turn"}, {"title": "2 RETRIEVAL-AUGMENTED GENERATION", "content": "Retrieval-Augmented Generation (RAG) integrates user queries with a collection of pertinent doc-\numents sourced from an external knowledge database, incorporating two essential elements: the\nRetrieval Component and the Generation Component. 1) The retrieval component is responsible\nfor fetching relevant documents or information from the external knowledge database. It identifies and\nretrieves the most pertinent data based on the input query. 2) After the retrieval process, the generation\ncomponent takes the retrieved information and generates coherent, contextually relevant responses. It\nleverages the capabilities of the language model to produce meaningful outputs. Formally, this RAG\nframework, denoted as M, can be defined as follows:\nM = (G, R = (\u03c6,\u03c8)), M(q; D) = G (q, \u03c8(q; D)), D = 4(D) (1)\nIn this framework, G and R represent the generation module and the retrieval module, respectively,\nwhile q denotes the input query and D refers to the external database. The retrieval module R"}, {"title": "3 THE LIGHTRAG ARCHITECTURE", "content": ""}, {"title": "3.1 GRAPH-BASED TEXT INDEXING", "content": "Graph-Enhanced Entity and Relationship Extraction. Our LightRAG enhances the retrieval\nsystem by segmenting documents into smaller, more manageable pieces. This strategy allows for\nquick identification and access to relevant information without analyzing entire documents. Next,\nwe leverage LLMs to identify and extract various entities (e.g., names, dates, locations, and events)\nalong with the relationships between them. The information collected through this process will be\nused to create a comprehensive knowledge graph that highlights the connections and insights across\nthe entire collection of documents. We formally represent this graph generation module as follows:\nD = (V, \u00ca) = Dedupe \u25cb Prof(V, E), V, E = UD;\u2208DRecog(Di) (2)\nwhere D represents the resulting knowledge graphs. To generate this data, we apply three main\nprocessing steps to the raw text documents Di. These steps utilize a LLM for text analysis and\nprocessing. Details about the prompt templates and specific settings for this part can be found in\nAppendix 7.3.2. The functions used in our graph-based text indexing paradigm are described as:\n\u2022 Extracting Entities and Relationships. R(\u00b7): This function prompts a LLM to identify entities\n(nodes) and their relationships (edges) within the text data. For instance, it can extract entities\nlike \"Cardiologists\" and \"Heart Disease,\" and relationships such as \"Cardiologists diagnose Heart\nDisease\" from the text: \"Cardiologists assess symptoms to identify potential heart issues.\" To\nimprove efficiency, the raw text D is segmented into multiple chunks Di.\n\u2022 LLM Profiling for Key-Value Pair Generation. P(\u00b7): We employ a LLM-empowered profiling\nfunction, P(.), to generate a text key-value pair (K, V) for each entity node in V and relation\nedge in E. Each index key is a word or short phrase that enables efficient retrieval, while the\ncorresponding value is a text paragraph summarizing relevant snippets from external data to aid in\ntext generation. Entities use their names as the sole index key, whereas relations may have multiple\nindex keys derived from LLM enhancements that include global themes from connected entities.\n\u2022 Deduplication to Optimize Graph Operations. D(\u00b7): Finally, we implement a deduplication\nfunction, D(\u00b7), that identifies and merges identical entities and relations from different segments of"}, {"title": "3.2 DUAL-LEVEL RETRIEVAL PARADIGM", "content": "To retrieve relevant information from both specific document chunks and their complex inter-\ndependencies, our LightRAG proposes generating query keys at both detailed and abstract levels.\n\u2022 Specific Queries. These queries are detail-oriented and typically reference specific entities within\nthe graph, requiring precise retrieval of information associated with particular nodes or edges. For\nexample, a specific query might be, \"Who wrote 'Pride and Prejudice'?\"\n\u2022 Abstract Queries. In contrast, abstract queries are more conceptual, encompassing broader topics,\nsummaries, or overarching themes that are not directly tied to specific entities. An example of an\nabstract query is, \u201cHow does artificial intelligence influence modern education?\"\nTo accommodate diverse query types, the LightRAG employs two distinct retrieval strategies within\nthe dual-level retrieval paradigm. This ensures that both specific and abstract inquiries are addressed\neffectively, allowing the system to deliver relevant responses tailored to user needs.\n\u2022 Low-Level Retrieval. This level is primarily focused on retrieving specific entities along with their\nassociated attributes or relationships. Queries at this level are detail-oriented and aim to extract\nprecise information about particular nodes or edges within the graph.\n\u2022 High-Level Retrieval. This level addresses broader topics and overarching themes. Queries at this\nlevel aggregate information across multiple related entities and relationships, providing insights\ninto higher-level concepts and summaries rather than specific details.\nIntegrating Graph and Vectors for Efficient Retrieval. By combining graph structures with\nvector representations, the model gains a deeper insight into the interrelationships among entities.\nThis synergy enables the retrieval algorithm to effectively utilize both local and global keywords,\nstreamlining the search process and improving the relevance of results.\n\u2022 (i) Query Keyword Extraction. For a given query q, the retrieval algorithm of LightRAG begins\nby extracting both local query keywords k(l) and global query keywords k(9).\n\u2022 (ii) Keyword Matching. The algorithm uses an efficient vector database to match local query\nkeywords with candidate entities and global query keywords with relations linked to global keys."}, {"title": "3.3 RETRIEVAL-AUGMENTED ANSWER GENERATION", "content": "Utilization of Retrieved Information. Utilizing the retrieved information \u03c8(q; D), our LightRAG\nemploys a general-purpose LLM to generate answers based on the collected data. This data comprises\nconcatenated values V from relevant entities and relations, produced by the profiling function P(\u00b7). It\nincludes names, descriptions of entities and relations, and excerpts from the original text.\nContext Integration and Answer Generation. By unifying the query with this multi-source text,\nthe LLM generates informative answers tailored to the user's needs, ensuring alignment with the\nquery's intent. This approach streamlines the answer generation process by integrating both context\nand query into the LLM model, as illustrated in detailed examples (Appendix 7.2)."}, {"title": "3.4 COMPLEXITY ANALYSIS OF THE LIGHTRAG FRAMEWORK", "content": "In this section, we analyze the complexity of our proposed LightRAG framework, which can be\ndivided into two main parts. The first part is the graph-based Index phase. During this phase, we use\nthe large language model (LLM) to extract entities and relationships from each chunk of text. As\na result, the LLM needs to be called total tokens times. Importantly, there is no additional overhead\ninvolved in this process, making our approach highly efficient in managing updates to new text.\nchunk size\nThe second part of the process involves the graph-based retrieval phase. For each query, we first\nutilize the large language model (LLM) to generate relevant keywords. Similar to current Retrieval-\nAugmented Generation (RAG) systems Gao et al. (2023; 2022); Chan et al. (2024), our retrieval\nmechanism relies on vector-based search. However, instead of retrieving chunks as in conventional\nRAG, we concentrate on retrieving entities and relationships. This approach markedly reduces\nretrieval overhead compared to the community-based traversal method used in GraphRAG."}, {"title": "4 EVALUATION", "content": "We conduct empirical evaluations on benchmark data to assess the effectiveness of the proposed\nLightRAG framework by addressing the following research questions: \u2022 (RQ1): How does LightRAG\ncompare to existing RAG baseline methods in terms of generation performance? \u2022 (RQ2): How do\ndual-level retrieval and graph-based indexing enhance the generation quality of LightRAG? \u2022 (RQ3):\nWhat specific advantages does LightRAG demonstrate through case examples in various scenarios? \u2022\n(RQ4): What are the costs associated with LightRAG, as well as its adaptability to data changes?"}, {"title": "4.1 EXPERIMENTAL SETTINGS", "content": "Evaluation Datasets. To conduct a comprehensive analysis of LightRAG, we selected four datasets\nfrom the UltraDomain benchmark (Qian et al., 2024). The UltraDomain data is sourced from 428\ncollege textbooks and encompasses 18 distinct domains, including agriculture, social sciences, and\nhumanities. From these, we chose the Agriculture, CS, Legal, and Mix datasets. Each dataset contains\nbetween 600,000 and 5,000,000 tokens, with detailed information provided in Table 4. Below is a\nspecific introduction to the four domains utilized in our experiments:\n\u2022 Agriculture: This domain focuses on agricultural practices, covering a range of topics including\nbeekeeping, hive management, crop production, and disease prevention.\n\u2022 CS: This domain focuses on computer science and encompasses key areas of data science and\nsoftware engineering. It particularly highlights machine learning and big data processing, featuring\ncontent on recommendation systems, classification algorithms, and real-time analytics using Spark."}, {"title": "4.2 COMPARISON OF LIGHTRAGWITH EXISTING RAG METHODS (RQ1)", "content": "We compare LightRAG against each baseline across various evaluation dimensions and datasets. The\nresults are presented in Table 1. Based on these findings, we draw the following conclusions:"}, {"title": "4.3 ABLATION STUDIES (RQ2)", "content": "We also conduct ablation studies to evaluate the impact of our dual-level retrieval paradigm and the\neffectiveness of our graph-based text indexing in LightRAG. The results are presented in Table 2.\nEffectiveness of Dual-level Retrieval Paradigm. We begin by analyzing the effects of low-level and\nhigh-level retrieval paradigms. We compare two ablated models-each omitting one module-against\nLightRAG across four datasets. Here are our key observations for the different variants:"}, {"title": "4.4 CASE STUDY (RQ3)", "content": "To provide a clear comparison between baseline methods and our LightRAG, we present specific\ncase examples in Table 3, which includes responses to a machine learning question from both\nthe competitive baseline, GraphRAG, and our LightRAG framework. In this instance, LightRAG\noutperforms in all evaluation dimensions assessed by the LLM judge, including comprehensiveness,\ndiversity, empowerment, and overall quality. Our key observations are as follows:\ni) Comprehensiveness. Notably, LightRAG covers a broader range of machine learning metrics,\nshowcasing its comprehensiveness and ability to effectively discover relevant information. This\nhighlights the strength of our graph-based indexing paradigm, which excels in precise entity and\nrelation extraction as well as LLM profiling. ii) Both Diversity and Empowerment. Furthermore,\nLightRAG not only offers a more diverse array of information but also delivers more empowering\ncontent. This success is due to LightRAG's hierarchical retrieval paradigm, which combines in-depth"}, {"title": "4.5 MODEL COST AND ADAPTABILITY ANALYSIS (RQ4)", "content": "We compare the cost of our LightRAG with that of the\ntop-performing baseline, GraphRAG, from two key\nperspectives. First, we examine the number of tokens\nand API calls during the indexing and retrieval pro-\ncesses. Second, we analyze these metrics in relation\nto handling data changes in dynamic environments.\nThe results of this evaluation on the legal dataset are\npresented in Table 2. In this context, Textract repre-\nsents the token overhead for entity and relationship extraction, Cmax denotes the maximum number\nof tokens allowed per API call, and Cextract indicates the number of API calls required for extraction."}, {"title": "5 RELATED WORK", "content": ""}, {"title": "5.1 RETRIEVAL-AUGMENTED GENERATION WITH LLMS", "content": "Retrieval-Augmented Generation (RAG) systems enhance LLM inputs by retrieving relevant infor-\nmation from external sources, grounding responses in factual, domain-specific knowledge Ram et al.\n(2023); Fan et al. (2024). Current RAG approaches Gao et al. (2022; 2023); Chan et al. (2024); Yu\net al. (2024) typically embed queries in a vector space to find the nearest context vectors. However,\nmany of these methods rely on fragmented text chunks and only retrieve the top-k contexts, limiting\ntheir ability to capture comprehensive global information needed for effective responses.\nAlthough recent studies Edge et al. (2024) have explored using graph structures for knowledge\nrepresentation, two key limitations persist. First, these approaches often lack the capability for\ndynamic updates and expansions of the knowledge graph, making it difficult to incorporate new\ninformation effectively. In contrast, our proposed model, LightRAG, addresses these challenges\nby enabling the RAG system to quickly adapt to new information, ensuring the model's timeliness\nand accuracy. Additionally, existing methods often rely on brute-force searches for each generated\ncommunity, which are inefficient for large-scale queries. Our LightRAG framework overcomes this\nlimitation by facilitating rapid retrieval of relevant information from the graph through our proposed\ndual-level retrieval paradigm, significantly enhancing both retrieval efficiency and response speed."}, {"title": "5.2 LARGE LANGUAGE MODEL FOR GRAPHS", "content": "Graphs are a powerful framework for representing complex relationships and find applications\nin numerous fields. As Large Language Models (LLMs) continue to evolve, researchers have\nincreasingly focused on enhancing their capability to interpret graph-structured data. This body of\nwork can be divided into three primary categories: i) GNNs as Prefix where Graph Neural Networks\n(GNNs) are utilized as the initial processing layer for graph data, generating structure-aware tokens\nthat LLMs can use during inference. Notable examples include GraphGPT Tang et al. (2024) and\nLLaGA Chen et al. (2024). ii) LLMs as Prefix involves LLMs processing graph data enriched with\ntextual information to produce node embeddings or labels, ultimately refining the training process\nfor GNNs, as demonstrated in systems like GALM Xie et al. (2023) and OFA Liu et al. (2024). iii)\nLLMs-Graphs Integration focuses on achieving a seamless interaction between LLMs and graph\ndata, employing techniques such as fusion training and GNN alignment, and developing LLM-based\nagents capable of engaging with graph information directly Li et al. (2023); Brannon et al. (2023)."}, {"title": "6 CONCLUSION", "content": "This work introduces an advancement in Retrieval-Augmented Generation (RAG) through the\nintegration of a graph-based indexing approach that enhances both efficiency and comprehension\nin information retrieval. LightRAG utilizes a comprehensive knowledge graph to facilitate rapid\nand relevant document retrieval, enabling a deeper understanding of complex queries. Its dual-level\nretrieval paradigm allows for the extraction of both specific and abstract information, catering to\ndiverse user needs. Furthermore, LightRAG's seamless incremental update capability ensures that\nthe system remains current and responsive to new information, thereby maintaining its effectiveness\nover time. Overall, LightRAG excels in both efficiency and effectiveness, significantly improving the\nspeed and quality of information retrieval and generation while reducing costs for LLM inference."}, {"title": "7 APPENDIX", "content": "In this section, we elaborate on the methodologies and experimental settings used in the LightRAG\nframework. It describes the specific steps for extracting entities and relationships from documents,\ndetailing how large language models (LLMs) are utilized for this purpose. The section also specifies\nthe prompt templates and configurations used in LLM operations, ensuring clarity in the experimental\nsetup. Additionally, it outlines the evaluation criteria and dimensions used to assess the performance\nof LightRAG against baselines from various dimensions."}, {"title": "7.1 EXPERIMENTAL DATA DETAILS", "content": "Table 4 presents statistical information for four datasets: Agriculture, CS, Legal, and Mix. The\nAgriculture dataset consists of 12 documents totaling 2,017,886 tokens, while the CS dataset contains\n10 documents with 2,306,535 tokens. The Legal dataset is the largest, comprising 94 documents and\n5,081,069 tokens. Lastly, the Mix dataset includes 61 documents with a total of 619,009 tokens."}, {"title": "7.2 CASE EXAMPLE OF RETRIEVAL-AUGMENTED GENERATION IN LIGHTRAG.", "content": "In Figure 3, we illustrate the retrieve-and-generate process. When presented with the query, \u201cWhat\nmetrics are most informative for evaluating movie recommendation systems?\", the LLM first extracts\nboth low-level and high-level keywords. These keywords guide the dual-level retrieval process on the\""}, {"title": "7.3 OVERVIEW OF THE PROMPTS USED IN LIGHTRAG", "content": ""}, {"title": "7.3.1 PROMPTS FOR GRAPH GENERATION", "content": ""}, {"title": "7.3.2 PROMPTS FOR QUERY GENERATION", "content": ""}, {"title": "7.3.3 PROMPTS FOR KEYWORD EXTRACTION", "content": ""}, {"title": "7.3.4 PROMPTS FOR RAG EVALUATION", "content": ""}, {"title": "7.4 CASE STUDY: COMPARISON BETWEEN LIGHTRAG AND THE BASELINE NAIVERAG.", "content": "To further illustrate LightRAG's superiority over baseline models in terms of comprehensiveness,\nempowerment, and diversity, we present a case study comparing LightRAG and NaiveRAG in\nTable 5. This study addresses a question regarding indigenous perspectives in the context of corporate\nmergers. Notably, LightRAG offers a more in-depth exploration of key themes related to indigenous\nperspectives, such as cultural significance, collaboration, and legal frameworks, supported by specific\nand illustrative examples. In contrast, while NaiveRAG provides informative responses, it lacks\nthe depth needed to thoroughly examine the various dimensions of indigenous ownership and\ncollaboration. The dual-level retrieval process employed by LightRAG enables a more comprehensive\ninvestigation of specific entities and their interrelationships, facilitating extensive searches that\neffectively capture overarching themes and complexities within the topic."}]}