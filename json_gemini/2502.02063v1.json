{"title": "CASIM: Composite Aware Semantic Injection for Text to Motion Generation", "authors": ["Che-Jui Chang", "Qingze (Tony) Liu", "Honglu Zhou", "Vladimir Pavlovic", "Mubassir Kapadia"], "abstract": "Recent advances in generative modeling and tokenization have driven significant progress in text-to-motion generation, leading to enhanced quality and realism in generated motions. However, effectively leveraging textual information for conditional motion generation remains an open challenge. We observe that current approaches, primarily relying on fixed-length text embeddings (e.g., CLIP) for global semantic injection, struggle to capture the composite nature of human motion, resulting in suboptimal motion quality and controllability. To address this limitation, we propose the Composite Aware Semantic Injection Mechanism (CASIM), comprising a composite aware text encoder and a text-motion aligner that learns the dynamic correspondence between text and motion tokens. Notably, CASIM is model and representation-agnostic, readily integrating with both autoregressive and diffusion-based methods. Experiments on HumanML3D and KIT benchmarks demonstrate that CASIM consistently improves motion quality, text-motion alignment, and retrieval scores across state-of-the-art methods. Qualitative analyses further highlight the superiority of our composite aware approach over fixed-length semantic injection, enabling precise motion control from text prompts and stronger generalization to unseen text inputs. Our code is available at our project page: https://cjerry1243.github.io/casim_t2m.", "sections": [{"title": "1. Introduction", "content": "Human motion generation from text descriptions has gained increasing attention from the community, due to its immense potential for animating and editing lifelike motions with free-form text prompts (Athanasiou et al., 2024; Goel et al., 2024). The advancements in generative modeling techniques and efficient motion tokenization methods have been driving the progress in text to motion generation, with particular focus on enhancing motion quality. Several diffusion based (Tevet et al., 2023; Zhang et al., 2022; Guo et al., 2023; Chen et al., 2023; Setareh et al., 2024; Chang et al., 2024b) and autoregressive based (Zhang et al., 2023a; Huang et al., 2024; Jiang et al., 2024; Chen et al., 2024) methods have shown impressive capability in generating realistic and diverse motions from the input prompts. Typically, these methods leverage the pre-trained CLIP model (Radford et al., 2021) to summarize the whole motion description with various lengths and complexity into a fixed-length embedding, denoted as [CLS]. The compressed text embedding is then used as a conditional vector, injected into the motion generation model. The assumption is that the [CLS] token contains rich and global semantics that are suitable for motion generation. While this approach may seem to work to a certain degree for most existing models, the composite nature of human motions, causal textual order, and the fine-grained alignment of the text and motion tokens are poorly preserved. As illustrated in Fig. 1, the fixed-length semantic injection struggles to produce distinguishable conditional vectors for longer descriptions, leading to poor generalization and limited control over generated motions.\nTo address these limitations, we introduce CASIM, Composite Aware Semantic Injection Mechanism (Sec. 3). It comprises a composite aware text encoder and a text-motion aligner that learns dynamic alignment between each text tokens and motion frames. Compared with fixed-length approaches, CASIM offers a more generalized and robust architecture, allowing each textual component to influence either specific composites or global attributes of the motion sequence. Another key advantage of CASIM is its model-agnostic nature \u2013 it can be easily integrated with both diffusion-based and autoregressive-based approaches while consistently improving text-motion matching and retrieval accuracy. Furthermore, our method is compatible with various motion representations, from raw motion (Tevet et al., 2023) to different quantization approaches including VQ-VAE (Zhang et al., 2023a), PoseCode (Huang et al., 2024), and RVQ-VAE (Guo et al., 2023).\nOur experimental results (Sec. 4) demonstrate remarkable quantitative improvements in FID, R-precision, and Multi-modality Distance, on HumanML3D and KIT benchmarks, as we apply our CASIM to 5 SOTA methods, including MDM (Tevet et al., 2023), MotionDiffuse (Zhang et al., 2022), T2MGPT (Zhang et al., 2023a), COMO (Huang et al., 2024), and MoMask (Guo et al., 2023). CASIM can also improve motion quality and text-motion alignment for long-term human motion generation (Shafir et al., 2024; Lee et al., 2024). Analysis of attention weights inside CASIM verifies that our semantic injection mechanism effectively learns the expected dynamic matching between text tokens and motion frames, validating our design principles. Qualitative results further demonstrate the superiority of CASIM over fixed-length semantic injection methods, showing precise motion control from text prompts and robust generalization to unseen text inputs."}, {"title": "2. Related Works", "content": "Generating realistic human motions has been a longstanding challenge in computer graphics and computer vision. The field has evolved to embrace various input modalities and conditions for motion synthesis. Image and video-based approaches have focused primarily on human pose and shape estimation (Zhao et al., 2019) and 3D body tracking (Stathopoulos et al., 2024), enabling motion reconstruction and prediction from visual inputs. Audio-driven motion generation is another important direction, with music-to-dance synthesis (Alexanderson et al., 2023) and speech-to-gesture generation (Chang et al., 2022; 2023) showing promising results in creating natural human movements that align with acoustic signals. Text-to-motion generation has gained significant attention, as it offers intuitive control over motion synthesis through free-form text prompts (Guo et al., 2022a). Scene-aware motion generation considers environmental constraints and spatial relationships, enabling the synthesis of contextually appropriate movements within 3D environments (Cen et al., 2024). Generating a coordinated group of human motions and interactions (Chang et al., 2024b;a) has recently emerged as a novel research directionh adds another layer of difficulty to single-person motion generation due to the complex human interactions. Lastly, several works (Li et al., 2024; Zhou et al., 2024) have attempted to unify motion generation, planning, and understanding in a single framework, extending the capabilities of large language models to human motion domains."}, {"title": "2.2. Text-to-Motion Generation Models", "content": "Text-to-motion generation models can be broadly categorized into two approaches: diffusion-based and autoregressive-based methods. Diffusion-based models leverage an iterative denoising scheme (Dhariwal & Nichol, 2021) to generate motions from textual conditions. Notable works include MDM (Tevet et al., 2023), MotionDiffuse (Zhang et al., 2022), MLD (Chen et al., 2023), GMD (Karunratanakul et al., 2023), FineMoGen (Zhang et al., 2023b), and GraphMotion (Jin et al., 2023). For example, MDM employs a transformer encoder within each diffusion step, processing concatenated motion frames with text and timestamp embeddings. While MLD adopts a similar architecture, it operates in a learned latent space by compressing motion sequences into fixed-length representations. Autoregressive-based approaches, including T2M (Zhang et al., 2023a), TM2T (Guo et al., 2022b), MotionGPT (Jiang et al., 2024), MotionLLM (Chen et al., 2024), T2MGPT (Zhang et al., 2023a), and CoMo (Huang et al., 2024), generate motions sequentially and typically require effective motion tokenization strategies. For instance, T2MGPT utilizes VQVAE (Van Den Oord et al., 2017) for motion tokenization and implements a decoder-only architecture for motion token generation. CoMo follows a similar generator architecture but distinguishes itself by adopting heuristics-based posecodes (Delmas et al., 2022) as its discrete motion representation."}, {"title": "2.3. Semantic Injection for Motion Generation", "content": "For text-to-motion generation, the input prompts are typically encoded into a latent space with a well-trained text encoder before being passed to motion generation models. Previous works, such as MDM, MLD, T2MGPT and CoMo, leverage the pretrained CLIP text embedding to represent the full text prompt. CoMo (Huang et al., 2024) and FGMDM (Shi et al., 2023) includes several fine-grained keywords and descriptions from GPT4 (OpenAI et al., 2024) as augmented prompts for motion generation. Finemogen (Zhang et al., 2023b) targets at fine-grained motion control and editing, by specifying the spatial and temporal motion descriptions. GraphMotion (Jin et al., 2023) parses the sentence structure into a hierarchical semantic graph for any given input texts. It utilizes a graph reasoning network and a coarse-to-fine diffusion model for motion generation. Our CASIM is conceptually similar to GraphMotion as both are aimed at strengthening the text-motion correspondence by design. However, GraphMotion uses heuristic knowledge to create a static semantic graph and is only tied to its coarse-to-fine model. CASIM learns the dynamic alignment and hierarchical structure in a soft manner. It can also be flexibly integrated with the most existing models."}, {"title": "3. Composite Aware Semantic Injection", "content": "CASIM, Composite Aware Semantic Injection Mechanism, is designed to capture fine-grained semantic relationships between text descriptions and motion sequences. It preserves the composite nature of human motions and their causal textual ordering and allows each motion frame to dynamically align with relevant textual components at different granularities. CASIM consists of two principal components: a composite aware text encoder and a text-motion aligner. CASIM exhibits model-agnostic properties, as it is applicable to both autoregressive and diffusion-based motion generators, which represent the two predominant genres for state-of-the-art models. In Section 3.1, we introduce the formulation of CASIM, detailing its two major components. Section 3.2 describes for autoregressive motion generators and how CASIM is integrated. Section 3.3 discusses diffusion-based motion generators and explains how to adopt CASIM in this framework."}, {"title": "3.1. CASIM Formulation", "content": "Unlike traditional approaches that compress text descriptions into fixed-length [CLS] token, our text encoder preserves composite aware semantics through individual token-level embeddings. As shown in Fig. 2 (left), the encoder comprises a pretrained text encoder inside which are blocks of multihead self-attention layers that learn the latent features for the input text. We leverage the pre-trained text encoder from CLIP (Radford et al., 2021), project the latent encoder output to another embedding space, and inject the resulting token embeddings to the motion generator. Compared to fixed-length CLIP embeddings, our injection method preserve the semantics as granular as the token-level, which are essential for composite aware motion generation."}, {"title": "Text-Motion Aligner", "content": "The text-motion aligner is the core design of CASIM and can be integrated inside various motion generation models. Specifically, it establishes dynamic correspondence between motion frames and text tokens using multi-head attention (MHA). As illustrated in Fig. 2 (middle), each motion token is used as query to attend with the keys and values obtained from all the text tokens. The subsequent motion embeddings then are updated through the attention-weighted aggregation from all relevant text tokens. Depending on the motion generation approach, the aligner employs multihead self-attention (MHSA) for autoregressive generation or multihead cross-attention (MHCA) for diffusion-based generation."}, {"title": "3.2. CASIM for Autoregressive Motion Generation", "content": "Autoregressive motion generation approaches, inspired by language modeling principles (Jiang et al., 2024; Chen et al., 2024), typically follow a two-stage process. First, they learn a discrete representation of motions through tokenization. Given a motion sequence X = {x1,..., xT}, an encoder-decoder network is trained to transform it into a sequence of motion tokens M = {m1,..., mt/l}, where each subsequence of l frames Xl = {xi,..., xi+l} is mapped to a discrete token mi = E(Xl). The decoder D learns to reconstruct the original motion: X = D(M). In the second stage, these methods train an autoregressive model to predict motion tokens sequentially conditioned on a text prompt C. The generation process is as follows:\n\n\nWhere:\n1.  P(M|C) = P(m1|C) * \u03a0 (from i=1 to T/l) P(mi+1|m1,..., mi, C)  \n\nwhere each new motion token is predicted based on both the text condition and previously generated tokens. The final motion is obtained by passing the generated tokens through the pretrained decoder D.\nFor autoregressive generation, CASIM leverages a GPT-style transformer (Radford et al., 2018; Zhang et al., 2023a) with MHSA blocks for the text-motion aligner to predict the motion tokens. At generation step i, we concatenate the text token sequence C = {c1,..., cN } with previously generated motion tokens M<i, denoted as C \u2295 M<i. As shown in Figure 2 (right), this concatenated sequence is processed by an GPT-style transformer with stacked MHSA blocks that enable dynamic interaction between motion and text tokens. The probability of generating the next token is computed as:\n\n\nWhere:\n1.  P(mi|m<i, C) = \u03c3(MHSA(C \u2295 M<i)) \n\nwhere \u03c3 represents the softmax function.\nThis generation step is performed iteratively until the end-of-sequence token is predicted. The integration of CASIM with autoregressive models enables each motion token to be generated with awareness of both previously generated motions and the full text description."}, {"title": "3.3. CASIM for Motion Diffusion Generation", "content": "Unlike autoregressive approaches, motion diffusion models generate motions through iterative denoising of a Gaussian noise sequence X\u03c4. Let T denote the sequence length and D denote the motion dimension, then X\u03c4 \u2208 RT\u00d7D represents the noised motion at diffusion step \u03c4. Given a noise-free motion sequence X0, the diffusion process at each step t samples the denoised motion according to:\n\n\nWhere:\n1.  X\u03c4-1 ~ N(\u03bc\u03b8(X\u03c4, \u03c4, C), \u03a3(\u03c4)) \n\nwhere \u03a3(\u03c4) is a fixed variance schedule, C \u2208 RL\u00d7d denotes the text tokens with sequence length L and embedding dimension d. The mean term \u03bc\u03b8(XT, \u03c4, C) can be parameterized as \u03bc\u03b8(XT, X0), where X0 = G\u03b8(XT, \u03c4, C) is the predicted noise-free motion by the denoiser G\u03b8.\nWhile there are no constraints on how the denoiser should be designed as long as the input and output shape matches, the transformer encoder and decoder are the two widely adopted options in the literature. To integrate CASIM with these denoiser variants in diffusion models, we adapt our text-motion aligner to employ MHSA for the transformer encoder and MHCA for the transformer decoder."}, {"title": "Denoising with Transformer Encoder", "content": "In the encoder-based approach, we first augment the text embeddings C with diffusion step embeddings TE(\u03c4), where TE(\u00b7) de-\n\n\nWhere:\n1.  X0 = MHSA((C + TE(\u03c4)) \u2295 XT) \n\nwhere \u2295 denotes sequence concatenation."}, {"title": "Denoising with Transformer Decoder", "content": "The decoder-based variant first processes motion features through self-attention layers, enabling each motion frame to attend to other frames. It then applies cross-attention (MHCA) between the processed motion embeddings X\u03c4 and the timestep-augmented text embeddings:\n\n\nWhere:\n1.  X0 = MHCA(X\u03c4, C + TE(\u03c4)) \n\nBoth formulations enable dynamic text-motion alignment during the denoising process. We analyze the performance of all model genres and variants in Section 4.2."}, {"title": "4. Experiments", "content": "We evaluate CASIM on two standard datasets through extensive quantitative and qualitative analyses. Section 4.1 describes our experimental setup, followed by quantitative results in Section 4.2, qualitative analysis in Section 4.3, and extension to long-form generation in Section 4.4."}, {"title": "4.1. Evaluation Setup", "content": "Datasets and Metrics. We evaluate on HumanML3D (Guo et al., 2022a) and KIT Motion Language (KIT-ML) (Plappert et al., 2016). HumanML3D is a large-scale motion language dataset, containing a total of 14,616 motions from AMASS (Mahmood et al., 2019) and HumanAct12 (Guo et al., 2020) datasets. Each motion is paired with 3 textual annotations, totaling 44,970 descriptions. KIT-ML dataset consists of a total of 3,911 motions and 6,278 text annotations, providing a small-scale evaluation benchmark. The datasets are split into train-valid-test sets with a ratio of 0.8:0.05:0.15. Performance is measured using the following metrics: (a) Frechet Inception Distance (FID), which evaluates the overall motion quality by computing the distributional difference between the latent features of the generated motions and those of real motions from test set; (b) R-Precision, which reports the retrieval accuracy between input text and generated motions from their latent space; (c) MM-Distance, which reports the distance between the input text and generated motions at the latent space; and (d) Diversity, which assesses the diversity of all generated motions. All metrics are computed using a separate text-motion matching network from (Guo et al., 2022a)."}, {"title": "Baseline Models", "content": "We conduct experiments for CASIM with five state-of-the-art (SOTA) models: T2MGPT (Zhang et al., 2023a), CoMo (Huang et al., 2024), MoMask (Guo et al., 2023), MotionDiffuse (Zhang et al., 2022), and MDM (Tevet et al., 2023), covering various genres of motion generation models as well as both continuous and discrete motion representations.\nT2MGPT and CoMo are autoregressive motion generation models, which first tokenize the motion sequence using a quantization-based encoder-decoder structure. We use the autoregressive form of CASIM, as detailed in Section 3.2, in place of the fixed-length text injection for their motion token generation. MotionDiffuse and MDM are diffusion-based motion generation models that conditionally denoise full motion sequences at each diffusion step with the fixed-length semantic embedding. We apply the composite aware text injection for both the encoder and decoder variants of MDM, while for MotionDiffuse, we use the encoder-based semantic injection. MoMask employs a hierarchical motion quantization scheme and a multi-layer masked motion generation framework. We apply the encoder-based semantic injection for their M- and R- Transformers in our study.\nAll the other settings follow the baseline methods and hyperparameters remain unchanged."}, {"title": "4.2. Quantitative Analysis", "content": "For quantitative evaluation, we report the results for each metric averaged over 20 repeated iterations on both datasets. Tab. 1 and 2 present quantitative comparison of CASIM-MDM and CASIM-T2MGPT with the SOTA text-to-motion generation methods on both datasets. Our method outperforms most SOTA methods in terms of R-Precision and MM-Dist, showing the effectiveness and robustness of CASIM in learning the text-motion correspondence. In terms of motion quality, both CASIM-MDM and CASIM-T2MGPT achieve comparable FID score with some leading methods, such as Graph-Motion, FineMoGen, and T2MGPT, on both benchmarks. Notably, the positive results are achieved through our semantic injection mechanism alone, without additional heuristic knowledge or textual semantics from external source like GPT4, demonstrating CASIM's ability to extract rich motion semantics directly from the input text without requiring external semantic augmentation.\nAcross all experiments, CASIM shows consistent performance improvements regardless of model architecture (diffusion or autoregressive), configuration choices (encoder/decoder, teacher forcing rates), or additional inputs (with/without keywords). This robust adaptability suggests that CASIM's semantic injection mechanism provides fundamental improvements in learning text-motion correspondence that generalize across different modeling approaches."}, {"title": "Architecture and configuration analysis", "content": "We conduct extensive experiments to validate CASIM's effectiveness across different architectural choices and configuration settings, as shown in Tab. 5. For diffusion-based MDM, we explore both encoder and decoder-based implementations of CASIM. Both variants demonstrate substantial improvements over their baseline (Encoder: Top1 R-Precision 0.471\u21920.489, FID 0.325\u21920.265; Decoder: Top3 R-Precision 0.608\u21920.793, FID 0.767\u21920.165), showcasing CASIM's adaptability to different architectural choices. Particularly noteworthy is that these improvements hold even with significantly reduced computation-using only 50 diffusion steps instead of 1000, both variants maintain strong performance gains while achieving 20x faster inference.\nFor autoregressive models like T2MGPT, we examine CASIM's behavior under different teacher forcing settings. During training, random masking (t = [0, 1] or \ud835\udf0f = 0.5) helps bridge the gap between training and inference, where the predicted tokens may differ from the ground truth. While the best performance occurs when \ud835\udf0f = 0.5, CASIM can further improve its text-motion alignment (R-Precision: 0.491\u21920.539, MM-Dist: 3.118 2.838). CASIM demonstrates robust performance across these hyperparameter configurations, with all t = 0, \ud835\udf0f = 0.5 and \u03c4 = [0, 1] achieving significant improvements, showing its resilience to different autoregressive model configurations.\nThe CoMo experiments reveal another interesting aspect of CASIM's capabilities. The original CoMo relies on 11 additional keywords per description, augmented through GPT-4 to provide detailed motion characteristics across body parts and styles. Remarkably, CASIM-CoMo without any keywords outperforms the keyword-augmented baseline (R-Precision: 0.539 vs 0.487, FID: 0.226 vs 0.263), demonstrating CASIM's ability to extract rich motion semantics from the input text without requiring external semantic augmentation.\nAcross all experiments, CASIM shows consistent performance improvements regardless of model architecture (diffusion or autoregressive), configuration choices (encoder/decoder, teacher forcing rates), or additional inputs (with/without keywords). This robust adaptability suggests that CASIM's semantic injection mechanism provides fundamental improvements in learning text-motion correspondence that generalize across different modeling approaches."}, {"title": "4.3. Qualitative Analysis", "content": "Fig. 3 demonstrates CASIM's ability to improve motion generation quality through representative examples from the HumanML3D test set. Compared to baseline models, both CASIM-MDM and CASIM-T2MGPT show superior ability in following complex action sequences and maintaining temporal order. Their generated motions closely align with ground truth (GT), particularly in capturing the nuaunced semantics for spatial relationships and action transitions.\nSpecifically, for the input text \u201ca man runs to the right then runs to the left then back to the middle\", both CASIM-MDM and CASIM-T2MGPT accurately capture directional changes and chronological order, while baseline models struggle with spatial positioning and temporal progression. For another text, \"a person is holding his arms straight out to the sides then lowers them, claps, and steps forward to sit in a chair\", our models precisely follow each action component in sequence, whereas baseline models either miss critical components or fail to maintain the correct order. Given the third prompt, \u201ca person walks forward then turns completely around and does a cartwheel\", CASIM-MDM and CASIM-T2MGPT successfully reproduce the complete action sequence including the cartwheel motion, despite its infrequent appearance in the dataset.\""}, {"title": "4.4. Long-term Motion Generation", "content": "While CASIM primarily targets single motion generation, we explore its effectiveness in long-term motion generation using DoubleTake (Shafir et al., 2024), a framework that employs an additional two-stage diffusion process to generate smooth transitions between the motion clips generated by diffusion models like MDM.\nAs shown in Tab. 6, CASIM consistently improves the motion clips across metrics and handshake sizes overall. For handshake size of 20 frames, CASIM significantly reduces the motion FID from 0.953 to 0.463 while improving text alignment (Top1: 0.309\u21920.358) and maintaining motion diversity (9.624\u21929.668). Similar improvements are observed with longer handshake periods of 30 and 40 frames, though the gains in FID gradually decrease as the transition period extends. Interestingly, the transition quality shows mixed results. With 20-frame handshake, the transition FID slightly increases (1.540\u21922.052), while longer handshakes see improvements (30 frames: 2.220\u21921.896; 40 frames: 2.410\u21921.905). This suggests that CASIM's semantic injection, while effective for individual motion generation, interacts differently for compositing several motion clips during the motion blending and transition generation process. The observations raise interesting questions about how semantic injection methods influence diffusion-based transition generation in long-term motion synthesis, particularly regarding the balance between local motion quality and smooth transitions."}, {"title": "5. Conclusion", "content": "We propose CASIM, a simple yet effective method for semantic injection that works with various text-to-motion models and representations, from autoregressive to denoising diffusion and discretized motion tokens to continuous raw motion sequences. Our experiments suggest that CASIM consistently improves the motion quality and strengthens the text-motion alignment across several state-of-the-art models on HumanML3D and KIT benchmarks. The method shows promise in enhancing long-term human motion generation. Notably, it enables more precise motion control through input text compared to fixed-length semantic injection approaches.\nWhile our semantic injection method shows potential for processing very long text inputs for zero-shot long-term motion generation, it sill relies on motion blending techniques such as DoubleTake. This limitation largely arises from the training dataset itself, which lacks long text and motion samples. Future work would focus on curating datasets with extended text-motion pairs and developing techniques to effectively leverage such data. Though CASIM preserves the composite nature for text injection, it shows limited improvements with methods like MLD (Chen et al., 2023) that encode motion as a fixed-length latent vector. This compression itself constrains the learning of fine-grained text-motion correspondence. Please refer to Supp. Mat. for more discussion."}, {"title": "Impact Statement", "content": "Our work on text-to-motion generation aims to advance machine learning and its relevant applications. The primary positive impact lies in enabling more intuitive and accessible ways to create human animations, potentially benefiting creative industries, interactive content creation, and assistive technologies. However, we acknowledge potential risks with the generation of inappropriate or misleading motions if the system is misused. While our method focuses on improving technical capabilities rather than introducing new application scenarios, we encourage future research and development to implement appropriate content filtering and user verification mechanisms to ensure a safe and responsible use of this technology."}, {"title": "A. Additional Qualitative Results", "content": "We provide a supplementary video on our project page, demonstrating side-by-side comparisons between baseline models (MDM, T2MGPT) and their CASIM-enhanced versions (CASIM-MDM, CASIM-T2MGPT). The video showcases the superiority of our method as CASIM enables more precise control over generated motions through text prompts, accurately capturing nuanced word differences and maintaining proper chronological order."}, {"title": "B. Ablation Study", "content": "To understand how different text encoders affect CASIM's performance, we compare CLIP with BERT using MDM as the base model. As shown in Tab. 7, BERT-based CASIM achieves better performance in text-motion alignment metrics (R-Precision: 0.511 vs. 0.478, MM-Dist.: 2.938 vs. 3.272) and motion diversity (9.630 vs. 9.468). While CLIP shows slightly better FID (0.303 vs. 0.346), the overall results suggest that BERT's contextualized word embeddings might be more suitable for capturing motion-relevant semantics.\nThis performance difference could be attributed to BERT's bidirectional context modeling and its pretraining objectives that focus on understanding relationships between word tokens, which aligns well with our goal of preserving composite-aware semantics and understanding the nuanced differences between token embeddings."}, {"title": "B.2. CLIP Embeddings from Different Layers for CASIM", "content": "While our main experiments use CLIP's latent embeddings for semantic injection, it is interesting to understand how CLIP token embeddings from different layers for CASIM could potentially influence the performance of text-to-motion generation. We compare the token embeddings from the output of the final layer and the output of the last layer before final projection. Tab. 8 presents this comparison using MDM as the base model. Using final layer embeddings shows stronger text-motion alignment (R-Precision Top1: 0.517 vs. 0.478) and lower multimodal distance (MM-Dist.: 2.945 vs. 3.272). However, this comes at the cost of motion quality, as indicated by the higher FID score (0.410 vs. 0.303). This trade-off suggests that while the final-layer embeddings might better capture text-motion correspondences, the latent embeddings may provide a better balance between motion quality and semantic alignment."}, {"title": "C. Limitation with MLD", "content": "We evaluate CASIM with MLD (Chen et al., 2023), a diffusion-based method that operates in a learned latent space by compressing motion sequences into fixed-length vectors. As shown in Tab. 9, CASIM provides limited improvements: while FID slightly improves (0.532\u21920.502), text-motion alignment metrics show marginal decreases (Top1 R-Precision: 0.469\u21920.452, MM-Distance: 3.282\u21923.389). This performance pattern differs from our results with other models and can be attributed to MLD's choice of using fixed-length latent vectors for motion representation. While this design benefits motion editing tasks, it inherently limits the model's ability to capture composite motion structures. The observation suggests that CASIM's effectiveness depends on the underlying motion representation's ability to preserve temporal and structural information."}, {"title": "D. Visualization of Text-Motion Attention", "content": "We visualize the text-motion attention in CASIM-MDM to understand how text tokens influence motion generation at different motion frames. Figure 5 shows the attention patterns for the prompt \"a person wave his arms and then sit down\u201d. The visualization demonstrates CASIM's ability to capture temporal dependencies: early frames (0-40) attend strongly to words related to the waving motion, while later frames (60-100) shift attention to sit-related tokens. This progressive attention transition validates our design of composite-aware semantic injection for handling sequential motion descriptions."}]}