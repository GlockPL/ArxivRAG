{"title": "Forecasting S&P 500 Using LSTM Models", "authors": ["Prashant Pilla", "Raji Mekonen"], "abstract": "With the volatile, complex nature of financial data which is also influenced by many external factors, forecasting the stock market has been seen to be a challenging task. Traditional models like ARIMA and GARCH were observed to be good with linear data. However, the stock market data involves non-linear dependencies and intricate patterns that are better handled by machine learning and deep learning approaches. Taking that a step further to patch hyper-parameter tuning and computational complexity that machine learning lacks, we get deep learning models like Long Short-Term Memory (LSTM) networks. In this report, we compare ARIMA and LSTM models in predicting the S&P 500 index, one of the most popular financial benchmarks. Using historical price data and technical indicators, we evaluated these models using the Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE) metrics. The ARIMA model showcased reasonable performance with an MAE of 462.1, RMSE of 614, and an accuracy of 89.8%. This demonstrated its effectiveness in capturing short-term trends but also showed that it is limited by its linear assumptions. The LSTM model, with favorable features, achieved an MAE of 369.32, RMSE of 412.84, and an accuracy of 92. 46%, capturing both short- and long-term dependencies. The LSTM model without features outperformed the version with all features, achieving an MAE of 175.9, RMSE of 207.34, and an accuracy of 96.41%, which showcased its ability to handle market data. Accurately forecasting the stock market is crucial because of its effect on investment strategies, risk assessments, and market stability. By taking advantage of the sequential processing capabilities of LSTM, this report confirms how deep learning methods can handle volatile market conditions when compared to traditional models. The results of our analysis not only reaffirm the transformative potential of LSTM but also provide steps that can be taken to improve upon the model. Through this comprehensive study forecasting financial data, we aim to showcase the insights, limitations, and potential for prediction accuracy.", "sections": [{"title": "1 Introduction", "content": "Stock price forecasting has always been a fundamental and challenging problem when dealing with financial time series. When it comes to financial market data, there are many factors in play such as high volatility, non-linear dynamics, and sensitivity to many factors including historical prices, trading volumes, macroeconomic indicators, and investor sentiment. Although predicting exactly where stock prices may move is considered impossible, there are many tools that help investors when trying to forecast a market. These tools allow them to find trends, patterns, and potential price movements in order to have well-considered choices. However, the complexity of stock prices and many other factors can make this a very hard problem.\nTraditional time series models such as Autoregressive Integrated Moving Average (ARIMA) and Generalized Autoregressive Conditional Heteroskedasticity (GARCH) are a good base when trying to solve the challenging problem of forecasting financial data. This is because of their ability to model linear relationships and short-term patterns. ARIMA has been shown to be useful for finding trends and seasonality, while GARCH is better suited for modeling time-varying volatility. Their strengths come with some drawbacks with certain time series data because the models assume stationarity and linearity within the data. These limitations include handling non- linear dependencies, uncovering complex patterns over time, and finding long-term relationships within the data it is given. These limitations are the reason why it is less effective with financial data as it is non-linear, has some long-range dependencies, and is influenced by many factors.\nSome of these challenges can be addressed by using machine/deep learning techniques, which have been shown to be better alternatives. Basic Recurrent Neural Networks (RNNs) introduced the ability to process sequential data by maintaining a hidden state that captures information from prior time steps. This allows the model to learn the temporal dependencies within the data for later use. However, RNNs face a critical limitation: they struggle to keep the information over long sequences because of the vanishing gradient problem. This happens when gradients used in the neural network become increasingly small during backpropagation, which ultimately hinders the network's ability to learn long-term dependencies.\nTo go a step further, Long Short-Term Memory (LSTM) networks were developed to overcome the drawbacks of basic RNNs. LSTMs have a unique architecture that includes input, forget, and output gates. The memory capabilities of LSTM networks make it well-suited for financial time series forecasting, allowing the model to capture both short and long-term trends by keeping or discarding certain information. There are also LSTM variants such as Bidirectional LSTM (BiL- STM), Gated Recurrent Units (GRU), and Attention-LSTM that further improve the performance by enhancing parts of the model. These features allow the model to be very well-rounded and suitable for financial data.\nWith constant change and many moving pieces in the stock market, it may be challenging for some investors to stay on top of a never-ending cycle of fluctuations. Being able to predict SPX prices can help aid investors in making the right decision and give insight into a never-ending stream of data. This paper explores the applications of machine learning models such as LSTM and traditional models such as ARIMA to forecast the S&P 500 Index (SPX). The goal of using data such as historical prices and other financial metrics is to see if any underlying patterns in the data could help give insight into the market flow. With a combined interest in computer science"}, {"title": "2 Literature Review", "content": "Traditional statistical methods have been shown to be great methods for time series forecasting due to their simplicity, interpretability, and ability to perform well on smaller data sets. These models, however, have drawbacks when they are applied to financial data as it is highly complex and nonlinear."}, {"title": "2.1 Traditional Statistical Methods for Stock Prediction", "content": "Traditional statistical methods have been shown to be great methods for time series forecasting due to their simplicity, interpretability, and ability to perform well on smaller data sets. These models, however, have drawbacks when they are applied to financial data as it is highly complex and nonlinear."}, {"title": "2.1.1 ARIMA Model", "content": "The Autoregressive Integrated Moving Average (ARIMA) model is highly used as a statistical approach for forecasting sequential data. As demonstrated by Adebiyi et al. [4], the ARIMA model can capture short-term dependencies and generate a reasonable prediction, which was used to forecast financial data.\nThe ARIMA model consists of three parts:\n\u2022 Autoregressive (AR): The linear relationship between its current value and its lagged observations.\n\u2022 Differencing (I): The number of times the series is differenced to achieve stationarity to remove trends and seasonality.\n\u2022 Moving Average (MA): Relates the current value to past forecast errors using a number of lagged errors.\nThe study by Adebiyi et al. applied ARIMA to predict daily stock prices and found that the model performed well on stationary time series. Their experiments involved rigorous parameter selection (using p, d, and q terms) and model validation. They also acknowledged that ARIMA struggles with capturing the nonlinear and long-term dependencies with the given data set, which are rooted in the stock price's movements."}, {"title": "2.1.2 ARCH/GARCH Models", "content": "While ARIMA focuses on the average behavior seen throughout the time series data, factoring in volatility is essential for forecasting financial data as it factors into the prices of the stock market."}, {"title": "2.2 Machine Learning Approaches for Stock Prediction", "content": "Machine learning approaches have been shown to be better alternatives to traditional statistical methods for stock prediction because they are able to model the complex, nonlinear relationships that can be found in financial time series data. Unlike traditional models, machine learning algo- rithms can analyze larger data sets, identify patterns, and adapt to changes made in the data over time. This makes them well suited for the dynamic and uncertain nature seen in the stock market."}, {"title": "2.2.1 Support Vector Regression (SVR) with Grey Correlation Degree", "content": "Support Vector Regression (SVR) is a machine learning method that is popular for its use in handling high dimensional and nonlinear data. Wang [2] showed how this model combined with Grey Correlation Degree (GCD) can be used to enhance accuracy by optimizing feature selection which improved stock price forecasting. GCD assigns weights to input features based on their correlation with its target variable. This allows SVR to focus on the most relevant features in order to make its prediction.\nSVR is comprised of a hyperplane in a high-dimensional space which minimizes the error bet between the predicted and actual values while keeping the model as generalized as possible. By introducing GCD, enhancements are made to SVR by reducing noise and focusing solely on the critical predictors which addresses the key challenge in stock prediction: finding the most impactful features. By improving the feature selection, Wang [2] showed a significant boost in accuracy when"}, {"title": "2.2.2 k-Nearest Neighbors (KNN)", "content": "The k-Nearest Neighbors (KNN) is an algorithm that predicts future values by finding past values that are most closely related to the current one. Alkhatib et al. [10] used the KNN algorithm to forecast stock prices for six major companies listed on the Jordanian Stock Exchange. The K\u039d\u039d algorithm measures the Euclidean distance between data points to identify the closest matches and uses these neighbors to make predictions. KNN does not make assumptions on the underlying data showing that it is a nonparametric method. Alkhatib et al. [10] showed that KNN was quite effective when predicting closing prices. This is because of its simple structure and its ability to adapt to nonlinear relationships within the data. During this study, the evaluation of KNN's robustness for stock price prediction by measuring error ratios was seen to be low. The authors highlighted that selecting the right amount of neighbors and distance metrics is key for performing well. It is also worth noting that this model does struggle with very large datasets and noisy data if the right preprocessing is not performed."}, {"title": "2.2.3 Random Forest for Stock Price Prediction", "content": "Random Forest is a machine learning method that combines multiple decision trees to improve prediction accuracy and reduce overfitting. Khaidem et al. [12] applied Random Forest to predict the direction of stock market prices using historical stock data and technical indicators to fore- cast stock for major companies like Apple (AAPL) and General Electric (GE). Random Forest generates several decision trees, each trained on random subsets of features and data points. The predictions from individual trees are combined by averaging to produce a final output. Khaidem et al. [12] showed this method achieves higher prediction accuracy due to its ability to model nonlinear patterns in financial data."}, {"title": "2.2.4 Hidden Markov Models (HMM)", "content": "Hidden Markov Models (HMM) are used widely for time series forecasting to model systems that transition between hidden states over time. Hasanbas [7] applied HMM to forecast financial time series to show its ability to capture the probabilistic states of stock market trends. In this applica- tion of the model, HMM assumes that the observed time series data are generated by a sequence of hidden states, each of which each follows a distinct probability distribution.\nHMM consists of three parts:\n\u2022 Hidden States: Unobserved states that the system transitions between over time\n\u2022 Transition Probabilities: Probabilities of transitioning from one hidden state to another at each time step."}, {"title": "2.2.5 Bayesian Time Series Analysis", "content": "The Bayesian method is another probabilistic method used for time series forecasting. Unlike the previous model, the Bayesian model introduces uncertainty into the predictions. Steel [8] showed the application of Bayesian time series analysis for stock prediction. This model involves constructing posterior distributions for model parameters using Bayes' theorem. For financial data, this model is well suited as it can handle noisy, incomplete, or volatile data. Steel [8] highlighted that Bayesian methods can outperform deterministic approaches in volatile markets by accounting for model and parameter uncertainties."}, {"title": "2.3 Deep Learning Methods", "content": "Deep learning methods have been seen to be an improvement upon traditional statistical and machine learning methods when it comes to very large complex data sets. These models excel at capturing long-term dependencies, modeling complex relationships, and being able to adapt to high-dimensional financial time series data."}, {"title": "2.3.1 Long Short-Term Memory (LSTM) Networks and Recurrent Neural Networks (RNNs)", "content": "Long Short-Term Memory (LSTM) networks have been shown to be one of the best choices when it comes to handling sequential data. They are an improvement upon RNNs, which struggle with the vanishing gradient problem, where information from early time steps tends to fade away. Their unique architecture comprises memory cells along with three gates:\n\u2022 Input Gates: Controls which parts of the previous memory are discarded.\n\u2022 Forget Gates: Determines what new information should be added to the memory.\n\u2022 Output Gates: Regulates what information is output for the current step.\nThese components work together to selectively retain, update, and output information to capture both the short-term fluctuations and long-term trends in the dataset. Zou and Qu [1] implemented"}, {"title": "2.3.2 Alpha-RNN", "content": "Another improved variant of RNNs is called a-RNN, which offers some computational efficiency while being able to maintain good prediction accuracy. Dixon and London [9] proposed using a-RNN for forecasting financial time series data. The a-RNN enhances traditional RNNs by in- troducing an exponential smoothing layer which helps the model retain long-term information in the data by combining past and current states. Dixon and London showed that a-RNNs are com- petitive in financial forecasting as they offer a reduced computational overhead when compared to LSTM. However, they lack fine-grained control over information flow which allows LSTM to handle complex, long-term dependencies."}, {"title": "2.3.3 Graph Neural Networks (GNNs) for Financial Time Series Prediction", "content": "Exploring spatial and relational information in financial data has led some researchers to use Graph Neural Networks (GNNs). GNNs are designed to process data that is represented as graphs, where each node and edge encode entities and their relationships. Xiang et al. [6] proposed using a variant of GNNs called Temporal and Heterogenous GNN (TH-GNN). This allowed them to try and forecast financial time series by modeling a relationship between several financial indicators and the market data. Unlike the sequential model LSTM, this model captures dependencies between different entities, such as different stocks and indices, over time providing a better understanding of the market."}, {"title": "2.3.4 Federated Learning with Large Language Models (LLMs)", "content": "Another emerging trend in deep learning is integrating Federated Learning (FL) and Large Lan- guage Models (LLMs) for financial forecasting. FL is a decentralized machine learning technique that allows multiple edge devices (clients) to collaboratively train a global model while keeping the data localized. LLMs are advanced machine-learning models designed to understand and gener- ate human-like text. They are built using the transformer architecture, where text is converted into numerical representations called tokens, which are then processed through a series of encoder and/or decoder layers that analyze relationships between words in a sentence. Abdel-Sater and Hamza [5] introduced an FL model which incorporated a pre-trained LLM. Instead of processing text, Abdel-Sater and Hamza adapted the LLM to handle stock data. This allowed them to use the collaborative training on multiple data sources without sharing any raw financial data. However, this comes with the cost of being less efficient on numerical financial data as opposed to the LSTM model."}, {"title": "2.4 Reflection", "content": "Stock price forecasting is a challenging task that has evolved from using traditional statistical meth- ods, such as ARIMA and GARCH, to more advanced machine learning and deep learning methods. Although machine learning methods improved the handling of non-linear data, as seen in SVR, KNN, and Random Forest, they have drawbacks in computational efficiency and handling high- dimensional data, such as time series. These drawbacks can be overcome by using Deep Learning methods such as LSTM, \u03b1-RNNs, Graph Neural Networks (GNNs), and Federated Learning with LLMs. LSTM networks above all have emerged to be a leading solution by effectively capturing short and long-term dependencies in financial time series data while being computationally efficient."}, {"title": "3 Approach to Solve the Problem", "content": "This study compares two models, ARIMA and LSTM, to forecast SPX. Both models leverage their own unique methodologies to forecast time series data, and by comparing both models, it will highlight their respective strengths and weaknesses."}, {"title": "3.1 Data Sources and Features", "content": "The dataset used in this study encompasses daily values for the S&P 500 (SPX) over the period of October 2013 to September 2024. The historical SPX data include:\n\u2022 50 and 200-day Moving Averages (MOV_AVG_50/200D)\n\u2022 14-day Relative Strength Index (RSI_14D)\n\u2022 Open and Closing Prices (PX_OPEN/CLOSE)\n\u2022 High and Low Prices (PX_HIGH/LOW)\n\u2022 Daily Price High-Low Difference (PX_HIGH_LOW_DIFFERENCE)\n\u2022 Daily Volume (PX-VOLUME)\n\u2022 30-day Volatility (VOLATILITY_30D)\n\u2022 Beta (BETA_ADJ_OVERRIDABLE)\nAlong with the historical SPX data, the following additional metrics are considered:\n\u2022 SPX Ratios:\nPrice-to-Earnings Ratio (PE_RATIO)"}, {"title": "3.2 Models", "content": "The ARIMA model is a statistical time-series model that incorporates the three components Au- toRegressive (AR), Integrated (I), and Moving Average (MA) for the parameters (p, d, q). This model requires the data to be stationary where the properties mean, variance, and autocorrelation should remain constant over time. Therefore, taking the difference from the current time step to the next one is done to remove trends. For this study, Auto-ARIMA is used to automate the selection of the optimal parameters (p, d, q) for the model.\nThe LSTM model is a deep learning model designed for sequential data. The network processes an input sequence, maintaining memory through three gates; forget, input, and output. For this study, the architecture of the neural network uses two LSTM layers with 64 neurons each with dropout layers to prevent overfitting by dropping a random 20% of neurons after each iteration. Also, a dense output layer will be used to predict the next price with the Adam optimizer and Mean Squared Error (MSE) as the loss function."}, {"title": "4 Experiment Design", "content": ""}, {"title": "4.1 Data Preprocessing", "content": "With all the features obtained, key features were selected based on their correlation with SPX closing price (Figure 1), through a correlation vector. Features that showed strong positive or negative correlations were prioritized, with absolute values greater than 0.5, ensuring the LSTM model used relevant predictive information. For the ARIMA model, only the SPX closing price was used as it only works with just one time series and doesn't take other features into account.\nData Splitting:"}, {"title": "4.2 Evaluation Metrics", "content": "To compare the models, the following metrics are used:"}, {"title": "5 Experiment Results", "content": ""}, {"title": "5.1 ARIMA", "content": "The ARIMA model showed a reasonable performance with the results showing an MAE value of 462.1 and RMSE value of 614. The results showed an accuracy percentage of 89.8%. The results (Figure 2) showed this model to be effective in capturing short-term trends in the data, but its reliance on linear assumptions limited its ability to capture the entire testing set."}, {"title": "5.2 LSTM", "content": "The LSTM model performed well, outperforming the ARIMA model in both forecasts. For the model that included favorable features (Figure 3), it achieved an MAE of 369.32 and an RMSE of 412.84. This showed an accuracy of 92.46%, showing it captured both the short-term and long-term dependencies. The model that did not include features (Figure 4) showed an MAE of 175.9 with"}, {"title": "6 Analysis of Results", "content": "The results from the ARIMA model showcased the performance of the statistical time series forecast. From the evaluation metrics, it can be seen that the model captured 89.8% of the variability in the"}, {"title": "7 Conclusion", "content": "In this study, the application of two time series models, ARIMA and LSTM, were used to forecast the S&P 500 (SPX) index.\nThe ARIMA model served as a traditional statistical benchmark, showing how effective it is in capturing short-term trends but being limited in handling nonlinearity and long-term dependencies. Our results showed that ARIMA achieved an MAE of 462.1, RMSE of 614.0, and an accuracy of 89.8%, with its performance declining over longer forecasts.\nIn contrast, the LSTM model, which is a deep learning approach designed for sequential data, significantly outperformed the ARIMA model. The LSTM model without additional features achieved the best results with an MAE of 175.9, RMSE of 207.34, and an accuracy of 96.41%. The model that included favorable features demonstrated slightly lower performance with an MAE of 369.32, RMSE of 412.84, and an accuracy of 92.46%. This difference in performance also high- lights the importance of good feature engineering and noise reduction. These results also align with some of the findings from prior literature such as Zou and Qu [1] and Sonkavde et al. [3], where LSTM models were shown to effectively capture long and short-term dependencies in financial data.\nAlthough the LSTM model showed superior performance, there are still areas where it can be improved. Future work includes:\n\u2022 Hybrid Model Integration: Combining the strengths of this model with others such as GARCH for its ability to handle volatility or GNNs to capture the relationship of the financial data with other features.\n\u2022 Optimizing LSTM Architecture: Looking into ways to incorporate LSTM variants such as Bidirectional LSTM, Attention-LSTM, or incorporate a transformer-based model as seen by Abdel-Sater and Hamza [5].\n\u2022 Generalizing the Model: Seeing the model performed well on SPX data, testing this out on other indices or stocks may show its robustness and capabilities across markets.\nThrough this study, we showed the potential of LSTM networks highlighting the powerful approach deep learning models are capable of. Our findings showed that the LSTM model offers a powerful way to handle volatile, nonlinear, complex data seen in financial markets, showing a direction to where stock prediction can go."}]}