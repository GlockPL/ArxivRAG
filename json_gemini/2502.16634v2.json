{"title": "OPTIONZERO: PLANNING WITH LEARNED OPTIONS", "authors": ["Po-Wei Huang", "Pei-Chiun Peng", "Hung Guei", "Ti-Rong Wu"], "abstract": "Planning with options a sequence of primitive actions \u2013 has been shown ef- fective in reinforcement learning within complex environments. Previous studies have focused on planning with predefined options or learned options through ex- pert demonstration data. Inspired by MuZero, which learns superhuman heuristics without any human knowledge, we propose a novel approach, named OptionZero. OptionZero incorporates an option network into MuZero, providing autonomous discovery of options through self-play games. Furthermore, we modify the dy- namics network to provide environment transitions when using options, allowing searching deeper under the same simulation constraints. Empirical experiments conducted in 26 Atari games demonstrate that OptionZero outperforms MuZero, achieving a 131.58% improvement in mean human-normalized score. Our behav- ior analysis shows that OptionZero not only learns options but also acquires strate- gic skills tailored to different game characteristics. Our findings show promising directions for discovering and using options in planning. Our code is available at https://rlg.iis.sinica.edu.tw/papers/optionzero.", "sections": [{"title": "INTRODUCTION", "content": "Reinforcement learning is a decision-making process in which an agent interacts with environments by selecting actions at each step to maximize long-term rewards. Actions, commonly referred to as primitive action, advance the state by one step each. While this granularity allows precise control at each time step, it can lead to inefficiencies in scenarios where predictable sequences of actions are beneficial. For example, in a maze navigation task, it is more efficient to choose a sequence of actions - such as following a straightforward path until reaching a new junction \u2013 rather than deciding an action at each time step. This approach reduces the frequency of decision-making and accelerates the learning process. To address these challenges, the concept of options (Sutton et al., 1999) has emerged, providing a framework for executing temporally extended actions based on the current state. Options bridge single-step decision-making and strategic long-term planning, not only speeding up the learning process to handle complex scenarios but also simplifying the decision- making by reducing the frequency of choices an agent must consider.\nPrevious works have proposed adopting the concept of options by either predefining options or learning from expert demonstration data (Sharma et al., 2016; Durugkar et al., 2016; de Waard et al., 2016; Gabor et al., 2019; Czechowski et al., 2021; Kujanp\u00e4\u00e4 et al., 2023; 2024). However, the pre- defined options often rely on a deep understanding of specific environments, and expert data may not be available for every environment, making it difficult to generalize these methods to other envi- ronments. Moreover, when planning with options, previous methods require recurrently executing each action within the option to obtain the next states (de Waard et al., 2016) or verifying whether the subgoal can be reached through primitive actions (Czechowski et al., 2021; Kujanp\u00e4\u00e4 et al., 2023; 2024). This increases the computational cost when executing longer options during planning, especially in scenarios where environment transitions are expensive.\nInspired by the success of MuZero (Schrittwieser et al., 2020), which employs a learned dynamics network to simulate the environment transitions during planning and achieves superhuman perfor- mance from scratch without requiring any human knowledge, this paper proposes a novel approach,"}, {"title": "RELATED WORKS", "content": "Numerous studies have explored the concepts of options in reinforcement learning. For example, de Waard et al. (2016) incorporated options from a predefined option set into Monte Carlo tree search (MCTS) and extended it to focus exploration on higher valued options during planning. Sharma et al. (2016) proposed using two policies for planning: one determines which primitive action to use, and the other determines how many times to repeat that action. Durugkar et al. (2016) explored the effects of repetition and frequency by statistics in Atari games. Vezhnevets et al. (2016) introduced a method which learns options through end-to-end reinforcement learning. Lakshminarayanan et al. (2017) proposed a method that allows agents to dynamically adjust rates of repeated actions. Bacon et al. (2017) derived an option-critic framework, which learns a policy over options and a policy within options. The option policy not only determines how to select and execute an action within options but also learns when to terminate the option. Kim et al. (2023) proposed to adaptively inte- grate multiple exploration strategies for options based on the option-critic framework. Riemer et al. (2020) introduced a parameter-sharing approach for deep option learning. Young & Sutton (2023) discovered options by learning the option policies and integrated them with a Monte Carlo search. Jinnai et al. (2019) formalized the problem of selecting the optimal option set, and produced an algorithm for discovering the suboptimal option set for planning. Veeriah et al. (2021) proposed a meta-gradient approach for discovering reusable, task-independent options. In addition, several works have studied subgoals, which represent a target state to achieve after several time steps, either segmented by predefined time step intervals or predicted dynamically by a learned network. For example, Gabor et al. (2019) used predefined subgoals for planning in MCTS. Czechowski et al. (2021) introduced a Subgoal Search method to obtain fixed-length subgoals with a low-level policy that predicts primitive actions for reaching subgoals. Kujanp\u00e4\u00e4 et al. (2023) proposed Hierarchical Imitation Planning with Search (HIPS), which learns subgoals from expert demonstration data. Ku- janp\u00e4\u00e4 et al. (2024) extended HIPS to HIPS-e, adding a low-level (primitive action) search to the high-level (subgoal) search, guaranteeing that subgoals are reachable. In summary, these previous works either adopt predefined options, learn subgoals from expert data, or do not incorporate options in MCTS planning. Compared to these works, our goal is to automatically discover options without relying on predefined options or expert data and to use options during planning."}, {"title": "MUZERO", "content": "MuZero (Schrittwieser et al., 2020) is based on the foundation of AlphaZero (Silver et al., 2018), distinguishing itself by learning environment transitions using neural networks. This allows MuZero to plan in advance without extra interaction with the environment, which is particularly advantageous in environments where such interactions are computationally expensive. Consequently, MuZero has achieved success in a wide range of domains (Schrittwieser et al., 2020; Danihelka et al., 2022; Antonoglou et al., 2021; Hubert et al., 2021; Mandhane et al., 2022; Wang et al., 2023)."}, {"title": "OPTIONZERO", "content": "4.1 \u039f\u03a1\u03a4\u0399ON NETWORK\nOptions are the generalization of actions to include temporally extended actions, which is applied interchangeably with primitive actions (Sutton et al., 1999; Bacon et al., 2017). In this context, options on Markov decision process (MDP) form a special case of decision problem known as a semi-Markov decision process (SMDP). Given a state st at time step t and an option length L, we enumerate all possible options, denoted as ot+1 = {at+1,at+2, ..., at+L}, by considering every sequence of primitive actions starting at st. When executing the option ot+1, we obtain a sequence of states and actions St, at+1, St+1, At+2, ..., St+L\u22121,At+L,St+L. Ideally, the probability of selecting each option can be calculated by multiplying the probabilities of each primitive action within the option, as illustrated in Figure 1a. For example, when L = 4, the probability of option o1 = {a1, a2, a3, a4} for so is P(a1) \u00d7 P(a2) \u00d7 P(a3) \u00d7 P(a\u2084) = 0.84 = 0.4096, where P(ai) is the probability of selecting action ai. A naive approach to obtaining the option probabilities involves using a policy network to evaluate all possible states from st to St+L. However, this approach is computationally expensive, and the number of options grows exponentially as the option length L increases, making it infeasible to generate all options.\nIn practice, since most options occur infrequently due to their lower probabilities, our primary in- terest lies in the dominant option. The dominant option, o1 = {a1, a2, ..., a\u2081}, is defined such that"}, {"title": "4.2 PLANNING WITH DOMINANT OPTION IN MCTS", "content": "This subsection describes the modifications to MCTS implemented in OptionZero to incorporate planning with the dominant option. For simplicity, we will use option to represent the dominant option in the rest of this paper. The planning generally follows the MuZero but with two modi- fications, including the network architecture and MCTS. For the network architecture, we add an additional option output to the prediction network, denoted as Nk,pk, vk = fo(sk), where Ok, pk, and vk are the option distribution, policy distribution, and value at state sk, respectively. Note that we use superscript sk instead of subscript sk in this subsection. This is because sk represents the hidden state, obtained after unrolling k steps by the dynamics network from the initial hidden state s\u00ba. In contrast, sk denotes the actual observed state in the environment at time step k. As illustrated in the previous section, we can derive the option of from \u03a9*. The dynamics network, denoted as sk+l pk+1,k+l = go(sk, Ak+1), is modified to predict the next hidden state sk+l and the accumu- lated discounted reward rk+1,k+l upon executing a composite action Ak+1 at sk. The composite action, Ak+1, can be either a primitive action ak+1 or an option ok+1 with the length l. The accu- mulated discounted reward rk+1,k+l is computed as \u03a3\u03af\u03b3-1pk+i,k+i, where ri,i represents the single immediate reward obtained by applying a\u00b2 at state si\u22121. Note that the dynamics network supports unrolling the option directly, eliminating the need to recurrently evaluate each subsequent state from sk to sk+l.\nNext, we demonstrate the incorporation of options within MCTS. The search tree retains the struc- ture of the original MCTS but includes edges for options that can bypass multiple nodes directly, as shown in Figure 2. This adaptation integrates options subtly while preserving the internal actions within options, allowing the tree to traverse states using either a primitive action or an option. Each"}, {"title": "4.3 TRAINING OPTIONZERO", "content": "We describe the optimization process using the self-play trajectory in OptionZero, as shown in Figure 3. For better illustration, we utilize three additional symbols, including \u041e, \u0418, \u0442, and \u2191. Given a state st at time step t, Oi represents the i-th executed composite action starting from st, Ui is defined as the discounted reward obtained after executing Oi, Ti denotes the action sequence length of Oi, and i = \u2211j=1 Tj is the accumulated length from O\u2081 to O\u00bf. For example, in Figure 3, from the perspective of st, we can obtain O\u2081 = 0t+1 = {at+1, at+2}, O2 = {at+3}, with corresponding discounted rewards U\u2081 = Ut+1 + yut+2, U2 = ut+3, action sequence lengths 7\u2081 = 2, T2 = 1, and accumulated lengths \u2191\u2081 = 2,72 = 3. Then, the observed discounted reward U\u2081 at st is calculated as yut+1+i, aggregating the observed rewards provided by the environment with a discount factor \u03b3. The n-step return z\u0142 is calculated as U\u2081 + y^1U2 + ... + y\u00ceT-1UT + y\u00ceT Vt+\u00eer, where \u00ceT = n. Note that Ut+n is not always available, as st+n may be bypassed when options are executed. Consequently, we identify the smallest T such that \u00eer \u2265 n, ensuring that the step count for the n-step return approximates n as closely as possible. In Figure 3, if n = 5, since st+5 is skipped by option, we then approximate the n-step return by using vt+6 as zt = U1 + y\u00b2U2 + y\u00b3U3 + y\u2074U4 + y6vt+6\u2022\nNext, we describe the training target for both the policy and option network. The search policy distribution \u03c0\u03c4 is calculated in the same manner as in MuZero. For the option network, given an option length L at state st, we examine its subsequent states to derive the training target, t = {$t, $t+1, ..., $t+L\u22121}. Each i is a one-hot vector corresponding to the training target for wi. Specifically, for any state s, if the option network predicts an option o = {a1, a2, ..., ar} that exactly matches the composite action O = {a1, a2,..., ar} executed in the environment, then the option learns the action sequence, i.e., $i = onehot(ai+1) for 0 \u2264 i \u2264 l \u2212 1. Conversely, if o \u2260 0, then the option learns to stop, i.e., $i = onehot(stop). We iterate this process to set each & from st to St+L\u22121. If Ot+i is set to learn stop, subsequent $t+j should follow, i.e., $t+j = onehot(stop) for i < j < L \u2013 1. Note that if the length of predicted option or is zero, or is defined as {ai+1}, where ai+1 = arg maxa pi(a) is determined according to the policy network. This method ensures that the option network eventually learns the cumulative probability of the dominant option, as described in subsection 4.1. Figure 3 shows an example of setting the training target for the option network. If Ot+2 \u2260 02, then the option network learns {at+1, at+2, stop}, {stop, stop, stop}, and {at+4, at+5, at+6}, for St, St+2, and st+3, respectively."}, {"title": "EXPERIMENT", "content": "5.1 OPTIONZERO IN GRIDWORLD\nWe first train OptionZero in GridWorld, a toy environment where the objective is to navigate an agent through a grid map with walls from a start position (S) to a goal (G) via the shortest possible route. The maximum option length is set to nine. Other training details are provided in Appendix A. Figure 4 shows the options learned by the option network at four stages of training: 25%, 50%, 75%, and 100% completion. It can be observed that the learning behavior of OptionZero evolves distinctly across different stages. In the early stage (25%), the model mainly relies on primitive actions, identifying options only when approaching the goal. In the middle stages (50% and 75%), the model begins to establish longer options, progressively learning options with lengths from two up to nine. In the final stage (100%), the model has learned the optimal shortest path using options. Notably, using only primitive actions, the optimal path requires an agent to take at least 30 actions. In contrast, OptionZero achieves this with just four options, accelerating the training process by approximately 7.5 times in this example. This substantial reduction highlights OptionZero's efficacy, especially in more complex environments. This experiment also shows that the option network can progressively learn and refine options during training, without requiring predefined options.\n5.2 OPTIONZERO IN ATARI GAMES\nNext, we evaluate OptionZero on Atari games, which are commonly used for investigating options (Sharma et al., 2016; de Waard et al., 2016; Durugkar et al., 2016; Bacon et al., 2017; Vezhnevets et al., 2016; Kim et al., 2023; Lakshminarayanan et al., 2017; Riemer et al., 2020) due to their visually complex environments and subtle frame differences between states, making training with primitive actions inefficient. We train three OptionZero models, denoted as l1, 13, and 16, each configured with maximum option lengths L = 1,3, and 6, respectively. Detailed experiment setups are provided in Appendix B. The model l1 serves as a baseline, identical to MuZero, where no options are used during training. In addition, we adopt the frameskip technique (Mnih et al., 2015), commonly used in training on Atari games, set to 4. Namely, this results in a frame difference between 24 states when executing an option of length 6. This requires OptionZero to strategically utilize options when necessary, rather than indiscriminately."}, {"title": "5.3 OPTION UTILIZATION AND BEHAVIOR ANALYSIS", "content": "This subsection analyzes how options are applied to better understand the planning process of Op- tionZero. Table 2 presents the average percentages of primitive actions (% a) and options (% o), and the distribution of different option lengths (% l) across 26 games. In addition, columns \u201cl\u201d, \u201c% Rpt."}, {"title": "5.4 \u039f\u03a1\u03a4\u0399ON UTILIZATION IN THE SEARCH", "content": "We further investigate the options used during planning. Table 3 lists the results for l1, 13, and 16, including the proportions of search trees that consist of at least one option edge is expanded in MCTS (\"% in Tree\"), the proportions of simulations that at least one option has been selected during search (\"% in Sim.\"), the average tree depth, the median tree depth, and the maximum tree depth. Detailed statistics for each game are provided in Appendix D.3. The results show that approximately 90% of search trees expand options, but only around 30% of search trees choose options during selection. Considering the nature of exploration in MCTS, it is reasonable that not all simulations"}, {"title": "DISCUSSION", "content": "This paper presents OptionZero, a method that integrates options into the well-known MuZero al- gorithm. OptionZero autonomously discovers options through self-play games and efficiently sim- ulates environment transitions across multiple states with options during planning, which not only eliminates the requirement for obtaining options in advance but also reduces the overhead for ex- amining consecutive states during planning. The empirical results on Atari games demonstrate a significant improvement of 131.58% in mean human-normalized scores, and the behavior analysis reveals that OptionZero effectively discovers options tailored to the specific challenges of each en- vironment. In conclusion, our findings suggest that OptionZero not only discovers options without human knowledge but also maintains efficiency during planning. This makes OptionZero easily applicable to other applications, further extending the versatility of the MuZero algorithm.\nAs OptionZero builds upon MuZero, it can be easily applied to various environments. For example, when applied to two-player games, OptionZero is expected to discover optimal strategies for both players at specific states. In strategic games such as StarCraft, our approach can learn skillfully combined options, enhancing further explainability and facilitating human learning, as illustrated in subsection 5.3. OptionZero can also be integrated with Sampled MuZero (Hubert et al., 2021) to support environments with complex action spaces, like robotic environments. Nevertheless, our experiments show that OptionZero does not improve performance in all games, especially in en- vironments with numerous option types or visually complex observations, the dynamics network might struggle to learn well. Future work could explore integrating OptionZero with other dynam- ics models, such as S4 (Gu et al., 2021) or Dreamer (Hafner et al., 2020). Finally, the current design of the option networks requires a predefined maximum option length. Dynamically extending this maximum option length could be a promising direction for future work. We hope our approach and findings provide promising directions in planning with reinforcement learning for future researchers."}, {"title": "A IMPLEMENTATION DETAILS", "content": "In this section, we detail our OptionZero implementation, which is built upon a publicly available MuZero framework (Wu et al., 2024).\nA.1 MCTS DETAILS\nThe MCTS implementation mainly follows that introduced in Section 4.2, with minor details de- scribed below.\nDirichlet noise To encourage exploration, in MuZero, Dirichlet noise is applied to the root node. Similarly, in OptionZero, since option can also be executed in the environment, we apply Dirichlet noise to both primitive selection and option selection at the root node.\nDefault estimated Q value For primitive selection, we follow the default estimated Q value for Atari games in the framework (Wu et al., 2024) that enhances exploration:\nFor option selection, since the contributions of option child node are included in the statistics of its corresponding predecessor primitive child node, we use a default estimated Q value that incorporates a virtually losing outcome:\nwhere N(s, a) is the visit counts of the primitive child node, and Q(s, a) is the mean value of the primitive child node.\nA.2 MCTS COMPLEXITY\nThe complexity of the modified MCTS remains the same as the original, with additional minor computational costs in introducing a new network head to predict and use the dominant option. Specifically, in the selection phase, the only added step is comparing the PUCT scores of option child nodes and primitive child nodes, as in equation 4. In the expansion phase, the option policy \u03a9 is evaluated along with policy p and value v. Since most network weights of the option policy head are shared with the rest of the network, the impact on runtime is negligible. While more nodes are initially expanded, each simulation evaluates only one node at a time. In the backup phase, as the statistics of option edges can be easily derived from primitive edges, only the statistics of primitive edges are maintained in practice, eliminating the additional cost of updating all possible option edges.\nA.3 GRIDWORLD ENVIRONMENT\nThe implementation is also built upon the same framework (Wu et al., 2024), with a custom Grid- World environment added. The reward of the environment is defined as follows: the initial total reward is 200 points, and for each action or option taken, one point is deducted from the reward.\nA.4 NETWORK ARCHITECTURE\nThe network architecture follows a structure similar to MuZero. As discussed in Section 4, the option network is incorporated into the prediction network. Specifically, besides the policy head, we add additional L \u2013 1 option heads for predicting \u03a9 = {w2, \u03c93, ..., WL }, initialized to predict the stop. Note that there is no need for extra prediction of w\u2081, since we can directly get the first action of the dominant option from policy head by choosing a\u2081 = arg max, p(a). Additionally, the dynamics network is modified to simulate the environment transitions of executing both primitive actions and"}, {"title": "B TRAINING OPTIONZERO", "content": "In this section, we describe the details for training OptionZero models used in the experiments. The experiments are conducted on machines with 24 CPU cores and four NVIDIA GTX 1080 Ti GPUs. For the training configurations, we generally follow those in MuZero, where the hyperparameters are listed in Table 4.\nAtari games For Atari games, each setting is trained for 3 runs on each game, with each model taking approximately 22 hours to complete. Since we introduce an additional head to predict option, the training time slightly increases as the max option length increases. For l\u2081, the training time is approximately 21.89 hours. For 13 and 16, the training times increase to around 22.28 hours and 22.95 hours, representing increases of 1.8% and 4.8%, respectively. The performance is measured based on the average score of the latest 100 completed games in each run during the training (Hessel et al., 2021). The training curves are shown in Figure 6.\nGridWorld In this toy example, we aim to clearly show that the length of the learned options can be extended as the training time increases. For training, we use a maximum option length L = 9, fixing the goal position and selecting random starting points. As for the evaluation, we fix the starting point and the goal as shown in Figure 4. The evaluation also uses 50 MCTS simulations, and the original softmax function is replaced with max selection."}, {"title": "C ABLATION STUDY FOR OPTIONZERO", "content": "For the ablation study, we train OptionZero with a maximum option length L = 3, but disable the execution of options in the environment, using them solely for MCTS planning, denoted as n- 13. Since all composite actions are primitive actions, we define each or as {ai+1}, where ai+1 = arg maxa pi (a) and train the option network according to the policy network. According to the results shown in Table 5, n-l3 achieves a mean human-normalized score of 1008.15%, which is 85.43% higher than the baseline l\u2081, indicating that OptionZero still enhances MCTS planning with options without executing them. Notably, in games that require precise step-by-step predictions, such as gopher, n-l3 outperforms l3, indicating that planning for every step remains crucial for certain games. However, in games that benefit from bypassing unimportant frames, such as seaquest, the performance of n-l3 is only comparable to baseline."}, {"title": "D IN-DEPTH BEHAVIOR ANALYSIS", "content": "In this experiment section, we conduct detailed analysis for 13 and 16 in 26 Atari games.\nD.1 OPTIONS APPLIED IN GAMES\nWe present the statistics in all 26 Atari games conducted for the behavior analysis in Section 5.3. Specifically, we provide the numbers of options types, option usages, proportions of options with repeated actions, and the average option lengths for 13 and 16 in Table 6 and Table 7, respectively. The columns \"# {a}\" and \"# {o}\" show the numbers of available primitive actions and the numbers of the options recorded during evaluation, columns \"% a\" and \"% o\" show the proportions of actions and options applied during the game, columns \"% Rpt.\" and \"% NRpt.\" show the proportions of options with repeated primitive actions and options with more than one action type, and column \"l\" shows the average options length (including primitive action). We also provide the proportions of options with different lengths for both l3 and 16 in Table 8.\nD.2 LEARNING PROCESS OF OPTIONS\nIn this experiment section, we show an example to illustrate how longer options are discovered during the training process, using an example of options involving only R in ms pacman, shown in Figure 7. In the 1st iteration, there are only primitive actions, where R is the most frequently used (only 9 primitive actions). Therefore, the agent begins using R with increased frequency, and the usage even exceeds 80% in the 3rd iteration. Meanwhile, due to the high usage, the model starts learning options involving more R. The options R-R and R-R-R are therefore becoming the majority in the 4th and the 6th iteration, respectively. Eventually, the agent explores other actions, thus making the R-related options suddenly decrease after the 7th iteration. Note that in different training trials, as the agent initially explores randomly, the first option learned does not consistently involve R. Depending on how the agent explores the primitive actions, options involving various combinations of actions are discovered at different stages of the training process.\nD.3 OPTIONS IN THE SEARCH TREE\nIn this experiment section, we present the detailed statistics in all 26 Atari games conducted for the behavior analysis in Section 5.4. Specifically, we provide the detailed proportions of options inside the search in Table 9, and the detailed tree depths in Table 10. In Table 9, the columns \u201c% Env.\u201d and \"% MCTS\" represent the proportions of options applied to the environments and the options suggested by the MCTS process. Note that due to softmax selection, the options suggested by the tree, \"% MCTS\", are not always applied, resulting in a lower \u201c% Env.\u201d. Nevertheless, if the sug- gested options have higher visit counts, they are more likely to be applied, such as in crazy climber, freeway, and kung fu master, where options are well-trained for specific purposes, like climbing, crossing the road, and attacking with an uppercut, respectively, and therefore have higher probabili- ties of application. Also, the columns \"% in Tree\" and \"% in Sim.\" represent the proportions of the search tree that contains at least one option (in its 50 simulations) and the proportions of simulations whose selection path contains an option. In Table 10, the columns \u201cAvg\u201d and \u201cMax\u201d represent the average and the maximum search tree depths; the columns \"P25\", \"P50\", and \"P75\" represent the 25th, 50th, and 75th percentiles. Based on the P50 and P75 depths, we observe that the search is not deep in most cases, implying that the models learn to perform deep searches only in certain states."}], "equations": ["Q(s^{k}, a_{k+1}) + P(s^{k}, a_{k+1}) \\times \\frac{\\sqrt{\\sum_{b} N(s^{k}, b)}}{1+ N(s^{k}, a_{k+1})} \\times Cpuct,", "Q(s^{k}, a_{k+1}) := \\frac{N(s^{k}, a_{k+1}) \\times Q(s^{k}, a_{k+1}) + G^{k+1}}{N(s^{k}, a_{k+1}) +1} \\text{ and } N(s^{k}, a_{k+1}) := N(s^{k}, a_{k+1}) + 1,", "G^{k+1} = \\sum_{\\tau=0}^{l-k-1} \\gamma^{\\tau} r_{k+1+\\tau} + \\gamma^{l-k}v^{l}", "L_{t} = \\sum_{k=0}^{K} l^{p} (\\pi_{t+k}, p_{t+k}) + l^{v} (Z_{t+k},v_{t+k}) + \\sum_{k=1}^{K} l^{r} (U_{t+k}, r_{k}) + c||\\theta||^{2},", "\\begin{cases}Q(s^{k}, o_{k+1}) + P(s^{k}, o_{k+1}) \\times \\frac{\\sqrt{\\sum_{b} N(s^{k},b)}}{1+ N(s^{k}, o_{k+1})} \\times Cpuct \\quad \\text{if option node,}\\\\ Q(s^{k}, a_{k+1}) + P(s^{k}, a_{k+1}) \\times \\frac{\\sqrt{\\sum_{b} N(s^{k},b)}}{(N(s^{k}, a_{k+1})-N(s^{k}, o_{k+1}))+1} \\times Cpuct \\quad \\text{if primitive node.}\n\\end{cases}", "P(s^{k}, a_{k+1}) = max(0, P(s^{k}, a_{k+1}) - P(s^{k}, o_{k+1}))", "\\frac{N(s^{l},a^{l+1})Q(s^{l},a^{l+1})-N(s^{l}, o^{l+1})Q(s^{l}, o^{l+1})}{N(s^{l},a^{l+1})-N(s^{l},o^{l+1})}", "Q(s^{k}, A_{k+1}) := \\frac{N(s^{k}, A_{k+1}) \\times Q(s^{k}, A_{k+1}) + G^{k+1}}{N(s^{k}, A_{k+1}) +1}", "N(s^{k}, A_{k+1}) := N(s^{k}, A_{k+1}) + 1,", "L_{t} = \\sum_{k=0}^{K} l^{p} (\\pi_{t+k}, p_{t+k}) + \\sum_{k=0}^{K} l^{o} (\\phi_{t+k}, \\Omega_{t+k}) + \\sum_{k=1}^{K} l^{r} (U_{t+k}, r_{k}) + c||\\theta||^{2},", "Q_{\\Sigma}(s) = \\begin{cases}\n        \\frac{\\sum_{b | N(s,b)>0}Q(s, b)}{\\sum_{b | N(s,b)>0}}\n        & \\text{if } N_{\\Sigma}(s) > 0 \\\\\n        1\n        & \\text{if } N_{\\Sigma}(s) = 0,\n        \\end{cases}", "\\hat{Q}(s) = \\frac{Q(s, a) \\times N(s,a)}{N(s, a) + 1},"]}