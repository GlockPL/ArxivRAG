{"title": "Is Linear Feedback on Smoothed Dynamics Sufficient for Stabilizing Contact-Rich Plans?", "authors": ["Yuki Shirai", "Tong Zhao", "H.J. Terry Suh", "Huaijiang Zhu", "Xinpei Ni", "Jiuguang Wang", "Max Simchowitz", "Tao Pang"], "abstract": "Designing planners and controllers for contact-rich manipulation is extremely challenging as contact violates the smoothness conditions that many gradient-based controller synthesis tools assume. Contact smoothing approximates a non-smooth system with a smooth one, allowing one to use these synthesis tools more effectively. However, applying classical control synthesis methods to smoothed contact dynamics remains relatively under-explored. This paper analyzes the efficacy of linear controller synthesis using differential simulators based on contact smoothing. We introduce natural baselines for leveraging contact smoothing to compute (a) open-loop plans robust to uncertain conditions and/or dynamics, and (b) feedback gains to stabilize around open-loop plans. Using robotic bimanual whole-body manipulation as a testbed, we perform extensive empirical experiments on over 300 trajectories and analyze why LQR seems insufficient for stabilizing contact-rich plans.", "sections": [{"title": "I. INTRODUCTION", "content": "Dexterous manipulation is full of contact-rich interactions, enabling various tasks through complex frictional interactions [1]. Historically, the non-smooth nature of contact has precluded a range of planning and control methods that rely on gradients of the dynamics. Recent advances have utilized contact smoothing where non-smooth dynamics are replaced by a continuously differentiable proxy to great effect as surrogate dynamics models for planning through contact [2], [3], [4], [5]. One may hope, then, that smoothing enables the use gradient-based control.\nThis work suggests that the above hope may face significant obstacles. We (1) introduce LQR control for contact-manipulation via contact smoothing. Furthermore, we (2) present and analyze robust trajectory optimization, hoping that the generated trajectories are robust to the model errors accumulated by using a surrogate dynamics model for control, and thus more amenable to LQR. Then, we (3) extensively evalute the performance of these methods, both in simulation and in hardware, on a bimanual whole-body manipulation as shown in Fig. 1. In short, we find:\nDespite its efficacy in planning through contact, dynamical smoothing alone is unsatisfactory as a means to obtaining linear control policies.\nFinally, we (4) identify the key factors leading to the inadequacies of linear control; namely, the unilaterality of contact, and the tendency of controllers to \u201cpush and pull\" unless the dynamics are only very-slightly smoothed."}, {"title": "II. RELATED WORK", "content": ""}, {"title": "A. Planning through Contact", "content": "One approach to motion-planning through contact enforces contact dynamics by including linear complementarity constraints [6], [2], [7], [3], [8], [9]. Another popular method to address contacts is based on Mixed-Integer Programming (MIP) [10], [11], [12], [13], [14], where discrete variables encode contact modes. However, due to the inherent non-smoothness of contact dynamics, both methods suffer from poor scalability as the number of contact modes increases.\nTo tackle this, contact smoothing has been proposed, replacing exact dynamics models with surrogate models that obey second-order smoothness [2], [4], [15], [5]. This approach introduces a \"force-at-a-distance\" effect where gradients through the dynamics convey information about nearby contacts. Since gradients of the dynamics become continuous, gradient-based optimizers are able to find solutions more efficiently. Our work employs a log-barrier smoothing scheme [4], [5], which is more efficient compared to stochastic smoothing schemes [15], [16]."}, {"title": "B. Control through Contact", "content": "Prior approaches to control through contact reason about contact modes [17], [18], [19], [20]. While these approaches can run in real time for simple systems with only a handful of contact modes, they have yet to scale to high-dimensional systems, multiple objects per scene, or contacts with objects of complex and/or irregular shapes due to the expensive computation.\nIn contrast, contact-smoothing offers a way to directly apply smooth control methods while being less constrained by the non-smoothness of contact dynamics, at the cost of introducing some smoothing bias [15]. In this work, we study LQR, which uses the underlying problem structure provided by contact smoothing, enabling its application to high-dimensional bimanual whole-body manipulation tasks."}, {"title": "C. Robust Planning through Contact", "content": "Previous work on robust planning through contact generally falls into two approaches: domain randomization, which involves stochastic optimization over a fixed distribution of dynamics models [20], [21], and worst-case optimization, which focuses on performance under worst-case scenarios within an uncertainty set [22], [23], [24], [25]. These methods effectively handle parametric uncertainties in the system dynamics, such as mass and friction, but do not discuss shape uncertainty (e.g., the radius of a cylinder). Shape uncertainty is critical for generalized manipulation to avoid unexpected contact events, yet integrating it into system dynamics remains challenging.\nIn this work, we incorporate domain randomization with an emphasis on shape uncertainty. Using contact smoothing, our method incorporates smoothed collision dynamics, enabling robust optimization to consider shape uncertainty. We also hope that using our technique enables LQR to track reference trajectories more easily."}, {"title": "III. PROBLEM STATEMENT", "content": "We focus on the manipulation of rigid objects. Throughout, we focus on bimanual tabletop manipulation, though in principle our approach extends beyond this regime."}, {"title": "A. Quasi-Dynamic Dynamics Model", "content": "We generate plans and linear feedback gains by considering a quasi-dynamic model of a robot manipulating a single rigid object [26]. We consider robots with $n_a$ actuated Degrees of Freedom (DoFs) and the objects with $n_u$ unactuated DoFs. We denote the configurations of the object and the robot as $q_u \\in \\mathbb{R}^{n_u}$ and $q_a \\in \\mathbb{R}^{n_a}$, respectively. In quasi-dynamics models, we assume velocities are small, and thus system states are the concatenation of both configurations $x := q := [q_u^T, q_a^T]^T \\in \\mathbb{R}^{n_x}$, where $n_x = n_u + n_a$.  We denote the change in system configuration from the current time step to the next time step as $dq := [dq_u, dq_a]^T$. We denote the control input as $u \\in \\mathbb{R}^{n_a}$, defined as the commanded positions of the robot's joints. We consider the linear feedback law $u = v + K\\Delta x$ where $v \\in \\mathbb{R}^{n_a}$ is the feedforward gain from trajectory optimization, $K \\in \\mathbb{R}^{n_a \\times n_x}$ is the feedback gain computed by LQR, and $\\Delta x$ is the state error, or deviation from nomial trajectory."}, {"title": "B. Quasi-Dynamic Dynamics Model with Contact Smoothing", "content": "For computing $dq$, we consider a log-barrier-smoothed formulation of quasi-dynamic dynamics from Eq. (34) in [5]. We denote $\\kappa$-smoothed forward dynamic map as\n$q_{t+1} = f_\\kappa(q_t, u_t) \\tag{1}$\nwhere $\\kappa > 0$ is the user-defined barrier parameter. Smaller $\\kappa$ corresponds to greater contact smoothing, and more \"force at a distance\" [5]. Recalling our notation $x = q$, the above can also be written as $x_{t+1} = f_\\kappa(x_t, u_t)$. When $\\kappa$ does not change, we will simply write $f$ for $f_\\kappa$.\nBoth our gradient descent-based trajectory optimizer and our synthesis of linear feedback rely on differentiation of the smoothed dynamics. Given (1), we define derivatives:\n$A_\\kappa = \\frac{\\partial f_\\kappa}{\\partial q} \\qquad B = \\frac{\\partial f_\\kappa}{\\partial u} \\tag{2}$\nSee [5] for the complete derivation of (2). Again, we use the shorthand $A = A_\\kappa$ and $B = B_\\kappa$."}, {"title": "C. Parametrized Quasi-Dynamic Dynamics Model", "content": "As described in Sec II-C, we consider uncertainty over the dynamics induced by object uncertainty. These can be parameterized by a parameter $p \\in U \\subseteq \\mathbb{R}^{N_p}$ which enters into $f_\\kappa$ in (1). To lighten notation, we also let the parameter $p$ encode an initial condition, $X_{init}(p)$."}, {"title": "IV. CONTACT-IMPLICIT TRAJECTORY OPTIMIZATION", "content": "In this section, we present both single-parameter and multi-parameter trajectory optimization baselines."}, {"title": "A. Single-Parameter Trajectory Optimization (SP-TrajOPT)", "content": "We formulate our optimal planning problem as follows.\n$\\begin{aligned} \\min_{\\{x_t\\}_{t=1}^T, \\{v_t\\}_{t=0}^{T-1}} & J (\\{x_t\\}_{t=1}^T, \\{v_t\\}_{t=0}^{T-1}; p) \\tag{3a} \\\\ \\text{s.t.} & x_{t+1} = f(x_t, v_t; p), \\forall t \\in T \\tag{3b} \\\\ & g(x_t, v_t; p) \\leq 0, \\forall t \\in T \\tag{3c} \\\\ & x_t \\in X, v_t \\in V, \\forall t \\in T \\tag{3d} \\\\ & x_0 = X_{init}(p), \\tag{3e} \\end{aligned}$\nwhere $T := \\{0,...,T \u2013 1\\}$, $J$ is the trajectory-wise cost function, $f = f_\\kappa$ is the contact dynamics of the system (see (1)). $v_t$ is the control input at time step $t$. (3c) is used to impose inequality constraints involving $x_t$ and $v_t$ such as joint torque constraints and non-collision constraints. $X$ and $V$ represent a convex polytope, consisting of a finite number of linear inequality constraints for bounding the decision variables. In (3e), the initial condition encoded by $p^i$ determines the initial state $x_0$ (see Section III-C)."}, {"title": "B. Multi-Parameter Trajectory Optimization (MP-TrajOPT)", "content": "An alternative to (3) is a robust formulation over multiple parameters. We focus on the simplest approach: optimizing average performance on $N$ realizations $(p^i)_{1\\leq i\\leq N}$, which for simplicity are manually chosen, inspired by [27], [20], using Sample Average Approximation (SAA) [28]. For each $p^i$, we optimize over a corresponding trajectories $(x^i)_{1\\leq i\\leq N}$.\n$\\begin{aligned} \\min_{\\{x^i_t\\}_{i=1}^N, \\{v_t\\}_{t=0}^{T-1}} & \\frac{1}{N} \\sum_{i=1}^N J (\\{x^i_t\\}_{t=1}^T, \\{v_t\\}_{t=0}^{T-1}; p^i) \\tag{4a} \\\\ \\text{s.t.} & x^i_{t+1} = f(x^i_t, v_t; p^i), \\forall t \\in T, \\forall i \\in I \\tag{4b} \\\\ & g(x^i_t, v_t; p^i) \\leq 0, \\forall t \\in T, \\forall i \\in I \\tag{4c} \\\\ & x^i_t \\in X, v_t \\in V, \\forall t \\in T, \\forall i \\in I \\tag{4d} \\\\ & x^i_0 = X_{init}(p^i), \\forall i \\in I \\tag{4e} \\end{aligned}$\nwhere $X := \\{x^i, \\forall t \\in T, \\forall i \\in I\\}$, $I := \\{0, ..., N \u2013 1\\}$. We emphasize that we do not have superscript $i$ on $v_t$ because our objective is to design a single plan that succeeds on average over the $N$ parameters. Note that (4) with $N = 1$ specializes to (3). We call this SP-TrajOPT (Single-Parameter Trajectory Optimization) as it only considers a single parameter. For $N > 1$, we refer to the procedure as MP-TrajOPT (Multi-Parameter Trajectory Optimization)."}, {"title": "V. FEEDBACK SYNTHESIS VIA LQR", "content": "In this section, we present an approach to feedback gain synthesis by solving an LQR problem through linearizations of the contact smoothed dynamics.\nUsing (2), we can compute LQR feedback gains $K$. The objective of using LQR is to design a controller that can locally stabilize the system. In this work, we consider the following optimal control problem given $x_t$ and $v_t$, $t \\in T$.\n$\\begin{aligned} \\min_{\\{\\Delta v_t\\}_{t=0}^{T-1}} & \\|\\Delta x_T\\|_{Q_T}^2 + \\sum_{t=0}^{T-1} (\\|\\Delta x_t\\|_Q^2 + \\|\\Delta v_t\\|_R^2) \\tag{5a} \\\\ \\text{s.t.} & \\Delta x_{t+1} = A_t\\Delta x_t + B_t\\Delta v_t, t \\in T \\tag{5b} \\\\ & \\Delta x_0 = x - x_0 \\tag{5c} \\end{aligned}$\nwhere $Q_t = Q$ is positive semidefinite and $R_t = R$ is positive definite at $t$. $x$ is the measurement of states at $t = 0$. $A_t$ and $B_t$ are error dynamics, which are obtained by linearizing true nonlinear contact dynamics of the system $f$ around $x_t$ and $v_t, t \\in T$ in accordance with Sec.III-B. It is worth noting again that $A_t$ and $B_t$ convey local contact information of the system dynamics. We use Riccati recursion [29] to compute $K$ for (5). All of the computation happens offline. Hence, there is no expensive computation online unlike other methods (e.g., model predictive control)."}, {"title": "VI. RESULTS", "content": "In this section, we demonstrate our proposed controller under various uncertainties such as perturbations of initial conditions and shape variations for a cylinder. Through this section, we answer the following questions.\n1) How well do TrajOPT and LQR work?\n2) To what extent does MP-TrajOPT improve the performance of LQR?\n3) Under what circumstances do our proposed controllers succeed or fail?"}, {"title": "A. Experimental Setup", "content": "Three sets of parameters are in Table I. For SP-TrajOPT, we use the nominal \u201c(a)\u201d shape in that table; for MP-TrajOPT we use all $N = 3$ \u201c(a)\u201d, \u201c(b)\u201d and \u201c(c)\" shapes.\n1) Data Generation Pipeline: Our pipeline is depicted in Fig. 2. We begin by sampling $N_{test} = 340$ reference trajectories using RRT in [5] with $N_{test}$ different initial and goal states. RRT uses the nominal \u201c(a)\u201d parameter in Table I for cylinder. These yield state and feedforward control input sequences, $(x_t, v_t)_{1<t T}^{n} $as illustrated in Fig. 3. For each $n = 1, ..., N_{test}$, we use $(x_t, v_t)_{1 \\le t \\le T}$ as a warm-start for computing trajectory-optimized plans $(x^{opt}_t, v^{opt}_t)_{1 \\le t \\le T}$.  The horizons $T$ are determined by the RRT warm-start and vary across reference trajectories, but always lie in the range $T \\in [4,64]$ with a mean of $T = 26$. For SP-TrajOPT, we solve (3) under the nominal \u201c(a)\u201d parameter. For MP-TrajOPT, we solve (4) for all $N = 3$ parameters (note that this produces $N$ sequences of states; we select $x^n_t$ to be the one under the nominal shape). Both use the following cost function:\n$\\sum_{t=1}^{T} ||x_t - x^*_t||^2_Q + \\sum_{t=0}^{T-1} ||v_t||^2_R \\tag{6}$"}, {"title": "B. Feedback Synthesis", "content": "For LQR, we compute gains $(K_t^{rrt})_{t=0}^{T-1}$ from $\\{(x_t^{rrt}, v_t^{rrt})_{t=0}^{T-1}\\}_{i=1}^{N_{test}}$. It means that, when the trajectories are supplied by SP- or MP-TrajOPT, gains are computed around the nominal parameter dynamics, (a) parameter in Table I. We use $h = 0.1$ seconds as the time step, but use a smaller smoothing parameter, $\\kappa = 800$, for smoother derivatives. We tune and set $Q = diag(10, 10, 10, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1)$, $R = diag(5, 5, 5, 5, 5, 5)$, $\\forall t \\in [0,...,T-1]$, $Q_T = diag(1000, 1000, 1000, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1)$ to achieve good performance across $N_{test}$ reference trajectories.\n1) Execution in DRAKE: While plans and gains are synthesized via smoothed quasi-dynamic dynamics, we evaluate performance in either (i) DRAKE or (ii) on real hardware. The planning/feedback synthesis phase returns a sequence of states, feedforward control inputs, and feedback gains at discrete knot points $(\\{x_t\\}, \\{v_t\\}, \\{K_t\\})_{t=0}^{T-1}$.\nWe adopt a higher control loop frequency in DRAKE simulations, which necessitates interpolation of these discrete-time quantities. To do so, we convert the knot points into continuous time using First-Order Hold (FOH): states $x^{FOH}(t)$, feedforward control input $v^{FOH}(t)$, and feedback control input trajectories $K^{FOH}(t)$. For $K^{FOH}(t)$, we interpolate elements of $K_t$ using FOH. At each time step in a DRAKE simulation, we measure the state $x^{mea}(t)$ and rollout out the controller $u(t) = v^{FOH}(t) + K^{FOH}(t) (x^{mea}(t) \u2212 x^{FOH}(t))$.\n2) Evaluation Metrics: To evaluate the performance of controllers, we define the following metrics:\n$\\begin{aligned} & S_{terminal} := d(x^{u,reference}_T, x^{u,mea}_T) \\tag{7a} \\\\ & \\Delta_{terminal} := \\frac{1}{N_{test}} \\sum_{i=1}^{N_{test}} d^i_{terminal} \\tag{7b} \\\\ & \\eta_{A}^{u,reference} := \\frac{\\Delta_{terminal}^{A}}{\\Delta_{terminal}^{B}} \\tag{7c} \\end{aligned}$\nwhere $d_{terminal}$ is the tracking error at the terminal time step. The superscript $u$ extracts the elements corresponding to $q_u$ and $x^{u,reference}$ is the reference trajectory the controller tries to track. Through experiments, we consider $x^{u,reference} = x^{mea}$ $d(\u00b7, \u00b7)$ computes the Cartesian and angular displacements between two object poses. Given $N_{test}$ reference trajectories computed from RRT, $d^i_{terminal}$ shows that it is the tracking error of the i-th result. $\\Delta_{terminal}$ represents the average terminal tracking error over $N_{test}$ demonstrations. Given two $\\Delta_{terminal}^{A}$ and $\\Delta_{terminal}^{B}$ obtained from controller A and B respectively, relative cost, $\\eta_{A}^{u,reference}$ represents how much the tracking errors are different between controller A and B."}, {"title": "C. Robustness Tests", "content": "To evaluate the robustness of controllers, we consider the perturbations to initial states and the radius of the cylinder.\n1) Robustness to Initial Condition: After we obtain $x^{FOH}(t), v^{FOH}(t), K^{FOH}(t)$ (see Section VI-B.1), we add perturbations $(\\delta x, \\delta y, \\delta \\theta)$ to the initial states of cylinder in DRAKE (i.e., $x_u^{mea}(0) \\leftarrow x_u^{mea}(0) + [\\delta x, \\delta y, \\delta \\theta]^T$) and then we start rolling out the controller in DRAKE. We consider 50 points $(\\delta x, \\delta y, \\delta \\theta) \\sim \\text{Uniform} ([-{\\delta x}_0, +{\\delta x}_0] \\times [-{\\delta y}_0, +{\\delta y}_0] \\times [-{\\delta \\theta}_0, +{\\delta \\theta}_0])$ per reference trajectory where ${\\delta x}_0 = {\\delta y}_0 = 0.025 m, {\\delta \\theta}_0 = 5^\\circ$.\n2) Robustness to Variation in Shape: After we obtain $x^{FOH}(t), v^{FOH}(t), K^{FOH}(t)$, we add perturbations $\\delta r$ to the radius of cylinder, $r$, in DRAKE (i.e., $r \\leftarrow r + \\delta r$) and then we start rolling out the controller in DRAKE with this updated $r$. We consider 20 points $\\delta r_i \\sim \\text{Uniform}([-{\\delta r}_0, +{\\delta r}_0])$ per reference trajectory where ${\\delta r}_0 = 0.01 m."}, {"title": "D. Results of SP-TrajOPT", "content": "Fig. 4 shows the coverage of SP-TrajOPT. Fig. 3 and Fig. 4 shows that SP-TrajOPT could successfully converge given reference trajectories by RRT. The success rate of SP-TrajOPT given 340 reference trajectories is 82.8%. To the best of the authors' knowledge, this is the highest success rate among contact-implicit trajectory optimization frameworks. We think this is because the contact complementarity constraints are implicitly imposed through $f_\\kappa$ in our formulation. We also observe that the average position and orientation errors of the cylinder are reduced, from 0.18 m and 71.9\u00b0 with RRT to 0.14 m and 55.0\u00b0 with SP-TrajOPT, respectively. Hence, our SP-TrajOPT could successfully generate more optimal trajectories."}, {"title": "E. Results of LQR with SP-TrajOPT", "content": "1) Perturbations to Initial Conditions: Fig. 5 shows SP-TrajOPT and LQR across variations of initial conditions for the cylinder. In Fig. 5a, SP-TrajOPT outperforms LQR. In Fig. 5b, both controllers show similar performance while SP-TrajOPT with LQR shows slightly better $\\Delta_{terminal}$."}, {"title": "F. Results of MP-TrajOPT", "content": "Fig. 7 shows the coverage of MP-TrajOPT. Compared to the result of SP-TrajOPT in Fig. 4, we observe that MP-TrajOPT shows much smaller coverage. The success rate of MP-TrajOPT is 10.6%, which is much lower than that of SP-TrajOPT. This is because MP-TrajOPT is much more complex than SP-TrajOPT, and thus, SNOPT might not be able to make any progress during optimization due to many reasons, such as poor scaling of the optimization problem."}, {"title": "G. Results of LQR with MP-TrajOPT", "content": "1) Perturbations to Initial Conditions: The results of perturbations to initial conditions are shown in Fig. 8. While MP-TrajOPT outperforms MP-TrajOPT with LQR in Fig. 8a, the relative cost in Fig. 8a, $\\eta_{MP-TrajOPT}^{SP-TrajOPT} = 3.7$. Since $\\eta$ in Fig. 5a is $\\eta_{SP-TrajOPT}^{NSP-TrajOPT} = 6.5$, we argue that MP-TrajOPT improves the LQR performance. For the orientation tracking error, Fig. 8b shows that MP-TrajOPT with LQR outperforms MP-TrajOPT while in Fig. 5b, SP-TrajOPT outperforms SP-TrajOPT with LQR. Therefore, we observe that MP-TrajOPT introduces some robustness, resulting in improved LQR performance. Also, $\\eta$ in Fig. 8b is $\\eta_{MP-TrajOPT}^{MP-TrajOPT} = 0.58$ while $\\eta$ in Fig. 5b is $\\eta_{SP-TrajOPT}^{NSP-TrajOPT} = 0.82$. Although our MP-TrajOPT is not designed to be robust against variation in initial conditions, the result suggests that the inherent robustness against parametric uncertainty of the model contributes to robustness under variation in initial conditions."}, {"title": "H. Hardware Experiments", "content": "We implement two controllers, open-loop using SP-TrajOPT and LQR controllers, and evaluate their tracking performance in hardware experiments. As shown in Fig. 1, we observe that LQR could track the specific reference trajectory with perturbations to the initial states."}, {"title": "VII. A CLOSER LOOK AT LQR", "content": ""}, {"title": "A. Results of Different Smoothing", "content": "We discuss the relation between the behavior of LQR and the smoothing parameter $\\kappa$. Here we have two results using"}, {"title": "B. Fundamental Shortcomings of Linearization", "content": "We here analyze why LQR does not work well on top of SP-TrajOPT-generated trajectories. One limitation of LQR is that the linearization does not sufficiently capture the unilateral nature of contact. To illustrate this, we consider a 1-step LQR problem in (8),\n$\\begin{aligned} \\min_{\\{\\Delta v_0\\}} & ||x_1|| \\tag{8a} \\\\ \\text{s.t.} & \\Delta x_1 = A_0\\Delta x_0 + B_0 \\Delta v_0, \\tag{8b} \\\\ & \\Delta x_0 = x - x_0. \\tag{8c} \\end{aligned}$\nConsider a simple 1D block-pushing system (Fig. 12), where an actuated block is trying to push an unactuated block into a desired location. When we visualize the linearized dynamics in Fig. 12, we can observe that in cases where the red ball must push to stabilize, this 1-step LQR takes a step in the right direction although overshooting occurs (Fig. 12, top row). If we ask LQR to recover from overshooting (Fig. 12, bottom row), however, a linearized model predicts that it can pull the object and take a step towards the opposite direction. Yet, due to the directional nature of contact, this has no effect on the unactuated ball. This mismatch between the linearized model and the true model leads to limitations of the LQR controller. We observe this pulling motion in Fig. 10 in Section VII-A."}, {"title": "VIII. CONCLUSION", "content": "Is linear feedback on smoothed dynamics sufficient for stabilizing contact-rich plans? Our analysis and experiments suggest that designing LQR for contact-rich plans does not work well in general. However, we observe that MP-TrajOPT enables LQR to improve its performance when MP-TrajOPT is solved, although MP-TrajOPT often fails to converge. Through this paper, we first present how contact smoothing technique can be used for designing trajectory optimization baselines and LQR. Then, we extensively conduct various experiments of LQR under different uncertainties. We hope that our analysis provides readers with insights into design of planners and controllers for contact-rich manipulation using differential simulators based on contact smoothing."}]}