{"title": "WHAT DIFFERENTIATES EDUCATIONAL LITERATURE? A MULTIMODAL FUSION APPROACH OF TRANSFORMERS AND COMPUTATIONAL LINGUISTICS", "authors": ["Jordan J. Bird"], "abstract": "The integration of new literature into the English curriculum remains a challenge since educators often lack scalable tools to rapidly evaluate readability and adapt texts for diverse classroom needs. This study proposes to address this gap through a multimodal approach that combines transformer-based text classification with linguistic feature analysis to align texts with UK Key Stages. Eight state-of-the-art Transformers were fine-tuned on segmented text data, with BERT achieving the highest unimodal F1 score of 0.75. In parallel, 500 deep neural network topologies were searched for the classification of linguistic characteristics, achieving an F1 score of 0.392. The fusion of these modalities shows a significant improvement, with every multimodal approach outperforming all unimodal models. In particular, the ELECTRA Transformer fused with the neural network achieved an F1 score of 0.996. The proposed approach is finally encapsulated in a stakeholder-facing web application, providing non-technical stakeholder access to real-time insights on text complexity, reading difficulty, curriculum alignment, and recommendations for learning age range. The application empowers data-driven decision making and reduces manual workload by integrating AI-based recommendations into lesson planning for English literature.", "sections": [{"title": "Introduction", "content": "The integration of new literature into education remains a significant challenge for educators, who often lack access to robust tools to evaluate and adapt texts for classroom settings. These issues are further exacerbated by the need to respond to trends and integrate popular contemporary works to retain student interest and enhance learning experiences. Currently, there are no scalable solutions that enable educators to respond quickly to trends by autonomously analysing the complexity of the text, aligning the literature with the appropriate educational stages, and generation of actionable insights for use in the education system.\nThis lack of tools thus leaves educators dependent on manual evaluation, which is a resource-intensive process at a time where education systems face ongoing issues of growing class sizes, budget cuts, and work-related stress leading to poor retention. In addition to these problems, decentralised manual evaluation can also lead to inconsistencies in capturing the nuanced demands of a diverse classroom.\nPopular books are wide-ranging in their complexities, thematic depths, and linguistic sophistication, which can make it difficult to determine their suitability across different educational stages. For example, Harper Lee's To Kill a Mockingbird, a text commonly found in the classroom, presents distinct challenges in aligning with specific educational stages. The book is written with relative accessibility and narrative style, making it appropriate for students in Key Stage 3 to develop their comprehension skills. In addition, the work explores themes such as growing up, the loss of innocence, and moral development, which are themes that resonate with the early stages of secondary education. In upper Key Stage 3, more complex discussions on systemic racism, class structure, and justice require a higher level of"}, {"title": "Related Work", "content": "The use of AI within educational processes has been observed to promote the personalisation of teaching materials, improve lesson planning procedures, promote efficiency, and create novel experiences to inspire students [1, 2, 3]. From a learner's perspective, exciting new methods of learning can be experienced and social inequalities can be alleviated through personalisation of the learning experience. For educators, technological assistance can alleviate workload demands, helping to maintain teaching quality while positively impacting both physical and mental well-being.\nReadability assessment is a particularly difficult task that forms an important open issue in the field. Zamanian and Heydari [4] provide a background of readability formulae and their reliance on features such as sentence length, word length, and frequencies. While these metrics held potential in early research, they are increasingly critically analysed and often fail to account for deeper semantic and domain-level features within a text. The authors note that scores from Flesch and Dale-Chall, for example, can provide estimates of reading difficulty, they cannot measure more intelligent concepts such as audience engagement. Adding to this discourse, the issues were outlined in depth by a letter to the editor from Alzaid, Ali, and Stapleton[5], who critically analyse traditional readability metrics and note that they focus predominantly on quantifiable properties, for example, readability scores, presence of part-of speech, diversity metrics, richness metrics, etc. which do not encapsulate qualitative features such as domain-level features. There also exist inconsistencies between formulae such as Flesch-Kincaid and SMOG, which are two of several features used in the unimodal linguistics model approach in this study prior to multimodal studies.\nIn [6], Sung et al. note the difficulty in autonomously recognising readability, with traditional methods often resulting in low classification accuracy. In the study, the authors proposed the use of linguistic features of four categories (word, semantics, syntax, and cohesion) for the analysis of Chinese text. The results showed that multilevel Support Vector Machine models could achieve 71.75% classification accuracy. Similarly, [7] also note that traditional linguistic features often do not allow machine learning models to generalise. The authors propose a deep learning-driven Ranked Sentence Readability Score, which is noted to correlate with human-assigned readability scores. In particular, BERT-based models are noted to outperform temporal and hierarchy-based models across English and Slovenian texts. The authors note the need for domain-specific challenges, which is the focus of this work. Lee et al. [8] build on these findings,"}, {"title": "Method", "content": "This section contains an overview of the methodology followed by this work from data collection to unimodal model training, to subsequent multimodal model training and comparison. In addition, this section also describes the technical design of the stakeholder-facing web application."}, {"title": "Data Collection and Preprocessing", "content": "Initial data collection was performed via Project Gutenberg, which is an online platform that distributes books within the public domain, i.e., those that can be used for non-commercial use without permission. Each book that was available in raw text format was downloaded, leading to an initial set of 2009 books for further processing. Following this, the set of books was cross-referenced with the Lexile book finder. If a Lexile score was not available, the book was discarded from the dataset. Following this filter, a total of 384 books with Lexile scores were recovered and then the Lexile score was converted from a numerical value to a nominal class label according to Table 2. Books belonging to Key Stage 1 were not available, and so are not considered in this study. The lowest score was The Monkey's Paw by W.W. Jacobs (Oxford Bookworms) at 420, and the highest was Discourse on the Method of Rightly Conducting the Reason by Ren\u00e9 Descartes at 1840.\nGiven the data requirements of several state-of-the-art transformer models having a maximum input length of 512 tokens, each book was then divided into chunks of a maximum of 512 tokens to the nearest complete sentence. This resulted in a large and unbalanced dataset with a total of 515,688 rows. To alleviate issues of data size and balance, the dataset was then resampled by selecting 5000 rows per class. This resulted in a full dataset size of 20,000 data objects, with 5,000 belonging to each Key Stage 2, 3, 4, and 5. Finally, the dataset was split for training and validation at an 80/20 split, resulting in 4000 rows for training and 1000 rows for testing for each Key Stage. The data produced and utilised within this study is publicly available under the MIT license\u00b9."}, {"title": "Linguistic Feature Generation", "content": "From each of the text excerpts divided to the maximum length by the nearest complete sentence, the numerical linguistic characteristics were extracted within ten categories. The features were selected based on the criteria for producing fixed-length vectors, given that machine learning models require fixed input types for training. The features described in this section were extracted using the TextBlob [12], NRCLex [13], and NLTK [14] Python libraries. The numerical features described in this Section were utilised for training the neural networks described in Section 3.2. These features were calculated as follows:\nBasic Text Metrics which include the number of words, sentences, unique words, and the average length of both sentences and words.\nDetailed Sentence Information, which includes the average number of characters and syllables per word, as well as the per-sentence averages of characters, syllables, words, types of words, paragraphs, long words, complex words, and Dale-Chall complex words.\nLexical Diversity and Richness features, where diversity refers to the variety of unique words within a text, and richness refers to the sophistication of vocabulary within a text. The measures of lexical diversity and richness include the Type Token Ratio $TTR = \\frac{V}{N}$, where V is the number of unique words and N is the total number of words. Higher values of TTR suggest a greater variety in vocabulary.\nYule's K:\n$K = 10^4 \\times \\frac{\\sum_{i=1}^{V} i^2 f_i - N}{N^2},$\nwhere K is a quantification of richness, and $f_i$ is the frequency of the $i^{th}$ word type. Higher values suggest greater diversity in the text.\nSimpson's D:\n$D = \\frac{\\sum_{i=1}^{V} f_i(f_i - 1)}{N(N-1)},$\nwhere D is the probability that two tokens selected at random are the same type, thus D aims to measure the repeated use of words. A lower value of D suggests that there is a higher diversity, since tokens are more likely to differ from one another.\nHerdan's C:\n$C = \\frac{log N}{log V},$\nwhere C is calculated by total words N and unique words V. Lower values denote greater diversity since V is relatively in comparison to N.\nBrun\u00e9t's W:\n$W = N^{(V^{-0.165})},$\nfor total words N and unique words V, with a constant used to prevent distortions when presented with longer text sequences. Lower values of W indicate a higher richness of vocabulary.\nHonor\u00e9's R:\n$R = 100 \\times \\frac{log N}{1 - \\frac{V_1}{V}},$\nR is the relationship between total words N, unique words V, and words that appear only once (hapax legomena) $V_1$. Higher values suggest rich vocabularies, especially when a wide range of infrequent words is used.\nReadability Scores which estimate how difficult a text is to read, often related to the US educational grade levels.\nKincaid Grade Level, which estimates the US educational grade level required to comprehend a given text:\n$Kincaid = 0.39 \\bigg(\\frac{Total \\: Words}{Total \\: Sentences}\\bigg) + 11.8 \\bigg(\\frac{Total \\: Syllables}{Total \\: Words}\\bigg) - 15.59.$\nThe Automated Readability Index (ARI). Similarly to the Kincaid level, ARI estimates the US grade level required to understand a text in relation to the number of characters:\n$ARI = 4.71 \\bigg(\\frac{Characters}{Words}\\bigg) + 0.5 \\bigg(\\frac{Words}{Sentences}\\bigg) - 21.43.$"}, {"title": "Machine Learning", "content": "In this study, three types of approach were explored. First, for text classification, the use of Transformer models. Secondly, for the classification of numerical linguistic features, deep neural networks were explored. Finally, a fusion"}, {"title": "Web Application for Inference and Reporting", "content": "This subsection describes the method for integrating the models as well as NLP techniques within an interface designed to be used by non-technical stakeholders such as English teachers or librarians. The general approach to interface with the web application can be seen in Figure 3. Following authentication and input of a given text, the processes described"}, {"title": "Results and Discussion", "content": "This section contains the results of the experiements described previously. The results of all the experiments can be found in Table 4. The best single modal model overall was BERT, which had an F1 score of 0.75. It can be observed that all multimodal models outperformed all transformer-based unimodal text classifiers. The overall best model was the multimodal combination of the ELECTRA text classifier and linguistics artificial neural network, which achieved an F1 score of 0.996.\nFigure 11 shows the best results for the unimodal classifiers. The best text classifier was BERT, with an F1 score of 0.75. It can be observed that neural networks trained on linguistic features perform poorly, with a pattern of higher"}, {"title": "Conclusion and Future Work", "content": "This study has explored the use of multimodal fusion for the classification of educational literature into appropriate UK Key Stages. Results showed that multimodal approaches significantly outperformed their constituent unimodal models. The proposed framework and web application combines linguistic feature analysis with transformer-based text classification, and forms a robust tool for non-technical stakeholders to rapidly assess the complexity and suitability of different educational texts. Among the models evaluated, it was found that the multimodal ELECTRA and ANN approach achieved the highest classification ability with an F1 score of 0.996.\nThe application developed to deliver the work performed in this article enables educators to analyse text and retrieve granular insights into Key Stage alignment. This real-time, stakeholder-facing tool addresses open challenges in the field, allowing rapid response to emerging trends for encapsulation within the education system, as well as reducing manual workload for educators. Future work could focus on expansion beyond the current data towards internationalisation, that is, tuning the models for appropriate in education systems outside the UK. Given the findings of this study, future work could also consider user feedback from domain experts such as teachers or librarians, with the aim of greater real-world impact. In terms of the methodology of this work, although multimodal models outperformed all unimodal models, the unimodal linguistics models were significantly weaker. Given this scientific limitation, further work could be explored in improving linguistic models prior to multimodal fusion. Due to limitations of computational resources, this study undersampled the full dataset, and thus future work could consider benchmarking the approaches on the full set of texts.\nBy alleviating some of the open challenges in the field of educational text analysis, this work has proposed approaches towards scalable, data-driven tools that have the potential to empower educators to adapt teaching materials dynamically with little manual effort required."}, {"title": "Data Availability Statement", "content": "The data produced and utilised within this study is publicly available under the MIT license\u00b2."}]}