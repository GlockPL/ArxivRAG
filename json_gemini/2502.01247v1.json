{"title": "Learnable polynomial, trigonometric, and tropical activations", "authors": ["Ismail Khalfaoui-Hassani", "Stefan Kesselheim"], "abstract": "This paper investigates scalable neural networks with learnable activation functions based on orthogonal function bases and tropical polynomials, targeting ImageNet-1K classification and next to-ken prediction on OpenWebText. Traditional ac-tivations, such as ReLU, are static. In contrast, learnable activations enable the network to adapt dynamically during training. However, stabil-ity issues, such as vanishing or exploding gra-dients, arise with improper variance management in deeper networks. To remedy this, we propose an initialization scheme that single-handedly pre-serves unitary variance in transformers and con-volutional networks, ensuring stable gradient flow even in deep architectures. Extensive experiments demonstrate that networks with Hermite, Fourier, and Tropical-based learnable activations signifi-cantly improve over GPT-2 and ConvNeXt net-works in terms of accuracy and perplexity in train and test, highlighting the viability of learnable ac-tivations in large-scale tasks. The activation func-tions developed here are the subject of a library coded entirely in pure PyTorch: torchortho\u00b9.", "sections": [{"title": "1. Introduction", "content": "Modern deep learning is largely built upon the Multi-Layer Perceptron (MLP) (McCulloch & Pitts, 1943; Rosenblatt, 1958) and the gradient backpropagation algorithm (Rumel-hart et al., 1986). The MLP can be described as a combina-tion of a multiplication by a matrix of learnable weights and the application of a nonlinear activation function. Gradi-ent backpropagation, on the other hand, relies on the chain rule to compute partial derivatives necessary for optimizing weights through gradient descent.\nIn a deep neural network, preserving variance across layers is critical to ensure stable training dynamics. He et al. (2015)"}, {"title": "2. Related Work", "content": "The use of polynomial activations has long been denigrated, probably by the rise of works such as (Pinkus, 1999) and (Leshno et al., 1993) which have mathematically demon-strated that the universal approximation property is equiv-alent to the use of a non-polynomial activation function. The classical Universal Approximation Theorem (Cybenko, 1989; Hornik et al., 1990) holds for neural networks of arbitrary width and bounded depth. However, recent work such as (Kidger & Lyons, 2020) shows that in the framework of bounded width and arbitrary depth, every activation function is possible to use in practice, including polynomial activa-tion functions. We show empirically in this work that poly-nomial activations can converge in the context of large-scale deep networks with large-scale tasks and datasets. The key to this success may lie in the fact that the coefficients of the latter are learnable and that adequate initialization is used. The empirical demonstration of the effectiveness of polyno-mial activations made here was achieved without the use of other functions intended to regularize convergence, such as the SoftSign function borrowed from (Turian et al., 2009) and used in (Lokhande et al., 2020) for Hermite activations, or a ReLU function, or any normalization as recently done in (Zhuo et al., 2024). This confirmation that polynomial activations are practicable opens the way to representing deep neural networks as multivariate polynomial mappings. As in (Kileel et al., 2019) and (Kubjas et al., 2024), which see that these types of networks have greater expressive potential, we show that deep polynomially activated neu-ral networks are multivariate polynomial mappings. The subject of learnable activation is a well-known one, but it has seen a resurgence thanks to the popularity enjoyed by the KAN article (Liu et al., 2024). In Appendix I, we'll digress for a while to explain how these are inspired by the Kolmogorov-Arnold theorem (Kolmogorov, 1957). An extended related work can be found in Appendix H."}, {"title": "3. Methods", "content": null}, {"title": "3.1. Variance Preserving Initialization", "content": "The variance-preserving principle (He et al., 2015) men-tioned in the introduction, is expressed in the following. Consider an input vector $x = (X_0,..., X_i,..., X_{C_{in}}) \\in \\mathbb{R}^{C_{in}}$, $C_{in} \\in \\mathbb{N}^*$, where all $x_i$ in are mutually indepen-dent and uniformly distributed. Preserving the variance in an MLP layer with a learnable weight tensor $W$ of inner dimension $C_{in}$ and an activation function $F$ amounts to:\n$\\text{Var}[x] = C_{in} \\text{Var}[WF(x)]$\nIf we suppose that $x$ and $W$ are independent and of finite variance, we have:\n$\\text{Var}[x] = C_{in} (\\text{Var}[W]E [F(x)^2] + \\text{Var}[F(x)]E [W]^2)$\nAssumption 3.1. We initialize $W$ such as $E [W] = 0$.\nSince we always assume that $W$ is initialized with a zero mean, Eq. 2 simplifies into:\n$\\text{Var}[x] = C_{in} \\text{Var}[W]E [F(x)^2]$\nThus, to calculate the variance of the weights, we should calculate the following ratios:"}, {"title": "Definition 3.2. The forward gain of the MLP layer is:", "content": "$\\alpha = \\frac{\\text{Var}[x]}{E [F(x)^2]}$\nSimilarly, and in a backward manner,\nDefinition 3.3. The backward gain is the gain of the deriva-tive of the activation with respect to x and is defined as:\n$\\alpha' = \\frac{\\text{Var}[x]}{E [F'(x)^2]}$\nSince a deep neural network is essentially a composition of MLP layers, an appropriate initialization method must avoid reducing or amplifying the input signals (He et al., 2015).\nAssumption 3.4. We'll assume from now on that both the input signal x and its gradient Ax follow a distribution of mean 0 and variance 1.\nTherefore, calculating the gains a and a' in an MLP (or equivalently a convolution layer) involves calculating only the inverse of the second-order moments of the activation functions and their derivatives.\nInterestingly, for the ReLU function, we have $a = a' = 2$. Hence the scaling of the standard deviation of the weights W in (He et al., 2015) by a factor $\\sqrt{2/C_{in}}$, more details can be found in Appendix A.\nGiven an arbitrary activation, equality of forward and back-ward gains is not always achieved by default as in ReLU. In the next section, we show the conditions for an activation function written in an orthonormal coordinate system to verify the forward-backward gain equality. To illustrate this point, we will calculate the second moment for Hermite and Fourier basis decompositions, given their compatibility with the normal and uniform distributions, respectively."}, {"title": "3.2. Second Moment of the Hermite Activation Function and Its Derivative", "content": "Definition 3.5. $\\forall n \\in \\mathbb{N}$, the probabilist Hermite polynomi-als can be defined as follows:\n$He_n(x) = (-1)^n e^{\\frac{x^2}{2}} \\frac{d^n}{dx^n} e^{-\\frac{x^2}{2}}$\nn is called the degree of the Hermite polynomial and we have the first terms:\n$He_0 (x) = 1$\n$He_1 (x) = x$\n$He_2 (x) = x^2 -1\n$He_3 (x) = x^3 - 3x$\nHermite polynomials constitute a suitable choice for cal-culating the moment of order 2 when x follows a standard normal distribution N (0, 1)."}, {"title": "Property 3.6. $\\forall m, n \\in \\mathbb{N}^2$, we have:", "content": "$\\int_{-\\infty}^{+\\infty} He_m (x) He_n(x) e^{-\\frac{x^2}{2}} dx = \\sqrt{2\\pi}n! \\delta_{nm}$\nWith $\\delta_{nm}$ the Kronecker delta function.\nDefinition 3.7. We define the Hermite activation F: R \u2192 R with its learnable coefficients $\\forall k \\in [0, n] \\: a_k \\in \\mathbb{R}$ as:\n$x \\rightarrow F(x) = \\sum_{k=0}^n a_k \\frac{He_k (x)}{\\sqrt{k!}}$\nProposition 3.8. The second moment of this activation is:\n$E [F(x)^2] = \\sum_{k=0}^n a_k^2$\nProof. The proof relies on the orthonormality property 3.6 and is detailed in Appendix B.\nProperty 3.9. The following recurrence property is derived directly from the equation 6. $\\forall k \\in \\mathbb{N} \\: \\forall x \\in \\mathbb{R}$:\n$He'_k(x) = x He_k(x) - He_{k+1}(x)$\nProperty 3.10. This property is shown by induction and by using the previous property 3.9. $\\forall k \\in \\mathbb{N}^* \\: \\forall x \\in \\mathbb{R}$:\n$He'_k(x) = k He_{k-1}(x)$\nProposition 3.11. Using the last property and by the linear-ity of the integral, the derivative of F (Eq. 8), F': R \u2192 R is written as follows:\n$x \\rightarrow F'(x) = \\sum_{k=1}^n k a_k \\frac{He_{k-1}(x)}{\\sqrt{k!}}$\nRemark 3.12. A first remark here is that $\\forall n > 2$: F' is unbounded ($\\lim_{x\\rightarrow\\infty} F'(x) \\rightarrow \\infty$). This means that F is not Lipschitz continuous. Lipschitz continuity is often desired (or even required) when training a deep neural network using gradient backpropagation. However, by a suitable initial choice of the coefficients $(a_k)_{k\\in[0,n]}$ we can keep the Lipschitz constant under control.\nProposition 3.13. The second moment of the derivative of the Hermite activation is:\n$E [F'(x)^2] = \\sum_{k=1}^n ka_k^2$"}, {"title": "Proposition 3.14. Equality between propositions 3.8 and 3.13 imposes that:", "content": "$a_0^2 = \\sum_{k=1}^n (k-1)a_k^2$\nTo satisfy the forward-backward gain equality, we could initialize the coefficients $(a_k)_{k\\in[0,n]}$ such as $\\forall n \\in \\mathbb{N}^*$:\n$\\forall k \\in [1, n] \\: a_k = 1$ and $a_0 = \\sqrt{\\frac{n(n-1)}{2}}$\nThis initialization works in practice for small n. However, this choice does not scale well with the degree n, as the leading coefficient $a_0$ diverges to infinity with n. Instead, we could opt for the following initialization inspired by the limit case n \u2192 +\u221e:\nTheorem 3.15. Scalable variance-preserving coefficient initialization of Hermite activation. Let p > 1, and\n$\\forall k \\in [1, n] \\: a_k = \\frac{1}{k^p}$ and $a_0 = \\sqrt{\\frac{1}{2}((2p - 1) - \\zeta(2p)}$\nwith $\\zeta$ the Riemann function $\\forall x \\in [1,+ \\infty]: \\zeta(x) = \\sum_{k=1}^{\\infty} \\frac{1}{k^x}$. Then in the limit case n \u2192 +\u221e, the gain for the activation and its derivative becomes the same and equals:\n$\\alpha = \\alpha' = \\frac{1}{(2p - 1)}$\nProof. In the limit case, by a simple injection of $a_k = \\frac{1}{k^p}$ in Prop. 3.14 and then in Prop. 3.13, we obtain the result.\nCorollary 3.16. The coefficient initialization in Theorem 3.15 could be divided by a factor $\\sqrt{(2p - 1)}$ in order to have unitary forward and backward gains. $\\forall k \\in [1, n] :\n$a_k = \\frac{1}{k^p \\sqrt{(2p - 1)}}$ and $a_0 = \\sqrt{\\frac{-\\zeta(2p)}{(2p - 1)}}$\nExample 3.17. If we take p = 3, by the corollary 3.16 we have:\n$\\forall k \\in [1, n] \\: a_k = \\frac{6}{\\sqrt{\\pi^2k^3}}$ and $a_0 = \\sqrt{\\frac{6\\zeta(3)}{\\pi^2}} \\approx 0.519$\nIn practice, we will use $p = \\frac{3}{2}$ in all our subsequent experi-ments where Hermite activations are involved.\nThe choice of an orthonormal family of functions depends on the input's probability distribution. For a normally dis-tributed input, Hermite polynomials simplify the compu-tation of second-order moments and related gains. For a uniform distribution over [-\u03c0, \u03c0], trigonometric func-tions (Fourier series) are appropriate. If the input follows a Wigner semi-circle distribution (of measure $\\sqrt{1 - x^2}dx$), then the Chebyshev polynomials of the second kind are the suitable choice."}, {"title": "3.3. Second Moment of the Fourier Activation Function and Its Derivative", "content": "The forward and backward gains for a Hermite activation have been calculated under the assumption that the input x follows a normal distribution, such that the initial coef-ficients provide equal gains. The subsequent analysis will establish the same result for a truncated Fourier series ex-pansion of order n \u2208 N.\nAssumption 3.18. The input x is assumed now to follow a uniform distribution on the interval [\u2212\u03c0, \u03c0], denoted as $x \\sim U (\u2212\u03c0, \u03c0)$.\nDefinition 3.19. We consider the following Fourier activa-tion F: R\u2192 R:\n$F(x) \\rightarrow a_0 + \\sum_{k=1}^n (a_k \\cos(kx) + b_k \\sin(kx))$\nwhere $(a_k)_{k\\in\\mathbb{N}}$ and $(b_k)_{k\\in\\mathbb{N}^*}$ are real learnable coefficients.\nProposition 3.20. The second moment of this activation is:\n$E[F(x)^2] = a_0^2 + \\frac{1}{2} \\sum_{k=1}^n (a_k^2 + b_k^2)$\nProof. The proof relies on the orthonormality property C.1 and is detailed in Appendix C.\nRemark 3.21. For an input x of distribution $x \\sim U(-\\sqrt{3}, \\sqrt{3})$, which has a variance of $\\text{Var}[x] = 1$ and which is more in line with deep neural networks that seek a unitary variance preserving property across layers, we could rescale the fundamental frequency given in the definition of F in Def. 3.19 by redefining it as:\n$F(x) \\rightarrow a_0 + \\sum_{k=1}^n (a_k \\cos(k \\frac{\\pi}{\\sqrt{3}}x) + b_k \\sin (k \\frac{\\pi}{\\sqrt{3}}x))$\nThe computation of the second moment stays the same. In what follows, we will consider $x \\sim U(-\\sqrt{3}, \\sqrt{3})$ as well as the definition of F as established in Eq. 22.\nProposition 3.22. The derivative of the Fourier activation F': R\u2192R from its definition in Eq. 22 is given by:\n$F'(x) \\rightarrow \\sum_{k=1}^n k \\frac{\\pi}{\\sqrt{3}} (-a_k \\sin(k \\frac{\\pi}{\\sqrt{3}}x) + b_k \\cos(k \\frac{\\pi}{\\sqrt{3}}x))$\nRemark 3.23. Contrary to the remark in 3.12, F' is bounded.\n$\\forall x \\in \\mathbb{R}: |F'(x)| \\leq \\frac{\\pi n (n + 1)}{\\sqrt{3}} \\max(|a_k|, |b_k|)_{k\\in [1,n]}$\nThis means that in the case of a Fourier activation, F is Lipschitz continuous."}, {"title": "Proposition 3.24. The second moment of the derivative of the Fourier activation is:", "content": "$E [F'(x)^2] = \\sum_{k=1}^n k^2 \\frac{\\pi^2}{3} (a_k^2 + b_k^2)$\nProof. an orthonormality argument as for the proof in Ap-pendix C suffices.\nProposition 3.25. Equality between 3.20 and 3.24 imposes that:\n$\\frac{1}{2} = \\frac{\\pi^2}{3} \\sum_{k=1}^n k^2 (a_k^2 + b_k^2)$\nTo satisfy the forward-backward gain equality, we could again initialize the coefficients such as \u2200n \u2208N*:\n$\\forall k \\in [1, n] \\: a_k = b_k = 1$ and $a_0 = \\sqrt{\\frac{\\pi^2n(n + 1)(2n + 1) - 18n}{6}}$\nHowever, this choice does not scale well with the degree n. Instead, we opt for an initialization akin to the one shown in theorem 3.15. More details can be found in Appendix C.\nRemark 3.26. In our implementation of Fourier activa-tion, not only the coefficients $(a_k)_{k\\in\\mathbb{N}}$ and $(b_k)_{k\\in\\mathbb{N}^*}$ were learnable, but also the frequencies that were initialized to $(f_k = k)_{k\\in\\mathbb{N}^*}$, yielding to what is known as \u201ccosine basis\u201d (Mallat, 2009) rather than Fourier series."}, {"title": "3.4. Tropical polynomial and rational activations", "content": "Definition 3.27. The max-tropical semiring T is the semir-ing T = (RU {+\u221e},\u2295,\u2297), with the operations, \u2200x, y \u2208 R\u222a{+\u221e}\u00b2:\n$x \u2295 y := \\max{x,y}$ and $x \u2297 y := x + y$\nEquivalently, we could define the min-tropical semiring by substituting the max operation in \u2295 with a min operation. By extension, we define for all a \u2208 N the tropical power of x raised to a as multiplying x to itself a times:\n$x^a := x \u2297 \u2026 \u2297 x = a \u2022 x$\nDefinition 3.28. The tropical polynomial activation F is defined as the tropicalization of a polynomial of degree n\u2208 N with \u2200k \u2208 [0, n] ak \u2208 R the learnable coefficients:\n$F: R\u2192R$\n$F(x)\u2192\\sum_{k=0}^n a_k x^{\u2297k} := \\max_{k=0}^n {a_k + kx}$\nWith $\\max_{k=0}^n {a_k + kx} := \\max(a_0, a_1 + x,\u2026, a_n + nx)$."}, {"title": "Remark 3.29.", "content": "In the following, we will only be interested in polynomial tropical activations, for which we will initialize all learnable coefficients to 1, a \"reasonable\" initialization that empirically holds. The computation of the second-order moment of a tropical activation involves a generalized extreme value distribution, which we will not discuss in this article, but rather in a later work."}, {"title": "3.5. Deep Polynomially Activated Neural Networks are Multivariate Polynomial Mappings", "content": "Deep MLPs are compositions of affine transformations and activation functions applied layer by layer. When the acti-vation functions are polynomial, the entire network can be expressed as a polynomial mapping.\nDefinition 3.30. Let n, m \u2208 N. A function F: Rn \u2192 Rm is called a polynomial mapping if each component function Fi: Rn \u2192 R, for i = 1,...,m, is a polynomial in n variables. Explicitly, this means that for each i, Fi has the form:\n$F_i(x_1,...,x_n) = \\sum_{|\u03b1|\\leq d_i} C_{i,\u03b1} x_1^{\u03b1_1} x_2^{\u03b1_2} \u2026 x_n^{\u03b1_n}$ where the sum is taken over all multi-indices \u03b1 = (\u03b11,..., \u03b1n) \u2208 Nn such that |\u03b1| = \u03b11+\u03b12+\u2026+\u03b1n \u2264 di, Ci,\u03b1 \u2208 R are real coefficients, and di \u2208 Nn.\nDefinition 3.31. A deep neural network with L layers, input dimension n, and output dimension m is a function F : Rn\u2192Rm of the form:\n$F(x) = W_LG(W_{L-1}\u03c3(\u2026\u03c3(W_1x + b_1)\u2026) + b_{L-1}) + b_L$, where \u2200i \u2208 [1,L] Ci \u2208 N*. Each Wi \u2208 RCi\u00d7Ci\u22121 is a weight matrix, bi \u2208 RCi is a bias vector, and \u03c3 is an activation function applied element-wise.\nProposition 3.32. Let F : Rn \u2192 Rm be a deep neural network with polynomial activation functions of degree d. Then F is a polynomial mapping of degree at most dL."}, {"title": "3.6. Practical Implementation", "content": "In what follows, we outline the considerations we have taken in order to implement Hermite, Fourier, and Tropical polynomial activations efficiently in PyTorch.\nWeight decay. An important aspect of training learnable ac-tivations is that their learnable coefficients should be trained without weight decay as it could bias them toward zero.\nExplicit Hermite formula. We can show by induction that"}, {"title": "4. Experiments", "content": null}, {"title": "4.1. Vision Task: ConvNeXt-T Image Classification on ImageNet1k", "content": "We evaluated the ConvNeXt-T model (Liu et al., 2022) on the ImageNet1k dataset (Deng et al., 2009) for single class image classification. The baseline ConvNeXt-T model em-ployed GELU as the activation function in its MLP blocks. To analyze the impact of our learnable activations, we re-placed GELU with Hermite polynomial, Fourier trigono-metric, and tropical polynomial activation functions. Each model was trained under identical conditions with fixed random seeds to ensure reproducibility and comparability. The evaluation metrics included: training loss, top-1 and top-5 validation accuracy. We report in Table 1 the extremal values of these metrics reached by each of these activation trials. The experimental setup followed the approach and hyperparameter configuration detailed in (Liu et al., 2022).\nAblation Studies. Additionally, ablation studies were per-formed on this vision task to establish the impact of the degree for the learnable activations (Table 3), the impact of"}, {"title": "4.2. Language Task: GPT-2 (124M) Next Token Prediction on OpenWebText", "content": "For the language modeling task, we trained the GPT-2 model (Radford et al., 2019) on the OpenWebText dataset (Gokaslan & Cohen, 2019) for next-token prediction. The baseline GPT-2 used GELU activation, and we compared it against the best Hermite, Fourier, and Tropical activa-tions configurations found using the ablation studies on the previous vision experiment. All models were trained with identical hyperparameters and initialization seeds to ensure consistent and reproducible comparisons. The evaluation metrics included: training and test losses and perplexities (which are simply the exponential of the loss). We report in Table 2 the extremal values of these metrics reached by each of these activation trials and in Figure 7 the overall valida-tion loss. The experimental design followed the guidelines established in (Radford et al., 2019) and the open source reproduction available at (Karpathy, 2022). We used a total batch size of 786, 432 with a context length of 1024 tokens for a total of 210, 000 iterations.\nBoth experiments were conducted under fixed configura-tions to ensure that any observed differences were solely due to the choice of activation function, allowing for fair and reproducible comparisons2."}, {"title": "4.3. Parameters, Memory, and Flops", "content": "The number of additional parameters introduced by the ac-tivations presented here is marginal compared to the other network parameters. The additional number of parameters introduced by a Hermite or Tropical polynomial activation of degree d are d + 1 and by a Fourier activation of de-gree d are 3d + 1 per activation. In terms of memory and computational complexity, the choice of a learnable activa-tion function can significantly impact the efficiency of the network. When evaluated using the explicit formula, the complexity of all activations is multiplied by a factor which is O(d2) in terms of Flops and memory, which constitutes a limitation of this algorithm. In contrast, using the recur-sive formulation reduces this to a factor O(d) in Flops and"}, {"title": "5. Discussion", "content": "The results presented in this paper demonstrate the potential of using learnable activation functions based on orthogo-nal function bases and tropical polynomials in large-scale neural network tasks. Our experiments on ImageNet-1K and OpenWebText with deep models such as ConvNeXt and GPT-2 show for the first time that such activations can lead to significant improvements over traditional static func-tions like ReLU and GELU, both in terms of classification accuracy and language modeling perplexity.\nThis challenges the long-standing notion that polynomial activations are inherently unsuitable for deep learning, as demonstrated by prior work. Our approach provides empiri-cal evidence that, with appropriate initialization, polynomial activations can indeed be competitive.\nOne of the key takeaways from our findings is the effec-tiveness of our proposed variance-preserving initialization scheme. The choice of orthogonal functions plays an es-sential role in achieving a closed-form expression for the second-order moment. Furthermore, the use of tropical poly-nomials, which are not orthogonal, introduces a Flops-light alternative approach to polynomial activations.\nWhile our approach shows promise, there are several av-enues for future exploration. Extending the framework to other activation families, such as wavelets is straightforward. Multiplying the Hermite activation presented in this work by the term $e^{-\\frac{x^2}{2}}$ gives what is known as Hermitian wavelets (Brackx et al., 2008; Pandey & Phukan, 2020), and applying the same to the Fourier activation yields the Morlet wavelet (Grossmann & Morlet, 1984) (or Gabor wavelet (Gabor, 1946)). Wavelets retain good orthogonal properties with respect to the adequate scalar product and the calculation of the second moment is slightly modified to take account of the additional decaying exponential term. Using wavelet activations instead of polynomials could enhance variance stability by providing finite function support, with potential bio-plausibility implications.\nBy expressing a Fourier series in its complex form, a net-work with Fourier activation can be viewed as a complex-valued neural network, offering a natural framework for modeling neuronal synchronization through the phase and amplitude relationships of oscillatory brain activity.\nExtension to other non-orthogonal functions such as rational functions could be done for example by means of a Laplace transform of the Fourier activation."}, {"title": "6. Conclusion", "content": "In this work, we introduced a novel framework for inte-grating learnable activation functions based on orthogonal function bases and tropical polynomials into deep neural networks, addressing challenges like variance preservation and stable gradient flow. Extensive experiments with the ConvNeXt model on ImageNetlk and the GPT-2 model on OpenWebText demonstrated that learnable activations outperform traditional static functions on large-scale tasks, showcasing their practical viability and challenging con-ventional beliefs about polynomial activations in neural networks. Our results pave the way for representing deep neural networks as polynomial mappings, with future work focused on exploring a careful relaxation of these mappings."}, {"title": "F. Rational Tropical Activation", "content": "Definition F.1. The tropical quotient of x over y is defined as:\n$x \\oslash y := x - y$\nDefinition F.2. The tropical rational activation F is defined as the quotient of two tropical polynomials F\u2081 and F2 of degree m, n \u2208 N\u00b2 respectively.\n$F: R\u2192R$\n$F(x) \u2192 F\u2081(x) \u2296 F\u2082(x) := F\u2081(x) - F\u2082(x)$\nAn example of fitting a classical activation (GELU) with a rational tropical activation is shown in Figure 6. Rational tropical activation is understood here in the general sense, i.e. with real powers."}, {"title": "I. A brief digression on Kolmogorov Arnold Networks (KANs)", "content": "What is left of the recently famous Kolmogorov-Arnold networks (KAN) (Liu et al., 2024)?\nKolmogorov-Arnold networks have been presented as a potential alternative to Multilayer-Perceptrons (MLPs), promoting several merits such as greater accuracy, fewer learnable parameters, and better interpretability. While the first two advantages could only be demonstrated for simple cases in the (Liu et al., 2024) article, the third benefit is more straightforward, as these networks overcome the \"black-box\" aspect of traditional non-linear activations MLPs by allowing the activation to be polynomial, piece-wise polynomial or rational, as in (Yang & Wang, 2024). From there, having learned the weights of the network and those of the activation, it becomes clear what approximation these functions (polynomial, rational, or trigonometric) have converged to.\nPresenting themselves as heirs to the celebrated Kolmogorov-Arnold representation theorem (KART) (Kolmogorov, 1957; Arnold, 1959), the use made of this theorem in the recent article KAN (Liu et al., 2024) is to be understood figuratively. This is merely an inspiration, as the Kolmogorov-Arnold representation theorem, cited below, states that any continuous multivariate function f: [0, 1] \u2192 R can be represented as a composition of addition and some functions of one variable denoted by q,p and \u03a6q:\nTheorem I.1. (Arnold (2009b;a)) Let f : In := [0,1]n \u2192 R be an arbitrary multivariate continuous function. Then it can be represented as follows:\n$f (x_1,...,x_n) = \\sum_{q=0}^{2n+1} \u03a6_q(\\sum_{p=1}^{2n} \u03c8_{qp} (x_p))$"}, {"title": "A wealth of literature can be found on this subject.", "content": "Yet, a common criticism of these papers is that they focus on a specific"}]}