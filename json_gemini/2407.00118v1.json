{"title": "From Efficient Multimodal Models to World Models: A Survey", "authors": ["Xinji Mai", "Zeng Tao", "Junxiong Lin", "Haoran Wang", "Yang Chang", "Yanlan Kang", "Yan Wang", "Wenqiang Zhang"], "abstract": "Multimodal Large Models (MLMs) are becoming a significant research focus, combining powerful large language models with multimodal learning to perform complex tasks across different data modalities. This review explores the latest developments and challenges in MLMs, emphasizing their potential in achieving artificial general intelligence and as a pathway to world models. We provide an overview of key techniques such as Multimodal Chain of Thought (M-COT), Multimodal Instruction Tuning (M-IT), and Multimodal In-Context Learning (M-ICL). Additionally, we discuss both the fundamental and specific technologies of multimodal models, highlighting their applications, input/output modalities, and design characteristics. Despite significant advancements, the development of a unified multimodal model remains elusive. We discuss the integration of 3D generation and embodied intelligence to enhance world simulation capabilities and propose incorporating external rule systems for improved reasoning and decision-making. Finally, we outline future research directions to address these challenges and advance the field.", "sections": [{"title": "I. THE DEVELOPMENT STATUS OF MULTIMODAL MODELS\nAND WORLD MODELS", "content": "World models are currently one of the hottest research\ndirections in the AI field. From OpenAI to Meta, major\nAI companies are striving to develop world models. The\nconcept of world models can be traced back to the fields\nof reinforcement learning and robotic control. Traditionally,\nreinforcement learning algorithms rely on agents learning\nthrough trial and error in real environments, which is not\nonly costly but sometimes infeasible. To overcome these\nlimitations, researchers began exploring methods for simu-\nlation and learning within internal environments. Jurgen et\nal. [1] have described a method for quickly training through\nunsupervised environments using generative recurrent neural\nnetworks (RNN) to compress spatiotemporal representations\nand simulate common reinforcement learning environments.\nJurgen et al. referred to this as a world model. In AI research,\nthe proposal of world models aims to distinguish this direction\nfrom another research focus: agents.\nWorld models gained widespread attention thanks to Yann\nLeCun's work [2]. Yann LeCun mentioned that human or\nanimal brains seem to run a simulation of the world, which he\ncalled a world model. This model helps humans and animals\nmake predictions about their surroundings. LeCun provided\nan example: a baby learns basic knowledge by observing the\nworld in the first few months after birth, such as understanding\ngravity when seeing an object fall. This ability to predict\nwhat will happen next comes from common sense, which\nLeCun believes is the essence of intelligence. The Sora model\nis a model developed by OpenAI for generating video. It\nutilizes multimodal learning techniques to generate realistic\nvideo content by combining text and image data. In recent\nstudies, OpenAI defined Sora in their report as a world\nsimulator capable of generating videos and considered Sora's\ntechnology a promising approach to building a general world\nmodel. We will introduce the differences between the two\nmain routes currently being explored: the multimodal large\nmodels developed by Meta under Yann LeCun's guidance and\nOpenAI's GPT series.\nIn summary, we can clearly define a world model as shown\nin Figure 1. A world model refers to a model that can predict\nand simulate environmental state changes by learning from\nvarious data in the environment. Unlike conventional data\ntesting scenarios where data does not change, a world model's\ndata can change independently, even generating data not in\nthe test dataset. The core function of a world model lies in\ncounterfactual reasoning, which allows it to infer the outcomes\nof decisions not encountered before. AI researchers' pursuit of\nworld models aims to achieve this counterfactual reasoning, a\nnatural ability of humans that current AI lacks."}, {"title": "B. Routes to World Models", "content": "Currently, there are two main routes to developing world\nmodels: autoregressive methods and JEPA (Joint Embed-\nding Predictive Architecture) methods. Autoregressive mod-\nels hold a significant place in the generative field, with\nnotable representatives including the GPT series and Sora.\nThese models, based on Transformer architecture [3], gen-\nerate data step-by-step, with each output depending on the\nprevious hidden state. This incremental generation allows the\nmodel to capture contextual information, producing coherent\nand logical sequences. The autoregressive model possesses\na robust contextual understanding capability and is facile to\ntrain, thus establishing itself as the predominant approach in\nthe field of world modeling. By capitalizing on previously\ngenerated content during generation, autoregressive models\ndemonstrate adeptness in comprehending and maintaining\ncontextual consistency, thereby yielding more coherent and\nmeaningful output. The training process for autoregressive\nmodels is relatively straightforward, involving step-by-step\nprediction and optimization based on known sequential data,\nwhich contributes to their commendable performance when\ntrained on large-scale datasets. While autoregressive models\nexcel in natural language processing tasks, generating high-\nquality text paragraphs through pre-training and fine-tuning,\ncritics argue that these models lack real-world common sense,\nobscured by vast amounts of information. For instance, a baby\nlearns how the world works and can predict outcomes with\nlittle practice compared to the extensive training data required\nfor large language models.\nIn response, Meta proposed the JEPA framework. The core\nidea of JEPA is hierarchical planning, a method of decision-\nmaking and control that is especially suited to handling com-\nplex tasks and large-scale problems. This approach involves\nbreaking down problems into multiple levels, each addressing\nsub-tasks at different levels of abstraction, simplifying the\noverall problem-solving process. LeCun illustrated this with\nan example: to travel from New York to Beijing, one must\nfirst get to the airport, then take a flight to Beijing, with\nthe overall cost function representing the distance from New\nYork to Beijing. Solving this involves decomposing the task\ninto millisecond-level control, finding the action sequence that\nminimizes the predicted cost. He believes all complex tasks\ncan be accomplished through such hierarchical methods, with\nhierarchical planning being the most significant challenge.\nJEPA models extract abstract representations of the world\nstate through a series of encoders and use different levels of\nworld model predictors to forecast various states at different\ntime scales. Inspired by the human brain's ability to understand\nand react to the environment in a hierarchical manner, JEPA\nuses a layered architecture to break down complex tasks into\nmultiple levels, each handling sub-tasks at different abstraction\nlevels. This approach enables JEPA to efficiently capture and\npredict changes in complex dynamic systems, improving the\nmodel's handling of long-time-span and multi-scale data. Its\nunique hierarchical prediction mechanism not only enhances\nunderstanding and prediction accuracy of environmental states\nbut also increases adaptability and robustness in dealing with\nlarge-scale, diverse data, showcasing significant advantages in\nmany practical applications.\nIn summary, we can summarize the route to the world model\ninto two, rules and data drivers."}, {"title": "C. Multimodal Models", "content": "Regardless of the route to world models, multimodal models\nare an indispensable part. Multimodal models refer to machine\nlearning models capable of processing and understanding data\nfrom different modalities, such as images, text, audio, and\nvideo [4], [5]. Human interaction with the real world involves\nmultiple modalities of information, including language, vi-\nsion, and audio. Therefore, world models must handle and\nunderstand multiple forms of data, meaning they must have\nmultimodal understanding capabilities. Additionally, world\nmodels simulate dynamic environmental changes to make pre-\ndictions and decisions, requiring robust multimodal generation\ncapabilities [6]. To put it simply, the world is multimodal,\nand the world simulator must be able to accept and generate\nmultimodal information. In essence, world models are general-\npurpose models (General-Purpose Models).\nThe research on multimodal models can be broadly cate-\ngorized into several technical approaches: alignment, fusion,\nself-supervision, and noise addition. Alignment-based methods\nmap data from different modalities to a common feature space\nfor unified processing. Fusion methods integrate multimodal\ndata at different model layers to fully utilize information from\neach modality. Self-supervised techniques pre-train models on\nunlabeled data, enhancing performance across various tasks.\nNoise addition enhances model robustness and generalization\nby introducing noise into the data.\nCombining these techniques allows multimodal models to\ndemonstrate strong capabilities in handling complex real-\nworld data. They can understand and generate multimodal\ndata, simulate and predict environmental changes, and aid\nagents in making more precise and effective decisions. Thus,\nmultimodal models play a crucial role in developing world\nmodels, marking a key step towards general artificial intel-\nligence (General AI). The following sections will detail the\ntechnical routes of multimodal models."}, {"title": "D. Structure of This Paper", "content": "In Section 2, we introduce the fundamental technologies\nof basic architectures. In Section 3, we will introduce the"}, {"title": "II. BASIC TECHNIQUES OF MULTIMODAL MODELS", "content": "In this chapter, we will introduce the basic techniques of\nmultimodal models commonly used in two routes, rule-driven\nand data-driven, from the underlying architecture to the block\narchitecture."}, {"title": "A. Transformers and Their Challengers", "content": "The Transformer architecture is currently one of the most\npopular deep learning model architectures, especially for nat-\nural language processing (NLP) and computer vision (CV)\ntasks. The Transformer is a deep learning model designed\nfor handling sequential data, employing an attention mech-\nanism to model long-range dependencies within sequences\n[4]. Unlike traditional Recurrent Neural Networks (RNNs),\nthe Transformer processes sequences without relying on their\norder, utilizing self-attention to simultaneously focus on all\npositions in the sequence, thereby greatly enhancing parallel\ncomputation efficiency. The Transformer consists of an en-\ncoder and a decoder, where the encoder maps the input se-\nquence into a continuous representation space, and the decoder\ngenerates the output sequence based on this representation.\nEach layer of the encoder and decoder includes a multi-head\nself-attention mechanism and a feed-forward neural network,\nstabilized by residual connections and layer normalization.\nDue to its efficient parallel computation and powerful repre-\nsentation learning capabilities, the Transformer has achieved\nremarkable success in natural language processing and other\ntasks requiring sequence data processing. However, the self-\nattention mechanism of Transformers has high computational\ncomplexity when processing long sequences, limiting its effi-\ncient in some applications. To address this issue, researchers\nhave proposed various methods to challenge the Transformer\narchitecture. Linear Attention simplifies the computation of\nself-attention, reducing its time and space complexity from\nO(N2) to O(N). Key models include Performer [7], Lin-\nformer [8], and Linear Transformers [9]. These models can\nefficiently handle long-sequence data, reducing computational\nresource consumption. Additionally, Grouped-query Attention\nand Multi-query Attention are important attention mechanism\nvariants. The former balances multi-head and multi-query\nattention by sharing a set of keys and values among groups\nof query heads, while the latter simplifies computation by\nsharing the same key and value for all query heads, thus\nimproving efficiency. Grouped Query Attention and Multi-\nquery Attention excel in reducing the size of key-value\npairs during inference, significantly improving throughput.\nMulti-Query Attention shares key-value pairs among multiple\nheads, achieving a 30-40% reduction in throughput. Grouped\nQuery Attention groups queries, sharing key-value pairs within\ngroups, achieving results comparable to Multi-Query Attention\nin both efficiency and performance. MQA and GQA are used\nin the well-known open-source large language models Llama-2\nand Llama-3 [10]\u2013[12].\nAt the block level, optimization methods include Compact\nArchitecture, which reduces layers and parameters for a com-\npact model structure, lowering computational costs; Pruning\n[13], which reduces redundant parameters through pruning\ntechniques, enhancing computational efficiency; Knowledge\nDistillation [14], which extracts knowledge from a large\nteacher model and applies it to a smaller student model,\nsignificantly reducing model complexity and computational\nresource requirements; and Quantization [15], which converts\nmodel parameters from high-precision floating points to lower-\nprecision formats, further reducing computational and storage\ncosts. These optimization methods collectively aim to enhance\nthe efficiency and performance of Transformers, enabling them\nto process and integrate data from different modalities more\nefficiently in multimodal tasks.\nAdditionally, there are architectural approaches challenging\nthe Transformer model, such as Gated Convolution [16] or\nGated MLP [17], Recurrent Models [18], State Space Mod-\nels (SSMs) [19], [20], H3 [21], RWKV [22], Mega [23],\nYan, JEPA [24], and the notable Mamba [25] and Mamba\n2 [26]. Gated Convolution introduces gating mechanisms in\nconvolutional neural networks (CNNs), enhancing the model's\nability to capture local and long-range dependencies while\nreducing computational load. Recurrent models like LSTM\nand GRU capture temporal dependencies in sequences through\ntheir recursive structures [27], [28], overcoming the vanishing\ngradient problem in traditional RNNs. State Space Models\nexplicitly model the relationship between system states and\nobservations, providing a flexible framework for handling\ntime-series data, including State Space Models, H3, Mamba\nand Mamba 2.\nThese approaches not only offer new theoretical insights\nbut also demonstrate their advantages in practice. This chapter\nwill provide detailed introductions to these basic techniques in\ngeneral architectures, focusing on the most representative and\nmainstream approaches."}, {"title": "B. Optimization Techniques for Attention Mechanisms", "content": "Multi-head attention mechanisms are a core component\nof Transformers, capturing dependencies between different\npositions in the input sequence through parallel computation\nof multiple attention heads. However, the standard multi-\nhead attention mechanism has high computational complexity,\nprompting researchers to propose various variants to optimize\nits performance.\nThe standard Transformer model faces efficiency bot-\ntlenecks when processing long sequences, with the time\nand space complexity of its self-attention mechanism being\nquadratic in sequence length O(n\u00b2). This issue arises from\nthe Softmax operation in the attention mechanism. As shown\nin Figure 2, without Softmax, attention computation simplifies\nto three matrix multiplications, which are associative, allowing\nthe calculation of $KT V$ first, followed by left-multiplying by\nQ. This reduces complexity from O(n\u00b2) to linear O(n), the\ncore idea of Linear Attention.\nAttention(Q, K, V) = softmax $\\frac{Q K^T}{\\sqrt{d_k}}$ V\nRemoving Softmax, the attention computation's complexity\ncan drop to linear O(n). Scaled-Dot Attention essentially\nweights V with $Q K^T$. Therefore, a generalized definition of\nattention can be proposed:\nAttention(Q, K, V) = f($Q K^T$)V\nHere, f is a general function, approximating the Softmax\noperation. To fit Softmax, f must ensure non-negativity f \u2265 0.\nThis generalized attention form is known as Non-Local Net-\nworks in computer vision.\nIf elements of Q and K are non-negative, their dot product\nis naturally non-negative. This suggests introducing kernel\nfunctions. By adding a non-negative activation function \u03c6 to\nQ and K:\nAttention(Q, K, V) = $\\phi(Q)\\phi(K)^TV$\nWhere \u03c6 is a non-negative activation function, such as\nReLU(x). This method, termed the kernel method, is discussed\nby A Katharopoulos et al [9]. Performers estimate conven-\ntional (softmax) full-rank attention Transformers using linear\nspace and time complexity while retaining provable accuracy,\nwithout relying on sparsity or low-rank priors.\nAnother approach utilizes Softmax's properties. In Efficient\nAttention [29], Q normalized along dimension d and K along\ndimension n naturally satisfy normalization conditions. Thus,\nSoftmax is applied separately to Q and K:\nAttention(Q, K, V) = softmax(Q) \u00b7 softmax($K^T$)V\nThis form is a special case of the generalized attention\ndefinition. Additionally, sparse attention methods [30], such\nas Sparse Attention by OpenAI [31], reduce computation by\nretaining values in local regions only, forcing most attention\nvalues to zero. After special design, the non-zero elements of\nthe attention matrix are O(n), achieving linear-level attention.\nReformer [32], another notable improvement, reduces at-\ntention complexity to O(nlogn) by using locality-sensitive\nhashing (LSH) to find the largest attention values and compute\nonly those, achieving sparse attention. Moreover, Reformer\nredesigns the backward propagation process by constructing\nreversible feed-forward networks (FFN), reducing memory\nusage. Despite solving sparse attention's first drawback, Re-\nformer remains complex, especially LSH-based attention and\nreversible network backward propagation.\nPerformers [7] adopt a novel fast attention method, Fast At-\ntention Via Positive Orthogonal Random features (FAVOR+):\nAtt(Q, K, V) = $D^{-1}AV$,\nA = exp($\\frac{Q K^T}{\\sqrt{d}}$),\nD = diag ($A1_L$)\nEquivalent to the above attention, the scaling factor $\\sqrt{d_k}$\nsimplifies A. Fast attention (FA) maps Q and K through a \u03c6\nfunction to Q' and K', approximating A as their product:\nA = exp($Q K^T$) \u2248 \u03c6(Q)\u03c6($K^T$) = Q'K'\n\u03c6 maps matrix row vectors. FAVOR+ simulates beyond\nsoftmax other kernelizable attention mechanisms effectively.\nThis capability is crucial for accurately comparing softmax\nwith other kernels on large-scale tasks, aiding in finding\noptimal attention kernels. Performers are fully compatible with\nconventional Transformers, offering strong theoretical guaran-\ntees: unbiased or nearly unbiased attention matrix estimation,\nunified convergence, and low estimation variance.\nLinformer [8] improves self-attention with low-rank matrix\napproximation, reducing complexity to linear O(n). Linformer\nretains the original Scaled-Dot Attention form but projects Q\nand K to low-dimensional space using n\u00d7k matrices before\nattention, reducing computation. While Linformer excels in\nsome tasks, its performance on long-sequence tasks remains\nto be verified. Moreover, Linformer faces challenges in au-\ntoregressive generation tasks due to the projection process\ncombining entire sequence information, complicating causal\nmasking.\nMulti-Query Attention and Group-Query Attention are no-\ntable variants. These methods optimize the attention computa-\ntion process, reducing computational complexity and memory\nconsumption while maintaining or enhancing model perfor-\nmance. Multi-Query Attention (MQA) shares keys and values\namong all attention heads, computing independent queries\nfor each head, thus lowering complexity and memory usage.\nIn MQA, all attention heads share the same key and value,\ndiffering only in queries:\n$Q_i = QW^Q, K = KW^K, V = VW^V$\nAttention $(Q_i, K, V) =$ softmax$\\frac{Q_i K^T}{\\sqrt{d_k}}V$"}, {"title": "III. OPTIMIZATION TECHNIQUES FOR MODEL\nARCHITECTURES", "content": "In the research of multimodal methods, addressing issues\nsuch as excessive parameters in Transformer models, re-\nsearchers have proposed various optimizations and improve-\nments to enhance model efficiency, reduce computational\ncomplexity, and improve performance. This section provides\na detailed introduction to these improvements in model archi-\ntectures."}, {"title": "A. Model Compression", "content": "Model compression aims to reduce the number of parame-\nters and computational load in deep neural networks, enhanc-\ning efficiency and reducing storage requirements. Pre-trained\ndeep neural network models often face over-parameterization,\nwhere only about 5% of the parameters are effective. Model\ncompression techniques include frontend and backend com-\npression, aiming to shrink model size without significantly\nreducing accuracy, thus improving usability and efficiency in\npractical applications.\nFrontend compression methods include knowledge distil-\nlation, compact model structure design, and filter pruning.\nKnowledge distillation transfers knowledge from a complex\nmodel to a smaller one, allowing the small model to maintain\nhigh computational efficiency while achieving the perfor-\nmance of the complex model. For example, Hinton et al.\n[35] proposed knowledge distillation techniques that transfer\nteacher model knowledge through softened output probability\ndistributions. Compact model structure design improves the\nconvolution method of neural networks (e.g., using depthwise\nseparable convolutions) to reduce computational parameters.\nMobileNet [36] is a successful example in this aspect. Filter\npruning removes unimportant weight matrices to reduce model\nredundancy.\nBackend compression methods include low-rank approx-\nimation and unrestricted pruning. Low-rank approximation\nreconstructs large weight matrices with several low-rank ma-\ntrices, reducing storage and computational resource consump-\ntion. For example, Singular Value Decomposition (SVD) [37]\nis widely used for matrix decomposition to achieve compres-\nsion. Unrestricted pruning includes unstructured pruning and\nstructured pruning. Unstructured pruning removes individual\nweights.\nIn addition to these traditional compression methods, Han\nSong's team proposed AutoML Model Compression (AMC)\n[38], utilizing reinforcement learning to automatically search\nfor model compression strategies, enhancing the efficiency of\ndeploying neural network models on mobile devices. AMC\nuses reinforcement learning to intelligently balance model\nsize, speed, and accuracy, automatically generating optimal\ncompression strategies more efficiently and effectively than\nmanually crafted heuristic rules.\nThese model compression techniques improve computa-\ntional and storage efficiency, allowing deep neural networks\nto be widely applied in resource-constrained environments."}, {"title": "B. Model Pruning", "content": "Pruning techniques remove redundant parameters and con-\nnections in a model to enhance computational efficiency and\nreduce model size. Pruning techniques can be categorized into\nunstructured pruning, structured pruning, and hybrid pruning.\nUnstructured pruning operates at a fine granularity, remov-\ning arbitrary \"redundant\" parameters in the network. However,\nthis method may result in irregular network structures that are\ndifficult to accelerate effectively. LeCun proposed the Optimal\nBrain Damage (OBD) algorithm [39] in the late 1980s, using\nthe second-order derivatives of the loss function to determine"}, {"title": "C. Knowledge Distillation", "content": "Knowledge Distillation (KD) is a model compression tech-\nnique that transfers knowledge from a complex model (called\nthe teacher model) to a smaller model (called the student\nmodel). This allows the student model to maintain high\ncomputational efficiency while achieving the performance of\nthe teacher model. Knowledge distillation was first proposed\nby Bucilu\u0103 et al., who trained compressed models with pseudo-\ndata classifiers to replicate the original classifier's outputs [14].\nKD can be divided into homomorphic KD and heteromorphic\nKD.\nHomomorphic KD means the student and teacher models\nhave similar or identical structures. In this approach, the\nstudent model learns by mimicking the teacher model's outputs\n(e.g., logits, feature layer outputs). Common homomorphic\nKD methods include logit-level distillation, feature-level dis-\ntillation, and module-level distillation. For instance, Tiny ViT\n[46] applies distillation during pre-training, storing logits from\na large teacher model on hardware to achieve memory and\ncomputational efficiency when transferring knowledge to a\nsmaller student Transformer. DeiT-Tiny [47] adopts patch-\nlevel distillation, training a small student model to match\nthe pre-trained teacher model's patch structure, then opti-\nmizing with decomposed manifold matching loss to reduce\ncomputational costs. Module-level methods like m2mKD [48]\nseparate the teacher module from a pre-trained unified model,\ncombining student modules with modular models, and using a\nshared meta-model for composition, enabling student modules\nto mimic teacher module behavior. Feature-level distillation\nmethods like MiniViT [49] combine weights from consecutive\nTransformer blocks for cross-layer weight sharing, introducing\ntransformations to enhance learning.\nHeteromorphic KD refers to student and teacher models\nwith different structures. In this approach, the student model\nlearns by mimicking the teacher model's outputs or intermedi-\nate features, despite different architectures. Heteromorphic KD\nenhances the student model's adaptability, enabling it to learn\nuseful information from the teacher model. Heteromorphic KD\nincludes soft label distillation, where the student model trains\nby mimicking the teacher model's soft label outputs.\nKD transfers knowledge from complex models to smaller\nmodels, achieving model compression and acceleration. Both\nhomomorphic and heteromorphic KD train by mimicking the\nteacher model's outputs or features. These methods not only\nimprove student model performance but also reduce com-\nputational and storage costs, enabling deep learning models\nto be widely applied in resource-constrained environments.\nStudies show that models processed by KD can perform well\nin resource-constrained environments such as mobile devices\nand embedded systems, further promoting deep learning tech-\nnology's wide deployment in practical applications."}, {"title": "D. Quantization Techniques", "content": "Quantization techniques convert model parameters from\nhigh-precision floating-point numbers (e.g., 32-bit or 64-bit) to\nlower-precision formats (e.g., 8-bit or 16-bit), reducing compu-\ntational and storage costs [50], [51]. For example, when train-\ning a cat-dog classification model on a laptop, its parameter\nsize might be 64MB. Deploying it on an Arduino Uno using an\nATmega328P microcontroller with 8-bit operations, quantizing\nthe model can reduce the weight storage size to 1/8 of the\noriginal, with negligible accuracy impact (about 1-3%). This\ndemonstrates quantization's significant advantages in reducing\nstorage needs and improving computational efficiency.\nWeights are trainable parameters in neural networks, ad-\njusted during training to minimize the model's loss function,\nenabling the model to learn from data. Each layer's weights\ntransform input features to output features through matrix\nmultiplication. Suppose the input vector is x, weight matrix\nW, and bias vector b, then the neural network layer output\ncan be represented as:\nZ Wx + b\nQuantization techniques convert model parameters from\nhigh-precision floating points to lower-precision formats, ef-\nfectively reducing computational and storage costs. Quantiza-\ntion methods include post-training quantization, quantization-\naware training, and hardware-aware quantization."}, {"title": "E. Synthetic Data Techniques", "content": "Synthetic data techniques generate data similar to real data\nbut without containing real personal information, expanding\ntraining datasets to improve model generalization and robust-\nness [52], [53]. In large model training, pure text synthesis\nis mostly done through other large models, while image\nsynthesis mainly uses generative models. Synthesized data\noften needs to be validated through statistical methods to\nensure conformity with real data distribution.\nStatistical methods generate synthetic data by performing\nstatistical analysis and modeling on real data, then using\nthese models to generate synthetic data. For example, using\nprobability distribution functions to simulate real data charac-\nteristics and distribution to generate synthetic data. Generative\nAdversarial Networks (GANs) are deep learning techniques\nused to generate realistic synthetic data. GANs consist of a\ngenerator and a discriminator, where the generator produces\nsynthetic data, and the discriminator distinguishes between real\nand synthetic data. Through continuous adversarial training,\nthe generator and discriminator compete, ultimately generating\nhigh-quality synthetic data. GANs have wide applications in\nmedical imaging, facial recognition, and autonomous driving.\nVariational Autoencoders (VAEs) are generative models that\nlearn data latent representations to generate synthetic data sim-\nilar to real data distribution. VAEs are particularly suitable for\nimage generation tasks, performing well in generating high-\nquality, realistic images. Sequence models generate synthetic\ndata for sequence data (e.g., text, time series) through models\nsuch as Markov Chains, Recurrent Neural Networks (RNNs),\nand Variational Autoencoders (VAEs), modeling sequence\nfeatures and dependencies to generate synthetic data."}, {"title": "F. Evaluation Techniques for Model Architectures", "content": "Evaluation techniques for model architectures measure and\ncompare the performance of different deep learning models to"}, {"title": "G. Fine-Tuning Techniques for Model Architectures", "content": "Fine-tuning techniques involve further training pre-trained\nmodels on specific task datasets to enhance model performance\nin that task. Below are common fine-tuning techniques and\ntheir recent advancements:\nLORA (Low-Rank Adaptation) [59] is a low-rank adap-\ntation technique that adds low-rank matrices to pre-trained\nmodels for fine-tuning, reducing computational and storage\ncosts while maintaining performance. QLoRA [60] is an\nimproved version that further optimizes the fine-tuning pro-\ncess through quantization techniques. Retrieval-Augmented\nGeneration (RAG) [61] combines information retrieval and\ngenerative models, enhancing generative model performance\nby retrieving relevant information from external data sources.\nThe LangChain [62] library provides various tools allowing\nlarge models to access real-time information from sources like\nGoogle Search, vector databases, or knowledge graphs, further\nimproving RAG effectiveness. LlamaIndex (GPT Index) [63],\n[64] is an integrated data framework designed to enhance large\nlanguage models (LLMs) by enabling the use of private or\ncustom data. LlamaIndex provides data connectors, indexing\nand graph-building mechanisms, and advanced retrieval and\nquery interfaces, simplifying data integration and information\nretrieval processes.\nBy applying these fine-tuning techniques appropriately, pre-\ntrained model knowledge can be fully utilized, improving\nperformance in new tasks while reducing training time and\ncomputational resource consumption."}, {"title": "H. Other Challengers to Model Architectures", "content": "In the field of multimodal large models, the Transformer\narchitecture is widely used for its excellent performance and"}, {"title": "IV. SPECIFIC TECHNIQUES OF MULTIMODAL MODELS", "content": "In the field of multimodal large models (MLM), re-\nsearchers have proposed various architectural techniques to\nachieve and optimize the performance and application of\nmultimodal models. Figure 6 shows a general architecture\ndesigned to handle data from text, vision, and audio modalities.\nIn this architecture, each modality's data is first processed\nthrough its respective encoder (Text Encoder, Vision Encoder,\nAudio Encoder) for feature extraction. The features are then\nnormalized and matched through alignment modules (Text\nAlign, Vision Align, Audio Align), followed by projection\nmodules (Text Projection, Vision Projection, Audio Projection)\nto map the features into a common feature space. Finally,\ndiffusion modules (Text Diffusion, Vision Diffusion, Audio\nDiffusion) further propagate and adjust the features. The large\nlanguage model (LLM) integrates these multimodal features\nto handle and generate complex cross-modal tasks.\nThis design allows different modalities of data to be fused\nand processed in a unified feature space, enhancing the un-\nderstanding and generation capabilities of multimodal data.\nSpecialized modules for encoding, alignment, projection, and\ndiffusion enable the LLM to efficiently process and integrate\ntext, vision, and audio data, thus improving overall model\nperformance and applicability.\nEnd-to-end learning is a crucial training strategy for multi-\nmodal large models, where the entire model is optimized as a\nwhole, rather than in stages. Compared to stage-wise training,\nend-to-end learning eliminates intermediate data processing\nand model design at each step. However, end-to-end learning\nfor multimodal large models has three major drawbacks.\nThe two biggest drawbacks are the requirement for large\namounts of data and computing power. Direct end-to-end\nlearning necessitates vast multimodal datasets and computa-\ntional resources. For example, OpenAI used approximately\n2.15e25 FLOPS, about 25,000 A100 GPUs, training for 90\nto 100 days, with an efficiency (MFU) of about 32% to 36%\nfor GPT-4 training, which included about 1.3 trillion tokens.\nFor full multimodal training, these requirements would at least\ndouble.\nThe final drawback is the difficulty in establishing complex\nrelationships. Manually designed modules often inject human\nprior knowledge, such as encoders, decoders, alignment layers,\netc., which can simplify models. For instance, if we aim\nto detect micro-expressions through video, the model design\ntypically involves keyframe selection, face cropping, facial ac-\ntion unit recognition, combined with micro-expression theory\nand statistics. An end-to-end model directly establishing con-\nnections between images and micro-expressions is evidently\nchallenging and complex.\nGiven these challenges, most multimodal large models do\nnot entirely use end-to-end training. Figure 7 shows two train-\ning strategies used in large model training. The left side shows\nthe Cold Start Training strategy, where the model trains from\nscratch. It starts with encoding data from different modalities\nusing text, vision, and audio encoders, followed by feature\npropagation through diffusion modules (Text Diffusion, Vision\nDiffusion, Audio Diffusion), then integrates them using a large\nlanguage model (LLM), and finally projects features through\nprojection modules (Text Projection, Vision Projection, Au-\ndio Projection) to generate output. The process emphasizes\ngradually expanding and adjusting features, ensuring effective\nintegration and processing of multimodal data.\nThe right side shows the Warm Start Training strategy,\nwhere the model starts with some pre-training. The pre-trained\nLLM directly processes input data through projection mod-\nules (Text Projection, Vision Projection, Audio Projection),\ngenerates initial features, and refines them through diffusion\nmodules (Text Diffusion, Vision Diffusion, Audio Diffusion).\nCompared to cold start, warm start leverages existing knowl-\nedge from pre-trained models, improving training efficiency\nand initial performance, suitable for scenarios with relevant\ndomain knowledge or foundational models. This approach\nenables models to quickly adapt to new tasks and exhibit high\nperformance early in training."}, {"title": "2) General Multimodal Encoders:", "content": "In terms of vision en-\ncoders, consistent with mainstream MLM practices, the pre-\ntrained CLIP model is usually chosen for visual encoding\nbecause it effectively aligns the feature spaces of visual and\ntextual inputs. Given the relatively small proportion of visual\nencoders in MLM parameters, lightweight optimization is\nless critical compared to language models. By combining\nmultiple visual encoders, a broad range of visual repre-\nsentations can be captured, enhancing model understanding.\\For example, Cobra [65] integrates DINOv2 and SigLIP as\nits visual backbone, combining DINOv2's low-level spatial\nfeatures with SigLIP's semantic attributes. SPHINX-X [66]\nuses two visual encoders, DINOv2 and CLIP-ConvNeXt, pre-\ntrained with different methods and architectures to provide"}, {"title": "3) General Multimodal Generative Models:", "content": "The generative\nprocess of models can be described as transforming latent sam-\nples z extracted from a prior distribution pz(z) into samples\nx' consistent with the target data distribution Pdata(x). Specif-\nically, the latent variables z are passed through a parameter\nfunction, usually implemented as a neural network, learning\nto map the prior distribution to the target data distribution."}, {"title": "B. Multimodal Optimization Techniques", "content": "Multimodal In-\nstruction Tuning (M-IT) is a technique that fine-tunes models\non instructions or task descriptions containing multimodal\ndata, enhancing their ability to understand and execute mul-\ntimodal tasks. Instruction tuning involves fine-tuning pre-\ntrained language models (LLMs) on datasets organized in an\ninstructional format, improving their generalization to unseen\ntasks [89], [90]. This method has been successfully applied\nin natural language processing models like ChatGPT, Instruct-\nGPT, FLAN [91], and OPT-IML [92].\nTraditional supervised fine-tuning relies on large amounts\nof task-specific data, while prompting methods reduce depen-\ndency on large-scale data through prompt engineering, albeit\nwith limited zero-shot performance. Unlike these methods,\ninstruction tuning emphasizes learning to generalize to unseen"}, {"title": "2) Multimodal In-Context Learning (M-ICL):", "content": "Multimodal\nIn-Context Learning (M-ICL) enhances models' understanding\nand processing of multimodal data by providing multimodal\ncontextual information during training or inference [93], [94].\nIn-Context Learning (ICL) is an important and emerging\ncapability of large language models (LLMs) [95]. ICL achieves\nfew-shot learning and complex task resolution through anal-\nogy learning, differing from traditional supervised learning\nparadigms that require large amounts of data to learn implicit\npatterns. In ICL settings, LLMs learn from few examples and\noptional instructions, generalizing to new problems to solve\ncomplex and unseen tasks. ICL is training-free and can flexibly\nintegrate into different frameworks' inference stages.\nIn the context of multimodal large models (MLMs), ICL\nextends to more modalities, forming Multimodal In-Context\nLearning (M-ICL). During inference, M-ICL can be achieved\nby adding a demonstration set (a set of context samples) to the\noriginal samples. Specifically, the difference between M-ICL\nand M-IT lies in constructing datasets with multimodal input-\noutput information, which is contextually related information\nrather than the expected model response. Through instructions\nand provided demonstrations, LLMs understand task goals and\noutput templates, generating expected answers. In scenarios\nteaching LLMs to use external tools, examples usually contain\nonly text information and are more detailed. These examples\nconsist of sequential steps to complete specific tasks, closely\nrelated to Chain of Thought (CoT). Combining these tech-\nniques, M-ICL extends models' capabilities to handle multi-\nmodal tasks and enhances their generalization and adaptability\nin various application scenarios."}, {"title": "3) Multimodal Chain of Thought (M-COT):", "content": "Large Lan-\nguage Models (LLMs) have demonstrated impressive perfor-\nmance in complex reasoning, particularly by using Chain of\nThought (CoT) prompts to generate intermediate reasoning\nchains to infer answers [4], [96]. However, existing CoT re-\nsearch primarily focuses on the language modality. Multimodal\nChain of Thought (M-COT) is a method that enables models\nto perform complex reasoning and decision-making through\nstep-by-step derivation and coherent thinking. As noted in\nprevious work, CoT is \"a series of intermediate reasoning\nsteps,\" proven effective in complex reasoning tasks. The core"}, {"title": "V. OVERVIEW OF MULTIMODAL MODELS", "content": "In this chapter, we will introduce the generation and basic\nmodels that are more influential in the multimodal field. Since\nmost of the models in multimodal generation and underlying\nmodels are closed sources, we will not going to make too\nmany statements here."}, {"title": "A. Multimodal Generative Models", "content": "Multimodal models have demonstrated significant potential\nand application prospects in processing and understanding data\nfrom different modalities. By analyzing existing multimodal\nmodels, it is evident that they have made remarkable progress\nin generating images, videos, audio, and 3D models. These\nmodels achieve cross-modal generation and transformation by\nhandling data from various modalities such as text, images,\nvideos, or audio.\nFor example, the DALL-E series models (including DALL-\nE [98], DALL-E 2 [82], DALL-E 3 [99]) developed by OpenAI\nsince 2021 have evolved to generate high-quality images based\non textual descriptions. DALL-E models leverage the power\nof large-scale datasets and transformer architectures to under-\nstand and generate highly detailed and creative images from\ntextual inputs. Each iteration has improved on the previous,\nwith DALL-E 3 incorporating more advanced techniques to\nproduce even more coherent and higher-quality images.\nMidjourney [100], launched in 2022, focuses on high-\nquality artistic image generation and is widely used in creative\ndesign. This model emphasizes generating visually appealing\nand artistically valuable images, making it popular among\nartists and designers.\nGoogle's Imagen and Imagen 2 [79], introduced in 2022 and\n2023, respectively, have further enhanced image generation ca-\npabilities by including editing features. Imagen models allow\nusers to make modifications to generated images, making the\nprocess of creating and refining images more interactive and\nuser-friendly."}, {"title": "B. Multimodal Foundation Models", "content": "Multimodal foundation models focus on understanding and\naligning multimodal data, enabling more complex reasoning\nand task execution. DeepMind's Flamingo, launched in 2022,\nunderstands image content and answers related questions,\nexcelling in visual question answering tasks. InstructBLIP\nand Blip-2, introduced between 2022 and 2023, improved vi-\nsual question answering performance through efficient visual-\nlanguage pre-training.\nDeepMind's Flamingo [104], launched in 2022, understands\nimage content and answers related questions, excelling in\nvisual question answering tasks. This model is designed to\nbridge the gap between visual inputs and textual queries,\nproviding accurate and contextually relevant answers.\nInstructBLIP and Blip-2 [105], [106], introduced between\n2022 and 2023, improved visual question answering perfor-\nmance through efficient visual-language pre-training. These\nmodels leverage large-scale pre-training to enhance their abil-\nity to understand and respond to visual queries, making them\nmore effective in various visual question answering scenarios.\nAlibaba DAMO Academy's video understanding models,\nVideoLlama and VideoLlama 2 [107], [108], released in 2023\nand 2024 respectively, can understand video content and per-\nform tasks such as video subtitle generation and video-audio\nunderstanding. These models are designed to process and\ncomprehend complex video data, enabling more sophisticated\nvideo analysis and understanding tasks.\nMultimodal models like MiniGPT4 [109], mPLUG-Owl"}, {"title": "VI. FROM MULTIMODAL MODELS TO WORLD MODELS", "content": "Based on current technology, there are two main approaches\nto constructing a world model from a multimodal model. The\nfirst approach relies on rule-based methods and requires only\na small amount of data. The second approach, exemplified by\nOpenAI, involves the use of large datasets. In the following\nsections, we will introduce these two approaches and explore\ntheir potential applicability."}, {"title": "A. 3D Generation and Rule Constraints", "content": "3D generation is an essential area in multimodal generation,\nleading towards world simulators through models that generate\nrealistic 3D models and incorporate rule constraints in the\ngeneration process to create highly realistic and controllable\nvirtual environments, akin to the metaverse.\n3D generation techniques mainly include explicit repre-\nsentation, implicit representation, and hybrid representation.\nExplicit representation includes point clouds and meshes,\nwhich generate 3D models by precisely describing the ge-\nometry of objects. Implicit representations, such as Neural\nRadiance Fields (NeRF) [113] and implicit surfaces, generate\nhigh-quality 3D content by learning latent representations of\ndata. Hybrid representations combine explicit and implicit\nfeatures, retaining geometric details while offering flexible\nrepresentation capabilities.\nSpecific generation methods include Generative Adversarial\nNetworks (GANs), diffusion models, autoregressive models,\nVariational Autoencoders (VAEs), and normalizing flows.\nThese methods generate realistic 3D data through various\nmechanisms. For example, GANs generate high-quality 3D\nmodels through adversarial training between generators and\ndiscriminators; diffusion models generate new samples by\nsimulating the diffusion process of data; autoregressive models\ngenerate 3D objects by progressively predicting the conditional\nprobability of each element; VAEs generate data by learning\nlatent representations of input data; normalizing flows use a\nseries of reversible transformations to map simple distributions\nto data distributions for data generation [114].\nOptimization-based generation methods use optimization\ntechniques to generate 3D models at runtime, often combining"}, {"title": "B. More Modal Information Leading to Embodied Intelligence", "content": "Another approach to achieving world simulators is through\nembodied intelligence models. Current multimodal models\ncover daily information media such as images, text, and au-\ndio. However, developing embodied intelligent robots requires\nexpanding these modal information to include coordinate\nsystems, point clouds, and depth, which are crucial for robots\nto understand and operate in the real world. Integrating these\nadditional modal information into multimodal models can\nachieve preliminary embodied intelligent robots [119], [120].\nEmbodied intelligence involves robots perceiving, under-\nstanding, and acting in the physical world. To achieve this,\nrobots need to process and understand data from multiple\nsensors, such as cameras, LiDAR, and depth sensors [121],\n[122]. These sensors provide information, including coordinate\nsystems, point clouds, and depth maps, enabling robots to\nconstruct and understand detailed 3D representations of their\nsurroundings. With this modal information, robots can navi-\ngate, recognize objects, and perform tasks in the real world.\nBy deploying robots in real life, sufficient multimodal\ninformation can be collected to achieve comprehensive data\ncollection of the real world. The performance of embodied\nintelligence can be further enhanced by combining sensor"}, {"title": "C. Incorporating More External Rule Systems", "content": "In the process of constructing world simulators, incorporat-\ning more external rule systems is a crucial approach. Humans\nrely on mathematical, physical, chemical, and biological tools\nin the objective world, using a series of theorems to derive\nand predict the outcomes of events that have not yet occurred.\nFor example, when we kick a ball, it will fly in an arc. These\npredictions based on physical laws help us understand and\noperate the real world.\nSimilarly, rule systems can help models achieve state mem-\nory and feedback. Suppose a flood breaks the dam; the model\nneeds to infer the subsequent flood state based on rules.\nThese rules stem from human common sense and theorem\nlibraries, summarized from long-term practice and experience.\nBy injecting these conclusions into the model, the model can\ninfer reasonable results with fewer data.\nIn constructing multimodal large models, integrating exter-\nnal rule systems can significantly enhance model understand-\ning and reasoning capabilities. For example, using mathemati-\ncal theorems, the model can accurately calculate the trajectory\nof objects; using physical laws, the model can predict complex\nenvironmental changes; using biological knowledge, the model\ncan simulate dynamic changes in ecosystems. These rule\nsystems provide the model with a framework, allowing it to\nmore accurately simulate the real world.\nIn practical applications, embodied intelligent robots can\nbenefit from these rule systems. When robots collect vast\namounts of multimodal data in real life, these data will com-\nbine with injected rule systems to enhance robots' prediction\nand decision-making abilities. For example, when a robot\ndetects rising water levels, it can predict potential flood ranges\nand impacts based on physical and geographical knowledge\nand take corresponding actions.\nBy incorporating these external rule systems, multimodal\nlarge models can excel in various application scenarios and\nachieve more complex and detailed tasks. This approach not\nonly enhances model intelligence but also provides a more\nsolid foundation for future development."}, {"title": "VII. DISCUSSION", "content": "Currently, the development of multimodal large models\n(MLMs) is still in its early stages, with many challenges and\nresearch questions in both related technologies and specific\napplications.\nThe perception capabilities of existing MLMs are limited,\nleading to incomplete or incorrect visual information, further\ncausing subsequent reasoning errors. This situation may result\nfrom the compromise between information capacity and com-\nputational burden in current models. For example, lowering\nimage resolution and simplifying feature extraction may lead\nto information loss, affecting the model's overall performance.\nThe reasoning chain of MLMs is fragile, especially when\nhandling complex multimodal reasoning problems. Even sim-\nple tasks sometimes result in incorrect answers due to broken\nreasoning chains. This indicates that there is still room for\nimprovement in models' understanding and linking of different\nmodal information, requiring more stable and coherent reason-\ning mechanisms to enhance accuracy and reliability.\nThe instruction compliance of MLMs needs further im-\nprovement. Even after instruction fine-tuning, some MLMs\nstill fail to output expected answers for relatively simple\ninstructions. This suggests that current fine-tuning methods\nand datasets have not fully covered the various instruction sce-\nnarios required by models, necessitating further optimization\nand expansion of training data.\nThe issue of object hallucination is prevalent, where MLM\noutputs responses that do not match the image content, fab-\nricating objects. This not only affects MLM reliability but\nalso reveals deficiencies in visual understanding and semantic\ngeneration. Solving this issue requires more precise visual-\nsemantic alignment and verification mechanisms.\nEfficient parameter training is another urgent issue. Due\nto the large capacity of MLMs, efficient parameter training\nmethods can unlock more MLM capabilities under limited\ncomputational resources. For example, introducing more effec-\ntive training strategies and hardware acceleration can signifi-\ncantly reduce model training time and resource consumption,\nenhancing model application potential.\nCurrently, there is no truly unified multimodal large model.\nAlthough GPT-40 might become the first, significant progress\nis yet to be seen. This indicates that many technical chal-\nlenges need to be solved before achieving a truly unified\nmultimodal world simulator. Whether through extensive data\ntraining by OpenAI or hierarchical planning with limited data\nproposed by Meta, or introducing more rules and knowledge\nbases as mentioned in this paper, these are feasible routes\nto world models. Fundamentally, extensive data simulate the\ninformation humans have encountered since the beginning of\ncivilization, while introducing rules with limited data simulates\nthe rapid learning of descendants using ancestors' summarized\nexperiences and theorems. Both approaches are intuitively\nreasonable. However, the core issue to be solved currently lies\nat the micro-level, especially in simplifying attention mech-\nanisms and adapting GPUs to linear attention mechanisms,\nwhich can significantly enhance model training efficiency. By\ndeploying edge devices and embodied intelligence to collect\ndata quickly, the arrival of world models is not far off."}, {"title": "VIII. SUMMARY", "content": "This work provides a comprehensive overview of the devel-\nopment and challenges of Multimodal Large Models (MLMs),\nhighlighting their potential in advancing artificial general\nintelligence and world models. It meticulously covers key\ntechniques such as Multimodal Chain of Thought (M-COT),\nMultimodal Instruction Tuning (M-IT), and Multimodal In-\nContext Learning (M-ICL), as well as the integration of 3D"}]}