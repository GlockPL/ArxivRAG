{"title": "From Efficient Multimodal Models to World Models: A Survey", "authors": ["Xinji Mai", "Zeng Tao", "Junxiong Lin", "Haoran Wang", "Yang Chang", "Yanlan Kang", "Yan Wang", "Wenqiang Zhang"], "abstract": "Multimodal Large Models (MLMs) are becoming a significant research focus, combining powerful large language models with multimodal learning to perform complex tasks across different data modalities. This review explores the latest developments and challenges in MLMs, emphasizing their potential in achieving artificial general intelligence and as a pathway to world models. We provide an overview of key techniques such as Multimodal Chain of Thought (M-COT), Multimodal Instruction Tuning (M-IT), and Multimodal In-Context Learning (M-ICL). Additionally, we discuss both the fundamental and specific technologies of multimodal models, highlighting their applications, input/output modalities, and design characteristics. Despite significant advancements, the development of a unified multimodal model remains elusive. We discuss the integration of 3D generation and embodied intelligence to enhance world simulation capabilities and propose incorporating external rule systems for improved reasoning and decision-making. Finally, we outline future research directions to address these challenges and advance the field.", "sections": [{"title": "I. THE DEVELOPMENT STATUS OF MULTIMODAL MODELS AND WORLD MODELS", "content": "World models are currently one of the hottest research directions in the AI field. From OpenAI to Meta, major AI companies are striving to develop world models. The concept of world models can be traced back to the fields of reinforcement learning and robotic control. Traditionally, reinforcement learning algorithms rely on agents learning through trial and error in real environments, which is not only costly but sometimes infeasible. To overcome these limitations, researchers began exploring methods for simulation and learning within internal environments. Jurgen et al. [1] have described a method for quickly training through unsupervised environments using generative recurrent neural networks (RNN) to compress spatiotemporal representations and simulate common reinforcement learning environments. Jurgen et al. referred to this as a world model. In AI research, the proposal of world models aims to distinguish this direction from another research focus: agents.\nWorld models gained widespread attention thanks to Yann LeCun's work [2]. Yann LeCun mentioned that human or animal brains seem to run a simulation of the world, which he called a world model. This model helps humans and animals make predictions about their surroundings. LeCun provided an example: a baby learns basic knowledge by observing the world in the first few months after birth, such as understanding gravity when seeing an object fall. This ability to predict what will happen next comes from common sense, which LeCun believes is the essence of intelligence. The Sora model is a model developed by OpenAI for generating video. It utilizes multimodal learning techniques to generate realistic video content by combining text and image data. In recent studies, OpenAI defined Sora in their report as a world simulator capable of generating videos and considered Sora's technology a promising approach to building a general world model. We will introduce the differences between the two main routes currently being explored: the multimodal large\nmodels developed by Meta under Yann LeCun's guidance and OpenAI's GPT series.\nIn summary, we can clearly define a world model as shown in Figure 1. A world model refers to a model that can predict and simulate environmental state changes by learning from various data in the environment. Unlike conventional data testing scenarios where data does not change, a world model's data can change independently, even generating data not in the test dataset. The core function of a world model lies in counterfactual reasoning, which allows it to infer the outcomes of decisions not encountered before. AI researchers' pursuit of world models aims to achieve this counterfactual reasoning, a natural ability of humans that current AI lacks."}, {"title": "B. Routes to World Models", "content": "Currently, there are two main routes to developing world models: autoregressive methods and JEPA (Joint Embed-ding Predictive Architecture) methods. Autoregressive mod-els hold a significant place in the generative field, with notable representatives including the GPT series and Sora. These models, based on Transformer architecture [3], gen-erate data step-by-step, with each output depending on the previous hidden state. This incremental generation allows the model to capture contextual information, producing coherent and logical sequences. The autoregressive model possesses a robust contextual understanding capability and is facile to train, thus establishing itself as the predominant approach in the field of world modeling. By capitalizing on previously generated content during generation, autoregressive models demonstrate adeptness in comprehending and maintaining contextual consistency, thereby yielding more coherent and meaningful output. The training process for autoregressive models is relatively straightforward, involving step-by-step prediction and optimization based on known sequential data, which contributes to their commendable performance when trained on large-scale datasets. While autoregressive models excel in natural language processing tasks, generating high-quality text paragraphs through pre-training and fine-tuning, critics argue that these models lack real-world common sense, obscured by vast amounts of information. For instance, a baby learns how the world works and can predict outcomes with little practice compared to the extensive training data required for large language models.\nIn response, Meta proposed the JEPA framework. The core idea of JEPA is hierarchical planning, a method of decision-making and control that is especially suited to handling com-plex tasks and large-scale problems. This approach involves breaking down problems into multiple levels, each addressing sub-tasks at different levels of abstraction, simplifying the overall problem-solving process. LeCun illustrated this with an example: to travel from New York to Beijing, one must first get to the airport, then take a flight to Beijing, with the overall cost function representing the distance from New York to Beijing. Solving this involves decomposing the task into millisecond-level control, finding the action sequence that minimizes the predicted cost. He believes all complex tasks can be accomplished through such hierarchical methods, with hierarchical planning being the most significant challenge.\nJEPA models extract abstract representations of the world state through a series of encoders and use different levels of world model predictors to forecast various states at different time scales. Inspired by the human brain's ability to understand and react to the environment in a hierarchical manner, JEPA uses a layered architecture to break down complex tasks into multiple levels, each handling sub-tasks at different abstraction levels. This approach enables JEPA to efficiently capture and predict changes in complex dynamic systems, improving the model's handling of long-time-span and multi-scale data. Its unique hierarchical prediction mechanism not only enhances understanding and prediction accuracy of environmental states but also increases adaptability and robustness in dealing with large-scale, diverse data, showcasing significant advantages in many practical applications.\nIn summary, we can summarize the route to the world model into two, rules and data drivers."}, {"title": "C. Multimodal Models", "content": "Regardless of the route to world models, multimodal models are an indispensable part. Multimodal models refer to machine learning models capable of processing and understanding data from different modalities, such as images, text, audio, and video [4], [5]. Human interaction with the real world involves multiple modalities of information, including language, vi-sion, and audio. Therefore, world models must handle and understand multiple forms of data, meaning they must have multimodal understanding capabilities. Additionally, world models simulate dynamic environmental changes to make pre-dictions and decisions, requiring robust multimodal generation capabilities [6]. To put it simply, the world is multimodal, and the world simulator must be able to accept and generate multimodal information. In essence, world models are general-purpose models (General-Purpose Models).\nThe research on multimodal models can be broadly cate-gorized into several technical approaches: alignment, fusion, self-supervision, and noise addition. Alignment-based methods map data from different modalities to a common feature space for unified processing. Fusion methods integrate multimodal data at different model layers to fully utilize information from each modality. Self-supervised techniques pre-train models on unlabeled data, enhancing performance across various tasks. Noise addition enhances model robustness and generalization by introducing noise into the data.\nCombining these techniques allows multimodal models to demonstrate strong capabilities in handling complex real-world data. They can understand and generate multimodal data, simulate and predict environmental changes, and aid agents in making more precise and effective decisions. Thus, multimodal models play a crucial role in developing world models, marking a key step towards general artificial intel-ligence (General AI). The following sections will detail the technical routes of multimodal models."}, {"title": "D. Structure of This Paper", "content": "In Section 2, we introduce the fundamental technologies of basic architectures. In Section 3, we will introduce the"}, {"title": "II. BASIC TECHNIQUES OF MULTIMODAL MODELS", "content": "In this chapter, we will introduce the basic techniques of multimodal models commonly used in two routes, rule-driven and data-driven, from the underlying architecture to the block architecture."}, {"title": "A. Transformers and Their Challengers", "content": "The Transformer architecture is currently one of the most popular deep learning model architectures, especially for nat-ural language processing (NLP) and computer vision (CV) tasks. The Transformer is a deep learning model designed for handling sequential data, employing an attention mech-anism to model long-range dependencies within sequences [4]. Unlike traditional Recurrent Neural Networks (RNNs), the Transformer processes sequences without relying on their order, utilizing self-attention to simultaneously focus on all positions in the sequence, thereby greatly enhancing parallel computation efficiency. The Transformer consists of an en-coder and a decoder, where the encoder maps the input se-quence into a continuous representation space, and the decoder generates the output sequence based on this representation. Each layer of the encoder and decoder includes a multi-head self-attention mechanism and a feed-forward neural network, stabilized by residual connections and layer normalization. Due to its efficient parallel computation and powerful repre-sentation learning capabilities, the Transformer has achieved remarkable success in natural language processing and other tasks requiring sequence data processing. However, the self-attention mechanism of Transformers has high computational complexity when processing long sequences, limiting its effi-ciency in some applications. To address this issue, researchers have proposed various methods to challenge the Transformer architecture. Linear Attention simplifies the computation of self-attention, reducing its time and space complexity from O($N^2$) to O(N). Key models include Performer [7], Lin-former [8], and Linear Transformers [9]. These models can efficiently handle long-sequence data, reducing computational resource consumption. Additionally, Grouped-query Attention and Multi-query Attention are important attention mechanism variants. The former balances multi-head and multi-query attention by sharing a set of keys and values among groups of query heads, while the latter simplifies computation by sharing the same key and value for all query heads, thus improving efficiency. Grouped Query Attention and Multi-query Attention excel in reducing the size of key-value pairs during inference, significantly improving throughput. Multi-Query Attention shares key-value pairs among multiple heads, achieving a 30-40% reduction in throughput. Grouped Query Attention groups queries, sharing key-value pairs within groups, achieving results comparable to Multi-Query Attention in both efficiency and performance. MQA and GQA are used\nin the well-known open-source large language models Llama-2 and Llama-3 [10]\u2013[12].\nAt the block level, optimization methods include Compact Architecture, which reduces layers and parameters for a com-pact model structure, lowering computational costs; Pruning [13], which reduces redundant parameters through pruning techniques, enhancing computational efficiency; Knowledge Distillation [14], which extracts knowledge from a large teacher model and applies it to a smaller student model, significantly reducing model complexity and computational resource requirements; and Quantization [15], which converts model parameters from high-precision floating points to lower-precision formats, further reducing computational and storage costs. These optimization methods collectively aim to enhance the efficiency and performance of Transformers, enabling them to process and integrate data from different modalities more efficiently in multimodal tasks.\nAdditionally, there are architectural approaches challenging the Transformer model, such as Gated Convolution [16] or Gated MLP [17], Recurrent Models [18], State Space Mod-els (SSMs) [19], [20], H3 [21], RWKV [22], Mega [23], Yan, JEPA [24], and the notable Mamba [25] and Mamba 2 [26]. Gated Convolution introduces gating mechanisms in convolutional neural networks (CNNs), enhancing the model's ability to capture local and long-range dependencies while reducing computational load. Recurrent models like LSTM and GRU capture temporal dependencies in sequences through their recursive structures [27], [28], overcoming the vanishing gradient problem in traditional RNNs. State Space Models explicitly model the relationship between system states and observations, providing a flexible framework for handling time-series data, including State Space Models, H3, Mamba and Mamba 2.\nThese approaches not only offer new theoretical insights but also demonstrate their advantages in practice. This chapter will provide detailed introductions to these basic techniques in general architectures, focusing on the most representative and mainstream approaches."}, {"title": "B. Optimization Techniques for Attention Mechanisms", "content": "Multi-head attention mechanisms are a core component of Transformers, capturing dependencies between different positions in the input sequence through parallel computation of multiple attention heads. However, the standard multi-head attention mechanism has high computational complexity, prompting researchers to propose various variants to optimize its performance.\nThe standard Transformer model faces efficiency bot-tlenecks when processing long sequences, with the time and space complexity of its self-attention mechanism being quadratic in sequence length O($n^2$). This issue arises from the Softmax operation in the attention mechanism. As shown in Figure 2, without Softmax, attention computation simplifies to three matrix multiplications, which are associative, allowing the calculation of KTV first, followed by left-multiplying by Q. This reduces complexity from O($n^2$) to linear O(n), the core idea of Linear Attention.\nRemoving Softmax, the attention computation's complexity can drop to linear O(n). Scaled-Dot Attention essentially weights V with QKT. Therefore, a generalized definition of attention can be proposed:\nAttention(Q, K, V) = f(QKT)V\nHere, f is a general function, approximating the Softmax operation. To fit Softmax, f must ensure non-negativity f \u2265 0. This generalized attention form is known as Non-Local Net-works in computer vision.\nIf elements of Q and K are non-negative, their dot product is naturally non-negative. This suggests introducing kernel functions. By adding a non-negative activation function to Q and K:\nAttention(Q, K, V) = \u03c6(Q)\u03c6(K)TV\nWhere \u03c6 is a non-negative activation function, such as ReLU(x). This method, termed the kernel method, is discussed by A Katharopoulos et al [9]. Performers estimate conven-tional (softmax) full-rank attention Transformers using linear space and time complexity while retaining provable accuracy, without relying on sparsity or low-rank priors.\nAnother approach utilizes Softmax's properties. In Efficient Attention [29], Q normalized along dimension d and K along dimension n naturally satisfy normalization conditions. Thus, Softmax is applied separately to Q and K:\nAttention(Q, K, V) = softmax(Q) \u00b7 softmax(K)TV\nThis form is a special case of the generalized attention definition. Additionally, sparse attention methods [30], such as Sparse Attention by OpenAI [31], reduce computation by\nretaining values in local regions only, forcing most attention values to zero. After special design, the non-zero elements of the attention matrix are O(n), achieving linear-level attention.\nReformer [32], another notable improvement, reduces at-tention complexity to O(nlogn) by using locality-sensitive hashing (LSH) to find the largest attention values and compute only those, achieving sparse attention. Moreover, Reformer redesigns the backward propagation process by constructing reversible feed-forward networks (FFN), reducing memory usage. Despite solving sparse attention's first drawback, Re-former remains complex, especially LSH-based attention and reversible network backward propagation.\nPerformers [7] adopt a novel fast attention method, Fast At-tention Via Positive Orthogonal Random features (FAVOR+):\nAtt(Q, K, V) = D-1AV,\nA = exp(\\frac{QKT}{\\sqrt{d}})\nD = diag (A1L)\nEquivalent to the above attention, the scaling factor $\\sqrt{dk}$ simplifies A. Fast attention (FA) maps Q and K through a $\\phi$ function to Q' and K', approximating A as their product:\nA = exp(QKT) \u2248 \u03c6(Q)\u03c6(K)T = Q'K'\n\u03c6 maps matrix row vectors. FAVOR+ simulates beyond softmax other kernelizable attention mechanisms effectively. This capability is crucial for accurately comparing softmax with other kernels on large-scale tasks, aiding in finding optimal attention kernels. Performers are fully compatible with conventional Transformers, offering strong theoretical guaran-tees: unbiased or nearly unbiased attention matrix estimation, unified convergence, and low estimation variance.\nLinformer [8] improves self-attention with low-rank matrix approximation, reducing complexity to linear O(n). Linformer retains the original Scaled-Dot Attention form but projects Q and K to low-dimensional space using n\u00d7k matrices before attention, reducing computation. While Linformer excels in some tasks, its performance on long-sequence tasks remains to be verified. Moreover, Linformer faces challenges in au-toregressive generation tasks due to the projection process combining entire sequence information, complicating causal masking.\nMulti-Query Attention and Group-Query Attention are no-table variants. These methods optimize the attention computa-tion process, reducing computational complexity and memory consumption while maintaining or enhancing model perfor-mance. Multi-Query Attention (MQA) shares keys and values among all attention heads, computing independent queries for each head, thus lowering complexity and memory usage. In MQA, all attention heads share the same key and value, differing only in queries:\nQi = QW, K = KWK, V = VWV\nAttention (Qi, K, V) = softmax(\\frac{QiKT}{\\sqrt{dk}})V"}, {"title": "III. OPTIMIZATION TECHNIQUES FOR MODEL ARCHITECTURES", "content": "In the research of multimodal methods, addressing issues such as excessive parameters in Transformer models, re-searchers have proposed various optimizations and improve-ments to enhance model efficiency, reduce computational complexity, and improve performance. This section provides a detailed introduction to these improvements in model archi-tectures."}, {"title": "A. Model Compression", "content": "Model compression aims to reduce the number of parame-ters and computational load in deep neural networks, enhanc-ing efficiency and reducing storage requirements. Pre-trained deep neural network models often face over-parameterization, where only about 5% of the parameters are effective. Model compression techniques include frontend and backend com-pression, aiming to shrink model size without significantly reducing accuracy, thus improving usability and efficiency in practical applications.\nFrontend compression methods include knowledge distil-lation, compact model structure design, and filter pruning. Knowledge distillation transfers knowledge from a complex model to a smaller one, allowing the small model to maintain high computational efficiency while achieving the perfor-mance of the complex model. For example, Hinton et al. [35] proposed knowledge distillation techniques that transfer teacher model knowledge through softened output probability distributions. Compact model structure design improves the convolution method of neural networks (e.g., using depthwise separable convolutions) to reduce computational parameters. MobileNet [36] is a successful example in this aspect. Filter pruning removes unimportant weight matrices to reduce model redundancy.\nBackend compression methods include low-rank approx-imation and unrestricted pruning. Low-rank approximation reconstructs large weight matrices with several low-rank ma-trices, reducing storage and computational resource consump-tion. For example, Singular Value Decomposition (SVD) [37] is widely used for matrix decomposition to achieve compres-sion. Unrestricted pruning includes unstructured pruning and structured pruning. Unstructured pruning removes individual weights.\nIn addition to these traditional compression methods, Han Song's team proposed AutoML Model Compression (AMC) [38], utilizing reinforcement learning to automatically search for model compression strategies, enhancing the efficiency of deploying neural network models on mobile devices. AMC uses reinforcement learning to intelligently balance model size, speed, and accuracy, automatically generating optimal compression strategies more efficiently and effectively than manually crafted heuristic rules.\nThese model compression techniques improve computa-tional and storage efficiency, allowing deep neural networks to be widely applied in resource-constrained environments."}, {"title": "B. Model Pruning", "content": "Pruning techniques remove redundant parameters and con-nections in a model to enhance computational efficiency and reduce model size. Pruning techniques can be categorized into unstructured pruning, structured pruning, and hybrid pruning.\nUnstructured pruning operates at a fine granularity, remov-ing arbitrary \"redundant\" parameters in the network. However, this method may result in irregular network structures that are difficult to accelerate effectively. LeCun proposed the Optimal Brain Damage (OBD) algorithm [39] in the late 1980s, using the second-order derivatives of the loss function to determine"}, {"title": "C. Knowledge Distillation", "content": "Knowledge Distillation (KD) is a model compression tech-nique that transfers knowledge from a complex model (called the teacher model) to a smaller model (called the student model). This allows the student model to maintain high computational efficiency while achieving the performance of the teacher model. Knowledge distillation was first proposed by Bucilu\u0103 et al., who trained compressed models with pseudo-data classifiers to replicate the original classifier's outputs [14]. KD can be divided into homomorphic KD and heteromorphic KD.\nHomomorphic KD means the student and teacher models have similar or identical structures. In this approach, the student model learns by mimicking the teacher model's outputs (e.g., logits, feature layer outputs). Common homomorphic KD methods include logit-level distillation, feature-level dis-tillation, and module-level distillation. For instance, Tiny ViT [46] applies distillation during pre-training, storing logits from a large teacher model on hardware to achieve memory and computational efficiency when transferring knowledge to a smaller student Transformer. DeiT-Tiny [47] adopts patch-level distillation, training a small student model to match"}, {"title": "D. Quantization Techniques", "content": "Quantization techniques convert model parameters from high-precision floating-point numbers (e.g., 32-bit or 64-bit) to lower-precision formats (e.g., 8-bit or 16-bit), reducing compu-tational and storage costs [50], [51]. For example, when train-ing a cat-dog classification model on a laptop, its parameter size might be 64MB. Deploying it on an Arduino Uno using an ATmega328P microcontroller with 8-bit operations, quantizing the model can reduce the weight storage size to 1/8 of the original, with negligible accuracy impact (about 1-3%). This demonstrates quantization's significant advantages in reducing storage needs and improving computational efficiency.\nWeights are trainable parameters in neural networks, ad-justed during training to minimize the model's loss function, enabling the model to learn from data. Each layer's weights transform input features to output features through matrix multiplication. Suppose the input vector is x, weight matrix W, and bias vector b, then the neural network layer output can be represented as:\nZ Wx + b\nQuantization techniques convert model parameters from high-precision floating points to lower-precision formats, ef-fectively reducing computational and storage costs. Quantiza-tion methods include post-training quantization, quantization-aware training, and hardware-aware quantization."}, {"title": "E. Synthetic Data Techniques", "content": "Synthetic data techniques generate data similar to real data but without containing real personal information, expanding training datasets to improve model generalization and robust-ness [52], [53]. In large model training, pure text synthesis is mostly done through other large models, while image synthesis mainly uses generative models. Synthesized data often needs to be validated through statistical methods to ensure conformity with real data distribution.\nStatistical methods generate synthetic data by performing statistical analysis and modeling on real data, then using these models to generate synthetic data. For example, using probability distribution functions to simulate real data charac-teristics and distribution to generate synthetic data. Generative Adversarial Networks (GANs) are deep learning techniques used to generate realistic synthetic data. GANs consist of a generator and a discriminator, where the generator produces synthetic data, and the discriminator distinguishes between real and synthetic data. Through continuous adversarial training, the generator and discriminator compete, ultimately generating high-quality synthetic data. GANs have wide applications in medical imaging, facial recognition, and autonomous driving. Variational Autoencoders (VAEs) are generative models that learn data latent representations to generate synthetic data sim-ilar to real data distribution. VAEs are particularly suitable for image generation tasks, performing well in generating high-quality, realistic images. Sequence models generate synthetic data for sequence data (e.g., text, time series) through models such as Markov Chains, Recurrent Neural Networks (RNNs), and Variational Autoencoders (VAEs), modeling sequence features and dependencies to generate synthetic data."}, {"title": "F. Evaluation Techniques for Model Architectures", "content": "Evaluation techniques for model architectures measure and compare the performance of different deep learning models to\nselect the best architecture. Evaluation methods can be divided into manual and automatic evaluations.\nManual evaluation involves experts or users assessing model outputs, suitable for tasks with strong subjectivity, such as the quality of generated text or the realism of generated images. However, manual evaluation is inefficient, costly, and difficult to scale.\nAutomatic evaluation measures model performance by com-puting various performance metrics, offering high efficiency and repeatability. Common automatic evaluation platforms and tools include Prompt Flow in Microsoft Azure AI Studio, Weights Biases combined with LangChain, LangSmith [54] in LangChain, DeepEval [] in Confidence-ai, and TruEra [55]. These platforms and tools provide various evaluation methods, such as rule-based and model-based evaluations.\nRule-based evaluation methods use predefined rules and metrics (e.g., accuracy, precision, recall, F1 score, ROC-AUC curve) to assess model performance. For example, datasets like MMLU [56], TriviaQA [57], and HumanEval [58] are widely used to evaluate language model understanding and generation capabilities. Model-based evaluation methods use pre-trained referee models (e.g., GPT-4, Claude) or adversarial evaluation (e.g., LLM Peer-examination) to assess model performance. These methods comprehensively evaluate model performance on complex tasks and multimodal data."}, {"title": "G. Fine-Tuning Techniques for Model Architectures", "content": "Fine-tuning techniques involve further training pre-trained models on specific task datasets to enhance model performance in that task. Below are common fine-tuning techniques and their recent advancements:\nLORA (Low-Rank Adaptation) [59] is a low-rank adap-tation technique that adds low-rank matrices to pre-trained models for fine-tuning, reducing computational and storage costs while maintaining performance. QLoRA [60] is an improved version that further optimizes the fine-tuning pro-cess through quantization techniques. Retrieval-Augmented Generation (RAG) [61] combines information retrieval and generative models, enhancing generative model performance by retrieving relevant information from external data sources. The LangChain [62] library provides various tools allowing large models to access real-time information from sources like Google Search, vector databases, or knowledge graphs, further improving RAG effectiveness. LlamaIndex (GPT Index) [63], [64] is an integrated data framework designed to enhance large language models (LLMs) by enabling the use of private or custom data. LlamaIndex provides data connectors, indexing and graph-building mechanisms, and advanced retrieval and query interfaces, simplifying data integration and information retrieval processes.\nBy applying these fine-tuning techniques appropriately, pre-trained model knowledge can be fully utilized, improving performance in new tasks while reducing training time and computational resource consumption."}, {"title": "H. Other Challengers to Model Architectures", "content": "In the field of multimodal large models, the Transformer architecture is widely used for its excellent performance and\nflexibility. However, as model size and application demands increase, the Transformer architecture faces challenges in computational complexity and memory bottlenecks. To address these challenges, researchers have proposed various optimiza-tion strategies and alternative architectures to enhance model efficiency and scalability. Besides Transformers, most other challenger architectures originate from recurrent neural net-works (RNNs), including Gated Convolution, Temporal Con-volutional Networks (TCN), RWKV, Mamba, and S4, which replace attention with recurrent structures. This approach uses fixed memory to remember previous information, although it can remember a certain length, achieving longer lengths is challenging. Another approach is improving Transformers, such as linear attention improvements mentioned earlier. Rep-resentative models include Mega, Yan, and JEPA. We will introduce some representative approaches among them.\nThe RWKV model [22] uses linear attention mechanisms, allowing the model to parallelize computations during training and maintain constant computational and memory complex-ity during inference. The RWKV model consists of stacked residual blocks, each containing time-mixing and channel-mixing sub-blocks, using a recurrent structure to leverage past information. The authors trained RWKV models with sizes ranging from 169 million to 14 billion parameters, making it the largest dense RNN trained to date. Experimental results show that RWKV performs comparably to Transformers of similar size, indicating future work can utilize this architecture to create more efficient models. However, RWKV models have some limitations, such as linear attention potentially limiting performance on tasks requiring long-term dependencies.\nThe Mega model [23] introduces sparse attention mech-anisms, zeroing out most elements in the attention matrix and retaining only a few important attention values. This method significantly reduces computational load and memory usage while maintaining predictive performance. Similar to Longformer and Sparse Transformer, the Mega model has unique optimizations in sparse strategies and implementations. By using sparse attention mechanisms, the Mega model greatly reduces computational complexity and memory usage, making it more efficient in handling long-sequence tasks.\nJEPA (Joint Embedding Predictive Architecture) [24] is a novel machine learning model designed to optimize complex tasks and large-scale problem handling through hierarchical decision-making and control methods. The core idea is to decompose problems into multiple layers, each handling sub-tasks at different abstraction levels, simplifying the overall problem-solving process. The concept and research of JEPA are mainly proposed by Yann LeCun's team at Meta, aiming to overcome the limitations of current large language models (LLMs) in handling complex tasks. A representative method is I-JEPA, a non-generative self-supervised learning method that learns highly semantic image representations by predicting representations of different target blocks in the same image from a single context block. This novel architecture combines the strengths of RNNs and Transformers while reducing their limitations.\nMamba and Mamba 2 [25], [26] are key directions of improvement. Mamba is an improvement of SSM. State Space\nModels (SSM) describe dynamic systems and are widely used in control theory, signal processing, and statistical modeling. SSM uses state variables to represent the system's internal state, described by state and output equations. The state equation is:\nh(t+1) = Ah(t) + Bx(t) + w(t)\nwhere h(t) is the state vector at time t, A is the state transition matrix, x(t) is the input vector at time t, B is the input matrix, and w(t) is process noise, usually assumed to be zero-mean Gaussian white noise.\nThe output equation is:\ny(t) = Ch(t) + Dx(t) + v(t)\nwhere y(t) is the output vector at time t, C is the output matrix, D is the direct transmission matrix, and v(t) is mea-surement noise, usually assumed to be zero-mean Gaussian white noise.\nMamba is a Selective State Space Model (SSSM) based on SSM improvements. Figure 4 shows the architectural differ-ences between SSM and Transformer when handling multi-dimensional input data. SSM processes each dimension inde-pendently, with high parallel computing capabilities and linear computational complexity. In contrast, Transformers capture global dependencies through multi-head attention mechanisms but have higher computational complexity.\nH3 architecture [21], a foundational design for homogenized architecture, improves the initial SSM structure, addressing SSM's challenge of remembering long-term data. As shown in Figure 5, researchers merged the previous SSM architectural design H3 with Gated MLP blocks into one block, selec-tively processing input information (Selection Mechanism), simplifying the deep sequential model architecture, forming a simple, homogeneous architecture with selective state spaces"}, {"title": "IV. SPECIFIC TECHNIQUES OF MULTIMODAL MODELS", "content": "General Multimodal Architectures and Training Strate-gies: In the field of multimodal large models (MLM), re-searchers have proposed various architectural techniques to achieve and optimize the performance and application of multimodal models. Figure 6 shows a general architecture designed to handle data from text, vision, and audio modalities. In this architecture, each modality's data is first processed through its respective encoder (Text Encoder, Vision Encoder, Audio Encoder) for feature extraction. The features are then normalized and matched through alignment modules (Text Align, Vision Align, Audio Align), followed by projection modules (Text Projection, Vision Projection, Audio Projection) to map the features into a common feature space. Finally, diffusion modules (Text Diffusion, Vision Diffusion, Audio Diffusion) further propagate and adjust the features. The large language model (LLM) integrates these multimodal features to handle and generate complex cross-modal tasks.\nThis design allows different modalities of data to be fused and processed in a unified feature space, enhancing the un-derstanding and generation capabilities of multimodal data. Specialized modules for encoding, alignment, projection, and diffusion enable the LLM to efficiently process and integrate text, vision, and audio data, thus improving overall model performance and applicability.\nEnd-to-end learning is a crucial training strategy for multi-modal large models, where the entire model is optimized as a whole, rather than in stages. Compared to stage-wise training, end-to-end learning eliminates intermediate data processing and model design at each step. However, end-to-end learning for multimodal large models has three major drawbacks.\nThe two biggest drawbacks are the requirement for large amounts of data and computing power. Direct end-to-end learning necessitates vast multimodal datasets and computa-tional resources. For example, OpenAI used approximately 2.15e25 FLOPS, about 25,000 A100 GPUs, training for 90 to 100 days, with an efficiency (MFU) of about 32% to 36% for GPT-4 training, which included about 1.3 trillion tokens. For full multimodal training, these requirements would at least double.\nThe final drawback is the difficulty in establishing complex relationships. Manually designed modules often inject human prior knowledge, such as encoders, decoders, alignment layers, etc., which can simplify models. For instance, if we aim to detect micro-expressions through video, the model design typically involves keyframe selection, face cropping, facial ac-tion unit recognition, combined with micro-expression theory and statistics. An end-to-end model directly establishing con-nections between images and micro-expressions is evidently challenging and complex.\nGiven these challenges, most multimodal large models do not entirely use end-to-end training. Figure 7 shows two training strategies used in large model training. The left side shows the Cold Start Training strategy, where the model trains from scratch. It starts with encoding data from different modalities using text, vision, and audio encoders, followed by feature propagation through diffusion modules (Text Diffusion, Vision Diffusion, Audio Diffusion), then integrates them using a large language model (LLM), and finally projects features through projection modules (Text Projection, Vision Projection, Au-dio Projection) to generate output. The process emphasizes gradually expanding and adjusting features, ensuring effective integration and processing of multimodal data.\nThe right side shows the Warm Start Training strategy, where the model starts with some pre-training. The pre-trained LLM directly processes input data through projection mod-ules (Text Projection, Vision Projection, Audio Projection), generates initial features, and refines them through diffusion modules (Text Diffusion, Vision Diffusion, Audio Diffusion). Compared to cold start, warm start leverages existing knowl-edge from pre-trained models, improving training efficiency and initial performance, suitable for scenarios with relevant domain knowledge or foundational models. This approach enables models to quickly adapt to new tasks and exhibit high performance early in training."}, {"title": "2) General Multimodal Encoders", "content": "In terms of vision en-coders, consistent with mainstream MLM practices, the pre-trained CLIP model is usually chosen for visual encoding because it effectively aligns the feature spaces of visual and textual inputs. Given the relatively small proportion of visual encoders in MLM parameters, lightweight optimization is less critical compared to language models. By combining multiple visual encoders, a broad range of visual repre-sentations can be captured, enhancing model understanding. For example, Cobra [65"}]}