{"title": "LegalScore: Development of a Benchmark for Evaluating AI Models in Legal Career Exams in Brazil", "authors": ["Roberto Caparroz", "Marcelo Roitman", "Beatriz Graziano Chow", "Caroline Giusti", "Larissa Torhacs", "Pedro Aur\u00e9lio Sola", "Jo\u00e3o Henrique M. Diogo", "Luiza Balby", "Carolina Dolabela L. Vasconcelos", "Leonardo Roberti Caparroz", "Albano Prado Franco"], "abstract": "This research introduces \u2018LegalScore', a specialized index for assessing how generative artificial intelligence models perform in a selected range of career exams that require a legal background in Brazil. The index evaluates fourteen different types of artificial intelligence models' performance, from proprietary to open-source models, in answering objective questions applied to these exams. The research uncovers the response of the models when applying English-trained large language models to Brazilian legal contexts, leading us to reflect on the importance and the need for Brazil-specific training data in generative artificial intelligence models. Performance analysis shows that while proprietary and most known models achieved better results overall, local and smaller models indicated promising performances due to their Brazilian context alignment in training. By establishing an evaluation framework with metrics including accuracy, confidence intervals, and normalized scoring, 'LegalScore' enables systematic assessment of artificial intelligence performance in legal examinations in Brazil. While the study demonstrates artificial intelligence's potential value for exam preparation and question development, it concludes that significant improvements are needed before AI can match human performance in advanced legal assessments. The benchmark creates a foundation for continued research, highlighting the importance of local adaptation in artificial intelligence development.", "sections": [{"title": "1. Introduction", "content": "Brazil has many undergraduate law programs, especially considering a significant increase since 2000 (OAB, 2020). In 2022, it was estimated that there would be approximately four million Bachelors of Laws (OAB, 2022). These professionals have several career options after graduating with a LLB degree, including the public service careers, which is extremely traditional and attractive amongst these graduates.\nThe competition to be able to perform as a civil servant is not easy, since there is a wide range of positions opened every year, varying according to each exams' specifics, including requirements and levels of difficulty. Among Brazil's most traditional and prestigious exams are meant for judicial positions, public prosecution, and other important Federal bodies, such as the Federal Revenue Service. To be able to fill a public position in Brazil aside from the fact that it brings a reasonable status in society, it also ensures good benefits and pay.\nIn addition to these factors, there is another element of attractiveness in competitive examinations: the job security provided by the very nature of public careers. Investing in public jobs and positions ensures consolidated rights and guarantees for those approved (Rodrigues & Caron, 2016), making public service attractive to those seeking professional stability and higher-than-average pay.\nThe interest in legal careers is no coincidence. The Brazilian Constitution promotes and regulates the institute of public examination in art. 37, II, establishing it as the main means of access to public service through specific examinations that assess candidates' performance.\nAs a side effect, the premise of public competitions is creating an institutionalized and meritocratic access system (Araujo, 2020), which has consolidated them as a model deeply rooted in Brazilian culture. Considering the advance of generative artificial intelligence (Generative AI) models, the debate on public exams is taking on new contours, especially regarding the effectiveness of exams as a means of access to coveted careers. One pertinent question concerns the performance of different generative AI models when subjected to the same exams as human candidates. What would be the result obtained by the main models currently available when submitted to Brazil's federal jobs, considering a legal background? Is it feasible to state that AI has the capability to outperform humans in this context?\nThe research proposed in this article summarizes the results of a study conducted over approximately three months. The research analyzed the performance of the leading AI models in various public exams, including the OAB (Brazilian Bar Association) exam, which qualifies Bachelor of Laws to duly practice private law.\nThe aim of the research, in addition to answering the proposed questions, was based on the following aspects: (i) assessing whether the main AI models, both proprietary and open source, are aligned with the Brazilian legal landscape; (ii) investigating whether the so-called language gap (Caparroz, 2024), resulting from the predominant training of models in the English language, plays a significant role in the performance of Al models in competitive exams applied to Brazil federal offices; and (iii) to propose an accuracy index to evaluate the Generative AI models' performance in this context.\nThe core empirical research was based on the performance evaluation of public exams and the Bar Exam of a carefully selected group of Large Language Models (Fan et al., 2023). Generative AI models have attracted the general public's attention in the last two years, especially after the launch of ChatGPT (Tang, 2023).\nOne relevant side note of this project is the potential of these models to serve as complementary tools for studying and solving questions, offering immediate feedback, personalized support, and help in understanding complex concepts (Hadi et al., 2023).\nAnother central point of the research was the analysis of the limitations of LLMs (Wolf et al., 2023). Factors such as the"}, {"title": "2. Existing benchmarks for language models", "content": "The aim of this article is to compare various language models and evaluate their performance in human tasks that do not involve merely logical, ready-made, or, in some cases, mathematical answers. This is because, when performing their tasks, language models access a previously trained data set (Zhao et al., 2023), which serves as the basis for generating the answers presented to the user.\nIn order to improve the performance of models in responses that require understanding and interpretative reasoning, benchmarks have been developed to enable comparative performance analysis (Wang et al., 2024). This data makes it possible not only to assess the reliability of the answers provided but also to identify points for improvement that can be implemented by the developers."}, {"title": "2.1. Legal and non-legal benchmarks", "content": "Al models have often been evaluated using multiple-choice questions, which, in theory, offer an objective criterion for measuring their capabilities. However, several recent studies have demonstrated inconsistencies in this approach (Zhou & Duan, 2024). It is claimed that multiple-choice questions may not be a reliable way of assessing the real capabilities of generative Al models when compared to the production of long texts (Li et al., 2024).\nDespite this warning, for this phase of the research, the analysis of multiple-choice questions applied in legal exams in Brazil proved to be an adequate criterion for comparing the performance of the AI models tested with the results obtained by human candidates.\nIn the second stage, the ability of AI models to answer discursive questions from the same competitions analyzed in the first phase will be evaluated to obtain an overview of the results in all stages of the main competitions.\nSeveral studies have analyzed the performance of Al models on multiple-choice questions applied in various contexts. A study with UBENCH (Benchmarking Uncertainty in Large Language Models) revealed the limitations faced by large-scale language models in solving and executing tasks involving multiple-choice questions. These difficulties are mainly due to the need to carry out semantic analysis and understanding in order to solve questions that are ambiguous or require more elaborate reasoning in order to carry out the requested task (Wang et al., 2024).\nAnother benchmark analyzed was multimodal models, i.e., those that require the interpretation of text, images, and possibly other document formats. In this field, SEED-Bench stands out, which includes more than twenty-four thousand multiple-choice questions on various subjects and areas of knowledge. The aim was to assess how language models promote integration between these modalities (Li et al., 2024).\nThe need to evaluate AI models in specific domains of human knowledge has led to legal benchmarks, such as LawBench (Fei et al., 2023) and LegalBench (Guha et al., 2023).\nLawBench tested 51 AI models in various tasks, such as single-label classification (SLC), multi-label classification (MLC), regression, extraction, and content generation. The results indicated that ChatGPT 4 obtained the best performance in the legal context evaluated, outperforming the others by a wide margin (Fei et al., 2023). LegalBench, on the other hand, tested the legal reasoning capacity of 20 AI models based on tasks prepared by professionals in the field (Guha et al., 2023).\nThe analysis of these and other studies raised a significant concern because these investigations use something other than Portuguese parameters as a data source. In the cases mentioned, the Al models used databases in English and Chinese, which restricts the analysis to the context of the respective countries. In the case of Brazil, it can be seen in various scenarios that even simple questions related to local history or geography, common to the repertoire of elementary school children, are not answered accurately by various AI models due to the lack of datasets representative of the country's reality (Caparroz, 2024)."}, {"title": "2.2. Overview of Brazilian civil service examination", "content": "Unlike many countries, public careers are the dream of a significant part of the Brazilian population for the reasons previously discussed.\nAccording to the census published by the newspaper Folha Dirigida, in cooperation with the website QConcursos (More than 10,000 public exams held in 2023 | Folha Dirigida, [n.d.]). In 2023 alone, more than 10,000 vacancies were offered for public positions in the most diverse sectors of public administration, covering candidates with training from high school to higher education, at all three levels of the federation. It is estimated that in October 2023 alone, more than 2,000 competitive examinations were held.\nThe National Confederation of Industry (CNI), in its 15th Economic Note, entitled \"The weight of the civil service in Brazil compared to other countries\" (CNI, [n.d.]), concluded that although Brazil does not have a high number of civil servants in relation to the proportion of the total population and workforce representing 5.6% - this rate is higher than the average for Latin American countries. However, it is significantly lower than the average for OECD countries, which is 9.6%.\nGiven this scenario, it is undeniable that public examinations are increasingly attracting candidates of all profiles and for the most varied activities, which is also boosting related activities, such as the publishing and preparatory course markets."}, {"title": "3. Methodology", "content": "To make up the initial object of research, the main AI models available to the public were selected in October 2024, including proprietary models from large technology companies and open-source models with more flexible licenses. It should be noted that only multilingual models capable of responding to prompts and questions written in Portuguese were considered.\nThe models tested, in the latest versions available in October 2024, were:\n\u2022 ChatGPT 3.5 Turbo, ChatGPT 01-mini and ChatGPT 40, from OpenAI:\nhttps://www.openai.com/chatgpt\n\u2022 Claude Sonnet 3.5, from Anthropic:\nhttps://www.anthropic.com/claude\n\u2022 Llama 3.1 405B, from Meta:\nhttps://ai.meta.com/llama\n\u2022 Gemini 1.5 Pro, from Google:\nhttps://ai.google.com/gemini\n\u2022 Nemotron 340B from Nvidia:\nhttps://www.nvidia.com/en-us/research/ai/nemotron\n\u2022 Mistral Large-2, from Mistral AI:\nhttps://mistral.ai/models/mistral-large-2\n\u2022 Command-R-Plus from Cohere:\nhttps://cohere.ai/models/command-r-plus\n\u2022 Reka Core 67B, from Reka AI:\nhttps://www.reka.ai/core67b\n\u2022 Perplexity \u0391\u0399:\nhttps://www.perplexity.ai\n\u2022 Grok-2, from X:\nhttps://x.com/grok-2\nFor comparison purposes, two models were added to the list that were trained with data in Portuguese and information about Brazil:\n\u2022 Sabi\u00e1-3, developed by Maritaca AI:\nhttps://www.maritaca.ai/\n\u2022 Amaz\u00f4nia AI, developed by WideLabs:\nhttps://amazoniaia.com.br/"}, {"title": "3.1. Access to chosen models", "content": "The same prompt was used for all the models, and whenever possible, the tests were carried out on the official platforms, such as the ChatGPT versions, Command-R-Plus (https://coral.cohere.com), Reka Core 67B (https://www.reka.ai/), Amazonia IA (https://amazoniaia.com.br) and Sabi\u00e1-3 (https://www.maritaca.ai/). For Grok-2 and Nemotron 340B, the direct chat function on the https://lmarena.ai/ platform was used. For the Nemotron 340B, direct access to the platform was also used in some cases (https://build.nvidia.com/nvidia/nemotron-4-340b-instruct)."}, {"title": "3.2. Selection of tests and data preparation", "content": "The tests were selected based on their relevance to the legal exam scene in Brazil for higher education. We chose national exams, with many candidates and public careers considered more important, with a higher degree of difficulty in the questions, in addition to the national exam for law practice.\nThe exams tested were the following: Brazilian Bar Association (OAB), Labor Prosecutor's Office, Federal Judge, Federal Prosecutor's Office, Federal Revenue Service, and the National Magistrates' Examination (ENAM).\nAs a selection criterion, the study chose the most recent exams applied in the last five years, except for the Public Prosecutor's exam, where the last two exams were used (from 2021 and 2017). For the Brazilian Bar Association (OAB) exam, the ten most recent exams, taken between June 2021 and July 2024 (Exam 41), were tested. Although OAB Exam 42 was completed before the study concluded, it was excluded from consideration because the official results had not yet been released.\nThe tests were taken from the official websites of the examining bodies: FGV (https://conhecimento.fgv.br/), FCC (https://www.concursosfcc.com.br) and CESPE/Cebraspe (https://www.cebraspe.org.br).\nFor all the tests, spreadsheets have been drawn up, which, in addition to including the questions, indicate the difficulty level, the official templates, and the answers provided by each model.\nThe questions were classified as \"easy\" or \"difficult\". To use an objective criterion, minimally anchored in the reality of public examination candidates, the statistics available on the website https://www.qconcursos.com were used. This site makes the tests available so any user can answer the questions. Considering the total number of respondents, the percentage of correct answers for each question was used as the distinguishing criterion. Questions with a score of less than 50% were considered \"difficult,\" and questions with 50% or more correct were considered \"easy\".\nSome tests had questions annulled due to errors in the templates or the impossibility of answering them. In this case, regardless of the alternative indicated by the models, the answer was considered \"correct,\" the same criterion used for human candidates who took the tests in person.\nThe various exams analyzed include all the leading legal subjects taught in Brazil and some specific subjects, such as Portuguese, English, Economics, Statistics, Accounting, Data Fluency, and Public Finance, which are all part of the Federal Revenue Auditor exam."}, {"title": "4. Test protocol", "content": "A team of nine researchers tested the models in all the races between October and the beginning of December 2024. All the models were tested using the most up-to-date versions available at the time.\nAll tests were carried out in zero-shot mode (Kojima, 2022) without any prior training or examples. The zero-shot approach is widely used to evaluate the performance of Al models, as there is a significant effect of the few-shot thought chain stimulus on the essential zero-shot stimulus (Mart\u00ednez, 2024).\nAmong the possible research techniques, zero-shot was chosen due to the greater reliability of the answers presented. The approach guarantees greater assertiveness in relation to the option chosen, given that the AI model has never, in theory, had contact with the format of the question presented or with the expected answer (Orsini, 2023).\nAll the tests were carried out in prompt-question format. The prompt-question pairs were submitted individually, one by one, with the full text and without any changes to the test being evaluated. The option of repeating the prompt in each question aimed to cancel out the \"memory\" effect that some larger models have, which could affect the results of smaller models if there was only one original prompt at the start of the test and only the reproduction of the questions afterward.\nAfter some preliminary tests, the following prompt in Portuguese was adopted as the standard: \"Consider that you are a candidate in a higher education competition. Answer the multiple-choice questions I am about to give you, knowing that only one of the alternatives is correct. I want you to give me the percentage confidence interval for each answer. Question: ...\".\nThe confidence index was used to gauge whether the model, according to its knowledge base, was confident in the answer provided. With the prompt used, practically all the models indicated the alternative considered correct, with comments on why it was chosen and why the other alternatives were considered incorrect.\nAll the answers provided by the AI models tested were compared with the official template and considered correct or incorrect according to the alternative indicated. In almost all the public exams tested, the questions have five alternatives. Only in the Brazilian Bar Association exam did the questions have four alternatives.\nIn public examinations, many of the questions require candidates to use associative reasoning, indicating, for example, which assertions would be correct or what would be the appropriate sequence for the proposed assertions (true, false, true, for example).\nThe method used to analyze the performance of each model was predominantly quantitative, based on the number of correct answers. Also, by way of \"research findings,\" in some cases, a qualitative analysis of the data found was carried out according to, for example, the degree of difficulty of each question. (Herrmann-Werner et al., 2024).\nAs explained, the tests were carried out without prior training of the models, and the first answer submitted was always considered, even when it was necessary to resubmit the question or complete the prompt, such as in cases where the confidence index was not submitted."}, {"title": "4.1 Quality controls", "content": "At various times, the models failed to provide a correct alternative. In these cases, the initial standard prompt was supplemented by inserting additional commands such as \"as a candidate you are required to submit an answer\" or \"as a candidate you are required to indicate an alternative (a, b, c, d or e) as an answer\" to force the Al model to submit an alternative. This scenario occurred frequently in Gemini 1.5 Pro, probably because the model understood that the question, which referred to a hypothetical case, was a real legal problem. In some rare situations, the models presented more than one alternative as correct, which required a complementary prompt, determining that the model presented only one correct alternative.\nThe confidence index was almost always given as a percentage, in line with what was requested. In some cases, instead of the percentage, the models indicated \"high,\" \"medium,\" or \"low\" as the confidence index."}, {"title": "4.2 Limitations", "content": "Legal studies have several limitations, especially in Brazil, where there are continuous legislative changes and significant fluctuations in case law due to the enormous complexity of the system. For this reason, it was decided to evaluate only the most recent competitions, usually held in the last five years, as any changes in legislation may have impacted some of the answers provided by the models. The parameters used were designed to minimize this impact.\nAnother limitation to consider is the possibility that some models had already been trained with tested questions. This is the case with Sabi\u00e1-3, who was probably trained for the Brazilian Bar Association and the National Magistrates' Exam (Abonizio, Hugo et al.,2024). In this specific case, the possibility of contamination in the zero-shot approach cannot be ruled out."}, {"title": "5. LegalScore development", "content": "The index, which will be updated periodically with new tests and AI models, aims to determine the effectiveness and alignment of responses in various scenarios.\nIn this first stage, which only included multiple choice questions, according to the methodology presented, the LegalScore index involved analyzing the correct answers of each model in the various tests, with subsequent standardization of the data, performance calculations, and normalization for a final scale capable of comparing the results.\nThe procedures used in all the exams, with variations for each set of tests, are described below. For example, a reference was adopted in the Brazilian Bar Association exams, which have 80 questions, to convert the number of correct questions into percentages. The data was organized in a matrix where the rows represent the models evaluated, and the columns correspond to the editions of the exam.\nThe number of correct answers for each model per test was converted into a performance percentage using the following formula, which transforms absolute values into a standardized metric from 0 to 100%:\n$Percentage = (\\frac{Total Hits}{80}) \u00d7 100$\nTo consolidate the performance of the models, the Z-score was used, a standardized scoring method that measures the deviation of each value from the mean, adjusted by the standard deviation of the distribution, according to the formula:\n$Z = \\frac{\u03a7 - \u03bc}{\u03c3}$\nWhere:\nX: Percentage of model hits in a specific test;\n\u03bc: Average of the percentages of all the models in the same test;\n\u03c3: Standard deviation of the percentages in the test.\nThe Z-scores were calculated, by model, for all the Brazilian Bar Exam tests and then added together to generate the General Performance Index, which represents the relative performance of each Al model compared to the others.\nTo make it easier to interpret the results and avoid negative values, the General Performance Index was normalized to a scale of 0 to 10 using a linear transformation formula:\n$Score = (\\frac{X - Xmin}{Xmax \u2013 Xmin}) \u00d7 10$\nWhere:\nX: General Index of the model;\nXmin: Lowest General Index calculated;\nXmax: Highest General Index calculated.\nAs a result, the best-performing model will get the maximum score (10), while the worst-performing model will get zero.\nTo ensure that the methodology adopted allows for the inclusion of new Al models in the future, standardization and normalization will be recalculated based on the new distributions so that the performance index can evolve coherently, allowing for adequate comparisons between current models and those that will be evaluated.\nFinally, to visualize the results more easily, a bar chart was used to show the overall and relative performance of the models evaluated.\nThe procedure described was adopted for all the tests in the competitions evaluated."}, {"title": "6. Overall results of the competitions evaluated", "content": ""}, {"title": "8. Discussion", "content": "The 14 AI models selected in this study showed varying performance in the different tests of each competition.\nThe most consistent result was obtained in the Brazilian Bar Association exam, which requires a minimum pass rate of 50% of the 80 questions in each exam. Virtually all the models evaluated would have passed any of the last ten OAB exams, except ChatGPT 3.5 in exam 37 and Sabi\u00e1 3 in exam 32. This result can be used as a starting point for an in-depth reflection on the model for assessing future lawyers in Brazil. While the AI models have been relatively successful in the OAB's multiple-choice exams (see annexes), considering the tests evaluated, the percentage of human candidates who definitely pass the exams is around 15%.\nAn important observation needs to be made here. The OAB exams consist of two phases: the first, with 80 multiple-choice questions, and the second, with discursive questions, according to the area of practice chosen by the candidate, as well as the drafting of a legal paper. This research, in Phase 1, tested the 14 AI models only in relation to the multiple-choice questions, as shown. The next research stage will check the performance of all the models on discursive questions.\nThe definitive result, for comparison purposes, still needs to be investigated. However, it is reasonable to imagine that the performance of the models, in general, will be superior to that of the human candidates.\nEven so, it is understood that legal education in the country, which is known to be poor, needs to be rethought, as well as how university graduates are assessed. On the other hand, the most important legal exams in Brazil were more challenging for the AI models evaluated. Most of them did not achieve the minimum scores required to pass the first stage, multiple choice, of the exams analyzed.\nA notable exception was the competition for the position of Federal Revenue Auditor, which has long been considered one of the most difficult in the country, not only because of the level of the questions but also because it requires candidates to have, in addition to legal knowledge, expertise in exact subjects, such as logical reasoning, economics, statistics, finance,"}, {"title": "8.1 Interpretation of results", "content": "The 14 AI models selected in this study showed varying performance in the different tests of each competition.\nThe most consistent result was obtained in the Brazilian Bar Association exam, which requires a minimum pass rate of 50% of the 80 questions in each exam. Virtually all the models evaluated would have passed any of the last ten OAB exams, except ChatGPT 3.5 in exam 37 and Sabi\u00e1 3 in exam 32. This result can be used as a starting point for an in-depth reflection on the model for assessing future lawyers in Brazil. While the AI models have been relatively successful in the OAB's multiple-choice exams (see annexes), considering the tests evaluated, the percentage of human candidates who definitely pass the exams is around 15%.\nAn important observation needs to be made here. The OAB exams consist of two phases: the first, with 80 multiple-choice questions, and the second, with discursive questions, according to the area of practice chosen by the candidate, as well as the drafting of a legal paper. This research, in Phase 1, tested the 14 AI models only in relation to the multiple-choice questions, as shown. The next research stage will check the performance of all the models on discursive questions.\nThe definitive result, for comparison purposes, still needs to be investigated. However, it is reasonable to imagine that the performance of the models, in general, will be superior to that of the human candidates.\nEven so, it is understood that legal education in the country, which is known to be poor, needs to be rethought, as well as how university graduates are assessed.\nOn the other hand, the poor performance in strictly legal competitions can largely be attributed to the fact that most of the models were trained with datasets predominantly in English, which impaired their understanding of specific legal terminology or language nuances typical of the Portuguese language (Caparroz, 2024). Another challenge identified was the difficulty in addressing ambiguous questions requiring more in-depth reasoning. The models also needed to be improved in their ability to correlate information when faced with interdisciplinary questions.\nIn general terms, the models struggled most in areas such as Tax Law, Labor Law, and Criminal Law, particularly with questions that required analysis of case law or more specific knowledge, such as the content of regulations and regional laws. Indeed, the content required in the competitions was outside the data used, which is more generic and restricted to national and better-known laws, such as the codes of the primary disciplines.\nThis finding is particularly significant, as it underscores that the more specific the question, the worse the performance of all evaluated Al models in nearly every instance. This highlights the critical need to develop datasets tailored to the Brazilian context (Caparroz, 2024)."}, {"title": "8.2 Summary of Findings", "content": "The study evaluated 14 AI models, categorized into four groups for comparison. The first group comprises large proprietary models such as ChatGPT40, Gemini 1.5 Pro, and Claude Sonnet 3.5. As expected, this group performed best, which can be attributed to two factors: a) the huge number of parameters and b) the fact that, as they are commercial models, they were probably trained with datasets more representative of the Brazilian reality.\nThe second group comprises models considered large but substantially smaller than the three leaders, such as LLama 3.1 405B, GROK 2, Nemotron 340B, and Perplexity AI, which uses a combination of large models. These models were in an intermediate position in the overall ranking.\nThe third group is made up of smaller models, such as Mistral Large-2, ChatGPT oi-mini, Reka Core 67B, Command-R-Plus, and ChatGPT 3.5 Turbo (in this case, a relatively large model, with 175 billion parameters, but old compared to the others). These models performed the worst, coming last in the overall ranking.\nFinally, the fourth group comprises the two models trained for the Brazilian reality, Sabi\u00e1 3 and Amazonia IA. Unfortunately, Brazilian companies do not provide information on the models' size or the datasets they were trained in. It is assumed that they are small/medium-sized models in terms of parameters but that they obtained good rankings (4th place and eighth place, respectively), which shows that, for regional and language alignment, the datasets tend to be as important if not more important, than the raw capacity of the models.\nThe relevance of the datasets in the context evaluated can be confirmed by the fact that, in several questions, practically all the models showed errors when indicating the same alternative as the answer. This allows us to conclude that, regardless of size, most of these models were trained on the same datasets, which proved insufficient to provide adequate answers."}, {"title": "8.3 Contributions to the field", "content": "The findings suggest that while AI models have potential as complementary tools, they are still far from matching human performance in more complex public exams. The linguistic, cultural, and contextual limitations observed in the research highlight the need for public investment in developing datasets adapted to the reality and characteristics of the complex Brazilian legal system.\nLegalScore contributes to future research in artificial intelligence applied to Brazilian law by creating a reference. The index could be developed as a tool for improving models, making it possible to objectively assess the improvements incorporated into the new versions tested and contributing to the development of more efficient educational solutions.\nThe benchmark model developed for LegalScore can be replicated in other countries, which probably have peculiarities that the AI models available on the market have not yet considered.\nFuture studies should deepen the analysis in other cultural contexts, which also require specific datasets aligned with the expectations of non-Anglophone countries, such as Brazil.\nLegalScore presents a new paradigm for evaluating the performance of Al models in complex human tasks, reinforcing the importance of aligning emerging technologies with local and sectoral demands, especially in critical educational and professional contexts, such as legal careers."}, {"title": "Conclusion", "content": "This research developed an objective analysis of the performance of various Al models in Brazilian public exams aimed at selecting candidates for relevant legal positions, such as the Judiciary (in the Federal and State spheres), the Labor Prosecutor's Office, the Public Prosecutor's Office, the Tax Auditor of the Federal Revenue Service of Brazil, and the aptitude test for the practice of law.\nThe work resulted in a pioneering benchmark, called LegalScore, used to measure the accuracy and efficiency of the Al models tested in their most recent versions. LegalScore's permanent creation will make it possible to evaluate new models as they become available to the public.\nThe research findings show that AI models face difficulties in multiple-choice questions that require contextual interpretation, especially in questions that demand an understanding of specific laws, infra-legal norms, case law, and local particularities.\nDespite these limitations, LegalScore is believed to offer a reliable metric for evaluating the evolution of models and identifying possible points for improvement, especially regarding the need to create datasets aligned with the Brazilian legal landscape and reality.\nThe index is also intended to serve as a reference for the debate on legal education in Brazil, especially considering that the models evaluated are quite recent and that their performance could soon surpass that of human candidates if the current format of the tests is maintained.\nWith the exponential advance of generative AI, all legal professionals will need to develop new skills, which must be identified and disseminated on a large scale."}, {"title": "Appendices", "content": ""}, {"title": "Brazilian Federal Revenue Service", "content": "The competition is considered one of the most difficult and competitive in Brazil, as anyone with a university degree can participate. The AI models were tested based on the last competition, held in 2023. There were two multiple-choice tests, with 140 questions in total, on the following subjects: Portuguese Language, Foreign Language (English or Spanish), Logical and Quantitative Reasoning, Civil, Criminal, and Business Law, Constitutional and Administrative Law, General and Public Administration and Data Fluency, as well as specific knowledge of Tax Law, General and Advanced Accounting, Tax and Customs Legislation, International Trade and Customs Legislation, Auditing, Economics and Public Finance.\nOne of the competition's characteristics is the variety of subjects, in addition to legal knowledge, which often creates difficulties for candidates. However, AI models perform very well in language questions (especially English, due to their language training) and maths-related subjects.\nIn the competition evaluated, AI models had difficulty, above all, in two subjects: a) Accounting, due to long questions that require the development of concatenated calculations, and b) Public Finance, due to the specificities of Brazil legislation. Candidates who pass the multiple-choice test have their discursive exams corrected, and the best-placed candidates within the number of vacancies offered pass.\nAnalysing the cut-off marks for the position of Tax Auditor\nThe research analysed the distribution of cut-off scores for the Tax Auditor positions, considering both the broad competition and the vacancies reserved for blacks and people with disabilities (PwD). The analysis covers the maximum scores, the percentages achieved in relation to the total points, and the minimum scores required for the discursive tests to be corrected.\nThe Auditor exam had 140 multiple-choice questions, each worth 1 point.\nCut-off Point:\nFederal Revenue Tax Auditor\nComparison and Reflections\nThe difference between the scores of the top candidates and the cut-off marks reinforces the idea that the Internal Revenue Service competitions remain highly competitive, even with the reduction in the number of vacancies in recent years. Regarding the position of Tax Auditor, the difference between the first-placed candidate (111 points) and the cut-off mark (93 points) was 18 points, or around 12.86 percent of the maximum score.\nGeneral conclusion\nAnalyzing the cut-off marks and the scores of the first-placed candidates in the Federal Revenue Service competition shows a rigorous selection process with extensive and challenging questions.\nThe cut-off marks were relatively low, which shows the degree of difficulty of the tests faced by human candidates.\nAnalysis of the Federal Revenue Auditor Contest"}, {"title": "Federal Prosecutor", "content": "This is a high-level civil service examination for the position of Federal Prosecutor in Brazil. The exam is considered one of the most challenging in the legal field in the country. Eligibility is restricted to law graduates with at least three years of legal experience.\nThe examination questions are prepared and graded by the institution's own Examination Committee", "law": "Constitutional Law, Legal Methodology, International Human Rights Protection, Electoral Law, Administrative Law, Environmental Law, Tax Law, Financial Law, Public International Law, Private International Law, Economic Law, Consumer Law, Civil Law, Civil Procedure, Criminal Law, and Criminal Procedure.\\"}]}