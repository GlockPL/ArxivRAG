{"title": "NutrifyAI: An AI-Powered System for Real-Time Food Detection, Nutritional Analysis, and Personalized Meal Recommendations", "authors": ["Michelle Han", "Junyao Chen"], "abstract": "With diet and nutrition apps reaching 1.4 billion users in 2022 [1], it's no surprise that health apps like MyFitnessPal, Noom, and Calorie Counter, are surging in popularity. However, one major setback [2] of nearly all nutrition applications is that users must enter food data manually, which is time-consuming and tedious. Thus, there has been an increasing demand for applications that can accurately identify food items, analyze their nutritional content, and offer dietary recommendations in real-time. This paper introduces a comprehensive system that combines advanced computer vision techniques with nutrition analysis, implemented in a versatile mobile and web application. The system is divided into three key components: 1) food detection using the YOLOv8 model, 2) nutrient analysis via the Edamam Nutrition Analysis API, and 3) personalized meal recommendations using the Edamam Meal Planning and Recipe Search APIs. Designed for both mobile and web platforms, the application ensures fast processing times with an intuitive user interface, with features such as data visualizations using Chart.js, a login system, and personalized settings for dietary preferences, allergies, and cuisine choices. Preliminary results showcase the system's effectiveness, making it a valuable tool for users to make informed dietary decisions.", "sections": [{"title": "I. INTRODUCTION", "content": "The rise of artificial intelligence has significantly impacted various aspects of human daily life, ranging from healthcare to entertainment. Among the numerous advancements in artificial intelligence, computer vision stands out for its potential in how users interact and interpret visual data. Particularly, computer vision offers a promising application in the domain of food recognition and nutrient analysis [3]. As the global population is becoming increasingly health conscious, there is a growing demand for technologies that can accurately identify food items and subsequently provide detailed nutritional information. This research aims to explore the capabilities of a computer vision model, specifically a YOLO (You Only Look Once) model, to accurately recognize food dishes and perform a reliable analysis of their nutritional content."}, {"title": "A. YOLO Models", "content": "YOLO (You Only Look Once) models are a family of real-time object detection algorithms that excel at processing images in a single evaluation, predicting bounding boxes and class probabilities simultaneously. Unlike traditional object detection models, which process images in multiple stages, YOLO's single-stage approach allows for much faster detection times without compromising accuracy [4]. This makes YOLO models particularly suitable for this application, which requires rapid and precise object detection. Other fields YOLO models are applicable include autonomous driving, surveillance [5], and notably, food recognition, where quick and accurate results are essential.\nYOLOv5 and YOLOv8 are two advanced versions of YOLO models, each incorporating techniques such as multi-scale predictions and anchor boxes [6], along with various architectural improvements to enhance detection performance. YOLOv5 is known for its balance between speed and accuracy, making it versatile for diverse applications, while YOLOv8 offers even better accuracy and efficiency through further optimization. In this paper, we test variations of both models."}, {"title": "II. LITERATURE REVIEW", "content": "Table I reviews key studies relevant to the development of NutrifyAI, focusing on food detection using YOLO models, the integration of nutritional APIs for real-time analysis, and Al-driven meal recommendations."}, {"title": "III. METHODOLOGY", "content": "In this research, we set out to explore the potential of AI technologies in enhancing daily food tracking through detection, analysis, and recommendations. Thus, we developed NutrifyAI, a web application, as the medium for this research to highlights its practicality and usability in people's daily lives. By offering a user-friendly interface, the app allows individuals to seamlessly integrate nutritional tracking into their routines. Users can simply capture an image of their meal,"}, {"title": "B. Data Collection and Preprocessing", "content": "1) Training Datasets: The training of our food detection model was based on datasets that had already been aggregated and preprocessed by the repository available at GitHub [7]. This repository integrates multiple datasets, that were publically accessible and available, to provide a comprehensive"}, {"title": "C. Model Architecture", "content": "The YOLOv8 model was trained using the aforementioned datasets, each labeled with food categories and bounding boxes. The dataset was split into 80% for training and 20% for validation. To enhance the model's generalization ability, we applied data augmentation techniques such as rotations, scaling, and flips to simulate different viewing conditions.\nThe training process utilized of Stochastic Gradient Descent with a momentum optimizer. We started with an initial learning rate of 0.01, which was adjusted throughout the training process using a learning rate scheduler to ensure steady improvement and prevent overfitting. The model was trained for 50 epochs, with early stopping used to halt training if the validation loss did not improve after 10 consecutive epochs.\nIn addition to YOLOv8, we used EfficientNet-B4 for further classification. EfficientNet-B4 was fine-tuned on our dataset, starting with pre-trained weights from ImageNet. The finetuning process involved training for 30 epochs using the Adam optimizer with a learning rate of 0.001, with similar data augmentation techniques applied to improve the model's robustness in recognizing diverse food items."}, {"title": "D. Model Evaluation", "content": "The evaluation of our food detection model was conducted using the Food Recognition 2022 dataset [8], a comprehensive annotated dataset specifically designed for semantic segmentation tasks, which includes 43,962 images with 95,009 labeled objects belonging to 498 different classes. The GitHub repository [7] from which we obtained our training data tested five variations of the YOLO model (YOLOv5s, YOLOv5m, YOLOv5l, YOLOv5x, YOLOv8s) with results measured based on mAP at 0.5 IoU.  The chart in Fig. 8 presents the mAP results for each YOLO variant, highlighting the superior performance of YOLOv8s with a mAP of 0.963. This high accuracy is why we chose YOLOv8s to power the NutrifyAI application.\nTo assess our model's performance, we selected a subset of the Food Recognition 2022 dataset that intersected with the classes present in our training data. This resulted in a test set comprising 55 food categories. Given that the Food Recognition 2022 dataset contains 498 different classes, many of which were not present in our training data, a crucial step in the evaluation process was label mapping. This involved"}, {"title": "E. API Integration", "content": "a) Edamam Nutrient Analysis API: This API was used to retrieve detailed nutritional information for the detected food items. After the YOLOv8 model identifies the food in an image, a request is sent to the Edamam API with the name of the detected food item. The API returns comprehensive nutritional data, including calories, fat, protein, fiber, and other key nutrients. In our user interface, the nutritional information retrived from the Edamam API is visualized using Chart.js. This allows users to quickly see the nutritional breakdown of their meals. This information is then stored in a Google Sheets database via the Google Sheets API for further processing and display.\nb) Edamam Recipe and Meal Planning API: To provide personalized meal recommendations based on the user's nutritional goals, the Edamam Recipe and Meal Planning APIs were integrated into the application. These APIs allow NutrifyAI to suggest recipes and meal plans that align with the user's dietary preferences, restrictions, and nutritional targets. Recommendations are generated based on the history of foods scanned by the user, offering tailored meal suggestions to help users meet their health objectives.\nc) Google Sheets API: This API was used to store and manage the nutritional data and meal recommendations retrieved from the Edamam APIs. Each time a food item is detected and analyzed, the nutritional information is automatically logged into a Google Sheet. This approach ensures that all user data is securely stored and easily accessible for future reference and analysis.\nd) Server-Side Implementation: The entire process is managed by a Flask server, which handles communication between the client-side web application, the YOLOv8 model, and the various APIs. Flask serves as the backbone of the server-side logic, managing requests, processing data, and returning results to the user. The Flask server is connected to a YOLOv8 model hosted on Google Colab via ngrok, which enables secure tunneling and facilitates real-time image processing.\ne) Client-Server Implementation: As illustrated in the attached image, the web client interacts with the server by sending images, videos, or URLs for food detection. Once the image is processed by the YOLOv8 model on the server, the detected food items are sent to the Edamam APIs for nutritional analysis and meal recommendations. The results are then returned to the web client, where they are displayed to the user through a user-friendly interface that utilizes Chart.js for visualizing nutritional data."}, {"title": "IV. RESULTS", "content": "After evaluation on the Food Recognition 2022 dataset as previously mentioned, we noted that while the original authors of the GitHub repository conducted mAP testing on five YOLO model variations, achieving the highest mAP of 0.963 with YOLOv8s at 0.5 IoU, we did not specifically test our model on mAP for this dataset during the final testing"}, {"title": "V. DISCUSSION", "content": "The primary objective of this study was to develop and evaluate a system that utilizes computer vision models and nutritional APIs for advanced food profiling and recommendations. The findings from our evaluation indicate that the system performs well in identifying food items and providing detailed nutritional analysis, supporting our original hypothesis that an AI-powered tool can streamline and enhance the process of dietary tracking.\nThe data moderately supports our hypothesis. NutrifyAI achieved a fair accuracy of 75.4%, with precision, recall, and F1 scores that indicate an acceptable detection system. The successful integration of the Edamam Nutrient Analysis"}, {"title": "A. Future Improvements", "content": "There are several ways we could improve the system's overall performance. Firstly, incorporating mean Average Precision (mAP) can better gauge the model's precision and recall across various confidence thresholds. This would provide a more detailed understanding of the model's strengths and weaknesses.\nFurthermore, one of the main limitations identified was the class imbalance and limited diversity in the training dataset. Expanding the dataset to include a broader range of food categories, particularly those that are underrepresented, could improve the model's generalization and recognition capabilities. A more diverse dataset would also help the model perform better on visually similar items, which currently present a challenge.\nWe also noticed that the detection confidence score histogram revealed a significant number of predictions clustered around the 0.2 confidence level, indicating uncertainty in those predictions. To improve the model's reliability, future work should focus on refining the confidence calibration, potentially through better training technique.\nFinally, introducing feedback mechanisms where users can correct or adjust predictions would allow the system to learn from real-world usage. This could lead to continuous improvement in accuracy and personalization, making the meal recommendation system more relevant and tailored to individual users."}, {"title": "VI. CONCLUSION", "content": "In summary, NutrifyAI demonstrates the potential of integrating computer vision models with nutritional APIs to create an effective tool for food recognition and nutritional analysis. While the system achieved promising results, several areas remain for further exploration and enhancement. Future research should focus on expanding the diversity of training datasets to improve the model's ability to recognize a wider array of food items. Additionally, refining the model's confidence calibration and incorporating advanced evaluation metrics, such as mAP, will be crucial for achieving greater reliability. Developing mechanisms for user feedback will enable the system to adapt to real-world usage, offering more personalized and accurate meal recommendations. As the field of AI-powered nutrition continues to evolve, there is significant potential for this technology to be applied in various domains, including healthcare and personalized diet planning, making the pursuit of these improvements highly worthwhile."}]}