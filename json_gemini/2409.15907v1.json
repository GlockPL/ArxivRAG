{"title": "Enhancing Text-to-SQL Capabilities of Large Language Models via Domain Database Knowledge Injection", "authors": ["Xingyu Ma", "Xin Tian", "Lingxiang Wu", "Xuepeng Wang", "Xueming Tang", "Jinqiao Wang"], "abstract": "Text-to-SQL is a subtask in semantic parsing that has seen rapid progress with the evolution of Large Language Models (LLMs). However, LLMs face challenges due to hallucination issues and a lack of domain-specific database knowledge(such as table schema and cell values). As a result, they can make errors in generating table names, columns, and matching values to the correct columns in SQL statements. This paper introduces a method of knowledge injection to enhance LLMs' ability to understand schema contents by incorporating prior knowledge. This approach improves their performance in Text-to-SQL tasks. Experimental results show that pre-training LLMs on domain-specific database knowledge and fine-tuning them on downstream Text-to-SQL tasks significantly improves the EXECUTION MATCH (EX) and EXACT MATCH (EM) metrics across various models. This effectively reduces errors in generating column names and matching values to the columns. Furthermore, the knowledge-injected models can be applied to many downstream Text-to-SQL tasks, demonstrating the generalizability of the approach presented in this paper.", "sections": [{"title": "1 Introduction", "content": "With the advancement of LLMs, certain large models like WizardCoder[13] and CodeLlama[20] have achieved satisfactory results in Text-to-SQL tasks. However, applying LLMs to Text-to-SQL tasks presents challenges due to polysemous terms and insufficient semantic information in column and table names.\n\u2022 Example of polysemous column names: A column named \"state\" can have several meanings (e.g. condition, situation, political entity and so on); If \"New York State\" is given, LLMs can easily understand the meaning of \"state\";\n\u2022 Example of columns with insufficient semantic clarity: A column named \"b_fund_stock_bond_share_d\" stands for \"daily total fund stock and bond position ratios\". This is a column name from a real-world database, and it's difficult to determine the meaning of the column based on the few words in the column name alone. However, if some values of the column are provided, then the model can easily understand the meaning of the column.\nThis could lead to the model being prone to two types of errors.: 1) Errors in generating column names and table names, 2) Mismatches between cell values, column names, and table names. \nNumerous studies have sought to resolve these two issues by regulating the decoding rules[21], achieving some progress. However, such optimizations of decoding rules have not enhanced the base model's comprehension of databases. While they can rectify certain errors in generating column names and table names, matching cell values with columns remains challenging to address through restricted decoding rules. Further, there are efforts to optimize this problem through schema linking[25], but the cell values and column names extracted via this method do not consistently reach a high level of accuracy. In LLMs, due to the pretrained nature of the models, it is difficult to alter the model structure to implement schema linking, with the only option being to incorporate schema information into prompts. However, LLMs lack the domain-specific database knowledge, which impairs their ability to comprehend and apply schema information provided within prompts. Therefore, we aim to enhance the ability of LLMs to comprehend database schema and cell values through the injection of domain database knowledge to optimize this issue. This approach is intended not only to improve the LLMs' comprehension of databases but also to achieve a certain level of optimization for such challenges.\nAlthough some work focuses on enhancing the understanding of data tables through methods like table pretraining [31, 33, 5, 23], these efforts are general, based on extensive datasets, and lack specificity; they fail to solve domain-specific issues like incorrect generation of column and table names. Consequently, this paper seeks to"}, {"title": "2 Methodology", "content": "We will introduce our method through three subsections. In Section 2.1, we will present which features of the database schema and cell values can be utilized to build prior knowledge. Section 2.2 will detail how to leverage these features to construct prior knowledge, while Section 2.3 will outline the specific design methods for each task."}, {"title": "2.1 What Knowledge can LLMs Learn From a Database?", "content": "Firstly, let's define the meanings of two types of knowledge:\n\u2022 Table and column names semantic knowledge: Database ad-ministrators follow certain conventions when creating tables, of-ten using column and table names that contain semantic informa-tion. For example, a table named \"user_info\" can be inferred to be a user data table, this is the semantic knowledge of column and table names. However, there still exists a significant number of ta-bles and columns where semantic information is incomplete, and we expect the model to utilize cell values to enhance the semantic knowledge of such column and table names.\n\u2022 Schema knowledge: The schema of a database refers to informa-tion such as table names, column names, and types of columns. We refer to this type of knowledge as schema knowledge.\n1. LLMs can learn the semantic knowledge of tables and columns using cell values\nThe data type of a column in a database table is typically a collection of data of the same type, so we can use the cell values contained in that column to enhance the seman-tic information of the column name. For columns with poly-semous words or columns with incomplete semantic informa-tion (especially in practical scenarios where column names in"}, {"title": "2. LLMs can learn schema knowledge to improve the co-occurrence frequency between table names and column names", "content": "We understand that language models primarily learn from the co-occurrence relationships of tokens and tend to generate vo-cabulary with higher co-occurrence frequency when predicting the next token[10]. Consequently, we aim to increase the co-occurrence frequency between column names and table names to make the model more apt to generate the correct names when predicting them. As depicted in Figure 3, we aim to boost the number of co-occurrences between column names and ta-ble names by constructing a mutual predictive relationship be-"}, {"title": "2.2 How to Build Training Tasks to Inject Domain Knowledge?", "content": "From the time the T5 model organized natural language processing (NLP) tasks as end-to-end text generation tasks[18], to the present day with the emergence of various LLMs, NLP tasks have almost all become end-to-end text generation tasks. Therefore, although the idea of training tasks is derived from classic NLP tasks such as Named Entity Recognition (NER), the form of training is also conducted as end-to-end text generation tasks. As shown in Figure 4, we first obtain the schema information and cell values from the database, then use pre-constructed question-and-answer templates to fill in the cell values and schema information into the training tem-plates to build training data. Our template construction mainly fo-cuses on three task objectives: enhancing column name semantics, enhancing table name semantics, and increasing the co-occurrence frequency between column names and table names through learning schema knowledge.\nSpecifically, given a database $D_i \\in \\mathbb{D}$, the schema information of the database $D_i$ is $S_i \\in \\mathbb{S}$, where $|\\mathbb{D}| = |\\mathbb{S}|$, and $i \\in [1, |\\mathbb{D}|]$, The database $D_i$ contains K tables $T_j$, where $j \\in [1, K]$, Each table has value information $V_{i,j} \\in \\mathbb{V}_i$, We have a set of question templates $\\mathbb{T}_q = \\{t_{q,1},t_{q,2}...t_{q,N}\\}$ and and a set of answer templates $\\mathbb{T}_a = \\{t_{a,1}, t_{a,2}...t_{a, N}\\}$, from which we construct the following training objectives.\n$P_m = \\mathbb{M}(\\sigma(t_{q,m}, S_i, V_{i,j}))$\n$\\mathbb{G}_m = \\delta(t_{a,m}, S_i, V_{i,j})$\n$\\min_{\\mathbb{M}} \\sum_{m=1}^{N} \\mathbb{L}(\\mathbb{G}_m, P_m)$\nWhere $\\delta(.,.,.)$ denotes the construction of the answer $\\mathbb{G}_m$ by uti-lizing the answer template $t_{a,m}$, along with the schema $S_i$ and cell vaules $V_{i,j}$, $\\sigma(\u00b7,\u00b7,\u00b7)$ represents the construction of a question using the question template $t_{q,m}$, schema $S_i$ and cell values $V_{i,j}$, $\\mathbb{M}$ sig-nifies the base model. $\\mathbb{L}$ denotes the loss function, $P_m$ represents the model's predicted results, and our training objective is to minimize the error between the model's predicted results $P_m$ and the actual results $\\mathbb{G}_m$."}, {"title": "2.3 Construction of Tasks and Cell Values Sampling", "content": "As shown in Figure 4(B), we aim to enhance the model's Text-to-SQL capabilities through three learning objectives. For each learning objective, we have designed some learning tasks based on classical NLP task concepts, such as ideas from NER tasks, text clustering, etc. The specific task designs are as follows.\nObjective 1. Learning column names semantic knowledge\nThe purpose of this task is to utilize cell values to enhance the se-mantic information of column names, thereby improving the model's understanding of domain-specific database column names. This is in-tended to reduce the incidence of incorrect column matching with cell values, such as Abbreviation=\"Jetblue Airways\" showing in Table 1. The specific task design is as follows:\n\u2022 Sample cell values $V_i$ from a column and train the model to gener-ate the corresponding column name $C_i$ for those cell values. For-mulation as $\\max P(C_i | V_i = \\{V_1, V_2...V_n\\})$.\n\u2022 Sample cell values $V = \\{V\\}_1^n$ from n columns and train the model to cluster these values according to their column names $\\{C\\}$. Formulation as $V_1,..., V_n = \\mathbb{M}^*(shuffle(V))$, where $V_n = \\{1, ...V_{||V_n||} \\}$, * is base model and the $shuffle$ is shuffle function to mix the values.\n\u2022 Sample cell values $V_i$ and column name $C_i$ and train the model to determine whether the cell values belong to that column. Formu-lation as follows, Where $I$ is indicator function and $C_j$ is the true column name of $V_i$.\n$\\max P(I(C_i, C_j)|V_i, C_i)$\ns.t.\n$I(C_i, C_j) = 1, \\quad if \\quad C_i = C_j$\n$I(C_i, C_j) = 0, \\quad if C_i \\neq C_j$\n\u2022 Given cell values $V_i$ and column name $C_i$, train the model to predict the data type of that column. Formulation as $\\max P(T_i | C_i, V_i)$, where $T_i$ is the data type, such as INT.\nObjective 2. Learning table names semantic knowledge\nThe purpose of this task is to enhance the model's understanding of tables. For instance, if there is a clear distinction between user tables and product tables, the model can benefit by preliminarily determining which table a query should reference based on the val-ues mentioned in the user's question. The specific task design is as follows:\n\u2022 Sample a row of data $R_i = \\{V_{i,1}, V_{i, 2}...V_{i,n}\\}$ from a table and train the model to generate the corresponding table name $\\{T_j\\}|\\mathbb{D}$. where $|\\mathbb{D}||$ is the number of tables,Formulation as $\\max P(T_j | R_i)$.\n\u2022 Sample some cell values from each table and train the model to cluster the cell values that belong to the same table together. For-mulation as follows:\n$V_1, .., V_{||D||} = \\mathbb{\\S}^*(shuffle(V))$\n$V = \\{V_1, V_2, ..., V_{||D||}\\}, V_{||D||} \\in \\mathbb{T}_{||D||}$\n$\\mathbb{T} = \\{T_1, T_2, ..., T_{||D||}\\}$\nWhere $|\\mathbb{D}||$ is the number of tables, $\\mathbb{T}$ is a set of tables, $V$ is the set of values from the $\\mathbb{T}$, $shuffle$ is a function used to mix $V$, and $\\mathbb{\\S}^*$ is base model.\nObjective 3. Learning schema knowledge\nThe goal of this task is to increase the frequency of co-occurrence between table names and column names by learning schema knowl-edge, so that when a table name is mentioned in the previous context, the model is more inclined to generate the correct column name for the next token prediction; likewise, when a column name is gener-ated in the previous context, it is more inclined to predict the correct table name for the next token. The specific forms of the task are as follows.\n\u2022 Sample column names $C_i$ from a table $T_i$ and train the model to generate the table name for these columns. Formulation as $\\maxP(T_i|C_i^\\mathbb{}\\})$, where $C_i \\in \\{C_{i,1}, C_{i, 2}, ..., C_{i, ||T_i ||} \\}$."}, {"title": "3 Experiments", "content": "The experimental dataset in this paper utilizes the SPIDER dataset [32], which is a large-scale, complex, multi-domain dataset and also the most widely used in the Text-to-SQL task. The SPIDER dataset"}, {"title": "3.1 Datasets", "content": "The experimental dataset in this paper utilizes the SPIDER dataset [32], which is a large-scale, complex, multi-domain dataset and also the most widely used in the Text-to-SQL task. The SPIDER dataset consists of training, development, and test sets, with no overlap be-tween the training set, development set, and test set. This paper uses the databases included in the SPIDER training and development sets to construct training data according to the knowledge injection fine-tuning task introduced in Section 2. A total of 49,878 task data en-tries were constructed, including 17,954 entries related to the co-occurrence frequency enhancement task for column and table names, and 31,924 entries related to the semantic enhancement task for col-umn and table names. Then, using 8,926 entries from the SPIDER training set for the second-phase data to fine-tune the downstream Text-to-SQL task, and the SPIDER development and test sets for ef-fectiveness validation."}, {"title": "3.2 Evaluation Metrics", "content": "We use two of the most mainstream evaluation metrics, EXECUTION MATCH (EX) and EXACT MATCH (EM), to assess the experimen-tal results. The EM metric represents the proportion of generated SQL statements that exactly match the labeled SQL statements. The EX metric involves comparing the execution results of the generated SQL statements with those of the labeled SQL statements after exe-cution; if the results are the same, it is deemed correct."}, {"title": "3.3 Models, Baseline and Environments", "content": "Our experiments primarily utilized open-source code models deepseek-coder-6.7b[7], CodeLlama-7b-Instruct, CodeLlama-13b-Instruct[20], and WizardCoder-15B-V1.0[13]. To validate the effec-tiveness of our method on general models, we also experimented with the general model Baichuan2-7B-Chat[29]. The baseline code for our downstream Text-to-SQL tasks was DB-GPT-HUB\u00b2, an open-source code aimed at Text-to-SQL tasks based on LLMs. The fine-tuning code for phase one database understanding enhancement tasks was implemented by us. Due to time constraints, all experimental code parameters were kept the same, with adjustments made only to batch size and deepspeed parameters in response to Out-Of-Memory is-sues, indicating that our experimental parameter settings might not"}, {"title": "3.4 Results", "content": "Table 2. The main experimental results, where SPIDER-DEV denotes the development set of SPIDER, and SPIDER-SYN is a dataset created by sub-stituting synonyms in the questions of the spider development set. We will introduce this dataset in the robustness testing section. The model names in the table have been abbreviated for brevity. \"KE\" stands for \"Knowledge En-hanced,\" indicating the evaluation results of models that have been augmented with domain-specific database knowledge. Results not marked with \"KE\" rep-resent the evaluation outcomes of the baseline.\nMain Results. The experimental results are presented in Table 2. Initially, it is observable that, on the T5-3B model, the method pro-posed in this study demonstrates an improvement of 1.2% in the EX metric and 0.6% in the EM metric compared to the baseline model. After incorporating PICARD to enforce decoding rules, our approach still yields enhancements in both the EX and EM metrics.\nMoreover, experiments on open-source LLMs have shown signif-icant improvements over the baseline models in terms of the Ex-act Match (EX) and Execution Match (EM) metrics. Firstly, the deepseek-coder-6.7b model saw the largest increase in the EX met-ric, with an improvement of 3.1%; on the CodeLlama-13b model, the EM metric saw the largest increase, rising by 3.4%, while the EX metric improved by 2.8%. This is because the baseline EM metric re-sults contained false negatives (i.e., the generated SQL was correct, but did not match the label SQL literally). Similar to CodeLlama-13b, on the CodeLlama-7b and Wizardcoder-15B models, the im-provement in the EM metric was better than that of the EX metric. For CodeLlama-7b, the EX and EM metrics increased by 2.1% and 2.8% respectively; on the WizardCoder-15B model, the EX and EM metrics improved by 1.2% and 2.4% respectively. It is evident that in four code models, three showed better improvement in the EM metric than the EX metric, which also suggests that the methods proposed in this paper are more conducive to the model generating correct column names and table names, thus reducing mismatches. More-over, a comparison of the four code models reveals that the choice of base model has a significant impact on the capability of SQL gen-eration; the evaluation results of WizardCoder-15b were only on par with those of CodeLlama-7b, while the performance of deepseek-coder-6.7b even exceeded that of CodeLlama-13b, which also tells us about the importance of choosing the right base model in the training of large domain models. Additionally, the method proposed in this study is not only effective on large code models but also shows an increase of 2.8% in the EX metric and 2.5% in the EM metric on the Baichuan2-7B-Chat general model.\nLastly, while many studies mainly report improvements in the EX metric, a decline in the EM metric may occur. This decline can be due to the exactness of execution accuracy only, implying that the query results are the same, but the SQL statements are logically in-correct. However, the method introduced in this paper achieves en-hancements in both the EX and EM metrics, with the most notable improvement observed in the EM metric. This underscores that the enhancement in the EX metric primarily results from reducing errors in column names, values, etc., thereby improving the EM metric and ensuring the generated SQL is more accurate.\nRobustness Tests. We modeled the relationships between column names and cell values as well as those between table names and cell values. The aim was to enhance the model's semantic understanding of column and table names, thereby enabling it to recognize syn-onyms for these names as expressed in questions. To verify this, we conducted robustness tests using the SPIDER-SYN dataset[6], which consists of questions from the SPIDER-DEV dataset that have been modified with synonym substitutions, resulting in column and ta-ble names not appearing directly in the questions. This approach, as demonstrated in Table 3, aligns with the diversity of user questions in real-world scenarios.\nWe used the same model to predict SQL on the validation set of SPIDER-SYN, and the results are presented in Table 2. From the re-sults in Table 2, it can be seen that the semantically enhanced model also shows some improvement in robustness, with both EX and EM metrics being higher than the baseline model. This indicates that the semantically enhanced model has a more effective understanding of the semantic information of column and table names.\nCase Study. We primarily focus on case analyses of the results from the CodeLlama-7b-Instruct and Baichuan2-7B-Chat models. From this column, it is evident that the SQL statements generated by the Baichuan2-7B-Chat model without domain database knowledge infusion exhibited value and column er-rors. For instance, the column mentioned in the question is \"type of pet,\" but the baseline model produced \"pet_type,\" indicat-ing the model's understanding of the question but unfamiliarity with the database's schema, resulting in an incorrect column name. In con-trast, the Baichuan2-7B-Chat model, after domain database knowl-edge infusion, accurately generated the column name. Additionally, regarding value errors, the baseline model incorrectly associated the value \"Aberdeen,\" which should belong to the \"City\" column of the \"AIRPORTS\" table, with the \"DestAirport\" column of the \"flights\" table. This mistake shows the model correctly extracted the value \"Aberdeen\" from the question but misconnected the col-umn name. Moreover, the logic of this statement did not align with the question, whereas the optimized model correctly generated the statement with logical consistency. This demonstrates that infusing domain database knowledge not only aids in correcting errors related to values and columns but also enhances the model's ability to under-"}, {"title": "Evaluating Improvements in the Handling of Hallucination Is-sues", "content": "We also performed an error analysis on the column names gen-erated in SQL because we aim to diminish the hallucination problem in LLMs and reduce the errors in generating column and table names by increasing their co-occurrence frequency. For this purpose, we conducted a SQL analysis using deepseek and Codellma as exam-ples. Our calculation method is $C_{correct}/C_{total}$, where $C_{total}$ is the total number of column names generated in the SQL, and $C_{correct}$ is the number of those generated column names that exist within the schema (meaning the column names are correct and not fabricated by the model).\nThe results are shown in Table 5. The data in the table reveals that the accuracy of the column names generated by the model, which has been enhanced with domain-specific database knowledge, has improved by 2% to 4%. This improvement also suggests a decrease in the generation of incorrect column names resulting from the hal-lucination problem in LLMs."}, {"title": "Performance on Unseen Database", "content": "During the semantic knowl-edge injection phase, our model utilized prior knowledge data con-structed from the SPIDER training and development set databases; hence, we did not perform semantic enhancement on the test set database (the databases of the test set and development set do not overlap). We validated our model, which had been subjected to knowledge injection using the training and development set databases, on SPIDER test set. On one hand, we aimed to verify whether the knowledge-injected model would affect its capability with unknown databases. On the other hand, we wanted to further confirm that the improvement from domain knowledge injection en-hances the model's understanding of databases it has learned about, while maintaining its original capabilities with unseen databases. This is because the method of this paper is not aimed at enhancing the model's generalizability, but rather at improving its understanding of domain-specific databases.\nThe experimental results, as shown in Table 6, indicate that the model's performance on unknown databases has not been impacted and remains similar to that of the baseline model. This also validates the idea presented in this paper."}, {"title": "Ablation Study", "content": "We conducted an ablation analysis using the CodeLlama-7b-Instruct model as an example, comparing the ef-fects of infusing knowledge related to database values and database schema information."}, {"title": "Necessity of Phased Training", "content": "We mixed domain knowledge data with training data for the Text-to-SQL tasks and trained them to-gether. When comparing the effects of integrating the knowledge in-jection task with the Text-to-SQL task, we found that the combina-tion affected the model's performance. As shown in table 8. This decline can be attributed to the misalignment between the training objectives of upstream database knowledge infusion tasks and down-stream Text-to-SQL tasks, coupled with the predominance of up-stream task data preventing sufficient training for the downstream task. This also demonstrates the necessity of phased training."}, {"title": "4 Related Work", "content": "In Text-to-SQL tasks, controlling the correctness of generated col-umn names and values due to the model's unpredictability has always been a challenge. Numerous studies focus on ensuring the accuracy of column names, value joins, and syntactic correctness. These ef-forts differ from the approach proposed in this paper; they either control at the decoding end, such as through syntax trees [21, 8, 30], or incorporate schema linking information during the fine-tuning phase of downstream tasks [25]. Additionally, some utilize graph neural networks to connect the relationships between data tables and columns [2, 4, 9]. Our method focuses on injecting knowledge into the base model to address these issues, thereby also enhancing the base model's understanding of databases.\nMany studies improve the model's table comprehension abilities through pre-training on massive tabular data [31, 33, 5]. These meth-ods require significant computational resources for pre-training on vast tabular data and lack specificity. In contrast, our approach specif-ically injects knowledge into domain databases, making it more tar-geted and resource-efficient.\nThe volume of Text-to-SQL work related to Large Language Mod-els (LLMs) is steadily increasing, with many studies achieving im-pressive results through in-context learning [3, 16, 12] or Chain-of-thought approaches [27, 16] on platforms like ChatGPT. Further-more, there are efforts focused on fine-tuning pre-trained models [22, 28, 17], with current state-of-the-art (SOTA) methods based on fine-tuning achieving notable success [11, 19], utilizing models with up to 3 billion parameters. However, methods involving even larger scale models require further exploration. Our proposed method pri-marily experiments on larger scale open-source models, providing a new experimental baseline for open-source LLMs."}, {"title": "5 Conclusion and Future Work", "content": "We propose a domain database knowledge injection method that in-corporates the semantic information of column names, table names, and schema into LLMs through the design of three training objec-tives. This method successfully elevates the EX and EM scores, but it does come with certain challenges and constraints. Firstly, train-ing with data from databases may pose privacy risks if it involves users' sensitive data, potentially leading to privacy breaches. Sec-ondly, we observed that even with knowledge injection, the model's comprehension of complex instructions and the quality of gener-ated complex SQL queries are still not satisfactory. This suggests that our method has limited effectiveness in improving instruction understanding and addressing complex problems, necessitating the optimization of instruction comprehension capabilities through addi-tional approaches. Lastly, the proposed method in this paper, due to the addition of an extra training stage, also incurs increased training and time costs.\nIn future work, we aim to utilize privacy protection techniques to safeguard sensitive data[15, 24, 1]. Additionally, we plan to ex-plore methods such as CoT (Chain-of-Thought) [27, 34] to optimize instruction comprehension and reasoning abilities. We will also con-sider how to inject knowledge during the downstream task training stage, thereby reducing training costs."}]}