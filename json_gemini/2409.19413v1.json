{"title": "Membership Privacy Evaluation in Deep Spiking Neural Networks", "authors": ["Jiaxin Li", "Gorka Abad", "Stjepan Picek", "Mauro Conti"], "abstract": "Artificial Neural Networks (ANNs), commonly mim- icking neurons with non-linear functions to output floating- point numbers, consistently receive the same signals of a data point during its forward time. Unlike ANNs, Spiking Neural Networks (SNNs) get various input signals in the forward time of a data point and simulate neurons in a biologically plausible way, i.e., producing a spike (a binary value) if the accumulated membrane potential of a neuron is larger than a threshold. Even though ANNs have achieved remarkable success in multiple tasks, e.g., face recognition and object detection, SNNs have recently obtained attention due to their low power consumption, fast inference, and event-driven properties. While privacy threats against ANNs are widely explored, much less work has been done on SNNs. For instance, it is well-known that ANNs are vulnerable to the Membership Inference Attack (MIA), but whether the same applies to SNNs is not explored. In this paper, we evaluate the membership privacy of SNNS by considering eight MIAs, seven of which are inspired by MIAS against ANNs. Our evaluation results show that SNNs are more vulnerable (maximum 10% higher in terms of balanced attack accuracy) than ANNs when both are trained with neuromorphic datasets (with time dimension). On the other hand, when training ANNs or SNNs with static datasets (without time dimension), the vulnerability depends on the dataset used. If we convert ANNs trained with static datasets to SNNs, the accuracy of MIAs drops (maximum 11.5% with a reduction of 7.6% on the test accuracy of the target model). Next, we explore the impact factors of MIAs on SNNs by conducting a hyperparameter study. Finally, we show that the basic data augmentation method for static data and two recent data augmentation methods for neuromorphic data can considerably (maximum reduction of 25.7%) decrease MIAs' performance on SNNs. Regardless, the accuracy of MIAS could still be between 51.7% and 66.4% with data augmentation, indicating data augmentation cannot fully prevent MIAS on SNNS.", "sections": [{"title": "I. INTRODUCTION", "content": "Artificial Neural Networks model the behavior of a neuron with non-linear functions. As large amounts of data are collected and computing capabilities are enhanced, ANNs, especially deep neural networks [1], demonstrate an amazing ability to solve real-world tasks like face recognition [2] and object detection [3]. The third generation of neural network models, Spiking Neural Networks [4], mimic the dynamics of a neuron in a way closer to the actual neurons in the brain."}, {"title": "II. BACKGROUND", "content": "In Section II-A, we introduce SNNs and three main methods to build SNNs. Next, we give a simple explanation of the pipeline of MIAs in Section II-B."}, {"title": "A. Spiking Neural Network", "content": "An SNN is an application of the biological neuron into AI for efficient and low-cost computation. There are three main methods to train SNNs.\n(1) Spike-Timing Dependent Plasticity (STDP). For a pair of presynaptic and postsynaptic neurons, the synaptic weight of the synapse between those two neurons alters according to the arrival time of spikes from the presynaptic neuron and the firing time of the postsynaptic neuron. As an example, we consider a neuron \"A\" and a neuron \"B\" to be connected. The faster \u201cA\u201d fires, the stronger the connection with \u201cB\u201d, and vice versa; when \u201cA\u201d does not fire as much, the connection is weakened. The weight will increase if the presynaptic neuron fires early and the postsynaptic neuron fires later. The weight will decrease if the presynaptic neuron fires later while the postsynaptic neuron fires early.\nHowever, we do not consider this strategy because STDP is suitable for shallow SNNs. For deep SNNs, finding suitable hyperparameters and training with STDP is difficult, as men- tioned in the previous work [29]. Our focus is deep SNNs, and we obtain deep SNNs trained via backpropagation or converted from ANNs.\n(2) Conversion from ANN. Under this strategy, a pre- trained ANN is converted to an SNN by replacing the ReLU activation layers (that are not used in SNNs) with spiking neurons and adding scaling operations like weight normal- ization and threshold balancing [13], [14], [18], [30], [15]. Cao et al. [13] proposed using the absolute values of negative activations, changing Tanh to ReLU, removing biases, and using spatial linear subsampling instead of max-pooling. Diehl et al. [14] further analyzed performance loss and suggested weight normalization methods to address the over- and under- activation of spiking neurons. Instead of applying the ReLU activation function while training an ANN, Hunsberger et al. [18] used a modified non-linearity LIF neuron and in- jected noise during training, improving the robustness of the converted SNN's approximation errors. Rueckauer et al. [30], [15] addressed approximation errors between spiking neuron fire rates and ANN activations, which degrade the accuracy of deep models, by resetting potentials through subtraction and retaining biases as constant input currents scaled by maximum ReLU activation.\nIn our experiments to attack converted SNNs with MIAs, we follow previous works [30], [15]: i) reset the potential by subtraction (the membrane potential threshold $V_{th}$ = 1.0), ii) re-scale weights and biases with the robust normalization (p = 99.9%), iii) achieve batch normalization via scaling weights and biases, and iv) directly feed the input to the converted SNN, and pass the output through a softmax func- tion. We do not consider max-pooling because it requires estimating presynaptic firing rates, which is computationally complex [30] and does not give much benefit. Hence, we follow the strategy of average pooling from the work of Dieh et al. [14].\n(3) Backpropagation-based supervised learning. Under this strategy, we train SNNs via backpropagation, the same way as training ANNs. Among various neural models [31], [32], the Leaky Integrate and Fire (LIF) model can mimic the behavior of the biological neuron with a minimum number of circuit elements [33] and bring a lower complexity. As LIF is frequently used in previous works [34], [19], [35], we utilize the LIF model as well. The membrane potential of a spiking neuron i under the LIF model is formulated as given in Eqs. (1) and (2). In Eq. (1), $V_{rest}$ is the membrane potential of a neuron without any input. We set $V_{rest}$ as 0 following the related work [35]. V(t) is the membrane potential of neuron i at time t. $\\tau$ is the membrane time constant. In Eq. (2), the neuron i has $n_i$ presynaptic neurons. For a presynaptic neuron j, it has a list of spiking times $t_{pre}$, $w_{ij}$ is the synaptic weight between neuron i and a presynaptic neuron j, and $\\delta$(t) is the Dirac delta function (if t $\\neq$ 0, $\\delta$(t) = 0, $\\int_{-\\infty}^{\\infty} \\delta$(t)dt = 1)."}, {"title": null, "content": "$\\tau \\frac{dV_i(t)}{dt} = -(V_i(t) \u2013 V_{rest}) + X_i(t).$"}, {"title": null, "content": "$X_i(t) = \\sum_{j=1}^{N_i} \\sum_{t_{pre} \\in t_{pre}} w_{ij} \\delta(t - t_{pre}).$"}, {"title": "To consider V(t) in discrete time, we obtain Eq. (3) after applying the Euler method to Eq. (1).", "content": "$H(t) = V(t-1) + \\frac{1}{\\tau}(-(V(t \u2212 1) - V_{rest}) + X(t)).$"}, {"title": "Apart from the neuronal dynamics, the neuron will elicit a spike S(t) to the subsequent neurons if the membrane potential is larger than a threshold $V_{th}$, which is formulated in Eq. (4).", "content": "$\\begin{aligned}S(t) & =\\begin{cases}1 & \\text{if } H(t) \\geq V_{th},\\\\0 & \\text{others.} \\end{cases}\\end{aligned}$"}, {"title": null, "content": "$V(t) = H(t)(1 \u2013 S(t)) + V_{reset}S(t).$"}, {"title": "To apply backpropagation to the training of SNN, it is a common practice to keep S(t) for the forward pass and replace S(t) with a differentiable surrogate function to calculate the backward gradient of the backpropagation as S(t) is not differentiable [36]. Among potential surrogate functions, we set Atan as the default one as the performance of the original task is higher than applying other explored surrogate functions, as shown in the hyperparameter study in Section V-B. The Atan function is formulated as Eq. (6), where Eq. (7) is its gradient. a is the pre-defined parameter with a default value of 2.0, following the implementation of SpikingJelly [37].", "content": "$S(t) = \\frac{1}{\\pi}arctan(2\\alpha H(t)) + \\frac{1}{2}$"}, {"title": null, "content": "$\\frac{\u2202S(t)}{\u2202H(t)} = \\frac{\\alpha}{2(1+(\\alpha H(t))^2)}.$"}, {"title": "In the forward computation of each frame within the model, SNN aligns the spiking outputs to the one-hot encoding representation of the target category to make the spiking neuron belonging to the target category output a spike while other neurons do not. An MSE loss is calculated according to the matching degree of the spiking output and the one- hot encoding representation. The optimization guided by the MSE loss via Adam [38] or SGD [39], standard optimization algorithms in ANNs, implements the alignment by updating the weights.", "content": null}, {"title": "B. Membership Inference Attack", "content": "MIAs try to infer if a data point has been used during a target model's training by analyzing it or by observing its behavior to arbitrary inputs. Since the fist works of MIA in machine learning [26], [27], many works have investigated MIAs with different adversary knowledge [40], [41], [42], [43], various model types [44], [45], [46], and distinct at- tack methods [47], [24], [28], [48], [49]. Formally, MIA is formulated as a function A : x, \u039c,\u03a9 \u2192 {0,1} with a data point x, the target model M, and external knowledge \u03a9 of the adversary. The output 1 means that x is a member of M's training data and 0 otherwise.\nCurrently, there are two main strategies for implement- ing MIAs: classifier-based and threshold-based methods. In classifier-based methods, the adversary trains the attack model, a classifier, to predict whether a data point is a member based on features (e.g., the confidence scores [26]) extracted from this data point from the target model. The adversary trains a shadow model to mimic the target model's behavior. The adversary does not have access to the original training data but to some data with a similar distribution to the original. Then, the adversary constructs an attack model by using a dataset extracted from the features obtained by querying the shadow model.\nFor the threshold-based methods, the adversary directly compares a data point's metric (e.g., loss [47]) with a threshold to predict its membership. Usually, this threshold is determined according to the average value of the metric in the training data or is selected as the one that obtains high performance on the metric values of the shadow model."}, {"title": "III. MEMBERSHIP PRIVACY EVALUATION", "content": "In this part, we define a data point in the neuromorphic dataset and the input and output of the SNN in Section III-A. In Section III-B, we provide the threat model. Section III-C explains the methods with detailed metrics to evaluate the membership privacy of SNNs."}, {"title": "A. Definitions", "content": "A data point in the training data of SNNs differs from a data point in the training data of ANNs. Let us take the image classification task as an example. A data point used for training ANNs is a three-dimensional (RGB) array, each of which is a pixel value (after normalization) ranging from 0 to 1. In rate coding SNNs, a data point is a list of time-series events measuring the brightness change during the relative movement between the object (or its image) and the Dynamic Vision Sen- sor (DVS) camera. The DVS camera will generate a positive polarity event with coordinate information if the brightness of a pixel increases over a threshold. If the brightness decreases below a threshold, the DVS camera will generate a negative polarity event with coordinate information. Hence, there are two types of events, categorized in two dimensions (positive and negative), in the list of events that belong to a data point. In neuromorphic datasets from SpikeJelly [37], a list of time- series events is accumulated into a fixed number (i.e., time steps) of frames for each data point. This indicates a data point of SNNs is a fixed number of frames.\nInstead of inferring the membership of one static image in ANNS, MIAS on SNNs need to consider the membership of multiple static images (frames). A data point with multiple static images has more neighboring data points than one with one static image. More neighboring data points make it harder to predict the existence of a specific data point, as the inclusion of neighboring data points could disturb the prediction of MIA [24]. To solve this problem, we utilize the fire rate of spiking neurons and the membrane potential as the features. The reason for selecting the fire rate is that the fire rate contains the spiking outputs of multiple frames of a data point rather than the spiking output of one step. Hence, the fire rate is suitable for representing the prediction towards multiple frames. As for the membrane potential, the forward of one frame in the SNN will modify the membrane potential of neurons in the SNN, and the accumulation of membrane potential on multiple frames represents the prediction situation of multiple frames to a certain extent. Hence, we choose the fire rate and the membrane potential as the features. For a data point $x \\in \\mathbb{R}^{T \\times 2 \\times H \\times W}$ (T, 2, H, and W are the time steps, positive and negative channels, height, and width of the input), the target SNN M outputs the spiking times among T steps as $M(x) \\in \\mathbb{R}^{T \\times n}$ (n is the number of classification categories), each of which represents whether a spiking neuron evokes a spike at current time spot. Therefore, the fire rates of Mon x is $Fr(x) = \\sum_{t=1}^{T}M(x)_t \\in \\mathbb{R}^{n}$ and $M(x)_t \\in \\mathbb{R}$ is spiking times at time spot t. For the last layer composed of spiking neurons (the number of neurons is m), the membrane potential of spiking neurons for x is $Mp(x) \\in \\mathbb{R}^{T \\times m}$. Therefore, the average membrane potential among T time steps is $AMp(x) = \\frac{1}{T} \\sum_{\u03c4=1}^{T}M_p^t(x)$ and $M_p^t(x) \\in \\mathbb{R}^m$ is the membrane potential at time step t. We utilize fire rates and average membrane potential of the target SNN M on a data point as the signal to predict its membership."}, {"title": "B. Threat Model", "content": "Following the common practice of MIA [26], the adversary holds a shadow dataset from the same distribution as a target dataset used for training and testing the target SNN \u039c. There are no overlapping data points between the shadow and target datasets. The adversary knows M's hyperparameters and model structure to train a shadow SNN M\u2083 for mimicking M, following previous works [26], [28]. Controlling the training and test data of Ms, the adversary can extract attack features from the training and test data of Ms by feeding data into Ms and label features as members (from training data) and non- members (from test data) separately. With the attack features and corresponding labels, the adversary trains a classifier or determines a threshold for distinguishing members and non- members. With the classifier (attack model) or threshold, the adversary evaluates the performance of MIA on the target SNN M. For attack features, we assume the adversary knows the fire rates Fr(x) and the average membrane potential $AM_p(x)$ of a data point x to evaluate the membership privacy of M under various MIAs. Finally, the adversary knows the loss, prediction label, and ground truth y of x, following the assumptions in previous MIAs on ANNs [26], [47]."}, {"title": "C. Methodology", "content": "We follow strategies of previous MIAs (with confidence scores) on ANNs to leverage fire rates to implement MIAS on SNNs. The reason is that the fire rate is a similar in- dicator to the confidence score to show the confidence of the prediction. The training of backpropagation-based SNNS aligns the spiking neurons' output with the one-hot encoding representation of the target category with MSE. The alignment aims for SNN to only output a spike in the neuron belonging to the target category at each time step. Ideally, the fire rate of the neuron belonging to the target category is one, and other neurons have a fire rate close to zero. Hence, the fire rate reflects the SNN's confidence in the prediction of the data point. In the previous work [26], the ANN'S prediction confidence gap on the training and test data makes MIAs feasible. Therefore, we follow previous strategies when handling confidence scores to use fire rates. We select five representative methods in the field of MIA, including the first MIA on machine learning [26], two methods relaxing assumptions of MIA [27], the performance improvement with modified entropy (Mentr) [50], and performance improvement with logit-scaled confidence [28].\nApart from the five mentioned methods, we also choose two methods from the work of Yeom et al. [47]. One is the MIA based on loss, and the other is based on the prediction correctness. The reason for selection is that the loss of the training data tends to be lower than the loss of the test data after training, and the MIA based on prediction correctness provides a baseline method to directly infer the data points correctly predicted by the target model as the training data. To compare the vulnerability of MIAs on ANNs and SNNS, we apply those seven methods. For the average membrane potential, we take it as the feature of the attack model to directly infer the membership following the basic strategy in [26]. For MIAs on ANNs, we also evaluate hinge loss based on logits (the output before the softmax layer in the ANN) due to its more straightforward computation than the logit-scaled confidence and competitive performance [28].\nIn summary, we evaluate MIAs on SNNs with eight methods based on fire rates and membrane potential. For comparison, we implement eight MIAs on ANNs based on (1) confidence scores [26], (2) loss [47], (3) prediction correctness [47], (4) top-3 confidence scores [27], (5) maximum confidence score [27], (6) logit-scaled confidence [28], (7) hinge loss [28], and (8) Mentr with confidence scores [50]. For MIAs against SNNs, we discuss the details of each method as follows.\n(1) fire rates. We take the fire rates Fr(x) of each data point as the feature of the attack classifier to predict the membership of each data point. (2) loss. We compare the single loss related to each data point with a threshold to determine its membership. The adversary evaluates MIAs on the shadow model and its training and test data to select the threshold that better distinguishes losses of training and test data of the shadow model.\n(3) prediction correctness. If the target SNN M correctly predicts the classification label of x, we predict x is a member and non-member otherwise.\n(4) top-3 fire rates. Instead of using all the fire rates, we select top-3 fire rates as the feature of the attack classifier, following the strategy in the previous work [27].\n(5) maximum fire rate. We use the maximum fire rate to compare with a threshold. If the maximum fire rate exceeds the threshold, we predict x as member and non-member otherwise.\n(6) logit-scaled fire rate. We compute the logit-scaled fire rate as Eq. (8), where Fr(x)y is the fire rate of the spiking neuron of the ground truth. We replace the confidence score of logit-scaled confidence [28] with the fire rate to obtain this metric. Similarly, we compare the logit-scaled fire rate with a threshold for the membership prediction."}, {"title": null, "content": "$\\frac{log(Fr(x)_y) - log \\sum_{y^\\neq y} Fr(x)_{y^\\prime}}{\\sum_{y \\neq y^\\prime} log Fr(x)_{y^\\prime}}$"}, {"title": "(7) Mentr with fire rates. Following the Mentr with confidence scores in the previous work [50], we calculate the Mentr with fire rates as Eq. (9). We compare this metric with a threshold while deciding on a membership of x.", "content": null}, {"title": null, "content": "$\\frac{-(1 - Fr(x)_y) log(Fr(x)_y) - \\sum_{y\\neq y^\\prime} Fr(x)_{y^\\prime} log(1 - Fr(x)_{y^\\prime})}{\\sum_{y\\neq y^\\prime}}$"}, {"title": "(8) average membrane potential. Apart from fire rates, we utilize the average membrane potential as the feature of the attack classifier to predict membership. Similarly, we train the attack classifier with features extracted from the shadow model on its data.", "content": null}, {"title": "IV. EXPERIMENTAL SETTINGS", "content": "We discuss the datasets used in our experiments in Sec- tion IV-A. Then, we discuss the ANN and SNN models in Section IV-B. In Section IV-C, we detail the settings of training models, including target, shadow, and attack models. Besides, we clarify the evaluation metric."}, {"title": "A. Datasets", "content": "We select three neuromorphic datasets, N-MNIST [51], CIFAR10-DVS [52], and N-Caltech101 [51], to explore MIAs against SNNs. For comparison, we choose corresponding static versions (i.e., MNIST, CIFAR10, and Caltech101) of those three datasets to train ANNs. Table I shows the shape of a batch of data points and the number of data points in each dataset. Among the numbers representing the shape, the T, \u0412, and the last two numbers separately indicate the time steps, batch size, and height and width of data points."}, {"title": "B. Models", "content": "We select three model structures to train SNNs, including the model defined in the work of Fang et al. [35] (we denote it CNN for convenience), VGG11 [53], and ResNet18 [54]. The number of convolutional, downsampling, and fully connected layers (i.e., Nconv, Ndown, and Nfc) in CNN from the work of Fang et al. [35] is given in Table II. Each downsampling layer comprises Nconv convolutional layers and a max-pooling layer. Ndown downsampling layers and Nfc fully connected layers make the final model. For N-Caltech101, we modified the model structure for DVS128 Gesture from [35] to fit the input size of N-Caltech101. For ANNs with structures of VGG11 and ResNet18, we replace the neuron formulated as a ReLU activation function with spiking neurons to construct the corresponding SNNs with structures of VGG11 and ResNet18. We keep the weights and connections between neurons. Con- sidering that there are two input channels for the neuromorphic data and that the original input of VGG11 and ResNet18 is expected to have three channels, we add a convolutional layer to increase the number of channels (upsampling). For the CNN originally defined for neuromorphic datasets in the work of Fang et al. [35], we modify the spiking neurons to artificial neurons with a ReLU activation function and change the input channel of the first convolutional layer to three as the number of channels in the static data is three. It is the opposite process compared to modifying the ANN to the SNN. Note that instead of directly utilizing the previous CNN (like AlexNet [55]) to train on static datasets, we modify the CNN originally defined for neuromorphic datasets (SNN) to the CNN used for static datasets. This modification allows a fair comparison between SNNs trained with neuromorphic datasets and ANNs trained with static datasets. For conversion from ANNs to SNNs, we discussed the conversion operation in Section II-A."}, {"title": "C. Settings and Evaluation Metric", "content": "For the classifier-based MIA, the attack model is a multi- layer perceptron (MLP) with 2 hidden layers, each with 64 neurons, following the previous work [43]. The single-value output of the MLP represents the probability of being predicted as a member. For ANNs and SNNs trained with neuromorphic and static datasets, per default, we utilize the identical learning rate of 0.001, Adam optimizer, and batch size range from 2 to 16 due to the input size. For ANNs, the loss function is cross-entropy loss, while the loss function is Mean Squared Error (MSE) for SNNs. The number of epochs is 30, 50, and 60 for MNIST (N-MNIST), CIFAR-10 (CIFAR10-DVS), and Caltech101 (N- Caltech101), respectively. The default number of time steps for ANNs is 16. For the attack model, the optimization algorithm is Adam, with a learning rate of 0.001. The number of training epochs is 300, and the batch size is 32. We use the Binary Cross Entropy (BCE) to guide the training of the attack model. In the hyperparameter study in Section V-B, we explore the impact of the optimizer, learning rate, and time steps on the performance of target models and MIAs, as they are vital settings for training SNNs.\nFor the evaluation metric of MIAs, we follow the previous works [26], [28] and use the balanced accuracy as the com- parison metric of MIAs."}, {"title": "V. RESULTS AND DISCUSSIONS", "content": "We compare the performance of MIAs on ANNs and SNNs in Section V-A. Next, we provide a hyperparameter study in Section V-B. Finally, we evaluate the basic augmentation method for static datasets and two augmentation mechanisms for neuromorphic datasets as the defenses against MIAs in SNNs in Section V-C."}, {"title": "A. MIAs on SNNs and ANNS", "content": "We first compare the vulnerability of SNNs and ANNs under MIAs. To make the comparison fair, we make the structures of SNNs and ANNs similar except for the neurons. The hyperparameters (time steps, learning rate, batch size, and dataset split) are identical when utilizing the same dataset. We do not leverage data augmentation or other techniques to improve the generalization of models here, as those techniques usually differ between ANNs and SNNs [56]. However, we explore how to defend against MIAs with data augmentation mechanisms by improving the generalization of models in Section V-C."}, {"title": "B. Hyperparameter Study", "content": "To explore the factors that impact the performance of MIAS on SNNs, we conduct a hyperparameter study to vary the spiking neuron type, surrogate function, optimizer, learning rate, and time steps during the training of SNNs.\n1) Varying Spiking Neuron Type and Surrogate Function: For the backpropagation-based SNNs, we not only train the SNN with the LIF neuron type and the surrogate function of ATan to get the results in Table VIII but also explore other neuron types and surrogate functions. We select neuro- morphic datasets for training SNNs as examples to analyze the impact of spiking neuron type and surrogate function. We utilize three spiking neuron types (LIF, EIF [58], and Izhikevich [59]) and two surrogate functions (ATan and Piece- wiseLeakyReLU [60]). The reason for selecting those three spiking neuron types and two surrogate functions is that they are common in the research of SNNs [58], [59], [60], [36], [11].\nTable III provides the accuracy of the target model and the MIA with the highest accuracy under each setting. For N-MNIST, the SNN with PiecewiseLeakyReLU function and Izhikevich neuron is the most vulnerable one with the high- est attack accuracy of 55.5%. For CIFAR10-DVS and N- Caltech101, SNNs with the ATan function and the LIF neuron have the highest attack accuracy (83.6% in CIFAR10-DVS and 77.6% in N-Caltech101) compared to other functions and neurons. This is mainly due to the larger generalization gap if the SNN is defined with the ATan function and the LIF neuron, even though the test accuracy is higher than that of other spiking neurons and surrogate functions.\n2) Varying Optimizer and Learning Rate: As the selection of optimizer and learning rate impacts the generalization of the final trained model, we explore their influence on the final accuracy of SNNs and the performance of MIAs on SNNs. Table IV provides the accuracy of the target model and the MIA with the highest accuracy while training with different optimizers and learning rates. The table shows two settings unsuitable for training because their final classification accuracy is similar to random guessing, including Adam, with a learning rate of 0.1, and SGD, with a learning rate of 0.001. Indeed, the attack accuracy of MIAs is close to 50% under those two settings. For N-Caltech101, a higher generalization gap (35.4%) of the SNN trained with Adam and a learning rate of 0.001 leads to a higher attack accuracy of 79.5% compared to the SNN trained with SGD and a learning rate of 0.1. For CIFAR10-DVS, the SNN optimized with Adam and a learning rate of 0.001 has a higher generalization gap (37.5%) than the SNN trained with the SGD and a learning rate of 0.1 (34.7%). However, MIAs have a smaller attack accuracy (83.9%) on the SNN trained with Adam and a learning rate of 0.001. This shows that a higher generalization does not always indicate a higher attack accuracy. From the training and test accuracy of SNNs with different optimizers and learning rates, we also observe that the training of SNNs is sensitive to those hyperparameters. A large (0.1) or a small learning rate (0.001) could both lead to the final trained SNN at the random guessing performance. The learning rate suitable for an optimizer might not be suitable for another optimizer, which indicates optimizers and learning rates should be considered together for training SNNs."}, {"title": "3) Varying the Number of Time Steps: The number of time steps is an important factor for SNNs, as each data point", "content": "obtains the final prediction result based on the fire rate of the last spiking neurons during those steps. For each step, SNN takes one frame from a data point and updates the membrane potential of its spiking neurons to incur final spikes. We explore the impact of the number of time steps on the accuracy of target models and MIAs as shown in Table V. We observe that the gap between the highest attack accuracy obtained with time steps 8 and 16 is within 1%. For N- Caltech101, the attack accuracy under 4 time steps is higher due to a larger generalization gap. If we increase the number of time steps from 4 to 8, the highest attack accuracy will increase for N-MNIST and CIFAR10-DVS, even though the generalization gap slightly decreases with larger time steps. Hence, increasing the number of time steps would usually slightly increase the vulnerability towards MIAs to a certain stable level. We postulate the main reason for this phenomenon is that increasing the time steps means more frames of a data point will go through the model for updating its parameters, making the model have deeper memorization of the data point and, thus, MIA easier."}, {"title": "C. Defense Evaluation", "content": "Even though reducing the performance of MIA, Differential Privacy (DP) also causes a large utility loss, as demonstrated in the case of ANNs [26], [28]. Hence, we apply strategies for improving the generalization of SNNs to defend against MIAs. Indeed, the generalization gap is a commonly accept- able cause for MIAs [47], and the initial works [26], [27] also attempted to defend against MIAs with regularization and model ensemble. For neuromorphic datasets, we apply two strategies for data augmentation. The first strategy is EventDrop [61] to drop events in 1) a time interval, 2) an area of the coordinate, or 3) randomly. Following [61], we augment each data sample (a group of events) with the three mentioned dropping methods or do not augment them. The second strategy applies geometric augmentations (NDA [56]), including rolling, rotation, cutout [62], shear, flip, and Cut- Mix [63]. Following the mechanism of \u201cM1N2\" in the original paper, we apply flip and CutMix and randomly select one of the four left augmentation methods for each data sample. For static datasets, we apply basic strategies, including horizontal flip, random crop, and resize for data augmentation. Data augmentation will reduce the target model's generalization gap, leading to the performance drop of MIAs. Besides, NDA maintains the test accuracy of SNN (VGG11) at 61.4%, which is slightly higher than the test accuracy of SNN (VGG11) with EventDrop (60.8%).\""}, {"title": "VI. RELATED WORKS", "content": "This part introduces previous works on MIA and attacks against SNNs."}, {"title": "A. Membership Inference Attack", "content": "As people and governments pay more attention to personal data privacy, the development and research on MIA are flour- ishing, as discussed next. Since the successful membership inference attack in 2017 [26", "27": [47], "28": [48], "64": [42], "49": [43]}]}