{"title": "Meta-Statistical Learning: Supervised Learning of Statistical Inference", "authors": ["Maxime Peyrard", "Kyunghyun Cho"], "abstract": "This work demonstrates that the tools and princi-ples driving the success of large language models (LLMs) can be repurposed to tackle distribution-level tasks, where the goal is to predict properties of the data-generating distribution rather than la-bels for individual datapoints. These tasks en-compass statistical inference problems such as parameter estimation, hypothesis testing, or mu-tual information estimation. Framing these tasks within traditional machine learning pipelines is challenging, as supervision is typically tied to in-dividual datapoint. We propose meta-statistical learning, a framework inspired by multi-instance learning that reformulates statistical inference tasks as supervised learning problems. In this ap-proach, entire datasets are treated as single inputs to neural networks, which predict distribution-level parameters. Transformer-based architec-tures, without positional encoding, provide a natural fit due to their permutation-invariance properties. By training on large-scale synthetic datasets, meta-statistical models can leverage the scalability and optimization infrastructure of Transformer-based LLMs. We demonstrate the framework's versatility with applications in hy-pothesis testing and mutual information estima-tion, showing strong performance, particularly for small datasets where traditional neural meth-ods struggle.", "sections": [{"title": "1. Introduction", "content": "Statistical inference is the backbone of many scientific inquiries, providing a rigorous framework for quantifying evidence, testing hypotheses, and estimating uncertainty (Walker and Lev, 1953; Casella and Berger, 2024). Across scientific disciplines, statistical inference is an ac-cepted mechanism through which scientists relate noisy observations to theoretical models; it underpins the design of experiments, the validation of theories, and the interpretation of empirical results (Barlow, 1993; Altman, 1990; James, 2006; Dienes, 2008; Salganik, 2019).\nHowever, the practice of statistical inference is notoriously difficult. Real-world data is noisy often deviating from idealized assumptions (Gurland and Tripathi, 1971; Hoekstra et al., 2012; Knief and Forstmeier, 2021; Czy\u017c et al., 2023). In particular, inference in low-sample regimes presents a persistent challenge, yet it is crucial across many applied sciences. In such settings, estimators must balance universality with bias and variance, whereas more robust estimators require strong assumptions on the underlying distribution (Casella and Berger, 2024). Some statistical quantities simply lack universally unbiased estimators \u2013 like the standard deviation \u2013 necessitating context-dependent correction strategies (Gurland and Tripathi, 1971; Bengio and Grandvalet, 2003). In general, designing statistical estimators requires making choices regarding the bias-variance and robustness-universality trade-offs, requiring manual effort to craft estimators to specific goals (Silvey, 2013).\nMachine learning, itself a form of statistical inference, can provide a flexible approach to these challenges. Instead of manually designing statistical estimators, we propose to learn them from data, leveraging amortized learning strategies to train models that generalize across diverse data distributions and adapt their estimation strategy contextually to new inputs. We call this approach meta-statistical learning, wherein entire datasets are treated as input objects and statistical inference tasks are directly framed as supervised learning problems (see Figure 1).\nMeta-statistical learning shifts the unit of analysis from individual data points to entire datasets. Unlike traditional supervised learning, where the goal is to predict a label y for an individual sample x drawn from a joint distribution Px,y, meta-statistical models learn to map datasets to their target statistical properties from large amounts of synthetic datasets. This formulation aligns naturally with modern deep learning tools, which can easily handle various input modalities such as images (Voulodimos et al., 2018), graphs (Ma and Tang, 2021), time-series (Gamboa, 2017; Lim and Zohren, 2021; Torres et al., 2021),"}, {"title": "2. Meta-Statistical Learning", "content": "Supervised learning aims to find a function f : X \u2192 Y that maps input data to output labels based on a finite dataset of observations 9 = {(xi,yi)}'=1, where (xi,yi) \u2208 X \u00d7 Y and i\u2208 {1,...,n}. Here, X denotes the input space (e.g., Rd for d-dimensional data), and I denotes the output space, which is continuous for regression or discrete for classification. The data points are assumed to be i.i.d. samples from an unknown joint distribution Px,y over XxY.\nThe function f is modeled by a parameterized family {fo : \u03b8\u2208 \u0398}, where @ represents the parameters (e.g., weights in a neural network). The quality of fe is evaluated using a loss function L : Y \u00d7 Y \u2192 R, which measures the discrepancy between predicted and true outputs.\nGeneralization. The goal is to minimize the expected risk: R(0) = E(x,y)~Px,y [L(fe(x), y)], but since Px,y is unknown, the empirical risk is minimized as a proxy. Generalization is achievable because machine learning algorithms perform induction, based on assumptions about the underlying structure of the data and the expectation of how new data relate to observed ones. Typically, we expect the model to generalize in distribution, where new instances are sampled from Px,y. However, we often also care about generalization out of distribution, where new instances are sampled from a different, but related distribution."}, {"title": "2.2. Meta-Statistical Learning", "content": "Instead of learning a mapping from individual data points to their labels, meta-statistical learning maps entire datasets to their labels. Meta-statistical learning remains within standard supervised learning with the dataset being just another modality representable by a neural network.\nSetup and notation. Meta-statistical learning aims to find a function \u03c6 : \u0413 \u2192 & that maps input datasets to labels based on a finite meta-dataset I = {(Di,yi)}=1, where Di \u2208 F is itself a dataset Di = {(xi,j)}=1. As in standard supervised learning, I denotes the output space, which is continuous for regression or discrete for classification. The meta-datapoints are assumed to be sampled i.i.d. from an unknown joint meta-distribution Pry, a distribution over datasets (their data-generating distribution) and their target labels. The function is modeled by a parameterized family {\u03c6\u03bf: \u03b8\u2208 \u0398} that can process entire datasets as input (e.g., a recurrent neural network, convolutional neural network, or Transformer). The quality of e is still evaluated using a loss function Lr : Y \u00d7 Y \u2192 R. The learning objective remains to minimize the expected risk, but taken over"}, {"title": "2.3. Structure of the Meta-Generalization Problem", "content": "To provide additional structure to the generative process that produces a meta-datapoint (Di,yi) ~ Pr,y, we decompose it into two steps: (i) sample a distribution Px, and (ii) sample a dataset D\u00a1 ~ Px. This process is illustrated in Fig-ure 1. The label can either be a property of the dataset itself, yi = A(Di), or a property of the distribution, y\u2081 = g(Px). When the label is a property A of the dataset, we refer to it as a descriptive label, such as the column-wise average. When the label is a property g of the distribution, we refer to it as an inferential label, such as determining whether the dataset was sampled from a normal distribution or estimating the mutual information between two variables.\nSeveral generalization questions arise from this setup:\n(i) Within-distribution generalization: The function e should generalize across different datasets resampled from the same distribution Px. In the inferential case, where the label depends only on Px, e should produce the same pre-diction for all datasets of fixed size sampled from Px. The predictions of e should not systematically overestimate or underestimate the label. This is measured by the variance and the bias of e as a statistical estimator of y = g(Px).\n(ii) Length generalization: The function e should gen-eralize to datasets of varying lengths. Statistical inference is harder for smaller datasets, so we expect performance to improve with larger datasets. This is measured by the consistency of fe as a statistical estimator of y = g(Px).\n(iii) In-meta-distribution generalization: Similar to stan-dard supervised learning, e should generalize to new meta-datapoints sampled from the same meta-distribution Pry. For example, if ye is trained to predict the stan-dard deviation of datasets sampled from exponential dis-tributions, it should generalize to exponential distributions with unseen rate parameters.\n(iv) Out-of-meta-distribution generalization: Analogous to out-of-distribution generalization, e could be expected to generalize to distributions and datasets sampled from a different meta-distribution than Pry. For instance, if ye is trained on datasets from Normal, Uniform, and Exponential distributions, it can be tested on datasets sampled from Log-normal, Cauchy, or Weibull distributions."}, {"title": "2.4. Related Work", "content": "The idea of processing multiple data points simultaneously originates from multi-instance learning, where models receive sets of instances and assign labels at the group level (Maron and Lozano-P\u00e9rez, 1997; Dietterich et al., 1997;"}, {"title": "3. Experimental Setup", "content": "Our experiments demonstrate the versatility of meta-statistical learning by achieving strong performance across diverse tasks with minimal task-specific effort. Here, we describe the template used to run experiments with various descriptive and inferential tasks."}, {"title": "4. Experiments on Descriptive Tasks", "content": "In descriptive tasks, the label y of a dataset is the out-put of an algorithm A applied to D, i.e., y = A(9). Simple tasks like median or correlation serve as unit testing of meta-statistical models. However, for more computationally intensive algorithms, such as optimal transport, meta-statistical models could serve as fast approximations. For datasets DE Rnxm, we consider four descriptive tasks: the per-column median label y \u2208 Rm consists of the me-dians of each column. The Pearson correlation coeffi-cient y \u2208 R is computed between the two columns. The win rate (Bradley-Terry) is the fraction of rows where the value in the first column exceeds that in the second: y=$\\frac{1}{n}$ \u03a3$_{i=1}^{n}$1(2,1 > Di,2), where I(\u00b7) is the indicator function. Finally, the 1D optimal transport (OT) label y \u2208 IR is the optimal transport cost between the empirical distributions of the two columns.\nMeta-Dataset Generation. To construct the meta-dataset, we sample datasets D from predefined probability distributions as described in Section 3. Once a dataset is sampled we simply compute the target label y by applying the target algorithm. We experiment with various numbers of columns m. By having k > 1, we produce k computation in parallel with one forward pass (independently of the batch dimension). We observe no significant difference when varying k and fix k = 2 in the experiments. The meta-dataset contains 30K training meta datapoints per task, with dataset sizes sampled from n \u2208 [5,300]. Details about meta-datasets and which distribution families are in- or out-of-meta-distribution are provided in Appendix A.\nMeta-Statistical Models. After optimizing hyperparam-eters and architecture choices (e.g., pooling mechanisms and head-to-dimensionality ratio) on a small validation set of 1K meta datapoints, we compare four meta-statistical model variants: LSTM, Vanilla Transformer (VT), and two ST2 variants with 16 or 32 inducing points. ST2(16) is the fastest model for both training and inference. In Appendix A.4, we show that VT scales quadratically, while LSTM and ST2 scale linearly, with better slopes for ST2. Additionally, ST2(16) achieves a 12x faster training time per batch normalized by parameters compared to VT, meaning an ST2(16) model with 12 times more parameters can be trained in the same time as VT. However, for consistency in reporting, we compare models with approximately the same number of parameters (~ 10K in this section).\nIn-meta-distribution performance. Table 1 shows the MSE of the four meta-statistical models on a test set sam-pled from the same meta-distribution as the training data. All models approximate the descriptive tasks well, but the LSTM-based model, lacking permutation invariance, performs worse than attention-based models. Notably, ST2,"}, {"title": "Generalization Performance.", "content": "We evaluate meta-statistical models' generalization capabilities on two as-pects: (i) Out-of-Meta-Distribution (OoMD): Datasets from unseen distributions. (ii) Length Generalization: Datasets with lengths outside the training range. Figure 2 shows strong length generalization, where models maintain their performance for larger datasets than seen during train-ing, both IMD and OoMD. They are also robust to OoMD datasets despite a small performance degradation. Manual inspection reveals that the degradation mainly comes from cases where the magnitude of the input values exceeds the range seen during training. This is discussed further in Sec-tion 6. Additional results and generalization plots are provided in Appendix A."}, {"title": "5. Experiments on Inferential Tasks", "content": "In inferential tasks, the label y represents a property g of the underlying distribution Px from which a dataset D is sampled: y = g(Px). We illustrate the meta-statistical framework with three such tasks: standard deviation estimation, normality testing, and mutual information estima-tion. Details on meta-dataset creation and models are in Appendix B. For all tasks in this section, the dataset sizes during training are sampled from n \u2208 [5,150], depicted by vertical red lines in the plots."}, {"title": "5.1. Standard Deviation Estimation", "content": "The standard deviation (\u03c3 = \u221aE[(X \u2013 E[X])2]) quantifies the spread of a distribution Px. Unlike the mean or variance, estimating o is non-trivial due to the square root's non-linearity (Gurland and Tripathi, 1971; Gupta, 1952). In fact, no universal unbiased estimator exists across all distributions (Gurland and Tripathi, 1971; Fenstad et al., 1980). We use this task to show meta-statistical learning in action.\nMeta-Dataset. To create the meta-dataset, we follow the procedure outlined in Section 3, keeping different distribution families for in- and out-of-meta-distribution. We use 100K meta datapoints for training.\nMeta-Statistical Model. We train two ST2-based models: ST2std, which predicts the standard deviation \u03c3, and ST2fsd,"}, {"title": "5.2. Normality Testing", "content": "The task is now to determine whether a dataset D ~ Px originates from a normal distribution, formulated as a bi-nary classification task: y = 1 if Px is normal, y = 0 oth-erwise. Normality testing is crucial in hypothesis test-ing, model selection, and preprocessing (Shapiro and Wilk, 1965; Razali et al., 2011), particularly before applying t-tests, linear regression, or ANOVA with small samples (Altman, 1990; Das and Imon, 2016; Kwak Sang Gyu, 2019). However, standard tests struggle in low-sample settings (Razali et al., 2011). We propose to train meta-statistical models for normality classification, aiming for robust generalization in such regimes. Details on meta-dataset creation and model properties are in Appendix C.\nMeta-dataset creation. We construct a balanced meta-dataset of normally and not normally distributed datasets"}, {"title": "Estimators.", "content": "We transform traditional normality tests into binary classifiers by thresholding their p-values, optimiz-ing the threshold on the training meta-dataset for maximum classification accuracy. We consider four widely used tests: the Shapiro-Wilk test (Shapiro and Wilk, 1965), known to be effective for small samples (Razali et al., 2011); the D'Agostino-Pearson test (D'agostino and Pearson, 1973), which combines skewness and kurtosis; the Kolmogorov-Smirnov test (Massey Jr, 1951), a non-parametric test based on cumulative distribution differences; and the Jarque-Bera test (Jarque and Bera, 1987), which assesses skewness and kurtosis deviations from theoretical expectations.\nWe then train two meta-statistical models: one based on VT and another on ST2 with 16 inducing points. Both use four layers, a hidden dimensionality of 32, and 12 atten-tion heads. The classification head is a single-layer MLP with 32 neurons, totaling approximately 50K parameters per model."}, {"title": "5.3. Mutual Information Estimation", "content": "Mutual information (MI) quantifies the dependency be-tween two random variables X and Y and is defined as:\nMI(X; Y) = // Px,y(x, y) log $\\frac{Px,y (x, y)}{Px(x)Py (y)}$ dxdy.\nHere, Px and Py denote the marginal distributions of X and Y, respectively.\nMI possesses key properties such as invariance to homeo-morphisms and adherence to the Data Processing Inequal-ity, making it fundamental in machine learning and related fields (Li et al., 2021; Belghazi et al., 2018; van den Oord et al., 2018; Tishby et al., 2000). However, MI estimation remains challenging, particularly for small sample sizes and non-Gaussian distributions (Song and Ermon, 2020; McAllester and Stratos, 2020; Czy\u017c et al., 2023).\nWe adopt a meta-statistical approach, training models to predict y = MI(X;Y) between two dataset columns. Fo-cusing on low-sample, non-Gaussian settings, but we restrict experiments to the one-dimensional case for simplic-ity. Details on meta-dataset creation, models, and extra re-sults are provided in Appendix D."}, {"title": "Meta-dataset Creation.", "content": "We construct a meta-dataset in-spired by the benchmark methodology in (Czy\u017c et al., 2023), where distributions with ground-truth MI are gen-erated in two steps: (i) by sampling a distribution with known MI, (ii) optionally applying MI-preserving transfor-mations. This process creates complex distributions and datasets with known MI. For generating meta-dataset in this way, we again follow the process described in Section 3 using different base-distribution and MI-preserving trans-formation between in-meta-distribution and out-of-meta-distribution. We use 50K meta datapoints for training.\nEstimators. We compare our approach with the best-performing 1D estimators from (Czy\u017c et al., 2023), includ-ing Kraskov-St\u00f6gbauer-Grassberger (KSG) (Kraskov et al., 2004), Canonical Correlation Analysis (CCA) (Murphy, 2023), and three neural estimators: MINE (Belghazi et al., 2018), InfoNCE (van den Oord et al., 2018), and NWJE (Nguyen et al., 2007; Nowozin et al., 2016; Poole et al., 2019). We train two meta-statistical models: one based on Vanilla Transformer (VT) and the other on Set Transformer 2 (ST2). Both models consist of five layers, with a hidden dimensionality of 256 and 12 attention heads. The regres-sion head is a single hidden-layer MLP with 128 neurons, resulting in models with approximately 1M parameters.\nEstimation Performance. The mean squared error (MSE) results for both in- and out-of-meta-distribution testing are shown in Table 3. Meta-statistical models outperform base-line estimators across all sample sizes, with significant ad-vantages in low-sample scenarios. Baseline models, partic-ularly neural ones, struggle with small sample sizes, while only KSG and CCA begin to match meta-statistical mod-els for sample sizes greater than 100 in the out-of-meta-distribution regime.\nBias and Variance of MI Estimators. We examine the bias and variance of MI estimators by resampling datasets from fixed distributions and measuring the variance and bias of the estimates. In Figure 5, we visualize the bias and variance for a challenging distribution identified by previ-ous works (Czy\u017c et al., 2023) (additive noise). Even at a sample size of n = 100, meta-statistical models show clear improvements in both bias (estimates centered around 0) and variance. A more detailed analysis of bias and variance is available in Appendix D (Table 7). Compared to base-line estimators, meta-statistical models demonstrate signif-icantly lower bias, close to zero, and lower or comparable variance. These results are promising, suggesting that fur-ther scaling could create even more robust meta-statistical MI estimators. Currently, the ST2 model can be trained in less than an hour on a single GPU, with inference orders of magnitude faster than existing neural baselines."}, {"title": "6. Discussion", "content": "With the meta-statistical framework, statistical inference becomes synonymous with inference in machine learning, and what is hard for statistical inference reveals itself as hard to learn for our models. Our experiments reveal in-teresting difficulties in statistical inference. Predicting nor-mality was the easiest task for meta-statistical models, re-quiring only 50K parameters for strong generalization. Es-timating mutual information, as expected, demanded sig-nificantly larger models (1M parameters). Surprisingly, predicting the standard deviation was particularly difficult: while small models (<10K parameters) could easily ap-proximate sample standard deviation (descriptive), nearly 1M parameters were needed to predict the standard devia-tion (inferential) better than np.std. Training a model to predict only the corrective term also required nearly 1M pa-rameters and yielded an estimator equivalent to directly es-timating the true standard deviation, suggesting that finite-sample errors is the main driver of difficulty in this prob-lem. This raises intriguing questions about what makes meta-statistical models work and fail: do these models im-plicitly perform Bayesian inference with input-dependent priors?\nStatistical inference is fundamentally constrained by irre-ducible errors arising from finite sample sizes, imposing in-herent limits on any estimator's performance (Casella and Berger, 2024). Consequently, no meta-statistical estima-tors can surpass these fundamental limits. However, the approach provides a flexible framework for finding estima-tors with a desired bias-variance trade-off by modifying the loss function. Moreover, the approach allows for the incor-poration of complex prior information through the choice of meta-distribution, effectively guiding the estimator's be-havior in a principled manner.\nLimitations and Future Work. While meta-statistical learning brings the advantages of machine learning to sta-tistical inference, it also imports its challenges. A key question is the choice of meta-distribution during train-ing-what constitutes a good meta-distribution to sam-ple from? Additionally, the evaluation of estimators be-comes more difficult; a model trained on a narrow meta-distribution might generalize poorly outside its training regime.\nInterpretability is another challenge. The precise algo-rithm computed at inference to perform the statistical infer-ence becomes unknown and difficult to interpret (Molnar, 2022; Carvalho et al., 2019; Teney et al., 2022). Also, like LLMs, meta-statistical models could exhibit unexpected failure cases and lack strict guarantees of validity. However, they also offer a promising testbed for mechanistic interpretability (Olah et al., 2020) research: they process structured numerical inputs without tokenization, operate in a single forward pass, and construct mathematical representations rather than linguistically ambiguous ones.\nFailure cases also merit further study. Models struggled when input scales exceeded training ranges and we found one case of poor generalization to one unseen distribu-tion family (log-normal) in the standard deviation estima-tion task (documented in Appendix B.4). In the normal-ity test setting, we believe that standardizing the datasets would make training harder but encourage better general-ization OoMD by preventing the meta-statistical estima-tors from picking up on spurious associations between the meta-distribution and the labels. Overall, like LLMs, these models would benefit from larger and more diverse train-ing data. Future directions include learned row embed-dings to accommodate varying input row dimensions and magnitudes, as well as scaling laws to guide the training of larger models with optimized data mixes. One limita-tion of this work is the focus on one-dimensional datasets to explore inference tasks in a controlled setting, but real-world inference involves high-dimensional data, where tra-"}, {"title": "Impact Statements", "content": "This paper presents work whose goal is to advance the field of Machine Learning. Meta-statistical learning aims to en-hance inference in low-sample settings, benefiting applied fields of Science like medicine and economics by improv-ing estimator reliability. Learned estimators may inherit biases from the data they are trained on, potentially leading to misleading conclusions if not carefully validated. Further, as with any data-driven methodology, interpretability remains a challenge; understanding why a model makes a particular statistical inference is crucial for scientific rigor."}, {"title": "A. Details about the Descriptive tasks experiments", "content": "To ensure reproducibility of the experiments, we de-scribe the synthetic data generation process in detail. The datasets were generated using a custom-built class, DescMetaDatasetGenerator, which allows for the creation of datasets with various distributions and cus-tomizable descriptive target variables. The key components and configurations are outlined below."}, {"title": "In-Meta-Distribution.", "content": "The set of distributions used to generate datasets during training is parameterized as fol-lows:\n\u2022 normal:: It has two parameters: the mean and the variance. Mean values are sampled from [-3, 3], and variances are sampled from [0.1, 1.5].\n\u2022 uniform:: It has two parameters: the lower bound and the upper bound. The lower bounds are sampled from [-3.5, -0.5] and the upper bounds from [0.5, 3.5].\n\u2022 beta:: It has two parameters: a and b. Parameters a and b are sampled from [1, 3] and [2, 5], re-spectively.\n\u2022 exponential:: It has one parameter: scale sampled from [1, 2]."}, {"title": "Out-of-Meta-Distribution.", "content": "The set of distribution used to test models for unseen distribution families is parametrized as follows:\n\u2022gamma:: It has two parameters: shape and scale. Shape parameters are sampled from [1, 5], and scale parameters from [1, 2].\n\u2022 log-normal:: It has two parameters: mean and variance. Means are sampled from [0, 1], and stan-dard deviations from [0.5, 0.75]."}, {"title": "Dataset Characteristics.", "content": "Once a distribution Px has been sampled, we use it to sample one dataset. In general, we could sample several dataset per distributions but we prefer to sample only one to maximize the diversity of distribu-tions seen during training. Each dataset is defined by the following parameters:\n\u2022 Number of variables (n_var): The number of fea-tures (columns) in the dataset. For our experiments, we set n_var = 2.\n\u2022 Number of rows (n_row_range): The number of samples (rows) in the dataset, sampled uniformly from the range [5, 300]. During testing, we ex-plore longer lengths to test the generalization of meta-statistical models.\n\u2022 To generate the target values y, each dataset is passed through the target descriptive functions: per-column-mean, per-column-median, correlation, win rate, opti-mal transport (1D)."}, {"title": "A.2. Examples of Training Curves", "content": "For meta-statistical models of approximately the same size (\u2248 10K parameters), we compare their convergence during training on the task of predicting the correlation between variable A and variable B, the two columns of the dataset. We consider the same meta-statistical models and the same meta-dataset generation parameters as considered in the results of the main paper. We report the results in Figure 6."}, {"title": "A.3. More Generalization Plots", "content": "In Figure 7, we report the same generalization plots as the main paper for other descriptive tasks. Similar conclusions holds: models generalize very well with lengths and tend to suffer from an offset of performance out-of-meta-distribution."}, {"title": "A.4. Details about Efficiency", "content": "In Figure 8, we present the inference time of meta-statistical models as a function of the input dataset size n. As expected, the VT scales quadratically, whereas LSTM and ST2 variants scale linearly with slopes in favor of ST2. We also compare the efficiency per parameter. For this we compute both the training and inference time of each model per batch averaged over 1K batches, and normalized by the number of parameters in the model. The results are re-ported in Table 4. Given the strong performance of ST2 and the clear computational advantage we see it as strong meta-statistical architecture."}, {"title": "B. Details about Standard Deviation Experiments", "content": "We construct a meta-dataset by generating datasets labeled with the ground truth standard deviation, using a set of dis-tributions for which the standard deviation is well-defined. To create each meta-datapoint, we first sample the base dis-tribution uniformly at random from a set of pre-defined dis-tribution families (see below). Then, we sample the param-eters of the distribution, resulting in a distribution Px. A dataset size n is then drawn uniformly at random from the range [10,150], and the dataset D is sampled with n rows. We generate 50K meta-datapoints for training and 3K for validation."}, {"title": "In-Meta-Distribution.", "content": "These are the distributions seen during training. The base distributions are the following, with the priors on their parameters:\n\u2022 normal: the mean is sampled from %(-1,1), and the variance is sampled from % (0.5,2.0).\n\u2022 uniform: the lower bound is sampled from (0,0.5), and the upper bound is sampled from U (0.5,1.5).\n\u2022 exponential: the scale parameter is sampled from U (1,2).\n\u2022 gamma: the shape parameter is sampled from (1,5), and the scale parameter is sampled from U (1,2)."}, {"title": "Out-of-Meta-Distribution.", "content": "These are the distributions seen during training. The base distributions are the fol-lowing, with the priors on their parameters:\n\u2022beta: the a parameter is sampled from (1,5), and the \u1e9e parameter is sampled from (1,5).\n\u2022 lognormal: the mean of the underlying normal dis-tribution is sampled from (0,1), and the standard deviation from (0.1,1).\n\u2022 weibull: the shape parameter is sampled from (1,5), and the scale parameter is sampled from U (1,2)."}, {"title": "B.2. Details about Meta-Statistical Models", "content": "For these experiments, we train two meta-statistical models based on Set Transformer 2 (ST2). Variants with different numbers of inducing points were tested, such as ST2 (16), which uses 16 inducing points (num_inds = 16).\n\u2022 ST2std: an ST2 (16) encoder with a regression MLP trained to predict the standard deviation opx of the dis-tribution.\n\u2022 ST2fsd: an ST2 (16) encoder with a regression MLP trained to predict the finite sample error made by the sample standard deviation, i.e., it predicts y = 0px - np.std(X)."}, {"title": "B.3. Bias and Variance", "content": "In Figure 9, we report an experiment targeted at measuring bias and variance by resampling 150 datasets from a fixed exponential distribution. Let the true standard devia-tion be denoted as o and the estimates for the i-th dataset be \u00f4i, for i = 1,..., 150.\nThe bias of the estimator is computed as:\nBias = $\\frac{1}{n}$ \u03a3$_{i=1}^{n}$\u00f4i - \u03c3,,\nwhere n = 50 is the number of resampled datasets.\nThe variance of the estimator is computed as:\nVariance = $\\frac{1}{n}$\u03a3($\\frac{1}{n}$\u03a3(\u00cei))$^{2}$.\nIt shows that the meta-statistical models can improve over the sample standard deviation np. std both in terms of bias and variance. Especially, the learned correction is ca-pable of reducing the bias of np. std indicating that it has not just learned a constant offset. We also report bias, vari-ance, and MSE across dataset sizes and over many meta-datapoints sampled from various distributions (both in- and out-of-meta-distribution) in Table 5."}, {"title": "B.4. Interesting Failure Case", "content": "While meta-statistical models generally demonstrate strong robustness when presented with datasets sampled from dis-tribution unseed during training (OoMD), we found a inter-esting failure case. For the log-normal family that was un-seen during training, meta-statistical estimators of standard deviations failed to provide improvements over np.std and even performed worse. This is illustrated in Figure 10 across dataset sizes. Log-normal is a skewed distribution which is particularly challenging estimators of standard deviation which can explain why meta-statistical model poorly generalize in this case. These models can benefit from a larger and more diverse training meta-dataset to ex-hibit even more robust generalization. Note, however, that the models do not fail for other unseen distribution families as it can be seen in Figure 3 of the main paper."}, {"title": "C. Details about Normality Tests Experiments", "content": "We construct a meta-dataset by generating datasets labeled with the ground truth binary indicator of normality, using a diverse set of alternative distributions. In previous stud-ies, the uniform distribution was used the contrast distri-bution Razali et al. (2011). To create each meta-datapoint,"}, {"title": "In-Meta-Distribution.", "content": "These are the distributions seen during training as non-normal distributions. Not how-ever that because the distribution sampled parameters and the datasets are then sampled from the distributions", "gamma": "the shape parameter is sampled from (1", "triangular": "the lower bound is sampled from (-3", "cauchy": "the location parameter is sampled from (-1", "laplace": "the location parameter is sampled from (-1", "weibull": "the shape parameter is sampled from U (0.5", "vonmises": "the mean direction \u03bc is sampled from U(-\u03c0", "arcsine": "the lower bound is sampled from (-3,0), and the"}]}