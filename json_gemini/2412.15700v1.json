{"title": "AIR: Unifying Individual and Cooperative Exploration in Collective Multi-Agent Reinforcement Learning", "authors": ["Guangchong Zhou", "Zeren Zhang", "Guoliang Fan"], "abstract": "Exploration in cooperative multi-agent reinforcement learning (MARL) remains challenging for value-based agents due to the absence of an explicit policy. Existing approaches include individual exploration based on uncertainty towards the system and collective exploration through behavioral diversity among agents. However, the introduction of additional structures often leads to reduced training efficiency and infeasible integration of these methods. In this paper, we propose Adaptive exploration via Identity Recognition (AIR), which consists of two adversarial components: a classifier that recognizes agent identities from their trajectories, and an action selector that adaptively adjusts the mode and degree of exploration. We theoretically prove that AIR can facilitate both individual and collective exploration during training, and experiments also demonstrate the efficiency and effectiveness of AIR across various tasks.", "sections": [{"title": "Introduction", "content": "Multi-agent reinforcement learning (MARL) has achieved outstanding results in complex cooperative tasks, such as flocking control (Xu et al. 2018; Gu et al. 2023), autonomous driving (Shamsoshoara et al. 2019; Zhang et al. 2023b), and sensor network (Zhao et al. 2023). To deal with the partial observability and the vast joint spaces of agents, most MARL methods follow the centralized training and decentralized execution (CTDE) paradigm. Especially, value-based approaches (Sunehag et al. 2017; Rashid et al. 2018; Yang et al. 2020; Wang et al. 2020a) enjoy high sample efficiency and great scalability, ultimately achieving superior performance in popular benchmarks. However, most value-based methods suffer from insufficient exploration due to the adoption of the vanilla e-greedy exploration strategy, and how to effectively enhance the exploration of value-based agents in cooperative tasks remains a challenging problem.\nIn some studies, the exploration of an agent is guided by its uncertainty towards the system. Zhang et al. (2023a) define the uncertainty as the correlation between an agent's action and other agents' observations, based on which they adjust exploration rate \\( \\epsilon \\) dynamically. EITI & EDTI (Wang et al."}, {"title": "Preliminaries", "content": "Dec-POMDP\nA fully cooperative multi-agent system (MAS) is typically represented by a decentralized partially observable Markov decision process (Dec-POMDP) (Oliehoek and Amato 2016), which is composed of a tuple \\( G = (S, U, P, Z, r, O, n, \\gamma) \\). At each time-step, the current global state of the environment is denoted by \\( s \\in S \\), while each agent \\( a \\in A := {1, . . ., n} \\) only receives a unique local observation \\( z_a \\in Z \\) generated by the observation function \\( O(s, a) : S \\times A \\rightarrow Z \\). Subsequently, every agent a selects an action \\( u_a \\in U \\), and all individual actions are combined to form the joint action \\( u = [u_1,..., u_n] \\in U = U^n \\). The interaction between the joint action u and the current state s leads to a change in the environment to state s' as dictated by the state transition function \\( P(s'| s, u) : S\\times U \\times S \\rightarrow [0, 1] \\). All agents in the Dec-POMDP share the same global reward function \\( r(s, u) : S \\times U \\rightarrow \\mathbb{R} \\), and \\( \\gamma \\in [0, 1) \\) represents the discount factor.\nValue Decomposition\nCredit assignment is a key problem in cooperative MARL which helps each agent to discern its contribution to the collective performance. To solve this, value decomposition (VD) methods assume that each agent has a specific value function, and the integration of all individual functions"}, {"title": "Diverse Behaviors across Agents", "content": "We begin the derivation of AIR by defining the concept of trajectory visit distribution:\nDefinition 1 (Trajectory visit distribution). In a multi-agent system, the distribution of the trajectory experienced by the agent k is called individual trajectory visit distribution and is defined as:\n\\begin{equation}\n\\begin{aligned}\n\\rho_k(\\tau, u) &= p(\\tau, u | agent\\_id = k) \\\\\n&= p(o_0, u_0, o_1, u_1, ..., o_T, u_T|z_k)\\\\\n&= \\prod_{t=0}^T \\pi_k (u_t|o_t) \\sum_{s_t} P(s_t) O(o_t | s_t, k), \\\\\n\\end{aligned}\n\\end{equation}\nwhere P(st) is the probability of the state being st at time step t, O(ot|st, k) is the probability of agent k receiving observation ot under state st, and \\( \\pi_k \\) is the policy of agent k. The trajectory visit distribution of the overall system containing n agents is called system trajectory visit distribution and is defined as:\n\\begin{equation}\n\\rho = \\frac{1}{n}\\sum_{k=1}^{n} p_k.\n\\end{equation}\nThe trajectory visit distribution, which indicates the probability of encountering a specific trajectory with a given policy from initial state so, encapsulates the diversity of policies"}, {"title": "Individual Exploration of Value-based Agents", "content": "In essence, exploration at the individual level drives deviation from established policies, prompting agents to take unfamiliar and unknown actions. A common individual exploration strategy used in policy gradient methods is to add the entropy of policy \\( \\pi_\\theta \\) as a regularization term in the optimization target \\( J(\\theta) \\). While it can adjust the degree of exploration dynamically during training, this approach does not work for VD methods because the value-based agent makes decision according to its value estimations towards the given observation and possible actions instead of an explicit policy distribution.\nTo facilitate exploration for VD methods, we introduce a novel adversarial mechanism somewhat similar to generative adversarial networks (GAN) (Goodfellow et al. 2014). Under the CTDE framework, we introduce two adversarial components: a centralized identity classifier trained to identify the agent responsible for generating the given trajectory-action pair (we directly reuse the discriminator \\( q_\\zeta \\) for collective exploration in practice), and a decentralized action selector that attempts to mislead the classifier. This adversarial mechanism can be interpreted by the following relationship (the proof is in Appendix A.3):\n\\begin{equation}\np(z_k | \\tau, u) = \\frac{\\prod_{t=0}^T \\pi_k(u_t|o_t)}{\\prod_{t=0}^T \\pi_k(u_t|o_t) + \\sum_{i=1, i \\neq k}^n \\prod_{t=0}^T \\pi_i(u_t|o_t)}.\n\\end{equation}\nAccording to Equation 9, the higher the probability that an action ut in the trajectory is selected by the policy, the higher the likelihood that the classifier will correctly identify the source agent k of the trajectory, and vice versa. Thus, even without an explicit policy function, we can leverage the classifier to measure action selection probabilities and encourage the agent to explore low-probability actions, achieving a similar effect to the entropy-based approach. Without distorting the individual Q-value estimation, the action selector promotes the agent k to choose unexplored actions with low posterior probabilities \\( q_\\zeta (z_k | \\tau, u) \\). We formally describe how the action selector works as:\n\\begin{equation}\nu = arg \\max_{u} Q_k (\\tau, u) = arg \\max_{u} [Q_k(\\tau, u) - \\alpha log q_\\zeta (z_k | \\tau, u)].\\end{equation}\nEquation 10 has a very similar form to Equation 7, and we take Equation 10 as the standard form. The value estimations of less exploited actions are raised more to increase their chances of being chosen when \\( \\alpha > 0 \\), thereby enhancing the individual exploration of the agent. When \\( \\alpha < 0 \\), it turns to collective exploration to induce diverse and cooperative behaviors among agents."}, {"title": "Adaptive Temperature", "content": "In Equation 10, the tuning of temperature value \\( \\alpha \\) is closely related to the magnitude of Q-value estimation, which is further determined by the magnitude of received rewards that"}, {"title": "Adaptive Temperature", "content": "differs not only across tasks but also over the training process due to the changing policy. Therefore, choosing the optimal temperature \\( \\alpha^* \\) is non-trivial. Besides, forcing the temperature to a fixed value during training is a poor solution, since the mode and degree of exploration are expected to be adjusted adaptively at different training stages. To dynamically tune the temperature during training, we formulate the learning target as a constrained optimization problem for the agent k:\n\\begin{equation}\n\\max_{ \\pi_\\theta } E_{ \\tau, u } [\\sum_{t=0}^T r(\\tau_t, u_t)] = \\max_{ \\pi_k } E_{ \\tau, u } [\\sum_{t=0}^T r(\\tau_t, u_t)] \\\\\\n\\text{s.t. } E_{(\\tau_t, u_t) \\sim p_k }[-log q_\\zeta(z_k | \\tau_t, u_t)] > H \\sqrt{t},\n\\end{equation}\nwhere \\( r(\\bar{\\tau}_t, u_t) \\) is the global reward function and \\( r(\\tau_t, u_t) \\) is the factored individual reward at time step t under IGM assumption, and H is a minimum expected entropy. Since the policy and trajectory at t would affect the future calculation, we rewrite the objective in a recursive form to perform dynamic programming backward:\n\\begin{equation}\n\\max_{\\pi_\\theta} (E_{\\pi_\\theta}[r(t_0, u_0)] + \\max_{\\pi_\\theta} (E_{\\pi_\\theta}[...] + E_{\\pi_\\theta}[r(\\tau_T, u_T)])),\n\\end{equation}\nsubjected to the same constraint in Equation 11. To focus on the temperature, we turn to the dual problem instead. We start with the last time step T. With the constraint in Equation 11,\n\\begin{equation}\n\\max_{ \\pi_k } E_{(\\tau_T, u_T) \\sim p_k } [r(\\tau_T, u_T)]\n\\end{equation}\n\\begin{equation}\n= min_{\\alpha_T \\geq 0} max_{ \\pi } E_{ \\pi } [r(\\tau_T, u_T) \\alpha_T log q_\\zeta(z_k | \\tau_T, u_T)] - \\alpha_T H,\n\\end{equation}\nwhere \\( \\alpha_T \\) is the dual variable. As \\( \\pi_k \\) is in the form of Equation 10 and only related to \\( q_\\zeta \\) when the Q-value function is fixed, we can replace \\( \\pi_k \\) in the subscripts of Equation 13 with \\( q_\\zeta \\). Here we employ strong duality since the objective is linear and the constraint in Equation 11 is a convex function with respect to \\( q_\\zeta \\). With the correspondence between the optimal policy deduced by \\( q_\\zeta \\) and the temperature value \\( \\alpha_T \\), the optimal dual variable \\( \\alpha_T \\) is solved by\n\\begin{equation}\narg \\max_{ \\alpha_T } E_{(\\tau_T, u_T) \\sim q_\\zeta } [\\alpha_T log q_\\zeta(z_k | \\tau_T, u_T) + \\alpha_T H].\n\\end{equation}\nTo simplify notation, we refer to the soft Bellman equation (Haarnoja et al. 2017)\n\\begin{equation}\nQ^* (\\tau_t, u_t) = r(\\tau_t, u_t) + E_{p_k} [Q^* (\\tau_{t+1}, u_{t+1}) - \\alpha log q_\\zeta(z_k | \\tau_{t+1}, u_{t+1})]\n\\end{equation}\nwith \\( Q_T(\\tau_T, u_T) = E[r(\\tau_T, u_T)] \\). We now take Equation 12 a step further using the dual problem and get:\n\\begin{equation}\n\\max_{ \\pi_{T-1}} E_{\\pi_{T-1}}[r(\\tau_{T-1}, u_{T-1})] + \\max_{\\pi_T} E_{\\pi_T}[r(\\tau_T, u_T)]\n\\end{equation}\n\\begin{equation}\n= max_{\\pi_{T-1}} (Q^* (\\tau_{T-1}, u_{T-1}) - \\alpha_T H)\n\\end{equation}\n\\begin{equation}\n= min_{\\alpha_{T-1} \\geq 0} max_{\\pi_{T-1}} (E_{\\pi_{T-1}} [Q^* (\\tau_{T-1}, u_{T-1})]\n\\end{equation}\n\\begin{equation}\n-E_{\\pi_{T-1}} [\\alpha_{T-1} log q_\\zeta(z_k | \\tau_{T-1}, u_{T-1})] - \\alpha_{T-1}H) + \\alpha_T H.\n\\end{equation}"}, {"title": "Algorithm 1: Adversarial Identity Recognition (AIR)", "content": "Thus Equation 11 can be optimized recursively from back to front. After obtaining the optimal Q-function Q* and the corresponding optimal policy \u03c0, the optimal dual variable a can be solved by:\n\\begin{equation}\n\\alpha^* = arg \\max_{ \\alpha_t } E_{u_t \\sim \\pi } [\\alpha_t log q_\\zeta(z_k | \\tau_t, u_t) + \\alpha_t H].\n\\end{equation}\nEquation 17 provides a theoretical approach for calculating the optimal temperature, but it is not feasible in practice since we can not obtain Q* directly but iteratively update the Q-function to approximate it. Therefore, we refer to dual gradient descent (Boyd and Vandenberghe 2004), which alternates between finding the optimal values of primal variables with fixed dual variables and applying gradient descent on dual variables for one step. It is also noted that while optimizing with respect to the primal variables fully is impractical, a truncated version that performs incomplete optimization (even for a single gradient step) can be shown to converge under convexity assumptions. Although the assumptions are not strictly satisfied, Haarnoja et al. (2018) still extended the technique to neural networks and found it working in practice. Thus, we compute gradients for a according to the objective in Equation 18. H is a hyperparameter and is calculated as the running mean of \\( - log q_\\zeta(z_k | \\tau_t, u_t) \\) in practice. The overall algorithm is displayed in Algorithm 1.\n\\begin{equation}\nJ(\\alpha) = E_{u_t \\sim \\pi } [\\alpha log q_\\zeta(z_k | \\tau_t, u_t) + \\alpha H]\n\\end{equation}"}, {"title": "Experiments", "content": "In this section, we conduct a large set of experiments to validate the efficiency and effectiveness of AIR across various scenarios. To be consistent with previous methods (Mahajan et al. 2019; Wang et al. 2020c; Yang et al. 2022), we similarly employ QMIX (Rashid et al. 2018) as the"}, {"title": "Conclusion", "content": "Exploration for value-based agents in cooperative multi-agent tasks remains a tough problem, partly due to the absence of an explicit policy. Previous works have made some progress mainly through individual exploration based on agent's uncertainty towards the system or collective exploration focusing on agents' behavioral diversity. In this paper, we propose Adaptive exploration via Identity Recognition (AIR), which is the very first approach to integrate both individual and collective in a unified framework. The core module in AIR is an identity classifier that distinguishes corresponding agents based on given trajectories. According to our theoretical analysis, the accuracy of the classifier is highly related to the exploration of agents. Consequently, we design a unified exploration mechanism that can switch the exploration mode between individual and collective exploration according to the training stage as well as tuning the degree of exploration. We conduct extensive experiments and studies on varied scenarios, and the results strongly demonstrate the superiority of our proposed method."}, {"title": "A.1 The Decomposition of KL-Divergence", "content": "Lemma 1. Given the system trajectory visit distribution p, of which the entropy H(p) can be decomposed as below:\n\\begin{equation*}H = E_z [D_{KL}(\\rho(\\tau, u|z)||\\rho(\\tau, u))] + H(\\rho|z)\\end{equation*}\nProof.\n\\begin{equation*}\n\\begin{aligned}\nH(\\rho) &= E_{(\\tau,u) \\sim \\rho} [-\\log p(\\tau, u)] \\\\\n&=E_{(\\tau,u,z) \\sim \\rho} [-\\frac{\\rho(\\tau, u,z)}{\\rho(\\tau, u)} \\log p(\\tau, u)] \\\\\n&=E_{(\\tau,u,z) \\sim \\rho} [\\frac{\\rho(\\tau, uz)}{\\rho(\\tau, u)} \\log \\frac{p(\\tau, u|z)}{p(\\tau, u)}] \\\\\n&=E_z [D_{KL} (\\rho(\\tau, u|z)||\\rho(\\tau, u))] + H(p|z)\n\\end{aligned}\n\\end{equation*}"}, {"title": "A.2 Equivalent Optimization Target", "content": "Lemma 2.\n\\begin{equation*}H(z|\\rho) \\propto -D_{KL} [p(\\tau, u)\\vert\\vert \\frac{1}{n} \\sum_{k=1}^n p(\\tau, u|z_k)]\\end{equation*}\nProof. According to the mutual information theory, the MI between the trajectory visit distribution and the agent identity can be calculated as:\n\\begin{equation*}I(\\rho; z) = H(\\rho) - H(\\rho|z) = H(z) - H(z|\\rho)\\end{equation*}\nH(z) is a constant as mentioned in the paper. Thus we have:\n\\begin{equation*}\n\\begin{aligned}\nH(z|\\rho) &= H(z) + H(\\rho|z) - H(\\rho)\\\\\n&\\propto E_{(\\tau,u,z) \\sim \\rho} [-\\log \\rho(\\tau, u|z)] - \\int_\\tau,u \\rho(\\tau, u) \\log p(\\tau, u) drdu\\\\\n&=E_{(\\tau,u,z) \\sim \\rho} [-\\log(p(\\tau, u|z))] - \\int -p(\\tau, u, z) \\log(p(\\tau, u)) drdudz\\\\\n&=E_{(\\tau,u,z) \\sim \\rho} [log(p(\\tau, u) - log(p(\\tau, u|z)]\n\\end{aligned}\n\\end{equation*}\nUsing total probability theorem,\n\\begin{equation*}\n\\rho(\\tau, u) = \\int \\rho(\\tau, u|z) p(z) dz = \\sum_{k=1}^n \\rho(\\tau, u|z_k)\n\\end{equation*}\nSubstituting it into the formula above, we obtain:\n\\begin{equation*}\n\\begin{aligned}\nH(z|\\rho) &\\propto E_{(\\tau,u,z) \\sim \\rho} [-\\log(\\sum_{k=1}^n p(\\tau, u|z_k)] - \\int p(\\tau, u, z) \\log p(\\tau, u)]\\\\n&\\propto - D_{KL} [p(\\tau, u)\\vert\\vert \\frac{1}{n} \\sum_{k=1}^n p(\\tau, u|z_k)]\n\\end{aligned}\n\\end{equation*}"}, {"title": "A.3 The Relationship between Identity Classifier and Policy", "content": "Lemma 3. The identity classifier \\( q_\\zeta(z_k|\\tau, u) \\) can be used to measure action selection probabilities of each agent, as the following relationship holds:\n\\begin{equation*}\np(z_k|\\tau_\\tau, u_\\tau) = \\frac{\\prod_{t=0}^T \\pi^k(u_t|o_t)}{\\prod_{t=0}^T \\pi^k(u_t|o_t) + \\sum_{i=1, i \\neq k}^n \\prod_{t=0}^T \\pi^i(u_t|o_t)}\n\\end{equation*}"}, {"title": "B Environment Details", "content": "B.1 SMAC\nSMAC is a simulation environment for research in collaborative multi-agent reinforcement learning (MARL) based on Blizzard's StarCraft II RTS game. It provides various micro-battle scenarios and also supports customized scenarios for users to test the algorithms. The goal in each scenario is to control different types of ally agents to move or attack to defeat the enemies. The enemies are controlled by a heuristic built-in AI with adjustable difficulty level between 1 to 7. In our experiments, the difficulty of the game AI is set to the highest (the 7th level). The version of StarCraft II is 4.6.2 (B69232) in our experiments, and it should be noted that results from different client versions are not always comparable. \nB.2 SMACv2\nSMACv2 (Ellis et al. 2023) is proposed to address SMAC's lack of stochasticity. In SMACv2, the attack ranges of different unit types are no longer the same. Besides, the field of view of the agents is further restricted to a sector shape rather than a circle. Therefore, the agent receives less information and suffers more severe partial observability problems. The team compositions and agent start positions are generated randomly at the beginning of each episode. These modifications make SMACv2 extremely challenging. It is worth noting that the disparity between the lineups of the two sides can be substantial in some episodes due to the randomness in initialization, so it is impossible for an algorithm to reach a 100% win rate. The version of the StarCraft II engine is also 4.6.2 (B69232) in our experiments."}, {"title": "C.1 Settings of Hyperparameters", "content": "We list the hyperparameters of the AIR in Table 2. The hyperparameters of the baselines in our experiments remain the same as their official implementations."}, {"title": "C.2 Experiments Compute Resources", "content": "We conducted our experiments on a platform with 2 Intel(R) Xeon(R) Platinum 8280 CPU 2.70GHz processors, each with 26 cores. Besides, we use a GeForce RTX 3090 GPU to facilitate the training procedure. The time of execution varies by scenario."}, {"title": "D Additional Experiments", "content": "Due to the space limitations, we include the experiments on SMACv2 here. The details of SMACv2 environment can be found in Appendix B.2. At the beginning of an episode, the unit type and location of each agent are initialized randomly. Besides, the field of view of the agents is further restricted to a sector shape rather than a circle. Therefore, the agent receives less information and suffers more severe partial observability problems, which increases the demand for additional sources of information. It is worth noting that, due to the random initialization, the power gap between two sides could be too wide for the ally agents to win this episode. Therefore, there exists an upper bound for the win rates.\nSince RODE (Wang et al. 2020c) and LDSA (Yang et al. 2022) are two methods about multi-agent exploration and perform relatively well in SMAC (RODE even outperforms AIR in corridor), we select them as baselines for further comparison. RODE and LDSA employ the episode runner to collect data, while AIR uses the parallel runner in our code to accelerate the training process but reduce the frequency of network updates. We expect a further improvement in AIR's performance upon switching to the episode runner. The experiment results are displayed in Figure 8. All the tested methods adopt the mixing network of QMIX for fairness. We observe that RODE (Wang et al. 2020c) suffers a severe deterioration in the effectiveness of policies, which indicates that it may learn open-loop policies in SMAC and could not deal with the randomness in SMACv2. The performance of LDSA (Yang et al. 2022) is relatively better but still far from the highest win rate records. On the contrary, AIR is significantly superior and even exceeds the highest win rate records of all baselines in the original paper of SMACv2 (Ellis et al. 2023)."}]}