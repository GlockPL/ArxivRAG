{"title": "A Safe Self-evolution Algorithm for Autonomous Driving Based on Data-Driven Risk Quantification Model", "authors": ["Shuo Yang", "Shizhen Li", "Yanjun Huang", "Hong Chen"], "abstract": "Autonomous driving systems with self-evolution capabilities have the potential to independently evolve in complex and open environments, allowing to handle more unknown scenarios. However, as a result of the safety-performance tradeoff mechanism of evolutionary algorithms, it is difficult to ensure safe exploration without sacrificing the improvement ability. This problem is especially prominent in dynamic traffic scenarios. Therefore, this paper proposes a safe self-evolution algorithm for autonomous driving based on data-driven risk quantification model. Specifically, a risk quantification model based on the attention mechanism is proposed by modeling the way humans perceive risks during driving, with the idea of achieving safety situation estimation of the surrounding environment through a data-driven approach. To prevent the impact of over-conservative safety guarding policies on the self-evolution capability of the algorithm, a safety-evolutionary decision-control integration algorithm with adjustable safety limits is proposed, and the proposed risk quantization model is integrated into it. Simulation and real-vehicle experiments results illustrate the effectiveness of the proposed method. The results show that the proposed algorithm can generate safe and reasonable actions in a variety of complex scenarios and guarantee safety without losing the evolutionary potential of learning-based autonomous driving systems.", "sections": [{"title": "I. INTRODUCTION", "content": "As an important component of intelligent transportation systems (ITS), autonomous driving systems can complete driving tasks without human intervention through perception, decision-making and electronic control technologies [1] [2]. Autonomous driving technology has the potential to enhance traffic efficiency, reduce the risk of traffic accidents, and offer sustainable solutions to transportation challenges, which has received widespread attention in recent years [3] [4]. However, due to the challenges of algorithmic limitations and technological maturity, there are still many barriers to the realization of high-level autonomous driving. It is necessary to further research related technologies to ensure the safety and efficiency of autonomous driving systems in a variety of complex scenarios [5].\nIn recent years, self-evolution algorithms with experience storage and learning upgrade as the core ideas have attracted more and more attention. Reinforcement learning(RL), as an important part of self-evolution algorithms, has been widely used in many fields and has reached beyond human performance in some areas. It can be seen that the autonomous driving algorithms using reinforcement learning techniques have the potential to adapt to more complex scenarios [6] [7] [8]. However, due to the exploration-exploitation mechanism and the black box nature of reinforcement learning algorithm, it is difficult to guarantee the safety in the process of evolutionary learning and deployment application. This is absolutely unacceptable for a safety-critical system such as autonomous driving [9].\nSafe reinforcement learning is a machine learning method that aims to enable a system to learn in uncertain or dangerous environments while minimizing the risk of producing undesirable outcomes [10] [11]. The approach consists of two main categories, one of which is to consider optimization criteria that minimize the violation of constraints during the reinforcement learning process [12] [13], and the other is to make stochastic exploration in the safety domain [14] [15]. Recently, many researches have applied safety reinforcement learning methods to autonomous driving tasks. These methods utilize a priori knowledge, either through historical data or human assistance to improve security during learning [16] [17] [18]. Regarding the method of changing the optimization criterion, Yang et al. proposed a model-free security reinforcement learning method through Neural Barrier Certificate, which minimizes constraint violation in the process of policy collection through joint learning of policy and Neural Barrier Certificate [19]. As for the design of the safety layer, the methods mainly include control barrier function methods [20] [21], rulebook methods [22] [23] and formalization methods [24]. Zhang et al. proposed a safety reinforcement learning method for autonomous vehicles based on Barrier Lyapunov Function(BLF), which reasonably organized and incorporated BLF items into the optimized inverse control method, and constrained the state variables within the designed safety region during the learning process [25]. Zhang et al. proposed a safety checker based on a responsibility sensitive safety model that provides reinforcement learning agent with fallback safety actions in unsafe situations during the training and evaluation phases [26]. Cao et al. proposed a decision-making framework called Trustworthy Improvement Reinforcement Learning (TiRL), which combines reinforcement learning and rule-based algorithms to allow self-improvement while maintaining better system performance [27].\nHowever, although the above methods can consider safety in the algorithm learning process, the method of changing the optimization criterion can only reduce the constraint violation, and it is still difficult to ensure the process safety in the reinforcement learning algorithm, which cannot fully meet the high safety requirements of autonomous driving. At the same time, safety constraint methods may lead to insufficient exploration of the agent, which leads to difficulties in achieving optimal performance. The design of safety constraints based on manual definitions may also lead to conservative policies, resulting in performance loss [28] [29]. To solve this problem, a feasible solution is to design an adjustable safety limits, and design a safety limits adjustment criterion based on the risk quantitative value in order to alleviate the problem of balancing safety and performance that exists in evolutionary algorithms.\nAt present, there are two main methods for quantifying risk: rule-based methods and artificial potential field-based methods. The rule-based methods aim to evaluate the safety of autonomous driving systems in different situations by formulating a series of rules and indicators to quantify potential risks [30] [31]. Some definitions, such as Time-to-collision (TTC) [32], Time-to-Lane-Change (TLC) [33], Responsibly-sensitive Safety (RSS) [34], etc. have been proposed for calculating risk indicators. However, in some cases, these risk indicators can only support simple driving operations [35]. The method based on artificial potential field, on the other hand, combine logic rules and physical models to quantify risk scenarios by simulating potential risk factors through potential energy calculations [36]. However, the above methods still require more domain expertise and manual rule formulation, which limits their application in complex and dynamic traffic scenarios [37]. Hence, it makes sense to study risk quantification methods based on data-driven approaches that autonomously extract key features from data through learning methods, without the need for human predefinition of rules, and thus have potential in terms of real-world scenario adaptability [38].\nTo sum up, in this paper, a safety self-evolution algorithm for autonomous driving based on a data-driven risk quantification model is proposed. The method combines the risk quantization method and the safety self-evolution decision-control method in order to solve the problem of limited evolutionary performance of safety reinforcement learning methods in dynamic scenarios. An attention mechanism model based on human risk cognition is proposed to quantify the risk of the surrounding environment. In order to avoid the adverse impact of overly conservative safety limits on the self-evolutionary performance of autonomous driving algorithms, we introduce the concept of Safe Critical Acceleration (SCA). On the basis of this concept, a safety-evolution decision-control integration algorithm with adjustable safety limits is designed, and the proposed risk quantization model is integrated into it. The proposed method can generate safe and reasonable action policies in a variety of complex scenarios, while ensuring that the self-evolutionary potential of the learning autonomous driving system is not compromised.\nThe contributions of this study are summarized as follows:\n1) This paper propose a risk quantification method based on a data-driven approach involves autonomously extracting key features from the data through machine learning methods, providing safety assessment indicators that consider human risk perception.\n2) This paper propose a safety self-evolution method with adjustable safety limits integrating risk quantification indicators, which allows the algorithms to guarantee safety without sacrificing evolutionary performance.\n3) This paper build a virtual-real interaction platfrom with the high-fidelity simulation software and the real autonomous vehicles, and use it to verify the effectiveness of the algorithm.\nThe paper is organized as follows. The proposed framework is introduced in Section II. The modeling of data-driven risk quantification model is introduced in Section III. The descriptions of safe self-evolution algorithm with adjustable safety limits are proposed in Section IV. In Section V, our method is verified and compared in simulation, and section VI concludes this paper."}, {"title": "II. PROPOSED FRAMEWORK", "content": "The proposed self-evolution algorithm for autonomous driving safety based on a data-driven risk quantification model aims to generate safe, efficient, and reasonable driving policies. The algorithm can not only ensure safety during learning and deployment phase, but also effectively prevent the sacrifice of performance improvement ability. The input of the proposed algorithm is the information of the surrounding traffic environment, and the output is the safe control action. The main components are described in Fig. 1, including a data-driven risk quantization model and a safety-evolution decision-control integration algorithm with adjustable safety limits.\nThe risk quantification model in this framework is designed as a transformer architecture. The proposed model simulates human risk perception by formulating a reinforcement learning problem that generates safety quantification indicators as output. Then, the output indicators are provided to the safety-evolution decision-control integration algorithm. The decision-control integration algorithm consists of three components, including an actor-critic framework, a local planner, and an adjustable safety limits mechanism. The actor-critic framework consists of an actor network and a critic network, which implement the algorithm evolution by maximizing the cumulative return. A local planner is combined with n adjustable safety limits mechanism to output reasonable actions that satisfy dynamic adjustable safety limits. The framework can reasonably and dynamically adjust the safety limit according to the surrounding traffic conditions, so that the performance improvement ability of the algorithm can be maintained as much as possible while ensuring the safety."}, {"title": "III. DATA-DRIVEN RISK QUANTIFICATION MODEL", "content": "A. Problem Definition\nIn this paper, we propose a risk quantification model modeling approach for assessing whether an autonomous vehicle is likely to enter an unsafe state during driving. With the goal of evaluating whether the current state is safe, the most intuitive idea is to consider the influence of the surrounding traffic situation on itself. However, it is worth noting that the risk quantification is not only related to the current state, but also to the driver's driving ability. An intuitive example is that in the same risk scenario, a skilled driver or higher-level autonomous driving system can be safe, but a novice or lower-level autonomous driving system may be at greater risk.\nTherefore, the input of the risk quantification model should be the surrounding environmental information and the driving agent itself, and the output should be the risk quantification results. To sum up, the risk quantification model $f_{RQ}$ can be defined as:\n$\\mathcal{A} = \\{f_{RQ} (s, A_{\\theta}) |i = 1, 2, ..., n\\},$ (1)\nwhere $s$ is state, which is used to represent the surrounding traffic situation; $A_{\\theta}$ is driving agent with $\\theta$ as parameters. The state input $s$ and the driving agent $A_{\\theta}$ together affect the risk quantification result $\\mathcal{A}$. $A_{\\theta}$ is defined as:\n$A_{\\theta} = \\pi (s) .$ (2)\nThat is, the agent's policy output is related to the state input $s$.\nB. The mechanisms of human perception of risk\nHumans demonstrate high intelligence while driving, being able to subjectively estimate and evaluate risks. This ability allows humans to make effective decisions in complex and dynamic traffic environments. Therefore, by simulating human risk perception, the safety and responsiveness of autonomous driving systems can be enhanced.\nBy analyzing the above mechanism, the principle of constructing a risk quantification model can be deduced.\n1) Human perception of risk: Fig. 2 presents a schematic of the human risk perception mechanism. Taking the highway scenario as an example, the driver will complete various driving tasks such as following and overtaking by controlling the vehicle according to the movement state of the current traffic vehicle on the basis of judging the risk.\nCase 1 illustrates a low risk situation. In this case, the driver focuses on the fact that the traffic ahead will have no effect on the ego vehicle due to the distance of the surrounding traffic. In terms of human cognitive behavior, the driver's attention is placed farther away, and therefore the trajectory planned in the driver's mind considers a more distant time span (denoted as $T_1$).\nCase 2 illustrates a high risk situation. In this case, the driver focuses on the fact that the ego vehicle will be affected by the proximity of the surrounding traffic vehicles. In terms of human cognitive behavior, the driver's attention is placed near away, and therefore the trajectory planned in the driver's mind considers a closer time span (denoted as $T_2$).\nTherefore, the construction of risk quantification model can be realized by simulating the human risk perception mechanism. Specifically, a human-like driving agent $\\pi$ is established, which should have the following capabilities:\n\u2022 Risk Quantification (RQ) : takes the current traffic situation as input and outputs a risk quantitative value given by the agent. The value is defined to range from 0 to 100%, i.e., the larger the value is, the higher the risk is.\n\u2022 Importance Ranking (IR): All traffic participants within the agent perception range are ranked and scored according to the risk degree. The higher ranked participants are, the more likely they are to pose a risk to the ego vehicle, and therefore the more worthy of attention.\nConsidering the mechanism of human risk perception, we abstract the above problem and further propose the construction method of human-like agent based on this basis.\nThe above two cases are introduced with a straight road as an example. However, road conditions in urban driving scenarios are often complex, including congestion, off-ramps, intersections, roundabouts, and more. The Frenet coordinate system determines the position, direction, and speed on a road based on the tangent, normal, and tangent vectors at a point on a curve. This system is widely used in the decision-making and planning algorithms of autonomous driving due to its ability to correlate actual geometric features of the road and adapt to different road topologies [39]. In summary, the proposed human risk perception mechanism and risk quantification method can be extended to more complex road environments based on straight roads through the Frenet coordinate system, as shown in Fig. 2. In this paper, we focus on the design scheme of the proposed method on straight roads.\n2) Problem abstraction: In low-risk situations, drivers' cognitive behavior tends to be more oriented towards long-term planning and smooth action strategies because safe driving is more easily achieved, whereas in high-risk situations, drivers may need shorter-term cognitive processes and more rapid reactivity, so drivers plan for shorter time spans and pay closer attention.\nHence, the above problem can be abstracted as: how to extract the smooth trajectory planned by the agent from the current position to complete the autonomous driving task, and how to use the corresponding time span of the trajectory to quantify the risk of the driver.\nThe local trajectory planning process of the agent is shown in Fig. 3. Quintic and quartic polynomial curves are respectively applied to describe the process of longitudinal and lateral trajectory planning, as shown in appendix.\nThe polynomial coefficients $p_d$ and $p_s$ can be solved by substituting the planning time $T_c$ with the boundary conditions in Eq. 21 and Eq. 22. Fig. 3 illustrates the change of the longitudinal and lateral planning trajectories for different planning times $T_c^1$, $T_c^2$ and $T_c^3$. It can be seen that the smaller $T_c$ is, the shorter the planning time span is and the faster the response of the policy is.\nAs an effective self-learning method, RL has advantages in solving the decision and planning problem of autonomous driving, which can adapt to complex traffic environments and output efficient and safe driving policies. To sum up, the DD-RQ model proposed in this paper can be constructed as follows: constructing a human-like autonomous driving agent utilizing RL algorithm and then extracting planning time $T_c$ as a benchmark for risk quantification.\nC. Model construction\n1) Analysis of Cross-domain migration: Limited by the learning efficiency issue and safety issue of reinforcement learning algorithms, it is difficult to collect enough data during actual operation and use it to train the DD-RQ Model. Therefore, training the RQ model in a simulation environment and deploying it in an application environment is a feasible solution.\nHowever, considering the sim2real gap problem in both simulation and application environments [40] [41], the cross-domain migration of the RQ model needs to be analyzed."}, {"title": "IV. SAFE SELF-EVOLUTION ALGORITHM WITH ADJUSTABLE SAFETY LIMIT", "content": "In this section, a safety-evolution decision-control integration algorithm with adjustable safety limits is proposed. The influence of overly conservative safety guard policy on the self-evolution ability is prevented by integrating the proposed RQ model.\nA. Agent implementation\nA RL problem based on soft actor critic algorithm is constructed to achieve integrated autonomous driving decision and control with evolutionary capability. It is worth emphasizing that there are some significant differences between the agent $\\pi_\\omega$ presented in this section and the driving agent $\\pi_{\\phi}$ described in section III-B. Specifically, the agent $\\pi_\\omega$ is used to deploy on the autonomous vehicle to actually control the ego vehicle to realize the driving task, and the driving agent $\\pi_{\\phi}$ is trained and deployed in the simulation to provide training data for the DD-RQ model. Hence, although these two agents share the framework of RL, they still have some differences in problem formulation and model design due to their different application goals.\n1) State space and action design: For an autonomous driving system, the input information accepted by the algorithm is fixed, that is, the required information during the driving process is predetermined. Thus, the state space $s$ of $\\pi_\\omega$ is designed to be consistent with $\\pi_{\\phi}$, as shown in Table I.\nThe design of action space needs to consider the actual goal of autonomous driving trajectory planning. For the agent $\\pi_\\omega$, different from Eq. 3, we directly choose the longitudinal acceleration of the ego vehicle $a_x$ as the control input. The new action space is designed as follows:\n$a = [d_{fn} a_x],$ (14)\nwhere $d_{fn}$ is the desired lateral offset and $a_x$ is the longitudinal acceleration of the ego vehicle. A simple PID controller is applied again to track the desired trajectory generated by the RL algorithm [42].\n2) Reward design: The reward function design is carried out on the basis of section III-C and the Appendix. In the actual autonomous driving process, in addition to the speed reward $r_s$, collision penalty $r_c$ and lane departure penalty $r_{ld}$, it is also necessary to consider the driving risk penalty $r_{risk}$ and comfort reward $r_{cf}$.\nAs described in Section II, the RQ model and the safety guard policy work together to ensure the safety. On this basis, if the collision risk is recognized, the driving risk penalty $r_{risk}$ will be triggered to take effect. As shown in Eq. 15.\n$r_{risk} = \\begin{cases} 0 & \\text{with risk}\\\n\\rho_{risk} & \\text{risk} \\end{cases},$ (15)\nwhere $\\rho_{risk}$ is driving risk penalty factor.\nThe comfort reward $r_{cf}$ is designed to reduce the amplitude of the longitudinal and lateral actions as much as possible. To make the training easier, a normalization method is applied as shown in Eq. 16.\n$r_{cf} = \\rho_{cf} \\times \\Big( \\frac{abs(d^{i}_{fn}-d^{i-1}_{fn})}{2 \\times L_{width}} + \\frac{abs(a^{i}_{x} - a^{i-1}_{x})}{\\overline{a}}\\Big),$ (16)\nwhere $d^{i}_{fn}$ and $d^{i-1}_{fn}$ are the expected lateral displacements at the current time and the last time, $a^{i}_{x}$ and $a^{i-1}_{x}$ are the longitudinal accelerations of the ego vehicle at the current time and the last time, $L_{width}$ is the lane width, $\\overline{a}$ is the normalized coefficient of the longitudinal acceleration, and $\\rho_{cf}$ is the comfort reward coefficient.\nTo sum up, the total reward is:\n$r = r_s + r_c + r_{ld} + r_{risk} + r_{cf}.$ (17)\nB. Safe Critical Acceleration\nConsidering that the rule-based safety limit logic will affect the learning effect of the algorithm, this paper proposes the concept of safe critical acceleration(SCA), which is used to integrate the DD-RQ proposed in Section III to realize the dynamic adjustment of safety limits.\nThe safety state of an autonomous vehicle is defined as the longitudinal distance between the ego vehicle and all traffic vehicles is greater than the safe distance $S_{safe}$. Combining the experience of experts and laws and regulations, the safe distance $S_{safe}$ is defined as:\n$S_{safe} = v_{ego} \\times 3.6,$ (18)\nwhere $v_{ego}$ is the longitudinal speed.\nSo far, the safety evolution problem of the learning-based autonomous driving algorithm can be defined as follows:\n$\\begin{cases}\nS_{obj} - S_{ego} + \\Delta S_{obj} = \\Delta S_{ego} + S_{safe}, S_{obj} - S_{ego} \\geq 0 \\\\\nS_{ego} - S_{obj} + \\Delta S_{ego} = \\Delta S_{obj} + S_{safe}, S_{obj} - S_{ego} < 0\\end{cases},$ (19)\nwhere $S_{obj}$ and $S_{ego}$ are the longitudinal displacements of the traffic vehicles and the ego vehicle, $\\Delta S_{obj}$ and $\\Delta S_{ego}$ are the relative longitudinal displacements of the traffic vehicles and the ego vehicle in time $T_c$, respectively.\nThe safe distance adjustment process is regarded as a constant acceleration movement, and the acceleration in the adjustment process is defined as the safe critical acceleration $A_{safe}$. $A_{safe}$ is used to characterize the acceleration limit of the ego vehicle when it does not exceed the safe distance(that is, not leave the safe state). On the basis of Eq. 19, the safe critical acceleration of the ego vehicle for the i-th traffic vehicles can be expressed as follows:\n$A_{safe i} = \\frac{2(\\Delta S - S_{safe} + T_c \\cdot \\Delta v) \\Delta v}{\\frac{\\left |\\Delta S\\right |}{\\Delta S} T_c^2},$ (20)\nwhere $\\Delta S = S_{obj} - S_{ego}$, $\\Delta v = v_{obj} - v_{ego}$.\nIn Eq. 20, $T_c$ will affect the $a_{safe}$. Specifically:\n\u2460: When $\\Delta S > 0$, the ego vehicle should slow down to adjust the distance between the ego vehicle and the preceding traffic vehicle to the safe distance $S_{safe}$. At this time, shortening the adjustment time $T_c$ will require the ego vehicle to pursue a safe distance $S_{safe}$ from the front vehicle by slowing down for a shorter time, so the required deceleration will be larger. At this point, the control output by the RL algorithm much less likely to satisfy the demand, so the safety guard is more likely to intervene.\n\u2461: When $\\Delta S < 0$, the ego vehicle should accelerate to adjust the distance between the ego vehicle and the following traffic vehicle to the safe distance $S_{safe}$. At this time, shortening the adjustment time $T_c$ will require the ego vehicle to pursue a safe distance $S_{safe}$ from the rear vehicle by accelerate for a shorter time, so the required acceleration will be larger. At this point, the control output by the RL algorithm much less likely to satisfy the demand, so the safety guard is more likely to intervene.\nIn summary, by adjusting $T_c$, $a_{safe}$ can be adjusted, which indirectly realizes the dynamic adjustment of the safety limits of the proposed evolutionary algorithm.\nC. Adjustable safety limits based on quantitative indicator\nAn excessively conservative safety guard intervention policy may have adverse effects on the self-evolution performance of autonomous driving algorithms. Therefore, combined with the RQ model proposed in Section III, this paper designs a safety self-evolution method with adjustable safety limits integrating risk quantification indicators.\nSpecifically, the RQ model outputs a quantitative value of risk RQ for the current traffic environment. $T_c$ is defined as a linear function of RQ and is used to dynamically adjust the safety limits. In high risk scenarios, the safety limits are tightened. This is to ensure that the system is able to adopt a more conservative policy in high-risk scenarios, thus reducing"}, {"title": "VI. CONCLUSION", "content": "This paper proposed a safe self-evolution algorithm for autonomous driving based on a data-driven risk quantification (DD-RQ) model. The method realized the safety situation estimation of the environment through a data-driven approach, and combined a safety-evolution decision-control integration algorithm with adjustable safety limits, which enabled the agent to ensure safety exploration without sacrificing the performance improvement ability. The proposed method was verified in a dense three-lane lane-changing scene. Experiments show that compared with the model-free RL algorithm and the regular safety RL algorithm, the proposed method can improve the average speed while ensuring the system collision-free in the training and verification process, and reduce the safety guard intervention ratio by 50%. Real vehicle test results show that the proposed algorithm can control the real autonomous vehicle to output safe, smooth and reasonable lane-changing overtaking actions. In the future research, we will combine large-scale natural driving data sets to extract the key features of the risk quantification model, and consider introducing the receding horizon optimization technology to improve the optimality of the algorithm."}, {"title": "APPENDIX", "content": "A. Quintic and quartic polynomial curves\nQuintic and quartic polynomial curves are respectively applied to describe the process of longitudinal and lateral trajectory planning:\n$\\begin{bmatrix}\n1 & 0 & 0 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 2 & 0 & 0 & 0 \\\\\n1 & t & t^2 & t^3 & t^4 & t^5 \\\\\n0 & 1 & 2t & 3t^2 & 4t^3 & 5t^4 \\\\\n0 & 0 & 2 & 6t & 12t^2 & 20t^3\n\\end{bmatrix} \\cdot P_d = \\begin{bmatrix} d_0 \\\\ \\dot{d_0} \\\\ \\ddot{d_0} \\\\ d_{fn} \\\\ \\dot{d_{fn}} \\\\ \\ddot{d_{fn}} \\end{bmatrix},$ (21)\n$\\begin{bmatrix} 1 & 0 & 0 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 & 0 \\\\\n0 & 0 & 2 & 0 & 0 \\\\\n1 & t & t^2 & t^3 & t^4 \\\\\n0 & 1 & 2t & 3t^2 & 4t^3 \\end{bmatrix} \\cdot P_s = \\begin{bmatrix} s_0 \\\\ \\dot{s_0} \\\\ \\ddot{s_0} \\\\ s_{f} \\\\ \\dot{s_{f}} \\end{bmatrix},$ (22)\n$P_d = [ a_0 a_1 a_2 a_3 a_4 a_5 ]^T,$\n$P_s = [ a_0 a_1 a_2 a_3 a_4 ]^T,$\n$P_a = [ a_0 a_1 a_2 ]^T,$ (23)\nwhere t is time variable, $p_d$ is coefficients of quintic polynomials for lateral planning, $[ d_0 \\dot{d_0} \\ddot{d_0} d_{fn} \\dot{d_{fn}} \\ddot{d_{fn}} ]^T$ is the boundary condition of a quintic polynomial, $p_s$ is coefficients of quartic polynomials for longitudinal planning and $[s_0 \\dot{s_0} \\ddot{s_0} s_{f} \\dot{s_{f}} ]^T$ is the boundary condition of a quartic polynomial."}]}