{"title": "A Safe Self-evolution Algorithm for Autonomous Driving Based on Data-Driven Risk Quantification Model", "authors": ["Shuo Yang", "Shizhen Li", "Yanjun Huang", "Hong Chen"], "abstract": "Autonomous driving systems with self-evolution capabilities have the potential to independently evolve in complex and open environments, allowing to handle more unknown scenarios. However, as a result of the safety-performance trade-off mechanism of evolutionary algorithms, it is difficult to ensure safe exploration without sacrificing the improvement ability. This problem is especially prominent in dynamic traffic scenarios. Therefore, this paper proposes a safe self-evolution algorithm for autonomous driving based on data-driven risk quantification model. Specifically, a risk quantification model based on the attention mechanism is proposed by modeling the way humans perceive risks during driving, with the idea of achieving safety situation estimation of the surrounding environment through a data-driven approach. To prevent the impact of over-conservative safety guarding policies on the self-evolution capability of the algorithm, a safety-evolutionary decision-control integration algorithm with adjustable safety limits is proposed, and the proposed risk quantization model is integrated into it. Simulation and real-vehicle experiments results illustrate the effectiveness of the proposed method. The results show that the proposed algorithm can generate safe and reasonable actions in a variety of complex scenarios and guarantee safety without losing the evolutionary potential of learning-based autonomous driving systems.", "sections": [{"title": "I. INTRODUCTION", "content": "As an important component of intelligent transportation systems (ITS), autonomous driving systems can complete driving tasks without human intervention through perception, decision-making and electronic control technologies [1] [2]. Autonomous driving technology has the potential to enhance traffic efficiency, reduce the risk of traffic accidents, and offer sustainable solutions to transportation challenges, which has received widespread attention in recent years [3] [4]. However, due to the challenges of algorithmic limitations and technological maturity, there are still many barriers to the realization of high-level autonomous driving. It is necessary to further research related technologies to ensure the safety and efficiency of autonomous driving systems in a variety of complex scenarios [5].\nIn recent years, self-evolution algorithms with experience storage and learning upgrade as the core ideas have attracted more and more attention. Reinforcement learning(RL), as an important part of self-evolution algorithms, has been widely used in many fields and has reached beyond human performance in some areas. It can be seen that the autonomous driving algorithms using reinforcement learning techniques have the potential to adapt to more complex scenarios [6] [7] [8]. However, due to the exploration-exploitation mechanism and the black box nature of reinforcement learning algorithm, it is difficult to guarantee the safety in the process of evolutionary learning and deployment application. This is absolutely unacceptable for a safety-critical system such as autonomous driving [9].\nSafe reinforcement learning is a machine learning method that aims to enable a system to learn in uncertain or dangerous environments while minimizing the risk of producing undesirable outcomes [10] [11]. The approach consists of two main categories, one of which is to consider optimization criteria that minimize the violation of constraints during the reinforcement learning process [12] [13], and the other is to make stochastic exploration in the safety domain [14] [15]. Recently, many researches have applied safety reinforcement learning methods to autonomous driving tasks. These methods utilize a priori knowledge, either through historical data or human assistance to improve security during learning [16] [17] [18]. Regarding the method of changing the optimization criterion, Yang et al. proposed a model-free security reinforcement learning method through Neural Barrier Certificate, which minimizes constraint violation in the process of policy collection through joint learning of policy and Neural Barrier Certificate [19]. As for the design of the safety layer, the methods mainly include control barrier function methods [20] [21], rulebook methods [22] [23] and formalization methods [24]. Zhang et al. proposed a safety reinforcement learning method for autonomous vehicles based on Barrier Lyapunov Function(BLF), which reasonably organized and incorporated BLF items into the optimized inverse control method, and constrained the state variables within the designed safety region during the learning process [25]. Zhang et al. proposed a safety checker based on a responsibility sensitive safety model that provides reinforcement learning agent with fallback safety actions in unsafe situations during the training and"}, {"title": "evaluation phases [26]. Cao et al. proposed a decision-making framework called Trustworthy Improvement Reinforcement Learning (TiRL), which combines reinforcement learning and rule-based algorithms to allow self-improvement while maintaining better system performance [27].", "content": "However, although the above methods can consider safety in the algorithm learning process, the method of changing the optimization criterion can only reduce the constraint violation, and it is still difficult to ensure the process safety in the reinforcement learning algorithm, which cannot fully meet the high safety requirements of autonomous driving. At the same time, safety constraint methods may lead to insufficient exploration of the agent, which leads to difficulties in achieving optimal performance. The design of safety constraints based on manual definitions may also lead to conservative policies, resulting in performance loss [28] [29]. To solve this problem, a feasible solution is to design an adjustable safety limits, and design a safety limits adjustment criterion based on the risk quantitative value in order to alleviate the problem of balancing safety and performance that exists in evolutionary algorithms.\nAt present, there are two main methods for quantifying risk: rule-based methods and artificial potential field-based methods. The rule-based methods aim to evaluate the safety of autonomous driving systems in different situations by formulating a series of rules and indicators to quantify potential risks [30] [31]. Some definitions, such as Time-to-collision (TTC) [32], Time-to-Lane-Change (TLC) [33], Responsibly-sensitive Safety (RSS) [34], etc. have been proposed for calculating risk indicators. However, in some cases, these risk indicators can only support simple driving operations [35]. The method based on artificial potential field, on the other hand, combine logic rules and physical models to quantify risk scenarios by simulating potential risk factors through potential energy calculations [36]. However, the above methods still require more domain expertise and manual rule formulation, which limits their application in complex and dynamic traffic scenarios [37]. Hence, it makes sense to study risk quantification methods based on data-driven approaches that autonomously extract key features from data through learning methods, without the need for human predefinition of rules, and thus have potential in terms of real-world scenario adaptability [38].\nTo sum up, in this paper, a safety self-evolution algorithm for autonomous driving based on a data-driven risk quantification model is proposed. The method combines the risk quantification method and the safety self-evolution decision-control method in order to solve the problem of limited evolutionary performance of safety reinforcement learning methods in dynamic scenarios. An attention mechanism model based on human risk cognition is proposed to quantify the risk of the surrounding environment. In order to avoid the adverse impact of overly conservative safety limits on the self-evolutionary performance of autonomous driving algorithms, we introduce the concept of Safe Critical Acceleration (SCA). On the basis of this concept, a safety-evolution decision-control integration algorithm with adjustable safety limits is designed, and the proposed risk quantization model is integrated into it. The proposed method can generate safe and reasonable action policies in a variety of complex scenarios, while ensuring"}, {"title": "that the self-evolutionary potential of the learning autonomous driving system is not compromised.", "content": "The contributions of this study are summarized as follows:\n1) This paper propose a risk quantification method based on a data-driven approach involves autonomously extracting key features from the data through machine learning methods, providing safety assessment indicators that consider human risk perception.\n2) This paper propose a safety self-evolution method with adjustable safety limits integrating risk quantification indicators, which allows the algorithms to guarantee safety without sacrificing evolutionary performance.\n3) This paper build a virtual-real interaction platfrom with the high-fidelity simulation software and the real autonomous vehicles, and use it to verify the effectiveness of the algorithm.\nThe paper is organized as follows. The proposed framework is introduced in Section II. The modeling of data-driven risk quantification model is introduced in Section III. The descriptions of safe self-evolution algorithm with adjustable safety limits are proposed in Section IV. In Section V, our method is verified and compared in simulation, and section VI concludes this paper."}, {"title": "II. PROPOSED FRAMEWORK", "content": "The proposed self-evolution algorithm for autonomous driving safety based on a data-driven risk quantification model aims to generate safe, efficient, and reasonable driving policies. The algorithm can not only ensure safety during learning and deployment phase, but also effectively prevent the sacrifice of performance improvement ability. The input of the proposed algorithm is the information of the surrounding traffic environment, and the output is the safe control action. The main components are described in Fig. 1, including a data-driven risk quantization model and a safety-evolution decision-control integration algorithm with adjustable safety limits.\nThe risk quantification model in this framework is designed as a transformer architecture. The proposed model simulates human risk perception by formulating a reinforcement learning problem that generates safety quantification indicators as output. Then, the output indicators are provided to the safety-evolution decision-control integration algorithm. The decision-control integration algorithm consists of three components, including an actor-critic framework, a local planner, and an adjustable safety limits mechanism. The actor-critic framework consists of an actor network and a critic network, which implement the algorithm evolution by maximizing the cumulative return. A local planner is combined with n adjustable safety limits mechanism to output reasonable actions that satisfy dynamic adjustable safety limits. The framework can reasonably and dynamically adjust the safety limit according to the surrounding traffic conditions, so that the performance improvement ability of the algorithm can be maintained as much as possible while ensuring the safety."}, {"title": "III. DATA-DRIVEN RISK QUANTIFICATION MODEL", "content": "A. Problem Definition\nIn this paper, we propose a risk quantification model modeling approach for assessing whether an autonomous vehicle"}, {"title": "is likely to enter an unsafe state during driving. With the goal of evaluating whether the current state is safe, the most intuitive idea is to consider the influence of the surrounding traffic situation on itself. However, it is worth noting that the risk quantification is not only related to the current state, but also to the driver's driving ability. An intuitive example is that in the same risk scenario, a skilled driver or higher-level autonomous driving system can be safe, but a novice or lower-level autonomous driving system may be at greater risk.", "content": "Therefore, the input of the risk quantification model should be the surrounding environmental information and the driving agent itself, and the output should be the risk quantification results. To sum up, the risk quantification model $f_{RQ}$ can be defined as:\n$\\Lambda = \\{f_{RQ} (S, A_{\\theta}) |i = 1, 2, ..., n\\},$ (1)\nwhere s is state, which is used to represent the surrounding traffic situation; $A_{\\theta}$ is driving agent with $\\theta$ as parameters. The state input s and the driving agent $A_{\\theta}$ together affect the risk quantification result $\\Lambda$. $A_{\\theta}$ is defined as:\n$A_{\\theta} = \\pi_{\\theta} (s) .$ (2)\nThat is, the agent's policy output is related to the state input s."}, {"title": "B. The mechanisms of human perception of risk", "content": "Humans demonstrate high intelligence while driving, being able to subjectively estimate and evaluate risks. This ability allows humans to make effective decisions in complex and dynamic traffic environments. Therefore, by simulating human risk perception, the safety and responsiveness of autonomous driving systems can be enhanced.\nBy analyzing the above mechanism, the principle of constructing a risk quantification model can be deduced.\n1) Human perception of risk: Fig. 2 presents a schematic of the human risk perception mechanism. Taking the highway scenario as an example, the driver will complete various driving tasks such as following and overtaking by controlling the vehicle according to the movement state of the current traffic vehicle on the basis of judging the risk.\nCase 1 illustrates a low risk situation. In this case, the driver focuses on the fact that the traffic ahead will have no effect on the ego vehicle due to the distance of the surrounding traffic. In terms of human cognitive behavior, the driver's attention is placed farther away, and therefore the trajectory planned in the driver's mind considers a more distant time span (denoted as $T_1$).\nCase 2 illustrates a high risk situation. In this case, the driver focuses on the fact that the ego vehicle will be affected by the proximity of the surrounding traffic vehicles. In terms of human cognitive behavior, the driver's attention is placed near away, and therefore the trajectory planned in the driver's mind considers a closer time span (denoted as $T_2$).\nTherefore, the construction of risk quantification model can be realized by simulating the human risk perception mechanism. Specifically, a human-like driving agent $\\pi_{\\theta}$ is established, which should have the following capabilities:"}, {"title": "Hence, the above problem can be abstracted as: how to extract the smooth trajectory planned by the agent from the current position to complete the autonomous driving task, and how to use the corresponding time span of the trajectory to quantify the risk of the driver.", "content": "The local trajectory planning process of the agent is shown in Fig. 3. Quintic and quartic polynomial curves are respectively applied to describe the process of longitudinal and lateral trajectory planning, as shown in appendix.\nThe polynomial coefficients $p_d$ and $p_s$ can be solved by substituting the planning time $T_c$ with the boundary conditions in Eq. 21 and Eq. 22. Fig. 3 illustrates the change of the longitudinal and lateral planning trajectories for different planning times $T_c^1$, $T_c^2$ and $T_c^3$. It can be seen that the smaller $T_c$ is, the shorter the planning time span is and the faster the response of the policy is.\nAs an effective self-learning method, RL has advantages in solving the decision and planning problem of autonomous driving, which can adapt to complex traffic environments and output efficient and safe driving policies. To sum up, the DD-RQ model proposed in this paper can be constructed as follows: constructing a human-like autonomous driving agent utilizing RL algorithm and then extracting planning time $T_c$ as a benchmark for risk quantification."}, {"title": "C. Model construction", "content": "1) Analysis of Cross-domain migration: Limited by the learning efficiency issue and safety issue of reinforcement learning algorithms, it is difficult to collect enough data during actual operation and use it to train the DD-RQ Model. Therefore, training the RQ model in a simulation environment and deploying it in an application environment is a feasible solution.\nHowever, considering the sim2real gap problem in both simulation and application environments [40] [41], the cross-domain migration of the RQ model needs to be analyzed."}, {"title": "For the RQ model, its running domain is defined as $\\Phi = (P_{sc}, G_{se})$, where $P_{sc}$ is the scenario distribution and $G_{se}$ is the performance improvement ability of the algorithm.", "content": "It is clear that the $P_{sc}$ and $G_{se}$ of the self-evolution algorithm are similar in the simulation operational domain $\\Phi_s$, and the application operational domain $\\Phi_a$. Therefore, the cross-domain migration process of the operational domain of the RQ model can be expressed as $\\Phi_s \\rightarrow \\Phi_a$, and the following derivation can be made:\nRemark 1: when $\\Phi_s \\sim \\Phi_a$ and $\\Phi_s \\rightarrow \\Phi_a$, the agent at the end of evolution have similar abilities, i.e., $A_{\\theta} \\sim A$.\nCombined with the Eq. 1, it can be deduced that:\nRemark 2: when $A_{\\theta} \\sim A$, the risk quantitative value $\\Lambda = f_{RQ}(s, A_{\\theta}) \\simeq f_{RQ} (S, A_{\\theta}) = A$, that is, the RQ model has the approximation principle.\nThe sim2real gap in the conventional sense reveals that the problem of mismatch in policy distribution occurs when the policies learned from the simulation environment are directly deployed to the application environment. In contrast, the safety limit algorithm proposed in this paper will take the current actual state as input and guarantee security through safe guard, which implies that the impact due to the quantitative variability of risk can be effectively eliminated.\n2) RL problem formulation: The decision and planning problem for autonomous driving can be modeled as an MDP problem. A RL problem is formulated for constructing the human-like agent $\\pi_{\\theta}$ proposed in Section III-B. Typical RL problem consists of three parts: state space design, action design, and reward design.\nThe state design needs to consider the information of the ego vehicle, the traffic vehicles and the pedestrian (if need). The design of state space s is shown in Table I:\nIn this paper, i = 1,2,3,4,5. Considering the actual situation of the lane change scenario, the maximum distance of the ego vehicle is set to $s_{max}$ = 760m, the maximum relative distance between the ego vehicle and the traffic vehicle is set to $s_{max}$ = 200m, the maximum relative velocity between the ego vehicle and the traffic vehicle is set to $v_{max}$ = 11.11m/s."}, {"title": "The design of the action space needs to consider the goal of the autonomous driving task. The goal of trajectory planning for autonomous driving is to find a safe trajectory without collision. Combined with the analysis in Section III-B, the action space of RL problem is designed as follows:", "content": "$a=[T_c, d_{fn}, s_f ],$ (3)\nwhere $T_c$ is planning time, $d_{fn}$ and $s_f$ are the expected lateral offset and expected longitudinal velocity after time $T_c$, respectively. A simple PID controller is applied for tracking the desired trajectory generated by the RL algorithm.\nThe design of the reward function plays a crucial role in RL problems, which directly affects the performance of the algorithm. In order to effectively guide the learning of the agent, the formulation of the reward function should follow several key principles.\nFirstly, positive or negative rewards that are too isolated may cause the algorithm to fall into unreasonable local optimal solutions, so both positive and negative rewards must be considered and the relationship between them must be balanced. Secondly, the design of the reward function should be consistent with the optimization goal of the agent, and appropriate rewards should be provided throughout the training process to avoid the problem of reward sparsity. In summary, the reward function is designed for the trajectory planning problem of autonomous driving.\nThe reward function is designed including the speed reward, the collision penalty and the lane deviation penalty, as detailed in the Appendix. The total reward function is:\n$r = r_s + r_c + r_{ld}$ (4)\n3) Risk Quantification Model: The construction of the risk quantification(RQ) model is shown in Fig. 4. First, based on the designed RL problem in this section, the soft actor critic algorithm is applied to train the driving agent. The training is performed in a high-fidelity simulation software, and the traffic vehicles are set to move according to a realistic traffic flow"}, {"title": "model. When the average return is stable and no longer grows, the training process ends, at which time we get a driving agent that can control ego vehicle to realize speed control and lane change to avoid obstacles.", "content": "To obtain RQ and IR results, RQ networks are introduced. The network uses the transformer decoding architecture. The actor network in the driving agent model is selected for collecting data, and the RQ network is trained by the behavior cloning method. Due to the introduction of attention mechanism, we can obtain the correlation degree between each state quantity, and then obtain the IR for the input of the state.\nRecently, the transformer model has gained a lot of attention. The core idea of the transformer model is to capture long-distance dependencies and global information in the input sequence through the self-attention mechanism. By assigning different weights to each position in the sequence, the model can better capture important parts of the sequence. The attention mechanism not only helps to better understand the intrinsic structure of the input sequence, but also makes the model interpretable.\nThe proposed RQ model is constructed based on the transformer encoder model. The Transformer encoder consists of alternating layers of self-attention and MLP block. Layer norm (LN) is applied before every block, and residual connections after every block.\nThe input of the neural network is as described in the state space design in this section, including the relative position and relative speed information of ego vehicle as well as all traffic vehicles. The input vector can be expressed as follows:\n$X = [X_{ego}, X_{obj}, X_{bj}, ..., X_{obj}].$ (5)\nThe standard transformer receives as input a 1D sequence of token embeddings. The embedding project E is applied to extracted feature from input vector:\n$h_0 = [X_{ego}E; X_{obj}E; X_{bj} E, ..., X_{bj}E],$  $E \\in \\mathbb{R}^{DXD}$ (6)\nwhere $X_{ego}$ and $X_{obj}, i = 1, 2, ..., n$ are the state information of the ego vehicle and the traffic vehicles, respectively.\nThe transformer encoder model can be expressed as:\n$z_l = MSA (LN (z_{l-1})) + Z_{l-1} l = 1...L,$ (7)\n$z_l = MLP (LN (z_{l-1})) + Z_{l-1} l = 1...L,$ (8)\n$y = LN (z_L).$ (9)\nIn Eq. 7, the self-attention criterion divides the input embedding into three vectors V, K and Q. The scaled dot-product attention is calculated as follows:\n$\\Theta = Attention(Q, K, V) = softmax (\\frac{QK^T}{\\sqrt{d_k}})V,$ (10)\nwhere $\\Theta$ is scores matrix, Q is a query vector, K is a key vector, V is a value vector, and $d_k$ is a normalization. The complete RQ model is shown in Fig. 4."}, {"title": "The training process of the RQ model is shown in Algorithm 1. Firstly, the driving agent dataset O is collected. To ensure the model generalization, the random traffic flow is generated in the simulation environment by randomizing the parameters $\\sigma$ of the traffic model $T_{tf}$. The converging actor network $\\pi_{\\phi}$ is deployed to the ego vehicle to perform the simulation, obtain the two-tuples (X, a[1]) and collect them into O. The RQ model $\\Psi_{\\Theta_{RQ}}$ is trained with data from O. In line 13, a batch B is randomly sampled. In line 14 and line 15, the loss function is calculated as the mean squared error (MSE) between the predicted and true values, where a and a represent the ground truth and predicted values, respectively. The model is trained using the Adams optimizer.", "content": "Through the normalization calculation, the RQ value can be obtained as follows:\n$RQ = \\frac{T_{RQ} (X) - T_{c_{min}}}{T_{c_{max}} - T_{c_{min}}} \\times 100\\%,$ (11)\nwhere $T_{c_{max}}$ and $T_{c_{min}}$ are the maximum and minimum values of $T_c$.\nThe scores matrix $\\Theta$ can be calculated using Eq. 10\n$\\Theta = \\begin{bmatrix}\n\\mu^{ego}_{1\\times 1} & \\mu^{obj}_{1 \\times 2} &...&  \\mu^{obj}_{1 \\times (n+1)}\\\\\n\\mu^{obj}_{2 \\times 1} & \\mu^{obj}_{2 \\times 2} &...&  \\mu^{obj}_{2 \\times (n+1)}\\\\\n:&:&:&\\\\\n\\mu^{Hobj}_{(n+1) \\times 1} & \\mu^{Hobj}_{(n+1) \\times 2} &...&  \\mu^{Hatt}_{(n+1) \\times (n+1)}\n\\end{bmatrix}$ (12)\nAs can be seen from Eq. 12, where $\\mu$ is the matrix of 5 \u00d7 5, and the lower right corner marker is used to distinguish the source of $\\mu$. $\\Theta$ can be used to characterize the feature correlation degree between each state. Since the correlation"}, {"title": "between the states of each vehicle is of most interest, the elements of the matrix $\\mu$ are summed to obtain the new matrix $\\Theta_{(n+1)x(n+1)}.$", "content": "To obtain the ranking of the feature correlation degree between ego and other traffic vehicles, the dimension 2 to n + 1 of the first row of $\\Theta_{(n+1)\u00d7(n+1)}$ is taken to form a new vector, and the index of the ordering of vector elements is obtained. As shown in Eq. 13:\n$A = argsort(\\Theta_{(n+1)\u00d7(n+1)}[2 : end], \\xi)$ (13)\nwhere the index matrix A is the vector of 1 \u00d7 n, which is the IR output by the RQ model."}, {"title": "IV. SAFE SELF-EVOLUTION ALGORITHM WITH ADJUSTABLE SAFETY LIMIT", "content": "In this section, a safety-evolution decision-control integration algorithm with adjustable safety limits is proposed. The influence of overly conservative safety guard policy on the self-evolution ability is prevented by integrating the proposed RQ model."}, {"title": "A. Agent implementation", "content": "A RL problem based on soft actor critic algorithm is constructed to achieve integrated autonomous driving decision and control with evolutionary capability. It is worth emphasizing that there are some significant differences between the agent $\\pi_{\\omega}$ presented in this section and the driving agent $\\pi_{\\theta}$ described in section III-B. Specifically, the agent $\\pi_{\\omega}$ is used to deploy on the autonomous vehicle to actually control the ego vehicle to realize the driving task, and the driving agent $\\pi_{\\theta}$ is trained and deployed in the simulation to provide training data for the DD-RQ model. Hence, although these two agents share the framework of RL, they still have some differences in problem formulation and model design due to their different application goals.\n1) State space and action design: For an autonomous driving system, the input information accepted by the algorithm is fixed, that is, the required information during the driving process is predetermined. Thus, the state space s of $\\pi_{\\omega}$ is designed to be consistent with $\\pi_{\\phi}$, as shown in Table I.\nThe design of action space needs to consider the actual goal of autonomous driving trajectory planning. For the agent $\\pi_{\\omega}$, different from Eq. 3, we directly choose the longitudinal acceleration of the ego vehicle $a_x$ as the control input. The new action space is designed as follows:\n$a = [d_{fn}, a_x],$ (14)\nwhere $d_{fn}$ is the desired lateral offset and $a_x$ is the longitudinal acceleration of the ego vehicle. A simple PID controller is applied again to track the desired trajectory generated by the RL algorithm [42]."}, {"title": "2) Reward design: The reward function design is carried out on the basis of section III-C and the Appendix. In the actual autonomous driving process, in addition to the speed reward $r_s$, collision penalty $r_c$ and lane departure penalty $r_{ld}$, it is also necessary to consider the driving risk penalty $r_{risk}$ and comfort reward $r_{cf}$.", "content": "As described in Section II, the RQ model and the safety guard policy work together to ensure the safety. On this basis, if the collision risk is recognized, the driving risk penalty $r_{risk}$ will be triggered to take effect. As shown in Eq. 15.\n$T_{risk} = \\begin{cases}\n0 & with \\ risk \\\\\n\\rho_{risk} & risk\n\\end{cases},$ (15)\nwhere $\\rho_{risk}$ is driving risk penalty factor\nThe comfort reward $r_{cf}$ is designed to reduce the amplitude of the longitudinal and lateral actions as much as possible. To make the training easier, a normalization method is applied as shown in Eq. 16.\n$T_{cf} = \\rho_{cf} \\times (\\frac{abs (d_{fn} - d_{fn}^{i-1})}{2 \\times L_{width}} +  \\frac{abs (a_x - a_x^{i-1})}{\\bar{a}} ) $ (16)\nwhere $d_{fn}$ and $d_{fn}^{i-1}$ are the expected lateral displacements at the current time and the last time, $a_x^i$ and $a_x^{i-1}$ are the longitudinal accelerations of the ego vehicle at the current time and the last time, $L_{width}$ is the lane width, $\\bar{a}$ is the normalized coefficient of the longitudinal acceleration, and $\\rho_{cf}$ is the comfort reward coefficient.\nTo sum up, the total reward is:\n$r = r_s + r_c + r_{ld} + r_{risk} + r_{cf}.$ (17)"}, {"title": "B. Safe Critical Acceleration", "content": "Considering that the rule-based safety limit logic will affect the learning effect of the algorithm, this paper proposes the concept of safe critical acceleration(SCA), which is used to integrate the DD-RQ proposed in Section III to realize the dynamic adjustment of safety limits.\nThe safety state of an autonomous vehicle is defined as the longitudinal distance between the ego vehicle and all traffic vehicles is greater than the safe distance $S_{safe}$. Combining the experience of experts and laws and regulations, the safe distance $S_{safe}$ is defined as:\n$S_{safe} = v_{ego} \\times 3.6,$ (18)\nwhere $v_{ego}$ is the longitudinal speed.\nSo far, the safety evolution problem of the learning-based autonomous driving algorithm can be defined as follows:"}, {"title": "during the learning process of the algorithm, when the vehicle is considered to be in or about to be in an unsafe state, the system should reach back to the safe state through lateral or longitudinal control inputs after an appropriate adjustment time $T_c$. As shown in Fig. 5, it's the schematic diagram of the safety distance adjustment process. This process can be expressed as follows:", "content": "$(19) \\begin{cases}\nS_{obj} - S_{ego} + \\Delta S_{obj} = \\Delta S_{ego} + S_{safe}, & S_{obj} - S_{ego} \\geq 0 \\\\\nS_{ego} S_{obj} + \\Delta S_{ego} = \\Delta S_{obj} + S_{safe}, & S_{obj} - S_{ego} < 0\\end{cases},$\nwhere $S_{obj}$ and $S_{ego}$ are the longitudinal displacements of the traffic vehicles and the ego vehicle, $\\Delta S_{obj}$ and $\\Delta S_{ego}$ are the relative longitudinal displacements of the traffic vehicles and the ego vehicle in time $T_c$, respectively.\nThe safe distance adjustment process is regarded as a constant acceleration movement, and the acceleration in the adjustment process is defined as the safe critical acceleration $A_{safe}$. $A_{safe}$ is used to characterize the acceleration limit of the ego vehicle when it does not exceed the safe distance(that is, not leave the safe state). On the basis of Eq. 19, the safe critical acceleration of the ego vehicle for the i-th traffic vehicles can be expressed as follows:\n$A_{safe}^i = \\frac{2(\\Delta S - S_{safe} + T_c. \\Delta v)}{\\frac{\\Delta v}{\\left |{\\Delta S}\\right |} T_c^2}$ (20)\nwhere $\\Delta S = S_{obj} - S_{ego}, \\Delta v = v_{obj} - v_{ego}$.\nIn Eq. 20, $T_c$ will affect the $a_{safe}$. Specifically:\n\u2460: When $\\Delta S > 0$, the ego vehicle should slow down to adjust the distance between the ego vehicle and the preceding traffic vehicle to the safe distance $S_{safe}$. At this time, shortening the adjustment time $T_c$ will require the ego vehicle to pursue a safe distance $S_{safe}$ from the front vehicle by slowing down for a shorter time, so the required deceleration will be larger. At this point, the control output by the RL algorithm much less likely to satisfy the demand, so the safety guard is more likely to intervene.\n\u2461: When $\\Delta S < 0$, the ego vehicle should accelerate to adjust the distance between the ego vehicle and the following traffic vehicle to the safe distance $S_{safe}$. At this time, shortening the adjustment time $T_c$ will require the ego vehicle to pursue a safe distance $S_{safe}$ from the rear vehicle by accelerate for a shorter time, so the required acceleration will be larger. At this point, the control output by the RL algorithm much less likely to satisfy the demand, so the safety guard is more likely to intervene.\nIn summary, by adjusting $T_c$, $a_{safe}$ can be adjusted, which indirectly realizes the dynamic adjustment of the safety limits of the proposed evolutionary algorithm."}, {"title": "C. Adjustable safety limits based on quantitative indicator", "content": "An excessively conservative safety guard intervention policy may have adverse effects on the self-evolution performance of autonomous driving algorithms. Therefore, combined with the RQ model proposed in Section III, this paper designs a safety self-evolution method with adjustable safety limits integrating risk quantification indicators.\nSpecifically, the RQ model outputs a quantitative value of risk RQ for the current traffic environment. $T_c$ is defined as a linear function of RQ and is used to dynamically adjust the safety limits. In high risk scenarios, the safety limits are tightened. This is to ensure that the system is able to adopt a more conservative policy in high-risk scenarios, thus reducing"}, {"title": "the risk of potential accidents. Whereas in low risk scenarios, the safety limits are relaxed. This allows the system to explore more flexibly in a safe environment, thus enhancing its ability to self-evolve and learn. This adjustment mechanism has the advantage of being clear and explainable to maximize the evolutionary potential of the algorithm while ensuring safety.", "content": "The proposed safety guard policy consists of two parts. Firstly, according to the initial action given by the RL algorithm, combined with the safe critical acceleration $a_{safe}$, to determine whether there is a risk in the lateral action and needs to be intervened. Afterwards, if the lateral action is judged to be risky, it is set to maintain the current lane and further judge whether the longitudinal action is risky enough to intervene.\nThe intervention principle of the safety guard is shown in Algorithm 2. The algorithm inputs include the RQ model obtained in Section III, the state input X representing the surrounding environment, and the actions $a = [d_{fn}, a_x]$ given by the RL algorithm. The main steps of the algorithm are as follows:\nLine 1- Line 3: Initializes the algorithm and sets parameters."}, {"title": "V. EXPERIMENTS", "content": "In this section, the proposed safe self-evolution algorithm for autonomous driving based on data-driven risk quantification model is validated in a challenging three-lane stochastic traffic scenario. There are two evaluation scenarios, including a) a dynamic dense traffic scenario and b) a mixed traffic scenario including pedestrian crossing. In scenario a), the training scenario is set within a 180 m range in front of the ego vehicle, with randomly generated traffic flows in the speed range 8 m/s to 12 m/s. Scenario (b) is then based on scenario a), which generates pedestrians crossing the road according to random locations and random speeds, and makes them interact with the traffic flow. The training environment was built in the simulation software CARLA [43]. The parameters of the"}, {"title": "B. Simulation results", "content": "The algorithm is deployed in the training environment. The computer is equipped with an Intel core i7-10700 CPU, NVIDIA GeForce GTX 1660 SYPER GPU.\nFig. 6 shows the visualization results of the DD-RQ model. The experiment is carried out"}]}