{"title": "INTERSECTIONZOO: ECO-DRIVING FOR BENCHMARKING MULTI-AGENT CONTEXTUAL REINFORCEMENT LEARNING", "authors": ["Vindula Jayawardana", "Baptiste Freydt", "Ao Qu", "Cameron Hickert", "Zhongxia Yan", "Cathy Wu"], "abstract": "Despite the popularity of multi-agent reinforcement learning (RL) in simulated and two-player applications, its success in messy real-world applications has been limited. A key challenge lies in its generalizability across problem variations, a common necessity for many real-world problems. Contextual reinforcement learning (CRL) formalizes learning policies that generalize across problem variations. However, the lack of standardized benchmarks for multi-agent CRL has hindered progress in the field. Such benchmarks are desired to be based on real-world applications to naturally capture the many open challenges of real-world problems that affect generalization. To bridge this gap, we propose IntersectionZoo, a comprehensive benchmark suite for multi-agent CRL through the real-world application of cooperative eco-driving in urban road networks. The task of cooperative eco-driving is to control a fleet of vehicles to reduce fleet-level vehicular emissions. By grounding IntersectionZoo in a real-world application, we naturally capture real-world problem characteristics, such as partial observability and multiple competing objectives. IntersectionZoo is built on data-informed simulations of 16,334 signalized intersections derived from 10 major US cities, modeled in an open-source industry-grade microscopic traffic simulator. By modeling factors affecting vehicular exhaust emissions (e.g., temperature, road conditions, travel demand), IntersectionZoo provides one million data-driven traffic scenarios. Using these traffic scenarios, we benchmark popular multi-agent RL and human-like driving algorithms and demonstrate that the popular multi-agent RL algorithms struggle to generalize in CRL settings. Code and documentation are available at https://github.com/mit-wu-lab/IntersectionZoo/.", "sections": [{"title": "1 INTRODUCTION", "content": "Having demonstrated impressive performance in simulated multi-agent applications such as Starcraft (Samvelyan et al., 2019), RL holds potential for various multi-agent real-world applications including autonomous driving (Kiran et al., 2021), robotic warehousing (Bahrpeyma & Reichelt, 2022), and traffic control (Wu et al., 2021). However, compared to simulated applications, the success of RL in real-world applications has been rather limited (Dulac-Arnold et al., 2021). A key challenge lies in making RL algorithms generalize across problem variations, such as when weather conditions change in autonomous driving. Problem variations are common in real-world applications but may not be designed to be explicitly assessed in simulated applications (Kirk et al., 2021).\nMoreover, there are many open challenges that affect the generalization of RL algorithms in real-world multi-agent applications, such as the effect of complex multi-agent dynamics under aleatory uncertainty and partial observability of states and problem variations, optimizing multiple objectives over long horizons, and physical constraints. This makes the generalization in multi-agent RL a class of problems rather than one problem as it encapsulates many open challenges (Kirk et al., 2021)."}, {"title": "2 PRELIMINARIES", "content": "Remark 1: With CRL, we focus on intra-task generalization, which means we train and test on environments stemming from the same task (e.g., eco-driving). A related parallel direction is inter-task generalization, training generally capable agents. In this work, we do not focus on that."}, {"title": "2.1 CONTEXTUAL REINFORCEMENT LEARNING", "content": "CRL formalizes the notion of solving a collection of tasks, each stemming from the same problem (e.g., eco-driving). A collection of tasks is formalized using CMDPs (Hallak et al., 2015). We utilize the formalism from Ghosh et al. (2021). Accordingly, a CMDP M is an MDP with a state space of the form $s = (s', c)$ where s' is the state and c defines the context for the state s'. The context c is fixed within an episode (e.g., a fixed random seed). Given an initial context distribution $p(c)$, the initial state distribution of a CMDP is defined by $p(s) = p(c)p(s'|c)$. Then, given a context c, the CMDP M is restricted to an MDP $M_c$ and is called a context-MDP. In other words, the CMDP manifests as a collection of context-MDPs. Note that context c is not always visible to the agents (e.g., a random seed). Then, a CMDP becomes a Contextual Partially Observable MDP (CPOMDP) (Kirk et al., 2021).\nWe seek to find a policy $\\pi^*$ that maximizes the overall expected return across all context-MDPs where $R(\\pi, M_c)$ is the expected return of policy $\\pi$ on context-MDP $M_c$,\n$\\pi^* = \\max_{\\pi} [\\mathbb{E}_{c\\sim p(c)} [R (\\pi, M_c)]].$ (1)\nIn multi-agent CRL, each context-MDP $M_c$ manifests as a multi-agent control problem. Further, these multi-agent control problems are often partially observable. This is also the case with cooperative eco-driving. Then, we leverage a Decentralized Partial Observable MDP (Dec-POMDP) formulation (Bernstein et al., 2002) following previous work (Yan et al., 2022) to define each context-MDP."}, {"title": "2.2 COOPERATIVE MULIT-AGENT ECO-DRIVING", "content": "Optimizing eco-driving across a full-traffic network is ideal but is impractical in large cities like Los Angeles, with nearly 5000 signalized intersections, and remains an open optimization challenge (Qadri et al., 2020). A common approach is to decompose the network into individual intersections for separate optimizations (Yang et al., 2016; Jayawardana et al., 2024; Yang et al., 2020) while regulating intersection throughput to prevent traffic spill-back due to possible increased throughput. Therefore, it is often assumed that vehicle flow is not at saturation, allowing for reasonable throughput improvements. We adopt the same modeling assumption.\nThe default objective of cooperative eco-driving at individual signalized intersections is to minimize the total exhaust emissions of a fleet of vehicles (both CVs and HDVs) while having a minimal impact on individual travel time. At a given time t, the number of CVs is $k_{CV}$, and HDVs is $k_{HDV}$ such that $k_{CV} + K_{HDV} = n_t$ where $n_t$ is the total number of vehicles in the fleet. Then, we control the longitudinal accelerations of all CVs decentrally using a learned policy to optimize,\n$\\min J = \\sum_{i=1}^{n} \\sum_{t=0}^{T_i} \\mathbb{E} (a_i(t), v_i(t)) + \\tau T_i.$ (2)\nHere, $T_i$ denotes the travel time of vehicle $i$ and time $t$ is a discretized time with increments of $\\delta$ (usually 0.5 seconds). The vehicular exhaust emission function is denoted by $E()$, which takes speed $v_i(t)$ and acceleration $a_i(t)$ of vehicle $i$ at time $t$ and outputs a vehicular emission amount (usually the amount of carbon dioxide). $\\tau$ is the trade-off hyperparameter. We seek to optimize $J$ subject to hard constraints of ensuring vehicle safety, connectivity via vehicle-to-vehicle and vehicle-to-traffic signal communication, and soft constraints of vehicle kinematics, control realism, traffic safety at the fleet level (e.g., minimum time to collision across all vehicles), and passenger comfort.\nHere, by MDP we generally refer to any form of MDPs such as Partially Observable MDPs, etc."}, {"title": "3 RELATED WORK", "content": "In Table 1, we present our assessment of several known single-agent, multi-agent, and CRL benchmarks, focusing on eleven key properties desired for CRL benchmarking (Kirk et al., 2021; Cobbe et al., 2020; Benjamins et al., 2022). We aim to identify whether they satisfy the given properties or whether repurposing or improvising the current benchmark could be used to satisfy the properties.\nRealistic task column assesses if a benchmark is based on real-world tasks. Many benchmarks focus on video games (Cobbe et al., 2020; Machado et al., 2018), strategy games (Wang et al., 2021b), grid worlds (Chevalier-Boisvert et al., 2023), or simple control tasks (Benjamins et al., 2022), which lack real-world complexity. This limitation can lead to exploitable structures (Mohan et al., 2024) or hinder algorithmic advancements (Ellis et al., 2024; Yu et al., 2022; Hu et al., 2021). For instance, in the SMAC benchmark, a policy based only on the timestep can achieve notable win rates (Ellis et al., 2024). Some work involves more realistic robotics tasks (James et al., 2020; Yu et al., 2020), but they often use tightly constrained context features like limited friction levels.\nData-driven context distribution checks if benchmarks provide real-world context distributions, while native CRL support assesses if a benchmark is primarily designed to support CRL. Except for MetaDrive (Li et al., 2022) and SMACv2 (Ellis et al., 2023), most native CRL benchmarks focus on single-agent CRL. MetaDrive is a close second to IntersectionZoo but lacks data-driven context distributions for CMDPs, limiting its ability to capture real-world complexity. Moreover, the initializable context distributions column checks if a benchmark facilitates initializing arbitrary user-defined context distributions. IntersectionZoo facilitates using both additional real-world context distributions (e.g., intersections of other cities) as well as procedurally generating contexts based on user-specified context feature distributions.\nTypically, there are two types of features defining the context distribution and thereby describing the context of a context-MDP: observed context features and unobserved context features. The key difference is whether these features are explicitly visible to the agent. Observed features are directly visible factors of variations, such as lane length in eco-driving. Unobserved features are mostly used for random variations such as those that arise from procedural content generation (PCG) (Cobbe et al., 2020) and are not explicitly visible to the agent (e.g., HDV aggressiveness). Both observed and unobserved features are desired for CRL benchmarking (Kirk et al., 2021). This enables systematic targeted evaluations of generalization such as systematicity (generalization using systematic recombination of known knowledge) and productivity (generalization beyond seen training data) (Hupkes et al., 2020) with non-trivial tasks.\nVarying S, O,T, R refers to variations in states (S), observations (O), transitions (T), and rewards (R). In IntersectionZoo, the variations in environments stem from states (e.g., single vs. multiple lane driving), observations (e.g., diverse vehicle sensor capabilities), rewards (e.g., the trade-off between travel time and emission reduction), and dynamics (e.g., vehicle behavior changes due to varying traffic signal timings). Multiple forms of variations enable diversity and provide more avenues for targeted evaluations of generalizations (Li et al., 2022). While benchmarks like CARL (Benjamins et al., 2022) and MDP Playground (Rajan et al., 2023) are categorized to consist of all forms of variations, they lack evaluation protocols spanning all these variations or consist of multiple tasks without including all variations in each task.\nMultiple objectives can bring another form of variation in CMDPs and are common in real-world problems. It thus manifests as another axis of targeted evaluation of generalization. While existing multi-objective RL benchmarks overlook generalization challenges (Felten et al., 2024), addressing"}, {"title": "4 INTERSECTIONZOO", "content": "Designing a CRL benchmark suite based on a real-world task is challenging. Identifying the factors of variations requires domain knowledge and expert opinion, and data-driven modeling is required"}, {"title": "4.1 DEFINING INTERSECTIONZOO SCENARIOS FOR CONSTRUCTING CMDPS", "content": "In IntersectionZoo eco-driving CMDPs are formulated as a collection of traffic context-MDPs. Each traffic scenario is a basis for a context-MDP. A traffic scenario manifests as a combination of a set of eco-driving factor values that have a known effect on emission benefits at signalized intersections. These factors are related to intersection topology, human driver behavior, vehicle characteristics, traffic flow, and atmospheric conditions.\nConcretely, an intersection is first defined by factors such as lane lengths, lane counts, road grades, turn lane configuration, and speed limit of each approach. Then, vehicle type, age, and fuel type distributions are used with appropriate traffic flow rates and HDV behaviors to define a realistic traffic flow. Each intersection scenario is further assigned representative atmospheric temperature and humidity values based on the season. Further scenario variations can be achieved by changing the eco-driving adoption level (0%-100%). We follow this procedure for every intersection, and the resultant traffic scenarios are the basis for the context-MDPS of each city."}, {"title": "4.2 DEFINING CONTEXT-MDPS", "content": "Here, we provide an overview of the context-MDP definition we use in IntersectionZoo and refer the reader to Appendix A.3 for more specific details. In cooperative eco-driving, each traffic scenario manifests as a partially observable multi-agent control problem. Then, we leverage Decentralized Partial Observable MDP (Dec-POMDP) formulation (Bernstein et al., 2002) following previous work (Yan et al., 2022) to define each context-MDP. For each CV, state, action, and reward are defined as follows.\nState Space: The design of the observed state of a vehicle is mainly based on the capabilities of existing sensor technologies. The observed state of a CV includes its own status, status of neighboring vehicles (leader and follower on all immediate nearby lanes), and status of the immediate traffic signal timing. Further, we provide a selected set of observed features based on the feasibility of obtaining them in the real world by the CV as observed context of the underlying environment.\nAction Space: Longitudinal acceleration of each CV. Standard rule-based controller is used (Erdmann, 2014) for lane changing, focusing IntersectionZoo on the continuous control aspect of eco-driving."}, {"title": "5 EVALUATIONS IN INTERSECTIONZOO", "content": "By default, IntersectionZoo provide interfaces for train/test split evaluations to measure generalization, which is often used with zero-shot policy transfer (Harrison et al., 2019; Higgins et al., 2017; Kirk et al., 2021). This means we train policies on one subset of context MDPs and test on another subset of context MDPs. This includes both IID and OOD evaluation protocols. Hence, OOD evaluation can be performed by training in one city (train CMDP) and testing in another city (test CMDP). Similarly, IID testing can be performed by train/test split of context-MDPs within a given city."}, {"title": "5.1 EVALUATION PROTOCOLS", "content": "5.2 EVALUATION METRICS\nThe performance of a given CRL policy trained to eco-drive is often benchmarked against human-like driving baselines (how much improvement can be obtained from driving differently than humans do) (Jayawardana & Wu, 2022; Jayawardana et al., 2024). IntersectionZoo provides calibrated Intelligent Driver Models using real-world human driving data (Appendix A.2) as human driving baseline for this purpose. We assess the performance of a policy using the average vehicular exhaust emission per vehicle and the intersection throughput of each intersection. Refer to Appendix A.5 for more details on the definitions of these metrics.\nA key requirement for assessing the performance is to ensure intersection throughput per intersection is never reduced by the learned eco-driving policy as explained in Section 2.2. If, for any intersection, the throughput is reduced (even if there are low emissions), its emission benefits are set to zero."}, {"title": "6 INTERSECTIONZOO BENCHMARKING", "content": "We benchmark popular RL algorithms used in multi-agent control to assess their generalization capacity using IntersectionZoo CMDPs. Considering their success in many cooperative multi-agent problems (Yu et al., 2022), we employ PPO (Schulman et al., 2017), DDPG (Silver et al., 2014), multi-agent PPO (MAPPO) with a centralized critic (Yu et al., 2022), and graph convolution networks (GCRL) for explicit cooperation modeling in multi-agent RL (Jiang et al., 2020). In assessing the performance of the algorithms, we assess their generalization capacity along three axes (three forms of targeted evaluations of generalizations) that are often considered as desired generalization properties. In measuring performance, we illustrate the emission benefits at the level of intersection incoming approaches instead of the whole intersection for a better understanding and visualization of the benefit distributions.\nAs evident, all four algorithms fail to generalize, having many failure cases. Failures include having higher emissions and/or lower throughput than the human-driving baseline (0% benefits case). While DDPG has a better success rate than the other three algorithms, it is still far from being successful.\nFurther, generalization performance in Atlanta is different than in SLC for each algorithm, indicating that different CMDPs may pose different challenges in generalization. Given all four algorithms struggle to generalize, for the next set of experiments, we use only PPO and DDPG, which show relatively better generalization than the MAPPO and GCRL.\nSystematicity in generalization: Systematicity is generalization using systematic recombination of known knowledge (Kirk et al., 2021; Hupkes et al., 2020). To test this ability of DDPG and PPO, we leverage IntersectionZoo's capability to procedurally generate context-MDPs. Following Kirk et al. (2021), we first define a set of context features and their corresponding values as a set of uniform distributions (per feature). Then we train policies by sampling feature values from each distribution to create context vectors. However, certain feature value combinations are never used during training. During testing, we only use the feature value combinations that were not used in training. This tests the algorithms' ability to systematically combine known knowledge to generalize. The resultant performance histogram is given in Figure 7a. Both DDPG and PPO fail to systematically generalize; baseline performs better in almost all cases. More details are given in Appendix A.8.\nProductivity in generalization: Productivity is when the learned policies generalization beyond seen training data (Kirk et al., 2021; Hupkes et al., 2020). To test this in PPO and DDPG, we perform an OOD evaluation by using a policy trained on SLC CMDP with zero-shot transfer to Atlanta CMDP. The resultant performance histogram is given in Figure 7b. Note that from the in-distribution performance analysis, it is visible that the DDPG policy trained on SLC CMDP seems to perform slightly better than the policy trained on Atlanta CMDP. However, even after the transfer, both DDPG and PPO seem to perform poorly, further indicating the limitations of existing RL algorithms when it comes to generalization across problem variations."}, {"title": "7 CONCLUSION", "content": "In this work, we propose IntersectionZoo, a comprehensive multi-agent CRL benchmark suite based on the real-world application of cooperative eco-driving. Using IntersectionZoo, we benchmark popular multi-agent RL and human-like driving algorithms and demonstrate that the popular multi-agent RL algorithms struggle to generalize in CRL settings. Specifically, we show popular multi-agent RL algorithms perform poorly when tested for in-distribution generalization, systematicity in generalization, and productivity in generalization with real-world data-driven eco-driving. A current limitation of the benchmark is it can be primarily used to benchmark only continuous control algorithms. However, IntersectionZoo provides discrete lane-changing control for interested users. Further, despite our best efforts to create realistic traffic scenarios, the provided scenarios may have variations from their real-world counterparts due to inevitable data errors and missing data. Overall, IntersectionZoo aims to advance generalization in multi-agent RL research by providing a rich benchmark suite that naturally captures many of the real-world problem characteristics that affect generalization."}, {"title": "A APPENDIX", "content": ""}, {"title": "A.1 DETAILS OF INTERSECTIONZOO CMDP CONTEXT DISTRIBUTIONS", "content": "IntersectionZoo provides 10 CMDPs based on 10 major metropolitan cities across the United States.\n\n\nA.2 MODELING HUMAN-DRIVEN VEHICLES\nFor the foreseeable future, human-driven vehicles will remain prevalent. To model human drivers in our simulations, we use the Intelligent Driver Model (IDM) (Treiber et al., 2000). IDM is a widely accepted car-following model that can produce realistic traffic waves. IDM calculates a vehicle's acceleration using Equation 4, with desired velocity $v_0$, space headway $s_0$, time headway T, maximum acceleration $a$, and comfortable braking deceleration $B$. The velocity difference with the leading vehicle is denoted as $\\Delta v(t)$, and $\\delta$ is a constant.\n$a(t) = a [1 - (\\frac{v(t)}{v_0})^{\\delta} - (\\frac{s^*(v(t), \\Delta v(t))}{s(t)})^2 ]$ (4)\n$s^*(v(t), \\Delta v(t)) = s_0 + \\max (0, v(t)T + \\frac{v(t)\\Delta v(t)}{2\\sqrt{\\alpha\\beta}})$ (5)\nFor simulation accuracy, precise calibration of parameters $v_0$, $s_0$, $T$, $a$, and $\\beta$ is crucial. Different regions may exhibit varying driving behaviors, such as American drivers versus British drivers. Thus, calibrating the IDM parameters is critical for accurate human driver modeling.\nOur IDM model calibration goals are threefold. We aim to align it with real-world human driving behavior at US signalized intersections, tailoring the five IDM parameters for human-like trajectories. We also need separate IDM models for distinct vehicle types (e.g., cars, buses, trucks) due to their"}, {"title": "A.3 DEFINING CONTEXT MDPS", "content": "State Space: The design of the observed state of a vehicle is mainly based on the capabilities of existing sensor technologies. The observed state includes the speed of the ego-vehicle, relative distance to the traffic signal, traffic signal state (red, green, or yellow) for the current phase, time remaining in the current phase, time remaining until the traffic signal turns green for the second and third cycle, vehicle location (i.e., a flag indicating whether the vehicle is approaching the intersection, at the intersection, or exiting the intersection), index of the vehicle's current lane, vehicle's intention to turn right, left, or go straight at the upcoming intersection, and for the follower and the leader vehicles on the same lane, adjacent right lane, and left lane of the ego-vehicle: speed, relative distance, turn signals status (turning right, left, or none).\nAs mentioned earlier, IntersectionZoo has both observed features and unobserved PCG-based features defining the context of a context-MDP. For users interested in conditioning the policies based on the context, we provide observable context features that include eco-driving adoption level, signal timing plan for the traffic signal phase relevant to the vehicle, atmospheric conditions such as temperature and humidity, the fuel type (electric or internal combustion engine), and information about the ego-vehicle's current approach (number of lanes, lane length, speed limit). The decision on which features are available for conditioning is also based on the feasibility of implementing them in the real world.\nAction Space: Longitudinal acceleration of each CV. For lane changing, a standard rule-based controller is used (Erdmann, 2014). This focuses IntersectionZoo on the continuous control aspect of eco-driving."}, {"title": "A.4 ADDITIONAL OBJECTIVES TERMS AND THEIR ENCODING SCHEMES", "content": "IntersectionZoo provides additional objective terms for users who wish to assess the effect of multiple objectives on generalization.\nMulti-objective Reward Function: The reward $r_i^t$ for each CV $i$ at time $t$ is defined in Equation 8 inspired by Jayawardana et al. (2024). Here, $n_t$ is the vehicle fleet size, $v_i$ is the velocity, and $e_i^t$ is the CO2 emissions of vehicle $i$ at time $t$. Hyperparameters include, $\\eta$, $\\alpha$, $\\beta$, and $\\tau$. The indicator function $1_{v_i^t \\leq \\tau}$ indicates whether the vehicle is stopped, while the term $e_i^t$ encourages low emissions. The velocity term captures the effect on travel time. Users can configure the parameter $\\eta$ to either get a fleet-based reward, agent-based reward, or a combination of both. All such formulations are acceptable.\n$r_i^t = \\frac{1}{n_t} [\\eta \\sum_{j=1}^{n_t} (\\alpha v_j^t + 1_{v_j^t \\leq \\tau} + \\beta e_j^t) + (1 - \\eta) (\\alpha v_i^t + 1_{v_i^t \\leq \\tau} + \\beta e_i^t)]$ (8)\nUsers can extend the above default objective with additional reward terms as explained in Section A.4.\nPassenger comfort: To accommodate passenger comfort, vehicles should maintain low accelerations and decelerations. To encourage this behavior, a reward term is defined as the $at$ where $a_t$ is the acceleration (or deceleration) of the vehicle at time t. When used with shared fleet-wise reward, the mean of $at$ across all vehicles is used.\nKinematic realism: Vehicles often cannot have high jerks (changes in accelerations in unit time) as actuators have jerk limits. To account for this, IntersectionZoo provides jerk control as $|at - at-1|$ where $at$ is the acceleration (or deceleration) of the vehicle at time t. When used with shared fleet-wise reward, the mean jerk across all vehicles is used.\nFleet-level safety: While individual vehicle safety is ensured using pre-defined rule-based checks, IntersectionZoo provide surrogate safety measures such as Time To Collision (TTC) to improve traffic flow level safety. These surrogate safety measures are commonly used by traffic engineers to measure the impact of new roadway interventions (Wang et al., 2021a).\nTime to Collision (TTC) for a vehicle is measured as the time it would take for the vehicle to collide if it were to continue moving along its current paths without any changes in speed or direction. Formally, $TTC = \\frac{\\Delta d}{\\Delta v}$ where $\\Delta d$ is the relative distance and $\\Delta v$ is the relative velocity. Both distance and velocity are measured relative to the leading vehicle of the ego-vehicle. In using TTC for fleet-level safety, we take the minimum TTC value across all vehicles at a given time step and share it with all vehicles."}, {"title": "A.5 EVALUTION METRICS", "content": "To measure the generalization performance of a learned CRL policy, we leverage two metrics.\nAverage emission benefits: This measures the per vehicle per time step CO2 emissions in grams based on the CRL policy and compares that with the human driving baseline. The lower the emissions, the better. Percentage emission reduction is given as the benefit.\nAverage intersection throughput benefits: This measures the number of vehicles that cross the intersection during an episode. We present it as the average throughput change percentage compared to the human driving baseline."}, {"title": "A.6 EXPERIMENTAL SETUP", "content": "All experiments are carried out using RLLib (Liang et al., 2018) with the default hyperparameter configuration. All policies are trained as multi-task learning policies where the context to condition the policy is as defined in Section A.3. We leverage 10 multiple workers in training the multi-task learning policies. Experiments were carried out in a computing cluster with 20 CPUs and an NVidia Volta V100 GPU with 32GB RAM. Each benchmarking run took roughly 24 hours in RLLib, with 5000 episodes (each with a horizon of 1000 steps with 50 warmups). We purposely ran each run for large number of iterations to ensure no further training can improve the policies. Benchmarking runs can be run for a shorter number of iterations, reducing computation times further."}, {"title": "A.7 ADDITIONAL RESULTS", "content": "For the reported results in Section 6, for each algorithm, we train with four random seeds. We train for 500 training iterations to ensure policies are well-converged. During the evaluation, we select the best-performing policy based on the rewards, vehicle throughout, and emission reductions."}, {"title": "A.8 SYSTEMATICITY CONTEXT DISTRIBUTION", "content": "For measuring the systematicity in Section 6, we define the Table 3 feature distributions for training and Table 4 defined feature distribution for evaluation. During training we sample intersections from Table 3 defined distribution but also not in Table 4 defined distribution. During evaluations, we only sample intersections from Table 4 defined distribution."}, {"title": "A.9 LICENSE DETAILS AND ACCESSIBILITY", "content": "Our code and the IntersectionZoo are released under the MIT License. The code, the intersection datasets of CMDPs, and the full documentation on how to use IntersectionZoo are available in the Github repository https://github.com/mit-wu-lab/IntersectionZoo/. The intersection datasets are also released under the MIT License. All data used for creating traffic scenarios are based on publicly available open data. SUMO traffic simulator is licensed under the EPL-2.0 with GPL v2 or later as a secondary license option (refer to SUMO website for more details)."}]}