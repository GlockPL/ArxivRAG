{"title": "PLM-Net: Perception Latency Mitigation Network for Vision-Based Lateral Control of Autonomous Vehicles", "authors": ["Aws Khalil", "Jaerock Kwon"], "abstract": "This study introduces the Perception Latency Mitigation Network (PLM-Net), a novel deep learning approach for addressing perception latency in vision-based Autonomous Vehicle (AV) lateral control systems. Perception latency is the delay between capturing the environment through vision sensors (e.g., cameras) and applying an action (e.g., steering). This issue is understudied in both classical and neural-network-based control methods. Reducing this latency with powerful GPUs and FPGAs is possible but impractical for automotive platforms. PLM-Net consists of two models: the Base Model (BM) and the Timed Action Prediction Model (TAPM). BM represents the original Lane Keeping Assist (LKA) system, while TAPM predicts future actions for different latency values. By integrating these models, PLM-Net mitigates perception latency. The final output is determined through linear interpolation of BM and TAPM outputs based on real-time latency. This design addresses both constant and varying latency, improving driving trajectories and steering control. Experimental results validate the efficacy of PLM-Net across various latency conditions.", "sections": [{"title": "I. INTRODUCTION", "content": "Vision-based Autonomous Vehicle (AV) control functions in a similar way to how the human brain interprets and responds to visual information. Both share the same perception-planning-control (sense-think-act) cycle. In this cycle, there is a latency between sensing the environment and applying a corresponding action, which makes the human reaction time always higher than zero [1]. Similarly, it is challenging to completely eliminate this latency in AV control [2], and reducing it through powerful GPUs and FPGAs is impractical in automotive platforms. This latency, if not properly mitigated, can result in a number of issues, ranging from trivial ones like uncomfortable rides to serious ones like endangering the safety of the passengers. Therefore, the ability of an AV driving system to effectively mitigate latency is crucial for ensuring safety. We hypothesize that the human brain have internal mechanisms such as anticipating or predicting future actions to mitigate the latency [3]. Inspired by this, we propose a deep neural network architecture with similar predictive capabilities.\nIn this paper, we refer to this latency as the perception latency d, as shown in Fig.1. When vehicle state is $x_t$ and we have observation $o_t$, the corresponding action $a_t$ is applied at time t + d rather than at time t ($o_t \\rightarrow a_{t+\\delta}$). By the time this action is applied, the vehicle state has changed and we have a new observation. The perception latency has two components: the algorithmic latency, which is the time required for the algorithm to infer an action from an observation, and the actuator latency, which is the time required to apply the inferred action. The actuator latency, known as steering lag in lateral control [4], can be considered constant [4]. However, as mentioned in [5], the high processing cost of visual algorithms leads to uneven time delays based on the driving scenario, which leads to an overall perception latency that is time-varying.\nTo address both constant and time-varying perception latency encountered during lane keeping, we propose a deep-learning-based approach, focusing primarily on vision-based AV lateral control. The contribution of this paper will be discussed after explaining the effect of the perception latency on AV lateral control."}, {"title": "B. Latency Effect on AV Lateral Control", "content": "Vision-based AV lateral control for lane keeping can be achieved using various methods. The traditional approach involves incorporating a computer vision module for lane-marking detection alongside a classical control module for path planning and control. Alternatively, a deep-learning-based approach, such as imitation learning [6], directly maps visual input to control actions, like steering angle. This paper adopts the latter method.\nThe effect of perception latency on AV lateral control during lane keeping is highly dependent on vehicle speed. At low speeds, the scene does not change significantly between the time the vehicle receives the observation at time t and the time it applies the action at time t + d, because the traveled distance during the latency period is minimal (d = \u03b4v). In this scenario, it is reasonable to assume that $a_t \\approx a_{t+\\delta}$, making the effect of d negligible. Thus, at low speeds, we can consider that at time t, the action $a_t$ corresponding to observation $o_t$ is applied immediately ($o_t \\rightarrow a_t$).\nIn this approach, to mitigate the perception latency and avoid this unstable driving behavior, the main objective would be to predict the correct action from the delayed observation input."}, {"title": "C. Contribution", "content": "We introduce the Perception Latency Mitigation Network (PLM-Net), outlined in Fig. 3. This novel deep learning approach is intended to work easily and without requiring any changes to the original vision-based LKA system of the AV. As depicted in Fig. 3, PLM-Net leverages a Timed Action Prediction Model (TAPM) alongside the Base Model (BM), where the latter represents the preexisting vision-based LKA system. The design of the TAPM is inspired by our prior work, ANEC [7], and the Branched Conditional Imitation Learning model (BCIL) proposed by Codevilla et al. [8]. It combines the concept of a predictive model capable of forecasting future action from current visual observation, akin to ANEC [7] (originally inspired by the human driver capability of dealing with the perception latency [9]), with the notion of employing multiple sub-models within the BCIL framework [8] to provide different predictive action values corresponding to different latency levels. Additionally, similar to the 'command' input used in BCIL to select a submodel, the real-time perception latency dt is used to determine the final action value. The final action value \u00e3PLM is determined through the function f(at, dt) where it performs linear interpolation based on the real-time latency value de given all the predictive action values provided by the TAPM ($\\tilde{a}_t^{TAPM}$) and the current action value provided by the BM ($\\tilde{a}_t^{BM}$). A comprehensive explanation of our proposed method is provided in Section III.\nThe structure of this paper proceeds as follows: Section II presents an overview of related research, while Section III outlines the proposed methodology. Following this, Section IV and Section V present the experimental setup and the experimental findings within the simulation environment, accompanied by a comprehensive analysis. Finally, Section VI summarizes the core findings of the study and provides valuable insights into potential avenues for future research."}, {"title": "II. RELATED WORK", "content": "In the domain of vision-based control, the issue of perception latency remains significantly understudied across both classical control models, and neural-network-based control models. In classical control methods, discussions surrounding latency in autonomous driving have largely revolved around computational delays associated with hardware deployment [10]-[13] and communication delays related to network performance [14]\u2013[19]. To the best of our knowledge, only two studies within the classical control domain have attempted to address aspects of perception latency, such as time-variant latency, input delay, or steering lag [4], [5]. However, these discussions primarily centered on classical-control-based solutions. On the other hand, within vision-based neural network control models, research efforts have primarily concentrated on model development, largely neglecting the perception latency issue [6], [20]\u2013[26]. Only few studies have discussed the perception latency [2], [7], [27]\u2013[29].\nLi et al. [2] underscored the significance of addressing latency in online vision-based perception systems. To tackle this issue, they introduced a methodology for assessing the real-time performance of perception systems, effectively balancing accuracy and latency. However, it is important to note that the paper primarily focuses on proposing a metric and benchmark for evaluating the real-time performance of perception systems, rather than offering a direct solution for vehicle control. While their approach provides valuable insights into quantifying the trade-off between accuracy and latency in perception systems, it does not directly address the challenges associated with mitigating latency in autonomous driving scenarios. Our previous work [7], addressed the perception latency issue by proposing the Adaptive Neural Ensemble Controller (ANEC). However, ANEC assumed perception latency to be constant and did not address time-varying latency. Weighted sum was used to combine the output from the two driving models, but the weight function parameters had to be carefully chosen and adjusted by hand. This makes it hard to replicate in different environments as you need to optimize these parameters for each new environment. Mao et al. [27] explored latency in the context of video object detection by analyzing the detection latency of various video object detectors. While they introduced a metric to quantify latency, their study focused on measurement rather than proposing solutions to mitigate latency. Kocic et al. [28] tried to decrease the latency in driving by altering the neural network architecture. Although this approach could potentially diminish latency, preserving the original accuracy poses a significant challenge. Wu et al. [29] emphasized that control-based driving models, which convert images into control signals, inherently exhibit perception latency and are susceptible to failure due to their focus on the current time step. In response, they developed Trajectory-guided Control Prediction (TCP), a multi-task learning system integrating a control prediction model with a trajectory planning model. However, this approach necessitates the extraction of precise trajectories, presenting a notable challenge.\nIn our approach, we acknowledge the inevitability of latency and strive to minimize its impact by intelligently integrating predicted future actions with the current action based on the real-time latency value. This combination mechanism allows our proposed method to effectively mitigate the latency, ensuring better performance in vision-based autonomous vehicle lateral control systems."}, {"title": "III. METHOD", "content": "This section outlines the comprehensive approach undertaken to develop and evaluate the Perception Latency Mitigation Network (PLM-Net). The section is divided into four key subsections. First, we introduce the proposed PLM-Net, detailing the conceptual framework and the rationale behind its design. Second, we delve into the architecture of the PLM-Net models, providing an in-depth analysis of the structural components and their interconnections. Third, we describe the training process of the PLM-Net models. Finally, we present the performance metrics used to evaluate the efficacy of the PLM-Net, highlighting the criteria and benchmarks for assessing its ability to mitigate perception latency in vision-based autonomous vehicle control systems."}, {"title": "A. Proposed Network PLM-Net", "content": "As shown in Fig.3, the PLM-Net has two major components, Base Model (BM) and Timed Action Prediction Model (TAPM). This novel deep learning approach smoothly integrates the TAPM with the original vision-based LKA system of the AV, represented by the BM. Fig.4 shows how these two models mitigate the perception latency where $\u03c0^{BM}$ is the BM policy and $\u03c0^{TAPM}$ is the TAPM policy. As explained in (1), given the vehicle state st at time t, \u03c0BM takes the input it = {ot, Ut} where ot is the visual observation and vt is the vehicle speed at time t and provides the action $\\tilde{a}_t^{BM}$.\n$\\tilde{a}_t^{BM} = \u03c0^{BM}(i_t) = \u03c0^{BM}(o_t, v_t)$.   (1)\nThe TAPM is a predictive model, meaning it can estimate the future state $s_{t+\\delta}$ of the vehicle. The policy $\u03c0^{TAPM}$ generates a set of predictive action values $\\tilde{a}_t^{TAPM}$, where each action corresponds to a future state at a specific latency value in $S^{TAPM}$.\nThe number of predictive actions in the vector $\\tilde{a}_t^{TAPM}$ depends on the number of submodels in TAPM. If there are N submodels, then $S^{TAPM} = [\\delta_1, \\delta_2, ..., \\delta_N]$. For example, if $\\delta_1 = 0.15$ seconds, then the action $a_{t+0.15}$ represents the action that the BM would take if the vehicle was in the state $s_{t+0.15}$. The process of obtaining the vector $\\tilde{a}_t^{TAPM}$ is described by (2), where the inputs to $\u03c0^{TAPM}$ are the output of the $\u03c0^{BM}$ (i.e., the action $\\tilde{a}_t^{BM}$) along with two feature vectors; the image feature vector $z_o^t$ and the vehicle velocity vector $z_v^t$, both derived from $\u03c0^{BM}$.\n$\\tilde{a}_t^{TAPM} = [\\tilde{a}_{t+\\delta_1}, \\tilde{a}_{t+\\delta_2}, ..., \\tilde{a}_{t+\\delta_N}] \\\\ = \u03c0^{TAPM} (\\tilde{a}_t^{BM}, z_o^t, z_v^t)$. (2)\nThe final action of the PLM-Net $\\tilde{a}_t^{PLM}$ is obtained through linear interpolation, as explained in Algorithm 1. This interpolation uses the output of both $\u03c0^{BM}$ and $\u03c0^{TAPM}$ based on the real-time perception latency value \u03b4. Since $\\tilde{a}_t^{BM}$ represents the current action value without latency mitigation, it is equivalent to $a_{t+0.0}$, so we add the value 0.0 to $S^{TAPM}$ and the action $\\tilde{a}_t^{BM}$ to $a^{TAPM}$ to form $\u03b4_{ref}$ and $a_{dref}$, respectively, as shown in (3).\n$\u03b4_{ref} = [0.0, S^{TAPM}] = [0.0, \u03b4_1, \u03b4_2, ..., \u03b4_N]$,\n$\\tilde{a}_{dref} = [\\tilde{a}_t^{BM}, a^{TAPM}] = [\\tilde{a}_{t+0.0}, \\tilde{a}_{t+\\delta_1}, \\tilde{a}_{t+\\delta_2}, ..., \\tilde{a}_{t+\\delta_N}]$. (3)\nThis makes the PLM-Net capable of mitigating both constant and time-variant perception latency \u03b4."}, {"title": "B. PLM-Net Models Architecture", "content": "The architecture of PLM-Net models is illustrated in Fig. 5. The network design of BM is presented in Fig. 5(a), and the network design of TAPM is presented in Fig. 5(b).\nThe BM network design is inspired by the NVIDIA PilotNet structure [30] with modifications tailored to our requirements. Specifically, we adapt the network to accept visual observations (ot) and vehicle speed (vt) as inputs and to predict steering angle (action $\\tilde{a}_t^{BM}$) as output, and we add dropout layers to improve generalization and avoid overfitting. The BM features two primary inputs: the visual observation ot and the vehicle speed vt. The visual observation undergoes processing through five convolutional layers, then a flatten layer to obtain the image feature vector $z_o^t$. Simultaneously, the vehicle speed input is directed through a fully-connected layer that has 144 neurons, resulting in the formation of the vehicle speed vector $z_v^t$. Subsequently, the image feature vector $z_o^t$ and the vehicle speed vector $z_v^t$ are concatenated and forwarded to a multi-layer perceptron (MLP) network. This MLP configuration consists of four fully-connected layers interspersed with three dropout layers. The fully-connected layers contain 512, 100, 50, and 10 neurons, respectively. The dropout layers maintains a dropout rate of 0.3. The final output of the BM $\\tilde{a}_t^{BM}$, the image feature vector $z_o^t$, and the vehicle speed vector $z_v^t$, are forwarded to the TAPM network.\nThe TAPM network design is the result of fusing two key ideas. Firstly, it incorporates a predictive model, similar to ANEC [7], which was inspired by human drivers' ability to mitigate perception latency. Secondly, it utilizes the BCIL framework [8], employing multiple sub-models to provide a range of predictive action values that align with different latency levels, and adding a 'command' input, representing the perception latency \u03b4, to influence the final action value. The TAPM network inputs are $\\tilde{a}_t^{BM}$, $z_o^t$, and $z_v^t$, forwarded from the BM. These three inputs goes to a 100-neuron, 500-neuron, and 100-neuron fully-connected layers, respectively. The outputs of these three layers are then concatenated to be forwarded to all sub-models. Each sub-model consist of three fully connected layers, with 200, 100, and 50 neurons, interspersed with two dropout layers with a dropout rate of 0.3. The sub-models outputs will result in $a^{TAPM}$ shown in (2)."}, {"title": "C. PLM-Net Models Training", "content": "The functionality of a vision-based LKA system, can be achieved through imitation learning where we directly map the steering angle (i.e., action at) to the input it = {Ot, Ut} where Ot is the visual observation and vt is the vehicle speed at time t. The training dataset collected by an expert driver can defined as D = {$i_t, a_t$}$_{t=1}^M$, where M is the total time-steps. Fig.6, explains the training process of the PLM-Net models. Learning the BM policy $\u03c0^{BM}$ is a supervised learning problem. The parameters $\\phi$ of the policy is optimized by minimizing the prediction error of at given the input it, as shown in (4), where we use the Mean Squared Error (MSE) to calculate the loss per sample. Once optimized, the BM can predict the action $\\tilde{a}_t^{BM}$ given the input it = {ot, vt} at time t as shown in (1).\narg $min_\\phi \\frac{1}{M} \\sum_{t=1}^M (\\tilde{a}_t^{BM} \u2013 a_t)^2$  (4)\nTo learn the TAPM policy $\u03c0^{TAPM}$, we generate a new dataset $D^{TAPM}$ from the original dataset D, mapping the input it = {ot,vt} to a N distinct future actions $\\tilde{a}_{t+\u03b4_i}^{TAPM}$ corresponding to N distinct latency values $S^{TAPM} = [\\delta_1,...,\\delta_N]$ (2), where\n$D^{TAPM} = {i_t,a_t^{TAPM}}_{t=1}^M = {i_t, [a_{t+\\delta_1}, ..., a_{t+\\delta_N}]}_{t=1}^M$\nThe optimization of the parameters \u03b8 of the TAPM policy $\u03c0^{TAPM}$, is explained in (5). We minimize the prediction error of $\\tilde{a}_t^{TAPM}$ given the input it, where $a_{t+\u03b4_i}^{TAPM}$ is the ground truth values of $\\tilde{a}_{t+\u03b4_i}^{TAPM}$. We use MSE to calculate the loss per sample.\narg $min_\\theta \\frac{1}{M} \\sum_{t=1}^M \\sum_{j=1}^N (a_{t+\u03b4_j} \u2013 a_{t+\u03b4_j})^2$ (5)"}, {"title": "III-D. Performance Metrics", "content": "In the context of lateral control for AVs, perception latency can lead to delayed or incorrect actions, particularly affecting the steering angle, which in turn impacts the overall trajectory of the vehicle as explained in Section I-B. Therefore, to assess PLM-Net's ability to mitigate this latency, two primary evaluation criteria can be employed: steering angle accuracy and trajectory similarity. Accurate steering is essential for maintaining lane position and following the desired path, ensuring precise vehicle control even under latency conditions. Meanwhile, trajectory similarity evaluates how closely the vehicle's path adheres to the intended trajectory, providing insight into the broader impact of perception latency on vehicle navigation and the efficacy of PLM-Net in mitigating it."}, {"title": "IV. EXPERIMENTAL SETUP", "content": "This section outlines the experimental setup for validating the Perception Latency Mitigation Network (PLM-Net). We begin by describing the simulator used for creating a controlled testing environment. Next, we detail the dataset for training and evaluation. We then discuss the methods for measuring driving performance, followed by the parameter tuning process. After that, an ablation study is presented to highlight key development decisions. Finally, we specify the computational environment and machine learning framework, including hardware and software configurations."}, {"title": "A. Simulator", "content": "Our experiments were carried out using the OSCAR simulator [31], renowned for its user-friendly interface and customizable features. OSCAR operates on ROS (Robotic Operating System) [32], seamlessly integrated with the Gazebo multi-robot 3D simulator [33]. Specifically, we utilized ROS Melodic and Gazebo 9 for our experiments."}, {"title": "B. Dataset", "content": "a) Training and Test Tracks: The track used to train the PLM-Net is the same training track used in our prior work [7]. To evaluate the PLM-Net ability to mitigate the perception latency, we designed a new three-lane test track that has several turns in left and right directions and straight road segments, as shown in Fig. 7.\nb) Data Collection: Our training dataset, denoted as D, was collected by a human driver navigating the training track using OSCAR simulator. This simulator captures visual observations via a mounted camera on the vehicle, alongside critical control values such as steering angle, throttle position, braking pressure, time, velocity, acceleration, and position. High-quality data was collected using the Logitech G920 dual-motor feedback driving force racing wheel with pedals and a gear shifter, resulting in approximately 115,000 clean training samples. Table I provides detailed statistics on the steering and velocity data.\nThe steering angle for the vehicle is represented on a scale from -1 to 1, where 0 denotes the center position. According to the right-hand rule, positive steering angles (0 to 1) correspond to rotations to the left, and negative steering angles (0 to -1) correspond to rotations to the right. The steering wheel has a maximum rotation angle of \u00b1450\u00b0. This mapping implies that a steering angle of 1 corresponds to a 450\u00b0 rotation to the left, while a steering angle of -1 corresponds to a 450\u00b0 rotation to the right. Thus, the steering angle range is scaled such that 0 to 1 maps to 0\u00b0 to 450\u00b0 and 0 to -1 maps to 0\u00b0 to -450\u00b0.\nc) Data Balancing: As shown in Table I, around 50% of the steering values in our dataset approximately equates to 0.0, indicating the vehicle traveling on a straight road segment. Training a driving model solely on this dataset would introduce bias. To address this, we conducted histogram-based data balancing to reduce the skew towards zero steering values. Fig. 8 illustrates the steering angle histogram before (left) and after (right) the balancing process. Post-balancing, our dataset comprised approximately 67,000 data samples.\nd) Data Augmentation: To improve the model's generalization capabilities and augment dataset diversity during training, we implemented data augmentation techniques. Each image presented to the network undergoes a random subset of transformations, including horizontal flipping, where the steering value is negated, and random changes in brightness, while preserving the original steering angle. These augmentation techniques have proven effective in enhancing the robustness of our model during training."}, {"title": "C. Driving Performance Evaluation", "content": "While Section III-D explained the rationale behind choosing the performance metrics and their importance, this section discusses the adopted methods to measure them in our experiments.\na) Steering Angle Accuracy: We analyzed the steering angle values under different conditions: BM driving without latency, BM driving with latency but without TAPM, and BM driving with latency and TAPM (utilizing PLM-Net). This analysis is conducted both qualitatively, through visual inspection, and quantitatively, by calculating metrics such as Mean Absolute Error (MAE), Mean Squared Error (MSE), and Root Mean Squared Error (RMSE).\nb) Trajectory Similarity: The performance metrics used to compare the driving trajectories were adopted from [34] and [35]. We measure the similarity between driving trajectories based on lane center positioning. We use partial curve mapping, Frechet distance, area between curves, curve length, and dynamic time warping from [34], and the Driving Trajectory Stability Index (DTSI) from [35]."}, {"title": "D. Parameter Tuning", "content": "For the TAPM, we used N = 5 submodels, meaning the TAPM predicts five future actions for five different latency values in $S^{TAPM}$. Specifically, the latency values starts with \u03b4\u2081 = 0.15 seconds, with increments of 0.05 seconds, resulting in $S^{TAPM} = [0.15, 0.20, 0.25, 0.30, 0.35]$ seconds. Consequently, the predictive action vector $\\tilde{a}_t^{TAPM}$ in (2) becomes:\n$a_t^{APM} = [a_{t+0.15}, \\tilde{a}_{t+0.20}, a_{t+0.25}, \\tilde{a}_{t+0.30}, a_{t+0.35}]$\nAs detailed in (3), the reference latency vector $\u03b4_{ref}$ and the corresponding action vector $\\tilde{a}_{dref}$ are formed by including the BM action for zero latency. Thus, $\u03b4_{ref}$ and $\\tilde{a}_{dref}$ become:\n$\u03b4_{ref} = [0.0, 0.15, 0.20, 0.25, 0.30, 0.35]$.\n$a_{dref} = [a_{t+0.0}, \\tilde{a}_{t+0.15}, a_{t+0.20}, a_{t+0.25}, \\tilde{a}_{t+0.30}, a_{t+0.35}]$\nThese configurations enable the PLM-Net to handle both constant and time-varying perception latency within the range [0 -0.35] seconds.\nBoth models, BM and TAPM, were trained using the Adam optimizer [36] with a batch size of 32 and a learning rate of 0.001. Table II, shows the number of trainable and non-trainable parameters for the BM and the TAPM. The BM has 6,006,191 parameters, in which all of them are trainable. The TAPM has 12, 256, 396 total parameters, where 6,250, 205 are trainable and 6,006,191 are non-trainable since the BM layers are set to be not trainable when training the TAPM. For the performance metric DTSI, we used the default parameters recommended in [35]."}, {"title": "E. Ablation Study", "content": "In our exploration of different model architectures for the TAPM network, we experimented with various configurations to optimize performance. Initially, we introduced an additional fully-connected layer with 500 neurons after the concatenation layer preceding the five sub-models. However, this adjustment overly complicated the learning process for the predictive action values. Since this layer was shared among all sub-models, it hindered their individual learning capacities, resulting in subpar outcomes. Further experimentation involved modifying the number of layers within the sub-models. Reducing the layers to two, with 100 and 50 neurons respectively, led to the model's inability to effectively learn predictive actions. Similarly, adding an extra fully-connected layer to each sub-model with 300 neurons yielded comparable (if not slightly inferior) results to the existing architecture, thus introducing unnecessary complexity without significant improvement. Additionally, we explored the integration of Long Short-Term Memory (LSTM) [37] layers into the sub-models to capture temporal information. However, this approach encountered substantial challenges. Firstly, the model's complexity increased significantly, impeding computational efficiency. Secondly, the nature of capturing temporal information hindered conventional data balancing techniques and random data sampling from the dataset to enhance batch diversity."}, {"title": "F. Computational Environment and Machine Learning Framework", "content": "Our machine learning framework is built upon TensorFlow and Keras libraries. Specifically, we utilized Keras version 2.2.5 in conjunction with TensorFlow-GPU version 1.12.0, leveraging CUDA 9 and cuDNN 7.1.2 for GPU acceleration. All computational experiments were conducted on a hardware setup featuring an Intel i7 - 10700K CPU, 32GB of RAM, and an NVIDIA GeForce RTX 2080 with 8GB of GPU memory. The operating system employed for these experiments was Ubuntu 18.04.6."}, {"title": "V. EXPERIMENTAL RESULTS", "content": "Our experimental design aimed to investigate the impact of perception latency on driving and assess the efficacy of our proposed solution, PLM-Net, in mitigating this effect (as detailed in Section I-B). To simulate latency, we introduced delays in the input data and apply closed-loop velocity control to make the vehicle drive at a constant speed (around 60 km/h), mimicking real-world scenarios where perception and decision-making processes are delayed. For instance, with a 0.2-second latency, the available visual observation at time t becomes $O_{t-0.2}$ instead of ot, causing the baseline model (BM) to generate delayed actions $\\alpha_{t-0.2}^{BM}$, which PLM-Net aims to correct to at.\nWe assess the impact of perception latency through a comparison of driving behaviors: BM without latency, BM with latency, and PLM-Net with latency. Successful mitigation by PLM-Net is indicated when its driving performance closely resembles that of the latency-free BM. Our evaluation involves analyzing steering angle accuracy and driving trajectory similarity, as detailed in Section IV-C, for both constant and time-variant perception latency."}, {"title": "A. Constant Perception Latency Mitigation", "content": "In evaluating PLM-Net under constant perception latency, we focus on a latency of 0.2 seconds, with similar trends observed for other constant latency values, as illustrated in Appendix A. Fig.9 provides a qualitative comparison of steering angles over time between BM with and without latency and PLM-Net with the same latency. In this figure, the blue line represents BM driving without latency, the green line represents BM driving with 0.2 seconds latency, and the red line represents PLM-Net driving with 0.2 seconds latency. Additionally, Fig.10 provides a visual representation of vehicle trajectories on the test track, colored based on steering angle, to further elucidate the comparative performance. Table III quantifies steering angle errors, demonstrating PLM-Net's superior performance in reducing errors compared to BM under identical latency conditions. Under a constant perception latency of 0.2 seconds, the performance of the BM significantly decreased. The Mean Absolute Error (MAE) between the BM without latency and the BM with latency is 0.1915, indicating a substantial degradation in steering angle accuracy. However, when using PLM-Net, the performance decline was mitigated, with the MAE reduced to 0.0726. This represents a 62.1% improvement in MAE compared to the BM under the same latency condition. Additionally, the Mean Squared Error (MSE) and Root Mean Squared Error (RMSE) were similarly improved with PLM-Net, showing reductions of 82.5% and 58.2%, respectively.\nFurthermore, Fig.11 presents trajectory comparisons qualitatively, while Table IV presents trajectory comparisons quantitatively, showing PLM-Net's ability to maintain accurate driving trajectories despite latency-induced challenges. Each color-coded trajectory corresponds to a different driving condition: blue for BM driving without latency, green for BM driving with 0.2 seconds latency, and red for PLM-Net driving with 0.2 seconds latency. Examining the deviation from the lane center on the full track, the Partial Curve Mapping metric for BM with 0.2 seconds latency increased by 238.7%, while PLM-Net maintained a much smaller increase of only 11.1%. Similarly, improvements in Frechet distance, area between curves, curve length, and DTSI demonstrate that PLM-Net significantly reduces the trajectory deviation caused by latency. The additional segments of Fig.11 and Table IV, specifically parts (b), (c), and (d), which depict the trajectories on a straight road, right turn, and left turn, respectively, also demonstrate that PLM-Net effectively mitigates latency, similar to the results observed on the full track."}, {"title": "B. Time-variant Perception Latency Mitigation", "content": "For time-variant perception latency, we evaluate PLM-Net against varying latency levels ([0.0 \u2013 0.35] seconds). Similar to the constant latency scenario, the upper image in Fig.12 illustrates qualitative steering angle comparisons over time, with the blue line representing BM driving without latency, the green line representing BM driving with time-variant latency, and the red line representing PLM-Net driving with time-variant latency. The lower image illustrates the varying latency values experienced by both models over time. Additionally, Fig.13 provides a visual representation of vehicle trajectories on the test track, colored based on steering angle values. Table V quantifies steering angle errors, demonstrating PLM-Net's effectiveness in reducing errors compared to BM under time-variant latency conditions. Under a time-variant perception latency of [0 - 0.35] seconds, the performance of the Base Model (BM) significantly decreased. The Mean Absolute Error (MAE) between the BM without latency and the BM with latency is 0.3336, indicating a substantial degradation in steering angle accuracy. However, when using PLM-Net, the performance decline was mitigated, with the MAE reduced to 0.0710. This represents a 78.7% improvement in MAE compared to the BM under the same latency condition. Additionally, the Mean Squared Error (MSE) and Root Mean Squared Error (RMSE) were similarly improved with PLM-Net, showing reductions of 94.2% and 76.0%, respectively.\nFig.14 presents trajectory comparisons qualitatively, while Table VI presents trajectory comparisons quantitatively, confirming PLM-Net's successful mitigation of time-variant perception latency. Each color-coded trajectory corresponds to a different driving condition: blue for BM driving without latency, green for BM driving with time-variant latency, and red for PLM-Net driving with time-variant latency. The Partial Curve Mapping metric revealed a 396.6% increase in deviation from the lane center for BM with [0.0 - 0.35] seconds time-variant latency on the full track, while PLM-Net showed a more modest increase of 114.0%. Similarly, the Frechet distance for BM increased by 254.9%, compared to a 66.2% increase for PLM-Net. This pattern of reduced deviation is also reflected in the improvements in area between curves, curve length, and DTSI, indicating that PLM-Net significantly mitigates the trajectory deviations caused by latency. Parts (b), (c), and (d) of Fig.14 and Table VI, which illustrate the trajectories during a straight segment, a right turn, and a left turn, respectively, exhibit results consistent with the full track, confirming that PLM-Net successfully mitigates latency.\nOverall, the experimental findings underscore PLM-Net's robustness in mitigating both constant and time-variant perception latency, indicating its potential for enhancing driving performance in real-world scenarios."}, {"title": "VI. CONCLUSION", "content": "In this paper, we proposed and evaluated PLM-Net, a novel approach to mitigate perception latency in vision-based lateral control systems for autonomous vehicles. By incorporating a Timed Action Predictive Model (TAPM) alongside a Base Model (BM), our system anticipates and adjusts for latency, enhancing stability and safety in lateral control. This predictive capability allows PLM-Net to maintain more accurate control even with time-varying latencies.\nOur approach leverages deep learning to predict future actions based on current observations andvehicle speed, providing a more responsive control system. The TAPM was trained to predict a series of future actions. Interpolation is then used on these actions and the action predicted by the original LKA system (i.e., BM) based on the real-time perception latency to provide the PLM-Net final output.\nThrough comprehensive experimentation, we demonstrated the effectiveness of PLM-Net in reducing the negative impacts of latency across varying levels, such as steering angle errors and unstable driving trajectories.\nFuture research could focus on enhancing the robustness of PLM-Net in diverse real-world environments, investigating its scalability to handle more complex driving scenarios, and exploring additional techniques to further optimize its performance and efficiency."}, {"title": "APPENDIX A CONSTANT PERCEPTION LATENCY", "content": "The following are the results from the different constant perception latency values that we tested to validate our model PLM-Net. The interpretation of these results is similar to that in Section V, so directly show the relevant figures and tables for each latency value.\nA. Perception Latency 0.15 seconds\nThis subsection illustrates the results when the perception latency was 0.15 seconds. Fig.15 and Table VII shows the steering comparison against time. Fig.16 compares driving trajectories colored based on steering angle values. Fig.21 and Table X shows the trajectory similarity.\nB. Perception Latency 0.25 seconds\nThis subsection illustrates the results when the perception latency was 0.25 seconds. Fig.17 and Table VIII shows the steering comparison against time. Fig.18 compares driving trajectories colored based on steering angle values. Fig.22 and Table XI shows the trajectory similarity.\nC. Perception Latency 0.30 seconds\nThis subsection illustrates the results when the perception latency was 0.30 seconds. Fig.19 and Table IX shows the steering comparison against time. Fig.20 compares driving trajectories colored based on steering angle values. Fig.23 and Table XII shows the trajectory similarity."}]}