{"title": "ChainStream: An LLM-based Framework for Unified Synthetic Sensing", "authors": ["Jiacheng Liu", "Yuanchun Li", "Liangyan Li", "Yi Sun", "Hao Wen", "Xiangyu Li", "Yao Guo", "Yunxin Liu"], "abstract": "Many applications demand context sensing to offer personalized and timely services. Yet, developing sensing programs can be challenging for developers and using them is privacy-concerning for end-users. In this paper, we propose to use natural language as the unified interface to process personal data and sense user context, which can effectively ease app development and make the data pipeline more transparent. Our work is inspired by large language models (LLMs) and other generative models, while directly applying them does not solve the problem - letting the model directly process the data cannot handle complex sensing requests and letting the model write the data processing program suffers error-prone code generation. We address the problem with 1) a unified data processing framework that makes context-sensing programs simpler and 2) a feedback-guided query optimizer that makes data query more informative. To evaluate the performance of natural language-based context sensing, we create a benchmark that contains 133 context sensing tasks. Extensive evaluation has shown that our approach is able to automatically solve the context-sensing tasks efficiently and precisely. The code is opensourced at https://github.com/MobileLLM/ChainStream.", "sections": [{"title": "1. Introduction", "content": "The wide adoption of mobile devices and sensors have enabled a great variety of context-aware applications (apps), i.e. the apps driven by situational information about the users, devices, and/or environments. For example, many apps provide location-based services (LBS) such as ride hailing, restaurant recommendation, weather reporting, etc. Some apps offer health-related services based on the user's health condition, activity and mood inferred from mobile sensors. Some apps can also analyze the situations of home, building and city based on distributed cameras. In the future, as more IoT devices and sensors are being deployed and connected, we anticipate that there will be a huge increasing de-mand to build context-aware applications.\nHowever, developing context-sensing programs is not easy today, and the challenges are two-fold. First, the diverse sensor types, data formats and fragmented APIs have made it difficult to write context-sensing programs. Developers usually need to study numerous documentations before actually starting to develop the context-aware apps, which essentially slows down the development and innovation in the field. Second, it has been a long-standing concern for end-users as such context-aware apps heavily rely on sensitive personal data. Permission-based access control systems in mobile systems (e.g. Android) have been criticized almost since its beginning due to its coarse granularity and poor un-derstandability. There is still no better alternative till today.\nThere are many existing approaches for the above two issues. For example, to ease the development of context-aware apps, several programming frameworks were in-troduced, offering better encapsulation of the key APIs required for context sensing [12, 17, 27, 31]. The recent advances of sensing AI have also made it much easier to infer high-level semantic information from different kinds of sensor data. To reduce users' privacy concerns, researchers have proposed new permission mechanisms [1, 28, 33], app analysis tools [3, 15], privacy-aware pro-gramming frameworks [16, 49] and annotation systems [25, 44] to improve the transparency and understand-ability of personal data access and processing behaviors in apps. Unfortunately, these approaches have limited practicability for both developers and end-users, since the new programming frameworks require developers to learn new APIs, and the abstract concepts (data flow, permission descriptions, etc.) extracted by automated privacy tools are still difficult for users to understand.\nInspired by the recent advances of large language models (LLMs) and large multimodal models (LMMs),"}, {"title": "2. Background and Related Work", "content": "Context-aware applications are designed to provide per-sonalized services based on various types of contextual information, collected by both hardware and software sensors. Common contextual information includes the temporal context (e.g., time), spatial context (e.g., posi-tion), physical context (e.g., ambient brightness), digital context (e.g., other apps in use), etc. [21]\nFor example, sports apps record users' workout history with time, location, and sensitive health information like heart rate and blood oxygen rate, to help make future plans. Smart home apps manage various connected de-vices within a home by monitoring physical contexts including temperature, brightness, and humidity. So-cial apps keep track of various user contexts like the user's location and recent behaviours to provided per-sonalized feeds, recommendations, and deliver targeted advertisements."}, {"title": "2.2. Developing Context Sensing Programs", "content": "Developing context-aware applications requires consid-erable efforts due to (1) the diverse and fragmented sensing APIs and (2) users' privacy concerns. To ease the development process, many programming frameworks have been proposed to provide a unified abstraction of different sensors' APIs, so that the developers can focus more on high-level data flow and context management, instead of low-level implementation details [12, 17, 27]. Meanwhile, as context-aware applications rely heavily on sensitive personal data, various privacy-preserving mechanisms have been proposed to address the users' privacy concerns, such as adopting privacy markers, per-mission mechanisms, app analysis tools, and privacy-aware programming systems [3, 25, 33, 44, 49].\nDespite the progress made by existing works, the chal-"}, {"title": "2.3. LLM for Sensing and App Development", "content": "Recent advancements in Large Language Models (LLMs) and Large Multimodal Models (LMMs) have enabled a wide range of applications that leverage sensor data for context-aware services, such as health monitoring, activity recognition, and environmental sensing [30, 32, 51]. Most existing works directly utilize LLMs/LMMs to analyze sensor data for specific tasks [10, 38, 39, 48]. These methods have shown promising results in achieving real-world tasks using sensor data. However, they are limited in addressing more complex sensing tasks. For instance, the task of \"recording microphone loudness every two minutes\" requires not only a deep understanding of sensing data but also logic control capabilities, which are better suited to code generation.\nAn alternative approach is to leverage the code gen-eration capabilities of LLMs to automatically create ap-plications or scripts for sensing-related tasks. Several frameworks have been proposed to handle complex tasks using multi-agent systems [23], self-refinement [50], or in-context learning [41]. However, these methods fall short in building real-world, context-aware applications due to fragmented sensing APIs and inconsistent data formats. Our approach provides a generalized frame-work that simplifies the process for LLMs to generate code to accomplish sensing-related tasks effectively."}, {"title": "2.4. Goal and Challenges", "content": "We aim to introduce a framework where developers and users can access and process personal data using natural language. Doing so can effectively simplify the develop-ment of context-aware apps and make the data process-ing pipeline more transparent for end-users. However, designing such a framework presents several challenges. First, generating executable code for complex sensing tasks is difficult, even for skilled developers. Sensor data is diverse, ranging from physical signals to digital text, and is produced in massive quantities, with streams generated every second. Writing code to handle these tasks efficiently is challenging. Second, the fragmented and inconsistent nature of sensor-related APIs leads to"}, {"title": "3. Programming Framework Design", "content": "principles: unification and simplification. Through uni-fication, we aim for different sensing programs to share common patterns, allowing the knowledge gained from one program to be flexibly transferred to others. Sim-plification involves reducing the number of APIs and statements required to implement each task, making the generated program shorter and less error-prone.\nInspired by the success of stream processing frame-works (e.g. Flink, Spark, etc.) in data processing, we introduce a stream-based context-sensing framework in ChainStream, as shown in Figure 2. Using the stream-style interface offers two important benefits. First, the stream-based programming model naturally makes the data-processing program more concise, consistent and less fragmented, making it easier to learn for both hu-man and Al programmers. Second, today's LLMs are typ-ically pretrained with large-scale code corpora, which already contains many programs based on existing pop-ular stream processing framework. Such knowledge can be transferred to our new framework.\nThe framework mainly consists of three layers: the API layer, the Abstraction layer, and the Runtime layer. At the API layer, we introduce a set of unified Stream APIs for all kinds of context-sensing tasks. Developers are expected to manipulate the streams with nested custom functions, generate the desired event streams, and trigger actions by listening to the events. The Ab-straction layer offers both a unified model abstraction and a unified data abstraction. The former enables the unified access and management of different foundation models, which can be used to construct various types of AI-based stream transformation functions. The lat-ter enables unified access to and management of sens-ing data sources, which can be transformed into other streams for more advanced objectives. Finally, the Run-time layer effectively manages the sensing programs with the Stream Flow Graph (SFG), making the system efficient and transparent."}, {"title": "3.1. Unified Stream Abstraction", "content": "ChainStream adopts the concept of streams to abstract all kinds of source data. Each stream can be conceptu-alized as a dynamic collection of data items that grows infinitely over time. Each data item is a dictionary of key-value pairs with fixed key names and value types. To help developers and AI models to understand, each stream in ChainStream is associated with a standard stream description. A stream description is a dictionary"}, {"title": "3.1.1. Core Concepts: Stream, Function, Program", "content": "ChainStream adopts the concept of streams to abstract all kinds of source data. Each stream can be conceptu-alized as a dynamic collection of data items that grows infinitely over time. Each data item is a dictionary of key-value pairs with fixed key names and value types. To help developers and AI models to understand, each stream in ChainStream is associated with a standard stream description. A stream description is a dictionary"}, {"title": "3.1.2. Abstraction Layer", "content": "Abstractions are used to unify the formats of data and program patterns. The most important aspects include sensor abstraction and model abstraction."}, {"title": "Sensor Abstraction.", "content": "Sensing data is the basic mate-rial for constructing sensing-programs. By nature, differ-ent sensors are developed by different providers, using different data formats and APIs. To enable the diverse sensing data to be manipulated by our unified API, we encapsulate different sensors within the same Stream abstraction. Specifically, each abstracted sensor in Chain-Stream would produce a stream of items in a predefined format. Currently, ChainStream provides 16 built-in sensor abstractions, as shown in Table 1. These include common hardware sensors (cameras, microphones, IMU, etc.) and software sensors (i.e. continuous stream of data items/events about the user/device/environment) [30].\nNote that the sensing data may originate from differ-ent devices. Therefore, to set up the sensor abstraction, we also develop the network abstraction so that the sens-ing data streams can be seamlessly centralized on the same device."}, {"title": "Model Abstraction.", "content": "An important feature of Chain-Stream is to use foundation models to construct the stream processing functions. However, the models are another source of heterogeneity\u2014foundation models from different developers typically use different APIs and different error handling logic. To reduce the cog-nitive load on users of our framework, ChainStream uses a type-based encapsulation for model selection and use. The model user only needs to specify the model by the types of input/output data (using get_fm API), and use the model with the same prompt construction pipeline (using build_prompt API). Such abstractions also enable flexible and seamless model replacement in the event of model service errors."}, {"title": "3.1.3. API Design", "content": "Table 2 lists the main APIs in the system, including three major categories: Stream, Buffer, and Model. When developing a sensing-program, developers need to declare how the program manipulates the streams in the main method. The Stream API abstracts data streams and provides operations for creating, getting, listening to, writing, and batching streams. The Buffer API is a data structure for caching and managing the intermediate stream data across different streams. The Model API encapsulates large language models (LLMs) and other multimodal foundation models, which can be used to construct different intelligent stream-functions. The get_fm API is used to obtain a foundation model based on the specified input/output types (e.g. text -> text, image+text -> text, etc). The build_prompt API can assemble a list of data elements with different types into a model prompt, and the Model. query API sends a prompt to the model and returns the model output.\nCompared to other streaming frameworks such as Flink, LangChain [5] and PrivacyStream [27] which often have more than dozens of APIs, our framework aims to reduce the number of APIs to make it easier for an LLM to understand and use. However, simplicity does not mean the API is limited. In fact, we can flexibly construct more complex operations (e.g. stream joins, aggregations, etc.) and build sensing-program to fulfill diverse sensing tasks. We will not provide the detailed definitions and descriptions of each API due to page limits. The following code snippet illustrates some basic usages of our APIs to achieve more complex tasks."}, {"title": "3.2. Graph-based Runtime Management", "content": "With our stream-based API, each sensing-program in ChainStream can be viewed as a graph, where the nodes are functions and edges are streams flowing between the functions. The responsibility of the runtime is to manage the programs with a global stream flow graph (SFG for short). The streams and stream-functions use an event-driven model - stream-functions are registered in advance to target streams, and are triggered to pro-cess new data only when such data appears in the target streams. For managing concurrency in the global com-putation graph, the system allocates an independent thread for each stream. When a thread has no stream-function registered, it remains idle. Otherwise, it oper-ates normally using the event-driven model. Different stream-function can be flexibly run in parallel (and their model requests can be batched), if they don't have data dependencies in the SFG. Additionally, each stream is allocated a message queue to facilitate inter-thread com-munication.\nTo manage the SFG, the ChainStream runtime in-cludes various system services, which are responsible for managing, maintaining, and scheduling different re-sources (processor, memory, model, etc.). Additionally, we have various loggers to track basic system informa-tion such as stream traffic, program overhead, model usage, and more. Ultimately, this logging information is aggregated into a unified dashboard, supporting analy-sis, debugging, and adjustments as needed.\nThe runtime offers an additional benefit of more flexi-ble information sharing and program reuse. In many mo-bile/edge scenarios, there are often many repetitive sens-ing processes. The global stream computation graph and atomic function approach in ChainStream inherently support flexible stream reuse. During sensing-program development, many basic sensing tasks (such as recog-nizing the current location and behavior of system users) can be accomplished by directly calling existing streams produced by other sensing-program, eliminating the need for redundant coding."}, {"title": "4. Iterative Program Generation", "content": "Based on the new programming framework, we are closer to achieving our ultimate goal - using natural language to process personal data and sense the con-text. In this section, we introduce the design of Chain-Stream Generator, which automatically generates pro-grams based on the ChainStream programming frame-work using LLMs. The generator acts as a programming assistant that continuously interacts with our frame-work and receives feedback. The generated programs are gradually optimized during the interactions.\nThe workflow of ChainStream Generator is shown in Figure 3. The whole process involves three key compo-nents, the Query Optimizer, the LLM-based Code Gen-erator, and the Sandbox-based Debugger.\nWhen ChainStream Generator receives a natural lan-guage request for context sensing, e.g. \u201cthe microphone loudness value in every 2 minutes\", ChainStream first passes the query into the Query Optimizer, which will extend the query (by adding the framework document, formatting guidelines, etc.) and return a much more informative one. The optimized query is then passed to a code-generation LLM (e.g. GPT-4) to generate the corresponding sensing program. Thanks to the unified and concise design of ChainStream programming frame-work, complex sensing tasks can be achieved with just a few lines of code. After a program is generated, it is executed in a sandbox environment. We implement the sandbox environment so that different ChainStream pro-grams can be interpreted with simulated data. During the simulated program execution, the sandbox records the sensing-program's behavior and generates a report, containing different kinds of program execution feed-back. This feedback can further be used to optimize the\""}, {"title": "4.1. Sandbox-based Program Debugger", "content": "The purpose of the debugger is to provide early feed-back to the program generator to refine the generation process. However, debugging stream-processing pro-grams is challenging [4]. The difficulty arises primarily because the stream programs are executed in a lazy, concurrent manner (functions are executed on demand rather than explicitly being called in the control flow). The difficulty is similar in ChainStream.Once started, the various stream-functions within a sensing-program are disaggregated, attached to many system compo-nents and distributed across different parts of the sys-tem. This distribution causes the error paths to differ from the original locations in the static sensing-program code. Additionally, the system behavior becomes more complex due to this distribution.\nTo facilitate better stream-based program debugging, we design the ChainStream Sandbox. The sandbox provides a primary operating environment similar to ChainStream Runtime, while with simpler control flow transformations, reduced parallelism, and more detailed checking messages. It allows to set up a simulated envi-ronment by passing in a Env structure, which defines the available streams, data, and recorded content. A sandbox simulation can be started by specifying a given sensing-program to be analyzed and the environment"}, {"title": "4.2. Feedback-guided Query Refinement", "content": "The Query Optimizer module is responsible for making the original context-sensing query more informative so that LLMs can generate better sensing-programs. It involves two processes: initial refinement and feedback-based incremental refinement. As shown in Figure 4, the initial refinement produces the Base Prompt, and the incremental refinement appends the Feedback History part to the query.\nThe initial refinement adds necessary background information to the original sensing query, as well as prompts to guide the thinking and output formatting [47, 50]. The base prompt first introduce the usage of ChainStream, including how to use various modules and the inputs and outputs of each module. Due to context length limitations, we can only use concise language to summarize the main APIs of the system. Fortunately, thanks to the minimalist API design of ChainStream, we can comprehensively cover all core usages within a short context length. Achieving similar conciseness in other frameworks is challenging. For instance, popular LLM agent frameworks (e.g. LangChain, MetaGPT, etc.) typi-cally contain hundreds of core APIs, which are relatively more complex for in-context learning with most LLMs. To help LLMs understand how to use the APIs to ac-complish different sensing goals, we follow the standard"}, {"title": "5. Benchmark", "content": "Since natural language-based context sensing is a new problem, there is no existing benchmark for evalu-ating such systems. Therefore, we create a bench-mark (named NL-Sense) that includes a set of manually crafted tasks for context sensing and data processing, as well as the oracle programs (based on ChainStream API) to solve this task."}, {"title": "5.1. Tasks and Data", "content": "Tasks. We collect and summarize the context sensing and data processing demands of various common sce-narios in daily life, and then construct the Task set in our benchmark based on these demands. Each task mainly consists of four parts: target stream description, avail-able stream description, available stream items and oracle code. The available streams mimic most of the existing source sensor streams mentioned in Table 1, while the target streams represent the functionalities that the generators under evaluation need to achieve.\nOur principles for designing tasks include: 1. Closely aligning with useful context sensing and data process-ing demands in real life. 2. Covering various scenarios, modalities, difficulty levels, and processing methods as much as possible. 3. Limiting the ambiguity of the tasks, including the randomness of the results and the diversity of the processing methods. Although randomness and diversity are main characteristics of real-world percep-tion and data processing programs, we have to sacrifice some of the flexibility to achieve relative stability in the results. Following these principles, we crafted 133 tasks in the benchmark. The statistical distribution of the tasks is shown in Figure 5. Among them, 87 tasks are relatively simple, focusing primarily on basic stream data processing such as filtering and aggregation, with"}, {"title": "5.2. Evaluation Method", "content": "Oracle Programs. Our benchmark also includes an or-acle program for each task to evaluate the generated program's logical correctness and performance in a com-parative manner. Because context sensing tasks are inherently complex and numerous, it is challenging to provide manually labeled outputs. Therefore, the task execution results are examined by comparing with the behavior and output of the oracle program under the same sensing objective. Specifically, we use the outputs of these standard programs produced by running them in the sandbox as the reference outputs. We also provide a batch testing interface, which allows evaluating the programs in a parallel, high-throughput manner.\nComparison Metrics. The quality of the generated sensing program is quantified with several metrics to compare the actual outputs and expected reference out-puts. If a program can produce results that are more similar to the reference outputs, it gets a higher score and can be considered more useful.\nWe originally attempted to use a code similarity ap-proach, i.e. comparing the key components in the gener-ated program with those in oracle programs. However, this approach cannot handle the diverse ways of im-plementing a task, especially when considering other development frameworks. Therefore, we arrived at two metrics that are independent of the program structure, Executable Rate and Result Score.\nExecutable Rate measures how likely the programs generated by a generator are to be executable. It is a direct metric that can be obtained by simply running the program in the simulated environment.\nResult score measures the similarity between the out-put data of the generated programs and the oracle pro-"}, {"title": "6. Evaluation", "content": "To evaluate the system's ability to solve context-aware tasks in an end-to-end manner, we employ 133 tasks from the proposed benchmark and two metrics: Executable Rate and Result Score, as described in Section 5. All field weights $w_k$ in Equation 1 are set to 1, meaning all fields are considered equally important. Moreover, the field similarity function $f$ is chosen as BLEU to compare field similarity. The weight $w_i$ in Equation 3 is equal to the length of the sequence, namely the result is weighted according to the length of the stream."}, {"title": "6.1. Experimental Setup", "content": "Tasks and Metrics. To evaluate the system's ability to solve context-aware tasks in an end-to-end manner, we employ 133 tasks from the proposed benchmark and two metrics: Executable Rate and Result Score, as described in Section 5. All field weights $w_k$ in Equation 1 are set to 1, meaning all fields are considered equally important. Moreover, the field similarity function $f$ is chosen as BLEU to compare field similarity. The weight $w_i$ in Equation 3 is equal to the length of the sequence, namely the result is weighted according to the length of the stream.\nBaselines. To the best of our knowledge, there is no existing end-to-end system that can be directly com-pared with. Therefore, we attempt to construct the baselines by adapting existing techniques. The first base-line category is code-free approaches (Represented as GPT4 and GPT40 in Table 4), where the foundation model processes data without generating code. We use state-of-the-art OpenAI GPT-4 and GPT-40 models to establish these baselines. The second category, code-based approaches, involves generating a program to handle the sensing task. We included two baselines in this category. One is to generate native Python code, the other uses a popular LLM agent development frame-work LangChain [5]. We include basic document of the frameworks in the program generation query like ours, although their complex APIs cannot be included in detail. The LLM we use with the frameworks is GPT-40 (same as our approach), which contains pre-trained knowledge of Python and LangChain. It is important to note that directly generating the programs based on existing frameworks is nearly impossible to achieve the complex sensing goals. Thus, we integrate the same stream-based abstraction of ChainStream to these baselines, allowing the code to access our stream data with get() and put() functions."}, {"title": "6.2. Generation Quality", "content": "We first evaluate the context-aware program generation performance of ChainStream and various baselines. For each method, we assess two scenarios: without a ref-erence example and with one reference example. Our method offers two approaches: the complete method (as shown in Chapter 4) and a one-time generation ap-proach without the feedback process (Ours w/o feed-back). We also record the best result each method could"}, {"title": "6.3. Latency and Cost Analysis", "content": "We analyze the average time and token overhead re-quired to generate code across all tasks, with the results shown in Table 5. The Python, LangChain, and Ours w/o feedback methods are all single-generation meth-ods, with their latency primarily dependent on the wait time for querying the cloud LLM. Since the prompt in-cludes the framework manual, the token overhead is relatively higher.\nOurs w/ feedback, on average, requires 3.6 iterations to complete the generation task, leading to greater la-tency and token overhead. The number of iterations varies depending on the difficulty of the task. The main factors contributing to the latency are the waiting time for the cloud LLM and the time required for the sandbox"}, {"title": "6.4. Fine-grained Performance Analysis", "content": "We conduct ablation experiments to investigate the role of each component in the generation process. We pri-marily explore three questions: 1. What influence does feedback have? 2. What influence does the reference example have? 3. What influence do different types of information in feedback have?"}, {"title": "6.4.1. The influence of the feedback mechanism", "content": "Figure 7 illustrates the influence of feedback on result scores and success rates in both scenarios with and without reference examples. It is evident that the dark-colored methods with feedback consistently outperform the methods without feedback in almost all aspects. Additionally, in the case of zero reference examples, feedback proves to be even more crucial, significantly improving the result score of the generated code and maintaining a higher success rate for the code."}, {"title": "6.4.2. The influence of in-context examples.", "content": "Figure 8 shows the effect of different numbers of exam-ples on result scores and success rates in both feedback and no-feedback scenarios. As the color of the bars deep-"}, {"title": "6.4.3. The influence of different components in the feedback", "content": "In the feedback process, besides the reference example, there are three main components: target stream output, error message, and logs. We conduct experiments by omitting each component separately, and the results are shown in Table 6. The absence of any part leads to a performance drop.\nIn our analysis, we observe that each component plays a distinct role: the error message indicates hard errors in the code, the output provides the result of the code, and the logs allow the LLM to debug by printing key information.\nWhen the output is missing, it causes the most damage to the result because the output is the final outcome, which not only increases the likelihood of the program not executable but also decreases the result score of the executable code. The absence of error messages also influences the executable rate, but it has less effect on the result scores of executable code. When logs are omitted, there is a higher executable rate, but the result score decreases. This is because the generation process"}, {"title": "7. Conclusion", "content": "We have presented an end-to-end system named Chain-Stream for automatically generating context-sensing programs based on natural language. The framework features an easy-to-use stream-based programming in-terface and a feedback-guided query refinement method to make LLM-based generation easier and more precise."}]}