{"title": "Elastic-DETR: Making Image Resolution Learnable with Content-Specific Network Prediction", "authors": ["Daeun Seol", "Hoeseok Yang", "Sihyeong Park", "Hyungshin Kim"], "abstract": "Multi-scale image resolution is a de facto standard approach in modern object detectors, such as DETR. This technique allows for the acquisition of various scale information from multiple image resolutions. However, manual hyperparameter selection of the resolution can restrict its flexibility, which is informed by prior knowledge, necessitating human intervention. This work introduces a novel strategy for learnable resolution, called Elastic-DETR, enabling elastic utilization of multiple image resolutions. Our network provides an adaptive scale factor based on the content of the image with a compact scale prediction module (< 2 GFLOPs). The key aspect of our method lies in how to determine the resolution without prior knowledge. We present two loss functions derived from identified key components for resolution optimization: scale loss, which increases adaptiveness according to the image, and distribution loss, which determines the overall degree of scaling based on network performance. By leveraging the resolution's flexibility, we can demonstrate various models that exhibit varying trade-offs between accuracy and computational complexity. We empirically show that our scheme can unleash the potential of a wide spectrum of image resolutions without constraining flexibility. Our models on MS COCO establish a maximum accuracy gain of 3.5%p or 26% decrease in computation than MS-trained DN-DETR.", "sections": [{"title": "1. Introduction", "content": "Object detection [49] is one of the fundamental research areas in computer vision that identifies the location of objects while determining their category. The success of transformers in Natural Language Processing (NLP) [34] led to the spread of transformer-based networks across diverse visual applications, including object detection. In this field, DETR (DEtection TRansformer) [3] introduced the first transformer-based detector, presenting outstanding performance with a simple architectural design.\nUnlike CNN-based detectors [26, 27], DETR employs a versatile architecture that incorporates learnable queries and bipartite matching. This mechanism facilitates the removal of static box assignment methods, such as non-maximum suppression, eliminating the need for manual anchor selection. Since anchor sizes act as key reference points for prediction, these sizes must be carefully selected, often relying on prior knowledge. Replacing this static procedure with a learnable method enables the network to possess a more adaptable training space while minimizing human involvement. This success raises a critical question: Is it possible to eliminate the necessity of prior knowledge on essential hyperparameters via a learnable strategy?\nIn conventional network scaling [31, 32], image resolution, depth, and width are regarded as crucial hyperparameters that determine network performance. In object detection, the resolution is primarily associated with prior knowledge due to its relationship with the distribution of object scales [24, 29]. A multi-scale (MS) approach [22], which utilizes multiple image resolutions, has become a de facto standard approach in modern object detectors. This technique determines the image resolution by randomly selecting it from a set of predefined hyperparameters, allowing the acquisition of variable scale information. However, the reliance on predefined parameters can constrain the adaptability of the resolution, given that these values are selected manually. This manual process often requires a deep understanding of the data distribution or extensive trial and error, resulting in a significant burden in practical implementations. If the resolution is optimized in a learnable manner, the network can dynamically adapt to various data distributions, enabling the network to be elastic and efficient.\nTo explore this potential, we initially focus on examining how the resolution impacts network performance for establishing an optimization goal for learnable resolution. displays the network's response to resolution changes across various hyperparameter configurations. We can observe the accuracy improvement across resolution increases, which produces extremely low gain after the resolution of 800. The randomized strategy cannot efficiently handle the wider range of hyperparameters, which presents more possibilities for enhancing performance. Moreover, when we apply the stochastic method during testing, performance degrades by 1-2% compared to MS training. This implies that the adaptiveness does not effectively transfer to testing, indicating the limitations of randomness. More analysis for this experiment is discussed in Sec. A.1.\nBased on these observations, our objectives can be defined as follows: 1) learnability, 2) elimination of dependency on prior knowledge, 3) capability of handling a wide spectrum, and 4) applicability during testing. To achieve these objectives, we propose a novel approach termed Elastic-DETR, which optimizes image resolution in a learnable fashion.  our network produces an image-level scale factor employed for resolution scaling. We utilize a compact network called a scale predictor to generate the scale factor ranging from specified minimum and maximum values. This scale factor is obtained in a content-specific manner according to the information of the image, capable of providing adaptiveness. This compact network is jointly trained with the detector, facilitating end-to-end training and testing mechanisms.\nThe primary challenge of our approach is determining the image resolution without prior information. Initially, we identify essential components for resolution determination from human behavior: To observe objects that are difficult to see, we move our position based on the size of the objects and our visual acuity. We propose loss functions for scale factor optimization derived from these elements: scale loss for size-based optimization and distribution loss for optimization based on detection ability. Scale loss enables improving the adaptiveness of the scale factor by adjusting it from the size of the objects. In this process, this optimization is determined based on the relative size between two size boundaries, corresponding to sizes that yield either the maximum or minimum value. Distribution loss optimizes these boundaries from a probability distribution, which describes the scale-specific detection ability of the network."}, {"title": "2. Related Work", "content": "As mentioned earlier, depth, width, and image resolution are considered key components in the classical scaling law [31, 32]. Typically, these are optimized through parameter searching [6, 7, 31] or manual scaling design [32, 42]. The other approach, dynamic neural network [12], presents dynamic modulation, capable of optimizing parameters on the fly. Layer-wise early exit [38] or cascading multiple networks [20, 25] allows handling adaptable depth. In CNN-based networks, channel-level skipping [13, 16] enables dynamic width adjustment by executing only crucial channels.\nFor the resolution optimization, branch-wise dynamic selection [41, 47] is proposed in image classification to handle various resolutions. These optimization schemes mainly utilize architectural modulation, which predicts a probability of execution of the defined architectural components. Instead, our strategy predicts such a defined component with only specifying a range."}, {"title": "2.2. Transformers for Object Detection", "content": "Conventional object detectors [1, 19, 26, 27, 33, 35] comprise fully convolutional layers and often incorporate multi-scale architecture [18, 46]. DETR [3] introduced a transformer-based detector that presents an outstanding performance advancement by utilizing a single-scaled encoder-decoder architecture. Despite its remarkable progress, there are remaining constraints, such as limited capacity for small objects or slow convergence speed during training. Various methods have been suggested to tackle these challenges, incorporating multi-scale features [2, 4, 43, 45, 48] or optimizing object queries [15, 23, 40, 44]. Dynamic network was proposed to alleviate such limitations via dynamic modulation [8] or dynamic query design [14, 21]. Our strategy also employs a dynamic strategy for image resolution, which leads to alleviating the problem of small objects. Additionally, our strategy addresses an another issue, which is reliance on prior knowledge of hyperparameters."}, {"title": "3. Methodology", "content": ""}, {"title": "3.1. Overview", "content": "Overall Procedure. Elastic-DETR utilizes an existing DETR-based detector while introducing a scale predictor that determines an image-specific scale factor. As depicted in Fig. 3, the scale predictor S is attached as a modular component before the detection network to facilitate adaptive resolution scaling. The network receives input as the image I and generates a scale factor \u03c6, which can be represented as \u03c6 = S(I). The image resolution is adjusted through a scaling operation Scale(\u00b7,\u00b7), which resizes the width and height of the image to \u03c6\u00b7 Iw and \u03c6\u00b7\u0399h, respectively. Then, the scaled image is fed into the detector D to predict box locations and classes from the input image. The whole process can be expressed as,\nY = D(Scale(I, S(I))).\nIn the absence of the predictor, the detector would directly predict the output as Y = D(I).\nTraining Objective. Note that the scale predictor S only optimizes the input image, which is jointly trained with the detection network D. Existing loss functions of the detector D, i.e., the classification loss Lcls and the localization loss Lloc, indirectly assist in obtaining a scale factor that maximizes the performance. However, these losses cannot provide adaptiveness across the scale factor due to the absence of modulation for resolution determination. This is because, unlike prior approaches that relied on branch-wise selection [37, 39, 41], we aim to optimize the scale factor independently of any prior architectural knowledge. In this process, as stated in Sec. 1, the scale factor is optimized via two newly defined loss functions: the scale loss, which enhances image-specific adaptiveness, and the distribution loss, which refines the overall bias of scale factors. These functions allow the scale factor to be trained for maximizing network performance, adapting it image-specifically informed by detection ability."}, {"title": "3.2. Architecture of Scale Predictor", "content": "For scale factor prediction, we construct the architecture of predictor S with two primary components: a backbone network for analyzing the visual property of images and head layers for predicting the scale factor. ResNet-18 [36], a well-known lightweight classification network, is employed as the backbone. To process the head layers, the features extracted from the backbone are vectorized into a one-dimensional vector. A compact transformer encoder, followed by a fully connected layer, then predicts a single scale factor for the given image. This encoder block is applied to increase the adaptiveness of scale factors, which consists of three layers that incorporate single-head attention.\nAfter the prediction, raw scale factors for each image,"}, {"title": "3.3. Loss Functions for Scale Factor Optimization", "content": ""}, {"title": "3.3.1. Scale Loss", "content": "As stated earlier, we define scale loss to optimize the scale factor based on the object's size, employing a high scale factor for small objects and a low scale factor for large objects. We intend to optimize this factor from a probability perspective by introducing an up-scaling probability denoted as Pup. This probability indicates the degree of up-scaling for the objects, which shares the same inverse relationship to object sizes as the scale factor.\nThis relationship allows optimization of the scale factor derived from this probability, accomplished by modifying the scale factor. We establish this modification as, normalizing by the maximum of Tmax and mapping the minimum of Tmin/Tmax to Pup = 0\u00b9, which can be expressed as Pup = map(\u03c6/Tmax) where map : [Tmin/Tmax, 1] \u2192 [0, 1]. Then, the scale factor optimization problem can be interpreted as single probability optimization.\nThe typical problem of handling single probability is binary classification, which adjusts the probability to 0 or 1. This optimization is achieved using Binary Cross Entropy (BCE) loss [11], which is formulated as,\nL_{BCE}(y, \\hat{y}) = \\frac{1}{N} \\sum_{i=1}^{N} (y \\log(\\hat{y}) + (1 - y) \\log(1 - \\hat{y})),\nwhere y and \\hat{y} denote target and predicted probability, respectively. This loss function modifies the probability \\hat{y} to attain a high value of 1 for the positive label (y = 1) while making a low probability of 0 for the negative label (y = 0). The primary distinction between our and the classification problem lies in the range of probability, which exhibits a continuous spectrum from 0 to 1 for Pup. This is because up-scaling and down-scaling probabilities mutually appear, except in cases where the object is excessively small or large.\nConsequently, we introduce a continuous-valued Yup to represent the target probability of Pup to utilize the basic form of BCE loss. To ascertain Yup, we establish learnable boundaries B where points yield maximum (=1) or minimum (=0) probability, identifying the overall degree of optimization. The target probability for the given object size is then determined as the relative ratio between these two variables. This computation of Yup is performed via a modified sigmoid function \u03c3B(\u00b7), providing the relative probability based on the boundaries B. This function produces a value reflected along the x-axis as,\n\\sigma_B(x) = \\begin{cases} 0.0 & x > B_u \\\\ \\sigma(-x) & B_l \\leq x \\leq B_u, \\\\ 1.0 & x < B_l \\end{cases}\nwhere Bl and Bu denote lower and upper boundaries, respectively. Subsequently, the value of Yup is determined as,\nY_{up} = \\sigma_B(b_{area}) = \\sigma_B(b_w \\cdot b_h),\nwhere bw, bh represent the width and height of the ground truth object b. The optimization of these boundaries is achieved through an additional loss function, distribution loss, which will be explained in the next section.\nGiven the input and target probability, the scale factor can be optimized at the object level as,\nL_{scale}^{obj}(b, \\phi) = - (Y_{up} \\log(\\hat{Y}_{up}) + (1 - Y_{up}) \\log(1 - \\hat{Y}_{up})),\nwhere \\hat{Y}_{up} = \\phi/T_{max}. The shape of this function is depicted in Fig. 4, illustrating a continuous and inversely pro-"}, {"title": "3.3.2. Distribution Loss", "content": "Note that the scale loss is used to optimize the scale factor with respect to the given boundaries B, where the sizes yield maximum or minimum probability. To enable adjustments of these boundaries based on detection performance, an additional loss called the distribution loss is introduced. As these boundaries are sizes, we aim to determine these parameters from the scale-specific ability of the network, such as finding sizes where the trend of performance changes. The objective of distribution loss is to train the per-scale tendency that describes the overall characteristic of the network and define the boundaries from the learned inclination. This optimization leads to the alignment of the overall degree of scale factor with network performance since the boundaries control the bias of the scale factor.\nIn this context, a learnable probability distribution is employed to represent the detection tendency. We utilize beta distribution Beta(\u03b1, \u03b2), which can express diverse shapes by modifying parameters \u03b1 and \u03b2. To define the target distribution, a loss value derived from the object, such as localization loss, is utilized. We incorporate an additional formulation of the loss to interpret the performance, which is a likelihood derived from the plain loss as e-loss. This value indicates a detectable likelihood of the object, demonstrating a normalized and widespread value as shown in Fig. 5. The target distribution is simply obtained by dividing the sum of input values.\nFor beta distribution close to the target, we utilize Wasserstein distance [10], which minimizes the distance between input and target probability distribution. Our distribution loss is defined as,\nL_{dist}(b) = Wasserstein(f(b_{area}), T_{loc}).\nwhere f(\u00b7) ~ Beta(\u03b1, \u03b2) and Tloc denotes the target distribution. The boundaries B are delineated by the mean \u00b5 and standard deviation \u03c3as \u03bc\u00b1 \u03c3.\nStabilization of Convergence. In the initial training stages, a convergence of distribution can be unstable since the network usually produces a noisy output during early iterations. We endeavor to stabilize this convergence by employing a low-pass filter (LPF), defined as LPF(X, x', x) = \u03bb\u00b7 x' + (1 \u2212 \u03bb)\u00b7x with a tunable parameter \u03bb. We intend to correlate the convergence with the degree of association between object scale and loss, as distribution loss utilizes the relationship between these two components. This can be achieved by coupling the parameter \u03bb with the correlation between the two components as,\nT_{smooth} = LPF(\\lambda \\cdot ||\\xi (b_{area}, T_{loc}) ||_1, T_{loc}, f(b_{area})),\nwhere \u03be(\u00b7,\u00b7) represents xi correlation coefficient [5], which can measure non-linear correlation. This normalized distribution can be used as the target distribution in Eq. (7)."}, {"title": "4. Experiments", "content": ""}, {"title": "4.1. Implementation Details", "content": "Architecture. Our Elastic-DETR is based on DN-DETR [15], which has a plain architecture as DETR while achieving faster convergence. Two lightweight backbones, ResNet-50 and Swin-Tiny, are utilized as backbone networks due to our GPU memory constraints. We set Tmin = 0.2 as fixed and adjust Tmax from 1.25 to 2.25, leading to control of the trade-off between accuracy and computation complexity. With a 0.25 increase in Tmax, the number of candidate resolutions increases by nearly 18. The predicted resolution is rounded for a shorter size with a multiple of 8 to fit the memory size as a common criterion.\nDataset & Training. Our model uses a base resolution of 600 with a maximum of 1000 due to the augmentation that preserves the spatial ratio of images. MS training employs 480 to 800 resolutions with increments of 32 and a maximum size of 1333. The models are trained with detrex framework [28] with 16-image batches."}, {"title": "4.2. Main Results", "content": "Performance Comparison. We train models with various spectrums of resolution to show the flexibility of image-wise optimization. The models demonstrate a consistent performance enhancement with increasing resolution, as illustrated in Fig. 2. Our network with the highest accuracy"}, {"title": "5. Conclusion", "content": "This paper presents a new method for learnable image resolution, enabling elastic utilization of the image. The optimization of the scale factor is achieved via scale loss and distribution loss, capable of adapting it for object size and detection ability. Our Elastic-DETR can demonstrate up to 3.5% of performance gain or 26% of reduced computation"}, {"title": "Appendix", "content": ""}, {"title": "A. Discussions", "content": ""}, {"title": "A.1. Multi-scale Image Resolution", "content": "The preliminary experiment depicted in Fig. 1 was performed to assess the flexibility of the MS technique. The performance improvement across resolution increase is related to the network's convergence speed, resulting in slower speeds for a greater number of resolutions. In this figure, DETR demonstrates low enhancement for resolution increase due to its extremely slow convergence speed. This enhancement in performance can be increased for networks that exhibit fast convergence speeds. Despite this possibility, the randomized fashion still suffers from obvious limitations, such as prior knowledge reliance or applicability during testing. This is a reason why the optimization of the image resolution is required, especially for a learnable strategy. Note that the preliminary experiment is for establishing the optimization goal of our method and our approach does not aim to improve performance but rather focuses on showing the possibility of eliminating prior knowledge."}, {"title": "A.2. Eyesight: Relation to Humans", "content": "We employ the idea of human behavior to optimize scale factors without prior knowledge, which is discussed in Sec. 1. Our image-wise optimization can be described as the role of glasses that assist individuals with impaired eyesight. Scale loss acts as a dynamic lens by establishing a one-to-one correspondence between the object size and degree of scaling. To acquire the glasses, we must measure our vision using words or symbols of varying sizes and determine whether we are nearsighted or farsighted. This measurement is identical to the process of distribution loss, which analyzes the tendency of per-scale detection performance. Similar to selecting types of lenses for refractive errors, we define the base location of bias in the scale factor by specifying the formulation of loss functions."}, {"title": "A.3. Relation to Human Behavior", "content": "In scale loss, the scale factor is optimized via up-scaling probability, which quantifies the degree of up-scaling. The image can be viewed as the information received by human vision, and alteration in the observation position leads to a change in the scale of its contents. We can associate the movement of human position and the concept of up-scaling probability with assuming that the minimum and maximum positions can be derived from the threshold of the scale factor\u00b3. Given the mimi-/maxi-num position, this association can be visualized as Fig. i, which allows an intuitive understanding of our methodology. Since the up-scaling proba-"}, {"title": "A.4. Goal of Image Resolution Optimization", "content": "The flexibility of image resolution allows for the optimization of this component to achieve specific objectives, such as reducing inference latency by executing up-scaling only for crucial images, enhancing network performance by increasing resolution, enhancing resolution-wise adaptiveness by utilizing multi-scale resolutions, or modifying it for different training or inference environments, e.g., various memory constraints for different target devices. The proposed method enables handling various optimization objectives with different resolution configurations. Our approach provides elastic image resolution with image-specific prediction, capable of controlling the trade-off between performance and computational complexity. This optimization also enables the utilization of multiple resolutions and optimization to the target environment by modifying configurations."}, {"title": "A.5. Limitations", "content": "Performance. Despite our methods enhancing accuracy by up to 3.5%, this result remains lower than the current state-of-the-art performance. Due to our GPU memory constraints, we are unable to experiment with backbones with high computational complexity, such as ViT-B or Swin-L. A more accurate backbone network can provide results close to state-of-the-art, necessitating additional experimentation.\nArchitectural Optimization. With joint training, the detector architecture can be trained to adapt elastic image resolution. Architectural optimization is necessary to achieve maximum effectiveness, i.e. an elastic number of queries, since the existing architecture is designed based on MS strategy. This problem was not addressed in our study, as it constitutes a separate research topic. This remains as future work.\nUsage of Annotation Information. We formulate loss functions for resolution optimization based on object dimensions resulting in application only when ground truth sizes are provided. If we can identify the salient region"}, {"title": "B. More Implementation Details", "content": "Models are trained on a server with an Nvidia A6000 using an 8-GPU configuration for Elastic-DETR-L-Swin-T, whereas the other models were trained on a 4-GPU environment. We use the AdamW optimizer with a learning rate of 1e-4 and random cropping and flipping augmentation as the baseline network. DOTAv2.0 [9], which will be explained in the next section, is cropped with 512\u00d7512 with a 256 sliding window. In this dataset, networks are trained with 20 epochs with the same augmentation as the COCO while utilizing a batch size of 2 and a learning rate of 2e-5."}, {"title": "B.1. Weighted Sum of Loss Functions", "content": "Since our resolution optimization plays a supporting role for the detector, we couple the convergence of the scale predictor with the detection network as,\n\\lambda_{\\{scale, dist\\}} = (\\sum_i \\ell^{'} + \\gamma \\sum_i \\ell'_i )\\lambda_i, \nwhere i corresponds to each base weight of scale and distribution loss and \\ell^{'} denotes a scalar value of loss function, which does not execute gradient propagation. This form helps to prevent harming the convergence of the detection network in the late training iterations caused by the high loss value. The total loss function is expressed as,\nL_{total} = \\lambda_{cls}L_{cls} + \\lambda_{loc}L_{loc} + \\lambda_{scale}L_{scale} + \\lambda_{dist} L_{dist},\nwith scalar-valued \\lambda_{cls} and \\lambda_{loc}."}, {"title": "B.2. Tricks for Reducing Computation", "content": "To minimize the computation of the scale predictor, we use a few tricks in designing network architecture, such as 1) down-sampling the input image by 0.42x, 2) early exit of the layer in ResNet-18, and 3) reducing feature size in vectorization with the FC layer. We select res3 block as an input feature of the head layers and then transform this feature into a 16-dimensional scalar vector."}, {"title": "C. More Studies and Ablations", "content": ""}, {"title": "C.1. Experiment with DOTA", "content": "To analyze the generalization ability for different datasets, we train our models with a DOTA dataset [9], which has more scale variation than COCO. As illustrated in Tab. i, the MS-trained network shows much lower accuracy than COCO with an AP of 23.8%. Hence, our method is capable of showing more effectiveness than COCO with 0.9% and 2.3% AP enhancement for medium and large models."}, {"title": "C.2. Effect of Network Size of Scale Predictor", "content": "Initial Image Resolution Size. Since our network predicts the scale factor from the initial image, the size of the resolution can affect the prediction quality of scale factors, as well as the quality of the scaled image. As shown in Tab. ii, the smaller size leads to lower accuracy, while the higher resolution results in higher performance. We select a size of 600 because of the small difference in the accuracy of 0.1% with the resolution of 800. Nevertheless, this size cannot be sufficient for Elastic-DETR-H, which utilizes Tmax = 2.25, as similar to the performance gain between 400(2.25) and 600(1.50). We refrained from utilizing the greater initial resolution for larger Tmax to reduce computational overhead.\nDimension of Vectorization. The effect of the dimension of vectorization is illustrated in Tab. iii. This shows the minor impact of the dimension, resulting in comparable performance across various sizes."}, {"title": "C.3. More utilization of MS strategy", "content": "Using the same strategy as Fig. ii, we test how our model can respond to unseen resolutions different from the training set, capable of measuring only the effect of the input scale optimization. As shown in Tab. iv, our models with a ResNet can achieve a maximum gain of 1.2%, surpassing the score of Fig. ii by 0.3%. Otherwise, the models utilizing a Swin backbone achieve the highest gain of 0.8%, which is the same maximum gain as Tab. 4. This indicates that the scale predictor trained with the ResNet demonstrates more adaptability for resolution prediction for the MS-trained model. Also, this implies that the scale predictor with the ResNet is less specifically optimized for the detection network during joint training, while the model with Swin shows more association with the detection network4. For models utilizing ResNet, the base model exhibits the most robust performance across varying resolutions, while this trend can be observed in the small model with Swin backbones. This result means the baseline network's robustness to input resolution, shows lesser robustness for high-"}]}