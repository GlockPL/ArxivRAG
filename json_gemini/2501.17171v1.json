{"title": "Separated Inter/Intra-Modal Fusion Prompts for\nCompositional Zero-Shot Learning", "authors": ["Sua Jung"], "abstract": "Compositional Zero-Shot Learning (CZSL) aims to recognize subtle differences in meaning or the\ncombination of states and objects through the use of known and unknown concepts during training. Existing\nmethods either focused on prompt configuration or on using prompts to tune the pre-trained Vision-\nLanguage model. However, these methods faced challenges in accurately identifying subtle differences in\nmeaning or combining states with objects. To jointly eradicate the above issues and construct an efficient\nand effective CZSL technique, we suggest a method to improve attribute recognition performance by\nutilizing diverse Prompt Learning with an Inter/Intra-Modality Fusion Synthesizer in scene understanding\ninvolving subtle semantic differences and multiple objects.", "sections": [{"title": "1. INTRODUCTION", "content": "When encountering a new thing, such as a blue cat, people often attempt to name it despite the\nchallenge of linking \u201cblue\u201d and \u201ccat\u201d together. Compositional Zero-Shot Learning (CZSL) aims\nto recognize and distinguish new concepts. In addition, a detailed textual analysis is needed to\nunderstand the semantic differences. Then, it leverages visual information for matching. Previous\nresearches [19], [10], [14], and [26] have focused on connecting attributes with object information\nto improve prediction accuracy for the final pair. After emerging CLIP [2], researchers use a\ncompatible encoder and prompt, leading to better performance. [16], [22], [9] use soft prompts\nfor pair labels, compared to hard prompts.\n\nA recent paper, DFSP [7], applies a pairing prompt that combined a soft and hard prompt\ncomposed of learnable tokens and ``a photo of [state] [object].\" However, it struggles to capture\nsubtle language differences using fixed prompts for pairs. Therefore, we develop a method for a\nbetter understanding of the connection and semantic differences of vision and language using hard\nand soft prompts. We conjecture those techniques are too simple to capture all the complexities\nin visuals and text, and aimed to develop a better method to understand the connection between\nvision and language for CZSL. We propose a learning method that applies hard and soft prompts\nto understand semantic differences.\n\nInstead of using just pair prompts, we suggest separating them into attribute, object, and pair\nprompts. In addition, we compared all possible prompt forms, i.e., hard, hard & soft, and soft\nprompt in Table III. The features are updated through the suggested Modal Fusion Synthesizer\nBlock (MFSB) to better understand complex intrinsic relationships. Decomposed text and image\nfeatures based on separated prompts can be helpful rich information to understand complex\nscenes. We introduce Inter & Intra-modal fusion and Separated Prompts to improve CZSL with\nprompt learning techniques."}, {"title": "2. RELATED WORKS", "content": "2.1. Compositional Zero-Shot Learning\nCompositional Zero-shot Learning (CZSL) improves traditional zero-shot learning by\nidentifying new unseen classes during training. The model can learn new classes by generalizing\nto unseen attribute combinations with algorithm [1], ALE. CZSL is necessary because new classes\nkeep appearing, making it impractical to retrain models for each one. OADis [20] and others [29],\n[14], [21] suggested decomposing the encoder and decoder for each state, object, and pair with\nseen and unseen feature. DFSP [8] others [12], [9] use fusion prompt with visual and linguistic\ncontent before predicting. The paper demonstrated the importance of utilizing different types of\nprompts, such as pair, object, and state, to identify the optimal form, as well as enhancing\nperformance through modal fusion techniques\n2.2. Prompt-based Learning\nThe prompt has been utilized as a tool for fine-tuning in NLP [24]. Multiple prompt-based\nlearning researches are done, for example, Visual Prompt [6] suggests the way to use the prompt\nin the computer-vision field. Red circle [22] has shown a remarkable visual prompt in fine-tuning\nwith only the insertion of the red circle. Those modify prompts for the model instead of adjusting\nthe entire architecture, which is more efficient. In CLIP [3] fine-tuning tasks, previous works\nadjusted the prompts to guide the model toward image classification goals. Prompt tuning can\nquickly adjust pre-trained models to diverse tasks without extra training. However, the research\nconcerning the identification of an optimal prompt for CZSL is currently lacking, indicating the\nimportance of this study."}, {"title": "3. PROPOSED METHOD", "content": "3.1. Preliminaries\nCompositional Zero-Shot Learning (CZSL) is the task of recognizing seen and unseen\ncompositions for state and object. The baseline is DFSP [8], which implemented the decomposing\nand fusing module for each vision and text feature. To obtain a text feature, DFSP only uses a\nhard prompt that contains both state and object information as an input of the CLIP text encoder.\nDFSP [8] simply fused image and text features using a prompt to narrow the gap for unseen.\nHowever, it was not enough to understand the subtle differences in visual and textual levels with\nonly a simple pair prompt. [8] The use of single fusion methods in conjunction with paired data\nmay present limitations in bridging the modality gap when attempting to comprehend language\nwithout separation."}, {"title": "3.2. Hard Pair Prompt", "content": "As used in DFSP [8], hard pair prompt is denoted as: $P_{pair}^{hard}$ = {a photo of, xs, xo}."}, {"title": "3.3. Soft Separated Prompts", "content": "Unlike previous CZSL [20], [27] studies, soft prompts have been applied to understand specific\ncontent to diverse features in CZSL [18], [8]. CoOp [30] has also improved recognition results by\nreplacing hard with soft prompts. Additionally, we hypothesize that the pair prompt alone is not\nenough to extract information from a complex scene. Upon presenting the soft separated prompt,\nit has been confirmed that soft yielded better outcomes, while hard prompt has shown the best\nperformance for a pair in Table III. In ours, the decomposition strategy for only state and object\nprompt differentiates information about individual elements, exchanging each information for\ndetailed understanding, referring to Table IV The soft separated prompt combines visual and\nlanguage data for objects and visual and textual features for attributes.\n3.3.1. Soft Attribute Prompt\nIt is composed of learnable vectors and labels, and the prompt set is denoted as: $P_{attr}^{soft}$ = {X0, X1, ..., Xp, Xs}, and Xo, ..., Xp is a prefix content, and xs is a state vocabulary. From the\nabove, the matching scores are: $P_{attr}^{soft}$ = sim(vattr, tattr). Based on extracted language and\nimage features being matched, the class probability of Separated Attribute Prompt is p and cross-\nentropy loss is measured with the target attribute class and denoted as: $L_{attr}^{soft}$ = CE($p_{attr}^{soft}$, y =\n(s)).\n3.3.2. Soft Object Prompt\nIt is composed of learnable vectors and labels, and the prompt set is denoted as: $P_{obj}^{soft}$  = {X0, X1, ..., Xp, Xo }, and Xo, ..., Xp is a prefix content, and xo is an object vocabulary. Then the\nsoft matching score is: $P_{obj}^{soft}$ = sim(vob, tobj). From the above, the cross-entropy loss is measured with the target object class, and denoted as: $L_{obj}^{soft}$ = CE($p_{obj}^{soft}$, y = (o))."}, {"title": "3.4. Modal Fusion Synthesizer Block (MFSB)", "content": "MFSB improves state and object features by combining and breaking down text and images\nthrough two types of fusion. First, combine vision and text features through fusion: Inter-modality\nfusion. Secondly, same-type fusion, Intra-modality fusion. The best choice for refining a feature\nis to first focus on Table V, Inter- and then Intra-modality fusion. It guides object, attribute, and\npairing information in fusion and decomposition stages and improves understanding of\ninformation processing over time. In each fusion expression, a and b alternate between vision\nand text, and v and t are images and text vectors, respectively.\n3.4.1. Inter-modality fusion\nIn the Inter-modality fusion, cross-attention mechanisms combine decomposed text and visual\nfeatures to merge previously separate text and image features. The visuals and text are enhanced\nto complement each other, and denoted as:"}, {"title": "3.4.1.1. Refined Pair Prompt Features", "content": "Since hard pair prompt is being used, so the hard pair textual feature $t_{pair}^{hard}$ incorporates with\nvisual feature $v_{pair}^{hard}$ through MFSB's Inter-modality fusion. After fusion and decomposition of\nthe {state, object} set, a refined pair prompt feature with updated information is created. Then,\nthe matching score with the ground-truth pair is $p_{pair}^{inter}$ = sim($v_{pair}^{inter}, t_{pair}^{inter}$), and calculated\ncross-entropy inter-pair loss is: $L_{pair}^{inter}$ = CE ($p_{pair}^{inter}$, y = (s, o))."}, {"title": "3.4.1.2. Refined Attribute Prompt Features", "content": "Similar with above, soft attribute prompt is being used, so the soft attribute textual feature\n$t_{attr}^{soft}$ incorporates with visual feature $v_{attr}^{hard}$ through MFSB's Inter-modality fusion. After\nfusion and decomposition, a refined attribute prompt feature is created. The matching score with\nthe ground-truth attribute is: $p_{attr}^{inter}$ = sim($v_{attr}^{inter}, t_{attr}^{inter}$). Then, we calculate cross-entropy inter-\nattribute loss as: $L_{attr}^{inter}$ = CE ($p_{attr}^{inter}$, y = (s))."}, {"title": "3.4.1.3. Refined Object Prompt Features", "content": "As mentioned, soft object prompt is being used, so the soft object textual feature $t_{obj}^{soft}$ incorporates with visual feature $v_{obj}^{soft}$ through MFSB's Inter-modality fusion. After fusion and decomposition, a refined object prompt feature is created. From the above, the matching score\ninter with the ground-truth object is $p_{obj}^{inter}$ = sim($v_{obj}^{inter}, t_{obj}^{inter}$). We then calculate cross-entropy\ninter-object loss as: $L_{obj}^{inter}$ = CE ($p_{obj}^{inter}$, y = (o))."}, {"title": "3.4.2. Intra-modality fusion", "content": "For the Intra-modality fusion modality, the output of Inter-modality fusion modality features is\nbeing used as an input. And those are denoted as:"}, {"title": "3.4.2.1. Intra-modality fusion #1. ATTR \u2190 OBJ", "content": "Intra-modality fusion #1) applies a cross-attention [5], CA. As a query, inter-fused soft attribute\nprompt visual feature $v_{attr}^{inter}$ is inserted and inter object prompt feature $v_{obj}^{inter}$ is used as key and\nvalue. In text-level, inter attribute prompt textual feature $t_{attr}^{inter}$ and inter soft object textual"}, {"title": "3.4.2.2. Intra-modality fusion #2. OBJ \u2190 ATTR", "content": "Intra-modality fusion #2) also utilizes CA, but vice-versa. As a query, inter-fused soft object\nprompt visual feature $v_{obj}^{inter}$ is inserted and inter attribute prompt feature $v_{attr}^{inter}$ is used as key\nand value. In text-level, inter object prompt textual feature $t_{obj}^{inter}$ and inter soft attribute textual\nfeature $t_{attr}^{inter}$ are applied. Those are described as:"}, {"title": "3.5. Total Loss", "content": "To understand how visual and textual prompts relate, the total loss is calculated by combining\nmultiple losses. The types of loss in this paper are 1) Hard Pair, 2) Soft Attr & Obj, 3) Inter-fused\nPair & Attr & Obj, 4) Intra-fused Pair & Attr & Obj. $L_{pair}^{hard+soft}$ indicates an existing loss from\nDFSP, baseline. $L_{attr}^{hard+soft}$ and $L_{obj}^{hard+soft}$ are added additionally based on $L_{pair}^{hard+soft}$ format."}, {"title": "4. EXPERIMENTS", "content": "4.1. Dataset and Details\nWe used three challenging benchmark datasets: MIT-States [4], UT-Zappos [28], and C-GQA\n[13] in Closed and Open world settings. MIT-States has 53,753 images with 245 objects and 115\nattributes. MIT-States [4] has 300 unseen and 1262 seen labels for validation, and 400 unseen\nlabels for the test set. The dataset has 50,025 shoe images labeled under 12 categories and 16\nattributes. In a closed-world scenario, there are 15 unseen validation instances, 18 unseen test\ninstances, and 83 seen labels. The C-GQA [13] dataset has 39,298 images with 870 object labels\nand 453 attribute labels. Metrics assess accuracy for both seen and unseen compositions.\nAccuracy is measured on seen and unseen categories in both Seen (S) and Unseen (U) data. The\nHarmonic Mean (HM) metric estimates total accuracy. The AUC metric measures the balance of\ntrue positive and false positive rates at varying decision thresholds. Our method is implemented\nwith PyTorch [19] 1.12.1 and Adam [2] optimizer is utilized for optimization. Each three\nchallenging datasets is trained for 20 epochs, and both image and text encoder are based on pre-\ntrained CLIP [3] Vit-L/14 model with 1 \u00d7 NVIDIA RTX 4090 GPU."}, {"title": "4.2. Results", "content": "We compare our method DFSP [8] with previous compositional zero-shot learning techniques,\nincluding AoP [16] and LE+ [12] TMN [21], SymNet [27], Comp-Cos [9], CGE [11], Co-CGE\n[10], SCEN [25], KG-SP [23], and CSP [17]. We investigate various fusion methods including\nSeparated Prompts and Modal Fusion Synthesizer Block (MFSB), including inter- and intra-\nfusion modality. Table I and Table II show that our method performs better on MIT-States [4],\nUT-Zappos [28], and CGQA [13] datasets. Ours outperforms in AUC scores of 5.4% on MIT-\nStates [4], 36.0% on UT-Zappos [28], and 10.5% on CGQA [13], surpassing by 4.3%. Also, there\nis a 6.8% increase in the harmonic mean on the MIT-States [4] dataset compared to other methods.\nAlso, it shows high accuracy on both seen and unseen datasets in these experiments."}, {"title": "4.3. Ablation Study", "content": "Ablation studies aim to demonstrate the effectiveness of proposed methods and identify the\nbest prompt for CZSL. Table 3 identifies the best prompt combination, and Table 4 compares the\ncomponents of the prompt. Table 5 demonstrates the comparison of fusion techniques. Every\nexperiment is conducted with the MIT-States dataset in the Open-world setting."}, {"title": "5. CONCLUSION", "content": "In this study, we introduce a novel method called Modal Fusion Synthesized Prompt to\naccurately identify nuanced distinctions in meaning for composing state and object during training.\nAs grounded from prompt-tuning, a hard pair and soft decomposed prompt are applied to diverse\nusage in training. Furthermore, the proposed prompts serve different purposes in gathering precise\nand comprehensive information from the training data. The hard pair prompt gathers detailed\ninformation about multiple objects in a scene, with separate soft prompts capturing specific\nfeatures based on complex scenarios and using inter- and intra-modal fusion to improve\ncomprehension of visual and linguistic contexts. Extensive experiments on three challenging\ndatasets demonstrate the effectiveness of our proposed method, Inter- & Intra-Modality Fusion\nwith Separated Prompts."}]}