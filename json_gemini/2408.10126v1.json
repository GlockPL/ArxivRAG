{"title": "Learning Brave Assumption-Based Argumentation\nFrameworks via ASP", "authors": ["Emanuele De Angelis", "Maurizio Proietti", "Francesca Toni"], "abstract": "Assumption-based Argumentation (ABA) is advocated\nas a unifying formalism for various forms of non-monotonic rea-\nsoning, including logic programming. It allows capturing defeasible\nknowledge, subject to argumentative debate. While, in much existing\nwork, ABA frameworks are given up-front, in this paper we focus on\nthe problem of automating their learning from background knowl-\nedge and positive/negative examples. Unlike prior work, we newly\nframe the problem in terms of brave reasoning under stable exten-\nsions for ABA. We present a novel algorithm based on transforma-\ntion rules (such as Rote Learning, Folding, Assumption Introduction\nand Fact Subsumption) and an implementation thereof that makes\nuse of Answer Set Programming. Finally, we compare our technique\nto state-of-the-art ILP systems that learn defeasible knowledge.", "sections": [{"title": "1 Introduction", "content": "Assumption-based Argumentation (ABA) [2, 4, 9, 29] is a form of\nstructured argumentation broadly advocated as a unifying formal-\nism for various formalisations of non-monotonic reasoning, includ-\ning logic programming [2]. It allows capturing defeasible knowledge\nsubject to argumentative debate, whereby arguments are deductions\nbuilt from rules and supported by assumptions and, in order to be\n\"accepted\", they need to deal with attacks from other arguments (for\nthe contraries of assumptions in their support).\nIn much existing work, fully-formed ABA frameworks are given\nup-front, e.g. to model medical guidelines [5] or planning [10]. In-\nstead, in this paper we focus on the problem of automating their\nlearning from background knowledge and positive and negative ex-\namples. Specifically, we consider the recent formulation of ABA\nLearning [21] for learning ABA frameworks from a background\nknowledge, in the form of an initial ABA framework, and positive\nand negative examples, in the form of sentences in the language of\nthe background knowledge. The goal of ABA Learning is to build a\nlarger ABA framework than the background knowledge from which\narguments for all positive examples can be \"accepted\" and no ar-\nguments for any of the negative examples can be \"accepted\". In this\npaper, for a specific form of ABA frameworks corresponding to logic\nprograms [2], we focus on a specific form of \"acceptance\", given by\nbrave (or credulous, as commonly referred to in the argumentation\nliterature) reasoning under the argumentation semantics of stable ex-\ntensions [2, 4]."}, {"title": "2 Related Work", "content": "Forms of ABA Learning have already been considered in [6, 21, 28].\nLike [21] we rely upon transformation rules, adopting a variant of\nSubsumption and omitting to use Equality Elimination. However, we\nadopt a novel formulation of brave ABA Learning. Like [6] we use\nASP as the basis for implementing ABA Learning, but, again, we\nfocus on brave, rather than cautious, ABA Learning. Finally, [28]\nfocuses on cautious ABA Learning and uses Python rather than ASP.\nOur strategy for ABA Learning differs from other works learning\nargumentation frameworks, e.g. [3, 8, 20], in that it learns a different\ntype of argumentation frameworks and, also, is based on brave rea-\nsoning rather than cautious (a.k.a. sceptical). Also, these approaches\ndo not make use of ASP for supporting learning algorithmically.\nABA can be seen as performing abductive reasoning (as assump-\ntions are hypotheses open for debate). Other approaches combine\nabductive and inductive learning [22], but they do not learn ABA\nframeworks. Moreover, while using a definition of abduction wrt\nbrave/credulous stable model semantics, [22] does not identify any\nproperty of brave induction and focuses on case studies, in the con-\ntext of the event calculus, with a unique answer set (where brave and\ncautious reasoning coincide). Some other approaches learn abduc-\ntive logic programs [13], which rely upon assumptions, like ABA.\nA formal comparison with these methods is left for future work.\nABA captures several non-monotonic reasoning formalisms, thus"}, {"title": "3 Background", "content": "3.1 Answer Set Programs\nWe use ASP [12] consisting of rules of the form\n$\\begin{array}{c}\np \\leftarrow q_{1}, \\ldots, q_{k}, \\text { not } q_{k+1}, \\ldots, \\text { not } q_{n} \\\\\n\\text { or } \\varepsilon \\leftarrow q_{1}, \\ldots, q_{k}, \\text { not } q_{k+1}, \\ldots, \\text { not } q_{n}\n\\end{array}$\nwhere $p, q_{1}, \\ldots, q_{n}$, are atoms, $k \\geq 0, n \\geq 0$, and not denotes\nnegation as failure. We assume that the reader is familiar with the\nstable model semantics for ASP [12], and we call answer set of $P$ any\nset of ground atoms assigned to $P$ by that semantics. $P$ is said to be\nsatisfiable, denoted sat $(P)$, if it has an answer set, and unsatisfiable\notherwise. An atom $p$ is a brave consequence of $P$ if there exists an\nanswer set $A$ of $P$ such that $p \\in A$.\n3.2 Assumption-Based Argumentation (ABA)\nAn ABA framework (as originally proposed in [2], but presented here\nfollowing [9, 29] and [4]) is a tuple $\\langle L, R, A,\\overline{\\cdot}\\rangle$ such that\n$\\bullet(L, R)$ is a deductive system, where $L$ is a language and $R$ is a set\nof (inference) rules of the form $s_{0} \\leftarrow s_{1}, \\ldots, s_{m}$ ( $m \\geq 0, s_{i} \\in L$,\nfor $1 \\leq i \\leq m$);\n$\\bullet A \\subseteq L$ is a (non-empty) set of assumptions; $^1$\n$\\bullet \\overline{\\cdot}$ is a total mapping from $A$ into $L$, where $\\overline{a}$ is the contrary of $a$,\nfor $a \\in A$ (also denoted as $\\{a \\leftrightarrow \\overline{a} \\mid a \\in A\\}$).\nGiven a rule $s_{0} \\leftarrow s_{1}, \\ldots, s_{m}$, $s_{0}$ is the head and $s_{1}, \\ldots, s_{m}$ is\nthe body; if $m=0$ then the body is said to be empty (represented\nas $s_{0} \\leftarrow$ or $s_{0} \\leftarrow$ true) and the rule is called a fact. In this paper\nwe focus on flat ABA frameworks, where assumptions are not heads\nof rules. Elements of $L$ can be any sentences, but in this paper we\nfocus on ABA frameworks where $L$ is a finite set of ground atoms.\nHowever, in the spirit of logic programming, we will use schemata\nfor rules, assumptions and contraries, using variables to represent\ncompactly all instances over some underlying universe. By vars $(E)$\nwe denote the set of variables occurring in atom, rule, or rule body $E$."}, {"title": "4 Brave ABA Learning under Stable Extensions", "content": "Here we present the instance of the ABA Learning problem that we\nconsider in this paper. We follow the lines of [21], but we focus on\na semantics based on brave consequences under stable extensions.\nAlso, we consider a further parameter: the set $T$ of predicates to\nbe learned, which do not necessarily coincide with the predicates"}, {"title": "5 Brave ABA Learning via Transformation Rules", "content": "To learn ABA frameworks from examples, we follow the approach\nbased on transformation rules from [21], but only consider a subset\nof those rules: Rote Learning, Folding, Assumption Introduction, and\n(a special case of) Subsumption, thus ignoring Equality Removal $^2$.\nFolding and Subsumption are borrowed from logic program transfor-\nmation [19], while Rote Learning and Assumption Introduction are\nspecific for ABA. Given an ABA framework $(R, A,\\overline{\\cdot})$, a transfor-\nmation rule constructs a new ABA framework $\\left(R^{\\prime}, A^{\\prime}, \\overline{\\cdot}^{\\prime}\\right)$ (below,\nwe will mention explicitly only the modified components).\nWe assume rules in $R$ are written in normalised form as follows:\n$p_{0}\\left(\\mathbf{X}_{0}\\right) \\leftarrow e q_{1}, \\ldots, e q_{k}, p_{1}\\left(\\mathbf{X}_{1}\\right), \\ldots, p_{n}\\left(\\mathbf{X}_{n}\\right)$\nwhere $p_{i}\\left(\\mathbf{X}_{i}\\right)$, for $0 \\leq i \\leq n$, is an atom (whose ground instances\nare) in $L$ and $e q_{i}$, for $1 \\leq i \\leq k$, is an equality $t_{1}=t_{2}$, with\n$t_{i}$ a term whose variables occur in the tuples $\\mathbf{X}_{0}, \\mathbf{X}_{1}, \\ldots, \\mathbf{X}_{n}$. In\nparticular, we represent a ground fact $p(t) \\leftarrow$ as $p(X) \\leftarrow X=t$.\nThe body of a normalised rule can be freely rewritten by using the\nstandard axioms of equality, e.g., $Y_{1}=a, Y_{2}=a$ can be rewritten as\n$Y_{1}=Y_{2}, Y_{2}=a$. For constructing arguments, we assume that, for\nany ABA framework, the language $L$ contains all equalities between\nelements of the underlying universe and $R$ includes all rules $a=a$, where $a$ is an element of the universe. We also assume that,\nfor all rules $H \\leftarrow B \\in R$, vars $(H) \\subseteq$ vars $(B)$. When presenting\nthe transformation rules, we use the following notations: (1) $H, K$\ndenote heads of rules, (2) Eqs (possibly with subscripts) denotes sets\nof equalities, (3) $B$ (possibly with subscripts) denotes sets of atoms.\nR1. Rote Learning. Given atom $p(t)$, add $p: p(X) \\leftarrow X=t$ to $R$.\nThus, $R^{\\prime}=R \\cup\\{p\\}$.\nWe will use R1 either to add facts from positive examples or facts\nfor contraries of assumptions, as shown by the following example."}, {"title": "6 A Brave ABA Learning Algorithm", "content": "The application of the transformation rules is guided by the\nASP-ABAlearn algorithm (see Algorithm 1), a variant of the one\nin [6], which refers to a cautious stable extensions semantics. The\ngoal of ASP-ABAlearn is to derive an intensional solution for the\ngiven brave ABA Learning problem, and to achieve that goal some\ntasks are implemented via an ASP solver.\nThe ASP-ABAlearn algorithm is the composition of two proce-\ndures RoLe and Gen:\n(1) RoLe repeatedly applies Rote Learning with the objective\nof adding a minimal set of facts to the background knowledge\n$\\\\left(R_{0}, A_{0}, \\overline{\\cdot}_{0}\\right)$, so that the new ABA framework $(R, A,\\overline{\\cdot})$ is a\n(non-intensional) solution of the brave ABA Learning problem\n$\\\\left(\\left\\langle R_{0}, A_{0}, \\overline{\\cdot}_{0}\\right\\rangle,(E+, E-), T\\right)$ given in input.\n(2) Gen has the objective of transforming $(R, A,\\overline{\\cdot})$ into an in-\ntensional solution. This is done by transforming each learnt non-\nintensional rule as follows. First, Gen repeatedly applies Folding, so\nas to get a new intensional rule. It may happen, however, that the\nABA framework with the new rule is no longer a solution of the given"}, {"title": "Appendix", "content": "Proof of Proposition 1\nBy induction on the structure of AFR S.\nCase 1: p1 is not the rule used at the root of A FR S. Let $p: s \\leftarrow$\n$s_{1}, \\ldots, s_{m}$ be the ground rule (where $p$ is not an instance of $p_{1}$)\nused to construct the root $s$ from the arguments $\\Delta_{1} \\vdash_{R_{1}} s_{1}$,\n..., $\\Delta_{m} \\vdash_{R_{m}} s_{m}$. By the inductive hypothesis, for $i=1, \\ldots, m$, there\nexists $\\Delta_{i} \\vdash_{R_{i}^{\\prime}} s_{i}$, with $R_{i}^{\\prime} \\subseteq R^{\\prime}$. Thus, $\\Delta \\vdash_{R_{1}^{\\prime} \\cup \\ldots \\cup R_{m}^{\\prime}} s$, with\n$R_{1}^{\\prime} \\cup \\ldots \\cup R_{m}^{\\prime} \\subseteq R^{\\prime}$.\nCase 2: p1 is the rule used at the root of A FR S. Let $p: H^{\\prime} \\leftarrow$\n$\\\\text{ Eqs }_{1}, B_{1}, B_{2}$ be the ground instance of rule $p_{1}$ with $H^{\\prime} \\leftarrow$\n$\\\\text{ Eqs }_{1}$ ground identities, $B_{1}=s_{1}, \\ldots, s_{k}$, and $B_{2}=s_{k+1}, \\ldots, s_{m}$.\nLet $\\Delta_{1} \\vdash_{R_{1}} s_{1}, \\ldots, \\Delta_{m} \\vdash_{R_{m}} s_{m}$ be the sub-trees of AFR S\nrooted at $s_{1}, \\ldots, s_{m}$, respectively. By the inductive hypothesis, for\n$i=1, \\ldots, m$, there exists $\\Delta_{i} \\vdash_{R_{i}^{\\prime}} s_{i}$, with $R_{i}^{\\prime} \\subseteq R^{\\prime}$. An argument\n$\\Delta \\vdash_{R^{\\prime}} s$, with $R^{\\prime} \\subseteq R^{\\prime}$ can be constructed by using (1) a ground\ninstance $H^{\\prime} \\leftarrow \\text { Eqs }_{2}, K^{\\prime}, B_{2}$ of $p_{3}$ at the top, (2) a ground instance\n$K^{\\prime} \\leftarrow \\text { Eqs }_{1}, \\text { Eqs }_{2}, B_{1}$ of $p_{2}$ (in R2 we require $p_{1} \\neq p_{2}$, and hence\n$p_{2} \\in R^{\\prime}$), and (3) for $i=1, \\ldots, m$, the arguments $\\Delta_{i} \\vdash_{R_{i}^{\\prime}} s_{i}$\nfor $B_{1}, B_{2}$. The condition vars $\\left(\\text { Eqs }_{2}\\right) \\cap$ vars $\\left(p_{1}\\right)=0$ allows the\nconstruction of the suitable ground instances at points (1) and (2),\nbecause the equalities $\\text { Eqs }_{2}$ do not add extra constraints on the vari-\nables of $H$, Eqs1 and B2.\nProof of Proposition 2\nBy the definition of a solution of a brave ABA Learning problem,\n$\\\\left(R_{1}, A_{1}, \\overline{\\cdot}_{1}\\right)$ admits a stable extension $\\Delta$ such that, for all $e \\in E^{+}$,\n$\\left\\langle R_{1}, A_{1}, \\overline{\\cdot}_{1}\\right\\rangle \\vDash_{\\Delta} e$ and, for all $e \\in E^{-},\\left\\langle R_{1}, A_{1}, \\overline{\\cdot}_{1}\\right\\rangle \\not \\vDash_{\\Delta} e$.\nLet us consider the rule $p_{4}: H \\leftarrow \\text { Eqs }_{1}, \\text { Eqs }_{2}, K, B_{1}, B_{2}, a(X)$\nobtained from $p_{1}$ by adding $\\text { Eqs }_{2}, K, a(X)$ to its body, where\nvars $\\left(\\{\\\\text{ Eqs }_{1}, B_{1}\\}\\right) \\subseteq X$, and let $R_{4}=\\left(R_{1} \\backslash\\left\\{p_{1}\\right\\}\\right) \\cup\\left\\{p_{4}\\right\\}$. By\ninduction on the argument structure, for all $s, \\Delta_{1} \\vdash_{R_{1}} s \\in \\Delta$ iff\n$\\Delta_{1} \\cup\\{a(X)\\} \\vdash_{R_{4}} s \\in \\Delta$ (note, in particular, that no rule for the\ncontrary $c\\_a(X)$ of $a(X)$ occurs in $\\left.R_{4}\\right)$. Let $B_{1}=s_{1}, \\ldots, s_{k}$,\nlet $S=\\{c\\_a(t) \\mid \\text { Eqs }_{1}\\{X / t\\}$ is a set of identities and $\\exists i \\in$\n$\\left\\{1, \\ldots, k\\right\\} . \\Delta_{1} \\cup\\{a(X)\\} \\not \\vdash_{R_{4}} s_{i}\\}$, let $C a$ be defined from $S$ as\nin the statement of this proposition, and let $R_{4}^{\\prime}=R_{3} \\cup C a$ (that is,\n$R_{4}^{\\prime}$ is obtained from $R_{4}$ by dropping Eqs $s_{1}, B_{1}$ from the body of $p_{4}$\nand adding to the resulting set of rules the facts for the contrary of\n$a(X))$. Now, we can construct a new extension $\\Delta^{\\prime}$ such that, for all\nclaims $s \\neq c\\_a(X), \\Delta_{1} \\vdash_{R_{1}} s \\in \\Delta$ iff $\\Delta_{1} \\vdash_{R_{3}} s \\in \\Delta^{\\prime}$, with\n$A \\subseteq\\left(\\Delta_{1} \\cup\\{a(X)\\}\\right) \\cap S$. Let $\\mathcal{F}_{4}=\\left(R_{4}, A_{1} \\cup\\{a(X)\\}, \\overline{\\cdot}_{1} \\cup\\right.$\n$\\{\\left.a(X) \\leftrightarrow c\\_a(X)\\right\\}\\right)$. Then, $\\Delta^{\\prime}$ is a stable extension such that, for\nall $e \\in E^{+}, \\mathcal{F}_{4} \\vDash_{\\Delta^{\\prime}} e$ and, for all $e \\in E^{-}, \\mathcal{F}_{4} \\not \\vDash_{\\Delta^{\\prime}} e$, and hence\n$\\mathcal{F}_{4}$ is a solution of $\\left(\\left\\langle R_{0}, A_{0}, \\overline{\\cdot}_{0}\\right\\rangle,(E+, E-), T\\right)$.\nProof of Theorem 1\nLet us consider the ASP program $P_{F}$ constructed from the ABA\nframework $\\mathcal{F}=\\left(R, A, \\overline{\\cdot}\\right)$ by replacing every assumption $a$ oc-\ncurring in the body of a rule in $R$ by the negation-as-failure literal\nnot $a$. By Theorem 3.13 of [2], there is a one-to-one correspon-\ndence between the stable extensions of $\\mathcal{F}$ and the answer sets of $P_{F}$.\nBy rewriting each ASP rule of $P_{F}$ of the form $p :-\\operatorname{B}$, not $a$ into the\npair of rules $p :-\\operatorname{B}, a, a :-\\operatorname{dom}(X)$, not $\\bar{a}$, where vars $(a)=X$, we\nget a new ASP program $P^{\\prime}$, consisting of the rules of points (a) and\n(b) of Definition 2. This rewriting preserves answer sets, and hence,"}]}