{"title": "Learning Brave Assumption-Based Argumentation Frameworks via ASP", "authors": ["Emanuele De Angelis", "Maurizio Proietti", "Francesca Toni"], "abstract": "Assumption-based Argumentation (ABA) is advocated as a unifying formalism for various forms of non-monotonic reasoning, including logic programming. It allows capturing defeasible knowledge, subject to argumentative debate. While, in much existing work, ABA frameworks are given up-front, in this paper we focus on the problem of automating their learning from background knowledge and positive/negative examples. Unlike prior work, we newly frame the problem in terms of brave reasoning under stable extensions for ABA. We present a novel algorithm based on transformation rules (such as Rote Learning, Folding, Assumption Introduction and Fact Subsumption) and an implementation thereof that makes use of Answer Set Programming. Finally, we compare our technique to state-of-the-art ILP systems that learn defeasible knowledge.", "sections": [{"title": "Introduction", "content": "Assumption-based Argumentation (ABA) [2, 4, 9, 29] is a form of structured argumentation broadly advocated as a unifying formalism for various formalisations of non-monotonic reasoning, including logic programming [2]. It allows capturing defeasible knowledge subject to argumentative debate, whereby arguments are deductions built from rules and supported by assumptions and, in order to be \"accepted\", they need to deal with attacks from other arguments (for the contraries of assumptions in their support).\nIn much existing work, fully-formed ABA frameworks are given up-front, e.g. to model medical guidelines [5] or planning [10]. Instead, in this paper we focus on the problem of automating their learning from background knowledge and positive and negative examples. Specifically, we consider the recent formulation of ABA Learning [21] for learning ABA frameworks from a background knowledge, in the form of an initial ABA framework, and positive and negative examples, in the form of sentences in the language of the background knowledge. The goal of ABA Learning is to build a larger ABA framework than the background knowledge from which arguments for all positive examples can be \"accepted\" and no arguments for any of the negative examples can be \"accepted\". In this paper, for a specific form of ABA frameworks corresponding to logic programs [2], we focus on a specific form of \"acceptance\", given by brave (or credulous, as commonly referred to in the argumentation literature) reasoning under the argumentation semantics of stable extensions [2, 4].\nWe base our approach to brave ABA Learning on transformation rules, in the spirit of [21]. We leverage on the well known correspondence [2] between stable extensions in the logic programming instance of ABA and Answer Set Programs (ASP) [12] to outline a novel implementation strategy for the form of ABA Learning we consider, pointing out along the way restrictions on ABA Learning enabling the use of ASP. We also show experimentally, on some standard benchmarks, that the resulting ASP-ABAlearn system performs well in comparison with ILASP [15], a state-of-the-art system in inductive logic programming (ILP) able to learn non-stratified logic programs. In summary, our main contributions are: (i) a novel definition of brave ABA Learning; (ii) a novel (sound and terminating) ASP-ABAlearn system for carrying out brave ABA Learning in ASP; (iii) an empirical evaluation of ASP-ABAlearn showing its strengths in comparison with the ILASP system. All proofs are given in the Appendix. The ASP-ABAlearn system is available at [7]."}, {"title": "Related Work", "content": "Forms of ABA Learning have already been considered in [6, 21, 28]. Like [21] we rely upon transformation rules, adopting a variant of Subsumption and omitting to use Equality Elimination. However, we adopt a novel formulation of brave ABA Learning. Like [6] we use ASP as the basis for implementing ABA Learning, but, again, we focus on brave, rather than cautious, ABA Learning. Finally, [28] focuses on cautious ABA Learning and uses Python rather than ASP.\nOur strategy for ABA Learning differs from other works learning argumentation frameworks, e.g. [3, 8, 20], in that it learns a different type of argumentation frameworks and, also, is based on brave reasoning rather than cautious (a.k.a. sceptical). Also, these approaches do not make use of ASP for supporting learning algorithmically.\nABA can be seen as performing abductive reasoning (as assumptions are hypotheses open for debate). Other approaches combine abductive and inductive learning [22], but they do not learn ABA frameworks. Moreover, while using a definition of abduction wrt brave/credulous stable model semantics, [22] does not identify any property of brave induction and focuses on case studies, in the con-text of the event calculus, with a unique answer set (where brave and cautious reasoning coincide). Some other approaches learn abductive logic programs [13], which rely upon assumptions, like ABA. A formal comparison with these methods is left for future work.\nABA captures several non-monotonic reasoning formalisms, thus"}, {"title": "Background", "content": "We use ASP [12] consisting of rules of the form\n$p \\leftarrow q_1, ..., q_k, not \\: q_{k+1},..., not \\: q_n$\nor\n$p \\leftarrow \\; q_1, ..., q_k, not \\: q_{k+1},..., not \\: q_n$\nwhere $p, q_1,..., q_n$ are atoms, $k \\geq 0$, $n \\geq 0$, and not denotes negation as failure. We assume that the reader is familiar with the stable model semantics for ASP [12], and we call answer set of P any set of ground atoms assigned to P by that semantics. P is said to be satisfiable, denoted sat(P), if it has an answer set, and unsatisfiable otherwise. An atom p is a brave consequence of P if there exists an answer set A of P such that $p \\in A$."}, {"title": "Answer Set Programs"}, {"title": "Assumption-Based Argumentation (ABA)", "content": "An ABA framework (as originally proposed in [2], but presented here following [9, 29] and [4]) is a tuple $\\langle \\mathcal{L}, \\mathcal{R}, \\mathcal{A}, {}^{-}\\rangle$ such that\n*   ($\\mathcal{L}$,$\\mathcal{R}$) is a deductive system, where $\\mathcal{L}$ is a language and $\\mathcal{R}$ is a set of (inference) rules of the form $s_0 \\leftarrow s_1,..., s_m$ ($m \\geq 0$, $s_i \\in \\mathcal{L}$, for $1 \\leq i \\leq m$);\n*   $\\mathcal{A} \\subseteq \\mathcal{L}$ is a (non-empty) set of assumptions;\u00b9\n*   ${}^{-}$ is a total mapping from $\\mathcal{A}$ into $\\mathcal{L}$, where $\\overline{a}$ is the contrary of $a$, for $a \\in \\mathcal{A}$ (also denoted as {$a \\leftrightarrow \\overline{a}$ | $a \\in \\mathcal{A}$}).\nGiven a rule $s_0 \\leftarrow s_1,..., s_m$, $s_0$ is the head and $s_1,..., s_m$ is the body; if m = 0 then the body is said to be empty (represented as $s_0 \\leftarrow$ or $s_0 \\leftarrow true$) and the rule is called a fact. In this paper we focus on flat ABA frameworks, where assumptions are not heads of rules. Elements of $\\mathcal{L}$ can be any sentences, but in this paper we focus on ABA frameworks where $\\mathcal{L}$ is a finite set of ground atoms. However, in the spirit of logic programming, we will use schemata for rules, assumptions and contraries, using variables to represent compactly all instances over some underlying universe. By vars(E) we denote the set of variables occurring in atom, rule, or rule body E.\nExample 1. We consider a variant of the well-known Nixon diamond problem [23], formalised as the ABA framework with\n$\\mathcal{L} = \\{$quaker(X), democrat(X), republican(X), person(X),\nvotes\\_dem(X), votes\\_rep(X), normal\\_quaker(X)\n| X $\\in\\$ {a, b, c, d, e}$\\}\\$\n$\\mathcal{R} = \\{$p_1 : quaker(a) $\\leftarrow$, $p_2 : quaker(b) $\\leftarrow$, $p_3 : quaker(e) $\\leftarrow$,"}, {"title": "Brave ABA Learning under Stable Extensions", "content": "Here we present the instance of the ABA Learning problem that we consider in this paper. We follow the lines of [21], but we focus on a semantics based on brave consequences under stable extensions. Also, we consider a further parameter: the set T of predicates to be learned, which do not necessarily coincide with the predicates"}, {"title": "Brave ABA Learning via Transformation Rules", "content": "To learn ABA frameworks from examples, we follow the approach based on transformation rules from [21], but only consider a subset of those rules: Rote Learning, Folding, Assumption Introduction, and (a special case of) Subsumption, thus ignoring Equality Removal \u00b2. Folding and Subsumption are borrowed from logic program transformation [19], while Rote Learning and Assumption Introduction are specific for ABA. Given an ABA framework ($\\mathcal{R}, \\mathcal{A}, {}^{-}$), a transformation rule constructs a new ABA framework ($\\mathcal{R'}, \\mathcal{A'}, {}^{-'}$) (below, we will mention explicitly only the modified components).\nWe assume rules in $\\mathcal{R}$ are written in normalised form as follows:\n$p_0(X_0) \\leftarrow eq_1, . . . , eq_k, p_1(X_1),...,p_n(X_n)$\nwhere $p_i(X_i)$, for $0 \\leq i \\leq n$, is an atom (whose ground instances are) in $\\mathcal{L}$ and $eq_i$, for $1 \\leq i \\leq k$, is an equality $t_1 = t_2$, with $t_i$ a term whose variables occur in the tuples $X_0, X_1, ..., X_n$. In particular, we represent a ground fact $p(t) \\leftarrow$ as $p(X) \\leftarrow X = t$.\nThe body of a normalised rule can be freely rewritten by using the standard axioms of equality, e.g., $Y_1 = a, Y_2 = a$ can be rewritten as $Y_1 = Y_2, Y_2 = a$. For constructing arguments, we assume that, for any ABA framework, the language $\\mathcal{L}$ contains all equalities between elements of the underlying universe and $\\mathcal{R}$ includes all rules $a = a$, where a is an element of the universe. We also assume that, for all rules $H \\leftarrow B \\in \\mathcal{R}$, vars(H) $\\subseteq$ vars(B). When presenting the transformation rules, we use the following notations: (1) H, K denote heads of rules, (2) Eqs (possibly with subscripts) denotes sets of equalities, (3) B (possibly with subscripts) denotes sets of atoms.\n*R1. Rote Learning*. Given atom p(t), add $p \\colon p(X) \\leftarrow X =t$ to $\\mathcal{R}$. Thus, $\\mathcal{R'} = \\mathcal{R} \\cup \\{p\\}$.\nWe will use R1 either to add facts from positive examples or facts for contraries of assumptions, as shown by the following example.\nExample 5. Let us consider the learning problem presented in Example 3. By Rote Learning we add to $\\mathcal{R}$ the following two rules:\n$p_{12} \\colon abnormal\\_quaker(X) \\leftarrow X = b$\n$p_{13} \\colon pacifist(X) \\leftarrow X = c$\nThe resulting ABA framework with rules $\\mathcal{R} \\cup \\{p_{12}, p_{13}\\}$ is a (non-intensional) solution."}, {"title": "A Brave ABA Learning Algorithm", "content": "The application of the transformation rules is guided by the ASP-ABAlearn algorithm (see Algorithm 1), a variant of the one in [6], which refers to a cautious stable extensions semantics. The goal of ASP-ABAlearn is to derive an intensional solution for the given brave ABA Learning problem, and to achieve that goal some tasks are implemented via an ASP solver.\nThe ASP-ABAlearn algorithm is the composition of two procedures RoLe and Gen:\n(1) RoLe repeatedly applies Rote Learning with the objective of adding a minimal set of facts to the background knowledge ($R_0, A_0, {}^{-0}$), so that the new ABA framework ($\\mathcal{R}, \\mathcal{A}, {}^{-}$) is a (non-intensional) solution of the brave ABA Learning problem (($R_0, A_0, {}^{-0}$), (E\u207a, E\u207b), T) given in input.\n(2) Gen has the objective of transforming ($\\mathcal{R}, \\mathcal{A}, {}^{-}$) into an intensional solution. This is done by transforming each learnt non-intensional rule as follows. First, Gen repeatedly applies Folding, so as to get a new intensional rule. It may happen, however, that the ABA framework with the new rule is no longer a solution of the given"}, {"title": "Implementation and Experiments", "content": "We have realised a proof-of-concept implementation [7] of our ASP-ABAlearnB strategy using the SWI-Prolog system and the Clingo ASP solver. We have used Prolog as a fully fledged programming language to handle symbolically the rules and to implement the nondeterministic search for a solution to the learning problem, while we have used ASP as a specialised solver for computing answer sets corresponding to stable extensions. In particular, our tool consists of two Prolog modules implementing RoLe and Gen and two further modules implementing (i) the ASP encoding of Definition 2, and (ii) the API to invoke Clingo from SWI-Prolog and collect the answer sets to be used by RoLe and Gen.\nTable 1 reports the results of the experimental evaluation we have conducted on a benchmark set consisting of seven classic learning problems taken from the literature (flies [8], innocent [2], nixon_diamond [23], and variants thereof), and three larger problems (i.e., tabular datasets) from [31], to show that our approach works for non-trivial, non-ad-hoc examples. The discussion on scalability is out of the scope of the present paper.\nIn the table, we compare ASP-ABAlearn with ILASP, a state-of-the-art learner for ASP programs. When running ILASP we have opted for adopting the most direct representations of the learning problems, in terms of mode declarations. \u2077 In the ILASP column, unsat indicates that the system halted within the timeout, but was unable to learn an ASP program. These unsat results are due to the fact that the predicates and the mode declarations specified in the"}, {"title": "Conclusions", "content": "We have designed an approach for learning ABA frameworks based on transformation rules [21], and we have shown that, in the case of brave reasoning under the stable extension semantics, many of the reasoning tasks used by that strategy can be implemented through an ASP solver. We have studied a number of properties concerning both the transformation rules and an algorithm that implements our learning strategy, including its termination, soundness, and completeness, under suitable conditions. A distinctive feature of our approach is that argumentation plays a key role not only at the representation level, as we learn defeasible rules represented by ABA frameworks, but also at the meta-reasoning level, as our learning strategy can be seen as a form of debate that proceeds by conjecturing general rules that cover the examples and then finding exceptions to them.\nEven if the current implementation is not optimised, it allows solv-ing some non trivial learning problems. The most critical issue is that the application of Folding, needed for generalisation, is non-deterministic, as there may be different choices for the rules to be used for applying it. Currently, we are experimenting various mech-anisms to control Folding for making it more deterministic. In addi-tion to refining the implementation, we are also planning to perform a more thorough experimental comparison with non-monotonic ILP systems (such as FOLD-RM [27, 31] and ILASP [15]). Further ex-tensions of our ABA Learning approach can be envisaged, exploiting the ability of ABA frameworks to be instantiated to different logics and semantics, and possibly address the problem of learning non-flat ABA frameworks [2, 4]. To this aim we may need to integrate ABA Learning with tools that go beyond ASP solvers (e.g. [16])."}, {"title": "Appendix", "content": "By induction on the structure of $A \\vdash^{\\mathcal{F}} s$.\nCase 1: $p_1$ is not the rule used at the root of $A \\vdash^{\\mathcal{F}} s$. Let $p \\colon s \\leftarrow s_1,..., s_m$ be the ground rule (where p is not an instance of $p_1$) used to construct the root s from the arguments $A_1 \\vdash^{\\mathcal{F}} s_1$, ... , $A_m \\vdash^{\\mathcal{F}} s_m$. By the inductive hypothesis, for i = 1, ... , m, there exists $A_i \\vdash^{\\mathcal{F'}} s_i$, with $\\mathcal{R_i} \\subseteq \\mathcal{R'}$. Thus, $A \\vdash^{\\mathcal{F'} \\cup ... \\cup \\mathcal{F'}} s$, with $R' \\cup ... \\cup \\mathcal{F'} \\subseteq \\mathcal{R'}$.\nCase 2: $p_1$ is the rule used at the root of $A \\vdash^{\\mathcal{F}} s$. Let $p \\colon H' \\leftarrow Eqs_1, B_1, B_2$ be the ground instance of rule $p_1$ with H' $\\leftarrow$ head(p\u2081) $Eqs_1$ ground identities, $B_1 = s_1,..., s_k$, and $B_2 = s_{k+1},...,s_m$. Let $A_1 \\vdash^{\\mathcal{F}} s_1$, ... , $A_m \\vdash^{\\mathcal{F}} s_m$ be the sub-trees of $A \\vdash^{\\mathcal{F}} s$ rooted at $s_1,..., s_m$, respectively. By the inductive hypothesis, for i = 1, ... , m, there exists $A_i \\vdash^{\\mathcal{F'}} s_i$, with $\\mathcal{R_i} \\subseteq \\mathcal{R'}$. An argument $A \\vdash^{\\mathcal{F'}} s$, with $\\mathcal{R'} \\subseteq \\mathcal{R'}$, can be constructed by using (1) a ground instance $H' \\leftarrow Eqs_2, K', B_2$ of $p_3$ at the top, (2) a ground instance $K' \\leftarrow Eqs_1, Eqs_2, B_1$ of $p_2$ (in $R_2$ we require $p_1 \\neq p_2$, and hence $p_2 \\in \\mathcal{R'}$), and (3) for i = 1, ... , m, the arguments $A_i \\vdash^{\\mathcal{F'}} s_i$ for $B_1$, $B_2$. The condition vars($Eqs_2$) $\\cap$ vars($p_1$) = $\\emptyset$ allows the construction of the suitable ground instances at points (1) and (2), because the equalities $Eqs_2$ do not add extra constraints on the variables of H, $Eqs_1$ and $B_2$.\nBy the definition of a solution of a brave ABA Learning problem, ($R_1, A_1, {}^{-1}$) admits a stable extension $\\Delta$ such that, for all e $\\in$ $E^+$, ($R_1, A_1, {}^{-1}$) $\\models_{\\Delta}$ e and, for all e $\\in$ $E^-$, ($R_1, A_1, {}^{-1}$) $\\not\\models_{\\Delta}$ e.\nLet us consider the rule $p_4: H \\leftarrow Eqs_1, Eqs_2, K, B_1, B_2, a(X)$ obtained from p1 by adding $Eqs_2$, K, a(X) to its body, where vars({$Eqs_1, B_1$}) $\\subseteq$ X, and let $R_4 = (R_1 \\setminus \\{p_1\\}) \\cup \\{p\\}$. By induction on the argument structure, for all s, $A_1 \\vdash^{\\mathcal{F_1}} s \\in \\Delta$ iff $A_1 \\cup \\{a(X)\\} \\vdash^{\\mathcal{F'}} s \\in \\Delta$ (note, in particular, that no rule for the contrary $c\\_a(X)$ of $a(X)$ occurs in $R_4$). Let $B_1 = s_1,..., s_k$, let $S = \\{ c\\_a(t) | Eqs_1 \\{X/t\\}$ is a set of identities and $\\exists i \\in \\{1, ..., k\\}. A_1 \\cup \\{a(X)\\} \\not\\vdash^{R_1} s_i\\}$, let $C_a$ be defined from S as in the statement of this proposition, and let $R_4 = R_3 \\cup C_a$ (that is, $R_4$ is obtained from $R_4$ by dropping $Eqs_1$, $B_1$ from the body of $p_4$ and adding to the resulting set of rules the facts for the contrary of $a(X)$). Now, we can construct a new extension $\\Delta'$ such that, for all claims s $\\neq c\\_a(X)$, $A_1 \\vdash^{\\mathcal{F_1}} s \\in \\Delta$ iff $A_1 \\vdash^{\\mathcal{F_3}} s \\in \\Delta'$, with $A \\subseteq (A_1 \\cup \\{a(X)\\} \\cap S)$. Let $F_4 = (R_4, A_1 \\cup \\{a(X)\\}, {}^{-1} \\cup \\{a(X) \\rightarrow c\\_a(X)\\})$. Then, $\\Delta'$ is a stable extension such that, for all e $\\in$ $E^+$, $F_4 \\models_{\\Delta'} e$ and, for all e $\\in$ $E^-$, $F_4 \\not\\models_{\\Delta'} e$, and hence $F_4$ is a solution of (($R_0, A_0, {}^{-0}$), ($E^+$, $E^-$), T).\nLet us consider the ASP program $P_F$ constructed from the ABA framework $F = (R, A, {}^{-})$ by replacing every assumption a occurring in the body of a rule in R by the negation-as-failure literal not a. By Theorem 3.13 of [2], there is a one-to-one correspon-dence between the stable extensions of F and the answer sets of $P_F$. By rewriting each ASP rule of $P_F$ of the form p :- B, not a into the pair of rules p :- B, a, a :- dom(X), not $ \\overline{a}$, where vars(a) = X, we get a new ASP program P', consisting of the rules of points (a) and (b) of Definition 2. This rewriting preserves answer sets, and hence,"}]}