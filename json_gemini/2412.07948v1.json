{"title": "Frechet Music Distance: A Metric For Generative Symbolic Music Evaluation", "authors": ["Jan Retkowski", "Jakub St\u0119pniak", "Mateusz Modrzejewski"], "abstract": "In this paper we introduce the Frechet Music Distance (FMD), a novel evaluation metric for generative symbolic music models, inspired by the Frechet Inception Distance (FID) in computer vision and Frechet Audio Distance (FAD) in generative audio. FMD calculates the distance between distributions of reference and generated symbolic music embeddings, capturing abstract musical features. We validate FMD across several datasets and models. Results indicate that FMD effectively differentiates model quality, providing a domain-specific metric for evaluating symbolic music generation, and establishing a reproducible standard for future research in symbolic music modeling.", "sections": [{"title": "Introduction", "content": "Generative symbolic music has emerged as a significant area of research within artificial intelligence, aiming to autonomously create structured, coherent, and expressive compositions in symbolic formats such as MIDI. Unlike audio-based music generation, which directly produces sound waveforms, symbolic music generation operates at an abstract, event-based level, encoding musical elements like pitch, duration, dynamics, and timing. The advancements of neural architectures, particularly recurrent neural networks, variational autoencoders and transformers, has accelerated the development of models capable of learning and generating complex symbolic sequences that exhibit stylistic diversity and musicality (Sturm et al. 2016) (Huang et al. 2018) (Shih et al. 2022). However, evaluating the quality of generated symbolic music remains challenging due to the multifaceted nature of musical perception and the lack of objective, domain-specific metrics (Ji, Yang, and Luo 2023).\nPrevious evaluation approaches often rely on subjective judgments or low-level statistical metrics, which can be labor-intensive and may not capture the full depth of musical coherence and diversity. To address this gap, we propose the Frechet Music Distance (FMD), a metric specifically designed for symbolic music, providing a scalable and reproducible standard for evaluating generative models in this field.\nOur contributions may be summarized as follows:\n\u2022 we introduce FMD, a new metric for evaluating generative symbolic music;\n\u2022 we evaluate FMD with several experiments and show its potential for capturing musical characteristics and usefulness in qualitative measurments of symbolic music that were previously not available;\n\u2022 we release a Python toolkit for computing FMD.\nTo the best of our knowledge, FMD is the first metric designed specifically to evaluate symbolic music generation by comparing distributions of embeddings aimed at capturing essential musical features. Unlike existing metrics that may rely on surface-level statistical comparisons or require extensive human evaluation, FMD leverages learned embedding spaces to provide an objective, reproducible assessment of musical quality and diversity, tailored to the requirements of symbolic music."}, {"title": "Related work", "content": ""}, {"title": "Frechet Distance metrics", "content": "Frechet distances (Dowson and Landau 1982) are computed between multivariate Gaussians. In the context of machine learning evaluation, the distributions $\\mathcal{N}(\\mu_r, \\Sigma_r)$ and $\\mathcal{N}(\\mu_t, \\Sigma_t)$ are estimated from two sets of embeddings: the reference set and the test set. The reference set typically represents a ground truth or real-world data distribution, while the test set is often generated by a generative model. These embeddings are derived using a pre-defined reference representation model, as illustrated in Figure 1. The Frechet distance is then calculated as:\n\nFD = ||\u03bc\u03b3 - \u03bct ||\u00b2 + Tr (\u03a3 2 + \u03a3 2 - (\u03a3\u03b3\u03a3\u03c4\nwhere Tr is the matrix trace.\nEvaluating generative models presents unique challenges due to the need to measure not only statistical accuracy but also abstract qualities like coherence and perceptual realism. The Frechet Inception Distance (FID) was proposed as an objective metric for image generation, utilizing deep learning-based embeddings to compare real and generated images in a feature space, capturing both fidelity and diversity (Heusel et al. 2017). In the domain of audio, the Frechet Audio Distance (FAD) extended this approach to evaluate"}, {"title": "Frechet Music Distance", "content": ""}, {"title": "Symbolic music embedding models", "content": "FMD leverages recent pioneering advancements in symbolic music representation learning, specifically the music encoders from CLaMP (Wu et al. 2023) and CLaMP 2 (Wu et al. 2024) which capture rich semantic musical representations of symbolic music. CLaMP is a contrastive music-text self-supervised model for symbolic music representation learning. It was trained on 1.4 million music-text pairs and tested on semantic search and classifiaction tasks. It also introduced the M3 music encoder for ABC symbolic music with bar patching, trained in a setting based on masked autoencoders (He et al. 2022). CLaMP 2 includes MIDI support and enhanced, multilingual text descriptions, allowing for improved alignment between music and several languages. The music encoder in CLaMP 2 supports ABC and uses a interleaved representation for mult-track symbolic music. It also supports MIDI, utilizing an intermediate, lossless representation called MTF (MIDI Text Format), which avoids common quantization issues often found in MIDI encoding."}, {"title": "Data preprocessing", "content": "For CLaMP 1, we use an ABC pre-processing pipeline adapted directly from (Wu et al. 2023). In the case of MIDI and CLaMP 2, we convert MIDI to an intermediate representation of MTF (MIDI Text Format), which allows for lossless representation of MIDI data as text. In the case of ABC and CLaMP2, we find that representative and consistent results require specific preprocessing, especially for files from MIDI conversion and single-voice generative models. This involves removing empty spaces at the start of lines and adding voice information to enable voice-interleaved formatting, as outlined in (Wu et al. 2024). However, for single-track outputs from simple generative models, this processing"}, {"title": "Experimental Evaluation", "content": "To evaluate FMD, we investigate several comparisons and compute three variants of FMD, with the subscript denoting the embedding model, and superscript denoting the modality: $FMD^{midi}_{clamp2}, FMD^{abc}_{clamp2}, FMD^{abc}_{clamp1}$."}, {"title": "Dataset benchmarks", "content": "As a measure of initial validation of our approach, we compute FMD on subsets of the same symbolic dataset. We use MAESTRO (Hawthorne et al. 2019), which is a dataset of virtuoso-level piano performances known for their high complexity and fluent rhythms, typical for classical piano music. It includes pieces by composers like Chopin, Liszt and Rachmaninoff. We also investigate MidiCaps (Melechovsky, Roy, and Herremans 2024), a recent large dataset of diverse MIDI paired with text descriptions."}, {"title": "Evaluation of generative models", "content": "We evaluate three symbolic generative models: MMT (Dong et al. 2023), FolkRNN (Sturm et al. 2016), and a GPT-2 transformer model (Radford et al. 2019). MMT is a recent multitrack MIDI transformer model. FolkRNN is a recurrent neural net originally used to generate folk music in the ABC format. The GPT-2 model is used for generating ABC music, following a methodology similar to (Geerlings and Merono-Penuela 2020). The GPT-2 we use for evaluation is a scaled down, minimal model with 4 blocks with 4 attention heads each and an embedding size of 256. It's trained on the same ABC dataset as FolkRNN we will call this dataset \"folk v2\" in following sections.\nTo ensure reproducibility, we utilize the implementations and pre-trained models of FolkRNN and GPT-2 provided by Symbotunes (Skiers et al. 2024). We sample a total of 10,000 tracks both from the GPT-2 and from the FolkRNN model. We compute $FMD^{lamp2}_{midi}, FMD^{abc}_{clamp2}, and FMD^{abc}_{clamp1}$. In addition to evaluating the outputs of these models against their training set (folk v2), we also test them against external datasets not used for training: MidiCaps, MAESTRO, and POP909 (Wang et al. 2020). Given the unique characteristics of the folk v2 dataset, we anticipate low FMD values for this training set and higher values for the other datasets, which is reflected in the results. These findings are presented in Table 1."}, {"title": "Outlier detection using FMD", "content": "The unexpectedly high FMD values between the MAESTRO dataset, which features classical piano music, and the MidiCaps songs tagged as classical and limited to piano as the sole instrument, as reported in Table 3, prompted further investigation. To explore this, we randomly sampled 10 songs from the MidiCaps subset, synthesized them, and conducted a simple listening test. Although the test involved only five participants, all were musicians. The participants were asked to decide whether the track they are listening to \"is classical piano music\" and unanimously agreed that only one of the sampled songs clearly aligned with that definition, suggesting potential ambiguity in the classification of songs as \"classical piano\" within the MidiCaps dataset.\nSubsequently, we computed the per-song FMD between MAESTRO and individual songs from the MidiCaps Classical Piano subset and created a new set containing only songs"}, {"title": "Musical sensitivity tests", "content": "The original folk v2 dataset (Sturm et al. 2016) comprises transcriptions in four musical modes: major, minor, dorian, and mixolydian, with the major mode being the most prevalent, appearing in 67% of the samples. Analysis reveals that our FolkRNN model exclusively generates samples in the dominant mode of C Major, whereas the GPT-2 model produces outputs across all modes. To examine whether FMD captures this characteristic, we use a reference subset consisting solely of C Major samples from folk v2. Compared to the results in Table 1, we expect improved results for FolkRNN and potentially diminished results for GPT-2. The outcomes of this experiment are presented in Table 5.\nAnother experiment involved augmenting MIDI tracks with random noise. For each note, there was a probability p of it being augmented. The augmentation consisted of adding Gaussian noise to a property of the MIDI note. We tested augmentations of both Note Pitch and Note Veloc-"}, {"title": "Feature distribution estimation methods", "content": "To make the metric more robust when dealing with small sample sizes, we evaluated several mean and covariance estimation methods. The methods tested include Bootstrapping, Basic Shrinkage, Ledoit-Wolf Shrinkage (Ledoit and Wolf 2004), and Oracle Approximating Shrinkage (OAS) (Chen et al. 2010). We also attempted to utilize Graphical Lasso (Friedman, Hastie, and Tibshirani 2008) and Minimum Covariance Determinant (Rousseeuw 1984) (Rousseeuw and Driessen 1999); however, both methods encountered numerical stability issues, likely due to covariance matrices being near singular. The MLE was implemented in the same manner as in the FAD, using NumPy (Harris et al. 2020). The Bootstrap estimator implemented by us. For all other estimators we used Scikit-learn (Pedregosa et al. 2011).\nThe selected methods were benchmarked against the baseline Maximum Likelihood Estimator (MLE). CLaMP 2 was used as the embedding model. These methods were evaluated in the context of both the regular FMD and FMD-inf metrics. For the evaluations, we utilized subsets of the MidiCaps dataset, categorized according to tagged genres. Each genre class had two subsets: one with 10,000 samples and another with 1,000 samples, both obtained through sampling without replacement. For each pair of genre classes, FMD was calculated using a reference subset of size 10,000 from one class and a test subset of size 10,000 from another class. This value served as the ground truth. Subsequently,"}, {"title": "Limitations and further work", "content": "While FMD provides a new metric for assessing the similarity between generated and reference music distributions, we take note of its limitations that warrant further investigation. First, FMD's reliance on embedding models such as CLaMP2 introduces a dependency on the pretraining data and architecture of these models, which may bias the metric toward certain musical styles or genres. Additionally, the sensitivity of FMD to subtle musical variations, such as mode shifts or instrument choices, is not fully understood, as highlighted by experiments with subsets of datasets like folk v2 and MidiCaps. The metric may also struggle with ambiguous genre classifications or datasets with heterogeneous tagging systems, potentially leading to misleadingly high or low scores - however, by doing so, it may also highlight inconsistencies and provide insights that could drive improvements in dataset quality and tagging practices.\nSeveral experiments highlight the critical importance of data pre-processing, particularly for ABC data. Even minor modifications in data formatting can have a substantial effect on the outcomes. Each pre-processing step is also sensitive to the integrity of the information within the file. Fully utilizing a voice interleaved form (Wu et al. 2024) could potentially enhance some results, but we have found it frequently encountered errors when the input data was in any way corrupted or lacked the necessary information for interleaving.\nFuture work could focus on investigating and refining the FMD to account for temporal and structural elements of music. This includes developing task-specific variants and deepening the analysis of musical content within FMD. Furthermore, comparing FMD with FAD, specifically examining the correlations and differences across the same datasets in different modalities may deepen the insight into particular musical traits and characteristics.\nLastly, extensive subjective validation through listening tests with musicians and other individuals possessing musical expertise should complement FMD to ensure that its quantitative assessments align with human perceptions of musical similarity."}, {"title": "Conclusion", "content": "We introduce the Frechet Music Distance (FMD), a specialized adaptation of the Frechet family of metrics designed to evaluate generative symbolic music while accounting for abstract musical characteristics. We evaluate it on ABC and MIDI representations with current state of the art symbolic music embedding models. We also analyze and highlight some of the limitations of FMD.\nBy introducing FMD, we aim to establish a reproducible standard for the quantitative evaluation of generative models in symbolic music, facilitating more nuanced comparisons and supporting advancements in the field."}]}