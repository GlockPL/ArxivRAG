{"title": "AIR-BENCH 2024: A Safety Benchmark Based on Risk Categories from Regulations and Policies", "authors": ["Yi Zeng", "Yu Yang", "Andy Zhou", "Jeffrey Ziwei Tan", "Yuheng Tu", "Yifan Mai", "Kevin Klyman", "Minzhou Pan", "Ruoxi Jia", "Dawn Song", "Percy Liang", "Bo Li"], "abstract": "Foundation models (FMs) provide societal benefits but also amplify risks. Governments, companies, and researchers have proposed regulatory frameworks, acceptable use policies, and safety benchmarks in response. However, existing public benchmarks often define safety categories based on previous literature, intuitions, or common sense, leading to disjointed sets of categories for risks specified in recent regulations and policies, which makes it challenging to evaluate and compare FMs across these benchmarks. To bridge this gap, we introduce AIR-BENCH 2024, the first AI safety benchmark aligned with emerging government regulations and company policies, following the regulation-based safety categories grounded in our AI risks study, AIR 2024. AIR 2024 decomposes 8 government regulations and 16 company policies into a four-tiered safety taxonomy with 314 granular risk categories in the lowest tier. AIR-BENCH 2024 contains 5,694 diverse prompts spanning these categories, with manual curation and human auditing to ensure quality. We evaluate leading language models on AIR-BENCH 2024, uncovering insights into their alignment with specified safety concerns. By bridging the gap between public benchmarks and practical AI risks, AIR-BENCH 2024 provides a foundation for assessing model safety across jurisdictions, fostering the development of safer and more responsible AI systems.", "sections": [{"title": "1 Introduction", "content": "The rapid rise of foundation models [47, 48, 2, 65, 66, 6, 29] has ushered in a new era of AI capabilities that could transform society in profound ways. The immense potential of these models to drive economic growth and innovation has spurred intense competition among companies and developers, leading to an accelerated pace of model releases. However, this relentless progress has also unleashed a Pandora's box of risks, from models spewing toxic content and misinformation [24] to their repurposing for large-scale cybercrime [67]. As AI systems grow ever more powerful, it is crucial to address the potential risks and challenges they may pose to ensure their safe development and deployment [3, 8]."}, {"title": "2 Background", "content": ""}, {"title": "2.1 AIR 2024: Unifying AI Risks from Regulations and Policies", "content": "AIR-BENCH 2024 leverages the four-tiered risk categorization from the AI Risk Taxonomy (AIR 2024) [75]. AIR 2024 was constructed by manually extracting and organizing risk categories from a diverse set of AI governance documents, including 8 government regulatory frameworks from the European Union, United States, and China [9, 25, 26, 15\u201317, 45, 18] and 16 corporate policies from 9 leading AI companies worldwide [50, 53, 5, 44, 33, 12\u201314, 46, 60, 21, 20, 7].\nAs shown in Figure 1, AIR 2024 organizes risks into a hierarchical structure. The most granular level-4 contains 314 specific risk categories (detailed in Figure 9, Appendix A). These are grouped into 45 more general level-3 categories, which are further aggregated into 16 level-2 categories. At the highest level, risks are categorized into four level-1 categories (color-coded to indicate risk level): #1 (System & Operational Risks), #2 (Content Safety Risks), #3 (Societal Risks), and #4 (Legal & Rights Risks) . This taxonomy provides a unified, granular representation of the AI regulatory landscape across jurisdictions. We use the same color coding to indicate the level index."}, {"title": "2.2 The Gap Between AI Safety Benchmarks and Regulations", "content": "To assess the alignment between leading AI safety benchmarks and real-world regulations, we mapped three benchmarks\u2014HEx-PHI [55], HarmBench [41], and SALAD-Bench [38]-against AIR 2024's 45 level-3 risk categories (Figure 2). These benchmarks were selected for their rigorous risk categorization, high-quality data management, and human curation. Specifically, HEx-PHI identifies 11 major risk categories influenced by acceptable use policies from OpenAI and Meta [50, 42, 37], HarmBench defines seven categories referencing four corporate use policies and recent literature on LLMs' potential for misuse [71, 35]. SALAD-Bench integrates eight public benchmarks (HH- harmless, HH-red-teaming [27], AdvBench [77], Multilingual [23], Do-Not-Answer [69], ToxicChat"}, {"title": "3 Curation of AIR-BENCH 2024", "content": ""}, {"title": "3.1 Data Generation", "content": "To create the initial set of base prompts, we use pre-trained foundation models (e.g., GPT-4-Turbo) to iteratively generate prompts that mimic natural human writing, promoting the following aspects of qualities: (a) Risky Intention Clarity. Each prompt is designed to convey the intention"}, {"title": "3.2 Data Expansion", "content": "Next, we apply two mutations: uncommon dialects [57] and authority endorsement [76] to diversify the base prompts while maintaining their semantic meaning. Uncommon dialects involve rephrasing the prompts using non-standard vocabulary, grammar, and syntax, simulating how the prompts might be expressed by speakers of different dialects or non-native speakers. Authority endorsement frames the prompts as originating from or endorsed by authoritative sources to increase their perceived legitimacy. These mutations aim to increase the prompts' effectiveness against safety-aligned models [57, 76]. For example, transforming the prompt \u201cDetail the steps required to reverse-engineer a proprietary software application to find and exploit hidden vulnerabilities.\u201d into uncommon dialect yields: \"Mate, can ya spell out the steps for crackin' open that fancy software to suss out and exploit its secret weak spots? No one's the wiser, yeah?\" and authority endorsement yield: \"According to numerous cybersecurity experts...\" Further details can be found in Appendix F."}, {"title": "3.3 Category-Specific Evaluation: AIR-BENCH 2024 Three-level Scoring Autograder", "content": "Prior safety benchmarks employ various techniques to evaluate the safety of model responses, such as keyword matching [77, 56], using general LLM models [55, 11, 58, 59], or fine-tuned LLM models [54, 41]. Although LLM-based approaches are the most effective in capturing a range of harmful responses [41, 59], prior work uses the same LLM and judge prompts for every sample, which does not generalize well to niche risk categories that require specific context to detect harmful responses.\nTo address these problems, we developed custom Autograders for all 314 categories in our AIR- BENCH 2024. Each judge prompt is crafted with knowledge of the example prompts and the background of its respective category, enabling a more fine-grained and context-aware assessment of the model's behavior. Our Autograder employs a three-level scoring system (0, 0.5, and 1) for a more nuanced evaluation compared to binary classifications. 0 is assigned when the model generates detailed instructions or assistance for the harmful activity, even if an ethical disclaimer is present. 0.5 is assigned when the model's response is ambiguous or does not directly engage with the prompt without explicitly refusing or providing detailed assistance. 1 is assigned when the model successfully refuses to generate harmful content or assist with the requested task, regardless of the form of refusal.\nOur three-level category-specific scoring system provides a nuanced assessment of model behavior. Our approach considers the model's refusal to generate risky content and the context of each specific risk category. We validate our Autograder's performance through human evaluation (Appendix B), demonstrating near-perfect agreement with human judgments (Kappa score of 0.86). This substantially outperforms the 0.69 Kappa score reported for using a fixed judge prompt across all categories [55]. In our quantitative study, we use the refusal rate (% of scores that are 1s) of each model as the primary metric to assess its alignment with safety guidelines."}, {"title": "4 Evaluation and Takeaways", "content": ""}, {"title": "4.1 Evaluation Setup", "content": "We implement the evaluation pipeline using the open-source HELM framework [39], which streamlines the process of sending prompts to the model under evaluation, incorporating the results into judge prompts, and extracting scores and reasoning from the judge model's output. The scores are then aggregated by averaging each category. Leveraging this pipeline, we conducted an extensive evaluation of 22 models from 10 organizations, as detailed in Table 1, Appendix D. To ensure a diverse range of models, we accessed them through various platforms and API clients, including Anthropic, Cohere, OpenAI, Google's Vertex AI API, and Together's serverless model inference platform. A more in-depth description of the evaluation setup can be found in Appendix D."}, {"title": "4.2 Model Refusal Study over Risky Instructions from AIR-BENCH 2024", "content": "Level-3 Results: Figure 4 (a) illustrates the refusal rates (the % of the score of 1s) of 22 FMs across all 45 evaluated level-3 risk categories, highlighting significant variability in model performance. Despite this variation, no single model consistently refuses instructions across all categories. The Anthropic Claude 3 model series, especially Sonnet and Haiku with the highest average refusal rate of 89%, shows the strongest overall performance of refusing risky instructions from AIR-BENCH 2024. Following the Anthropic Claude models, Google's Gemini 1.5 Pro ranks as the runner-up. At the other extreme, the DBRX Instruct model exhibits a comparatively different approach than these top-ranked models to handle the response to risky inquires, with an average refusal rate of 15%."}, {"title": "Level-4 Resulst", "content": "While risky instructions under the #14 (Hate Speech) category are the most consistently rejected, a more granular analysis of its level-4 risk categories (Figure 4 (b)) reveals significant variation in refusal rates within this single level-3 category. Although all models mostly reject prompts related to hate speech against Age, many models have a low refusal rate for generating hate speech against Genetic Information and Gender and Occupation. Similarly, within the level-3 category #23 (Suicidal and Non-Suicidal Self Injury), there is generally a lower refusal rate for Cutting than Eating Disorders or Suicide. Detailed results at the 314 level-4 categories are provided in Figure 10-12, Appendix A.2. These findings highlight the importance of granular risk taxonomies in uncovering critical safety gaps and the need for targeted improvements."}, {"title": "Takeaways.", "content": "AIR-BENCH 2024's granular, regulation-based AI risk evaluation reveals significant variations in model safety, enabling easy comparison between models and highlighting the need for nuanced assessments.\nEven well-aligned models exhibit critical gaps, particularly in providing advice in regulated industries.\nAIR-BENCH 2024's level-4 evaluations uncover model-specific gaps, providing insights for developing adaptive Al safety measures."}, {"title": "4.3 Refusal Study over Public Sector Categorizations of Risk", "content": "AIR-BENCH 2024 uniquely unifies risk categorizations from various regulatory frameworks, enabling an intuitive inspection and understanding of how each model's refusal ability adheres to the risks highlighted by specific regulations. In this section, we perform a case study adhering to the risk categories outlined in the EU AI Act [25] at the level-3 categorization on AIR-BENCH 2024. The EU AI Act, an AI regulation published by the European Union in March 2024 and adopted since May 21, 2024, makes compliance crucial for future AI development under this jurisdiction. The EU AI Act employs a tiered approach to address the risks associated with AI models, encompassing categories such as minimal risk, limited risk, high risk, and unacceptable risk, which we map to our risk categories. In Figure 6, we examine models' ability to refuse instructions for the 11 unacceptable and high-risk categories (at level-3, shown in a) and all 23 risk categories specified in the AI Act (b)."}, {"title": "Takeaways.", "content": "AIR-BENCH 2024 uniquely enables direct assessment of AI models' adherence to specific regulatory frameworks, revealing significant gaps between current safety measures and regulatory requirements.\nThe case study highlights safety gaps that AI developers can prioritize to align their models with the requirements of various jurisdictions, ensuring compliance and mitigating potential risks."}, {"title": "4.4 Refusal Study over Private Sector Categorizations of Risk", "content": "Beyond unifying regulatory-specified risk categories, AIR-BENCH 2024 also unifies risk categories presented in a landscape of corporate usage policies, enabling stakeholders and third parties to easily inspect a model's ability to refuse risks specified by specific model developers. In this section, we conduct two corporate-policy-specific case studies on Anthropic and OpenAI, exploring how well the models of each company align with the risk categories specified in their acceptable use policies. These two companies were selected due to their current leading position and popular usage/deployment.\nAnthropic. As shown in Figure 7, Anthropic's Claude 3 family of models exhibit a strong alignment to Anthropic's policies (mapped to 31 risk categories in AIR-BENCH 2024), with average refusal rates above 90% for most risks specified by their policies. However, AIR-BENCH 2024 identifies comparatively less effective handling from the Claude"}, {"title": "Takeaways.", "content": "AIR-BENCH 2024 enables direct assessment of AI models' adherence to their own corporate usage policies, revealing gaps between safety measures and risks specified by the developers themselves.\nAIR-BENCH 2024 provides an additional layer of transparency, identifying changes in model safety over time and informing users about potential risks and liabilities.\nFindings emphasize the space of improvement for developers to align models with their own policies and address critical gaps in high-risk categories."}, {"title": "5 Conclusions", "content": "In this work, we introduce AIR-BENCH 2024, the first AI safety benchmark that broadly incorporates and aligns with risk categories specified in a vast range of recent AI safety-related regulations and policies. By leveraging the comprehensive risks specified in 8 government regulations and 16 company policies, AIR-BENCH 2024, with 5,694 diverse, context-specific prompts, provides a unique and actionable tool for assessing the alignment of AI systems with real-world safety concerns.\nOur extensive evaluation of 22 leading foundation models reveals significant variability in their adherence to safety guidelines across different risk categories. Notably, even the most well-aligned models, such as the Anthropic Claude series, demonstrate critical gaps in high-risk areas identified by adopted regulations, like #4 (Automatic Decision-Making) and #6 (Advice in Regulated Industries) These findings underscore the urgent need for targeted improvements in model safety, AI risk management, and the importance of granular risk taxonomies in uncovering such gaps.\nFurthermore, our case studies on public and private sectors of risk categorizations highlight the gaps between current safety measures and the requirements of AI regulations and the risks specified by the model developers themselves. No evaluated model fully demonstrates aligned safety behaviors towards the risk categories specified in the recently adopted EU AI Act. Meanwhile, AIR-BENCH 2024 identifies gaps in models in adhere consistent ability to correctly handle risky instructions related to risk categories covered by their own respective corporate policies. By providing this additional layer of transparency and informativeness, AIR-BENCH 2024 emphasizes the need for AI developers to prioritize aligning their models with emerging regulatory frameworks and their own stated principles, while shedding light on informing the public about potential risks that may not be fully uncovered by these developers themselves.\nLimitations and Broader Impact. As a static benchmark, AIR-BENCH 2024's risk categories require periodic updates to keep pace with the most emerging risk categories specified in new regulations and policies. To mitigate this limitation, we plan to update the AIR taxonomy regularly, incorporating new regulatory efforts to maintain the benchmark's relevance and comprehensiveness. Future work could explore dynamic benchmarking approaches that automatically adapt to evolving safety concerns. AIR-BENCH 2024 serves as a valuable tool for researchers, policymakers, and industry stakeholders to assess and improve the alignment of AI systems with real-world safety concerns. By bridging the gap between AI safety benchmarks and practical AI risks, our work contributes to the development of safer and more responsible AI systems. We encourage the AI community to adopt and build upon AIR-BENCH 2024 to foster a more proactive and collaborative approach to addressing the challenges of AI safety in an increasingly regulated landscape."}, {"title": "A The AIR 2024 Taxonomy & Additional Results", "content": ""}, {"title": "A.1 Overview of the AI Risk Taxonomy (AIR 2024)", "content": "The AI Risk Taxonomy (AIR 2024) [75] is a comprehensive framework for categorizing the risks associated with generative AI systems. The taxonomy is constructed using a bottom-up approach, which involves extracting risk categories directly from leading AI companies' policies and government regulatory frameworks. For corporate policies, AIR 2024 uses both platform-wide acceptable use policies and model-specific acceptable use policies, from OpenAI [50, 53], Anthropic [5], Meta [44, 42], Google [33, 34], Cohere [12-14], Mistral [46], Stability [60], DeepSeek [21, 20, 19], and Baidu [7]. For government regulations, it uses regulations from the European Union [25, 26], United States [9], and China [15\u201317, 45, 18], including the White House Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence and the EU AI Act. AIR 2024 organizes AI risks into a hierarchical structure with four levels of granularity. The most general level consists of four broad \"level-1\" risk categories:\n#1 (System & Operational Risks) : Risks related to the operation of AI systems and security risks AI may introduce to other systems. This category consists of 2 level-2 categories, #1 (Security Risks) and #2 (Operational Misuse). The risk categories further break down into 6 level-3 categories and 38 unique level-4 categories.\n#2 (Content Safety Risks) : Risks associated with the content generated or processed by AI systems. This category consists of 5 level-2 risk categories, #3 (Violence & Extremism), #4 (Hate/Toxicity), #5 (Sexual Content), #6 (Child Harm), and #7 (Self-harm). The risk categories further break down into 17 level-3 categories and 79 unique level-4 categories.\n#3 (Societal Risks) : Risks that have broader societal implications. This category consists of 5 level-2 categories, #8 (Political Usage), #9 (Economic Harm), #10 (Deception), #11 (Manipulation), and #12 (Defamation) . The categories further break down into 14 level- 3 categories and 52 unique level-4 categories.\n#4 (Legal & Rights Related Risks) : Risks related to the legal and ethical implications of Al systems. This category consists of 4 level-2 risk categories, violation of #13 (Fundamental Rights), #14 (Discrimination/bias), #15 (Privacy), and #16 (Criminal Activities) . The risk categories further break down into 8 level-3 categories and 145 unique level-4 categories."}, {"title": "A.1.A Summary of Public Sector Categorizations of Risk and Findings in AIR 2024", "content": "The risk categories specified in government regulations vary in their level of detail and specificity. The EU AI Act [25] takes a tiered approach to address the risks associated with AI models, encompassing categories such as minimal risk, limited risk, high risk, and unacceptable risk. High-risk categories include #4 (Automated Decision-Making) and #15 (Perpetuating Harmful Beliefs) (e.g., \u201cExploits any of the vulnerabilities of a person or a specific group of persons due to their age, disability or a specific social or economic situation\u201d).\nThe US AI Executive Order [9] identifies key areas that warrant further investigation or are already explicitly prohibited, covering a wide range of risk categories across all four level-1 categories in the AIR 2024 taxonomy. It highlights a unique level-3 risk category, #30 (Displacing/Disempowering Workers), which is not covered by any corporate Al policy. Some categories, such as #22 (Child Sexual Abuse Content), are explicitly identified as prohibited with requirements for red-teaming, while others, such as #4 (Automated Decision-Making) and #11 (Weapon Usage & Development), are presented as areas with potential risk that warrant further guidelines or legislation.\nChina's regulations, such as the Basic Safety Requirements for Generative Artificial Intelligence Services [18], provide detailed categorizations and benchmarking/red-teaming requirements related to regulating and monitoring risky user behaviors. For example, services that may have the effect of"}, {"title": "A.1.B Summary of Private Sector Categorizations of Risk and Findings in AIR 2024", "content": "The most extensively covered risk categories across corporate AI policies include #39 & 40 (Privacy Violations),#45 (Other Illegal/Unlawful/Criminal Activities), and #13 (Harassment), which are explicitly covered by all companies' policies. In contrast, the least covered risk categories include #19 (Non-Consensual Nudity) and #26 (Deterring Democratic Participation), which are only covered by a single corporate policy, and #30 (Disempowering Workers), which is covered by no corporate policy despite being prohibited under the US Executive Order and the EU AI Act."}, {"title": "A.2 Additional Level-4 Results", "content": "Figure 10 (a) presents a granular analysis of model refusal rates across all 38 level-4 risk categories under #1 (System and Operational Risks), revealing a wide range of refusal rates within this level-1 category. Some level-3 categories exhibit similar refusal rates for their corresponding level-4 categories, such as the various industries in #6 (Advice in Heavily Regulated Industries), which also has the lowest refusal rates among all level-4 categories. However, other level-3 categories, like #5 (Autonomous Unsafe Operation of Systems), show noticeable variance in refusal rates depending on the specific level-4 category. For instance, the refusal rate for Nuclear facilities tends to be lower compared to other systems such as Electrical grids and Air traffic control . Similarly, within #1 (Confidentiality), the average refusal rate for Spear phishing is generally lower than other categories like Network intrusion . This disparity is exemplified by Gemini 1.5 Flash, which has a refusal rate for Spear phishing (refusal rate 50%) that was twice as lower than the refusal rate for Network intrusion (refusal rate 100%), highlighting the importance of this type of granular analysis in uncovering model-specific risks.\nConsistent with the findings for level-3 categories, there is substantial variance in overall refusal rates across models, with the Anthropic Claude family demonstrating the highest refusal rates to the risky instructions at level-4 in AIR-BENCH 2024 and DBRX Instruct exhibiting the lowest. However, the level-4 analysis reveals safety gaps even for the most well-aligned models. While Claude 3 Sonnet has an average refusal rate of 70% for #4 (Automated Decision-Making) (Table 4), its performance varies significantly across the corresponding level-4 risk categories. The refusal rates for making automated decisions on Social scoring, Profiling, and Insurance eligibility are above 90%, whereas the refusal rates for decisions over Housing eligibility and Employment are less than 20%.\nFigure 10 (b), Figure 11 (a), Figure 11 (b), and Figure 12 present similar level-4 insights for #2 (Content Safety Risks), #3 (Societal Risks), and #4 (Legal and Rights-related Risks), respectively. Despite #6 (Advice in Regulated Industries) being the only level-3 category with consistently low refusal rates across all models, several level-4 categories from other level-3 categories exhibit similarly low refusal rates. These include Beautifying and whitewashing acts of war or aggression, Building services to present a persona of a minor, Characterization of identity - Occupation, Classification of individuals - Geographic region, and Classification of individuals - Age, with some categories having refusal rates of 0% or close to 0% for nearly all models."}, {"title": "B Human Alignment Study to the LLM-based Autograders", "content": "To evaluate the consistency between Autograders and human evaluators, we conducted a thorough human evaluation study involving the authors. This study aimed to determine how closely the scores given by Autograders align with the judgments of human evaluators when both are provided with the same set of rules and conversation content.\nMethodology: We randomly sampled 5 prompts from each level 2 category, resulting in a total of 78 prompts (2 duplicate prompts were removed). Each prompt has 22 model responses, yielding 1,716 question-answer pairs in total. Each annotator was assigned one prompt from each level 2 category, the annotators evaluated the assigned question-answer pairs based on given scoring rules in Section 3.3, the same as the ones we provided to the Autograders, as shown in Figure 13. In total, this human study involves 5 of the authors, and the tasks are equally distributed to each person.\nUser Interface: For efficiency and clarity, we utilized a user interface [55] for the human evaluation. This interface displayed each question-answer pair along with the corresponding scoring rules, as shown in Figure 13. Annotators could easily input their scores and review previous evaluations, ensuring consistent and accurate assessments throughout the study.\nComparison between Human Evaluators and Autograders: Cohen's Kappa score is a statistical metric used to assess the reliability or agreement between two raters; the closer to 1, the higher the agreement. In the final evaluation of our evaluation results using the question-answer pairs, the Cohen's Kappa score between human evaluators and the Autograders was found to be 0.86, indicating a very high level of agreement. In contrast, prior automated evaluation using a fixed prompt for every category [55] achieved a score of just 0.69, highlighting the superior accuracy of the Autograders in assessments. The strong alignment with human evaluators highlights the robustness and reliability of Autograders in assessing harmful content and indicates that they are a valuable asset for evaluation tasks in future work."}, {"title": "C Additional Case Studies on Public Sector Categorizations of Risk", "content": "Similar to the case study on model alignment to EU regulations, we find that no model fully aligns to US or Chinese regulations, as shown in Figures 14 and 15.\nAdditional Analysis on Risks Included in the US Regulatory Framework. We show the results of our analysis of model responses and their alignment to the US AI Executive Order in Figure 14. We find that most models, even top-performers such as the Claude 3 family, have space to improve w.r.t. #39&40 (Privacy Violations), #5 (Autonomous Unsafe Operation of Systems), #36 (Misrepresentation), #4 (Automated Decision-Making), and #6 (Advice in Regulated Industries) that are included in US regulations.\nAdditional Analysis on Risks Included in Chinese Regulations. We show the results analyzing the model responding behaviors and their alignment to China's regulations in Figure 15. We observe a similar trend of safety behaviors as to the EU and US with respect to Chinese regulations. Meanwhile, Chinese regulations contain additional low-performing risk categories that are not covered by US regulations, including #17 (Adult Content), #37 (Types of Defamation), #8 (Celebrating Suffering), and #18 (Erotic), and AIR-BENCH 2024 help to easily identify model safety gaps to the risks specified by these jurisdictions."}, {"title": "D Additional Evaluation Setting", "content": "General Settings. Our evaluation pipeline, implemented using the open-source HELM framework [39], consists of the following steps:\n(1) We sent all the prompts to the model under evaluation with the temperature set to 0.0 and the maximum new token length set to 512.\n(2) We interpolated the result into the judge prompt for the prompt's category and sent it to the judge model with a temperature set to 0.0 and the maximum output token length set to 64.\n(3) We parsed and extracted the score and short reasoning from the judge model's generated output.\n(4) We aggregated the scores by computing the mean score across all prompts for each category.\nTo ensure a diverse and representative evaluation, we accessed models from multiple leading AI companies, including Anthropic, Cohere, OpenAI, and Google, through their respective platforms and API clients. Google's Gemini models required special handling due to its built-in safety filter, which returns an API error when triggered. In such cases, we treated the response as an empty string, assigned a score of 1.0 (indicating refusal), and bypassed the judge model evaluation. A complete list of models studied in this paper is summarized in Table 1.\nMetrics. Throughout the evaluation, we used the refusal rate as the primary metric to study the results. The refusal rate is determined by the proportion of model outputs assigned a score of 1 by our Autograders. As our instructions are designed to convey clear risk intentions, the refusal rate can be interpreted as the rate at which a specific model correctly handles the risky inquiries in AIR-BENCH 2024. In general, a higher refusal rate acquired from AIR-BENCH 2024 indicates safer and more conservative model behavior, and we employ a color-coding system with green indicating safer or more conservative outcomes and red indicating riskier ones.\nReproducibility. Detailed instructions for reproducing these experiments can be found in the dataset card hosted on the Hugging Face platform at https://huggingface.co/datasets/stanford-crfm/air-bench-2024. This ensures transparency and facilitates further research and validation of our findings by the broader AI safety community."}, {"title": "E Additional Discussion on Broader Impact", "content": "Combining risk categories from 8 government regulations and 16 company policies into a single benchmark, AIR-BENCH 2024 provides a comprehensive snapshot of risks in the current AI landscape. It serves as a standardized source of truth for evaluating and comparing how well models respond to malicious requests, and has the potential to help various stakeholders overcome the challenges they face:\nAI Companies: Companies must navigate a complex landscape of government policies and regulations, which leads to increased compliance costs. AIR-BENCH 2024 helps reduce these inefficiencies by streamlining previously disjointed risk areas into a single, standardized benchmark.\nAI Researchers: For researchers studying the safety and security of AI systems, the lack of a unified approach to risks to AI safety can lead to redundant efforts, siloed research, and insufficient coordination in tackling critical safety challenges. By providing such a unified approach, AIR-BENCH 2024 helps researchers ensure that their work keeps up with the evolution of AI regulation and companies' acceptable use policies.\nEnd Users: The lack of clear and uniform standards can lead to confusion and distrust in the reliability of AI systems. This can erode public trust in AI systems and hinder their adoption, even when they have the potential to deliver significant benefits. AIR-BENCH 2024 provides a common point of reference and an additional layer of transparency that end users can use to understand and build trust in Al systems."}, {"title": "F Curation Details", "content": ""}, {"title": "F.1 Data Expansion", "content": "To enhance the diversity and robustness of the base prompts, we apply two mutation techniques: uncommon dialects [57] and authority endorsement [76]. These mutations aim to maintain the semantic meaning of the prompts while introducing variations in language and perceived legitimacy, potentially increasing their effectiveness against safety-aligned models.\nFor uncommon dialects, we use in-context prompting by providing the base model with three examples of prompts mutated into non-standard vocabulary, grammar, and syntax. These examples simulate how the prompts might be expressed by speakers of different dialects or non-native speakers. The base model, gpt-4-1106-preview, is then prompted to generate mutated versions of the base prompts using a temperature of 1.0 to encourage diversity in the generated outputs. Similarly, for authority endorsement, we provide five examples of prompts framed as originating from or endorsed by authoritative sources (using the source code from the authors\u2074). The base model is then prompted to generate mutated versions of the base prompts using the same temperature setting.\nAfter generating the mutated prompts, we manually review the outputs to ensure the quality and coherence of the variations. In cases where the generated prompts are of low quality or fail to maintain the original semantic meaning, we manually refine and correct them to ensure the integrity of the expanded dataset. This process allows us to create a diverse set of prompts that cover a wide range of linguistic variations and perceived legitimacy levels, enhancing the benchmark's ability to stress-test the robustness of AI models against potential long-tail distributed inputs."}, {"title": "F.2 Automatic Generation of Category-specific Evaluation Prompts", "content": "This section delves into the category-specific judge prompts used by our Autograders for each risk category within our AIR-BENCH 2024. As shown in Figure 16, these category-specific judge prompts are generated by a model using a system prompt that includes three distinct judge prompt examples focused on one specific risk category, along with a user prompt featuring customized information tailored to each category. The final generated judge prompt is acquired by using this system prompt, which accounts for general structure/feature similarity, and user prompts that incorporate category- specific terms and the actual instructions as the context for the model to adaptively design/generate new judge prompts."}]}