{"title": "MBA-RAG: a Bandit Approach for Adaptive Retrieval-Augmented Generation through Question Complexity", "authors": ["Xiaqiang Tang", "Qiang Gao", "Jian Li", "Nan Du", "Qi Li", "Sihong Xie"], "abstract": "Retrieval Augmented Generation (RAG) has proven to be highly effective in boosting the generative performance of language model in knowledge-intensive tasks. However, existing RAG framework either indiscriminately perform retrieval or rely on rigid single-class classifiers to select retrieval methods, leading to inefficiencies and suboptimal performance across queries of varying complexity. To address these challenges, we propose a reinforcement learning-based framework that dynamically selects the most suitable retrieval strategy based on query complexity. Our approach leverages a multi-armed bandit algorithm, which treats each retrieval method as a distinct \"arm\" and adapts the selection process by balancing exploration and exploitation. Additionally, we introduce a dynamic reward function that balances accuracy and efficiency, penalizing methods that require more retrieval steps, even if they lead to a correct result. Our method achieves new state of the art results on multiple single-hop and multi-hop datasets while reducing retrieval costs. Our code are available at https://github.com/FUTUREEEEEE/MBA", "sections": [{"title": "1 Introduction and Related Work", "content": "Retrieval Augmented Generation (Lewis et al., 2020) have shown significant promise in addressing knowledge-intensive natural language processing (NLP) tasks by integrating a updated knowledge base with a language model. This combination allows language model access to retrieve up-to-date knowledge, improving their faithfulness and reducing hallucinations.\nMost existing RAG frameworks employ a retrieve-and-generate setup that indiscriminately performs retrieval based on the input. However, this approach may hinder the versatility of language models or introduce unnecessary or off-topic passages. To address this issue, methods such as SEAKR (Yao et al., 2024a) and FLARE (Jiang et al., 2023) have been designed to enable them perform active retrieval only when necessary.\nFurthermore, AdaptiveRAG (Jeong et al., 2024) argues that real-world queries often vary in difficulty, such as the number of reasoning steps required or the depth of information needed to answer a query. Applying a single retrieval method across all queries can be ineffective. Simple queries may incur unnecessary computational overhead when complex retrieval strategies are used, while complex, multi-step queries may not be adequately addressed. To tackle this issue, AdaptiveRAG introduces an adaptive router that selects the retrieval methods based on the complexity of the query.\nHowever, AdaptiveRAG simplifies the generation strategy into a single-choice task by using heuristic supervision that favors only one process with the least retrieval cost. This supervision is inaccurate for two main reasons. First, it assumes only one strategy is optimal for one query. For example, while multiple strategies (such as directly answering without retrieval, retrieving once, or performing iterative retrieval) might all provide correct answers depending on the query, only direct answer method will be marked as correct. This strict supervision ignores scenarios where more complex strategies might offer better context or more comprehensive answers. Second, this heuristic approach is ambiguous because retrieval costs vary according to the query's difficulty; what constitutes the least cost can differ significantly between queries of different complexities.\nTo overcome these limitations, we propose a Multi-arm Bandit-based framework for Adaptive Retrieval-Augmented Generation (MBA-RAG) Fig. 1, that introduces both flexibility and cost-awareness into the generation process.\nFirst, to address the rigidity of single-label supervision, we employ a multi-armed bandit algo-"}, {"title": "2 Method", "content": null}, {"title": "2.1 Preliminaries", "content": "We define a Large Language Modelas $LLM$, which takes a user query x as input and produces an output y. The RAG process (Lewis et al., 2020) includes a retrieval stage where the module R retrieves relevant documents D for a query x, and a generation stage where the LLM uses x and D to generate the response $\\hat{y} = LLM(y_t|x, D)$.\nUsing a single retrieval method for all queries may not always be effective.\nIn this section, we introduce a RAG framework based on a multi-arm bandit approach that dynamically selects the optimal generation method to balance generation quality and computational efficiency."}, {"title": "2.2 Query Encoding and Arm Selection", "content": "To overcome the limitations of attributing each query to a single generation method, we reformulate the generation selection as a trial-and-error reinforcement learning problem, where each generation method is treated as an action (i.e. an arm) in a multi-armed bandit framework.\nWe employ DistilBERT, a lightweight pre-trained language model (PLM), to encode user queries, generating an action distribution $z = f_\\theta(x)$. Directly supervise this output could cause the system to overly favor a single generation method, leading to sub-optimal local convergence.\nTo prevent this, we incorporate an epsilon-greedy strategy (Langford and Zhang, 2007) to balance the trade-off between exploration and exploitation (Auer, 2002; Auer et al., 2002). This ensures that the system primarily utilizes the best-performing generation methods while still exploring other options to improve long-term performance. Specifically, the \u201carm\u201d is chosen as $a = \\arg \\max(z)$ with probability $1 - \\epsilon$, or a random generation method with probability $\\epsilon$."}, {"title": "2.3 Learning Algorithm", "content": "After selecting a generation method, the model updates its parameters based on both the quality and the cost of the language model's response. Unlike traditional RAG systems that prioritize accuracy alone, our approach incorporates a fine-grained reward function that accounts for both accuracy and computational cost, thereby guiding the model to minimize unnecessary computational overhead.\nThe objective function, given by\n$\\min_\\theta (r_a - f_\\theta(x)_a)^2$ (1)\nis to minimize the squared error between the actual reward $r_a$ and the predicted reward $f_\\theta(x)$ for action a:\n$\\theta_{t+1} = \\theta_t - \\alpha \\nabla_{\\theta} ((r_a - f_\\theta(x)_a)^2)$\nHere, $\\theta_{t+1}$ represents the parameter set at iteration t + 1, $\\theta_t$ at iteration t, and $\\alpha$ is the learning rate. The reward $r_a$ for the selected retrieval method a is calculated as $r_a = A(y, \\hat{y}_a) \u2013 \\lambda C(a)$, where A is an generation quality metric (e.g., exact match defined as $A(y, \\hat{y}_a) = \\mathbb{I}{y = \\hat{y}_a}$ if the generated answer $\\hat{y}_a$ matches the ground truth y)., C(a) represents the computational cost of method a (such as the number of retrieval steps), and $\\lambda$ is a scaling factor balancing accuracy and efficiency."}, {"title": "3 Experiments", "content": null}, {"title": "3.1 Datasets", "content": "To demonstrate the performance of our method for queries of different complexities, following the work of Adaptive-RAG (Jeong et al., 2024), we selected three single-hop QA and three multi-hop QA datasets as our experimental datasets.\nFor Single-hop QA, we choose SquAD v1.1 (Rajpurkar et al., 2016), Natural Questions (Kwiatkowski et al., 2019) and TriviaQA (Joshi et al., 2017). These datasets consist of each query and its related articles that contain the answers.\nFor Multi-hop QA, we choose MuSiQue (Trivedi et al., 2022b), HotpotQA (Yang et al., 2018) and 2WikiMultiHopQA (Ho et al., 2020). For these multi-hop datasets, the answers cannot be directly obtained and require multi-step reasoning based on the information in the articles to arrive at the final answer."}, {"title": "3.2 Baseline Models", "content": "We selected a range of related models as baselines to compare with our model, including:\n1) No-Retrieval: Directly generates answers without performing retrieval. 2) Adaptive-Retrieval (Mallen et al., 2023): Dynamically determining whether retrieval is necessary. 3) Self-RAG (Asai et al., 2023): Dynamically determines whether retrieval is needed. 4) DRAGIN (Su et al., 2024): evaluating the uncertainty of each token to activate retrieval model. 5) SEAKR (Yao et al., 2024a) Introduce self-aware uncertainty to decide whether to activate the retrieval model based on its value. 6) Adaptive-RAG (Jeong et al., 2024): Using a classifier to dynamically choose the most suitable retrieval strategy based on the complexity of the query."}, {"title": "3.3 Metrics", "content": "Following the approach of Jeong et al. (2024), in addition to reporting performance metrics of the generated results such as Exact Match (EM), F1, and Accuracy (Acc), we also report efficiency metrics of the retrieval strategy, Step. EM measures whether the predicted result exactly matches the ground truth, F\u2081 measures the overlap of words between the predicted answer and the ground truth, and Acc indicates whether the predicted answer contains the ground truth. Step denotes the number of retrieval steps required by the selected retrieval strategy."}, {"title": "3.4 Experiment Settings", "content": "We use DistilBERT (Sanh et al., 2020) as the query encoding model. This model allows for efficient query encoding and ensures that the retrieval method selection is based on a robust representation of the query. The retrieval settings, dataset configurations, and generation model setups all follow the approach used by Adaptive-RAG (Jeong et al., 2024). We utilize BM25 as the retrieval model and FLAN-T5-XL (Chung et al., 2022) as the generation model. For the MBA-RAG retrieval, we employ the epsilon-greedy algorithm (Sutton and Barto, 1998) as the action strategy. The learning rate for the encoding model is set as 5e-5."}, {"title": "3.5 Main Results", "content": "We report the average results across six datasets as shown in Table 1, and the results for each individual dataset as shown in Table 2. Note that while we focus on performance metrics such as EM,F1, and Acc, we also consider efficiency metrics like Step. The experimental results demonstrate that our method achieves a balance between the two, surpassing the baseline in performance metrics while requiring relatively fewer steps.\nAs shown in Table 2, our method has achieved performance improvements on all single-step datasets, with significant gains observed on the"}, {"title": "4 Conclusion", "content": "In this study, we introduced an adaptive retrieval-augmented generation framework using a multi-armed bandit approach that dynamically selects"}, {"title": "Limitations", "content": "Our reinforcement learning-based approach for adaptive retrieval in language models demonstrates promising results; however, it is not without limitations. The framework's dependence on the specific structure of the multi-armed bandit algorithm can introduce challenges in scalability and adaptability to new, unseen query types. Future work could explore more efficient algorithms that maintain performance while reducing computational demands."}, {"title": "A Experiments Appendix", "content": null}, {"title": "A.1 Implementation Details", "content": "For the external document corpus, we use different sources depending on the dataset type: the Wikipedia corpus preprocessed by(Karpukhin et al., 2020) for single-hop datasets, and the preprocessed corpus by (Trivedi et al., 2022a) for multihop datasets."}, {"title": "A.2 Reward Setting", "content": "For single-hop and multi-hop datasets, due to the differences in their tendencies for retrieval strategy selection, the single-hop dataset tends to choose the \"zero\" and \"one\" retrieval strategies, while the multi-hop dataset favors the \"one\" and \"multiple\" strategies. Therefore, for the MAB algorithm, the same set of rewards cannot achieve optimal results on both types of datasets simultaneously. Consequently, during our experiments, we set two different groups of reward settings.\nFor the single-hop dataset, we adopted a dynamic reward approach, combining strategy selection with step costs to balance performance and efficiency, the rewards are set at 1, 0.9, and 1-step/10 respectively for the three retrieval strategies \u201czero,\u201d \"one,\u201d and \"multiple,\u201d. When none of the three options can answer the question, the reward is set to -1.\nFor the multi-hop dataset, rewards are set at 4.3, 2.3, and 1.15."}, {"title": "A.3 Multi-label Classification", "content": "Due to Adaptive-RAG defining the selection of retrieval strategies as a single-label classification problem, where multiple strategies can yield correct results, it selects the strategy with the fewest steps as the correct label, treating other choices as incorrect. This approach is somewhat unreasonable because the other choices are also correct, albeit with a higher step cost. Therefore, to make the classification choice more rational while considering step costs, we implemented a multi-label classification setup, treating all retrieval strategies that could yield the correct answer as correct labels. We trained the classification model to predict multiple potential labels and selected the most likely label during the final inference."}, {"title": "A.3.1 Setting", "content": "In the experimental setup, for Multi-label classifier, we maintained consistency with the Adaptive-"}, {"title": "A.4 Results", "content": "The experimental results show that classifiers trained with multi-labels are highly prone to overfitting. Due to the high proportion of the \u201cmultiple\" labels, the final model tends to exclusively choose \"multiple.\" While this achieves good performance in metrics, the step cost is excessively high, even for single-step datasets. Such choices are overly inefficient. In contrast, our MBA-RAG is essentially a form of multi-label classification, and the dynamic reward settings enable the model to make the most rational choices based on the combination of step costs, making it more practical and feasible.\nAlthough the Adaptive-RAG-MultiLabel yields generally high results, this is because under this training mode, the model ultimately opts for \u201cmultiple\" labels. During the construction of the training set, a large number of samples are tagged with multiple labels, leading the model to completely overfit on the \"multiple\" label after training. However, this approach results in an excessively high number of steps, and most problems do not require multi-step retrieval."}]}