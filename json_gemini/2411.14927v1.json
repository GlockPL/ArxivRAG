{"title": "LiDAR-based End-to-end Temporal Perception for\nVehicle-Infrastructure Cooperation", "authors": ["Zhenwei Yang", "Jilei Mao", "Wenxian Yang", "Yibo Ai", "Yu Kong", "Haibao Yu", "Weidong Zhang"], "abstract": "Abstract\u2014Temporal perception, the ability to detect and track\nobjects over time, is critical in autonomous driving for main-\ntaining a comprehensive understanding of dynamic environ-\nments. However, this task is hindered by significant challenges,\nincluding incomplete perception caused by occluded objects\nand observational blind spots, which are common in single-\nvehicle perception systems. To address these issues, we introduce\nLET-VIC, a LiDAR-based End-to-End Tracking framework for\nVehicle-Infrastructure Cooperation (VIC). LET-VIC leverages\nVehicle-to-Everything (V2X) communication to enhance temporal\nperception by fusing spatial and temporal data from both vehicle\nand infrastructure sensors. First, it spatially integrates Bird's\nEye View (BEV) features from vehicle-side and infrastructure-\nside LiDAR data, creating a comprehensive view that mitigates\nocclusions and compensates for blind spots. Second, LET-VIC\nincorporates temporal context across frames, allowing the model\nto leverage historical data for enhanced tracking stability and\naccuracy. To further improve robustness, LET-VIC includes\na Calibration Error Compensation (CEC) module to address\nsensor misalignments and ensure precise feature alignment.\nExperiments on the V2X-Seq-SPD dataset demonstrate that LET-\nVIC significantly outperforms baseline models, achieving at least\na 13.7% improvement in mAP and a 13.1% improvement in\nAMOTA without considering communication delays. This work\noffers a practical solution and a new research direction for\nadvancing temporal perception in autonomous driving through\nvehicle-infrastructure cooperation.", "sections": [{"title": "I. INTRODUCTION", "content": "Temporal perception, which encompasses both detection\nand tracking, is crucial for advanced autonomous driving. It\nenables Connected and Autonomous Vehicles (CAVs) to con-\ntinuously identify and characterize objects while monitoring\ntheir dynamic changes over time, providing a detailed under-\nstanding of the dynamic environment. However, single-vehicle\ntemporal perception systems face significant challenges, par-\nticularly in complex urban settings where occlusions and\nblind spots can limit the field of view. These limitations\nhinder the ability of CAVs to obtain a complete and reliable\nunderstanding of their surroundings.\nVehicle-Infrastructure Cooperation (VIC), enabled by ad-\nvancements in Cooperative Intelligent Transportation Systems\n(C-ITS) and Vehicle-to-Everything (V2X) communication [1]-\n[3], offers a solution to the limitations of vehicle-only per-\nception. Through V2X, data can be exchanged seamlessly\nbetween CAVs and infrastructure, allowing the integration of\ninfrastructure-based perception information. This cooperation\nenhances situational awareness and fills the gaps caused by\nocclusions or restricted views on the vehicle side. In particular,\nVIC strengthens temporal perception by providing a more\nholistic, continuous view of the surroundings, as illustrated in\nFig. 1 and Fig. 2. By combining data from both vehicle and\ninfrastructure sensors, VIC offers more accurate and robust\ntemporal perception compared to standalone vehicle sensors.\nDespite the advantages of VIC, current VIC methods are\nlimited in their use of temporal information. Existing ap-\nproaches typically leverage temporal context only during the\ntracking stage, treating detection as an isolated task for each\nindividual frame. This frame-by-frame detection fails to fully\nexploit the sequential information across frames, which could\notherwise improve detection accuracy. Moreover, most VIC\napproaches focus on spatial data fusion, integrating multi-\nview data from vehicle-side and infrastructure-side sensors at\na single moment, without extending this fusion to encompass\ntemporal information. Consequently, these approaches are lim-\nited by their reliance on instantaneous data, missing out on\nthe benefits of historical context for more informed decision-\nmaking.\nAn end-to-end approach to temporal perception, which\nutilizes sequential data across frames for both detection and"}, {"title": "II. RELATED WORKS", "content": "vehicle-side and infrastructure-side sensors, delivering precise\nand robust 3D object perception. Rigorous testing on the V2X-\nSeq-SPD dataset [9], which captures challenging real-world\ndriving scenarios, demonstrates that LET-VIC consistently\noutperforms existing cooperative tracking methods.\nThe contributions are summarized as follows:\n\u2022 We introduce LET-VIC, a LiDAR-based End-to-end\nTracking framework for VIC that leverages a novel VIC\ncross-attention mechanism to fuse BEV features from\nboth vehicle and infrastructure side sensors, providing\na unified solution for 3D object perception in complex\ntraffic scenarios.\n\u2022 We propose a self-supervised method to optimize spa-\ntial error correction during coordinate transformation.\nBy learning offsets Ax and Ay within BEV queries,\nthis method corrects misalignments caused by calibration\nerrors, enhancing robustness against coordinate inaccura-\ncies.\n\u2022 LET-VIC consistently outperforms existing cooperative\ntemporal perception methods on the V2X-Seq-SPD\ndataset [9]. Our experimental design also included evalu-\nations under various delay conditions ranging from 0 ms\nto 300 ms, demonstrating LET-VIC's strong stability and\nadaptability, even though it was not explicitly optimized\nfor delay handling.\nThe remainder of this paper is organized as follows. Section\nII reviews the related works relevant to this study. In Section\nIII, we present LET-VIC, our proposed LiDAR-based end-\nto-end framework for VIC tracking. Section IV details the\nexperimental results and analysis. Finally, Section V concludes\nthe paper.\nA. Cooperative Perception\nWith advancements in Vehicle-to-Everything (V2X) com-\nmunication technologies [10]\u2013[13], cooperative perception in\nautonomous driving enhances perception capabilities through\ndata sharing and fusion across multiple agents. This ap-\nproach addresses challenges such as occlusion, long-distance\nperception, and sensor failures that single-vehicle perception\nmay struggle with. Cooperative perception research primarily\nfocuses on two types: Vehicle-to-Vehicle (V2V) and Vehicle-\nto-Infrastructure (V2I) cooperation.\nIn V2V cooperation, several methodologies have been ex-\nplored. Early Fusion techniques, as seen in Cooper [14] and\nDiscoNet [15], integrate raw sensor data from multiple vehi-\ncles, improving detection accuracy and extending perception\nrange. Feature Fusion approaches, such as OPV2V [16], F-\nCooper [17], V2VNet [18], and COOPERNAUT [19], com-\nbine intermediate-level features from multiple vehicles for\nenhanced perception. Notably, How2comm [20] focuses on\ntransmitting BEV features extracted from point clouds to\nenhance cooperative 3D object detection.\nIn contrast, V2I cooperation has been explored through\nmodels such as V2X-ViT [21], FFNet [22], CTCE [23] and\nV2X-Graph [24], which adopt a middle-fusion approach to\nintegrate data from both vehicles and infrastructure. These"}, {"title": "III. METHOD", "content": "tracking, offers a promising alternative to current VIC meth-\nods [4]\u2013[8]. This paradigm enables a coherent understanding\nof object dynamics, resulting in a more robust perception sys-\ntem. When implemented with LiDAR technology, end-to-end\ntemporal perception becomes even more effective. Compared\nto traditional camera-based systems, LiDAR provides precise\ndistance measurements and maintains reliable performance in\nadverse weather and low-light conditions, making it an ideal\nfoundation for advanced temporal perception. While LiDAR-\nbased end-to-end temporal perception has shown strong po-\ntential in vehicle-only systems, extending this approach to a\nVIC-based framework that fully leverages both spatial and\ntemporal contexts remains underdeveloped. Developing such\na framework could significantly enhance safety and reliability\nin autonomous driving applications.\nIn this study, we propose LET-VIC, the first LiDAR-based\nEnd-to-end Tracking framework for VIC, which designed to\naddress major perception challenges in autonomous driving,\nincluding limited perception range and occlusion. LET-VIC\nfully utilizes LiDAR point clouds from both vehicle-side\nand infrastructure-side sensors, incorporating temporal data to\ngreatly enhance object detection and tracking in complex traf-\nfic environments. The framework features an advanced feature\nfusion strategy that uses a VIC cross-attention mechanism to\nseamlessly integrate Bird's Eye View (BEV) features from the\ninfrastructure-side into the vehicle-side BEV representation.\nFurthermore, We employ a Calibration Error Compensation\n(CEC) module to compensate for calibration errors between\nThis section describes the LET-VIC method utilized in our\nproposed LiDAR-based end-to-end tracking framework for\nVIC. At the core of LET-VIC is the VIC Cross-Attention\nmechanism, which dynamically integrates LiDAR features\nfrom both vehicle-side and infrastructure-side perspectives,\nwith a focus on each point within the ego vehicle's sur-\nrounding area. As illustrated in Fig. 3, LET-VIC leverages\nPointPillars [38] for feature extraction from both vehicle\nand infrastructure LiDAR data. The features are then fused\nusing the VIC Cross-Attention module to enhance feature\nintegration on the vehicle side, followed by detection and\ntracking using the MOTR approach [6]. Additionally, the\nframework addresses the significant impact of sensor cali-\nbration errors on detection and tracking accuracy through an\nonline sensor calibration compensation module. We begin by\ndefining the VIC temporal perception problem and providing\nrelevant background information. Following this, we present\nour approach to generating BEV features and detail the\nmethodology for effectively fusing multi-view data. We then\naddress the significant challenge of sensor calibration errors\nand propose a compensation mechanism to mitigate their\nimpact. Subsequently, we present our end-to-end temporal\nperception strategy. Finally, we discuss the evaluation metrics\nemployed to assess the overall effectiveness of LET-VIC.\nVIC Temporal Perception encompasses the dual objectives\nof detecting and tracking objects using sequential point cloud\ndata collected from both vehicle and infrastructure sensors.\nThis collaborative approach enhances the overall perception\nof the environment, allowing for more accurate identification\nand monitoring of objects within the vicinity of the ego\nvehicle [9]. The study focuses specifically on point cloud\ndata, which contains critical depth information obtained from"}, {"title": "A. Task Description"}, {"title": "B. BEV Feature Generation", "content": "vehicle-side and infrastructure-side sensors, delivering precise\nand robust 3D object perception. Rigorous testing on the V2X-\nSeq-SPD dataset [9], which captures challenging real-world\ndriving scenarios, demonstrates that LET-VIC consistently\noutperforms existing cooperative tracking methods.\n\u2022 We introduce LET-VIC, a LiDAR-based End-to-end\nTracking framework for VIC that leverages a novel VIC\ncross-attention mechanism to fuse BEV features from\nboth vehicle and infrastructure side sensors, providing\na unified solution for 3D object perception in complex\ntraffic scenarios.\n\u2022 We propose a self-supervised method to optimize spa-\ntial error correction during coordinate transformation.\nBy learning offsets Ax and Ay within BEV queries,\nthis method corrects misalignments caused by calibration\nerrors, enhancing robustness against coordinate inaccura-\ncies.\n\u2022 LET-VIC consistently outperforms existing cooperative\ntemporal perception methods on the V2X-Seq-SPD\ndataset [9]. Our experimental design also included evalu-\nations under various delay conditions ranging from 0 ms\nto 300 ms, demonstrating LET-VIC's strong stability and\nadaptability, even though it was not explicitly optimized\nfor delay handling.\nThe remainder of this paper is organized as follows. Section\nII reviews the related works relevant to this study. In Section\nIII, we present LET-VIC, our proposed LiDAR-based end-\nto-end framework for VIC tracking. Section IV details the\nexperimental results and analysis. Finally, Section V concludes\nthe paper.\nWith advancements in Vehicle-to-Everything (V2X) com-\nmunication technologies [10]\u2013[13], cooperative perception in\nautonomous driving enhances perception capabilities through\ndata sharing and fusion across multiple agents. This ap-\nproach addresses challenges such as occlusion, long-distance\nperception, and sensor failures that single-vehicle perception\nmay struggle with. Cooperative perception research primarily\nfocuses on two types: Vehicle-to-Vehicle (V2V) and Vehicle-\nto-Infrastructure (V2I) cooperation.\nIn V2V cooperation, several methodologies have been ex-\nplored. Early Fusion techniques, as seen in Cooper [14] and\nDiscoNet [15], integrate raw sensor data from multiple vehi-\ncles, improving detection accuracy and extending perception\nrange. Feature Fusion approaches, such as OPV2V [16], F-\nCooper [17], V2VNet [18], and COOPERNAUT [19], com-\nbine intermediate-level features from multiple vehicles for\nenhanced perception. Notably, How2comm [20] focuses on\ntransmitting BEV features extracted from point clouds to\nenhance cooperative 3D object detection.\nIn contrast, V2I cooperation has been explored through\nmodels such as V2X-ViT [21], FFNet [22], CTCE [23] and\nV2X-Graph [24], which adopt a middle-fusion approach to\nintegrate data from both vehicles and infrastructure. These"}, {"title": "C. BEV Feature Transmission", "content": "Fig. 4 illustrates how the PointPillars framework [38] is\nemployed on both vehicle and infrastructure sides to pro-\ncess LiDAR point clouds. The raw point cloud data is first\norganized into pillars-grids that aggregate spatial informa-\ntion-enabling efficient feature extraction for subsequent pro-\ncessing.\nThe pipeline for BEV feature generation begins by pre-\nprocessing the 3D point clouds (x, y, z). In the voxelization\nstep, the point clouds are discretized into 3D grid cells\n(voxels), with distinct regions of interest (ROI) for vehicle\nand infrastructure sides. The vehicle side ROI spans x: [-51.2,\n51.2] meters, y: [-51.2, 51.2] meters, and z: [-5.0, 3.0] meters,\nwhile the infrastructure side ROI covers x: [0, 102.4] meters,\ny: [-51.2, 51.2] meters, and z: [-5.0, 3.0] meters.\nVoxelization uses a uniform voxel size of 0.2 meters along\nthe x and y axes, and 8 meters along the z-axis. As a result,\nthe vehicle-side grid is 512 \u00d7 512, and the infrastructure-side\ngrid is also 512 \u00d7 512. Each voxel stores its corresponding\npoints and their coordinates.\nFollowing voxelization, PillarFeatureNet extracts high-\ndimensional features from the voxels, transforming the point\ncloud into a 2D BEV representation. These pseudo-image\nfeatures are scattered using the PointPillarsScatter layer and\nare further refined by SECOND convolutional layers. Finally,\na Feature Pyramid Network (FPN) integrates multi-scale fea-\ntures, outputting a set of multi-level features to enhance object\nperception capabilities across different spatial resolutions.\nIn V2X communication, three primary data types are trans-\nmitted: raw data, instance-level perception data, and feature-\nlevel intermediate data. Each type incurs different transmission\ncosts and offers varying levels of information retention.\n\u2022 Raw data includes unprocessed sensor information, like\nLiDAR point clouds or images. It has the highest trans-\nmission cost due to its large volume but retains the most\ninformation as it is unfiltered and uncompressed.\n\u2022 Instance-level perception data consists of processed out-\nputs like detected objects and attributes or queries that\ncontain perception results but are not yet decoded [39]. It\nhas the lowest transmission cost due to high compression,\nbut this also leads to the highest information loss because\nit transmits only selected information.\n\u2022 Feature-level intermediate data comprises key features\nextracted from raw data, such as BEV features [40]. It\nbalances transmission costs and information retention.\nOur V2X communication framework prioritizes transmitting\nBEV features due to the trade-off between transmission cost\nand information retention. BEV features capture essential spa-\ntial relationships and environmental context with reduced data\nvolume, ensuring critical information for accurate perception\nand decision-making is retained. This approach significantly\nreduces the required communication bandwidth compared to\nraw data transmission. Additionally, the VIC Cross-Attention\nmechanism effectively integrates BEV features from both\nvehicle and infrastructure sides."}, {"title": "D. BEV Feature Fusion", "content": "The deformable attention mechanism plays a critical role in\nLET-VIC's ability to dynamically fuse features from multiple\nviewpoints. Traditional attention mechanisms can struggle\nwith multi-view fusion due to high computational costs and\ndifficulties in aligning spatial features across different perspec-\ntives. In contrast, deformable attention significantly reduces\ncomputational overhead by focusing on a sparse set of key\nfeatures, known as reference points, and dynamically adjusting\nits focus based on local feature distributions.\nIn LET-VIC, the VIC Cross-Attention Module employs\ndeformable attention to integrate BEV features from both\nvehicle-side and infrastructure-side sensors. The process be-\ngins by projecting BEV queries to each perspective and gen-\nerating initial reference points. A learnable network then pre-\ndicts calibration adjustments to refine these reference points,\ncompensating for any misalignments between the two per-\nspectives. Deformable attention is subsequently applied around\nthe refined points, enabling the model to sample key features\nefficiently. This approach ensures that only the most relevant\nfeatures from both viewpoints are fused, enhancing the effi-\nciency and accuracy of feature integration while accounting\nfor sensor misalignments.\nThe ability to adaptively focus on critical features enables\nthe deformable attention mechanism to excel in complex,\nmulti-view tracking tasks where spatial alignment may vary\ndue to sensor calibration errors. Fig. 5 illustrates the VIC\nCross-Attention mechanism, showcasing its ability to dynami-\ncally integrate data across different sensors. The process of\nVIC Cross-Attention (VICCrossAttn) can be formulated as"}, {"title": "E. Calibration Error Compensation", "content": "following:\n$VICCrossAttn(Q_{bev}, F_{veh}, F_{inf}, p_{veh}, p_{inf}) =\\frac{1}{N_{ref}} \\sum_{i=1}^{N_{ref}} [\\frac{1}{1 + M_{inf}} (MSDeformAttn(Q_{bev}, p_{veh}^i, F_{veh}) +\\frac{1}{1 + M_{inf}} (MSDeformAttn(Q_{bev}, p_{inf}^i, F_{inf}))].$\n(2)\nWhere i indexes the reference points, and $N_{ref}$ is the total\nreference points for BEV Query $Q_{bev}$. $p_{veh} \\in [0,1]^2$ are\nthe normalized coordinates of the i-th reference points on the\nvehicle side. $p_{inf} \\in [0,1]^2$ are the normalized coordinates of\nthe i-th reference points on the infrastructure side. $M_{inf}$ is\nthe mask matrix ensuring that any reference point projected\nonto the infrastructure side multi-level LiDAR BEV feature is\nexcluded if it falls outside the feature's bounds.\nThe multi-scale deformable attention (MSDeformAttn)\nfunction is defined as:\n$MSDeformAttn(Q_i, P_i, {F_l}_{l=1}^{L}) =\n\\sum_{m=1}^{M} \\sum_{l=1}^{L} \\sum_{k=1}^{K}\nA_{mlqk}W_mF (R_l(P_i) +\n\\Delta p_{mlqk})\\,$\n(3)\nWhere i \u2208 \u03a9; indexes a query element with representation\nfeature $Q_i \\in R^C$, where C is the feature dimension. m\nindexes the attention head, l indexes the input feature level,\nand k indexes the sampling point. $\u0394p_{mlqk}$ represents the\nsampling offset, and $A_{mlqk}$ denotes the attention weight of\nthe k-th sampling point in the l-th feature level and the\nm-th attention head. The scalar attention weight $A_{mlqk}$ is\nnormalized by $\\sum_{l=1}^{L} \\sum_{k=1}^{K} A_{mlqk} = 1$. $P_i\\in [0,1]^2$ are\nthe normalized coordinates for clarity in scale formulation.\nThe function $R_l(P_i)$ re-scales the normalized coordinates $P_i$\nto the input feature map of the l-th level. $\u0394p_{mlqk} \u2208 R^2$\nare 2-dimensional real numbers with an unconstrained range.\n$W_m \u2208 R^{C/M\u00d7C}$ and $W_m \u2208 R^{C\u00d7C/M}$ are learnable weights\nfor the m-th attention head.\nCalibration errors in sensors, arising from precision vari-\nations, environmental factors, and calibration method uncer-\ntainties, pose significant challenges to perception accuracy\nin autonomous driving systems. Compensating for sensor\ncalibration errors is crucial for LET-VIC as it ensures accurate\ndata fusion and reliable cooperative perception performance.\nTherefore, we implemented CEC module to optimize spatial\nerror correction during coordinate transformation, delivering\nprecise and robust temporal perception.\nInitially, the positions of vehicle-side or infrastructure-side\nfeatures corresponding to each point in the BEV features are\ndetermined through coordinate system transformation. Sub-\nsequently, CEC employs a set of learnable parameters that\nadapt during training to learn adaptive offsets, which include\n$\u0394x^{inf}$, $\u0394y^{inf}$, $\u0394x^{veh}$, and $\u0394y^{veh}$. Finally, the positions of\nvehicle-side or roadside features corresponding to each BEV\nfeature are adjusted by adding this set of offsets, forming\ncompensated reference points as shown in Eqs. 4 and 5.\nThen, we normalize the original reference point coordinates\naccording to the specified point cloud range, as detailed in\nEqs. 6 and 7.\n$\\begin{aligned}\\left[\\begin{array}{l}x_{veh}\\\\ y_{veh}\\end{array}\\right]_i = R_{bev2veh} \\cdot R_{ebev} + T_{bev2veh} + \\left[\\begin{array}{l}\\Delta x_{veh}\\\\ \\Delta y_{veh}\\end{array}\\right],\\end{aligned}$\n(4)\n$\\begin{aligned}\\left[\\begin{array}{l}x_{inf}\\\\ y_{inf}\\end{array}\\right]_i = R_{bev2inf} \\cdot R_{ebev} + T_{bev2inf} + \\left[\\begin{array}{l}\\Delta x_{inf}\\\\ \\Delta y_{inf}\\end{array}\\right],\\end{aligned}$\n(5)\n$p_{veh} = \\left[\\frac{x_{i}^{veh} - X_{min}^{veh}}{X_{max}^{veh} - X_{min}^{veh}}, \\frac{y_{i}^{veh} - Y_{min}^{veh}}{Y_{max}^{veh} - Y_{min}^{veh}}\\right],$\n(6)\n$p_{inf} = \\left[\\frac{x_{i}^{inf} - X_{min}^{inf}}{X_{max}^{inf} - X_{min}^{inf}}, \\frac{y_{i}^{inf} - Y_{min}^{inf}}{Y_{max}^{inf} - Y_{min}^{inf}}\\right],$\n(7)\nWhere $x^{veh}, y^{veh}$ represents the i-th original coordinates\nof the reference point on the vehicle-side and $x^{inf}, y^{inf}$\nrepresents the i-th original coordinates of the reference point\non the infrastructure-side. $R_{bev2inf}$ represents the rotation\nmatrix from the vehicle side LiDAR coordinate system to the\npseudo-image coordinate system of infrastructure-side point\ncloud BEV features, and $T_{bev2inf}$ represents the translation\nmatrix from the vehicle side LiDAR coordinate system to\nthe pseudo-image coordinate system of infrastructure-side\npoint cloud BEV features. $\u0394x_{inf}$ and $\u0394y_{inf}$ represent the\ncalibration errors corresponding to the infrastructure side at i-\nth reference point; $R_{ebev}$ represents the i-th reference points\nin the pseudo-image of vehicle side point cloud BEV features,\n$R_{bev2veh}$ represents the rotation matrix from the vehicle side\nLiDAR coordinate system to the pseudo-image coordinate\nsystem of vehicle side point cloud BEV features, and $T_{bev2veh}$\nrepresents the translation matrix from the vehicle side LiDAR\ncoordinate system to the pseudo-image coordinate system of\nvehicle side point cloud BEV features; $\u0394x_{veh}$ and $\u0394y_{veh}$\nrepresent the calibration errors corresponding to the vehicle\nside at i-th reference point. $X_{min}^{veh}, x_{max}^{veh}$ are the minimum\nand maximum values of the vehicle side point cloud range\nin the x-direction, $Y_{min}^{veh}, Y_{max}^{veh}$ are the minimum and\nmaximum values of the vehicle side point cloud range in the y-\ndirection. $X_{min}^{inf}, x_{max}^{inf}$ are the minimum and maximum\nvalues of the infrastructure side point cloud range in the x-\ndirection, $Y_{min}^{inf}, Y_{max}^{inf}$ are the minimum and maximum"}, {"title": "F. End-to-End Tracking", "content": "E. Calibration Error Compensation\nvalues of the infrastructure side point cloud range in the y-\ndirection. $p_{veh}, p_{inf}$ are the input parameters defined in\nEq. 2.\nThe end-to-end tracking capability of LET-VIC is powered\nby an advanced architecture inspired by the TrackFormer\nframework [5], [6], [34], which unifies object detection and\ntracking tasks within a single Transformer-based pipeline.\nThis integration bypasses the conventional two-step tracking-\nby-detection approach, where detection is followed by post-\nprocessing for tracking. Instead, LET-VIC directly associates\nobjects across frames during detection, allowing for joint\noptimization of both tasks.\nWithin LET-VIC, detect queries and track queries play\ndistinct but complementary roles in the TrackFormer decoder.\nDetect queries are responsible for identifying new objects in\neach frame by interacting with the current frame's features,\nfocusing on objects not previously tracked. Track queries, on\nthe other hand, are used to maintain object identities across\nframes, carrying information from previous frames to ensure\ntemporal consistency by associating the same object across\nconsecutive frames. This combination allows for simultaneous\ndetection and tracking, ensuring seamless object tracking\nthroughout sequences.\nLET-VIC's end-to-end design benefits from a joint loss\nfunction that simultaneously optimizes detection and tracking.\nThe loss function includes components for object classifi-\ncation, bounding box regression, and identity preservation.\nThis comprehensive approach enhances overall accuracy and\nensures that objects are correctly identified and tracked even\nin complex and dynamic environments."}, {"title": "G. Evaluation Metrics and Analysis"}, {"title": "V. CONCLUSION", "content": "To evaluate LET-VIC's performance, we selected a range\nof metrics that comprehensively assess both detection and\ntracking accuracy [41]\u2013[43]. These metrics are standard in 3D\ntracking benchmarks and provide a balanced view of system\nperformance under various conditions:\n\u2022 MAP (Mean Average Precision): mAP is a key metric\nfor evaluating object detection performance. It measures\nhow accurately detected objects match ground-truth ob-\njects across different recall levels, typically using distance\nthresholds. For each distance threshold, precision-recall\ncurves are generated, and the area under each curve gives\nthe Average Precision (AP). The final mAP score is the\nmean of AP values across all classes and distance thresh-\nolds. A higher mAP indicates better detection quality. The\nformulations for AP and mAP are:\n$\\mathrm{AP} = \\int_{0}^{1} \\mathrm{precision}(r) dr,$\n(8)\n$\\mathrm{mAP} = \\frac{1}{N} \\sum_{c=1}^{N} \\mathrm{AP}_{c},$\n(9)\nWhere r represents recall levels from 0 to 1, N is the\nnumber of classes, and APc is the average precision for\nclass c. In practice, the area under the PR curve is often\napproximated by taking the precision at discrete recall\nlevels.\n\u2022 AMOTA (Average Multi-Object Tracking Accuracy):\nAMOTA is a central metric for evaluating tracking accu-\nracy in multi-object scenarios. It builds on Multi-Object\nTracking Accuracy (MOTA), which assesses tracking ac-\ncuracy by accounting for false positives, false negatives,\nand identity switches between tracked objects and ground\ntruth. AMOTA averages MOTA scores across different\ndifficulty levels and recall thresholds, providing an overall\nmeasure of tracking accuraciy. Higher AMOTA values\nare preferred, as they reflect consistent tracking accuracy\nacross recall levels. The formulations for MOTA and\nAMOTA are:\n$\\mathrm{MOTA} = 1 - \\frac{\\sum_{t}(\\mathrm{FP}_{t} + \\mathrm{FN}_{t} + \\mathrm{IDS}_{t})}{\\sum_{t}(\\mathrm{GT}_{t})},$\n(10)\n$\\mathrm{AMOTA} = \\frac{1}{R} \\sum_{r=1}^{R} \\mathrm{MOTA}_{r},$\n(11)\nWhere FPt is the number of false positives at time t,\nFNt is the number of false negatives at time t, IDSt is\nthe number of ID switches at time t, GT is the total\nnumber of groundtruth objects at time t, R is the number\nof recall levels, and MOTA is the MOTA score at recall\nlevel r.\n\u2022 AMOTP (Average Multi-Object Tracking Precision):\nAMOTP is an essential metric for evaluating tracking pre-\ncision in multi-object scenarios. It builds on Multi-Object\nTracking Precision (MOTP), which measures the spatial\nalignment between tracked and ground-truth object posi-\ntions by calculating the average positional error. AMOTP\naverages MOTP scores across different difficulty levels\nand recall thresholds, providing an overall measure of\ntracking precision. Lower AMOTP values are preferred,\nas they reflect consistent tracking precision across recall\nlevels. The formulations for MOTP and AMOTP are:\n$\\mathrm{MOTP} = \\frac{\\sum_{i,t} d_{i,t}}{\\sum_{t} (c_{t})},$\n(12)\n$\\mathrm{\u0410\u041c\u041e\u0422\u0420} = \\frac{1}{R} \\sum_{r=1}^{R} \\mathrm{MOTP}_{r}$\n(13)\nWhere dit is the positional error for the i-th matched\nobject at time t, ct is the number of successfully matched\nobject pairs at time t, R is the number of recall levels,\nand MOTP is the MOTP score at recall level r.\nThese metrics offer a thorough evaluation of LET-VIC's\ndetection and tracking performance under varied conditions.\nWe select mAP as our primary detection metric because\nit effectively measures the accuracy of object identification,\nproviding insight into the model's ability to detect objects with\nprecision. For tracking, we emphasize AMOTA and AMOTP\nas they capture tracking accuracy and spatial precision across\nmultiple recall thresholds. These metrics provide a compre-\nhensive assessment of LET-VIC's robustness and consistency,"}, {"title": "Ablation Study", "content": "This paper introduces LET-VIC", "Work": "LET-VIC", "Setup": "We compare the performance of\nthe LET-VIC model with and without the CEC module under\ndelays of 0 ms", "Details": "Due to limited com-\nputational resources", "Analysis": "The results presented in Table II demonstrate\nthat LET-VIC with the CEC module consistently outperforms\nthe standard LET-VIC across all delay settings, highlighting\nthe CEC module's effectiveness in enhancing perception ro-\nbustness. Specifically, LET-VIC with calibration error com-\npensation achieves +6.4% mAP and +4.7% AMOTA improve-\nments, along with a -0.038 AMOTP reduction at 0 ms delay;\n+7.5% mAP, +5.5% AMOTA, and -0.079 AMOTP at 100\nms delay; +5.0% mAP, +1.8% AMOTA, and -0.044 AMOTP\nat 200 ms delay; and +5.2% mAP, +4.2% AMOTA, and"}]}