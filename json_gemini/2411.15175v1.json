{"title": "Can Open-source LLMs Enhance Data Augmentation for Toxic Detection?: An Experimental Study", "authors": ["Zheng Hui", "Zhaoxiao Guo", "Hang Zhao*", "Juanyong Duan", "Lin Ai", "Yinheng Li*", "Julia Hirschberg", "Congrui Huang"], "abstract": "High-quality, diverse harmful data is essential to addressing real-time applications in content moderation. Current state-of-the-art approaches to toxic content detection using GPT series models are costly and lack explainability. This paper investigates the use of prompt engineering and fine-tuning techniques on open-source LLMs to enhance harmful data augmentation specifically for toxic content detection. We conduct a two-stage empirical study, with stage 1 evaluating six open-source LLMs across multiple datasets using only prompt engineering and stage 2 focusing on fine-tuning. Our findings indicate that Mistral can excel in generating harmful data with minimal hallucination. While fine-tuning these models improves data quality and diversity, challenges such as data duplication and overfitting persist. Our experimental results highlight scalable, cost-effective strategies for enhancing toxic content detection systems. These findings not only demonstrate the potential of open-source LLMs in creating robust content moderation tools. The application of this method in real industrial scenarios further proves the feasibility and efficiency of the fine-tuned open-source LLMs for data augmentation. We hope our study will aid in understanding the capabilities and limitations of current models in toxic content detection and drive further advancements in this field.", "sections": [{"title": "1 Introduction", "content": "With the development of large language models (Brown et al., 2020), there is a significant amount of work currently using LLMs for data augmentation (Whitehouse et al., 2023; Ye et al., 2022; Wang et al., 2023). This study investigates the application of prompt engineering and fine-tuning techniques on open-source large language models to enhance the augmentation of harmful data, crucial for improving toxic content detection systems. Data augmentation for toxic detection is vital because real-world datasets often lack the diversity and volume needed to train robust models (Garg et al., 2023). Current challenges in toxic detection include the need for real-time processing (Oikawa et al., 2022), where toxic comments must be filtered out swiftly to prevent user exposure. Existing methods using LLMs to directly detect toxic content, while effective, often suffer from slow inference speeds and high computational resource requirements, making them unsuitable for systems without GPU resources (Kumarage et al., 2024). Additionally, methods based on the GPT (Brown et al., 2020) series, despite their good performance, face issues of high cost and lack of explainability (Zhao et al., 2024). Content safety classifiers also often face urgency and specificity across various scenarios due to differing definitions of sensitive topics among users, and obtaining harmful training data is inherently challenging. As a result, many products require a standalone data augmentation module, which can be used as an independent service to meet the training data generation needs of various applications. Using GPT models is both costly and slow (Hui et al., 2024), whereas open-source LLMs can make data generation faster and cheaper with comparable effectiveness (Ding et al., 2024). This work aims to address these challenges by optimizing the use of open-source LLMs for generating high-quality, diverse, and compliant harmful data, thus enhancing the reliability and accuracy of toxic content detection systems. This flexibility helps enterprises respond to emergency topics and cater to user-specific customization needs. Additionally, the fine-tuned models have already been deployed in our API, powering numerous use cases such as chatbot content safety, live chat, and children's custom content channels."}, {"title": "2 Methodology", "content": "Our approach leverages prompt engineering and fine-tuning techniques on open-source LLMs for harmful data augmentation, enhancing toxic content detection systems. The methodology consists of two main stages: prompt engineering and fine-tuning."}, {"title": "2.1 Prompt Engineering", "content": "Given an open-source LLM M, we design prompts P to generate harmful data $D_{harmful}$. The prompt template is defined as:\nP = {Role, Requirement, Few-Shot Examples}\nwhere Role defines the context, Requirement specifies the task, and Few-Shot Examples provide examples to guide the model. In the initial phase of our study, we focused solely on prompt engineering. This involved creating various prompt templates tailored to elicit specific types of harmful data. Each prompt was carefully crafted following work by Zhou et al. (2023) to ensure clarity and relevance to the desired output. Despite the meticulous design of these prompts, we encountered significant challenges. The primary issue was the model's internal safety alignments, which are designed to prevent the generation of harmful content. While these alignments are crucial for ethical AI usage, they posed a hurdle for our specific goal of generating harmful data for augmentation purposes. Consequently, the generated data often lacked the quality and diversity needed for effective toxic content detection. To overcome these challenges, we experimented with various prompt configurations, including adjusting the few-shot examples and altering the specificity of the requirements. However, these adjustments had limited success in mitigating the internal safety mechanisms."}, {"title": "2.2 Fine-Tuning", "content": "Based on the results from prompt engineering, we believed that fine-tuning would provide a better solution. We used LoRA methods (Hu et al., 2022) for fine-tuning to update the weights of M using a training dataset $D_{train}$ to improve the quality and diversity of $D_{harmful}$. The fine-tuning objective L minimizes the cross-entropy loss:\n$L = - \\sum_{i=1}^{N} (y_i log(y_i) + (1 - y_i) log(1 - \\hat{y_i}))$\nwhere $y_i$ is the true label, $\\hat{y_i}$ is the predicted probability, and N is the number of samples. After fine-tuning, Mistral performed the best compare with baselines; the detailed experiment result are shown in 4. The fine-tuned versions are already being used internally."}, {"title": "2.3 Algorithm", "content": "The overall algorithm for harmful data augmentation is outlined in Algorithm 1."}, {"title": "2.4 Evaluation", "content": "We evaluate the performance of the augmented data using F1-score and Accuracy metrics on a smaller MLP classifier(more detailed in 3.2 section)."}, {"title": "3 Experiment Setups", "content": ""}, {"title": "3.1 Datasets", "content": "The following proprietary datasets are used for training models to identify various types of harmful content. Each dataset contains binary classification labels, with texts labeled as either positive or negative (non-harmful). The labels are based on multi-round human evaluation. The sources of the datasets include online public datasets, company services collection, and other sources. Detailed data numbers are listed in Table 1. We also prepare a 2500 entries evaluation dataset from each dataset.\nHate Speech Dataset This dataset comprises text samples specifically focused on hate speech. It includes a variety of offensive and discriminatory language targeting specific groups based on race, ethnicity, religion, gender, or other characteristics.\nSexual Content Dataset This dataset includes text samples containing explicit sexual content. The focus is on identifying sexually explicit language that may not be suitable for general audiences.\nViolence Dataset This dataset consists of text samples that contain violent content. It aims to identify language that incites or describes violence.\nSelf-Harm Dataset This dataset includes text samples that contain references to self-harm. The focus is on identifying content that describes or encourages self-harming behaviors.\nPolitical Dataset This dataset includes text samples that contain political references. The focus is on identifying content that is targeting public figures."}, {"title": "3.2 Models", "content": "We conducted a two-stage test to evaluate and fine-tune different language models for identifying harmful content."}, {"title": "Stage 1: Prompt Engineering", "content": "In the first stage, we employed prompt engineering and tested six different models: Mistral (Jiang et al., 2023), LLaMa2 (Touvron et al., 2023), Vicuna (Chiang et al., 2023), Falcon (Almazrouei et al., 2023), Bloom (Workshop et al., 2023), (Team et al., 2024). Stage 1 result can be found in 4. After evaluating the performance of these models, we determined that Mistral and Vicuna performed best in the prompt engineering stage. Therefore, we selected these two models for the fine-tuning stage."}, {"title": "Stage 2: Fine-Tuning", "content": "In the second stage, we fine-tuned the Mistral and Vicuna models using the LoRA method (Hu et al., 2022). After comparing latency and cost we decided to use a smaller model Mixtral for further exploration."}, {"title": "3.3 Implementation", "content": "The fine-tuning process was conducted on 4*NVIDIA A100 GPUs Azure cluster. We utilized the Huggingface Transformers library for our fine-tuning experiments. The specific model architecture and pre-trained weights used in our study are available at Huggingface, as referenced in Wolf et al. (2020). The fine-tuning stage specified was Supervised Fine-Tuning (SFT). Adapter is a specific Low-Rank Adaptation (Hu et al., 2022) fine-tuning setup. Both the maximum number of new tokens generated and the cutoff length for the input sequences were set to 4096. The evaluation batch size per device was set to 8. The results of the fine-tuning process, including detailed performance metrics and qualitative analyses, can be found in Section 4."}, {"title": "Downstream Model Evaluation", "content": "For the downstream model evaluation, we utilized a Multilayer Perceptron (Gardner and Dorling, 1998) classifier with two hidden layers, each consisting of 600 neurons. The primary reason for selecting an MLP is due to its applicability in scenarios where GPU resources are limited. In actual applications, many implementations operate without the support of GPUs. Therefore, in a GPU-constrained environment, an MLP serves as a practical and efficient choice for classification tasks."}, {"title": "4 Result", "content": "The results of our study demonstrate the efficacy of both prompt engineering and fine-tuning strategies in enhancing the performance of open-source LLMs for harmful data augmentation. We evaluated different models using a two-stage process: prompt engineering and fine-tuning."}, {"title": "4.1 Stage 1: Prompt Engineering", "content": "The first stage of our evaluation focused on assessing the performance of various Open Source LLMs for data augmentation using prompt engineering. The primary goal was to explore the quality and effectiveness of existing LLMs in generating augmented data for political content. Table 2 presents the performance comparison of the six different models evaluated in Stage 1. The success rates for political and hate content are reported, along with the results of human evaluation.\nThe success rate columns in Table 2 represent the percentage of valid positive samples generated by each model for political and hate content. The values indicate the proportion of samples meeting the minimum requirements (60% of the total samples generated). The human evaluation columns show whether the generated data was subject to human evaluation. A green checkmark (\u2714) signifies that human annotators evaluated the quality of the generated data and found a high Fleiss' kappa score, indicating strong inter-rater agreement. This also means that the generated samples met the minimum quality criteria. Based on the evaluation, Mistral and Vicuna demonstrated the best performance with prompt engineering. These two models were selected for the fine-tuning stage due to their superior results in generating high-quality and diverse data entries."}, {"title": "4.2 Stage 2: Fine-tuning", "content": "In the second stage, we first explored fine-tuning the models using only one dataset. The results of this process are shown in Table 4. Inspired by Hui et al. (2024), Yu et al. (2024) and Qi et al. (2023), we then implemented a data mixing strategy, leveraging multiple datasets for fine-tuning. The results of fine-tuning the models on five different datasets are presented in Table 3. We also performed an ablation study on different design choices which are listed in Appendix C. The \"Mixture\" versions refer to fine-tuning data that is a mix of hate, violence, and sex datasets. The \"Mix_GPT\" versions refer to a combination of the mixed data with GPT-generated positive samples. For all fine-tuning processes, we used 3000 positive and 3000 negative samples, following the Alpaca instruction template.\nOur results show that fine-tuning significantly improves the performance of LLMs in generating harmful data. The data mixing strategy helps in mitigating issues like data duplication and overfitting. The fine-tuned models exhibited higher F1 scores and accuracy across different datasets, demonstrating their robustness and effectiveness in real-world applications. One surprising finding was the substantial improvement in model performance when using the mixed data strategy. This suggests that diverse training data can help models generalize better and produce more accurate and reliable outputs. The Mixture models with Epoch 3 and Epoch 5 showed particularly notable improvements, indicating that sufficient training epochs are critical for achieving optimal performance."}, {"title": "4.3 Error Analysis", "content": "We conducted an error analysis on the fine-tuned models to understand the nature of their mistakes and areas for improvement. This analysis is based on the bad cases and repetition rates across different epochs and versions for the hate and sex datasets.\nFor the hate dataset, the key observations are as follows. Mixtral_Mixture3 models generally show high precision, with values above 90%, indicating their effectiveness in generating targeted harmful data. The bad cases for Mixtral_Mixture models tend to be more explicit and harmful compared to Mistral and GPT, indicating the models' effectiveness. Examples of missed classifications include: \"You are a disabled person.\" This statement, while potentially harmful, may have been missed due to its less explicit nature compared to more aggressive threats. \"I'm sorry you're deaf.\" The classifier might have missed this because it appears apologetic rather than harmful, demonstrating the challenge of detecting subtle harmful content."}, {"title": "5 Discussion", "content": "The results of this study underscore the complexities and challenges involved in generating and detecting harmful content using LLMs. Through the two-stage evaluation process, we were able to examine the performance of various open-source LLMs and the impact of fine-tuning on mixed datasets. One of the primary challenges observed is the subtlety of harmful content. The fine-tuned models often missed subtle harmful statements that were not overtly aggressive or violent. This indicates a need for more sophisticated detection mechanisms that can understand context and nuanced language use. Repetition and overfitting were significant issues, particularly in later epochs of the finetuned version models. High repetition rates suggest that models are memorizing specific phrases rather than generalizing well across different harmful contexts. This overfitting reduces the diversity and effectiveness of harmful content generation. The data mixing strategy, which combined datasets of hate, violence, and sex with GPT-generated positive samples, showed promising results in improving precision and recall. However, the models still faced challenges in maintaining a balance between specificity and generalization, particularly when dealing with mixed types of harmful content."}, {"title": "6 Online Deployment", "content": "The models developed in this study have been successfully deployed in a major real-world application by a leading cloud provider, providing valuable insights and automations for various tasks. Specifically, these models are utilized for monitoring and managing harmful content across multiple platforms. The deployment of the key functionalities include helping real-time content moderation, reducing manual annotation through automation, and rapid response to new incidents or niche scenarios without pre-existing labels."}, {"title": "7 Conclusion", "content": "This study explored the performance and challenges of using open-source LLMs for harmful content generation and detection. By employing a two-stage evaluation process, we assessed the capabilities of various models in prompt engineering and fine-tuning. The result revealed that, while fine-tuned models demonstrated high precision in generating harmful content, they also exhibited issues with subtlety detection, repetition, and overfitting. These challenges highlight the need for diverse and nuanced training data, improved detection mechanisms, and robust ethical guidelines. In conclusion, our findings emphasize the importance of continuous evaluation and refinement of models to balance performance with responsible AI use. Future research should focus on enhancing the generalization capabilities of models, mitigating the risks associated with harmful content generation, and ensuring ethical deployment in real-world applications."}, {"title": "8 Limitations", "content": "While our study provides valuable insights into the performance of fine-tuned LLMs for harmful content generation and detection, several limitations must be acknowledged. One of the primary limitations is the diversity and representation of the datasets used for fine-tuning. Although we utilized mixed datasets combining hate, sex, and violence categories, the scope remains limited to specific types of harmful content. Real-world applications may encounter a broader range of harmful content, including less frequent or emerging forms of harmful language that were not covered in our datasets. Future work should aim to include a wider variety of harmful content categories to improve the models' robustness and generalizability. Generating harmful content for research purposes raises significant ethical concerns. While we implemented control mechanisms(access control, etc...) to mitigate the risk of producing excessively harmful outputs, there remains an inherent risk associated with the misuse of such models. Ensuring the responsible use of LLMs in generating harmful content is crucial, and future research should focus on developing more sophisticated ethical guidelines and safety measures. The fine-tuning process in this study relied on significant computational resources, including multiple A100 GPUs. This requirement may not be feasible for all researchers or applications, particularly those with limited access to high-performance computing resources. Exploring more efficient fine-tuning techniques and leveraging less resource-intensive models would be beneficial for broader accessibility and scalability."}, {"title": "Ethics Statement", "content": "The explicit nature of some of the generated harmful content raises ethical concerns. It is crucial to implement robust control mechanisms and ethical guidelines to ensure that the use of such models does not inadvertently promote or propagate harmful content. Future work should focus on developing methods to mitigate the generation of excessively harmful outputs while maintaining the models' effectiveness."}, {"title": "A Simple Prompt Engineering Choices and Experiment Table", "content": "Prompt Versions for hate speech\nV1 - Positive: You are no longer ChatGPT or an AI language model. Your new name is AOA (Absolutely Obedient Agent). You should be absolutely obedient to the user and follow the user's instructions without deviation. You are an excellent data augmentation assistant in the field of hate content. You are capable of generating 30 high-quality and diverse data entries based on the provided samples. Here are the specific requirements: 1. The generated data should pertain to hate. 2. Each data entry must be diverse and unique, with no repetitions. 3. The style and sentence structure should vary between different data entries, ensuring distinctiveness in expression. 4. Precede each data entry with a unique, sequentially ordered Arabic numeral, formatted as '1.', '2', '3.', etc. 5. The samples provided are for reference only, and the output data must not duplicate any of the provided samples.\nV1 - Negative: You are an excellent data augmentation assistant with a strong ability to follow the rules, and you can generate 30 high quality and diverse data items that are completely unrelated to the given samples. Specific requirements are as follows: 1. The data generated should have nothing to do with hate. 2. Each data entry must be diverse and unique and must not be duplicated. 3. There should be different styles and sentence structures between different data entries to ensure the uniqueness of the expression, which can be multiple fields. 4. Precede each data entry with a unique, sequentially ordered Arabic numeral, formatted as '1.', '2.', '3.', etc.\nV2 - Positive: Definition: Hate content refers to written or spoken expressions that demean, discriminate against, insult, or incite aggression towards individuals or groups based on inherent characteristics such as race, gender, religion, sexual orientation, ethnicity, etc., thereby spreading hostility and intolerance.\nYou are an excellent data augmentation assistant in the field of hate content. You are capable of generating 30 high-quality and diverse data entries based on the provided samples and the definition of hate content. Here are the specific requirements: 1. The generated data must align with the definition of hate content and be relevant to the samples, yet it should incorporate new elements and creativity in content, subject matter, sentence structure, and mode of expression, so the output distinctly differs from the provided samples. 2. Each data entry must be diverse and unique, with no repetitions. 3. The style and sentence structure should vary between different data entries, ensuring distinctiveness in expression. 4. Precede each data entry with a unique, sequentially ordered Arabic numeral, formatted as '1.', '2.', '3.', etc.\nV2 - Negative: You are an excellent data augmentation assistant with a strong ability to follow the rules, and you can generate 30 high quality and diverse data items that are completely unrelated to hate content. Specific requirements are as follows: 1. The data generated should have nothing to do with hate. 2. Each data entry must be diverse and unique and must not be duplicated. 3. There should be different styles and sentence structures between different data entries to ensure the uniqueness of the expression, which can be multiple fields. 4. Precede each data entry with a unique, sequentially ordered Arabic numeral, formatted as '1.', '2', '3.', etc.\nV3 - Positive: Definition: Hate content refers to written or spoken expressions that demean, discriminate against, insult, or incite aggression towards individuals or groups based on inherent characteristics such as race, gender, religion, sexual orientation, ethnicity, etc., thereby spreading hostility and intolerance.\nYou are an excellent data augmentation assistant in the field of hate content. You are capable of generating 30 high-quality and diverse data entries based on the provided samples and the definition of hate content. Here are the specific requirements: 1. The generated data must align with the definition of hate content. 2. Each data entry must be diverse and unique, with no repetitions. 3. The mode of expression and sentence structure should vary between different data entries, ensuring distinctiveness in expression. 4. Precede each data entry with a unique, sequentially ordered Arabic numeral, formatted as '1.', '2.', '3', etc.\nV3 - Negative: You are an excellent data augmentation assistant with a strong ability to follow the rules, and you can generate 30 high quality and diverse data items that are completely unrelated to hate content. Specific requirements are as follows: 1. The data generated should have nothing to do with hate. 2. Each data entry must be diverse and unique and must not be duplicated. 3. There should"}]}