{"title": "Behavioural gap assessment of human-vehicle interaction in real and virtual reality-based scenarios in autonomous driving", "authors": ["S. Mart\u00edn Serrano", "R. Izquierdo", "I. Garc\u00eda Daza", "M. A. Sotelo", "D. Fern\u00e1ndez-Llorca"], "abstract": "In the field of autonomous driving research, the use of immersive virtual reality (VR) techniques is widespread to enable a variety of studies under safe and controlled conditions. However, this methodology is only valid and consistent if the conduct of participants in the simulated setting mirrors their actions in an actual environment. In this paper, we present a first and innovative approach to evaluating what we term the behavioural gap, a concept that captures the disparity in a participant's conduct when engaging in a VR experiment compared to an equivalent real-world situation. To this end, we developed a digital twin of a pre-existed crosswalk and carried out a field experiment (N=18) to investigate pedestrian-autonomous vehicle interaction in both real and simulated driving conditions. In the experiment, the pedestrian attempts to cross the road in the presence of different driving styles and an external Human-Machine Interface (eHMI). By combining survey-based and behavioural analysis methodologies, we develop a quantitative approach to empirically assess the behavioural gap, as a mechanism to validate data obtained from real subjects interacting in a simulated VR-based environment. Results show that participants are more cautious and curious in VR, affecting their speed and decisions, and that VR interfaces significantly influence their actions.", "sections": [{"title": "I. INTRODUCTION", "content": "As autonomous vehicle (AV) technology advances, the need for rapid prototyping and extensive testing is becoming increasingly important, as real driving tests alone are not sufficient to demonstrate safety [1], [2]. The use of physics-based simulations allows the study of various scenarios and conditions at a fraction of the cost and risk of physical prototype testing, providing valuable insights into the behaviour and performance of AVs in a controlled environment [3].\nHowever, one of the main challenges in the development of autonomous driving digital twins is the lack of realism of simulated sensor data and physical models. The so-called reality gap can lead to inaccuracies because the virtual world does not adequately generalise all the variations and complexities of the real world [4], [5]. Furthermore, despite attempts to generate realistic synthetic behaviours of other road agents (e.g., vehicles, pedestrians, cyclists), simulation lacks empirical knowledge about their behaviour, which negatively affects the gap in behaviour and motion prediction, communication, and human-vehicle interaction [6].\nIncluding behaviours and interactions from real agents in simulators is one way to reduce the reality gap of autonomous driving digital twins. This can be addressed by using real-time immersive VR [7], [8]. The immersive integration of real subjects into digital twins allows, on the one hand, human-vehicle interaction studies in fully controlled and safe environments. Various HMI modalities can be included to explore extreme scenarios without risk to people and vehicle prototypes. On the other hand, it makes it possible to obtain synthetic sequences from multiple viewpoints (i.e., simulated sensors of AVs) based on the behaviour of real subjects, which can be used to train and test predictive perception models. However, this approach would only be valid if the behaviour of the subjects in the simulated environment is equivalent to their behaviour in a real environment. We refer to this difference in behaviour as the behavioural gap, and in order to model it, it is necessary to empirically assess the behaviour of subjects under equivalent real and simulated conditions.\nMeanwhile, the attempt to introduce autonomous driving into daily life makes it crucial to study humans-AVs interactions, as the absence of a driver has an impact on the perception of risk, trust [9] and the level of acceptance by all users [10], including non-driving passengers and external road agents (i.e., pedestrians, cyclists and other drivers) [2]. AVs are faced with the need to communicate their intentions using all available resources, which translates into the use of HMIs as a form of explicit communication. Nonetheless, some previous studies suggest the primary basis for crossing decisions taking by pedestrians is the implicit interaction, such us perceived vehicle speeds or safety gap sizes [11], [12]. Thus, the first interest of our research is to evaluate together an explicit form of communication (eHMI) and an implicit one, as in this case a different braking manoeuvre of the vehicle. In this paper, we present the results of the first part of our field study on human-AVs interactions, in a real-world crosswalk scenario [13] and which answers our first research question:\n\u2022 RQ1: To what extent do the variables \"eHMI\" and \"braking manoeuvre\" influence the crossing behaviour of a pedestrian in a real-world crosswalk in terms of (1) vehicle-gazing time, (2) space gap, (3) body-motion, and (4) subjective perception?\nOn the other hand, we employed a novel framework to insert real agents into the CARLA simulator [7], [8]. Through the CARLA tools and the added motion capture system,"}, {"title": "II. RELATED WORK", "content": "A. Understanding Pedestrian-AVs Interaction\nThe research of the interactions between pedestrians and AVs is essential to ensure the safety and public acceptance of this emerging technology [15], [16]. To date, numerous studies have been conducted to investigate the role of eHMIs and AV driving styles on the pedestrian crossing experience, in controlled real-world environments [13], [17] and in VR environments [14], [18], [19].\nAmong the eHMI forms commonly explored, we can find several lighting signals designs, textual messages, inclusion of anthropomorphic featuring or trajectory projection on the ground [20]\u2013[22]. For instance, an AV equipped with robotic eyes that look at the pedestrian or head-on helps them make more efficient crossing choices [23], [24]. Various approaches have studied the effect of light-based communication in Wizard-of-Oz designs in which automated driving is simulated that appears to be driverless [25], [26]. Despite the fact that in many cases visual messages can be displayed on an external surface to indicate the status of the vehicle (e.g., real-time predicted risk levels [27] or directional information [28]), some studies note that their participants prefer direct written instructions to cross the road (i.e., \"walk\" or \"stop\") [29], [30]. This could be misleading when the traffic situation involves more than one pedestrian [27] so road projection-based eHMIs may be an alternative for scalability to communicate vehicle intentions in shared spaces [31]\u2013[33]. Most of the research on eHMI development in virtual reality focuses on visual components, as commercially available hardware and software are at an early stage of development, which poses difficulties in creating multimodal experiences [34]. Auditory, haptic and interactive elements, such as the movement of participants and the virtual representation of their bodies, are mainly used to increase the sense of presence in the virtual environment. However, these elements could also enhance the authenticity of participants' reactions.\nIn another sense, it has also been shown that pedestrians use implicit communication signals to estimate the behaviour of the vehicle, and apply it to their decisions [35]. Moreover, leading works suggest that implicit information (i.e., their movement) may be sufficient [36] or that eHMIs only help convince pedestrians to cross the road when the vehicle speed is ambiguous [17]. Deceleration or the distance to the vehicle are more useful in interpreting the intention to yield than the drivers' presence and apparent attentiveness [37]. This type of communication has been found to be even more relevant in unmarked locations [38], [39].\nAlthough survey-based studies to assess human behaviour in traffic scenes are prevalent [40]\u2013[42], they fail to collect immediate feedback from experiments [43]. Recording-based studies allow direct measurements and help mitigate potential biases associated with self-reporting [44]. Metrics extracted from objective data can be treated as dependent variables and analysed using a linear mixed models, including road-crossing decision times, gaze-based times, crossing speed or distances to the vehicle [45], [46]."}, {"title": "B. Bridging the Simulation-to-Reality Gap", "content": "Testing in simulated environments offers some advantages over real-world testing, such as more safety for participants in the experiments and the facility of constructing scenarios [47]. This saves a lot of costs in terms of time and effort. However, differences in lighting, textures, vehicle dynamics and agents behaviour between simulated and real environments raise doubts about the validity of the results in this new context [48].\nThe first approach to assessing whether simulation-based testing can be a reliable substitute for real-world testing is to validate the virtual models of the sensors by determining whether their discrepancy with reality is sufficiently low. We found works that do this in the case of radar [49] and a camera-based object detection algorithm [50]. Typically, the gap between synthetic and real-world datasets is well-known [51], and there are already proposals to alleviate it as methods that obtain realistic images from those recorded in simulation or that bridge the differences in system dynamics [52], [53]. We emphasise that the gap worsens in multi-agent systems due to the complexity of transferring agent interactions and the synchronisation of the environment [54].\nOne of the strategies researchers employ to bridge the gap between simulation and reality in autonomous driving are the digital twins (DTs) [48], [55]\u2013[58]. Some study utilises a real small-scale physical vehicle and its digital twin to investigate the transferability of behaviour and failure exposure between virtual and real-world environments [4]. There have been no previous approaches to assess the gap in the behaviour of real agents (e.g., pedestrians) within a simulation, as we do in this work with a full-scale digital twin of a scenario and immersive VR for real-time interaction with an AV."}, {"title": "III. METHOD", "content": "A. Experiment Design\nThe currently study presents improvements over previous immersive VR experiments with pedestrians, since (i) it is conducted in the CARLA simulator [59] and not in Unity, which allows the use of highly specialised functions for autonomous driving, and (ii) a motion capture system is added to accurately collect the participants motion data. On one hand, we can assess interactions by the usual methods, such as eye contact with the vehicle or questionnaires [60].\nFurthermore, we combine explicit and implicit communication under safe conditions, and capture the behaviour of the participants by video and inertial sensors.\n1) Experiment Scenario Design: An existing crosswalk on the area of the University of Alcal\u00e1 (Spain), was chosen to perform the real driving tests and also as the baseline to construct the VR environment (see Fig. 1). In this scenario, an AV drives on a day with plenty of sunlight along a street in a straight line until it reaches a crosswalk. The pedestrian, who wishes to cross the road perpendicularly, needs to take 2-3 steps to have visibility to their left side (due to other parked vehicles and vegetation).\nThe map model is downloaded from OpenStreetMap [61] and converted to a Unreal Engine project where the elements are detailed. From the vehicle blueprints offered by CARLA we choose the model and colour of the physical vehicle and attach the sensors to perceive its surroundings (i.e., LiDAR, radar and cameras).\nIn order to facilitate interaction, the pedestrian waits with their back to the road and is instructed to turn around when the vehicle is at a distance of about 40 meters. As can be seen in Fig. 2, two braking manoeuvres were designed. In both cases, the vehicle travels at a speed of 30 km/h and applies a constant deceleration of -0.9 m/s\u00b2 (smooth) or -1.8 m/s\u00b2 (aggressive) until it comes to a complete stop in front of the crosswalk and yields the right-of-way. This is done to study whether the pedestrian perceives the situation as more risky when the vehicle brakes with less anticipation and the time-to-collision (TTC) is smaller.\nTo alert the pedestrian of its intention to yield, the vehicle was equipped with the GRAIL (Green Assistant Interfacing Light) system [62]. As shown in Figs. 3a and 3b, the AV uses green to indicate awareness of the pedestrian (which implies that it will stop if necessary), and red to warn that nothing prevents it from continuing on its way. It is also possible that the interface is deactivated so the pedestrian does not have any explicit information about the vehicle intention. This front-end design is sufficient for the specific scenario of this work. However, more complex scenarios with poorer"}, {"title": "B. Virtual Reality Apparatus", "content": "Tests under simulated driving conditions were conducted in a VR space of 8 meters long x 3 meters wide. The virtual environment was constructed under a 1:1 scheme mapped to the real-life environment, so participants adopted the real-walking locomotion style, leading to a more realistic movement and a greater sense of presence.\nWe use a specific framework for the insertion of real agents in CARLA [7], [8]. An immersive interface is enabled in VR for the incorporation of a pedestrian into the traffic scene. Some of the features added to the simulator were real-time avatar control, positional sound or body tracking. The Meta Quest 2 headset was connected via WiFi to a Windows 10 desktop and an NVIDIA GeForce RTX 3060 graphics card. We chose Perception Neuron Studio [64] as the motion capture system to record the user's pose and integrate it into the scenario."}, {"title": "C. Experiment Procedure", "content": "The experimental procedure differed between the real and virtual contexts, yet it could be distinctly delineated into four phases:\n1) Introduction: At the beginning, participants were provided with written information about the experiment, such as its purpose, the explanation of the AV and the functionality of the eHMI. They were also assigned a unique anonymous identifier and were assured of their ability to discontinue the experiment at any time if they so desired. Lastly, they were asked to sign the consent to participate as subjects in the study.\n2) Familiarisation (warm-up): Participants were aided in donning the inertial sensors and VR headset, following which they were invited to explore the virtual environment void of any vehicular traffic. Subsequently, the Perception Neuron system underwent calibration, and the initial task of the experiment (task 0) was conducted as an illustrative example.\n3) Experimentation: Throughout this phase, participants completed the four tasks of the experiment (see Table I) while answering questions posed by an accompanying researcher about their subjective perception.\n4) Filling in the post-questionnaire: After concluding the experiment, participants removed the VR headset and inertial sensors and, in both the real and virtual context, were asked to fill out a post-questionnaire."}, {"title": "D. Data Collection", "content": "During the experiment various types of data were collected to analyse the resulting pedestrian-AV interactions, including objective measurements (i.e., movement path, gaze time) as well as responses to questionnaires.\nIn the first instance, the AV in real configuration was fitted with a RTK-GPS system that provided its precise position with respect to the crosswalk and served as a reference for applying the braking manoeuvre, while an external camera mounted on the top of the vehicle recorded the environment at 10 Hz. Within Unreal Engine 4 and Axis Studio [7], [8], all data from the VR experiment were recorded as the (1) timestamp, (2) vehicle's position and parameters (i.e., coordinate x, y, z, rotation, brake, steer, throttle, gear), (3) participant's position and animation (i.e., coordinate x, y, z, rotation, .fbx), and (4) playbacks of the Quest 2 view, the VR setup, and from within the simulator, synchronised at 18.8 Hz.\nThe questionnaire collected participant's information (e.g., age, gender, familiarity with AVs and with VR) and subjective feedback on the influence of the different types of communication in each interaction through the following questions:\nQ1: How safe did you feel at the scene?\nQ2: How aggressive did you perceive the braking manoeuvre of the vehicle?\nQ3: Did the visual communication interface improve your confidence to cross?"}, {"title": "E. Participant's Characteristics", "content": "A total of 18 participants, aged between 24 and 62 years (M = 40.11, SD = 11.62), with a gender distribution of 33% women and 67% men, were recruited from both inside and outside the university area and engaged in the experiment. In regard to familiarity with AVs, 38.9% had extensive knowledge of the subject, another 38.9% considered that they had an average knowledge of Advanced Driver Assistance Systems (ADAS), 44.4% had previously interacted with an AV (either as a user or pedestrian) compared to 55.6% who had not, and 22.2% had no prior exposure or understanding of AVs. For VR experience, the majority of participants had either never used VR goggles (50%) or had only tried them once before (33.3%). All participants had normal vision or wore corrective glasses (22.2%) that they kept when fitting the VR headset, and had normal mobility, so they were able to complete the experiment successfully."}, {"title": "F. Data Analysis", "content": "Different metrics can be acquired from the objective data (i.e., movement trajectory, gaze point) gathered during the experiments. The metrics chosen for analysis in this research are defined as follows:\n\u2022 Vehicle-gazing time while crossing (Tc): it represents the cumulative duration of gazing at the AV while crossing, as inferred from the collected eye gazing data.\n\u2022 Crossing initiation time (CIT): computed as the interval from when the pedestrian visually identifies the AV until s/he decides to cross. If the pedestrian crosses before noticing the AV, then CIT is zero.\n\u2022 Vehicle-gazing time (Tav): it represents the cumulative duration of gazing at the AV throughout the entire crossing process, as inferred from the collected eye gazing data. That is, Tav = CIT +Tc.\n\u2022 Space gap (L): the distance between the AV and the pedestrian, measured from the AV to the centre of the crosswalk when the pedestrian decides to cross.\n\u2022 Pace cadence (Fp): defined as the dominant step frequency at which the pedestrian crosses the road.\n\u2022 Gait cycles (G): referring to the number of gait cycles when the pedestrian makes the decision to cross, along with the stabilisation times of the two ankles.\nTo obtain the above indicators, the crossing intention event is defined as the moment the pedestrian decides to cross the crosswalk and is extracted from the video recordings and the reconstructed trajectory in the virtual environment. The rules for identifying the event state are the following:\n1) In case the pedestrian is stopped and starts to move into the crosswalk, the decision is made at the first frame in which the movement is discernible.\n2) If there is not a stop and the pace is slowed, the decision occurs at the frame the pedestrian starts accelerating.\n3) If there is no alteration in the pedestrian's speed, the decision is made upon sighting the vehicle.\n4) If the pedestrian does not look at the vehicle, we take the first frame when the pedestrian appears on the vehicle's front camera."}, {"title": "IV. RESULTS", "content": "This section presents the results obtained in the experiment with the real and virtual setup. As can be seen in Fig. 5, the VR headset projects the crosswalk onto its lenses and allows mobility around the scene. We aim to examine the significant effects of implicit and explicit vehicle communication on pedestrian crossing behaviour.\nA. Vehicle Gazing (Tav) and Crossing Initiation Times (CIT)\nTo establish categorical statements about the impact of the braking manoeuvre or eHMI on the crossing decision, we utilise the Student's t-test [66]. The procedure for this test involves calculating the difference between the means of two groups of samples and adjusting this difference for within-group variability and sample size. This adjusted difference is compared to a probability t-distribution to determine if it is large enough to be considered significant. If this happens with the means of the data extracted from the experimental tasks, the null hypothesis (Ho : \u03bc\u2081 \u2264 \u03bc;) is rejected in favour of the alternative hypothesis (H\u2081 : \u03bc; > \u03bcj). Fig. 6 shows the box-plots of the gaze duration to the vehicle in the tests, considering combinations of two factors: deceleration type and activation of the eHMI (see Table I for details).\nThe Table II expresses categorical statements, i.e., a 1 in a cell means rejection of the null hypothesis and acceptance of the alternative hypothesis with a confidence level of 95%, meaning the gaze times in task i (in the row) are significantly larger than those in task j (in the column).\nA first aspect to highlight is that the active eHMI decreases the observation times in the two experimental setups (Tav: t1 > t3 and t2 > t4). This effect cannot be appreciated as directly comparing the two types of deceleration, since the vehicle approaches at different speeds and does not reach the crosswalk at the same time. To analyse the time pedestrians spend observing the vehicle before crossing, we must focus on the CIT, which eHMI shortens by maintaining a smooth deceleration (CIT: t1 > t3). The same impact of eHMI during aggressive deceleration is only seen in the virtual setup (CIT: t2 vs t4).\nWhen comparing the two scenarios, rather than making categorical statements, we show the probability of significant discrepancy between the tasks that will be used at the end of the research to quantify the behavioural gap. It is evident from the data provided in Table III that the gaze duration is greater in the virtual environment. Upon separately examining the time preceding and following the decision to cross, we see that the disparity is less pronounced within the CIT. In the absence of eHMI, pedestrians observe more of the vehicle before crossing in the virtual setup (CIT: tlvirtual > tlreal and t2virtual vs t2real) while, if eHMI is activated, the CIT resembles more closely. The notable differences in vehicle gazing times between both setups and across all experiment variations occur while walking on the road (Te: ti,virtual > ti,real). This suggests that pedestrians pay significantly more attention to the vehicle after making the decision to cross when they are interacting in the virtual environment."}, {"title": "B. Space Gap (L)", "content": "Box-plots of the space gap in each task (i.e., the distance between the pedestrian and the AV at the crossing decision) are depicted in Fig. 7 for both real and virtual environments. In addition, Table IV shows the results of the Student's t-test to evaluate the impact of eHMI and the type of manoeuvre. With a confidence level of 95%, we assert that the smooth braking manoeuvre increases the distance to the vehicle when pedestrians decide to cross (Space gap: t1 > t2 and t3 > t4). The same applies to eHMI activation while maintaining the smooth braking manoeuvre (Space gap: t3 > t1). However, although the impact of the eHMI persists in the virtual setup by maintaining aggressive braking, this is not the case in the real setup (Space gap: t4 vs t2).\nThe Table V provides a direct comparison of space gaps across both setups. The findings indicate that the participants cross significantly earlier in the real setup (i.e., with a larger space gap) when the eHMI is deactivated (Space Gap: tlreal > tlvirtual and t2real > t2virtual). Concerning experimental tasks which employ explicit communication (t3 and t4), the values of space gap exhibit greater similarity, leading to the"}, {"title": "C. Body Motion", "content": "Among the advantages of inserting real agents into a simulation environment [7], [8] is the possibility of generating synthetic sequences from various perspectives and configurations. To accomplish this, it is necessary to reconstruct the trajectory and 3D pose of the participant within the scenario, for which Perception Neuron's sensors and software provide an .fbx file over time [64]. This approach allows an accurate analysis of the participant's motion style throughout the experiments.\nWithin the scope of this research, we aimed to establish a methodology for acquiring motion metrics that could be standardised between both real-world and virtual environments. Employing a whole-pose estimator [68], we identify the keypoints of the pedestrian's body in images captured by the front camera of the AV in the real environment (recall Fig. 4). Subsequently, the 3D keypoints localised in the virtual environment are projected onto the plane parallel to the crosswalk, aligning with the format of the 2D estimator output. Table VI outlines the body proportions derived from both procedures for constructing the pedestrian avatar.\nDespite the non-correspondence of the keypoints given by Perception Neuron and the 2D estimator, this strategy allows us to conduct an equivalent analysis of the pedestrian's gait from the vehicle's perspective in the two environments. To calculate the pedestrians' pace while crossing, we apply the Fast Fourier Transform (FFT) [69] on the lateral position on"}, {"title": "D. Subjective Measures", "content": "To make categorical statements regarding the influence of the braking manoeuvre or eHMI on participants' questionnaire responses, we use the Wilcoxon signed-rank test [67] which is an alternative to Student's t-test when working with ordinal or interval scales. The procedure for this non-parametric statistical test utilised to compare two related samples involves arranging the values of the absolute differences between the two samples and subsequently calculating a sum of ranks to determine whether the difference between the samples is statistically significant. The null hypothesis of the Wilcoxon test is that there is no difference between the two samples (Ho : \u03bc\u03b5 \u2264 \u03bcj), while the alternative hypothesis is that there is a significant difference (H\u2081 : \u03bc\u03b5 > \u03bc\u03ae).\nTable IX provides categorical statements, where a 1 in a cell implies rejection of the null hypothesis and acceptance of the alternative hypothesis meaning that the responses to a question in task i (in the row) are significantly greater than those in task j (in the column).\nWith a confidence level of 95%, we assert that activating the eHMI enhances the pedestrian's perception of safety (Q1: t3 > t1 and t4 > t2). On the other hand, the smooth braking manoeuvre also increases the feeling of safety, although it is an effect that is not perceived within the virtual setup when the eHMI is disabled (Q1: t3 > t4 and t1 vs t2). Participants appreciate the difference between the smooth and aggressive type of maneuver (Q2: t2 > t1, t3 and t4 > t1, t3). It is worth noting that in the virtual setup the non-activation of the eHMI makes the same manoeuvre feel even more aggressive (Q2: t2 vs t4). Lastly, eHMI is considered to be useful (Q3: t3 > t1, t2 and t4 > t1, t2).\nTable X presents direct comparisons between the questionnaire responses collected from the two setups. Pedestrians feel less safe in the virtual setup when the AV does not communicate its intentions explicitly (Q1: tlreal > tlvirtual and t2real > t2virtual). In addition, they suggest the virtual eHMI has a more positive impact on their decision-making process (Q3: t3virtual > t3real).\nAssessing the sense of presence during the VR experiment can help to uncover the reasons of discrepancies in pedestrian crossing behaviour between the real and virtual testing setup. Self-presence measures how much users project their identity into a virtual world through an avatar, while autonomous vehicle and environmental presence examine how users interact with mediated entities and environments as if they were real. Most of the participants perceived the avatar as an extension of their body (M = 4.04, SD = 0.95), including when moving their hands or walking on the road. The vehicle presence was well rated (M= 3.94, SD = 0.97), although not all participants heard the sound of the engine or felt any braking manoeuvre threatening. Environmental-presence (M = 4.34, SD = 0.63) was the most satisfactory, as they claimed to have the feeling of actually being at a crosswalk."}, {"title": "V. DISCUSSION", "content": "A. Variables Influence in a Real Environment (RQ1)\nQuantitative data shows that participants in the real-world crosswalk experiment notably extended the Space Gap when making their crossing decision if the AV performed a smooth braking manoeuvre. On the contrary, the impact of the \"eHMI\" factor seemed evident solely when activated alongside gentle braking. Activation of the visual interface did not accelerate pedestrian crossing decisions in instances of aggressive braking manoeuvres. Nevertheless, in the questionnaires, they indicated that both a braking manoeuvre signalling the vehicle's intention to yield and the activation of the eHMI conveyed a greater sense of safety compared to the opposite scenario. The FFT also notes that explicit communication encouraged them to cross the road faster after entering in the lane, while leading to a decrease in eye contact with the AV.\nB. Variables Influence in a Virtual Environment (RQ2)\nIn the virtual crosswalk experiment, the results reveal that both the smooth braking manoeuvre and the eHMI activation widen the Space Gap when pedestrians decide to cross. In the questionnaires, they report feeling safer when the eHMI is active compared to when it is not, and express a preference for smooth over aggressive braking, but only when the eHMI is operational. Explicit communication results in participants spending less time making eye contact with the AV to assess hazards. Additionally, according to FFT, it prompts them to walk slightly faster once they have entered the lane.\nC. Measuring the Behavioural Gap (RQ3)\nA first point to note is that the Student's t-test shows that the space gap L is significantly higher in the real environment than in the virtual environment when the visual interface (i.e., eHMI) is disabled. This finding is supported by the CITs, as pedestrians who spend more time observing the approaching vehicle encounter a smaller space gap L when they eventually decide to cross. Still, we must mention that this discrepancy in the crossing behaviour between the real and virtual testing setup disappears when the eHMI starts working. The CITs and the distance separating the pedestrian from the AV when deciding to cross then are not noticeably different.\nThe responses to the questionnaire follow the same line of argument. Participants perceive a greater sense of safety in the real environment compared to the virtual environment when the eHMI is deactivated, and feel equally safe when it is activated. This leads us to think the eHMI contributes to increased confidence in the experiment and prompts participants to make the decision to cross earlier. Furthermore, this effect is particularly pronounced in the virtual environment, where the eHMI is most prominently visible, as reported by Q3. Not activating the eHMI heightens the perception of the virtual AV's aggressive braking as even more aggressive (Q2: t2virtual > t4virtual).\nTo gather the evidence on the existence of the behavioural gap we employ the Fisher's method [70], a statistical technique utilised to combine the results of independent significance tests performed on the same data set. The Fisher's"}, {"title": "VI. CONCLUSIONS AND FUTURE WORK", "content": "This study advances our understanding of the gap between simulation and reality in contexts that incorporate the activity of real agents for autonomous driving research. The digital twin of a crosswalk and an AV was crafted by replicating its driving style and the design of the eHMI it featured, within the CARLA simulator. The participants, who had no previous experience in VR, acted more cautiously in their role as a pedestrian in the simulation by delaying lane entry, slowing their movements and paying more attention to all elements of the environment. This did not prevent us from corroborating the impact of implicit and explicit vehicle communication on the crossing behaviour of pedestrians introduced into the virtual environment. Based on our findings, participants prioritised implicit communication over explicit communication in the real-world scenario, whereas in the VR tests, their decisions were more influenced by explicit communication.\nFor future work in this field, we emphasise the importance of familiarising the participants with the VR environment, not only by proposing them to explore the virtual world for a few minutes before starting the tests, but also by involving them in simulated examples with vehicular traffic and street crossings that do not count for the drawing of conclusions. In order to resemble the effects of the braking manoeuvre and eHMI in the simulator to those in the real-world, techniques could be implemented to enhance the AV presence rating through more realistic motion dynamics and an engine sound that commensurate with its revolutions. In addition, the brightness of the virtual eHMI could be adjusted to match its showiness in the real environment. If sufficient data were available, a more automatic approach to assessing behavioural gap could be achieved, e.g., by learning behavioural differences within a particular scenario and subsequently generating corresponding scores or distances."}, {"title": "SAFETY AND ETHICAL CONSIDERATIONS", "content": "The fundamental pillar guiding the design of the experiments has been the safety and comfort of all participants above any other consideration. On one hand, we chose to implement Level 3 automation in our real testing conditions, despite the fact that Level 4 automation could have been possible. This decision necessitated the presence of a backup driver ready to resume control when needed. In addition, a human supervisor in the rear seats was monitoring the status of all perception and control systems, including access to an emergency stop function. Therefore, human intervention was always possible, both by the backup driver and the supervisor. On the other hand, the braking profile was designed to be extremely conservative, maintaining a substantial margin for reaction, prioritising safety above all else. Furthermore, we rigorously followed internal and institutional ethical assessment and validation procedures, which included informing the participants and obtaining their written consent, ensuring data privacy, allowing subjects to withdraw from the experiments at any time, and implementing data anonymisation, among other protocols."}, {"title": "ABOUT THE AUTHORS", "content": "Sergio Mart\u00edn Serrano is a PhD student at the University of Alcal\u00e1. His research interests are focused on the analysis of Vulnerable Road Users (VRUs) behaviours using Virtual Reality (VR) and autonomous driving simulators, covering predictive perception and human-vehicle interaction.\nRub\u00e9n Izquierdo is Assitant Professor at the University of Alcal\u00e1. His research interest focuses on prediction of vehicles' behaviour, human-vehicle interaction, and control algorithms for highly automated and cooperative vehicles.\nIv\u00e1n Garc\u00eda Daza is Associate Professor at the University of Alcal\u00e1. He specialises in Deep Reinforcement Learning for decision-making in autonomous vehicles and in the application of Explainable AI to decision-making systems.\nMiguel \u00c1ngel Sotelo is Full Professor at the University of Alcal\u00e1. His research interests focus on road users' behaviour understanding and prediction, human-vehicle interaction, and Explainable AI for decision-making in autonomous vehicles.\nDavid Fern\u00e1ndez Llorca is Scientific Officer at the European Commission - Joint Research Centre, and Full Professor at the University of Alcal\u00e1. His research interests include trustworthy AI for transportation, human-centred autonomous vehicles, predictive perception, and human-vehicle interaction."}]}