{"title": "Densing Law of LLMs", "authors": ["Chaojun Xiao", "Jie Cai", "Weilin Zhao", "Guoyang Zeng", "Biyuan Lin", "Jie Zhou", "Xu Han", "Zhiyuan Liu", "Maosong Sun"], "abstract": "Large Language Models (LLMs) have emerged as a milestone in artificial intelli-\ngence, and their performance can improve as the model size increases. However,\nthis scaling brings great challenges to training and inference efficiency, particularly\nfor deploying LLMs in resource-constrained environments, and the scaling trend is\nbecoming increasingly unsustainable. This paper introduces the concept of \u201ccapac-\nity density\u201d as a new metric to evaluate the quality of the LLMs across different\nscales and describes the trend of LLMs in terms of both effectiveness and efficiency.\nTo calculate the capacity density of a given target LLM, we first introduce a set\nof reference models and develop a Scaling Law to predict the downstream perfor-\nmance of these reference models based on their parameter sizes. We then define\nthe effective parameter size of the target LLM as the parameter size required by\na reference model to achieve equivalent performance, and formalize the capacity\ndensity as the ratio of the effective parameter size to the actual parameter size of\nthe target LLM. Capacity density provides a unified framework for assessing both\nmodel effectiveness and efficiency. Our further analysis of recent open-source base\nLLMs reveals an empirical law (the Densing Law) that the capacity density of\nLLMs grows exponentially over time. More specifically, using some widely used\nbenchmarks for evaluation, the capacity density of LLMs doubles approximately\nevery three months. The law provides new perspectives to guide future LLM\ndevelopment, emphasizing the importance of improving capacity density to achieve\noptimal results with minimal computational overhead.", "sections": [{"title": "1 Introduction", "content": "In recent years, large language models (LLMs) have garnered significant attention in the field of\nartificial intelligence, demonstrating remarkable improvements across various tasks (Bommasani\net al., 2021; Qiu et al., 2020; Han et al., 2021; Touvron et al., 2023a; OpenAI, 2023). The Scaling\nLaw for LLMs further reveals that model performance continues to improve as model parameters\nand training data increase (Kaplan et al., 2020; Henighan et al., 2020; Hoffmann et al., 2022). This\ndiscovery has led to the development of LLMs with hundreds of billions of parameters, such as GPT-3\n175B (Brown et al., 2020), PaLM 540B (Chowdhery et al., 2023), and Llama-3.1-405B (Dubey et al.,\n2024), which have demonstrated exceptional capabilities in a wider range of applications.\nBesides, with the advancement of LLMs, enhancing inference efficiency has become increasingly\nurgent: 1) As LLMs are deployed in an expanding array of scenarios, inference costs have surpassed\ntraining costs, becoming the main bottleneck in practical applications (Sardana et al., 2024; Yun et al.,\n2024; OpenAI, 2024a). 2) There is a growing need to deploy LLMs on resource-constrained end\ndevices like smartphones, serving as personal assistants, which requires models to be more efficient\nand compact (Gunter et al., 2024; Xue et al., 2024; Hu et al., 2024). 3) The inference Scaling Law\nindicates that allowing LLMs to generate more tokens for \u201cthinking\u201d during the inference stage is\ncrucial for improving performance in complex reasoning tasks (Brown et al., 2024; OpenAI, 2024b;\nSnell et al., 2024), further increasing the demand for efficient inference. To address these challenges,\nmany efforts have been devoted to developing efficient LLMs with only billions of parameters to\nreduce inference overhead, such as OpenAI's GPT-40-mini (OpenAI, 2024a) and Apple's apple\nintelligence (Gunter et al., 2024).\nGiven these two seemingly contradictory paths \u2013 scaling up LLMs for effectiveness versus scaling\ndown LLMs for efficiency \u2013 natural questions arise: Can we quantitatively evaluate the quality of\nLLMs with different scales? Is there a law that reflects the efficiency trend in LLMs, like the Scaling\nLaw does for parameter and data scales?\nTo this end, we introduce the concept of capacity density, which serves as a metric for evaluating and\ncomparing the training quality of LLMs on various scales. Accurately measuring all aspects of an\nLLM's capabilities, or its level of intelligence, is quite challenging. In this article, we design a method\nto assess the relative capacity density\u00b2. Specifically, we use a reference model and then estimate\nits scaling function between the performance on downstream tasks and parameter sizes. Based on"}, {"title": "2 Density for Large Language Models", "content": "In this section, we formally define the density for LLMs, which is calculated as the ratio of the\neffective parameter size to the actual parameter size. In the following sections, we will first describe\nthe overall framework and formal definition of LLM density. Then we introduce how to utilize the\nScaling Law to estimate the effective parameter size."}, {"title": "2.1 Overall Framework and Definition", "content": "The core of LLM density lies in the effective parameter size, which refers to the number of parameters\nrequired for a reference model to achieve the same performance as a given model. To achieve this,\nwe need to fit a function that relates the parameter sizes of the reference model to its performance.\nSpecifically, for a given model M with NM parameters, assume its performance score on the\ndownstream tasks is SM. This score can be calculated using various metrics depending on the\ndownstream task, such as accuracy, F1 score, etc. To compute the effective parameter size, we train\na series of reference models with varying scales of parameters and training data. Based on these\nmodels, we fit a function between the parameter size and performance: S = f(N), where S denotes\nthe downstream performance, and N represents the parameter sizes of the reference model. Then we\ncan calculate the effective parameter size as \u00d1(S) = f-1(S) and the density for M is defined as:\n$\\rho(\u039c) = \\frac{\\tilde{N}(S_M)}{N_M} = \\frac{f^{-1}(S_M)}{N_M}$                                                                 (1)"}, {"title": "2.2 Loss Estimation", "content": "To predict the performance of downstream tasks, the first step involves fitting a function between the\nparameter size and language model loss using the Scaling Law widely adopted for LLM pre-training.\nPrevious Scaling Laws primarily focus on language modeling loss on the whole sequences, which\nreflects the model's ability to estimate the probability of a given corpus. However, instances in the\ndownstream tasks usually encompass both input instructions and output answers and we are primarily\nconcerned with the probability of the output answers. Therefore, in this work, we focus on fitting the\nconditional loss L = -log(P(answer | instruction)). Concretely, we estimate a power-law function\nbetween the conditional loss L, and parameter size N, as well as the number of training tokens D:\n$L = aN^{-\\alpha} + bD^{-\\beta}$,                                                                            (2)\nwhere a, \u03b1, b, and \u03b2 are parameters need to be fitted.\nIn previous research on Scaling Laws (Kaplan et al., 2020), the loss typically needs to be specified on\na validation corpus, and the average loss is calculated over all tokens in this corpus. In this work,\nour goal is to fit the model's performance on downstream tasks, which require models to output\nthe answers based on the input instructions. Therefore, we directly calculate the conditional loss\non downstream tasks, meaning the loss incurred by the model when generating answers given the\ntask inputs. (1) For multiple-choice problems, calculating the loss solely based on the content of\nthe correct option can lead to inaccurate estimates, as it ignores the content of incorrect options.\nBesides, if we only calculate the loss on the final option labels, the loss for single token is also\nunstable. Therefore, we concatenate the problem and its multiple options as inputs, and the output is\nthe analysis for the input problem as well as the final answer label. (2) For most complex problems,\nsuch as mathematical questions, we often require the model to generate a sequence of reasoning steps\nbefore providing the final answer. For these tasks, when calculating the loss, we include both the\nreasoning steps and the correct answer as the output to compute the model's loss. It is important to\nnote that most datasets do not provide reasoning steps for each instance. For both two types of tasks,\nwe use GPT-40 (OpenAI, 2023) to generate reasoning steps for all test instances. These approaches\nallow us to better estimate the model's performance by considering the specific requirements and\nformats of different tasks."}, {"title": "2.3 Performance Estimation", "content": "In the second step, we need to predict downstream task performance based on the loss on test sets.\nIn the loss estimation step, the Scaling Law models trained with limited training computes usually\ncannot achieve meaningful scores on downstream tasks, with most Scaling Law models performing\nonly at the level of random guessing. Thus, it is impossible to predict the downstream performance\nwith only these models. To address this issue, we incorporate well-trained open-source models\nfor function fitting and calculate their loss and performance on the test set. Considering that the\nperformance for most downstream tasks is bounded, we use a sigmoid function for fitting. The\nsigmoid function naturally maps all input values to the range of 0 to 1. Additionally, when the loss\nis particularly large, the model's performance should approximate that of random guessing, and\nwhen the loss is particularly small, the model's performance should approach the upper bound. This\ncharacteristic aligns with the properties of the sigmoid function, which is very flat at both extremes"}, {"title": "2.4 Density", "content": "After fitting Equation 2 and 3, given the performance SM of a model M, we can infer the effective\nparameter size by utilizing the inverse functions of these equations. It is important to note that in\nEquation 2, the loss L is a bivariate function of both the parameter count N and the training data size\nD. Therefore, when calculating the effective parameter size, it is necessary to specify a particular\ntraining data size D. Here, to calculate the effective parameter size, we defaultly use D = D0 = 1T\ntokens. Then the effective parameter size can be explained as the parameter size the reference model\ntrained with Do tokens needs to achieve equivalent performance. Concretely, we can compute the\neffective parameter size as:\n$L(S_M) = l - \\frac{1}{\\gamma} ln (\\frac{c}{S_M - d} -1); \t \\tilde{N}(S_M) = (\\frac{L(S_M) - bD_0^{-\\beta}}{a})^{-\\frac{1}{\\alpha}}$                                                                               (4)\nNow, we have established the relationship between the downstream performance and effective\nparameter size. The density of the given model M is $\\rho(M) = \\frac{\\tilde{N}(S_M)}{N_M}$. Intuitively, if one model can\nachieve better performance with the same scale of parameters, then the model's density is higher.\nTherefore, in the future, considering the limited computation resources of deployment devices, we\nshould devote great effort to improving the model's density instead of merely increasing the model\nparameter scales for better performance."}, {"title": "3 Density Evolution", "content": "In this section, we evaluate various empirical results from the perspectives of loss estimation and performance estimation."}, {"title": "3.1 Evaluation Settings", "content": "Dataset In this work, we adopt the following widely-used datasets for evaluation: MMLU (Hendrycks\net al., 2020) for English knowledge-intensive tasks, BBH (Suzgun et al., 2023) for challenging\nlogic reasoning tasks, MATH (Hendrycks et al., 2021) for mathematical reasoning tasks, and Hu-\nmanEval (Chen et al., 2021), MBPP (Austin et al., 2021) for coding tasks. We apply the open-source\ntools (OpenCompass, 2023; Liu et al., 2024) for evaluation. Here, we evaluate all models in a few-shot\nin-context learning manner and these models are required to generate the final answer label based\non the given demonstrations and inputs of test instances. Following widely-used settings, MMLU,\nBBH, MATH, HumanEval, and MBPP are evaluated under the 5-shot, 3-shot, 4-shot, 0-shot, and\n3-shot settings, respectively. Besides, for BBH, MATH, and MBPP, we adopt the chain-of-thought\nprompting technique (Wei et al., 2022b).\nLoss Estimation Models In the loss estimation step, we need to run a series of models with different\nscales of parameters and training data. These models will be used as the reference models for further\ndensity computation. In this work, we adopt the training corpus of MiniCPM-3-4B (Hu et al., 2024),"}, {"title": "3.2 Loss and Performance Estimation Results", "content": "We present the estimation results of the two-step process in Figure 2. From the results, we can observe\nthat the two-step estimation process can effectively fit the performance of different-sized models\non three downstream tasks. With the decrease in the loss on the test instances, the performance\nsignificantly improves as a sigmoidal curve, and the loss has a power-law relationship with the number\nof parameters and training tokens.\nTo evaluate the effectiveness of our estimated method, we use models with parameters of less than\n4 billion to fit the loss-performance curve and preserve larger models for prediction. Triangles in\nFigure 2(b) are two models with tens of billions of parameters. From the results, we can observe that\nwe effectively predict the downstream performance based on the loss values."}, {"title": "3.3 Densing Law", "content": "After fitting the loss scaling curve and the performance scaling curve, we further measured the density\nof widely used open-source models since the release of Llama-1 (Touvron et al., 2023a). We present\nthe density of each model along with their release dates in Figure 1. From the figure, we can observe\nthat: (1) The density of LLMs has rapidly increased over time. Notably, the density of Llama-1,\nreleased in February 2023, is below 0.1, whereas more recently released models like Gemma-2-9B\nand MiniCPM-3-4B have densities reach 3. This increase in density is largely attributed to the growth\nin the scale of pre-training data and improvements in the quality of that data. For example, Llama-1\nis pre-trained on 1.4 trillion tokens, whereas Llama-3 utilizes 15 trillion tokens with careful data\ncleaning. (2) Better performance does not always lead to better density. Llama-3.1-405B is currently\none of the state-of-the-art open-source models due to its large-scale parameters. However, it is not\nthe model with the highest density. This is because constrained by computational resources and the\nscale of pre-training data, we usually cannot fully optimize the training settings for extremely large\nmodels, making them sub-optimal in terms of cost-effectiveness.\nTo further illustrate the growth trend of the LLMs' density, we perform a linear fit on the envelope\nline in Figure 1. Specifically, we assume that the logarithmic value of the maximum density increases\nlinearly over time. Formally, we fit the following linear function:\nln($\\rho_{max}$) = At + B,                                                                                          (5)\nwhere t is the time interval (unit: days) since the release date of Llama-1, \u03c1 is the maximum\ndensity value at time t, and A, B are the parameters to be fitted. Through the fitting process, we"}, {"title": "3.4 Corollaries of Densing Law", "content": "Based on Densing Law and our evaluation results, in this section, we discuss several corollaries and\nhope our discovery can promote the development of LLMs.\nInference Costs Decrease Exponentially The density of LLMs shows an exponential growth trend,\ndoubling approximately every three months. Here, density is defined as the ratio of the effective\nparameter size to the actual parameter size. This implies that in three months, we can achieve\nperformance comparable to current models using only half the actual parameter size. Consequently,\nunder the condition of achieving the same performance, the actual parameter size of LLMs will also\ndecrease exponentially. This reduction in actual parameter count translates to decreased computational\ncosts during inference. Therefore, the exponential increase in LLMs' density will directly result in an\nexponential decrease in inference costs for models achieving the same level of performance.\nTo better illustrate the decreasing trend in in-\nference costs for LLMs, we present the API\npricing of LLMs that have achieved superior\nperformance to GPT-3.5 since its release in Fig-\nure 3. From the figure, we can observe that the\nprices of LLMs exhibit an exponential decline.\nSpecifically, in December 2022, GPT-3.5 cost\n$20 for one million tokens, whereas by August\n2024, Gemini-1.5-Flash costs only $0.075 for\nthe same number of tokens, a reduction of 266.7\ntimes. Roughly speaking, the inference costs for\nLLMs halve approximately every 2.6 months.\nThe exponentially decreasing trend of LLM API\npricing is also observed in Appenzeller (2024).\nIn addition, we can observe that the rate of de-\ncline in inference costs is faster than the growth\nrate of LLMs' density. This is because infer-\nence costs depend not only on the actual param-\neter size but also heavily on the inference infras-\ntructure. In recent years, inference systems for\nLLMs have garnered significant attention from\nresearchers, including optimizations in mem-\nory access speed for self-attention layers (Kwon\net al., 2023; Dao et al., 2022; Dao, 2023) and\nsparse computation optimizations for feed-forward networks (Song et al., 2023; Liu et al., 2023).\nThese advancements have greatly contributed to the reduction in inference costs for LLMs.\nDensing Law Meets Moore's Law Densing Law describes the exponential trend of increasing model\ndensity over time, focusing on improvements at the algorithmic level of LLMs. On the other hand,\nMoore's Law, which states that computing power increases exponentially, highlights advancements\nin hardware technology (Moore, 1965). The combination of these two principles suggests a rapidly\napproaching future where high-quality LLMs can run efficiently on consumer-grade devices, such"}, {"title": "4 Discussion", "content": "Accurate Capacity Measurement Capability density reflects the abilities of an LLM per unit of\nparameters. However, with current technology, we cannot accurately assess the absolute capability\nlevel of LLMs, meaning that quantifying intelligence remains a great challenge. Therefore, in this\nwork, we design a method to measure the relative density value of LLMs. Besides, we use widely-\nused benchmarks to evaluate the performance of LLMs. However, the limited number of benchmarks\nand potential data contamination issues introduce bias in performance evaluation. Thus, advancing\naccurate measurement of LLMs' capabilities or intelligence levels in the future will enable better\ncalculation of their density.\nConnection between Densing Law and Scaling Law The Scaling Law of LLMs reveals the\nrelationship between an LLM's performance and its parameter and data sizes, reflecting the intrinsic\ncharacteristics of complex systems composed of vast numbers of neurons. The Densing Law further\nhighlights the trend in the development of LLMs' efficiency and effectiveness over time, marking\na technological advancement trend as humanity pursues high-level AI models. Formally, under\nconditions of sufficient training data, the Scaling Law explains the relationship between model loss"}, {"title": "5 Limitations and Future Directions", "content": "In this section, we discuss the limitations and future directions of our proposed method to evaluate\nthe capacity density of LLMs.\nFair and Comprehensive Evaluation The capacity density measurement of LLMs relies on existing\nbenchmarks to evaluate model performance. Therefore, the benchmark quality greatly impacts the\ndensity measurement results. In this work, we use those benchmarks widely adopted by researchers\nto evaluate various LLMs. However, several challenges remain: (1) Comprehensive evaluation: With\nthe development of LLMs, the capabilities of LLMs significantly expand, such as the ability to handle\ncomplex reasoning tasks (OpenAI, 2024b). Consequently, the capacity density measurement needs to\nbe continually updated by incorporating more comprehensive evaluation datasets that reflect evolving\ncapabilities. (2) Fair evaluation: With the increasing scale of pre-training data and the construction\nof synthetic data, some LLMs are overoptimized towards benchmarks, leading to inflated scores.\nTo address this, we plan to use newly constructed datasets to evaluate model performance, thereby\nmitigating the overfitting risk and ensuring accurate density estimation.\nMulti-modal Density In this work, we focus on measuring the capacity density of language models.\nHowever, measuring the density and trends in large multimodal models is also crucial as multimodal\napplications increase. In the future, designing reasonable density evaluation methods for multimodal\nmodels will be an important research direction.\nInference Densing Law Recent research has highlighted that more inference computational costs\nallow LLMs to engage in deeper reasoning, effectively enhancing their performance on complex\ntasks (OpenAI, 2024b). In this work, we use the parameter size as the basis to evaluate model\ncapacity density. However, as the importance of chain-of-thought reasoning continues to grow,\ndensity evaluation should shift towards being based on inference FLOPs. Specifically, capacity\ndensity could be formalized as the ratio of effective inference FLOPs to actual inference FLOPs. In\nthis way, we hope that LLMs achieve optimal results with the minimum number of reasoning steps."}, {"title": "6 Conclusion", "content": "To illustrate the recent trend towards efficient LLMs and to quantitatively measure the training quality\nof LLMs, this paper introduces a method for evaluating the capacity density of LLMs. By measuring\nthe capacity density of open-source base LLMs released since 2023, we show an empirical law:\nthe capacity density of LLMs increases exponentially over time. The evaluation results on some\nwidely-used LLM benchmarks indicate that the density of LLMs doubles every three months. This\nimplies that, within three months, a model with only half the parameters can achieve performance\ncomparable to the current state-of-the-art models. This finding highlights the rapid development and"}]}