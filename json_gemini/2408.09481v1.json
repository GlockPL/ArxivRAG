{"title": "PanoSent: A Panoptic Sextuple Extraction Benchmark for Multimodal Conversational Aspect-based Sentiment Analysis", "authors": ["Meng Luo", "Hao Fei", "Bobo Li", "Shengqiong Wu", "Qian Liu", "Soujanya Poria", "Erik Cambria", "Mong-Li Lee", "Wynne Hsu"], "abstract": "While existing Aspect-based Sentiment Analysis (ABSA) has received extensive effort and advancement, there are still gaps in defining a more holistic research target seamlessly integrating multimodality, conversation context, fine-granularity, and also covering the changing sentiment dynamics as well as cognitive causal rationales. This paper bridges the gaps by introducing a multimodal conversational ABSA, where two novel subtasks are proposed: 1) Panoptic Sentiment Sextuple Extraction, panoramically recognizing holder, target, aspect, opinion, sentiment, rationale from multi-turn multi-party multimodal dialogue. 2) Sentiment Flipping Analysis, detecting the dynamic sentiment transformation throughout the conversation with the causal reasons. To benchmark the tasks, we construct PanoSent, a dataset annotated both manually and automatically, featuring high quality, large scale, multimodality, multilingualism, multi-scenarios, and covering both implicit&explicit sentiment elements. To effectively address the tasks, we devise a novel Chain-of-Sentiment reasoning framework, together with a novel multimodal large language model (namely Sentica) and a paraphrase-based verification mechanism. Extensive evaluations demonstrate the superiority of our methods over strong baselines, validating the efficacy of all our proposed methods. The work is expected to open up a new era for the ABSA community, and thus all our codes and data are open at https://PanoSent.github.io/.", "sections": [{"title": "1 Introduction", "content": "The quest for human-level artificial intelligence encompasses not only possessing intelligence but also understanding human emotions, thus propelling sentiment analysis and opinion mining to become the key area of research focus. Through decades of research, sentiment analysis has seen significant developments across various dimensions and aspects [7, 55, 59]. The field has evolved from traditional coarse-grained analysis, such as document and sentence-level analysis [72, 85], to fine-grained one (e.g., ABSA) [60, 67, 91], incorporating a wide array of emotional elements and evolving to extract different sentiment tuples, including the extraction of targets, aspects, opinions, and sentiments. Moreover, the sentiment analysis scope has broadened from purely textual content to multimodal content such as images and videos [24, 32, 40, 42, 50, 86]. Such expansion recognizes that in real-world scenarios, users often convey their opinions and emotions more accurately through diverse multimedia, providing additional information beyond text, such as micro-expressions, tone of voice, and other cues. Additionally, research has expanded beyond single-text scenarios to consider more complex conversational contexts [38, 95], where individuals frequently engage in multi-turn, multi-party discussions on social media platforms (e.g., Twitter, Facebook) about services, products, sports, etc.\nTo fill these gaps, this paper proposes Multimodal Conversational Aspect-based Sentiment Analysis, where we aim to provide a more comprehensive and holistic ABSA definition that includes both Panoptic Sentiment Sextuple Extraction (subtask-I) and Sentiment Flipping Analysis (subtask-II), as exemplified in Figure 1. Our focus is on conversational scenarios covering the four most common modalities for emotional expression in daily life, i.e., text, image, audio, video. On the one hand, we extend the current ABSA quadruple extraction definition to sextuple extraction, including holder, target, aspect, opinion, sentiment, and rationale, fully covering finer-grained emotional elements to offer a panoramic view of sentiment. On the other hand, we define a task to monitor the dynamic sentiment change towards the same target and aspect by the same holder throughout the conversation, and also identify the trigger reasons behind these flipped sentiments. For both sextuple extraction and sentiment change identification, we also emphasize discerning the underlying causal rationale or trigger, striving to not only know how but also why from a cognition perspective.\nTo benchmark the novel task, we accordingly construct a large-scale high-quality dataset, PanoSent. PanoSent covers more than 100 common domains and scenarios, which, based on multi-turn and multi-party conversational contexts, the sentiment elements within a sextuple may cross utterances. To mimic real human emotional expression habits, where 1) elements can originate from both textual and non-textual (audio or visual) modalities, and 2) emotions may be expressed implicitly, the data covers both implicit and explicit sentiment elements. To ensure the benchmark generalizability, the dataset includes three mainstream languages: English, Chinese, and Spanish. We collect the data from real-world sources, carefully annotated manually. To enlarge the quantity, we further automatically synthesize the dataset via OpenAI GPT-4 [1] with multimodal retrieval. Strict human inspection and cross-validation ensure high-quality standards. In total, we obtain 10,000 annotated dialogues for PanoSent.\nCompared to existing ABSA tasks, the new task proposed in this work poses greater challenges, such as the need to understand complex conversational contexts and flexibly extract features from various modalities, especially discerning causal reasons at a cognitive level. Considering the recent great successes of Multimodal Large Language Models (MLLMs) in powerful semantic understanding across multiple modalities [23, 43, 47, 76], we construct a backbone MLLM system, Sentica, for encoding and understanding multimodal conversational content. Inspired by the human process of sentiment analysis, we further develop a Chain-of-Sentiment (CoS) reasoning framework for a high-performing task solution, which, based on the Chain-of-Thought [73] idea, breaks down the task into four progressive reasoning steps, from simpler to more complex. The system allows to more effectively extract the elements of the sentiment sextuple and identify flipped sentiments step by step, while simultaneously inducing the corresponding rationale and triggers. A paraphrase-based verification (PPV) mechanism enhances the robustness of the CoS reasoning process. Evaluations on the PanoSent dataset across multiple subtasks and languages show our method outperforms strong LLM-based baselines, validating Sentica, CoS, and PpV. Comprehensive analyses are included for clarity."}, {"title": "2 Related Work", "content": "This work majorly focuses on the track of ABSA [10, 90]. ABSA has evolved from its initial objective of identifying sentiment polarity to more complex tasks such as recognizing targets, aspects, and opinions [33, 44, 48]. The complexity of ABSA tasks has increased with the introduction of combinations of these elements, ranging from paired extraction [9, 78] to triplet [53, 62] and quadruple extractions [6, 37]. Concurrently, multimodal SA [30], a pivotal topic within the multimodal research community [19, 20, 77, 82], has garnered increasing attention, incorporating modalities beyond text, such as images, audios, and videos. The trend in multimodal sentiment analysis has shifted from coarse-grained to fine-grained. The proposed methods mainly focus on exploring feature extraction and fusion from diverse modal inputs [23, 29, 46, 74, 86, 94], relying on additional structured knowledge [19, 21]. Furthermore, in terms of application scenarios, there has been a shift from analyzing single pieces of text to engaging in multi-turn, multi-party dialogues [88, 92], aiming to recognize emotions within dialogues to better align with real-world applications. Subsequently, dialogue sentiment analysis has gradually evolved into dialogue ABSA [37], incorporating non-textual modalities in the analysis.\nHowever, we find that current ABSA benchmarks still lack a combined perspective and comprehensive definition across granularity, multimodality, and dialogue contexts. For instance, there is an absence of benchmarks for fine-grained sentiment analysis in multimodal dialogue scenarios [60, 91]. Regarding granularity, there is potential to go beyond the four elements of target, aspect, opinion, and sentiment, to include the consideration of the sentiment holder, which also plays a pivotal role in a dialogue context.\nMoreover, previous research has not fully leveraged the role of multimodality in ABSA. In most cases, multimodal information is merely considered as supplementary clues to assist in determining opinions or sentiments [54, 68], with most of the other elements (e.g., targets, aspects) coming from texts. However, we argue that multimodality can also serve as a crucial source of information for the implicit identification of all elements more than sentiment. For example, a 'cellphone' may not be mentioned in the utterance, but the image showing a phone might feature it as the 'target' element. Beyond that, two other key aspects have not been sufficiently addressed in the existing ABSA. First, the dynamic nature of sentiments, especially within the context of dialogues, has not been explored. Second, the cognitive causes and intentions behind sentiments have been overlooked. In response, this work introduces a new benchmark, PanoSent, aiming to bridge all the above gaps, and provide a platform for the next phase of more comprehensive and in-depth ABSA research. Table 1 summarizes the key differences between ours and existing benchmarks.\nBeyond contributing new data, we also propose an advanced methodology for this benchmark. We take full advantage of the significant success of existing MLLM [22, 75, 83, 89] in understanding multimodal data. To address the challenges posed by the new tasks, which rely on cognitive-level reasoning, we introduce a novel reasoning framework, CoS. Inspired by the existing CoT strategy, which breaks down the problem into smaller chained steps for step-by-step resolution [17, 73], we decompose the two tasks in PanoSent, significantly enhancing the task-solving efficacy. Overall, our new benchmark data and methods are poised to open up a new era for the ABSA community."}, {"title": "3 Task Definition", "content": "We formally give the definitions of two subtasks, which also are illustrated in Figure 1 with specific examples.\nSubtask-I: Panoptic Sentiment Sextuple Extraction. Given a dialogue $D = {u_1, ..., u_n}$ with the replying structure ${(u_i, u_j), ...}$ (i.e., $u_i$ replies to $u_j$), the task is to extract all sextuples (h, t, a, o, s, r). Each utterance $u_i = {w_1, ..., w_{m_i}}$ contains $m_i$ words in the text (denoted as $I^t$), occasionally with associated non-text information piece, i.e., image ($I^i$), audio ($I^a$), video ($I^v$). The elements h (holder), t (target), a (aspect), o (opinion), and r (rationale) can be either the continuous text spans explicitly mentioned in utterances, or implicitly inferred from contexts or non-text modalities. s represents the sentiment category (positive, negative, or neutral)."}, {"title": "4 New benchmark: PanoSent", "content": "Here we elaborate on the construction of the new dataset for multimodal conversational ABSA, as well as its key characteristics.\n4.1 Dataset Construction\nConstructing via Human Annotation. The corpus of dialogues is collected by crawling via publicly available APIs from various social media or forum platforms in different languages, such as Twitter, Facebook, Reddit, Weibo, Xiaohongshu, BeReal, and more. While the majority of these dialogues are text-based, some also include multimodal interactions. Then, we conduct a rigorous screening process (via both manual inspection and automated filters, e.g., keyword and Toxic-BERT detection\u00b9), to eliminate content (e.g., multimodal information) or instances that are harmful, private or unrelated to the dialogue. After obtaining a cleansed corpus, we commence the annotation of aspect-based sentiment sextuples. We stick to the SemEval guidelines [63] and customize the annotation manual to accommodate both subtasks of our benchmark. We recruit annotators, training them according to the manual. To guarantee reliability, each dialogue is annotated independently by at least three distinct annotators. After annotation, we calculate the Cohen's Kappa score [12], achieving a score of 0.85, which reflects the high quality of our annotated dataset. In instances with inconsistent annotations, linguists and native speakers will collaboratively determine the final annotation. For unresolved ambiguities, the instances will be dropped.\nConstructing via Auto-Synthesis. We find the cost and workload in the above manual annotation process to be significantly high. The key issue is that real-world data sources that can provide a sufficient data volume meeting our task definition (to cover various modalities) are very rare. Hence, we consider automating data synthesis to substantially expand the volume, with the basic idea of 'automatic synthesis + multimodal retrieval'. We first leverage the powerful LLMs for synthesizing dialogues and sextuples. A considerable amount of existing related work [15, 57, 61] has already demonstrated that OpenAI's GPT-4 can generate data of very high quality that almost perfectly matches the real distribution. Specifically, following the prior practices [15, 79], we prepare template prompts to guide GPT-4 to generate pseudo-dialogues, along with sextuple and flipping annotations. Besides, for a portion of dialogue utterances, we also instruct GPT-4 to create appropriate captions as the image, audio, and video placeholders, according to the contexts. With the annotated dialogues, we now use the captions to retrieve the piece of information in the corresponding modality (image, audio or video) from the external multimodal databases, with only the top-10 retrieved candidates kept. Specifically, we consider multiple large-scale databases, including COCO [45], Flickr30k [84], AudioSet [25], WaveText5K [13], and WebVid [2], etc. Also we consider direct retrieval from the Google search engine, to ensure comprehensive coverage. For the associated multimodal contents, three annotators will assign a ranking score (1-10) to the 10 candidates, which are further ranked via their averaged scores, and the highest-scored one is elected as the determined multimodal information piece. Finally, each synthesized dialogue, the annotations of two subtasks, and the multimodal contents will be thoroughly examined by at least two workers. All the possibly problematic instances will be dropped. We also calculate the Cohen's Kappa score across workers, achieving a score of 0.82, ensuring a high consistency of the synthesized annotations.\n4.2 Data Insights\nWe select a portion of the real data to serve as developing and testing sets, while the remainder of the real data and all the synthesized data are used as the training set. Ultimately, the ratio of the train/dev/test sets for each language is 8:1:1. Following we briefly summarize the key characteristics and highlights of our PanoSent dataset.\nPanoptic Fine-grained Sentiment Definition. In contrast to existing ABSA datasets, such as TOWE [16], ASTE [62], and DiaASQ [37], PanoSent dataset encompasses the most comprehensive elements, featuring six key items for ABSA.\nCognitive Causal Rationale. We for the first time introduce the rationale element in ABSA, enhancing the definition by providing deeper insights into the motivations behind sentiments, allowing an interpretable sentiment understanding at a cognitive level.\nDynamic Sentiment Flipping. Going beyond the traditional ABSA benchmark, PanoSent pioneers the examination of sentiment flips, studying the dynamics nature of ABSA.\nMulti-scenario. PanoSent takes the dialogue as the context backbone, covering 10 main real-life domains across over 100 subdomains, ensuring an extensive diversity that supports research into sentiment analysis from various perspectives.\nMultimodality. Beyond textual content (56%), PanoSent comprises three other modalities of information, including images (22%), audio (6%), video (4%), and mixed modalities (12%).\nMultilingualism. PanoSent covers three mainstream languages, English (60%), Chinese (30%), and Spanish (10%), allowing a cross-lingual study of ABSA."}, {"title": "5 Methodology", "content": "The two tasks in PanoSent encompass non-trivial challenges, e.g., complex conversational context understanding, multimodal feature extracting, and cognitive-level ABSA reasoning. To address these, we propose a comprehensive solution. Below, we detail the models proposed, the reasoning framework, the verification mechanism, and the learning approach.\n5.1 Multimodal LLM Backbone\nCurrently, LLMs demonstrate remarkable capabilities in understanding language semantics. Correspondingly, MLLMs have been developed, exhibiting powerful abilities to comprehend multimodal data [39]. Building on the success of MLLMs, we consider leveraging them to help solve our task, where a thorough understanding of multimodal information is required. To this end, we develop a novel MLLM, Sentica, as presented in Figure 2. We adopt the Flan-T5 (XXL) [11] as the core LLM for semantics understanding and decision-making. For non-text inputs, we use multimodal models to encode signals into LLM-understandable representations. We use ImageBind as the unified encoder for all three non-text modalities due to its strong capabilities, followed by a linear layer that connects ImageBind [26] to the LLM for representation projection.\n5.2 CoS Reasoning Framework\nResolving two tasks, Panoptic Sentiment Sextuple Extraction and Sentiment Flipping Analysis, is challenging, not only due to the complex task definitions but also the cognitive-level requirement on the causal rationale and trigger detection. Inspired by the recent Chain-of-Thought (CoT) reasoning paradigm [73], here we also consider a human-like process of sentiment understanding and propose a Chain-of-Sentiment (CoS) reasoning framework. Previous ABSA studies [18] reveal that various ABSA elements can play hierarchical roles in depicting the overall sentiment puzzle. For example, the opinion should be detected before determining the sentiment polarity; likewise, identifying the target and aspect has a higher priority over recognizing the opinion. Thus, our main idea is that we deconstruct the two subtasks into four progressive, chained reasoning steps, from simpler to more complex. Using the capability of Sentica, solving each step incrementally accumulates key clues and insights for the follow-up steps. Figure 2 also illustrates how the CoS reasoning works with Sentica.\nStep 1: Target-Aspect Identification. Given input dialogue D possibly with multimodal signals and with specific instruction P1, the initial step aims to prompt Sentica to identify all the possible targets and their specific aspects discussed within the dialogue, i.e., {(ti, ai)}.\nThis step can be formulated as:\n{(ti, ai)} \\leftarrow f_1(D|P_1). (1)\nStep 2: Holder-Opinion Detection. The second step is to detect the holders $h_j$ and their specific opinions $o_j$, regarding the identified targets and aspects. We require Sentica to output a set of quadruples consisting of the holder, target, aspect, and opinion ${(h_j, t_i, a_i, o_j)}$. After this step, we construct holder-target-aspect-opinion quadruples, which lay the foundation for understanding the further sentiment.\nThis step is formulated as:\n{(h_j, t_i, a_i, o_j)} \\leftarrow f_2(D, {(t_i, a_i)}|P_2). (2)\nStep 3: Sentiment-Rationale Mining. The third step then analyzes the sentiment $s_k$ with each opinion and identifies the rationale $r_i$, based on the identified holder-target-aspect-opinion quadruples. We ask Sentica to output a set of sextuplets, by further adding sentiment and rationale to the previous quadruples to form ${(h_j, t_i, a_i, o_j, s_k, r_i)}$.\nWe denote this step as:\n{(h_j, t_i, a_i, o_j, s_k, r_i)} \\leftarrow f_3(D, {(h_j, t_i, a_i, o_j)}|P_3) . (3)\nStep 4: Sentiment Flipping Trigger Classification. With all the sextuplets detected, the final step of discerning sentiment flipping would be much effortless. Specifically, we prompt Sentica to first summarize any changes (i.e., from an initial sentiment ($\\zeta_k$) to a flipped sentiment ($\\phi_k$)) in sentiment of same holder-target-aspect, and then classify the trigger ($t_m$) label for each sentiment flip. The output is a set of sextuplets: ${(h_j, t_i, a_i, \\zeta_k, \\phi_k, \\tau_m)}$.\nThis step can be marked as:\n\\left\\{\\begin{array}{ll}\nNONE, & \\text { if no flip } \\\\\n(h, t, a, \\zeta, \\phi, \\tau), & \\text { if flip }\n\\end{array}\\right\\} \\leftarrow f_4 (D, {(h_j, t_i, a_i, o_j, s_k, r_i)}|P_4). (4)\n5.3 Paraphrase-based Verification\nGiven that we designed the entire two-task solution as a step-wise process, a potential issue is that CoS could lead to error accumulation. For example, an error in the first step could directly impact the outcome of all subsequent steps. Therefore, it's crucial to perform verification at every reasoning step. Existing work has verified that compared to structured data, LLMs excel more in understanding natural language [36, 70]. This implies that having LLMs directly check the correctness of each obtained k-tuple is sub-optimal. A more intuitive approach is to first convert the structured k-tuples into natural language expressions through paraphrasing, effectively creating a claim that conveys the same meaning in a different format. Then, let the LLM check whether this claim is in an entailment or contradiction relationship [34, 66] with the given dialogue context and information. We refer to this as a Paraphrase-based Verification (PPV) mechanism. If the relationship is one of entailment, the verification is successful, and the process moves on to the next reasoning step. If it's a contradiction, the current step is rerun until a reasonable result is yielded. This process not only ensures that each reasoning step is built on verified information but also enhances the overall robustness of sentiment analysis, effectively mitigating the negative impact of hallucinations [31, 65] inherent in LLMs.\n5.4 Instruction Tuning\nTo empower Sentica with the reasoning capabilities of the CoS framework, we conduct instruction tuning, entailing a three-phase training process. In the first stage, we enable the LLM to understand multimodal representations bound to images, audios and videos. We consider training directly on existing 'text-X' pair datasets (where 'X' refers to image, audio, or video), i.e., inputting 'X' and having the LLM output the corresponding caption text."}, {"title": "6 Experiments", "content": "6.1 Settings\nEvaluations. For Task-I, we follow DiaASQ [37], considering evaluation under three dimensions: 1) element-wise detection; 2) pairwise extraction; 3) overall sextuple extraction. For the explicit elements, we use the exact match F1 metric. For the implicit elements, we use the binary match F1, where we use GPT-4 to evaluate if the gold element is semantically identical to the prediction (1 if yes, otherwise 0). Since a correct rationale element may not need to strictly match gold term boundaries (i.e., only coinciding with the critical part), we take the proportional match F1 for its evaluation. For the compound evaluation, a pair or overall sextuple is correct only when all elements are correct. Here, the score for rationale above 0.5 is deemed a correct prediction. For sextuple extraction, micro F1 evaluates the entire sextuple, while identification F1 measures the sextuple without sentiment polarity. For subtask-II, we mainly measure three targets: 1) if both Initial Sentiment & Flipped Sentiment (Flip) are correct, 2) if the flipping trigger's category (Trig) is correct, and 3) if both Flip-Trig is correct simultaneously. For (1) and (3), we use exact match F1; for (2), we adopt macro F1.\nBaselines. Since no prior research or methods can be directly adopted here for comparisons, we consider maintaining several baselines via our implementations. We first retrofit the UGF [81] and DiaASQ [37] so that they can execute the multimodal sextuple extraction tasks, where the small-size LMs are used, e.g., Multilingual BERT (Base) [14] and mT5 (XXL) [80]. We also consider existing MLLMs (supporting T/A/I/V) for comparisons, including Unified-IO 2 [49] and NEXT-GPT [76]. All systems are fine-tuned using the PanoSent training set for fairness.\nImplementations. Given the varying capabilities of different LLMs across languages, we use Flan-T5 (XXL) for English data, ChatGLM2 6B for Chinese data, and Vicuna 7B for Spanish data. Our Sentica is tuned via LoRA [28], allowing for the least parameter updating. The experiments were conducted on hardware with 8*A100 GPUs, each boasting 80GB of memory. To ensure the reliability and reproducibility of our results, we tune the system on a developing set and used five different random seeds, selecting our experimental outcomes based on the average scores from five runs.\n6.2 Main Results\nPerformances on Panoptic Sentiment Sextuple Extraction. Table 3 compares the performances of different methods on Subtask-I, where we can gain the following observations. First, due to the presence of many implicit elements in our data, the performance of extraction-based baselines (such as DiaASQ and UGF) can be inferior. The generative nature of LLM-based methods, however, effectively addresses this, resulting in overall better performance. Comparing the performance of Sentica with Unified-IO 2 and NEXT-GPT (M3&4 vs. M5), we see that our method performs better. Sentica, when equipped with the CoS framework, shows significant improvement over the direct prompting paradigm (M7 vs. M5). Moreover, comparing M6 and M7 shows a clear advantage of our proposed CoS reasoning framework over the vanilla CoT method. Most importantly, when Sentica combines both the CoS and PpV mechanisms, the complete system (M8) exhibits the strongest global performance. As seen, across different task evaluation granularities and languages, our system achieves the best scores. In both ZH and SP languages, our system also demonstrates a significant superiority over the Sentica CoT-based variant. Finally, we can observe task evaluation from different perspectives. For different elements, the identification of the holder and target is more accurate, while the determination of rationale is more challenging. Similarly, the recognition of sentiment-rationale pairs is also more difficult. The overall identification of sextuples poses the greatest challenge, providing a challenging benchmark for follow-up research.\nResults on Sentiment Flipping Analysis. For Task 2, we present the overall results in Table 4. Similar trends to those observed in"}, {"title": "6.3 Analysis and Discussion", "content": "We take one step over the overall performance, further delving into the analyses of the proposed data and methods.\nQ1: Is It Necessary to Construct Synthetic Data? In the above experiments, we train the model by combining real data with synthetic data. Therefore, we plan to train the model using these two types of data separately and compare the performance. The results for the two subtasks under different languages are shown in Figure 3. Overall, it is observable that training on real-life data yields better results compared to training on synthetic datasets, even though the latter are more plentiful. This is because real data possess a more authentic distribution of information, enabling the model to learn a richer set of features. Moreover, our test set is also sampled from real data. Most importantly, we discover that once synthetic data is used as an additional supplement to substantially expand the quantity of real data, it can significantly enhance the final performance, consistently. This proves the necessity to construct synthetic data.\nQ2: How Significant Is the Role of Multimodal Information? Although multimodal information has been utilized in existing multimodal sentiment analysis research [35, 54], it is mostly treated as supplementary to textual information for aiding in the determination of sentiment polarity. In this work, the role of multimodal information is comprehensive and all-encompassing. It not only assists in determining sentiment polarity but also serves as a direct source of information for judging the sextuple elements (i.e., in an implicit manner). We demonstrate the impact of removing"}, {"title": "A What To Do Next with PanoSent?", "content": "In this paper, we introduce a novel benchmark for Multimodal Conversational Aspect-based Sentiment Analysis, which includes two innovative subordinate tasks: Panoptic Sentiment Sextuple Extraction and Sentiment Flipping Analysis. We have proposed the Chain-of-Sentiment reasoning method based on our MLLM, which has demonstrated strong benchmark performance on our dataset, PanoSent. We firmly believe that this pioneering work will inaugurate a new era for the sentiment analysis community. Several important directions for future research can emerge from our work.\n\u25ba Exploring Multimodality in PanoSent In this paper, we encode multimodal information in a straightforward manner using common techniques. Given the critical role of multimodal information for this task, future efforts should focus on developing more powerful methods for multimodal feature extraction and integration. Additionally, investigating the impact of different modalities on sentiment recognition across various scenarios promises to be a fruitful area of research.\n\u25ba Identifying Implicit Sentiment Elements Compared to explicit sentiment elements, the identification of implicit elements poses a greater challenge. Our approach, based on MLLM, autonomously determines the recognition of implicit sentiment elements through an understanding of the input data's content. We believe there are more accurate methods to be discovered for identifying implicit elements.\n\u25ba Sentiment Cognition and Reasoning Mechanisms Our new task involves complex sentiment cognition, for which we propose a reasoning framework. Future research should delve deeper into the mechanisms of interaction and triggering among sentiment elements, as well as the mechanisms behind Sentiment flipping, in order to develop more robust sentiment reasoning solutions.\n\u25ba Modeling Dialogue Context Dialogue scenarios closely resemble the natural ways in which people express emotions. This work processes the overall content of dialogues through the model, allowing it to understand conversations autonomously. Next steps in research could focus on how to more effectively enhance the model's ability to model dialogue context, thus better addressing cross-utterance issues. For example, further consideration could be given to modeling dialogue structure and speaker coreference resolution features.\n\u25ba Sentiment-aware Instruction Fine-tuning Our work involves tasks based on a MLLM, which is fine-tuned on our training set. Research indicates that the setup of instruction fine-tuning significantly affects the LLM's performance on downstream tasks. We believe that developing superior methods for instruction fine-tuning, such as designing approaches that increase the LLM's sensitivity to sentiment, holds great promise.\n\u25ba Cross-lingual Transfer Learning Our dataset includes three popular languages from different language families: English, Chinese, and Spanish, with non-parallel annotations across languages. Subsequent research could explore cross-lingual transfer learning in a multimodal scenario, investigating the supportive role of language-invariant features (multimodal information) for sentiment learning across languages.\n\u25ba Cross-domain Transfer Learning Our dataset is extensive, covering hundreds of different domains and everyday scenarios. It would be interesting to study the variations of panoptic sentiment across different scenes and domains, making cross-domain transfer learning a meaningful direction for future work.\n\u25ba Weak/Unsupervised Sentiment Analysis Our paper primarily focused on supervised learning using a large amount of annotated data. However, MLLMs already possess significant unsupervised generalization capabilities. It is crucial to leverage our benchmark for weak or even unsupervised sentiment recognition. In the subsequent part of the Appendix, we provide an analysis and exploration of few-shot sentiment recognition."}, {"title": "B Ethic Considerations", "content": "In conducting this research and developing the PanoSent benchmark, several ethical considerations have been taken into account to ensure the responsible use and application of the technologies involved.\n\u25ba Privacy and Data Protection Given that the raw dataset includes multimodal dialogues that may contain personal information, rigorous measures have been implemented to anonymize and protect any potentially sensitive data. This includes the removal of personally identifiable information (PII) from texts, images, audio, and video content. Additionally, the dataset has been reviewed to ensure compliance with relevant data protection regulations such as GDPR and CCPA, aiming to respect user privacy fully. Our data collection procedures have been carefully designed to focus on factual knowledge acquisition without infringing on privacy rights, thereby upholding our strong commitment to privacy and ethical research standards.\n\u25ba Data Collection For the creation of the PanoSent dataset, all data was collected from publicly available sources or through contributions from individuals who were informed about the purposes of the research and provided their explicit consent. Efforts were made to ensure that contributors understood their rights, including the right to withdraw their data at any point.\n\u25ba Annotator and Compensation Acknowledging the significant role of human annotators in the creation of the PanoSent dataset, we have engaged a diverse group of annotators including well-trained individuals from crowdsourcing platforms, native speakers, and senior postgraduate students with specialized training for the annotation tasks. The estimated time required for annotating each dialogue utterance is between 4 to 6 minutes, reflecting the complexity and detailed nature of the task.\n\u25ba Intellectual Property Protection The PanoSent dataset includes content collected from publicly available sources on a popular Chinese social media platform, utilizing its officially open API. This collection method ensures compliance with intellectual property laws and respects the terms of service of the platform. Permission for the use, distribution, and modification of this content is granted under the terms of the Weibo API distribution agreement. This approach safeguards the intellectual property rights of the content creators while facilitating academic research and development."}, {"title": "Bias and Fairness", "content": "Recognizing the potential for bias in Al systems, this research includes an analysis of the PanoSent dataset for biases related to gender, ethnicity, language, and other sociodemographic factors. Steps have been taken to mitigate these biases through diverse and representative data collection across multiple languages and scenarios. However, it is acknowledged that complete eradication of bias is challenging, and continuous efforts are required to identify and address biases as the benchmark evolves.\n\u25ba Misuse Potential The research team is aware of the potential misuse of sentiment analysis technologies, such as applications in surveillance or the manipulation of public opinion. Therefore, alongside the release of the PanoSent benchmark and the associated models, guidelines have been developed to encourage ethical use. These guidelines emphasize the importance of consent, transparency, and accountability in any application or further development of the technologies presented in this paper.\n\u25ba Accessibility and Inclusivity In line with our commitment to fostering an inclusive research community, all code and data related to the PanoSent benchmark will be made openly available. This ensures that researchers and practitioners from diverse backgrounds and with varying levels of resources have equal opportunities to contribute to, and benefit from, the advancements in multimodal conversational aspect-based sentiment analysis."}, {"title": "C More Details of Datasets", "content": "C.1 Extended Details of Data Construction\nC.1.1 Data Acquisition\n\u25ba Step1. Platform Selection and Data Collection. Our initial step involves identifying a diverse range of social media and forum platforms as sources for our dataset, including but not limited to Twitter, Facebook, Reddit, Weibo, Xiaohongshu, BeReal. These platforms are chosen for their rich conversational content across multiple languages and the vast user engagement they facilitate. We target some influential bloggers within specific domains and the discussions surrounding trending topics related to our research themes. Conversations on these platforms typically originate from a root post, with users participating in multi-thread and multi-turn dialogues based on the initial post. In addition to text, these interactions often include multimodal content such as images, videos, and audios. While less common than text, this multimodal interaction is a crucial component of our dataset, and we make extra efforts to collect conversations incorporating these elements. Given that these platforms generally do not support audio replies as a standalone feature, we extract the audio tracks from video content to collect audio modal information. Data collection is automated through publicly available APIs provided by these platforms, with conversations being categorized based on their thematic relevance and the types of modal information they contain. The process of data acquisition and preprocessing is depicted in Figure 6.\n\u25ba Step2. Data Cleaning and Re-organization. To ensure the dataset is free from harmful content, privacy violations, irrelevant, or low-quality conversations, we employ a combination of manual inspection and automated tools. A keyword library is constructed based on previous related studies and the expertise of team members in social media analysis and specific thematic areas. This library includes keywords indicating potential harm, privacy infringement, and irrelevance to the research topic. Scripts are developed to automatically scan the collected data for these keywords, with flagged conversations undergoing manual review to determine their suitability for inclusion in the dataset. Additionally, we utilize the Toxic BERT model, capable of identifying various forms of harmful speech, including insults, discrimination, and harassment, by analyzing extensive online textual data. This model provides probability scores for detected categories and identifies the specific"}]}