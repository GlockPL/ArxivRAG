{"title": "Optimizing Large Language Models for Turkish: New Methodologies in Corpus Selection and Training", "authors": ["H. Toprak Kesgin", "M. Kaan Yuce", "Eren Dogan", "M. Egemen Uzun", "Atahan Uz", "Elif \u0130nce", "Yusuf Erdem", "Osama Shbib", "Ahmed Zeer", "M. Fatih Amasyali"], "abstract": "In this study, we develop and assess new corpus selection and training methodologies to improve the effectiveness of Turkish language models. Specifically, we adapted Large Language Model generated datasets and translated English datasets into Turkish, integrating these resources into the training process. This approach led to substantial enhancements in model accuracy for both few-shot and zero-shot learning scenarios. Furthermore, the merging of these adapted models was found to markedly improve their performance. Human evaluative metrics, including task-specific performance assessments, further demonstrated that these adapted models possess a greater aptitude for comprehending the Turkish language and addressing logic-based queries. This research underscores the importance of refining corpus selection strategies to optimize the performance of multilingual models, particularly for under-resourced languages like Turkish.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, the development of language models has been an important area of research in artificial intelligence. In particular, multilingual models have the potential to overcome data gaps in different languages and improve language learning processes [1]. To realize this potential, it is important to evaluate and optimize the performance of multilingual models. Multilingual models leverage shared knowledge across languages, allowing for more robust and comprehensive language understanding [2]. However, for these models to be effective, there must be sufficient quantity and quality of data in each language. There can be large differences between the quantity and quality of data in different languages, which can cause models to underperform in some languages. For languages with limited data sources, such as Turkish, multilingual models must be trained and optimized to overcome these shortcomings. The focus of this study is to improve the performance of Turkish language in multilingual models and enable them to produce more accurate responses. In this context, we investigate how we can improve the performance of existing multilingual models for Turkish.\nIn this study, English datasets, which were found to enhance the performance of the models, were translated into Turkish and utilized in the training of the models. After training, the performance of the models was evaluated both by human"}, {"title": "II. CORPUS CREATION", "content": "Zero-shot and few-shot methods are commonly employed to assess the performance of large language models. In this study, we concentrated on enhancing the performance of Turkish models on these datasets. In our initial study, we sought models that are relatively small in size but demonstrate relative proficiency on few-shot evaluation datasets. Model selection was primarily based on the Cosmopedia dataset [5], which also served as the main training dataset for the Cosmolb model. The Cosmopedia dataset comprises approximately 30 million files and 25 billion tokens. It was created using the Mixtral-8x7B-Instruct-v0.1 model [6] and comprises eight subsets of varying sizes, derived from sources including synthetic textbooks, blog posts, stories, posts, and WikiHow articles. The number of examples and the size of the text in each subset are provided in Table I. Furthermore, the OpenOrca dataset [7], an open-source instruction completion dataset comprising 3.7GB of text, was employed for the model to execute human instructions. All these datasets are in English and translated into Turkish using the Google Translate API.\nGiven their relatively small sizes, the Stanford, Khan Academy, WikiHow, and OpenStax datasets were combined to achieve meaningful results. This combination of datasets was used in our evaluation process as detailed in Table II."}, {"title": "III. TRAINING WITH CORPUS SELECTION", "content": "We selected the Llama3-8b model [11], which gives the highest few-shot scores for Turkish, and the instruct versions of the same model to test whether the models that increase success in this 750m parameter model can also increase success in larger models. We trained this model using the fullfine training with the SKWO, Stories and OpenOrca data sets selected according to Table I. During training, we only tracked the performance on the ARC dataset due to resource and time constraints. We saw that the success of the model gradually increased with the checkpoints we received during training. As a result, our base model, with an accuracy of 48.72% on the instruction dataset, achieved 49.15% on the ARC dataset. Best of our knowledge, our Base and Our Instruct models are the highest performing open-source Turkish models in the range of 7-8 billion parameters.\nTraining details are as follows. Training was conducted over 1 epoch to optimize learning within the constraints of our resources and timelines. The decision to use a batch size of 1 with gradient accumulation set at 512 was guided by our hardware limitations and the need to handle large-scale data efficiently. A learning rate of le-6 was chosen based on preliminary tests that indicated it balances training speed and model stability effectively. Gradient clipping was set at 0.05 to prevent the exploding gradient problem, enhancing the stability of training over extensive datasets. The 8-bit AdamW [12] optimizer was selected for its efficiency in handling large models and datasets, providing a balance between computational demand and performance."}, {"title": "IV. MODEL MERGING", "content": "Model merging is an important technique that has emerged recently [13]. In recent advancements, model merging has proven effective in enhancing model performance by combining the strengths of various trained models. In this technique, the weights of models trained with different data sets with the same architecture can be combined with different techniques. After these combinations, the combined model can be more successful than the 2 models. For this purpose, this technique was employed to merge our trained models with the base and instruct versions of Llama3, resulting in significant performance improvements. The linear merging method, which is the classic merge method, was used for this purpose. The few-shot and human voting comparisons of the two fine-tuned models and their merged versions are described in detail in Comparison of Turkish Language Models section. This comprehensive evaluation aims to provide empirical evidence on the effectiveness of model merging as a viable method for enhancing language model performance."}, {"title": "V. COMPARISON OF TURKISH LANGUAGE MODELS", "content": "In this section, we present a detailed comparison of Turkish language models to evaluate the effectiveness of our proposed methods. The comparison is carried out using both dataset performance metrics and human judge evaluations to provide a comprehensive understanding of the improvements achieved. The models compared include existing Turkish language models, models trained using traditional methods, and the models developed through our proposed approach."}, {"title": "A. Human Judge Evaluation Metrics", "content": "We employed metrics like the ELO score and Win Percentage in human assessments to gauge model performance. ELO scores are used to determine the comparative skill levels of participants in zero-sum games. The ELO metric is widely used to compare the performance of language models against each other based on human voting [14]. In this process, judges assessed responses from the models to questions in the V dataset."}, {"title": "C. Comparison with Human Judge Voting", "content": "In addition to quantitative metrics, we also conducted evaluations through human judge voting to obtain qualitative insights into the model's performance. Human judges evaluated the models based on their responses to various tasks, considering aspects such as creativity, math, logic, and finding similarities. The models' ELO ratings and winning percentages are shown in Table V. The scores for all tasks are shown in Figure 1, showing the superiority of the models over each other in different areas."}, {"title": "D. Correlations", "content": "Figure 2 presents a correlation matrix of human judges preferences, showcasing the relationships between ratings from different voters (R1 to R10) in the context of model performance comparison. These insights are critical for understanding the consistency and reliability of human evaluations in linguistic model assessments. The matrix reveals both high and low correlations among the human judges' ratings, with values ranging from approximately 0.5 to 0.9. This variation highlights the subjective nature of human evaluation and emphasizes the importance of considering multiple perspectives when assessing model performance.\nFigure 3 presents the correlation matrix between different evaluation metrics, that are used to assess the models. Each cell indicates the correlation coefficient between two metrics, with values ranging from 0.0 to 0.9. This matrix helps identify which metrics tend to align closely, indicating that improvements or declines in one metric are often reflected in the other.\nFigure 4 presents the correlation matrix between different evaluation categories, as assessed by voters. Each cell indicates the correlation coefficient between two categories, with values ranging from -0.4 to 0.9. High positive correlations, such as 0.94 between Sentiment Analysis and Sentence Completion, suggest that performance in one category is strongly associated with performance in the other. This matrix helps identify which categories tend to be evaluated similarly by voters, revealing insights into model strengths and weaknesses across different types of tasks. The correlation matrix between evaluation categories provides insightful observations about the"}, {"title": "VI. CONCLUSION AND FUTURE STUDIES", "content": "This study demonstrates the efficacy of novel adaptation dataset generation, refined corpus selection methodologies, and efficacious training strategies in enhancing the performance of Turkish language models. Our research has revealed that these innovative approaches have led to substantial enhancements in model performance. In particular, optimized corpus selection methodologies and training strategies have enabled Turkish language models to generate more accurate and comprehensive responses. Synthetic datasets have significant potential for languages with limited data sources, such as Turkish. Our study has demonstrated that such datasets play a pivotal role in enhancing the comprehension and responsiveness of language models. Synthetic and translation datasets have been particularly instrumental in addressing Turkish language data gaps and expanding model capabilities. The findings of the study indicate that enhancements made in small-scale models are reflected in large-scale models in a positive manner. This substantiates the assertion that optimizations made on small models during the model development process provide a robust foundation for the transition to larger and resource-intensive models. Optimizing small models can enhance the overall performance of the model while optimizing cost and resource utilization."}], "equations": ["winpct = \\frac{win+both}{total}"]}