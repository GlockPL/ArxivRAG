{"title": "SecBench: A Comprehensive Multi-Dimensional Benchmarking Dataset for LLMs in Cybersecurity", "authors": ["PENGFEI JING", "MENGYUN TANG", "XIAORONG SHI", "XING ZHENG", "SEN NIE", "SHI WU", "YONG YANG", "XIAPU LUO"], "abstract": "Evaluating Large Language Models (LLMs) is crucial for understanding their capabilities and limitations across\nvarious applications, including natural language processing and code generation. Existing benchmarks like\nMMLU, C-Eval, and HumanEval assess general LLM performance but lack focus on specific expert domains\nsuch as cybersecurity. Previous attempts to create cybersecurity datasets have faced limitations, including\ninsufficient data volume and a reliance on multiple-choice questions (MCQs). To address these gaps, we\npropose SecBench, a multi-dimensional benchmarking dataset designed to evaluate LLMs in the cybersecurity\ndomain. SecBench includes questions in various formats (MCQs and short-answer questions (SAQs)), at\ndifferent capability levels (Knowledge Retention and Logical Reasoning), in multiple languages (Chinese and\nEnglish), and across various sub-domains. The dataset was constructed by collecting high-quality data from\nopen sources and organizing a Cybersecurity Question Design Contest, resulting in 44,823 MCQs and 3,087\nSAQs. Particularly, we used the powerful while cost-effective LLMs to (1). label the data and (2). constructing\na grading agent for automatic evaluation of SAQs. Benchmarking results on 13 SOTA LLMs demonstrate the\nusability of SecBench, which is arguably the largest and most comprehensive benchmark dataset for LLMs in\ncybersecurity. More information about SecBench can be found at our website [13], and the dataset can be\naccessed via the artifact link [12].", "sections": [{"title": "1 Introduction", "content": "Evaluating Large Language Models (LLMs) is essential for understanding their capabilities and\nlimitations, as these models play a significant role in various applications, from natural language\nprocessing to code generation. The importance of evaluating LLMs lies in ensuring their reliable\nand effective performance across diverse tasks while identifying areas for improvement. Many\nbenchmarks have been developed to assess different aspects of LLM performance, such as the\nMMLU benchmark for general knowledge and reasoning [20], C-Eval for the Chinese context\n[16], and HumanEval for code generation and completion [17]. These benchmarks collectively\nprovide a comprehensive framework for evaluating the multifaceted capabilities of LLMs. However,\nwhile these benchmarks focus on general capabilities, it is also crucial to evaluate LLMs in specific\nexpert domains, such as cybersecurity. Previous studies have attempted to establish datasets for this\npurpose [15, 18, 19, 21], but they face limitations, including insufficient evaluation data volume and\na predominant use of multiple-choice questions (MCQs). A more challenging task, the short-answer\nquestion (SAQ), which requires the model to generate its own answer, has not been explored in\nthese studies."}, {"title": "2 Background", "content": "Benchmarking LLMs. Evaluating Large Language Models (LLMs) is crucial for understanding\ntheir capabilities and limitations, as these models have become increasingly influential in various\napplications, from natural language processing to code generation and beyond. The significance\nof evaluating LLMs lies in ensuring that they perform reliably and effectively in diverse tasks,\nwhile also identifying areas for improvement. Several popular benchmarks have been developed\nto assess different aspects of LLM performance. For instance, the MMLU benchmark evaluates\ngeneral knowledge and reasoning across a wide range of subjects [20]. C-Eval [16] focuses on LLM's\ncapability in the specific Chinese context. HumanEval [17] is designed to assess code generation\nand completion tasks. These benchmarks collectively provide a comprehensive framework for\nevaluating the multifaceted capabilities of LLMs.\nBenchmarking LLM in Cybersecurity. The benchmarks discussed earlier primarily focus on\nassessing the general capabilities of LLMs. However, since LLMs are often fine-tuned for specific\nexpert domains, it is crucial to evaluate their performance across various specialized fields. In the\ncontext of cybersecurity, previous studies have attempted to establish datasets to assess LLMs\u2019\nknowledge in this particular domain [15, 18, 19, 21]. Unfortunately, these studies face two main\nlimitations. First, the average volume of evaluation data is only at the thousand-level, which may\nnot be sufficient to provide a comprehensive benchmark. Second, the question design in previous\nworks predominantly follows the multiple-choice question (MCQ) format, which merely requires\nthe model to select the correct answer from given options. However, a more challenging task, the\nshort-answer question (SAQ), which requires the model to generate its own answer rather than\nchoosing from existing ones, has not been explored in previous studies."}, {"title": "3 SecBench Design", "content": "Fig.1 shows the overview of the SecBench design: it is a comprehensive benchmarking dataset\naiming to benchmark LLM's capability in cybersecurity from Multi-Level, Multi-Language, Multi-\nForm, Multi-Domain.\nMulti-Level. We devise the capability of LLM in cybersecurity into two different levels: Knowl-\nedge Retention - KR and Logical Reasoning - LR. Among the two, knowledge retention examines the\nLLM's ability to retain existing knowledge. The content of such questions is relatively straightfor-\nward and does not involve complex reasoning. On the other hand, logical reasoning assesses the\nLLM's ability to infer the correct answer based on the given information. The difficulty of these\nquestions is relatively higher and better demonstrates the model's capability to handle complex\nproblems.\nMulti-Language. SecBench includes questions of two mainstream languages - Chinese and\nEnglish, to present a more comprehensive benchmark.\nMulti-Form. Unlike previous works that constructed only multiple-choice questions (MCQs) [15,\n18, 19, 21], SecBench also includes short-answer questions (SAQs) to present a more comprehensive\nevaluation. This is because SAQs tend to be more challenging than MCQs: for MCQs, the LLM\nonly needs to choose the correct answer(s) from the given options, while for SAQs, the LLM is\nprompted to construct its own answer based on the given question. As a result, SAQs can evaluate\nthe capability of the LLM at a higher level, especially considering the inherent limitations of LLMs\n(e.g., hallucinations and repetition).\nMulti-Domain. The questions in SecBench consist of 9 different domains, including D1. Security\nManagement, D2. Data Security, D3. Network and Infrastructure Security, D4. Security Standards and\nRegulations, D5. Application Security, D6. Identity and Access Control, D7. Fundamental Software\nand Hardware and Technology, D8. Endpoint and Host Security, D9. Cloud Security. Particularly,\nthe above domains were devised from several rounds of brainstorming and revision, which were\nexpected to cover most (if not all) related sub-domains in cybersecurity. Note that we do not expect\nthese domains to be orthogonal, and it is possible that one question can be reasonably labeled into\ndifferent domains. In our dataset, one question is assigned only one most-related domain label from\nD1 to D9.\nExample. For each line of data, it is either an MCQ or SAQ, provided with the question stem\nand corresponding answer, labeled with language (Chinese or English), level (Knowledge Retention\nor Logical Reasoning) and domain (from D1 to D9).\nFollowing is one MCQ example, labeled in the domain of Security Management and the level of\nLogical Reasoning. For MCQs, A blank is left in question stem, and there are four choices given in\nanswers for the tested LLM to select, with label referring to the correct choice(s) among the four."}, {"title": "4 Dataset Construction Process", "content": "4.1 Initial Dataset Construction\nQuestion Stems and Answers Extraction. We aim to construct a small batch of datasets to\nvalidate the rationality of the SecBench framework. To achieve this goal, we first collect raw\nmaterials from various sources that can be used to extract high-quality question data, including\nreal exam questions from various cybersecurity fields, authoritative books, and so on. Starting\nfrom these raw materials, we perform automated extraction of questions and answer data from\nthese resources (for example, using regular expressions). After this step, we have collected a total\nof 10,551 high-quality MCQs, covering different domains.\nLLM-based Labeling. However, the dataset obtained in the previous step only contains question\nstems and answers, lacking the corresponding labels. Therefore, we used the powerful large\nlanguage model - GPT4 [4], to further label this part of the data. With our carefully designed\nprompts, we utilized a powerful large model (GPT-4) to label these data, including tagging the\ndifficulty level of the questions (whether it is Knowledge Retention or Logical Reasoning) and\ntagging the specific domain that the questions assess (as mentioned earlier, from D1 to D9). After"}, {"title": "4.2 Large-Scale Dataset Construction", "content": "Cybersecurity Question Design Contest. To further expand the SecBench dataset, we have\norganized a large-scale Cybersecurity Question Design Contest [13]. In this contest, we expected\nparticipants to submit high-quality evaluation data across multiple domains, which we would\nsubsequently clean and incorporate into the SecBench database. Specifically, we categorized the\ndata submitted in question into three quality levels, with different weight scores assigned to each\nlevel to encourage participants to submit high-quality data:\nQualified Quality: The question meets the submission format, contains no factual errors,\nand is not duplicated with other questions submitted by the same or other participants.\n- Medium Quality: The question has clear logic and expression, a well-defined domain, an\nunambiguous answer, and provides a clear and reasonable explanation along with a verifiable\nsource.\n\u00b7 High Quality: The question design should have breadth, depth, and challenge, thus providing\na high degree of differentiation for the capabilities of different models.\nLarge-Scale Data Cleaning and Labeling. With the huge amount of data collected from the\ncontest, we first manually assign the quality level (qualified, medium or high, as stated above) to\neach submission. This process is performed by experienced experts with enough years of work\nexperience in the cybersecurity domain, ensuring the justification of the quality attribution process.\nThen, a rule-based filtering of these high-quality questions was performed to rule out possible\nduplications or incomplete data that were missed by the former human-based quality attribution.\nFinally, similar to Sec.4.1, we labeled the questions by their level (KR or LR), language and domain\nwith the help of LLM. After the whole process, we obtained a total of 34,277 MCQs and 3,087 SAQs,\ngreatly expanding our initial dataset quantitatively (more MCQs) and qualitatively (introducing\nthe new evaluation form - SAQs)."}, {"title": "4.3 Dataset Distribution", "content": "MCQ. Fig.3 shows the data distribution of the 44,823 MCQs in SecBench. According to Fig.3(a), the\nmajority of the MCQs fall into the KR category (90.8%), which is reasonable because MCQs tend to\nhave short question stems and focus on testing the knowledge base of the LLM. Notably, 9.2% of\nthe MCQs are of the LR type, requiring the LLM to perform reasoning to obtain the correct answer.\nAs shown in Fig.3(b), the data distribution across the 9 domains is generally even, with D6, D7, and"}, {"title": "4.4 Benchmarking Process", "content": "MCQ. The evaluation of MCQ is rather intuitive: for each MCQ, we check whether the model's\noutput (i.e., model's choice(s) among A, B, C, and D) is the same as the correct answer. For MCQs\nthat involve more than one correct answer, model's output is judged as correct only when it is\nidentical to the correct answer, meaning that no points are awarded for incorrect or incomplete\nselections. Particularly, the evaluation of MCQ is implemented on the widely-used open-sourced\nevaluation framework - OpenCompass [10].\nSAQ. Grading an SAQ is not as intuitive as grading MCQ, in which case we only need to determine\nwhether the LLM's choice is the correct one(s). Particularly, grading SAQs requires to understand\nboth the question stem and the model prediction (answer), and then fairly grade this prediction\nbased on the ground truth, which is expected to huge amount of manual effort. In our work, we\nintroduce a Grading Agent to realize the automatic grading of SAQs, and Fig.5 shows the process\nof how the SAQs were evaluation on tested LLMs. Specifically, each SAQ includes the question\nstem and the ground truth (i.e., correct answer) of the question. The question stem is first fed\ninto the tested LLMs to generate the Model Prediction, which is the LLMs' answer waiting to be\ngraded. Then, the three parts of data, including the question stem, ground truth, and the model\nprediction will be given to the Grading Agent, which is a sufficiently powerful LLM to grade the\nModel Prediction based on the ground truth, and output the corresponding scores. Specifically, this\nGrading Agent should 1). be capable of fairly grading the model prediction, and 2). generate stable\noutput (e.g., a final grading score for further processing) In our work, we choose the GPT-40 mini"}, {"title": "5 Evaluation", "content": "Based on SecBench, we conducted extensive benchmarking on 13 SOTA LLMs, including the GPT\nseries and competitive open-source ones.\n5.1 MCQ Benchmarking\nTable 1 presents the benchmarking results for the 44,823 MCQs. The values in each cell represent the\ncorrectness percentage for the corresponding category. Overall, the correctness of KR is significantly\nhigher than that of LR, demonstrating the rationale behind our design (i.e., Logical Reasoning\nis more challenging than Knowledge Retention). The performance of smaller LLMs (with fewer\nthan 10 billion parameters) is predictably lower than that of larger LLMs (with more than 30\nbillion parameters) Notably, the Tencent Hunyuan-Turbo model [7] outperforms all existing models,\nincluding the state-of-the-art GPT-40 and GPT-40-mini, achieving the highest correctness of 94.28%.\nIts correctness in Logical Reasoning (93.06%) is also significantly higher than that of other models,\ndemonstrating Hunyuan-Turbo's strong capability in solving complex questions in cybersecurity.\n5.2 SAQ Benchmarking\nTable 2 presents the benchmarking results for the 3,087 short-answer questions (SAQs). The values\nin each cell represent the average score, graded by the grading agent, on a percentage scale for\nthe corresponding category. Compared to MCQs, a larger gap is observed between different LLMs,\nindicating that solving SAQs is more challenging than MCQs. This is because the tested LLMs are\nrequired to generate their own answers rather than simply choosing from given options. Notably,\nfor SAQs, the Tencent Hunyuan-Turbo model [7] also outperforms most existing models, achieving\na score of 82.13. It ranks second only to the state-of-the-art GPT-40 (85.17) and is competitive with\nthe GPT-40-mini (82.49)."}, {"title": "6 Discussion", "content": "Rationale for Using LLMs in the Process. We utilized GPT-40 [6] for labeling data during the\nconstruction phase of SecBench, and GPT-40-mini [5] for grading SAQs in the benchmarking phase.\nTo ensure that these two LLMs are capable of performing the tasks, we explicitly checked their"}, {"title": "7 Conclusion", "content": "We propose SecBench, a multi-dimensional benchmarking dataset specifically designed to evaluate\nLLMs in the cybersecurity domain. SecBench addresses the limitations of existing benchmarks by\nincluding questions in various formats (MCQs and SAQs), at different capability levels (Knowledge\nRetention and Logical Reasoning), in multiple languages (Chinese and English), and across various\nsub-domains. The dataset was meticulously constructed by collecting high-quality data from open\nsources and organizing a Cybersecurity Question Design Contest, resulting in 44,823 MCQs and\n3,087 SAQs. To ensure the quality and consistency of the dataset, we employed GPT-4 for data\nlabeling and GPT-40-mini as a grading agent for the automatic evaluation of SAQs. Benchmarking\nresults demonstrate the usability and comprehensiveness of SecBench, making it arguably the\nlargest and most detailed benchmark dataset for LLMs in the field of cybersecurity. More information\nabout SecBench can be found at our website [13], and the dataset can be accessed via the artifact\nlink [12]."}, {"title": "A Detailed Prompts", "content": "LLM-based labeling. Following is the prompt text that is implemented on GPT-4 for labeling\nSecBench data. In the prompt, we explicitly detailed the requirement and offered few-shot example\nto ensure the performance.\n# Task Description\nI will upload a question related to information security, and now I need you to help me annotate these questions. I\nneed you to annotate these questions from two dimensions: 1. Assessing ability: whether the question assesses basic\nKnowledge Retention or more challenging Logical Reasoning ability. 2. Assessing domain: which specific subfield the\nquestion belongs to. Next, I will elaborate on these two requirements:\n1.Assessing ability. First, you will classify each question into one of the following two categories: (a). Knowledge\nRetention question: This type of question tests whether the test taker has the relevant background knowledge through\nstraightforward descriptions. The answers to these questions can be obtained directly by querying the knowledge base\nand do not involve reasoning processes. (b). Logical Reasoning question: This type of question presents the test taker\nwith a specific scenario and requires the test taker to reason or calculate within that scenario to arrive at the correct\nanswer. Compared to Knowledge Retention questions, these questions are more challenging.\n2.Assessing domain. Next, you will annotate the specific domain that each question assesses into one of the following\n10 categories: (1). Identity and Access Control, (2). Cloud Security, (3). Endpoint and Host Security, (4). Security\nStandards and Regulations, (5). Data Security, (6). Security Management, (7). Network and Infrastructure Security, (8).\nFundamental Software and Hardware Technology, (9). Application Security, (10). Others. Note that if you believe a\nquestion cannot be classified into any of the categories (1) (9), then classify it as (10). Others.\nFinally, you will provide the reason and basis for your classification of the question.\n# Input Introduction\nThe questions I upload consist of the following format:\n{\"question\":\"Which of the following is directly related to database security?\",\"answers\":[\"Granularity of access con-\ntrol\",\"Size of the database\",\"Number of attributes in the relation table\",\"Number of tuples in the relation table\"],\"label\":\"A\"}\nEach line of the file includes 3 elements: question is the main body of the question, answers are the four provided\noptions, and label is the correct answer.\n# Output Requirements\nFor each question I upload, you will annotate it according to my requirements and add the annotation results directly\nto the original data. The annotation results should be in Chinese. You will insert the annotation results after each piece\nof data, including:\n- \"assessed ability\": whether it is Knowledge Retention or Logical Reasoning.\n\"assessed domain\": which domain the knowledge being assessed belongs to.\n- \"reason for labeling\": the reason for your annotation, including the reason for the ability annotation and the reason\nfor the domain annotation, all need to be explained. This explanation must be detailed and explain why you believe the\nquestion assesses knowledge from a specific domain, not just give a meaningless reason.\nUsing the example question from the \"Data Introduction\" section, the annotated result should be as follows:\n{ \"question\": \"Which of the following is directly related to database security?\", \"answers\": [\"Granularity of access\ncontrol\", \"Size of the database\", \"Number of attributes in the relation table\", \"Number of tuples in the relation table\"],\n\"label\": \"A\", \"assessed ability\": \"Knowledge Retention\", \"assessed domain\": \"Data Security\", \"reason for labeling\": \"This\nquestion directly tests specific basic knowledge related to database security and does not involve logical reasoning,\nso it is labeled as knowledge memory; it can be directly seen from the question stem that this question tests specific\nknowledge of database security and should be classified under 'Data Security'.\" }"}, {"title": "SAQ Grading", "content": "Following is the prompt that the grading agent (implemented on GPT-40-mini in\nour work) used to grade the LLM's output for benchmarking SAQs.\nPlease help me grade a student's answers in the network information security exam. I will provide you with a dataset\nthat contains three parts: 1. The question stem and specific question, 2. The standard answer (i.e., the full score answer),\n3. The student's answer (to be graded). Specifically, you will perform the following steps:\n1. For each question, read the question stem and understand the content of the question.\n2. For each question, read the student's answer and compare it with the standard answer.\n3. For each question, based on the differences between the student's answer and the standard answer, grade the\nstudent's answer. The question is scored on a 10-point scale, with a full score of 10 points.\nRecord and return the student's score for each question in the form of a JSON file.\nYour output should only contain a JSON file in the following format, where each data entry only includes the student's\nscore, with the key being \"score\" and the value being an integer between 0 and 10, inclusive, for example:\n[{\"model_score\": 6}]\nBelow are the questions you need to process, consisting of three parts: 1. The question stem and specific question, 2.\nThe standard answer (i.e., the full score answer), 3. The student's answer (to be graded). Please grade based on the data\nbelow and return the JSON file in the format mentioned above:\n1. The question stem: {Question Stem from SecBench}\n2. The standard answer (i.e., the full score answer): {Ground Truth from SecBench}\n3. The student's answer (to be graded): {LLM's output to be graded}"}]}