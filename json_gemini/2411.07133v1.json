{"title": "Stronger Models are NOT Stronger Teachers for Instruction Tuning", "authors": ["Zhangchen Xu", "Fengqing Jiang", "Luyao Niu*", "Bill Yuchen Lin", "Radha Poovendran"], "abstract": "Instruction tuning has been widely adopted to ensure large language models (LLMs) follow user instructions effectively. The resulting instruction-following capabilities of LLMS heavily rely on the instruction datasets used for tuning. Recently, synthetic instruction datasets have emerged as an economically viable solution to provide LLMs diverse and high-quality instructions. However, existing approaches typically assume that larger or stronger models are stronger teachers for instruction tuning, and hence simply adopt these models as response generators to the synthetic instructions. In this paper, we challenge this commonly-adopted assumption. Our extensive experiments across five base models and twenty response generators reveal that larger and stronger models are not necessarily stronger teachers of smaller models. We refer to this phenomenon as the Larger Models' Paradox. We observe that existing metrics cannot precisely predict the effectiveness of response generators since they ignore the compatibility between teachers and base models being fine-tuned. We thus develop a novel metric, named as Compatibility-Adjusted Reward (CAR) to measure the effectiveness of response generators. Our experiments across five base models demonstrate that CAR outperforms almost all baselines.", "sections": [{"title": "Introduction", "content": "Instruction tuning (Figure 1) has been widely adopted to tailor the behavior of base Large Language Models (LLMs) to align with specific tasks and user intents (Zhang et al., 2023). This approach leverages instruction datasets, consisting of samples pairing an instruction with a corresponding response. The success of instruction tuning depends on the availability of high-quality instruction datasets. Initially, constructing these datasets required large human effort in generating and curating instruction-response pairs (Databricks, 2023;"}, {"title": "Related Work", "content": "Synthetic Data Generation for Instruction Tuning. While human-crafted instruction datasets (Databricks, 2023; Zheng et al., 2024; Zhao et al., 2024) have been used for LLM instruction tuning, they are time-consuming and labor-intensive. Consequently, synthetic dataset generation has emerged as a promising alternative. Early approaches (Wang et al., 2023; Taori et al., 2023; Xu et al., 2023a,b; Wang et al., 2024b; Luo et al., 2023; Sun et al., 2023) focused on prompting LLMs to generate synthetic instructions, starting with a small set of human-annotated seed instructions and expanding these through few-shot prompting (Li et al., 2024a). Another line of work (Ding et al., 2023; Li et al., 2024a) summarized world knowledge to generate more diverse synthetic datasets. Recent advancements (Xu et al., 2024; Chen et al., 2024) further simplified the process by leveraging single prompts to sample instructions directly from LLMs, requiring minimal human oversight. While existing work primarily focused on generating large, diverse, and high-quality instructions, the impact of response generators is often overlooked.\nMetrics for Data Selection. Instruction tuning data selection involves determining which instruction-response pairs to be included in the training dataset and how to sample them (Albalak et al., 2024). The most widely-used metric for selecting instruction data is quality, which is often assessed using LLM evaluators (Chen et al., 2023; Liu et al., 2024a), reward models (Dubey et al., 2024; Xu et al., 2024), gradient similarity search (Xia et al., 2024a), or a combination of these methods (Cao et al., 2024). Another key metric is difficulty, where higher difficulty is considered more valuable for learning. For instance, Li et al. (2024c) introduces IFD, which measures the instruction-"}, {"title": "Which Models are the most effective teachers for instruction tuning?", "content": "3.1 Preliminaries\nInstruction Datasets. An instruction dataset can be represented as $D = \\{(x_i, y_i)\\}$, where each sample $(x_i, y_i)$ consists of an instruction $x_i$ and its corresponding response $y_i$. In this paper, we investigate how the response generator, denoted as $M$, impacts the instruction-following capabilities of models fined-tuned with $D$ with $y_i = M(x_i)$.\nSupervised Fine-Tuning. Supervised fine-tuning (SFT) is widely adopted to enhance instruction-following capabilities of LLMs. The SFT process updates the parameters $\\theta$ of a pre-trained language model to minimize the negative log-likelihood loss over the instruction dataset $D$. The SFT loss can be formally expressed as:\n$L_{SFT}(\\theta) = \\frac{1}{|D|} \\sum_{(x_i, Y_i) \\in D} -log p_{\\theta} (Y_i|x_i)$.", "latex": ["D = \\{(x_i, y_i)\\}", "y_i = M(x_i)", "L_{SFT}(\\theta) = \\frac{1}{|D|} \\sum_{(x_i, Y_i) \\in D} -log p_{\\theta} (Y_i|x_i)"]}, {"title": "Experimental Setup", "content": "Instruction Sets. To construct diverse and high-quality instructions, we sample from the Magpie-Air-3M dataset (Xu et al., 2024), and obtain a subset of 100K high-quality instructions, denoted as Magpie-100K. A detailed categorization of instruction tasks is provided in Appendix A.1. Additionally, we extracted another 100K high-quality instructions from multiple sources, including Ultra-Feedback (Cui et al., 2023), WildChat (Zhao et al., 2024), Lmsys-Chat-1M (Zheng et al., 2024), and Alpaca-GPT-4 (Gallego, 2023). This instruction"}, {"title": "Empirical Evaluation", "content": "This section evaluates the instruction-following capabilities of models fine-tuned over datasets whose responses are generated by various response generators. By default, we utilize the Magpie-100K dataset as our primary instruction set. Figure 2 provides a comprehensive overview of the AP across different base models and response generators, and the detailed benchmark scores of AE2 and AH are deferred to Table 6 in Appendix B.1.\nWe observe that the Gemma-2 and Qwen2 families consistently demonstrate superior performance across all base models evaluated. Notably, Gemma-2-9b-it and Qwen2.5-72B-Instruct emerge as the two best response generators, as evidenced by their consistently high AP scores. In addition, we report the following key findings.\nFinding 1: [Larger Models' Paradox] Larger response generators \u21d2 improved instruction-following capabilities."}, {"title": "How can we determine the most effective response generators without instruction tuning?", "content": "4.1 Measure the Effectiveness of Response\nGenerators\nIt is computationally expensive to brute-force all response generators to identify the most effective"}, {"title": "Baseline Methods Fails to Measure the Effectiveness of Response Generators", "content": "In what follows, we demonstrate that the effectiveness of response generators indicated by baseline methods does not match the performance of models fine-tuned on various synthetic instruction datasets. As shown in Figure 4, AR consistently increases with model size within model families (except Phi-3 family). However, this trend fails to explain the \"Larger Models Paradox\" discussed in Section 3. Notably, since AR measures human preference,"}, {"title": "A Compatibility-Aware Metric to Measure Effectiveness", "content": "In this section, we present a new metric to measure the effectiveness of response generators, making the \"Larger Models Paradox\" explainable. Our key insight to capture the compatibility of response generators with base models. To reflect such compatibility, we use the loss of the response $r_i$ in the base model being fine-tuned as the key metric. Intuitively, a lower loss of response $y_i$ on the base model indicates that the response aligns well with the base model's existing knowledge and capabilities, thus is more learnable compared to the response with higher loss.\nWhile compatibility is crucial, it alone cannot fully measure effectiveness. Consider a scenario where a response generator consistently produces"}, {"title": "Experimental Results", "content": "Table 4 compares the Spearman's $p$ correlation coefficient of baseline metrics with our CAR when using datasets generated by different response generators to fine-tune various base models. For CAR calculation, we employ Skywork-Reward-Gemma-2-27B as the reward model and set $\\beta = 3$. The results in Table 4 demonstrate that our proposed CAR consistently outperforms other baseline metrics across almost all settings, indicating its potential to predict the effectiveness of different response generators without instruction tuning."}, {"title": "Conclusion and Future Work", "content": "This paper investigates the impact of response generators in synthetic dataset generation for instruction tuning. We uncovered the Larger Models' Paradox, wherein larger response generators do not necessarily enhance a base model's instruction-following capabilities compared to their smaller counterparts within the same model family. To explain this phenomenon, we considered the compatibility between response generators and the base model, and proposed the Compatibility-Adjusted Reward (CAR). Our metric achieved better performance in identifying the effectiveness of different response generators without the need for fine-tuning, outperforming existing baselines in alignment dataset selection.\nWe will explore several promising directions. First, efficiently transforming existing datasets to achieve better compatibility can lead to more effective use of available instruction tuning datasets. Second, investigating theoretical foundations of compatibility would enhance our understanding of the underlying mechanisms of instruction tuning. Lastly, studying the impact of different response"}, {"title": "More on Experimental Setups", "content": "A.1 Instruction Set Details\nFigure 5 demonstrates the task category of instructions in our sampled Magpie-100K. We follow (Xu et al., 2024) and use Llama-3-8B-Instruct to tag the task categories. We note that this instruction set covers wide range of instructions across different task categories."}, {"title": "Supervised Fine-Tuning Setups", "content": "Table 5 demonstrates the detailed supervised fine-tuning (SFT) hyper-parameters. We perform experiments on a server with four NVIDIA A100-SXM4-80GB GPUs, an AMD EPYC 7763 64-Core Processor, and 512 GB of RAM. These experiments were conducted using Axolotl\u00b9."}, {"title": "More Experimental Results", "content": "B.1 Detailed Benchmark Scores of\nInstruction-Tuned LLMs\nTable 6 details the benchmark scores of AE2 and AH when tuning 5 base models with different response generators. These results complement the Average Performance shown in Figure 2.\nB.2 Visualization of baseline methods in measuring the effectiveness of response generators.\nFigure 6 presents the output length of synthetic datasets for each response generator. Figure 7 visualizes the PPL-GPT2 and IFD-GPT2 across different response generators. Figure 8 and 9 reports PPL-Self and IFD-Self, respectively. We observe that although PPL-Self and IFD-Self have higher correlation compared with measuring using GPT2, they still to fail to effectively predict the effectiveness of different response generators, with low Spearman's rank correlation coefficients demonstrated in Table 4."}], "latex": ["D = \\{(x_i, y_i)\\}", "y_i = M(x_i)", "L_{SFT}(\\theta) = \\frac{1}{|D|} \\sum_{(x_i, Y_i) \\in D} -log p_{\\theta} (Y_i|x_i)", "PPL(Y_i | x_i) = \\frac{1}{N} exp(-\\sum_{j=1}^{N} log P_{\\theta} (Y_{i,j} | x_i, Y_{i,1:j-1}))", "IFD(Y_i | x_i) = \\frac{PPL(Y_i X_i)}{PPL(Y_i)}", "\\rho = 1 - \\frac{6 \\sum d_i^2}{n(n^2 - 1)}", "CAR(D_i, \\theta) = \\frac{r(D_i)}{1 + \\beta \\cdot L(D_i, \\theta)}"]}