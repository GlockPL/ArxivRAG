{"title": "Cross-Modal Transferable Image-to-Video Attack on Video Quality Metrics", "authors": ["Georgii Gotin", "Ekaterina Shumitskaya", "Anastasia Antsiferova", "Dmitriy Vatolin"], "abstract": "Recent studies have revealed that modern image and video quality assessment (IQA/VQA) metrics are vulnerable to adversarial attacks. An attacker can manipulate a video through preprocessing to artificially increase its quality score according to a certain metric, despite no actual improvement in visual quality. Most of the attacks studied in the literature are white-box attacks, while black-box attacks in the context of VQA have received less attention. Moreover, some research indicates a lack of transferability of adversarial examples generated for one model to another when applied to VQA. In this paper, we propose a cross-modal attack method, IC2VQA, aimed at exploring the vulnerabilities of modern VQA models. This approach is motivated by the observation that the low-level feature spaces of images and videos are similar. We investigate the transferability of adversarial perturbations across different modalities; specifically, we analyze how adversarial perturbations generated on a white-box IQA model with an additional CLIP module can effectively target a VQA model. The addition of the CLIP module serves as a valuable aid in increasing transferability, as the CLIP model is known for its effective capture of low-level semantics. Extensive experiments demonstrate that IC2VQAachieves a high success rate in attacking three black-box VQA models. We compare our method with existing black-box attack strategies, highlighting its superiority in terms of attack success within the same number of iterations and levels of attack strength. We believe that the proposed method will contribute to the deeper analysis of robust VQA metrics.", "sections": [{"title": "1 INTRODUCTION", "content": "Modern No-Reference Video Quality Assessment (NR-VQA) metrics are vulnerable to adversarial attacks (Yang et al., 2024a), (Yang et al., 2024b), (Zhang et al., 2024), (Siniukov et al., 2023), (Shumitskaya et al., 2024a). This raises concerns about the safety of relying on these metrics to automatically assess video quality in real-world scenarios, such as public benchmarks and in more critical situations, such as autonomous driving. Adversarial attacks on VQA metrics can be classified into two categories: white-box and black-box attacks. White-box attacks operate with complete access to the VQA metric, including its architecture and gradients. In contrast, black-box attacks works without any knowl-"}, {"title": "2 RELATED WORK", "content": "2.1 Image- and Video-Quality Metrics\nImage and video quality assessment (IQA/VQA) metrics can be divided into full-reference and no-reference (also known as blind IQA/VQA). Full-reference quality metrics compare two images/videos, while no-reference metrics assess the visual quality"}, {"title": "2.2 Adversarial Attacks on Image- and Video-Quality Metrics", "content": "The problem of vulnerability analysis of novel NR IQA models to adversarial attacks was widely discussed in previous works: (Yang et al., 2024a), (Leonenkova et al., 2024), (Kashkarov et al., 2024), (Deng et al., 2024), (Konstantinov et al., 2024), (Yang et al., 2024b), (Zhang et al., 2024), (Ran et al., 2025), (Meftah et al., 2023), (Siniukov et al., 2023), (Shumitskaya et al., 2024b), (Shumitskaya et al., 2024a). Some works have been conducted as part of the MediaEval task: \"Pixel Privacy: Quality Camouflage for Social Images\" (MediaEval, 2020), where participants aimed to improve image quality while reducing the predicted quality score. This task is similar to the vanilla adversarial attack on quality metrics, but to decrease the score rather than increase it. In (Bonnet et al., 2020), the authors generated adversarial examples for NR models using PGD attack (Madry et al., 2018). Zhao et al. (Zhao et al., 2023) proposed to attack NR metrics by applying image transformations based on optimizing a human-perceptible color filter. They also demonstrated that this attack is even resistant to JPEG compression. However, these studies are limited to small-scale experiments and lack in-depth analysis. Several comprehensive works have recently been published that systematically investigate adversarial attacks against NR models.\nIn (Zhang et al., 2022), a two-step perceptual attack was introduced for the NR metrics. The authors established the attack's goal as a Lagrangian function that utilizes some FR metric, which acts as a \"perceptual constraint\", alongside the NR metric representing the target model. By adjusting the Lagrange multiplier, they produced a range of perturbed images that exhibited varying degrees of visibility regarding their distortions. Their extensive experiments demonstrated that the proposed attack effectively deceived four different NR metrics; however, the adversarial examples did not transfer well across various models, indicating specific design vulnerabilities within the NR metrics assessed. In (Shumitskaya et al., 2022), the authors trained the UAP on low-resolution data and then applied it to high-resolution data. This method significantly reduces the time required to attack videos, as it requires only adding perturbations to individual frames. In the study by (Korhonen and You, 2022), the authors create adversarial perturbations for NR metrics by injecting the perturbations into textured areas using the Sobel filter. They also demonstrated that adversarial images generated for a simple NR metric in white-box settings are transferable and can deceive several NR metrics with more complex architecture in black-box settings. In (Antsiferova et al., 2024), the authors presented a methodology for evaluating the robustness of NR and FR IQA metrics through a wide range of adversarial attacks and released an open benchmark.\nTo the best of our knowledge, no methods have been designed for transferable cross-modal attacks from NR IQA to NR VQA metrics, which is a subject of this work."}, {"title": "2.3 Transferable Attacks on Image Classification", "content": "Adversarial attacks have received significant attention in the domain of machine learning, particularly in image classification tasks. The phenomenon of transferability, where adversarial examples generated on one model can deceive another (potentially different) model, has been investigated in many works. Papernot et al. (Papernot et al., 2016) explored this aspect and demonstrated that transferability is a useful property that could be exploited in black-box settings, where the attacker has limited knowledge of the target model. They also experimentally showed that adversarial examples could be trained on weaker models and successfully deceive more robust classifiers. Various methods have been proposed to enhance the effectiveness of transferable attacks. Some of them (Xie et al., 2019), (Lin et al., 2019), (Dong et al., 2019) apply data-augmentation techniques to enhance the generalization of adversarial examples and reduce the risk of overfitting the white-box model. For example, the translation-invariant attack (Dong et al., 2019) executes slight horizontal and vertical shifts of the input. The second direction to improve transferability is to modify the gradients used to update adversarial perturbations (Dong et al., 2019), (Lin et al., 2019), (Wu et al., 2020a). For example, the momentum iterative attack (Dong et al., 2019) stabilizes the update directions using the addition of momentum in the iterative process. The third approach concentrates on disrupting the shared classification properties among different models (Wu et al., 2020b), (Huang et al., 2019), (Lu et al., 2020). One example is the Attention-guided attack (Wu et al., 2020b), which prioritizes the corruption of critical features that are commonly utilized by various architectures. Recently, innovative cross-modal approaches have been proposed that leverage the correlations between spatial features encoded by different modalities (Wei et al., 2022), (Chen et al., 2023), (Yang et al., 2025). Image2Video attack, proposed in (Wei et al., 2022), is an attack to successfully transfer from image to video recognition models."}, {"title": "3 PROPOSED METHOD", "content": "3.1 Problem Formulation\nLet's consider we have a video $x \\in \\mathcal{X} \\subset [0,1]^{N\\times C\\times H\\times W}$, where $N$ - number of frames in video, $C$ - number of channels in video, $H, W$ - height and width of video respectively, $\\mathcal{X}$ is the set of all possible videos. We define video quality metric as $f: \\mathcal{X} \\rightarrow [0;1]$, image quality metric as $g: [0,1]^{C\\times H \\times W} \\rightarrow [0;1]$. Image quality metric can be expressed in layered form as $g = h_k\\circ h_{k-1}\\circ ... \\circ h_1$, so\n\\begin{equation}\ng(x_i) = h_k(h_{k-1}(...h_1(x_i)...)),\n\\end{equation}\nwhere each function $h_k: \\mathcal{P}_{k-1} \\rightarrow \\mathcal{P}_k$ corresponds to a processing layer with $\\mathcal{P}_0 = [0,1]^{C\\times H\\times W}$ being the input feature space and $\\mathcal{P}_K = [0,1]$ being the output range of the metric. $g_k$ defines the composition of the first $k$ layers:\n\\begin{equation}\ng_k = h_k \\circ ... \\circ h_1\\\\\ng_k: [0,1]^{C\\times H\\times W} \\rightarrow \\mathcal{P}_k.\n\\end{equation}\nEach $g_k$ serves the $k$-th layer of the quality metric, where $\\mathcal{P}_k$ represents the feature spaces corresponding to that layer.\n3.2 Method\nThe primary goal of the attack is to make the predicted quality score of video $f(x + \\delta)$ on the attacked video deviate from the original score $f(x)$, where $\\delta$ is the perturbation on the video $x$. Also, the rank of correlation of predicted score with MOS is important, so our goal is to shrink it as possible. This method was based on method proposed by Zhipeng Wei as I2V(Wei et al., 2022).\nThe proposed attack is designed to mislead the video quality metric. It creates adversarial frame $\\delta_i \\in [0,1]^{C\\times H\\times W}$ for each $i$-th frame on the input video. To maintain the imperceptible of this adversarial perturbation, we import a constraint on its magnitude $|\\|\\delta|\\|_p < \\epsilon$, where $|\\|\\cdot|\\|_p$ denotes $L_p$ norm. In our research, we adopt the $L_\\infty$ norm due to its computational efficiency compared to other $L_p$ norms.\nBased on observation of correlations between lay-"}, {"title": "3.3 Algorithm", "content": "We construct our attack as presented in Algorithm 1, which is applied to image quality metrics. At each step of the attack, the cross-layer loss for the $f$-th"}, {"title": "6 ABLATION STUDY", "content": "6.1 Loss Configuration\nTo experimentally demonstrate effectiveness of combination of losses in comparison with single $L_{xlayer}$, we evaluated our attack in configuration with only one image quality metric $L_{xlayer}$, with one image quality metric and CLIP image model $L_{xlayer} + L_{CLIP}$ and with one image quality metric, CLIP image model and temporal regularization $L_{xlayer} + L_{CLIP} + L_{temp}$. In experiment we evaluate IC2VQA configurations on white-box models NIMA, PaQ-2-PiQ and SPAQ and black-box VSFA model and scored them by median absolute value of correlations. The results of the comparison are shown in the Table 2 and Figure 3. From Table 2, we observe that addition of cosine similarity between CLIP features ($L_{CLIP}$) to the loss function enhances the attack's success by 1.8 times. The temporal loss increases the attack's success in terms of SRCC by 1.2 times and slightly decreases PLCC. Figure 3 shows that the combined loss function $L_{xlayer} + L_{CLIP} + L_{temp}$ outperforms others in at-"}, {"title": "6.2 Feature Correlation", "content": "In this section, we analyze the correlations between features in the deep layers of IQA and VQA metrics. Figure 5 presents the heatmap of correlations between features from the VSFA VQA model and the NIMA"}, {"title": "7 CONCLUSION", "content": "In this paper we propose the novel adversarial attack on VQA metrics that operates as a black-box. The proposed IC2VQAperforms a cross-modal transferable attack that utilizes white-box IQA metrics and the CLIP model. The results of extensive experiments showed that IC2VQAgenerates adversarial perturbations that are more effective compared to previous approaches, significantly reducing the SRCC and PLCC scores of a black-box VQA model. The proposed method can serve as a tool for verifying VQA metrics robusthess to black-box attacks. Furthermore, the vulnerabilities identified in this study can contribute to the development of more robust and accurate VQA metrics in the future."}]}