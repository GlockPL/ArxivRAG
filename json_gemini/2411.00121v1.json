{"title": "I CAN HEAR YOU: SELECTIVE ROBUST TRAINING FOR DEEPFAKE AUDIO DETECTION", "authors": ["Zirui Zhang", "Wei Hao", "William Lin", "Emanuel Mendiola-Ortiz", "Aroon Sankoh", "Junfeng Yang", "Chengzhi Mao"], "abstract": "Recent advances in AI-generated voices have intensified the challenge of detecting deepfake audio, posing further risks for the spread of scams and disinformation. To tackle this issue, we establish the largest public voice dataset to date, named DeepFakeVox-HQ, comprising 1.3 million samples, including 270,000 high-quality deepfake samples from 14 diverse sources. Despite previously reported high accuracy, existing deepfake voice detectors struggle with our diversely collected dataset, and their detection success rates drop even further under realistic corruptions and adversarial attacks. We conduct a holistic investigation into factors that enhance model robustness and show that incorporating a diversified set of voice augmentations is beneficial. Moreover, we find that the best detection models often rely on high-frequency features, which are imperceptible to humans and can be easily manipulated by an attacker. To address this, we propose the F-SAT: Frequency-Selective Adversarial Training method focusing on high-frequency components. Empirical results demonstrate that our training dataset boosts baseline model performance (without robust training) by 33%, and our robust training further improves accuracy by 7.7% on clean samples and by 29.3% on corrupted and attacked samples, over the state-of-the-art RawNet3 model.", "sections": [{"title": "1 INTRODUCTION", "content": "AI-generated voices have become increasingly realistic due to larger datasets and enhanced model capacities (Ju et al., 2024; Neekhara et al., 2024), and they have been used in many important applications (Calahorra-Candao & Mart\u00edn-de Hoyos, 2024). However, the success of AI-synthesized human voices poses significant security risks, including deepfake voice fraud and scams (Tak et al., 2021; Sun et al., 2023; Yang et al., 2024). A recent CNN report reveals a fraud in Hong Kong where a finance worker sent $25 million to scammers after a video call with a deepfake 'chief financial officer'. The voice was created by an AI model, highlighting the risk of such technology.\nDue to the importance of this problem, a number of work has investigated detecting AI-generated audio. Despite previously reported high detection accuracy on public datasets (Todisco et al., 2019; Frank & Sch\u00f6nherr, 2021), existing deepfake voice detectors perform poorly under real-world conditions (Xu et al., 2020; M\u00fcller et al., 2022; Radford et al., 2023). This is because the established benchmarks are often trivial, small, outdated, and homogeneous. Consequently, models trained and validated solely on these datasets fail to generalize to more diverse and challenging real-world deep-fake samples. Moreover, deep learning models for audio are particularly vulnerable to adversarial attacks (Szegedy et al., 2013) (Zhang et al., 2019), where an attacker can subtly alter audio inputs in ways that are imperceptible to humans but mislead models into incorrect classifications. Figure 1 illustrates a dramatic shift in the models' prediction scores when exposed to these factors, underscoring the need for more robust training methodologies.\nTo address the above limitations, we first created the largest deepfake audio dataset to date, DeepFakeVox-HQ, including 270,000 high-quality deepfake samples from 14 diverse and distinct origins. We show that simply training on our collected dataset can produce new state-of-the-art models.\nMoreover, we find that even the state-of-the-art AI-voice detection models often depend on high- frequency features to make decisions (see Figure 2), which are imperceptible to humans. On the other hand, the low frequency signals can be heard by humans but are not relied on by the model to make predictions. As a result, natural corruptions in high frequency or attackers can easily manipulate the model by changing the high frequency signals, reducing the detection's robustness.\nIn an initial study, we observed that standard adversarial training on raw waveforms not only fails to bolster robustness but also diminishes performance on unattacked data. To address these shortcomings, we propose Frequency-Selective Adversarial Training (F-SAT), which focuses on high-frequency components. Since our adversarial training is targeted, we can mitigate specific vulnerabilities without touching the true features at lower frequencies, thus enhancing the model's resilience to corruptions and attacks while maintaining high accuracy on clean data.\nVisualizations and empirical experiments demonstrate that using only our training dataset, we can produce state-of-the-art models, achieving a 33% improvement on the out-of-distribution portion of our test set, which includes 1,000 deepfake samples from the top five AI voice synthesis companies and 600 samples from social media. Additionally, by incorporating random audio augmentations,"}, {"title": "2 RELATED WORK", "content": "AI-synthesized human voice: AI voice synthesis generally falls into two categories: text-to-speech (TTS) and voice conversion (VC). TTS systems convert written text into spoken audio using a desired voice. This process typically involves three main components: a text analysis module that transforms text into linguistic features, an acoustic model that converts these features into a mel- spectrogram, and a vocoder. Some of the leading TTS models include StyleTTS (Li et al., 2024), VoiceCraft (Peng et al., 2024), XTTS (Casanova et al., 2024), and Tortoise-TTS (Betker, 2023), and are known for their ability to produce high-quality audio that closely mimics real human speech.\nVC models, in contrast, take an audio sample from one person and transform it to sound like another person for the same speech content. Recent VC approaches primarily operate within the mel-spectrum domain (Ju et al., 2024; Shen et al., 2023; Popov et al., 2021), using deep neural networks to shift the mel-spectrograms from the source to the target voice.\nDetection Method: AI deepfake detection models based on deep learning can be grouped into two main categories: those processing raw audio end-to-end and those analyzing spectrum. The first category includes models like RawNet2, which employs Sinc-Layers (Ravanelli & Bengio, 2018) to extract features directly from waveforms, and RawGAT-ST, which utilizes spectral and temporal sub-graphs (Tak et al., 2021). RawNet3 (Jung et al., 2022), which begins by using parameterized filterbanks (Zeghidour et al., 2018) to extract a time-frequency representation from the raw waveform and then is followed by three backbone blocks with residual connections, a structural approach that sets it apart from ECAPA-TDNN (Desplanques et al., 2020).These models process the audio data in its raw form to capture nuanced details directly impacting model performance.\nThe second category of AI deepfake detection models involves transforming raw audio into spectrograms for analysis. This process utilizes extracted features such as Mel Frequency Cepstral Coefficients (MFCCs) (Sahidullah & Saha, 2012), Constant Q Cepstral Coefficients (CQCCs) (Todisco et al., 2017), bit-rate (Borz\u00ec et al., 2022), and Linear Frequency Cepstral Coefficients (LFCCs) (Li et al., 2021). The analysis of these features is conducted using traditional machine learning methods like Gaussian Mixture Models (GMMs) (Todisco et al., 2019) or advanced neural networks such as Bidirectional Long Short-Term Memory (Bi-LSTM) (Akdeniz & Becerikli, 2021), ResNet (Alzantot et al., 2019), and Transformers (Zhang et al., 2021). These methods enable deeper and more intricate pattern recognition, enhancing the model's ability to identify and classify deepfake audio accurately.\nAdversarial Attack: Adversarial training, originally developed for image processing systems (Madry et al., 2017; Mao et al., 2019), has been increasingly adapted to the audio domain (Chen et al., 2023), particularly to enhance the robustness of applications such as Automatic Speech Recognition (ASR) and speaker verification systems. This method aims to mitigate vulnerabilities to deceptive audio inputs, thereby improving security (Hussain et al., 2021). Another defensive strategy employs diffusion models to counter adversarial audio attacks (Wu et al., 2023), effectively smoothing out perturbations and hindering attackers from altering audio signals without significantly compromising signal quality.\nWhile these techniques enhance robustness, they hurt the detection on unattacked audio, highlighting a trade-off between robustness and accuracy (Zhang et al., 2019; Tsipras et al., 2018). This compromise is particularly critical in scenarios that demand high accuracy and user satisfaction. Furthermore, the generalizability of adversarially trained models to new and unseen attacks remains limited (Rajaratnam et al., 2018), raising questions about their effectiveness in rapidly evolving threat environments."}, {"title": "3 DEEPFAKEVOX-HQ", "content": null}, {"title": "3.1 TRAINING DATASET", "content": "In this section, we introduce a new training dataset and a rigorous test set. In contrast to prior dataset, our dataset is large, diversified, realistic, and up-to-date, as shown in Table 1. Prior detectors show poor generalization capabilities in realistic settings, as shown in Figure 3. Both our training and test- ing datasets integrate the latest advancements in AI voice synthesis technologies. Additionally, the testing dataset includes several new models not covered in the training dataset, specifically designed to test the generalization ability of our detection systems.\nHigh quality deepfake samples: The limitations of existing public datasets, which often lack high-quality deepfake samples, can potentially impair model performance. To address this, we have in- vestigated more than 30 recent advancements in Text-to-Speech (TTS) and Voice Conversion (VC) models developed in the past few years. Our training dataset now includes deepfake audio sam- ples generated using the top seven TTS models: MetaVoice-1B (Liu et al., 2021), StyleTTS-v2 (Li et al., 2024), VoiceCraft (Peng et al., 2024), WhisperSpeech (Radford et al., 2023), VokanTTS, XTTS-v2 (Casanova et al., 2024), and Elevenlabs. We use four datasets-VCTK (Yamagishi, 2012), LibriSpeech (Panayotov et al., 2015), In-The-Wilds (M\u00fcller et al., 2022), and AudioSet (Gemmeke et al., 2017) to generate deepfake audio. These datasets include both clean, high-quality and noisy, low-quality real audio, ensuring that the deepfake audio produced is highly diverse and accurately reflects real-world conditions.\nReference data: For both fake and real audio, having only high-quality samples is insufficient. A broader diversity of samples is essential for the training dataset. Thus, for real audio, we utilize portions from six public audio datasets: VCTK, LibriSpeech, AudioSet, ASRspoof2019 (Todisco et al., 2019), Voxceleb1 (Nagrani et al., 2017), and ASRspoof2021 (Liu et al., 2023), with half consisting of clean audio and the other half of noisy audio. For fake audio, we include two low-quality fake audio datasets: WaveFake (Frank & Sch\u00f6nherr, 2021) and ASRspoof2019 to further enhance the diversity of the training material."}, {"title": "3.2 TESTING DATASET", "content": "Our test dataset comprises approximately 7,000 samples, balanced equally between real and fake audio. Real audio samples are sourced from recent celebrity speeches and conversational videos on platforms such as YouTube. For the fake audio, we not only utilize samples created using the seven latest TTS models but have also expanded our dataset to include contributions from eight of the most advanced AI voice synthesis models or commercial software currently available, namely Cosy Voice (Du et al., 2024), Resemble, Speechify, LOVO AI, Artlist, and Lipsynthesis. Additionally, this set includes fake audio directly collected from social media platforms like YouTube and X, further enriching the dataset with a diverse range of real-world scenarios. This comprehensive composition is strategically designed to rigorously test the generalization capabilities of our model.\nInsight: Figure 3 shows that training with previous public datasets yields lower accuracy on ours. Additionally, removing high-quality deepfake samples from our training set significantly also re-duces accuracy, highlighting their importance.\""}, {"title": "4 \u041c\u0415\u0422\u041dOD", "content": "In this section, we present our selective adversarial training approach, F-SAT. We also present a taxonomy of the most common corruptions and attacks in audio processing, which can be used for robust evaluation in realistic settings. Additionally, we discuss the implementation of Rand-Augmentation for audio to further enhance the robustness of our detection system.\nhodologies."}, {"title": "4.1 F-SAT: FREQUENCY-SELECTIVE ADVERSARIAL TRAINING", "content": "Let x be a waveform input audio, and y be its ground-truth category label, To perform classification, neural networks commonly learn to predict the category \u0177 = $F_\\theta$(x) by optimizing the cross-entropy H(\u0177, y) between the predictions and the ground truth. We use RawNet3 (Jung et al., 2022) that can process waveform audio input. The network parameters \u03b8 are estimated by minimizing the expected value of the objective:\n$L_c(x, y) = H(F_\\theta(x), y)$,\nTime domain Attack: Adversarial attacks on audio can be directly added to the waveform. Let the additive perturbations be \u03b4. For attacks in the time domain, we directly add \u03b4 to the waveform x. Due to the huge amount of freedom of the attack vector \u03b4, the added attack vectors are often high frequency. We formulate this attack as: x' = x + \u03b4.\nFrequency Domain Attack: Attacks can also be applied in the frequency domain, accessed through a reversible Fourier transformation of the waveform. We transform the waveform x to the frequency domain X using the Short-Time Fourier Transform (STFT). The attack modifies X by adding \u03b4, yielding X' = X + \u03b4. The Inverse Short-Time Fourier Transform (ISTFT) then reverts X' back to the time domain, creating the manipulated audio waveform x'.\nFrequency-Selective Attack: Compared to time-domain attacks, frequency-domain attacks provide an inductive bias that enables the crafting of more controlled perturbations, such as those within a restricted bandwidth. Building on this, we introduce a Frequency-Selective Attack that targets specific frequency ranges within the magnitude component of a waveform. We define this attack as\nx' = s(x, \u03b4, $f_l$, $f_u$)\nwhere $f_l$ and $f_u$ represent the lower and upper frequency boundaries of the range where the attack is to be applied. The procedure can be described as follows:\nStarting with STFT result X, we first compute its magnitude component $X_\\rho$, and phase component $X_\\phi$. To map the frequency range to the index range of the STFT spectrum, we calculate the lower and upper boundary indices $r_l$ and $r_u$ of the FFT spectrum using formulas:\n$r_l = \\frac{f_l \\times n_{fft}}{s_r}$,\n$r_u = \\frac{f_u \\times n_{fft}}{s_r}$\nwhere $n_{fft}$ is the number of FFT points, and $s_r$ is the sampling rate of the original signal x. We then define a mask function M using a diagonal matrix D such that\n$D_{kk} = \\begin{cases} 1 & \\text{if } r_l \\leq k \\leq r_u \\\\ 0 & \\text{otherwise} \\end{cases}$\nThis matrix D selectively targets the desired frequency components within the specified range [$r_l$, $r_u$] for perturbation. The selective perturbation is then defined as:\n$\\delta_\\zeta = M(\\delta, r_l, r_u) = D \\cdot \\delta$.\nBy adding this perturbation \u03b4, to the magnitude component of the spectrogram, the perturbed spec- trogram can be represented as:\nX' = $(X_\\rho + \\delta_\\zeta) \\cdot e^{jX_\\phi} = (X_\\rho + D \\cdot \\delta) \\cdot e^{jX_\\phi}$\nFinally, we employ the ISTFT to convert the attacked spectrogram X' back into the time-domain signal x', thus generating the attacked audio waveform x'.\nMoreover, attackers design these worst-case perturbations to disrupt a trained network $F_\\theta$, aiming to maximize misclassifications by optimizing the objective:\nx' = $\\arg \\max_{x'} L_\\theta(x', y)$, s.t. $\\|x' - x\\|_q < \\epsilon$,\nwhere the perturbation \u03b4 = x' \u2212 x is constrained by the q norm to be less than \u03f5, ensuring minimal deviation.\nF-SAT: Based on the Frequency-Selective Attack, we propose F-SAT, an adversarial training method which optimizes perturbations in the frequency domain by targeting the magnitude component within specific frequency ranges.\n$X_{n+1} = \\Pi_{X+\\mathcal{S}}(X_n + \\alpha \\cdot M \\cdot sgn(\\nabla_{X'}L(s(x, \\delta, f_l, f_u), y)), f_u, f_l)$,\nwhere $\\Pi_{X+\\mathcal{S}}$ represents the projection onto the allowable perturbation set S, defined by the condition $\\|x' - x\\|_p \\leq \\epsilon$. The parameter \u03b1 denotes the step size of the update, and $\\nabla_{X'}L$ is the gradient of the loss function with respect to the perturbed input x'.\nAs illustrated in Figure 4, after K iterations (where n \u2265 K), we feed the most perturbed sample back into the detection model and calculate the cross-entropy loss between it and the ground truth label:\n$L_{robust}(x', y) = H(F_\\theta(x'), y)$,\nAdditionally, to maintain a balance between accuracy on original and attacked samples, we train the model using both the original and perturbed samples. We introduce a parameter \u03b3 to control the trade-off between clean loss and robust loss. The clean loss is defined as:\n$L_{clean}(x, y) = H(F_\\theta(x), y)$,\nThe total loss $L_{total}$ is then computed as a weighted sum of clean and robust losses.\n$L_{total} = L_{clean} + \\gamma \\cdot L_{robust}$\nSince time domain attacks are often high frequency, by focusing the adversarial training on the high frequency, our approach not only enhances model robustness against frequency-targeted attacks, but also improves defenses against time-domain attacks."}, {"title": "4.2 RANDAUGMENT FOR AUDIO", "content": "Realistic Corruptions for Deepfake Audio Generation: As shown in Figure 5, we present a tax-onomy for analyzing the robustness of deepfake audio detection systems that incorporate the most common corruptions and attacks. Corruptions in audio result from unintentional modifications dur- ing recording, processing, or transmission, impacting noise levels and frequency responses. Ad-versarial attacks, in contrast, are intentional manipulations aimed at deceiving detection systems through subtle changes across various attack methodologies.\nRandAugment: Experiments show that the best detection model, RawNet3, experiences a great drop in accuracy when faced with audio corruptions. Inspired by the image-based RandAug- ment (Cubuk et al., 2020), which improves model robustness, we adapted this method for audio. We selectively apply N transformations from the available options, each assigned a uniform prob-ability. An additional probability \u221adetermines whether each transformation is applied at a given instance, to balance the accuracy between original and corrupted samples. The magnitude of each transformation is controlled within predefined boundaries, with the intensity randomly sampled from this range."}, {"title": "5 EXPERIMENTS", "content": "We first compare our approach to existing state-of-the-art methods across three benchmarks and demonstrate improved accuracy. We then assess its robustness against corruption and adversarial attacks. We finally conduct ablation study on enhancements to the detection system's robustness."}, {"title": "5.1 DATASET", "content": "All models are trained on our training dataset and then evaluated on various benchmark datasets to test their generalization capabilities.\nDeepFakeVox-HQ (test) Our custom test dataset incorporates 14 of the latest and highest quality TTS and VC models to generate fake audio. It also includes fake audio samples directly collected from social media platforms such as YouTube and X, providing a diverse set of real-world scenarios."}, {"title": "5.2 BASELINE", "content": "RawNet2 (Tak et al., 2021) employs Sinc-Layers (Ravanelli & Bengio, 2018) to directly extract features from audio waveforms. These layers function as band-pass filters that enhance the detection of spoofed audio content.\nRawGAT-ST (Tak et al., 2021) utilizes spectral and temporal sub-graphs integrated with a graph pooling strategy, effectively processing complex auditory environments.\nTE-ResNet (Zhang et al., 2021) processes synthetic speech detection by first extracting MFCC from input speech. These coefficients are used as features for a CNN that extracts spatial features, fol- lowed by a Transformer that analyzes these to detect characteristics of synthetic speech effectively.\nRawNet3 (Jung et al., 2022) begins by using parameterized filterbanks to extract a time-frequency representation from the raw waveform. This is followed by three backbone blocks with residual connections, a structural approach that deviates from ECAPA-TDNN (Desplanques et al., 2020)."}, {"title": "5.3 MAIN RESULT", "content": "Results on Original Data: We trained all models on DeepFakeVox-HQ and tested them across three benchmarks: our test set, ASVspoof2019, and WaveFake, as shown in Table 2. Our method achieved state-of-the-art results across all three benchmarks. Compared with RawNet3, our method shows improvements of 7.7% points on DeepFakeVox-HQ (test), 8.4% points on ASVspoof2019, and 0.1% points on WaveFake.\nMoreover, our baseline is divided into two categories: waveform-processing models, such as RawGAT-ST, RawNet2, and RawNet3, and the spectrum-processing TE-TesNet. Compared to the spectrum-based TE-TesNet, which only performs well on simpler datasets like WaveFake, the wave- form models demonstrate superior effectiveness on our more diverse and complex dataset. This in-dicates that models with raw audio inputs are better equipped to capture detailed features in complex scenarios.\nFigure 6 outlines results across various Al-voice synthesis models on our test set. For the fake sources are included in the training, all models demonstrated high accuracy. However, for un- foreseen source, our method significantly outperformed others, particularly those from real-world social media platforms like YouTube and Lipsync, by up to 50% points, highlighting its superior generalization capabilities on out-of-distribution data.\nResult on Corrupted data: Figure 7a shows detailed results for various corruptions. Our methods are depicted in the green region with F-SAT, and in the red region without F-SAT. They outperform"}, {"title": "5.4 ABLATION STUDY AND ANALYSIS", "content": "Choice of attack type for F-SAT: As discussed in Section 4.2, there are three types of adversarial attacks on audio: time domain, frequency domain on magnitude, and frequency domain on phase. All of these can be employed for adversarial training. However, our focus on targeting the magnitude component in attacks is explained in Figure 8a. Through our evaluation, we found that attacks based on magnitude in the frequency domain are particularly effective at degrading model performance. This is due to the fact that changes in magnitude directly affect the amplitude of audio signals, crucial for maintaining core acoustic features. Such changes alter the sound intensity across frequencies, complicating the model's task of distinguishing key characteristics between real and fake audio. Conversely, phase attacks have minimal impact on the model's predictions, as phase alterations primarily influence spatial audio perception and do not significantly affect feature detection."}, {"title": "Impact of frequency range for F-SAT", "content": "The importance of high-frequency components is under- scored in Figure 8b, which shows the model's performance under gradient-based adversarial attacks across different frequency ranges. Attacks targeting higher frequencies notably degrade deepfake detection more than lower frequencies, underscoring the vulnerability of high-frequency features. By focusing attacks on high-frequency regions, we preserve low-frequency integrity, simplifying the model's task of distinguishing deepfake audio features. This approach maintains high accuracy on unattacked data and improves adversarial robustness.\nFurther analysis on selecting the optimal frequency range for F-SAT is presented in Table 4. Ad- versarial training within the 4k to 8k frequency range yields the best performance. We observe that increasing the attack frequency range decreases accuracy on original data due to the distortion of more critical features, adding complexity to the model's learning process.\nMoreover, we observe that narrowing the frequency range of F-SAT increases the accuracy for at-tacked fake data while decreasing it for attacked real data. This supports the findings shown in Figure 2, which highlight a significant gap between the frequency domains where real and fake au-dio features predominantly exist. Fake audio features are concentrated in higher frequency ranges compared to real audio. In most real-world applications, criminals aim to make fake audio sound real enough to deceive detectors for committing fraud. Therefore, focusing adversarial training on high frequencies effectively enhances the robustness of fake audio."}, {"title": "Balance Between Clean and Attacked Data", "content": "Figure 9a demonstrates that F-SAT achieves optimal performance with an attack magnitude of \u03f5 = 0.01, maintaining high accuracy on both attacked and clean data. Figure 9b reveals that a ratio of 0.1 between robust and clean loss, ($L_{robust}/L_{clean}$), results in the highest overall accuracy. The results show that excessive focus on attacked data does not consistently boost robustness and may decrease clean data accuracy. Similarly, an undue em-phasis on clean data does not reliably enhance accuracy and can weaken robustness. Thus, striking a balance between clean and attacked samples is critical for optimal model performance."}, {"title": "6 CONCLUSION", "content": "In our study, we introduce a dataset DeepFakeVoc-HQ that addresses diversity and quality issues in prior datasets, and provide a taxonomy to explore common audio corruptions and attacks. We find that leading AI voice detection models depend on vulnerable high-frequency features. This discovery leads us to develop F-SAT, a targeted adversarial training method that focuses on high- frequency components while while preserving the integrity of low-frequency features. Our approach effectively maintains accuracy on unattacked data and enhances robustness against various attacks. These results pioneer robust training for detecting fake audio for the first time, opening up a new direction for identifying such threats."}, {"title": "A APPENDIX", "content": null}, {"title": "A.1 EXPLANATION OF SOME TYPES OF CORRUPTIONS", "content": "Air Absorption: Air absorption refers to the phenomenon where high-frequency sound waves are more strongly attenuated by air molecules. As sound travels through the air, it loses energy, partic- ularly at higher frequencies, due to air viscosity and thermal conduction.\nRoom Simulator: A room simulator is a digital tool that emulates the acoustics of different rooms or spaces. It includes parameters such as room size, wall materials, and shape, which affect how sound reflects and diffuses.\nPeaking: Peaking refers to adjusting a specific range of frequencies around a central frequency. It is commonly used in parametric equalizers.\nAliasing: Aliasing occurs when high-frequency components are sampled below the Nyquist rate, causing them to appear as lower frequencies. This can be represented using the Nyquist theorem:\n$f_{sample} > 2f_{max}$,\nwhere $f_{sample}$ is the sampling frequency, and $f_{max}$ is the maximum frequency of the signal. When this condition is violated, the signal is folded back into the lower frequencies, creating aliasing artifacts.\nBit-Crush: Bit-crushing reduces the bit depth of an audio signal, causing quantization errors.\nTanh Distortion: Tanh distortion is a form of soft clipping achieved using the hyperbolic tangent function. The output y is given by:\ny = tanh(kx),\nwhere x is the input signal, and k controls the amount of distortion. As k increases, the function approximates a hard clipping effect.\nGain Transition: Gain transition refers to smoothly adjusting the amplitude over time.\nSeven-Band Parametric EQ: A seven-band parametric EQ provides independent control over seven frequency bands. Each band can be adjusted using three parameters: center frequency (fo), gain (G), and bandwidth (Q). The overall transfer function is a combination of individual band filters:\n7\nH(f) = $\\prod$ Hi(f),\ni=1\nwhere Hi(f) represents the frequency response of the i-th band.\nTime Mask: Time masking involves temporarily silencing or removing a segment of audio."}, {"title": "A.2 RANDAUGMENT FOR AUDIO", "content": "audio_transforms = [\n    'background_noise', 'color_noise', 'short_noise', 'gaussian_noise', 'air_absorption', 'room_simulator', 'band_pass',\n    'band_stop', 'high_pass', 'low_pass', 'high_shelf', 'low_shelf', 'peaking', 'aliasing', 'bit_crush', 'clip',\n    'tanh_distortion', 'gain_transition', 'seven_band_parametric_EQ', 'pitch_shift', 'time_mask', 'time_stretch'\n]\ndef rand_augment_audio(sample, N, p):\n    \"\"\"Apply random augmentations to an audio sample.\n    Args:\n        sample: An audio sample N: Number of augmentations to apply p: Probability of applying each augmentation\n    Returns:\n        An augmented audio sample\n    \"\"\"\n    operations = np.random.choice (audio_transforms, N)\n    return operations (sample) if random.random() < p else sample"}, {"title": "A.3 COMPREHENSIVE ANALYSIS OF ALL ATTACK TYPES", "content": "Table 5 presents the detailed adversarial attack results for RawNet3 and our method under vari-ous conditions. It includes both white-box and black-box approaches and examines the impacts of attacks in both the time and frequency domains, detailing the different attack hyperparameters used."}, {"title": "A.4 GENERALIZATION TO SPECTURM-BASED MODEL", "content": "We further investigate whether our method can be generalized to spectrum-based models, despite its relatively inferior performance compared to models utilizing raw waveform inputs. We selected a prior baseline model, TE-ResNet, for evaluation. Unlike models trained on raw waveforms, TE-ResNet exhibits a different vulnerability pattern, with its susceptible features primarily concentrated in the lower frequency ranges. To address this, we applied targeted F-SAT on the 0-4kHz frequency range. The results, as shown in Table 6, indicate that incorporating RandAug into TE-ResNet im- proves model accuracy on the original data by 6.9%, while the additional F-SAT boosts overall robustness under attack scenarios, enhancing Attack Overall accuracy by 7.7%. These findings sug- gest that our method can be adapted to spectrum-based models, yielding enhanced performance and robustness."}]}