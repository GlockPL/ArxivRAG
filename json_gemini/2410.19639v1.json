{"title": "Planning-Aware Diffusion Networks for Enhanced Motion Forecasting in Autonomous Driving", "authors": ["Yunhao Liu", "Hong Ding", "Ziming Zhang", "Huixin Wang", "Jinzhao Liu", "Suyang Xi"], "abstract": "Autonomous driving technology has seen significant advancements, but existing models often fail to fully capture the complexity of multi-agent environments, where interactions between dynamic agents are critical. To address this, we propose the Planning-Integrated Forecasting Model (PIFM), a novel framework inspired by neural mechanisms governing decision-making and multi-agent coordination in the brain. PIFM leverages rich contextual information, integrating road structures, traffic rules, and the behavior of surrounding vehicles to improve both the accuracy and interpretability of predictions. By adopting a diffusion-based architecture, akin to neural diffusion processes involved in predicting and planning, PIFM is able to forecast future trajectories of all agents within a scenario. This architecture enhances model transparency, as it parallels the brain's method of dynamically adjusting predictions based on external stimuli and other agents' behaviors. Extensive experiments validate PIFM's capacity to provide interpretable, neuroscience-driven solutions for safer and more efficient autonomous driving systems, with an extremely low number of parameters.", "sections": [{"title": "I. INTRODUCTION", "content": "Autonomous driving has revolutionized the automotive industry, promising significant improvements in safety and efficiency [1], [2]. However, the complexity of real-world driving environments, where multiple dynamic agents such as vehicles and pedestrians interact, remains a challenge. One of the key problems that autonomous systems face is accurate trajectory prediction, which involves forecasting the future movements of surrounding agents based on their current states and the environment [3], [4].\nTraditionally, motion forecasting models predict the trajectories of individual agents without accounting for the interdependencies between them. In multi-agent environments, this approach leads to suboptimal predictions, as it fails to capture the complex interactions among agents [5], [6], [7]. To overcome these limitations, recent works have started incorporating planning information into the trajectory prediction process [8], [9], allowing systems to make more informed decisions. The integration of planning data enhances both accuracy and interpretability, but there remains a need for a comprehensive model that can handle multi-agent interactions effectively.\nTrajectory prediction for autonomous driving has evolved significantly over the past few years, with various approaches leveraging multi-modal fusion techniques to improve accuracy. Early models relied on historical trajectories and map data, often concatenating these inputs to provide predictions [10], [11]. More recent works, such as those using transformer-based architectures, have introduced attention mechanisms to better integrate these diverse modalities [12], [13]. These models have improved the predictive capability of autonomous systems, particularly in scenarios where agents interact dynamically with their surroundings. Planning-aware models have also emerged as a promising avenue for improving trajectory prediction [8], [9]. PiP [9] and PRIME [8] explicitly integrate planning information into the prediction framework, allowing the system to consider future goals during trajectory generation. However, these methods often struggle with real-time multi-agent scenarios due to the computational complexity involved in processing such rich data. Diffusion-based models have recently been proposed to address these issues by reducing the computational burden while maintaining high predictive performance [6].\nIn this work, we present the Trajectory-Informed Planning Diffusion (TIP-D) model, which builds on the strengths of diffusion-based frameworks and integrates planning features directly into the motion forecasting process. The TIP-D model incorporates planning information to enhance both the accuracy and interpretability of trajectory predictions. Our approach is capable of predicting the trajectories of multiple agents simultaneously by leveraging cross-attention mechanisms that dynamically fuse planning features with environmental data. Moreover, the TIP-D model achieves a significant reduction in computational complexity, decreasing it by over 80% compared to existing state-of-the-art approaches, while maintaining high accuracy in complex, multi-agent driving scenarios."}, {"title": "II. RELATED WORK", "content": "In motion forecasting, accurate trajectory predictions hinge on the integration of diverse modalities of information, such as an agent's historical trajectory and map data. To better capture the complexities of motion dynamics, recent studies have expanded to include additional modalities like planning trajectories [9], [8], [14], traffic states [15], and lane heading [16]. These enhancements aim to provide a more comprehensive understanding of dynamic environments, allowing models to predict trajectories with higher accuracy. Models like Wayformer [10], Scene Transformer [12], and Latent-Former [13] exemplify advanced approaches in integrating these diverse information sources. [10] optimize attention mechanisms to enhance computational efficiency, while [12] use a unified architecture to effectively manage multi-agent interactions. Similarly, [13] employ a transformer-based approach, incorporating latent variables to improve prediction precision. These strategies demonstrate the effectiveness of integrating multiple information modalities for more accurate and reliable motion forecasting.\nPlanning-aware trajectory prediction is a crucial aspect of multi-modal prediction methods, where planning features containing dynamic vehicle information are integrated into high-level encoded features. For instance, [9] introduce a two-module system where the planning-coupled module injects future planning into interaction features, and the target fusion module encodes and decodes future interactions among agents. PRIME [8] further refines this approach by using model-based scene context to generate feasibility-guaranteed future trajectories through querying various tensors. TPP [14] focuses on improving planning trajectories by incorporating tree-structured planning results from an ego motion sampler, although it still uses the sampler's output as a direct input rather than interacting with the map. Additionally, models like Multipath++ [6] has demonstrated the effectiveness of these techniques in achieving state-of-the-art performance in motion prediction challenges, particularly by using advanced attention and diffusion-based models that effectively integrate these multi-modal information sources. Our approach further develops this concept, achieving performance almost comparable to Multipath++ with an extremely low number of parameters."}, {"title": "III. METHODOLOGY AND DISCUSSION", "content": "The model uses a multi-modal embedding strategy to transform various data sources, such as historical trajectories, HD maps, and planned trajectories, into a unified high-dimensional space. This ensures that the model captures the complex spatial and temporal relationships required for accurate trajectory prediction."}, {"title": "A. Input Representation", "content": "1) Historical trajectory data consists of spatial coordinates over time for each agent, denoted as $S_i = \\{s_t\\}_{t=-T_h}^{t=0}$, where $T_h$ is the number of history time steps, and $s_t^i$ represents the position of agent $i$ at time $t$. To project these trajectories into a high-dimensional space, we apply a Multi-Layer Perceptron (MLP), resulting in $H_i = MLP(S_i)$. This transformation is performed via a series of linear projections and non-linear activations, defined as $H_i' = f(W_2 f(W_1 s_t^i + b_1) + b_2)$, where $W_1$ and $W_2$ are weight matrices, $b_1$ and $b_2$ are bias terms, and $f(\\cdot)$ is a non-linear activation function, such as ReLU. After processing, the historical trajectory embeddings for all agents are represented as\n$H = \\{H_i'\\}_{i=1}^{N} (S_i, \\hat{S_i})$\nwhere N is the number of agents.\n2) HD map data provides the geometry of the environment and is represented by a set of lane nodes within a specific radius R around the ego agent. The map is defined as $M = \\{m_j\\}_{j=1}^{L}$, where L is the number of lanes and each lane $m_j = \\{m_l\\}_{l=1}^{K}$ contains K nodes. To extract meaningful features from the HD map, we use a 1D Feature Pyramid Network (FPN), projecting the map data into a high-dimensional space as $M' = \\Phi_{map}(M)$, where $\\Phi_{map}$ represents the mapping function. The final HD map embedding is of dimension $R^{L \\times K \\times d}$, and the features from all pyramid levels are concatenated to obtain\n$I = concat(M_1, M_2, ..., M_L) (\\hat{S_i}, S_i)$\n3) Planned trajectories are generated using the Frenet framework, which decomposes the path into longitudinal and lateral components. For each planned trajectory, the trajectory is represented relative to a reference path $(u)$ at longitudinal position $u$ with lateral displacement $d$. The trajectory is given by $P_i(u, d) = r(u) + d n(u)$, where $n(u)$ is the normal vector at position $u$. The set of planned trajectories is denoted as $P = \\{P_i\\}_{i=1}^{U}$, where U is the total number of planning trajectories. The optimization of the trajectory is achieved through the minimization of a cost function $C_a = k_j J_t(d) + k_t t + k_d d_t$, where $J_t(d)$ is the jerk, $t$ represents the travel time, and $d_t$ is the distance to the centerline. Hyperparameters $k_j$, $k_t$, and $k_d$ control the relative importance of jerk, travel time, and distance, respectively. Finally, the planned trajectories are projected into a high-dimensional feature space using FPN, similar to the HD map embedding, with the representation\n$P' = concat(P_1, P_2, ..., P_U) (\\hat{S_i}, S_i)$"}, {"title": "B. Model Architecture and Fusion Mechanism", "content": "The model processes four input data sources: $F_{track}$ (track data), $I_{traj}$ (trajectory proposal data), $I_{plan}$ (planning data), and $I_{map}$ (map data). Initially, each input undergoes self-attention to capture intra-modality dependencies. For each modality X, the self-attention mechanism is computed as follows:\n$Attention(Q_X, K_X, V_X) = softmax(\\frac{Q_X K_X^T}{\\sqrt{d_k}})V_X$\nwhere $Q_X$, $K_X$, and $V_X$ represent the query, key, and value matrices derived from the input X, and $d_k$ is the dimension of the keys. This allows each modality- $I_{track}$, $I_{traj}$, $I_{plan}$, and $I_{map}$ -to independently extract its critical spatiotemporal features.\nOnce the self-attention has refined the internal structure of each data source, cross-attention mechanisms are applied. The first cross-attention mechanism aligns the self-attention-processed track embeddings $\\epsilon_{track}$ with the trajectory proposal embeddings $\\epsilon_{traj}$. This results in a joint representation $\\epsilon_{track-traj}$ that captures both the historical movement and future trajectory predictions, aligning agent dynamics with proposed trajectories:\n$\\epsilon_{track-traj} = softmax(\\frac{\\epsilon_{track} K_{traj}^T}{\\sqrt{d}})V_{traj}$\nNext, the planning data embeddings $\\epsilon_{plan}$ are cross-attended with the map data embeddings $\\epsilon_{map}$, resulting in $\\epsilon_{plan-map}$, which integrates the spatial context with the planned trajectories. These fused representations are further processed through a final cross-attention mechanism, where $\\epsilon_{plan-map}$ is combined with the intermediate representation $\\epsilon_{track-traj}$. This step ensures that both past dynamics, environmental context, and planned trajectories are aligned:\n$\\epsilon_{final} = softmax(\\frac{\\epsilon_{track-traj} K_{plan-map}^T}{\\sqrt{d}})V_{track-traj}$\nThe final fused representation $\\epsilon_{final}$ is passed to the prediction header for future trajectory estimation, effectively leveraging the spatiotemporal relationships across all modalities to generate contextually aware predictions.\nThe overall workflow of the model starts with encoding the historical trajectories using an LSTM to produce $O_{history}$; encoding the planned trajectories using an LSTM to produce $O_{plan}$. Self-attention is then applied to each modality to enhance the feature representations. Subsequently, cross-attention integrates the features from these different modalities. Finally, the integrated features are decoded into future trajectory predictions using an LSTM decoder, and the final linear layer outputs the predicted positions:\n$\\{(X_t, Y_t)\\}_{t=1}^{T}$"}, {"title": "C. Training objective", "content": "We formulate the loss as a sum of multiple task loss and use an auxiliary learning [17] approach to balance them. During the training phase, we also utilized a joint loss to the final output and the output of the prediction header following the TrackFormer.\n$L_{sum} = \\frac{1}{\\alpha_1} LDAA_{reg} + \\frac{1}{\\alpha_2} L_{conf} + \\frac{1}{\\alpha_2} L_{cls} + \\sum_{i=1}^{3}log(\\alpha_i + 1)$\nwhere weights of each task loss are all learnable.\n1) Drivable-area-aware(DAA) regression Loss: Huber loss is a commonly used loss function for constraining the prediction. In our work, we inject planning information to increase the prediction's rationality and utilize the drivable area information to make the predicted results more compliant with traffic rules. We propose a drivable area-aware regression loss, where the loss function remains the definition of Huber loss when the predicted trajectory points are in the drivable area. However, if the predicted points are out of the drivable area, we introduce an additional penalty of $log(1 + D_E)L_{Huber}$, where $D_E$ is the distance between the predicted points and the ground truth.\n$LDAA_{reg} = A_{coeff} * \\frac{1}{N} \\sum_{i=1}^{N}L_{Huber}(S_i, \\hat{S_i})$\n$A_{coeff} = \\begin{cases}1, & \\text{if } pred_k \\in R_{safe} \\\\1 + log(1 + D_E(y, \\hat{y})), & \\text{if } pred_k \\notin R_{safe}\\end{cases}$\nwhere $D_E (S, \\hat{S}) = \\sqrt{(s - \\hat{s})^2}$, $R_{da}$ is the region of drivable area.\n2) Confidence loss: To score the proposed region based on the probability of driving intention, we utilize the Kullback-Leibler Divergence as our loss function. This loss function restricts the contribution of probabilities from the proposals based on the distance between the predicted trajectory and ground truth.\n$L_{conf} = \\sum_{j=1}^{N} D_{KL}(\\chi(s_i) || \\gamma(s_i))$\nwhere $\\chi(s_i)$ is the distribution of predicted proposals, $\\gamma(s_i)$ is the distribution of ground-truth proposals.\n3) Classification loss: We cluster all endpoint coordinates into K categories and label the ground truth and prediction proposals with the nearest cluster centroid category label. We employ cross-entropy as an additional loss function to enhance the capability of predicting the driving intention of the vehicle.\n$L_{cls} = \\sum_{j=1}^{N} CrossEntropy(p(s_i), p'(s_i))$\nwhere $p(s_i)$ is the probability distribution of predicted proposals, $p'(s_i)$ is the ground-truth distribution of ground-truth proposals."}, {"title": "IV. EXPERIMENT", "content": "The Argoverse collection comprises 324,000 scenarios including detailed trajectory sequences, sensor data such as"}, {"title": "E. Ablation Study", "content": "We conducted ablation studies on the Argoverse dataset using minFDE, minADE, Brier-minFDE, and Brier-minADE, with mmdiffusion [19] as the baseline. The Planning-Aware Encoder, integrating PreFusion-D, improved minFDE, minADE, Brier-minFDE, and Brier-minADE by 7.81%, 0.24%, 1.87%, and 0.33%. Extending joint loss improved minFDE by 0.24% and minADE by 1.45%. Adding a regression to all agents led to further gains of 0.82% and 1.47%."}, {"title": "V. CONCLUSIONS", "content": "This paper proposes a planning-aware stacked diffusion network, a new framework in motion forecasting. Planning-aware diffusion predicts future trajectory with the multi-modal feature, especially the prior planning feature. To get a better fusion performance, we design and explore four fusion modules to aggregate the planning information into the stacked diffusion. We also propose a novel loss function to force the network to pay attention to the driveable area. Experiments on the Argoverse motion forecasting benchmark demonstrate the effectiveness of our model."}]}