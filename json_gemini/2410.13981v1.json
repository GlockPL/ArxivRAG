{"title": "ON THE LEARN-TO-OPTIMIZE CAPABILITIES OF TRANSFORMERS IN IN-CONTEXT SPARSE RECOVERY", "authors": ["Renpu Liu", "Ruida Zhou", "Cong Shen", "Jing Yang"], "abstract": "An intriguing property of the Transformer is its ability to perform in-context learning (ICL), where the Transformer can solve different inference tasks without parameter updating based on the contextual information provided by the corresponding input-output demonstration pairs. It has been theoretically proved that ICL is enabled by the capability of Transformers to perform gradient-descent algorithms (Von Oswald et al., 2023a; Bai et al., 2024). This work takes a step further and shows that Transformers can perform learning-to-optimize (L2O) algorithms. Specifically, for the ICL sparse recovery (formulated as LASSO) tasks, we show that a K-layer Transformer can perform an L2O algorithm with a provable convergence rate linear in K. This provides a new perspective explaining the superior ICL capability of Transformers, even with only a few layers, which cannot be achieved by the standard gradient-descent algorithms. Moreover, unlike the conventional L2O algorithms that require the measurement matrix involved in training to match that in testing, the trained Transformer is able to solve sparse recovery problems generated with different measurement matrices. Besides, Transformers as an L2O algorithm can leverage structural information embedded in the training tasks to accelerate its convergence during ICL, and generalize across different lengths of demonstration pairs, where conventional L2O algorithms typically struggle or fail. Such theoretical findings are supported by our experimental results.", "sections": [{"title": "1 INTRODUCTION", "content": "Since its introduction in Vaswani et al. (2017), Transformers have become the backbone in various fields such as natural language processing (Radford, 2018; Devlin, 2018), computer vision (Dosovitskiy, 2020) and reinforcement learning (Chen et al., 2021), significantly influencing subsequent research and applications. A notable capability of Transformers is their good performance for in-context learning (ICL) (Brown et al., 2020), i.e., without further parameter updating, Transformers can perform new inference tasks based on the contextual information embedded in example input-output pairs contained in the prompt. Such ICL capability facilitates state-of-the-art few-shot performances across a multitude of tasks, such as reasoning and language understanding tasks in natural language processing (Chowdhery et al., 2023), in-context dialog generation (Thoppilan et al., 2022) and in-context linear regression (Garg et al., 2022; Fu et al., 2023).\nGiven the significance of transformers' ICL capabilities, extensive research efforts have been directed toward understanding the mechanisms behind their ICL performance. In this context, the ICL capability of a pre-trained Transformer is understood as the Transformer's implicit implementation of learning algorithms during the forward pass. Von Oswald et al. (2023a), Dai et al. (2022), and Bai et al. (2024) suggest that these learning algorithms closely approximate gradient-descent-based optimizers, thus making the Transformer a universal solver for various ICL tasks. Specifically, these works demonstrate that Transformers can approximate gradient descent steps implicitly through"}, {"title": "2 RELATED WORKS", "content": "ICL Mechanism for Transformers. Brown et al. (2020) first show that GPT-3, a Transformer-based LLM, can perform new tasks from input-output pairs without parameter updates, suggesting its ICL ability. This intriguing phenomenon of Transformers has attracted many attentions, leading to various interpretations and hypotheses about its underlying mechanism. For example, Han et al. (2023) empirically hypothesize that Transformers perform kernel regression with internal represen-"}, {"title": "3 PRELIMINARIES", "content": "Notations. For matrix X, we use $[X]_{p:q,r:s}$ to denote the submatrix that contains rows p to q and columns r to s, and we use $[X]_{:,i}$ and $[X]_{j,:}$ to denote the i-th column and j-th row of X respectively. In some places, we also use $[X]_i$ to denote its i-th column for convenience. We use $||X||_F$ to denote its Frobenius norm. For vector x, we use $||x||_1$, $||x||$ and $||x||_\\infty$ to denote its $l_1$, $l_2$ and $l_\\infty$ norms,"}, {"title": "3.1 TRANSFORMER ARCHITECTURE", "content": "In this work, we consider the decoder-based Transformer architecture (Vaswani et al., 2017), where each attention layer is masked by a decoder-based attention mask and followed by a multi-layer perception (MLP) layer.\nDefinition 3.1 (Masked attention layer). Denote an M-head masked attention layer parameterized by $\\{(V_m, Q_m, K_m)_{m\\in[M]}\\}$ as Attn$\\{(V_m,Q_m,K_m)\\}(\u00b7)$, where $V_m, Q_m, K_m \\in \\mathbb{R}^{D\\times D}, \\forall m \\in [M]$.\nThen, given an input sequence $H \\in \\mathbb{R}^{D\\times(N+1)}$, the output sequence of the attention layer is\nAttn$\\{(V_m,Q_m,K_m)\\}(H) = H + \\sum_{m=1}^M (V_mH) \\times \\text{mask}(\\sigma((K_mH)(Q_mH)))$,\nwhere $\\text{mask}(\u00b7)$ is defined as\n$\\text{mask}(M) = \\begin{cases} 1 & i\\leq j \\\\ 1/2 & i=j+1 \\\\ 0 & otherwise \\end{cases}$ for $M\\in \\mathbb{R}^{(N+1)\\times(N+1)}$.\nDefinition 3.2 (MLP layer). Given $W_1 \\in \\mathbb{R}^{D'\\times D}, W_2 \\in \\mathbb{R}^{D\\times D'}$ and a bias vector $b \\in \\mathbb{R}^{D'}$, an MLP layer following the decoder attention layer, denoted as MLP$\\{W_1,W_2,b\\}$, maps each token in the input sequence (i.e, each column $h_i$ in $H \\in \\mathbb{R}^{D\\times N}$ ) to another token as\nMLP$\\{W_1,W_2,b\\} (h_i) = h_i + W_2\\sigma(W_1h_i + b)$,\nwhere $\\sigma$ is the non-linear activation function.\nIn this work, we set the activation function in Definition 3.1 and Definition 3.2 as the element-wise ReLU function. Next, we define the one-layer decoder-based Transformer structure.\nDefinition 3.3 (Transformer layer). A one-layer decoder-based Transformer is parameterized by $\\Theta := \\{W_1,W_2, b, (V_m, Q_m, K_m)_{m\\in[M]}\\}$, denoted as $TF_\\Theta$. Therefore, give input sequence $H \\in \\mathbb{R}^{d\\times N}$, the output sequence is:\n$TF_\\Theta(H) = MLP\\{W_1,W_2,b\\} (Attn\\{(V_m,Q_m,K_m)\\}(H)).$"}, {"title": "3.2 IN-CONTEXT LEARNING BY TRANSFORMERS", "content": "For an in-context learning (ICL) task, a trained Transformer is given an ICL instance $\\mathcal{I} = (\\mathcal{D}, x_{N+1})$, where $\\mathcal{D} = \\{(x_i, y_i)\\}_{i\\in[N]}$ and $x_{N+1}$ is a query. Here, $x_i \\in \\mathbb{R}^d$ is an in-context example, and $y_i$ is the corresponding label for $x_i$. We assume $y_i = f_\\beta(x_i) + \\epsilon_i$, where $\\epsilon_i$ is an added random noise, and $f_\\beta$ is a deterministic function parameterized by $\\beta$. Unlike conventional supervised learning, for each ICL instance, $\\beta \\sim P_\\beta$, i.e., it is randomly sampled from a distribution $P_\\beta$.\nTo perform ICL in a Transformer, we first embed the ICL instance into an input sequence $H \\in \\mathbb{R}^{D\\times N'}$. The Transformer then generates an output sequence $TF(H)$ with the same size as H, based on which a prediction $\\hat{y}_{N+1}$ is generated through a read-out function F, i.e., $\\hat{y}_{N+1} = F(TF(H))$. The objective of ICL is then to ensure that $\\hat{y}_{N+1}$ closely approximates the target value $y_{N+1} = f_\\beta(x_{N+1}) + \\epsilon_{N+1}$ for any ICL instance."}, {"title": "4 TRANSFORMER AS A LISTA-TYPE ALGORITHM FOR IN-CONTEXT SPARSE RECOVERY", "content": "In this section, we demonstrate that a decoder-based Transformer can implement a novel LISTA-type L2O algorithm, specifically LISTA-VM, as detailed in Theorem 4.1, for in-context sparse recovery. We begin by formally defining the in-context sparse recovery problem."}, {"title": "4.1 IN-CONTEXT SPARSE RECOVERY", "content": "Sparse recovery is a fundamental problem in fields such as compressed sensing, signal denoising, and statistical model selection. The core concept of sparse recovery is that a high-dimensional sparse signal can be inferred from very few linear observations if certain conditions are satisfied. Specifically, it aims to identify an S-sparse vector $\\beta^* \\in \\mathbb{R}^d$ from its noisy linear observations $y = X\\beta^* + \\epsilon$, where $X \\in \\mathbb{R}^{N\\times d}$ is a measurement matrix, $\\epsilon \\in \\mathbb{R}^N$ is an isometric Gaussian noise vector with mean vector $0_N$ and covariance matrix $I_{N\\times N}$. Typically, we assume $d \\gg N$, which is the so-called under-determined case. One common assumption for the measurement matrix X (Pitaval et al., 2015; Ge et al., 2017; Zhu et al., 2021) is that each row of the matrix is independently sampled from an isometric sub-Gaussian distribution with zero mean and covariance matrix diag$(\\sigma_1^2, \u00b7\u00b7\u00b7 , \\sigma_d^2)$, denoted as $\\mathcal{P}_X$, which guarantees the critical restricted isometry property under mild conditions on the sparsity level (Candes and Tao, 2007). In this work, we also assume $\\beta^*$ is randomly sampled from a distribution $P_\\beta$, which admits an S-sparse vector with random support.\nA popular approach to tackling sparse recovery is the least absolute shrinkage and selection operator (LASSO), which aims to find the optimal sparse vector $\\beta \\in \\mathbb{R}^d$ that minimizes the following loss:\n$\\mathcal{L}(\\beta) = \\frac{1}{2}||y - X\\beta||_2^2 + \\alpha||\\beta||_1$.\nHere $\\alpha$ is a coefficient controlling the sparsity penalty. We denote the transpose of the i-th row in X by $x_i$, i.e., $x_i = [X^T]_{:,i}$.\nIn this work, we study how Transformers solve the sparse recovery problem in context. Specifically, we assume a Transformer has been pre-trained with randomly sampled sparse recovery instances. Then, during ICL, for a given sparse recovery instance with $\\beta \\sim P_\\beta, \\{x_i\\}_{i=1}^N \\sim P_X, \\{\\epsilon_i\\}_{i=1}^N \\sim P_\\epsilon$, the Transformer aims to predict $y_{N+1}$ with input $(X, y, x_{N+1})$, where $y_{N+1} = x_{N+1}^T\\beta + \\epsilon_{N+1}$."}, {"title": "4.2 CLASSICAL ALGORITHMS", "content": "Gradient descent is known to struggle in solving the LASSO problem due to its inefficiency in effectively handling the sparsity constraint (Chen et al., 2018). This inefficiency has led to the development of more specialized algorithms that can better address the unique challenges posed by the LASSO formulation. A popular approach to solving the LASSO problem is the Iterative"}, {"title": "4.3 TRANSFORMER CAN PROVABLY PERFORM LISTA-TYPE ALGORITHMS", "content": "Noting that LISTA-type algorithms can efficiently solve sparse recovery problems, in this section, we argue that a trained Transformer can implement a LISTA-type algorithm and efficiently solve a sparse recovery problem in context. To distinguish the algorithm implemented by the Transformer with the classical LISTA-type algorithms, we term it as LISTA with Varying Measurements (LISTA-VM). Towards this end, we provide an explicit construction of a K-layer decoder-based Transformer as follows. A K-layer Transformer is the concatenation of K blocks, where each block comprises a self-attention layer followed by an MLP layer. The input to the first self-attention layer, denoted as $H^{(1)}$, is an embedding of the given in-context sparse recovery instance $\\mathcal{I} = (X, y, x_{N+1})$.\nEmbedding. Given an in-context sparse recovery instance $\\mathcal{I} = (X,y,X_{N+1})$ we embed the instance into an input sequence $H^{(1)} \\in \\mathbb{R}^{(2d+2)\\times(2N+1)}$ as follows:\n$H^{(1)}(1) = \\begin{bmatrix} [X^T]_{:,1} & [X^T]_{:,1} & \\dots & [X^T]_{:,N} & [X^T]_{:,N} & X_{N+1} \\\\ y_1 & 0 & \\dots & y_N & 0 & 0 \\\\ \\beta^{(1)}_1 & \\beta^{(1)}_2 & \\dots & \\beta^{(1)}_{2N-1} & \\beta^{(1)}_{2N} & \\beta^{(1)}_{2N+1} \\\\ 1 & 0 & \\dots & 1 & 0 & 1 \\end{bmatrix}$,\nwhere $\\{\\beta^{(1)}_i\\}_{i\\in[2N+1]} \\in \\mathbb{R}^d$ are implicit parameter vectors initialized as $0_d$, and $x_i$ is the i-th column of the transposed measurement matrix, i.e, $[X^T]_{:,i}$.\nSelf-attention layer. The self-attention layer takes as input a sequence of embeddings and outputs a sequence of embeddings of the same length. Let the K-layer decoder-based Transformer feature four attention heads uniquely indexed as +1, -1, +2, and -2. We construct these heads according to the structure specified in Appendix C.1. This construction ensures that the self-attention layer performs the $\\beta$ updating inside the soft-thresholding function in Equation (4.1). Furthermore, with our construction, the learnable matrix $D^{(k)}$ in LISTA-CP becomes context-dependent instead of fixed. In the update rule implemented by the self-attention layer, this matrix becomes $D^{(k)} = \\frac{1}{2N+1} [X]_{1:n,:}(M^{(k)})$, where $M^{(k)} \\in \\mathbb{R}^{d\\times d}$ is fixed.\nMLP layer. For the MLP layer following the k-th self-attention layer, it functions as a feedforward neural network that takes the output of the self-attention layer as its input, and outputs a transformed"}, {"title": "5 PERFORMANCE OF TRANSFORMERS FOR IN-CONTEXT SPARSE RECOVERY", "content": "In this section, we demonstrate the effectiveness of the constructed Transformer in implementing the LISTA-VM algorithm and solving in-context sparse recovery problems. We first show that the LISTA-VM algorithm implemented by the Transformer recovers the underlying sparse vector in context at a convergence rate linear in K. We then demonstrate that the Transformer can accurately predict $y_{N+1}$ at the same time."}, {"title": "5.1 SPARSE VECTOR ESTIMATION", "content": "Theorem 5.1 (Convergence of ICL). Let $\\delta \\in (0, 1)$, $N_0 = \\frac{8(4S - 2)^2 log d + log S - log \\delta}{c}$, $\\alpha_n = -\\frac{log d - log \\delta}{N_c}$, where c is a positive constant and $\\gamma$ is a positive constant satisfies $\\gamma \\leq \\frac{3}{7}$. For a K-layer Transformer model with the structure described in Section 4.3, under Assumption 1, there exists a set of parameters such that for any $n \\in [N_0 : N]$, with probability at least $1 - \\delta$, we have\n$||\\beta_{2n+1}^{(K+1)} - \\beta^*|| \\leq b_{\\beta}e^{-\\alpha_nK}$.\nMain challenge and key ideas of the proof. Similar to the proofs in Chen et al. (2018) and Liu and Chen (2019), the core step in proving convergence is to ensure that $D^{(k)}$ exhibits small coherence with X, i.e., $(D^{(k)})^TX \\approx I_{d\\times d}$. In Chen et al. (2018) and Liu and Chen (2019), such a $D^{(k)}$ is obtained by minimizing the generalized mutual coherence to a fixed X. However, this results in poor generalization across different X's. In our proof, we consider X as a random matrix and leverage its sub-Gaussian properties to prove that if $D^{(k)} = X(M^{(k)})^T$, where $M^{(k)}$ is associated with the covariance of X, then $D^{(k)}$ will have small coherence with X with high probability. We defer the detailed proof of Theorem 5.1 to Appendix D.1.\nRemark 2 (Generalization across measurement matrix X). Theorem 5.1 shows that for any X satisfying Assumption 1, the Transformer can estimate the ground-truth sparse vector $\\beta^*$ in-context at a convergence rate linear in K. This is in stark contrast to traditional LISTA-CP type of algorithms, which only work for fixed X. Such generalization is enabled by the input-dependent matrices $\\{D^{(k)}\\}_k$. Besides, we also note that $\\beta_{2n+1}^{(K+1)}$ only depends on $x_1,...,x_n$. This implies that even if the measurement matrix X is of dimension $n \\times d$ instead of $N \\times d$, when $n \\in [N_0, N]$, the Transformer can still recover $\\beta^*$ accurately. Such results demonstrate the robustness of Transformers to variations in in-context sparse recovery tasks.\nRemark 3 (Effective utilization of the hidden patterns in ICL tasks). We note that the Transformer can be slightly modified to exploit certain hidden structures in the in-context sparse recovery tasks. Specifically, if the support of $\\beta$ lies in a subset $S \\subset [1 : d]$ with $S < |S| < d$, then by slightly modifying the parameters of the Transformer to ensure $[\beta^{(k)}]_i = 0$ for all $i \\notin S$, the ICL performance can be improved by replacing all d involved in Theorem 5.1 by $|S|$. We defer the corresponding result and analysis to Appendix D.2."}, {"title": "5.2 LABEL PREDICTION", "content": "In Section 5.1, we have demonstrated that Transformers can successfully recover the ground-truth sparse vector $\\beta^*$ with linear convergence by implementing a LISTA-type algorithm. In this section, we bridge the gap between this theoretical claim and the explicit objective of in-context sparse recovery, which is to predict $\\hat{y}_{n+1}$ given an in-context instance $(X, y, x_{N+1})$. This gap might seem trivial at first glance, as given $X_{N+1}$ and an accurate estimate of $\\beta$, the label $\\hat{y}_{N+1}$ can be obtained through a simple linear operation. However, we will show that, for a decoder-based Transformer, generating $\\hat{y}_{N+1}$ using the predicted sparse vector $\\beta$, which is implicitly embedded within the forward-pass sequences, critically depends on the structure of the read-out function.\nTheorem 5.2. Under the same setting as in Section 4.3, with probability at least $1 - \\eta\\delta - \\delta'$, we have\n$||\\hat{Y}_n - Y_n || \\leq b_x (1 - \\frac{\\gamma}{3})^K + \\eta (1 - \\frac{\\gamma}{3})^{K-1}$\nwhere $c_4, C_5$ are constants."}, {"title": "6 EXPERIMENTAL RESULTS", "content": "Problem setup. In all experiments, we adhere to the following steps to generate in-context sparse recovery instances. First, we sample a ground truth sparse vector $\\beta^*$ from a d = 20 dimensional standard normal distribution, and we fix the sparsity of $\\beta^*$ to be 3 by randomly setting 17 entries in $\\beta^*$ to zero. Next, we independently sample N = 10 vectors form a d dimensional standard normal distribution and then contract the measurement matrix $X \\in \\mathbb{R}^{10\\times 20}$ (each sampled d dimensional random vector is a row in X). We also sample an additional $X_{N+1}$ from the d-dimensional standard Gaussian distribution. We follow the noiseless setting in Bai et al. (2024) for sparse recovery, i.e., $y = X\\beta^*$.\nBaselines. The baselines for our experiments include traditional iterative algorithms such as ISTA and FISTA (Beck and Teboulle, 2009). We also evaluate three classical LISTA-type L2O algorithms: LISTA, LISTA-CP, ALISTA (Gregor and LeCun, 2010; Chen et al., 2018; Liu and Chen, 2019). For all of these algorithms, we set the number of iterations K = 12. We generate a single fixed measurement matrix X. For each training epoch, we create N = 50,000 instances from 50,000 randomly generated sparse vectors. We also evaluate the LISTA-VM algorithm introduced in Theorem 4.1, where we set the number of iterations K = 12 as well. For each training epoch, we randomly sample 100 measurement matrices, each generating 500 instances from 500 randomly generated sparse vectors, which results in a total of 50,000 instances. For comparison, we also meta-train LISTA and LISTA-CP using the same training method as LISTA-VM. We do not perform meta-training for AL-ISTA, as the training process of ALISTA involves solving a non-convex optimization problem for each different measurement matrix X, which makes meta-training for ALISTA unrealistic. For all baseline algorithms, we minimize the sparse vector prediction loss $\\sum_{j=1}^N ||\\beta_j - \\hat{\\beta_j}||_2^2$ using gradient descent for each epoch. We run all baseline experiments for 340 epochs.\nTransformer structure. We consider two Transformer models, i.e., a small Transformer model (denoted as Small TF) and GPT-2. Small TF has 12 layers, each containing a self-attention layer followed by an MLP layer. Each self-attention layer has 4 attention heads. We set the embedding dimension to D = 42, and the embedding structure according to Equation (4.2). For GPT-2, we employ 12 layers, and set the embedding dimension to 256 and the number of attention heads per layer to 8. In order to train Small TF and GPT-2, we randomly generate 64 instances per epoch and train the algorithms for $10^6$ epochs. The training process minimizes the label prediction loss $\\sum_{j=1}^T (y_{N+1,j} - \\hat{y}_{N+1,j})^2$. We run the experiments for Small TF and GPT-2 on an NVIDIA RTX A5000 GPU with 24G memory. The training time for Small TF is approximately 8 hours, while the training time for GPT-2 is around 12 hours.\nResults. We test the prediction performance of the baseline algorithms and Transformers on a sparse recovery instance $(X, y, x_{N+1})$, and plot the label prediction loss in Figure 1.\nWe first do not impose any support constraint on $\\beta$. We start with the general setting where the testing instance is randomly generated (Varying X). As shown in Figure 1a, GPT-2 outperforms Small TF, followed by LISTA-VM, which outperforms iterative algorithms FISTA and ISTA, while the classical LISTA-type algorithms LISTA, LISTA-CP, and ALISTA perform the worst. Such results highlight the efficiency of LISTA-VM, Small TF and GPT-2 in solving ICL sparse recovery problems, corroborating our theoretical result in Theorem 5.1. Meanwhile, classical LISTA-type"}, {"title": "7 CONCLUSION", "content": "In this work, we demonstrated that Transformers' known ICL capabilities could be understood as performing L2O algorithms. Specifically, we showed that for sparse recovery tasks, Transformers can execute the LISTA-VM algorithm with a provable linear convergence rate. Our results highlight that, unlike existing LISTA-type algorithms, which are limited to solving individual sparse recovery problems with fixed measurement matrices, Transformers can address a general class of sparse recovery problems with varying measurement matrices without requiring parameter updates. Experimentally, we demonstrated that Transformers can leverage prior knowledge from training tasks and generalize effectively across different lengths of demonstration pairs, where traditional L2O methods typically fail. These insights not only deepen our understanding of Transformers' capabilities in ICL but also suggest new potential applications for utilizing Transformers in other optimization tasks."}, {"title": "A ADDITIONAL RELATED WORKS", "content": "General L2O Techniques. L2O leverages machine learning to develop optimization algorithms, aiming to improve existing methods and innovate new ones. As highlighted by Sucker et al. (2024) and Chen et al. (2022), L2O intersects with meta-learning (also known as \u201clearning-to-learn\u201d) and automated machine learning (AutoML).\nUnlike meta-learning, which focuses on enabling models to quickly adapt to new tasks with minimal data by leveraging prior knowledge from diverse tasks (Finn et al., 2017; Hospedales et al., 2021), L2O aims to improve the optimization process itself by developing adaptive algorithms tailored to specific tasks, leading to faster convergence and enhanced performance in model training (Andrychowicz et al., 2016; Li and Malik, 2016). Thus, while meta-learning enhances task adaptability, L2O refines the efficiency of the optimization process. In contrast, AutoML focuses on model selection, optimization algorithm selection, and hyperparameter tuning (Yao et al., 2018); L2O distinguishes itself by its ability to generate new optimization techniques through learned models.\nL2O has demonstrated significant potential across various optimization fields and applications. For instance, Andrychowicz et al. (2016) introduced a method where optimization algorithms are learned using recurrent neural networks trained to optimize specific classes of functions. Li and Malik (2016) proposed learning optimization algorithms through reinforcement learning, utilizing guided policy search to develop optimization strategies. Furthermore, Hruby et al. (2022) applied L2O to address the \u201cminimal problem\u201d, a common challenge in computer vision characterized by the presence of many spurious solutions. They trained a multilayer perceptronmodel to predict initial problem solutions, significantly reducing computation time."}, {"title": "B TABLE OF NOTATIONS", "content": null}, {"title": "C DEFERRED PROOFS IN SECTION 4.3", "content": null}, {"title": "C.1 TRANSFORMER STRUCTURE", "content": "Attention layer. Consider a model consisting of K Transformer layers, where each layer is equipped with four attention heads. These heads are uniquely indexed as +1, -1, +2, and -2 to distinguish their specific roles within the layer.\n$Q^{(k)}_{+1} = \\begin{bmatrix} 0_{(d+1)\\times(d+1)} & 0_{(d+1)\\times d} & 0_{d+1} \\\\ 0_{d\\times(d+1)} & I_{d\\times d} & 0_d \\\\ 0_{1\\times(d+1)} & M_{Q,+1}^{(k)} & 0_1 \\end{bmatrix}, Q^{(k)}_{+2} = \\begin{bmatrix} 0_{(d+1)\\times(2d+1)} & 0_{d+1} \\\\ M_{Q,+2}^{(k)} & 0_{d\\times(d+1)} & 0_d \\\\ 0_{1\\times(2d+1)} & 1 \\end{bmatrix}$\n$K^{(k)}_{+1} = \\begin{bmatrix} 0_{(d+1)\\times(d+1)} & 0_{d+1} & 0_{(d+1)\\times d} \\\\ I_{d\\times d} & 0_d & 0_{d\\times d} \\\\ 0_{1\\times(d+1)} & 1 & 0_{1\\times d} \\end{bmatrix}, K^{(k)}_{+2} = \\begin{bmatrix} 0_{(d+1)\\times(d+1)} & 0_{d+1} & 0_{(d+1)\\times d} \\\\ 0_{d\\times(d+1)} & 0_d & I_{d\\times d} \\\\ 0_{1\\times(d+1)} & 1 & 0_{1\\times d} \\end{bmatrix}$\n$V^{(k)}_{+1} = \\begin{bmatrix} 0_{(d+1)\\times d} & 0_{d+1} & 0_{(d+1)\\times (d+2)} \\\\ M_{V,+1}^{(k)} & 0_d & 0_{d\\times(d+2)} \\\\ 0_{1\\times d} & 1 & 0_{1\\times (d+2)} \\end{bmatrix}, V^{(k)}_{+2} = \\begin{bmatrix} 0_{(d+1)\\times d} & 0_{(d+1)\\times (d+2)} \\\\ M_{V,+2}^{(k)} & 0_{d\\times(d+2)} \\\\ 0_{1\\times d} & 0_{1\\times(d+2)} \\end{bmatrix}$\nwhere $M_{Q,+1}^{(k)}, M_{Q,+2}^{(k)}, M_{V,+1}^{(k)}, M_{V,+2}^{(k)}$ are all $d \\times d$ matrices, and $m_{+2}^{(k)}, m_{-2}^{(k)}$ are scalars."}, {"title": "MLP layer.", "content": "For the MLP layer following the k-th self-attention layer, we set\n$W_1 = \\begin{bmatrix} W_{1,sub} \\\\ -W_{1,sub} \\end{bmatrix}, W_2 = \\begin{bmatrix} I_{(2d+2)\\times(2d+2)} \\\\ -I_{(2d+2)\\times(2d+2)} \\end{bmatrix}, b^{(k)} = \\begin{bmatrix} 0_{5d+5} \\\\ -\\theta^{(k)}.1_d \\\\ 0_{d+2} \\end{bmatrix}$\nwhere the submatrix $W_{1,sub}$ is defined as\n$W_{1,sub} = \\begin{bmatrix} 0_{(d+1)\\times(d+1)} & 0_{(d+1)\\times d} & 0_{d+1} \\\\ 0_{d\\times(d+1)} & I_{d\\times d} & 0_d \\\\ 0_{1\\times(d+1)} & 0_{1\\times d} & 0 \\end{bmatrix}$\nTherefore, we obtain that the output of the MLP layer is\nMLP$(h_i) = \\begin{bmatrix} [h_i]_{1:d+1} \\\\ S_{\\theta^{(k)}}([h_i]_{d+2:2d+1}) \\\\ [h_i]_{2d+2} \\end{bmatrix}$\nwhere $S_{\\theta^{(k)}}$ is the soft-thresholding function parameterized by {\\theta^{(k)}."}, {"title": "C.2 PROOF OF THEOREM 4.1", "content": "We start by stating an equivalent form of Theorem 4.1 below, where we specific $M^{(k)}$ in Theorem 4.1 to be $y^{(k)}M_V$.\nTheorem C.1 (Equivalent form Theorem 4.1). Suppose Assumption 1 holds. For a Transformer with K layers as described in Section 4.3, set the input sequence as:\nH(1) = \\begin{bmatrix} [X^T]_{:,1} & [X^T]_{:,1} & \\dots & [X^T]_{:,N} & [X^T]_{:,N} & X_{N+1} \\\\ y_1 & 0 & \\dots & y_N & 0 & 0 \\\\ \\beta^{(1)}_1 & \\beta^{(1)}_2 & \\dots & \\beta^{(1)}_{2N-1} & \\beta^{(1)}_{2N} & \\beta^{(1)}_{2N+1} \\\\ 1 & 0 & \\dots & 1 & 0 & 1 \\end{bmatrix},\nDenote $H^{(k+1)}$ as the output of the K-th layer of the Transformer and define $\\beta_{2n+1}^{(k+1)} = [H^{(k+1)}]_{d+2:2d+1,2n+1}$. There exists a set of parameters within the Transformer such that for all $k \\in [1, K]$, we have:\n$\\beta_{2n+1}^{(k+1)} = S_{\\theta^{(k)}}( \\beta_{2n+1}^{(k)} - \\gamma^{(k)} (D_n)^T ([X]_{1:n,:} \\beta_{2n+1}^{(k)} - Y_{1:n}))$,\nwhere $D_n = \\frac{1}{2n+1} [X]_{1:n,:}(M_V)$ and $M_V \\in \\mathbb{R}^{d\\times d}$ is embedded in the k-th Transformer lsyer."}, {"title": "D DEFERRED PROOFS IN SECTION 5.1", "content": null}, {"title": "D.1 PROOF OF THEOREM 5.1", "content": "In Section 4.1, we assume that each row of the measurement matrix X is independently sampled from an isometric sub-Gaussian distribution with zero mean and covariance diag"}]}