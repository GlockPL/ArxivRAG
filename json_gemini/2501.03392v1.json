{"title": "Over-the-Air Fair Federated Learning via Multi-Objective Optimization", "authors": ["Shayan Mohajer Hamidi", "Ali Bereyhi", "Saba Asaad", "H. Vincent Poor"], "abstract": "In federated learning (FL), heterogeneity among the local dataset distributions of clients can result in unsatisfactory performance for some, leading to an unfair model. To address this challenge, we propose an over-the-air fair federated learning algorithm (OTA-FFL), which leverages over-the-air computation to train fair FL models. By formulating FL as a multi-objective minimization problem, we introduce a modified Chebyshev approach to compute adaptive weighting coefficients for gradient aggregation in each communication round. To enable efficient aggregation over the multiple access channel, we derive analytical solutions for the optimal transmit scalars at the clients and the de-noising scalar at the parameter server. Extensive experiments demonstrate the superiority of OTA-FFL in achieving fairness and robust performance compared to existing methods.", "sections": [{"title": "I. INTRODUCTION", "content": "MACHINE learning (ML) models have traditionally been trained in a centralized manner, where all training data is collected and stored in a central data center or cloud server. However, in many modern applications, sharing sensitive data with remote servers is often infeasible due to privacy concerns. Federated learning (FL) addresses this issue by enabling devices to collaboratively train a global model using their local datasets, coordinated by a parameter server (PS) [1]. In this framework, only local model updates are transmitted to the PS, ensuring that raw data remains private. Given that these updates are typically transmitted over wireless channels [2], extensive research has focused on integrating FL into wireless networks [3], [4].\nTo achieve efficiency in wireless FL, over-the-air (OTA) computation has been widely adopted for effective uplink model transmission [3]\u2013[6]. By leveraging the waveform superposition property of multiple-access channel (MAC), OTA enables simul- taneous model transmission and aggregation. This approach significantly reduces communication latency and conserves uplink communication bandwidth. A notable challenge in FL, including OTA-FL, stems from the heterogeneity in the distributions of clients' local datasets [7]. This heterogeneity can lead to poor performance when the global model is applied to individual clients' private datasets, resulting in an unfair global model [8]. Specifically, while the average accuracy across clients may be high, clients with data distributions that differ significantly from the majority are more likely to experience degraded model performance. To highlight the importance of designing a fair FL algorithm, consider the development of personalized healthcare models based on data from multiple hospitals. In this scenario, it is essential to ensure that the healthcare model is equitable and does not produce biased treatment recommendations for certain patient groups.\nIn the literature, several studies have sought to train fair models under the assumption that the PS receives noise- free gradients from clients [8]\u2013[10]. While these methods achieve fairness in idealized settings, they fall short in practical scenarios where wireless channel imperfections introduce noise into the received local updates [11]. To address this, [11] proposed a fair FL method that accounts for wireless channel imperfections. However, their approach requires the PS to have direct access to each client's individual gradient vector, making it incompatible with OTA computation techniques.\nTo bridge this gap, this paper introduces an over-the-air fair federated learning algorithm (OTA-FFL). Inspired by [12], [13], OTA-FFL formulates FL as a multi-objective minimization (MoM) problem, aiming to minimize all local objective functions simultaneously. This ensures that the global model resides on the Pareto front of the local loss functions, promoting fairness among clients. Every solution on the Pareto front satisfies a fairness criterion, as no local loss function can be improved without degrading at least one of the others.\nTo solve this MoM problem, each communication round begins with clients transmitting their local loss functions to the PS. The PS then computes an adaptive weighting coefficient for gradient aggregation using a modified version of the Chebyshev approach [14], a well-established technique in MoM. To facil- itate gradient aggregation based on these adaptive weighting coefficients via the MAC channel, we design optimized transmit scalars for the clients and a de-noising receive scalar for the PS. This ensures that when clients simultaneously transmit their gradients, the distortion between the aggregated signal and the desired weighted gradients is minimized.\nIn summary, the key contributions of this letter are as follows: (i) to the best of the authors' knowledge, OTA-FFL is the first fair FL algorithm compatible with OTA computation principles; (ii) within the OTA-FFL framework, we propose a modified version of the Chebyshev approach that enables the PS to compute adaptive weighting coefficients for each client's gradient; (iii) we derive analytical solutions for the optimal transmit scalars at the clients and the denoising scalar at the PS, ensuring minimal distortion in gradient aggregation; (iv) we validate the effectiveness of the proposed OTA-FFL algorithm through extensive performance evaluations.\nNotation: The operators (.) and (\u00b7)H denote the transpose, Hermitian transpose, respectively. Symbol [K] denotes the set of integers {1,2,\u2026\u2026\u2026, K}, {fk}k\u2208[K] = {f1, f2, ..., fk } for a scalar/function f. Let u[d] denote the d-th element of vector u. For two vectors u, v \u2208 RD, we say u < v iff u[d] < v[d] for"}, {"title": "II. PRELIMINARIES", "content": "To incorporate the concept of fairness into the OTA-FL frame- work, we invoke notions from multi-objective optimization. This section provides some preliminaries.\nA. Multi-Objective Optimization\nIn MoM, multiple objective functions are to be minimized simultaneously. These functions can be conflicting or incom- patible. Specifically, let f(\u03b8) = [f1(\u03b8),..., fk (\u03b8)]T be a set of K scalar objective functions defined over the same domain \u0398. The goal in MoM is to find an optimal point \u03b8\u2217, when the minimization is defined with respect to partial ordering. This point is called Pareto-optimal, which we define in the sequel.\nDefinition 1 (Pareto-optimal). The point \u03b8\u2217 is called Pareto- optimal and is represented as\n\u03b8\u2217 = argmin f(\u03b8),\n\u0398\u2208\u0398\n(1)\nif there exists no \u03b8 \u2260 \u03b8\u2217, such that fk(\u03b8) < fk(\u03b8\u2217) for an integer k \u2208 [K], and fj(\u03b8) \u2264 fj(\u03b8\u2217) for all j \u2260 k. In other words, \u03b8\u2217 is Pareto-optimal if it is impossible to decrease any fk without increasing at least one other objective fj for j \u2260 k.\nPareto-optimal solutions form the Pareto front, the set of function values at all such points. A weaker notion, weak Pareto- optimality, is satisfied if no \u03b8 strictly improves all objectives, i.e., f(\u03b8) < f(\u03b8\u2217). In addition, Pareto-optimality relates to stationary points in MoM, termed Pareto-stationary:\nDefinition 2. (Pareto-stationary): A point \u03b8\u2217 is Pareto- stationary if a convex combination of objective gradients at \u03b8\u2217 equals zero, i.e., if weights \u03bb1,..., \u03bb\u03ba \u2265 0 adding to one exist such that \u2211\u03bbk\u2207fk (\u03b8\u2217) = 0.\nLemma 1 ([15]). Any Pareto-optimal solution to (1) is Pareto- stationary. Conversely, if f1,..., fk are convex any Pareto- stationary solution is weakly Pareto-optimal.\nB. Scalarization and Chebyshev Method\nLemma (1) suggests to search for a Pareto-optimal solution within the set of Pareto-stationary points. Several approaches have thus been developed to identify this set. In the sequel, we discuss two popular approaches: namely, the linear scalariza- tion and Chebyshev method.\n\u2022 Scalarization: Scalarization simplifies multi-objective op- timization (MoM) by reformulating it as a single-objective minimization (SoM) problem. Specifically, given \u03bb \u2208 \u2206K for some A\u2286R, the MoM in (1) is transformed into the SoM:\nfSoM (\u03b8) = \u2211\u03bbkfk (\u03b8),\nk=1\n(2)\nwhere \u03bb represents the weights assigned to each objective. It can be shown that for certain choices of \u03bb, the stationary point of fSOM corresponds to a Pareto-stationary solution of (1). In practice, \u03bb is selected based on some prior knowledge.\n\u2022 Chebyshev Method: Scalarization can only converge to Pareto-optimal points that lie on the convex envelop of the Pareto front [16]. However, in many problems the Pareto front is non-convex, and hence points identified via scalarization are not Pareto-optimal. An alternative approach which can also converge to non-convex points on the Pareto front is the Chebyshev method. This method focuses on the maximum deviation rather than forming a linear combination of objectives.\nTo find a point on the Pareto-front, the Chebyshev method solves the following minimax problem.\nmin max (f(\u03b8) \u2212 \u03c2),\n\u0398\u2208\u0398 \u039b\u2208\u2206K\n(3)\nfor some \u2206 \u2286 RK and a fixed \u03c2 \u2208 RK, which ideally serves as a lower bound for f. Note that in this formulation, \u039b is optimized in the inner loop, and hence does not represent a linear combination of the objectives. This non-linearity enables the optimization algorithm to explore solutions that might not be attainable with linear scalarization."}, {"title": "III. PROBLEM FORMULATION", "content": "Consider an FL setting with K clients whose local datasets D1,..., DK are drawn from heterogeneous, i.e., generally different, distributions. The clients collaborate with a PS to train a shared model with parameters \u03b8 \u2208 Rd. To this end, client k determines its local empirical risk as\nfk (\u03b8) = 1/|Dk| \u2211 l(u, v|\u03b8),\n(u,v)\u2208Dk\n(4)\nfor some loss function l(u, v|\u03b8) that measures the difference between the output of the learning model, parameterized with \u03b8, to the input sample u and the ground-truth v. In iteration t, client k starts from the actual global model \u03b8t and trains it locally by performing (multiple iterations of) a gradient-based algorithm on fk (\u03b8) and computing the first-order information gt,k \u2208 Rd. It shares the information with the PS through its communication link. The PS receives an aggregation of the transmitted local parameters and process them into a global information gt \u2208 Rd. It then updates the global model to \u03b8t+1.\nThe communication between the clients and the PS occurs over a wireless link, with aggregation performed directly over the air. The model for the communication links and the OTA computation scheme are specified in Section V, where we present the proposed OTA-FFL scheme. Note that due to the use of OTA computation scheme, the PS has access only to the aggregated local parameters gt,k, not the individual ones.\nA. Notion of Fairness\nDespite using shared parameters, clients may experience varying learning performance due to heterogeneity, leading to unfairness [8]. We quantify fairness using a standard metric commonly adopted in the FL literature [8].\nDefinition 3 (Fairness metric). A model \u03b81 is said to be fairer than \u03b82 if the test performance distribution of \u03b81 across the network is more uniform than that of \u03b82, i.e.,\nstd [f1(\u03b81),..., fk (\u03b81)] < std [f1(\u03b82), ..., fk (\u03b82)],\n(5)\nwhere std [\u00b7] denotes the standard deviation.\nDefinition 3 helps us formulate the objective of fair FL (FFL): the aim is to develop a distributed training algorithm, which converges to the fairest model."}, {"title": "IV. FAIR FL VIA MODIFIED CHEBYSHEV SCHEME", "content": "The classical approaches to Fair FL via MOM\nThe FFL framework can be observed as a multi-objective optimization: we intend to find a shared model \u03b8 that is optimal for all local risks. This problem mathematically reduces to the MOM (1) whose objective fk is the empirical risk in (4), and the optimal FFL design is given by a Pareto-optimal solution. Nevertheless, among all Pareto-optimal models, we look for the one that is fairest in the sense defined in Definition 3.\nClassical FL algorithms estimate the Pareto-optimal model using different MOM techniques. We briefly discuss the two important ones that are related to our proposed scheme.\n\u2022 FedAvg: The conventional FedAvg suggests to use linear scalarization technique: the global model is found by estimating the solution of the SOM in (2) with the weights\n\u03bbavg = |Dk|/\u2211k'\u2208K |Dk'|,\n(6)\nensuring that they are proportional to the sizes of local datasets [1]. As discussed in preliminaries, this approach can only converge to the Pareto-optimal points that lie on the convex envelop of the Pareto-front. Nevertheless, in FL the Pareto- front is non-convex [13], implying that FedAvg cannot reach a Pareto-optimal global model. It thus trains an unfair model where the accuracy differs across clients [8], [13].\n\u2022 Agnostic FL: The agnostic FL (AFL) algorithm, proposed in [17], suggests to find the Pareto-optimal models via the Chebyshev method. More precisely, AFL finds the global model by solving the minmax problem in (3) with local empirical risks as objectives, while setting \u03b6 = 0.\nRemark 1. Other FFL schemes, such as the multiple gradient descent algorithm (MOM) [18], rely on perfect knowledge of individual local parameters gt,k. However, this information is unavailable in our setting due to the communication-efficient aggregation scheme based on OTA computation; see Section V.\nIV. FAIR FL VIA MODIFIED CHEBYSHEV SCHEME\nThe naive application of the Chebyshev method to the FL problem can leas to an FFL scheme that compromises in terms of the average accuracy of the global model. In this section, we propose a modified version of the Chebyshev method that draws a trade-off between Pareto-optimality and average performance. We use this modified method to develop a new FFL scheme.\nA. Modified Chebyshev Scheme\nA closer examination of the minmax problem in (3) reveals that the Chebyshev method promotes fairness by prioritizing the minimization of the loss function for the worst-performing client. This one-sided focus on fairness can compromise the average performance. This is in contrast to FedAvg, whose primary goal is to optimize average performance. It is therefore reasonable to modify (3) to establish a trade-off between fairness and average performance. To this end, we propose constraining the inner optimization of (3) as\nmin max \u039b (f(\u03b8) \u2212 \u03b6) s.t. ||\u039b \u2212 \u039bavg||\u221e \u2264 \u03f5,\n\u0398\u2208\u0398 \u039b\u2208\u2206K\n(7)\nwhere \u039bavg \u2208 RK whose k-th entry is \u03bbkavg defined in (6). It is easily seen that (7) draws a trade-off between the FedAvg and AFL schemes: (i) for \u03f5 = 0, it reduces to FedAvg. (ii) By setting \u03f5 = 1, it recovers the Chebyshev approach, where \u039b is"}, {"title": "A. Modified Chebyshev Scheme", "content": "B. FFL Algorithm based on Modified Chebyshev Scheme\nWe next integrate the modified Chebyshev scheme into the FL framework by developing an iterative scheme that solves the problem in (7) in a distributed fashion. Note that at this stage, we only develop the distributed optimizer and postpone its implementation in the wireless network to Section V.\nWe propose a two-tier optimizer whose inner and outer tiers solve the inner maximization and outer minimization of (7), respectively. These tiers are illustrated in the sequel.\n\u2022 Inner Tier: At iteration t, the PS shares the current global model \u03b8t with all clients. Client k then sends its local risk value fk(\u03b8t) to the PS. Note that this requires exchange of a scalar parameter per FL iteration. This is a negligible overhead that can be integrated in control signaling being performed periodically in the network. Upon receiving the risk values, the PS solves the following maximization problem\n\u039bt = argmax\u039b (\u039bT (f(\u03b8t) \u2212 \u03b6)) s.t. ||\u039b \u2212 \u039bavg||\u221e \u2264 \u03f5.\n\u039b\u2208\u2206K\n(8)\nNote that both the constraint and feasible set are convex in this optimization problem. We can hence deploy projection onto convex sets (POCS) algorithm to efficiently find \u039b.\n\u2022 Outer Tier: Once \u039bt is determined, the outer tier solves the following minimization.\nmink (\u2211\u039bk (fk (\u03b8) \u2212 \u03b6k)),\n\u03b8 k=1\n(9)\nfor \u03b6k given in (7). This minimization describes the same scalarization form that is solved by FedAvg. We hence invoke the standard approach based on distributed (stochastic) gradient descent (DSGD): each client computes its local gradient at \u03b8t, i.e., it sets gt,k = \u2207 (fk(\u03b8t) \u2212 \u03b6k), and sends it to the PS. The PS ideally aggregates these local gradients into their weighted sum whose weights are proportional to \u039b, i.e.,\ngt = \u2211\u039bt,kgt,k\nk=1\n(10)\nThe PS then performs one step of gradient descent to update the global model parameter as \u03b8t+1 = \u03b8t \u2212 \u03b7tgt for some global learning rate \u03b7t > 0.\nThe proposed FFL scheme iterates between the inner and outer tiers until convergence, allowing the model to be updated based on both fairness and average performance considerations. In general, the inner and outer tiers can be updated at different rates. Our numerical however shows that the proposed scheme performs efficiently even in its basic form."}, {"title": "A. Modified Chebyshev Scheme", "content": "Remark 2. Intuitively, the proposed FFL scheme is an \u201cadaptive\u201d form of FedAvg: unlike FedAvg whose weighting \u03bbavg remains constant throughout the training loop, it uses \u039bt in iteration t, scheduled via the modified Chebyshev method."}, {"title": "V. EXTENSION TO OTA-FFL SCHEME", "content": "We extend the proposed scheme to the underlying wireless setting. In this respect, we develop an analog computation and scheduling scheme to aggregate gradients over the air. This is crucial, as it can reduce communication overhead significantly."}, {"title": "A. Communication Model and Over-the-Air Computation", "content": "The clients are connected to the PS through a fading Gaussian MAC. Without loss of generality, we assume that clients and PS are equipped with a single antenna, and that at the beginning of iteration t, a subset of clients, denoted by St \u2286 [K], are scheduled to share their information. Client k computes xt,k \u2208 Rd from gt,k, which satisfies the power constraint E[|xt,k[i]|2] \u2264 Po for i \u2208 [d] and a positive Po. It then transmits the signal over the MAC. Assuming the channel remains constant during one FL iteration, the received signal after d transmissions, denoted by yt \u2208 Cd, is given by\nyt = \u2211ht,kxt,k + nt,\nk\u2208St\n(11)\nwhere ht,k \u2208 C is the channel coefficient between client k and the PS in iteration t, and nt is complex zero-mean white Gaussian noise with variance \u03c32, i.e., nt \u223c CN(0, \u03c32Id).\nTo reduce overhead, we leverage OTA computation, which allows the PS to estimate the global information gt directly from the superimposed received signal yt without any need to allocate separate time or frequency resources to each client. The OTA computation scheme consists of two modules: K encoders with its k-th one at client k constructing xt,k from gt,k, and a decoder at the PS that estimates gt from yt."}, {"title": "B. OTA Encoding-Decoding Scheme", "content": "Local gradient entries can vary drastically in magnitude. The clients hence need to normalize them for pre-equalization and power control; see [4]. To this end, let us focus on iteration t, where in its inner tier the PS computes \u039b. The selected client k \u2208 St normalizes its local gradient gt,k into a symbol vector st,k \u2208 Cd as follows: it first computes the mean and variance of its local gradient, denoted by mt,k and vt,k, respectively and shares it with the PS over the uplink control channels. Note that these statistics are computed independent of loss values fk(\u03b8t), used by the FFL scheme, and hence can be shared through same control link. The PS upon collecting the local statistics {(mt,k, vt,k) : k \u2208 St} computes the global mean and variance by weighted averaging as\nmt = \u2211 \u039bt,kmt,k,\nk\u2208St\nvt = \u2211 \u039bt,kvt,k\nk\u2208St\n(12a)\nThe PS broadcasts the global statistics (mt, vt), and client k normalizes its gradient as st,k = \u221avgt (gt,k \u2212 mt1d). It is straightforward to show that st,k satisfies E[st,k[l]sl\u2032\u2217,k[l]] = 0d for l \u2260 l\u2032 and E[st,k[l]sl\u2217,k[l]] = Id for l = l\u2032.\nThe selected clients use linear scaling to encode their symbol vectors: client k \u2208 St transmits xt,k = bt,kst,k for some bt,k which satisfies the transmit power constraint, i.e.,\nE[|bt,kst,k[i]|2] = |bt,k|2 \u2264 Po,\n(13)\nfor i \u2208 [d], over d channel uses. The received signal vector at the PS after d channel uses is hence given by\nyt = \u2211ht,kbt,kst,k + nt,\nk\u2208St\n(14)\nfor some noise process nt as defined in (11).\nThe received vector in (14) can be seen as a noisy aggre- gation of normalized local gradients. We can hence process it via a decoder to compute an estimator of (10) directly without individually estimating each local gradient. This explains the"}, {"title": "B. OTA Encoding-Decoding Scheme", "content": "idea of OTA computation. Consider the linear encoding by clients, the PS further uses a linear decoding: it applies a de-noising receive scalar ct to the received signal to compute an estimator of the normalized global gradients, and then utilizes the global statistics to estimate gt in (10) when computed over the selected clients. The estimator is given by\ngt = \u221avtct yt + mt1d = \u2211Ct(ht,kbt,kst,k) + Ctnt.\nk\u2208St\n(15)\nWe design the tunable design parameters, i.e., bt,k and ct, such that \u02c6gt computes an unbiased estimator of gt with minimal variance. For an unbiased estimator, \u02c6gt needs to satisfy\nE[\u02c6gt] = gt.\nUnder this condition, the variance is given by\nE[(\u02c6gt, gt)] = E[||\u02c6gt \u2212 gt||2].\n(16)\n(17)\nThe transmit scalars {bt,k} and the de-noising receive scalar Ct are hence designed, such that (16) is satisfied, and (17) is minimized simultaneously. The optimal design under these constraints is given in the following lemma whose proof is differed to the Appendix.\nLemma 2. For an unbiased estimator \u02c6gt, {bt,k} and ct that minimize the estimation variance are given by\nbt,k = Ct = min k\u2208St .\n(18)\nFor these optimal choices, the estimation variance is\nE\u2217[||\u02c6gt \u2212 gt||2] = min \n(19)\nC. Client Scheduling\nThe error in (19) depends on the selected devices, St, creating a trade-off: expanding St increases OTA computation error but reduces the variance of gt. Optimal scheduling of St is NP- hard [19], leading to sub-optimal methods like channel-based scheduling [3] or sparse recovery [19]. We adopt the efficient Gibbs sampling method from [5], omitting details for brevity."}, {"title": "VI. NUMERICAL EXPERIMENTS", "content": "We next validate the OTA-FFL algorithm through experiments benchmarking it against state-of-the-art methods\u00b9.\nA. Settings and Baselines\n\u2022 Benchmarks: We consider (i) OTA-FedAvg which is a straightforward integration of FedAvg into OTA computation, (ii) OTA-TERM, which is an adaptation of the TERM algorithm, whose local losses fk(\u00b7) are replaced with exp{\u03bbfk(\u00b7)} [9], and (iii) OTA-q-FFL which is an adaptation of the q-FFL algorithm that replaces the local losses with q\u221afk(\u00b7) [8].\n\u2022 Datasets and setups: We consider four datasets, with the following experimental configurations: for (i) CIFAR-10, we distribute the data non-iid among K = 10 clients using Dirichlet allocation with \u03b2 = 0.5. As the model, we use ResNet-18 with Group Normalization and perform 100 communication rounds with all clients participating. We further set batch-size is set to 64, consider a single local epoch at clients (e = 1), and \u03b7t = 0.01. For (ii) CINIC-10, use the same approach to distribute data non-iid among K = 50 clients, and train the same model with the same parameters over 200 rounds with all"}, {"title": "B. Numerical Results", "content": "Numerical results validate the effectiveness of our approach.\nAPPENDIX\nStarting from (15), it is readily seen that (16) is satisfied when bt,k = \u039bt,kCt/ht,k for k \u2208 St. Replacing in (17), we conclude that E[(\u02c6gt, gt)] = dvt/c2 using the distribution of nt.\nTo guarantee the satisfaction of the transmit power con-\nstraint in (13), we need to have |bt,k| = \u2264 Po.\nConsequently, |ct|2 \u2264 Po|ht,k|2/(\u039bt,k)2 or equivalently"}, {"title": "B. Numerical Results", "content": "Ct \u2264 min\nk\u2208St .\nThis concludes the proof."}, {"title": "VII. CONCLUSION", "content": "We have proposed a novel FFL algorithm based on the MoM formulation for FL, leveraging the Chebyshev method to minimize variation in local losses across clients. This approach ensures satisfactory performance for all clients. Extending to a wireless setting, we have integrated OTA computation, allowing the PS to aggregate local models directly over the air."}]}