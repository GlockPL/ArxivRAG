{"title": "DESIGN OPTIMIZATION OF NUCLEAR FUSION REACTOR\nTHROUGH DEEP REINFORCEMENT LEARNING", "authors": ["Jinsu Kim", "Jaemin Seo"], "abstract": "This research explores the application of Deep Reinforcement Learning (DRL) to optimize\nthe design of a nuclear fusion reactor. DRL can efficiently address the challenging issues attributed\nto multiple physics and engineering constraints for steady-state operation. The fusion reactor\ndesign computation and the optimization code applicable to parallelization with DRL are developed.\nThe proposed framework enables finding the optimal reactor design that satisfies the operational\nrequirements while reducing building costs. Multi-objective design optimization for a fusion reactor\nis now simplified by DRL, indicating the high potential of the proposed framework for advancing the\nefficient and sustainable design of future reactors.", "sections": [{"title": "Introduction", "content": "Nuclear fusion energy, which can be generated by combining two light atomic nuclei while releasing massive\namounts of energy, has been highlighted as a promising way to address the world's growing energy demands for\nsustainable and clean energy. A magnetic confinement fusion reactor called Tokamak is promising for realizing\nsustainable nuclear fusion energy. However, designing an optimal fusion reactor has remained challenging. It requires\nboth satisfying multiple constraints for steady-state operation [1-3] and reducing the size of a reactor [4]. In more detail,\nthe optimally designed tokamak reactor should prioritize to consider at least four fundamental operational limits [2],\nincluding density limit [5, 6], beta limit [7], the kink safety instability limit [8], and the achievable bootstrap fraction [9].\nIn addition, the reactor design should not exceed the maximum allowable neutron wall load and stress induced by\nelectromagnetic force to TF coils [1, 2, 10]. Finally, the practical aspect should include reducing costs and increasing\nenergy gain [4, 11]. Previous investigation in this field has been conducted [1\u20134, 11\u201318], but the question about how to\noptimize multiple reactor design objectives effectively has still been issued.\nRecently, a data-driven approach, enabling mitigating the high complexity associated with underlying physical\nconstraints, has been developed to cope with this problem. Deep Reinforcement Learning (DRL), combined with a\nneural network and Reinforcement Learning (RL) [19], has demonstrated a remarkable generalization ability to unseen\ndata, applicable to repeating optimization tasks, including design optimization [20]. This can provide the optimal policy\nthat satisfies the maximization of the cumulative reward associated with the system's objective through exploitation and\nexploration. Several studies have shown the validity of this approach in airfoil design optimization [20\u201323] and strain\ndesign optimization [24].\nThis research explores the application of a Deep Reinforcement Learning (DRL) framework to optimize a\ntokamak fusion reactor design. Again, a tokamak fusion reactor requires steady-state operation conditions to avoid\ninstabilities and achieve high power generation by self-ignition. The optimal reactor design should achieve all these\nmultiple design objectives, making it challenging to find the optimal configuration. DRL, however, can search for\noptimal conditions with its simplified process by replacing design objectives with scalarized rewards. Later, we will\nshow that our optimal design satisfies all operational constraints with cost reduction in an efficient way compared to"}, {"title": "Tokamak Fusion Reactor Design Computation", "content": "To design fusion tokamak reactors, it is necessary to consider not only the aspect of plasma physics but also\nengineering and nuclear physics [2, 18]. Previous research has shown that these constraints affect the overall design of a\ntokamak reactor [2], and we developed the computation code for calculating the tokamak reactor design parameters and\nperformance, including plasma pressure, density, and cost. Note that our computation code considers the maximum allowable current density and stress on materials [1,2, 10]\nin addition to neutron radiation wall loading [2, 18] to determine the designs concerning the physically achievable\nconstruction of a reactor.  If the input parameters are given, the code\ncomputes the output parameters, as shown in . The details of the reactor design computation are presented in\nAppendix A."}, {"title": "Design Optimization through Deep Reinforcement Learning", "content": "Design optimization requires exploring a high-dimensional design parameter space, indicating a high computa-\ntional cost for high-fidelity numerical simulation processing [22, 23]. Investigations on optimizing design parameters\nhave been conducted using two main approaches: Gradient and gradient-free methods. Gradient methods utilize the\ngradient of the design objective functions, and gradient computation can be reduced through the adjoint method [29],\nbut easily trapped in local optimal and sensitive to the initial points [22]. On the other hand, gradient-free methods,\nincluding genetic algorithm [30], are efficient in finding global optimal and less sensitive to initial points, yet have\nhigher computational cost than gradient methods [22,31]. Alternative methods based on machine learning for surrogate\nmodels [32] have been highlighted to cover the computational part of the simulations and design performance, but they\ndepend on the dataset's quality [23].\nRecently, several studies have demonstrated the effectiveness of deep reinforcement learning in design optimiza-\ntion fields [20-23]. Reinforcement Learning (RL), a data-driven approach for learning to make decisions through trial\nand error [19], provides a high generalization capability to unobserved system configurations [20]. Deep Reinforcement\nLearning (DRL) [27], combined with Deep Neural Network (DNN) and Reinforcement Learning (RL), has emerged\nand achieved high performance in diverse fields [28]. Since neural networks can approximate arbitrary nonlinear\nfunctions guaranteed by the universal function approximation theorem [33], the integration of deep neural network\nand reinforcement learning enables agents to learn the approximation of an optimal policy efficiently while allowing\nunstructured and high-dimensional input data. Our work utilized Proximal Policy Optimization (PPO) [34], classified\nas a policy-gradient method [26], to optimize the reactor design. PPO has several benefits, including the capability to\nhandle the continuous action space and the stability with robust optimization by incorporating a form of trust region\noptimization [35]."}, {"title": "Reward engineering for a reactor design optimization", "content": "To apply deep reinforcement learning, defining the reward function based on design objectives is necessary.\nTwo types of reward functions are suggested to reflect our design purpose. The minimum requirements for steady-state\noperation can be expressed as the inequalities with the ratio of the plasma parameters to their maximum allowable\nvalues, including Greenwald density $n_G$ and Troyon beta $\u03b2_T$, as well as with the ratio of the parameters to their\nminimum required values, including $q_{kink}$ and $f_{BS}$. All parameters of an optimal-designed reactor should satisfy these\ninequalities.\n\n1. Density limit: $\\tilde{n} < n_G = \\frac{I_p}{\\pi a^2}$\n\n2. Beta limit: $\u03b2 < \u03b2_T = \\frac{B_N I_p}{a B_0}$\n\n3. Kink safety factor limit: $q > q_{kink} = 2$\n\n4. Bootstrap fraction limit: $f_{NC} > f_{BS}$\n\nThen, by defining the reward function as Equation 1, we can induce the agent to find the optimal design that\nsatisfies both avoiding the operational limits and enabling the values in the valid range. In this equation, $x$ and $X_{lim}$\ncorrespond to the plasma parameter and operational safety margin value, respectively. This term can be managed with\nhyperparameter $\\alpha$. $x_l$ and $x_u$ represent each parameter's lower and upper bound for physical validity. $r_f$ is a penalty to\ninduce the agent to acquire physically valid design configurations.\n\n$R(x) = \\tanh \\alpha (\\frac{x}{X_{lim}} - 1) + r_f [\\theta(x - x_u) + \\theta(x_l - x)]$\n\nIn addition, the quantitative metrics, including the energy confinement time and cost parameter, can be expressed\nas the ratio of those from the reference design and the target design. Then, Equation 2 can be a suitable function\nenabling the designed reactor to have higher values than the reference. In the equation below, x and xref describe the\nplasma parameter from the designed reactor and reference, respectively.\n\n$R(x) = \\tanh(\\frac{x}{x_{ref}} - 1)$\n\nEach objective can be represented as one of two types of reward functions, as mentioned above. Finally, the\nweighted sum of each reward, as Equation 3, obtained from the environment, is given to the agent, resulting in the\noptimal reactor design maximizing the total reward.\n\n$R_{total} = w_{RT}R_{T} + w_{cost} R_{cost} + w_{p}R_{\u03b2} + w_{q}R_{q} + w_{ne} R_{ne} + w_{fbs} R_{fbs} + w_{QR}R_{Q}$"}, {"title": "Experimental setup", "content": "For a reactor design optimization, we applied DRL-based optimization as a proposed method and grid search\nalgorithm as a comparison. The proposed method was based on Proximal Policy Optimisation (PPO) [34] with a\nmaximum local time step (memory buffer size) equal to 4 and 100,000 episodes. The optimization process was\nconducted over 100,000 episodes, at which saturation was observed. The actor-critic network has three layers of\n64 hidden dimensions, while the deviation for sampling policy distribution is 0.25. The loss function for PPO in\nthis research used the weighted sum of policy surrogate loss combined with CLIP loss and entropy bonus term and\nSmoothL1Loss for the value function [34]. In more detail, the clipping coefficient for CLIP loss $e$ is 0.2, and the entropy\ncoefficient is 0.05. The learning rate for training the network is 0.001, while a discount factor $\u03b3$ is 0.999. The optimizer\nused in this research is RMSProps."}, {"title": "Results", "content": "In this section, the results of the optimization process are represented. The optimal designs refer to the cases\nwhen the best reward was acquired while achieving the steady-state operation conditions during the optimization. We\nconducted DRL-based optimization to find the optimal cases and compared the performance with the grid search case\nto show the efficiency of the proposed one.\nNext, we compared two different algorithms to show the efficiency of DRL in a reactor design optimization.\nThe grid search algorithm has been mainly utilized for parametric search in finding compact tokamak reactor design [3].\nHowever, this algorithm randomly selects the input parameters, thus requiring a large number of processes. However,\nthe proposed algorithm can find the optimal designs satisfying operational constraints while reducing the cost parameter\nwith fewer episodes, as seen in . This only requires half of the episodes to achieve the highest reward and\nsatisfy the constraints, highlighting the competence of finding the optimal reactor design parameters."}, {"title": "Conclusion", "content": "In this research, an innovative approach was proposed for tokamak reactor design optimization based on Deep\nReinforcement Learning (DRL), aiming to satisfy the minimum requirements for steady-state operation and cost-\nefficient design. Since designing a tokamak device requires the consideration of plasma physics and nuclear engineering,\nan efficient multi-objective optimization technique is necessary to find the optimal reactor design. We demonstrated\nthat DRL can successfully search for cost-reduced designs that satisfy multiple design objectives, including operational\nrequirements, while the reference failed. Our framework has shown a relatively low computational cost for finding the\ncost-efficient optimal designs with only 100000 trials compared to the conventional method. Moreover, this can simplify\nthe multiple objectives by replacing those with rewards, enabling the reduction of computation for optimization. Since\nbuilding a tokamak device requires optimizing multiple design objectives, our proposed method has a substantial benefit\nbecause it is efficient and straightforward to apply in multiple design objective optimization for a tokamak reactor.\nFuture research requires optimizing profile shape and materials for walls. DRL can also handle discontinuous\nvariables and high-dimensional input data, indicating the ability to optimize 1D parameters and material composition.\nThis will provide more practical and accurate insight for conceptualizing the optimal tokamak reactor design."}, {"title": "Appendix A Implementation: Fusion Reactor Design Computation Code", "content": "In this section, we describe the details of the computation process for the design and plasma parameters of a\ntokamak reactor. Our code is based on [2], aiming to determine the parameters subject to the engineering and nuclear\nphysics constraints [3, 18]. The computational process consists of two steps: (1) the geometrical design computation\nand (2) the estimation of plasma parameters. These parameters can then calculate the reactor design performance related\nto operational constraints.\nStarting from the core design, the input variables from Table 1 are required. First, the major radius of the core\nplasma can be determined through the neutron wall loading constraint. In more detail, the neutron wall loading at\nthe first wall should not exceed the maximum allowable neutron wall load Pw. From electric power $P_E$ and thermal\nefficiency $\u03b7_\u03c4$, the approximate neutron power at the wall can be estimated; thus, the major radius can be computed\nas Equation 4. Equation 4 assumes that maximum neutron wall load Pw ~ 4MW/m\u00b2 [2] with $E_n$ = 14.1MeV and\n$E_F$ = 22.4MeV. In addition, the geometrical variables are also utilized, including elongation $\u03ba$ and aspect ratio A, to\ncompute the surface area and the minor radius from A = $R/a$.\n\n$R=\\sqrt{\\frac{1}{4\u03c0^2} \\frac{E_n}{E_F} \\frac{P_E}{\\eta_\u03c4 P_W (1+\u03ba^2)}} A^{0.5}$\n\nThe computation of blanket thickness is based on a simple planar structure with the governing equations for\nneutron slowing down and lithium breeding. Slowing down is explained as the collision of neutrons with Li-7 at the\nblanket, while the breeding process is for the reaction between neutrons and Li-6. Thus, the energy balance equation for\nthe collisional process and the mass balance equation from the nuclear reaction can be described as Equation 2 from [2],\nenabling to derive the estimation of the blanket thickness as Equation 5. From the hard-sphere collision model, the\nscattering cross-section is estimated that $\u03c3_s$ ~ 2barns, thus $\u03bb_s$ = 0.1m [38]. In addition, $a_\u03b2 = \\frac{a_B}{E_T}( \\frac{\u03c3_s}{\u03bb_s} )^{0.5}$ while\n$E_T$ = 0.025eV [38]. $\\frac{\\Gamma_0}{\\Gamma_b}$ is a flux ratio between the inward and outward of the blanket, assumed to be 10-5 [2].\n\n$\u2206_{Blanket} = \u03bb_s (1 + a_\u03b2 \\ln \\frac{\\Gamma_0}{\\Gamma_b})$\n\nThen, it is now available to compute the TF coil thickness, which is supposed to be a flat circular magnet. The\nTF coil should maintain its strength against the material stress from the centering and tensile force due to the magnetic\nfield. After the simple calculation for deriving each component of the stresses [2], the TF coil thickness is estimated as\nEquation 6.\n\n$\u2206_{TFcoil} = R_o [2(1 \u2013 \u03b5_B) - \\sqrt{(1 \u2013 \u03b5_B)^2 \u2013 \u03b1_M} - \\sqrt{(1 \u2013 \u03b5_B)^2 \u2013 \u03b1_J}]^{0.5}$\n\nwhere $\u03b1_M$ and $\u03b1_j$ are represented as below. $\u03c3_{max}$ = 600MPa is the maximum allowable material stress applied to the\ncoil, and $J_{max}$ = 20MA/m\u00b2 is the maximum allowable overall current density for the superconducting coils [1,2, 10].\n\n$\u03b1_M = \\frac{2\u03b5_B B_0^2}{\u03bc_0 \u03c3_{max}} \\frac{1}{1 + \u03b5_B}$\n$\u03b1_J = (\\frac{2B_0}{\u03bc_0 R_o J_{max}})^2 [ \\frac{1}{2} + \\frac{\u03b5_B}{1 - \u03b5_B} \\ln(\\frac{1 + \u03b5_B}{1 - \u03b5_B}) ]$\nAfter the geometric design computation, it is now available to estimate plasma parameters required to calculate\nthe design performance. In this step, the average plasma temperature T and the enhancement factor H are input\nvariables to determine the rest of the parameters. Following the given profiles and microscopic cross-section \u3008\u03c3\u03bd\u3009,\nmentioned in Equations 20 and 21 from [2], we can compute average plasma pressure, density, and plasma beta, as\ngiven in Equation 8, 9, and 10.\n\n$p=\\frac{4}{3} \\frac{(1+\u03bd_T)}{(1+\u03bd_n)} \\frac{1}{\\sqrt{2}} \\frac{P_E}{\\eta_\u03c4 E_F} \\frac{T^2}{R_0 a^2 \u03ba} [ \\frac{1}{(1 \u2013 \u03c1^2) \\int_0^1 \u03bd_\u03b7 \u27e8\u03c3\u03bd\u27e9 \u03c1 d\u03c1}]^{0.5}$"}, {"title": "Appendix B Implementation: PPO-Based Reactor Design Optimization", "content": "Our reactor design optimization code is based on Proximal Policy Optimization (PPO) [34] with Actor-Critic\nstyle, as shown in Algorithm 1. PPO, an improved version of Trust Region Policy Optimization (TRPO) [35], utilizes a\nclipped surrogate objective. While TRPO maximizes a surrogate objective with a penalty on KL divergence related to\npolicy change [35], PPO changes the objective by clipping the policy ratio [34], as seen in Equation 19.\n\n$L^{CLIP} (\u03b8) = E_t [min(r_t (\u03b8) A_t, clip(r_t(\u03b8), 1 \u2013 \u03b5, 1 + \u03b5) \u00c2_t)]$\n\nIn this equation, $\u00c2_t$ = -V(s_t) + r_t + y r_{t+1} + \u00b7\u00b7\u00b7 + y^{T-t+1}r_{T-1} + y^{T-t}V(s_T) refers to an estimator of the\nadvantage function at t, and e is a hyperparameter. Since the Actor-Critic network shares the parameters of a neural\nnetwork with the policy and value estimator, the objective should also combine the policy surrogate objective and a value\nfunction error. Thus, the combined loss function is described in Equation 20. In this equation, $L^{VF}(\u03b8) = (V_\u03b8 (s_t) \u2013 V_t)^2$\nis value estimation error term and $S[\u03c0_\u03b8]$ refers to an entropy bonus $S[\u03c0_\u03b8] = E_{s~\u03c0_\u03b8}[-log \u03c0_\u03b8(a_t|s_t)]$, enhancing the\nexploration of the policy [42]. Lastly, $c_1$ and $c_2$ are hyperparameters.\n\n$L^{PPO} = E_t [L^{CLIP}(\u03b8) \u2013 c_1L^{VF}(\u03c6) + c_2S[\u03c0_\u03b8] (s_t)]$\n\nThis objective is maximized during the training process, as described in Algorithm 1. For each iteration, the\nactor collects the T timesteps of samples by interacting with the environment. Then, the surrogate loss mentioned above\nis computed and optimized with the collected samples."}]}