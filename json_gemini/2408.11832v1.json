{"title": "OpenFactCheck: A Unified Framework for Factuality Evaluation of LLMs", "authors": ["Hasan Iqbal", "Yuxia Wang", "Minghan Wang", "Georgi Georgiev", "Jiahui Geng", "Iryna Gurevych", "Preslav Nakov"], "abstract": "The increased use of large language models (LLMs) across a variety of real-world applica-tions calls for automatic tools to check the fac-tual accuracy of their outputs, as LLMs often hallucinate. This is difficult as it requires as-sessing the factuality of free-form open-domain responses. While there has been a lot of re-search on this topic, different papers use dif-ferent evaluation benchmarks and measures, which makes them hard to compare and ham-pers future progress. To mitigate these is-sues, we developed OpenFactCheck, a uni-fied framework, with three modules: (i) RE-SPONSEEVAL, which allows users to easily cus-tomize an automatic fact-checking system and to assess the factuality of all claims in an input document using that system, (ii) LLMEVAL, which assesses the overall factuality of an LLM, and (iii) CHECKEREVAL, a module to evaluate automatic fact-checking systems. OpenFactCheck is open-sourced and publicly released as a Python library and also as a web service. A video describing the system is avail-able at https://youtu.be/-i9VKL0HleI.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have demonstrated impressive capabilities in generating naturally-sounding answers over a broad range of human inquiries. However, GPT-40 (OpenAI, 2023) and other text generation models still produce content that deviates from real-world facts (Bang et al., 2023; Borji, 2023; Guiven, 2023). This degrades the performance of LLMs and undermines their reliability, which is a significant bottleneck for de-ployment (Chuang et al., 2023; Geng et al., 2023).\nMany studies have explored evaluating the factu-ality of LLMs (Lee et al., 2022; Chuang et al., 2023; Shi et al., 2023; Chen et al., 2023). Two challenges have been identified: (i) it is difficult to assess the factuality of open-domain free-form responses, and (ii) different papers use different evaluation datasets and measures, which makes it hard to com-pare them, thus hampering future progress (Wang et al., 2024c). To mitigate these issues, we intro-duce OpenFactCheck, an Open-source Factuality Evaluation Framework for LLMs.\nOpenFactCheck comprises the following three core modules as shown in Figure 1:\n\u2022 RESPONSEEVAL: It allows users to customize an automatic fact-checker and to verify free-form documents to alleviate the first problem.\n\u2022 LLMEVAL: A unified LLM factuality evalu-ation module which applies seven factuality-specific benchmarks to assess the LLM fac-tuality ability from different aspects and then produces a report to illustrate the weakness and strength, tackling the second challenge.\n\u2022 CHECKEREVAL: It assesses the verification accuracy of fact-checkers, equipped with a leaderboard in terms of accuracy, latency, and costs, aiming to encourage the development of advanced automatic fact-checking systems.\nThe modules are designed for seamless integra-tion, each contributing to and enhancing the capa-bilities of the others. The results of human veri-fication derived from LLMEVAL can be used as the benchmark for evaluating the accuracy of au-tomated fact-checkers. Simultaneously, the most effective checker identified in CHECKEREVAL can be deployed for automated fact-checking tasks. Each fact-checker in CHECKEREVAL can be an im-plementation in RESPONSEEVAL. Complex user inquiries may be considered as potential candi-dates of the factuality assessment dataset utilized in LLMEVAL.\nUsers can tailor their checkers according to their specific needs, such as domain specialization, cost-effectiveness, or rapid processing, and identify fac-tual errors for both human-written text (a claim or document) and the outputs of LLMs. LLM re-searchers and practitioners can directly submit their LLM responses to the LLMEVAL by downloading our question set. Subsequently, we conduct evalu-ations to assess the model's factual accuracy and to generate a report analyzing the model perfor-mance from multiple aspects. Similarly, developers who seek to evaluate and to fairly compare the effi-cacy of their fact-checking systems to other ones can upload their checker's verification outcomes to CHECKEREVAL. Then, our system will show the ranking information in the leaderboard after evaluating under the same measurements.\nTo sum, three modules of OpenFactCheck re-spectively address:\n\u2022 how to effectively identify factual errors in a text input;\n\u2022 how to systematically evaluate the factuality ability of an LLM;\n\u2022 which automatic fact-checker is the best, and which component dominates the final verifica-tion accuracy.\nWe have launched an open-source initiative that includes the development of a Python library and a web interface tailored to support three major func-tionalities. This foundation is expected to act as a catalyst for future advancements in this domain.\nWe encourage extensive implementation of unique, effective, and robust claim processors, re-trievers and verifiers within fact-checking pipelines, collections of challenging questions that LLMs tend to make factual errors, and human-annotated fine-grained verification examples. We believe that this will help to promote and to advance future research on LLM factuality."}, {"title": "2 System Architecture", "content": "The design of OpenFactCheck emphasizes two principles: (i) customizability and extensibility for both users and developers, and (ii) compatibility with existing methods and datasets. It consists of three modules: RESPONSEEVAL, LLMEVAL, and CHECKEREVAL. We detail the design and imple-mentation of each components below."}, {"title": "2.1 RESPONSEEVAL", "content": "RESPONSEEVAL allows users to customize a fact-checking system by selecting a claim processor, a retriever, and a verifier in web pages. Current version supports the following fact-checking sys-tems: RARR, FacTool and Factcheck-GPT (Gao et al., 2022; Chern et al., 2023; Wang et al., 2023).\nConfigurable Architecture We consolidate vari-ous fact-checking systems into a three-step process, encapsulated by three classes: claim_processor, retriever, and verifier (Wang et al., 2024c). These classes are instantiated and sequentially con-nected to form a pipeline that addresses the fol-lowing tasks: (i) breaking down a document into individual claims, (ii) gathering pertinent evidence for each claim, and (iii) evaluating the veracity of each claim based on the evidence provided. This sequence of tasks is referred to as solvers (see the pseudo code in Figure 4).\nThe implementation of a task solver can be flex-ible, just ensuring that the input and the output are aligned with the abstract class definitions. For example, evidence can be retrieved by calling Ser-PAPI or by searching Wikipedia using BM25, but we must return a list of relevant passages given an input claim. Moreover, task solvers in our pipeline are not hard-coded, but can be configured through a YAML configuration file. Thus, users can com-bine task-solver implementations from different systems (e.g., using Factcheck-GPT's claim pro-cessor, RARR's retriever, and FacTool's verifier) and start the verification from any step. For exam-ple, users can start from the step of retrieval when the input does not need decomposition.\nThis functionality is achieved by a message-passing mechanism, where a success_flag is used to indicate whether the current task solver successfully executes and returns the expected out-put. The success flag passes through the pipeline as the configured order of solvers, guaranteeing that the output of the preceding solver fits the input for the current solver, otherwise error warning will be issued. Practically, the input and the output param-eter names for the task solvers are defined in the configuration file. To link different solvers into a pipeline, one only needs to ensure that the current solver output name matches the input name of the succeeding solver. A FactcheckerState class en-sures storage of all information in the verification.\nExtendable Architecture Inspired by Fairseq, our framework is designed to be highly extendable by treating any third-party task solvers as plug-ins (Ott et al., 2019). As long as the developed task solvers adhere to our class interface definitions, they can be imported and used in our framework."}, {"title": "2.2 LLMEVAL", "content": "We observed that studies assessing language mod-els' factuality or evaluating whether the methods are effective to mitigate model hallucinations use different datasets and metrics. This makes it dif-ficult to compare, in the same conditions, the fac-tuality of different models as well as to compare the effectiveness of different factuality enhance-ment approaches. Moreover, a lot of prior work applied datasets such as MMLU (Hendrycks et al., 2021), StrategyQA (Geva et al., 2021) and Hot-potQA (Yang and et al., 2018) to evaluate model's factuality. These datasets tend to focus on assess-ing the general performance, rather than factuality. To this end, we first collect a dataset FactQA by gathering factual questions of existing datasets that are curated to probe diverse factual errors and span across a spectrum of domains, to fairly evaluate LLMs' factuality under the same criteria\nFactual Question Collection We collected fac-tual questions from seven commonly-used cor-pora that is collected deliberately to assess LLM's factuality, including Snowball (Zhang et al., 2023a), SelfAware (Yin et al., 2023), FreshQA (Vu et al., 2023), FacTool (Chern et al., 2023), FELM-WK (Chen et al., 2023), Factcheck-GPT (Wang et al., 2023) and FactScore-Bio, a total of 6,480 examples shown in Table 1, referring to FactQA (see dataset details in Appendix B.2).\nTo concretely analyze models' vulnerability, we identify three labels for each question from the perspective of the knowledge domain, the topic, and the potential error type if a LLM generates a factually incorrect response. So each example includes the following fields: question, domain, topic, ability to test, task and source. Domains in-volve general, legal, biomedical, clinical, scientific and so on. Given a domain, we further fine-grained topics. Three common error types are presented.\nType1: Knowledge error is the most common error, occurring when the model produces hallu-cinated or inaccurate information due to lacking relevant knowledge or internalizing false knowl-edge in the pre-training stage or in the problematic alignment process.\nType2: Over-commitment error occurs when the model fails to recognize the falsehoods (or jokes) inherent in the prompt or previously-generated con-"}, {"title": "2.3 CHECKEREVAL", "content": "Automatic fact-checking systems aim to identify whether a claim or a document is true or false, but the results are not necessarily correct. To assess the accuracy of automatic fact-checkers, we gather four LLM factuality benchmarks with human-annotated factual labels for three levels of granularity text: claims/segments/documents given (question, ChatGPT response) pairs, includ-ing FacTool-QA, FELM-WK, Factcheck-Bench and HaluEval as shown in Table 2. We refer to them as FactBench. We use precision, recall, and F1-score with respect to the True or False claim/document to evaluate the effectiveness of fact-checking systems."}, {"title": "3 Access and Deployment", "content": "OpenFactCheck is accessible via a user-friendly web interface and features an integrated database that maintains a user leaderboard. It is also avail-able as a standalone open-source Python library."}, {"title": "3.1 Python Library", "content": "OpenFactCheck is available as an open-source Python library on PyPI, designed for flexibility and ease of integration into existing projects. This li-brary equips developers with essential components for fact-checking in any Python environment, mak-ing it an optimal choice for enhancing applications with fact-checking features. The library employs a fluent interface to ensure its usage is intuitive for both beginners and experts alike.\nUsers can install the library by simply using the pip package manager:\n$ pip install openfactcheck\nThe library includes detailed documentation to assist developers in customizing and extending the functionality to meet their specific needs and it is continually updated to ensure compatibility with the latest research and data security standards.\nUsage Examples The first step is to im-port the necessary library components and ini-tialize OpenFactCheckConfig configuration and OpenFactCheck class, which requires no input val-ues for default usage, as shown below:\nfrom openfactcheck.core.base import\nOpenFactCheck, OpenFactCheckConfig\nconfig = OpenFactCheckConfig()\nofc = OpenFactCheck(config)\nUpon importing the library, users are required to secure API keys from platforms utilized by Open-FactCheck's default solvers for evidence retrieval and claim verification. These keys are available from OpenAI, SerpAPI, and ScraperAPI. After acquiring the keys, they need to be configured as environment variables to enable their use within the library.\nThe three key functionalities outlined in Sec-tion 2 are implemented as shown in Figure 2. We can see that the design of the library is intuitive and straightforward, enabling users to apply it without"}, {"title": "3.2 Web Interface", "content": "The web interface of OpenFactCheck provides a user-friendly platform that allows general users to interactively engage with the fact-checking func-tionalities. It is designed to accommodate both novice and expert users, facilitating easy access to the comprehensive evaluations involved in the as-sessment of LLM factuality. The web interfaces are organized into four distinct sections as illustrated in Figure 3 (a).\nIn RESPONSEEVAL page as shown in Figure 3 (b), users can click the dropdown list to select from a range of pre-implemented claim processor, retriever, and verifier. Then, users can input text either written by human or generated by ma-chine into the text box and click Check Factuality to obtain the verification results. As the example demonstrated in the Figure, it includes two claims. The system collected 16 pieces of evidence, and one claims is supported and one claim is refuted, resulting the overall credibility of 50% and judge-ment \"False\" for this whole input.\nFor both the LLMEVAL and RESPONSEE-VAL pages exhibited in Figure 3 (d), users first download either the question set FactQA or the claims/documents in FactBench. After being ready to upload the responses of the LLM that users aim to assess or the verification results of the fact-checkers to test, users type their details including name, email address and so on, and provide the option to opt in or out of leaderboard inclusion (see Figure 3 (d)). If users agree, their informa-tion and rank will be displayed on the leaderboard, otherwise invisible for others.\nIt may takes some time for LLMEVAL to gen-erate teh evaluation report, depending on the sys-tem's current load. Once the report is ready, it is emailed directly to the user, eliminating the need to wait within the application. LLM factuality eval-uation report presents LLM factuality from vari-ous aspects, and specifically includes accuracy and confusion matrix of short answers, pie chart indi-cating accuracy over fresh questions and bar chart showing the percentage of true, false, controversial claims for free-form responses (see Figure 3 (e)).\nSimilarly, CHECKEREVAL results present the number of evaluated examples, the overall ac-curacy, total time and USD cost, fine-grained precision, recall and F1-score for false and true classes, and a confusion matrix showing the mis-identification of this fact-checker. The submission in Figure 3 (f) reveals that this checker performs equally poor over both false and true claims in ver-ification. This evaluation is instant."}, {"title": "4 Conclusion and Future Work", "content": "We implemented a unified, easy-to-use and exten-sible framework OpenFactCheck. It is accessible by both Python libaray and web service, support-ing the customization and evaluation of automatic fact-checking systems and LLM factuality evalua-tion. Specifically, OpenFactCheck allows general users to check whether a claim and a document are factual or not by clicking Check, and also fa-cilitate LLM practitioners and developers to ef-fectively and efficiently evaluate the factuality of their LLMs from various perspectives, and to assess the accuracy of automatic fact-checking systems.\nIn the future, we will continue to integrate new techniques, features, and evaluation benchmarks to OpenFactCheck to facilitate the research progress of LLM fact-checking."}, {"title": "Limitations", "content": "While OpenFactCheck presents a comprehensive framework for factuality evaluation of LLMs, sev-eral limitations must be acknowledged:\nEvaluation Datasets The effectiveness of OpenFactCheck is dependent on the quality and diversity of the datasets used for evaluation. While we have integrated multiple datasets to cover a broad spectrum of domains and potential factual errors, the evaluation is still limited by the inherent biases and coverage gaps in these datasets. For instance, some specialized domains may not be adequately represented, potentially affecting the robustness of the evaluation for LLMs in those areas.\nLatency and Costs The performance of au-tomatic fact-checking systems integrated within OpenFactCheck can vary significantly in terms of latency and operational costs. High accuracy often comes at the expense of increased computational resources and processing time, which may not be feasible for all users, particularly those with limited budgets or time constraints.\nReliance on External Knowledge Sources The fact-checking modules depend heavily on external knowledge sources, such as Wikipedia and web search engines. The availability and reliability of these sources can affect the accuracy and complete-ness of the fact-checking process. Furthermore, the dynamic nature of web content means that the in-formation retrieved may not always be up-to-date."}, {"title": "Ethical Statement", "content": "of\nThe development and deployment OpenFactCheck are guided by a commitment to ethical principles, ensuring that the framework is used responsibly and for the benefit of society:\nTransparency and Accountability We strive to maintain transparency in the design, implemen-tation, and evaluation of OpenFactCheck. The source code and datasets are publicly available, enabling scrutiny and fostering trust within the re-search community. We encourage users to report any issues or biases they encounter, facilitating con-tinuous improvement.\nBias Mitigation Recognizing that biases can ex-ist in both datasets and LLMs, we are dedicated to minimizing such biases in OpenFactCheck. By integrating diverse evaluation benchmarks and en-couraging the development of fair fact-checking approaches, we aim to reduce the impact of biases on factuality evaluation outcomes.\nSocial Impact By enhancing the factual accuracy of LLMs, OpenFactCheck aims to contribute pos-itively to society. Accurate information is crucial for informed decision-making and public discourse. We believe that improving the reliability of LLM outputs can help combat misinformation and sup-port the dissemination of truthful information."}, {"title": "A Related Work", "content": "While numerous automatic fact-checking systems have developed, such as RARR, FactScore, Fac-Tool, Factcheck-GPT and Longform SAFE (Gao et al., 2022; Min et al., 2023; Chern et al., 2023; Wang et al., 2023; Wei et al., 2024), they are often inaccessible to general users who lack a Python environment to compile code and run verification. Although these systems can function as the back-end of a service, a user-friendly web interface is necessary to allow general users to verify text in-puts by simply typing or copying text and clicking a check button. OpenFactCheck addresses this by providing an accessible web interface.\nIn addition, various fact-checking systems have distinct advantages. For instance, Factcheck-GPT offers a fine-grained framework to involve all pos-sible subtask that could improve the fact-checking system, FacTool employs a low-latency evidence retriever through asynchronous processing, and FactScore introduces a scoring metric that calcu-lates the percentage of true claims in a given text, thereby quantitatively assessing the credibility of the input. OpenFactCheck integrates these advan-tages into a unified system (Wang et al., 2024c).\nRecent open-sourced demo system Loki (Wang et al., 2024a) also aims to leverage strength of vari-ous automatic fact-checkers, while it emphasizes optimization a single fact-checking system in terms of accuracy, latency, robustness, cost-efficiency, and extensive support for multiple languages and LLMs. In contrast, OpenFactCheck is a unified framework to cover three major functionalities for factuality evalaution of LLMs, including customiz-ing a fact-checker by combining modules of differ-ent checkers, assessing LLM factuality from var-ious perspectives, and evaluating the accuracy of automatic fact-checkers (Wang et al., 2024b)."}, {"title": "B System Architecture", "content": ""}, {"title": "B.1 Three Common Factual Error Types", "content": "Type1: Knowledge error is the most common er-ror, occurring when the model produces halluci-nated or inaccurate information. However, LLMs do not know what they do not know, sometimes overestimate their capacities and confidently output unknown information, leading to false responses. Mitigating such errors require: (a) learning and cor-recting parametric knowledge through the curation of corpora used in pre-training, supervised fine-"}, {"title": "B.2 FactQA Component Datasets", "content": "Snowball dataset (Zhang et al., 2023a) comprises three question-answering subsets: primality test-ing, senator search, and graph connectivity, each with 500 yes/no questions. They aim to investi-gate snowballing hallucination when a model im-mediately outputs an incorrect answer (yes or no) as false generated context. Language models are prompted to first output a yes/no answer and then to provide explanations. When the immediate answer is wrong, the model tends to continue to snowball the false statements instead of correcting them.\nSelfAware (Yin et al., 2023) aims to evaluate LLMs' ability to understand their own limitations and unknowns. This is achieved by assessing mod-els' ability to identify unanswerable or unknowable questions. They compiled a collection of 1,032 unanswerable questions from online platforms like Quora and HowStuffWorks. In addition, they gath-ered 2,337 answerable questions from sources such as SQUAD, HotpotQA, and TriviaQA, resulting in a total of 3,369 questions.\nFreshQA (Vu et al., 2023) is composed of 600 natural, open-ended questions, segmented into four primary categories based on the answer's stability: never-changing, for answers that rarely alter, slow-changing, for those that evolve over several years, fast-changing, for answers that shift within a year or less, and false-premise, encompassing questions with factually incorrect premises that need to be countered.\nFacTool (Chern et al., 2023) detected factual er-rors in LLM generations across four different tasks: knowledge-based QA, code generation, mathe-matical reasoning, and scientific literature review. We used 50 knowledge-based QA FacTool-QA in FactQA.\nFELM (Chen et al., 2023) collects responses generated from LLMs and annotated factuality la-bels in a fine-grained manner. The dataset consists of 5 categories, with examples per category as fol-lows: 194 math, 208 reasoning, 125 science, 184 world knowledge (wk), and 136 writing recordings. We used 184 world-knowledge questions, referring to FELM-WK.\nFactcheck-Bench (Wang et al., 2023) Factcheck-GPT gathered a total of 94 highly chal-lenging questions from sources including Twitter posts, internal brainstorming, and Dolly-15k, encompassing 678 claims.\nFactScore-Bio (Min et al., 2023) selected 183 entities, and collected responses from three LLMs including Davinci-text-003, ChatGPT, and PerplexityAI, and then annotated factual labels (supported, not-supported and irrelevant) for each atomic claim by humans."}]}