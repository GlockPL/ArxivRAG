{"title": "OpenFactCheck: A Unified Framework for Factuality Evaluation of LLMs", "authors": ["Hasan Iqbal", "Yuxia Wang", "Minghan Wang", "Georgi Georgiev", "Jiahui Geng", "Iryna Gurevych", "Preslav Nakov"], "abstract": "The increased use of large language models (LLMs) across a variety of real-world applications calls for automatic tools to check the factual accuracy of their outputs, as LLMs often hallucinate. This is difficult as it requires assessing the factuality of free-form open-domain responses. While there has been a lot of research on this topic, different papers use different evaluation benchmarks and measures, which makes them hard to compare and hampers future progress. To mitigate these issues, we introduce OpenFactCheck, a unified framework, with three modules: (i) RESPONSEEVAL, which allows users to easily customize an automatic fact-checking system and to assess the factuality of all claims in an input document using that system, (ii) LLMEVAL, which assesses the overall factuality of an LLM, and (iii) CHECKEREVAL, a module to evaluate automatic fact-checking systems. OpenFactCheck is open-sourced and publicly released as a Python library and also as a web service. A video describing the system is available at https://youtu.be/-i9VKL0HleI.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have demonstrated impressive capabilities in generating naturally-sounding answers over a broad range of human inquiries. However, GPT-40 (OpenAI, 2023) and other text generation models still produce content that deviates from real-world facts (Bang et al., 2023; Borji, 2023; Guiven, 2023). This degrades the performance of LLMs and undermines their reliability, which is a significant bottleneck for deployment (Chuang et al., 2023; Geng et al., 2023).\nMany studies have explored evaluating the factuality of LLMs (Lee et al., 2022; Chuang et al., 2023; Shi et al., 2023; Chen et al., 2023). Two challenges have been identified: (i) it is difficult to assess the factuality of open-domain free-form responses, and (ii) different papers use different evaluation datasets and measures, which makes it hard to compare them, thus hampering future progress (Wang et al., 2024c). To mitigate these issues, we introduce OpenFactCheck, an Open-source Factuality Evaluation Framework for LLMs.\nOpenFactCheck comprises the following three core modules as shown in Figure 1:\n\u2022 RESPONSEEVAL: It allows users to customize an automatic fact-checker and to verify free-form documents to alleviate the first problem.\n\u2022 LLMEVAL: A unified LLM factuality evaluation module which applies seven factuality-specific benchmarks to assess the LLM factuality ability from different aspects and then produces a report to illustrate the weakness and strength, tackling the second challenge.\n\u2022 CHECKEREVAL: It assesses the verification accuracy of fact-checkers, equipped with a leaderboard in terms of accuracy, latency, and costs, aiming to encourage the development of advanced automatic fact-checking systems.\nThe modules are designed for seamless integration, each contributing to and enhancing the capabilities of the others. The results of human verification derived from LLMEVAL can be used as the benchmark for evaluating the accuracy of automated fact-checkers. Simultaneously, the most effective checker identified in CHECKEREVAL can be deployed for automated fact-checking tasks. Each fact-checker in CHECKEREVAL can be an implementation in RESPONSEEVAL. Complex user inquiries may be considered as potential candidates of the factuality assessment dataset utilized in LLMEVAL.\nUsers can tailor their checkers according to their"}, {"title": "2 System Architecture", "content": "The design of OpenFactCheck emphasizes two principles: (i) customizability and extensibility for both users and developers, and (ii) compatibility with existing methods and datasets. It consists of three modules: RESPONSEEVAL, LLMEVAL, and CHECKEREVAL. We detail the design and implementation of each components below."}, {"title": "2.1 RESPONSEEVAL", "content": "RESPONSEEVAL allows users to customize a fact-checking system by selecting a claim processor, a retriever, and a verifier in web pages. Current version supports the following fact-checking systems: RARR, FacTool and Factcheck-GPT (Gao et al., 2022; Chern et al., 2023; Wang et al., 2023).\nConfigurable Architecture We consolidate various fact-checking systems into a three-step process, encapsulated by three classes: claim_processor, retriever, and verifier (Wang et al., 2024c). These classes are instantiated and sequentially connected to form a pipeline that addresses the following tasks: (i) breaking down a document into individual claims, (ii) gathering pertinent evidence for each claim, and (iii) evaluating the veracity of each claim based on the evidence provided. This sequence of tasks is referred to as solvers (see the pseudo code in Figure 4).\nThe implementation of a task solver can be flexible, just ensuring that the input and the output are aligned with the abstract class definitions. For"}, {"title": "2.2 LLMEVAL", "content": "We observed that studies assessing language models' factuality or evaluating whether the methods are effective to mitigate model hallucinations use different datasets and metrics. This makes it difficult to compare, in the same conditions, the factuality of different models as well as to compare the effectiveness of different factuality enhancement approaches. Moreover, a lot of prior work applied datasets such as MMLU (Hendrycks et al., 2021), StrategyQA (Geva et al., 2021) and HotpotQA (Yang and et al., 2018) to evaluate model's factuality. These datasets tend to focus on assessing the general performance, rather than factuality. To this end, we first collect a dataset FactQA by gathering factual questions of existing datasets that are curated to probe diverse factual errors and span across a spectrum of domains, to fairly evaluate LLMs' factuality under the same criteria\nFactual Question Collection We collected factual questions from seven commonly-used corpora that is collected deliberately to assess LLM's factuality, including Snowball (Zhang et al., 2023a), SelfAware (Yin et al., 2023), FreshQA (Vu et al., 2023), FacTool (Chern et al., 2023), FELM-WK (Chen et al., 2023), Factcheck-GPT (Wang et al., 2023) and FactScore-Bio, a total of 6,480 examples shown in Table 1, referring to FactQA (see dataset details in Appendix B.2).\nTo concretely analyze models' vulnerability, we identify three labels for each question from the perspective of the knowledge domain, the topic, and the potential error type if a LLM generates a factually incorrect response. So each example includes the following fields: question, domain, topic, ability to test, task and source. Domains involve general, legal, biomedical, clinical, scientific and so on. Given a domain, we further fine-grained topics. Three common error types are presented.\nType1: Knowledge error is the most common error, occurring when the model produces hallucinated or inaccurate information due to lacking relevant knowledge or internalizing false knowledge in the pre-training stage or in the problematic alignment process.\nType2: Over-commitment error occurs when the model fails to recognize the falsehoods (or jokes) inherent in the prompt or previously-generated con-"}, {"title": "2.3 CHECKEREVAL", "content": "Automatic fact-checking systems aim to identify whether a claim or a document is true or false, but the results are not necessarily correct. To assess the accuracy of automatic fact-checkers, we gather four LLM factuality benchmarks with human-annotated factual labels for three levels of granularity text: claims/segments/documents given (question, ChatGPT response) pairs, including FacTool-QA, FELM-WK, Factcheck-Bench and HaluEval as shown in Table 2. We refer to them as FactBench. We use precision, recall, and F1-score with respect to the True or False claim/document to evaluate the effectiveness of fact-checking systems."}, {"title": "3 Access and Deployment", "content": "OpenFactCheck is accessible via a user-friendly web interface and features an integrated database that maintains a user leaderboard. It is also available as a standalone open-source Python library."}, {"title": "3.1 Python Library", "content": "OpenFactCheck is available as an open-source Python library on PyPI, designed for flexibility and ease of integration into existing projects. This library equips developers with essential components for fact-checking in any Python environment, making it an optimal choice for enhancing applications with fact-checking features. The library employs a fluent interface to ensure its usage is intuitive for both beginners and experts alike.\nUsers can install the library by simply using the pip package manager:\n$ pip install openfactcheck\nThe library includes detailed documentation to assist developers in customizing and extending the functionality to meet their specific needs and it is continually updated to ensure compatibility with the latest research and data security standards.\nUsage Examples The first step is to import the necessary library components and initialize OpenFactCheckConfig configuration and OpenFactCheck class, which requires no input values for default usage, as shown below:\nfrom openfactcheck.core.base import\nOpenFactCheck, OpenFactCheckConfig\nconfig = OpenFactCheckConfig()\nofc = OpenFactCheck(config)\nUpon importing the library, users are required to secure API keys from platforms utilized by OpenFactCheck's default solvers for evidence retrieval and claim verification. These keys are available from OpenAI, SerpAPI, and ScraperAPI. After acquiring the keys, they need to be configured as environment variables to enable their use within the library.\nThe three key functionalities outlined in Section 2 are implemented as shown in Figure 2. We can see that the design of the library is intuitive and straightforward, enabling users to apply it without"}, {"title": "3.2 Web Interface", "content": "The web interface of OpenFactCheck provides a user-friendly platform that allows general users to interactively engage with the fact-checking functionalities. It is designed to accommodate both novice and expert users, facilitating easy access to the comprehensive evaluations involved in the assessment of LLM factuality. The web interfaces are organized into four distinct sections as illustrated in Figure 3 (a).\nIn RESPONSEEVAL page as shown in Figure 3 (b), users can click the dropdown list to select from a range of pre-implemented claim processor, retriever, and verifier. Then, users can input text either written by human or generated by machine into the text box and click Check Factuality to obtain the verification results. As the example demonstrated in the Figure, it includes two claims. The system collected 16 pieces of evidence, and one claims is supported and one claim is refuted, resulting the overall credibility of 50% and judgement \"False\" for this whole input.\nFor both the LLMEVAL and RESPONSEE-VAL pages exhibited in Figure 3 (d), users first download either the question set FactQA or the claims/documents in FactBench. After being ready to upload the responses of the LLM that users aim to assess or the verification results of the fact-checkers to test, users type their details including name, email address and so on, and provide the option to opt in or out of leaderboard inclusion (see Figure 3 (d)). If users agree, their information and rank will be displayed on the leaderboard, otherwise invisible for others.\nIt may takes some time for LLMEVAL to generate teh evaluation report, depending on the system's current load. Once the report is ready, it is emailed directly to the user, eliminating the need to wait within the application. LLM factuality evaluation report presents LLM factuality from various aspects, and specifically includes accuracy and confusion matrix of short answers, pie chart indicating accuracy over fresh questions and bar chart showing the percentage of true, false, controversial claims for free-form responses (see Figure 3 (e)).\nSimilarly, CHECKEREVAL results present the number of evaluated examples, the overall accuracy, total time and USD cost, fine-grained precision, recall and F1-score for false and true classes, and a confusion matrix showing the misidentification of this fact-checker. The submission in Figure 3 (f) reveals that this checker performs equally poor over both false and true claims in verification. This evaluation is instant."}, {"title": "4 Conclusion and Future Work", "content": "We implemented a unified, easy-to-use and extensible framework OpenFactCheck. It is accessible by both Python libaray and web service, supporting the customization and evaluation of automatic fact-checking systems and LLM factuality evaluation. Specifically, OpenFactCheck allows general users to check whether a claim and a document are factual or not by clicking Check, and also facilitate LLM practitioners and developers to effectively and efficiently evaluate the factuality of their LLMs from various perspectives, and to assess the accuracy of automatic fact-checking systems.\nIn the future, we will continue to integrate new techniques, features, and evaluation benchmarks to OpenFactCheck to facilitate the research progress of LLM fact-checking."}, {"title": "Limitations", "content": "While OpenFactCheck presents a comprehensive framework for factuality evaluation of LLMs, several limitations must be acknowledged:\nEvaluation Datasets The effectiveness of OpenFactCheck is dependent on the quality and diversity of the datasets used for evaluation. While we have integrated multiple datasets to cover a broad spectrum of domains and potential factual errors, the evaluation is still limited by the inherent biases and coverage gaps in these datasets. For instance, some specialized domains may not be adequately represented, potentially affecting the robustness of the evaluation for LLMs in those areas.\nLatency and Costs The performance of automatic fact-checking systems integrated within OpenFactCheck can vary significantly in terms of latency and operational costs. High accuracy often comes at the expense of increased computational resources and processing time, which may not be feasible for all users, particularly those with limited budgets or time constraints.\nReliance on External Knowledge Sources The fact-checking modules depend heavily on external knowledge sources, such as Wikipedia and web search engines. The availability and reliability of these sources can affect the accuracy and completeness of the fact-checking process. Furthermore, the dynamic nature of web content means that the information retrieved may not always be up-to-date."}, {"title": "Ethical Statement", "content": "The development and deployment of OpenFactCheck are guided by a commitment to ethical principles, ensuring that the framework is used responsibly and for the benefit of society:\nTransparency and Accountability We strive to maintain transparency in the design, implementation, and evaluation of OpenFactCheck. The source code and datasets are publicly available, enabling scrutiny and fostering trust within the research community. We encourage users to report any issues or biases they encounter, facilitating continuous improvement.\nBias Mitigation Recognizing that biases can exist in both datasets and LLMs, we are dedicated to minimizing such biases in OpenFactCheck. By integrating diverse evaluation benchmarks and encouraging the development of fair fact-checking approaches, we aim to reduce the impact of biases on factuality evaluation outcomes.\nSocial Impact By enhancing the factual accuracy of LLMs, OpenFactCheck aims to contribute positively to society. Accurate information is crucial for informed decision-making and public discourse. We believe that improving the reliability of LLM outputs can help combat misinformation and support the dissemination of truthful information."}, {"title": "A Related Work", "content": "While numerous automatic fact-checking systems have developed, such as RARR, FactScore, Fac-Tool, Factcheck-GPT and Longform SAFE (Gao et al., 2022; Min et al., 2023; Chern et al., 2023; Wang et al., 2023; Wei et al., 2024), they are often inaccessible to general users who lack a Python environment to compile code and run verification. Although these systems can function as the backend of a service, a user-friendly web interface is necessary to allow general users to verify text inputs by simply typing or copying text and clicking a check button. OpenFactCheck addresses this by providing an accessible web interface.\nIn addition, various fact-checking systems have distinct advantages. For instance, Factcheck-GPT offers a fine-grained framework to involve all possible subtask that could improve the fact-checking system, FacTool employs a low-latency evidence retriever through asynchronous processing, and FactScore introduces a scoring metric that calculates the percentage of true claims in a given text, thereby quantitatively assessing the credibility of the input. OpenFactCheck integrates these advantages into a unified system (Wang et al., 2024c).\nRecent open-sourced demo system Loki (Wang et al., 2024a) also aims to leverage strength of various automatic fact-checkers, while it emphasizes optimization a single fact-checking system in terms of accuracy, latency, robustness, cost-efficiency, and extensive support for multiple languages and LLMs. In contrast, OpenFactCheck is a unified framework to cover three major functionalities for factuality evalaution of LLMs, including customizing a fact-checker by combining modules of different checkers, assessing LLM factuality from various perspectives, and evaluating the accuracy of automatic fact-checkers (Wang et al., 2024b)."}, {"title": "B System Architecture", "content": "B.1 Three Common Factual Error Types\nType1: Knowledge error is the most common error, occurring when the model produces hallucinated or inaccurate information. However, LLMs do not know what they do not know, sometimes overestimate their capacities and confidently output unknown information, leading to false responses. Mitigating such errors require: (a) learning and correcting parametric knowledge through the curation of corpora used in pre-training, supervised fine-"}]}