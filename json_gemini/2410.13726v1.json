{"title": "DAWN: DYNAMIC FRAME AVATAR WITH NON-AUTOREGRESSIVE DIFFUSION FRAMEWORK FOR TALKING HEAD VIDEO GENERATION", "authors": ["Hanbo Cheng", "Limin Lin", "Chenyu Liu", "Pengcheng Xia", "Pengfei Hu", "Jiefeng Ma", "Jun Du", "Jia Pan"], "abstract": "Talking head generation intends to produce vivid and realistic talking head videos from a single portrait and speech audio clip. Although significant progress has been made in diffusion-based talking head generation, almost all methods rely on autoregressive strategies, which suffer from limited context utilization beyond the current generation step, error accumulation, and slower generation speed. To address these challenges, we present DAWN (Dynamic frame Avatar With Non-autoregressive diffusion), a framework that enables all-at-once generation of dynamic-length video sequences. Specifically, it consists of two main components: (1) audio-driven holistic facial dynamics generation in the latent motion space, and (2) audio-driven head pose and blink generation. Extensive experiments demonstrate that our method generates authentic and vivid videos with precise lip motions, and natural pose/blink movements. Additionally, with a high generation speed, DAWN possesses strong extrapolation capabilities, ensuring the stable production of high-quality long videos. These results highlight the considerable promise and potential impact of DAWN in the field of talking head video generation. Furthermore, we hope that DAWN sparks further exploration of non-autoregressive approaches in diffusion models. Our code will be publicly at https://github.com/Hanbo-Cheng/DAWN-pytorch.", "sections": [{"title": "1 INTRODUCTION", "content": "Talking head generation aims at synthesizing a realistic and expressive talking head from a given portrait and audio clip, which is garnering growing interest due to its potential applications in virtual meetings, gaming, and film production. For talking head generation, it is essential that the lip motions in the generated video precisely match the accompanying speech, while maintaining high overall visual fidelity (Guo et al., 2021a). Furthermore, natural coordination between head pose, eye blinking, and the rhythm of the audio is also crucial for a convincing output (Liu et al., 2023).\nRecently, Diffusion Models (DM) (Ho et al., 2020) have achieved significant success in video and image generation tasks (Rombach et al., 2022; Ho et al., 2022b;a; Peebles & Xie, 2023; Ni et al., 2023). However, their application in talking head generation (Shen et al., 2023; Bigioi et al., 2024) still faces several challenges. Although many methods yield high-quality results, most of them rely on autoregressive (AR) (Tian et al., 2024; Ma et al., 2023) or semi-autoregressive (SAR) (Xu et al., 2024b; He et al., 2023) strategies. In each iteration, AR generates one frame, while SAR generates a fixed-length video segment. The two strategies significantly slow down the inference speed and fail to adequately utilize contextual information from future frames, which leads to constrained performance and potential error accumulation, especially in long video sequences (Stypu\u0142kowski et al., 2024)."}, {"title": null, "content": "To address these challenges, we present DAWN (Dynamic frame Avatar With Non-autoregressive diffusion), a novel approach that significantly improves both the quality and efficiency of talking head generation. Our approach leverages the DM to generate motion representation sequences from a given audio and portrait. These motion representations are subsequently used to reconstruct the video. Unlike other methods, our approach produces videos of arbitrary length in a non-autoregressive (NAR) manner. However, employing the NAR strategy to generate long videos often results in either over-smoothing or significant content inconsistencies due to limited extrapolation (Qiu et al., 2023). In the context of talking head generation, we suggest that the model's temporal modeling capability is significantly hindered by the strong coupling relationship among multiple motions. Typically, the motions in talking head include (1) lip motions and (2) head pose and blink movements. The temporal dependency of head and blink movements extends over several seconds, far longer than that of lip motions (Zhou et al., 2020). Training models to capture these long-term dependencies requires extensive sequences, thus increasing the difficulty and cost of training. Fortunately, head and blink movements can be represented as low-dimensional vectors (Zhang et al., 2023), enabling the design of a lightweight model that learns these long-term dependencies by training on extended sequences. Thus, to further enhance the temporal modeling and extrapolation capabilities of DAWN, we disentangle the motion components involved in talking head videos. Specifically, we use the Audio-to-Video Flow Diffusion Model (A2V-FDM) to learn the implicit mapping between the lips and audio, while generating the head pose and blinks via explicit control signals. Additionally, we propose a lightweight Pose and Blink generation Network (PBNet) trained on long sequences, to generate natural pose/blink movements during inference in a NAR manner. In this way, we simplify the training of A2V-FDM as well as achieve the long-term dependency modeling of the pose/blink movement. To further strengthen the convergence and extrapolation capabilities of A2V-FDM, we propose a two-stage training strategy based on curriculum learning to guide the model in generating accurate lip motion and precise pose/blink movement control.\nThe main contributions of this work are as follows: 1) We present DAWN (Dynamic frame Avatar With Non-autoregressive diffusion) for generating dynamic-length talking head videos from portrait images and audio clips in a non-autoregressive (NAR) manner, achieving faster inference speeds and high-quality results. To the best of our knowledge, this is the first NAR solution based on diffusion models designed for general talking head generation. 2) To compensate for the limitations of extrapolation in NAR strategies and enhance the temporal modeling capabilities for long videos, we decouple the motions of the lips, head, and blink, achieving precise control over these movements. 3) We propose the Pose and Blink generation Network (PBNet) to generate natural head pose and blink sequences exclusively from audio in a NAR manner. 4) We introduce the Two-stage Curriculum Learning (TCL) strategy to guide the model in mastering lip motion generation and precise pose/blink control, ensuring strong convergence and extrapolation ability."}, {"title": "2 RELATED WORKS", "content": "Audio-driven talking head generation. Initial approaches for talking head generation employed deterministic models to map audio to video streams (Fan et al., 2016), with later methods introducing generative models such as GANs (Isola et al., 2016), VAEs (Kingma & Welling, 2022), and diffusion models (DMs) (Ho et al., 2020). GAN-related methods (Vougioukas et al., 2020; Pumarola et al., 2018; Hong et al., 2022) improved visual realism but faced convergence and mode collapse issues (Xia et al., 2022). Following this, VAEs (Kingma & Welling, 2022) generated 3D priors like 3D Morphable Models (3DMM) (Blanz & Vetter, 1999) followed by high-fidelity rendering (Ren et al., 2021), which limited the realism and vividness. In contrast, DMs have been introduced into talking head generation due to their good convergence, excellent generation performance, and diversity. Stypu\u0142kowski et al. (2024) presented a DM-based talking head generation solution using an AR strategy to generate videos frame-by-frame iteratively. Subsequently, Tian et al. (2024) improved this AR strategy by incorporating motion conditions extracted by a VAE-based network as priors for each iteration, effectively mitigating degradation issues. Concurrently, Xu et al. (2024b); He et al. (2023) advocated for motion modeling instead of image modeling. They utilized DMs to iteratively generate latent motion representations over a fixed number of frames in a SAR manner, subsequently converting these motion representations into video frames. While most diffusion-based methods produce promising results, their AR or SAR strategies incur slow generation speeds and collapse in long-video generation. Although methods like Tian et al. (2024); Xu et al. (2024b) alleviate issues such as inconsistencies in content across iterations and long video generation collapse, the risk of"}, {"title": null, "content": "error accumulation remains unresolved. While methods like Du et al. (2023) use identity-specific NAR strategies, to the best of our knowledge, none have addressed NAR talking head generation for arbitrary identities. Consequently, we propose a novel NAR dynamic frame generation framework based on DM, which aims to achieve the low-cost and high-quality rapid generation of realistic talking head video through a clip of audio and arbitrary portrait.\nAudio-driven pose and blink generation. Head pose and blink movements significantly impact the naturalness of talking head videos. However, the mapping from audio to pose and blink movement is a one-to-many problem, which presents a significant challenge (Xu et al., 2024a; Chen et al., 2020). Early works primarily focused on controlling poses directly using facial landmarks or video references (Zhou et al., 2021; Guo et al., 2021b). However, these approaches require additional guidance information, which impairs the diversity of the results. Later studies considered generating both pose and blink within the context of talking head generation (Zhou et al., 2020). However, simultaneously generating all facial movements can cause interference and ambiguity (Zhang et al., 2023). Therefore, some works attempted to decouple the speaker's actions into components like lip, head pose, and blink, using discriminative models to predict these conditions separately (Wang et al., 2021; He et al., 2023). Later, researchers recognized that probabilistic modeling is better suited for the one-to-many mapping relationship, leading to the proposal of a VAE-based pose generation framework (Liu et al., 2023). However, most existing pose generation strategies also depend on AR or SAR approaches, negatively impacting efficiency, smoothness, and naturalness. To address these issues, we design a VAE-based NAR pose generation method to produce vivid and smooth pose and blink movements while maintaining the NAR generation of the entire framework."}, {"title": "3 METHOD", "content": "As shown in Figure 1, DAWN is divided into three main parts: (1) the Latent Flow Generator (LFG); (2) the conditional Audio-to-Video Flow Diffusion Model (A2V-FDM); and (3) the Pose and Blink generation Network (PBNet). First, we train the LFG to estimate the motion representation between different video frames in the latent space. Subsequently, the A2V-FDM is trained to generate temporally coherent motion representation from audio. Finally, PBNet is used to generate poses and blinks from audio to control the content in the A2V-FDM. To enhance the model's extrapolation ability while ensuring better convergence, we propose a novel Two-stage Curriculum Learning (TCL) training strategy. We will first discuss preliminaries, then present the specific details of DAWN's three main components, namely LFG, A2V-FDM, and PBNet in Sections 3.2, 3.3, and 3.4, respectively. Finally, we will introduce the TCL strategy in Section 3.5."}, {"title": "3.1 PRELIMINARIES", "content": "Task definition. The task of talking head generation involves creating a natural and vivid talking head video from two inputs: a static single-person portrait, \u00e6src, and a speech sequence, $Y_{1:N} = \\{Y_0, Y_1, \\ldots, Y_N \\}$. The static image xsrc and the speech sequence y can originate from any individual, and the output is $2_{1:N} = \\{2_0, 2_1,...,2_N\\}$, where N represents the total number of frames.\nDiffusion models. Diffusion models generate samples conforming to a given data distribution by progressively denoising Gaussian noise (Ho et al., 2020). Let xo represent data sampled from a given distribution q(x0). In the forward diffusion process, Gaussian noise is progressively added to x0 after T steps, resulting in noisy data x\u0442 (Nichol & Dhariwal, 2021; Song et al., 2020), and the conditional transition distribution at each step is defined as:\n$q(x_t|x_0) = N(x_t; \\sqrt{\\bar{\\alpha}_t} x_0, (1 - \\bar{\\alpha}_t)I)$         (1)\nwhere $\\bar{\\alpha}_t = \\prod_{i=1}^{t} \\alpha_i$. The reverse diffusion process gradually recovers the original data from the Gaussian noise \u00e6\u0442 ~ N(0, I), utilizing a neural network to predict po(xt-1|xt), where @ represents the parameters of the neural network. The model is trained using the following loss function:\n$\\mathcal{L}_{simple} = \\mathbb{E}_{t,x_0,\\epsilon}[||\\epsilon - \\epsilon_\\theta (x_t, t)||^2]$     (2)\nwhere e is the Gaussian noise added to x0 in the forward diffusion process to obtain xt, and eo(xt,t) is the noise predicted by the model. In video-related tasks, the denoising model can be implemented via a 3D U-Net (Ho et al., 2022b; \u00c7i\u00e7ek et al., 2016)."}, {"title": "3.2 LATENT FLOW GENERATOR", "content": "The Latent Flow Generator (LFG) is a self-supervised training framework designed to model motion information between the source image src and the driving image \u00e6dri. As illustrated in Figure 1 (a), LFG consists of three trainable modules: the image encoder E, the flow predictor P, and the image decoder D. During training, sre, dri \u2208 RH\u00d7W\u00d73 are images randomly selected from the same video. The image encoder & encodes the source image \u00e6src into a latent code Zsrc \u2208 RHz\u00d7Wz\u00d7Cz. The flow predictor estimates a dense flow map f and a blocking map m (Siarohin et al., 2021; 2020), corresponding to xsrc and dri:\n$f, m = P(x_{src}, x_{dri})$         (3)\nThe flow map f \u2208 RHz\u00d7Wz\u00d72 describes the feature-level movement of \u00e6dri relative to \u00e6src in horizontal and vertical directions. The blocking map m \u2208 RHz\u00d7W\u2082\u00d71 ranging from 0 to 1, indicates the degree of area blocking in the transformation from xsrc to adri. The flow map f is used to perform the affine transformation A, serving as a coarse-grained warping of zsrc. Subsequently, the blocking map m guides the model in repairing the occlusion area, thereby serving as fine-grained repair. Finally, the image decoder D converts the warped latent code into the target image \u00e6gen, where the is the element-wise product:\n$\\hat{x}_{gen} = D(A(z_{src}, f) \\otimes m)$          (4)\nThe LFG is trained in an unsupervised manner and optimized using the following reconstruction loss:\n$\\mathcal{L}_{LFG} = \\mathcal{L}_{rec} (x_{gen}, x_{dri})$            (5)"}, {"title": "3.3 CONDITIONAL AUDIO2 VIDEO FLOW DIFFUSION MODEL", "content": "Through the LFG in Section 3.2, we can specify a \u00e6src, and extract the identity-agnostic motion representations z of the talking head video clip X1:N as well as the latent code zsrc. Therefore, we design the A2V-FDM to generate the motion representations $z_N = [f_{1:N}, m_{1:N}]$ of each frame relative to xsrc:\n$z_N = DM(x_{src}, Y_{1:N}, P_{1:N})$        (6)\nwhere the DM refers the diffusion model. After generating N, we use the decoder D in LFG to reconstruct the 2 to the target video frames, via the Equation 4. The structure of A2V-FDM is illustrated in Figure 1 (c). The A2V-FDM model includes a 3D U-Net denoising backbone (Ho et al., 2022b). The residue block in the 3D U-Net contains temporal attention and spatial attention modules, which handle frame-level and pixel-level dependencies respectively. The parameters of the temporal attention module are independent of the input length, so we believe that 3D U-Net can theoretically process video sequences of any length. However, to ease training difficulty, we train on short sequences and aim for long-sequence inference. To enhance the 3D U-Net's extrapolation ability when handling long sequences, we used Rotary Positional Encoding (RoPE) (Su et al., 2023) instead of traditional absolute position embeddings in the temporal attention module.\nConditioning. We incorporate the following conditions to control its generative behavior: audio embedding a1:N, pose/blink signal p1:N, and source image latent code zsrc. The audio embedding a1:N, extracted from the audio y1:N using Hubert (Hsu et al., 2021), implicitly controls the lip motion. Due to the strict alignment with video frames, we apply the audio embedding to its corresponding image. Additionally, the avatar's pose and blink are controlled via explicit signals. The pose is described by a 6D vector Ji et al. (2022). During training, the pose is extracted from video using an open-source tool (Guo et al., 2020). For the blink signal, we adopt the aspect ratio of the left and right eyes, following (Zhang et al., 2023). To account for the arbitrary pose and eye-opening degree of the source image xsrc, we use the difference between the current frame xi and the source frame $x_{src}: \\Delta \\rho_i = p_i - p_{src}$, which models the transition of state rather than the state itself. The model is provided with features zsrc to supply facial visual details. Each frame's latent code performs cross attention with the audio embedding a\u017c and pose/blink information \u0394\u03c1\u017c, respectively. This process injects these conditions into the latent code with different spatial weights, controlling specific regions of the generated content. The image feature zsrc is regarded as a global condition and is concatenated directly with the noisy data as the initial input to the 3D U-Net. We also utilize landmarks to create a face region mask for \u00e6src, embedded with a lightweight convolutional network, similar to the approach by Tian et al. (2024). This mask adds to the denoising process in the same manner as zsrc.\nLoss function. We employed the DM regular denoising loss, Lsimple, in Equation 2 to train our model. The synchronization of lip motions with audio is crucial for the talking head task, while the lips often constitute only a small portion of the frame. Consequently, during training, we employed landmarks to isolate the lip region by generating a lip mask, mlip. We then applied an additional weight, Wlip, to the denoising process of this region, similar to Stypu\u0142kowski et al. (2024). Ultimately, our loss function is defined as:\n$\\mathcal{L} = \\mathcal{L}_{simple} + W_{lip} (\\mathcal{L}_{simple} \\otimes m_{lip})$     (7)\nwhere the symbol denotes element-wise product."}, {"title": "3.4 AUDIO-DRIVEN POSE AND BLINK GENERATION", "content": "To prevent the generated results from exhibiting overly monotonous and minimal movements, we design a separate module, namely the Pose and Blink generation Network (PBNet). As shown in Figure 1 (b), PBNet learns the mapping between audio and pose/blink movements. To maintain the"}, {"title": "3.5 TWO-STAGE CURRICULUM LEARNING FOR TALKING HEAD GENERATION", "content": "Empirical evidence indicates that training our A2V-FDM model solely with fixed-length short video clips leads to inaccurate control of poses and blinks, as well as poor generalization to longer videos. We argue that a one-step training approach impedes the model's convergence to an optimal solution and fails to achieve satisfactory results in the complex task of talking head generation. To address these issues, we propose an innovative Two-Stage Curriculum Learning (TCL) strategy inspired by the theory of curriculum learning (Bengio et al., 2009).\nOverall, the goal of the A2V-FDM during the training process can be expressed as:\n$2_{1:N} = D(DM(X_{src}, Y_{1:N}, P_{1:N}))$      (10)\nIn the first stage, we set Xsrc = x1, and the sequence length N = K' is a fixed, relatively small constant. This stage primarily focuses on enabling the model to generate basic lip motions. However, utilizing x\u2081 as the source image often exhibits limited variations in poses and blinks, and using short clips can result in a scarcity of training samples with significant pose or blink movements. Therefore, in the second stage, we set \u00e6src \u2208 X randomly, where X is the set of frames in the entire video, to learn control capabilities of large pose transformation. Additionally, differing from stage one, we randomly set N\u2208 [Kmin, Kmax], Kmin > K'. This approach aims to enhance control over poses and blinks while maintaining precise lip motions, as longer clips contain more diverse pose and blink movements. Training with random-length sequences also helps the model avoid a bias towards fixed-length sequences, further enhancing the model's extrapolation."}, {"title": "3.6 INFERENCE", "content": "Our inference process consists of four steps: 1) Extract the audio embedding a1:N from the input audio. 2) Provide the initial pose/blink state psrc, extracted from the source image xsrc, to the PBNet. Along with a1:N and a latent space vector h1:N sampled from a standard Gaussian distribution, PBNet generates the pose/blink sequences P1:N. 3) Input xsrc, a1:N, and P1:N into the A2V-FDM, which generates the motion representation sequences $2_{1:N}$. 4) Finally, decode the video sequence $2_{1:N}$ from xsrc and $2_{1:N}$.\nOur method leverages non-autoregressive (NAR) generation during the inference process. To enhance extrapolation during inference, we utilize local attention (Luong, 2015) in the temporal attention module for both the PBNet decoder and the 3D U-Net in A2V-FDM, which restricts the attention scores to a local region. This approach effectively models local dependencies in talking head videos. To accommodate the different temporal dependencies of lip motions and pose/blink movements, we use a larger window size in the local attention mechanism of PBNet compared to A2V-FDM."}, {"title": "4 EXPERIMENT", "content": null}, {"title": "4.1 SETUP", "content": "Dataset. Our method is evaluated on two datasets: CREMA (Cao et al., 2014) and HDTF (Zhang et al., 2021). The CREMA dataset was collected in a controlled laboratory environment and contains 7,442 videos from 91 identities, with durations ranging from 1 to 5 seconds. The HDTF dataset consists of 410 videos, with an average duration exceeding 100 seconds. These videos are gathered from wild scenarios and feature over 10,000 unique sentences for speech content, along with diverse head pose movements. We partitioned the CREMA dataset into training and testing sets following Stypu\u0142kowski et al. (2024). As for the HDTF dataset, we conducted a random split with a 9:1 ratio between the training and testing sets. We resized the videos at a resolution of 128 \u00d7 128 and a frame rate of 25 frames per second (fps) without any additional preprocessing.\nImplementation details. The architecture of the encoder and decoder in our model aligns with the design proposed by Johnson et al. (2016), while the flow predictor is implemented based on MRAA (Siarohin et al., 2021). The PBNet model is trained using pose and blink movement sequences of 200 frames. During the inference phase of the PBNet model, a local attention mechanism with a window size of 400 is employed. The motion representation space of the A2V-FDM model is 32 x 32 x 3. The training of the A2V-FDM model is conducted using the TCL strategy, which involves two stages: 1) the model is trained using video clips of 20 frames, with the source image set as the first frame in the clip; 2) training is performed on sequences ranging from 30 to 40 frames. For the inference phase of the A2V-FDM model, local attention with a window size of 80 is applied. In our evaluation, the length of one-time inference for CREMA is dynamic and depends on the ground truth, while for HDTF, it is fixed at 200 frames for better comparison."}, {"title": null, "content": "Evaluation metrics. We evaluate the performance of our method using various metrics. Specifically, we employ the Fr\u00e9chet Inception Distance (FID) (Heusel et al., 2017) to assess the image quality. We utilize the FVD16 and FVD32 scores, which calculate the Fr\u00e9chet Video Distance (FVD) based on window sizes of 16 and 32 frames, respectively, to evaluate video quality across different temporal scales. Furthermore, we assess the perception loss of lip shape using the confidence score (LSEc) and distance score (LSE) (Chung & Zisserman, 2017). To evaluate the preservation of speaker identity during the generation, we use ArcFace (Deng et al., 2019) to extract features from both the ground truth image and the generated image, and use the cosine similarity (CSIM) between the two as the evaluation metric. Moreover, we employ the Beat Align Score (BAS) (Siyao et al., 2022) to evaluate the synchronization of head motion and audio, and calculate the number of blinks per second (blink/s) to assess the liveliness of the eyes."}, {"title": "4.2 OVERALL COMPARISON", "content": "We compared our method with several state-of-the-art methods: Wav2Lip (Prajwal et al., 2020), MakeItTalk (Zhou et al., 2020), Audio2Head (Wang et al., 2021), SadTalker (Zhang et al., 2023), and Diffused Heads (Stypu\u0142kowski et al., 2024). To ensure a fair comparison, we provided the same conditions for each baseline and generated the same number of frames in a single generation. The quantitative experimental results are presented in Table 1.\nAccording to the results, our method achieves the best performance in FID, FVD32, FVD16, CSIM, BAS, and Blink/s metrics for both the CREMA and HDTF datasets. Additionally, our method achieves nearly best scores for LSEC and LSED, closely matching those of the Audio2Head and Wav2Lip baselines. These results demonstrate that our method produces talking head videos with the highest visual quality, rhythmic head poses, and natural blink movements. Additionally, it preserves the identity features of the source image to the greatest extent and achieves highly synchronized lip motions with the audio. Additionally, we test the generation speed of the aforementioned methods in Appendix A.2.2, suggesting our method achieves the fastest or near-fastest generation speed and requires significantly less time compared to the previous diffusion-based method, Diffused Heads.\nFor the qualitative experiment, as shown in Figure 2, we visualize the generation results for each baseline on different datasets. Our method evidently achieves the visual quality most similar to the ground truth, showcasing the most realistic and vivid visual effects. Compared to our method, SadTalker relies on the 3DMM prior, which limits its animation capability to the facial region only, resulting in significant artifacts when merging the face with the static torso below the neck."}, {"title": "4.3 COMPARISON WITH OTHER GENERATION STRATEGIES", "content": "We compared our non-autoregressive generation strategy with two regular video generation strategies: 1) semi-autoregressive (SAR) generation similar to He et al. (2023), and 2) two-temporal resolution (TTR), which trains two models with different temporal resolutions (Harvey et al., 2022). The time cost represents the time required to generate an 8-second talking head video. The models were evaluated on the CREMA dataset, and the results are shown in Table 2. According to the results, our non-autoregressive method produces videos with the highest quality and fastest speed. The generation speed of the SAR method is faster than TTR, while TTR produces higher video quality than SAR. This is because autoregressive methods suffer from degradation issues during iterative generation. Although TTR somewhat alleviates the degradation issue, it compromises generation speed. In summary, our non-autoregressive method addresses the degradation problem while preserving fast generation speed."}, {"title": "4.4 EXTRAPOLATION VALIDATION", "content": "To evaluate the extrapolation ability of our method, we conducted experiments to assess the impact of inference length on model performance using the HDTF dataset. We set the inference length to range from 40 to 600 frames in a single generation process. The results are illustrated in Table 3, suggesting that performance remains relatively stable as the inference length increases. Specifically, the image and video quality metrics, FID and FVD, are not significantly affected by the inference length. Besides, we found that the LSEC and LSED scores tend to improve with increasing inference length while stabilizing when the length is sufficiently large. It suggests that audio with sufficient length helps the model produce more precise lip movement."}, {"title": "4.5 ABLATION STUDY", "content": "Ablation study on TCL strategy. Since we only use the TCL strategy on A2V-FDM, we excluded the PBNet in the evaluation process by using ground truth pose/blink to drive the video. The results are illustrated in Table 4. We separately trained A2V-FDM using either stage 1 or stage 2 of TCL strategy. According to the results, using only stage 1 causes an overall performance decrease across all metrics except FID. This is because, in the first stage, the model is trained on shorter clips. It tends to perform minor warping on the source image, which results in a lower FID score. Although minor warping can improve FID scores, it significantly diminishes the vividness of longer videos, leading to poorer FVD scores. Furthermore, training with a fixed length introduces biases into the"}, {"title": null, "content": "non-autoregressive generation process, impacting the video's smoothness and the accuracy of lip motions. This, in turn, results in worse FVD and penalties in LSEc and LSED scores. Using only stage 2 also negatively affects the model's performance. Compared to using only stage 1, the model achieves better LSEc and LSED scores. This improvement is because training with longer sequences is beneficial for enhancing the precision of lip motions (Tian et al., 2024). However, the worse FID and FVD scores result from the model struggling to learn both pose/blink control and lip movement simultaneously. Using either stage 1 or stage 2 alone clearly falls behind our proposed TCL strategy, demonstrating its significant contribution to performance.\nAblation study on PBNet. We evaluate the effectiveness of the PBNet in Table 4. The term \"w/o PBNet\" indicates that the PBNet module was removed from the architecture, requiring the A2V-FDM to simultaneously generate pose, blink, and lip motions from the audio by itself. The results suggest an overall enhancement of all evaluation metrics with the inclusion of PBNet. This is because modeling the long-term dependency of pose and blink movements through PBNet simplifies training for the A2V-FDM. We also visualized the effectiveness of PBNet in Appendix A.2.4."}, {"title": "4.6 POSE/BLINK CONTROLLABLE GENERATION", "content": "In addition to generating lifelike avatars, our method also enables the controllable generation of pose and blink actions. Users can either use pose and blink information generated by our PBNet or provide these sequences directly, such as by extracting them from a given video. The results, as shown in Figure 3, demonstrate that our method not only provides high-precision control over the pose/blink movements of the generated avatars, but also effectively transfers rich facial dynamics, including expressions, blinks, and lip motions."}, {"title": "5 CONCLUSION", "content": "We introduce DAWN, an innovative architecture that non-autoregressively generates dynamic frames of talking head videos from given portraits and audio. We utilized the LFG to extract motion representations from speech videos. To produce vivid talking head videos, we propose PBNet and"}, {"title": null, "content": "A2V-LDM. The PBNet generates natural pose/blink movements from speech, while A2V-LDM produces motion representations conditioned on audio and pose/blink movements. Finally, these generated motion representations are decoded into videos using LFG. We demonstrate on two datasets that our model can generate extended talking head videos with high-quality dynamic frames in a single pass, achieving realistic visual effects, accurate lip synchronization, and strong extrapolation capabilities."}, {"title": "A APPENDIX", "content": null}, {"title": "A.1 SOCIAL IMPACT CONSIDERATION", "content": "DAWN focuses on creating realistic talking head videos with the aim of generating positive social impact. We firmly oppose the malicious misuse of our method, including fraud, creating fake news, and violating portrait rights. Thus, we assert that our open-source code and model should be used exclusively for research purposes. We hope that our technique can provide social benefits in the future, such as promoting education, enabling face-to-face communication for separated people, and treating certain psychological disorders."}, {"title": "A.2 ADDITIONAL EXPERIMENTS", "content": null}, {"title": "A.2.1 EXPERIMENT ON HIGHER RESOLUTION AND DIFFERENT PORTRAIT STYLES", "content": "We further investigate the generalization ability of our method on higher-resolution images and different portrait styles. Training DAWN on the HDTF dataset with a resolution of 256 \u00d7 256, we tested it on multiple out-of-dataset source images featuring diverse styles, as showcased in Figure 4. The results indicate that our method yields promising outcomes in high-resolution generation and demonstrates considerable generalization ability across various image styles, including photos, paintings, anime, and sketches."}, {"title": "A.2.2 \u0421\u043eMPARISON EXPERIMENT ON GENERATION TIME COST", "content": "We experimented to test the time cost of video generation. Using the aforementioned methods, we generated 8-second talking head videos with the same source image and audio, then recorded the time consumption for each method. To ensure a fair comparison, we excluded the audio encoding step for all methods. The testing was performed on a single V100 16G GPU. As shown in Figure 5, our method achieves the fastest or near-fastest generation speed and requires significantly less time compared to the previous diffusion-based method, Diffused Heads."}, {"title": "A.2.3 ABLATION STUDY ON THE LOCAL ATTENTION MECHANISM", "content": "In our work, we utilized a local attention mechanism to enhance the extrapolation capability of our model. We conducted experiments to evaluate the effect of varying the window size of the local attention mechanism in A2V-FDM, ranging from"}]}