{"title": "Go-Oracle: Automated Test Oracle for Go Concurrency Bugs", "authors": ["Foivos Tsimpourlas", "Chao Peng", "Carlos Rosuero", "Ping Yang", "Ajitha Rajan"], "abstract": "The Go programming language has gained significant traction for developing software, especially in various infrastructure systems. Nonetheless, concurrency bugs have become a prevalent issue within Go, presenting a unique challenge due to the language's dual concurrency mechanisms\u2014communicating sequential processes and shared memory. Detecting concurrency bugs and accurately classifying program executions as pass or fail presents an immense challenge, even for domain experts. We conducted a survey with expert developers at Bytedance that confirmed this challenge. Our work seeks to address the test oracle problem for Go programs, to automatically classify test executions as pass or fail. This problem has not been investigated in the literature for Go programs owing to its distinctive programming model.\nOur approach involves collecting both passing and failing execution traces from various subject Go programs. We capture a comprehensive array of execution events using the native Go execution tracer. Subsequently, we preprocess and encode these traces before training a transformer-based neural network to effectively classify the traces as either passing or failing. The evaluation of our approach encompasses 8 subject programs sourced from the GoBench repository. These subject programs are routinely used as benchmarks in an industry setting. Encouragingly, our test oracle, Go-Oracle, demonstrates high accuracies even when operating with a limited dataset, showcasing the efficacy and potential of our methodology. Developers at Bytedance strongly agreed that they would use the Go-Oracle tool over the current practice of manual inspections to classify tests for Go programs as pass or fail.", "sections": [{"title": "I. INTRODUCTION", "content": "Go is a statically typed programming language designed by Google in 2009 [1] for efficient and reliable concurrent programming. In recent years, Go has gained increasing popularity in building software in many infrastructure systems [2], [3], [4], [5]. Go provides lightweight goroutines and recommends passing messages using channels as a less error-prone means of thread communication. However, a recent empirical study shows that concurrency bugs, exist widely in Go. These bugs severely hurt the reliability of Go concurrent systems.\nTest input generation techniques to help detect concurrency bugs is a promising approach [6] but generate substantially more tests than manual approaches. This abundance poses a challenge when determining the correctness of test executions, a procedure referred to as the test oracle, that heavily relies on manual assessment. Surveys on the test oracle problem [7], [8], [9] show that automated oracles based on formal specifications, metamorphic relations [10] and independent program versions are not widely applicable and difficult to use in practice. This is confirmed in the industry setting within Bytedance Research where developers use automated input generation techniques for testing Go programs that generates a large number of inputs. However, determining whether the test input execution passed or failed expectation (disregarding obvious program crashes) is still subject to manual inspection and in most cases is not trivial, taking up significant expert time.\nWe seek to address the test oracle problem for Go programs that has not been investigated in the literature. We aim to develop Go-Oracle, an automated oracle that can reliably evaluate the correctness of test executions for Go programs. Go-Oracle is intended to act as an aid for developers when encountered with the daunting task of coming up with an expected output for each of the tests in a large test suite.\nDesigning Go-Oracle involves collecting a diverse dataset of both passing and failing execution traces, with labelled concurrency bug type. We use the native Go execution tracer to collect traces, that captures a comprehensive array of execution events like creation, start and end of Go routines, events that block/unblock go routines (syscalls, channels, locks), network I/O related events, system calls and garbage collection. We then use these labelled traces to train a transformer model. This model effectively embeds the traces into a representation that holds essential information about the execution sequence, enabling subsequent classification as either passing or failing traces. It is worth noting that the paper's contribution lies in the classification of execution traces based on the presence or absence of concurrency bugs. Hence, during training, all failing traces are attributed to concurrency bugs, aligning with the focus of this research. However, the model is easily extendable to encompass classification for other bug types in the future.\nWe conducted an empirical study using eight subject programs from the GoBench repository [11] containing real-world Go concurrency bugs. The primary objective was to evaluate the effectiveness of Go-Oracle in classifying execution traces from subject programs that were not part of its"}, {"title": "II. BACKGROUND", "content": "In this section, we discuss basic concepts in Go programming and the concurrency mechanism utilised by Go.\nA. The Go Programming Language\nGo was first released in 2009 and gained popularity among developers for its simplicity, efficiency, and concurrency capabilities. The syntax of Go is similar to that of C, making it easy for developers who are familiar with C or C++ family languages. Go also has a garbage collector, which automatically frees up memory that is no longer in use, reducing the likelihood of memory leaks. With its growing community and numerous libraries, Go has become a popular language for building web applications and network servers.\nB. Concurrent Programming in Go\nGo is designed for concurrent programming in earnest: it treats concurrency as part of the language instead of an afterthought. Go provides two mechanisms for concurrent programming: communicating sequential processes (CSP) and concurrency through shared memory.\nCSP emphasises communicating between threads rather than sharing memory and provides concurrency mechanisms to enabled communication.\n\u2022 Goroutines are lightweight threads managed by the Go runtime and allow for concurrent execution of functions. They are extremely efficient, as many goroutines can run on a single operating system thread. Goroutines are created using the go keyword, which spawns a new goroutine to run the function in the background while the parent function continues to execute.\n\u2022 Channels are used for communication and synchronisation between goroutines. They allow for safe and efficient communication between goroutines by sending and receiving messages. Channels can be used to coordinate the execution of multiple goroutines and to share data between them. Together, goroutines and channels provide a powerful concurrency model that makes it easy to write efficient, parallel code in Go.\n\u2022 Select statements allow for the management of multiple channels. A goroutine can select between multiple channels and wait for the availability of a specific channel.\nGo also supports traditional shared memory accesses and provides various synchronisation primitives including Mutex (lock and unlock), condition variable, atomic read and write operations and a primitive to wait for multiple goroutines to finish their execution."}, {"title": "D. Go execution traces", "content": "The Go execution tracer [13] was released with Go 1.5 to allow for detailed profiling of Go programs. When enabled, it produces a compact file that encodes relevant events in a proprietary binary format, as seen in Listing 3."}, {"title": "III. RELATED WORK", "content": "In this section, we discuss existing work on program analysis, testing and automated test oracle for concurrency bugs in Go programs.\nA. Concurrency Bug Detection\nDetecting concurrency bugs has been studied by the research and industry community for decades and representative approaches include lockset-based [14] and happens-before analysis [15]. Static lockset based concurrency bug detection technique employs race-violation rules and checks whether locks are held correctly for all shared variable accesses. Lockset-based approaches have been successfully applied to analysing C [16] and Java [17], [18], [19], [20], [21] programs to detect data races. Happens-before based approach records read and write accesses to shared variables by tracking synchronisation events. If there exist two accesses, with one of them being write, to the same shared variable in an undetermined order, a data race error is reported.\nFor Go programs, Tu et al. [12] present the first study on concurrency bugs in Go programs and classify them into blocking and non-blocking bugs. Based on an industry-scale study on 2,100 microservices implemented in Go, Chabbi et al. [22] further report that the abundant usage of concurrency primitives and the language idioms themselves actually make Go programs prone to concurrency bugs.\nExisting static analysis tools [23], [24], [25], [26], [27], [12], [28], [29] suffer from generating false alarms and do not scale to larger real-world Go projects. For instance, Goat [24] fails on 70% of the evaluations on real-world Go projects and has a 30% false positive rate. In the open-source community, Vet [29] and StaticCheck [28] are two representative collections of static concurrency bug detectors for Go programs based on pattern matching and are specific to pre-defined bug patterns.\na) Dynamic Go Concurrency Bug Detectors::\nGFuzz [30] uses message reordering to proactively trigger concurrency bugs via order mutation, order prioritisation and runtime detection. However, it does not support other concurrency primitives such as locks and channel operators. Goleak[31] is a detection tool that focuses on the state of Go routines. For each Go routine, Goleak records a stack, which includes its state, creation function, and a full execution trace. As the program executes, Goleak gathers information about each Go routine. Based on the stack trace information for the Go routines, Goleak detects the presence of different concurrency bugs (deadlocks, blocking bugs, channel misuse,"}, {"title": "IV. METHODOLOGY", "content": "We present Go-Oracle, a deep learning oracle for Go concurrency test executions. The GO-Oracle model design has the following steps:\nStep 1: Instrument the PUT to gather traces when executing the test inputs.\nStep 2: Preprocess the traces to prune unnecessary information.\nStep 3: Encode execution trace vectors into embedding representations.\nStep 4: Design a Transformer-based NN that learns to classify execution traces as \u201cpassing\u201d or \u201cfailing\u201d.\nIn Figure 1, we illustrate the steps in our approach. We discuss each of the steps in the rest of this Section."}, {"title": "A. Instrument and Gather Traces", "content": "GoBench[11], is a comprehensive benchmark suite for Go concurrency bugs. It comprises a total of 185 bugs, categorized into two subsets: GoReal and GoKer. The GoReal subset includes 82 real-world concurrency bugs and the GoKer subset contains 103 synthetic bugs that aim to exploit specific vulnerabilities of Go code[12]. They exist in the form of minimal reproducible examples of the desired bug. To reproduce a GoReal bug, a Docker container clones the relevant repository, checks out the last commit known to contain the bug, and executes the test that triggers the bug. In this work, we modify GoBench to generate execution traces and label them based on the type of concurrency bug detected.\nGoBench uses four tools (goleak, go-deadlock, dingo-hunter, and the Go runtime's race detector Go-rd) to check whether the bug was reproduced on a test execution. Based on the results of these checks, it classifies the test run as positive (if a concurrency bug happened) or negative (if none did).\nTo maintain trace integrity without contamination from the testing environment (e.g., GoBench internal function calls), we carefully manage trace collection during the execution of the test binary. Listings 4 and 5 illustrate the precise points in the code where trace collection was incorporated. Additionally, two utility functions, namely PathToTrace (utilized in line 15 of Listing 4 and line 7 of Listing 5) and Trason (utilized in line 20 of Listing 4 and line 14 of Listing 5), play key roles. PathToTrace ensures that trace files are organized in a directory tree based on expected bug characteristics and test outcomes. On the other hand, Trason harnesses Go's internal trace package to parse trace files into JSON format for further processing."}, {"title": "B. Preprocessing", "content": "Every execution trace we collect is parsed into JSON format using Go's internal parser, which extracts two arrays from the execution trace file. The structure of these arrays is shown in Listing 6. The Events array, encapsulates information about stack status, the program counter, the called function arguments, return values and data types and other metadata related to the event. The Stacks array is an unordered collection of Frames, each of which represents the memory stack state at the time of its associated Event."}, {"title": "C. Neural Network Architecture", "content": "We use deep neural networks (DNN) to encode our runtime execution traces and classify them as pass or fail. First, we tokenize traces into numerical vectors. We allocate one token per string keyword in the execution trace's dictionary. For numerical values, we apply a digit by digit tokenization. We prefer this method over dedicating one token per distinct numerical value because they can be very high leading to a prohibitively large and sparse vocabulary.\nNext, we use an embedding layer followed by a transformer encoder architecture [43] to extract features from our encoded trace information. We select the transformer as the most appropriate deep learning architecture for this task for two reasons. First, attention-based architectures are the state of the art for sequence encoding tasks, such as our execution traces that are represented as a sequence of data types and fields. Second, accuracy with a transformer model scales better as the training volume increases compared to other sequential models such as the LSTM. The transformer's only limitation is its fixed sequence length limit which we set to 4,096 tokens. Execution traces of smaller length are padded to the sequence length. Those that exceed it are truncated. For the transformer encoder, we use 2 layers with 8 attention heads and an embedding dimension size of 256.\nThe transformer model processes execution traces and converts them into meaningful representations within a two-dimensional feature space. This encoding process is crucial for subsequent classification. Following the encoding step, a"}, {"title": "V. EXPERIMENTS", "content": "In our evaluation, we use eight repositories from GoBench, treating each repository as a distinct subject program. During training, we optimize model parameters tailored for classifying traces across seven of the eight subject programs. Subsequently, we evaluate the model's performance on unseen execution traces from the remaining, eighth program. We train and evaluate leaving out each of the eight programs. This evaluation approach emphasizes the adaptability and robustness of the model across a spectrum of different repositories. We describe the configurations used in training GO-Oracle, along with the specific parameters employed in both the training and evaluation phases, to ensure transparency of our approach. Additionally, we conduct an ablation study. This study meticulously analyzes the sensitivity of the model's performance to various components and features, shedding light on the factors that significantly influence its effectiveness."}, {"title": "A. Platforms", "content": "We train GO-Oracle and conduct all our experiments on two 64-bit systems each having one Intel Xeon E5-2620 16-core CPU, 2x Nvidia GeForce GTX 1080 GPU and 32 Gigabytes of RAM. We use Ubuntu 18.04, PyTorch 1.9.1 [44], CUDA version 11.4, Nvidia driver version 510.47.03 and Go version 1.20.1. GO-Tool is a Transformer-based architecture with a sequence length of 2,048 tokens, an embedding size of 128 parameters, 2 encoder layers and 2 attention heads."}, {"title": "B. Evaluation Setup", "content": "We collect 8 subject programs from GoBench repository. Table I provides an overview of the subject programs, number of passing and failing traces associated with them, along with a brief description. We instrument each repository and executed its included test cases to collect a total of 203 execution traces. We train GO-Tool for 2,500 steps using a batch size of 8 for each excluded subject program separately. We evaluate each of the trained model instances (with 7 of the 8 subject programs) on the execution traces that belong to the remaining unseen subject program. We use two metrics to measure the model's performance in classifying GO execution traces as passing or failing: True Negative Rate (TNR) that measures accuracy"}, {"title": "A. Missed Failing Traces", "content": "The detection of buggy traces during testing is of paramount importance. Despite Go-Oracle demonstrating high accuracies in classifying failing traces, it misclassified eight failing traces as passing, shown in Table III, across all eight subject programs. We found failing traces containing the bugs, grpc#3017 and kubernetes#81091, are duplicated, as they are part of both the GoKer and GoReal data sets, leading to their duplicate inclusion. Furthermore, for every bug that was not detected by the oracle, other bugs of the same characteristics were detected. An example of each is included in Table III for reference in the Alternative column. The remaining four undetected bugs in the last four rows of Table III stem from the Go-Real dataset, characterized by longer traces with a lot of irrelevant information that misleads Go-Oracle. Addressing this challenge entails enriching the training dataset with similar traces that helps the model learn to focus on the more important trace parts."}, {"title": "B. Ablation study", "content": "To gain deeper insights into how Go-Oracle encodes execution trace information and identifies significant trace segments related to passing or failing traces, we conduct an ablation study. Beginning with a randomly sampled dataset with 50 passing traces and 150 failing traces, we randomly allocate 20% of the traces for testing purposes, reserving the remaining 80% for model training. This separation allows for a comprehensive study without data contamination.\nWe commence by training the model using the complete execution trace information, setting the baseline accuracy for our ablation study. The baseline model achieves 90% accuracy in classifying both passing (9/10 traces classified correctly) and failing (27/30 traces classified correctly) traces. We then systematically retrain Go-Oracle, while using the same training and test data. During this process, we deliberately remove one section of the trace at a time to assess its influence on model accuracy.\nWe expect that the elimination of trace sections crucial to correctness to exert a significant influence on the model's accuracy. Figure 3 illustrates this impact, with each bar on the y-axis representing the removal of a different trace section. The zero line from the x-axis indicates traces with all the information. Our analysis reveals a consistent decline in accuracy for both failing and passing trace detection when trace sections are removed, illustrated by bars extending to the left of zero indicating a negative effect. This underscores the critical role of specific trace properties in accurate classification. Surprisingly, removing off marginally enhances failing trace detection accuracy by 3%. However, this improvement is deemed insignificant and likely attributed to noise given its small magnitude. Some notable results include:\n\u2022 Passing trace accuracy is most affected by removing P, the logical processor ID. Detection accuracy for passing falls from 90% when all trace information is used to 50% when P is removed. We believe this is because related Events happening simultaneously on the same logical processor are more likely to result in concurrency bugs.\n\u2022 The accuracy of failing trace detection in Go-Oracle is notably impacted by Ts, representing the Event's timestamp. Upon its removal, detection accuracy drops from 90% to 77%. Similarly, trace information using P and G (the Event's goroutine ID) also significantly influence failure trace detection, each resulting in an accuracy reduction of 10% when removed. This outcome"}, {"title": "C. Survey conducted with Go Developers", "content": "The difficulty in manually classifying tests for each of the subject Go programs as scored by the three developers is shown in Figure 4. Developer 1 (Dev#1) found it very easy to manually classify most tests as pass or fail across all subject programs. This was mainly due to his expertise in container development and familiarity with the Go subject programs that are used as a standard benchmark in industry. Dev#2 had slightly mixed ratings, finding tests for Kubernetes and Syncthing easy or very easy to classify. While, there is a mix of ratings for other subject programs with easy, medium and a few hard difficulty rating. Dev#3 found it harder to manually classify test outcomes when compared to the other two developers as seen by the mixed ratings across tests and programs in Figure 4. The increased difficulty encountered by Dev#3 was because of lack of familiarity with the subject programs, having only worked on the server side. Although, the difficulty ratings varied across developers, they all concurred that the time it took to inspect and classify tests was too long.\nWhen asked if they would use the Go-Oracle tool for test classification in place of manual classification, all three developers gave the Go-Oracle tool highest preference for all the subject programs. The developers were impressed by the accuracy of the Go-Oracle tool in predicting failing test outcomes. The lower accuracy in predicting passing tests and the overhead of inspecting the false positive cases did not cause serious concerns as the workload compared to inspecting all tests was still dramatically reduced. The developers felt certain the Go-Oracle tool would result in significant time savings in their routine testing process. They also expressed a strong desire for having Go-Oracle integrated into their workflow. The developers provided the following additional feedback (verbatim) on the current practice:\n\u2022 At ByteDance, tests similar to the survey are done in the form of end-to-end tests. Different from unit tests for functional testing that are usually a few test cases, we need to write a lot of end-to-end scenario test cases for Kubernetes-like projects.\n\u2022 Functions are created to check the final results of these tests. However, due to real-time networking issues (e.g. network speeds vary across test runs), we need to manually check the results.\n\u2022 When the network links become complicated, we need to enumerate different combinations of links and the manual inspection also becomes complicated.\n\u2022 When a model like Go-Oracle is able to predict the test results, even with its current accuracy, it will be very helpful in saving us time and we will greatly appreciate it in our workflow.\nIn summary, the survey results provided evidence that there is need for an automated test oracle when testing Go programs owing to the excessive time consumed by manual inspections and classifications of test results. The survey also showed that the Go-Oracle tool was welcomed by developers who felt it would save significant testing time and expressed a strong preference to integrate it into their workflow."}, {"title": "D. Comparison to SOTA Bug Detectors", "content": "Table IV presents a summary of the detection outcomes for three SOTA tools using the bugs in the GoKer dataset. We first"}, {"title": "VI. RESULTS", "content": "We took precautions to avoid data contamination during model training. Each model was trained separately, with all the traces from the subject program used in testing deliberately excluded from the training set. The evaluation results for each subject program is illustrated in Figure 2, showing the accuracy in classifying passing traces (TNR) and failing traces (TPR), as well as the total accuracy (identifying both passing and failing traces correctly).\nAcross subject programs, the model consistently demonstrates higher accuracy in detecting failing traces (86 \u2013 100%), underlining its efficacy in identifying bugs in the majority of evaluated programs. Go-Oracle achieves 100% TPR in classifying failing traces for 5 out of the 8 subject programs. The remaining 3 programs have a small number of misclassifications \u2013 a total of 8 misidentified failing traces as passing"}, {"title": "a) Blocking Bugs:", "content": "Goleak distinguishes itself through its comprehensive detection strategy, analyzing the state and stack trace of each go routine. This universal approach ensures robust detection capabilities across various bug types. Goleak"}, {"title": "b) Non-Blocking Bugs:", "content": "Goleak stands as the only tool equipped with detection ability among the three tools for this bug type. It employs modules for identifying misuse of channels and data races, exhibiting high effectiveness. However, Goleak's performance in uncovering nonblocking bugs arising from other causes is less satisfactory."}, {"title": "c) Comparison against Go-Oracle:", "content": "As discussed in Section VI-A and seen in Table III, Go-Oracle fails to detect a total of 8 bugs, considering both the GoReal and GoKer datasets. Notably, 2 bugs are duplicated between these datasets, resulting in a total of 6 unique missed bugs, all of which are present in the GoReal dataset. In the GoKer dataset (utilized by the state-of-the-art tools in Table IV), Go-Oracle misses only 2 bugs. One is associated with the subcause Double Locking, and the other pertains to the WaitGroup subcause. Intriguingly, all other GoKer bugs linked to failing traces are successfully detected by Go-Oracle.\nThis stands in stark contrast to state-of-the-art tools, where the maximum detection rate is 75% for Goleak, still missing 26 bugs in GoKer. Following this, GoAT detects 51% (missing 50 bugs), and finally, GFuzz only identifies 21% of the bugs (missing 81 bugs). In summary, Go-Oracle far outperforms SOTA dynamic concurrency bug detectors in accurately identifying failing traces with concurrency bugs."}, {"title": "E. Threats To Validity", "content": "Due to the inherent challenge in detecting and labeling Go concurrency bugs, the dataset used to train our model is exceedingly limited, solely sourced from GoBench. This limitation, understandably, impacts the classification accuracy of Go-Oracle, contributing to the observed lower True Negative Rate (TNR) in our experiments. In the future, we will augment GoBench with an enriched dataset containing more labeled data for both passing and failing traces to mitigate this limitation.\nFurthermore, with the increasing prevalence of Go programs, we anticipate that more examples of passing and failing traces will become available in open-source repositories. This broader availability of diverse data will be instrumental in enhancing the training and robustness of Go-Oracle for improved performance. Another aspect impacting the validity of our results is the accuracy of the labeled data, upon which Go-Oracle's accuracy is contingent. To mitigate this potential threat, we have incorporated a vetted benchmark of concurrency bugs that has been used in previous studies. This ensures that the labeled data used in training and evaluation is reliable, reducing the risk of inaccuracies influencing the outcomes of our study.\nFinally, our survey was conducted with just three developers at Bytedance making it harder to generalise the results. It is, however, worth noting that the developers in the survey were experienced with Go programs in an industry setting and routinely deployed them at a large scale. Their feedback is still relevant to other industry developers."}, {"title": "VII. CONCLUSION", "content": "In this paper, we present Go-Oracle, an automated test oracle designed to classify test executions from Go routines into passing and failing traces, specifically focusing on identifying concurrency bugs. Go-Oracle is trained using labeled passing and failing traces from Go routines, utilizing a transformer to summarize trace information. The trace summaries are then fed into a multilayer perceptron for classification into pass or fail categories. We evaluated the effectiveness of Go-Oracle using eight subject programs from GoBench, containing both real and synthetic programs with concurrency bugs. Notably, Go-Oracle demonstrated impressive accuracy in classifying failing traces (average of 96%), outperforming three state-of-the-art tools that monitor concurrency bugs from traces that only have a maximum bug detection accuracy of 75%. A survey conducted with three developers at Bytedance revealed that manually classifying Go test outputs was cumbersome and time consuming, and that was the current practice followed. The developers expressed a strong preference for Go-Oracle's test classification and would consider integrating it into their testing pipeline."}]}