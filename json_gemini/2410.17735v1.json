{"title": "New insight in cervical cancer diagnosis using convolution neural network architecture", "authors": ["Ach Khozaimi", "Wayan Firdaus Mahmudy"], "abstract": "The Pap smear is a screening method for early cervical cancer diagnosis. The selection of the right optimizer in the convolutional neural network (CNN) model is key to the success of the CNN in image classification, including the classification of cervical cancer Pap smear images. In this study, stochastic gradient descent (SGD), root mean square propagation (RMSprop), Adam, AdaGrad, AdaDelta, Adamax, and Nadam optimizers were used to classify cervical cancer Pap smear images from the SipakMed dataset. Resnet-18, Resnet-34, and VGG-16 are the CNN architectures used in this study, and each architecture uses a transfer-learning model. Based on the test results, we conclude that the transfer learning model performs better on all CNNs and optimization techniques and that in the transfer learning model, the optimization has little influence on the training of the model. Adamax, with accuracy values of 72.8% and 66.8%, had the best accuracy for the VGG-16 and Resnet-18 architectures, respectively. Resnet-34 had 54.0%. This is 0.034% lower than Nadam. Overall, Adamax is a suitable optimizer for CNN in cervical cancer classification on Resnet-18, Resnet-34, and VGG-16 architectures. This study provides new insights into the configuration of CNN models for Pap smear image analysis.", "sections": [{"title": "1. INTRODUCTION", "content": "Cancer is the second leading cause of death worldwide after heart attack [1]. The global cancer observatory (GCO) reported that in 2018, there were an estimated 570,000 new cases of cervical cancer and 311,000 deaths from the disease. Without efforts to reduce incidence rates, cervical cancer will kill an estimated 460,000 people annually by 2040 [2]. Approximately 90% of cervical cancer deaths occur in the developing world [3]. In Indonesia, cervical cancer is the second leading cause of death after breast cancer [4]. Cervical cancer screening is one of the efforts to reduce the risk of death [5]. Cervical cancer screening is aimed at detecting abnormal cell growth and proliferation. Malignant cells that invade one organ have the potential to spread to other organs [6]. Papanicolaou smear is an important screening method for detecting and classifying cervical cancer [3].\nThere are five steps in the process of machine-learning-based Pap smear analysis: image acquisition, preprocessing, segmentation, feature extraction, and classification [7]. At each stage, the following algorithms are required: histogram equalization [8] for preprocessing and Perona Malik diffusion for noise reduction [9]; global thresholding [10] for segmenting the region of interest; fuzzy c-mean for feature extraction; and"}, {"title": "2. METHOD", "content": "This study used a dataset of Pap smear images derived from Kaggle, namely the SipakMed dataset. The SipakMed image is divided into five classes based on the degree of malignancy of cervical cancer and consists of 4049 cropped cell images. This dataset was then divided into three parts: 80% of the training data, 10% of the test data, and 10% of the validation data. Preprocessing was performed on the Pap smear images. It consists of image resizing to 224\u00d7224 pixels and image augmentation.\nThis study uses three CNN architectures, VGG16, Resnet 18, and Resnet 34, with and without TL.\nThe RMSProp, Adam, SGD, Adadelta, Adagrad, Adamax, and Nadam optimizer techniques are compared in this study. The parameter settings for training the CNN model included a batch size of 16, random seed of 1, 30 epochs, and learning rate of 0.001. The performance metrics used in this research were the accuracy and"}, {"title": "2.1. Convolution neural network", "content": "CNN are a specific type of artificial neural network that can handle structured data, particularly images [14], [19]. CNNs share a structural resemblance with regular feedforward neural networks. The CNN architecture consists of a convolutional layer, pooling layer, and fully connected layers [12]. However, they possess additional layers that enhance their proficiency in addressing computer vision problems. CNNs have been widely and successfully used to classify medical images [13], [20]. CNN modifications have also been performed to improve model performance, such as hybrid CNN models [21]-[25], CNN layer arrangement [25], [26], and CNN hyperparameter optimization [27]-[30].\nWhen implementing a CNN, optimization techniques are used to optimally manage network learning by minimizing the loss function of the CNN, thereby improving training efficiency [31]. This helps CNN networks learn quickly and efficiently and achieve better results in image recognition tasks. Some optimization techniques for implementing CNN learning include SGD, RMSprop, Adam, AdaGrad, AdaDelta, Adamax, and Nadam."}, {"title": "2.2. Stochastic gradient descent", "content": "The SGD is a fundamental optimization method. It has been employed in neural networks. SGD utilizes the gradient of slack function J with respect to each parameter to update the model. This was achieved by altering the weights of the parameters during each iteration [31]. SGD updates the parameters on each training data x(i) serta label y (i) with the following formula, where \u03b7 is the learning rate as:\n$\\theta = \\theta - \\eta . \\nabla_{\\theta}J(\\theta; x^{(i)}y^{(i)})$                                       (1)\nThe SGD has several advantages. These include computational efficiency, fast convergence, the regularization effect, and the ability to escape from local minima. The SGD has several important applications. These methods include deep learning, natural language processing, and binary classification. The weakness of this technique is that it is sensitive to the learning rate, can be trapped in a local minimum, is inconsistent, susceptible to noise, and requires continuous updating of the weight values [16]. In addition to these weaknesses, SGD has several advantages. First, SGD is fast because it does not use the entire dataset for training, thereby increasing computational speed. Second, to minimize the occurrence of local minima, SGD can adaptively modify the estimation of the gradient matrix for each parameter, both in terms of first- and second-order, based on the loss function [32]. Several studies have used SGD for CNN optimizers [33]-[36]."}, {"title": "2.3. Root mean square propagation", "content": "RMSProp is an optimization approach employed for training neural networks. This approach is an extension of gradient descent optimization that incorporates a decaying average of partial gradients to adjust the step size for each parameter. Let's assume you have a parameter @ that you are optimizing, and gt is the gradient of the loss function with respect to @ at time step t. The update rule for RMSprop is as:\n$E[g\u00b2]t = 0.9E[g\u00b2]t+1 + 0.1g\u00b2$\n$\\Theta_{t+1,i} = \\Theta_{t,i} - \\frac{\\eta}{\\sqrt{E[g_{t+1,i}^{2}] + \\varepsilon}}$     (3)\n$E [g\u00b2] is the exponentially decaying average of past squared gradients., \u03b7 learning rate, e is a small constant added for numerical stability (usually around) 10-8. The algorithm adapts the learning rate for each parameter based on the magnitude of recent gradients. If a parameter has been receiving large gradients, its effective learning rate is reduced, and vice versa. This aids in faster convergence and more stable training [36], [37]."}, {"title": "2.4. Adaptive moment estimation", "content": "Adam optimizer is a widely used optimization technique in neural network training. The Adam algorithm integrates concepts from the RMSprop and momentum algorithms to offer a dynamic learning rate and momentum for every parameter. This enhances the efficacy of gradient-based optimization methods. Adam calculates and tracks two types of moving averages for each parameter: the first moment, which represents the mean, and the second moment, which represents the uncentered variance [38], [39]. The purpose of these moving averages was to dynamically modify the learning rates for each parameter. Formula for Adam:\nLet t be the current iteration, a is learning rate, B\u2081 and B\u2082 be the decay rates for the moment estimates, and a be a small constant for numerical stability (usually set to 10-8). Initial first moment estimate (m = 0 ); vo = 0 (initial second moment estimate), t = 0. The update rule for Adam is as:\nm\u2081 = \u03b2\u2081.mt \u2212 1 + (1 \u2212 \u03b2\u2081). gt\n(4)\nVt\n= \u03b22. vt\u22121 + (1 \u2212 \u03b2\u2082). gt\n(5)\ngt is the gradient of the loss with respect to the parameter at time t.\nm\u2081 = mt/1-\u03b2\n(6)\nVt\n= m\u2081/1 - \u03b2\n(7)\nThis step corrects the bias introduced by the moving averages at the beginning of training. In (6) and (7) used for bias correction as (8).\n\u03b8\u03c4 +1,1 = \u03b8\u03c4,\u03af\n.\n\u221avt+E"}, {"title": "2.5. Adagrad", "content": "Adagrad, also known as the adaptive gradient, is a frequently employed optimization strategy in machine learning, specifically for training neural networks. Adagrad adjusts the learning rate of each parameter by proportionally scaling them according to the past squared gradients of the parameters. Consequently, parameters linked to rare features receive more substantial adjustments, whereas those linked to common features receive more modest updates [42]. Adagrad maintains a per-parameter learning rate that adapts over time based on the historical gradient information for each parameter. Formula for Adagrad as:\n\u03b8t+1 = \u03b8t -\\frac{\\eta}{\\sqrt{G_t + \\epsilon}}.g_t                                       (9)\ngt is a diagonal matrix where each diagonal element corresponds to the sum of squared gradients for a particular parameter up to time t. n is the learning rate.\nAdagrad adjusted the learning rates by considering the past squared gradients of each parameter. This results in more substantial updates to parameters linked to rare features, and fewer updates to parameters linked to common features. However, one drawback of Adagrad is that the learning rates may become very small over time, leading to a slow convergence. This issue has led to the development of other adaptive optimization algorithms such as RMSprop and Adam [43]."}, {"title": "2.6. AdaDelta", "content": "Adadelta is an optimization algorithm designed to address some of the limitations of Adagrad, specifically the issue of continually decreasing learning rates. Adadelta dynamically adjusts the learning rates during training, allowing it to adapt to the changing characteristics of the optimization landscape. Adadelta employed a method to calculate the average of the squared gradients from previous iterations to dynamically adjust the learning rates for each parameter [16], [44]. The algorithm eliminates the need for an initial learning rate and tracks the running average of the squared parameter updates.\nRMS[g\u00b2]t = E[g\u00b2]t = \u03b2.E[g\u00b2]t+1 + (1 \u2212 \u03b2). g\u1ec9                              (10)\nE[g\u00b2] is a running average of squared gradients.\n\u0394\u03b8\u03b5\nRMS|40|t+1\nRMSgt gt\n(11)\nThis (11) is for compute update value.\n\u03b8(t+1) = \u03b8\u03c4 + \u0394\u03b8\u03b5\n(12)\nThis formula (12) is used to update parameter.\nAdadelta dynamically adjusts the learning rates based on past squared gradients and the past squared parameter updates. This aids in alleviating the problems of rapidly increasing or decreasing learning rates and enables a more stable and efficient training of neural networks. Some studies used the Adadelta optimizer for the CNN model [34], [43]."}, {"title": "2.7. Adamax", "content": "Adamax is a modified version of the Adam optimization method that specifically targets the L-infinity norm (maximum norm) of the gradients in the update rule. Adamax, like Adam, calculates the moving averages of the first moment (mean) and the exponentially decaying average of the infinity norm of the gradients. Adamax adjusts the learning rates for each parameter by considering both the mean and maximum norms of the gradients. Let t be the current iteration, \u03b8 be the parameter being optimized, gt be the gradient of the loss with respect to eat time t, \u03b2\u2081 and \u03b2\u2082 be the decay rates for the moment estimates.\n\u03b8\u03c4 +1 = \u03b8\u03c4\n\ub044 mt\n(13)\nut\nut = \u03b22\u00b0. vt + (1 \u2212 \u03b22\u00b0).|gt|\u221e = max(\u03b22. vt-1,|gt|)\n(14)"}, {"title": "2.8. Nadam", "content": "Nadam is an optimization algorithm that merges concepts from the nesterov accelerated gradient (NAG) and Adam optimizer. This is an abbreviation for the nesterov-accelerated Adam estimation. This incorporates the advantages of the nesterov momentum and adaptive learning rates. Nadam is a variant of the Adam optimization algorithm that integrates Nesterov momentum into the parameter update process. This modification aims to improve the convergence speed and generalization. This is the formula for the Nadam optimizer.\n\u03b8t +1 = \u03b8t -\\frac{\\eta}{\\sqrt{Gt+\\epsilon}} (\\beta_1 . m_t + \\frac{(1-\\beta_1) g_t}{1-\\beta^t_1})                                                                  (15)\n@ be the parameter being optimized, gt be the gradient of the loss with respect to eat time t, B\u2081 and B2 be the decay rates for the moment estimates, e be a small constant for numerical stability (usually set to 10-8), and n be the learning rate. Nadam incorporated the Nesterov momentum into the update rule of the Adam optimizer. This combination is intended to provide faster convergence and better handling of saddle points compared with standard momentum methods. This is particularly useful for training deep neural networks [47]."}, {"title": "3. RESULTS AND DISCUSSION", "content": "In this study, we investigated the performance of different optimizers in cervical cancer diagnosis using CNNs and compared their efficacy. Cervical cancer remains a significant global public health concern, and its early detection plays a crucial role in improving patient outcomes. CNNs have shown promise in automating the diagnosis process, but the choice of optimizer can significantly affect model performance. Optimizer performance test results for cervical cancer detection using CNN on VGG-16, ResNet-18, and ResNet-34 architectures with and without TL are as follows:"}, {"title": "3.1. General result", "content": "In addition, we were able to determine which of these models had been trained on more data than those used in our study (SipakMed), so we were able to determine which of these models had been trained on more data than those used in our study (SipakMed). The results of this test also show that the test for the TL CNN architecture is stable for all optimizer types used. In other words, if we use a trained model or TL, our optimizer will not have a significant effect on that model."}, {"title": "3.2. VGG-16", "content": "The test results for the VGG-16 architecture are listed in Table 1. The Nadam optimizer with TL achieved an accuracy value of 88.6%. This was followed by Adamax, with an accuracy of 85.6%. When testing the VGG16 model without TL, the Adamax optimizer performed better with an accuracy value of 66.8%, whereas the second position was occupied by Nadam with an accuracy value of 66.1%. The difference in the accuracy between Nadam and Adamax was 0.7%. The test results on this VGG16 architecture show that adadelta performs worst for models without TL, with an accuracy value of 0.205, whereas for models using TL, RMSProp performs worst, with an accuracy value of 0.730."}, {"title": "3.3. Resnet-18", "content": "The next test was performed on the Resnet-18 architecture, the results are presented in Table 2. Adamax, with accuracy values of 0.884 and 0.728, respectively, had the best performance compared with the"}, {"title": "3.4. Resnet-34", "content": "Table 3 presents a detailed comparison of various optimization algorithms based on the key performance metrics for the Resnet-34 architecture. The metrics include accuracy, which reflects the proportion of correctly classified instances and losses and serves as a measure of the model's error. Among the featured optimizers, Adam had the lowest accuracy at 0.205 and a corresponding loss of 0.051. By contrast, Adagrad boasts the highest accuracy at 0.507, coupled with a relatively low loss of 0.040. Notably, the TL results indicate how well these optimizers adapt to a related task. Adam scenarios with an accuracy (TL) of 0.850 and a minimal loss of 0.014, reinforcing its robustness across diverse tasks. Conversely, Nadam exhibited a significant increase in loss during TL, thereby highlighting its potential limitations. These findings underscore the critical role of optimizer selection in shaping model performance, with Adam emerging as a favorable choice for both the initial task and TL contexts. Although slightly less accurate than Nadam, Adamax is an optimizer that can be an option for the Resnet-34 architecture without TL."}, {"title": "4. CONCLUSION", "content": "This study investigated the importance of optimizer selection in CNN cervical cancer image classification models. The results highlight the importance of TL, showing consistently better performance across all CNN topologies and optimization techniques. Adamax was shown to be the most successful optimizer, achieving the highest accuracy of 72.8% on the VGG-16 architecture and 66.8% on the Resnet-18 design. On Resnet-34, Adamax achieved an accuracy of 54.0% but was slightly outperformed by Nadam by 0.034%. Adamax is a reliable and effective optimizer for CNNs in the classification of cervical cancer using Pap smear images on Resnet-18, Resnet-34, and VGG-16 architectures. Future research can investigate the effect of noise and contrast on the performance of optimizers by reducing the noise and increasing the contrast in the Pap smear images."}]}