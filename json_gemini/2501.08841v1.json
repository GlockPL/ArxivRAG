{"title": "Exploring Task-Level Optimal Prompts for Visual In-Context Learning", "authors": ["Yan Zhu", "Huan Ma", "Changqing Zhang"], "abstract": "With the development of Vision Foundation Models (VFMs) in recent years, Visual In-Context Learning (VICL) has become a better choice compared to modifying models in most scenarios. Different from retraining or fine-tuning model, VICL does not require modifications to the model's weights or architecture, and only needs a prompt with demonstrations to teach VFM how to solve tasks. Currently, significant computational cost for finding optimal prompts for every test sample hinders the deployment of VICL, as determining which demonstrations to use for constructing prompts is very costly. In this paper, however, we find a counterintuitive phenomenon that most test samples actually achieve optimal performance under the same prompts, and searching for sample-level prompts only costs more time but results in completely identical prompts. Therefore, we propose task-level prompting to reduce the cost of searching for prompts during the inference stage and introduce two time-saving yet effective task-level prompt search strategies. Extensive experimental results show that our proposed method can identify near-optimal prompts and reach the best VICL performance with a minimal cost that prior work has never achieved.", "sections": [{"title": "Introduction", "content": "With the development of Vision Foundation Models(VFMs), many tasks in visual scenes no longer require training new models but can be solved by VLFs provided by model service providers, which are more affordable and convenient (Li et al. 2023; Wu, Sun, and Ouyang 2023). However, the performance of directly deploying VFMs for specific tasks is often unsatisfactory, and task-specific adaptation is necessary for better performance. In comparison to modifying model weights, Visual In-Context Learning (VICL) (Yang et al. 2024; Chen et al. 2024) is a better choice. Unlike retraining or fine-tuning models, VICL does not necessitate modifying the model's weights or architecture, but only needs to teach VFMs how to solve problems using prompts with a set of demonstrations. However, VICL does not always achieve good results under arbitrary prompts, so determining the appropriate demonstrations to construct prompts is the key challenge in enhancing VICL performance (Li et al. 2024; Dedhia et al. 2024).\nCurrently, there are primarily two ways to select demonstrations for constructing prompts: the rule-guided strategy and the reward-model-based strategy. The former strategy involves selecting demonstrations based on rules, like UnsupPR (Zhang, Zhou, and Liu 2023) and prompt-SelF (Sun et al. 2023), which calculate the similarity between the query samples and the training samples using features extracted by the pretrained CLIP (Radford et al. 2021) model to select prompts. Although this type of method is very simple, these heuristic methods struggle to guarantee performance. To solve the issue, another strategy is to train a reward model that can predict performance under different prompts. For example, both SupPR (Zhang, Zhou, and Liu 2023) and InMeMo (Zhang et al. 2024) train an additional reward model on the validation set to predict the compatibility between prompts and query samples. However, to obtain such a reward model, it requires a large amount of labeled data and incurs a high computational cost, or the reward model will suffer from severe overfitting. Unfortunately, this high computational cost severely hinders the deployment of VICL.\nIs it really necessary to spend a significant amount of computation to find the optimal prompt for each individual sample? Perhaps it is not actually needed. As shown in Fig. 1(d), we find that under a specific task, the prompts resulting in the best performance for different samples are always the same. The prompt that performs best on the task, even if it cannot achieve optimal performance for some samples, can still obtain relatively good results compared with most prompts. In other words, there is no need to spend a massive amount of computational effort at the risk of overfitting to select different demonstrations to form a prompt for distinct samples (experimental results show that using a single prompt for all samples performs even better than those costly sample-level methods).\nIn this paper, we propose a reward-based, training-free approach to find the optimal task-level prompt. This strategy not only lessens the time taken during inference but also sustains performance when compared to more time-consuming methods. Specifically, we present two new strategies for effectively searching for the best demonstrations: (1) Top-K strategy and (2) Greedy search strategy. We concentrate on a general setting where a labeled set of size N is provided. The aim of our strategies is to carry out combinatorial optimization over this set to discover optimal demonstrations. In particular, Top-K strategy uses a straightforward method that first measures the performance of each individual demonstration (i.e., one-shot prompting) and then picks the top K best demonstrations to create the final prompts. Note that Top-K strategy assumes that the optimal prompt is typically built from demonstrations that perform well when used on their own. The Greedy search strategy follows the standard greedy search procedure, finding the best solution by making the best local choices at each step. At each step of the algorithm, the chosen demonstration is the one that allows the updated prompts to achieve the best performance.\nTo evaluate the effectiveness of our strategies, we conduct extensive experiments on various downstream tasks, such as foreground segmentation, single object detection, and colorization. Our results indicate that our method can significantly enhance the VFM's in-context learning performance in an effective and interpretable manner. The overall contribution is summarized as follows:\n\u2022 We introduce a task-level prompt to avoid the significant computational cost and the risk of overfitting associated with sample-level methods, which select different demonstrations for different samples.\n\u2022 We propose two time-saving and effective prompt search strategies to identify near-optimal prompts and achieve SOTA performance with minimal cost, which has not been achieved in prior work.\n\u2022 The effectiveness of these two strategies is demonstrated in various tasks. While saving more than 98% of the prompt searching time, consistent relative improvements of over 6.2% are observed across different downstream tasks compared to state-of-the-art methods."}, {"title": "Related works", "content": "The emergence of large language models (LLMs) like GPT-3 (Brown et al. 2020), BLOOM (Workshop et al. 2022), and LLaMA (Touvron et al. 2023) introduces a new learning paradigm, In-Context Learning(ICL) (Li, Tang, and Mei 2019; Li et al. 2022; Wang et al. 2024; Baldassini et al. 2024), which refers to the process of conditioning an LLM to solve various downstream NLP tasks using prompts constructed from a few demonstration input-output pairs (Liu et al. 2022; Cho et al. 2023; Ma et al. 2024a) (i.e., few-shot prompting). In visual domain, MAE-VQGAN (Bar et al. 2022) utilizes the model's grid inpainting capability to propose the first visual ICL model. Similarly, Painter (Wang et al. 2023a) performs standard masked image modeling on the stitch of input and output image pairs to train an ICL model. Additionally, SegGPT (Wang et al. 2023b), a variant of Painter, enhances the segmentation ability of the ICL model by randomly coloring similar semantic categories or objects. Previous works demonstrate the effectiveness of visual ICL models, and that selecting appropriate prompts ensures the model's understanding of task knowledge."}, {"title": "Visual Prompt Selection", "content": "To fully leverage the powerful reasoning capabilities of visual context learning models like MAE-VQGAN (Bar et al. 2022) and Painter (Wang et al. 2023a), researchers are dedicated to exploring new algorithms for selecting appropriate prompts for different query samples(tasks) (Huang et al. 2023; Ma et al. 2024b). These visual prompt selection methods can be divided into two categories, which are reward-model-based strategy and the rule-guided strategy:\n\u2022 Rule-guided strategy methods include UnsupPR (Zhang, Zhou, and Liu 2023) and prompt-SelF (Sun et al. 2023). Both algorithms calculate the similarity between the query samples and the training samples using features extracted by the pretrained CLIP (Radford et al. 2021) model to select demonstrations. The latter enhances the selected prompts to make full use of prompt information. However, such unsupervised algorithms do not utilize the labeled information of the training set and do not introduce task content constraints in the selection process, thus limiting the performance of the algorithms.\n\u2022 Reward-model-based strategy methods include SupPR (Zhang, Zhou, and Liu 2023) and In-MeMo (Zhang et al. 2024). Both algorithms train an additional scoring model on the training set to predict the compatibility between prompts and query samples. The latter proposes a prompt enhancer to improve the prompts and obtain a higher-performing scoring model. However, such supervised algorithms are costly to train, require a large amount of labeled data, and are prone to overfitting when the training samples are too few.\nTherefore, to fully utilize the labeled information under few-shot supervision data, this paper proposes two simple greedy task-level prompt selection methods, which are Top-K prompt selection method and Greedy prompt selection method. The former has a time complexity of O(N),and the latter has a time complexity of O(N2) in the worst case."}, {"title": "Methods", "content": "Let $S = \\{(x_i, Y_i)\\}_{i=1}^N$ be a validation set consisting of N image-label pairs, where $x_i$ denotes an image, and $Y_i$ is the corresponding label (e.g., 0/1 masks in a segmentation task). The target of VICL is to select a subset of samples from S to create a demonstration set, denoted as P = $\\{(x_i, Y_i)\\}_{i=1}^K$, which is used to prompt a pretrained foundation model f. This prompt aims to achieve the following goal: given a new query sample (xq, yq), the foundation model should generate a prediction $\\hat{y}_q$ = f(P, xq) for Xq that is as close as possible to the ground-truth yq, which can be formally represented as minimizing L($\\hat{y}_q$, Yq), where L(\u00b7) denotes the loss metric used for various specific tasks."}, {"title": "Sample-level Prompt", "content": "Sample-level prompt selection methods aim to find a $P^*$ for each query sample:\n$P^* = \\underset{P \\subset S}{\\text{argmin }} L(f(P,x_q),Y_q), $                                                                                                                                 (1)\nSince yg is unknown, previous works (Zhang, Zhou, and Liu 2023; Zhang et al. 2024; Sun et al. 2023) focus on constructing a scoring function g (which can be a manually set rule, like IOU, accuracy and so on, or parameters obtained from supervised learning) to automatically select the most suitable example(s) from the validation dataset S for a query sample xq. The prompt selection strategy is:\n$x^* = \\underset{x \\in S}{\\text{argmax }} g(x,x_q).$                                                                                                                                               (2)\nThese methods rank the training examples based on their scores and choose the top-K example pairs. When K = 1, they choose the optimal example pair as the prompt, $P_q$ = {($x^*$, $y^*$)}.\nSample-level requires a significant amount of time to obtain a reward model (scoring model) and evaluate each query sample before VICL, which hinders the flexible deployment of VICL. However, these efforts seem to yield little benefit. In previous works, researchers find that differences in prompts can lead to significant variations in final performance, and intuitively search suitable prompts for different query samples. However, we discover a counter-intuitive phenomenon: although different prompts can cause dramatic changes in VICL's performance (as shown in Fig. 1(b)), the prompts that perform well on every different query sample are always the same (as shown in Fig. 5). Therefore, we only need to spend a small amount of computational cost to find the best task-level prompt, rather than spending a large amount of computational cost to determine the same prompt for all samples individually."}, {"title": "Task-level Prompt", "content": "As the prompts that perform well on every different query sample are always the same, we propose the objective of task-level prompt selection methods to construct a demonstration set P from labeled data S shared across different query samples xq:\n$P^* = \\underset{P \\subset S}{\\text{argmin }} E_D L(f(P, x_q), Y_q),$                                                                                                                            (3)\nwhere D includes all the unseen query samples, namely (xq, yq) \u2208 D. Since obtaining D is difficult, we substitute D with known data S based on the generalization of dataset:\n$P^* = \\underset{P \\subset S}{\\text{argmin }} \\sum_{S-P} L(f(P, x_q), y_q),$                                                                                                    (4)\nwhere S \u2260 P, and (xq, yq) \u2208 S \u2013 P. The most straightforward approach in this method is to obtain all possible combinations of P from S and use the performance of P on S \u2013 P to obtain loss.\nHowever, this approach still has a time complexity that is excessively high, with worst-case and best-case scenarios both being $O(2^N)$ (we integrate the features of prompt set by summation pooling without considering the effect of order, and it will be O(N!) when considering order), making it difficult to achieve the optimal solution. Inspired by the method proposed in (Ma et al. 2024a), which aims to address the prompt selection in LLM, this paper extends two simple task-level prompt selection methods, which are Top-K prompt selection method and Greedy prompt selection method. The former has a time complexity of O(N),and the latter has a time complexity of O(N2) in the worst case."}, {"title": "Top-K Prompt Selection Method", "content": "The main idea of our Top-K Prompt Selection Method proposed in this section is to simplify the basic idea of task-level prompt selection by using prior human knowledge to constrain the demonstration sets obtained from S, appropriately reducing the combination possibilities to achieve a reduction in time complexity. Specifically, this means moving from using $O(2^N)$ time to obtain the optimal demonstration set to using O(N) time to obtain the optimal single prompt:\n$x^* = \\underset{x \\in S}{\\text{argmin }} \\sum_{S-\\text{\\{x\\}}} L(f(\\{x\\},x_q),Y_q),$                                                                                                                          (5)\nwhere (xq, yq) \u2208 S\u2212{x}. However, for a given task, a single prompt sometimes cannot provide enough information to complete the task, and multiple prompts are needed. Therefore, to obtain a demonstration set composed of multiple prompts, we sort the scores of the single prompts and select the top K prompts to form the demonstration set. The specific algorithm is shown in Algorithm 1.\nNevertheless, the Top-K prompt selection method heavily relies on the choice of the hyperparameter K. When K is too small, the information may be insufficient; when K is too large, it may have a negative effect (as shown in Fig. 3). To automate the determination of the length of the demonstration set, this paper proposes a greedy prompt selection method."}, {"title": "Greedy Prompt Selection Method", "content": "Our Greedy Prompt Selection Method proposed in this section follows the basic greedy approach, where at each stage, the current optimal prompt set is identified to guide the next stage's operation. This involves selecting the most promising new sample to add to the existing demonstration set based on:\n$x_{\\text{greedy }} = \\underset{x\\in (S-P)}{\\text{argmin }} \\sum_{S-P-\\{x\\}} L(f(P + \\{x\\},x_q), y_q),$                                                                           (6)\nwhere (xq, yq) \u2208 S \u2212 P \u2212 {x}. Although the new sample represents the optimal solution for the current demonstration set, simply increasing the quantity of prompts does not necessarily improve performance and can even have adverse effects (as shown in Fig. 3). Therefore, it's crucial to compare the score $a_{new}$ = g(P + {x}, S \u2013 P \u2013 {x}) obtained from adding the new sample with the score $a_{ori}$ = g(P,S \u2013 P) of the original demonstration set. This comparison facilitates early stopping and pruning of the algorithm, aiming to achieve global optimal solution through local optimal solutions. When $a_{ori}$ < $a_{new}$, it indicates that the new demonstration set performs better on the validation set, allowing the new sample to be included in demonstration set for the next iteration:\nP = P + {$x_{greedy}$}.                                                                                                                                                                              (7)\nHowever, when $a_{ori}$ > $a_{new}$, it suggests that adding the locally optimal sample no longer enhances the performance on the validation set, prompting the algorithm to halt the iteration and output the current demonstration set as the solution, $P^*$ = P. The specific algorithm is illustrated in Algorithm 2."}, {"title": "Experiments", "content": "In this section, we discuss the performance comparison between the two proposed task-level prompt selection methods and other unsupervised prompt selection methods, as well as the performance impact of different methods for constructing S. Additionally, we explore the necessity of using a greedy strategy to achieve a global solution, and conduct fine-grained, in-depth experiments starting from individual samples."}, {"title": "Setup", "content": "According to (Bar et al. 2022; Zhang, Zhou, and Liu 2023; Sun et al. 2023), we conduct few-shot evaluations on three out-of-distribution (OOD) computer vision tasks:\n\u2022 Foreground Segmentation. We use the Pascal-5i (Shaban et al. 2017) dataset, which is comprised of 4 different image splits, where each split contains data from 5 categories. For evaluation, we report the mean Intersection Over Union (mIOU) metric.\n\u2022 Single Object Detection. We use the Pascal VOC 2012 (Everingham et al. 2015) dataset, which contains images and their associated detection boxes. The evaluation method is similar to Foreground Segmentation, and we report the mIOU metric.\n\u2022 Colorization. We use a subset of the ImageNet (Russakovsky et al. 2015) dataset, which contains data from 1000 categories. We randomly sample 50,000 example pairs and image queries from the ImageNet validation set. For evaluation, we report the Mean Squared Error (MSE) loss (scaled up by a factor of 100)."}, {"title": "Results", "content": "We conduct experiments on different settings and report the results of three runs (with seed = {0,1,2}). In Tab. 1, it presents the main results of the experiments. The following findings can be observed: (1) Our methods achieve state-of-the-art performance across various datasets, both when compared to sample-level methods and task-level methods, demonstrating the effectiveness of our approaches. In Tab. 1, our methods achieve optimal results in detection and segmentation tasks, and even achieve a global optimal solution in the coloring task. (2) Under certain conditions, our methods can reach the global optimal solution, with overall results very close to Oracle. In the detection task, our Greedy method is less than 3% away from Oracle, and in the segmentation task, the average results across different splits are less than 6% away from Oracle. Additionally, in the coloring task, the results achieve the global optimal solution, consistent with Oracle (even if the results did not reach the optimal in the comparison of different methods). In Fig. 2, it shows that the results of the validation set and the test set are highly consistent, and our Greedy method can obtain results very close to the global optimal solution. (3) As shown in Tab. 3, our task-level methods not only achieve state-of-the-art performance, but also have the lowest worst-case complexity. Task-level vs. Sample-level. As shown in Fig. 1(d), the task-level optimal prompt enables 27% of the samples to achieve the optimal solution, while current sample-level prompt selection methods can only find the optimal solution for 15.03% of the samples. Moreover, we conduct a detailed study comparing the performance of prompts selected by the task-level method and the sample-level method on individual samples. As illustrated in Fig. 5, we compare the selected prompt with all possible demonstration sets on individual samples and plot the ranking frequency of these samples across the entire test dataset. It can be observed that both the task-level and sample-level methods exhibit long-tail distributions in their frequency curves, indicating that under a specific task, the prompts resulting in the best performance for different samples are always the same. However, the distribution of the task-level method is significantly more concentrated than that of the sample-level method, with frequency increasing more rapidly as performance improves. It demonstrates that the simple task-level method is more effective than the complex sample-level method. The prompt that performs best on the task, even if it cannot achieve optimal performance for some samples, can still obtain relatively good results compared with most prompts. In other words, there is no need to spend a massive amount of computational effort at the risk of overfitting to select different demonstrations to form a prompt for distinct samples.\nTop-K vs. Greedy. Based on the simple assumption that the optimal prompt is typically built from demonstrations that perform well, the most straightforward approach is to sort each sample based on its performance on the remaining validation set and select the top K as the target demonstration set, which has a lower time complexity of O(N).\nTherefore, in this section, we compare the Greedy method with the Top-K method in Tab. 2. The Top-K method is also effective, and when K = 1, its results are very similar to Greedy (except for split-0 in the segmentation task). When comparing different K values, we can observe that for the segmentation task, a larger K value can lead to better results. However, simply choosing a higher K value does not necessarily improve performance shown in Figure 3. Particularly in the detection and colorization tasks, increasing the K value results in a significant performance decline. This illustrates a limitation of the Top-K method: it cannot reliably determine an appropriate hyperparameter K. Our proposed Greedy method, on the other hand, avoids this issue by adaptively determining the length of the demonstration set.\nWe visualize the in-context results of different datasets under various unsupervised prompt selection methods in Fig. 4, which are most distinguishable. Additionally, the association between UnsupPR and SelF methods is noticeable. It is evident that some prompts selected by UnsupPR cause the model to find shortcuts, resulting in outputs that have low relevance to the query image but high relevance to the prompt image. For instance, in the coloring task, the outputs of UnsupPR are severely distorted. Consequently, since UnsupPR performs poorly on these samples, SelF, which is an enhanced method based on UnsupPR, also performs poorly, resulting in scattered and messy integrated results. These sample results partially reflect the robustness of our method, but also reveal its limitations. The selected prompts do not handle some details well, such as bird legs and wall cracks."}, {"title": "Conclusion", "content": "In this paper, based on the observation that most test samples achieve optimal performance under the same prompt, we propose the task-level prompt strategy that significantly reduces inference computational costs. Furthermore, we introduce two train-free demonstration search strategy which can identify a near-optimal combination of demonstrations with minimal computational cost. Comprehensive experiments validate the effectiveness of our proposed method, demonstrating its ability to identify better demonstration combinations at a reduced cost compared to previous methods. These insights hold great promise for the further development and application of VICL, paving the way for more efficient and cost-effective paradigms."}]}