{"title": "RECLAIMING THE SOURCE OF PROGRAMMATIC POLICIES: PROGRAMMATIC VERSUS LATENT SPACES", "authors": ["Tales H. Carvalho", "Kenneth Tjhia", "Levi H. S. Lelis"], "abstract": "Recent works have introduced LEAPS and HPRL, systems that learn latent spaces of domain-specific languages, which are used to define programmatic policies for partially observable Markov decision processes (POMDPs). These systems induce a latent space while optimizing losses such as the behavior loss, which aim to achieve locality in program behavior, meaning that vectors close in the latent space should correspond to similarly behaving programs. In this paper, we show that the programmatic space, induced by the domain-specific language and requiring no training, presents values for the behavior loss similar to those observed in latent spaces presented in previous work. Moreover, algorithms searching in the programmatic space significantly outperform those in LEAPS and HPRL. To explain our results, we measured the \"friendliness\" of the two spaces to local search algorithms. We discovered that algorithms are more likely to stop at local maxima when searching in the latent space than when searching in the programmatic space. This implies that the optimization topology of the programmatic space, induced by the reward function in conjunction with the neighborhood function, is more conducive to search than that of the latent space. This result provides an explanation for the superior performance in the programmatic space.", "sections": [{"title": "1 INTRODUCTION", "content": "Programmatic representations of policies for solving reinforcement learning problems can offer important advantages over alternatives, such as neural representations. Previous work showed that due to the inductive bias of the language in which such policies are written, they tend to generalize better to unseen scenarios (Inala et al., 2020; Trivedi et al., 2021). The programmatic nature of policies also allows modularization and reuse of parts of programs (Ellis et al., 2023; Aleixo & Lelis, 2023), which can speed up learning. Previous work also showed that programmatic policies can be more amenable to verification (Bastani et al., 2018) and interpretability (Verma et al., 2018; 2019).\nThe main challenge with programmatic representations is that, in the synthesis process, one needs to search in very large and often discontinuous policy spaces. While some domain-specific languages (DSLs) are differentiable and gradient descent methods can be used (Qiu & Zhu, 2022; Orfanos & Lelis, 2023), more expressive languages that allow the synthesis of policies with internal states (Inala et al., 2020; Trivedi et al., 2021; Liu et al., 2023) are often full of discontinuities, and thus one must use combinatorial search algorithms to find suitable programs. In an attempt to ease the process of searching for policies, recent work introduced Learning Embeddings for Latent Program Synthesis (LEAPS) (Trivedi et al., 2021), a system that learns a latent space of a DSL with locality in program behavior. That is, if two vectors are near each other in the latent space, then they should decode to programs with similar behavior. Once the latent space is learned, LEAPS uses a local search algorithm to find a latent vector that is decoded into a program encoding a policy for a target task. Liu et al. (2023) extended LEAPS to propose a hierarchical framework, HPRL, to allow the synthesis of programs outside the distribution of programs used to learn the latent space.\nIn this paper, we evaluate local search algorithms operating in the programmatic space induced by the DSL, and compare them with LEAPS and HPRL. Searching in the original programmatic space involves defining an initial candidate solution (i.e., a program) and a neighborhood function that returns the neighbor programs of a candidate solution. We generate neighbors by following"}, {"title": "1.1 RELATED WORKS", "content": "Most of the early work on programmatic policies considered stateless programs, such as decision trees. For example, Verma et al. (2018) and Verma et al. (2019) learn tree-like programs with no internal states. Bastani et al. (2018) use imitation learning to induce decision trees encoding policies. Qiu & Zhu (2022) learn programmatic policies by using a language of differentiable programs, which are identical to oblique decision trees. Learning programmatic policies with internal states, such as programs with while-loops, can be more challenging. This is because the search spaces are often discontinuous and thus not amenable to gradient descent optimization. Inala et al. (2020) presented an algorithm for learning policies in the form of finite-state machines, which can represent loops. Similarly, Trivedi et al. (2021) and Liu et al. (2023) also consider programmatic policies with internal states, which are given by the lines in which the program stops and resumes its execution while interating with the environment. There is also work on programmatic policies in the multi-agent context, where the search spaces are also discontinous and the policies are learned with combinatorial search algorithms (Medeiros et al., 2022; Aleixo & Lelis, 2023; Moraes et al., 2023).\nProgram synthesis problems also pose problems similar to the ones we discuss (Waldinger & Lee, 1969; Solar-Lezama et al., 2006), where one must search in the programmatic spaces for a program that satisfies the user's intent. These problems can be solved with brute-force search (Udupa et al., 2013; Albarghouthi et al., 2013) or with algorithms guided by a function that is often learned (Odena et al., 2021; Barke et al., 2020; Shi et al., 2022; Ellis et al., 2023; Wong et al., 2021; Ameen & Lelis, 2023). A common method to learning such guiding functions is to use a self-supervised approach"}, {"title": "2 PROBLEM FORMULATION", "content": "We consider episodic partially observable Markov decision processes (POMDPs) with deterministic dynamics, deterministic observations, and undiscounted reward functions. This setting can be described by (S, A, O, p, q, r, So). In this formulation, S is the set of states, A is the set of actions and O is the set of observations. The function p : S \u00d7 A \u2192 S determines the state transition dynamic of the environment, q : S \u2192 O the observation given a state and r : S \u00d7 A \u2192 R the reward given a state and action. Finally, So defines the distribution for the initial state of an episode.\nWe consider that agents can interact in the environment following policies with internal states. The functions p, q, and r are hidden from the agent, in the sense that the agent can only observe their output. Policies with internal states are defined by the function \u03c0 : 0 \u00d7 H \u2192 A \u00d7 H, where H represents the internal state of the policy, initialized as a constant ho. Given an initial state 50 ~ So and following the deterministic state transition st+1 = p(st, at) and the policy (at, ht+1) = \u03c0(q(st), ht) to determine the next states, we can define the trajectory as a function of the policy and initial state \u03c4(\u03c0, 8\u03bf) = (\u03b1\u03bf, \u03b11, . . ., \u03b1\u03c4) for an episode with T time steps.\nThe goal of an agent acting in a POMDP is to maximize the cumulative reward over an episode. As the rewards during the episode depend uniquely on the initial state so and the policy \u03c0, we can define the return of an episode as g(so, \u03c0) = \u2211t=or(st, at). Our objective is to find an optimal policy \u03c0* given a policy class II."}, {"title": "2.1 PROGRAMMATIC POLICIES", "content": "A programmatic policy class IIDSL defines the set of policies that can be represented by a program p within a DSL. Figure 1 shows a context-free grammar that defines the DSL for KAREL THE ROBOT, the problem domain we use in our experiments. A context-free grammar is represented by the tuple (\u03a3, V, R, I). Here, \u2211 and V are sets of terminal and non-terminal symbols of the grammar. R defines the set of production rules that can be used to transform a non-terminal symbol into a sequence of terminal and non-terminal ones. Finally, I is the initial symbol of G. In Figure 1, the non-terminal symbols are p, s, b, n, h and a, where p is the initial symbol; terminal symbols include WHILE, REPEAT, IF, etc. An example of a production rule is a := move, where the non-terminal a is replaced by the action move. This grammar accepts strings defining functions with loops, if-statements, and Boolean functions, such as frontIsClear, over the observation space O. The policy class IDSL is defined as the set of all programs that the grammar accepts. The problem is to search in the space of programmatic policies IDSL for a policy that solves Equation 1.\nPrograms are represented as abstract syntax trees (ASTs). In an AST, each internal node represents a non-terminal symbol and each leaf node a terminal symbol of the grammar. Moreover, each node and its children represent a production rule. For example, for the AST shown in Figure 2, the root and its children represent the production rule p := DEF run m( s m). The non-terminal s is transformed with the production rule s := IF c(bc) i(si). The Boolean expression of the if statement is given by b := hand h := markersPresent. Finally, the body of the if statement is given by s := s; s, which branches into s := pickMarker and s := move.\nA programmatic policy p\u2208 IDSL is a policy with internal state, where ht can be interpreted as the pointer in the program p after the action taken at time step t - 1. The internal state ht and the current observation q(st) are sufficient to uniquely determine the action at the programmatic policy returns, thus the trajectory a policy generates given an initial state is also deterministic."}, {"title": "3 SEARCH SPACES FOR PROGRAMMATIC POLICIES", "content": "We formalize searching for a programmatic policy as a local search problem. This involves specifying a feasible set R \u2286 IpsL and a corresponding neighborhood function NK : R \u2192 RK which, given a feasible solution p\u2208 R, defines its K-neighborhood. In this work, we evaluate two search spaces: PROGRAMMATIC SPACE, which uses the DSL directly, and LATENT SPACE, which uses a learned embedding of the DSL."}, {"title": "3.1 PROGRAMMATIC SPACE", "content": "In this formulation, Rprog is a subset of the programs the DSL accepts. We define the subset Rprog by imposing constraints on the size of the programs. In particular, we limit the number of times the production rule s := s; s (statement chaining) can be used, and we also limit the height of the abstract syntax tree (AST) of every program. These constraints are the ones used to determine the distribution of programs used to train the latent space of LEAPS, which we describe in Appendix A.\nThe K-neighborhood of a program p\u2208 Rprog, Neros (p), consists of K samples of a mutation applied in p. A mutation is defined by uniformly sampling a node n in the AST of p and removing one of n's children that represents a non-terminal symbol; the child that is removed is also chosen uniformly at random. In this newly created \u201chole\u201d, we generate a new sub-tree by sequentially sampling a suitable production rule following the probability distribution used to generate the programs to train the latent space of LEAPS (Appendix B). We continue to sample production rules from the grammar until the newly created sub-tree does not have any leaf node representing a non-terminal symbol and the resulting neighbor program is in Rprog. We ignore programs that are not in Rprog through a sample rejection scheme. That is, if the neighbor of p is not in Rprog, we sample a new neighbor until we obtain one that is in Rprog."}, {"title": "3.2 LATENT SPACE", "content": "LEAPS (Trivedi et al., 2021) and HPRL (Liu et al., 2023) introduce a method for defining a continuous search space for programmatic policies. This is defined by a variational auto-encoder (VAE),"}, {"title": "4 LOCAL SEARCH ALGORITHMS", "content": "Once the LATENT SPACE is learned, LEAPS relies on the Cross-Entropy Method (CEM) (Rubinstein, 1999) to search for a vector that will decode into a program that approximates a solution to Equation 1. In addition to CEM, we also consider Cross-Entropy Beam Search (CEBS), a method inspired by CEM that retains information about the best candidate solutions from a population. We also consider Hill Climbing (HC), as it is an algorithm that does not offer any mechanism for escaping local minima, and thus can be used to measure properties related to the space topology. In all search algorithms, we break ties arbitrarily."}, {"title": "5 EXPERIMENTS", "content": "In this section, we describe our methodology for comparing the PROGRAMMATIC SPACE and the LATENT SPACE with respect to how conducive they are to local search algorithms. We have two sets of experiments. In the first set, we compare CEM searching in the LATENT SPACE, as presented in LEAPS' original paper, CEBS also searching in the LATENT SPACE, HPRL, which implements a hierarchical method over the LATENT SPACE, and HC searching in the PROGRAMMATIC SPACE. In the second set, we compare the spaces in a controlled experiment, where we fix the search algorithm to HC for both spaces. We also explain KAREL THE ROBOT, the domain used in our experiments."}, {"title": "5.1 KAREL THE ROBOT DOMAIN", "content": "KAREL THE ROBOT was first introduced as a programming learning environment (Pattis, 1994) and, due to its simplified structure, it has recently been adopted as a test-bed for program synthesis and reinforcement learning (Bunel et al., 2018; Chen et al., 2018; Shin et al., 2018; Trivedi et al., 2021). KAREL is a grid environment with local Boolean perceptions and discrete navigation actions.\nTo define the programmatic policy class for KAREL, we adopt the DSL of Bunel et al. (2018) (Figure 1). This DSL represents a subset of the original KAREL language. Namely, it does not allow the creation of subroutines or variable assignments. The language allows the agent to observe the presence of walls in the immediate neighborhood of the robot, with the perceptions {front|left|right}IsClear, and the presence of markers in the current robot location with markersPresent and noMarkersPresent. The agent can then move the robot with the actions move and turn{Left|Right}, and interact with the markers with {put|pick}Marker.\nWe consider the KAREL and KAREL-HARD problem sets to define tasks. The KAREL set contains the tasks STAIRCLIMBER, MAZE, FOURCORNERS, TOPOFF, HARVESTER and CLEANHOUSE, all introduced by Trivedi et al. (2021). The KAREL-HARD problem set includes the tasks DOORKEY, ONESTROKE, SEEDER and SNAKE, designed by Liu et al. (2023) as more challenging problems. Trivedi et al. (2021) showed that these domains are challenging for reinforcement learning algorithms using neural representations, so LEAPS and HPRL represent the current state of the art in these problems. A detailed description of each task in both sets is available in Appendix D."}, {"title": "5.2 FIRST SET: REWARD-BASED EVALUATION", "content": "Our first evaluation reproduces the experiments of Trivedi et al. (2021) and Liu et al. (2023), where we add the results of HC searching in the PROGRAMMATIC SPACE. We use K = 250 as the neighborhood parameter for the HC. For CEBS, we set the dimension of the latent vector d = 256, the neighborhood size K = 64, the elite size E = 16, and the noise \u03c3 = 0.25. The hyperparameters for CEM and HPRL are exactly as described in their papers.\nFor each method, we estimate the expected return of a policy by averaging the returns collected over trajectories starting from a set of initial states. In this experiment, we consider a set of 16 initial states for each problem. We limit the execution of each method to a budget of 106 program evaluations. If an algorithm fails to converge but its execution is still within the budget, we re-sample an initial program and restart the search. We report results over 32 independent runs (seeds) of each method."}, {"title": "5.3 SECOND SET: TOPOLOGY-BASED EVALUATION", "content": "To understand the discrepancy between HC and the algorithms searching in the LATENT SPACES, we analyze the PROGRAMMATIC and LATENT SPACE while controlling for the search algorithm."}, {"title": "5.3.1 LOCAL BEHAVIOR SIMILARITY ANALYSIS", "content": "We first analyze the behavior loss used to train the LATENT SPACE in the two search spaces. We define a metric that measures the loss in the neighborhood of randomly sampled programs in a search space. The similarity between two programmatic policies p and p', with trajectories from an initial state so given by \u03c4(\u03c1, so) = (ao, ..., \u03b1\u03c4) and \u03c4(\u03c1', so) = (\u03b1'\u03bf, . . ., \u03b1'\u03c4'), as\n\\[p-similarity(p, p', so) = \\frac{max\\{0 \u2264 t \u2264 1 | ao:t = a'o:t\\}}{L},\\]\nwhere l = min{T, T'} and L = max{T, T'}, and xo:t = (x0,...,xt). The p-similarity returns the normalized length of the longest common prefix of the action sequences p and p' produced from 80."}, {"title": "5.3.2 CONVERGENCE ANALYSIS", "content": "Next, we look at the topology of each search space with respect to the return function of the tasks we want to solve. Specifically, we want to measure how conducive a given space is to search. To do so, we use a search algorithm that cannot escape local minima, HC, and measure how likely the search is to converge to a solution of a given quality in the search space of interest.\nWe define the convergence rate of a search space given by the neighborhood function NK, with initial program distribution Po and initial state distribution So of a given POMDP. The initial program distribution for the PROGRAMMATIC SPACE is given by the LEAPS probabilistic context-free grammar (Appendix B); for the LATENT SPACE, it is given by the programs one decodes after sampling a latent vector from N(0, Ia). The convergence rate is measured in terms of gtarget \u2208 [0, 1] as follows.\n\\[convergence-rate(NK, 9target) = \u0395po~Po,so~So[1\\{gsearch(po, so) \u2265 9target\\}],\\]\nwhere gsearch (po, so) is the return of the best-performing program encountered in the search starting with candidate po and with the return computed by rolling the policies out from 80."}, {"title": "6 CONCLUSION", "content": "In this paper, we showed that, despite recent efforts in learning latent spaces to replace programmatic spaces, the latter can still be more conducive to search. Empirical results in KAREL THE ROBOT showed that a simple hill-climbing algorithm searching in the programmatic space can significantly outperform the current state-of-the-art algorithms that search in latent spaces. We measured both the learned latent space and the programmatic space in terms of the loss function used to train the former. We discovered that both have similar loss values, even though the programmatic space does not require training. We also compared the topology of the two spaces through the probability of a hill-climbing search being stuck at local maxima in the two spaces, and found that the programmatic space is more conducive to search. Our results suggest that the use of the original programmatic space is an important baseline that was missing in previous work. Our results also suggest that learning latent spaces for easing the process of synthesizing programmatic policies for solving reinforcement learning problems is still an open and challenging research question."}, {"title": "A SETTINGS FOR GENERATING PROGRAMS", "content": "In this work, we use the same constraints for generating programs as the LEAPS project (Trivedi et al., 2021), as described below.\n\u2022 Maximum AST height: 4;\n\u2022 Maximum statement chaining (s := s; s rule): 6;\n\u2022 Maximum program length (in number of symbols in the program text representation): 45."}, {"title": "B PROBABILITIES FOR DSL PRODUCTION RULES", "content": "We adopt a fixed probability for each DSL production rule, described in Figure 6 as a probabilistic context-free grammar. The adopted probabilities are based on the LEAPS project specifications (Trivedi et al., 2021)."}, {"title": "CALGORITHM DETAILS OF SEARCH ALGORITHMS", "content": "We present pseudo-code implementations of HC and CEBS as described in Section 4 in Algorithms 1 and 2, respectively."}, {"title": "D KAREL PROBLEM SETS", "content": "In this Section, we specify the initial state and return function of every task in KAREL and KAREL-HARD problem sets. Further details of each task are present in LEAPS (Trivedi et al., 2021) and HPRL (Liu et al., 2023), works that introduced KAREL and KAREL-HARD, respectively. All tasks time out an episode after 10, 000 actions."}, {"title": "D.1 KAREL", "content": "STAIRCLIMBER This environment is given by a 12 \u00d7 12 grid with stairs formed by walls. The agent starts on a random position on the stairs and its goal is to reach a marker that is also randomly initialized on the stairs. If the agent reaches the marker, the agent receives 1 as an episodic return and 0 otherwise. If the agent moves to an invalid position, i.e. outside the contour of the stairs, the episode terminates with a -1 return.\nMAZE A random maze is initialized on an 8 \u00d7 8 grid, and a random marker is placed on an empty square as a goal. The agent starts on a random empty square of the grid and its goal is to reach the marker goal, which yields a 1 episodic return. Otherwise, the agent receives 0 as a return.\nTOPOFF Markers are placed randomly on the bottom row of an empty 12 \u00d7 12 grid. The goal of the agent, initialized on the bottom left of the map, is to place one extra marker on top of every marker on the map. The return of the episode is given by the number of markers that have been topped off divided by the total number of markers.\nFOURCORNERS Starting on a random cell on the bottom row of an empty 12 \u00d7 12 grid, the goal of the agent is to place one marker in each corner of the map. Return is given by the number of corners with one marker divided by four.\nHARVESTER The agent starts on a random cell on the bottom row of an 8 \u00d7 8 grid, that starts with a marker on each cell. The goal of the agent is to pick up every marker on the map. Return is given by the number of picked-up markers divided by the total number of markers."}, {"title": "D.2 KAREL-HARD", "content": "DOORKEY The agent starts on a random position on the left side of an 8 \u00d7 8 grid that is vertically split into two chambers. The agent goal is to pick up a marker on the left chamber, which opens a door connecting both chambers and allows the agent to reach a goal marker. Picking up the first marker yields a 0.5 reward, and reaching the goal yields an additional 0.5.\nONESTROKE Starting on a random position of an empty 8 \u00d7 8 grid, the goal of the agent is to visit every grid cell without repeating. Visited cells become a wall that terminates the episode upon touching. The episodic return is given by the number of visited cells divided by the total number of cells in the initial state.\nSEEDER The environment starts as an empty 8 \u00d7 8 grid, with the agent placed randomly in any square. The agent's goal is to place one marker in every empty cell of the map. The return is given by the number of cells with one marker divided by the total number of empty cells at the start of the episode.\nSNAKE In this task, the agent and one marker are randomly placed on an empty 8 \u00d7 8 grid. The agent acts like the head of a snake, whose body grows each time a marker is collected. The goal of the agent is to touch the marker on the map without colliding with the snake's body, which terminates the episode. Each time the marker is collected, it is placed in a new random location, until 20 markers are collected. The episodic return is given by the number of collected markers divided by 20."}, {"title": "E RUNNING TIME COMPARISON OF PROGRAMMATIC AND LATENT SPACES", "content": "In this section, we compare the neighborhood generation process of each search space in terms of running time. We do this by measuring the time the PROGRAMMATIC SPACE and the LATENT SPACE take to generate one neighbor from a given candidate program, sampled from the initial distribution, and present the results in Table 2. We see that sampling from the programmatic space is more than 10 times faster than sampling from the latent space. This means that the gap between the search in PROGRAMMATIC SPACE and LATENT SPACE would be larger if the results reported in the paper were in terms of running time instead of episodes."}, {"title": "F EXAMPLES OF OBTAINED SOLUTIONS", "content": "In this section, we show representative examples of programmatic policies from HC and CEBS across some relevant tasks. We selected programs that yield the highest return for each algorithm. Results are presented in Tables 3 and 4 for HC and CEBS, respectively."}, {"title": "G EVALUATION ON CRASHABLE KAREL", "content": "To further evaluate the search algorithms, we propose a modification of the KAREL environment. In this version, which we name CRASHABLE, invalid actions terminate episodes. This change implies"}, {"title": "H EVALUATING THE IMPACT OF INITIALIZATION METHODS", "content": "To measure the impact of the initialization methods of the search algorithms, we evaluate an alternative version of all search algorithms using the initialization rule of the opposed search space. For CEM and CEBS, this version of the algorithms initializes the search with policies sampled from the defined probabilistic context-free grammar. And for HC in PROGRAMMATIC SPACE, search is initialized by decoding a latent sampled from N(0, Ia) in the Latent Space. Figure 8 compares the performance of HC in PROGRAMMATIC SPACE with the alternative initialization scheme, while Figures 9 and 10 compare the performance of CEM and CEBS, respectively, with the alternative initialization scheme.\nResults show that the performance of the alternative version of the algorithms was very similar to the original algorithm in most cases, and marginally inferior in others. This suggests that both initialization methods are similar and one does not provide a significant advantage over the other."}, {"title": "I CONVERGENCE RATE FOR DIFFERENT NEIGHBORHOOD SIZES", "content": "We expand the convergence rate analysis by adopting neighborhood functions NK with different neighborhood sizes K in Equation (5). The convergence-rate analysis on a space given by a lower K is related to how robust the search space is to conduct a search process, given that it relies on a small number of samples. On the other hand, the result of convergence-rate on higher K gives us information about how capable the search space is, as it can expand a search state further to find the better candidate. Figure 11 compares the convergence-rate estimation of both PROGRAMMATIC and LATENT SPACE adopting K = {10, 250, 1000}, and suggests that the PROGRAMMATIC SPACE is"}, {"title": "J CONVERGENCE RATE OF CEM AND CEBS", "content": "We further analyze the convergence rate of the LATENT SPACE by using CEM and CEBS as the search algorithm in Equation 5. Figure 12 compares the convergence rate obtained with HC (original setting), CEM, and CEBS, with K = 64 (neighborhood size). Although CEM and HC have a similar convergence rate across all tasks, we see that CEBS outperforms both in HARVESTER, DOORKEY, ONESTROKE and SEEDER. These results highlight the ability of CEBS to escape local optima. Despite the superior performance of CEBS, HC searching in the PROGRAMMATIC SPACE performs better than CEBS searching in the LATENT SPACE (see Figure 3)."}, {"title": "K VALIDATING THE EXISTENCE OF HIGH-PERFORMING POLICIES IN LATENT SPACE", "content": "To complement the performance and topology analysis of the LATENT SPACE, we designed an additional experiment to validate the existence of high-performing policies in the space. Specifically, we are interested in confirming if policies with an episodic return larger than 0.5 in the DOORKEY task exist in the LATENT SPACE.\nWe start by selecting a high-performing policy found by searching in the PROGRAMMATIC SPACE. This policy is then encoded as a latent vector to be represented in the LATENT SPACE. Note that the encoded policy does not necessarily decode to the same original policy. To account for that, we use the encoded policy as the initial candidate for an HC search on the LATENT SPACE with neighborhood size K = 1,000. Table 5 shows a particular choice of initial candidate that led to the discovery of a policy in the LATENT SPACE that yields an episodic return of 0.71875 in DOORKEY.\nThis experiment confirms that high-performing policies exist in the LATENT SPACE. However, the convergence rate analysis shows that searching with HC in the space did not result in such policies, after considering 10, 000 different search initializations."}]}