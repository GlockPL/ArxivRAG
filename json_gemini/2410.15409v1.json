{"title": "PEAS: A Strategy for Crafting Transferable Adversarial Examples", "authors": ["BAR AVRAHAM", "YISROEL MIRSKY"], "abstract": "Black box attacks, where adversaries have limited knowledge of the target model, pose a significant threat to machine learning systems. Adversarial examples generated with a substitute model often suffer from limited transferability to the target model. While recent work explores ranking perturbations for improved success rates, these methods see only modest gains. We propose a novel strategy called PEAS that can boost the transferability of existing black box attacks. PEAS leverages the insight that samples which are perceptually equivalent exhibit significant variability in their adversarial transferability. Our approach first generates a set of images from an initial sample via subtle augmentations. We then evaluate the transferability of adversarial perturbations on these images using a set of substitute models. Finally, the most transferable adversarial example is selected and used for the attack. Our experiments show that PEAS can double the performance of existing attacks, achieving a 2.5x improvement in attack success rates on average over current ranking methods. We thoroughly evaluate PEAS on ImageNet and CIFAR-10, analyze hyperparameter impacts, and provide an ablation study to isolate each component's importance.", "sections": [{"title": "1 Introduction", "content": "Adversarial examples are subtly altered inputs that mislead machine learning models. These samples pose a significant threat to the security of AI systems. Of particular concern are black box attacks, where the adversary lacks detailed knowledge of the target model's architecture or parameters. This scenario reflects the reality of most commercial AI systems deployed in the cloud or embedded in products where the adversary can only interact with the model through queries.\nA common strategy for black box attacks relies on the use of substitute models. The adversary trains a substitute model (f') and generates adversarial examples tailored to it, hoping for transfer-ability to the target model (f) due to gradient alignment [5]. However, adversarial transferability remains a significant challenge. Inherent differences in model architectures, training data, and optimization techniques can lead to gradients that point in vastly different directions in the input space. This mismatch between the substitute model and the target model often results in adversarial examples that are highly effective against the substitute model but fail to fool the target model.\nOur key observation is that there often exist numerous samples that are perceptually equivalent to the original input (x), yet exhibit significant variability in their alignment with other models' decision boundaries. If the adversary can discover a perceptually equivalent sample that has good"}, {"title": "2 Related Works", "content": "Our study introduces a novel method to boost the transferability of adversarial examples made using substitute models. We start by reviewing the concept of transferability and then examine how previous research has aimed to improve it by (1) enhancing perturbation robustness and (2) choosing the best perturbation for each sample, known as ranking.\nTransferability. The term transferability refers to the phenomenon where adversarial examples generated using a substitute model can effectively deceive another model. This principle was first highlighted by Szegedy et al. [26] and further explored by Goodfellow et al. [12] that showed that adversarial training can alleviate transferability slightly, and by Papernot et al. [22] who demonstrated the ability of adversarial perturbations to generalize across different models. The reason for this transferability is often attributed to the similarity in gradient directions or deci-sion boundaries between the models, a phenomenon known as gradient alignment. This concept suggests that despite variations in architecture or training data, different models may still exhibit vulnerabilities to the same adversarial examples. Demontis et al. [5] further reveal this concept by examining the role of gradient alignment in transferability, providing a more technical foundation for understanding why and how adversarial examples can deceive multiple models.\nWith transferability, an attacker can take a sample x, craft an adversarial example x' using an arbitrary model f', and expect some level of success when using it on the victim's model f. However, the attack success rates in this naive transferability setting are usually quite low [21].\nImproving Transferability. To improve attack success rates, researchers have looked for ways to increase the likelihood of transferability. The general approach is to increase diversity in the process of creating x' to simulate the loss surface and decision boundaries of unknown models [2]. Works such as [3, 6, 10, 17, 19, 20, 23] increase model diversity by using multiple substitute models with different architectures. The idea is that if x' works on a set of different models (i.e., crosses their decision boundaries), then it is likely to work on an unknown model. Other works, such as [7] modify the optimization algorithm to mitigate the issue of overfitting to f'.\nAnother approach has been to increase input diversity to f'. For example, Xie et al. showed that it is possible to make a robust adversarial perturbation by applying random transformations (i.e., augmentations such as random resizing and padding) to the sample at each iteration during its generation on f' [28]. This process is similar to expectation over transformation (EOT) [1] and has been used in various different ways to make transferable perturbations [16, 29, 31]. Dong et al. improved the process further by applying an augmentation kernel to the perturbation itself, making the entire process more efficient and effective [8].\nAll of these works have been trying to solve the problem of making a robust perturbation for x using f', whether it be by using multiple models or by performing transformations on x during the optimization process. Our work aims to solve a different problem: of all the perceptually identical images to x, which one gives us the most advantageous starting point for creating an adversarial example with higher likelihood of transferability?"}, {"title": "3 Perceptual Exploration Attack Strategy (PEAS)", "content": "In this section we present our novel attack strategy. First, we introduce the concept of percep-tual equivalence and then discuss how it can be used in conjunction with ranking to boost the transferability of adversarial examples in black box settings."}, {"title": "3.1 Perceptual Equivalence", "content": "An adversary's goal is to generate an adversarial example x' that fool a target classifier f while remaining indistinguishable from the original input x. This stealthiness is often achieved by limiting adversarial changes to lie within an e-ball around x, as measured by a p-norm distance metric $\\|x - x'\\|_p < \\epsilon$.\nHowever, the p-norm metric doesn't perfectly align with human perception. We can make changes to an image that drastically increase its p-norm while remaining visually imperceptible to a casual human observer. For example, by shifting an image by two pixels. Therefore, we define two images $x_i$ and $x_j$ as perceptually equivalent if a casual human observer would deem them the same, with no suspicions about $x_j$.\nWe argue that adversaries can exploit perceptual stealth rather than relying solely on p-norm constraints.\nAn interesting observation is that an image with a subtle augmentation has its robust features (i.e., the main features used in classification) perturbed, whereas an image with additive noise has its non-robust features (noise patterns) perturbed. In either case, an alteration to either type of feature will affect the sample's location with respect to the model's loss surface. We will now discuss the implication of this phenomenon."}, {"title": "3.2 Starting Points & Transferability", "content": "A common strategy for improving adversarial examples is to try running the attack multiple times from different locations near x and by selecting the best result [25]. This strategy, known as \u2018random starts,' is effective because different start points can lead to different optima on the loss surface of f'. In the context of transferability, we seek a starting point which has good gradient alignment with an unknown model f.\nLet S(x) denote a sampling function that produces a sample near x. As shown by Levy et al. [15], among the samples produced by S(x), there exists a sample which, if attacked using substitute f',"}, {"title": "3.4 Sampling Functions", "content": "The strength of PEAS depends on how well the sampling function S perturbs the robust features in x. In this work, we propose two basic sampling functions that can be used with PEAS: $S_1$ and $S_2$.\nLet A be a set of augmentation algorithms, each configured to perform a subtle augmentation (e.g., one may rotate an image randomly on the range [-2, 2] degrees). The function $S_1$ applies a random augmentation from A to x. $S_2$ applies all augmentations in A for each to x (e.g., the \u2018Mix' example from Fig. 2). Overall, the tradeoff between the two is that $S_1$ is stealthier while $S_2$ is more effective at exploring transferable versions of x."}, {"title": "3.5 Improving Black Box Attacks with PEAS", "content": "PEAS can be seen as a method for moving samples closer to common model boundaries. Line 4 in Algorithm 1 enables us to apply this strategy to existing black box attack algorithms; increasing their likelihood of success. In general, black box attacks either utilize substitute model(s) to create x' (e.g., [8, 17]), query the victim f to refine x' (e.g., [4, 13]) or do both (e.g., [3, 19]). We'll discuss how PEAS can be integrated in all cases:\nAttacks which use Substitute Models: To use PEAS in these attacks, all we need to do is replace \u201cAttack\u201d on line 4 in Algorithm 1 with the chosen attack. By doing so, we are effectively using the other black box attack as a means for searching for samples with better transferability.\nAttacks which Query the Victim: In this setting, we do not want to execute the attack as part of PEAS since this would result in an increased query count on the victim (which is not covert) and would lead to an overt adversarial example since we'd be repeatedly applying augmentations to the same sample. To resolve this, we first execute PEAS and then pass x* to the other black box attack -giving it a better starting point. Doing so not only increases the likelihood of success but can also reduce the query count."}, {"title": "4 Evaluation", "content": "In this section, we evaluate the performance of PEAS as a \u2018plug-and-play' strategy for improving existing black box attacks. We also investigate why the attack is effective through an ablation study. To reproduce our work, the reader can find the source code to PEAS online."}, {"title": "4.1 Experiment Setup", "content": "Attack Model. We assumed the following attack model in our experiments: the adversary is operating in a black box setting where there is no knowledge of the target model's parameters or architecture. We assume that the adversary knows the training data's distribution, as commonly assumed in other works [3, 23, 27, 30]. In our setting, the attacker wishes to perform an untargeted attack where the objective is to cause an arbitrary classification failure: f (x') \u2260 y. Although PEAS can be easily adapted to the targeted setting, we leave this analysis to future work.\nDatasets. To evaluate PEAS, we used two well-known benchmark datasets: CIFAR-10 and ImageNet. CIFAR-10 is an image classification dataset consisting of 60K images with 10 classes having a resolution of 32x32. ImageNet contains approximately 1.2M images with 1000 classes rescaled to a resolution of 224x224. For both datasets, we used the original data splits. As mentioned, we used the same training data for f and f' following the work of other black box attack papers [3, 23, 27, 30]. Following the setup of other similar works (e.g., . [3, 11, 18]) we evaluated 1000 random samples from the testing data of each dataset. To avoid bias, we only used samples that were correctly classified by f."}, {"title": "4.2 Baseline Evaluation", "content": "Boosting with Ranking. In Table 1, we compare (1) the performance of ranking random starts using noise [15] (Vanilla) and ranking random augmentations (BTA-PEAS). In both cases we use PGD to generate the perturbations on the starting points. The lower bound is BTA (basic transferability attack) and the upper bound is the simulated case where a \u2018perfect ranking algorithm' is used in PEAS with $S_2$.\nThe table shows that Vanilla ranking (ranking random starts) is ineffective, as seen by its comparison to the lower bound (BTA). In contrast, BTA-PEAS is much more effective, achieving an average improvement in attack success rates of 1.7x with $S_1$ and 2.5x with $S_2$, with some cases reaching a 6.3x gain. This validates our hypothesis that Vanilla's additive noise does not effectively perturb transferability-critical features, while PEAS targets them effectively with augmentations. Although PEAS performs significantly better, there is still room for improvement, as indicated by the upper bound. Enhancing the ET ranking algorithm and developing better sampling functions can achieve this.\nIn summary, PEAS's augmentation ranking strategy significantly outperforms both baseline transferability and Vanilla ranking, highlighting the importance of targeting robust features for improved adversarial transferability."}, {"title": "Boosting Black Box Attacks", "content": "In Table 2, we compare the performance of different attack strategies before and after applying PEAS. The strategies are (1) a basic transfer attack using a surrogate (BTA), (2) a transfer attack using input diversity (FGSM-TIMI), and (3) an iterative attack using query feedback (Simba). Here, sampling strategy $S_2$ is used. The results show that by boosting the basic transfer attack, PEAS can obtain a performance 7.4x and 1.6x better than TIMI and Simba respectively, on average. We can also see that even when input diversity (TIMI) is used or when the attack is querying the black box victim (Simba), PEAS can increase the ASR by a factor of 1.35.\nIn Table 3, we show the performance of two state-of-the-art black box attacks (PGN and SSA) and show how PEAS boosts their ASR for different epsilon budgets. Note that an epsilon of 25.5/255 is not considered stealthy. We also present the performance of the simple BTA attack as reference. Both PGN-PEAS and SSA-PEAS outperform their original versions by a significant margin.\nOverall, these results demonstrate that PEAS can be effectively leveraged as a performance-enhancing strategy for various existing black box attacks, including modern ones."}, {"title": "4.3 Ablation Study", "content": "Algorithm Components. In Table 4, we investigate the contribution of each component of PEAS in selecting the best sample (augmentation) from S. For example, \u2018Top-1 Adversarial Example' is the proposed PEAS algorithm where we first attack each sample in S and then select the top sample using ET with F. Here we are boosting the basic transfer attack (BTA).\nOur first observation is that PEAS succeeds not because augmentations cause misclassifications, but because they provide better starting points for attacks. This can be seen by contrasting the column \u201cRandom Augmentation\u201d (simply using augmentations as the attack) to \u201c(filtered) Top-1 Adversarial Example\u201d (where we only use samples that don't cause a natural misclassifcation). Regardless, in a real attack some augmentations may increase the ASR due to natural misclassifica-tions. However, we argue that these are legitimate perturbations an adversary can use, as they are subtle. The key contribution is selecting the best one to use.\nOur second observation highlights the role of augmentation in transferability: randomly selected augmented samples yield poor performance, but the top-1 augmented sample performs decently. This demonstrates that (1) ET works on augmented samples, and (2) PEAS attacks can be performed without adversarial perturbations. However, adding a perturbation on top of the augmented sample"}, {"title": "5 Conclusion", "content": "In conclusion, our Perception Exploration Attack Strategy (PEAS) can boost black box adversarial attacks by finding an ideal perceptually equivalent starting point which enhances transferability. This work both introduces an effective attack strategy and deepens our understanding of adversarial transferability, highlighting perceptual equivalence as a powerful tool in adversarial machine learning."}]}