{"title": "SASSHA: Sharpness-aware Adaptive Second-order Optimization with Stable Hessian Approximation", "authors": ["Dahun Shin", "Dongyeop Lee", "Jinseok Chung", "Namhoon Lee"], "abstract": "Approximate second-order optimization methods often exhibit poorer generalization compared to first-order approaches. In this work, we look into this issue through the lens of the loss landscape and find that existing second-order methods tend to converge to sharper minima compared to SGD. In response, we propose SASSHA, a novel second-order method designed to enhance generalization by explicitly reducing sharpness of the solution, while stabilizing the computation of approximate Hessians along the optimization trajectory. In fact, this sharpness minimization scheme is crafted also to accommodate lazy Hessian updates, so as to secure efficiency besides flatness. To validate its effectiveness, we conduct a wide range of standard deep learning experiments where SASSHA demonstrates its outstanding generalization performance that is comparable to, and mostly better than, other methods. We provide a comprehensive set of analyses including convergence, robustness, stability, efficiency, and cost.", "sections": [{"title": "1. Introduction", "content": "Approximate second-order methods have recently gained a surge of interest due to their potential to accelerate the large-scale training process with minimal computational and memory overhead (Yao et al., 2021; Liu et al., 2024; Gupta et al., 2018). However, studies also suggest that these methods may undermine generalization, trying to identify underlying factors behind this loss (Wilson et al., 2017; Zhou et al., 2020; Zou et al., 2022). For instance, Amari et al. (2021) shows that preconditioning hinders achieving the optimal bias for population risk, and Wadia et al. (2021) points to negative effect of whitening data.\nWhile the precise understanding is still under investigation, many studies have suggested a strong correlation between the flatness of minima and their generalization capabilities (Keskar et al., 2017), spurring the development of optimization techniques aimed at inducing flat minima (Chaudhari et al., 2017; Izmailov et al., 2018; Foret et al., 2021; Orvieto et al., 2022). Inspired by this, we raise an important question in this work: what type of minima do second-order methods converge to, and is there any potential for improving their generalization performance based on that?\nTo answer these questions, we first measure the sharpness of different second-order methods using diverse metrics, suggesting that they converge to significantly sharper minima compared to stochastic gradient descent (SGD). Then, we propose SASSHA\u2014Sharpness-aware Adaptive Second-order optimization with Stable Hessian Approximation\u2014designed to enhance the generalization of approximate second-order methods by explicitly reducing sharpness (see Figure 1 for the basic results).\nSASSHA incorporates a sharpness minimization scheme similar to SAM (Foret et al., 2021) into the second-order optimization framework, in which the Hessian diagonal is estimated. Such estimates, however, can become numerically unstable when enforcing the sharpness reduction process. To increase stability while preserving the benefits of reduced sharpness, we make a series of well-engineered design choices based on principles studied in the literature. This not only smoothly adjusts underestimated curvature, but also enables efficient reuse of previously computed Hessians, resulting in a stable and efficient algorithm.\nWe extensively evaluate the effectiveness of SASSHA across diverse vision and natural language tasks. Our results reveal that SASSHA consistently achieves flatter minima and attains stronger generalization performance, all compared to existing practical second-order methods, and interestingly, to first-order methods including SGD, AdamW, and SAM. Furthermore, we provide an array of additional analyses to comprehensively study SASSHA including convergence, robustness, stability, efficiency, and cost."}, {"title": "2. Related Works", "content": "Second-order optimization for deep learning. First-order methods such as SGD are popular optimization methods for deep learning due to their low per-iteration cost and good generalization performance (Hardt et al., 2016). However, these methods have two major drawbacks: slow convergence under ill-conditioned landscapes and high sensitivity to hyperparameter choices such as learning rate (Demeniconi & Chawla, 2020). Adaptive methods (Duchi et al., 2011; Hinton et al., 2012; Kingma & Ba, 2015) propose using empirical Fisher-type preconditioning to alleviate these issues, though recent studies suggest their insufficiency to do so (Kunstner et al., 2019). This has led to recent interest in developing approximate second-order methods such as Hessian-Free Inexact Newton methods (Martens et al., 2010; Kiros, 2013), stochastic quasi-Newton methods (Byrd et al., 2016; Gower et al., 2016), Gauss-Newton methods (Schraudolph, 2002; Botev et al., 2017), natural gradient methods (Amari et al., 2000), and Kronecker-factored approximations (Martens & Grosse, 2015; Goldfarb et al., 2020). However, these approaches still incur non-trivial memory and computational costs, or are difficult to parallelize, limiting their applicability to large-scale problems such as deep learning. This has driven growing interest in developing more scalable and efficient second-order approaches, particularly through diagonal scaling methods (Bottou et al., 2018; Yao et al., 2021; Liu et al., 2024), to better accommodate large-scale deep learning scenarios.\nSharpness minimization for generalization. The relationship between the geometry of the loss landscape and the generalization ability of neural networks was first discussed in the work of Hochreiter & Schmidhuber (1994), and the interest in this subject has persisted over time. Expanding on this foundation, subsequent studies have explored the impact of flat regions on generalization both empirically and theoretically (Hochreiter & Schmidhuber, 1997; Keskar et al., 2017; Dziugaite & Roy, 2017; Neyshabur et al., 2017; Dinh et al., 2017; Jiang et al., 2020). Motivated by this, various approaches have been proposed to achieve flat minima such as regularizing local entropy (Chaudhari et al., 2017), averaging model weights (Izmailov et al., 2018), explicitly regularizing sharpness by solving a min-max problem (Foret et al., 2021), and injecting anti-correlated noise (Orvieto et al., 2022), to name a few. In particular, the sharpness-aware minimization (SAM) (Foret et al., 2021) has attracted significant attention for its strong generalization performance across various domains (Chen et al., 2022; Bahri et al., 2022; Qu et al., 2022) and its robustness to label noise (Baek et al., 2024). Nevertheless, to our knowledge, the sharpness minimization scheme has not been studied to enable second-order methods to find flat minima as of yet."}, {"title": "3. Practical Second-order Optimizers Converge to Sharp Minima", "content": "In this section, we investigate the sharpness of minima obtained by approximate second-order methods and their generalization properties. We posit that poor generalization of second-order methods reported in the literature (Amari et al., 2021; Wadia et al., 2021) can potentially be attributed to sharpness of their solutions.\nWe employ four metrics frequently used in the literature: maximum eigenvalue of the Hessian, the trace of Hessian, gradient-direction sharpness, and average sharpness (Hochreiter & Schmidhuber, 1997; Jastrz\u0119bski et al., 2018; Xie et al., 2020; Du et al., 2022b; Chen et al., 2022). The first two, denoted as $\\lambda_{\\max}(H)$ and $\\mathrm{tr}(H)$, are often used as standard mathematical measures for the worst-case and the average curvature computed using the power iteration method and the Hutchinson trace estimation, respectively. The other two measures, $\\mathcal{L}_{\\mathrm{grad}}$ and $\\mathcal{L}_{\\mathrm{avg}}$, assess sharpness based on the loss difference under perturbations. $\\mathcal{L}_{\\mathrm{grad}}$ evaluates sharpness in the gradient direction and is computed as $L(\\mathbf{x}^* + \\rho \\nabla L(\\mathbf{x}^*)/||\\nabla L(\\mathbf{x}^*)||) - L(\\mathbf{x}^*)$. $\\mathcal{L}_{\\mathrm{avg}}$ computes the average loss difference over Gaussian random perturbations, expressed as $\\mathbb{E}_{\\mathbf{z} \\sim \\mathcal{N}(0, I)}[L(\\mathbf{x}^* + \\rho \\mathbf{z}/||\\mathbf{z}||) - L(\\mathbf{x}^*)]$. Here we choose $\\rho = 0.1$ for the scale of the perturbation.\nWith these, we measure the sharpness of the minima found by three approximate second-order methods designed for deep learning; Sophia-H (Liu et al., 2024), AdaHessian (Yao et al., 2021), and Shampoo (Gupta et al., 2018), and compare them with SASSHA as well as SGD for reference. We also compute the validation loss and accuracy to see any correlation between sharpness and generalization of these solutions.\nWe observe that existing second-order optimizers produce solutions with significantly higher sharpness compared to SASSHA in all sharpness metrics, which also correlates well with their generalization. We also provide a visualization of the loss landscape for the found solutions, where we find that the solutions obtained by second-order methods are indeed much sharper than that of SASSHA (Figure 2)."}, {"title": "4. Method", "content": "In the previous section, we observe that the generalization performance of approximate second-order algorithms anti-correlates with the sharpness of their solutions. Based on this, we introduce SASSHA\u2014a novel adaptive second-order method designed to improve generalization by reducing sharpness without adversely impacting the Hessian.\n### 4.1. Sharpness-aware Second-order Optimization\nWe consider a min-max problem, similar to Keskar et al. (2017); Foret et al. (2021), to minimize sharpness. This is defined as minimizing the objective $f$ within the entire $\\rho$-ball neighborhood:\n$\\displaystyle \\min_{\\mathbf{x} \\in \\mathbb{R}^d} \\max_{\\|\\epsilon\\|_2 \\le \\rho} f(\\mathbf{x} + \\epsilon),$   (1)\nBased on this, we construct our sharpness minimization technique for second-order optimization as follows. We first follow a similar procedure as Foret et al. (2021) by solving for $\\epsilon$ on the first-order approximation of the objective, which exactly solves the dual norm problem as follows:\n$\\displaystyle \\epsilon^* = \\arg \\max_{\\|\\epsilon\\|_2 \\le \\rho} f(\\mathbf{x}_t) + \\epsilon^\\top \\nabla f(\\mathbf{x}_t) = \\rho \\frac{\\nabla f(\\mathbf{x}_t)}{\\|\\nabla f(\\mathbf{x}_t)\\|_2}$.   (2)\nWe plug this back to yield the following perturbed objective function:\n$\\displaystyle f_t(\\mathbf{x}) = f \\left( \\mathbf{x} + \\rho \\frac{\\nabla f(\\mathbf{x}_t)}{\\|\\nabla f(\\mathbf{x}_t)\\|_2} \\right),$ which shifts the point of the approximately highest function value within the neighborhood to the current iterate.\nWith this sharpness-penalized objective, we proceed to make a second-order Taylor approximation:\n$\\displaystyle \\mathbf{x}_{t+1} = \\arg \\min_\\mathbf{x} \\ f_t(\\mathbf{x}_t) + \\nabla f_t(\\mathbf{x}_t)^\\top (\\mathbf{x} - \\mathbf{x}_t)\n+ \\frac{1}{2} (\\mathbf{x} - \\mathbf{x}_t)^\\top H_t(\\mathbf{x}_t) (\\mathbf{x} - \\mathbf{x}_t),$\nwhere $H_t$ denotes the Hessian of $f_t$. Using the first-order optimality condition, we derive the basis update rule for our sharpness-aware second-order optimization:\n$\\begin{aligned}\n\\mathbf{x}_{t+1} &= \\mathbf{x}_t - H_t(\\mathbf{x}_t)^{-1} \\nabla f_t(\\mathbf{x}_t) \\\\\n&= \\mathbf{x}_t - \\widehat{H}(\\mathbf{x}_t + \\epsilon^*)^{-1} \\nabla f(\\mathbf{x}_t + \\epsilon^*),\n\\end{aligned}$\nwhere $\\widehat{H}$ denotes the Hessian of the original objective $f$.\nPractical second-order methods must rely on approximately estimated Hessians (i.e., $H \\rightarrow \\widehat{H}$) since the exact computation is prohibitively expensive for large-scale problems. We choose to employ the diagonal approximation via Hutchinson's method. However, as we will show in our analysis (Section 6.2), we find that these estimates can become numerically unstable during the sharpness reduction process,"}, {"title": "4.2. Improving Stability", "content": "Alleviating divergence. Approximate second-order methods can yield overly large steps when their diagonal Hessian estimations underestimate the curvature (Dauphin et al., 2015). However, this instability seems to be more present under sharpness minimization, presumably due to smaller top Hessian eigenvalue $\\lambda_1$ (Agarwala & Dauphin, 2023; Shin et al., 2024) yielding smaller estimated diagonal entries on average:\n$\\begin{aligned}\n\\mathbb{E} [\\widehat{H}]_{ii} = \\frac{1}{d} \\sum_{i=1}^d \\widehat{H}_{ii} = \\frac{1}{d} \\mathrm{tr}(\\widehat{H}) = \\frac{1}{d} \\sum_{i=1}^d \\lambda_i \\le \\lambda_1.\n\\end{aligned}$\nThis tendency toward zero intensifies numerical instability during Hessian inversion, increasing the risk of training failures.\nConventional techniques such as damping or clipping can be employed to mitigate this, although their additional hyperparameters require careful tuning. Instead, we propose square rooting the Hessian (i.e., $|\\widehat{H}|^{1/2}$), which effectively mitigates instability, allowing improved generalization performance over other alternatives without additional hyperparameters. We present empirical validation of this in Section 6.2 and Appendix E.1.\nIts benefits can be understood from two perspectives. First, the square root smoothly increase the magnitude of the near-zero diagonal Hessian entries in the denominator (i.e., $h < \\sqrt{h}$ if $0 < h < 1$) while damping and clipping either shift the entire Hessian estimate or abruptly replace its certain entries to a predefined constant, potentially leading to performance degradation without careful tuning. Alternatively, it can be interpreted as a geometric interpolation between the identity matrix and the preconditioning matrix $\\widehat{H}^{\\alpha} I^{1-\\alpha}$, which has been demonstrated to allow balancing between the bias and the variance of the population risk, thereby improving generalization (Amari et al., 2021). We specifically adopt $\\alpha = 1/2$ (i.e., square root), as it has consistently demonstrated robust performance across various scenarios (Amari et al., 2021; Kingma & Ba, 2015).\nAbsolute Hessian scaling In neural network training, the computed Hessian often contains negative entries. However, since the square-root operation we introduced applies only to positive values, these entries must first be transformed. To tackle this, we attend to the prior works of Becker et al. (1988); Yao et al. (2021) and employ the absolute function to adjust the negative entries of the diagonal Hessian to be positive, i.e.\n$\\begin{aligned}\n\\widehat{H} := \\sum_{i=1}^d \\widehat{H}_{ii} \\mathbf{e}_i \\mathbf{e}_i^\\top,\n\\end{aligned}$   (3)\nwhere $\\widehat{H}_{ii}$ and $\\mathbf{e}_i$ are the $i$th diagonal entry of the approximate diagonal Hessian and the $i$th standard basis vector, respectively. Importantly, this also preserve the same optimal rescaling as Newton's method, which can provide a more effective second-order step compared to alternatives like clipping (Nocedal & Wright, 1999; Murray, 2010; Dauphin et al., 2014; Wang et al., 2013). Additionally, this transformation mitigates the risk of convergence to critical points such as saddle or local maxima. We empirically validate the effectiveness of this approach in Appendix H and Appendix E.2."}, {"title": "4.3. Improving Efficiency via Lazy Hessian Update", "content": "While the diagonal Hessian approximation can significantly reduce computations, it still requires at least twice as much backpropagation compared to first-order methods. Here we attempt to further alleviate this by lazily computing the Hessian every $k$ steps:\n$\\displaystyle D_t = \\begin{cases}\n\\beta_2 D_{t-1} + (1-\\beta_2) |\\widehat{H}(\\mathbf{x}_t + \\epsilon^*)| & \\text{if } t \\mod k = 1 \\\\\nD_{t-1} & \\text{otherwise}\n\\end{cases},$ where $D_t$ and $\\beta_2$ are the moving average of the Hessian and its hyperparameter, respectively. This reduces the overhead from additional Hessian computation by $1/k$. We set $k = 10$ for all experiments in this work unless stated otherwise.\nHowever, extensive Hessian reusing will lead to significant performance degradation since it would no longer accurately reflect the current curvature (Doikov et al., 2023). Interestingly, SASSHA is quite resilient against prolonged reusing, keeping its performance relatively high over longer Hessian reusing compared to other approximate second-order methods. Our investigation reveals that along the trajectory of SASSHA, the Hessian tends to change less frequently than existing alternatives. We hypothesize that the introduction of sharpness minimization plays an integral role in this phenomenon by biasing the optimization path toward regions with lower curvature change, allowing the prior Hessian to remain relevant over more extended steps. We provide a detailed analysis of the lazy Hessian updates in Section 6.3."}, {"title": "4.4. Algorithm", "content": "The exact steps of SASSHA is outlined in Algorithm 1. We also compare SASSHA with other adaptive and second-order methods in detail in Appendix B, where one can see the exact differences between these sophisticated methods."}, {"title": "4.5. Convergence Analysis", "content": "In this section, we present a standard convergence analysis of SASSHA under the following assumptions.\nAssumption 4.1. The function $f$ is bounded from below, i.e., $f^* := \\inf_{\\mathbf{x}} f(\\mathbf{x}) > -\\infty$.\nAssumption 4.2. The function $f$ is twice differentiable, convex, and $\\beta$-smooth. That is, $0 < \\nabla^2 f < \\beta$.\nAssumption 4.3. The gradient $\\nabla f(\\mathbf{x}_t)$ is nonzero for a finite number of iterations, i.e., $\\nabla f(\\mathbf{x}_t) \\neq 0$ for all $t \\in \\{1, 2, ..., n\\}.\nUnder these assumptions, we derive a descent inequality for $f(\\mathbf{x}_t)$ by leveraging Adam-like proof techniques from Li et al. (2023) to handle the diagonal Hessian and employing smoothness-based bounds to account for the perturbation step based on analyses of Khanh et al. (2024). Now we give the convergence results as follows:\nTheorem 4.4. Under Assumptions 4.1-4.3, given any initial point $\\mathbf{x}_0 \\in \\mathbb{R}^d$, let $\\{\\mathbf{x}_t\\}$ be generated by the update rule SASSHA Equation (5) with step sizes $\\eta_t$ and perturbation radii $\\rho_t$ satisfying $\\sum_{t=1}^\\infty \\eta_t = \\infty$, $\\sum_{t=1}^\\infty \\eta_t^2 < \\infty$, $\\sum_{t=1}^\\infty \\rho_t \\eta_t < \\infty$. Then, we have $\\lim \\inf_{t \\rightarrow \\infty} ||\\nabla f(\\mathbf{x}_t)|| = 0$.\nThis preliminary result indicates that any limit point of SASSHA is a stationary point of $f$, ensuring progress towards optimal solutions. We refer to Appendix C for the full proof details."}, {"title": "5. Evaluations", "content": "In this section, we demonstrate that SASSHA can indeed improve upon existing second-order methods available for standard deep learning tasks. We also show that SASSHA performs competitively to the first-order baseline methods. Specifically, SASSHA is compared to AdaHessian (Yao et al., 2021), Sophia-H (Liu et al., 2024), Shampoo (Gupta et al., 2018), SGD, AdamW (Loshchilov & Hutter, 2018), and SAM (Foret et al., 2021) on a diverse set of both vision and language tasks. We emphasize that we perform an extensive hyperparameter search to rigorously tune all optimizers and ensure fair comparisons. We provide the details of experiment settings to reproduce our results in Appendix D. The code to reproduce all results reported in this work is made available for download at https://github.com/LOG-postech/Sassha.\n### 5.1. Image Classification\nWe first evaluate SASSHA for image classification on CIFAR-10, CIFAR-100, and ImageNet. We train various models of the ResNet family (He et al., 2016; Zagoruyko & Komodakis, 2016) and an efficient variant of Vision Transformer (Beyer et al., 2022). We adhere to standard inception-style data augmentations during training instead of making use of advanced data augmentation techniques (DeVries & Taylor, 2017) or regularization methods (Gastaldi, 2017). Results are presented in Table 2 and Figure 3.\nWe begin by comparing the generalization performance of adaptive second-order methods to that of first-order methods. Across all settings, adaptive second-order methods consistently exhibit lower accuracy than their first-order counterparts. This observation aligns with previous studies indicating that second-order optimization often result in poorer generalization compared to first-order approaches. In contrast, SASSHA, benefiting from sharpness minimization, consistently demonstrates superior generalization performance, outperforming both first-order and second-order methods in every setting. Particularly, SASSHA is up to 4% more effective than the best-performing adaptive or second-order methods (e.g., WRN-28-10, ViT-s-32). Moreover, SASSHA continually surpasses SGD and AdamW, even when they are trained for twice as many epochs, achieving a performance margin of about 0.3% to 3%. Further details are provided in Appendix G.\nInterestingly, SASSHA also outperforms SAM. Since first-order methods typically exhibit superior generalization performance compared to second-order methods, it might be intuitive to expect SAM to surpass SASSHA if the two are viewed merely as the outcomes of applying sharpness minimization to first-order and second-order methods, respectively. However, the results conflict with this intuition. We attribute this to the careful design choices made in SASSHA, stabilizing Hessian approximation under sharpness minimization, so as to unleash the potential of the second-order method, leading to its outstanding performance. As a support, we show that naively incorporating SAM into other architectures, where the Hessian spectrum varies significantly across different blocks. This characteristic is known to make SGD perform worse than adaptive methods like Adam on Transformer-based models (Zhang et al., 2024). Since SASSHA leverages second-order information via pre-conditioning gradients, it has the potential to address the ill-conditioned nature of Transformers more effectively than SAM with first-order methods.\nTo push further, we conducted additional experiments. First, we allocate more training budgets to SAM to see whether it compares to SASSHA. The results are presented in Table 4. We find that SAM still underperforms SASSHA, even though it is given more budgets of training iterations over data or wall-clock time. Furthermore, we also compare SASSHA to more advanced variants of SAM including ASAM (Kwon et al., 2021) and GSAM (Zhuang et al., 2022), showing that SASSHA performs competitively even to these methods (Appendix I). Notably, however, these variants of SAM require a lot more hyperparameter tuning to be compared."}, {"title": "6. Further Analysis", "content": "### 6.1. Robustness\nNoisily labeled training data can critically degrade generalization performance (Natarajan et al., 2013). To evaluate how SASSHA generalizes under these practical conditions, we randomly corrupt certain fractions of the training data and compare the validation performances between different methods. The results show that SASSHA outperforms other methods across all noise levels with minimal accuracy degradation (Table 5). Additionally, we also observe the same trend on CIFAR-10 (Table 20).\nInterestingly, SASSHA surpasses SAM (Foret et al., 2021), which is known to be one of the most robust techniques against label noise (Baek et al., 2024). We hypothesize that its robustness stems from the complementary benefits of the sharpness-minimization scheme and second-order methods. Specifically, SAM enhances robustness by adversarially perturbing the parameters and giving more importance to clean data during optimization, making the model more resistant to label noise (Foret et al., 2021; Baek et al., 2024). Also, recent research indicates that second-order methods are robust to label noise due to preconditioning that reduces the variance in the population risk (Amari et al., 2021)."}, {"title": "6.2. Stability", "content": "To show the effect of the square-root function on stabilizing the training process, we run SASSHA without the square-root (No-Sqrt), repeatedly for multiple times with different random seeds. As a result, we find that the training diverges most of the time. A failure case is depicted in Figure 4.\nAt first, we find that the level of training loss for No-Sqrt is much higher than that of SASSHA, and also, it spikes up around step 200 (Figure 4a). To look into it further, we also measure the update sizes along the trajectory (Figure 4b). The results show that it matches well with the loss curves, suggesting that the training failure is somehow due to taking too large steps.\nIt turns out that this problem stems from the preconditioning matrix $D$ being too small; i.e., the distribution of diagonal entries in the preconditioning matrix gradually shifts toward zero values (Figure 4c); as a result, $D^{-1}$ becomes too large, creating large steps. This progressive increase in near-zero diagonal Hessian entries is precisely due to the sharpness minimization scheme that we introduced; it penalizes the Hessian eigenspectrum to yield flat solutions, yet it could also make training unstable if taken naively. By including square-root, the preconditioner are less situated near zero, effectively suppressing the risk of large updates, thereby stabilizing the training process. We validate this further by showing its superiority to other alternatives including damping and clipping in Appendix E.1."}, {"title": "6.3. Efficiency", "content": "Here we show the effectiveness of lazy Hessian updates in SASSHA. The results are shown in Figure 5. At first, we see that SASSHA maintains its performance even at $k = 100$, indicating that it is extremely robust to lazy Hessian updates (Figure 5a). We also measure the difference between the current and previous Hessians to validate lazy Hessian updates more directly (Figure 5b). The result shows that SASSHA keeps the changes in Hessian to be small, and much smaller than other methods, indicating its advantage of robust reuse, and hence, computational efficiency.\nWe attribute this robustness to the sharpness minimization scheme incorporated in SASSHA, which can potentially bias optimization toward the region of low curvature sensitivity. To verify, we define local Hessian sensitivity as follows:\n$\\displaystyle \\max_{\\delta \\sim \\mathcal{N} (0, I)} \\left\\| \\widehat{H} \\left( \\mathbf{x} + \\rho \\frac{\\delta}{\\|\\delta\\|_2} \\right) - \\widehat{H} (\\mathbf{x}) \\right\\|_F,$   (4)\ni.e., it measures the maximum change in Hessian induced from normalized random perturbations. A smaller Hessian sensitivity would suggest reduced variability in the loss curvature, leading to greater relevance of the current Hessian for subsequent optimization steps. We find that SASSHA is far less sensitive compared to other methods (Figure 5c)."}, {"title": "6.4. Cost", "content": "Second-order methods can be highly costly. In this section, we discuss the computational cost of SASSHA and reveal its competitiveness to other methods.\nSASSHA requires one gradient computation (GC) in the sharpness minimization step, one Hessian-vector product (HVP) for diagonal Hessian computation, and an additional GC in the descent step. That is, a total of 2GCs and 1HVP are required. However, with lazy Hessian updates, the number of HVPs reduces drastically to 1/k. With $k = 10$ as the default value used in this work, this scales down to 0.1HVPs.\nIt turns out that this is critical to the utility of SASSHA, because 1HVP is known to take about \u00d73 the computation time of 1GC in practice (Dagr\u00e9ou et al., 2024). Compared to conventional second-order methods (1GC + 1HVP ~ 4GCs), the cost of SASSHA can roughly be a half of that (2.3GCs). It is also comparable to standard SAM variants (2GCs).\nFurthermore, we can leverage a momentum of gradients in the perturbation step to reduce the cost. This variant M-SASSHA requires only 1.3GCs with minimal decrease in performance. Notably, M-SASSHA still outperforms standard first-order methods like SGD and AdamW (Appendix K).\nTo verify, we measure the average wall-clock times and present the results in Table 6. First, one can see that the theoretical cost is reflected well on the actual cost; i.e., the time measurements scales proportionally roughly well with respect to the total cost. More importantly, this result indicates the potential of SASSHA for performance-critical applications. Considering its well-balanced cost, and that it has been challenging to employ second-order methods efficiently for large-scale tasks without sacrificing performance, SASSHA can be a reasonable addition to the lineup."}, {"title": "7. Conclusion", "content": "In this work, we focus on addressing the issue of poor generalization in approximate second-order methods. To this end, we propose a new method called SASSHA that stably minimizes sharpness within the framework of second-order optimization. SASSHA converges to flat solutions and achieves state-of-the-art performance within this class. SASSHA also performs competitively to widely-used first-order, adaptive, and sharpness-aware methods. SASSHA achieves this in robust, stable, and efficient ways without incurring much cost or requiring extra hyperparameter tuning. All of these are rigorously assessed with extensive experiments.\nNonetheless, there are many limitations to be addressed in this work for more improvements which may include, but are not limited to, extending experiments to extreme scales of various models and different data, and consolidating theoretical foundations. Seeing it as an exciting opportunity, we plan to investigate further in future work."}, {"title": "B. Algorithm Comparison", "content": "In this section, we compare our algorithm with other adaptive and approximate second-order methods designed for deep learning to better illustrate our contributions within concurrent literature. We present a detailed comparison of each methods in Table 8.\nAdam (Kingma & Ba, 2015) is an adaptive method popular among practitioners, which rescales the learning rate for each parameter dimension by dividing by the square root of the moving average of squared gradients. This adaptive learning rate effectively adjusts the gradient (momentum) at each descent step, accelerating convergence and improving update stability. Although Adam is not explicitly a second-order method, its process is related to second-order methods as it can be viewed as preconditioning via a diagonal approximation of the empirical Fisher information matrix. AdamW (Loshchilov & Hutter, 2018) proposes to improve Adam by decoupling the weight decay from the update rule for better generalization. This is also shown to be effective in most approximate second-order methods, thus employed in all subsequently mentioned algorithms.\nAdaHessian (Yao et al., 2021) is one of the earliest approximate second-order optimization methods tailored for deep learning. To reduce the prohibitive cost of computing the Hessian, it uses Hutchinson's method (Hutchinson, 1989; Roosta-Khorasani & Ascher, 2014) to estimate a diagonal Hessian approximation $\\widehat{H}_t$ and applies a moving average to reduce variance in the estimation. The authors also propose spatial averaging of the Hessian estimate, denoted as $(\\widehat{H}^{(s)})$, which involves averaging the diagonal element within a filter of a convolution layer for filter-wise gradient scaling. Sophia (Liu et al., 2024) is an approximate second-order method specifically designed for language model pretraining. Its primary feature is the use of the clipping mechanism $\\text{clip}(z) = \\max\\{\\min\\{z, \\rho\\}, -\\rho\\}$ with a predefined threshold $\\rho$ to control the worst-case update size resulting from errorneous diagonal Hessian estimates in preconditioning. Additionally, a hard adjustment is applied to each Hessian entry, substituting negative and very small values with a constant $\\epsilon$, such as $\\widehat{H}^{(c)} = \\max\\{\\widehat{h}_t, \\epsilon\\}$ to prevent convergence to saddle points and mitigate numerical instability. Furthermore, Sophia also incorporates lazy Hessian updates to enhance computational efficiency. This works without significant performance degradation as the clipping technique and hard adjustment prevent a rapid change of the Hessian, keeping the previous Hessian relevant over more extended steps.\nOur method SASSHA adds perturbation $\\epsilon$ before computing the gradient and Hessian estimation to penalize sharpness during the training process for improved generalization-an approach not previously explored in the literature. For stability, we additionally introduce two techniques: an absolute function and a square root to the Hessian estimates. The absolute function enforces Hessian estimates to be semi-positive definite while preserving their magnitude. Also, the square root smoothly adjusts underestimated curvature, stabilizing the Hessian estimates. Consequently, the blend of sharpness reduction and Hessian stabilization enables the efficient reuse of previously computed Hessians, resulting in a stable and efficient algorithm."}, {"title": "C. Convergence Analysis of SASSHA", "content": "In this section", "f": "mathbb{R"}, "d \\rightarrow \\mathbb{R}$ is convex, $\\beta$-smooth, and bounded from below, i.e., $f^* := \\inf_{\\mathbf{x}} f(\\mathbf{x}) > -\\infty$.\nAdditionally, the gradient $\\nabla f(\\mathbf{x}_t)$ is non-zero for a finite number of iterations, i.e., $\\nabla f (\\mathbf{x}_t) \\neq 0$ for all $t \\in \\{1, 2, ..., n\\}.\nAssumption C.2. Step sizes $\\eta_t$ and perturbation radii $\\rho_t$ are assumed to satisfy the following conditions:\n$\\displaystyle \\sum_{"]}