{"title": "BRIDGING VISUALIZATION AND OPTIMIZATION: MULTIMODAL LARGE LANGUAGE MODELS ON GRAPH-STRUCTURED COMBINATORIAL OPTIMIZATION", "authors": ["Jie Zhao", "Kang Hao Cheong", "Witold Pedrycz"], "abstract": "Graph-structured combinatorial challenges are inherently difficult due to their nonlinear and intricate nature, often rendering traditional computational methods ineffective or expensive. However, these challenges can be more naturally tackled by humans through visual representations that harness our innate ability for spatial reasoning. In this study, we propose transforming graphs into images to preserve their higher-order structural features accurately, revolutionizing the representation used in solving graph-structured combinatorial tasks. This approach allows machines to emulate human-like processing in addressing complex combinatorial challenges. By combining the innovative paradigm powered by multimodal large language models (MLLMs) with simple search techniques, we aim to develop a novel and effective framework for tackling such problems. Our investigation into MLLMs spanned a variety of graph-based tasks, from combinatorial problems like influence maximization to sequential decision-making in network dismantling, as well as addressing six fundamental graph-related issues. Our findings demonstrate that MLLMs exhibit exceptional spatial intelligence and a distinctive capability for handling these problems, significantly advancing the potential for machines to comprehend and analyze graph-structured data with a depth and intuition akin to human cognition. These results also imply that integrating MLLMs with simple optimization strategies could form a novel and efficient approach for navigating graph-structured combinatorial challenges without complex derivations, computationally demanding training and fine-tuning.", "sections": [{"title": "1 Introduction", "content": "Graph-structured problems are crucial across various fields due to their ability to model complex relationships [1, 2, 3]. In social networks, identifying key nodes can improve information dissemination and marketing strategies [4]. Public health also benefits, as targeting influential nodes helps develop effective immunization strategies to prevent disease spread [5]. Meanwhile, graph-structured problems are challenging because, unlike traditional Euclidean problems that leverage geometric properties for optimization, graphs are discrete structures lacking clear spatial relationships. This irregularity complicates the application of standard continuous optimization methods. In real-world applications, many graph-structured problems are NP-hard [6]. As the number of nodes and edges grows, the combinatorial explosion of possible configurations renders brute-force methods impractical within a reasonable timeframe.\nMeta-heuristic algorithms [7, 8] are effective for complicated problems but face scalability challenges with large datasets. As the problem size increases, the search space expands exponentially, making it harder to find optimal solutions efficiently. Moreover, evaluating solutions is computationally expensive, especially when many iterations are required, further limiting their scalability. Recent years have witnessed incredible progress in the use of graph neural networks (GNNs) on many graph-related tasks [9, 10], such as node classification [11, 12] and graph classification"}, {"title": "2 Related work", "content": "In this section, we review existing studies on graph-structured combinatorial problems, focusing on influence max-imization and network dismantling, as well as recent advancements involving LLMs and MLLMs applied to graph problems."}, {"title": "Influence maximization", "content": "Influence Maximization (IM) is a computational problem in network science where the goal is to identify a set of key nodes in a network that maximizes the spread of information through the network. By setting a predefined diffusion model, the greedy algorithm [4] was employed to iteratively identify the node with the largest influence spread. The Cost-Effective Lazy Forward (CELF) algorithm [26] was then proposed to significantly reduce computational complexity by leveraging the submodularity of the influence function to avoid unnecessary recalculations. Heuristic methods in influence maximization offer a balance between simplicity and effectiveness, such as degree centrality [27], betweenness centrality [28], closeness centrality [29], eigenvalues [30]and PageRank [31], etc [32]. On the other hand, several meta-heuristics have been proposed based on different bio-inspired evolutionary techniques to solve this complex combinatorial problem due to their flexible representation of solutions and effectiveness [33]. Gong et al. [7] proposed a particle swarm optimization to search for the optimal seed. Other techniques are also explored in this task, such as ant colony [34], memetic algorithm [35] and differential evolution [36].\nThe capability of GNNs has shifted research focus from traditional tasks such as node classification to more complex combinatorial optimization challenges [37]. For instance, Yu et al. redefined the influence maximization problem as a regression task by transforming the adjacency matrix into embeddings via GNNs [38]. More recently, Ling et al. introduced DeepIM [39], which seeks to capture the latent representations of seed nodes through end-to-end training."}, {"title": "Network Dismantling", "content": "Network Dismantling refers to identifying the minimal group of nodes whose removal most rapidly leads to the network's fragmentation, as outlined in the optimal percolation problem [2]. A straightforward approach involves targeting nodes based on their centrality measures, with the node degree being a primary metric. This method targets highly connected nodes or hubs [40, 41]. Various other heuristic measures of centrality are also applicable for pinpointing these critical nodes. Drawing inspiration from decycling-based techniques, CoreHD focuses on decycling a network by sequentially removing the highest-degree nodes within the 2-core [42]. Another approach, known as explosive immunization has been introduced by considering explosive percolation (EP) with strategies to keep network clusters highly fragmented. Additionally, there have been advancements in applying machine learning to network attacks, such as graph dismantling with machine learning (GDM) [43] and FINDER [44]."}, {"title": "LLMs and MLLMs on graph-structured problems", "content": "LLMs and MLLMs on graph-structured problems: LLMs have proven effective in many areas, leading to the question of their applicability to graph-structured data. Chen et al. employed LLMs as an enhancer and a predictor, respectively [22]. The LLM-based enhancer augments node features, while the LLM-based predictor directly outputs the classification. A model combining LLMs and graph learning methods named GraphLLM was proposed [45] to enhance the accuracy of reasoning tasks on the text-attributed graphs (TAGs).\nHowever, TAGs are not prevalent as it is challenging to build the label and textual feature for a huge number of nodes. Thus, these LLM-based work is still not enough to tackle real-world problems where there is only structural information available. Thus, some studies sought to directly encode graph structures into text through different prompt engineering techniques [24, 25], enabling LLMs to comprehend and analyze these structures. However, experimental results show that LLMs have significantly limited reasoning capabilities, even with small-scale networks, let alone large-scale real-world networks.\nWei et al. [46] introduced a framework that systematically converts graphs to images and feeds them into MLLMs for seven fundamental graph reasoning tasks. It provides detailed comparisons with LLMs and GNNs to showcase the advantages of using MLLMs with image representations of graphs with its visual intelligence. Similarly, VisionGraph [47] explored leveraging large multimodal models for graph theory problems in a visual context, establishing a toolchain for eight complex graph problem tasks. Beyond the basic graph problems, there are a few work using MLLMs on"}, {"title": "3 Methodology", "content": "In this section, we aim to answer critical questions on the application of MLLMs to combinatorial problems: (1) How to properly visualize the networks, especially the large-scale networks to make it can be processed by MLLMs? (2) How to refine the solution suggested by MLLMs efficiently and effectively?"}, {"title": "3.1 Visualization", "content": "Directly visualizing the network, particularly large-scale networks, on a limited canvas can result in a loss of critical structural information, such as community structures. However, when applying standard community detection algorithms like Fastgreedy [50], the number of communities detected can often exceed practical utility, especially in large networks. These algorithms tend to identify many small communities that may be of less relevance or too granular for specific applications. In such a case, it is also very difficult to reflect the essential structural information, as shown in the upper side of Figure 2. Therefore, there is a need for a method to merge these smaller communities into larger, more meaningful groups.\nHere, we propose an algorithm to merge small communities into fewer, larger communities while maintaining the integrity and connectivity of the original network structure. The goal is to reduce the number of communities to a more manageable size, aligning with the specific analytical need. That is: Given a graph G, an initial set of communities C, and a target number of communities T, we will merge smaller communities into their nearest neighbors until the number of communities is reduced to T. This algorithm is detailed as follows:\n1. Identify the Smallest Community: In each iteration, the algorithm identifies the smallest community by comparing the sizes of all communities.\n2. Count Edges to Other Communities: For each edge in the graph, the algorithm checks if the edge connects the smallest community to any other community. It keeps track of how many edges each neighboring community has connected to the smallest community.\n3. Find the Closest Community: The community with the highest number of edges connected to the smallest community is chosen as the \"closest\" community.\n4. Merge Communities: All nodes in the smallest community are reassigned to the closest community. The indices of the other communities are adjusted accordingly to reflect the reduction in the number of communities.\n5. Repeat: This process continues until the number of communities equals the target number.\nTo ensure that important nodes do not overlap and can be easily recognized by MLLMs, it is necessary to position top-ranked nodes (e.g., those with high degrees) farther apart, while other nodes are arranged closer to the community centroid. To this end, we propose a method to adjust the positions of nodes in a graph G according to their community structure and layout style, with a specific emphasis on spatial differentiation of influential nodes. Each node's initial coordinates are computed by a graph layout algorithm. Given a community structure $C = {C_1, C_2,..., C_k}$, where each $C_i$ represents a set of nodes belonging to the same community, the centroid $c_i$ of community $C_i$ is calculated as:\n$\\frac{1}{|C_i|}\\sum_{v \\in C_i} p_v$\nwhere $p_v$ denotes the position of node $v$. Nodes within each community $C_i$ are ranked based on their degrees, identifying the top-N highest-degree nodes (denoted as $T_i \\subset C_i$ with cardinality $|T_i| = N$) for further adjustment.\nFor each node $v \\in C_i$, a new position $p'_v$ is calculated to reflect its distance from the community centroid $c_i$, controlled by an adjustment parameter $d$. Specifically,\n\u2022 For nodes in $T_i$, the adjustment distance remains unchanged."}, {"title": "3.2 Local search", "content": "To further enhance the influence spread of a seed set initially suggested by MLLMs, we propose a local search method: It iteratively attempts to improve the seed set by exploring replacements: for each node in the current seed set, and the algorithm examines its neighbors (sorted by degree or betweenness) to find a suitable candidate for replacement. If replacing a node with a top-ranked neighbor increases the influence spread, the seed set is updated. The process continues until no further improvements can be made or the maximum iteration is reached. The pseudocode of the process can be found in Algorithm 1. The influence spread is evaluated using a predefined influence diffusion model, such as the Independent Cascade (IC) and Linear Threshold (LT) models. For the sake of efficiency, the iteration number is set to 5 and the simulation number of the spreading process is 5,000."}, {"title": "4 Influence maximization", "content": "Influence Maximization (IM) aims to find a subset of seed nodes $S \\subset V$ that maximizes the overall influence spread across a network. This spread is governed by a probabilistic diffusion model. The goal of the problem is to maximize $\\sigma(S)$ where $\\sigma(S)$ denotes the expected spread of influence starting from the seed set $S$."}, {"title": "4.1 Benchmarks", "content": "Degree measures the number of direct connections a node has. For a node $v$, degree centrality $DC(v)$ is given by:\n$DC(v) = \\sum_{u\\in V} a_{vu}$ \nwhere $a_{vu}$ is the element of the adjacency matrix indicating the presence of an edge between nodes $v$ and $u$.\nBetweenness measures the extent to which a node lies on the shortest paths between other nodes. For a node $v$, betweenness centrality $BC(v)$ is given by:\n$BC(v) = \\sum_{s\\neq v \\neq t} \\frac{\\sigma_{st}(v)}{\\sigma_{st}}$\nwhere $\\sigma_{st}$ is the total number of shortest paths from node $s$ to node $t$, and $\\sigma_{st}(v)$ is the number of those paths that pass through $v$.\nCloseness measures how close a node is to all other nodes in the network. For a node $v$, closeness centrality $CC(v)$ is given by:\n$CC(v) = \\frac{1}{\\sum_{u \\in V} d(v, u)}$\nwhere $d(v, u)$ is the shortest distance between nodes $v$ and $u$.\nPageRank measures the influence of a node based on the idea that connections to high-scoring nodes contribute more to the score of the node. For a node $v$, it is evaluated by:\n$PR(v) = \\frac{1-\\alpha}{|V|} + \\alpha \\sum_{u\\in N(v)} \\frac{PR(u)}{out(u)}$\nwhere $\\alpha$ is a damping factor and $N(v)$ is node $v$'s neighbors.\nCollective influence (CI) of a node at distance $l$ is determined by taking into account both the degree of the node and the degrees of nodes that are $l$ steps away [51]. Specifically, the CI of a node $v$ in a network is defined as:\n$CI_l(v) = (k_v - 1) \\sum_{u \\in \\partial B_l(v)}(k_u - 1)$,\nwhere $k_v$ is the degree of the node $v$ and $\\partial B_l(v)$ represents the set of nodes that are exactly $l$ steps away from $v$ (the boundary of the ball of radius $l$ around $v$). $k_u$ is the degree of a node $u$ in the boundary set.\nDeepIM is a GNN-based framework that models the seed set's representation within a latent space. This representation is concurrently trained with a model that comprehends the fundamental network diffusion mechanism with an end-to-end training approach [39]."}, {"title": "4.2 Experimental setting", "content": "As our work is not aiming to compare the performance of MLLMs but to explore a novel solution to graph tasks, we directly select the state-of-the-art model gpt-4o-2024-08-06 as our backbone. In network dismantling, the agent makes 20 attempts on each network. The structural details of the analyzed networks are presented in Table 1. Networks with fewer than 150 nodes are classified as small networks, while those with 150 or more nodes are classified as large networks. For influence maximization, we design 4 agents for the partial-label case and 3 agents for the full-label case, with each agent sampling nodes 10 times. In the validation, we use the Monte Carlo method to simulate 100,000 spreading processes for the IC and LT models. The infection probability of the IC model is set to 0.1. The effectiveness of influence maximization of different methods is examined with two spreading models Independent Cascade model [52] and Linear Threshold model [53]. In the following experiment, MLLM refers to the best seeds among all attempts of agents and MLLM-ls refers to the best seeds among all attempts of agents after local search. The prompt of influence maximization is shown in Table 2."}, {"title": "4.3 Small-scale network", "content": "In this section, we employ an agent-based method for IM. Each agent is equipped with unique criteria. The visualization method and agent vary with network sizes. Here, seed nodes in IM are selected simultaneously, introducing additional challenges: (1) MLLMs must account for the global pattern and interconnections among seeds; (2) The selected seeds must satisfy specific requirements, such as seed size, and ensure no repetition.\nFigure 3 shows the MLLM-based IM in small-scale networks where all nodes are visualized on a single canvas with labeled node IDs. The full-label network will be input to MLLM as an image for multiple-node selection in one go. We design each agent focusing on a different criterion. Agent 1 solely relies on the intelligence of MLLM while Agents 2 and 3 are equipped with specific hints, focusing on the distributed and central parts, respectively. The prompt for Agent 1 is also placed in front of the prompt for the other agents as the leading sentence to explain the task.\nMLLM agents are capable of selecting seed sets that align with the specified criteria in the full-label case: Due to the LLM hallucination [54, 55], we examine the feasibility and correctness of selected seeds by MLLM. The criteria include checking for repetitive or invalid nodes in the seed nodes and ensuring that the selected seed size meets our specifications. Table 3 shows that across three networks, the validation results are consistently high, with most metrics achieving 100% accuracy for all agents.\nMLLM plus local search would become a new paradigm for combinatorial optimization: Figure 4 shows the results of IM using various strategies. In both IC and LT models, the MLLM-ls consistently outperforms other strategies, achieving a higher number of infected nodes across all seed sizes compared to traditional centrality methods such as degree, betweenness, and CI, as well as representation learning-based DeepIM, in selecting seeds for IM within"}, {"title": "4.4 Large-scale network", "content": "The details of agents for the large-scale networks are shown in Figure 6. Due to the substantial number of nodes of large-scale networks, it is impractical to plot all the labels in a canvas of limited size. Thus, only a certain ratio of high-degree nodes of each network is displayed in the image.\nIn this case, the input to MLLM becomes an image with partial labels. As seen from the prompt for agents and the input image, we also include the community information compared to the full-label case. This is because (1) While MLLM shows strong spatial intelligence, we still need some assistance to explicitly guide it in selecting area nodes when incorporating selection biases. (2) There is still a lack of visualization tools that effectively display the network structure globally. Thus, we utilize community detection to cluster densely connected nodes and separate loosely connected parts for better visualization. Advancements in visualization will unlock significant potential for MLLM in large-scale graph-structured problems, which will be discussed further in Section 7."}, {"title": "5 Network dismantling", "content": "Network Dismantling (ND) ais to identify a minimal set of nodes $S \\subset V$ whose removal causes a significant reduction in the size of the largest connected component, effectively fragmenting the network. Given a network with N nodes, the robustness defined as: $R = \\frac{1}{N} \\sum_{Q=1}^{N} s(Q)$, where $s(Q)$ represents the size of the largest connected component after the removal of Q nodes."}, {"title": "5.1 Benchmark", "content": "In the comparative study of network dismantling, two commonly used benchmarks are included:\nHigh-degree (HD) repeatedly identifying and removing the node with the highest degree in the remaining network. This process is dynamic, as the degree of nodes changes after each removal, ensuring that the most connected node at each step is eliminated.\nHigh-collective influence (HCI) is similar to HD, where at each step, the node with the highest collective influence in the remaining network is removed."}, {"title": "5.2 Experimental result", "content": "MLLMS possess a strong grasp of graph structure: The prompt for network dismantling can be found in Table 5 and Figure 11 where the latter also illustrates an attempt of the network dismantling process guided by an MLLM. In traditional approaches like degree centrality, the nodes with the highest degree, such as 32 or 33, would be prioritized for removal to minimize the size of the largest connected component (LCC). However, the MLLM suggests removing node 0 first, which leads to a more rapid reduction in the LCC size, immediately to 27. This result implies the MLLM's ability to predict the cascading effects of node removal beyond the most intuitive observation (degree)."}, {"title": "6 MLLM on basic graph-related tasks", "content": "In this section, we will investigate the MLLM on some basic graph-structured tasks and identify factors affecting the performance of MLLM."}, {"title": "6.1 Synthetic network", "content": "Three types of random networks are utilized: Barab\u00e1si-Albert (BA) network, Erd\u0151s-R\u00e9nyi (ER) network and Watts-Strogatz (WS) network. Table 7 lists the structural information of these networks where BA is viewed as dense network and WS and ER are relatively sparse sometimes containing multiple connected components.\n\u2022 Erd\u0151s-R\u00e9nyi (ER) network model is a foundational concept in random graph theory [56]. In an ER network, a graph is constructed by connecting nodes randomly with a given probability p.\n\u2022 Barab\u00e1si-Albert (BA) model generates scale-free networks featured by a power-law degree distribution [57].\n\u2022 Watts-Strogatz (WS) network exhibits high clustering and short average path lengths [58]. The WS model starts with a regular ring lattice where each node is connected to k nearest neighbors and with a probability p, each edge is randomly rewired then.\nThe visualization of networks has different layouts. In this work, we have tested three types to investigate the influence of layouts on the effectiveness of MLLMs.\n\u2022 Fruchterman-Reingold Layout is a force-directed algorithm that simulates physical forces between the nodes and edges of a graph. Nodes repel each other like charged particles, while edges act like springs that pull connected nodes together, to minimize edge crossings and evenly distribute them.\n\u2022 Circle Layout denotes the layout that all nodes are placed at equal distances from each other along the circumference of a circle.\n\u2022 Grid Layout arranges nodes in a regular grid pattern, with each node occupying a unique position. This layout is effective for displaying nodes in a structured, non-overlapping manner, making it easier to compare their positions and relationships."}, {"title": "7 Discussion and Prospect", "content": "In addition to the aforementioned spatial intelligence of MLLMs on graph-structured problems, another key strength of MLLMs lies in their remarkable scalability, which is particularly advantageous when dealing with large-scale networks. Real-world networks are typically massive [59], making it impractical to encode the entire network into a text-based prompt. In contrast, by leveraging visual inputs in the form of network images, MLLMs bypass this limitation. Regardless of how large or complex the network is, the input remains a fixed-size image, allowing the MLLM to interpret and process it efficiently. Unlike adjacency matrices and learned embeddings, which trade off structural information for computation, images serve as the most intuitive representation of graph structures, effectively preserving valuable high-order information such as community structures, paths, and motifs, and so on."}, {"title": "8 Conclusion", "content": "In this work, we have demonstrated the effectiveness of MLLMs in addressing complex graph-structured combinatorial problems, such as network dismantling and influence maximization. By utilizing simple prompts combined with local search strategies, our approach achieves superior performance over traditional methods and GNN-based approaches. We provided a comprehensive analysis of MLLMs' capabilities on fundamental graph tasks and identified key factors that enhance their effectiveness. Our findings reveal the potential of MLLMs to revolutionize large-scale graph problem-solving, marking a significant step toward harnessing their full capacity in practical, real-world applications. Our Future work will explore integrating visualization tools with MLLMs for interactive graph exploration, enhancing reasoning on large networks and enabling comprehensive analysis."}]}