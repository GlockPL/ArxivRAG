{"title": "Historical Trajectory Assisted Zeroth-Order Federated Optimization", "authors": ["Xiaoyu He", "Chenlin Wu", "Zike Li", "Zibin Zheng"], "abstract": "Federated learning is a distributed learning framework which enables clients to train models individually and to upload their model updates for aggregation. The local training process heavily relies on distributed gradient descent techniques. In the situation where gradient information is not available, the gradients need to be estimated from zeroth-order information, which typically involves computing finite-differences along isotropic random directions. This method suffers from high estimation errors, as the geometric features of the objective landscape may be overlooked during the isotropic sampling. In this work, we propose a non-isotropic sampling method to improve the gradient estimation procedure. Gradients in our method are estimated in a subspace spanned by historical trajectories of solutions, aiming to encourage the exploration of promising regions and hence improve the convergence. We implement this method in zeroth-order federated settings, and show that the convergence rate aligns with existing ones while introducing no significant overheads in communication or local computation. The effectiveness of our proposal is verified on several numerical experiments in comparison to several commonly-used zeroth-order federated optimization algorithms.", "sections": [{"title": "1. Introduction", "content": "This work considers solving the unconstrained federated optimization problem [1] defined as\n\nmin f(w) = \\frac{1}{N} \\sum_{i=1}^{N} E_{\\xi \\sim D_i} [F(w; \\xi)] \\tag{1}\n\nwhere N is the number of clients, \\xi is a data sample, \\mathcal{D}_i is the local data distribution associated with client i, and w \\in \\mathbb{R}^n is the decision vector. The task is to optimize the global objective f with clients only having access to their local data samples. The data distributions \\mathcal{D}_i and \\mathcal{D}_j may differ for i \\neq j, which is also known as the non independent and identically distributed (non-IID) setting.\nEfficiently solving problem (1) is the key to implementing federated learning systems. One representative of federated optimization methods is FedAvg [2]. The clients in FedAvg fetch the global parameters from the server, update them using the stochastic gradient descent (SGD) method with their local data, and upload the parameters back to the server for aggregation. The server aggregates the obtained parameters without accessing client data or"}, {"title": "2. Non-isotropic Gaussian smoothing", "content": "In this section, we assume the decision space \\mathbb{R}^n is equipped with some generic vector norm || . ||. Its dual norm is denoted by || . ||^*.\n\nWe firstly define the Gaussian smoothing based gradient estimation method that involves a non-isotropic covariance matrix, and start with the smoothness property of a function:\nDefinition 1 (Smoothness). We say a function h is L-smooth if it is differentiable and its gradient satisfies:\n\n||\\nabla h(x) - \\nabla h(y)||^*\n\nfor some constant L \\in \\mathbb{R_+}.\nThe smoothness of a function h implies it has a quadratic bound:\n\n|h(y) - h(x) - \\langle\\nabla h(x), y - x\\rangle| \\leq \\frac{L}{2} ||y - x||^2, \\forall x, y \\tag{3}\n\nGaussian smoothing is a method to generate a smooth surrogate for a function h:\nDefinition 2 (Non-isotropic Gaussian Smoothing). Given a function h : \\mathbb{R}^n \\to \\mathbb{R}, its Gaussian smoothing is defined as:\n\nh_{\\mu,C}(x) = E_{v \\sim N(0,C)}[h(x + \\mu v)] \\tag{4}\n\nwhere \\mu \\in \\mathbb{R_+} is called the smoothing radius, and C \\in \\mathbb{R}^{n \\times n} is a positive-definitive covariance matrix. When C = I_n equals to the identity matrix, the function h_{\\mu,C}(x) reduces to the standard (isotropic) Gaussian smoothing as in [14]. For generic settings of the covariance matrix C, we call h_{\\mu,C}(x) the non-isotropic Gaussian smoothing.\nThe Gaussian smoothing h_{\\mu,C} is differentiable, and its gradient has a closed-form expression that only depends on zeroth-order information [13]:\nLemma 1 (Differentiability). For a function h : \\mathbb{R}^n \\to \\mathbb{R}, its Gaussian smoothing is differentiable and has the gradient defined as:\n\n\\nabla h_{\\mu,C}(x) = E_{v \\sim N(0,C)}\n\n\\frac{h(x + \\mu v) - h(x - \\mu v)}{2\\mu}\n\nC^{-1} v\\tag{5}\n\nThe above shows that we can compute an ascent direction of h_{\\mu,C} as\n\ng = \\frac{h(x + \\mu v) - h(x - \\mu v)}{2\\mu} v, \\tag{6}\n\nwhere v is drawn from N(0, C). It is easy to see\n\nE[\\langle g, \\nabla h_{\\mu,C}(x) \\rangle] = ||\\sqrt{C} \\nabla h_{\\mu,C}(x)||^2_2 \\geq 0.\n\nThus, g is indeed a stochastic ascent direction of h_{\\mu,C}. Hereinafter we will call g the stochastic gradient of \\nabla h_{\\mu,C}(x).\nThe importance of Gaussian smoothing is that it acts as a surrogate to the original function provided the latter is smooth:"}, {"title": "2.2. Covariance matrix specification", "content": "Lemmas 1 and 2 state that the Gaussian smoothing h_{\\mu,C} serves as a good surrogate to the original objective function h, as it is smooth and has closed-form gradient. Optimizing h_{\\mu,C} instead of h would nevertheless introduce an approximation error due to the difference between these two functions. On the other hand, (8) shows that this error is bounded, and the bound depends on the covariance matrix C. It implies we can control the covariance matrix to reduce the error, which is exactly why we need the non-isotropic Gaussian smoothing. A trivial choice to minimize the bound in (8) is letting C \\to 0. This is however meaningless, as it would bring rounding errors in practice.\nIn this work we suggest constraining the covariance matrix as\n\nC = (1 - \\alpha)I_n + \\alpha QQ^T, \\tag{9}\n\nwhere \\alpha \\in [0, 1] is a constant and Q \\in \\mathbb{R}^{n \\times m} is a projection matrix satisfying QQ^T = I_m. The term m here is a user specified hyperparameter that should be smaller than n. The matrix Q is kept orthogonal, as its magnitude is not important: the magnitude of C can be absorbed into the constant \\mu. With \\alpha \\to 1, the gradients output by Gaussian smoothing are located in a subspace spanned by Q. If these gradients are used for guiding the search, the algorithm will keep staying in this subspace. On the contrary, taking \\alpha \\to 0 leads to the standard Gaussian smoothing, which estimates the gradient in the whole space \\mathbb{R}^n and hence encourages the algorithm to explore unexplored regions. In other words, the covariance matrix C controls the balance between exploration and exploitation.\nWe propose that an appropriate covariance matrix C should capture insensitive directions over the objective landscape. A direction is called sensitive if moving along this direction causes a rapid change on the objective value and vice versa. To see why the sensitiveness matters, consider an ideal setting where the objective is a quadratic function f(x) = x^T H x with symmetric positive definitive Hessian H \\in \\mathbb{R}^{n \\times n}. The objective function is 1-smooth w.r.t. the Euclidean norm ||x||_2 = x^T \\sqrt{H} x and its dual norm ||x||^*_2 = x^T \\sqrt{H^{-1}} x. In this setting, the matrix Q for minimizing the bound in (8) can be found by solving the following problem:\n\n\\begin{aligned}\n&\\underset{Q \\in \\mathbb{R}^{n \\times m}}{\\text{min}} & E_{v \\sim N(0,C)} [||v||_2^2] \\\\\n&\\text{s.t.} & C = (1 - \\alpha)I_n + \\alpha QQ^T \\\\\n& & Q^T Q = I_m\n\\end{aligned} \\tag{10}"}, {"title": "3. The Proposed Method", "content": "We apply the non-isotropic Gaussian smoothing described above to the federated optimization problem (1) and propose the zeroth-order federated optimization algorithm assisted by historical trajectories (ZOFedHT)."}, {"title": "3.1. Implementation", "content": "The ZOFedHT algorithm is given in Algorithm 1. Apart from the initial solution x_0, the algorithm receives several additional hyperparameters: a sequence of step-sizes \\eta_r, a smoothing radius \\mu, a positive integer \\tau denoting the length of historical trajectories, a"}, {"title": "3.2. Complexity", "content": "Since the covariance matrix C_r is not built from scratch but instead recovered from the projection matrix Q_r \\in \\mathbb{R}^{n \\times \\tau}, only n\\tau additional entries need to be broadcast to the clients (line 7). In addition, the broadcast is performed at every \\tau rounds, so the communication overhead is O(n\\tau) per-round, which is insignificant compared to the standard FedAvg or its zeroth-order implementations.\nComputation overheads yield on both the client side and the server side. On the server side, the thin QR factorization takes O(n\\tau^2) time for every \\tau rounds, so the averaged time complexity is O(n\\tau). On the client side, additional computation is caused by the Gaussian sampling (line 13). Recall that the covariance matrix takes the form of C_r = (1 - \\alpha)I_n +"}, {"title": "3.3. Convergence properties", "content": "We make the following assumptions regarding problem (1).\nAssumption 1. The objective function F(x;\\xi) is L-smooth in x \\in \\mathbb{R}^n for all \\xi.\nAssumption 2. The client-side data sampling has bounded variance, i.e., there exists some constant \\sigma_i \\in \\mathbb{R_+} such that\n\nE_{\\xi \\sim D_i} [||\\nabla F(x; \\xi) - E_{\\xi \\sim D_i} [\\nabla F(x; \\xi)]||^2_2] \\leq \\sigma_i^2, \\forall i \\in \\{1, ..., N\\}, x \\in \\mathbb{R}^n.\n\nAssumption 3. The dissimilarity between each local gradient and the global gradient is bounded, i.e., there exists some constant \\sigma_g \\in \\mathbb{R_+} such that\n\n||E_{\\xi \\sim D_i} [\\nabla F(x; \\xi)] - \\nabla f(x)|| \\leq \\sigma_g, \\forall i \\in \\{1, ..., N\\}, x \\in \\mathbb{R}^n\n\nAssumption 4. The global objective is bounded from below by some constant f_*, i.e., f(x) \\geq f_* for all x \\in \\mathbb{R}^n.\nAssumptions 1, 2 and 4 are customary in analyzing zeroth-order stochastic optimization algorithms. Assumption 3 measures the heterogeneity of the data distribution. For example, when \\sigma_g = 0, the data become IID and the problem degenerates to the classical distributed optimization problem.\nBelow we characterize the convergence property of ZOFedHT w.r.t. the standard l_2 norm. The proof can be found in Appendix D.\nTheorem 1. Let Assumptions 1 to 4 hold with l_2 norm ||\\cdot|| = ||\\cdot||^* = ||\\cdot ||_2. Choose constant step-sizes\n\n\\eta = \\eta_r = \\frac{1}{6} \\sqrt{\\frac{(f(x_0) - f_*)M}{(n + 4)(\\sigma_g^2 + \\sigma_i^2)RKL}} \\tag{11}\n\nand constant client set-sizes |\\mathcal{W}_r| = M. Suppose the number of rounds R is sufficiently large and the number of local updates satisfies K \\leq n. Then, we have\n\n\\frac{1}{R} \\sum_{r=0}^{R-1} E [||\\nabla f(x_r)||^2] \\leq \\frac{96}{1 - \\alpha} \\sqrt{\\frac{(f(x_0) - f_*) (n + 4)(\\sigma_g^2 + \\sigma_i^2)}{RKLM}} + \\frac{8\\mu^2 L^2 (n + 6)^3}{(1 - \\alpha)^2}\n\nThe above states that ZOFedHT achieves a convergence rate of O(\\sqrt{\\frac{1}{RKM}}) when \\mu is sufficiently small. The dependence on R, K, and M coincides with that of modern FedAvg implementations [15]. ZOFedHT suffers an n-dependent slowdown, which is the price paid for not knowing the gradient. This aligns with the best known convergence rate achieved by zeroth-order federated optimization given in [1]. On the other hand, the impact of non-isotropic Gaussian smoothing is unknown yet, as choosing a non-zero \\alpha does not tighten the above bound. In the next section we verify the effectiveness of using non-isotropic Gaussian smoothing via numerical studies."}, {"title": "4. Numerical Studies", "content": "We verify the performance of ZOFedHT via training three machine learning models including logistic regression (LR), support vector machine (SVM), and multilayer perceptron (MLP). The LR model is convex and smooth; we choose it to test the local exploitation ability of ZOFedHT. SVM is also convex. But it employs a hinge loss and therefore deviates from the smoothness assumption. We choose this model to test the robustness of ZOFedHT against the landscape irregularity. The MLP model has a fully connected hidden layer with 50 neurons and uses the sigmoid activation function at both the hidden and output layers. The model is smooth but non-convex, and we consider this model mainly for verifying the algorithms' global exploration ability.\nWe implement two competitors namely ZOFedAvg-SGD and ZOFedAvg-GD by equipping FedAvg with zeroth-order versions of SGD and GD respectively. ZOFedAvg-SGD can be considered as a special instance of ZOFedHT with \\alpha = 0 and all other settings are kept the same as ZOFedHT. ZOFedAvg-GD differs from ZOFedAvg-SGD in that it uses the standard Gaussian smoothing to estimate the full-batch gradients on the client side. In all algorithms, the smoothing radius is \\mu = 10^{-4} and the number of local updates is K = 50. All algorithms use step-sizes decreasing over rounds as \\eta_r = \\eta_0 / \\sqrt{r + 1}, where \\eta_0 is tuned with a grid-search in {0.1,1,10}. In both ZOFedHT and ZOFedAvg-SGD, minibatching is used in the client-side data sampling and the batch size is fixed to 64. For ZOFedHT, the parameter \\alpha is tuned in the range {0.1, 0.2, ..., 0.9} and the parameter L is fixed to 5. On each test instance, all algorithm run three times independently, and we report the results from the run achieving the best final training loss.\nThree widely used benchmark datasets, including mnist [16], fashion-mnist [17], and rcv1 [18], are chosen. Our experiments simulate a binary classification problem using these datasets. For mnist and fashion-mnist, which have 10 labels in the range {0,1,...,9}, we classify the samples with labels in the range {0, 1, . . ., 4} as one category and the remaining samples as another. This ensures that the number of samples in these two categories is equal so that the sampling procedure in the experiments can be fair. We set the number of clients N to 100, and at each round 10 clients are sampled uniformly. Both the IID setting and non-IID setting are considered.\nWe first consider the IID case. In this case, we partition the dataset randomly and uniformly into N parts and assign each part to a distinct client before the optimization. Figure 1 displays the convergence trajectories of the algorithms. ZOFedHT is the best performer on all test instances, demonstrating the effectiveness of use of non-isotropic Gaussian smoothing."}, {"title": "5. Related Work", "content": "Gaussian smoothing is perhaps the most well-known technique for gradient estimation [14, 20]. Although using an identity covariance matrix could be effective in certain cases [19], it has been long recognized that incorporating second-order information in Gaussian smoothing"}, {"title": "6. Conclusion", "content": "We present in this article the ZOFedHT algorithm for zeroth-order federated optimization. ZOFedHT senses promising subspaces from historical trajectories of the global solution and improves convergence via using a non-isotropic Gaussian smoothing procedure on the client side. ZOFedHT aligns with existing methods in terms of the convergence rate, introducing no significant computation or communication overheads. The numerical studies suggest that ZOFedHT performs competitive to or better than the state-of-the-arts especially in dense problems."}, {"title": "Appendix A. Proof of Lemma 1", "content": "By [13, lemma 1] we have\n\n\\nabla h_{\\mu,C}(y) = E_{v \\sim N(0,C)} \\frac{h(x + \\mu v) - h(x)}{{\\mu}}C^{-1} v.\n\nSubstituting v with -v and using the fact v \\sim N(0, C) \\Leftrightarrow -v \\sim N(0, C), we have\n\n\\nabla h_{\\mu,C}(y) = E_{v \\sim N(0,C)} \\frac{h(x) - h(x - \\mu v)}{{\\mu}}C^{-1} v."}, {"title": "Appendix B. Proof of Lemma 2", "content": "For simplicity, we denote E_{v \\sim N(0,C)} by E. The first statement can be proved by definition:\n\n||\\nabla h_{\\mu,C}(y) - \\nabla h_{\\mu,C}(x)||_* = ||\\nabla E[h(y + \\mu v)] - \\nabla E[h(x + \\mu v)]||_*\n= ||E[\\nabla h(y + \\mu v) - \\nabla h(x + \\mu v)]||_*\n\\leq E[||\\nabla h(y + \\mu v) - \\nabla h(x + \\mu v) ||_*]\n\\leq L||y - x||,\n\nwhere the first equation follows from definition 2, the first inequality uses Jensen's inequality, and the last step is due to the L-smoothness of h.\nThe second statement can be derived from the smoothness assumption and the properties of Gaussian variables:\n\n\\begin{aligned}\n||\\nabla h_{\\mu,C}(x) - \\nabla h(x)|| & \\overset{(5)}{=} \\Bigg|E \\frac{h(x + \\mu v) - h(x - \\mu v)}{2\\mu} C^{-1}v - \\nabla h(x) \\Bigg|_*\\\\\n& \\overset{(a)}{=} \\Bigg|E \\frac{h(x + \\mu v) - h(x - \\mu v)}{2\\mu} C^{-1}v - C^{-1}E[v]\\nabla h(x) \\Bigg|_*\\\\\n& = \\Bigg|E \\frac{h(x + \\mu v) - h(x - \\mu v)}{2\\mu} C^{-1}v - C^{-1}E[v] \\nabla h(x) \\Bigg|_*\\\\\n& = E\\Bigg| \\frac{h(x + \\mu v) - h(x - \\mu v)}{2\\mu} - v^T\\nabla h(x)\\Bigg| ||C^{-1}v||_*\\\\\n& \\overset{(b)}{\\leq} \\frac{L\\mu}{2} E[||v|| ||C^{-1}v||_*],\n\\end{aligned}\n\nwhere (a) uses the properties of the Gaussian distribution, and (b) is due to\n\n|h(x + \\mu v) - h(x - \\mu v) - 2(\\nabla h(x), \\mu v)| \\leq L||\\mu v||^2. \\tag{B.1}\n\nwhich follows from (3).\nThe third statement, again, can be obtained using the smoothness assumption.\n\n\\begin{aligned}\n|h_{\\mu,C}(x) - h(x)| & = |E[h(x + \\mu v)] - h(x)| \\\\\n& \\overset{(a)}{=} |E[h(x + \\mu v) - h(x) - \\mu v \\nabla h(x)]| \\\\\n& \\overset{(b)}{\\leq} E |[h(x + \\mu v) - h(x) - \\mu v \\nabla h(x)]| \\\\\n& \\leq \\frac{L\\mu^2}{2} E [||v||^2],\n\\end{aligned}\n\nwhere (a) is due to E[v] = 0, and (b) uses Jensen's inequality."}, {"title": "Appendix C. Solution to problem (10)", "content": "Let u_1 \\sim N(0, I_n), u_2 \\sim N(0, I_m) be two independent vectors. Let v = \\sqrt{1 - \\alpha} u_1 + \\sqrt{\\alpha} Qu_2. It is then clear that v \\sim N(0, C). Therefore, we can write the objective function as\n\n\\begin{aligned}\nE[||v||_2^2] & = E[v^T H v] \\\\\n& = E[(\\sqrt{1 - \\alpha} u_1 + \\sqrt{\\alpha} Qu_2)^T H (\\sqrt{1 - \\alpha} u_1 + \\sqrt{\\alpha} Qu_2)] \\\\\n& = (1 - \\alpha) E[u_1^T H u_1] + \\alpha E[u_2^T Q^T H Q u_2]\n\\end{aligned}\n\nwhere the first equation follows from the quadratic function assumption, and the last uses the fact E[u_1 u_2^T] = 0 and E[u_2 u_1^T] = 0. The first term on the rightmost side of the above does not involve Q, so\n\n\\begin{aligned}\n\\underset{Q}{\\text{arg min}} E[||v||_2^2] & = \\underset{Q}{\\text{arg min}} \\alpha E[u_2^T Q^T H Q u_2] \\\\\n& = \\underset{Q}{\\text{arg min}} E[Tr[Q^T H Q u_2 u_2^T]] \\\\\n& = \\underset{Q}{\\text{arg min}} Tr[Q^T H Q E[u_2 u_2^T]] \\\\\n& = \\underset{Q}{\\text{arg min}} Tr[Q^T H Q]\n\\end{aligned}\n\nwhere the last step uses the fact E[u_2 u_2^T] = I_m. The solution to the above minimization problem, under the constraint Q^T Q = I_m, is well known, and is given by the eigenvectors corresponding to the m smallest eigenvalues of H. That is, Q corresponds to the m principal components of H^{-1}."}, {"title": "Appendix D. Proof of Theorem 1", "content": "We suppose that the number of rounds R is sufficiently large such that the step-size \\eta defined in (11) satisfies the following conditions:\n\n\\eta \\leq \\frac{1}{8LK\\sqrt{n + 4}} \\tag{D.1}\n\n\\eta \\leq \\frac{1}{LK^{1.5}\\sqrt{10M}} \\tag{D.2}\n\n\\eta \\leq \\frac{1 - \\alpha}{LK} \\tag{D.3}\n\n\\eta \\leq \\frac{M(1 - \\alpha)}{128L(n + 4)} \\tag{D.4}\n\n\\eta \\leq \\frac{1}{MLK^2} \\tag{D.5}\n\n\\eta \\leq \\frac{1}{5LK} \\tag{D.6}\n\n\\eta \\leq \\frac{4L(1 - \\alpha)}{M} \\tag{D.7}\n\nFor simplicity, denote f_i^r as the local objective function of client i and \\tilde{f}_i^r as its Gaussian smoothing at round r, i.e.,\n\nf_i^r(x) = E_{\\xi \\sim D_i}[F(x;\\xi)] and \\tilde{f}_i^r(x) = E_{v \\sim N(0,C_r)} [f_i^r(x + \\mu v)].\nWe first state some properties of Gaussian smoothing under the l_2 norm assumption."}, {"title": "Appendix E. Proof of Lemma 3", "content": "The proof follows [20, Theorem 4] with slight modifications due to the non-isotropic covariance matrix.\n\n\\begin{aligned}\nE[||g||^2] & \\overset{(6)}{=} E \\Bigg[\\frac{h(x + \\mu v) - h(x - \\mu v)}{2\\mu} \\Bigg]^2 \\\\\n& = \\frac{1}{4\\mu^2} E [h(x + \\mu v) - h(x - \\mu v) - 2(\\nabla h(x), \\mu v)v + 2(\\nabla h(x), \\mu v)v]^2 \\\\\n& \\leq \\frac{1}{2\\mu^2} E [h(x + \\mu v) - h(x - \\mu v) - 2(\\nabla h(x), \\mu v)]^2 ||v||^2 + \\frac{1}{2\\mu^2} ||(2\\nabla h(x), \\mu v)v||^2 \\\\\n& = \\frac{1}{2\\mu^2} E[|h(x + \\mu v) - h(x - \\mu v) - 2(\\nabla h(x), \\mu v)| ||v||^2] + 2 \\frac{(\\nabla h(x), v)^2}{||v||^2} \\\\\n& \\overset{(B.1)}{\\leq} \\frac{1}{2\\mu^2} (L||\\mu v||^2)^2 ||v||^2 + 2(\\nabla h(x), v)^2 ||v||^2 \\\\\n& = \\frac{\\mu^2 L^2}{2} ||v||^6 + 2(\\nabla h(x), v)^2 ||v||^2\n\\end{aligned}"}, {"title": "Appendix F. Proof of Lemma 4", "content": "Applying Lemma 3 to g_i^{r,k} (via replacing h(x) with F(w_i^{r,k}; \\xi_i^k)), we have\n\nE \\Bigg[\\frac{h(x + \\mu v) - h(x - \\mu v)}{2\\mu} \\Bigg]^2 \\leq \\frac{\\mu^2}{2} L^2(n + 6)^3 + 2(n + 4)E [||\\nabla F(w_i^{r,k}; \\xi_i^k)||^2] \\tag{F.1}"}, {"title": "Appendix G. Proof of Lemma 5", "content": "By construction, we bound the per-round descent step as\n\n\\begin{aligned}\nE[x_{r+1} - x_r] & = -E\\Bigg[\\frac{1}{M} \\sum_{i \\in \\mathcal{W}_r} \\eta g_i^{r,K}\\Bigg] \\\\\n& = -\\eta E \\Bigg[\\frac{1}{M} \\sum_{i \\in \\mathcal{W}_r} C\\nabla \\tilde{f}_i^r(w_i^{r,k})\\Bigg] \\\\\n& = - \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{k=0}^{K-1} E \\Bigg[\\nabla \\tilde{f}_i^r(w_i^{r,k})\\Bigg],\n\\end{aligned}"}, {"title": "Appendix H. Proof of Lemma 6", "content": "By construction, we have\n\n\\begin{aligned}\nx_{r+1} - x_r &= \\eta \\frac{1}{M} \\sum_{i \\in \\mathcal{W}_r} \\sum_{k=0}^{K-1} g_i^{r,k} \\\\\n&= \\frac{\\eta}{M} \\sum_{i \\in \\mathcal{W}_r} \\sum_{k=0}^{K-1} (g_i^{r,k} - \\nabla \\tilde{f}_i^r(w_i^{r,k})) \\\\\n&+ \\eta \\Bigg(\\frac{1}{M} \\sum_{i \\in \\mathcal{W}_r} \\sum_{k=0}^{K-1} \\nabla \\tilde{f}_i^r(w_i^{r,k}) - \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{k=0}^{K-1} \\nabla \\tilde{f}_i^r(w_i^{r,k})\\Bigg) \\\\\n&+ \\frac{\\eta}{N} \\sum_{i=1}^{N} \\sum_{k=0}^{K-1} \\nabla \\tilde{f}_i^r(w_i^{r,k}).\n\\end{aligned}\n\nBy the property of the Gaussian smoothing, we have E[B] = 0. In addition, since the clients are uniformly sampled, we have E[C] = 0. We therefore obtain\n\nE [||x_{r+1} - x_r||^2] = \\eta^2 E [||B||^2] + \\eta^2 E [||C||^2] + \\eta^2 E \\Bigg[\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{k=0}^{K-1} \\nabla \\tilde{f}_i^r(w_i^{r,k}) \\Bigg]. \\tag{H.1}\n\nThe term B can be bounded as\n\n\\begin{aligned}\nE [||B||^2] & \\overset{(a)}{=} E \\Bigg[\\frac{1}{M^2} \\sum_{i \\in \\mathcal{W}_r} \\sum_{k=0}^{K-1} [g_i^{r,k} - \\nabla \\tilde{f}_i^r(w_i^{r,k})] \\Bigg]^2 \\\\\n& \\overset{(b)}{\\leq} \\frac{1}{M^2} E \\sum_{i \\in \\mathcal{W}_r} \\sum_{k=0}^{K-1} ||g_i^{r,k}||^2 \\\\\n& \\overset{(c)}{\\leq} \\frac{1}{MN} E \\sum_{i=1}^{N} \\sum_{k=0}^{K-1} ||g_i^{r,k}||^2 \\\\\n& \\overset{(D.10)}{\\leq} \\frac{2K}{M} (\\psi + 8(n + 4) ||\\nabla f(x_r)||^2),\n\\end{aligned}\n\nwhere (a) uses the fact E[g] = \\nabla \\tilde{f}_i^r(w_i^{r,k}), (b) uses the fact E[||a - E[a]||^2] < E[||a||^2], and (c) is due to the uniform sampling of the clients."}]}