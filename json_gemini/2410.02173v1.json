{"title": "Efficiently Deploying LLMs with Controlled Risk", "authors": ["Michael J. Zellinger", "Matt Thomson"], "abstract": "Deploying large language models in production requires simultaneous attention to efficiency and risk control. Prior work has shown the possibility to cut costs while maintaining similar accuracy, but has neglected to focus on risk control. By contrast, here we present hierarchical chains with multi-level abstention (HCMA), which use model-intrinsic uncertainty to delegate queries along the LLM intelligence hierarchy, enabling training-free model switching based solely on black-box API calls. Our framework presents novel trade-offs between efficiency and risk. For example, deploying HCMA on MMLU cuts the error rate of Llama3 405B by 30% when the model is allowed to abstain on 20% of the queries. To calibrate HCMA for optimal performance, our approach uses data-efficient logistic regressions (based on a simple nonlinear feature transformation), which require only 50 or 100 labeled examples to achieve excellent calibration error (ECE), cutting ECE by 50% compared to naive Platt scaling. On free-form generation tasks, we find that chain-of-thought is ineffectual for selective prediction, whereas zero-shot prompting drives error to 0% on TruthfulQA at high abstention rates. As LLMs are increasingly deployed across computing environments with different capabilities (such as mobile, laptop, and cloud), our framework paves the way towards maintaining deployment efficiency while putting in place sharp risk controls.", "sections": [{"title": "Introduction", "content": "Given the recent excitement around large language models, researchers have been intensely focused on evaluating potentially transformative applications across domains. However, as the technology matures and its scale increases, efficiency starts to matter. In addition, real-world consequences begin to emerge, calling for greater risk control."}, {"title": "Related Work", "content": "The NLP community has addressed the need for greater efficiency by optimizing the internal mechanics of the transformer-based neural networks (Dao et al. (2022)), developing new sampling schemes (Leviathan et al. (2023)), performing model compression and distillation (Hinton et al. (2015), Xu et al. (2024)), caching model responses (Bang (2023)), and routing queries between different models (Chen et al. (2023b), Jiang et al. (2023), Hu et al. (2024), Aggarwal et al. (2024), Ding et al. (2024)). On risk control, researchers have considered probabilistic calibration and various approaches to selective prediction, in which a model is allowed to abstain from queries based on a confidence signal such as single-token or sequence-level probabilities (Xin et al. (2021)), entropy-based scores and consistency measures (Manakul et al. (2023), Kuhn et al. (2023), Chen et al. (2024)), lightweight probes on hidden layer embeddings (Azaria and Mitchell (2023), Kossen et al. (2024)), or outputs of separate neural networks trained to predict correctness (Xin et al. (2021), Yoshikawa and Okazaki (2023), Varshney and Baral (2023), Gupta et al. (2024), Cobbe et al. (2021), Kadavath et al. (2022)).\nHowever, previous work has not simultaneously addressed efficiency and risk control. Following previous routing approaches in which small models pass queries to larger models based on a confidence threshold (Kag et al. (2023), Chen et al. (2023b), Aggarwal et al. (2024), Wang et al. (2024), Ding et al. (2024), Sakota et al. (2024)), we incorporate selective prediction by adding \"abstention thresholds\" that let each model reject queries on behalf of the whole LLM system. In this paper, we explore the use of LLM token probabilities to instantiate this system, which we call \u201chierarchical chains with multi-level abstention.\u201d Although the most performant LLM confidence signals \u2013 when considered in isolation \u2013 are based on hidden layer embeddings, repeated sampling, and neural-network correctness predictors, these approaches are not suitable for meeting our twin goals of 1) improving efficiency while simultaneously imposing risk control, and 2) creating a framework based on black-box API calls that is easily adaptable to new tasks without requiring much data. By contrast, model-intrinsic probabilities have the advantage that they do not require white-box access to model internals, are widely available as part of API calls to third-party LLM inference providers, do not require large amounts of training data, and avoid compute overhead from repeated sampling.\n\u2022 Uncertainty Quantification: our framework of hierarchical LLM chains relies on uncertainty quantification to distinguish easy queries from difficult ones. Previous work has shown that on text classification tasks (such as MMLU), the maximum softmax probability effectively predicts mistakes, even if it is not calibrated (Hendrycks and Gimpel (2018), Plaut et al. (2024)). On natural language generation, the community has made progress with several different approaches including prompting an LLM to verify whether a response to a query is correct (Lin et al. (2022), Kadavath et al. (2022), Xiong et al. (2024)), assessing the consistency between multiple sampled LLM outputs (Lin et al. (2024), Manakul et al. (2023), Kuhn et al. (2023), Aichberger et al. (2024), Nikitin et al. (2024)), and training lightweight probes on models' internal states (Ren et al. (2023), Azaria and Mitchell (2023), Chen et al. (2024), Kossen et al. (2024))."}, {"title": "Our Contributions", "content": "First, we provide a novel formula explaining why uncertainty-based delegation between language models works. Second, we introduce a nonlinear feature transformation that makes Platt scaling a highly effective calibration technique for LLM token probabilities, providing an alternative to temperature scaling grounded in a rigorous statistical model (logistic regression). Third, we present hierarchical chains with multi-level abstention (HCMA), the first (to our knowledge) routing LLM model that incorporates selective prediction alongside cost efficiency. By computing the full two-dimensional Pareto frontier of efficient HCMA configurations on MMLU, using formulas we derive, we show that an LLM routing approach based on model-intrinsic probabilities improves over the selective prediction performance of"}, {"title": "Mathematical Framework", "content": "We present an LLM system called \u201chierarchical chains with multi-level abstention,\" which is based on delegating queries from small to large models, but allowing each model to abstain on behalf of the entire chain. First, we examine the act of delegation and explain why it works."}, {"title": "Why Does Delegation Work?", "content": "By delegation, we mean passing queries from a small model to a larger model based on estimated difficulty. Prior work has shown that this strategy outperforms random query assignment, but has not provided an explanation (Xin et al. (2021)). Although the result is intuitively plausible, the situation is nuanced because differently sized models seem to share a common sense of difficulty. Figure 1 shows this trend by exhibiting logistic regressions that predict correctness of Llama3 8B, 70B, and 405B on MMLU solely based on the transformed probability of the 8B model.\nGiven that differently sized models share a common notion of difficulty, the effectiveness of delegation depends on the net result of two opposing forces: the small model makes fewer mistakes because it only answers the easy queries, but the large model makes more mistakes since it answers only difficult queries. Thus, the effectiveness of delegation rests on the large model being less sensitive to incremental difficulty than the small model.\nThe proposition below frames our reasoning in mathematical terms:\nProposition 1. Compared to randomly assigning queries to a small language model $M_{sm}$ and $M_{lg}$, the reduction in error from forwarding queries based on a delegation decision $D$ is\n$\\Delta E = Cov(\\mathbb{1}_D, \\mathbb{1}_{M_{lg} \\text{ makes an error}}) - Cov(\\mathbb{1}_D, \\mathbb{1}_{M_{sm} \\text{ makes an error}}).\\tag{1}$\nIn this formula, the covariances $Cov(\\mathbb{1}_D, \\mathbb{1}_{M_{sm} \\text{ makes an error}})$ and $Cov(\\mathbb{1}_D, \\mathbb{1}_{M_{lg} \\text{ makes an error}})$ measure the impact of the delegation decision on each model's propensity to make a mistake. Empirically, both covariances are positive if $D$ is based on query difficulty, since both models' performance degrades with incremental difficulty. However, as long as $Cov(\\mathbb{1}_D, \\mathbb{1}_{M_{sm} \\text{ makes an error}}) > Cov(\\mathbb{1}_D, \\mathbb{1}_{M_{lg} \\text{ makes an error}})$, meaning that the smaller model is more sensitive to difficulty, the change in error in (1) is negative, and delegation outperforms random query assignment."}, {"title": "Hierarchical Chains with Multi-Level Abstention", "content": "A HCMA consists of a chain $M_1 \\rightarrow ... \\rightarrow M_k$ of LLMs. A query is first sent to $M_1$, which makes a decision whether to provide an answer to the query (ACCEPT), delegate it"}, {"title": "Analyzing System Performance: Efficiency and Risk", "content": "To measure performance of an HCMA in terms of efficiency and risk control, we propose the following metrics: error rate, cost, and abstention rate (the fraction of queries on which the chain refuses to give an answer, in order to control risk). We measure cost in terms of either money ($) or latency (ms). Since a HCMA delegates queries sequentially along the chain, costs add up as a query propagates more deeply into the chain. For example, suppose the HCMA $M_1 \\rightarrow M_2 \\rightarrow M_3$ has model-specific costs $c_1$, $c_2$, and $c_3$. Then, whenever the query ends at model i, the effective cost is $C_i = \\sum_{j=0}^i c_j$.\nThe following proposition provides formulas for the HCMA performance metrics, based on the graph structure in Figure 2."}, {"title": "Making Model-Intrinsic Uncertainty Work For HCMAs", "content": "The policy (2) of each model $M_j$ in a HCMA $M_1 \\rightarrow ... \\rightarrow M_k$ requires an estimated probability of correctness $p_{M_j}(\\cdot)$ for $j = 1,2,..., k$. As we rely on each model's intrinsic"}, {"title": "Results", "content": "Our exploration of hierarchical chains with multi-level abstention (HCMA), based on model-intrinsic uncertainty, yields several findings. First, our nonlinear transformations make Platt scaling much more effective in calibrating LLM output probabilities, yielding a statistically grounded way of performing LLM calibration. Second, mapping out the Pareto frontier of efficient HCMA configurations shows novel risk and efficiency trade-offs that outperform single-model strategies for selective prediction with model-intrinsic uncertainty. Finally, we apply our calibrated correctness prediction methodology to the open-ended QA benchmark TruthfulQA. We discover that chain-of-thought prompting reduces the utility of the abstention signal, highlighting the need for caution in applying established prompting techniques in uncertainty quantification."}, {"title": "Transforming Raw Probabilities Significantly Enhances Platt Scaling", "content": "Table 1 shows that our modified Platt scaling approach for risk control based on logistic regression strongly outperforms naive Platt scaling. The data shows the results for n = 50 training examples on MMLU, showing that our approach is highly data-efficient. Each number is the result of randomly sampling 50 training examples from the MMLU validation set and evaluating on the remaining 1480, repeated 100 times (except in the case of Llama3 405B, where we repeated 500 times)."}, {"title": "HCMA Provides Novel Risk & Efficiency Trade-Offs", "content": "We study the risk and efficiency trade-offs attainable on MMLU using a HCMA consisting of the Llama3 8B, 70B, and 405B models. To do this, we first tabulate achievable performance metrics \u2013 error, cost, and abstention rate \u2013 for each 5-dimensional HCMA configuration (involving acceptance thresholds $a_{8B}$, $a_{70B}$ and rejection thresholds $r_{8B}$, $r_{70B}$, and $r_{405B}$) by performing a grid search along the quantiles of the estimated correctness probabilities with 2.5% resolution, resulting in >50 million distinct configurations. For the simulation, we use cost values of 0.3, 0.8, and 5.0 dollars per million tokens, which are close to the current pricing of LLM inference providers (Artificial Analysis (2024)). From the achievable performance profiles, we compute the efficient Pareto frontier using the Skyline operator (B\u00f6rzs\u00f6nyi et al. (2001))."}, {"title": "Early Abstention Makes Lowest Risk Cheaper", "content": "To ablate our finding that the HCMA outperforms the selective prediction performances of the off-the-shelf Llama3 models, we consider whether multi-level abstention yields a benefit. Specifically, we compare an HCMA to a constrained version of an HCMA in which only the last model in the chain may abstain, disallowing early abstentions from smaller models.\nIn the case of a two-model HCMA in which Llama3 8B delegates to 70B (\"8B\u219270B\"), we report two findings. First, allowing early abstention can yield a cost advantage in both dollar cost and latency, yielding respective improvements of 7% and 6% on MMLU (where we measured latency by querying the Fireworks AI API from our location in California). Second, when imposing a constraint that the average cost must be less than or equal to some value, allowing early abstention strictly dominates the error-abstention curve without multi-level abstention at higher abstention rates between 20% and 50%."}, {"title": "Chain-of-Thought Prompting May Impede Selective Prediction", "content": "On TruthfulQA, we observe that token probabilities derived from chain-of-thought based verification perform poorly with our approach, since the transformed proba-"}, {"title": "The policy", "content": "$\\pi_{M_j}(p_{M_j}) =\\begin{cases}\\text{REJECT} &\\text{if } p_{M_j} < r_j,\\\\text{DELEGATE} &\\text{if } p_{M_j} \\geq r_j \\text{ but } p_{M_j} < a_j,\\\\text{ACCEPT} &\\text{if } p_{M_j} \\geq a_j,\\end{cases}\\tag{2}$"}, {"title": "Metrics", "content": "$P(\\text{Error}) = \\sum_{j=1}^k P_{(x,y)\\sim p(x,y)} (\\pi_1 = \\text{DELEGATE}, ..., \\pi_{j-1} = \\text{DELEGATE}, \\pi_j = \\text{ACCEPT}, Y_j \\neq y),\\tag{3}$\n$P(\\text{Abstain}) = \\sum_{j=1}^k P_{(x,y)\\sim p(x,y)} (\\pi_1 = \\text{DELEGATE}, ..., \\pi_{j-1} = \\text{DELEGATE}, \\pi_j = \\text{REJECT}),\\tag{4}$\n$E[\\text{Cost}] = \\sum_{j=1}^k P_{(x,y)\\sim p(x,y)}(\\pi_1 = \\text{DELEGATE}, ..., \\pi_{j-1} = \\text{DELEGATE}, \\pi_j \\neq \\text{DELEGATE}) C_j,\\tag{5}$"}, {"title": "Monte Carlo", "content": "$P(\\text{Error}) = \\sum_{j=1}^k\\frac{1}{n}\\sum_{i=1}^n \\mathbb{I}[p_1(x^{(i)}) \\in [r_1, a_1), ..., p_{j-1}(x^{(i)}) \\in [r_{j-1}, a_{j-1}), p_j(x^{(i)}) \\geq a_j] (1 - p_j(x^{(i)})),\\tag{6}$\n$P(\\text{Abstain}) = \\sum_{j=1}^k\\frac{1}{n}\\sum_{i=1}^n \\mathbb{I}[p_1(x^{(i)}) \\in [r_1, a_1), ..., p_{j-1}(x^{(i)}) \\in [r_{j-1}, a_{j-1}), p_j (x^{(i)}) < r_j],\\tag{7}$\n$E[\\text{Cost}] = \\sum_{j=1}^k(\\frac{1}{n}\\sum_{i=1}^n \\mathbb{I}[p_1 (x^{(i)}) \\in [r_1, a_1), ..., p_{j-1}(x^{(i)}) \\in [r_{j-1}, a_{j-1}), p_j (x^{(i)}) \\notin [r_j, a_j]]) C_j,\\tag{8}$"}, {"title": "The Probability Formula", "content": "$p_{tr}(p_{raw}) = log(\\frac{1}{1 - p_{raw}}),\\tag{9}$"}, {"title": "Applying Transforamtion", "content": "$p_{tr} (p) =\\begin{cases}log(\\frac{p}{1-p}) &\\text{if } p \\geq 0.5\\\\log(2) - log(\\frac{1}{1-p}) &\\text{if } p < 0.5\\end{cases}\\tag{10}$"}]}