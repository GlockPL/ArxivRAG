{"title": "TinySubNets: An efficient and low capacity continual learning strategy", "authors": ["Marcin Pietro\u0144", "Kamil Faber", "Dominik \u017burek", "Roberto Corizzo"], "abstract": "Continual Learning (CL) is a highly relevant setting gaining traction in recent machine learning research. Among CL works, architectural and hybrid strategies are particularly effective due to their potential to adapt the model architecture as new tasks are presented. However, many existing solutions do not efficiently exploit model sparsity, and are prone to capacity saturation due to their inefficient use of available weights, which limits the number of learnable tasks. In this paper, we propose TinySubNets (TSN), a novel architectural CL strategy that addresses the issues through the unique combination of pruning with different sparsity levels, adaptive quantization, and weight sharing. Pruning identifies a subset of weights that preserve model performance, making less relevant weights available for future tasks. Adaptive quantization allows a single weight to be separated into multiple parts which can be assigned to different tasks. Weight sharing between tasks boosts the exploitation of capacity and task similarity, allowing for the identification of a better trade-off between model accuracy and capacity. These features allow TSN to efficiently leverage the available capacity, enhance knowledge transfer, and reduce computational resources consumption. Experimental results involving common benchmark CL datasets and scenarios show that our proposed strategy achieves better results in terms of accuracy than existing state-of-the-art CL strategies. Moreover, our strategy is shown to provide a significantly improved model capacity exploitation.", "sections": [{"title": "Adaptive quantization and non-linear quantization", "content": "Algorithm 1 showcases the adaptive quantization process. The procedure begins by applying non-linear quantization to the task-pruned model using minimal values for the parameters w (number of centroids) and 4 (bit-width) (line 1). This initial step utilizes the non-linear quantization algorithm described in Algorithm 2. Following the quantization, the model's accuracy is evaluated (line 2). If the accuracy drop exceeds a predefined threshold 8 (line 3), an iterative process is initiated to incrementally increase the bit-width (lines 4-7). This adjustment continues until the accuracy loss is within acceptable limits.\nThis adaptive approach ensures that the quantized model maintains a performance close to the original model while optimizing memory usage. By incrementally increasing the bit-width only when necessary, the algorithm finds a balance between model size and accuracy, making it effective for scenarios with strict memory constraints.\nAlgorithm 2 presents our adopted non-linear quantization approach, which was inspired by (Pietron et al. 2019). The process begins with identifying the number of centroids w and creating an empty codebook K to store the mapping between codes and weight values for each layer (lines 1-2). Additionally, we generate a copy of the model, where the weights will contain codes from the codebook (centroid indices) instead of actual values.\nThe next step involves clustering the weight values of each layer \u03b8\u2081 using the K-Means clustering algorithm, with a specified number of clusters (w) (line 5). This step produces two outputs: a list of centroids C and a list of weight-to- centroid-index assignments A. Each weight is then assigned its corresponding centroid index (lines 6-8).\nTo maintain the mapping between centroid indices and centroids, a codebook is constructed for each layer (lines 9-11). The algorithm ultimately returns the model copy with quantized weights, now represented as codes from the code- book, and the codebook that is essential for converting the codes back to their actual weight values.\nIt is important to note that the number of centroids is directly related to the desired bit-width (as indicated in line 1). Fewer centroids result in reduced memory requirements for storing the weights, as each weight is stored as an index of its closest centroid. This approach ensures that memory usage is minimized while retaining the essential information needed for the model's performance."}, {"title": "Formal details", "content": "Pruning is defined as a function:\n$P : \\theta_\\iota \\rightarrow \\theta_\\iota', \\theta_\\iota' = M_\\iota \\odot \\theta_\\iota$  (1)\nwhere: $M_i$ is a binary matrix with bits assigned to single weights\n$\\sum_{i=1}M_i = \\Upsilon$  (2)\nQuantization assigns centroids to weights:\n$Q : \\theta_\\iota \\rightarrow Y_{K_\\iota},Y_{K_\\iota} = min\\forall Y_{K_\\iota} |Y_{K_\\iota} - \\theta_\\iota|$ (3)\n$K_\\iota : X \\rightarrow Y, X \\in Z \\land Y \\in R$ (4)\n$|X| = 2^{\\omega_t}$ (5)\nThe equation for accuracy loss can be defined as:\n$L_{Cl} = L_{ce}(Y, FQ(P(\\Theta)) + L_{mse}(fQ(P(\\Theta_{i})), f_{Q(P(\\Theta_{i}))}))$ (6)\nKullback-Leibler for computing tasks divergence is defined as:\n$D_{KL}(D_t||D_{t-1}) = P(D_t)\\cdot \\frac{P(D_t)}{Q(D_{t-1})}$ (7)"}, {"title": "Huffman encoding and decoding", "content": "Each tasks binary mask is represented as binary matrix. Each value indicates if weight is used (value 1) or not used by the task (value 0). It is just sparse matrix with nonzero 1 values. Therefore each mask can be compressed. In presented approach mask is encoded using huffman algorithm. The most frequent sub-sequences with their probabilities are are extracted. Then mask is represented as a tree and the dictionary in which mapping is stored (pairs with sequence in original mask and its corresponding code). When mask is loaded the decoding process is run to receive original mask."}, {"title": "Decoding quantized weights", "content": "The quantized weights are encoded in following manner (from fig.2):\n\u2022 bank\n\u2022 prefix (which indicates to which task weight belongs to)\n\u2022 quantized weight value in the codebook\nExample: 000 11 represents the value -0.3576 (weight $W_{0,1,3}$ in layer 1 and task 1, see fig.2a)"}, {"title": "Case study", "content": "In this subsection we describe more in detail an example of how TinySubNetworks manages memory capacity, as shown in Figure ?? (with weight sharing, without replay memory, task 1, 2 and 3 share the weights, TSN-wr):\n\u2022 Original model has capacity in bits: $CAP_{L1} = 632b, CAP_{L2} = 632b, \\rightarrow 384b$.\n\u2022 After pruning of task 1 capacity: 4 weights in first layer and 4 in second layer, after pruning and quantization of task 1: $CAP_{t1} = 426 + 42b + 136b \\rightarrow 152b$.\n\u2022 After pruning of task 2 capacity: it adds 1 weight in first layer and 1 weight in second layer, after pruning and quantization of task 2: $CAP_{t2} = 152b + 2 * 1b + 66b \\rightarrow 220b$.\n\u2022 Task 3 uses weights added by second task and shares some subset of weights from task 1: $CAP_{t3} = 220b$."}, {"title": "Hyperparameter optimization", "content": "In regards to hyperparameter optimization and overfitting, we resorted to basic heuristics for suitable values for most hyperparameters. Regarding the adaptive pruning process, after the learning process for a given task is completed, the global consolidated mask is updated, and codebook are updated. This process leverages a validation set in order to prevent overfitting. Moreover, sparsity levels in Algorithm 2 are optimized resorting to a greedy non-gradient fine-tuning approach, which does not incur in overfitting issues."}, {"title": "Additional results", "content": "In this subsection, we present different results that highlight specific aspects of our method. Results in Table 1 show that the capacity occupation in different scenarios for the com- pressed model. Masks and weights are a small portion of the original model (after pruning and quantization). Size of the codebooks is negligible."}, {"title": "Comparison with WSN and Ada-Q-Packnet", "content": "While we recognize that our proposed approach is inspired by the best features of WSN (Kang et al. 2022) (weight sharing) and Ada-QPacknet (Pietron et al. 2023) (pruning and quantization), we did not simply adopt these features. Instead, we developed more effective versions of the same, and devised our own method to synergically combine them."}, {"title": "Quantitative comparison with NPCL and QDI", "content": "Regarding NPCL (Jha et al. 2024), the authors leverage a ResNet-18 model backbone across all class incremen- tal experiments and two-layer fully connected network for task incremental (p-MNIST). In contrast, we adopt model backbones that are significantly smaller than Resnet-18 for most class incremental scenarios: a reduced AlexNet vari- ant (s-CIFAR100), and TinyNet (TinyImagenet), whereas we adopt Resnet-18 only for the two most complex scenarios (5 datasets and Imagenet100). In case of task incremental we are using similar achitecture: a 2-layer neural network with fully connected layers (p-MNIST). Nevertheless, we are able to achieve a better performance in terms of accuracy in all these scenarios. A comparison of the experimental results show that our method outperforms NPCL in terms of Accu- racy in different scenarios: S-CIFAR-100 (77.27 vs. 71.34), TinyImagenet (80.10 vs. 60.18), and p-MNIST (97.14 vs. 95.97).\nAs for QDI (Madaan et al. 2023), model backbones adopted in the paper are comparable to ours. Comparing the results shows that our method outperforms QDI on Tiny- Imagenet (80.10 vs. 66.79) and QDI outperforms our method on s-CIFAR100 (77.27 vs. 88.30). However, one important consideration is that the setup used for TinyImagenet is 10 tasks with 20 classes, whereas our setting is 40 tasks with 5 classes. At the same time, the setup used in QDI for s- CIFAR100 presents 20 tasks with 5 classes, whereas in our work we consider 10 tasks with 10 classes. Moreover, consid- ering accuracy in isolation may lead to a reductive analysis. A potential pitfall in the QDI paper is that there is no dis- cussion (or empirical analysis) about model capacity (NPCL runs the models with similar size but there is no additional step of capacity reduction). This makes it difficult to gauge the trade-off between accuracy and memory impact of the method, which is one of our distinctive goals.\nIt is also worth noting that NPCL (Jha et al. 2024) and QDI (Madaan et al. 2023) papers did not provide experi- ments with the two most complex scenarios (5 datasets and Imagenet100) considered in our study, where our method achieves a remarkable performance."}, {"title": "Ablation analysis", "content": "In regards to model capacity, our findings reveal that our pruning step, which involves weight sharing, is directly re- sponsible for reducing memory footprint. As a result, com- paring our approach to Ada-QPacknet and WSN leads to a significant improvement in capacity (e.g. from 81.25% and 77.73% to 22.65% for p-MNIST \u2013 other examples are pre- sented in Table 1). We argue that in case of Ada-QPacknet the quantization step contributes to model capacity to a lesser degree than adaptive pruning, since the bit-width achieved in our experiments is comparable to Ada-QPacknet (Pietron et al. 2023). The quantization step contributes significantly when we try to compare TSN with WSN (WSN has no reduc- tion stage for the bit-width of the parameters and activations). Fine-tuning just minimally contributes to model capacity.\nAs for model accuracy, we argue that performance gains are obtained thanks to our pruning approach with dynamic masks updated during the training process. This approach provides a significant improvement over other methods where masks are statically initialized, such as Ada-QPacknet (see Table 2)."}, {"title": "Open challenges", "content": "As for potential limitations of the proposed approach, we envision that if the divergence between tasks is too high (low inter-task similarity) adaptive pruning may struggle to iden- tify suitable shared weights, leading to memory saturation to accommodate all tasks. This possible limitation can be also identified in other continual learning methods such as WSN (Kang et al. 2022) and Ada-QPacknet (Pietron et al. 2023). Similar considerations can be drawn for very long scenarios (e.g. characterized by hundreds of tasks). In this particular case, adaptive pruning could be less effective, and quantization is expected to be the main driver for memory reduction. In general, future research is required to assess the robustness of continual learning methods in very complex scenarios."}]}