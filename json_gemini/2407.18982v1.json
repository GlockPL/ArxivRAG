{"title": "Low-Latency Privacy-Preserving Deep Learning Design via Secure MPC", "authors": ["Ke Lin", "Yasir Glani", "Ping Luo"], "abstract": "Secure multi-party computation (MPC) facilitates privacy-preserving computation between multiple parties without leaking private information. While most secure deep learning techniques utilize MPC operations to achieve feasible privacy-preserving machine learning on downstream tasks, the overhead of the computation and communication still hampers their practical application. This work proposes a low-latency secret-sharing-based MPC design that reduces unnecessary communication rounds during the execution of MPC protocols. We also present a method for improving the computation of commonly used nonlinear functions in deep learning by integrating multivariate multiplication and coalescing different packets into one to maximize network utilization. Our experimental results indicate that our method is effective in a variety of settings, with a speedup in communication latency of 10~20%.", "sections": [{"title": "1. Introduction", "content": "Secure multi-party computation (MPC) [1, 2] enables parties to compute securely over their private data without revealing the data to each other. Secure MPC offers privacy-preserving property, which makes it suitable for most privacy-sensitive domains, such as medical research and finance. Upon the development of deep learning techniques, the ability to capture important information from large datasets of neural models raises concerns regarding the surveillance of individuals [3]. In this case, the prospects of secure MPC demonstrate its application to secure machine learning and deep learning. While MPC-based deep learning frameworks have achieved significant performance in general scenarios, most works suffer from the limitations caused by 1. network communication due to the nature of exchanging intermediate information during MPC execution, 2. excessive computation introduced by complex MPC protocols. Since the computation of MPC protocols is largely determined by their sophisticated design, optimizing the protocol itself would seem to be difficult and infeasible. Thus, some studies [4] are concerned with improving the communication stage of MPC protocols to make them more practical.\nIn this paper, we present an approach to reduce the communication latency of the MPC protocol through optimized multivariate multiplication.\nIn general, privacy-preserving deep learning frameworks usually adopt secret-sharing-based techniques to avoid extensive computational overheads [5, 6, 7, 8]. Consequently, secret-sharing-based methods require multiple exchanges of intermediate results to achieve collaborative MPC operations. As these MPC techniques are based on linear computations, such as addition and multiplication, modern deep learning techniques that inherently rely on linear algebra benefit significantly from them. Considering the heavy dependency on linear operations, our research aims to reduce unnecessary communication rounds following [9] during the execution of MPC protocols.\nOur main contributions are as follows:\n\u2022 We propose a low-latency secret-sharing-based method for computing multivariate multiplications and univariate polynomials using network communication that is efficient and reduces unnecessary communication rounds on the fly.\n\u2022 We improve the computation of nonlinear functions by integrating the proposed multivariate multiplication and coalescing different packets into one single packet to maximize network utilization.\n\u2022 We conducted experiments to evaluate the effectiveness of our method in the context of"}, {"title": "2. Background", "content": ""}, {"title": "2.1. Arithmetic Secret Sharing Based Scheme", "content": "Our setup is primarily focused on arithmetic operations, so we represent all inputs and intermediate results in terms of linear secret sharing between n parties, especially in the context of additive secret-sharing schemes.\nApart from the general (n,t)-Shamir secret sharing scheme [10], which relies on the degree-t polynomials over n parties, we adopt the simple arithmetic secret sharing scheme based on (n, 0)-Shamir secret sharing. In other words, we share a scalar value $x \\in Z/QZ$ across n parties $P$, where $Z/QZ$ denotes a ring with Q elements, following the notations of [5]. The sharing of x is defined as $[x] = \\{[x]_p\\}_{p\\in p}$, where $[x]_p$ is the party p's share of x. The ground-truth value x could be reconstructed from the sum of the shares of each party, i.e. $x = \\sum_{p\\ep}[x]_p$.\nWhen parties wish to share a value x, they generate a pseudorandom zero-share that sums to 0. The party that possesses the value adds x to their share in secret. To represent floating-point numbers, we adopt a fixed-point encoding to encode any floating-point number $X_F$ into a fixed-point representation, x. Alternatively, we consider that each x is the result of multiplying a floating-point number $X_F$ by a scaling factor $B = 2^L$ and rounding to the nearest integer, i.e. $x = [BX_F]$. Here L is the precision of the fixed-point encoding. To decode a ground-truth floating-point value $X_F$ from x, we compute as follows: $X_F \\approx x/B$."}, {"title": "2.2. Arithmetic Secret Sharing Based MPC", "content": "It is noteworthy that arithmetic secret shares are homomorphic and can be used to implement secure MPC, especially in the context of linear computation in most cases.\nAddition. The sum of two secret shared values [x] and [y] could be directly computed as $[z] = [x] + [y]$, where each party $p \\in P$ computes $[z]_p = [x]_p + [y]_p$ without multi-party communications.\nMultiplication. Two secret shared values [x] and [y] are multiplied using a random Beaver triple [11] generated by the Trusted Third Party (TTP): a triplet ($[a], [b], [ab]$). It should be noted that the Beaver triple could be shared in advance by each party. The parties first calculate $[e] = [x] - [a]$ and $[\\u03b4] = [y] - [b]$. In this way, the [e] and [$\\delta$] are then revealed to all parties (denoted as Reveal()) without compromising information since the ground-truth values a, b remain unknown to each party except for the TTP. The final results could be computed as $[xy] = [c] + \\epsilon[b] + [\\alpha]\\delta + \\epsilon\\delta$. Algorithm 1 illustrates the multiplication using Beaver triples.\nLinear functions. It is possible to implement functions that consist of linear operations by combining additions and multiplications. Common operations in deep learning, such as element-wise product and convolution, are allowed in a linear paradigm.\nNonlinear functions. Due to the inherent infeasibility of nonlinear functions in the standard arithmetic secret-sharing scheme, most works use approximation methods to simulate the outcome of nonlinear functions. In particular, Taylor Expansion, Newton-Rhapson, and Householder methods are commonly used to approximate nonlinear functions using only linear operations. All reciprocal functions, exponential functions, loss functions, kernel functions, and other useful functions in deep learning are calculated this way, for example."}, {"title": "2.3. Notations", "content": "This section summarizes the notations used throughout this work. We denote [x] as a secret sharing of x. Reveal([x]) means that the ground-truth value x is revealed to every party involved in the computation through one-round communications. Since most linear operations are also applicable to"}, {"title": "3. Related Work", "content": "To achieve communication-efficient MPC, various approaches have been developed to optimize the communication rounds and the throughput of communication. Ishai and Kushilevitz [12] proposes a new representation of polynomials for round-efficient secure computation, dividing high-degree polynomials into multiple low-degree polynomials that are easy to solve. Mohassel and Franklin [13] performs operations directly on polynomials, such as polynomial multiplication and division. Dachman-Soled et al. [14] improves the evaluation of multivariate polynomials with different variables being held as private inputs by each party. Then, Lu et al. [4] proposes an efficient method for evaluating high-degree polynomials with arbitrary numbers of variables.\nWhile the current research has focused on improving the calculation of polynomials, our study aims to develop a communication-efficient and effective MPC system for use in modern deep learning frameworks by leveraging arithmetic tuples computation from Krips et al. [9]. This system is not confined to only computing polynomials within finite rings, as seen in previous studies.\nIn recent years, several deep learning frameworks that preserve privacy have emerged to enable the secure inference of neural network models. Wagh et al. [8] implements a maliciously secure 3-party MPC protocol from SecureNN [15] and ABY\u00b3 [16]. Knott et al. [5] provides flexible machine-learning APIs with a rich set of functions for secure deep learning. Li et al. [7] presents a fast and performant MPC Transformer inference framework designed to be privacy-preserving. Our low-latency linear MPC implementation is built on top of Knott et al.'s CrypTen framework and provides a significant improvement in communication latency."}, {"title": "4. Methodology", "content": ""}, {"title": "4.1. Multivariate Multiplication", "content": "Since Beaver triples illustrate how to multiply two variables with pre-shared triplets, a classic multiplication between multiple variables, such as [xyz], requires several rounds of binary multiplication, i.e. Mul(Mul([x], [y]), [z]). This naive implementation, however, introduces additional communication rounds during the on-the-fly Reveal process. In general, a n-ary multiplication requires n - 1 rounds of communication.\nTo reduce the communication rounds involved in the multivariate multiplications, the basic binary Beaver triple is extended into a general n-ary Beaver triple. This results in only one round of communication required throughout the entire process.\nAssume the n inputs could be represented as ${\\{[x_i]\\}}_{i=1}^n$. The precomputation and preshared information required by the extended protocol is ${\\{A_i\\}}_{i=1}^n$. Here $A_1 := \\{[a_i]\\}_{i=1}^n$ is defined as the set of n auxiliary shared values used to blind the the inputs ${\\{[x_i]\\}}_{i=1}^n$, which is also similar to the Beaver's idea. Then $A_i (i > 2)$ is defined as the set of shared degree-i cross-terms consisting of the variables in $A_1$. For example, $A_2$ could be defined as $A_2 := \\{[a_ia_j] | i \\neq j \\land 1 < i,j < n\\}$, and $A_3 := \\{[a_ia_ja_k] | i \\neq j\\neq k \\land 1 < i,j,k \\le n\\}$, and so on. Similar to the construction of [e] and [$\\delta$] in Section 2.2, we define the difference between the inputs and the masks as $[d_i] := [x_i] - [a_i]$. The secret-shared $[d_i]$ is then made public across all parties without leakage to the ground-truth value of both $[x_i]$ and $[a_i]$. The improvement of our method originates from the following equation:\n$\\prod_{i=1}^n x_i = \\prod_{i=1}^n (\\delta_i + a_i)$ = $\\prod_{i=1}^n a_i + \\sum_i \\delta_i \\prod_{A_i} a_i + \\sum_{i,j, i\\neq j} \\delta_i\\delta_j\\prod_{A_i...a_j} a_i + \u2026\u2026+ \\prod_{a_i}$.\nHere we informally use the fractional representation, such as $\\prod_{A_i...a_j} a_i$ to denote the products of all the terms except for certain ones. Note that this fractional form does not involve any actual division. Also, each secret-shared term of $\\prod_{a_i...a_j} a_i$ could be found in the auxiliary sets ${\\{A_i\\}}_{i=1}^n$, which is preshared across all parties.\nAdaptation of Equation 1 in secret-sharing scheme is as follows:\n$[\\prod_{i=1}^n x_i] = \\prod_{i=1}^n \\delta_i + \\sum_i \\delta_i [\\prod_{A_i} a_i] + \\sum_{i,j, i\\neq j} \\delta_i\\delta_j [\\prod_{A_i...a_j} a_i] + ... + [\\prod a_i]$.\nSince Equation 2 is linear to the secret-sharing terms, all communications could be conducted in parallel, i.e. in a single round of communications. In this case, we could simply reveal all the secret-sharing terms in ${\\{A_i\\}}_{i=1}^n$ and compute the sharing"}, {"title": "4.2. Univariate Polynomials", "content": "The formal form of univariate polynomials is defined as $P(x) = \\sum_{i=0}^n b_ix^i$, where $b_i$ refers to the coefficients of the degree-i term. The use of univariate polynomials enables efficient evaluation and manipulation of polynomial expressions. According to Damg\u00e5rd et al. [17], we can compute all required $[x^i]$ in parallel using multivariate multiplications, then multiply them with corresponding plaintext coefficients. Despite its benefits, this trick has the disadvantage of exponentially increasing the size of transmitted data, which becomes unbearable when the exponent exceeds 5.\nThis method can be implemented in practice by computing a tuple of base terms and then multiplying the tuple by a certain term iteratively, as in the exponentiating by squaring method or the fast modulo algorithm. In other words, a tuple g = $(1, x, ..., x^{m-1})$ of size $||g|| = m$ could be multiplied by $x^{||g||}$ repeatedly to iterate all the $x^i$ terms."}, {"title": "4.3. Nonlinear Approximations", "content": "In this section, we take one step further to optimize the commonly used nonlinear functions by leveraging the property of parallelization of our proposed multivariate multiplication.\nExponentiation. Since exponential functions grow in geometrical speed, approximations based on series expansion generally suffer from a significant reduction in accuracy since we do not know the exact value of the input. Consequently, we resort to the naive iterative approximation, which is capable of utilizing multivariate multiplication effectively:\n$e^{[x]} = \\lim_{n\\rightarrow\\infty} (1 + \\frac{[x]}{d^n})$.\nDuring each iteration, the d-th power of the previous result is calculated. With increasing iteration rounds n, the answer would become closer to the actual results.\nLogarithm. The calculation of logarithms relies on the higher-order iterative methods for a better"}, {"title": "4.4. Communication Coalescing", "content": "The key to achieving low-latency secret-sharing computation is to reduce the total number of rounds of communications among different parties. While we introduce a latency-friendly implementation of basic math operations, other kinds of communications, such as precision checking, still require an additional but independent communication round. In general, the communication involved in multiple math operations could be abstracted as a communication graph, or strictly, as a communication tree. Accordingly, we observe some independent communications that do not affect downstream results can be deferred and combined into one single round of communication. This process is referred to as communication coalescing, and it eliminates unnecessary rounds of communication and improves the utilization of network bandwidth."}, {"title": "5. Security Analysis", "content": "The correctness of the multivariate multiplication is trivial based on the observation in Equation 1 and 2. As univariate polynomials are implemented using the same method as the extended fast modulo algorithm, their effectiveness could also be demonstrated by the correctness and security of multivariate multiplication. Coalescing mechanisms only alter the order of communication rounds without modifying the payload, which is also reliable and secure.\nMultivariate computations are similarly secure as traditional Beaver multiplications under semi-honest conditions. It is intuitively obvious that since $a_i$ is chosen at random by TTP, the $d_i = x_i - a_i$ value is indistinguishable from a random number. Consequently, the disclosure of $[d_i]$ does not reveal any critical information regarding $x_i$. This assumption holds even if multiple parties, except for the TTP, collude.\nTo clarify the security of multivariate multiplication formally, we denote $[x]_p$ as the secret share of x for party $p\\in P$. The global equations of the multivariate system are as follows:\n$\\sum_{p\\in P} [i]_p = x_i$\n$\\sum_{p\\in P} a_i = A_i$\n$\\sum_{p\\in P} [a_ia_j]_p = a_iaj$\n$\\sum_{p\\in P} [a_1...a_n] = a_1...a_n$\n$[x_i]_p - [a_i] = [\\delta_i]_p$.\nwith known $[\\delta_i]_p$ for every $p\\in P$ to each party. From each party's view, these $2^n + 2n - 1$ equations have $(2^n ||P||)$ unknown variables. This indicates the difficulty in determining the exact value of $x_i$, as shown in [18].\nA party's view represents all the values it can obtain during its execution. Then the following theorem holds:\nTheorem 1. Let {x} and {x'} be random values. The distribution of the view of each party is identical when $x_i = x'_i$; or $x_i = x_i$.\nThis guarantees the security of multivariate multiplication by ensuring the indistinguishability between the random distribution and the view's distribution."}, {"title": "6. Experiments", "content": ""}, {"title": "6.1. Experimental Setup", "content": "As part of our proposed methodology, we use CrypTen [5] as the basic MPC deep learning framework, which has already provided na\u00efve implementations of secret-sharing-based computations. In most of our experiments, we use 3-party MPC on CPUs. Additionally, we allow a maximum of 4-ary multiplication as stated in Section 4.1, and we set d = 3 for exponentiation and k = 8 for logarithm as described in Section 4.3.\nTo measure the performance, we perform several experiments with deep learning models with different sizes: (a) Linear Support Vector Classification (LinearSVC) with L2 penalty; (b) LeNet [19] with shallow convolutional and linear layers along with ReLU activation functions; (c) ResNet-18 model [20] with multiple convolutional, linear, pooling, and activation layers; (d) Transformer Encoder model [21] with a single multi-head attention layer and BatchNorm [22] in place of LayerNorm [23]. We employ several datasets for classification tasks with appropriate adaption to specific models, including MNIST [19], CIFAR-10 [24], ImageNet [25], and Sentiment140 [26] datasets.\nEach of our experiments is conducted in a simulated multi-node environment using Docker. TTP is conducted in an independent environment separate from the normal parties. To manually simulate different network environments concerning bandwidth and latency, we utilize the docker-tc tool to adjust the docker network settings accordingly."}, {"title": "6.2. Metrics", "content": "To provide a comprehensive evaluation of our proposed method, we adopt metrics from a variety of perspectives.\n\u2022 tcomp: The computational time cost for evaluating a single data sample in one round.\n\u2022 tcomm: The time cost of communication associated with evaluating a single data sample"}, {"title": "6.3. Latency & Throughput", "content": "To assess the efficiency of our proposed method, we simulate networks with different network latencies: (a) network Now with 0.1ms latency, (b) network Nmed with 5ms latency, (c) and network Nhigh with 40ms latency. All of these networks have a bandwidth of 1Gbps. Our simulated multi-node settings include 3 nodes with an additional TTP by default.\nAs shown in Table 1, the computation cost of each model is negligible in medium and high latency network settings in comparison to the communication cost. Therefore, we will focus only on the communication costs associated with our proposed method.\nCompared to the na\u00efve method implemented by Crypten, our method illustrated in Section 4.1 remains close since it does not introduce a substantial amount of additional communication payload if the maximum number of input variables is set appropriately. For instance, a 3-ary or 4-ary multiplication would not produce a significant increase in the total size of communications.\nIt is noteworthy that our proposed method reduces the communication cost in every network setting as compared to the na\u00efve implementation of MPC. Overall, we achieve an improvement of 10~20%, which shows significant enhancement in the performance of high-latency environments for practical purposes.\nFurthermore, we observe that our proposed method behaves differently with neural models with different architectures. Figure 1 illustrates the communication occupation percentage of ResNet basic blocks and Attention blocks. As can be seen, the attention mechanism is constrained by its communication bottleneck in Softmax operation, while CNN is"}, {"title": "6.4. Evaluation", "content": "In this section, we examine the side effects and factors associated with the basic settings, such as the downstream tasks' accuracy, the number of parties involved, and the trade-off between network latency and bandwidth.\nTo evaluate the drop in accuracy, we compare our method with both the original baseline and the na\u00efve implementation without a low-latency design. Figure 2 shows that, in relatively small scenarios, both the na\u00efve implementation and our methods are capable of achieving perfect performance as the baselines. Nevertheless, both MPC-based implementations obtain lower accuracy in complex scenarios than the baseline, while our methods perform slightly worse than the na\u00efve implementation. We hypothesize that the multivariate multiplication introduces additional precision requirements, which in turn reduces accuracy.\nThe throughput and latency of MPC-based methods are also affected by the number of parties involved in the computation. From Figure 2, it can be seen that the communication data size of both methods increases linearly as the number of parties involved increases. There is, however, a tendency for the latency to be worse when there are more parties involved.\nMoreover, Figure 3 illustrates how network bandwidth affects communication costs. When sufficient bandwidth is available, our method can still optimize the network latency. It is important to note, however, that when the bandwidth becomes the bottleneck, our method would not be any more effective in reducing the overall costs of communication. This indicates that bandwidth remains an important factor in a multi-node MPC setting, especially as the number of nodes in use grows."}, {"title": "7. Discussion", "content": "Since the proposed multivariate multiplication is based on a finite ring, it is likely to have precision issues that lead to incorrect results. Fortunately, a loss in precision would not significantly affect the overall performance of deep learning, since the loss could be interpreted as random noise and distortion in the input data.\nMoreover, our proposed method is only applicable to functions that are based on linear MPC operation. To avoid heavy communications, a modern MPC-based deep learning framework would also involve other protocols, such as Homomorphic Encryption [27], Garbled Circuit [28], and Function Secret Sharing[29]. Though these works may have less communication, our approach could still be seamlessly integrated with the current secret-sharing framework and achieve a latency improvement of ~20% without adding excessive computational workload."}, {"title": "8. Conclusion", "content": "This study proposes a secret-sharing-based MPC method for enhancing the linear computation required in deep learning through increased communication utilization. By utilizing the multivariate multiplication and communication coalescing mechanisms, we can reduce the number of unnecessary communication rounds during the execution of both linear and nonlinear deep learning functions. In our experiments, we demonstrate that our proposed methods achieve an overall improvement in latency of 10~20% when compared to the na\u00efve MPC implementation. Additionally, it indicates that throughput and downstream task performance are comparable to na\u00efve implementations, which demonstrate the method's validity and efficiency. We hope that this work will inspire future improvements in privacy-preserving deep learning techniques and lead to more practical MPC applications."}]}