{"title": "A Simple and Fast Way to Handle Semantic Errors in Transactions", "authors": ["Jinghan Zeng", "Eugene Wu", "Sanjay Krishnan"], "abstract": "Many computer systems are now being redesigned to incorporate LLM-powered agents, enabling natural language input and more flexible operations. This paper focuses on handling database transactions created by large language models (LLMs). Transactions generated by LLMs may include semantic errors, requiring systems to treat them as long-lived. This allows for human review and, if the transaction is incorrect, removal from the database history. Any removal action must ensure the database's consistency (the \"C\" in ACID principles) is maintained throughout the process.\nWe propose a novel middleware framework based on Invariant Satisfaction (I-Confluence), which ensures consistency by identifying and coordinating dependencies between long-lived transactions and new transactions. This middleware buffers suspicious or compensating transactions to manage coordination states. Using the TPC-C benchmark, we evaluate how transaction generation frequency, user reviews, and invariant completeness impact system performance. For system researchers, this study establishes an interactive paradigm between LLMs and database systems, providing an \"undoing\" mechanism for handling incorrect operations while guaranteeing database consistency. For system engineers, this paper offers a middleware design that integrates removable LLM-generated transactions into existing systems with minimal modifications.", "sections": [{"title": "1 INTRODUCTION", "content": "For decades, system design has been centered around humans as the primary users and operators. In database management systems (DBMS), transactions are usually initiated and committed by human users. While syntactic errors can be easily detected and corrected through standard error handling, users may also introduce semantic errors or need additional time to decide whether to commit a transaction. To manage these challenges, researchers and engineers have developed \"undo\" mechanisms, ensuring both data integrity and system recoverability.\nThe ARIES protocol (Algorithm for Recovery and Isolation Exploiting Semantics) has long been the gold standard for recovery, combining write-ahead logging, undo, and redo operations to efficiently manage fine-grained rollbacks [21]. However, managing semantic errors requires transactions to remain long-lived for user review, making ARIES potentially too storage-intensive for such use cases. Beyond ARIES, alternative approaches such as sagas have been developed for long-running transactions in distributed systems, enabling partial rollbacks through compensating transactions while maintaining system consistency [12]. ACTA is derivative work for sagas, and it extends sagas to include extended transactions while still providing compensating transactions and maintaining consistency [7]. Similarly, advancements in multi-version concurrency control (MVCC) allow systems to maintain historical states, enabling rollbacks to previous valid database states [2]. Escrow methods, often utilized in high-concurrency environments, only allowed conditional updates and guarantees to facilitate safe rollbacks without locking resources [24]. These mechanisms either allow \"undo\" or give users a chance to store long-lived transactions and review the transactions to fix semantic errors.\nLarge Language Models (LLMs) are increasingly being seen as active operators within systems, either by translating human commands for system interaction or functioning autonomously to maintain the system based on predefined prompts. LLMs have demonstrated their ability to understand system structures and effectively query data [6, 10, 20, 39]. While these actions typically do not modify the original database state and instead provide more accessible methods for data extraction, more complex interactions involve \"write\" actions that alter the database state [25, 26, 34, 37, 39]. These write actions can introduce errors, which are often manageable through validation, as well as semantic errors, which may require user review for correction [26, 38, 39].\nAn intuitive solution is to improve the prediction accuracy and reduce semantic errors of LLMs in adhering to user instructions and expectations. However, due to inherent limitations in LLMs, achieving perfect performance is unrealistic. Thus, we must treat incorrect transactions as errors and remove them when necessary. Existing approaches like ARIES, sagas, escrow, and MVCC provide partial solutions but fall short of addressing the challenges posed by long-lived transactions, especially those generated by LLMs. ARIES, for instance, does not support long-lived transactions. Sagas undo transactions by predefined independent compensating transactions, which cannot handle inter-dependent transactions that violate system constraints. Sagas also requires system implementation case by case. ACTA briefly explains how to handle transactions that cannot be undone. However, it does not discuss inter-dependent transactions, which may violate database constraints based on the other transaction's state, not just an independent transaction itself. Escrow focuses on counter-type data, limiting its applicability. MVCC does not support selective removal of specific transactions. Approaches like rewriting history rely on commutativity checks to remove incorrect transactions and transactions affected by incorrect transactions, but commutativity imposes overly strict constraints [18]. A more practical approach would relax this standard by enforcing database constraints rather than insisting on identical database states. Thus, we adapted and extended Invariant Confluence to check whether LLM-generated transactions require review and may need coordination with newer transactions. [1]. Coordinations stall certain newer transactions to guarantee that the undoing of long-lived transactions always leads to a consistent database state. We also reviewed additional papers on long-lived transactions; see the details in the discussion section.\nIn this paper, Section 2 outlines the system settings and requirements. Sections 3 and 4 summarize the processes for identifying"}, {"title": "2 SYSTEM SETTING", "content": "We consider a system that operates in an environment that accepts transactions from human users and LLMs (Large Language Models). Occasionally, transactions may later be identified as incorrect by users, such as when users decide they no longer want to proceed with a transaction or when a semantic error in LLM-generated transactions leads to an imperfect execution of users' instructions. For example, in Figure 1, after transaction 2 (T2), an account balance is 50 USD, and transaction 3 (T3) increments 10 USD while all new transactions T4 - T8 have a net effect that decreases by 60 USD. One database constraint requires that the account balance must be positive. T3 is an LLM-generated transaction; the user reviewed it and decided to remove it. In this case, T3 cannot be removed; otherwise, it will lead to a violation of the database constraint. This setting raises two key questions:\n(1) How can we isolate or remove these questionable transactions from LLMs, especially when they may no longer be needed, from database history?\n(2) In certain situations, removing a suspicious transaction might not be feasible while maintaining the consistency of the data system due to subsequent new transactions' modification to the database.\nSolution for (1): In this paper, we mainly suggest two approaches for suspicious transaction removal/separation, and we discussed the problem of other approaches:\nBuffering Suspicious Transactions: Rather than committing suspicious transactions directly to the database, the web developer can store these transactions in a buffer. Once an administrative user reviews and either accepts or removes the transaction, the transaction manager within the system can then commit or discard the transaction accordingly.\nBuffering Compensating Transactions: A compensating transaction is a specialized transaction used to \"undo\" the effects of a previously committed transaction by applying an opposite action to restore the system to a consistent state [16]. Unlike a rollback, which cancels a transaction before it is committed, a compensating transaction is applied after commitment to address cases where reversal is necessary without altering the original transaction history. For instance, in a banking system, if 100 USD is mistakenly transferred from Account A to Account B, the compensating transaction would transfer 100 USD back to Account A to correct the error. Compensating transactions is essential in complex systems where direct rollbacks may be impractical because the transaction has already been committed.\nIn our context, the system commits the transaction upon receipt and buffers its corresponding compensating transaction. If an administrative user reviews and decides to remove the transaction, the transaction manager within the system can then commit a compensating transaction to negate the effect of the suspicious transaction on the database. If the transaction is approved, no further action is needed for the database.\nTo ensure the database maintains consistency while also allowing for the removal or separation of suspicious transactions, we consider several approaches:\n1) Full Database Locking: The simplest approach is locking the entire database whenever a suspicious transaction request is received. The system continues running but restricts other transactions until users review the suspicious transaction. This enforces strict serializability and ensures ACID properties, particularly consistency. 2) Sandbox Simulation: Another option is to create a sandbox to simulate transactions. As long as a compensating transaction can remove the effects of suspicious transactions while maintaining consistency, even after subsequent transactions, the suspicious transaction is safe to undo. 3) Granular Locking: Locking specific database parts (e.g., rows or tables) can also support removability. However, determining the appropriate granularity"}, {"title": "2.1 Middleware", "content": "For web developers and system engineers, introducing a new framework in a modern enterprise is often discouraged due to concerns about cost and system reliability. Frameworks like LangChain address this by offering tool packages that integrate into larger systems as microservices [17]. Similarly, this project focuses on creating middleware compatible with existing web frameworks, minimizing disruption to current workflows. By avoiding changes to the database or data schema, our solution ensures data integrity and reliability without requiring complex system migrations.\nThe Model-View-Controller (MVC) [4] is a widely used design pattern in software development, especially for web applications, which divides an application into three interconnected components: Model, View, and Controller. The Model manages data and business logic, representing the application's core functionality by handling data storage, retrieval, and processing. The View serves as the presentation layer, displaying data from the Model to the user and relaying user commands to the Controller. The Controller acts as an intermediary, processing user inputs, updating the Model, and instructing the View to display the updated data.\nIn our setting (in Figure 2), after the view receives the request from either users or LLM agents, it will not directly talk to the controller. Instead, the control workflow will be delegated to the middleware, which ensures that new transactions that depend on buffered transactions can be appropriately coordinated to maintain system consistency.\nAs we discussed before, there are two approaches to buffering: buffering suspicious transactions and buffering compensating transactions. We will use the second one as an example to explain how the middleware works."}, {"title": "2.2 Dependency-Check for a Middleware", "content": "We assumed web development based on MVC, and the middleware only received the transaction name and parameters. Usually, a transaction request triggers predefined transaction logic according to"}, {"title": "2.3 Buffering Compensating Transactions", "content": "2.3.1 Request from the User. Assume that the User or LLM interacts with the system via an HTML interface, with actions on the webpage submitted through URL requests. Rather than directly calling database systems or ORM operations, these URL-based requests are first placed into a queue, where the Transaction Manager processes them periodically. Each transaction request is assigned a unique transaction ID before entering the queue. This ID is also logged or updated in the transaction status table with a status of \"submitted,\" which allows for universal tracking and identification of each transaction's status.\nIn a separate process running parallel to the user-facing request handler, the Transaction Manager periodically retrieves requests from the queue, assigning each request for dependency check. The Transaction Manager (details in section 4) decides whether a new transaction should be buffered or committed based on its dependency on buffered transactions. It also checks if any transactions in the buffer are ready to be removed or executed.\n2.3.2 Review a Buffered Transaction. If an admin or user identifies a suspicious buffered LLM-generated transaction, they can choose to accept or remove it. Once the approval or removal is triggered and submitted to the system via a URL request, the request is added to a decision buffer.\nDuring each iteration, the Transaction Manager retrieves results from the decision buffer and removes the corresponding buffered transaction. It then commits these removed or \"mature\" buffered transactions and updates the status table accordingly before accepting new transactions.\n2.3.3 Checking the Status. When a request for transaction status is received, the system retrieves the result or status from the status table using the transaction ID."}, {"title": "2.4 Buffering Suspicious Transactions", "content": "Same framework except for buffering suspicious transactions instead of compensating transactions."}, {"title": "2.5 API Documentation", "content": "(1) Endpoint: transaction_request\nDescription: Initiates a transaction request and returns a unique transaction ID.\nHTTP Method: POST\nParameters:\n\u2022 transaction_name (string): The name of the transaction.\n\u2022 transaction_parameters (object): Specific parameters required for the transaction.\nReturns:\n\u2022 transaction_id (string): A unique identifier for the requested transaction.\n(2) Endpoint: transaction_review\nDescription: Accept or remove an LLM-generated and suspicious buffered transaction.\nHTTP Method: POST\nParameters:\n\u2022 transaction_id (string): The unique identifier of the transaction to be processed.\nReturns:\n\u2022 status (string): The processing result, indicating whether the transaction has been accepted or removed.\n(3) Endpoint: transaction_status\nDescription: Retrieves the status of the database query result of a specific transaction.\nHTTP Method: POST\nParameters:\n\u2022 transaction_id (string): The unique identifier of the transaction."}, {"title": "3 CONSTRAINTS IN LONG-LIVED TRANSACTIONS", "content": "3.1 Invariant Satisfaction\nInvariant confluence: Invariant confluence [1, 36], originally applied in distributed systems, ensures that, after merging, distributed data stores still satisfy a common invariant (i.e., database constraints). Specifically, starting from a common ancestor database state, there are two concurrent branches, each with a sequence of transactions, as shown in Figure 4. If each sequence results in a consistent database state (All S are consistent state) and the two branches can be merged without violating consistency, this state is called invariant confluence.\nA key theorem of invariant confluence states that a set of transactions T can execute without coordination and converge to a consistent state if and only if T is invariant confluent. Coordination here means that if two transactions are not invariant confluent, one should be held to prevent concurrent execution. In the setting of this paper and Invariant Satisfaction, coordination requires that the latter be held until the former finishes reviewing.\nThe invariant confluence between two transactions can be assessed by logically verifying their operations against database invariants. If two transactions are not invariant confluent and the first transaction executes earlier, we say that Transaction 2 depends on Transaction 1.\nInvariant Satisfaction: In our setting, while a suspicious transaction remains buffered, new transactions may be committed to the database in ways that prevent the buffered transaction from committing consistently. When treating the buffered transaction and a new transaction as parallel, concurrent operations, this scenario is similar to invariant confluence.\nWe propose that if the buffered transaction and the new transaction are invariant confluent, they can proceed without coordination. If they are not invariant confluent, the new transaction must be held until the buffered transaction is either accepted or removed by the user.\nA key difference from traditional invariant confluence is that our setting involves only a single transaction in each \"branch.\" We refer to this as Invariant Satisfaction (I-Satisfaction) to specify the relationship between buffered and new transactions. This also implies that if the transactions are not I-satisfied, and the buffered transaction occurs before the new transaction, then the new transaction depends on the buffered transaction."}, {"title": "3.2 Invariants Analysis", "content": "In this section, we outline a series of invariants that web developers should consider. We list and explain pairs of invariants and operations that do not meet Invariant Satisfaction requirements and, therefore, require coordination. (assuming 1. Transaction A is a suspicious transaction that might be removed and not exist; 2. Transaction B is a new transaction; 3. Transaction A occurs before Transaction B; 4. Transaction A and Transaction B are not committed to the database for a constraint validation check but are logically"}, {"title": "5 TRANSACTION MANAGER", "content": "In this section, we consider Scenario 2: transaction type A adds x1 USD to the account balance of the row y1, and transaction type B deducts x2 USD from the account balance of the row y2. Invariant: The account balance must remain > 0.\nIsolation Coordination: Isolation in this paper refers to the \"I\" in the ACID properties of a database system, ensuring that transactions are processed independently to avoid conflicts [14]. Isolation coordination involves concurrency control to guarantee the serializability of transactions. When two transactions have write-write, read-write, or write-read conflicts and are running concurrently, isolation coordination ensures that the outcome is equivalent to executing these operations in a specific sequential order. In our framework, the database manages isolation, while the middleware we proposed is responsible solely for maintaining consistency.\nConsistency Coordination: We have previously discussed consistency and dependency. To ensure system consistency while allowing for the potential removal of suspicious transactions, coordination between transactions is necessary. For instance, if we have two transactions that are both type A (increment operations) in a given scenario, there is no dependency, and no coordination"}, {"title": "5.1 Processes vs. Transaction Manager", "content": "We may need a machine to coordinate transactions. In theory, all uncommitted transactions could be managed in separate processes and threads, which would host transactions waiting for locks, perform simulations, or carry out logical dependency checks between new transactions and all buffered suspicious or compensating transactions. However, this approach has high bandwidth costs. More importantly, it is impractical in a long-lived transaction scenario, as we discussed, where processes might need to wait for days for user reviews. A more efficient solution is to build a transaction manager that can track transactions, manage their state, and commit them as appropriate."}, {"title": "5.2 Transaction Manager Environment", "content": "Committed Buffered New\nT1, T2, T3 (T4) (T5) (T6) T7\nIn the above diagram, we provide an example of dependency checking, and there are three types of transactions:\nCommitted transactions constitute the current state of the database. Buffered transactions include either suspicious transactions or compensating transactions to support transaction separation/removal. Buffered transactions also include those that depend on other previous buffered transactions. New transactions are added to the buffer if they are marked as suspicious. The system then checks whether a new transaction depends on any buffered transaction. If there are N buffered transactions, each new transaction requires N dependency checks. When a dependency is identified, it is recorded accordingly, as characterized in Table 1.\n5.2.1 How to manage/buffer transactions: The transaction manager handles all incoming new transactions serially, one at a time. Each transaction is evaluated against Invariant-Satisfaction for dependency check. If there is no dependency: The transaction is materialized (committed) into the database. If dependencies exist: The transaction is retained in the buffer for future processing.\n5.2.2 How to check dependency: Assume we have two transactions, T1 and T2. Go to Table 1 and check whether these two have actions paired in the same row of the table. Then, based on the completeness of the information at the time of commit, they can get to know the dependency criteria. For example, T1 is UPDATE key = 1 from table1 where id = 1, AND T2 is Update key = 2 from table1 where id = 1, and we know the \"key\" field has a uniqueness constraint. Then we go to table 1, go to row 1, and the \"Complete Query with Invariant\" column since we see it mentioned the same actions as T1 and T2, and we have complete query information that we know those transactions are working on the same \"key\" field which has uniqueness constraints. Then, dependency and coordination are needed."}, {"title": "5.2.3 How to represent the interdependence between transactions:", "content": "Transactions are retained in the buffer not just because they are suspicious but because their dependencies are logged in the Dependency Matrix. The Dependency Matrix tracks relationships between transactions, indicating interdependence, as shown in the table below."}, {"title": "5.2.4 Algorithm for Adding a New Transaction.", "content": "This algorithm is implemented as process_transactions().\nInput: DB, Transaction Buffer, Dependency Matrix DM, New transaction T\nOutput: Updated DB, Updated Transaction Buffer, Updated Dependency Matrix\nIf T is suspicious, it will be added to the DM and wait for administration processing. Then, the transaction manager checks T's dependency against all existing transactions in the dependency matrix. For any transaction with more than one dependency identified, it specifies its dependency in the dependency matrix.\nIf T is not suspicious, Then the transaction manager checks T's dependency against all existing transactions in the dependency matrix. Only after the dependency is identified will it be added to the transaction buffer, and its dependency will be specified. For non-suspicious transactions, if no dependency is detected, they will be committed to the DBMS."}, {"title": "5.2.5 Algorithm for Materializing a Buffered Transaction.", "content": "This algorithm is implemented as check_for_materialization().\nInput: DB, Transaction Buffer, Dependency Matrix DM\nOutput: Updated DB, Updated Transaction Buffer, Updated Dependency Matrix\nThe Transaction Manager regularly checks the Dependency Matrix. For each transaction in the Dependency Matrix:\n(1) First, check whether it is approved by the administrator or if it is not a suspicious transaction.\n(2) Then, check the dependency table vertically to see whether the transaction is still waiting for others.\nIf both conditions (1 and 2) are satisfied, the Transaction Manager will:"}, {"title": "6 EXPERIMENT", "content": "In our system setting, we characterize the availability of the middleware and web service pipeline by the buffered rate. While end-to-end performance-from the user's request to transaction completion-may serve as a more comprehensive availability metric, it is influenced by the specific implementations of the transaction manager and middleware, which are not the primary focus of this paper. Also, database performance can impact end-to-end performance. Thus we assume that the database completes all transactions immediately once they exit the middleware. We only simulate transaction management and do not commit it to the database.\nBuffered rate = \n\\frac{Number\\ of\\ buffered\\ transactions}{Number\\ of\\ total\\ test\\ transactions}\nTPC-C (Transaction Processing Performance Council Benchmark C) [35] is a standardized benchmark for evaluating the performance of transaction processing systems. It simulates an order-entry environment, which includes operations such as processing new orders, payment transactions, order status checks, deliveries, and stock level monitoring. TPC-C is widely used to measure and compare the efficiency and scalability of database and transaction management systems under a complex, real-world workload.\nWe have a naive intuition that with more dependency between transactions, the buffered rate will increase. In a web service system, if the proportion of inter-dependent transactions is higher, the buffer rate will accordingly increase. Thus, a fixed distribution of transactions could give us a better experiment parameter control to explore other parameters that affect the buffered rate. This is also the reason we use TPC-C as the test framework because it assumes a roughly fixed distribution between 5 types of transactions.\nA natural intuition suggests that as dependencies between transactions increase, the buffered rate will also rise. In a web service system, a higher proportion of interdependent transactions leads to a corresponding increase in the buffered rate. Therefore, using a fixed distribution of transactions provides better control over experimental parameters, allowing us to explore other factors affecting the buffered rate. This rationale also underlies our choice of the TPC-C framework for testing, as it maintains a roughly fixed distribution across five transaction types.\nWe implement our test frame by adapting and rewriting a TPC-C benchmark tool (py-tpcc) [27]. It is originally used to generate TPC-C transactions, dock transactions to a database, and measure the throughput performance of the target database. We rewrite this package, dock the transactions to our transaction manager, and control the flow of transaction generation. In our experiments, we mainly control 2 factors, the interval between two LLM-generated suspicious transactions (SI), and the interval between two user reviews to accept or remove the suspicious transaction, (RI). For simplicity, we assume equal intervals for both RI and SI.\nQuestion 1: Benchmark Comparison: how does the completeness of the query and invariant affect the buffered rate"}, {"title": "Question 2: Which type of transaction is most likely to cause congestion or hold new transactions, leading to an increase in the buffered rate?", "content": "From the manual analysis and invariant confluence, in TPC-C, the dependency exists between New-order and New-order Transactions; and between Delivery and Delivery Transactions."}, {"title": "Question 3: What factors related to suspicious transactions affect the transaction acceptance rate?", "content": "Conclusion 3.1 If the RI is set to infinity-meaning there is no review to remove buffered transactions that may congest new transactions-the buffered rate does not change significantly as the number of transactions increases.\nIn this scenario, we assume an SI of 5, where one out of every five transactions is labeled as an LLM-generated or suspicious transaction (e.g., True, False, False, False, False, True, ...). We calculate all buffered rates 5 times and average the results. As shown in Figure 8, the average buffered rate remains steady at around 0.6 if the number of transactions is larger than 400, regardless of the increasing number of transactions.\nConclusion 3.2 If the RI is set to infinity, a longer SI leads to a decrease in the average buffered rate.\nWe varied the interval between two LLM-generated transactions, testing SIs of 2, 5, 10, and 50 (e.g., one LLM-generated transaction for every 2 transactions, with 20 trials per SI). As shown in Figure 9, the test case with higher SIs results in a lower average buffered rate.\nFact 3.3: If the user reviews and either approves or rejects all transactions at any point, all buffered transactions will be processed, resulting in a buffered transaction rate of 0.\nConclusion 3.4 A shorter RI results in a lower number of average buffered transactions.\nSince reviewing all transactions would result in a buffered rate of 0, we tested the effect of partially reviewing transactions by randomly removing 80% of the transactions in the buffer. Assuming that human reviews occur less frequently than LLM-generated transactions, we set the RI (user review interval) to be a multiple of the SI (suspicious transaction interval), with values of (1, 2, 5, 10, 100) times the SI. For this test, we fixed the SI at 5, giving RI values of (5, 10, 25, 50, 500), with 20 trials per RI.\nWe focused on cases where the number of transactions exceeded the RI to ensure human processing within each test case, omitting scenarios without human processing. Consequently, a few lines in Figure 10 are truncated. Also, as shown in Figure 10, a shorter RI results in fewer buffered transactions.\nConclusion 3.5: When RI is fixed, the number of buffered transactions remains relatively stable, even as the total number of transactions increases.\nConclusion 3.6: Because the number of buffered transactions remains stable, the buffered transaction rate decreases as the total number of transactions increases (see Figure 11)."}, {"title": "7 DISCUSSION", "content": "7.1 Mechanism Comparison\nWe have discussed several mechanisms and approaches for addressing our system setting. However, we also mentioned they are not a good choice for our questions. How do these mechanisms compare, and why do we prefer buffering suspicious or compensating transactions with the invariant-satisfaction dependency check?"}, {"title": "7.1.1 Naive Buffering.", "content": "In naive buffering, incoming removable transactions are stored in a buffer and committed or removed if they are reviewed by the admin users. The naive buffering strategy does not assume any prior knowledge about invariants. Essentially, suspicious transactions are moved to the end of the transaction history,"}, {"title": "7.1.2 Transaction Simulation.", "content": "Transaction simulation periodically checks if a compensating transaction can be used to remove an erroneous transaction. When a new transaction arrives, we set up a sandbox environment and commit various combinations of the new and compensating transactions to a database snapshot. As long as all combinations preserve system consistency, the new transaction can be committed. This simulation approach ensures that any committed transaction can be removed if necessary using its corresponding compensating transaction.\nIf there are buffered compensating transactions and new transactions ready for undo simulation, let N be the number of buffered transactions and n the number of new transactions. \\((2^{N}) \\times n\\) simulations may be needed to test all combinations. Meanwhile, our proposed buffering compensating/suspicious approach only requires N x n I-satisfaction dependency checks. Additionally, it's important to note that simulation involving physical I/O is significantly more time-intensive than in-memory logical I-satisfaction checks."}, {"title": "7.1.3 Buffering Suspicious Transaction Is Better than Buffering Compensating Transaction?", "content": "One key drawback of buffering compensating transactions is if a transaction is reviewed and removed by the user, the system actually interacts with the database twice for committing. It commits the original suspicious transaction and the undo compensating transaction. This doubles the transaction overhead if users remove all LLM-generated transactions. For buffering suspicious transaction approaches, no additional actions are required other than removing transactions from the buffer, and no database execution is needed."}, {"title": "7.1.4 Buffering Compensating Transaction is better than Buffering Suspicious Transaction:", "content": "Invariant satisfaction only provides guidance on whether two transactions need coordination if they are requested by users concurrently. However, invariant satisfaction does not provide any guarantee whether this transaction can be committed and satisfy database consistency. We summarize the constraints validation of the above-mentioned approaches below:\n\u2022 Pure Buffering: Constraint validation is done at the point of committing or through extra application logic.\n\u2022 Simulation: Constraint validation is tested at the simulation stage.\n\u2022 Buffering Suspicious Transactions with Invariant Satisfaction: If transaction 1 depends on transaction 2, the transaction will be constraint-validated until transaction 1 has been reviewed by the user and submitted to the database. If transaction 2 is not suspicious, it needs to wait until transaction 1 is finished. Thus, constraint validation has been delayed significantly.\n\u2022 Buffering Compensating Transactions with Invariant Satisfaction: The suspicious transaction can be committed and validated when the user requests. The corresponding compensating transaction is the same. There is no delay for the consistency validation check."}, {"title": "7.2 Lock", "content": "A dependency check based on invariant satisfaction is similar to locking in a database system. The dependency check identifies conflicts between two transactions and holds the second transaction until the first one is completed. In databases, locks are primarily used to ensure isolation between transactions. If there is a write-write (WW), write-read (WR), or read-write (RW) conflict between two transactions, the lock is applied to the necessary data, requiring one transaction to wait until the other releases the lock [9]. However, locks for isolation typically have fixed granularity, such as row-level, table-level, or system-level locks, which may be insufficient for maintaining consistency in complex coordination scenarios which has varying granularity requirements.\nFor example, In the context of ensuring consistency, 2PL may require a table-level lock. To be more specific, for consistency coordination required invariants like uniqueness, the former transaction must lock the whole table to make sure all later transactions will not insert any row to lower the former transaction's consistency priority (shrinking the former transaction's range of unique names) [9]. Holding a table lock is also true for a foreign key since if I want to delete a referenced foreign key tuple, I need to lock the whole referencing table to make sure no new transaction will reference the deleted referenced key. For Invariant >, <, the system may only need to lock the corresponding rows to make sure no other transaction can modify it and negatively affect the former transaction's consistency priority. Since the lock-based approach does not have information to invariant beforehand, it is necessary to assume a table-based lock to guarantee consistency.\nFor example, a two-phase locking (2PL) might enforce a table-level lock. It is a good granularity choice for invariants like uniqueness, where the first transaction must lock the entire table to prevent subsequent transactions from inserting rows that could violate the uniqueness constraint. However, it is not necessary for CHECK invariants with conditions (e.g., >, <); the system may only need to lock specific rows to prevent other transactions from altering them and compromising consistency. This fixation on granularity leads to coordination overhead."}, {"title": "7.3 Strict Serialization and Commutativity in Transactions", "content": "Transaction orders can lead to different database states if two transactions are not non-commutative [9]. If we assume there is a \"correct\" order of transaction, any change to this order may lead to an \"incorrect\" state. Our framework can also ensure a favorable and \"correct\" outcome by coordinating these transactions based on commutative checks [29].\nFor buffered suspicious transactions and new transactions, to ensure the final state is identical to that of the \"correct\" order, we can use a commutativity check to identify necessary coordination."}, {"title": "7.4 Undo with Compensating Transaction", "content": "A compensating transaction may not exist if there is no logical inverse for a given operation. For example, setting a value to zero eliminates any knowledge of its previous state, making it impossible to reverse mathematically. Similarly, sending a notification or email represents an external action that cannot be \"unsent\" or undone once it leaves the system. This absence of an inverse action means that the system cannot revert these transactions in a straightforward manner, as the original state or condition cannot be restored.\nCascading compensating transactions occur when other transactions use the results of the current transaction [12]. If one transaction depends on or builds upon the outcome of another, reversing the initial transaction requires creating compensating transactions for all dependent ones. This cascade can become increasingly complex as each linked transaction must be undone or adjusted to maintain consistency."}, {"title": "7.5 Related Works", "content": "Undoing a transaction involves two key components: determining how to reverse the transaction's effects and ensuring that compensating actions maintain system consistency. Traditional approaches (e.g., compensating transactions, ACTA, sagas) have largely addressed the first aspect-how to perform the compensation. However, the approach described here addresses both. Previous studies have explored ensuring consistency for long-lived transactions under weaker serializability assumptions.\nCertain work has discussed verifying whether a compensating transaction can be applied based on write-read dependencies and introduced the notion of r-soundness, a weaker form of consistency ensuring that compensated or rolled-back states satisfy database constraints [16]. While that research established the concept, our method provides an exact procedure for verifying r-soundness through Invariant-C.\nAn earlier analysis addressed concurrency control and recovery together but focused on short-lived operations and traditional recovery mechanisms, not on long-lived transactions or compensating strategies [31].\nAdditional studies employed abstract data types (ADTs) to manage long-lived transactions [33]. This approach sometimes permitted concurrent execution without violating constraints, conceptually aligning with our integrity constraints (IC). However, while those works utilized alternative structures and focused less on preventing constraint violations, our approach employs a matrix-based mechanism explicitly designed to maintain database constraints.\nOther efforts demonstrated that certain compatible transactions can commit without coordination if their semantics are aligned [11], a concept similar to IC, yet they did not offer a comprehensive mechanism to identify and handle all potential conflicts as our IC-based solution does.\nFurther research categorized interactions among concurrent transactions using semantic considerations, discussing semantic and commutative compatibilities that maintain integrity and allow for free interleaving when operations result in equivalent states [22]. However, these approaches did not focus on systematically enforcing database constraints.\nFinally, some papers have summarized various serializability and correctness models-such as Predicate-wise Serializability (PSR), Quasiserializability (QSR), Cooperative Serializability (CoSR), and Setwise Serializability (SWSR). These models are group-based and restrict uncoordinated operations to occur only between groups rather than at the individual transaction level [28]. PSR, while conceptually related to IC, has been shown to improve the availability of long-lived transactions but only allows concurrent transactions between groups [30]. This is stricter than r-soundness. In contrast, our Invariant-C approach allows for greater flexibility at the transaction level while still preserving global integrity."}, {"title": "8 CONCLUSION", "content": "In this work, we characterized the workflow of interactions between LLMs and databases in the presence of semantic errors. We introduced the concept of Invariant Satisfaction, which identifies the necessary coordination between long-lived, buffered transactions and new transactions. This ensures that new transactions do not lead to states where buffered transactions become irrecoverable. Invariant Satisfaction extends beyond database constraints to include application logic constraints, dynamically reducing coordination requirements based on the completeness of constraint and query information. We proposed a middleware framework for coordinating undo"}]}