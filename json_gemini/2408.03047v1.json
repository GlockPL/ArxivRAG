{"title": "OpenOmni: A Collaborative Open Source Tool for Building Future-Ready Multimodal Conversational Agents", "authors": ["Qiang Sun", "Yuanyi Luo", "Sirui Li", "Wenxiao Zhang", "Wei Liu"], "abstract": "Multimodal conversational agents are highly desirable because they offer natural and human-like interaction. However, there is a lack of comprehensive end-to-end solutions to support collaborative development and benchmarking. While proprietary systems like GPT-40 and Gemini demonstrating impressive integration of audio, video, and text with response times of 200-250ms, challenges remain in balancing latency, accuracy, cost, and data privacy. To better understand and quantify these issues, we developed OpenOmni, an open-source, end-to-end pipeline benchmarking tool that integrates advanced technologies such as Speech-to-Text, Emotion Detection, Retrieval Augmented Generation, Large Language Models, along with the ability to integrate customized models. OpenOmni supports local and cloud deployment, ensuring data privacy and supporting latency and accuracy benchmarking. This flexible framework allows researchers to customize the pipeline, focusing on real bottlenecks and facilitating rapid proof-of-concept development. OpenOmni can significantly enhance applications like indoor assistance for visually impaired individuals, advancing human-computer interaction. Our demonstration video is available https://www.youtube.com/watch?v=zaSiT3c1WqY, demo is available via https://openomni.ai4wa.com, code is available via https://github.com/AI4WA/OpenOmniFramework.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) (Zhao et al., 2023; Minaee et al., 2024) demonstrated remarkable capabilities in understanding user intentions and following instructions. However, text-only human-computer interaction (HCI) is often insufficient (Zhang et al., 2023). OpenAI recently released their new flagship model, GPT-40, which can reason across audio, video, and text in real time. The impressive performance is achieved with response times between 200-250ms, which is acceptable for large-scale applications\u00b9. Google soon followed with their latest multimodal competitors, indicating a clear trend towards multimodal generative models and applications2. LLaVA (Liu et al., 2023) is one of the early publicly available solutions for multimodal large models integrating text and images. However, there is currently no open source end-to-end conversational agents implementation and demonstration publicly available online. The ideal form of multimodal HCI should mirror human interactions, incorporating video and audio inputs with audio outputs. Despite the availability of various modular components, there is no comprehensive integrated open-source implementation to foster research and innovation in this field. Integrating existing models\u2014such as audio speech recognition (Speech2Text), multimodal large models (MLMs), and text-to-speech synthesis (TTS) into a multimodal conversation system reveals significant challenges in balancing latency and accuracy. Historically, accuracy has been a major hurdle; however, advancements in large language models (LLMs) have substantially improved contextual relevance. The main challenge is reducing end-to-end latency while maintaining accuracy. While OpenAI and Google have shown it's possible, the open-source community lacks alternatives that replicate this performance.\nAnother issue is data privacy. The GPT-4 family of solutions also raise concerns about cost and data privacy. Since GPT-4 is closed-source, users must upload their data to the server via a paid API, raising privacy issues. The privacy policy of GPT3 informs users that various forms of personal information, including account details, user content, communication information, and social media data,"}, {"title": "2 Related works", "content": "Solution options Traditional end-to-end multimodal conversation systems, as shown in Figure 2, typically use a divide-and-conquer strategy, splitting the process into sub-tasks: speech-to-text (automatic speech recognition), image-to-text, text generation, and text-to-speech (Kusal et al., 2022). Speech-to-text converts spoken language into text, while image-to-text generates textual descriptions of images. Text generation, often powered by large language models, produces contextually appropri-"}, {"title": "3 System design", "content": ""}, {"title": "3.1 Requirement analysis", "content": "The system receives audio and video input, produces audio as the output. Initially, we need two modules: one to collect audio and video data from the microphone and camera, and another to play audio through a speaker. These Client modules should support diverse devices, such as a smartphone, a laptop, or a Raspberry Pi. The collected data will then be fed to a server.\nThe server, referred to as API, should manage audio, video data, and metadata. It should have access to a storage layer that includes a relational database, file management, and a graph database for potential GraphRAG integration. While the API can reside on the same instance as the Client module, we prefer them to be separate for better adaptability. This separation introduces the challenge of sharing large volumes of data between modules. If the API is cloud-based, the audio and video data need to be uploaded to the cloud, for example using AWS S3, Azure Blob Storage, or Google Cloud Storage. However, the upload process can become a bottleneck, making the data transfer time-consuming. If the server is local, within the same network as the Client, transfer latency will be reduced. However, this setup requires running the large language model locally, addressing data ownership and privacy concerns but potentially increasing model inference latency and compromising accuracy due to limited computing resources. Another solution is edge computing, where video data is pre-processed on edge devices and summarized for the API. While this can be a research direction, data compression may cause information loss and reduce end-to-end performance.\nThe pipeline components will need modification if developers want to adopt the framework and integrate with their work. To ensure flexibility, this part should be an independent module that can run locally or in the cloud. Researchers and developers should be able to easily integrate new components into this Agent module, further challenging the sharing of large datasets between modules.\nLastly, we want to generate benchmarks to understand the latency and accuracy performance of the entire pipeline. For tasks that are hard to evaluate automatically, such as determining the appropriateness of the LLM response, we propose and develop an annotation module to allow human annotators to easily evaluate results and generate benchmark reports."}, {"title": "3.2 System architecture", "content": "Based on the requirements, we designed our system as shown in Figure 1. The system is divided into five modules: Client, API, Storage, User Interface, and Agent, all primarily developed in Python. The Client module includes two submodules: the Listener for collecting video and audio data, and the Responder for playing audio. The Storage module consists of file storage for media, a relational database (PostgreSQL) for metadata, and a graph database (Neo4j) for potential GraphRAG integration. The API module, built with the Django framework, extends Django's admin interface and permission control system to develop the benchmark and annotation interface. Django's maturity and large support community make it ideal for production development. The Agent module, also in Python, includes all agent related submodules, allowing deployment on suitable compute nodes without altering the architecture. Communication between the Client, API, and Agent modules will be via RESTful endpoints. For sharing large data between modules, local deployments (e.g., Client"}, {"title": "4 Demonstration", "content": ""}, {"title": "4.1 Datasets", "content": "Most multimodal question answering datasets focus on multiple-choice questions rather than open-ended conversations (Sundar and Heck, 2022). Some, like Image-Chat (Shuster et al., 2018), involve multimodal conversations with images as extra input, but the output is often multiple-choice or text-based (Liu et al., 2022). A major hurdle in developing multimodal conversational agents is the lack of appropriate datasets.\nWhile there is no shortage of data from human-human interactions or extracted from movies and YouTube videos, we lack efficient methods to organize this data into structured datasets. For specific domain applications, collecting data from human interactions and extracting datasets to train systems would be beneficial, allowing the agents to mimic human behavior. Our OpenOmni Framework provides both capabilities: extracting conversational datasets from videos and testing them through the pipeline to evaluate agents' responses, or collecting data from real-world scenarios to generate datasets for further research."}, {"title": "4.2 Can \u201cAI\u201d be your president?", "content": "One intensive conversational scenario is a debate. We extracted segments from the US Presidential Debate 2024 between Biden and Trump, focusing on Biden addressing the public and handling questions. These segments were fed into our pipeline to evaluate its performance under different configurations: OpenAI Whisper for speech-to-text, GPT-40 vision model, and text-to-speech (GPT4O_ETE); a locally deployed quantization LLM with Whisper, text-to-speech, and our emotion detection model for video input (QuantizationLLM_ETE); a version using HuggingFace LLM for inference"}, {"title": "4.3 Assist the visually impaired", "content": "While latency and the need for external information currently preventing AI from mission critical tasks, conversational agents can be production-ready and useful for non-latency-critical areas that do not require extensive external knowledge. Assisting indoor activities for the visually impaired is one such application. We prepared questions for the visually impaired, including locating objects, navigating indoors, and inquiries about the surroundings. Six questions were sampled and fed to the GPT4O_ETE pipeline. One scenario demonstration is included in our provided YouTube video. \nAnnotated results show a 4.7/5 accuracy, but the agent lacks specific skills for assisting the visually impaired. For example, ideally, it should provide step-by-step instructions on grabbing a coffee cup rather than just a general description. This indicates that while conversational agents are nearly ready for assisting the visually impaired with indoor activities, improvements in latency and response quality are still needed."}, {"title": "5 Conclusion", "content": "Multimodal conversational agents offers a more natural human-computer interaction, exemplified by models like GPT-40. However, real-world constraints necessitate balancing cost, latency, and accuracy, which may explain why GPT-40's full capabilities are not yet accessible.\nThere are several technical options to achieve this, including traditional divide-and-conquer methods, fully end-to-end models like GPT-40, and Hybrid approaches. The fully end-to-end approach inherently allows for lower latency, while the divide-and-conquer method faces latency issues when coordinating multiple components. Both approaches must address the challenge of handling large data I/O. If models are deployed locally, local network I/O issues can be more manageable. However, OpenAI's models are closed-source, making local deployment impractical. While deploying other vision models locally is feasible, achieving high accuracy may be limited by local computational resources. Hybrid solutions provides alternative approaches: pre-processing or compressing large data locally and then utilizing cloud-based models, or converting video to text and integrating it into the end-to-end voice model.\nWe developed the OpenOmni framework to enable researchers to integrate their work into an end-to-end pipeline. The framework supports various solutions, allows for pipeline customization, generates latency performance reports, and provides an annotation interface for accuracy review. These features facilitate the creation of benchmark reports to identify and address key issues.\nTesting with the US Presidential debate scenario highlighted latency as a critical issue, particularly with large video data. Integrating external knowledge remains a challenge, emphasizing the need for efficient Retrieval-Augmented Generation (RAG). For applications like indoor assistance for the visually impaired, latency improvements and model adaptation are both essential.\nThe OpenOmni framework can significantly benefit the research community by facilitating the collection and management of new datasets, integrating various conversational agents approaches, and generating automatic latency benchmarks. Its annotation interface aids in accuracy performance review, making OpenOmni production-ready for suitable application scenarios and fostering further development in multimodal conversational agents."}]}