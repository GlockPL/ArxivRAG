{"title": "OpenOmni: A Collaborative Open Source Tool for\nBuilding Future-Ready Multimodal Conversational Agents", "authors": ["Qiang Sun", "Yuanyi Luo", "Sirui Li", "Wenxiao Zhang", "Wei Liu"], "abstract": "Multimodal conversational agents are highly\ndesirable because they offer natural and human-\nlike interaction. However, there is a lack of\ncomprehensive end-to-end solutions to sup-\nport collaborative development and benchmark-\ning. While proprietary systems like GPT-40\nand Gemini demonstrating impressive integra-\ntion of audio, video, and text with response\ntimes of 200-250ms, challenges remain in bal-\nancing latency, accuracy, cost, and data pri-\nvacy. To better understand and quantify these\nissues, we developed OpenOmni, an open-\nsource, end-to-end pipeline benchmarking tool\nthat integrates advanced technologies such as\nSpeech-to-Text, Emotion Detection, Retrieval\nAugmented Generation, Large Language Mod-\nels, along with the ability to integrate cus-\ntomized models. OpenOmni supports local\nand cloud deployment, ensuring data privacy\nand supporting latency and accuracy bench-\nmarking. This flexible framework allows re-\nsearchers to customize the pipeline, focus-\ning on real bottlenecks and facilitating rapid\nproof-of-concept development. OpenOmni can\nsignificantly enhance applications like indoor\nassistance for visually impaired individuals,\nadvancing human-computer interaction. Our\ndemonstration video is available https://www.\nyoutube.com/watch?v=zaSiT3c1WqY, demo\nis available via https://openomni.ai4wa.\ncom, code is available via https://github.\ncom/AI4WA/OpenOmniFramework.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) (Zhao et al.,\n2023; Minaee et al., 2024) demonstrated remark-\nable capabilities in understanding user intentions\nand following instructions. However, text-only\nhuman-computer interaction (HCI) is often insuf-\nficient (Zhang et al., 2023). OpenAI recently re-\nleased their new flagship model, GPT-40, which\ncan reason across audio, video, and text in real\ntime. The impressive performance is achieved with\nresponse times between 200-250ms, which is ac-\nceptable for large-scale applications\u00b9. Google soon\nfollowed with their latest multimodal competitors,\nindicating a clear trend towards multimodal gener-\native models and applications2. LLaVA (Liu et al.,\n2023) is one of the early publicly available solu-\ntions for multimodal large models integrating text\nand images. However, there is currently no open\nsource end-to-end conversational agents implemen-\ntation and demonstration publicly available online.\nThe ideal form of multimodal HCI should mir-\nror human interactions, incorporating video and\naudio inputs with audio outputs. Despite the avail-\nability of various modular components, there is\nno comprehensive integrated open-source imple-\nmentation to foster research and innovation in this\nfield. Integrating existing models\u2014such as au-\ndio speech recognition (Speech2Text), multimodal\nlarge models (MLMs), and text-to-speech synthe-\nsis (TTS) into a multimodal conversation system\nreveals significant challenges in balancing latency\nand accuracy. Historically, accuracy has been a\nmajor hurdle; however, advancements in large lan-\nguage models (LLMs) have substantially improved\ncontextual relevance. The main challenge is reduc-\ning end-to-end latency while maintaining accuracy.\nWhile OpenAI and Google have shown it's possi-\nble, the open-source community lacks alternatives\nthat replicate this performance.\nAnother issue is data privacy. The GPT-4 fam-\nily of solutions also raise concerns about cost and\ndata privacy. Since GPT-4 is closed-source, users\nmust upload their data to the server via a paid API,\nraising privacy issues. The privacy policy of GPT3\ninforms users that various forms of personal in-\nformation, including account details, user content,\ncommunication information, and social media data,"}, {"title": "2 Related works", "content": "Solution options Traditional end-to-end multi-"}, {"title": "3 System design", "content": "For research purposes, it includes tools for easy\nannotation and benchmarking, offering real-time\nmonitoring and performance evaluation of latency.\nUsers can annotate individual components and\nentire conversations, generating comprehensive\nbenchmark reports to identify bottlenecks. The\nopen-source nature of OpenOmni allows for adap-\ntation across different application domains, such\nas aged care, personal assistant, etc. Each pipeline\ncomponent can be enabled or disabled based on\nspecific use cases, facilitating flexible and efficient\ndeployment. Additionally, the framework supports\nthe easy addition of extra models, enabling compar-\nisons and further experimentation. The OpenOmni\nframework allows researchers to focus on solving\ncritical bottlenecks without reinventing the wheel,\nfostering innovation in multimodal conversational\nagents. It enables rapid proof-of-concept develop-\nment, such as indoor conversational robots assist-\ning visually impaired individuals."}, {"title": "3.1 Requirement analysis", "content": "The system receives audio and video input, pro-\nduces audio as the output. Initially, we need two\nmodules: one to collect audio and video data from\nthe microphone and camera, and another to play\naudio through a speaker. These Client modules\nshould support diverse devices, such as a smart-\nphone, a laptop, or a Raspberry Pi. The collected\ndata will then be fed to a server.\nThe server, referred to as API, should manage\naudio, video data, and metadata. It should have\naccess to a storage layer that includes a relational\ndatabase, file management, and a graph database\nfor potential GraphRAG integration. While the\nAPI can reside on the same instance as the Client\nmodule, we prefer them to be separate for better\nadaptability. This separation introduces the chal-\nlenge of sharing large volumes of data between\nmodules. If the API is cloud-based, the audio and\nvideo data need to be uploaded to the cloud, for\nexample using AWS S3, Azure Blob Storage, or\nGoogle Cloud Storage. However, the upload pro-\ncess can become a bottleneck, making the data"}, {"title": "3.2 System architecture", "content": "Based on the requirements, we designed our system\nas shown in Figure 1. The system is divided into\nfive modules: Client, API, Storage, User Inter-\nface, and Agent, all primarily developed in Python.\nThe Client module includes two submodules: the\nListener for collecting video and audio data, and\nthe Responder for playing audio. The Storage\nmodule consists of file storage for media, a rela-\ntional database (PostgreSQL) for metadata, and a\ngraph database (Neo4j) for potential GraphRAG in-\ntegration. The API module, built with the Django\nframework, extends Django's admin interface and\npermission control system to develop the bench-\nmark and annotation interface. Django's maturity\nand large support community make it ideal for pro-\nduction development. The Agent module, also\nin Python, includes all agent related submodules,\nallowing deployment on suitable compute nodes\nwithout altering the architecture. Communication\nbetween the Client, API, and Agent modules will\nbe via RESTful endpoints. For sharing large data\nbetween modules, local deployments (e.g., Client"}, {"title": "4 Demonstration", "content": "on Raspberry Pi, API and Agent on local servers)\nwill use FTP for file synchronization. In cloud so-\nlutions (e.g., AWS), files will be uploaded to AWS\n$36, triggering a Lambda function to download\nfiles to an AWS Elastic File Storage (EFS) 7 shared\nby the API and Agent modules. We use Docker\nand Docker Compose to manage all modules, al-\nlowing easy setup with a single docker compose\nup command."}, {"title": "4.1 Datasets", "content": "Most multimodal question answering datasets fo-\ncus on multiple-choice questions rather than open-\nended conversations (Sundar and Heck, 2022).\nSome, like Image-Chat (Shuster et al., 2018), in-\nvolve multimodal conversations with images as\nextra input, but the output is often multiple-choice\nor text-based (Liu et al., 2022). A major hurdle in\ndeveloping multimodal conversational agents is the\nlack of appropriate datasets.\nWhile there is no shortage of data from human-\nhuman interactions or extracted from movies and\nYouTube videos, we lack efficient methods to orga-\nnize this data into structured datasets. For specific\ndomain applications, collecting data from human\ninteractions and extracting datasets to train systems\nwould be beneficial, allowing the agents to mimic\nhuman behavior. Our OpenOmni Framework pro-\nvides both capabilities: extracting conversational\ndatasets from videos and testing them through the\npipeline to evaluate agents' responses, or collecting\ndata from real-world scenarios to generate datasets\nfor further research."}, {"title": "4.2 Can \u201cAI\u201d be your president?", "content": "One intensive conversational scenario is a debate.\nWe extracted segments from the US Presidential\nDebate 2024 between Biden and Trump, focusing\non Biden addressing the public and handling ques-\ntions. These segments were fed into our pipeline to\nevaluate its performance under different configura-\ntions: OpenAI Whisper for speech-to-text, GPT-40\nvision model, and text-to-speech (GPT4O_ETE);\na locally deployed quantization LLM with Whis-\nper, text-to-speech, and our emotion detection\nmodel for video input (QuantizationLLM_ETE);\na version using HuggingFace LLM for inference"}, {"title": "4.3 Assist the visually impaired", "content": "While latency and the need for external information\ncurrently preventing AI from mission critical tasks,\nconversational agents can be production-ready and\nuseful for non-latency-critical areas that do not\nrequire extensive external knowledge. Assisting\nindoor activities for the visually impaired is one\nsuch application. We prepared questions for the\nvisually impaired, including locating objects, nav-\nigating indoors, and inquiries about the surround-\nings. Six questions were sampled and fed to the\nGPT4O_ETE pipeline. One scenario demonstra-\ntion is included in our provided YouTube video.\nThe latency statistics in Figure 8 show responses\nwithin approximately 30 seconds.\nAnnotated results show a 4.7/5 accuracy, but the\nagent lacks specific skills for assisting the visually\nimpaired. For example, ideally, it should provide\nstep-by-step instructions on grabbing a coffee cup\nrather than just a general description. This indicates\nthat while conversational agents are nearly ready\nfor assisting the visually impaired with indoor activ-\nities, improvements in latency and response quality\nare still needed."}, {"title": "5 Conclusion", "content": "Multimodal conversational agents offers a more\nnatural human-computer interaction, exemplified\nby models like GPT-40. However, real-world con-\nstraints necessitate balancing cost, latency, and ac-\ncuracy, which may explain why GPT-40's full ca-\npabilities are not yet accessible.\nThere are several technical options to achieve\nthis, including traditional divide-and-conquer\nmethods, fully end-to-end models like GPT-40,\nand Hybrid approaches. The fully end-to-end ap-\nproach inherently allows for lower latency, while\nthe divide-and-conquer method faces latency is-\nsues when coordinating multiple components. Both\napproaches must address the challenge of handling\nlarge data I/O. If models are deployed locally, local\nnetwork I/O issues can be more manageable. How-\never, OpenAI's models are closed-source, making\nlocal deployment impractical. While deploying\nother vision models locally is feasible, achieving\nhigh accuracy may be limited by local computa-\ntional resources. Hybrid solutions provides alter-\nnative approaches: pre-processing or compressing\nlarge data locally and then utilizing cloud-based\nmodels, or converting video to text and integrating\nit into the end-to-end voice model.\nWe developed the OpenOmni framework to en-\nable researchers to integrate their work into an end-\nto-end pipeline. The framework supports various\nsolutions, allows for pipeline customization, gen-\nerates latency performance reports, and provides\nan annotation interface for accuracy review. These\nfeatures facilitate the creation of benchmark reports\nto identify and address key issues.\nTesting with the US Presidential debate scenario\nhighlighted latency as a critical issue, particularly\nwith large video data. Integrating external knowl-\nedge remains a challenge, emphasizing the need for\nefficient Retrieval-Augmented Generation (RAG).\nFor applications like indoor assistance for the vi-\nsually impaired, latency improvements and model\nadaptation are both essential.\nThe OpenOmni framework can significantly ben-\nefit the research community by facilitating the col-\nlection and management of new datasets, integrat-\ning various conversational agents approaches, and\ngenerating automatic latency benchmarks. Its an-\nnotation interface aids in accuracy performance\nreview, making OpenOmni production-ready for\nsuitable application scenarios and fostering further\ndevelopment in multimodal conversational agents."}]}