{"title": "Adventures in Demand Analysis Using AI", "authors": ["Philipp Bach", "Victor Chernozhukov", "Sven Klaassen", "Martin Spindler", "Jan Teichert-Kluge", "Suhas Vijaykumar"], "abstract": "This paper advances empirical demand analysis by integrating multimodal product representations derived from artificial intelligence (AI). Using a detailed dataset of toy cars on Amazon.com, we combine text descriptions, images, and tabular covariates to represent each product using transformer-based embedding models. These embeddings capture nuanced attributes, such as quality, branding, and visual characteristics, that traditional methods often struggle to summarize. Moreover, we fine-tune these embeddings for causal inference tasks. We show that the resulting embeddings substantially improve the predictive accuracy of sales ranks and prices and that they lead to more credible causal estimates of price elasticity. Notably, we uncover strong heterogeneity in price elasticity driven by these product-specific features. Our findings illustrate that AI-driven representations can enrich and modernize empirical demand analysis. The insights generated may also prove valuable for applied causal inference more broadly.", "sections": [{"title": "1 Introduction", "content": "Almost a century ago, The Journal of the American Statistical Association published early empirical studies of demand that applied statistical methods to measure how consumers respond to price changes: Work by scholars such as Wright (1929), Working (1943), Schultz (1933), Mills (1931, 1937a,b), and Stigler (1939) moved economics from theory towards quantitative measurement. By doing so, this work provided a foundation for econometrics, a field of statistical analysis focusing on economic problems. Their research established a tradition of using data to understand market behavior and inform economic models.\nToday, advances in artificial intelligence (AI) and machine learning offer opportunities to build on this tradition. Instead of relying solely on simple numeric variables, researchers can now incorporate AI-generated product representations derived from text descriptions and images. These methods draw on hedonic modeling approaches (Griliches, 1971; Pakes, 2003) and integrate recent machine learning techniques (Devlin et al., 2019; Dosovitskiy et al., 2021), allowing economists to represent products more richly and capture nuances that standard covariates do not.\nUsing sales ranking and price data for toy cars on Amazon.com, we demonstrate how transformer-based models can leverage multiple, rich sources of product information for demand analysis. Our data include text descriptions, images, sales ranks, and prices. These multimodal inputs yield highly informative numerical embeddings that capture demand-relevant product attributes not easily summarized by standard human-encoded tabular variables\u2014such as quality, branding, and visual characteristics\u2014as we illustrate in Section 2.\nWe then fine-tune these embeddings to predict price and quantity signals, as these predictions are critical inputs to our causal inference problem. The resulting models achieve higher predictive accuracy than simpler specifications which rely solely on tabular data. Embeddings capture subtle distinctions between products\u2014such as quality, branding, or visual characteristics\u2014that influence consumer demand and market prices, but are difficult to quantify using conventional methods. This improvement in predictive power suggests that AI-generated representations can meaningfully enhance empirical demand analysis and other causal inference tasks.\nFinally, we address the challenge of estimating the price elasticity of demand, a central economic parameter. In our setting, simple cross-sectional regressions yield implausibly small elasticity estimates because they fail to capture product visibility and quality as key confounders. This motivates us to formulate a dynamic model with multimodal product attributes, along with lagged quantity and price signals, all of which serve both as confounders and as price-elasticity modifiers. By estimating such a dynamic model, we obtain more realistic price elasticities. Furthermore, we uncover pronounced heterogeneity in price elasticities that varies with product characteristics, as well as with how expensive and popular the products are. This underscores the economic value of AI-based representations: when properly fine-tuned, they yield more nuanced and credible estimates of how consumers respond to price changes across different products.\nOur approach contributes to multiple strands of the literature. It extends empirical demand analysis by employing AI-generated, multimodal representations of products. Our work also builds on the emerging intersection of econometrics and machine learning (Athey and Imbens, 2019; Mullainathan and Spiess, 2017; Varian, 2014; Chernozhukov et al., 2018a) and complements recent studies that apply AI-based text analysis and other modern methods to economic questions (Belloni et al., 2014; Bajari et al., 2023; Compiani et al., 2023). In doing so, it provides a framework for combining flexible product representations with established econometric tools for identification and inference. It also introduces the idea that embeddings can and should be fine-tuned with causal inference in mind\u2014which is critical in our context and potentially useful in other applications.\nOur key empirical result is that AI-based embeddings are strong determinants/modifiers of"}, {"title": "2 Using AI to Understand and Represent Products", "content": ""}, {"title": "2.1 The Data and Measurement of Prices and Quantities", "content": "Our analysis uses a data set of toy cars from Amazon.com, compiled and provided by the data aggregator Keepa.com. For each item i, we collected its sales rank and price at time points spanning from April to December 2023. We also gathered each product's description, image, and additional tabular features (e.g., its subcategory on Amazon.com), as summarized in Table 1. Figure 1 illustrates a typical product page containing the product image and description. Overall, our data set comprises N = 9,613 unique products.\nFor our analysis, we define the quantity signal as\n$Q_{it} = log(1/Time-Averaged\\ Sales\\ Rank\\ of\\ i\\ in\\ period\\ t)$,\nand the price signal as\n$P_{it} = log(Time-Averaged\\ Price\\ of\\ i\\ in\\ period\\ t)$.\nEach period, indexed by $t = 1,...,T$, spans 4 weeks. We have $T = 9$ periods in total, each separated by 1 week. We structure our data set this way to limit inter-temporal feedback in our price elasticity analysis in Section 3. We also examine temporal changes in these signals,\n$\\Delta Q_{it} := Q_{it} - Q_{i(t-1)}$ and $\\Delta P_{it} := P_{it} - P_{i(t-1)}$."}, {"title": "2.2 Using AI to Represent Products", "content": "To convert product data into useful numerical features, we employ various encoding models based upon the transformer neural network architecture proposed by Vaswani et al. (2017). We convert text descriptions into embeddings $T_i$; using language models such as RoBERTa (Liu et al., 2019) or LLAMA 3 (Touvron et al., 2023), convert images into dense embeddings $I_i$ using the BEiT model (Bao et al., 2022), and transform tabular data into embeddings using the SAINT model (Somepalli et al., 2022). The Appendix provides additional implementation details.\nWhile the aforementioned models are designed to work with the data we have on hand, the underlying approach is well suited for generalization to new types of data. In order to succeed in our context, these models must also be integrated and fine-tuned appropriately for estimating the price sensitivity. We discuss three key aspects of this approach which help explain its success and facilitate generalization to new contexts: self-supervised learning, the attention-based transformer architecture, and fine-tuning motivated by orthogonalized estimation of causal effects.\nSelf-Supervision. A significant challenge in machine learning is the scarcity of high-quality labeled data, as manual annotation is both expensive and time-consuming. Self-supervision addresses this limitation by creating labeled examples directly from unlabeled data. In this process, a portion of the input is deliberately masked or corrupted, and the model learns to predict these masked elements. This approach, fundamental to models like BERT (Devlin et al., 2019), effectively transforms each input sample into a self-labeled instance.\nConsider the example sentence: S = \u201cWell made diecast model truck with metal body.\u201d We create a masked version: W \u201cWell made [m] model truck with [m] body\u201d. The original sequence S serves as an auxiliary label that the model attempts to reconstruct from the corrupted input W. By applying this approach to billions of sentences, models learn to capture syntactic and semantic relationships without explicitly annotated labels.\nThe resulting internal representations, called embeddings, are extracted from the model's hidden layers and represent features of words or sentences. The approach generalizes well to other data types: for images, masking out patches and asking the model to predict the missing parts (He et al., 2022) enables the extraction of informative, context-dependent embeddings.\nAttention. Transformer-based models employ so-called attention mechanisms (Vaswani et al.,"}, {"title": "Causal Fine-Tuning", "content": "After a model has learned embeddings through self-supervision, it can be adapted for various downstream tasks. In our setting, the embeddings are important inputs to our causal inference problem: we are interested in how prices affect demand, holding fixed both product characteristics and other demand determinants. Our fine-tuning updates the pre-trained model parameters for this specific end-goal, by optimizing prediction of quantity and price signals.\nThis is precisely the right target for orthogonal estimation of the price elasticity, as we further discuss in Section 3.4.\nDuring fine-tuning, the embeddings serve as inputs to a specialized prediction layer. The errors from the prediction layers are then used to inform and update the parameters of the embeddings through gradient descent steps, which are computed via back-propagation (Rumelhart et al., 1986).\nThe following diagram summarizes the process:"}, {"title": "2.3 Evaluating the Embeddings", "content": "After obtaining the embeddings, we must assess whether they effectively represent the products and \u201cunderstand\u201d their characteristics. We first take the concatenated embeddings $E_i = (T_i, I_i)$, where $T_i$ also includes tabular embeddings, and then apply a Johnson\u2013Lindenstrauss projection of these embeddings onto a 256-dimensional vector $\\bar{E}_i$; Johnson (1984). This projection approximately preserves distances and is therefore considered (at least approximately) information-lossless. We then center and normalize the embeddings so they lie on a hypersphere:\n$X_i^s := \\frac{\\bar{E}_i - \\frac{1}{N} \\sum_i \\bar{E}_i}{\\| \\bar{E}_i - \\frac{1}{N} \\sum_i \\bar{E}_i \\|}$.\nWe use these normalized embeddings in our subsequent analysis.\nWe evaluate these embeddings through two approaches:\n1. Qualitative. We examine similar products or clusters of products on this hypersphere and assess the results qualitatively.\n2. Quantitative. We determine whether these AI-generated features improve predictions of price and quantity signals, where predictions serve as key inputs into downstream causal inference.\nBoth approaches are crucial for demand analysis, including the computation of hedonic inflation prices, forecasting demand and prices for new products, and understanding how demand responds to price variations."}, {"title": "2.3.1 Qualitative Assessment", "content": "For the clustering task, we perform k-means clustering to group products into five clusters based on their embeddings. To examine the influence of images, we first cluster using both text and image embeddings, and then using only text embeddings. We visualize the resulting product clusters in three-dimensional space by projecting the embeddings onto the first three principal components, as shown in Figures 3 and 4.\nWhen text and image embeddings are combined, the projection yields a \u201cfull\u201d ball of product points with distinctly separated clusters. In contrast, using text-only embeddings produces a \u201cstripe on a sphere,\u201d where the points are concentrated near the boundary and around the equator of the ball. Nevertheless, the clusters remain well-separated even without the image information."}, {"title": "2.3.2 Quantitative Assessment", "content": "While the previous discussion provides a qualitative indication that the model can represent products effectively, we now present a more quantitative assessment of the model's predictive performance.\nWe begin by examining how well the embeddings\u2014and their \u201ccompressed\u201d versions-predict price and quantity levels, as well as their changes. Formally, our targets are $Y \\in {Q,P,\\Delta Q,\\Delta P}$.\nWe also use these predictive regressions to fine-tune the embeddings themselves.\nAs shown in Table 5, simple linear regressions using only tabular data perform poorly. Boosted trees yield substantial gains in predictive accuracy, and neural networks with text embeddings perform even better. Including image embeddings offers further improvements, though the additional gains are modest (e.g., a 1.5 percentage-point increase in $R^2$ for $Q_{it}$). Nonetheless, these gains are meaningful.\nWe also assess each model's ability to predict changes in quantities and prices, rather than their levels. As expected, predicting changes is notably more difficult, leading to a sharp decline"}, {"title": "3 Estimating Price Effects", "content": "Understanding how price changes affect consumers' choices is a central challenge in empirical economics and marketing. One common way to measure this relationship is through the elasticity of demand with respect to price. Although a regression of sales on prices may appear a straightforward way to estimate elasticity, it can yield biased estimates if key confounding factors are not properly accounted for. In this section, we explore various approaches to uncover the true causal price sensitivity and discuss their respective strengths and limitations."}, {"title": "3.1 Initial Approach and Challenges", "content": "A natural starting point is to estimate the relationship between price and product performance using a predictive model. Consider a regression of the (log) inverse ranking of product i at time t, denoted $Q_{it}$, on the (log) price $P_{it}$ and a set of controls $X_{it} = (X^s_i,X^v_{it})$, where $X^s_i$ represents other tabular controls:\n$L[Q_{it} | P_{it}, X_{it}] = \\delta P_{it} + g_t(X_{it})$,\nwhere $g_t(\\cdot)$ is a function describing how the control variables influence the outcome over time. We allow $g_t$ to vary with t. The notation $L[Y | P, X]$ denotes the projection of the random variable $Y$ onto the space of partially linear prediction rules of the form $\\alpha P+g(X)$.\nIn practice, directly estimating this model often suggests a very small price sensitivity (or \u201celasticity\u201d), captured by the coefficient $\\delta$: $\\delta \\sim [0, -0.2]$. From a causal perspective, this result is implausible: the notion that a price change exerts virtually no effect on ranking or sales is"}, {"title": "3.2 The Causal Dynamic Model and Regressions", "content": "To address the issue above, we introduce a simple dynamic panel data model to guide our statistical analysis. We can view the outcomes and key variables as arising from the following structural equation model (SEM):\n$Q_{it} = a_t(S_{it}, \\epsilon^q_{it}) P_{it} + q_t(S_{it}, \\epsilon^q_{it}),$ (1)\n$P_{it} = p_t(S_{it}, \\epsilon^p_{it}),$ (2)\n$S_{it} = s_t(S_{i,t-1}, \\epsilon^s_{it}); S_{it} = (Q_{i,t-1},P_{i,t-1},X^s_i, X^v_{it}),$ (3)\nwhere $a_t, q_t, p_t$, and $s_t$ are nonparametric structural functions, and $\\epsilon^q_{it}, \\epsilon^p_{it}$, and $\\epsilon^s_{it}$ are i.i.d. stochastic vectors that are mutually independent.\nThis specification defines an autoregressive model in which the quantity signal $Q_{it}$ depends on the price signal $P_{it}$ and other state variables $S_{it}$. The state variables include lagged quantity and price, $Q_{i,t-1}$ and $P_{i,t-1}$, time-invariant product characteristics $X^s_i$ (captured through embeddings), and time-varying characteristics $X^v_{it}$ (such as ratings and the number of reviews). Among these variables, the lagged quantity $Q_{i,t-1}$ is arguably a key confounder, reflecting both product visibility and quality\u2014a conclusion reinforced by our empirical findings below. In particular, including the lagged quantity in the model substantially shifts the estimated price elasticity into a more plausible range.\nBecause the model follows a Markovian structure, each period updates the state variables, after which prices and quantities respond to the new state vector. Figure 5 illustrates this SEM"}, {"title": "3.3 Empirical Models", "content": "In the empirical analysis, we examine two forms of the CACE function:\nI. Homogeneous Effect:\n$a_t(S_{it}) = a_t;$\n(5)\nII. Heterogeneous Effect:\n$a_t(S_{it}) = a_{0t} + \\sum_{k=1}^{K} a_{kt} X^{sim}_{ik} + b_{1t} P_{i,t-1} + b_{2t} Q_{i,t-1}.$\n(6)\nThe first specification is very simple and serves as our baseline. The second is more elaborate yet still structured, allowing the elasticity function s\u2192 a\u2084(s) to depend on product characteristics as well as past quantities and prices:\n\u2022 The first component of a(s) captures product characteristics in the product space, represented by similarity vectors describing the product's position.\n\u2022 The second part lets the elasticity vary with how popular the products are (lagged quantity) and how expensive they are (lagged price).\nWe show empirically that both components matter. In presenting our results, we assume time homogeneity by setting a\u2084(\u00b7) = a(\u00b7). Empirically, this did not affect any findings; we adopt this simplification purely for clarity of presentation."}, {"title": "Orthogonal Inference of Causal Effects", "content": "We identify and estimate the causal effects using the following projection equation:\n$Q_{it}^{\\perp} = \\delta_t(S_{it})P_{it}^{\\perp} + e_{it}, \\ \\ \\ e_{it} \\perp P_{it}^{\\perp} | S_{it},$\n(7)\nwhere $e_{it} \\perp P_{it}^{\\perp} | S_{it}$ means $E[e_{it} P_{it}^{\\perp} | S_{it}] = 0$. The pair $(Q_{it}^{\\perp}, P_{it}^{\\perp})$ consists of the residuals\n$Q_{it}^{\\perp} = Q_{it} - E[Q_{it} | S_{it}], \\ \\ P_{it}^{\\perp} = P_{it} - E[P_{it} | S_{it}].$\nThe coefficient function $\\delta_t(S_{it})$ is the conditional predictive effect (CAPE) of a shock in the exposure variable on a shock in the outcome:\n$\\delta_t(S_{it}) := \\frac{E[Q_{it}^{\\perp} P_{it}^{\\perp} | S_{it}]}{E[P_{it}^{\\perp 2} | S_{it}]}$"}, {"title": "Some Reflections on the Limitations of the Analysis", "content": "Our statistical estimates carry a well-defined causal interpretation under the stated, relatively strong assumptions. The main threat to this interpretation is the presence of latent, time-varying factors that bias the relationship between $P_{it}$ (the price signal) and $Q_{it}$ (the quantity signal). Indeed, we might suspect that price endogenously responds to demand shocks $\\epsilon_{it}$ from the outcome equation, effectively making $\\epsilon_{it}$ a confounder. The DAG below illustrates such a scenario.\nHowever, in our setting, we suspect that the link $\\epsilon_{it} \\rightarrow P_{it}$ may be weak, as prices often follow \u201csticky,\u201d piecewise-constant paths that do not change as frequently as quantity signals (sales ranks); see, for example, Figures 2 in Section 2. Formally, if the edge from $\\epsilon_{it}$ to $P_{it}$ is zero, our earlier identification strategy holds, and our estimates are indeed causal. Otherwise, we can treat them as approximations of the causal estimates. For methods to bound the effect distortion caused when $\\epsilon_{it}$ affects $P_{it}$, see Chernozhukov et al. (2021)."}, {"title": "4 Concluding Remarks", "content": "This study highlights the significant potential of AI-generated multimodal embeddings in demand analysis. By integrating text, image, and tabular data into a causal econometric framework, we improve both the precision of demand and price forecasts and the credibility of elasticity estimates. Our findings show that these rich embeddings not only enhance predictive accuracy but also reveal substantial heterogeneity in consumer price sensitivity, providing nuanced insights into demand behavior. These advancements create a methodological bridge between machine learning and econometrics, illustrating how modern AI tools can enrich traditional economic analyses. In future research, we hope to further explore the intersection of AI and causal inference, including bias-bounding and instrumental variable strategies to address time-varying confounding."}, {"title": "A Addendum: Robustness to Estimated Embeddings", "content": "One may suspect that using estimated embeddings instead of \"optimal\u201d ones could complicate inference. However, under mild conditions, this is not the case. To illustrate this point in a simple manner, consider the homogeneous model:\n$Q_{it}^{\\perp} = \\delta_t P_{it}^{\\perp} + e_{it}, \\ \\ \\ e_{it} \\perp P_{it}^{\\perp} | S_{it}$.\nLet $Y_{it}$ denote the predictive target, which is either $P_{it}$ or $Q_{it}$. Let $X_i^s(\\hat{\\phi})$ denote the estimated and fine-tuned embeddings, where $\\hat{\\phi}$ denotes estimated parameters, and $S_{it}(\\hat{\\phi})$ the derived controls. We assume that $\\hat{\\phi}$ is obtained from data that are independent of the main data used in the analysis.\nSimilarly, let $X_i^s$ and $S_{it}$ denote the ideal embeddings and controls, in the sense that\n$E[Y_{it} | S_{it}, S_{it}(\\hat{\\phi})] = E[Y_{it} | S_{it}]$.\nIn other words, after including $S_{it}$, the best prediction rule for $Y_{it}$ given both $S_{it}$ and $S_{it}(\\hat{\\phi})$ depends only on $S_{it}$.\nConsider a learner $\\hat{\\gamma}(S_{it}(\\hat{\\phi}))$ that minimizes empirical risk $E_{i \\in A}{[Y_{it} - \\gamma(S_{it}(\\hat{\\phi}))]^2}$ over control functions $\\gamma$ in the convex model $F$, conditional upon fine-tuned embeddings $\\hat{\\phi}$ (this includes our considered models 1-3). Here A is a subset of ${1, .., n}$ whose size is proportional to n. The DML inference approaches using ideal and estimated embeddings are first-order equivalent under the following two key conditions:\n(E1) For the given $\\hat{\\phi}$, the square root of the offset Rademacher complexity (Liang et al., 2015) of the class $F_A = {\\gamma(S_{it}(\\hat{\\phi})) : \\gamma \\in F }$ is $o_p(n^{-1/4})$.\n(E2) The approximation error of the model $F$ with estimated embeddings $\\hat{\\phi}$ is sufficiently small:\n$inf_{\\gamma \\in F} \\sqrt{E[{E[Y_{it} | S_{it}] - \\gamma(S_{it}(\\hat{\\phi}))^2 | \\hat{\\phi}]} = o_p(n^{-1/4})$.\nFor condition (E1), it is useful to recall that for high-dimensional parametric models with d parameters, the square root of the offset complexity scales as $\\sqrt{d/n}$, so the condition above requires the dimension d is o($\\sqrt{n}$); see Liang et al. (2015) and Bach (2024) for further discussion and bounds for other classes of nonparametric learners. Condition (E2) depends on the specification of the model $F$ as well as the quality of the fine-tuned embeddings $\\hat{\\phi}$. It also tells us that fine-tuning should be done with the goal of predicting the labels $Y_{it}$; the better we do this, the more plausible condition (E2) becomes."}]}