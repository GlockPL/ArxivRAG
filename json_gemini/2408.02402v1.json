{"title": "Enhancing AI-based Generation of Software Exploits with Contextual Information", "authors": ["Pietro Liguori", "Cristina Improta", "Roberto Natella", "Bojan Cukic", "Domenico Cotroneo"], "abstract": "This practical experience report explores Neural Machine Translation (NMT) models' capability to generate offensive security code from natural language (NL) descriptions, highlighting the significance of contextual understanding and its impact on model performance. Our study employs a dataset comprising real shellcodes to evaluate the models across various scenarios, including missing information, necessary context, and unnecessary context. The experiments are designed to assess the models' resilience against incomplete descriptions, their proficiency in leveraging context for enhanced accuracy, and their ability to discern irrelevant information. The findings reveal that the introduction of contextual data significantly improves performance. However, the benefits of additional context diminish beyond a certain point, indicating an optimal level of contextual information for model training. Moreover, the models demonstrate an ability to filter out unnecessary context, maintaining high levels of accuracy in the generation of offensive security code. This study paves the way for future research on optimizing context use in AI-driven code generation, particularly for applications requiring a high degree of technical precision such as the generation of offensive code.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, the integration of AI-code generators into the programming workflow has marked a paradigm shift, significantly streamlining coding tasks and facilitating the interpretation of complex instructions through natural language (NL) using advanced ML models. This evolution has been particularly transformative in fields that demand high technical skills, such as offensive security, where the automation of coding tasks promises to boost productivity and innovation [1]. Offensive security, with its emphasis on developing proof- of-concept attacks to uncover and exploit vulnerabilities, oc- cupies a critical junction in cybersecurity efforts. It aids in understanding the mechanics of attacks, motivating timely patches and mitigations [2]. However, the manual crafting of exploit code, given its dependence on deep system-level knowledge, presents a considerable bottleneck. Herein lies the appeal of automatic exploit generation (AEG): an AI- driven solution to enhance the efficiency of security analysts by generating functional exploits for security assessments. Recent advancements have seen AI-based code generators, employing Neural Machine Translation (NMT) models, make significant strides in translating NL descriptions (intents) into precise programming code snippets [3], [4]. This capability is particularly noteworthy within the offensive security domain, where code generators translate intents detailing the exploita- tion of system vulnerabilities into actionable code [5], [6]. Yet, this domain's complexity introduces distinct challenges. The need for a stringent vocabulary to articulate low-level op- erations essential for exploiting system vulnerabilities means that the variability inherent in technical descriptions can severely impact the functionality of generated code. Variations in developers' technical knowledge, terminology use, and description specificity can lead to significant semantic discrep- ancies [7], [8]. Such variability underscores the necessity for code generators to exhibit robustness, ensuring effective opera- tion across a wide spectrum of sentence structures, vocabular- ies, and detail levels. The absence of this robustness could limit code generators' real-world applicability, constraining their usbaility in practice [9], [10]. Hereby, exploit genera- tion is pivotal in bridging the gap between theoretical NMT capabilities and practical, high-stakes security applications. This practical experience report offers a critical examination of the potential and limitations of AI-code generators when faced with NL descriptions of varying accuracy and detail in the realm of applications requiring high levels of precision and contextual awareness, such as in the development of offensive security code. More precisely, this work delves into the poten- tial benefits of training the models to comprehend the context of a sentence, aiming to discern the extent to which context understanding can compensate for the ambiguities of the NL. To this aim, we employ a prompt-engineering solution that feeds models with context information (contextual learning) by using the concatenation of inputs, i.e., by merging previous NL descriptions with the current one [11]\u2013[14]. By harnessing the power of contextual learning, we demon- strate how AI code generators can significantly improve in translating NL descriptions into code for software exploits. Our study performs an extensive evaluation involving six state-of-the-art NMT models, each assessed for its ability to navigate the complexities of generating code from NL descriptions that vary in detail and context. Furthermore, the construction of a unique dataset from real-world exploits for model training and evaluation underscores the practical relevance and applicability of our findings."}, {"title": "II. RELATED WORK", "content": "Automatic exploit generation (AEG) research challenge consists of automatically generating working exploits [2]. This task requires technical skills and expertise in low-level languages to gain full control of the memory layout and CPU registers and attack low-level mechanisms (e.g., heap metadata and stack return addresses) not otherwise accessi- ble through high-level programming languages. Given their recent advances, AI-code generators have become a new and attractive solution to help developers and security testers in this challenging task. Although these solutions have shown high accuracy in the generation of software exploits, their robustness against new inputs has never been studied before. Liguori et al. [15] released a dataset containing NL descrip- tions and assembly code extracted from software exploits. They performed an empirical analysis showing that NMT models can correctly generate assembly code snippets from NL and that in many cases can generate entire exploits with no errors. The authors extended the analysis to the generation of Python offensive code used to obfuscate software exploits from systems' protection mechanisms [16]. Yang et al. [17] proposed a data-driven approach to software exploit generation and summarization as a dual learning problem. The approach exploits the symmetric structure between the two tasks via dual learning and uses a shallow Transformer model to learn them simultaneously. Yang et al. [18] proposed a novel template- augmented exploit code generation approach. The approach uses a rule-based template parser to generate augmented NL descriptions and uses a semantic attention layer to extract and calculate each layer's representational information. Ruan et al. [19] proposed PT4Exploits, an approach for software exploit generation via prompt tuning. They designed a prompt template to build the contextual relationship between English comment and the corresponding code snippet, simulating the pre-training stage of the model to take advantage of the prior knowledge distribution. Xu et al. [20] introduced an artifact-assisted AEG solution that automatically summarizes the exploit patterns from artifacts of known exploits and uses them to guide the generation of new exploits. The authors implemented AutoPwn, an AEG system that automates the generation of heap exploits for Capture-The-Flag pwn com- petitions. Recent work also explored the role of GPT-based models, including ChatGPT and Auto-GPT, in the offensive security domain. Botacin [21] found that, by using these models, attackers can both create and deobfuscate malware by splitting the implementation of malicious behaviors into smaller building blocks. Pa et al. [22] and [23] proved the feasibility of generating malware and attack tools through the use of reverse psychology and jailbreak prompts, i.e., maliciously crafted prompts able to bypass the ethical and privacy safeguards for abuse prevention of AI code generators like ChatGPT. Gupta et al. [23] also examined the use of AI code generators to improve security measures, including cyber defense automation, reporting, threat intelligence, secure code generation and detection, and attack and malware detection. These studies do not take into account that, since developers have different levels of expertise and writing styles, code descriptions may be missing some information or reference previous instructions. The issue of inferring the context of the current sentence from the surrounding ones has been widely addressed for translation tasks between different natural languages. Proposed solutions vary from data-driven methods, to structural modifications to the model's architecture, or hybrid solutions. As for the former approach, previous studies concatenate previous and subsequent sentences, separated by a special token, to provide additional information to the model when translating the current phrase [11], [13], [24]. Regarding architectural solutions, these include the integration of multiple encoders to encode not only the source-sentence, but also the context, i.e., the previous or next sentences [14], [25], or to encode the global context of the document [12], [26]. These techniques have recently been applied also in the"}, {"title": "III. PROBLEM STATEMENT", "content": "AI code generation, powered by NMT models, represents a significant leap forward in bridging the gap between the conceptual intent expressed in NL and its translation into code. These advanced tools, trained on extensive datasets of intent- code pairs, have the capability to predict code snippets from a single NL description, encompassing everything from simple instructions to complex multi-line code segments. However, the deployment of NMT models for code gen- eration is not without its challenges. A primary concern is the inherent variability of NL descriptions, which can signif- icantly impact the models' effectiveness. Indeed, developers rarely provide exhaustive details in their code descriptions, often omitting information that they consider self-evident or redundant [10]. This tendency towards brevity or assumed knowledge poses a significant challenge for NMT models, which rely on the completeness and clarity of the input to generate accurate code. The example provided in TABLE II serves as a case in point, illustrating the discrepancies between human interpretation of NL instructions and the literal output of an NMT model. In this example, the NMT model correctly interprets the first instruction to subtract a value from a byte in the ESI register. However, it falters when presented with a subsequent instruction that refers to the operation's outcome simply as \"the result.\" A human developer would instinctively understand this reference to imply the same register involved in the preceding operation, yet the model fails to make this connection, instead defaulting to a generic placeholder \u201cvar\u201d due to the lack of explicit mention of the register's name. This scenario underscores a critical limitation of current NMT models in code generation: their struggle with inferential reasoning and contextual understanding. For NMT models to be truly effective and reliable in a development setting, they must be capable of beyond-literal interpretation, grasping the unspoken implications of a given instruction based on its context within the broader scope of the code."}, {"title": "IV. RESEARCH STUDY", "content": "To enhance the model's ability to decipher the complexity of NL descriptions for code generation, our study adopts a solution that enriches the contextual awareness (i.e., the contextual learning) of NMT models during their training phase. Contextual learning, within the domain of AI code generation, refers to the capacity of NMT models to interpret and utilize the broader sequence of operations implied by a series of coding instructions [29]. Unlike traditional learning approaches that treat each instruction in isolation, contextual learning emphasizes understanding each piece of information within the continuum of previous and forthcoming instruc- tions. This approach enables the model to make informed inferences about ambiguous or incomplete descriptions by leveraging the context provided by surrounding intents. To enforce contextual learning in NMT models, we adopt a prompt engineering strategy that concatenates preceding intents to the current source intent. This method involves the strategic integration of additional context directly with the cur- rent instruction set, demarcated by a specially designed token, the _BREAK token [11]. This token serves a pivotal role in delineating the segments of concatenated intents, allowing the model to distinguish between the current actionable intent and its contextual backdrop. Specifically, we design two strategies to provide contextual information: \u2022 2to1 context: we concatenate the previous intent to the current source intent, separated by the _BREAK token, to form a single NL code description as the input; the corresponding target code snippet represents the output."}, {"title": "A. Research Questions", "content": "We designed this research study with the aim of answering the following research questions (RQs): \u25b7 R1: How do NMT models perform in generating offensive security code from NL descriptions when faced with missing information? This question seeks to understand the resilience of NMT mod- els to incomplete or ambiguous NL descriptions, a common scenario in real-world applications. By evaluating the models' ability to fill in the gaps and make informed guesses about missing details, we aim to assess their practical utility in of- fensive security contexts, where the stakes of misinterpretation are high, and the cost of errors can be significant. \u25b7 R2: Does contextual learning enhance the robustness of the NMT models in the generation of offensive security code? With RQ2, we delve into the potential of leveraging previous intents or contextual clues to improve model performance. This inquiry focuses on the models' ability to use additional, contextually relevant information to understand and accurately execute the current task. By exploring the impact of contextual learning, we seek to determine whether incorporating preced- ing intents as context can effectively mitigate the challenges posed by sparse or unclear NL descriptions. \u25b7 R3: Does unnecessary information negatively impact the performance of the NMT models in the generation of offensive security code? The third question addresses the potential drawbacks of con- textual information, particularly when it is irrelevant or super- fluous. This aspect of the research is crucial for understanding the models' ability to discern and filter out unnecessary infor- mation, ensuring that their focus remains on relevant details crucial for the accurate generation of code. By investigating the impact of unnecessary context, we aim to reveal insights into how NMT models manage information overload and identify strategies for optimizing their training to improve focus and relevance in code generation tasks."}, {"title": "B. Code Generation Task", "content": "To perform a rigorous evaluation of how the use of con- textual information coming from previous intents affects the translation ability of the models, we follow the best practices in the field. Hence, we support the models with data processing operations. Data processing is an essential step to support the"}, {"title": "C. Fine-tuning Dataset", "content": "To assess the impact of contextual learning in the generation of offensive security code from NL, we curated a dataset of real-world shellcodes from reputable online databases and developer resources [32], [33]. Since models require not just an understanding of individual code instructions but, crucially, how these instructions interact and depend on each other within a given context, our dataset consists of sequences of code that are contextually interconnected. In the domain of offensive code generation, shellcode gen- eration represents a critical and widely studied topic [16], [18], [19]. A shellcode is a list of machine code instructions to be loaded in a vulnerable application at runtime. The traditional way to develop shellcodes is to write them using the assembly language, and by using an assembler to turn them into opcodes (operation codes, i.e., a machine language instruction in binary format, to be decoded and executed by the CPU) [34], [35]. Common objectives of shellcodes include spawning a system shell, killing or restarting other processes, causing a denial- of-service (e.g., a fork bomb), leaking secret data, etc. To build the dataset, we first used 20 real shellcodes adopted in previous work to test models [16].  These programs encompass a wide range of functionalities and complexities, ensuring a comprehensive evaluation of the models' code-generation capabilities. To further extend our collection, we selected the 510 multi-line snippets from Shellcode_IA32 [36], a dataset comprising instructions in assembly language for IA-32 architectures from publicly-available security exploits and described in English. A multi-line sample represents a set of instructions that it would be meaningless to consider as separate because of the strong contextual relation between them. Hereby, these samples are perfectly suited for our training objectives since they embody the contextual relationships between consecutive code snippets. These lines correspond to code descriptions that generate multiple lines of shellcodes (separated by the newline character \n). In order to ensure a diverse and non-redundant dataset, we carefully checked that there were no duplicates between the 20 collected shellcodes and Shellcode_IA32. We opted not to include the entire Shellcode_IA32 dataset, as it primarily comprises single-line snippets that lack the contextual relationships (e.g., mov eax, 1). These snippets do not provide the sequential or logical linkages found in real-"}, {"title": "V. EXPERIMENTAL SETUP", "content": "In our experiments, we used a machine with a Debian-based distribution, 8 vCPU, 16 GB RAM, and one Nvidia T4 GPU.  Each analysis aims to explore a specific aspect of the model's capabilities, such as handling missing information, leveraging contextual learning, and discerning unnecessary context. The dataset for each experiment is systematically divided into training (i.e., the data used to fine-tune the models), validation (i.e., the data used to tune the model's parameters), and test sets (i.e., the data used to evaluate the model in the generation of code starting from new NL descriptions), adhering to a common ratio of 80%/10%/10% [36]\u2013[38], ensuring consistency and reliability in model evaluation."}, {"title": "A. NMT Models", "content": "To perform the code generation task, we consider a com- prehensive set of state-of-the-art models, which are described in the following. The first three (i.e., CodeBERT, CodeT5+, and PLBart) are based on an encoder-decoder architecture, where the input sequence is encoded into a context vector and then decoded to generate the output sequence. The latter (i.e., CodeGen, CodeGPT, and CodeParrot) are decoder-only models, which read the input sequence and predict subsequent words one at a time, well-suited for generation tasks. \u25a0 CodeBERT [40] is a large multi-layer bidirectional Trans- former architecture [41] pre-trained on millions of lines of code across six different programming languages. Our imple- mentation uses an encoder-decoder framework where the en- coder is initialized to the pre-trained CodeBERT weights, and the decoder is a transformer decoder, composed of 6 stacked layers. The encoder follows the ROBERTa architecture [42], with 12 attention heads, hidden layer dimension of 768, 12 encoder layers, and 514 for the size of position embeddings. We set the learning rate \\(a = 0.00005\\), batch size = 32, and beam size = 10. \u25a0 CodeT5+ [43] is a new family of Transformer models pre- trained with a diverse set of pretraining tasks including causal language modeling, contrastive learning, and text-code match- ing to learn rich representations from both unimodal code data and bimodal code-text data. We utilize the variant with model size 220M, which is trained from scratch following T5's architecture [44]. It has an encoder-decoder architecture with 12 decoder layers, each with 12 attention heads and hidden layer dimension of 768, and 512 for the size of position embeddings. We set the learning rate \\(a=0.00005\\), batch size = 16, and beam size = 10. \u25a0 PLBart [45] is a multilingual encoder-decoder (sequence- to-sequence) model primarily intended for code-to-text, text- to-code, code-to-code tasks. The model is pre-trained on a large collection of Java and Python functions and natural lan- guage descriptions collected from GitHub and StackOverflow. We use the PLBart-large architecture with 12 encoder layers and 12 decoder layers, each with 16 attention heads. We set the learning rate \\(a=0.00005\\), batch size=16, and beam size=10. \u25a0 CodeGen [46], is an autoregressive language model for program synthesis with an architecture that follows a standard transformer decoder with left-to-right causal masking. Specif- ically, we leverage CodeGen-350M-Multi, initialized from CodeGen-NL and further pre-trained on BigQuery [46], a large-scale dataset of multiple programming languages from GitHub repositories, which consists of 119.2B tokens and"}, {"title": "VI. EXPERIMENTAL EVALUATION", "content": "This section describes the experimental setup and results obtained from evaluating several NMT models on the task of generating assembly shellcodes from NL descriptions. The experiments were designed to assess the models' ability to handle missing information, leverage extended context effec- tively, and discern and utilize unnecessary information. A. Missing Information First, we evaluated the performance of NMT models when generating offensive security code from NL descriptions with no additional contextual information (see TABLE VI). This scenario simulates real-world conditions where developers may provide incomplete or vague descriptions due to oversight or assumption of implicit knowledge. Understanding how NMT models cope with such missing information is crucial for assessing their practical applicability in automated code generation tasks. CodeBERT and CodeT5+ exhibit superior performance across all metrics, with CodeT5+ showing particularly high scores in EM (59.35), METEOR (74.87), and ROUGE-L (73.44). CodeBERT also performs well, with its best score in METEOR (67.41). This indicates a strong capability of the models in accurately translating NL descriptions to code, suggesting a robust understanding of the language and the task requirements even in the absence of context. PLBart struggles significantly in this setup, with much lower scores across the board, particularly in EM (7.44) and ROUGE-L (27.04). This may indicate difficulties in capturing the essence of the NL descriptions and translating them into accurate code snippets without contextual cues. CodeGen, CodeGPT, and CodeParrot display moderate performances, with CodeGen performing relatively better among the three, especially in terms of EM (30.92) and ED (60.97). These models seem to have a moderate capability in understanding and generating code from NL descriptions, with varying degrees of success in handling the missing context. The Average scores across models (EM: 30.34, ED: 58.77, METEOR: 49.80, ROUGE-L: 48.31) reflect a collective moderate proficiency in dealing with NL descriptions devoid of additional context. This underscores a variation in how different models process and generate code"}, {"title": "RQ1: How do NMT models perform in generating offensive security code from NL descriptions when faced with missing information?", "content": "NMT models exhibit varied levels of proficiency in generating offensive security code from less detailed NL descriptions. The comparative analysis reveals a marked impact of missing information on the accuracy of code generation, particularly for CodeGPT, which saw a sig- nificant performance dip compared to its prior achieve- ments. Meanwhile, CodeBERT demonstrates a relative steadiness, albeit with room for improvement to match its earlier performance. This juxtaposition highlights the critical role of detailed NL descriptions in achieving high accuracy in code generation and also points to the essential need for enhancing NMT models' capabilities in dealing with sparse information. It emphasizes an ongoing research imperative to develop models that can more effectively bridge the gap between minimalistic NL instructions and the precise requirements of low-level programming tasks, particularly in the contextually rich domain of offensive security."}, {"title": "B. Contextual Learning", "content": "Then, we focused on analysing the impact of contextual learning on the performance of NMT models in generating assembly shellcodes from NL descriptions. The extended con- text was provided in two forms: one additional previous intent (2to1 context) and two additional previous intents (3to1 con- text) by using the 2to1 Context Test and the 3to1 Context Test, respectively (see TABLE VI). This investigation is particularly important for determining how much contextual information can effectively aid in enhancing model performance, especially in comparison to baseline performance without any extended context. The results show that all models improved performance in the 2to1 context compared to the no-context scenario, with CodeT5+ (EM: 62.40) and CodeBERT (EM: 61.07) leading the pack. This indicates a substantial benefit from including"}, {"title": "RQ2: Does contextual learning enhance the robustness of the NMT models in the generation of offensive security code?", "content": "Contextual learning enhances the robustness of NMT models in generating offensive security codes, affirming the positive impact of incorporating contextual infor- mation. Specifically, the addition of a single preced- ing instruction (2to1 context) markedly improves model performance across various metrics, demonstrating that even minimal context can significantly aid models in understanding code generation tasks more accurately. Comparing the 3to1 context to the no-context scenario reveals that extended context still offers benefits, with all models performing better than without any context. This further corroborates the value of contextual information in enhancing model performance. However, the slight decline in metrics when moving from a 2to1 to a 3to1 context suggests an optimal level of contextual informa- tion beyond which the effectiveness of additional context may diminish or even detract from model performance."}, {"title": "C. Unnecessary Information", "content": "Finally, we explored the impact of incorporating irrelevant contextual information (Unnecessary Context 2to1 and 3to1) on the performance of NMT models in generating offensive security code (see TABLE VI). This setup allows us to investigate the ability of the models to generate precise code in the presence of unnecessary context. The overall performance increased by 12.25, 8.46, 5.53, and 7.94 points for EM, ED, METEOR, and ROUGE-L, respec- tively, over the no-context baseline, indicating that models are capable of filtering out irrelevant information to some extent and still improving upon the no-context scenario. Decoder-only models like CodeGen, CodeGPT, and Code- Parrot showed significant improvements in EM scores (61.83,"}, {"title": "RQ3: Does unnecessary information negatively impact the performance of the NMT models in the generation of offensive security code?", "content": "Unnecessary information does not universally negatively impact the performance of NMT models in generating offensive security code. In the unnecessary 2to1 con- text setup, models demonstrate a remarkable ability to disregard irrelevant information, with decoder-only mod- els even showing improved performance over both the no-context and necessary 2to1 context scenarios. This improvement suggests that models can extract useful patterns or ignore distractions, focusing on the task- relevant aspects of the input, also based on their ar- chitecture. However, as the complexity of unnecessary context increases (unnecessary 3to1), we observe a gen- eral performance decrease compared to unnecessary 2to1 context, highlighting a limit to the models' ability to filter out noise. This performance decline, though slight, emphasizes the challenge of managing more extensive unrelated information, which can obscure relevant details and impede accurate code generation."}, {"title": "VII. THREATS TO VALIDITY", "content": "Models Selection. The external validity of the study might be impacted by the choice of models. To mitigate this, we carefully selected models with distinct architectures and ca- pabilities, ensuring a representation of current advancements in the field [47], [57]-[59]. This careful selection aims to ensure that our findings reflect broader trends in NMT model performance for code generation tasks. Moreover, we did not consider public AI models such as GitHub Copilot and Ope- nAI ChatGPT because they impose restrictions on malicious uses and, as a consequence, they often \"refuse\" to generate"}, {"title": "VIII. CONCLUSION", "content": "This work presented a comprehensive investigation into the capabilities of NMT models in the domain of offensive security code generation. Through a series of designed ex- periments, we investigated the complexities surrounding the models' interaction with missing information, their utilization of contextual learning, and their resilience in the face of unnecessary context. Our results showed that the introduction of contextual infor- mation substantially improved model performance, highlight- ing the value of leveraging contextual information to deal with undetailed NL descriptions. However, the results also showed the diminishing returns associated with extensive context, pointing to the existence of an optimal context threshold that maximizes model performance. In scenarios involving un- necessary context, our experiments demonstrated the models' ability to filter out irrelevant information, maintaining high performance in the code generation task."}]}