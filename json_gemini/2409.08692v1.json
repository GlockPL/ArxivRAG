{"title": "B4: Towards Optimal Assessment of Plausible Code Solutions with Plausible Tests", "authors": ["Mouxiang Chen", "Zhongxin Liu", "He Tao", "Yusu Hong", "David Lo", "Xin Xia", "Jianling Sun"], "abstract": "Selecting the best code solution from multiple generated ones is an essential task in code generation, which can be achieved by using some reliable validators (e.g., developer-written test cases) for assistance. Since reliable test cases are not always available and can be expensive to build in practice, researchers propose to automatically generate test cases to assess code solutions. However, when both code solutions and test cases are plausible and not reliable, selecting the best solution becomes challenging. Although some heuristic strategies have been proposed to tackle this problem, they lack a strong theoretical guarantee and it is still an open question whether an optimal selection strategy exists. Our work contributes in two ways. First, we show that within a Bayesian framework, the optimal selection strategy can be defined based on the posterior probability of the observed passing states between solutions and tests. The problem of identifying the best solution is then framed as an integer programming problem. Second, we propose an efficient approach for approximating this optimal (yet uncomputable) strategy, where the approximation error is bounded by the correctness of prior knowledge. We then incorporate effective prior knowledge to tailor code generation tasks. Both theoretical and empirical studies confirm that existing heuristics are limited in selecting the best solutions with plausible test cases. Our proposed approximated optimal strategy B4 significantly surpasses existing heuristics in selecting code solutions generated by large language models (LLMs) with LLM-generated tests, achieving a relative performance improvement by up to 50% over the strongest heuristic and 246% over the random selection in the most challenging scenarios. Our code is publicly available at https://github.com/ZJU-CTAG/B4.", "sections": [{"title": "1 INTRODUCTION", "content": "Code generation is an important task in the field of software engineering [23], aiming to generate code solutions that satisfy the given requirement. In practice, we often face the problem of selecting the best code solution from multiple generated alternatives [7, 22]. A common practice is using some validators (e.g., test cases) to assess the validity of each solution and choose the best one [5, 33, 36, 46]. However, in real-world scenarios, reliable test cases are not always available. Developing and maintaining reliable test cases can also be resource-intensive and laborious. With advancements in deep learning and large language models (LLMs), using auto-generated test cases has gained popularity among researchers and practitioners [20, 26, 27, 35]. Unfortunately, selecting code solutions based on these potentially unreliable tests poses significant challenges, since incorrect test cases can disrupt our decision-making. Few studies systematically explore how to assess plausible code solutions and select the best using plausible test cases. Under the assumption that the generated test cases are (mostly) correct, some existing research favors the solutions that pass the most test cases [18, 19, 22, 33]. However, this strategy is ineffective when test cases are merely plausible, indicated by our theoretical analysis (see Section 4). Other research addresses this challenge by designing clustering-based heuristic rules. For instance, Shi et al. [36] and Li et al. [22] clustered code solutions based on test outputs, and selected the solutions from the largest cluster. Chen et al. [5] similarly clustered code solutions based on the passed test cases, and selected the best cluster according to the count of solutions and passed test cases in each. However, these heuristics rely on human-designed rules and lack strong theoretical foundations, leading to potentially suboptimal performance. To the best of our knowledge, the optimal selection strategy for this problem is still an open question.\nIn this work, we aim to develop a general framework to define and compute the optimal selection strategy. We first show that under a Bayesian framework, the optimal strategy can be defined based on the posterior probability of the observed passing states between solutions and tests. The problem of identifying the optimal strategy is then framed as an integer programming problem. Under a few assumptions, this posterior probability can be further expanded into four integrals, which cannot be directly computed due to four unknown prior distributions. We then leverage Bayesian statistics techniques to deduce a computable form for approximating this posterior probability and optimize the integer programming from exponential to polynomial complexity. The approximation error is bounded by the correctness of prior knowledge. Based on this bound, we investigate two effective priors and incorporate them into our framework to enhance code generation performance. Given that the approximated optimal strategy involves scoring code solutions with four Beta functions [10], we refer to it as B4.\nBased on our developed framework, we further provide a theoretical analysis to compare B4 with existing heuristics. We observe that some heuristics require sufficient correct test cases, while others necessitate a higher probability of correct code solutions, as confirmed by subsequent simulated experiments. In real-world applications involving selecting LLM-generated code solutions with LLM-generated test cases, B4 significantly outperforms existing heuristics across five LLMs and three benchmarks.\nIn summary, our paper makes the following contributions:\n\u2022 Optimal Strategy. We systematically address the challenging problem of selecting plausible code solutions with plausible tests and establish an optimal yet uncomputable strategy.\n\u2022 Technique. We derive an efficiently computable approach to approximate the uncomputable optimal strategy with an error bound. While our framework is broadly applicable, we adapt it to code generation by incorporating two effective priors.\n\u2022 Theoretical Study. Using our framework, we explore the conditions under which existing heuristics are effective or ineffective and compare them to the approximated optimal strategy B4.\n\u2022 Empirical Study. We empirically evaluate our selection strategy with five code LLMs on three benchmarks. Experimental results show that our strategy demonstrates up to a 12% average relative improvement over the strongest heuristic and a 50% improvement in the most challenging situations where there are few correct solutions."}, {"title": "2 PRELIMINARIES", "content": "Notations. We use bold lowercase letters to denote vectors (e.g., x and y), bold uppercase letters to denote matrices (e.g., E), and thin letters to denote scalars (e.g., x and y). We also use thin uppercase letters to denote random variables (e.g., X, Y, and E). $e_i$ denotes the i-th row in matrix E. The index set [N] denotes {1, 2,..., N}. $\\{0, 1\\}^N$ denotes a length-N binary vector, and $\\{0, 1\\}^{N \\times M}$ denotes an $N \\times M$ binary matrix.\n2.1 Problem Definition\nCode generation is a crucial task in software engineering, which aims at automatically generating a code solution x from a given context c. We explore the selection of the best code solution from N code solutions generated based on c, with M test cases (also generated based on c) to aid this selection. It is worth noting that the correctness of both code solutions and test cases is plausible; they might be either correct or incorrect, which is unobserved however. All we can observe is a matrix $E = \\{e_{ij}\\}_{N \\times M} \\in \\{0,1\\}^{N \\times M}$ where $e_{ij} = 1$ indicates the i-th code solution passes the j-th test case, and 0 indicates failure. We term E as passing matrix, and $e_{ij}$ as passing state.\nLet $x = \\{x_1,\\ldots, x_N\\} \\in \\{0, 1\\}^N$ denote the ground-truth correctness of each code solution (unknown to us), in which 1 denotes correct and 0 denotes incorrect. We assume at least one code solution is correct since designing a selection strategy would be meaningless without any correct code. Similarly, the correctness of each test case is denoted by $y = \\{y_1, ..., y_m\\} \\in \\{0,1\\}^M$. We assume that all correct code solutions share identical functionality and all tests"}, {"title": "2.2 Existing Heuristics", "content": "In this section, we briefly review two representative heuristic methods for addressing this problem. The first family of methods MAXPASS [18, 19, 22, 33] always rewards passing test cases. The best code solution can be selected by counting the passed cases, i.e.,\nSelect code solution i, where $i = \\arg \\max_{i \\in [N]} \\sum_{j=1}^M e_{ij}$.\nThe other family of methods examines the consensus between code solutions and test cases, and clusters the code solutions with the same functionality [5, 22, 36]. One of the most representative methods is CODET [5]. It divides the code solutions into K disjoint subsets based on functionality: $S^* = \\{S_1, \\ldots, S_K\\}$, where each set $S_i \\hspace{0.1cm} (i \\in [K])$ consists of code solutions that pass the same set of test cases, denoted by $S'_i$. The tuple $(S_i, S'_i)$ is termed a consensus set. Taking Fig. 1 as an example, there are three consensus sets: (\\{x1, x2\\}, \\{y1, y2, y3\\}), (\\{x3\\}, \\{y2, y3, y4, y5\\}) and (\\{x4\\}, \\{y3, y4\\}). CODET proposes that a consensus set containing more code solutions and test cases indicates a higher level of consensus, and thus the more likely they are correct. Therefore, CODET scores each consensus set based on the capacity and selects the code solutions associated with the highest-scoring set, i.e.,\nSelect code solutions $i \\in S_k$, where $k = \\arg \\max_{k\\in[K]} |S_k||S'_k|$.\nSimilarly, other clustering methods, such as MBR-EXEC [36] and AlphaCode-C [22], also cluster the code solutions based on test cases, but only score each set by the number of code solutions $|S_k|$. We focus our analysis on CODET as it was verified to outperform other existing scoring strategies [5]."}, {"title": "3 METHODOLOGY", "content": "In this section, we outline our proposed methodology to address this problem.\n3.1 Optimal Strategy\nWe use $X = \\{X_1,\\ldots,X_N\\} \\in \\{0,1\\}^N$, $Y = \\{Y_1,\\ldots,Y_M\\} \\in \\{0, 1\\}^M$, and $E = \\{E_{ij}\\}_{N \\times M} \\in \\{0,1\\}^{N \\times M}$ to denote random variables of code solutions' and tests' correctness, and the passing matrix, respectively. Note that all X, Y, and E depend on the same context C, which we omit for ease of notation. A strategy's estimation for X and Y is denoted by $\\hat{x} = \\{\\hat{x_1},\\ldots, \\hat{x_N}\\}$ and $\\hat{y} = \\{\\hat{y_1},\\ldots, \\hat{y_M}\\}$. To answer RQ1, our goal is to find the most probable $\\hat{x}$ and $\\hat{y}$ given an observation E = E. This motivates us to design the optimal strategy by modeling P(X, Y | E). Based on Bayes' theorem, we have:\n$P(X, Y | E) = \\frac{P(E | X, Y)}{P(E)}P(X, Y) \\propto P(E | X, Y) P(X, Y)$.\nTherefore, we propose to use maximum a posteriori (MAP) estimator to obtain the best solution [11]:\n$\\hat{x}^*, \\hat{y}^* = \\arg \\max_{\\hat{x} \\in \\{0,1\\}^N, \\hat{y} \\in \\{0,1\\}^M} P(E = E | X = \\hat{x}, Y = \\hat{y}) P(X = \\hat{x}, Y = \\hat{y}).$\nThat is to say, we exhaustively explore all $2^N$ possible configurations of $\\hat{x}$ and $2^M$ configurations of $\\hat{y}$, computing the likelihood and prior for each pair. We then find the $\\hat{x}^*$ and $\\hat{y}^*$ that yield the highest posterior and select the correct code solutions and test cases indicated by $\\hat{x}^*$ and $\\hat{y}^*$. This optimization problem is a 0/1 integer programming problem, in which all variables are restricted to 0 or 1. The following then answers RQ1.\nAnswer to RQ1: Given a passing matrix E, the optimal selection strategy can be framed as a 0/1 integer programming problem, by finding the one $\\hat{x} \\in \\{0, 1\\}^N$ and $\\hat{y} \\in \\{0, 1\\}^M$ that maximizes the posterior probability $P(X = X, Y = \\hat{y} | E = E)$.\nBefore calculating Eq.(2), we first introduce the following two assumptions which are necessary for our subsequent computation.\nASSUMPTION 2. The code solutions X and the test cases Y are independent and randomly sampled.\nASSUMPTION 3. Each Eij is only dependent by the Xi and Yj, $\\forall i \\in [N], j\\in [M]$."}, {"title": "3.2 Practical Implementation", "content": "Recall that to compute the optimal strategy, we need to compute likelihood (Eq.(6)) and prior (Eq.(7)), which is not computable however due to complicated integrals and unknown prior distributions. In this section, we describe how to design an efficient approach to approximate the optimal strategy.\nComputing integrals. In Bayesian statistics, employing conjugate distributions for prior distributions is a standard technique to simplify integrals in posterior computation [31]. In our case, all the variables X, Y, and E follow the Bernoulli distributions, whose conjugate prior is the Beta distribution [4]. Thus, we assume the four parameters follow Beta distributions, formally,\n$P(\\theta_0) \\propto \\theta_0^{\\alpha_0-1}(1 - \\theta_0)^{\\beta_0-1}, P(\\theta_1) \\propto \\theta_1^{\\alpha_1-1} (1 - \\theta_1)^{\\beta_1-1},$\n$P(\\theta_x) \\propto \\theta_x^{\\alpha_x-1}(1 - \\theta_x)^{\\beta_x-1}, P(\\theta_y) \\propto \\theta_y^{\\alpha_y-1}(1 - \\theta_y)^{\\beta_y-1},$\nwhere $\\alpha$ and $\\beta$ are eight hyperparameters that reflect our existing belief or prior knowledge. We ignore all probability normalizing constants for ease of notation since they will not change the selection decision. These hyperparameters allow us to integrate some effective prior knowledge, which will be elaborated in Section 3.3.\nTo illustrate how Beta distributions simplify computation, we take $\\theta_x$ as an example. Combining the integral about $\\theta_x$ in Eq.(7) with $P(\\theta_x)$ in Eq.(8), we obtain:\n$\\int_0^1 \\theta_x^{n_x}(1 - \\theta_x)^{N-n_x} P(\\theta_x)d\\theta_x = \\int_0^1 \\theta_x^{n_x}(1 - \\theta_x)^{N-n_x} \\theta_x^{\\alpha_x-1}(1 - \\theta_x)^{\\beta_x-1}d\\theta_x$\n$= \\int_0^1 \\theta_x^{n_x+\\alpha_x-1} (1 - \\theta_x)^{N-n_x+\\beta_x-1}d\\theta_x = B (n_x + \\alpha_x, N - n_x + \\beta_x)$,\nwhere B (.) is known as the Beta function [10], which can be efficiently computed by modern scientific libraries like SciPy [41]. This deduction is applicable to $\\theta_1, \\theta_0$, and $\\theta_y$ as well. Combining Eq.(2), Eq.(4), Eq.(6), and Eq.(7), and applying the similar transformation to integrals yields the formula for the computable posterior:\n$P(E | x, y)P(x, y) = P_1 \\cdot P_0 \\cdot P(x, y)$\n$\\propto P_1 \\cdot [B (n_1 + \\alpha_1, |E_1| - n_1 + \\beta_1) B (n_0 + \\alpha_0, |E_0| - n_0 + \\beta_0)] \\cdot [B (n_x + \\alpha_x, N - n_x + \\beta_x) B (n_y + \\alpha_y, M - n_y +\\beta_y)]$\nThis formula implies that the posterior probability can be approximated by multiplying four Beta functions, multiplied by a term $P_1$ indicating whether x, y, and E are consistent. We next present an error bound for this approximation.\nTHEOREM 1 (APPROXIMATION ERROR BOUND). Let $\\Delta$ denote the absolute error between the true posterior (i.e., P(x, $\\hat{y}$ | E)) and the estimated posterior probability (i.e., multiplying the four Beta functions with the probability normalizing constants in Eq.(8)). Then:\n$\\Delta \\leq (c_1\\Delta_{\\theta_1} + c_0 \\Delta_{\\theta_0} + c_x\\Delta_{\\theta_x} + c_y\\Delta_{\\theta_y}) / 2P(E)$,\nwhere $\\Delta_{\\theta_1}$ is the total variance distance [38] between P($\\theta_1$) and our assumed Beta prior distribution for $\\theta_1$. $\\Delta_{\\theta_0}, \\Delta_{\\theta_x}$, and $\\Delta_{\\theta_y}$ are defined similarly. $c_1, c_0, c_x$, and $c_y$ are some positive constants less than 1.\nTheorem 1 shows that the difference of scores given by the approximated approach and the optimal strategy (i.e., the true posterior probability) is bounded by the approximation errors in the prior distributions of the four parameters. If we can accurately give the prior distributions for each parameter $\\theta$, then $\\Delta_{\\theta_1} = \\Delta_{\\theta_0} = \\Delta_{\\theta_x} = \\Delta_{\\theta_y} = 0$ and this approach can reduce to the optimal strategy. This highlights the importance of incorporating appropriate prior knowledge for different contexts.\nReducing computation complexity. Recall that the MAP strategy in Eq.(2) requires enumerating all $2^{N+M}$ combinations. Although the posterior probability is computable in Eq.(9), the enumeration cost still constrains the efficient identification of the optimal solution. Fortunately, given the role of the indicator $P_1$, only consistent combinations where $P_1 = 1$ need consideration. To be specific, for any $\\hat{x} \\in \\{0, 1\\}^N$ and $\\hat{y} \\in \\{0, 1\\}^M$ combination:"}, {"title": "3.3 Incorporating Prior Knowledge", "content": "We have derived a general explicit expression for the posterior probability in Eq.(9), which includes eight hyperparameters corresponding to the Beta distribution for four $\\theta$. According to Theorem 1, we should incorporate proper prior knowledge to effectively approximate the optimal strategy. In this section, we investigate how to achieve this in the context of code generation.\nPriors for $\\theta_0$ and $\\theta_1$. In practical scenarios, a test suite, not to mention a test case, is often incomplete. Therefore, a correct test case can fail to identify an incorrect solution, causing incorrect solutions to have a moderate probability of passing correct test cases (i.e., $\\theta_1$). Conversely, to pass incorrect test cases that validate flawed functionalities, incorrect solutions must \"accidentally\" match this specific flaw to pass, making such occurrences ($\\theta_0$) relatively rare. This suggests that in practice, $\\theta_0$ may be very small, but $\\theta_1$ may not have a clear pattern.\nBased on this finding, we propose adopting a prior distribution approaching zero for $\\theta_0$ and a uniform prior distribution for $\\theta_1$. Therefore, we choose a beta prior distribution parameterized by $(\\alpha_0 = 1, \\beta_0 \\gg 1)$ for $\\theta_0$, and choose $(\\alpha_1 = \\beta_1 = 1)$ for $\\theta_1$.  In practice, $\\beta_0$ serves as a tunable hyperparameter.\nPriors for $\\theta_x$ and $\\theta_y$. As discussed previously, each consistent (x, y) corresponds to a consensus set. Chen et al. [5] identified a heuristic rule that the consensus set with the largest capacity (i.e.,"}, {"title": "3.4 Further Analysis of Algorithm B4", "content": "Given that the score in Eq.(10) is multiplied by four Beta functions, we name this practical strategy B4. In this section, we provide a detailed analysis of the proposed B4 to deepen the understanding.\nUnderstanding Beta functions. To further explore the role of two hyperparameters used in the B4 and our scoring strategy, we visualize two Beta functions related to two hyperparameters $\\beta_0$ and $\\alpha_{xy}$."}, {"title": "4 THEORETICAL ANALYSIS", "content": "In this section, we address RQ4 by a theoretical accuracy analysis of the two representative heuristics, MAXPASS and CODET, to investigate under what conditions they can and cannot work. MAXPASS is a widely-used heuristic  and CODET is the state-of-the-art heuristic for code generation. Furthermore, these theoretical analyses further explain why the priors for P(x, y) introduced in Section 3.3 are chosen. We assume that Assumptions 1-3 are satisfied, and the data follows the generation process in Fig. 2.\nWe begin with a theorem which assesses MAXPASS's accuracy when there is a large number of test cases:\nLEMMA 4.1. Suppose there exist $n_y$ correct test cases and $\\tilde{n}_y$ incorrect test cases ($n_y + \\tilde{n}_y = M$). When both $n_y$ and $\\tilde{n}_y$ are large enough, the probability of any incorrect code passing Y (Y \u2265 ny) test cases is:\n$P(Y \\geq n_y) \\sim \\Phi (\\frac{n_y\\theta_0 - \\tilde{n}_y (1 - \\theta_1)}{\\sqrt{n_y\\theta_1(1 - \\theta_1) + \\tilde{n}_y\\theta_0(1 - \\theta_0)}})$,\nwhere $\\Phi$ is the cumulative distribution function (CDF) of the standard normal distribution. $\\theta_0$ and $\\theta_1$ are defined in Eq.(3).\nTHEOREM 2 (IMPACT OF CORRECT TEST CASES FOR MAXPASS). If $\\theta_1 < 1$, the accuracy of MaxPass (i.e., the probability of all incorrect solutions passing less than $n_y$ test cases) can exponentially converge to 1 as $n_y\\rightarrow\\infty$.\nTHEOREM 3 (IMPACT OF INCORRECT SOLUTIONS FOR MAXPASS). If there are $\\tilde{n}_x$ incorrect solutions, the accuracy of MaxPass can exponentially converge to 0 as $\\tilde{n}_x \\rightarrow\\infty$.\nTheorem 2 demonstrates the working condition for MAXPASS: it requires a large amount of correct test cases ny to make the accuracy converge to 1. However, Theorem 3 also underscores a limitation of MAXPass: it lacks scalability to the number of code solutions N. As N increases, $\\tilde{n}_x$ increases and the accuracy of MAXPAss will exponentially converge to zero.\nFollowing this, we analyze the error of CODET. Considering the problem's complexity, we fix the M test cases and explore how the error evolves as the number of generated code solutions N grows, as shown in the following theorem.\nLEMMA 4.2. Suppose the correctness of code solutions and test cases are x and y. Let $n_x = \\sum x$ and $n_y = \\sum y$ denote the number of correct code solutions and test cases, respectively. For any incorrect consensus set that corresponds to a prediction $\\hat{x}$ and $\\hat{y}$, similarly let $\\hat{n_x} = \\sum \\hat{x}$ and $\\hat{n_y} = \\sum \\hat{y}$. For arbitrary y and $\\hat{y}$, if N is sufficiently large, the"}, {"title": "5 EXPERIMENT", "content": "In this section, we conduct experiments to further answer RQ4 and RQ5. We start with exploring the conditions under which existing heuristics can work efficiently through simulation experiments in different controlled environments, to validate the theoretical insights discussed in Section 4. Subsequently, we compare the performance of B4 with existing heuristics on real-world datasets.\n5.2 Real-world Experiments\n5.2.1 Experiment setup. We conducted experiments on three public code generation benchmarks, HumanEval [7], MBPP [2] (sanitized version), and APPS [16] with three difficulty levels. These benchmarks have been widely used by LLM-based code generation studies"}, {"title": "6 DISCUSSION", "content": "In this section, we discuss the limitations and threats to the validity of this study.\n6.1 Limitations\nAssumption 2 and 3. These assumptions are related to independence. Assumption 2 considers the correctness of code solutions and test cases are independent, which can be violated if there is a causal relationship in their generation, such as using a generated test case as input to an LLM for further generation. Assumption 3 states that passing probability is solely determined by the correctness of the associated code and test case. However, the independence of passing states may be broken by other unobserved factors hidden in the code. For example, if two incorrect solutions exhibit similar structures and similar error types, their passing states might be positively correlated. Considering the significant complexity introduced by the lack of independence, further exploration of the dependence case is deferred to future research.\nPrior for $\\theta_0$. This prior assumes that $\\theta_0$ (i.e., the probability of incorrect solutions passing incorrect test cases) is typically low. However, when LLMs misinterpret a problem, incorrect test cases may coincidently specify the functionality of incorrect solutions and potentially increase $\\theta_0$. Considering that this prior can bring considerable benefits  we argue that its advantages significantly outweigh the limitations.\nPriors for $\\theta_x$ and $\\theta_y$. These priors, similar to the heuristic rule of CODET, suggest that larger consensus sets are more likely to be correct. We have validated its theoretical effectiveness under the conditions of large N and high $\\theta_x$, as detailed in Section 4. Even though its efficacy may diminish when these conditions are not met, the prior for $\\theta_0$ effectively compensates for this situation.\nHyperparameters. Our method includes two hyperparameters, axy and $\\beta_0$, which may pose challenges in tuning across different usage scenarios. Fortunately, we have found that using consistent hyperparameters across all benchmarks can still yield significant improvements in our experimental scenarios. The tuning of hyperparameters for specific applications, potentially using a validation set to optimize them, remains an area for future research.\nTheoretical results. To derive a closed form of the probabilities, we used the Law of Large Numbers to examine the scenarios where N and M are sufficiently large. Besides, in Lemma 4.2, we focus on a"}, {"title": "7 RELATED WORK", "content": "Reranking and selection for plausible solutions. Using external validators (e.g., test cases) to assess, rerank, or select the generated solutions is widely used in various software engineering tasks. In code generation, Lahiri et al. [18] incorporated user feedback to choose test cases for code selection. In APR, Yang et al. [46] used test cases generated by fuzz testing to validate automatically generated patches. In code translation, Roziere et al. [33] leveraged EvoSuite [12] to automatically generate test cases for filtering out invalid translations. These methods are developed by assuming that the validators are reliable and can be reduced to the MAXPAss strategy in our work. However, it may be ineffective when the validators are plausible, as evidenced in Section 4. In code generation, several cluster-based strategies are proposed to leverage incomplete or plausible test cases to rerank LLM-generated code solutions [5, 22, 36]. Li et al. [22], Shi et al. [36] and Chen et al. [5] clustered code solutions based on their test results and scored each with the cluster capacity. These cluster-based heuristics, particularly CODET [5], can work well when the test cases are plausible but are susceptible to the incorrectness of solutions.\nSome research uses deep learning techniques for ranking LLM-generated code snippets without executable test cases. Inala et al. [17] introduced a neural ranker for predicting the validity of a sampled program. Chen et al. [7] and Zhang et al. [49] leveraged the LLM likelihood of the generated program for selecting the most probable code snippets. These strategies fall beyond the scope"}, {"title": "8 CONCLUSION AND FUTURE WORK", "content": "In this study, we introduce a systematic framework to derive an optimal strategy for assessing and selecting plausible code solutions using plausible test cases. We then develop a novel approach that approximates this optimal strategy with an error bound and tailors it for code generation tasks. By theoretical analysis, we show that existing heuristics are suboptimal. Our strategy substantially outperforms existing heuristics in several real-world benchmarks.\nFuture work could explore adapting our framework to other generation tasks in software engineering, such as automatic program repair and code translation. Also, the effectiveness of our proposed priors in these contexts, as well as the potential for alternative priors, remains an open question."}]}