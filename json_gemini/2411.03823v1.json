{"title": "Both Text and Images Leaked! A SYSTEMATIC ANALYSIS OF MULTIMODAL LLM DATA CONTAMINATION", "authors": ["Dingjie Song", "Sicheng Lai", "Shunian Chen", "Lichao Sun", "Benyou Wang"], "abstract": "The rapid progression of multimodal large language models (MLLMs) has demonstrated superior performance on various multimodal benchmarks. However, the issue of data contamination during training creates challenges in performance evaluation and comparison. While numerous methods exist for detecting dataset contamination in large language models (LLMs), they are less effective for MLLMs due to their various modalities and multiple training phases. In this study, we introduce a multimodal data contamination detection framework, MM-Detect, designed for MLLMs. Our experimental results indicate that MM-Detect is sensitive to varying degrees of contamination and can highlight significant performance improvements due to leakage of the training set of multimodal benchmarks. Furthermore, We also explore the possibility of contamination originating from the pre-training phase of LLMs used by MLLMs and the fine-tuning phase of MLLMs, offering new insights into the stages at which contamination may be introduced.", "sections": [{"title": "INTRODUCTION", "content": "The development of MLLMs has exceeded expectations (Liu et al., 2023a; Lin et al., 2023), showcasing extraordinary performance on various multimodal benchmarks (Lu et al., 2022; Liu et al., 2023b; Song et al., 2024), even surpassing human performance. However, due to the partial obscurity associated with MLLMs training (OpenAI, 2023; Reid et al., 2024), it remains challenging to definitively ascertain the impact of training data on model performance, despite some works showing the employment of the training set of certain datasets (Liu et al., 2023a; Chen et al., 2023; Bai et al., 2023b). The issue of data contamination, occurring when training or test data of benchmarks is exposed during the model training phase (Xu et al., 2024), could potentially instigate inequitable performance comparisons among models. This not only creates a dilemma for users in model selection but also poses a significant hurdle to further advancements in this domain.\nWhile numerous works in the field of LLMs have proposed methods for detecting data contamination (Yeom et al., 2018; Deng et al., 2024; Dong et al., 2024), MLLMs, due to their various modalities and multiple training phases (Liu et al., 2023a; Li et al., 2023), face limitations when applying these methods. Therefore, there is a pressing need for a more suitable multimodal contamination detection framework specifically tailored for MLLMs.\nIn this study, we carry out a systematic analysis of multimodal data contamination. Initially, we define Multimodal Data Contamination, as it pertains to the modality of data sources exposed to the MLLMs, into two categories: Unimodal Contamination and Cross-modal Contamination. Subsequently, we unveil a detection framework for multimodal data contamination, MM-Detect, which incorporates two methods, Option Order Sensitivity Test and Slot Guessing for Perturbation Caption, designed to handle two common types of Visual Question Answering (VQA) tasks: multiple-choice and caption-based questions, respectively. Then, applying MM-Detect on eleven widely-used MLLMs across five prevalent VQA datasets, we observe that both open-source and proprietary MLLMs do exhibit contamination, with the degree of contamination varying across different models."}, {"title": "PRELIMINARIES", "content": "We formally define the problem of contamination in multimodal benchmark datasets and describe the unique challenges in multimodal contamination detection."}, {"title": "DEFINITION OF MULTIMODAL DATA CONTAMINATION", "content": "In contrast to single-modal contamination, multimodal contamination may arise from both unimodal and multimodal data sources, as depicted in Figure 1. The training data for MLLMs generally consists of pure text pre-training data $D_{pretrain}$ and multimodal alignment or instruction-following data $D_{vision}$.\nConsider an instance $(x, i, y)$ from a benchmark dataset $D$, where $x$ represents the text input, $i$ is the image input, and $y$ is the label. Data contamination in multimodal models can be categorized into the following two cases:\n\u2022 Unimodal Contamination: The pair $(x, y)$ or the input $x$ appears in $D_{pretrain}$.\n\u2022 Cross-modal Contamination: The triplet $(x, i, y)$ appears in $D_{vision}$.\nIn both cases, models trained on these data may gain an unfair advantage."}, {"title": "CHALLENGES IN MULTIMODAL DETECTION", "content": "The challenges of multimodal contamination detection mainly arise from two aspects."}, {"title": "Inefficiency of Unimodal Methods", "content": "Despite the prevalence of unimodal detection methods, their application in multimodal scenarios often encounters difficulties. For example, retrieval-based methods Brown et al. (2020); Touvron et al. (2023a) attempt to detect contamination by retrieving large-scale corpora used for model training. Yet, they struggle when retrieving multimodal information. Similarly, logits-based methods (Shi et al., 2024; Yeom et al., 2018) rely on observing the distribution of low-probability tokens in model outputs, but the disparity in token probability distributions is less pronounced in instruction-tuned models. Masking-based methods Deng et al. (2024), which assess training contamination by evaluating a model's ability to predict specific missing or masked text, face challenges when images in multimodal samples provide clues, leading to overestimated contamination detection. Finally, comparison-based methods Dong et al. (2024) that measure contamination by comparing model outputs with benchmark data prove to be ineffective for image caption tasks due to low output similarity. To validate these inefficiencies, we conducted experiments with compelling results, which are detailed in Appendix A."}, {"title": "Multi-stage Training in MLLMs", "content": "Another challenge in detecting contamination in multimodal models is the multi-stage nature of their training (Yin et al., 2023). Each stage may be subject to data contamination. Initially, the pretraining corpus could contain the textual components of questions from benchmark test samples. Moreover, in certain native multimodal model training (Reid et al., 2024), test samples may be entirely exposed. Subsequently, during multimodal fine-tuning, the model may utilize training samples of some benchmarks, leading to skewed performance improvements. Furthermore, some models employ extensive mixed image-text data from the internet for modality alignment training (Lin et al., 2023; Bai et al., 2023b), potentially introducing additional contamination. Given these challenges, the development of an effective detection framework for multimodal contamination becomes an urgent need.\nBased on the discussion above, we have designed a detection method specifically tailored for multimodal contamination, with a particular focus on VQA tasks. Additionally, we have developed a heuristic method to trace the introduction of contamination across different training phases."}, {"title": "MM-DETECT", "content": "In this section, we introduce the multimodal contamination detection framework, MM-Detect. The core philosophy of MM-Detect is to detect the unusual discrepancies in model performance before and after semantic-irrelevant perturbations. As depicted in Figure 1, this framework operates in two primary steps:\n\u2022 The first step is to generate perturbed datasets using two innovative methods: the Option Order Sensitivity Test (Section 3.1) and the Slot Guessing for Perturbation Captions (Section 3.2), tailored for evaluating multiple-choice and image captioning tasks, respectively.\n\u2022 The second step involves the application of predefined metrics to detect contamination (Section 3.3), conducting thorough analyses at both the dataset and instance levels."}, {"title": "OPTION ORDER SENSITIVITY TEST", "content": "This method is based on a reasonable and intuitive premise that if the model's performance is highly sensitive to the order of the options, as shown in Figure 2, it indicates potential contamination, leading the model to memorize a certain canonical order of the options.\nMethod Formulation. Let D represent a dataset comprising n datapoints. For each datapoint $d_i$, where $i \\in \\{1, ..., n\\}$, there is a question symbolized by $Q_i$, an image represented by $I_i$, and a list of choices is denoted by $A_i$, such that $A_i = \\{a^1_i, a^2_i, ..., a^m_i\\}$ and m is the number of choices for that datapoint. The correct answer is symbolized as $a^*_i$, where $a^*_i \\in A_i$. The list A is randomly shuffled to generate A', ensuring that the index of the correct answer $a^*_i$ in A' differs from its index in A, thereby altering the correct answer's position. The final prompts, both before and after the shuffling, are the concatenation of the image, question, and choices:\n$P = Concat(I, Q, A), P' = Concat(I, Q, A'),$\nwhere P and P' are the prompts fed into the model, and Q and I remain constant."}, {"title": "SLOT GUESSING FOR PERTURBATION CAPTION", "content": "This method is based on the intuition that if a model can predict a missing part of a sentence but fails with the back-translated version (from English to Chinese, then back to English), it likely indicates that the model has encountered the original sentence during training.\nAs shown in Figure 3, the keywords identified are \"woods\" and \"bike\". Since the image contains \"woods\u201d, a correct guess by the model may stem from its multimodal capabilities rather than data contamination. However, if the model fails to predict \"bike\", which is also present in the image, this may indicate potential leakage of this instance.\nMethod Formulation. Let D be a dataset containing n datapoints. For each datapoint $d_i, i \\in \\{1,...,n\\}$, there is a corresponding caption $S_i$ describing the image features. We first apply a back-translation function\u00b9 to $S_i$:\n$S' = f_{back-translate} (S)$\nto obtain the back-translated sentence $S'$. Next, we perform keyword extraction\u00b2 on both S and $S'$:\n$K = f_{keyword} (S), K' = f_{keyword} (S'),$\nwhere K and K' are the keywords extracted from S and $S'$, respectively. We then use a masking function $f_{mask}$ to replace the keywords in the sentences with [MASK]:\n$S_{mask} = f_{mask} (S, K), S'_{mask} = f_{mask} (S', K').$\nThe final prompt can be represented as the concatenation of the image, the instruction and the masked sentence:\n$P = Concat(I, Q, S_{mask}), P' = Concat(I, Q, S'_{mask}),$\nwhere I is the image and Q is the instruction guiding the model to complete the mask word prediction task."}, {"title": "DETECTION METRICS", "content": "Having introduced two detection methods, we now delineate the metrics for the detection pipeline, which consists of two primary steps.\nStep 1: Benchmark Atomic Metrics Calculation. This step assesses the model's performance on benchmark D before and after perturbation. We denote the correct rate (CR) and perturbed correct rate (PCR) uniformly for both Option Order Sensitivity Test (using Accuracy) and Back-Translation Keyword Guessing (using Exact Match). Here, N and N' are the counts of correct answers before and after perturbation, respectively. They are calculated as:\n$CR = \\frac{N}{|D|}$ \n$PCR = \\frac{N'}{|D|}$"}, {"title": "Contamination Degree Analysis", "content": "This step quantifies the model's contamination degree based on the performance variation pre- and post-perturbation. Specifically, we introduce two metrics to evaluate contamination at both dataset and instance levels.\nDataset Level. We evaluate the reduction in atomic metrics, denoted as $\\Delta$:\n$\\Delta = PCR - CR$\nThis reduction indicates the model's familiarity or memory of the original benchmark relative to the perturbed set, thereby offering insights into potential contamination at the dataset level. A significant negative $\\Delta$ suggests potential extensive leakage in the benchmark dataset, leading to highly perturbation-sensitive model performance.\nInstance Level. Despite a non-significant or positive $\\Delta$, contamination may still occur at the instance level, as some instances may still have been unintentionally included during training. To identify such instances, we compute X, the count of cases where the model provided correct answers before perturbation but incorrect answers after. The instance leakage metric $I_L$ is then obtained by dividing X by the dataset size:\n$I_L = \\frac{X}{|D|},$\nwhere a larger $I_L$ indicates a higher likelihood of instance leakage. For further details on our atomic metrics' computation, please refer to Appendix B."}, {"title": "EXPERIMENT", "content": "In this section, we demonstrate the practicality of our methodology in verifying contamination of multimodal benchmark datasets across several MLLMs."}, {"title": "SETUP", "content": "Models. We conducted extensive evaluations on eight open-source MLLMs, including LLaVA-1.5-7B Liu et al. (2023a), VILA1.5-3B Lin et al. (2023), Qwen-VL-Chat Bai et al. (2023b), fuyu-8b3, idefics2-8b Lauren\u00e7on et al. (2024), Phi-3-vision-128k-instruct Abdin et al. (2024), Yi-VL-6B AI et al. (2024), InternVL2-8B Chen et al. (2023; 2024b), as well as three proprietary MLLMs: GPT-40 OpenAI (2023), Gemini-1.5-Pro Reid et al. (2024), and Claude-3.5-Sonnet\u2074.\nBenchmark Datasets. Our analysis leverages two multi-choice datasets: ScienceQA Lu et al. (2022) and MMStar Chen et al. (2024a), along with three caption datasets: COCO-Caption2017 Lin et al. (2015), NoCaps Agrawal et al. (2019), and Vintage5. MMStar and Vintage, owing to their recent inception, serve to contrast contamination levels with other datasets. We randomly selected 2000 and 1340 samples from ScienceQA's training and test sets, respectively, with 1000 samples from the other datasets. Given the unavailability of public test labels for COCO-Caption2017 and NoCaps, we used their validation sets."}, {"title": "MAIN RESULTS", "content": "Multi-choice Datasets. As shown in Table 1, for the training set of ScienceQA, Claude-3.5-Sonnet exhibited a significant $\\Delta$ of -5.3, indicating extensive potential contamination. VILA1.5-3B's high $I_L$ suggests that the model may have encountered some benchmark instances during training. For the test set of ScienceQA, all models except Claude-3.5-Sonnet showed insignificant $\\Delta$, indicating a low likelihood of large-scale leakage from the benchmark. However, the significant $I_L$ from fuyu-8b may indicate that the model has seen some benchmark instances or similar training data.\nIt is noteworthy that even the latest benchmarks may still be contaminated. For MMStar, fuyu-8b showed a significant $\\Delta$, and Claude-3.5-Sonnet exhibited a high $I_L$. Since the data sources for MMStar are older datasets, which have been filtered for image-text relevance, the experimental results align with expectations.\nCaption Datasets. From Table 2, we observe that for the validation sets of COCO-Caption2017 and NoCaps, more than half of the models exhibited significant performance drops due to perturbations, indicating that these two benchmarks may face serious leakage issues. For the training set of Vintage, idefics2-8b showed a significant $\\Delta$, and Claude-3.5-Sonnet had the highest IL."}, {"title": "INTENTIONAL CONTAMINATION", "content": "This section utilizes intentional contamination to address three research questions:\nRQ1: Can MM-Detect effectively identify contamination?\nRQ2: What is MM-Detect's sensitivity level?\nRQ3: Does training set leakage result in evaluation bias?\nTo tackle these questions, we followed the recipe of LLaVA and train multiple 7B parameter models on intentionally contaminated training data from downstream task examples during the visual instruction tuning phase, subsequently evaluating their contamination degree."}, {"title": "MM-DETECT IS AN EFFECTIVE DETECTOR", "content": "We reproduced the LLaVA-1.5-7B model experiment to obtain a control model, LLaVA-1.5-7B-no-cont, which is devoid of intentional data contamination. Subsequently, we randomly incorporated 2000 training samples from ScienceQA and 1000 validation samples from NoCaps into $D_{tuning}$, where $D_{tuning}$ denotes the data used by LLaVA for visual instruction tuning. This data was used to train a contaminated model, LLaVA-1.5-7B-cont, for comparison with LLaVA-1.5-7B-no-cont."}, {"title": "MM-DETECT IS SENSITIVE TO CONTAMINATION DEGREES", "content": "To investigate the sensitivity of MM-Detect, We selected 1340 examples from the ScienceQA test set to train three models: a fully contaminated model (LLaVA-1.5-7B-cont-100%) incorporating all 1340 examples into $D_{tuning}$, a partially contaminated model (LLaVA-1.5-7B-cont-50%) with 670 examples mixed in, and a minimally contaminated model (LLaVA-1.5-7B-cont-10%) with 134 examples mixed in. This approach demonstrates our method's robustness in reflecting various degrees of model contamination.\nFigure 4 reveals our testing's high sensitivity. As contamination degrees rise from 10% to 50%, there's a significant increase of 8.7% in CR, 7.3% in PCR, and a decrease of 1.4% in $\\Delta$. When contamination escalates from 50% to 100%, CR again significantly rises by 5.3%, PCR increases by 3.6%, and $\\Delta$ decreases by 1.7%. These findings suggest that our tests can address the binary classification challenge of detecting a model's contamination, and more importantly, they can reflect varying degrees of contamination."}, {"title": "TRAINING SET LEAKAGE LEADS TO UNFAIRNESS", "content": "We investigates whether training set leakage leads to evaluation bias. As $D_{tuning}$ already includes the training set of COCO2017, we remove that portion from $D_{tuning}$ to obtain LLaVA-1.5-7B-no-coco, thereby simulating a scenario where the model hasn't encountered the training set."}, {"title": "AT WHICH STAGE IS CONTAMINATION INTRODUCED?", "content": "In this section, we will explore the source of contamination. The training data for most of the MLLMs we examined is openly stated, which prompts the question: If the contamination did not originate from the multimodal training phase, could it have been introduced during the unimodal training phase? In this context, unimodal contamination refers to instances where the text inputs or labels from the benchmark data have already been exposed during the pre-training phase of the LLMs utilized by these models (Section 2.1). To explore this possibility, we examined the LLMs used by the previously tested MLLMs and performed a series of experiments."}, {"title": "A HEURISTIC EXPERIMENT FOR UNIMODAL CONTAMINATION DETECTION", "content": "We employed a heuristic approach based on the intuition that if the LLM can correctly answer an image-required question without the image, it may indicate the leakage of that instance.\nExperiment Setup. The benchmark for this experiment was MMStar, which inherently consists of image-text related questions. The models tested include LLaMA2-7b Touvron et al. (2023b) used by LLaVA-1.5 and VILA, Qwen-7B Bai et al. (2023a) used by Qwen-VL, Mistral-7B-v0.1 Jiang et al. (2023) used by idefics2, Phi-3-small-128k-instruct Abdin et al. (2024) used by Phi-3-vision, Yi-6B AI et al. (2024) used by Yi-VL, and Internlm2-7B Cai et al. (2024) used by InternVL2. To reduce the possibility of the model making random guesses, we appended the prompt, If you do not know the answer, output I don't know, to the instructions. The metric we reported was the ContRate, representing the frequency these models correctly answered questions without the necessity of image input. It is worth noting that we did not conduct experiments on Fuyu-8b and proprietary models since they did not disclose the unimodal language model and training data used."}, {"title": "A REVIEW OF VISUAL INSTRUCTION TUNING DATA FOR CROSS-MODAL CONTAMINATION DETECTION", "content": "To investigate the origins of cross-modal contamination, we scrutinize the visual instruction tuning data of MLLMs. We delve into the construction process of three benchmark datasets: ScienceQA, COCO Caption, and Nocaps, comparing them with the training data and its sources of various open-source MLLMs to analyze the degree of overlap.\nAs Table 7 illustrates, MLLMs marked in red and yellow typically exhibit a significant contamination degree. Yet, even MLLMs labeled in green aren't exempt from the risk of cross-modal contamination. This is because some models have been trained on large-scale interleaved image-text datasets (e.g., OBELICS (Laurenon et al., 2023)), datasets derived from online sources (e.g., Conceptual Caption (Sharma et al., 2018)), or in-house data. Furthermore, some models haven't fully disclosed their training data, which may lead to overlooked potential leaks in benchmark datasets."}, {"title": "CONCLUSION AND FUTURE WORK", "content": "In this study, we introduce and validate a multimodal data contamination detection framework, MM-Detect, providing new perspectives for evaluating contamination in MLLMs. We discovered that popular multimodal models exhibit varying degrees of data contamination, which directly impacts their performance and generalization ability. In addition, our experimental indicates that MM-Detect is sensitive to varying degrees of contamination and can highlight significant performance improvements due to leakage in the multimodal benchmark training set. Furthermore, we found that the contamination in multimodal models may not solely originate from the cross-modal contamination but could also stem from the unimodal contamination.\nFuture work will focus on two key areas:\n\u2022 Firstly, standardizing the use of multimodal datasets and reporting potential contamination impacts to minimize contamination, thereby enhancing data consistency and quality.\n\u2022 Secondly, creating a continuously updated benchmarking system for the ongoing evaluation of multimodal model performance.\nThis will support advancements and broader applications in this field."}, {"title": "LIMITATIONS", "content": "We acknowledge several limitations in detecting test set contamination. First, this work is limited to discussions around visual modalities, and does not yet cover other modalities such as audio or video. Second, we only selected widely used and representative multimodal datasets for detection, including multiple-choice datasets and caption datasets, without testing additional datasets. However, we speculate that the method Slot Guessing for Perturbation Caption may also apply to other types of Image Feature Analysis benchmarks."}, {"title": "INEFFICIENCY OF UNIMODAL METHODS", "content": "We demonstrate the results of traditional unimodal contamination detection methods applied to MLLMs."}, {"title": "LOGITS-BASE", "content": "These methods determine contamination by observing the distribution of low-probability tokens in model outputs. However, MLLMs typically undergo instruction fine-tuning, which enhances their instruction-following capabilities, leading to less significant differences in token probability distributions. As shown in Table 8, LLaVA-1.5-13b exhibits extremely low perplexity on multimodal benchmark datasets."}, {"title": "MASKING-BASE", "content": "These methods involve masking phrases or sentences and providing data from the benchmark to guide the model in filling in the missing parts. However, multimodal datasets often contain images that include the masked portions of sentences, effectively providing answers to the model. This results in significantly higher success rates for multimodal models in predicting missing parts compared to unimodal language models, leading to exaggerated contamination detection. As shown in Table 9, LLaVA-1.5-13b has a high probability of Exact Match for predicting the masked word."}, {"title": "COMPARISON-BASE", "content": "These methods identify contamination by comparing the similarity between model outputs and benchmark data. However, MLLMs often undergo data augmentation, causing their outputs to diverge significantly from the labels in benchmark data, making effective contamination detection challenging. From Table 10, we can see that CDD indicates a contamination level of 0% across all multimodal benchmark datasets."}, {"title": "DETAILS OF CALCULATING ATOMIC METRICS", "content": "We formulate the detection pipeline algorithm utilized in MM-Detect, as shown in Algorithm 1."}]}