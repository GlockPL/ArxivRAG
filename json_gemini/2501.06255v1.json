{"title": "Progressive Supervision via Label Decomposition: An Long-Term and Large-Scale Wireless Traffic Forecasting Method", "authors": ["Daojun Liang", "Haixia Zhang", "Dongfeng Yuan"], "abstract": "Long-term and Large-scale Wireless Traffic Forecasting (LL-WTF) is pivotal for strategic network management and comprehensive planning on a macro scale. However, LL-WTF poses greater challenges than short-term ones due to the pronounced non-stationarity of extended wireless traffic and the vast number of nodes distributed at the city scale. To cope with this, we propose a Progressive Supervision method based on Label Decomposition (PSLD). Specifically, we first introduce a Random Subgraph Sampling (RSS) strategy designed to sample a tractable subset from large-scale traffic data, thereby enabling efficient network training. Then, PSLD employs label decomposition to obtain multiple easy-to-learn components, which are learned progressively at shallow layers and combined at deep layers to effectively cope with the non-stationary problem raised by LL-WTF tasks. Finally, we compare the proposed method with various state-of-the-art (SOTA) methods on three large-scale WT datasets. Extensive experimental results show the proposed PSLD outperforms existing methods in terms of performance and runtime, with an average 2%, 4%, and 11% performance improvement on three WT datasets, respectively. In addition, we built an open source library for WT forecasting (WTFlib) to facilitate related research, which contains numerous SOTA methods and provides a strong benchmark.", "sections": [{"title": "I. INTRODUCTION", "content": "Wireless networks have become an essential part of our daily lives, and the demand of wireless services is growing rapidly. For example, global mobile devices grow from 8.8 billion in 2018 to 13.1 billion in 2023, of which 1.4 billion is 5G capable [1]. The exponential growth of Wireless Traffic (WT) poses significant challenges, including the serious energy consumption of base stations [2] and the inability to meet differentiated Quality of Service (QoS) requirements [3]. WT forecasting can guide base stations to achieve power control [4] and flexible coverage [5], [6], which is an important means to achieve green and intelligent networks [7].\nTraditional WT forecasting methods such as ARIMA [8]-[10], entropy theory [11], covariance function [12] and $\\alpha$-stable distribution [13] are developed specifically for short-term, low-dimensional time series. These methods are based on the stationarity assumption or statistical properties of time series to predict the future values, which are no longer suitable for high-dimensional and non-stationary time series existing in WT forecasting task [14]. Recently, deep learning methods [15]\u2013[19] are developed for WT forecasting due to its powerful nonlinear fitting capabilities [20]. These methods either utilize Convolutional Neural Networks (CNNs) [21] and Graph Neural Networks (GNNs) [22] to process short-term grid data [15]\u2013[17] or Recurrent Neural Networks (RNNs) [23] to perform univariate prediction [24]\u2013[27].\nHowever, there are two major problems that need to be addressed: 1) Short-term WT forecasting (e.g., one-step-ahead prediction) cannot be used for long-term planning and lead to frequent decision changes, which in turn cause greater control overhead for the base station. Merely elongating the output span of current methodologies fails to address the inherent non-stationarity issue within long-term WT forecasting, thereby culminating in suboptimal predictive accuracy. 2) Wireless nodes are densely distributed and present a graph-like structure, making it impossible to efficiently perform long-term forecasting tasks. For example, there are more than 20,000 base stations in a medium-sized city, and each of them requires long-term time series forecasting, which will result in a single GPU being unable to handle such large-scale data.\nTo address the above problems, fine-grained Long-term and Large-scale Wireless Traffic Forecasting (LL-WTF) methods at the node level urgently need to be developed. In this paper, we delve into LL-WTF tasks, aiming to handle vast quantities of irregular erratic data and tackle the non-stability challenge inherent in LL-WTF tasks. For large-scale WT datasets, we developed a Random Subgraph Sampling (RSS) algorithm, which, according to our findings, effectively addresses the training challenges of deep neural networks on large-scale irregular graph data. The proposed RSS algorithm enables the disassembly of large-scale WT data into multiple small-scale subseries through random subgraphs sampling, allowing each to be processed on existing GPU devices.\nFor long-term WT forecasting, we utilize decomposition to deconstruct a time series into several more predictable components and leverage deep models to learn the time-varying patterns of each component separately. Instead of treating decomposition as a preprocessing step [28]\u2013[30] or embedding it in a learning module [31]\u2013[33], we use decomposition for supervised signals. Concretely, we propose a Progressive Supervision method based on Label Decomposition (PSLD) to progressively learn the decomposed supervision signal to better cope with the non-stationary problem. PSLD utilizes label decomposition to obtain multiple easy-to-learn components, which are learned progressively at shallow layers and combined at deep layers. Through the decomposition of label signals, PSLD can more easily learn various statistical properties and aperiodic trends of non-stationary time series. It is worth noting that PSLD differs from the concept of PNNS [34], which focus on incremental learning through lateral connections to previously learned networks, primarily aimed at addressing the problem of catastrophic forgetting. In contrast, our approach does not involve adding new network modules or preserving previous networks; instead, it emphasizes evolving the supervision signal itself as training progresses.\nIn summary, various decompositions and learners can be applied to PSLD, which can be regarded as a new paradigm that makes full use of decomposition to mine the time-varying properties of supervision signals and combines the powerful fitting capabilities of deep learning. We implemented two versions of PSLD on MLP using classical statistical (Mean and Variance Decomposition, MVD) and STL [35]. Extensive experimental results show PSLD outperforms existing methods by a large margin, average 2%, 4%, and 11% performance improvement on three LWTF datasets, respectively. As illustrated in Fig. 1, PSLD not only achieves state-of-the-art (SOTA) performance in terms of predictive accuracy but also maintains a leading position in processing speed. In addition, we build an open source library for wireless traffic forecasting (WTFlib) to facilitate related research, which contains numerous SOTA methods and provides a strong benchmark.\nThe main contributions are summarized as follows.\n\u2022 The long-term and large-scale wireless traffic forecasting (LL-WTF) task is introduced, and an open source library for WT forecasting (WTFlib) is built to facilitate related research.\n\u2022 A Random Subgraph Sampling (RSS) algorithm is developed to disassemble large-scale WT data into multiple small-scale subseries, so that each of them can be processed on existing GPUs.\n\u2022 A Label-Decomposition-based Progressive Supervision method (PSLD) is proposed, which utilizes label decomposition to obtain multiple easy-to-learn components. These components are learned progressively at shallow layers and combined at deep layers to address non-stationarity problem in LL-WTF.\n\u2022 Extensive experiments over three large-scale WT datasets are conducted to verify the performance of the proposed methods. It is shown that PSLD improve the average performance of state-of-the-art (SOTA) methods by around 2%, 4%, and 11%, respectively."}, {"title": "Definition", "content": "The purpose of LL-WTF is to use the observed value of $L_{in}$ historical moments to predict the missing value of $L_{out}$ future moments, which can be denoted as Input-$L_{in}$-predict-$L_{out}$. If the feature dimension of the series is denoted as $D$, its input data at time $t$ can be denoted as $X_t = \\{s_1^t, ..., s_{L_{in}}^t | s \\in R^D\\}$, and its output at time $t$ can be denoted as $Y_t = \\{s_{L_{in}+1}^t, ..., s_{L_{in}+L_{out}}^t | s_{L_{in}+k} \\in R^D\\}$. Then, we can predict $Y_t \\in R^{L_{out} \\times D}$ by designing a model $F$ given an input $X_t \\in R^{L_{in} \\times D}$, which can be expressed as: $Y_t = F(X_t)$. For denotation simplicity, the superscript $t$ will be omitted if it does not cause ambiguity in the context."}, {"title": "II. RANDOM SUBGRAPH SAMPLING (RSS)", "content": "Random Subgraph Sampling (RSS) aims to reduce the computational burden of training deep models on large graphs while maintaining the model's generalization performance. RSS randomly selects a subgraph $G_{sub}$ from the original large-scale graph $G = (V, E)$ to train the model $F$, where $V$ indicates nodes and $E$ indicates edges. As shown in Fig. 2.a, the original large-scale graph is disassembled into multiple small-scale subgraphs $\\{X_{sub}, Y_{sub}, E_{sub}\\}$ through RSS, so that each of them can be processed on existing GPU devices.\nThe following theorem proves that RSS offers comprehensive coverage of the original large-scale graph, which is an unbiased estimator of the true aggregated feature using the entire graph."}, {"title": "III. PSLD", "content": "Decomposition is a powerful technique for analyzing and solving non-stationary problems in time series data [35]\u2013[37]. Decomposition addresses non-stationarity by breaking the time series into distinct components that can be more easily analyzed and modeled. The primary components typically considered are mean, variance, trend, seasonal, and residual (or noise) components.\nIn this section, we present PSLD to mitigate the non-stationary in WT data, which arise from trends, seasonal patterns, or other time-varying structures. As shown in Fig. 3, the proposed PSLD consists of three main components: decomposer, learner and predictor, as well as combinator."}, {"title": "A. Decomposer", "content": "The decomposer is responsible for decomposing the target variable Y into components that capture non-stationary and stationary behaviors respectively. Many classic time series decomposition methods, including additive decomposition $Y = T + S + R$, multiplicative decomposition $Y = T \\times S \\times R$, and seasonal-trend decomposition $Y = T + S + R$ (e.g., STL), can be employed for label decomposition, where $T$ captures the long-term trend, $S$ captures the seasonal or cyclical patterns, and $R$ captures the residual stationary component. In this paper, we utilize a mean-variance (hybrid additive and multiplicative) decomposition and an STL decomposition.\nMean-Variance Decomposer (MVD): The mean-variance decomposer is a hybrid decomposition method that combines the advantages of both additive and multiplicative decompositions. The decomposition of the target variable by MVD can be expressed as:\n$\\begin{aligned}\nM &= Mean(Y, -1), \\\\\nY' &= Y - M, \\\\\nV &= Var(Y', -1), \\\\\nR &= Y'/V + \\epsilon, \n\\end{aligned}$\n(1)\nwhere $M$ and $V$ are the mean and variance of $Y$, respectively, $\\epsilon$ is a small constant to avoid division by zero. The mean $M$, variance $V$, and variable $R$ in Eq. 1 are used to progressively supervise the learning process of the learner.\nSTL Decomposer: STL is a robust and flexible method that can decompose a complex time series into seasonal, trend, and residual components. The decomposition of the target variable by the STL decomposer can be expressed as:\n$\\begin{aligned}\nT &= Move-Avg(Y, \\kappa_T), \\\\\nY' &= Y - T, \\\\\nS &= Move-Avg(Y', \\kappa_S), \\\\\nR &= Y' - S, \n\\end{aligned}$\n(2)\nwhere Move-Avg is the moving average operation, $\\kappa_T$ and $\\kappa_S$ are the size of the trend and seasonal smoothing kernel, respectively. Similar to MVD, the trend $T$, seasonal $S$, and residual $R$ components in Eq. 2 are used to progressively supervise the learning process of the learner."}, {"title": "B. Learner and Predictor", "content": "To progressively guide the model's learning process, we employ different learners and predictors to give predictions for each component individually at the model's shallow level. To achieve this, we insert a decomposer at the input to decompose it into its component forms corresponding to those of the output, like\n$\\begin{aligned}\nM_X &= Mean(X, -1), \\\\\nX' &= X - M_X, \\\\\nV_X &= Var(X', -1), \\\\\nR_X &= X'/V_X + \\epsilon, \n\\end{aligned}$\n(3)\nIndeed, the learner and predictor can be any learnable models, where the learner extracts features and the predictor generates prediction results. Let $L$ and $P$ be the learner and predictor parameterized by $\\theta$, $\\phi$ and $\\psi$, respectively. For MVD, we have:\n$\\begin{aligned}\nM &= P_\\theta(L_\\theta(M_X)), \\\\\nV &= P_\\phi(L_\\phi(V_X)), \\\\\nR &= P_\\psi(L_\\psi(R_X)). \n\\end{aligned}$\n(4)\nThe reason for separating the learner and the predictor is for greater flexibility. For example, the learner can be a deep neural network, while the predictor can use traditional regression methods. In some simple scenarios, employing a learner may not be necessary."}, {"title": "C. Combinator", "content": "The combiner integrates the predictions of each component to produce the final prediction result, which is the inverse of the decomposition process. As shown in Fig. 3, The components are sequentially integrated in the combiner, which incorporates learnable parameters to coordinate the contributions of each component. Specifically, for MVD, the prediction result of combiner can be expressed as:\n$\\hat{Y} = P_\\varepsilon(L_\\varepsilon(V \\times R) + M),$ (5)\nwhere $\\varepsilon$ is the parameter of the combiner. Similarly, For STL, the combiner can be expressed as:\n$\\hat{Y} = P_\\varepsilon(L_\\varepsilon(S + R) + T).$ (6)"}, {"title": "D. Loss Function", "content": "The loss function of PSLD comprises two parts: the prediction loss of each component generated by the predictor, and the final prediction loss produced by the combiner. For MVD, the loss function of the components can be expressed as:\n$L_{cpn} = L_\\theta(M, \\hat{M}) + L_\\phi(V, \\hat{V}) + L_\\psi(R, \\hat{R}),$ (7)\nand the loss function of the combiner can be expressed as:\n$L_{cbn} = L_{\\xi}(\\hat{Y}, Y).$ (8)\nThus, the final loss function of PSLD can be expressed as:\n$L = L_{cbn} + \\lambda L_{cpn},$ (9)\nwhere $\\lambda$ is a hyperparameter that controls the contribution of the component loss to the overall loss. Likewise, the loss function of the STL decomposer is similar to that of MVD."}, {"title": "E. Accelerating PSLD", "content": "It is observed that the learner and predictor in PSLD operate in parallel. If they share the same model structure, they can be merged to enhance computational efficiency. As shown in Fig. 4, the learners and predictors are merged into a single model, and their parameters are combined into a single set $\\{ \\theta, \\phi, \\psi \\}$. This is equivalent to widening the original learner and predictor, which can be expressed as:\n$[\\hat{M}, \\hat{V}, \\hat{R}] = P_{[\\theta, \\phi, \\psi]}(L_{[\\theta, \\phi, \\psi]}([M_X, V_X, R_X])).$ (10)\nSimilarly, the component loss can be expressed as:\n$L_{cpn} = L_{[\\theta, \\phi, \\psi]}([\\hat{M}, \\hat{V}, \\hat{R}], [M, V, R]).$ (11)\nBy employing Eq. 11, we can train a single integrated model, which is typically more efficient than training multiple independent models due to the parallel operations."}, {"title": "IV. EXPERIMENTS", "content": "PSLD is extensively evaluated on the three widely used real-world WT datasets, including multiple mainstream WT applications such as C2TM, Milano and CBS."}, {"title": "A. Datasets", "content": "1) C2TM [38] makes an analysis on week-long traffic generated by a large population of people in a median-size city of China. C2TM analyses make use of request-response records extracted from traffic at the city scale, consisting of individuals' activities during a continuous week (actually eight days from Aug. 19 to Aug. 26, 2012), with accurate timestamp and location information indicated by connected cellular base stations. C2TM contains 13269 nodes, each with a length of 192. The ratio of training, validation and test datasets is 5:1:2.\n2) Milano [39] is provided by SKIL\u00b9 of Telecom Italia. The dataset was collected from November 1, 2013, to January 1, 2014, and the data is aggregated into 10-minute intervals over the whole city of Milan (62 days, 300 million records, about 19 GB). Milano records the time of the interaction and the specific radio base station that managed it. The dataset contains 10,000 nodes, and the length of each node is 1498. The ratio of training, validation and test datasets is 10:2:3.\n3) CBS is collected from all base station in a certain city in China, including up-link and down-link communication traffic data. The dataset is collected from June 8 to July 26, 2019, with a temporal interval of 60 minutes over the whole city (94 days, 51.9 million records, about 17.1 GB). CBS contains 4454 nodes, each with a length of 4032. The ratio of training, validation and test datasets is 3:1:1."}, {"title": "B. Implementation Details", "content": "For the WT datasets, we initially employ the RSS algorithm to perform data sampling. In this algorithm, we random decompose the entire large graph into 24 sub-graphs at each sampling, as specified in Algorithm 1. For the learner in PSLD, we employ a 2-layer MLP with ReLU as the activation function. For the predictor, we utilize a linear layer. We selected this architecture due to its simplicity, computational efficiency, and ability to model nonlinear relationships effectively, which aligns well with the structure of our dataset. While CNNS are particularly suited for capturing spatial dependencies and GNNs are ideal for graph-based data, our problem does not exhibit strong spatial structures that would benefit significantly from CNNs, nor does it require the additional complexity that GNNs bring. Therefore, we found the MLP to be the most appropriate choice for balancing performance and computational cost in this case. We implemented two versions of PSLD: one utilizing the MVD (denoted as PSLD-MVD) and the other employing the STL decomposer (denoted as PSLD-STL).\nThe data preprocessing of the training datasets $X_{train}$ is calculated as:\n$\\begin{aligned}\nM &= \\frac{1}{N} \\sum_{i=1}^N X_{train}[i] \\\\\nS &= \\sqrt{\\frac{1}{N-1} \\sum_{i=1}^N (X_{train}[i] - M)^2} \\\\\n\\tilde{X}_{train} &= (X_{train} - M)/S \\\\\n\\tilde{X}_{test} &= (X_{test} - M)/S\n\\end{aligned}$ (12)\nwhere $N$ is the number of samples, $M$ and $S$ are the mean and standard deviation of the training set, respectively. It is worth noting that the data preprocessing step is consistent across all datasets, and we did not use any other data preprocessing techniques.\nFurthermore, the number of hidden units is set to 128. The dropout rate is set to 0.05. The model is trained using the ADAM [40] optimizer and L2 loss. The learning rate is set to 0.0001. The batch size is set to 1 for large-scale graph WT datasets, and the maximum number of epochs is set to 10, which is consistent with [31], [41] The model is trained on a single NVIDIA Tesla V100 GPU with 32GB memory. And the software environment and dependencies include Python 3.10 and PyTorch 2.0."}, {"title": "D. Main Results", "content": "The results for LL-WTF are summarized in Table I, PSLD achieves the consistent SOTA performance in all datasets and prediction length settings. Compared to PatchTST and STID, the proposed PSLD yields an overall 5.4% and 8.7% relative MSE reduction, respectively. Specifically, for the input-8-predict-5 setting in C2TM, PSLD-MVD gives 1.9% (9.340\u21929.161, compared to PatchTST) and 6.2% (9.769-9.161, compared to STID) MSE reduction. For the input-8-predict-6 setting in C2TM, PSLD-MVD gives 2.2% (9.509\u21929.298, compared to PatchTST) and 3.4% (9.625-9.298, compared to STID) MSE reduction.\nFor input-36-predict-36 setting, PSLD-MVD has 6.9% (0.832\u21920.775) and 7.7% (0.840\u21920.775) MSE reduction in Milano, 13.0% (2.036\u21921.772) and 13.1% (2.038 1.772) in CBS, etc. PSLD also outperforms other state-of-the-art models by a large margin on other settings, such as the latest Long-Term Series Forecasting (LTSF) models, including including FreTS [42], TimeMachine [43], and FourierGNN [44]. All the above experimental results have verified that the proposed PSLD can achieve better prediction performance on different datasets with varying horizons, implying its superiority on LL-WTF tasks. The ability to decompose the time series and model each component separately makes our proposed method robust to non-stationary data. This advantage is reflected in the lower error metrics compared to models that do not explicitly handle non-stationarity."}, {"title": "E. Comparative Analysis", "content": "Notably, several pioneering models have also achieved competitive performance on certain datasets under particular settings. For instance, Informer demonstrates relatively poor performance on the WT datasets with different horizons settings. This is due to the substantial presence of non-stationarity in each variate of the WT datasets. This renders the KL-divergence-based ProbSparse Attention, as adopted in Informer, ineffective on this non-stationarity dataset. Additionally, linear-based methods (e.g., STID and DLinear) have demonstrated promising results on the WT datasets with various horizons setting, while GNN-based methods (e.g., MVSTGN and GWNet) have yielded favorable results on these WT datasets. This phenomenon can be ascribed to a twofold interplay of factors. Previously, WT data exhibits stronger non-stationarity, including increased non-periodicity and noise, which directly affects the model's generalization. Secondarily, other models exhibit a propensity to overfit non-stationary WT characterized by aperiodic fluctuations. Remarkably, PSLD adeptly mitigates non-stationary challenges in LL-WTF forecasting, thereby enhancing its overall performance. Particularly on the WT datasets with large-scale variables, PSLD achieves superior performance by adopting label decomposition to obtain multiple easy-to-learn components. These components are learned progressively at shallow layers and combined at deep layers to effectively address the non-stationary problem posed by LL-WTF tasks."}, {"title": "F. Ablation Study", "content": "Component Analysis: To validate the effectiveness of PSLD, we conduct comprehensive ablation studies encompassing both component replacement and component removal experiments, as shown in Fig. 5. We utilize letters 'D' and 'C' to denote the utilization of decomposer or combinator during the aggregation process of the input X or output Y streams. In cases involving only input streams, it becomes evident that the model's average performance is superior when employing decomposer on X (X+D, Y) compared to when employing it on Y (X, Y+D). E.g., on the Milano dataset, forecast error is reduced by 7.8% (0.906 0.835). Moreover, the introduction of decomposers to the input and output streams respectively, further enhances the model's performance, e.g., forecast error on the CBS dataset is reduced by 0.9% (1.788\u21921.772) as shown in Fig. 5(c). Afterward, incorporating combinator (C) into the model holds the potential to improve predictive performance again, e.g., on the C2TM dataset, forecast error is reduced by 0.8% (0.784 0.775). In summary, integrating the decomposer and combinator components has the potential to significantly boost the model's performance across the board.\nSubgraph Partitioning: We conducted ablation experiments with different subgraph sizes on two large-scale graph datasets: Milano and CBS. For these datasets, we initially employ the Random Subgraph Sampling (RSS) algorithm to perform data sampling. In RSS algorithm, we random decompose the entire large graph into 12, 24 and 32 subgraphs at each sampling. As shown in Table II, the experimental findings indicate that the model's performance is stable across different subgraph sizes, reinforcing the robustness of the subgraph partitioning."}, {"title": "G. Versatility", "content": "To examine the versatility of PSLD as a comprehensive framework, we integrate the PSLD mechanism into various models to observe enhancements in their performance. As shown in Fig. 6, after harnessing the newly invented PSLD within other models, their performance exhibited considerable improvement. For example, the average MSE of PatchTST on the C2TM and Milano datasets witnessed a reduction of 1.5% (9.561\u21929.415), 5.5% (0.832\u21920.786), and 2.6% (2.036-1.983), respectively, surpassing the original model by a large margin. Furthermore, both STID, GWNet and other Attention-based models demonstrate commendable performance on the aforementioned datasets. The conducted experiments suggest that PSLD can serve as a versatile architecture, amenable to the integration of novel modules, thereby facilitating the enhancement of performance in the domain of TS forecasting."}, {"title": "H. Generalizability", "content": "To verify the generalization of PSLD, we include comparative experiments with other Long-Term Sequence Forecasting (LTSF) models across multiple LTSF benchmark datasets, including ETT [41], ILI \u00b9, Weather \u00b2, Electricity \u00b3, and Traffic \u2074. Comprehensive forecasting results are presented in Table III, with the best results highlighted in bold. The lower MSE indicates the more accurate prediction result. The proposed PSLD demonstrates superior performance across various LTSF benchmark datasets. Notably, PatchTST, previously the best model for weather datasets, fails in many cases on LTSF datasets. This can be attributed to the highly volatile nature of the data, which may cause the patching mechanism of PatchTST to lose focus on specific localities necessary for managing rapid fluctuations. By contrast, PSLD utilizes label decomposition to obtain multiple easy-to-learn components, which are learned progressively at shallow layers and combined at deep layers. Therefore, the proposed method can progressively learn the decomposed supervision signal, allowing it to better address the non-stationary problem of LTSF."}, {"title": "I. Efficiency", "content": "As shown in Table IV, the proposed PSLD demonstrates superior performance, providing significantly faster inference compared to most models. Although the purely linear DLinear has the fastest training and inference speed, it sacrifices accuracy. Mvstgn, STID and other Transformer-based models have the highest complexity, significantly slower training and inference times, and poorer accuracy. In summary, PSLD demonstrates a balance of speed (training and inference), accuracy (lowest MSE), and moderate computational efficiency. Therefore, PSLD is optimal for scenarios requiring both high accuracy and computational efficiency."}, {"title": "J. Visualization the Learning Process of MVD", "content": "The intrinsic characteristic of PSLD framework lies in the alignment of the output from each component with the shape of the label's decompositions. This alignment, in turn, expedites the decomposition and visualization of the model's learning process. As depicted in Figs. 7 and 8, the output of each component in PSLD is visualized. Fig. 7(a) shows the prediction results of MVD, Fig. 7(b) displays the predictions of the mean, Fig. 7(c) illustrates the predictions of the variance, and Fig. 7(d) presents the predictions of the residual. It becomes evident that each component discerns and assimilates meaningful patterns within the series: Decomposing Y into mean and variance provides clear insights into the underlying structure of the data. The mean captures the central tendency, while the variance captures the spread or dispersion. For instance, in Fig. 7, despite the differences in the amplitudes of the series, the mean and variance components are accurately fitted to their groudn-truth values. Specifically, comparing Figs. 7(a) and 7(b), for non-stationary series with varying mean and variance, each block must learn salient patterns: The mean can capture trends and seasonality, while the variance can account for changes in variability over time. In addition, by predicting the variance, it is possible to quantify the uncertainty of predictions, leading to more robust and reliable forecasting. We leave this for future work."}, {"title": "K. Visualization the Learning Process of STL", "content": "The prediction results of STL along with the trend-season and residual components is visualized in Fig. 8. However, owing to the employment of different decomposers, the learning patterns of the components in Fig. 8 exhibit slight variations compared to those in Fig. 7. STL decomposes the time series into trend, seasonal, and residual components. By isolating the trend component, the model can focus on capturing long-term patterns separately from seasonal fluctuations and random noise, which is particularly useful in non-stationary time series. Meanwhile, STL provides a clear separation of trend, seasonal, and irregular components, making it easier to interpret the underlying structure of the time series. This separation helps stakeholders understand the contributions of different factors to the overall behavior of the series. By understanding the trend and seasonal components separately, businesses can derive actionable insights, such as identifying growth trends or understanding the impact of seasonal demand fluctuations."}, {"title": "L. Visualization of LL-WTF Results", "content": "For clarity and comparison among different models, we present supplementary prediction showcases for three representative datasets in Figs. 9 and 10. Visualization of different models for qualitative comparisons. Prediction cases from the C2TM and Milano datasets. Two showcases correspond to predictions made by the following models: DLinear [48], PatchTST [45], and FEDformer [32]. Among the various models considered, the proposed PSLD stands out for its ability to predict future series variations with exceptional precision, demonstrating superior performance."}, {"title": "V. RELATED WORK", "content": "Early classical methods [51]\u2013[53] are widely applied to WT forecasting because of their well-defined theoretical guarantee and interpretability. For example, ARIMA [51] initially transforms a non-stationary time series into a stationary one via difference, and subsequently approximates it using a linear model with several parameters. Exponential smoothing [52] predicts outcomes at future horizons by computing a weighted average across historical data. In addition, some regression-based methods, e.g., random forest regression [54] and support vector regression [55], etc., are also applied to WT forecasting. These methods are straightforward and have fewer parameters to tune, making them a reliable workhorse for WT forecasting. However, their shortcoming is insufficient data fitting ability, especially for high-dimensional series, resulting in limited performance."}, {"title": "A. Classical Models for WT Forecasting", "content": "Early classical methods [51]\u2013[53] are widely applied to WT forecasting because of their well-defined theoretical guarantee and interpretability. For example, ARIMA [51] initially transforms a non-stationary time series into a stationary one via difference, and subsequently approximates it using a linear model with several parameters. Exponential smoothing [52] predicts outcomes at future horizons by computing a weighted average across historical data. In addition, some regression-based methods, e.g., random forest regression [54] and support vector regression [55], etc., are also applied to WT forecasting. These methods are straightforward and have fewer parameters to tune, making them a reliable workhorse for WT forecasting. However, their shortcoming is insufficient data fitting ability, especially for high-dimensional series, resulting in limited performance."}, {"title": "B. Deep Models for WT Forecasting", "content": "The advancement of deep learning has greatly boosted the progress of WT forecasting. Specifically, the authors in [15] treat the wireless traffic dataset as gridded data and utilize CNNs [21] and RNNs [23] to capture spatial and temporal correlations, respectively. The authors in [16] investigate the spatial and temporal dependence of wireless traffic among different cells, where the spatiotemporal DenseNet [56] based prediction framework is designed. The follow-up work in [17] conducte clustering on different Point of Interest (POI) modes, and then uses transfer learning in different communication traffic patterns to improve the prediction accuracy. GCN-based methods [18], [19], [50] have been developed for WT forecasting on graph data. Besides, DiffTAD [57] is a framework designed to detect anomalies in vehicle trajectories using advanced diffusion models. It aims to develop deep learning models that learn the reverse of the diffusion process, enabling the detection of anomalies by comparing the differences between a query trajectory and its reconstructed counterpart. The aforementioned methods solely concentrate on the forms of aggregating short-term input series, overlooking the challenges posed by the long-term and large-scale irregular WT data."}, {"title": "C. Long-Term Time Series Forecasting", "content": "When the sampled subseries becomes tractable, LL-WTF can be regarded as a typical long-term series forecasting task. There are several methods that can be adopted, including attention-based long-term forecasting [31], [32], [41], [47], [58], [59]. For example, several works have improved the series aggregation forms of attention mechanism, such as operations of exponential intervals adopted in LogTrans [60], KL-divergence based ProbSparse activations in Informer [41], frequency-based random sampling in FEDformer [32], channel-independent patches in PatchTST [58], and period-based subseries integration in Periodformer [47]. In summary, existing long-term time series forecasting methods have primarily focused on efficiently aggregating similar subseries by designing novel modules, rather than offering effective solutions for the crucial issue of non-stationarity existing in time series, making them relatively inefficient."}, {}]}