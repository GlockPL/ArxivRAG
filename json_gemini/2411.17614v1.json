{"title": "Automating Chapter-Level Classification for Electronic Theses and Dissertations", "authors": ["Bipasha Banerjee", "William A. Ingram", "Edward A. Fox"], "abstract": "Traditional archival practices for describing electronic theses and dissertations (ETDs) rely on broad, high-level metadata schemes that fail to capture the depth, complexity, and interdisciplinary nature of these long scholarly works. The lack of detailed, chapter-level content descriptions impedes researchers' ability to locate specific sections or themes, thereby reducing discoverability and overall accessibility. By providing chapter-level metadata information, we improve the effectiveness of ETDs as research resources. This makes it easier for scholars to navigate them efficiently and extract valuable insights. The absence of such metadata further obstructs interdisciplinary research by obscuring connections across fields, hindering new academic discoveries and collaboration. In this paper, we propose a machine learning and AI-driven solution to automatically categorize ETD chapters. This solution is intended to improve discoverability and promote understanding of chapters. Our approach enriches traditional archival practices by providing context-rich descriptions that facilitate targeted navigation and improved access. We aim to support interdisciplinary research and make ETDs more accessible. By providing chapter-level classification labels and using them to index in our developed prototype system, we make content in ETD chapters more discoverable and usable for a diverse range of scholarly needs. Implementing this AI-enhanced approach allows archives to serve researchers better, enabling efficient access to relevant information and supporting deeper engagement with ETDs. This will increase the impact of ETDs as research tools, foster interdisciplinary exploration, and reinforce the role of archives in scholarly communication within the data-intensive academic landscape.", "sections": [{"title": "I. INTRODUCTION", "content": "Electronic theses and dissertations (ETDs) represent a core component of academic scholarship, comprising extensive research, diverse methodologies, and findings that contribute to knowledge across numerous fields. These documents often contain multiple chapters that vary in focus, incorporating interdisciplinary perspectives or methodological shifts within a single work. Given this complexity, conventional archival practices, which typically describe documents at a general level with metadata such as author, title, and subject, fall short of providing the granularity needed to fully represent ETDs. This limitation restricts readers' ability to locate specific content within these documents, as document-level descriptions lack chapter-specific metadata that could direct users to relevant sections. To address these challenges, this study explores the use of artificial intelligence (AI) to automate chapter-level classification within ETDs, with the goal of improving the effectiveness of information retrieval across academic disciplines.\nInstitutional repository records for ETDs typically include only document-level descriptive metadata, which does not capture chapter-level information within these complex works. A typical ETD contains multiple chapters, each addressing a different aspect of the research. For example, a dissertation in environmental science might include chapters on statistical data analysis, policy implications, and ecological fieldwork findings, each relevant to different research fields. With only document-level descriptions available, researchers are often compelled to navigate entire ETDs manually to locate specific sections, increasing the likelihood of overlooking valuable content embedded within individual chapters.\nThis paper presents a research process to automate chapter-level classification in ETDs. Chapter-level classification labels enable researchers to use categories to quickly search for and find chapters relevant to their interests, thereby enhancing the overall access and discovery of knowledge buried in ETDs, as demonstrated in a prototype system we have built [1]. The process involves two main tasks: segmentation and classification. First, segmentation identifies chapter boundaries within ETDs, a task complicated by the lack of support for this in PDFs, and the variation in discipline-specific formatting norms, such as APA or IEEE style guidelines, which affect headers, section markers, and other structural cues. Second, the classification assigns detailed descriptions to each chapter, generating chapter-specific metadata that allows researchers to locate precise information within these works.\nWe explore how language models can be used to create chapter-specific metadata for ETDs. By generating detailed classification descriptors for each chapter, we aim to help researchers to locate specific sections, supporting more efficient academic use, particularly in interdisciplinary research.\nWe explore effective approaches for classifying ETD chapters by comparing traditional machine learning classifiers, bidi-"}, {"title": "II. RELEVANT LITERATURE", "content": "Archival science has evolved over recent decades. Although its mission of managing and preserving information remains unchanged, its scope has expanded. The field now includes re-searchers from archival, information, and computing sciences, adapting to the complex demands of data-intensive research. Terry Cook [2] challenged traditional archival principles, in-troducing postmodern theory and emphasizing the subjective and socially embedded nature of archival work. Cook's theory called for diversity and representation in archives, challenging modern archivists to better reflect varied societal perspectives. Dougherty et al. [3] emphasize the role of archivists in supporting interdisciplinary studies, arguing for proactive web archiving practices that meet the diverse needs of researchers in social sciences and humanities. The authors in [4] stress the importance of research data management in academic libraries to support collaboration across disciplines. This aligns with the archivist's role in supporting interdisciplinary research by providing comprehensive metadata descriptions that facilitate the discovery of research findings across fields.\nLanguage Models, especially large language models (LLMs), perform exceptionally well on tasks involving natural language understanding and generation [5]. LLMs are trained on massive amounts of data and have been shown to achieve outstanding performance on various natural language process-ing (NLP) tasks, such as classification, question answering, and summarization. OpenAI's Chat-GPT [6] introduced the world to LLMs and generative AI. Although LLMs have gained popularity, the foundational technology has been de-veloping for decades. The core concept of language models is to determine the probability of the next word occurring in a sentence. Bengio et al. [7] proposed statistical language mod-eling by using neural networks to learn word representations.\nLLMs are built on a deep learning architecture known as the Transformer [8]. The self-attention mechanism within the Transformer model enables the network to dynamically weigh the relevance of each token in a sentence or passage, capturing contextual relationships across the entire sequence, regardless of positional distance. This architecture allows for parallel processing of tokens, enhancing the model's efficiency and its capacity to handle complex contextual dependencies in text. Early transformer-based language models such as BERT [9], SciBERT [10], and RoBERTa [11] handle text with short context length, whereas models such as BigBird Pegasus [12] and Longformer [13] are capable of handling up to a 4096 token length. BERT models are bidirectional, meaning that they consider both preceding and following words to predict the word relevant to the context. Autore-gressive LLMs such as GPT [14], Llama [15], Phi-3 [16], Mistral [17], and Claude [18] generate text by predicting each word in sequence based only on prior tokens. These so-called generative models learn from large amounts of data to produce coherent, contextually relevant sequences of words, sentences, or even paragraphs, effectively \"generating\u201d content.\nWhile generative models are adept at producing open-ended text responses, traditional machine learning classifiers like support vector machines (SVM) [19] and random forest (RF) [20] continue to be used for various classification prob-lems. Jude [21] used these traditional machine learning clas-sifiers for classifying ETD chapters into one of 28 ProQuest subject categories [22]. In a classification study [23] building on that approach, the performance of fine-tuned language mod-els was compared with that of their pre-trained counterparts, highlighting the evolution from traditional machine learning to advanced language models for text classification. Additional experiments to evaluate classification using machine learning, fine-tuned language models, and large language models across academic datasets, methodologies, and evaluation strategies were reported in [24.\nDomain adaptation of language models involves fine-tuning and instruction-tuning to tailor the model to specific data and tasks. Fine-tuning incorporates domain nuances and increases the model vocabulary on a task-specific labeled dataset. LLMs can also be instruction-tuned. The difference lies in how the model was trained and the dataset used for this process. Instruction-tuning LLMs [25] is a fine-tuning approach where an LLM is trained on a labeled dataset of instructional prompts and outputs. Alongside fine-tuning, prompting LLMs is a technique to guide the model's re-sponses based on task-specific instructions without altering its internal parameters. Prompting leverages the model's pre-existing knowledge by framing questions or directives that align with the desired output. This approach is particularly useful for adapting LLMs to new tasks quickly, as it does not require extensive re-training. By crafting effective prompts, users can tap into the model's capacity to handle nuanced domain-specific tasks with minimal adjustment.\nA pre-trained language model can be used to perform spe-cific tasks or work within particular domains without starting from scratch. Brown et al. [26] found that prompt-based approaches could achieve comparable performance to fine-tuning on several downstream tasks. With zero-shot prompt-ing [27], [28], the model is applied to a new task without any specific task-related examples in its training data. The model uses its pre-existing understanding of language to interpret instructions and generate responses relevant to the task. Few-shot learning [29], [30] involves providing the model with a"}, {"title": "III. DATASETS", "content": "In prior work [35], we amassed a collection of over half a million ETDs from several universities in the United States. Exploratory analysis of our dataset was reported in [23]. The statistics in Table I show the data subsets used in various experiments discussed in later sections of this paper."}, {"title": "IV. METHODOLOGY", "content": "We designed a workflow to segment, extract, and classify ETD chapters for accurate categorization. Fig. 1 illustrates the complete process. We begin by segmenting each ETD into individual chapters. To extract text from these segmented chapters, we use a hybrid method that combines AWS Tex-tract [36] with object detection techniques [37]. The extracted chapter text is then passed through our classification module, which includes pre-trained and fine-tuned language models. Our classification module generates three types of labels.\n1) Single label: This is from a multi-class classification task in which the model predicts a single class from multiple possible categories.\n2) Top three labels: This results from multi-label classification with a sigmoid activation function to predict the three most relevant labels for each chapter.\n3) 2-level label: An LLM produces a more granular, hierarchical classification with two levels of categories.\nA. Segmentation\nChapter-level classification requires ETDs to be segmented accurately into individual chapters. To the best of our knowl-edge, no openly available ETD dataset includes chapter-level segmentation. Although automated segmentation meth-ods, such as those in [37] and [38] were considered to establish chapter boundaries, both methods failed to produce segments with the necessary precision and accuracy. Consequently, we manually segmented the ETDs in our collection into individual chapters to ensure high-quality data. Details of this segmented dataset, referred to as ETD-SGT, are provided in Section III-A.\nB. Text Extraction\nTo extract clean chapter text from ETD chapters in the ETD-SGT dataset, we initially explored open-source Python libraries like PDFPlumber and PyMuPDF. These tools are commonly used for basic PDF processing, but we found they were unable to reliably separate chapter text from other page elements like tables, figures, equations, and captions, necessitating an alternative method. To overcome this, we combined AWS Textract with an object detection model to achieve more precise text extraction. AWS Textract is a paid machine-learning-enabled text extraction service that provides structured text outputs with positional information. Using an object detection model [37] helps isolate specific page elements. The text extraction process follows these steps:\n1) AWS Textract: We convert each page into an image and apply AWS's Textract's detect_document_text API. The service classifies text into \"BlockType\" tags as a page, line, or word. It also returns the extracted text, bounding box information, confidence scores, and IDs of the related extracted block elements. We store the results in JSON format.\n2) Object Detection: Using the ETD object detection model as described in [37], we generate bounding boxes for specific page elements in each ETD page. The model outputs bounding box coordinates, labels, and page numbers, which we saved in a text file.\n3) Label Filtering and Normalization: We use the label information from Step 2 to filter out unwanted elements from extracted text, such as page headers, footers, captions, figures, and equations. Since each method yields bounding box coordinates based on different page sizes, we normalize the coordinates to ensure consistency, enabling accurate alignment across both techniques.\nC. Classification\nOur classification methodology consists of three main stages: comparing different classification approaches, fine-tuning language models on ETD-specific content, and applying multi-label classification techniques to address the interdisci-plinary nature of ETD chapters.\n1) Model Evaluation: We compare traditional machine learning classifiers, specifically support vector machines (SVM) and random forests (RF), against language model classifiers (BERT and SciBERT) and large language models (LLMs) such as Llama-2 and Llama-3.\n2) Fine-tuning on ETD Data: We fine-tune BERT and SciBERT on our ETD corpus to determine if domain-specific fine-tuning improves classification accuracy.\n3) Multi-label Classification for Interdisciplinary Con-tent: Given the interdisciplinary scope within ETD chap-ters, we apply two multi-label classification approaches:\n\u2022 Language Model Classifiers: Using a sigmoid activation function in our BERT and SciBERT vari-ations, we generate independent probability scores for each class, selecting the top three predictions per chapter to evaluate accuracy.\n\u2022 LLM-Prompted Multi-label Prediction: With Llama-2 and Llama-3, we prompt the models to generate multiple category labels per chapter, evaluating the generated labels against ground truth using cosine similarity.\nThis methodology allows us to evaluate the strengths and limitations of each approach in accurately classifying ETD content. Full experimental setups and results are detailed in the following section."}, {"title": "V. EXPERIMENTS AND RESULTS", "content": "This section details the classification approaches, experimental setups for each method, and the corresponding results.\nA. Comparing Machine-learning Classifiers with Language Model-Based Classifiers\nWe use support vector machines (SVM) and random forests (RF) as our machine-learning classifiers, as these models previously were reported to be the best-performing classifiers [21]. The classification task in this experiment is a multi-class problem where the model predicts a single label from a set of provided classes. As shown in Table II, SVM achieved the highest performance between the machine-learning models. However, language model-based classifiers consistently out-performed both SVM and RF, with higher overall F1 scores. In addition to precision, recall, and F1 scores, we evaluated model performance using receiver operating characteristic (ROC) curves for both machine-learning and language model-based classifiers. The ROC curve provides insights into model performance across various threshold levels. Figs. 2 and 3 present select results, showcasing the highest-performing clas-sifiers. We observe that the language model-based classifiers have a larger area under the curve (AUC) compared to the machine-learning classifiers, indicating better performance.\nB. Comparing Pre-trained vs. Fine-tuned Language Models\nWe evaluated language models for ETD classification, com-paring pre-trained BERT and SciBERT models with versions fine-tuned on the FTD dataset (see Section III-D). BERT and SciBERT are initially pre-trained on general and domain-specific corpora, respectively, but we further fine-tuned on our"}, {"title": "VI. DISCUSSION", "content": "In this research, we evaluate various approaches for auto-matically classifying ETD chapters, examining multiple clas-sifiers to identify the most effective for this task. Our findings related to RQ1 suggest that language model-based classifiers"}, {"title": "VII. CONCLUSION AND FUTURE WORK", "content": "This study proposes a methodology for classifying ETD chapters. Our machine learning and AI-driven chapter-level classification approach can improve ETD discoverability and accessibility by providing detailed chapter-level descriptions; future work will aim to quantify the improvement. We find that LLM-based approaches show promise in classifying ETD chapters but come with their own set of challenges. LLM-generated outputs are often not constrained, making post-processing difficult. The absence of well-formatted output makes it challenging to assess model performance using traditional automatic evaluation metrics. Careful and precise prompts with the newer versions of LLMs are improving the models' ability to follow desired output formats. LLMs were able to predict several categories, but the predicted output set predicted subject categories and combinations that were not an exact match to our classification labels. Due to the nature of our scholarly data, we need subject matter expertise to judge if they are correct. Getting subject experts to evaluate these generated labels can be time and resource-intensive. LLMs with many parameters require large amounts of GPU RAM. However, it is getting easier with the newest generation of LLMs, such as Phi-3 and Mistral, that have a smaller memory footprint. The latest generation of LLMs also has an increased context window, making it easier to work with longer text, such as ETD chapters.\nOur future work should improve LLM-based results by adding more robust generation and evaluation techniques. For the generation task, we are experimenting with prompting approaches. We will refine and optimize the existing prompts in an attempt to outperform the current model's performance. In addition to zero-shot, few-shot, and instruction tuning, we will also use chain-of-thought prompting approaches. We will use the newer version LLMs, such as Llama-3.2 and Phi-3.5, which have longer context windows and require less GPU memory. This enables us to instruction-tune and fine-tune the models to better adapt to the domain and task.\nFor evaluation, we have performed some preliminary user studies that confirm that our LLM methodology has promising results. In addition to using standard evaluation metrics for classification, we plan to continue identifying and verifying different LLM evaluation techniques that can help obtain more detailed insights into LLM performance."}]}