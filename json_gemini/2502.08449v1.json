{"title": "CordViP: Correspondence-based Visuomotor Policy for Dexterous Manipulation in Real-World", "authors": ["Yankai Fu", "Qiuxuan Feng", "Ning Chen", "Zichen Zhou", "Mengzhen Liu", "Mingdong Wu", "Tianxing Chen", "Shanyu Rong", "Jiaming Liu", "Hao Dong", "Shanghang Zhang"], "abstract": "Achieving human-level dexterity in robots is a key objective in the field of robotic manipulation. Recent advancements in 3D-based imitation learning have shown promising results, providing an effective pathway to achieve this goal. However, obtaining high-quality 3D representations presents two key problems: (1) the quality of point clouds captured by a single-view camera is significantly affected by factors such as camera resolution, positioning, and occlusions caused by the dexterous hand; (2) the global point clouds lack crucial contact information and spatial correspondences, which are necessary for fine-grained dexterous manipulation tasks. To eliminate these limitations, we propose CordViP, a novel framework that constructs and learns correspondences by leveraging the robust 6D pose estimation of objects and robot proprioception. Specifically, we first introduce the interaction-aware point clouds, which establish correspondences between the object and the hand. These point clouds are then used for our pre-training policy, where we also incorporate object-centric contact maps and hand-arm coordination information, effectively capturing both spatial and temporal dynamics. Our method demonstrates exceptional dexterous manipulation capabilities with an average success rate of 90% in four real-world tasks, surpassing other baselines by a large margin. Experimental results also highlight the superior generalization and robustness of CordViP to different objects, viewpoints, and scenarios. Code and videos are available on https://aureleopku.github.io/CordViP.", "sections": [{"title": "I. INTRODUCTION", "content": "Dexterous manipulation is a fundamental capability in human daily life such as assembling small parts and opening boxes. Achieving human-level dexterity in real-world scenarios is crucial for integrating robots into everyday human activities. Recent advancements in imitation learning have demonstrated significant potential in various robotic manipulation tasks. Some existing methods leverage 2D images as input to directly predict actions [7, 67, 51, 60]. While these vision-based"}, {"title": "II. RELATED WORK", "content": "A. Dexterous Manipulation\nDexterous Manipulation is a long-standing research topic in robotics that aims to give robots the ability to perform delicate operations like humans [5, 1, 42, 62]. Traditional methods often rely on trajectory optimization based on dynamic models to solve operational problems [26, 32, 55], but these methods have limitations in complex tasks because they simplify the contact dynamics and are difficult to deal with uncertainties in dynamic environments. In contrast, Reinforcement Learning (RL) does not rely on accurate physical models but learns operational policies through interaction with the environment, which is highly adaptable. RL has achieved remarkable results in many dexterous manipulation tasks, such as object reorientation [3,\n40, 36, 20] and sequential manipulation [6, 15]. However, RL methods often suffer from several challenges, such as the need for extensive reward engineering and system design, as well as limited generalization to unseen scenarios. Additionally, while Sim-to-Real is a common technique employed in RL, the gap between simulations and the real world degrades the performance of the policies once the models are transferred into real robots [68]. Imitation learning (IL), as another effective learning method, can quickly learn effective control policies by imitating expert demonstrations [64, 51]. In this work, we propose a correspondence-based visual imitation learning policy that utilizes spatial information between various components, enabling the acquisition of complex skills with a minimal number of expert demonstrations.\n2"}, {"title": "B. Imitation Learning", "content": "Imitation learning (IL) allows a robot to directly learn from experts. Behavioral Cloning (BC) is one of the simplest imitation learning algorithms, which treats the problem of learning behavior as a supervised learning task [37]. The modeling methods commonly used in traditional BC, such as MSE, discretization [29], and K-Means [16], have limitations when modeling complex action distributions. They fail to effectively capture the diversity and nuances of human behavior [35]. Over the past few years, diffusion models have emerged as a new modeling approach in BC, becoming powerful tools that enable robots to learn from demonstrations, handle uncertainty, and perform complex multi-step tasks with precision. From the early applications of DDPMs to the recent innovations in BESO [44], \u041e\u0421\u0422\u041e [49], and CrossFormers [10], these models have continually pushed the boundaries of what's possible in robotic behavior generation. While traditional BC policies typically rely on 2D image-based representations [7, 67, 51, 60, 28], recent advancements have extended imitation learning to 3D visual representations [66, 4, 13, 52, 53, 30]. These 3D approaches provide a more comprehensive understanding of spatial relationships and 3D structures, further enhancing robotic behavior learning."}, {"title": "C. Correspondence Learning", "content": "Correspondence refers to the relationship or alignment between different entities or components, with the aim of establishing meaningful connections. Correspondence learning has been shown to improve performance in various robotic tasks, including grasping [33, 9], perception [27, 4], pose estimation [21] and garment manipulation [58]. In this paper, correspondence specifically refers to the alignment between hand-object spatial interaction and hand-arm temporal coordination. By incorporating correspondence, we enhance feature extraction capabilities, thereby enabling more accurate and coordinated movements in downstream tasks."}, {"title": "III. METHOD", "content": "The overview of our framework is shown in Figure 2, which operates in three phases: (1) Interaction-aware generation of 3D point clouds. We acquire relatively accurate and complete 3D observations during real-world dexterous manipulation tasks even under significant occlusions, as described in III-B. (2) Contact and coordination-enhanced feature extraction. By leveraging large-scale play data and incorporating contact maps and hand-arm coordination, we improve spatial interaction perception and capture cooperative motion features, detailed in III-C. (3) Correspondence-based diffusion policy. The pre-trained encoder is used to extract 3D representations, which guide the training of a visuomotor policy, as outlined in III-D."}, {"title": "A. Problem Formulation", "content": "We formulate our problem as learning a visuomotor policy \u03c0 : O \u2192 A from expert demonstrations of the form of {(o1, a1), (o2, a2),..., (on, an)}, where O represents the"}, {"title": "B. Interaction-aware Generation of 3D Point Clouds", "content": "Motivated by the superior generalization and efficiency of the 3D-based diffusion policy [66, 53, 65, 4], the key intuition behind our solution is to focus on the interactions between the hand and the manipulated object in 3D space. Although intuitively reasonable, achieving this goal is challenging in practice. On the one hand, real-world point cloud data, typically captured using stereo cameras or low-cost RGB-D scanners, suffers from geometric and semantic loss due to factors such as light reflection, material transparency, and limitations of sensor resolution and viewing angle. On the other hand, during dexterous manipulation with multi-fingered hands, occlusions frequently occur, resulting in the loss of critical contact and interaction information, which is vital for precise and effective manipulation. To this end, we propose the interaction-aware generation of 3D point clouds, enabling the reconstruction of crucial spatial information.\nReal-to-Sim for Digital Twin Generation. To achieve the goal of obtaining a complete and accurate static point cloud of the manipulated object, we aim to reconstruct the digital twin from a single-view image. Referring to the approach TripoSR [50], we implement a 3D generation technique, which utilizes its strong priors and broad understanding of visual concepts in the 3D world to generate 3D digital assets. To ensure the accuracy of point cloud flow tracking, we maintain consistency between the geometric and material properties of the reconstructed assets and their real-world counterparts. Subsequently, we uniformly sample points on the surface of the generated digital twin to obtain the initial 3D point clouds, providing a robust and accurate initial observation for both 3D spatial perception and pose tracking.\nPose-Driven Point Cloud Tracking. We have successfully obtained the initial point cloud of the object. However, tracking the object's point cloud in complex, real-world environments poses a significant challenge, particularly in scenarios with severe occlusions. To overcome this, we leverage foundation models to ensure precise pose estimation of the manipulated"}, {"title": "C. Contact and Coordination-Enhanced Feature Extraction", "content": "Interaction-aware Generation of 3D Point Clouds provides us with accurate and complete point cloud observation. However, fine-grained dexterous manipulation often requires"}, {"title": "Contact Map Synthesis.", "content": "A contact map serves as a critical piece of information in dexterous manipulation tasks, which captures the interaction between the hand and the object. Given the complete point clouds, we first calculate object-centric contact map C as the normalized distances from the object's surface point to the hand surface. Given the point clouds of the object Pobj and the point cloud of the hand Phand, the aligned distance D(o,h) between each point vo on the object surface and the surface of the dexterous hand is defined as follows:\n$D(o,h) = \\min_{vh \\in Phand} (1-<vn-vo,no>) ||Vo - Vh||^2$, (2)\nwhere no is the surface normal of the object, which is computed using the K-Nearest Neighbors methods [8] by considering the local geometric properties of the point cloud. y is the scaling factor, which is empirically set to 1.\nBased on the aligned distance, we compute the contact map following Jiang et al. [24]:\n$C = 1-2(Sigmod(\\theta \\cdot D(o,h)) \u2013 0.5)$ (3)\nwhere \u03b8 is the scaling factor and each point's contact value Ci \u2208 C is bounded within the [0, 1] range."}, {"title": "Point Cloud Feature Extraction.", "content": "We employ two encoders with identical architecture to extract point cloud embeddings, denoted as foO(Pobj) and foH(Phand), for the object and hand, respectively. In detail, we adopt PointNet [38] as the point cloud encoder, which excels in capturing local structures and integrating global information. To establish correspondences between the hand and the object features, we apply two multi-head cross-attention transformers \u0434\u03b8o, \u0434\u03b8H to fuse their respective embeddings, which maps the hand and object features to two sets of aligned representations, denoted as \u03c6H and \u03c6O:\n$\\Phi^H = \\delta_{\\theta^H} (f_{\\theta^H}(P_{hand}), f_{\\theta^O}(P_{obj})) + f_{\\theta^H}(P_{hand})$\n$\\Phi^O = \\delta_{\\theta^O} (f_{\\theta^O}(P_{obj}), f_{\\theta^H}(P_{hand})) + f_{\\theta^O}(P_{obj})$ (4)\nTo help the point cloud encoder learn the intrinsic features, we designed a contact map prediction task. Since 3D point cloud observations implicitly contain contact information, we utilize a three-layer MLP to predict the object-centric contact map Cpred given the point cloud observations of both the hand and the object. The MSE loss is calculated between C and Cpred. This pre-training approach enables the encoder to learn the interactions and relationships within the environment."}, {"title": "Hand-Arm Coordination Enhancement.", "content": "To help the robot system learn the features of hand-arm coordination, we also propose a correspondence-based design for action prediction. The arm and hand states are first projected into vectors of identical dimensionality through a linear layer, after which the same cross-attention transformers are employed to establish correspondences between the hand and the arm. We predict the action sequence of the robot arm based on the point clouds and the state of the hand. Similarly, we also predict the action sequence of the hand using point clouds and the arm state. We use MSE loss to compute the loss between the reconstructed and original action. By further predicting the action sequence respectively, the encoder is able to learn intrinsic features of motion and capture collaborative dynamics.\nGiven the aforementioned losses, our overall training objec-tive during the pre-training phase is defined as:\n$minL = L_{contact} + \\lambda L_{coordination}$, (5)\n$\\epsilon$\nwhere \u03b5 represents the encoder of the observation, and \u03bb is a hyperparameter that controls the relative strengths of the losses."}, {"title": "D. Correspondence-based Diffusion Policy", "content": "After obtaining the pre-trained encoder, we utilize an imitation learning framework to learn visuomotor policy for dexterous manipulation tasks. Specifically, we adopt conditional denoising diffusion model [22, 7, 35] as our backbone, which conditions on 3D visual features \u03a6H,O and robot states features \u03a8A,H. Beginning with a Gaussian noise AK, the denoising network \u03f5\u03b8 performs k iterations to gradually denoise AK into the noise-free action A0:\n$A^{k-1} = \\alpha_k(A^k \u2013 \\gamma_k \\epsilon_\\theta (\\Phi^{H,O}, \\Psi^{A,H}, A^k, k)) + \\sigma_k \\mathcal{N}(0, I)$, (6)\nwhere N(0, I) is Gaussian noise, \u03b1k, \u03b3k and \u03c3k are functions of k, determined by the noise scheduler. This formulation allows the model to capture the distribution of action without the cost of inferring future states.\nWe use the DDIM scheduler [47] to accelerate the inference speed in real-world experiments. The training objective is to predict the noise added to the original data:\n$L = MSE(\\epsilon_k, \\epsilon_\\theta (\\tilde{\\alpha}_k A^0 + \\beta \\epsilon_k, \\phi^{H,O}, \\psi^{A,H}, k))$ (7)\nUnlike the original diffusion-based policy, we incorporate consistency-related features as a condition for policy learning using the pre-trained encoder and fine-tuning the encoder during downstream tasks training. This encoder implicitly extracts contact and coordination information, thereby enhancing the policy's understanding of spatial relationships."}, {"title": "IV. EXPERIMENTS", "content": "We conduct comprehensive real-world experiments to answer the following questions:\n\u2022\nTo what extent can our framework promote the learning of the visuomotor policy for dexterous manipulation across diverse real-world scenarios (Section IV-B)?\n\u2022\nHow promising is CordViP in terms of sample efficiency and generalization capability (Section IV-C, IV-D)?\n\u2022\nWhat role does each of the system components play in enhancing its overall performance (Section IV-E, IV-F)?"}, {"title": "V. CONCLUSIONS AND LIMITATIONS", "content": "In this paper, we present CordViP, a novel framework that learns correspondence-based visuomotor policy for dexterous manipulation in the real world. First, we leverage robust 6D pose estimation of objects and robot proprioception to"}, {"title": "APPENDIX", "content": "A. Real-World Task Description\nPickPlace: This task requires coordinated motion of the robot's hand and arm, involving all four fingers. The Leaphand first locates the position of the toy chicken based on visual input. The palm then approaches the chicken, with all four fingers gradually wrapping around it. Once the chicken is grasped, the wrist is raised, and the hand is moved towards a blue bowl. Upon reaching a position directly above the bowl, the fingers are released, allowing the chicken to be placed inside. This task presents challenges in accurately locating and grasping the toy chicken, coordinating finger movements for a stable grip, and precisely manipulating the hand to place the object into the bowl while avoiding obstacles and ensuring a gentle release. Success is achieved if the toy chicken is placed into the bowl.\nFlipCup: This task requires the robot to flip a cup from a lying position on the table to an upright standing position. The Leaphand approaches the cup and places its hand on the top of the cup, lifts it, and then flips it to an upright position. The hand must apply controlled force to rotate the cup while maintaining stability to prevent it from tipping over. The challenge lies in the fact that the cup may undergo changes in its orientation, requiring the hand to dynamically adjust its position and force to stabilize the cup's motion and achieve the desired outcome. Success is achieved when the cup stands upright on the table.\nAssembly: This task requires the robot to assemble a cylindrical cup onto a kettle. The Leaphand first approaches the cup and grasps it, positioning it accurately to align with the kettle's opening. Using coordinated finger and wrist movements, the hand carefully attaches the cup to the kettle, ensuring a secure fit. The challenges lie in precise alignment and making fine adjustments based on feedback while handling high occlusion and ambiguity. Success is achieved when the cup is securely assembled onto the kettle.\nArtiManip: This task requires the robot to open a box using its thumb and index fingers. The robot needs to first reach the box, grasp the box's lid, and then carefully adjust its fingers to open the box without pushing it. The hand must coordinate the motion of the thumb with fine adjustments to apply the right amount of force, ensuring the lid is opened smoothly and safely. This task presents a challenge in handling articulated objects with multiple moving parts while maintaining delicate control over the manipulation process. Success is achieved when the lid is fully open.\nWe list the parameters of expert demonstrations for different tasks in Table IX. For all demonstrations of a given task, we maintain a consistent number of steps. \"Demo\" refers to the number of demonstrations collected for each task, \"Episode Length\" denotes the duration of each episode in a task, \u201cTeleop. Times\u201d indicates the teleoperation time required to collect a single demonstration, and \u201cMax Steps\u201d represents the maximum execution time for a task during evaluation."}, {"title": "B. Implementation Details", "content": "Network Architecture. For point cloud encoding, we first use PointNet[38] to process point cloud data without RGB information, outputting a set of point feature vectors at the dimension of 1024. The PointNet consists of three fully connected layers, each followed by LayerNorm for normalization and ReLU activation.\nFor the cross-attention transformer, we adopted the architecture design from Eisner et al. [12], using a multi-head attention block of 4 heads. The state features of the robotic arm and the dexterous hand are each passed through a linear layer, mapped to 16 dimensions. The features are then processed through the same Transformer architecture for cross-attention, enabling feature fusion. The fused features are subsequently combined with the original features using a residual connection.\nDemonstrations Process. We utilize the RealSense L515 camera to capture RGB-D images with a resolution of 480 \u00d7 640. The depth data are aligned with the RGB data to ensure accurate spatial correspondence. All data collection is managed through ROS and data recording begins once both the camera feed and robot teleoperation inputs are received. For our method, we use only RGB and depth data to track the object's pose. In contrast, for other baselines, we synthesize the point cloud from RGBD data, and both the pose and the point clouds are transformed into the world coordinate system. We crop point clouds with the range of x \u2208 [\u22120.4m, 0.1m], y \u2208 [-0.7, -0.4], z \u2208 [0.1,0.51], which has been verified to be suitable for observation. as shown in Figure 9."}, {"title": "Normalizations.", "content": "The range of training data has a significant impact on the training stability of CordViP. We linearly scale the minimum and maximum values of each action and observation dimension to the range of [-1, 1]. This step is necessary for DDIM [47] and DDPM [22], as they clip the predicted results to the range of [-1, 1] for training stability."}, {"title": "C. Failure Analysis of Baselines", "content": "3D Diffusion Policy. DP3 [66] appears to struggle in learning meaningful actions from our demonstration data. We have already analyzed in the paper that potential reasons include factors such as the camera's viewpoint and the quality of the point cloud. Wang et al. [53] also points out that the type of motion patterns can affect the quality of demonstration learning. Instead of nature actions, axis-wise actions were used in DP3's demonstration data. This is because the robotic arm is controlled by the keyboard, which inherently limits the motion representation to axis-wise actions."}]}