{"title": "Enhancing Temporal Action Localization: Advanced S6 Modeling with Recurrent Mechanism", "authors": ["Sangyoun Lee", "Juho Jung", "Changdae Oh", "Sunghee Yun"], "abstract": "Temporal Action Localization (TAL) is a critical task in video analysis, identifying precise start and end times of actions. Existing methods like CNNs, RNNs, GCNs, and Transformers have limitations in capturing long-range dependencies and temporal causality. To address these challenges, we propose a novel TAL architecture leveraging the Selective State Space Model (S6). Our approach integrates the Feature Aggregated Bi-S6 block, Dual Bi-S6 structure, and a recurrent mechanism to enhance temporal and channel-wise dependency modeling without increasing parameter complexity. Extensive experiments on benchmark datasets demonstrate state-of-the-art results with mAP scores of 74.2% on THUMOS-14, 42.9% on ActivityNet, 29.6% on FineAction, and 45.8% on HACS. Ablation studies validate our method's effectiveness, showing that the Dual structure in the Stem module and the recurrent mechanism outperform traditional approaches. Our findings demonstrate the potential of S6-based models in TAL tasks, paving the way for future research.", "sections": [{"title": "1. Introduction", "content": "Temporal Action Localization (TAL) is a crucial video analysis task that identifies the precise start and end times of actions in videos. As video content becomes increasingly complex and abundant, accurate TAL methods are essential for effectively capturing and analyzing meaningful actions in applications like sports analytics, surveillance, and interactive media. However, significant challenges remain in TAL, particularly in effectively capturing long-range dependencies and temporal causality in video data.\nTraditional approaches to TAL, including CNNS, RNNS, GCNs, and Transformers, each bring unique strengths but also have inherent limitations. CNNs are effective at capturing spatial features but struggle with long-range dependencies due to limited receptive fields. RNNs can model temporal sequences but face challenges such as vanishing gradients, which hinder their ability to capture long-term dependencies. GCNs are powerful for relational data but are not inherently designed for sequential temporal data. Transformers have revolutionized TAL with their ability to model global context using self-attention mechanisms. However, their reliance on attention scores to capture relationships within a sequence does not inherently account for the temporal causality and history of visual elements over time. This limitation makes Transformers less optimal for tasks requiring precise temporal causality, such as TAL, where understanding the sequential nature and dependencies of actions is crucial.\nThe State Space Model (SSM) has emerged as a promising alternative for sequence modeling by addressing the limitations of traditional methods, especially in capturing temporal causality. Within the SSM framework, the Selective State Space Model (S6) stands out for TAL tasks due to its ability to maintain and leverage historical context through its selection mechanism and gating operation. These properties enable S6 to dynamically adjust the influence of new inputs-specifically, the spatiotemporal feature vectors extracted from the current video clip ensuring that the model retains and utilizes critical temporal information while integrating new data. This dynamic adjustment and selective retention enable S6 to capture long-range dependencies and temporal causality effectively, providing understanding of action sequences essential for accurately pinpointing the start and end times of actions in TAL.\nActionMamba, an S6-based TAL method, has demonstrated that S6-based method can surpass Transformers in sequence modeling by replacing Transformer blocks with S6 blocks. ActionMamba simply substitutes the Transformer-based blocks for sequence modeling in the ActionFormer architecture with S6-based blocks. The S6 blocks use a bi-directional processing approach and incorporate weight sharing between networks operating in each direction. However, this study lacks a thoughtful design focused on effective TAL methods, instead offering a straightforward replacement of Transformer blocks with slightly enhanced S6 ones. While ActionMamba highlights the potential for S6-based sequence modeling to outperform Transformer-based approaches, it falls short of fully exploring this potential or providing clear guidelines for leveraging S6 effectively in TAL tasks.\nOur research aims to explore the potential of S6-based TAL methods by building on insights from previous studies on CNNs, RNNs, GCNs, and Transformers. We propose a novel architecture that leverages the strengths of these traditional models while capitalizing on the unique capabilities of S6.\nThis paper makes the following contributions to the field of TAL:\n1.  Advanced Dependency Modeling with S6: We conduct a pioneering exploration of S6's potential in TAL tasks, particularly focusing on its dependency modeling capabilities. By introducing an advanced dependency modeling technique based on the Feature Aggregated Bi-S6 (FA-Bi-S6) block design and the Dual Bi-S6 structure, we enable robust and effective modeling of dependencies within video sequences. The FA-Bi-S6 block employs multiple Conv1D layers with different kernel sizes to capture various granularities of temporal and channel-wise features, while the Dual Bi-S6 structure processes features along both the temporal and channel dimensions to enhance the integration of spatiotemporal dependencies. This approach provides direction for TAL modeling, enabling more effective utilization of S6 in this domain.\n2.  Efficiency through Recurrent Mechanism: Our study reveals that using a recurrent mechanism to repeatedly apply a single S6-based model outperforms the traditional approach of stacking multiple blocks. This recurrent application enhances the model's performance without increasing the number of parameters, providing an effective solution for improving TAL models.\n3.  State-of-the-Art Performance: We achieve state-of-the-art (SOTA) results across multiple benchmark datasets, including THUMOS-14, ActivityNet, FineAction, and HACS. Our ablation studies analyze the effectiveness of each component of our proposed architecture, confirming the performance improvements brought by our method."}, {"title": "2. Related works", "content": "Convolutional Neural Networks (CNNs) Early TAL research used 2D CNNs to process spatial information, with initial attempts like FV-DTF combining spatial and temporal data but handling them separately. The introduction of 3D CNNs, as seen in CDC , marked a significant advancement by capturing spatiotemporal features with three-dimensional convolution kernels. However, temporal resolution loss inherent in traditional 3D CNNs was still a challenge to conquer. To cope with this, methods such as TPC and FSN aimed to balance spatial and temporal feature processing. GTAN and PBRNet further optimized temporal intervals and hierarchical feature extraction. TPC maintained temporal receptive fields while downsampling spatial fields, and FSN captured finer-grained dependencies by sequentially processing spatial and temporal features.\nOur FA-Bi-S6 block builds on these advances by incorporating multiple Conv1D layers with varying kernel sizes in parallel to capture a wide range of local contexts. The resulting feature map is processed bi-directionally by the Bi-S6 network, enhancing the model's ability to capture complex dynamics effectively.\nRecurrent Neural Networks (RNNs) To address the temporal challenges that CNNs alone couldn't solve, RNNs were integrated into TAL frameworks. Early efforts like PSDF and advancements such as AS used RNNs to enhance temporal context modeling from dense trajectories and refine spatial features for detailed analysis. More sophisticated integrations followed, such as GRU-Split , which employed GRUs to refine action boundaries and probabilities. However, RNNs introduced challenges like managing long sequences and vanishing gradients. RCL addressed these issues by using a recurrent module to dynamically adjust action segment predictions.\nOur research transcends the limitations of CNNs and RNNs by incorporating a recurrent mechanism within our S6-based architecture. This mechanism, integrated with our Backbone's Stem module, enhances temporal context modeling using the efficiency and precision of state space models.\nGraph Convolutional Networks (GCNs) The limitations of RNNs led to the exploration of GCNs in the TAL domain. GCNs structure video data as graphs, with nodes representing spatiotemporal features and edges defining their relationships, allowing for more comprehensive modeling of temporal dependencies. A notable advancement, P-GCN , expanded the range of dependencies that could be modeled but faced challenges in scalability and efficiency due to computational overhead. G-TAD addressed these issues with a dual-stream graph convolution framework, efficiently capturing both fixed and adaptive temporal dependencies. Building on GCN insights, we developed the Dual Bi-S6 structure, integrating the TFA-Bi-S6 and CFA-Bi-S6 blocks. TFA-Bi-S6 captures temporal dependencies, while CFA-Bi-S6 handles dependencies between spatiotemporal features by focusing on the channel dimension. This combined approach enhances the robustness and accuracy of TAL by effectively modeling both temporal and channel-wise contexts.\nTransformers The limitations of GCNs in handling extensive temporal dependencies led to the adoption of"}, {"title": "3. Proposed Methods", "content": "We introduce our approach, emphasizing advanced dependency modeling for TAL by integrating the S6 model to improve long-range dependency handling. Our key components include the Feature Aggregated Bi-S6 Block Design, Dual Bi-S6 Structure, and Recurrent Mechanism.\n3.1. Preliminary: Selective Space State Model (S6)\nOur architecture uses the S6 model with selective mechanisms and gating operations to capture complex temporal dynamics and capture long-range dependencies effectively. The S6 model operates with parameters ($\\Delta_t, A, B, C$), discretized to manage sequence transformations:\n$\\begin{aligned}h_t&= A h_{t-1}+ B x_t, \\ Y_t &= C h_t\\end{aligned}$\nHere, $x_t$ represents the input at time step $t$, which, in the case of TAL, is the spatiotemporal feature vector extracted from single clip. The hidden state at time step $t$, $h_t$, captures the temporal context of the sequence. The output at time step $t$, $y_t$, represents the processed feature. The state matrix $A$ determines how the previous hidden state $h_{t-1}$ and the historical information from all previous steps influence the current hidden state , contributing to precise action localization. The input matrix $B$ defines how the input $x_t$ affects the hidden state $h_t$. Finally, the output matrix $C$ translates the hidden state $h_t$ into the output $Y_t$.\nThe process starts with the input $x_t$ being projected to derive $B$, $C$, and $\\Delta_t$. This step transforms raw input features into suitable representations for state-space modeling. Specifically, the projection functions apply linear transformations to the input $x_t$:\n$B = \\text{Linear}(x_t), C = \\text{Linear}(x_t)$\nTo dynamically manage information flow, the S6 model employs selection mechanism and gating function. The dynamically adjusted parameter $A_t$ controls the discretization of the state-space model based on the relevance of the input $X_t$, functioning similarly to a gating mechanism in RNNs. The projection function $s_{\\Delta}(x_t)$, which includes learnable parameters, projects the input $x_t$ to one dimension before broadcasting it across channels:\n$A_t = \\text{softplus}(s_{\\Delta}(x_t))$\nNext, the discretization step adjusts the parameters $A$ and $B$ for the current time step $t$, ensuring that the parameters are appropriately scaled for discrete-time processing:\n$\\begin{aligned}A_t &= \\text{exp}(\\Delta_tA) \\ B_t &= (A \\Delta_t)^{-1}(\\text{exp}(\\Delta_t A) - I) \\cdot \\Delta_t B\\end{aligned}$\nThe hidden state $h_t$ is updated using $A_t$ and $B_t$, and the output $y_t$ is generated using $C_t = C$:\n$h_t = A_th_{t-1}+B_tx_t, \\quad Y_t = C_th_t$\nThe selective update of the hidden state can be understood as:\n$h_t = (1 - \\Delta_t)h_{t-1} + \\Delta_tx_t$\nwhere $\\Delta_t$ functions similarly to the gating function $g_t$ in RNNs, determining the influence of the input $x_t$ on the hidden state $h_t$. This dynamic adjustment helps the model focus on relevant portions of the input, ensuring effective handling of long-range dependencies.\nS6 is particularly effective in TAL tasks due to its ability to maintain and refine temporal context over extended sequences. By dynamically adjusting $\\Delta_t$, the model can selectively retain important temporal features."}, {"title": "3.2. Overview", "content": "Our architecture, inspired by ActionFormer and ActionMamba, consists of four primary components: a Pretrained video encoder, a Backbone, a Neck, and Heads. The overview of architecture is depicted in Figure 1a.\nPretrained Video Encoder The Pretrained video encoder extracts spatiotemporal attributes from video clips. Trained on diverse datasets such as UCF, Kinetics, Something-Something, and vision-language multi-modal datasets like WebVid and InternVid, it leverages the vast training data from InterVideo2-6B/1B . The pretrained video encoder's example of receiving each clip and extracting spatiotemporal features is shown in Appendix A.\nBackbone The Backbone captures dependencies and extracts features at various temporal resolutions from the sequence data. As illustrated in Figure 1a, it consists of three main modules:\n\u2022 Embedding Module: This module captures the coarse local context of spatiotemporal features. As shown in Figure 2a, the sequence is first passed through a Conv1D to increase the dimensionality from $C_{in}$ to $C_{emb}$, followed by Layer Normalization (LN) and ReLU activation. This process is followed by $B_e$ sequential Conv1D with dimensions $C_{emb}$ to $C'_{emb}$, each followed by LN and ReLU activation, resulting in an embedded sequence of shape $[B, C'_{emb}, L]$.\n\u2022 Stem Module: This core component processes the embedded sequences to capture long-range dependency using the Dual Bi-S6 Structure. As shown in Figure 2b, it applies two main blocks in parallel: the Temporal Feature Aggregated Bi-S6 (TFA-Bi-S6) block and the Channel Feature Aggregated Bi-S6 (CFA-Bi-S6) block, which focus on capturing temporal and channel-wise dependencies, respectively. Each of these blocks is stacked $B_s$ times. The TFA-Bi-S6 block handles input sequences reshaped from $[B, C_{emb}, L]$ to $[B, L, C_{emb}]$ and outputs back to $[B, C'_{emb}, L]$. The CFA-Bi-S6 block processes the temporal-pooled output of TFA-Bi-S6 with shape $[B, C_{emb}, 1]$ and scales it using a sigmoid activation. The outputs from these blocks are combined through point-wise multiplication with the TFA-Bi-S6 output. This combined output then goes through an affine transformation with a drop path and skip connection, followed by LN to enhance capacity. This process uses a Recurrent Mechanism, repeating $r$ times, with a weight-shared network applied at each repetition to refine temporal dependency modeling.\n\u2022 Branch Module: This module handles temporal multi-scale dependencies. As shown in Figure 2c, each branch applies the Temporal Bi-S6 (T-Bi-S6) block, which is a modified version of the Bi-S6 block used in ActionMamba , followed by an affine drop path and residual connection. After this, the output undergoes LN and max pooling along the temporal dimension, effectively obtaining various temporal resolutions. The T-Bi-S6 block processes the input sequence reshaped from $[B, C'_{emb}, L/2^{d-1}]$ to $[B, L/2^{d-1}, C_{emb}]$ and outputs back to $[B, C'_{emb}, L/2^{d-1}]$. This process is repeated for each downsampling index $(d = 1, 2, ..., 5)$, where the output shape becomes $[B, C'_{emb}, L/2^d]$.\nNeck and Heads The Neck is designed with simplicity and efficiency in mind, utilizing layer normalization for channel-wise normalization, which is the same as the LN used in the Branch module. This step ensures that the temporal multi-scale sequences reflecting precise temporal dependencies processed by the Backbone are normalized and ready for subsequent processing.\nThe Heads leverage the normalized features from the Neck to carry out two primary tasks: action classification and temporal boundary regression. The action classification head generates channels equal to the number of action categories, predicting class scores for each category. Simultaneously, the temporal boundary regression head outputs two channels to predict the frame indices marking the start and end of an action. This dual-head design ensures that the model can accurately classify actions and determine their temporal boundaries within the video segments."}, {"title": "3.3. Advanced Dependency Modeling for TAL", "content": "Feature Aggregated Bi-S6 (FA-Bi-S6) Block Design The FA-Bi-S6 block design is one of our contributions, enabling robust and effective modeling of dependencies within video sequences. This block design incorporates multiple Conv1D layers, each with different kernel sizes, operating sequentially within two main blocks: the TFA-Bi-S6 block and the CFA-Bi-S6 block, as shown in Figure 3a and 3b.\nIn the TFA-Bi-S6 block, the input sequence of shape $[B, C_{emb}, L]$ is first passed through a linear layer that adjusts the dimensions from $[B, C_{emb}, L]$ to $[B, L, 2C_{emb}]$. The sequence is then divided into two chunks, and one of these chunks is flipped. These chunks are processed through multiple Conv1D layers with varying kernel sizes (2, 3, 4), each capturing different granularities of temporal features. The outputs from these Conv1D layers are summed to create an aggregated feature map, which is then processed through a S6 network focusing on temporal dependencies. The output of the S6 blocks is then multiplied pointwise with the original chunked input processed through the SiLU activation. The results from each chunk are concatenated, which handle bi-directional temporal dependencies. The final output is obtained by combining the results, which are then processed through a linear layer and reshaped back to $[B, C_{emb}, L]$.\nIn the CFA-Bi-S6 block, the process is similar to the TFA-Bi-S6 block with adaptations for channel-wise dependency modeling. The input sequence is first adaptively pooled to $[B, C_{emb}, L_a]$ before the linear layer processing. The Conv1D layers in this block have varying kernel sizes (2, 4, 8) to capture different scales of channel-wise dependencies. After processing through the S6 blocks and linear layer, the final output is average pooled to $[B, C_{emb}, 1]$. These adjustments enable the CFA-Bi-S6 block to focus on capturing diverse channel-wise dependencies and enhance the overall capacity to model complex spatiotemporal interactions within video sequences.\nBy integrating the Bi-S6 block with the aggregated feature map, our design leverages the strengths of both multi-scale feature extraction and bi-directional processing. The combined architecture allows the model to effectively capture and utilize spatiotemporal features across a wide range of context, addressing the limitations of traditional single convolutional approaches. This design is particularly advantageous for TAL tasks, where actions may occur over varying temporal spans, and the local context provided by surrounding frames is crucial for accurate localization.\nDual Bi-S6 Structure The Dual Bi-S6 structure is a novel component of our proposed architecture, designed to enhance the modeling of spatiotemporal dependencies by processing features along both the temporal and channel dimensions. This dual-path approach ensures that the model can capture and integrate the rich contextual information present in video sequences, thereby improving the accuracy of TAL.\nAs shown in Figure 2b, the Dual Bi-S6 structure consists of two parallel paths: the TFA-Bi-S6 and the CFA-Bi-S6. Each path processes the input sequence differently to extract complementary information. The TFA-Bi-S6 reflects temporal dynamics within the video sequence, providing a detailed temporal analysis of the input. Simultaneously, the CFA-Bi-S6 captures the interactions between different spatiotemporal features, and its output is then scaled using a sigmoid function to transform the values into a range suitable for modulation.\nAfter processing the input through both paths, the outputs of the TFA-Bi-S6 and CFA-Bi-S6 are combined using point-wise multiplication. This fusion step integrates the temporal dependencies captured by the TFA-Bi-S6 with the channel-wise dependencies modeled by the CFA-Bi-S6. The point-wise multiplication ensures that the combined features reflect both types of dependencies, with the TFA-Bi-S6 handling global dependencies between clips and the CFA-Bi-S6 addressing local dependencies between spatiotemporal features within clips. The design intention behind this structure is to leverage the strengths of both paths: the TFA-Bi-S6 captures temporal dependencies and dynamics, while the CFA-Bi-S6 emphasizes the relationships between spatiotemporal features. By scaling the output of the CFA-Bi-S6 and multiplying it with the TFA-Bi-S6 output, the model effectively combines temporal analysis with channel-wise context, leading to a more comprehensive understanding of the video.\nRecurrent Mechanism This mechanism, integrated with our Stem module in the Backbone, enhances the accuracy of temporal context modeling by leveraging the efficiency and precision of state space models. As shown in Figure 2b, the process begins by passing the input sequence through the Stem module to capture initial temporal dependencies. The output is combined with the original input sequence and reprocessed by the Stem module, repeating this process $r$ times. Each iteration refines the temporal dependencies further, enhancing the model's ability to capture long-range dependencies and intricate temporal patterns. This recurrent mechanism provides a robust framework for refining temporal context, allowing the model to improve its understanding of temporal dependencies dynamically.\nThe effectiveness of this recurrent mechanism in speech separation tasks highlights its potential for TAL tasks as well. In speech separation, recurrent mechanisms have proven to excel in capturing long-range dependencies and intricate temporal patterns. This iterative refinement process, which involves passing the input sequence through a module multiple times to capture and refine temporal dependencies, allows models to handle complex long-range dependencies with greater precision. Such capabilities are directly applicable to TAL tasks, where identifying precise segments within a video also requires understanding temporal dependencies over extended periods."}, {"title": "4. Experiments", "content": "We provide a comprehensive evaluation of our TAL method through extensive experiments. We demonstrate its effectiveness using various benchmark datasets and conduct ablation studies to assess the impact of various components of our proposed approach.\n4.1. Evaluation on Benchmarks\nTo evaluate the effectiveness of the proposed method for TAL, we utilized the benchmark datasets THUMOS-14, ActivityNet, FineAction, and HACS. Detailed descriptions of each benchmark can be found in Appendix B. Table 1a presents experimental results on THUMOS-14. We compared our method with various approaches, including CNNs, RNNs, GCNs, Transformers-based, and the latest SOTA S6-based model. Our method achieved an average mAP of 74.2%, surpassing the previous SOTA by 1.5%. In Table 1b, we summarize our performance on ActivityNet. Despite its larger scale and variety of classes, which generally result in lower scores, our method achieved an average mAP of 42.9%, surpassing the previous SOTA by 0.9%. The outcomes on FineAction are presented in Table 1c. This benchmark, being relatively new, lacked RNN-based studies for comparison. Therefore, we included studies utilizing CNN, GCN, Transformer, and S6 models. FineAction's high class variety relative to its size makes it particularly challenging, generally resulting in lower mAP scores. Nonetheless, our approach achieved an average mAP of 29.6%, which is 0.6% higher than the previous SOTA. Finally, Table 1d displays our experimental performance on HACS. Most studies focused on Transformer-based approaches due to the dataset's large scale. Despite this, our proposed method achieved an average mAP of 45.8%, exceeding the previous SOTA by 1.2%.\n4.2. Ablation Studies\nStem module structure and Block quantities We investigated the impact of varying the structure of the Stem module and the number of blocks in the Embedding, Stem, and Branch modules to understand their effect on performance.\nThe results, presented in Table 2a, demonstrate the superiority of the Dual structure in the Stem module, which utilizes both temporal and channel blocks, consistently outperforming the Single structure that only uses the temporal block. This finding suggests that addressing both temporal and channel-wise dependencies provides a more comprehensive understanding for TAL. Additionally, using a single block in each module often yielded better performance than multiple blocks, indicating that simpler, less complex model structures help prevent overfitting and effectively capture essential spatiotemporal features. Notably, omitting the Stem module ($B_s$ = 0) results in a significant performance drop, highlighting its importance in sequence interpretation.\nKernel sizes and Aggregation methods We evaluated the performance impact of different kernel size combinations for TFA-Bi-S6 and CFA-Bi-S6 blocks and various aggregation methods using the Dual structure. This analysis, detailed in Table 2b, explores how different configurations influence the model's ability to capture temporal and channel-wise local context."}, {"title": "5. Conclusion", "content": "In this paper, we introduced a novel architecture leveraging S6 to provide effective solutions for TAL tasks based on insights from previous studies. By integrating the Feature Aggregated Bi-S6 block and the Dual Bi-S6 structure, our approach captures multi-scale temporal and channel-wise dependencies. The recurrent mechanism further refines temporal context modeling, enhancing performance without increasing parameter complexity. Consequently, our approach achieves state-of-the-art results on various benchmark datasets, with average mAP scores of 74.2% on THUMOS-14, 42.9% on ActivityNet, 29.6% on FineAction, and 45.8% on HACS. Additionally, ablation studies confirm the advantages of our design, demonstrating that the Dual structure in the Stem module outperforms the Single structure, the recurrent mechanism is more effective than merely stacking additional blocks, and Temporal Aggregation further boosts performance. These findings pave the way for future research to further explore the potential of state space models in TAL tasks."}, {"title": "A. Example of Pretrained Video Encoder Extracting Spatiotemporal Features from Each Clip", "content": "To understand the design intention of the Dual Bi-S6 structure, it is crucial to explain how the Pretrained video encoder extracts spatiotemporal features from each clip, clarifying the information contained in the sequence.\nFor instance, when processing the THUMOS dataset using the same Pretrained video encoder as ActionMamba, we start with RGB videos at 30 fps and a spatial resolution of 224x224. We segment 16 frames into a single clip, setting a frame interval of 4 (stride=4) between clips, yielding multiple clips from each video, each clip measuring [3, 16, 224, 224]. Within each frame, patches of size 14x14 are generated, producing 256 patch tokens per frame. Each patch token, representing spatial information and RGB channels, is flattened to a dimension of [256, 588]. These spatial tokens are projected to a channel size of 3200, forming patch embedding tokens with dimensions [16, 256, 3200]. Adding 3D sine-cosine positional embeddings to both the patch and frame dimensions, and then flattening these dimensions, results in position-embedded tokens with dimensions [4096, 3200]. Next, a proportion p of tokens is masked, and the channels are projected to 3200, followed by multihead self-attention and a feedforward layer with a hidden channel size of 12800, repeated 48 times to incorporate spatiotemporal context, resulting in contextual embedded tokens with dimensions [4096(1 \u2013 \u03c1), 3200]. Finally, multi-head self-attention and mean pooling are applied along the token dimension to produce an encoded feature vector with dimensions [1, 3200] for each clip. This process is repeated for all clips, stacking the encoded feature vectors sequentially over time to generate the sequence data, excluding the first and last two clips, which may lack video information, as shown in Figure 4."}, {"title": "B. Benchmark Datasets for Temporal Action Localization", "content": "To provide a comprehensive evaluation of TAL methods, we employ several benchmark datasets that vary in size, complexity, and focus. Here, we describe the key characteristics and evaluation metrics of the datasets utilized in this study:\nTHUMOS-14: This large-scale dataset is specifically designed for video action recognition and includes detailed temporal frame index annotations for 20 action classes. The primary evaluation metric for THUMOS-14 is mean Average Precision (mAP), which is calculated at various temporal Intersection over Union (tIoU) thresholds of 0.3, 0.4, 0.5, 0.6, and 0.7. This allows for a thorough assessment of the model's performance across different levels of temporal precision.\nActivityNet: Significantly larger and more complex than THUMOS-14, ActivityNet comprises approximately 20,000 videos spanning 200 action classes. The diverse range of classes in ActivityNet presents a more challenging scenario for TAL models. The mAP evaluation metric is also employed here, with tIoU thresholds set at 0.5, 0.75, and 0.95, providing a stringent test for action localization performance.\nFineAction: Consisting of around 16,000 videos featuring 106 action classes, FineAction emphasizes everyday activities and sports. The high variety of classes relative to its size makes it a particularly challenging dataset. Evaluation methods are akin to those used for ActivityNet, utilizing mAP scores at multiple tIoU thresholds.\nHACS (Human Action Clips and Segments): This extensive dataset includes approximately 50,000 videos covering 200 action classes, primarily capturing various actions from everyday life. Evaluation of the HACS dataset is conducted using the same methodology as ActivityNet, ensuring a consistent benchmark for comparing TAL model performance across different datasets.\nThese detailed descriptions of the datasets underscore the diverse and comprehensive nature of the benchmarks used in this study, providing a robust framework for evaluating the effectiveness of TAL methods."}]}