{"title": "Enhancing Temporal Action Localization: Advanced S6 Modeling with Recurrent Mechanism", "authors": ["Sangyoun Lee", "Juho Jung", "Changdae Oh", "Sunghee Yun"], "abstract": "Temporal Action Localization (TAL) is a critical task in video analysis, identifying precise start and end times of actions. Existing methods like CNNs, RNNs, GCNs, and Transformers have limitations in capturing long-range dependencies and temporal causality. To address these challenges, we propose a novel TAL architecture leveraging the Selective State Space Model (S6). Our approach integrates the Feature Aggregated Bi-S6 block, Dual Bi-S6 structure, and a recurrent mechanism to enhance temporal and channel-wise dependency modeling without increasing parameter complexity. Extensive experiments on benchmark datasets demonstrate state-of-the-art results with mAP scores of 74.2% on THUMOS-14, 42.9% on ActivityNet, 29.6% on FineAction, and 45.8% on HACS. Ablation studies validate our method's effectiveness, showing that the Dual structure in the Stem module and the recurrent mechanism outperform traditional approaches. Our findings demonstrate the potential of S6-based models in TAL tasks, paving the way for future research. Our code is available at https://github.com/lsy0882/RDFA-S6.", "sections": [{"title": "1. Introduction", "content": "Temporal Action Localization (TAL) is a crucial video analysis task that identifies the precise start and end times of actions in videos. As video content becomes increasingly complex and abundant, accurate TAL methods are essential for effectively capturing and analyzing meaningful actions in applications like sports analytics, surveillance, and interactive media [4, 15, 23, 43]. However, significant challenges remain in TAL, particularly in effectively capturing long-range dependencies and temporal causality in video data.\nTraditional approaches to TAL, including CNNS, RNNS, GCNs, and Transformers, each bring unique strengths but also have inherent limitations. CNNs are effective at capturing spatial features but struggle with long-range dependencies due to limited receptive fields [31]. RNNs can model temporal sequences but face challenges such as vanishing gradients, which hinder their ability to capture long-term dependencies [10, 26]. GCNs are powerful for relational data but are not inherently designed for sequential temporal data [17]. Transformers have revolutionized TAL with their ability to model global context using self-attention mechanisms [2, 32]. However, their reliance on attention scores to capture relationships within a sequence does not inherently account for the temporal causality and history of visual elements over time. This limitation makes Transformers less optimal for tasks requiring precise temporal causality, such as TAL, where understanding the sequential nature and dependencies of actions is crucial [11, 13].\nThe State Space Model (SSM) [11, 13] has emerged as a promising alternative for sequence modeling by addressing the limitations of traditional methods, especially in capturing temporal causality. Within the SSM framework, the Selective State Space Model (S6) [11] stands out for TAL tasks due to its ability to maintain and leverage historical context through its selection mechanism and gating operation. These properties enable S6 to dynamically adjust the influence of new inputs-specifically, the spatiotemporal feature vectors extracted from the current video clip ensuring that the model retains and utilizes critical temporal information while integrating new data. This dynamic adjustment and selective retention enable S6 to capture long-range dependencies and temporal causality effectively, providing understanding of action sequences essential for accurately pinpointing the start and end times of actions in TAL.\nActionMamba [7], an S6-based TAL method, has demonstrated that S6-based method can surpass Transformers in sequence modeling by replacing Transformer blocks with S6 blocks. ActionMamba simply substitutes the Transformer-based blocks for sequence modeling in the ActionFormer [42] architecture with S6-based blocks. The S6 blocks use a bi-directional processing approach [45] and incorporate weight sharing between networks operating in each direction. However, this study lacks a thoughtful design focused on effective TAL methods, instead offering a straightforward replacement of Transformer blocks with slightly enhanced S6 ones. While ActionMamba highlights the potential for S6-based sequence modeling to outperform Transformer-based approaches, it falls short of fully exploring this potential or providing clear guidelines for leveraging S6 effectively in TAL tasks.\nOur research aims to explore the potential of S6-based TAL methods by building on insights from previous studies on CNNs, RNNs, GCNs, and Transformers [10,17,26,31,32]. We propose a novel architecture that leverages the strengths of these traditional models while capitalizing on the unique capabilities of S6.\nThis paper makes the following contributions to the field of TAL:"}, {"title": "1. Advanced Dependency Modeling with S6:", "content": "We conduct a pioneering exploration of S6's potential in TAL tasks, particularly focusing on its dependency modeling capabilities. By introducing an advanced dependency modeling technique based on the Feature Aggregated Bi-S6 (FA-Bi-S6) block design and the Dual Bi-S6 structure, we enable robust and effective modeling of dependencies within video sequences. The FA-Bi-S6 block employs multiple Conv1D layers with different kernel sizes to capture various granularities of temporal and channel-wise features, while the Dual Bi-S6 structure processes features along both the temporal and channel dimensions to enhance the integration of spatiotemporal dependencies. This approach provides direction for TAL modeling, enabling more effective utilization of S6 in this domain."}, {"title": "2. Efficiency through Recurrent Mechanism:", "content": "Our study reveals that using a recurrent mechanism to repeatedly apply a single S6-based model outperforms the traditional approach of stacking multiple blocks. This recurrent application enhances the model's performance without increasing the number of parameters, providing an effective solution for improving TAL models."}, {"title": "3. State-of-the-Art Performance:", "content": "We achieve state-of-the-art (SOTA) results across multiple benchmark datasets, including THUMOS-14 [15], ActivityNet [4], FineAction [23], and HACS [43]. Our ablation studies analyze the effectiveness of each component of our proposed architecture, confirming the performance improvements brought by our method."}, {"title": "2. Related works", "content": "Convolutional Neural Networks (CNNs) Early TAL research used 2D CNNs to process spatial information, with initial attempts like FV-DTF [25] combining spatial and temporal data but handling them separately. The introduction of 3D CNNs, as seen in CDC [29], marked a significant advancement by capturing spatiotemporal features with three-dimensional convolution kernels. However, temporal resolution loss inherent in traditional 3D CNNs was still a challenge to conquer. To cope with this, methods such as TPC [37] and FSN [38] aimed to balance spatial and temporal feature processing. GTAN [24] and PBRNet [20] further optimized temporal intervals and hierarchical feature extraction. TPC maintained temporal receptive fields while downsampling spatial fields, and FSN captured finer-grained dependencies by sequentially processing spatial and temporal features.\nOur FA-Bi-S6 block builds on these advances by incorporating multiple Conv1D layers with varying kernel sizes in parallel to capture a wide range of local contexts. The resulting feature map is processed bi-directionally by the Bi-S6 network, enhancing the model's ability to capture complex dynamics effectively.\nRecurrent Neural Networks (RNNs) To address the temporal challenges that CNNs alone couldn't solve, RNNs were integrated into TAL frameworks. Early efforts like PSDF [40] and advancements such as AS [1] used RNNs to enhance temporal context modeling from dense trajectories and refine spatial features for detailed analysis. More sophisticated integrations followed, such as GRU-Split [16], which employed GRUs to refine action boundaries and probabilities. However, RNNs introduced challenges like managing long sequences and vanishing gradients. RCL [34] addressed these issues by using a recurrent module to dynamically adjust action segment predictions.\nOur research transcends the limitations of CNNs and RNNs by incorporating a recurrent mechanism within our S6-based architecture. This mechanism, integrated with our Backbone's Stem module, enhances temporal context modeling using the efficiency and precision of state space models.\nGraph Convolutional Networks (GCNs) The limitations of RNNs led to the exploration of GCNs in the TAL domain. GCNs structure video data as graphs, with nodes representing spatiotemporal features and edges defining their relationships, allowing for more comprehensive modeling of temporal dependencies. A notable advancement, P-GCN [41], expanded the range of dependencies that could be modeled but faced challenges in scalability and efficiency due to computational overhead. G-TAD [36] addressed these issues with a dual-stream graph convolution framework, efficiently capturing both fixed and adaptive temporal dependencies.\nBuilding on GCN insights, we developed the Dual Bi-S6 structure, integrating the TFA-Bi-S6 and CFA-Bi-S6 blocks. TFA-Bi-S6 captures temporal dependencies, while CFA-Bi-S6 handles dependencies between spatiotemporal features by focusing on the channel dimension. This combined approach enhances the robustness and accuracy of TAL by effectively modeling both temporal and channel-wise contexts.\nTransformers The limitations of GCNs in handling extensive temporal dependencies led to the adoption of Transformer-based models in TAL. Transformers use self-attention to extend temporal dependencies beyond GCN constraints. TRA [44] used variable temporal boundary proposals with multi-head self-attention for flexible temporal modeling, though it faced challenges in maintaining temporal causality over long sequences. ActionFormer [42] improved on this by using local self-attention and a multiscale feature pyramid to capture various temporal resolutions, but it still struggled with capturing long-range dependencies and maintaining precise temporal causality.\nTo address these issues, we introduced the S6 network into our TAL system. The S6 network uses selective mechanisms and gating functions to modulate the impact of each time step's spatiotemporal features. This approach allows S6 to preserve critical historical information while integrating new spatiotemporal features, effectively capturing long-range dependencies and temporal causality. By leveraging these capabilities, S6 enhances the accuracy of feature extraction and action localization, addressing the limitations of Transformer-based models in TAL."}, {"title": "3. Proposed Methods", "content": "We introduce our approach, emphasizing advanced dependency modeling for TAL by integrating the S6 model to improve long-range dependency handling. Our key components include the Feature Aggregated Bi-S6 Block Design, Dual Bi-S6 Structure, and Recurrent Mechanism."}, {"title": "3.1. Preliminary: Selective Space State Model (S6)", "content": "Our architecture uses the S6 model with selective mechanisms and gating operations to capture complex temporal dynamics and capture long-range dependencies effectively. The S6 model operates with parameters ($ \\Delta_t, A, B, C $), discretized to manage sequence transformations:\n$ h_t= Ah_{t-1}+ Bx_t, \n Y_t = Ch_t $\nHere, $ x_t $ represents the input at time step t, which, in the case of TAL, is the spatiotemporal feature vector extracted from single clip. The hidden state at time step t, $ h_t $, captures the temporal context of the sequence. The output at time step t, $ y_t $, represents the processed feature. The state matrix A determines how the previous hidden state $ h_{t-1} $ and the historical information from all previous steps influence the current hidden state $ h_t $ [12], contributing to precise action localization. The input matrix B defines how the input $ x_t $ affects the hidden state $ h_t $. Finally, the output matrix C translates the hidden state $ h_t $ into the output $ Y_t $.\nThe process starts with the input $ x_t $ being projected to derive B, C, and $ \\Delta_t $. This step transforms raw input features into suitable representations for state-space modeling. Specifically, the projection functions apply linear transformations to the input $ x_t $:\n$B = Linear(x_t), C = Linear(x_t)$\nTo dynamically manage information flow, the S6 model employs selection mechanism and gating function. The dynamically adjusted parameter $ \\Delta_t $ controls the discretization of the state-space model based on the relevance of the input $ X_t $, functioning similarly to a gating mechanism in RNNs. The projection function $ s_{\\Delta}(x_t) $, which includes learnable parameters, projects the input $ x_t $ to one dimension before broadcasting it across channels:\n$ \\Delta_t = softplus(s_{\\Delta}(x_t)) $\nNext, the discretization step adjusts the parameters A and B for the current time step t, ensuring that the parameters are appropriately scaled for discrete-time processing:\n$ A_t = exp(\\Delta_tA) $\n$ B_t = (\\Delta_tA)^{-1}(exp(\\Delta_tA) \u2013 I)\\cdot \\Delta_tB $\nThe hidden state $ h_t $ is updated using $ A_t $ and $ B_t $, and the output $ y_t $ is generated using $ C_t= C$:\n$h_t = A_th_{t-1}+B_tx_t, Y_t = C_th_t$\nThe selective update of the hidden state can be understood as:\n$ h_t = (1 - \\Delta_t)h_{t\u22121} + \\Delta_tx_t $\nwhere $ \\Delta_t $ functions similarly to the gating function $ g_t $ in RNNs, determining the influence of the input $ x_t $ on the hidden state $ h_t $. This dynamic adjustment helps the model focus on relevant portions of the input, ensuring effective handling of long-range dependencies.\nS6 is particularly effective in TAL tasks due to its ability to maintain and refine temporal context over extended sequences. By dynamically adjusting $ \\Delta_t $, the model can selectively retain important temporal features."}, {"title": "3.2. Overview", "content": "Our architecture, inspired by ActionFormer [42] and ActionMamba [7], consists of four primary components: a Pretrained video encoder, a Backbone, a Neck, and Heads. The overview of architecture is depicted in Figure 1a.\nPretrained Video Encoder The Pretrained video encoder extracts spatiotemporal attributes from video clips. Trained on diverse datasets such as UCF, Kinetics, Something-Something, and vision-language multi-modal datasets like WebVid and InternVid, it leverages the vast training data from InterVideo2-6B/1B [35]. The pretrained video encoder's example of receiving each clip and extracting spatiotemporal features is shown in Appendix A.\nBackbone The Backbone captures dependencies and extracts features at various temporal resolutions from the sequence data. As illustrated in Figure 1a, it consists of three main modules:\n\u2022 Embedding Module: This module captures the coarse local context of spatiotemporal features. As shown in Figure 2a, the sequence is first passed through a Conv1D to increase the dimensionality from $ C_{in} $ to $ C_{emb} $, followed by Layer Normalization (LN) and ReLU activation. This process is followed by $ B_e $ sequential Conv1D with dimensions $ C_{emb} $ to $ C_{emb}\u2019$, each followed by LN and ReLU activation, resulting in an embedded sequence of shape [B, $ C_{emb} $, L].\n\u2022 Stem Module: This core component processes the embedded sequences to capture long-range dependency using the Dual Bi-S6 Structure. As shown in Figure 2b, it applies two main blocks in parallel: the Temporal Feature Aggregated Bi-S6 (TFA-Bi-S6) block and the Channel Feature Aggregated Bi-S6 (CFA-Bi-S6) block, which focus on capturing temporal and channel-wise dependencies, respectively. Each of these blocks is stacked $ B_s $ times. The TFA-Bi-S6 block handles input sequences reshaped from [B, $ C_{emb} $, L] to [B, L, $ C_{emb} $] and outputs back to [B, $ C\u2019_{emb} $, L]. The CFA-Bi-S6 block processes the temporal-pooled output of TFA-Bi-S6 with shape [B, $ C_{emb} $, 1] and scales it using a sigmoid activation. The outputs from these blocks are combined through point-wise multiplication with the TFA-Bi-S6 output. This combined output then goes through an affine transformation with a drop path and skip connection, followed by LN to enhance capacity. This process uses a Recurrent Mechanism, repeating r times, with a weight-shared network applied at each repetition to refine temporal dependency modeling.\n\u2022 Branch Module: This module handles temporal multi-scale dependencies. As shown in Figure 2c, each branch applies the Temporal Bi-S6 (T-Bi-S6) block, which is a modified version of the Bi-S6 block used in Action-Mamba [7], followed by an affine drop path and residual connection. After this, the output undergoes LN and max pooling along the temporal dimension, effectively obtaining various temporal resolutions. The T-Bi-S6 block processes the input sequence reshaped from [B, $ C\u2019_{emb} $, L/2d\u22121] to [B, L/2d\u22121, $ C_{emb} $] and outputs back to [B, $ C\u2019_{emb} $, L/2d]. This process is repeated for each downsampling index (d = 1, 2, ..., 5), where the output shape becomes [B, $ C\u2019_{emb} $, L/2d].\nNeck and Heads The Neck is designed with simplicity and efficiency in mind, utilizing layer normalization for channel-wise normalization, which is the same as the LN used in the Branch module. This step ensures that the temporal multi-scale sequences reflecting precise temporal dependencies processed by the Backbone are normalized and ready for subsequent processing.\nThe Heads leverage the normalized features from the Neck to carry out two primary tasks: action classification and temporal boundary regression. The action classification head generates channels equal to the number of action categories, predicting class scores for each category. Simultaneously, the temporal boundary regression head outputs two channels to predict the frame indices marking the start and end of an action. This dual-head design ensures that the model can accurately classify actions and determine their temporal boundaries within the video segments."}, {"title": "3.3. Advanced Dependency Modeling for TAL", "content": "Feature Aggregated Bi-S6 (FA-Bi-S6) Block Design The FA-Bi-S6 block design is one of our contributions, enabling robust and effective modeling of dependencies within video sequences. This block design incorporates multiple Conv1D layers, each with different kernel sizes, operating sequentially within two main blocks: the TFA-Bi-S6 block and the CFA-Bi-S6 block, as shown in Figure 3a and 3b.\nIn the TFA-Bi-S6 block, the input sequence of shape [B, $ C_{emb} $, L] is first passed through a linear layer that adjusts the dimensions from [B, $ C_{emb} $, L] to [B, L, 2$ C_{emb} $]. The sequence is then divided into two chunks, and one of these chunks is flipped. These chunks are processed through multiple Conv1D layers with varying kernel sizes (2, 3, 4), each capturing different granularities of temporal features. The outputs from these Conv1D layers are summed to create an aggregated feature map, which is then processed through a S6 network focusing on temporal dependencies. The output of the S6 blocks is then multiplied pointwise with the original chunked input processed through the SiLU activation. The results from each chunk are concatenated, which handle bi-directional temporal dependencies. The final output is obtained by combining the results, which are then processed through a linear layer and reshaped back to [B, $ C_{emb} $, L].\nIn the CFA-Bi-S6 block, the process is similar to the TFA-Bi-S6 block with adaptations for channel-wise dependency modeling. The input sequence is first adaptively pooled to [B, $ C_{emb} $, $ L_a $] before the linear layer processing. The Conv1D layers in this block have varying kernel sizes (2, 4, 8) to capture different scales of channel-wise dependencies. After processing through the S6 blocks and linear layer, the final output is average pooled to [B, $ C_{emb} $, 1]. These adjustments enable the CFA-Bi-S6 block to focus on capturing diverse channel-wise dependencies and enhance the overall capacity to model complex spatiotemporal interactions within video sequences.\nBy integrating the Bi-S6 block with the aggregated feature map, our design leverages the strengths of both multi-scale feature extraction and bi-directional processing. The combined architecture allows the model to effectively capture and utilize spatiotemporal features across a wide range of context, addressing the limitations of traditional single convolutional approaches. This design is particularly advantageous for TAL tasks, where actions may occur over varying temporal spans, and the local context provided by surrounding frames is crucial for accurate localization.\nDual Bi-S6 Structure The Dual Bi-S6 structure is a novel component of our proposed architecture, designed to enhance the modeling of spatiotemporal dependencies by processing features along both the temporal and channel dimensions. This dual-path approach ensures that the model can capture and integrate the rich contextual information present in video sequences, thereby improving the accuracy of TAL.\nAs shown in Figure 2b, the Dual Bi-S6 structure consists of two parallel paths: the TFA-Bi-S6 and the CFA-Bi-S6. Each path processes the input sequence differently to extract complementary information. The TFA-Bi-S6 reflects temporal dynamics within the video sequence, providing a detailed temporal analysis of the input. Simultaneously, the CFA-Bi-S6 captures the interactions between different spatiotemporal features, and its output is then scaled using a sigmoid function to transform the values into a range suitable for modulation.\nAfter processing the input through both paths, the outputs of the TFA-Bi-S6 and CFA-Bi-S6 are combined using point-wise multiplication. This fusion step integrates the temporal dependencies captured by the TFA-Bi-S6 with the channel-wise dependencies modeled by the CFA-Bi-S6. The point-wise multiplication ensures that the combined features reflect both types of dependencies, with the TFA-Bi-S6 handling global dependencies between clips and the CFA-Bi-S6 addressing local dependencies between spatiotemporal features within clips. The design intention behind this structure is to leverage the strengths of both paths: the TFA-Bi-S6 captures temporal dependencies and dynamics, while the CFA-Bi-S6 emphasizes the relationships between spatiotemporal features. By scaling the output of the CFA-Bi-S6 and multiplying it with the TFA-Bi-S6 output, the model effectively combines temporal analysis with channel-wise context, leading to a more comprehensive understanding of the video.\nRecurrent Mechanism This mechanism, integrated with our Stem module in the Backbone, enhances the accuracy of temporal context modeling by leveraging the efficiency and precision of state space models. As shown in Figure 2b, the process begins by passing the input sequence through the Stem module to capture initial temporal dependencies. The output is combined with the original input sequence and reprocessed by the Stem module, repeating this process r times. Each iteration refines the temporal dependencies further, enhancing the model's ability to capture long-range dependencies and intricate temporal patterns. This recurrent mechanism provides a robust framework for refining temporal context, allowing the model to improve its understanding of temporal dependencies dynamically.\nThe effectiveness of this recurrent mechanism in speech separation tasks highlights its potential for TAL tasks as well. In speech separation, recurrent mechanisms have proven to excel in capturing long-range dependencies and intricate temporal patterns [6, 14]. This iterative refinement process, which involves passing the input sequence through a module multiple times to capture and refine temporal dependencies, allows models to handle complex long-range dependencies with greater precision. Such capabilities are directly applicable to TAL tasks, where identifying precise segments within a video also requires understanding temporal dependencies over extended periods."}, {"title": "4. Experiments", "content": "We provide a comprehensive evaluation of our TAL method through extensive experiments. We demonstrate its effectiveness using various benchmark datasets and conduct ablation studies to assess the impact of various components of our proposed approach."}, {"title": "4.1. Evaluation on Benchmarks", "content": "To evaluate the effectiveness of the proposed method for TAL, we utilized the benchmark datasets THUMOS-14 [15], ActivityNet [4], FineAction [23], and HACS [43]. Detailed descriptions of each benchmark can be found in Appendix B.\nTable 1a presents experimental results on THUMOS-14. We compared our method with various approaches, including CNNs, RNNs, GCNs, Transformers-based, and the latest SOTA S6-based model. Our method achieved an average mAP of 74.2%, surpassing the previous SOTA by 1.5%. In Table 1b, we summarize our performance on ActivityNet. Despite its larger scale and variety of classes, which generally result in lower scores, our method achieved an average mAP of 42.9%, surpassing the previous SOTA by 0.9%.\nThe outcomes on FineAction are presented in Table 1c. This benchmark, being relatively new, lacked RNN-based studies for comparison. Therefore, we included studies utilizing CNN, GCN, Transformer, and S6 models. FineAction's high class variety relative to its size makes it particularly challenging, generally resulting in lower mAP scores. Nonetheless, our approach achieved an average mAP of 29.6%, which is 0.6% higher than the previous SOTA. Finally, Table 1d displays our experimental performance on HACS. Most studies focused on Transformer-based approaches due to the dataset's large scale. Despite this, our proposed method achieved an average mAP of 45.8%, exceeding the previous SOTA by 1.2%."}, {"title": "4.2. Ablation Studies", "content": "Stem module structure and Block quantities We investigated the impact of varying the structure of the Stem module and the number of blocks in the Embedding, Stem, and Branch modules to understand their effect on performance.\nThe results, presented in Table 2a, demonstrate the superiority of the Dual structure in the Stem module, which utilizes both temporal and channel blocks, consistently outperforming the Single structure that only uses the temporal block. This finding suggests that addressing both temporal and channel-wise dependencies provides a more comprehensive understanding for TAL. Additionally, using a single block in each module often yielded better performance than multiple blocks, indicating that simpler, less complex model structures help prevent overfitting and effectively capture essential spatiotemporal features. Notably, omitting the Stem module ($B_s$ = 0) results in a significant performance drop, highlighting its importance in sequence interpretation.\nKernel sizes and Aggregation methods We evaluated the performance impact of different kernel size combinations for TFA-Bi-S6 and CFA-Bi-S6 blocks and various aggregation methods using the Dual structure. This analysis, detailed in Table 2b, explores how different configurations influence the model's ability to capture temporal and channel-wise local context.\nThe results show that using multiple kernel sizes for Conv1D layers in both TFA-Bi-S6 and CFA-Bi-S6 blocks improves performance, demonstrating the benefit of capturing a diverse range of local contexts at multiple scales for TAL. However, configurations with four or more kernel sizes per block resulted in decreased performance, likely due to overfitting, as the increased model complexity led to learning noise and less relevant patterns.\nThe absence of Conv1D layers led to reduced performance, underscoring the importance of capturing temporal and channel-wise local context through these layers. Furthermore, the Sum aggregation method outperformed the Concat method, indicating that summing feature maps effectively integrates information across different scales without adding excessive complexity.\nRecurrent mechanism iterations We examined the impact of varying the number of iterations r in the recurrent mechanism, along with the Dual structure and Feature Aggregation. This analysis, detailed in Table 2c, assesses how iterative refinement of temporal dependencies affects model performance compared to increasing the number of Stem blocks ($B_s$).\nThe results show that increasing the number of recurrent iterations r generally improves performance up to a certain point. Beyond this point, however, additional iterations resulted in a slight performance drop, likely due to an imbalance in temporal dependency. This suggests that there is an optimal number of iterations after which the benefits begin to diminish. In contrast, increasing the number of Stem blocks ($B_s$) while keeping r fixed at 1 led to a decrease in performance, indicating that simply adding more Stem blocks is not effective for improving TAL.\nThis comparison shows that adopting a recurrent approach, with $B_s$ set to 1 and increasing r, is more efficient and effective than stacking additional blocks. The recurrent mechanism improves temporal precision and long-range dependency modeling while optimizing memory usage, crucial for accurately understanding extended actions in video sequences and boosting performance, making it a practical strategy for TAL tasks using the S6-based model."}, {"title": "5. Conclusion", "content": "In this paper, we introduced a novel architecture leveraging S6 to provide effective solutions for TAL tasks based on insights from previous studies. By integrating the Feature Aggregated Bi-S6 block and the Dual Bi-S6 structure, our approach captures multi-scale temporal and channel-wise dependencies. The recurrent mechanism further refines temporal context modeling, enhancing performance without increasing parameter complexity. Consequently, our approach achieves state-of-the-art results on various benchmark datasets, with average mAP scores of 74.2% on THUMOS-14, 42.9% on ActivityNet, 29.6% on FineAction, and 45.8% on HACS. Additionally, ablation studies confirm the advantages of our design, demonstrating that the Dual structure in the Stem module outperforms the Single structure, the recurrent mechanism is more effective than merely stacking additional blocks, and Temporal Aggregation further boosts performance. These findings pave the way for future research to further explore the potential of state space models in TAL tasks."}]}