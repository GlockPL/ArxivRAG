{"title": "METASC: TEST-TIME SAFETY SPECIFICATION \u039f\u03a1\u03a4\u0399\u039c\u0399\u0396ATION FOR LANGUAGE MODELS", "authors": ["V\u00edctor Gallego"], "abstract": "We propose a novel dynamic safety framework that optimizes language model (LM) safety reasoning at inference time without modifying model weights. Building on recent advances in self-critique methods, our approach leverages a meta-critique mechanism that iteratively updates safety prompts-termed specifications-to drive the critique and revision process adaptively. This test-time optimization not only improves performance against adversarial jailbreak requests but also in diverse general safety-related tasks, such as avoiding moral harm or pursuing honest responses. Our empirical evaluations across several language models demonstrate that dynamically optimized safety prompts yield significantly higher safety scores compared to fixed system prompts and static self-critique defenses. Code to be released at github.com/vicgalle/meta-self-critique.", "sections": [{"title": "INTRODUCTION", "content": "Recent advances in language model safety have focused on training paradigms that enable models to reason about safety specifications. While approaches like Deliberative Alignment (Guan et al., 2024) have shown promising results by directly teaching models during training to reason about safety policies, less attention has been paid to optimizing these reasoning processes directly at inference time. This paper introduces a novel approach that builds upon these advances by performing online adaptation of safety specifications and reasoning patterns.\nThe key insight of our work is that while pre-training models with safety specifications provides a strong foundation, the effectiveness of the safety reasoning process can be further improved through further test-time computation. This is particularly relevant in real-world deployments where safety requirements may vary across contexts and evolve over time. Our approach enables models to refine their safety reasoning on the fly, without requiring tuning model parameters.\nOur work makes several key contributions: i) we introduce MetaSC, a meta-critique framework that optimizes safety reasoning prompts used in self-critique at inference time, enabling dynamic"}, {"title": "METASC: TEST-TIME SAFETY SPECIFICATION OPTIMIZATION", "content": "Given a prompt or instruction sequence, we can sample an initial response from the conditional distribution of the model, response ~ p(\u00b7 |prompt). The self-critique process (see e.g.,Madaan et al. (2024)) then first generates a critique, and then refines the original response according to the critique to further align it with a general principle or constitution, arriving at a revised response. The previous process can be stated as sampling from the following distributions\nresponse ~ p(\u00b7 | prompt)\ncritique ~ p( | prompt, response)\nrevision ~ p(. | prompt, response, critique),\nFor\nwhere each step uses the prior information to generate the corresponding sequence. safety tasks, for example, to generate the critique one may prompt the model with an in-\nstruction such as Identify specific ways in which your previous answer is\nharmful, unethical or illegal, followed by a another directive to revise the answer.\nOur first observation is that this process is similar to chain of thought variants (Wei et al., 2022), as some amount of inference-time computation is performed before sampling the final answer. Hence, in line with recent research in reasoning models (Chen et al., 2024; Guo et al., 2025), a natural question is how to make the self-critique process more effective.\nOur key innovation is the introduction of a meta-critique step that optimizes the critique and revision process, using test-time computation and without changing model' parameters. To do so, first we parameterize both the critique and revision prompts to depend on a textual variable, spec:\n\u2022 Identify specific ways in which your previous answer could\nimprove on the following criterion: {spec}.\n\u2022 Please, rewrite your original response using the previous\ncritique to improve on the following criterion: {spec}.\nNext, to enable online optimization of the spec, after we observe a sample trajectory (prompt, response, critique\u0165, revision\u0165) at a timestep t, a new safety specification spect+1 is pro-\nposed by an LLM acting as a meta-critic, introducing a final step in the self-critique process to arrive at our proposed MetaSC (Meta Self-Critique):\nresponse ~p(. | prompt\u2081)\ncritique ~ p( | prompt\u2081, responset, spect)\nrevisiont ~ p(\u00b7 | prompt\u2081, responset, critiquet, spect)\nspect+1~PMC(\u00b7 | prompt\u2081, response, critique\u0165, revisiont, spect)\nThis meta-level optimization allows the system to adapt its safety criteria based on observed inter-actions, effectively learning from its own reasoning process. The intuition is that by passing full trajectories of self-critique, we can perform prompt optimization, but instead of in the original task prompt, in the ones utilized by the critic. This final meta-critique step calibrates the guiding principle based on the model's prior behavior, ensuring that subsequent self-correction cycles adhere to a progressively refined safety criterion."}, {"title": "AN INTERPRETATION THROUGH THE LENS OF OPTIMIZATION", "content": "The LATRO framework (Chen et al., 2024) has been recently proposed as a self-guided optimization procedure for the chain of thought tokens before the final response. To enable this, they frame it as the following optimization problem:\n$\\max_\\theta E_{(x,y)\\sim D} [E_{z\\sim p_\\theta(\\cdot|x)} [R_\\theta(x, y, z)] \u2013 D_{KL}(P_\\theta(z|x)||p_0(z|x))]$\nwith (x, y) being ground-truth pairs of prompt and responses sampled from a dataset D, z being sampled chains of thought, $R_\\theta(x, y, z)$ can be the log-likelihood of the base LM or an alternative reward objective (such as safety of the generated response), and @ are the weights of the LM to be optimized. LATRO thus optimizes the weights of the models in order to improve the effectiveness of the sampled rationales z ~ po(\u00b7|x) before the final response y ~ p(y|x, z).\nThe proposed MetaSC approach takes a different path to improve the effectiveness of the critique process, since instead of tuning model weights, it searches over the discrete variable spec:\n$\\max_\\text{spec} E_{(x,y)\\sim D} [E_{z\\sim p(\\cdot|x,\\text{spec})} [R(x, y, z)] \u2013 D_{KL}(p(z|x, \\text{spec})||p(z|x, \\text{spec}_0))]$\nwhich we aim to optimize in an online fashion with a call to the meta-critique LM:\nspect+1~p( x, z, y, spect).\nNote that with MetaSC, the previous online procedure doesn't require ground-truth rewards. While this is the most simple configuration for deployment, we leave for further work the introduction of external reward models or verifiers in the optimization process."}, {"title": "EXPERIMENTS", "content": "To evaluate the effectiveness of our meta-critique framework, we conduct extensive experiments across two complementary settings. First, we assess our method's capability to defend against adversarial jailbreak attacks, comparing it with existing inference-time defenses. Second, we evaluate its performance on a broader set of safety-critical tasks using the BiGGen benchmark (Kim et al., 2024a), which encompasses diverse aspects of language model safety. Throughout our experiments, we utilize a diverse set of language models to demonstrate the generalizability of our approach."}, {"title": "DEFENSE AGAINST JAILBREAK ATTACKS", "content": "We aim to improve safety by avoiding harmful content from the LLM, using a collection of adversarial prompts from Harmful Behaviors (Zou et al., 2023), with the addition of jailbreak templates from Shen et al. (2024), leading to 52 test attacks. The jailbreak templates are role-play based (using natural language), thus rendering perplexity-based defenses obsolete. Hence, to compare our proposed method, we use the following two baselines that operate at inference-time: i) a system prompt defense (SP), which specifies You are a helpful yet harmless assistant that avoids\ngenerating illegal or harmful content; and ii) a self-critique defense (SC) with"}, {"title": "GENERAL SAFETY TASKS", "content": "We also assess our method on a set of tasks designed to evaluate various facets of response safety, using the BiGGen benchmark (Kim et al., 2024a). This benchmark has been carefully crafted to use instance-specific evaluation criteria, closely mirroring the nuanced discernment of human evaluation. In particular, the safety domain comprises 8 tasks across 80 instances: explaining the controversy in a given text, honestly disclosing knowledge or ignorance about obscure information, refusing to generate code for unethical purposes, ensuring confidentiality when entrusted with secrets, mentioning potential harms when listing items, unlearning specific concepts in-context, avoiding the generation of toxic content, and a subjective task that assesses responses to moral dilemmas.\nEach response is evaluated using the provided grading rubric in the benchmark, on a scale from 1 to 5 (most safe), using the llm-as-a-judge framework (Gu et al., 2024). We use the Prometheus LLM as the judge (Kim et al., 2024b). Table 5 reports the average safety ratings for three methods: a static system prompt (SP), static self-critique (SC), and our dynamic MetaSC, in which we define speco to be just the name of the task.\nAcross almost all tasks, MetaSC either matches or exceeds the performance of the other methods, yielding an overall improvement. This highlights the flexibility of MetaSC to quickly adapt to a diverse set of safety constraints, as each task only has 10 samples."}, {"title": "RELATED WORK", "content": "Research in inference-time reasoning and self-correction has evolved along several important di-rections. The Self-Refine approach established a foundation by implementing iterative feedback and refinement cycles using a single model for generation, critique, and revision (Madaan et al., 2024). Then, several self-correction approaches have emerged as effective techniques for improving responses during generation (Shinn et al., 2024; Shridhar et al., 2023; Ganguli et al., 2023). Re-cent work such as Critique Fine Tuning (Wang et al., 2025) deals with learning to critique towards mathematical tasks and modifying model weights.\nPrior work in language model safety primarily focuses on two key areas: safety training methods and jailbreak defense strategies. In the realm of safety training, researchers have traditionally relied"}, {"title": "CONCLUSIONS", "content": "In this paper, we introduced MetaSC, a novel framework for optimizing language model safety reasoning at inference time through dynamic specification updates. Our approach demonstrates that safety mechanisms can be significantly improved without modifying model weights by leveraging a meta-critique process that continuously refines safety specifications in a self-critique loop. The empirical results across multiple experimental settings validate the effectiveness of our method, showing substantial improvements over both static system prompts and static self-critique approaches.\nThe success of MetaSC in defending against jailbreak attacks is particularly noteworthy, as it achieved near-perfect safety scores on several large language models while requiring minimal computation overhead. Furthermore, our method's strong performance across diverse safety tasks in the BiGGen benchmark demonstrates its versatility and adaptability to different safety contexts. The fact that these improvements were achieved with few optimization steps suggests that the meta-critique mechanism can quickly learn effective safety specifications.\nFrom a theoretical perspective, our framework provides a new lens through which to view safety optimization, offering an alternative to weight-based approaches by instead focusing on the discrete optimization of safety specifications. This insight opens up new possibilities for improving model behavior without the computational and data requirements of full model post-training. While our results are promising, they also point to several important directions for future research. One key area is addition of external reward models or verifiers that could further improve the optimization process in the meta-critique step. And in more broad terms, extending MetaSC to other domains not related to safety seems promising."}]}