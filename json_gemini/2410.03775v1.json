{"title": "BEYOND CORRELATION: THE IMPACT OF HUMAN UNCERTAINTY IN MEASURING THE EFFECTIVENESS OF AUTOMATIC EVALUATION AND LLM-AS-A-JUDGE", "authors": ["Aparna Elangovan", "Ling Liu", "Jongwoo Ko", "Sravan Bodapati", "Lei Xu", "Mahsa Elyasi", "Dan Roth"], "abstract": "The effectiveness of automatic evaluation of generative models is typically measured by comparing it to human evaluation using correlation metrics. However, metrics like Krippendorff's $\\alpha$ and Randolph's $\\kappa$, originally designed to measure the reliability of human labeling, make assumptions about human behavior and the labeling process. In this paper, we show how relying on a single aggregate correlation score can obscure fundamental differences between human behavior and automatic evaluation methods, including LLM-as-a-Judge. Specifically, we demonstrate that when the proportion of samples with variation or uncertainty in human labels (gathered during human evaluation) is relatively high, machine labels (generated by automatic evaluation methods) may superficially appear to have similar or better correlation with the human majority label compared to human-to-human (HH) correlation. This can create the misleading impression that automatic evaluation is accurate enough to approximate the human majority label. However, as the proportion of samples with consistent human labels increases, the correlation between machine labels and human majority labels declines, falling below HH correlation. Based on these findings, we first propose stratifying results by human label uncertainty to provide a more robust analysis of automatic evaluation performance. Second, recognizing that uncertainty and variation are inherent in perception-based human evaluations, such as those involving attitudes or preferences, we introduce a new metric - binned Jensen-Shannon Divergence for perception for such scenarios to better measure the effectiveness of automatic evaluations. Third, we present visualization techniques \u2013 perception charts, to compare the strengths and limitations of automatic evaluation and to contextualize correlation measures appropriately.", "sections": [{"title": "1 INTRODUCTION", "content": "Despite the increasing importance of automatic evaluation for generative large language models (LLMs), measuring and interpreting the effectiveness of these methods remains challenging. Automatic evaluation methods include N-gram-based methods such as Rouge (Lin, 2004) and BERTScore (Zhang* et al., 2020), task-specific model-based methods (TM) such as Align-Score (Zha et al., 2023) and Prometheus (Kim et al., 2024), and LLM-as-a-Judge-based methods (LJ) where a general purpose LLM, using prompts, is instructed to perform evaluation. The use of automatic evaluation is tied to the key question \u2013 How can the efficacy of automatic evaluation methods be measured in comparison to human evaluation? A widely accepted practice is to use agreement or rank correlation metrics such as Krippendorff's-a (Krippendorff, 2011), Cohen's-\u043a (Cohen, 1960; McHugh, 2012), Spearman's $\\rho$, and Kendall's $\\tau$-b (Fabbri et al., 2021; Deutsch et al., 2022; Chiang & Lee, 2023; Liu et al., 2023b), where a higher correlation between machine labels (generated by automatic evaluation methods) and human labels (gathered during human evaluation) implies a more effective automatic evaluation. Variation or uncertainty in human labels (Tversky & Kahneman, 1974) is almost impossible to avoid when humans are involved (Kahneman et al., 2021). In this paper, we first highlight how comparing the overall correlation score between human labels and machine labels can obscure key discrepancies in machine generated labels. Therefore, we propose that measurements be stratified by label uncertainty."}, {"title": "2 BACKGROUND ON CORRELATION MEASURES", "content": ""}, {"title": "2.1 CORRELATION MEASUREMENTS AND ASSUMPTIONS", "content": "Correlation measures have traditionally been used to measure the inter-rater agreement (IRA) of human-generated labels, where there is no ground truth available, to measure human-to-human (HH) correlation. There are 2 broad categories of correlation measures, (a) non-chance-adjusted indices and (b) chance-adjusted metrics (Xinshu Zhao & Deng, 2013). Chance-adjusted measures take into account a scenario where raters can choose the same label by chance even when they do not know the correct answer. Chance-adjusted measures, such as Krippendorff's-a are generally more popular IRA measures than non-chance-adjusted metrics such as percentage agreement. The key difference between various chance-adjusted IRA measures is how chance agreement is computed (Meyer et al., 2014; Xinshu Zhao & Deng, 2013). For instance, Randolph's-\u043a (Randolph, 2005) assumes that all labels are equally likely and assigns a probability of 1/k for chance agreement, where k is the number of possible label choices. On the other hand, Fleiss's \u043a (Fleiss, 1971) and Krippendorff's $\\alpha$ calculate chance agreement empirically, based on the actual human labels of the study. These measurements can be useful in cases where certain labels (e.g., 3 on a Likert scale of 1-5) are more likely to be chosen when humans are uncertain. A high proportion of such labels, even if consistent, may indicate unreliability in the labeling process, particularly when the minority labels show lower agreement. Therefore, chance-adjusted correlation measures rely on key assumptions about the types of errors humans are likely to make. Furthermore, Krippendorff's-$\\alpha$, specifies that the agreement coefficient is an indicator of reliability if and only if (1) the raters duplicate the process of rating, i.e., when two independent raters use the same guidelines or instructions and use the same units, (2) raters must be treated as interchangeable, and (3) correct answers are not known (Krippendorff, 2004). Human labels and machine labels are clearly not interchangeable even when using current state-of-the-art LLMs to generate machine labels, as humans and LLMs are fundamentally different."}, {"title": "2.2 DIFFERENCES BETWEEN CORRELATION MEASURES AND RAW ACCURACY", "content": "The assumptions above, especially the assumption that raters must be treated as interchangeable, raise a fundamental question as to why even use correlation measures to compare machine and human performance, instead of measures such as accuracy and F1 using human labels as ground truth? Firstly, one of the main challenges with using scores like accuracy or F1 is that human labels tend to be noisy regardless of the task or the dataset, where some label choices are more acceptable than others, and measures such as F1 or accuracy do not account for variations in human labels and hence correlation measures appear to be a better measure to compare human vs. machine performance. However, as mentioned previously, many of the common chance-adjusted correlation measures aim to account for random chance agreement in HH agreement, whereas the errors make by machines tend to be more systematic. A key characteristic of random errors is that they are non-reproducible which result in variations when repeated, resulting in low IRA, which in-turn"}, {"title": "2.3 TYPES OF RAW DATA COLLECTED DURING EVALUATION", "content": "The type of the data collected, such as nominal, ordinal, interval, ratio, discrete or continuous data, dictate how such data can be aggregated and interpreted. Nominal and ordinal measures can be considered as qualitative measurements, while continuous values can be interpreted as quantitative (Mishra et al., 2018). The most common types of values collected during automatic or human evaluation are:\nNominal values: Nominal data, such as gender, have no natural ranking or ordering. Such data are primarily used in human evaluation tasks such as fact checking, where the aim is to verify whether a given text can be inferred from reference content (Liu et al., 2023c) and usually is modeled as a binary classification task. Preference judgments such as \u201cIs the output of model-A or Model-B better?\u201d are also common examples of nominal values collected during human evaluation. IRA measures for nominal values include percentage agreement (McHugh, 2012), Randolph's-k, Fleiss's-k, Krippendorff's-$\\alpha$ and Cohen's-\u043a.\nOrdinal values: Ordinal values have an implicit order, e.g., ratings of quality (very good, good, fair, poor, very poor) (Mishra et al., 2018). The most common use of ordinal values in human evaluation are through Likert scales. Likert scales are rating systems of attitude scale to measure the degree to which a person agrees/disagrees with a given statement (Taherdoost, 2019) and commonly to gauge user perception on qualitative aspects such as readability, coherence, and fluency (van der Lee et al., 2021). IRA measures for ordinal values should ideally account for the fact that ratings that are closer to one another (1 vs. 2) for a given item indicate a higher agreement than ratings that are further apart (1 vs. 4). Such IRA measures include Krippendorff's-$\\alpha$ for ordinal values, where disagreements are weighted by distance between the ratings. Rank correlations such as Kendall's-$\\tau$ and Spearman's-$\\rho$ are also widely used to measure item-level correlation between machines and humans for ordinal values. However, there's a caveat on using these measures: their ability to handle a relatively large proportion of ties is widely debated, even when measuring system correlations that involve a much smaller proportion of ties (Deutsch et al., 2023). When the number of items to rank is n, the number of ordinal values to assign is r, and when r << n, it results in many ties or rank collision. For instance, when assigning a Likert scale of 1-5 to 100 items, at least 20% of the items will have the same rank. While ordinal values preserve order, the objective of using ordinal values is not to rank the item in relation to other items, therefore the proportion of ties affects the interpretability and reliability of the rank correlation scores.\nContinuous values: Continuous values are typically obtained when using automatic evaluation methods such as Rouge (Lin, 2004) and BERTScore (Zhang* et al., 2020)."}, {"title": "3 ANALYSIS SETTINGS AND RESULTS", "content": "Datasets: We use datasets that have multiple human annotations used in typical evaluation tasks for (a) Likert-based ordinal qualitative evaluation, (b) NLI datasets for fact or consistency checking, and (c) preference datasets for understanding human preferences as follows: 1. SummEval (Fabbri et al., 2021) is human evaluation of summary quality using four criteria \u2013 fluency, coherence, consistency, and relevance, with Likert scores assigned from 1 to 5. This dataset has 1600 samples, where each item is labeled by 3 expert annotators. 2. SNLI (Bowman et al., 2015) is an NLI inference task dataset with 10K samples and 3 labels \u2013 entailment, neutral and contradiction. Each item is assigned 5 labels by human annotators. 3. MNLI (Williams et al., 2018) is also an NLI inference task dataset with 10K samples and 3 labels \u2013 entailment, neutral and contradiction. Each item is assigned 5 labels by human annotators. It contains two splits \u2013 matched and mismatched. 4. MT-Bench (Zheng et al., 2023) is a preference dataset which compares human preferences of several model outputs on open-ended questions using multi-turn conversations. From this dataset, we only use samples that have at least 3 human ratings.\nModels: For SummEval dataset, we reuse the original G-Eval results (Liu et al., 2023b) which rates the quality of summaries on a scale of 1-5. In addition, we also use Claude Sonnet 3 and Mistral"}, {"title": "3.1 RQ1: HOW DOES UNCERTAINTY IN HUMAN LABELS IMPACT CORRELATION METRICS WHEN WE MEASURE THE EFFICACY OF AUTOMATIC EVALUATION METHODS?", "content": "To study the impact of human uncertainty, we stratify samples and then compare HH and HM correlation measurements for each group. We primarily stratify by percentage agreement, which is the proportion of labels that reflect the majority (for nominal data) or the median label (for ordinal data) among all human labels assigned to an item. For example, if 3 out of 5 people assign the same label, the percentage agreement is 60%. We also stratify samples by the number of unique human labels to ensure that our findings are consistent regardless of the stratification method.\nAt surface level, HM correlation seems to improve with human certainty, as shown in Table 1, (column HwMW). On closer inspection, another trend emerges where in any given stratified group when the noisy or uncertain samples increases (as measured by low HH correlation), the HM correlation seems to outperform HH correlation as highlighted. However, as the proportion of samples with noisy labels decreases and HH correlation approaches near-perfect agreement, HH correlation is much higher than HM as highlighted. Thus, the illusion that machines (specifically LJs) approximate majority human behavior is largely due to the presence of noisy labels in the data. As a corollary, if LJs were truly approximating the human majority, the A would approach 0 under perfect HH correlation \u2013 which is clearly not the case. We also replicated this behavior on SummEval dataset which relies on ordinal labels in Table 2, using correlation metrics such as Krippendorff's-$\\alpha$ for ordinal data, Kendall's-\\T and Spearman's-$\\\rho$. The same patterns are observed on preference data in Table 3."}, {"title": "3.2 RQ2: HOW CAN WE MEASURE HUMAN-MACHINE AGREEMENT THAT ACCOUNT FOR HUMAN UNCERTAINTY AS A RESULT OF VARIATION IN HUMAN PERCEPTION?", "content": "Judging the quality of a free-form model generated output, such as a summary, largely depends on human perception. As a result, the human ratings can vary depending on a wide range of factors"}, {"title": "3.3 RQ3: HOW CAN WE VISUALIZE THE UNDERLYING DATA TO DRAW MEANINGFUL INSIGHTS WHEN WE COMPARE AUTOMATIC LABELS AND HUMAN LABELS?", "content": ""}, {"title": "3.3.1 PERCEPTION-BASED ORDINAL RATING VISUALIZATION", "content": "We propose the \"Ordinal HM perception comparison chart\" as shown in Fig. 3 to compare humans' perception of the content vs. how machines rate them. Here, we bin the samples by the human median rating and plot the corresponding distribution of human and machine ratings. For instance, say for a grading task using label values between 1-5, each item is rated 3 times by humans. For a sample set of 10 items, the total number of judgments would be 30 (10 items * 3 ratings per item). Let's say 7 items get a median rating of 4, these 7 items would fall in bin H = 4. We then plot the human label distribution for that bin to include all the 21 judgments (7 items * 3 ratings per item = 21), we also plot all the machine labels for the corresponding bin. We repeat this for each of the label from 1-5 as shown in Fig. 3. This allows us to visualize the variation in human perception, thus including all the ratings rather than a single gold rating, and compare how the corresponding machine labels vary.\nCorrelation numbers are challenging to interpret, and do not provide sufficient insights to the meaning behind the numbers unless supported by visualization. When does a correlation score imply the model is good enough to replace human evaluation as indicated in Table 2? And what are the gaps in the LJ? In Table 2, LJ Mistral achieves a Krippendorff's-$\\\\alpha$ HM of 0.49. Visualization in Fig. 3 shows that the LJ does not rate any item as 1 and negligible amount of items are rated 5, while humans have assigned over 24% of the items a median rating of 5. This shows the key difference between LJ Mistral's judgments and humans. This type of insight can potentially be useful in optimizing the prompts used by the LJ to minimize the gap with humans judgments, demonstrating the importance of effective visualization techniques.\nAnother key aspect to note is how human uncertainty varies across different median labels. Human labels tend to be more certain when they assign extreme ratings (1, 5) compared to the middle ratings (2-4), demonstrated by the distribution difference between the median H and all H ratings, as shown in Fig. 3. Relatively higher consistency in extreme ratings is a common pattern observed when humans are asked to rate using star-rating schemes and Likert scales, an observation typically reported in recommendation systems (Amatriain et al., 2009). This type of differences in the extent of human uncertainty depending on the rating further demonstrates the deficiencies in assuming a single gold rating for tasks that rely on human perception, clearly illustrating the need for a HM correlation metric that does not assume a single gold label, such as the JS we proposed in Sec. 3.2 in RQ2."}, {"title": "3.3.2 PERCEPTION-BASED PREFERENCE VISUALIZATION", "content": "For visualizing pairwise model preferences to compare human and machine judgments, we propose the \"Pairwise preference HM perception chart\". The concept behind this visualization is similar to the Ordinal HM perception comparison chart, the main difference is that is bins are separated by"}, {"title": "4 DISCUSSION AND CONCLUSION", "content": ""}, {"title": "4.1 HUMANS ARE UNCERTAIN AND THEIR LABELS ARE NOISY, SO MACHINES ARE BETTER?!", "content": "Evaluation procedures have predominantly circumvented dealing with human uncertainty, except for recent limited works (Chen et al., 2020; Wang et al., 2023; Chen et al., 2024), and have thus relied on the simplified assumption that a single gold label is sufficient. A single gold label is only sufficient when there is little or no ambiguity, while being able to quantify human uncertainty is crucial in measuring the effectiveness of automatic evaluation. The ubiquitous nature of uncertainty is studied in human psychology (Kahneman et al., 2021) and is also illustrated in a widely used MNLI dataset, which was carefully curated with 5 human labels per item. Over 40% of the samples have some degree of uncertainty, as shown in Table 1. In addition, given the challenges in human evaluation, including obtaining consistent labels (Elangovan et al., 2024), recent datasets have started to rely on collecting one human label per item (Bai et al., 2022; Ganguli et al., 2022; Stiennon et al., 2020). This further demonstrates the urgent need to acknowledge that quantifying human uncertainty is essential for measure the effectiveness of automated evaluation.\nThe machine learning community cannot afford to dismiss uncertainty as simply \"poor quality labels from humans\", key systematic errors and performance gaps in models become apparent under high human certainty as shown in Sec. 3.1. As a corollary, under high uncertainty even a random labeler can appear to have similar or better correlation with humans, as shown in Fig. 2. More importantly, for safety critical applications such as medicine, when humans labels are highly robust and certain, but the corresponding machine assigned labels differ, it can point to serious deficiencies in the automated system. Furthermore, the implications of uncertainty include changes to ranking of the automatic evaluator as well as the corresponding absolute correlation scores as shown in Table 2 and Table 3 depending on the reference human label. We acknowledge that collecting high quality labels from humans is expensive, and that in some cases gathering just one label per item might be adequate, such as when attempting to understand if one LLM is better than the other, provided the humans end up overwhelmingly preferring the output of one LLM over the other. In the case where the performance of two LLMs are close to one another, uncertainty in human judgments may be a signal that says the model outputs are quite similar. Thus, in some cases uncertainty can be a potential indicator of lower quality of labels requiring improvements to the underlying label collection process (Elangovan et al., 2024), in other cases uncertainty might be inevitable and therefore a valuable signal. Under uncertainty, comparison of automatic evaluation to human evaluation is challenging, and traditional correlation measures also fail to adequately account for this. The proposed binned JSD for perception is an effort in that direction so that metrics do not penalize human uncertainty, whilst comparing machine perception or attitudes with humans."}, {"title": "4.2 CHALLENGES IN METRICS AND INTERPRETABILITY", "content": "Deciding which statistic is appropriate depends on the data and how well it fits the assumptions made by the statistical measure (Eubanks, 2017). IRA metrics were never designed for systematic errors, and any accommodation for errors were based on assumptions about typical human behavior. Errors made by LLMs are rarely predictable, yet they are not random; rather, they are reproducible, making them systematic errors. The unpredictable nature of LLMs makes it difficult to design an effective metric that compares them with humans, given the uncertainty associated with human labels.\nDespite the deficiencies in metrics, some metrics may be a better fit, depending on the aim and the results of the study. For instance, in the case of perception-based preference experiments, where the goal is to understand which among a given pair of models is better, and when the result is almost unanimous with one model overwhelmingly preferred over the other, then the agreement in rarer (minority) labels may not matter. Hence, measures such as Krippendorff's-a and Fleiss-\u043a which estimate chance agreement based on the observed label distribution can result in substantially lower IRA scores even when there is high agreement on the majority label (winning model), whereas Randolph's-k assumes that the labels are equally distributed and hence is a better fit. For perception-based ordinal values, the extreme labels (such as 1 or 5 in the case of Likert 1-5) are usually strong indicators of human preferences, especially when supported by visualization as shown in Fig. 3. These extreme labels, might form the minority case, but might be crucial to interpreting the results. Hence, measures such as Krippendorff's-a and Fleiss-k estimate chance agreement based on the observed label distribution are a better fit compared to measures such as Randolph's-k that assumes uniform distribution of all labels. For tasks where a single majority label is not sufficient to represent human preferences, measures that do not assume a single gold label such as the proposed approach binned JSD for perception, can be a better choice to compare human and machine performance.\nIn addition, statistical analysis, such as null-hypothesis and significance testing, is essential for determining whether one model outperforms another by random chance. Here, the chance component includes 2 aspects, (1) chance due to the nature of samples in the evaluation set (2) uncertainty in human labels. A third aspect, even harder, is estimating the error rate as a result of systematically unpredictable erroneous labels from any automated evaluator. Future studies should explore these problems, including approaches like resampling (Deutsch et al., 2021). Incorporation of chance in rank correlation is also an important aspect to account for when two models differ in rank, but the corresponding difference in absolute scores is negligible, then the difference in the rank may not be meaningful.\nIRA metrics are challenging to interpret, and hence visualization is key to understanding gaps and strengths of any given metric. Effective visualization is a trade-off between plotting every single data point (too much information that is hard to synthesize) and an aggregate view (summarized view where key information might be obscured). The proposed perception charts are a step towards emphasizing how an aggregate number may not be sufficient in capturing the underlying data, depicting how human uncertainty varies across different labels, which in turn affects how machine performance can be interpreted across labels as shown in Fig. 3 and 4."}, {"title": "4.3 RECOMMENDATIONS FOR REPORTING EFFECTIVENESS OF AUTOMATIC METHODS", "content": "To conclude, comprehensive analysis of performance involves investigation beyond a single aggregate number. To that effect, in the case of comparing automatic evaluation with human evaluation, we recommend the following steps:\n1. Stratification by uncertainty levels: As discussed in Sec. 3.1, uncertainty in human labels can obfuscate performance gaps between machines and human evaluators. Hence, we strongly recommend stratifying results by uncertainty proportions.\n2. Multi-metric reporting: If there was no uncertainty, measures such as F1 would have worked. However, as a result of uncertainty, no single metric can capture important insights about every type of data as demonstrated in Sections 3.1, 3.2 and 3.3. Thus, we recommend reporting on multiple metrics belonging to different families, such as chance and non-chance-adjusted measures, so each metric in its own way can assist in bringing the less obvious but critical aspects about the underlying to the forefront.\n3. Visualization of results: A single non-parametric aggregate metric can rarely capture the entirety of underlying raw data, and hence visualization is key to understanding performance gaps, as discussed in Section 3.3. The proposed perception charts are a step towards making aggregate correlation more interpretable, as well as highlighting the strengths and gaps of the automatic labeler."}, {"title": "A.4 PERCENTAGE AGREEMENT", "content": "Percentage agreement: The percentage agreement, as defined by McHugh (2012), for n items where $X_1,X_2,..., X_n$ represent each item \u2013 that is labeled, with R raters and C categories, as the maximum percentage of votes the most frequent item gets as long as it is greater than 1. Formally:\n$\\frac{1}{n}\\sum_{i=1}^n max_{c \\in \\{1,...,C\\}} (\\frac{1}{R} \\sum_{r=1}^R \\mathbb{1}\\{x^r_{i,c}=c\\})  \\cdot \\mathbb{1}(\\sum_{r=1}^R \\mathbb{1}\\{x^r_{i,c}=c\\} > 1)$      (3)\nwhere $x^r_i$ is an annotation for $x_i$ by the rth rater. Note that the $\\mathbb{1}_A$ is the indicator function that returns 1 if condition A is true and 0 otherwise."}]}