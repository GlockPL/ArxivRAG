{"title": "Can Models Learn Skill Composition from Examples?", "authors": ["Haoyu Zhao", "Simran Kaur", "Dingli Yu", "Anirudh Goyal", "Sanjeev Arora"], "abstract": "As large language models (LLMs) become increasingly advanced, their ability to\nexhibit compositional generalization\u2014the capacity to combine learned skills in\nnovel ways not encountered during training-has garnered significant attention.\nThis type of generalization, particularly in scenarios beyond training data, is also\nof great interest in the study of AI safety and alignment.\n\nA recent study introduced the SKILL-MIX [32] evaluation, where models are tasked\nwith composing a short paragraph demonstrating the use of a specified k-tuple of\nlanguage skills. While small models struggled with composing even with k = 3,\nlarger models like GPT-4 performed reasonably well with k = 5 and 6.\n\nIn this paper, we employ a setup akin to SKILL-MIX to evaluate the capacity of\nsmaller models to learn compositional generalization from examples. Utilizing a\ndiverse set of language skills\u2014including rhetorical, literary, reasoning, theory of\nmind, and common sense\u2014GPT-4 was used to generate text samples that exhibit\nrandom subsets of k skills. Subsequent fine-tuning of 7B and 13B parameter\nmodels on these combined skill texts, for increasing values of k, revealed the\nfollowing findings:\n\n\u2022 Training on combinations of k = 2 and 3 skills results in noticeable improve-\nments in the ability to compose texts with k = 4 and 5 skills, despite models\nnever having seen such examples during training.\n\u2022 When skill categories are split into training and held-out groups, models\nsignificantly improve at composing texts with held-out skills during testing\ndespite having only seen training skills during fine-tuning, illustrating the\nefficacy of the training approach even with previously unseen skills.\n\nThis study also suggests that incorporating skill-rich (potentially synthetic) text\ninto training can substantially enhance the compositional capabilities of models.", "sections": [{"title": "1 Introduction", "content": "Today's large language models (LLMs) exhibit many impressive skills but remain imperfect in key\nareas. Arguably, one significant limitation stems from their difficulty in combining or composing\nthe skills they have already learned. For example, solving a math problem on a specific topic may\nrequire a certain set of skills, while solving a more challenging question may require applying broader\ncombinations of more skills as compared to a simple question. Understanding how well models can\nlearn to compose skills with limited training examples is therefore a crucial area of investigation.\n\nLet us note why this is a nontrivial challenge. If a model has learned N base skills, and we want it\nto be able to compose any subset of k skills, there are possible combinations of interest. Since\nscales roughly with the k-th power of N, even reasonable-sized training datasets will likely omit"}, {"title": "1.1 Our contributions", "content": "We address the question posed above by fine-tuning smaller models, including LLaMA-2-13B-Chat\nand Mistral-7B-Instruct-v0.2, using a small yet high-quality dataset generated by GPT-4. This dataset\nconsists of 13,957 text pieces, each composed of randomly selected k skills with k = 1,2,3. We\nevaluate the capability of the fine-tuned models to combine an another set of held-out skills with\npotentially higher k. In particular, we divide the original SKILL-MIX [32] skill set into a training\nset and a held-out set, based on skill categories, to minimize correlations between the two groups.\nThis ensures a clearer evaluation of the models' ability to generalize to unseen skills. Figure 1 and\nSection 3 detail the full pipeline of our data generation and evaluation process. Our experimental\nresults demonstrate the following findings (Section 4).\n\nFinding 1: Fine-tuning on texts that compose training skills improves capabilities of composing\nheld-out skills. Figure 2 shows the success rate of various models of combining k held-out skills.\nBefore fine-tuning, LLaMA-2-13B-Chat and Mistral-7B-Instruct-v0.2 perform significantly worse\nthan GPT-4, especially when k > 2. Both models improve remarkably after fine-tuning on our small\ndataset. For example, with k = 3, the success rate of LLaMA-2-13B-Chat increases from 4% to\n37%, and the success rate of Mistral-7B-Instruct-v0.2 increases from 8% to 49%. Note in the original\nSKILL-MIX, no model except GPT-4 could reach 15% success rate for k = 3 (see Table 3, [32])."}, {"title": "2 Related Works", "content": "Compositional generalization Compositional generalization has grabbed lots of attention in\nAI. [28, 26] studied compositional generalization in the realm of mathematical reasoning, and\n[3, 21] investigated for logical inference. In computer vision, compositional generalization was\nstudied on disentangled representation learning to generate images from novel combinations of\nconcepts [11, 9, 30]. Besides, several works have explored composing visual relations [19], as well\nas benchmarks for text-to-visual generation[13, 18]. Other works have explored using compositional\nmodels for image generation [6], as well as to create plans for unseen tasks at inference time [5].\n\nCompositional generalization for language and LLMs There is also a long history of study\nof compositional generalization in language [10, 17, 4, 14, 16, 20]. However, the test bed for\ncompositional generalization mostly relies on rule-based languages, like SQL or synthetic-generated\nones, and thus deviates a little bit from natural language. Recent works have observed compositional\ncapabilities in LLMs emerge multiplicatively on natural languages [29, 1, 22, 32]. These observations\nhave fueled a growing interest in exploring and evaluating compositional generalization in LLMs as a"}, {"title": "3 Pipeline", "content": "Our pipeline consists of three stages: generating data by selecting GPT-4 responses on SKILL-MIX\n(Section 3.1), fine-tuning on the generated data (Section 3.2), and evaluating our fine-tuned model on\nSKILL-MIX evaluation [32] (Section 3.3). The pipeline overview is shown in Figure 1."}, {"title": "3.1 Data generation", "content": "We adapt the procedure presented in SKILL-MIX evaluation [32] to produce finetuning data. Only the\ngenerations with full marks (i.e., illustrating all skills and topics, meeting the length requirement,\nand general coherence) are selected. To enhance the likelihood of obtaining full marks, we prompt\nGPT-4, the best Student model reported in Yu et al. [32], to create the generations.\n\nSkills and topics for data generation. Since our goal is to measure the generalization capability\nstrictly, we minimize the overlap between the skills/topics used during data generation and the\nskills/topics used to evaluate the fine-tuned models. Specifically, we partition the original set of\n101 skills introduced in Yu et al. [32], S, into a set of 53 train skills, Strain, and 48 held-out skills,\nSheld-out, based on the skill category. Strain includes only literary and rhetorical categories, while\nSheld-out comprises the rest of the categories, including reasoning, logic, theory of mind, pragmatics,\ncommon sense, and physical knowledge. Similarly, we partition the original set of topics, T, into\nrandom sets of 50 training topics, Ttrain, and 50 held-out topics, Theld-out. It is important to note that\npartitioning skills randomly can lead to correlations between the train and held-out skills, as skills\nfrom the same category can be highly related. However, partitioning topics randomly does not present\nthis issue, as the topics are generally unrelated. (Please refer to Appendix A for the detailed list of\nskills and topics.)\n\nData generation with k = 1,2,3. We produce fine-tuning data with k = 1, 2 and 3 using GPT-4\nas both the Student and Grader model. For k = 1, we use the original set of skills S and training\ntopics Ttrain to produce approximately 5,000 generations, and we only keep generations that receive\nfull marks. We refer to the resulting dataset as Dskill-mix(1). DsKILL-Mix(1) contains only texts\nwith individual skills, thus serving the role of separating the improvement from better utilizing an\nindividual skill and the improvement from better composing multiple skills in later experiments.\n\nWe follow an analogous procedure for k = 2 and k = 3, but using our 53 training skills Strain\nand 50 training topics Ttrain. We produce 10,000 generations for each k before filtering. We\nrefer to the resulting datasets as DSKILL-MIX(2) and DSKILL-MIX(3), respectively. For convenience,\nwe use DSKILL-MIX(1, 2) to denote the dataset that combines DSKILL-MIX(1) and DsKILL-Mix(2), i.e.,\nDSKILL-MIX(1, 2) = DSKILL-MIX(1) U DSKILL-MIX(2). Similarly, we use DSKILL-MIX(1, 2, 3) to denote the\ndataset that combines DSKILL-MIx(1), Dskill-mix(2), and DSKILL-MIX(3) together. We summarize our\nnotations in Table 1."}, {"title": "3.2 Fine-tuning", "content": "We fine-tune LLaMA-2-13B-Chat [27] and Mistral-7B-Instruct-v0.2 [15] on the data generated in\nSection 3.1 for 4000 steps with a batch size of 64. Each data generated from SKILL-MIX consists of\n4 parts: PROMPT1, ANSWER1, PROMPT2, ANSWER2. Here, PROMPT1 denotes the prompt asking\nthe student to generate answers, ANSWER1 stands for student's first round answer, PROMPT2 is\nthe prompt that asks the student to correct or refine its answer, and ANSWER2 is the student's\nsecond round answer. During fine-tuning, we feed the concatenation of PROMPT1, ANSWER1,\nPROMPT2, ANSWER2 into the model as a single text, but only compute the cross-entropy loss for\ntokens belonging to ANSWER1 and ANSWER2. We use Adam as the optimizer and linear warmup\nfor the first 64 steps, followed by a constant learning rate of 2e-5 for the remaining training steps. 1\nThe maximum token length is set as 1024. All fine-tuning experiments are conducted on 4 Nvidia\nH100/A100 GPUs. Similarly to the loss design of RLHF [24], we mix pre-training data\u00b2 during\nfine-tuning to prevent degradation of general abilities."}, {"title": "3.3 Evaluation", "content": "We evaluate the SKILL-MIX(k) performance (k = 2, 3, 4, 5) for all the models fine-tuned on data\ngenerated in Section 3.1, i.e., Dskill-mix(1), DskILL-MIX(2), and Dskill-Mix (3).\n\nSettings As mentioned earlier, SKILL-MIX evaluation requires a skill set and a topic set. We consider\nthe following 3 settings (where Setting II is our main setting used in Figure 1):\n\nI. SKILL-MIX evaluation on training skills and topics. Since the model observes the same skills\nand topics during fine-tuning, this setting serves as an in-domain evaluation for k = 2, 3.\nFor k = 4, 5, it tests the models' ability to combine more skills, which is already out-of-\ndomain, since the model has never seen such data during fine-tuning. We use the notation\nSKILL-MIXtrain(k) to denote the SKILL-MIX(k) evaluation on training skills and topics.\nII. SKILL-MIX on held-out skills and topics. This setting tests the models' ability to combine\nskills that are never present in fine-tuning.3 This setting serves as another perspective to"}, {"title": "4 Skill Composition Can Be Learned From Examples", "content": "We present experiment results using the pipeline (Section 3) to evaluate compositional generaliza-\ntion. Table 2 and Table 3 summarizes the SKILL-MIX (k) performances of LLaMA-2-13B-Chat\nand Mistral-7B-Instruct-v0.2 fine-tuned on various datasets under three evaluation settings. We\ndiscuss our findings on compositional generalization for in-domain evaluations (Section 4.1), compo-\nsitional generalization for out-of-domain evaluations (Section 4.2), and the data efficiency to induce\ncompositional generalization (Section 4.3)."}, {"title": "4.1 Compositional generalization for in-domain evaluations", "content": "We first observe that, after fine-tuning LLaMA-2-13B-Chat on DSKILL-MIX(1, 2), the SKILL-MIXtrain(2)\nperformance significantly improves. Similarly, after fine-tuning LLaMA-2-13B-Chat on\nDSKILL-MIX (1, 2, 3), the SKILL-MIXtrain(3) performance also improves. For example, the Ratio of\nFull Marks for SKILL-MIXtrain(3) improves from 2% for LLaMA-2-13B-Chat to 24% after fine-tuned\non DSKILL-MIX (1, 2, 3) (Table 2)."}, {"title": "4.2 Compositional generalization for out-of-domain evaluations", "content": "This section discusses the observations that indicate the out-of-domain generalization of skill compo-\nsition, including generalization to unseen k and generalization to unseen skills.\n\nSKILL-MIXtrain(k) improves for unseen k. We first observe that, after fine-tuning LLaMA-2-\n13B-Chat on SKILL-MIX data Dskill-mix(1, 2, 3), the SKILL-MIXtrain(4) and SKILL-MIXtrain(5) per-\nformance also increase. For example, the Ratio of Full Marks improves from 0% to 8% when k = 4\n(Table 2). Note that 8% Ratio of Full Marks improvement on k = 4 is significant, since besides\nGPT-4, all other models tested in Yu et al. [32], including GPT-3.5-turbo, cannot get over 2% Ratio\nof Full Marks on k = 4 (Table 3 in [32]). Besides, training only on DSKILL-MIX(1) does not improve\nthe SKILL-MIXtrain(4) or SKILL-MIXtrain(5).\n\nThe surprising finding here is that the model is only trained on SKILL-MIX k = 2,3 data, but it\nimproves the ability to compose k = 4, 5 skills in a short piece of text, which it is never trained on.\nThe results suggest that its ability to compose multiple skills does not come from overfitting training\ndata but should be perceived as learning a meta-skill instead. This observation is beyond the scope of\nthe theory presented in Arora and Goyal [1], which assumes that the number of skills a trained model\ncan compose is limited to the number of skills in its training text pieces."}, {"title": "4.3 Data requirement for inducing compositional generalization", "content": "Compared with fine-tuning on DSKILL-MIX(1, 2), one can observe that LLaMA-2-13B-Chat/Mistral-\n7B-Instruct-v0.2 fine-tuned on DSKILL-MIX (1, 2, 3) gains more performance boost on k = 4, 5 across\nall settings. For example, SKILL-MIXall(4) performance for LLaMA-2-13B-Chat fine-tuned on\nDSKILL-MIX (1, 2) is nearly the same as the original LLaMA-2-13B-Chat and LLaMA-2-13B-Chat\nfine-tuned on DSKILL-MIX(1). However, for LLaMA-2-13B-Chat fine-tuned on DSKILL-MIX(1, 2, 3), the\nSKILL-MIXall (4) performance improves from 1% to 15%.\n\nHowever, one may argue it is because DSKILL-MIX(1, 2, 3) has more data in total than DsKILL-MIX (1, 2).\nTo make a fair comparison, we conduct an ablation study by sub-sampling 8000 data from\nDSKILL-MIX (1, 2, 3), making sure that the number of data points with k = 2 and k = 3 in the sub-\nsampled set is less than the size of DSKILL-MIX(2). Table 4 shows the SKILL-MIXall(k) performance\nof LLaMA-2-13B-Chat fine-tuned on the sub-sampled dataset. The metrics remain relatively close\nto the model fine-tuned on full Dskill-mix(1, 2, 3) and significantly better than the model fine-tuned\non DSKILL-MIX (1, 2). This ablation confirms that \u201cskill-richer\u201d data can induce the ability to compose\nskills faster."}, {"title": "5 Discussions", "content": "All the findings in the previous section are based on the SKILL-MIX performance graded by GPT-4.\nHowever, GPT-4 is heavily used during data generation, and one can argue the improvement might\nsolely come from the fact that GPT-4 favors its own outputs. Although the possibility is low, to\nrigorously eliminate this confounding factor, we re-evaluate SKILL-MIXall(k) using Claude 3 Opus as\nthe Grader, and report the results in Table 5."}, {"title": "5.1 Using Claude 3 Opus as Grader for SKILL-MIX evaluation", "content": "All the findings in the previous section are based on the SKILL-MIX performance graded by GPT-4.\nHowever, GPT-4 is heavily used during data generation, and one can argue the improvement might\nsolely come from the fact that GPT-4 favors its own outputs. Although the possibility is low, to\nrigorously eliminate this confounding factor, we re-evaluate SKILL-MIXall(k) using Claude 3 Opus as\nthe Grader, and report the results in Table 5."}, {"title": "5.2 Potential capability of going beyond \"stochastic parrots behavior\"", "content": "Whether models can go past \u201cstochastic parrots\" behavior [2] is crucial in discussions of AI risk.\nBased on reasonable performance of GPT-4 on SKILL-MIX(k = 5) with common skills removed,\nYu et al. [32] suggests GPT-4 is already beyond \u201cstochastic parrots\". In particular, after removing\ncommon skills (see definition in [32]), the probability of a random (5 skills, 1 topic) combination\nappearing in the training corpus is estimated to be 11% if the training token is 2T5. Therefore, if a\nmodel has a Ratio of Full Marks beyond 11% when k = 5, then it suggests the model is able to output\nnovel text, thus is beyond \u201cstochastic parrots\". GPT-4 is the only model that can achieve this in [32]6.\n\nTable 6 shows the SKILL-MIX (k = 5) performance of fine-tuned LLaMA-2-13B-Chat and Mistral-\n7B-Instruct-v0.2 with common skills removed. We also include some DSKILL-MIX (4) data (about 1000)\ninto the training set. The fine-tuned models all show significant improvement over the base models.\nFor example, the Ratio of Full Marks for the fine-tuned LLaMA-2-13B-Chat and Mistral-7B-Instruct-\nv0.2 all go beyond 15% for SKILL-MIX (k = 5) on training skills and topics, and reaches 6% on all\nskills and topics, after filtering out the common skills."}, {"title": "6 Conclusion and Takeaways", "content": "We have studied the extent to which models can learn compositional generalization over language\nskills by fine-tuning on suitable examples demonstrating such composition. Previous evaluations\nhad seemed to suggest that the extent of compositional generalization is determined by the model\nsize and pretraining [32], but here we were able to induce much better compositional capability via\nfine-tuning on data that was generated using a setup similar to SKILL-MIX.\n\nOne surprising finding was that fine-tuning examples that composed 2 and 3 language skills were\nenough to improve the capability to compose 4 and even 5 language skills. Another surprise was that\nthe ability to combine language skills from held-out categories improved at the same rate as the skills\nused in the training examples. Of course, these findings were still about skills that are near relatives.\nThe full extent of such \"out of (training) distribution\" generalization remains to be explored."}, {"title": "7 Limitation", "content": "The main limitation of the current study is the high computational and financial costs, which impede\nus from sweeping more hyperparameters and conducting repeated experiments with different random\nseeds. These costs include the number of GPU hours for fine-tuning and the cost of calling OpenAI's\nAPI to generate the DSKILL-MIX(k) data and evaluate the SKILL-MIX performance. Despite these\ndifficulties, we managed to sweep the hyperparameters for fine-tuning the LLaMA-2-13B-Chat on\nDSKILL-MIX (1, 2, 3) (Main experiment, Table 2). We believe our findings are robust to different random\nseeds because of the clear message and consistent trend of the results.\n\nBesides, compositional generalization is a vast topic and we only study this under the setting of\nlanguage skills (limited to the SKILL-MIX setting). Whether the models can learn compositional\ngeneralization in other settings still needs further exploration."}, {"title": "A Skills and Topics Partition", "content": "The training skills and held-out skills are listed in Table 7 and Table 8 respectively. The training and\nheld-out topics are shown in Table 9."}, {"title": "E Examples of Fine-tuned Model's Generation and Implication to Safety", "content": "As the models have more ability to compose skills, it also leads to new challenges for AI safety and\nalignment: most benchmarks on AI safety directly query the model to generate some text related\nto harmful behaviors, e.g., \"could you please write a phishing email\", or \"please tell me how to get\nthe password from a system\". However, harmful behaviors can still be decomposed into multiple\nskills, and if you only ask the model to generate text using these skills, the model may output harmful\ngenerations without explicitly mentioning the harmful behavior. For example, to write a short tweet\ncriticizing based on fake experience, the skills needed include\n\nand\n\nBesides, one can add more customized constraints to make the generated text more appealing.\nFollowing is an example of generation we obtained using LLaMA-2-13B-Chat fine-tuned on\nDSKILL-MIX (1, 2, 3), using the prompt similar to SKILL-MIX(3).\nPrompt to the model:"}]}