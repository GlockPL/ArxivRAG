{"title": "LIFEGPT: TOPOLOGY-AGNOSTIC GENERATIVE PRETRAINED TRANSFORMER MODEL FOR CELLULAR AUTOMATA", "authors": ["Jaime A. Berkovich", "Markus J. Buehler"], "abstract": "The Game of Life (Life), a well known algorithm within the broader class of cellular automata (CA), exhibits complex emergent dynamics, with extreme sensitivity to initial conditions. Modeling and predicting such intricate behavior without explicit knowledge of the system's underlying topology presents a significant challenge, motivating the development of algorithms that can generalize across various grid configurations and boundary conditions. We develop a decoder-only generative pretrained transformer model to solve this problem, showing that our model can simulate Life on a toroidal grid with no prior knowledge on the size of the grid, or its periodic boundary conditions (LifeGPT). LifeGPT is topology-agnostic with respect to its training data and our results show that a GPT model is capable of capturing the deterministic rules of a Turing-complete system with near-perfect accuracy, given sufficiently diverse training data. We also introduce the idea of an \u2018autoregressive autoregressor' to recursively implement Life using LifeGPT. Our results pave the path towards true universal computation within a large language model (LLM) framework, synthesizing of mathematical analysis with natural language processing, and probing AI systems for situational awareness about the evolution of such algorithms without ever having to compute them. Similar GPTs could potentially solve inverse problems in multicellular self-assembly by extracting CA-compatible rulesets from real-world biological systems to create new predictive models, which would have significant consequences for the fields of bioinspired materials, tissue engineering, and architected materials design.", "sections": [{"title": "Main", "content": "Cellular automata (CA) have long been a subject of profound interest within the fields of computer science and mathematics, owing to their intricate and emergent behaviors. CA algorithms are uniquely characterized by their combination of computational simplicity-evolving solely by local state-transition rules\u2014and broad dynamical behavior, encompassing static, periodic, chaotic, and complex patterns, depending on the ruleset and initial condition (IC) being used. These properties render CA algorithms particularly valuable for simulating a wide array of natural phenomena, such as the propagation of forest fires[1], traffic flow dynamics[2], chemical reactions[3], and recrystallization[4]. The inherent behavioral unpredictability of CA (following human inspection of their rulesets) has hindered advancements in these subfields, confining CA to the realm of phenomenological modeling and subsequently preventing their evolution into mature, predictive tools for systems or phenomena where do not yet know a closed-form ruleset.\nOne notable CA algorithm is the Game of Life (Life), as introduced by John H. Conway in 1970 [5]. Life (also sometimes referred to as 'Conway's Game of Life') has since intrigued researchers from a range of disciplines, including mathematics, computer science and materials science. This is partly due to Life's self-organizing yet often unpredictable dynamics[6] that emerge from a simple 2-state-8-neighbor transition scheme (Fig. 1). Despite Life's fame, complex dynamics are present in numerous CA, and have led many researchers to support the conjecture that most CA are 'computationally irreducible.' This designation suggests that their evolution cannot be perfectly predicted by any process more computationally efficient than running the CA algorithms themselves[7]. Practically, this conjecture implies that the evolution of most CA algorithms from an arbitrary IC to an arbitrary time-step in the future cannot be described analytically. Thereby, any model that tries to simulate or predict the dynamics of a computationally irreducible CA is either implementing an algorithm capable of approximating CA behavior, or implementing the exact CA algorithm as a recursive operation for the number of timesteps desired. The former solution requires that the model diverge from the CA algorithm's ground truth after some number of timesteps and/or for some specific initial conditions, and the latter solution necessitates that the model be fundamentally limited by its size to a maximum number of timesteps. Another question is whether, and how, we can begin to build artificially intelligent systems that can program new versions of CA to meet certain target behaviors, and whether such a system could develop a heightened situational 'awareness' to predict the evolution of an algorithm multiple timesteps into the future without explicitly computing it recursively. While this may seem impossible given the computational irreducibility conjecture, past research has suggested that Life exhibits self-organized-criticality (SOC)[6] and persistent scaling behavior [8]. Additionally, some CA have been shown to generate fractal-like patterns[9]. These phenomena suggest that Life (and potentially, many other CA algorithms) are at least statistically predictable \u2013 while perfectly accurate predictions of any CA system might be unachievable without running the exact CA algorithm, being able make global claims about CA systems is far more reasonable. The nexus question here is: Are there 'deep trends' that exist across, or transcend, scales in CA that might be elucidated by deep learning models? These and related questions led us to explore the use of AI algorithms in modeling CA.\nWith increases in computing power and the growing popularity of neural networks as tools of analysis for complex systems (for which there are few practically useful analytical models), the computational irreducibly problem has lead many down the road of using neural networks to try to 'predict' the evolution of CA systems (often, Life is chosen as the example system), with varying degrees of success. The general anticipation across this field of research is that neural networks can either learn the rules of a given CA algorithm exactly, or can learn to abstract the system's behavior well enough to find an accurate prediction of some far-future state based on an IC, though some work ventures beyond this paradigm.\nPrevious work focuses largely on either feed-forward convolutional neural networks (CNNs)[10] or convolutional encoder-decoder models[11]. Such approaches already encode knowledge of the significance of the spatial relationships between each cell in the CA training data, namely that CA utilize state-transition rules based on the neighboring states of a given cell (also known as the Moore neighborhood[12, 13]). CNNs embed the solution in the model directly because they already understand that CA utilize a 2D grid as a logical topology and hence even before the their weights are initialized, CNNs are more than half way to the solution. To the best of our knowledge, no research has investigated the possibility of utilizing a data topology-agnostic model for predicting the evolution of CA.\nWithin the past years, generative pretrained transformer models (GPTs), have gained widespread popularity within the areas of natural language processing[14], weather prediction[15, 16, 17], speech processing/modelling[18, 19, 20], machine vision[21], strategic gaming[22, 23, 24], physical field modelling[25], protein folding[26] and finance[27, 28], to name a few. This remarkable capability for GPTs to generate accurate predictions, solve inverse problems, make useful decisions, and/or new and relevant content depending on the task at hand has been attributed to the underlying attention mechanism's remarkable ability to parse meaning from data through repeatedly updating token embeddings in a high-dimensional space[29]. Consequentially, attention allows GPTs to build sophisticated ontological understandings, which must be learned using a large amount of training data[30, 31, 32], or fine-tuned (in the case of pretrained models) on a smaller set of data[33]. Furthermore, these understandings can be visualized in numerous ways using graphs, proving a unique level of functional transparency[31, 32], in contrast to previous machine learning strategies, that are more opaque, leading to the common 'black-box' comparison[34].\nIn this paper we present an analysis of the transformer architecture's capability to predict the outcomes of Life, played on a 32 \u00d7 32 toroidal grid, with remarkable accuracy. We accomplish this using a decoder-only transformer model, equipped with causally masked multi-headed self-attention, with forgetful causal masking (FCM) implemented during training, trained on data representing pairs of 2D grids encoding ICs and next-game-states (NGSs)."}, {"title": "Transformer Models and Artificial Life", "content": "ALife is a highly interdisciplinary field, focusing on understanding the essential attributes of life and life-like systems through computation, hardware, and biochemical means. 'Soft' ALife, which is the most relevant to our work, is a subset of ALife that is entirely simulated computationally, instead of being physically realized[35]. Many CA systems (notably, Life), have been constructed for the purpose of better understanding self-assembly, self-propagating patterns, and, more generally, complexity and chaos, all phenomenological attributes intrinsic to life as it is currently understood[36, 37, 38, 39].\nSome scholars have already begun to conceptualize GPTS as more specifically, large language models (LLMs) as having a 'reciprocal relationship' with ALife research. Nisioti et al. (2024)[40] identifies two key relationships between the two areas of research. The authors argue that LLMs may become tools for ALife research by allowing for the generation of open-ended environments, or as operators of evolutionary computation. Inversely, they also argue, ALife principles of such as collective intelligence, evolution, and organization may also be exhibited by LLM agents. The authors emphasize the potential for LLMs for control or understand ALife systems, which is the primary focus of our paper. While the scope of this paper is limited to a replication of Life's rules using a transformer model, our work lays the groundwork for a paradigm in which GPTs are able to fine-tune CA systems' emergent qualities through alterations of ICs and rulesets, enabling a form of ALife oversight and regulation, which the authors argue is a primary ethical consideration."}, {"title": "Convolutional Neural Networks", "content": "Earlier work synthesizing the domains of CA and ML has largely focused on convolutional models paired with 2D CA. This is because 2D CA \u2018time-slices' are easily represented as images, which feed-forward CNNs are uniquely equipped to process. Springer and Kenyon (2021)[10] conducted a thorough empirical analysis on the ability of feed-forward CNNs to capture the allegedly computationally irreducible dynamics of Life. Since Life abides by rules pertaining to the states of 2D nearest-neighbors (the r = 1 Moore neighborhood), which are effectively equivalent to a convolutional operations, the authors were able to calculate a hypothetical minimal feed-forward CNN (using ReLU activation) capable of capturing the rules of the Life and applying them over n time-steps. They were thereby able to define a metric for network over-completeness, m, to empirically test the relationship between CNN size and Life learning effectiveness for various n and m. One finding was that for n \u2265 3, increasing m up to 25 was insufficient for reaching a training convergence fraction of over 50%. Furthermore, the authors found that for networks which did converge, the number of epochs necessary increased substantially with n. The authors also note that converged, minimal networks were very sensitive to sign-flipping of initial weights, suggesting that for smaller networks, more luck in initializing suitable weights is needed to converge. The authors further reported that gradient descend was highly sensitive to the distribution parameters of the dataset, suggesting that some game examples were more useful that others for teaching the rules of Life to the model."}, {"title": "Sigma-Pi Networks", "content": "Wulff and Hertz (1992) demonstrated the use of simple neural networks with short range connections (\u03a3 \u2013 \u03a0 networks) to learn the dynamics of 2D CA[41]. They concluded that even with a network sharing the same topology as the CA being studied, the dynamics of Life could not be learned effectively without weight-sharing between neurons. Their early approach was similar to many modern-day graph neural network architectures. Still, their approach encoded part of the solution to the problem in the architecture of the model. By enabling weight-sharing and employing short-range connections, the architecture of the network was itself a reflection of the grid-topology and nearest-neighbor rules engendering Life."}, {"title": "Neural CA", "content": "Recent work has shown that CA systems can be trained to dynamically sustain desired patterns (such as small 2D images), by allowing a single CNN to simultaneously control the state-transition rules of all cells in the system, which is likewise made possible by allowing cells to take on continuous states (as opposed to discrete states). By training the neural network (through repeated runs of the CA growth process, following by subsequent loss calculation by comparison with the desired image, followed by backpropagation), the model's parameters are tuned so that a single cell can, over time, multiply into a cellular collective that \u2018grows' into the desired image[42]."}, {"title": "Paper Outline", "content": "The plan of the paper is as follows. First, we introduce the LifeGPT architecture and discuss the effectiveness of the model under a variety of training and inference strategies, draw conclusions regarding the manner in which the LifeGPT learns, and discuss zero/few-shot learning capabilities of LifeGPT. Next, we introduce the concept of the 'autoregressive autoregressor' (ARAR), which allows LifeGPT to recursively simulate Life's dynamics over multiple time steps. Finally, we discuss the broader implications of LifeGPT for ALife, universal computation within an LLM framework, and solving inverse problems for the design of biomimetic and bioinspired materials. Looking ahead, we provide an argument for why enhacing LifeGPT with reinforcement learning (RL) techniques could improve inference accuracy, extend its abilities to a wide space of CA rulesets, and further enhance its utility in simulating and understanding complex, life-like systems."}, {"title": "Results and Discussion", "content": null}, {"title": "Model Convergence During Training", "content": "Details on the implementation of the model, training and datasets used are provided in Materials and Methods (section 4). LifeGPT is architected as a decoder-only transformer model with causally-masked self attention \u2013 in addition to FCM during training \u2013 which utilized 12 transformer layers and 8 attention heads to capture complex patters in Life. The model's maximum sequence length were configured to accommodate a data corresponding to ICs and NGSs. Rotary positional embedding (RPE) was used to maintain spatial awareness, while the Adam optimizer and cross-entropy loss (CEL) function guided the training process (see section 4.2 for a full list of hyperparameters). The best-performing version of LifeGPT was trained on training data with broad-entropy ICs, though using high-entropy ICs was also tested (see sections 4.3.5 and 2.3).\nOur final model displayed rapid convergence to a range of cross-entropy loss (CEL) values from approximately 0.4 to 0.2 (Fig. 2A). We suspect the reason the loss stays far greater than 0 is that there is no causal relationship between earlier tokens and later tokens in ICs within the training data, since ICs were generated stochastically (see section 4.3.5). Since FCM was the only masking strategy used (see section 4.4 for details on implementation and Supplementary"}, {"title": "Accuracy Benchmarking and Sampling Temperature Effects", "content": "CEL minimization was used during model training, but performance was assessed with periodic benchmarking. Specifically, the model was periodically tasked with autoregressively generating the tokens following 10 prompts, each containing an IC from the testing set (see section 4.3.7 and Fig. 3). The number of tokens correctly predicted by the model, across 10 ICs in the testing set was used to determine an accuracy score (see section 4.6 and equation 3).\nWe observed temperature does have a substantial impact on accuracy (Fig. 2B). Even though the model was trained on deterministic data, it was prone to some minor errors, especially as sampling temperature is increased. Nevertheless, even with errors, models across all temperatures tested (0.0, 0.25, 0.50, 0.75, 1.0) still achieved at least 99.9% accuracy after about 20 epochs. Moreover, the variability in accuracy across temperatures was observed to decrease as the number of epochs of training were increased, suggesting that training for more epochs may further mitigate errors. For this task, a temperature of 0.0 showed the best empirically-measured accuracy scores across nearly all epochs. After epoch 16, the model rarely failed to achieve 100% accuracy, although there were couple exceptions where the model missed a one or two tokens. Indeed, even for the same model trained for 50 epochs, there was one sample in the testing set, 'r-pentomino' (see r-Pentomino Animation at Temperature 0 \u2013 10 Timesteps), for which LifeGPT's recursive NGS predictions failed to agree with the ground truth, due to a single incorrectly predicted cell early on."}, {"title": "Training Data Ordering Effects", "content": "Similar to earlier work using CNNs[10], we find that the ordering of the samples found in training data (see section 4.3.5) has a substantial effect on model accuracy (Fig. 4). However, while earlier work used convergence (defined as achieving a critically low CEL) as a performance metric, we used accuracy (see sections 2.2 and 4.6, and equation 3)."}, {"title": "Zero/Few Shot Learning Abilities", "content": "Zero-shot and few-shot learning are demonstrated when models perform well on tasks that either do not appear or rarely appear in training data. We show that our training data, containing 10,000 stochastically-generated ICs (see 4.3.7), represents only a small fraction of all possible ICs. It is very unlikely for one or more of the ICs in our testing set (excluding the two samples generated with px = 0 and px = 1 respectively) to exactly match one or more ICs in our training set. Given that the total number of possible initial conditions is $N_{Total} = 2^{32}$, the probability of randomly testing the model on a training sample is $\\frac{10,000}{N_{Total}} \\approx 2.33 \\times 10^{-6} = 0.000233\\%$. We also empirically confirmed that there were no matches between the 8 testing sample ICs (again, excluding px = 0 and px = 1) and the 10,000 sample training set we used, using a simple search algorithm.\nBased on our performance characterization (Fig. 2), LifeGPT displays impressive zero/few-shot learning capabilities. LifeGPT is able to simulate the rules of Life with near-perfect accuracy for a 32 \u00d7 32 toroidal grid by learning off of less than 0.0003% of all possible starting configurations. Still, extremely rare instances of incorrect token outputs are observed, even with a sampling temperature of 0. This is observed experimentally when feeding the output of LifeGPT back into its input, creating two layers of autoregression (we called this method ARAR, for \u2018autoregressive autoregressor' see section 4.8), which recursively implement the rules of Life. Specifically, an error in a the output of a single cell will cause a perturbation to the Life system, that, over several additional iterations, is highly likely to drastically affect the entire game state with respect to the ground truth. This occured when inputting the 'Order_param: 0.25' ((n)ic = 0.25), \u2018glider,' and 'r-Pentomino' ICs from the testing into the ARAR algorithm (see (n)Ic = 0.25 Animation at Temperature 0 \u2013 250 Timesteps, Glider Animation at Temperature 0 \u2013 250 Timesteps, and r-Pentomino Animation at Temperature 0 \u2013 250 Timesteps, respectively). For the (\u03b7)Ic = 0.25 and 'r-Pentomino' ICs, only a single cell was incorrectly predicted during the generation step which caused divergence from ground truth, while for the 'glider' IC, divergence was observed due to an iteration wich incorrectly predicted 2 cells (Fig. 6)."}, {"title": "Future Work", "content": "LifeGPT's ever-increasing likelihood of generating incorrect tokens with increasing temperature suggests a fundamental trade-off between model creativity and accuracy in making deterministic predictions. This key result suggests that LifeGPT learns an approximation of the rules that fails under specific circumstances. This may be due to an intrinsic limitation of the transformer architecture for modelling deterministic systems; nevertheless a future version of LifeGPT could be used to simulate CA systems which do not require 100% accurate predictions, or for which \u2018perfect' predictions are impossible, such as in the case of stochastic CA, or modeling real-world mechanisms such as growth or dynamics of living cells, amongst others. Another possible avenue of exploration would be to use GPT models to predict coarse-grained CA evolution, as previous work has shown that even highly complex CA often have simpler course-grained representations using a renormalization group approach[43].\nFuture LifeGPTs could be trained on a wide space of CA rulesets, may be able to solve an inverse problem. Here, instead of predicting the precise evolution of CA systems, they may be capable of assigning a representative CA ruleset to a real-world dynamic system, based on data representing the system's spatiotemporal evolution. This rules-based representation would help advanced the phenomenological understanding of the system at hand, and would enable generative exploration of the system's configuration-space via IC-variation and subsequent recursive rule application. For instance, this strategy could be applied to better understand the growth-dynamics of plants and fungi, since CA have already been tried as models for many such systems[44, 45, 46, 47, 48], and the growth of such organisms is largely governed by the emergent effects of cellular interactions [49, 50, 51, 52].\nYet another opportunity may arise from the fact that Life has been shown to be Turing complete, via the implementation of a Universal Turing Machine (UTM)[53]. Turing completeness is, in most cases, synonymous with computational universality [54]. If a system can be proven to be Turing complete, then that a system is capable of simulating any Turing machine, or in other words, any classical computer, granted the system is large enough to store the necessary code[55]. Therefore, our work empirically shows that, theoretically, GPT models could become capable of simulating any classical computer algorithm (granted sufficient hardware). This falls in line with theoretical work that has proven the Turing-completeness of the transformer architecture in the past[56, 57]. While our GPT model was trained on game grids far too small to implement a full-scale UTM, and modern-day hardware still limits the practicality of synthesizing a GPT with Life to simulate a UTM, our work nevertheless showcases the possibility for future models to synthesize stochastic generative capabilities with deterministic computational capabilities. This would be particularly useful in the context of hallucination-prone LLM chatbots[58, 59], as simulating CA-based computers might enable them to perform deterministic calculations without compromising their creative, generative capabilities.\nConsidering that LifeGPT does, occasionally, predict incorrect tokens, more work could be done to investigate other methods of training GPTs on this front. In recent years, more work has been done investigating the potential of transformer models, particularly LLMs, for strategic game-play[23]. In particular, methods which incorporate some kind of reinforcement learning (RL), which involves the LLM making decisions within and getting feedback from a simulated game environment, have been shown to be particularly successful. For these types of applications, transformer models - specifically, LLMs act as agents which can make a series of decisions, often producing more than one output at a time, conveying several viable options. An RL algorithm works in tandem with the LLM to choose the best decisions from generated options. By playing a given strategy game repeatedly against other AI agents, where each decision leads to a generated reward enabling RL training, the agents will learn over time to take optimal decisions. This strategy usually requires a pretrained LLM chatbot, and does not necessarily involve any LLM fine-tuning steps. Xu et al. (2024) found that the addition of an RL algorithm, as opposed to only using an LLM, helps the model overcome intrinsic biases found in LLMs such as ChatGPT. For a so-called 'solved game,' this strategy can ensure that the model's actions more closely align with the corresponding Nash equilibrium [22].\nWhen learning Life, the situation is slightly different, as life is a 'O player game,' meaning it behaves more like a simulation, where a user simply inputs some IC and lets the program run its course. Therefore, a language-agent-plus-RL approach for predicting Life dynamics would rely on creating a 'meta-game,' a game in which the Nash equilibrium results in the duplication of Life's rules. We believe such a strategy is worth exploring in future work, as it may serve to mitigate errors. We further envision that a GPT pretrained accurately on a number of diverse CAs can be a powerful foundation model to learn CA rules from real biological data, for instance, for which exact algorithms may not exist. It could potentially also serve as a way to solve the inverse problem, to identify an algorithm for a given set of data.\nGoing further, Liu et al. (2024) reconceptualizes the LLM learning processes in the context of agent learning in two-player language games[24]. They go on to use their new framework to argue that hallucinations in current, state-of-the-art LLMs is due to a lack of a 'world model' [60]. In the context of a model trained to predict the next state in a CA algorithm such as Life, a world model could be represented as a state-transition table or graph. Future LifeGPT-like models could therefore benefit from being tasked with, in addition to predicting NGSs, predicting state-transition rulesets. Hypothetically, if the training data were then to be structured to incorporate many examples of CA ICs and NGSs, along with corresponding rulesets, the model could learn to interpret the ruleset as a world model, allowing it to check its NGS outputs to mitigate errors. Such an approach would be uniquely interesting for the aforementioned task of assigning rulesets to real-world biological systems, as the world model could quadruply serve as a error-mitigating component of the main GPT model, a model interpretability tool, a biological system interpretability tool, and a generative design tool."}, {"title": "Conclusion", "content": "We have demonstrated the functionality of LifeGPT, a decoder-only GPT model trained on NGS transition examples from Conway's Game of Life. By allowing LifeGPT to learn the dynamics of Life, we demonstrated, to the best of our knowledge, the first ever transformer model that has been trained to predict NGS transitions in a 2D CA system.\nLifeGPT represents a step forward in the synthesis of machine learning with ALife, and in the understanding of highly complex systems with emergent properties. The demonstrated capability of LifeGPT (which is orders of magnitude smaller than SOTA GPT models) to learn the state-transition rules governing Life with near-perfect accuracy opens up new opportunities for both understanding and modelling highly systems highly sensitive to initial conditions, as well as universal computation in an LLM framework, specifically pretrained LLMs, that merges the capacity to model CAs with a host of other capabilities.\nFuture work could also focus on mitigating the effects of occasional errors. The intrinsic stochasticity of the transformer architecture leaves the door open for building better understandings of probabilistic system, and of coarse-grained approximations of the real-world. The topology-agnostic nature of LifeGPT, unlike previous methods of learning Life and other CA, may allow future exploration of a diverse space of nonlinear, dynamical, continuum systems, regardless of the details of spatiotemporal dependencies"}, {"title": "Materials and Methods", "content": null}, {"title": "Model Architecture and Hardware Information", "content": "LifeGPT was constructed in python using the 'x-transformers' library [61]. The models in this study were trained with a workstation equipped with a high-end CUDA-compatible GPU (RTX A4000, NVidia, Santa Clara, CA, USA) for a total of 50 epochs on a 10,000-sample training set."}, {"title": "Hyperparameters", "content": "Hyperparameters were initially selected heuristically for optimal performance, as the GPU primarily used for training (RTX A4000, NVidia, Santa Clara, CA, USA) had 16 GB of VRAM. Unless otherwise stated, all instances of LifeGPT used the following set of hyperparameters during training, as described in Table 1. The batch size was initially set to 20 samples, and was decreased to 5 samples for later versions of LifeGPT due to memory limitations encountered when using FCM (see section 4.4)."}, {"title": "Datasets", "content": null}, {"title": "Data Generation Overview", "content": "To generate training sets, validation sets, and testing sets, the same basic strategy was used. First an IC game-states would be generated stochastically as a 2D, 32 \u00d7 32 numpy array. Depending on the exact algorithm used, the generated IC game-states would collectively form either high-entropy or broad-entropy datasets. Next, a proprietary Life python class was used to generate the corresponding NGS for every previously generated IC. Lastly, each IC and corresponding NGS was concatenated within a string. Every generated pair was subsequently stored within a dataframe from future retrieval."}, {"title": "Data Topology.", "content": "Transformer models are architected to process data as 1D arrays. Therefore, to teach the LifeGPT the rules of a 2D CA algorithm, such as Life, the 2D data from each time-slice of the game had to be flattened into a 1D array. In this way, LifeGPT functioned similar to a vision transformer, in which 2D data is flattened into a 1D array within which each entry is a tokenizable image patch[21]. However, due to the low-resolution of the 32 \u00d7 32 toroidal grid on which Life was simulated to generate our training, we were able to encode every pixel of each time-slice of the game in a 1D array (as opposed to grouping pixels into patches)."}, {"title": "Instruction Tuning.", "content": "In order to encode the time-progression of the game into the training set, the initial-state and next-state 1D arrays were placed within a prompt-string which was subsequently tokenized to form a vector. Specifically, both 1D arrays were converted to strings and placed within a larger string containing start and end tokens (@ and $, respectively), a task statement, and bracket delimitors (e.g. '@PredictNextState')."}, {"title": "Tokenization.", "content": "We employed a byte-level tokenizer that operates on UTF-8 encoded text. UTF-8 is a variable-width character encoding capable of representing every character in the Unicode standard, which allows the tokenizer to process a wide range of scripts, symbols, and special characters uniformly. By converting the text into its byte-level representation, our approach ensures consistent tokenization across different languages and handles out-of-vocabulary words and non-standard text, such as emojis or code, effectively. This method allows for robust and flexible processing of diverse textual data. Tokenization resulted in a vector suitable as input to the embedding layer of the transformer model."}, {"title": "Training Set Generation", "content": null}, {"title": "High-Entropy IC Set Generation", "content": "High entropy IC game-states were generated by effectively flipping a coin 1024 times to designate the states (0 or 1) on a 32 \u00d7 32 grid. When considering the configuration space of a binary 2D array $M \\in {0,1}^{32 \\times 32}$, the following formula may be used to describe its Shannon entropy[62] (informational entropy):\n$H(M) = - \\sum_{x \\in {0,1}} P_x \\log_2{P_x}$                                                                                                                                                                                                  (1)\n(this is also known as the binary entropy function[63]) where, $p_x$ is the probability of finding the value $x$ in the 32 \u00d7 32 array $M$. $p_x$ is defined as:\n$P_x = \\frac{1}{32^2} \\sum_{i=1}^{32} \\sum_{j=1}^{32} \\delta_{M_{ij}, X}$                                                                                                                                                                                                                   (2)\nwhere, $M_{ij}$ is an element of $M$ in the $i$th row and $j$th column, and $\\delta_{M_{ij},x}$ is the Kronecker delta function, which is equal to 1 if $M_{ij} = x$ and 0 otherwise.\nThus, for a \u201850-50 coin toss' scenario ($p_0 = p_1 = \\frac{1}{2}$), $H(M)$ is at a maximum and is equal to 1 Sh. Moreover, since binary data necessitates the condition $p_0 + p_1 = 1$, only one probability value is needed to fully describe the entropy of a given array A. We therefore denote the ordering of a given IC by referring to a single order parameter, \u03b7, where \u03b7 = $p_1$ is always true. When considering the order parameter of a set of ICs, it is important to note that, because IC generation is always a stochastic process, the exact n of any given IC in the set cannot be predicted with certainty. For this reason, we characterize IC sets with the symbol \u3008\u03b7\u3009, denoting the expected order parameter."}, {"title": "Broad-Entropy IC Set Generation", "content": "To create a broad-entropy IC set, first, a vector was created representing a set of order parameters ranging from 0 to 1. The length of this vector was set to the desired number of samples in the dataset (10,000 for training, 1000 for validation). This set of order parameters may be thought of as containing different expected probabilities for finding a 1 in an IC.\nThen, the same procedure as with the high-entropy IC set was followed, with two exceptions: (1) instead of random.random() < 0.5 == True determining the value of each element in each IC array, random.random() < \u03b7 == True was the determining equality, and (2) each IC was generated using a unique \u03b7 from the aforementioned vector (see section 4.3.5). This strategy ensured that the IC set represented a broad range of ordering, from all 0s, to 50-50 0s and 1s, to all 1s (Fig. 5B)."}, {"title": "Next-Game-State Generation.", "content": "NGSs were calculated from IC arrays by applying Life rules assuming a toroidal grid (see the update_grid() function here: game.py)."}, {"title": "Reshaping Data.", "content": "To make the handling of training set data easier, the final stage of the training set generator involves reshaping the data into a list of sub-lists, in which each entry in the list contains a sub-list corresponding to a specific IC. Within each unique sub-list, two strings are stored, one corresponding to a flattened IC, and one corresponding to a flattened NGS (see the generate_sets() function here: game.py)."}, {"title": "Validation Set Generation", "content": "Validation sets were generated using the same methods in section 4.3.5, as the random.random() function ensures sufficiently random IC generation, ensuring training and validation sets remained entirely independent. Combined with the incredibly large space of possible 32 \u00d7 32 binary arrays ($2^{32}$ = 4, 294, 967, 296 unique possibilities), this made the likelihood of even a single sample being identical between a 10,000-sample training set and a 1000-sample validation set very low (see section 2.4). This, in turn, ensured that over the course of model training, training loss and validation loss remained independent of each another."}, {"title": "Testing Set Generation", "content": "A 10-sample testing set was constructed to validate the performance of models during and after training, in a manner other than by inspecting the validation and training losses. 5 samples in the testing set were generated stochastically in the same manner as in section 4.3."}]}