{"title": "LAGUNA: LAnguage Guided UNsupervised Adaptation with structured spaces", "authors": ["Anxhelo Diko", "Antonino Furnari", "Luigi Cinque", "Giovanni Maria Farinella"], "abstract": "Unsupervised domain adaptation remains a critical challenge in enabling the knowledge transfer of models across unseen domains. Existing methods struggle to balance the need for domain-invariant representations with preserving domain-specific features, which is often due to alignment approaches that impose the projection of samples with similar semantics close in the latent space despite their drastic domain differences. We introduce LAGUNA - LAnguage Guided UNsupervised Adaptation with structured spaces, a novel approach that shifts the focus from aligning representations in absolute coordinates to aligning the relative positioning of equivalent concepts in latent spaces. LAGUNA defines a domain-agnostic structure upon the semantic/geometric relationships between class labels in language space and guides adaptation, ensuring that the organization of samples in visual space reflects reference inter-class relationships while preserving domain-specific characteristics. Remarkably, LAGUNA surpasses previous works in 18 different adaptation scenarios across four diverse image and video datasets with average accuracy improvements of +3.32% on DomainNet, +5.75% in GeoPlaces, +4.77% on GeoImnet, and +1.94% mean class accuracy improvement on EgoExo4D.", "sections": [{"title": "1. Introduction", "content": "Domain shift challenges trained models to generalize across scenarios with differing distributions and presents a significant hurdle for supervised learning in computer vision [42]. While fine-tuning with labeled data from the target domain can mitigate this problem, obtaining such labels often proves prohibitive. Unsupervised domain adaptation (UDA) [4] offers a compelling alternative, enabling knowledge transfer to novel domains without relying on expensive labeled data, which has garnered significant attention [70], promising cost-effective solutions for real-world applications prone to domain shift. The typical UDA setting considers the availability of a labeled source domain and an unlabeled target domain. In general, source and target domains are semantically equivalent but drawn from distinct data distributions, e.g., real images versus clipart of chairs, TVs, and mugs. Thus, the main challenge of UDA is to effectively mitigate the distribution shift between domains, which is often addressed by reducing the distribution discrepancy of source and target representation spaces either by minimizing some discrepancy measure [21, 58, 63], using adversarial learning [31, 33, 40, 49, 67, 68, 77], aligning data around centroids [33], or leveraging pseudo-labels [27, 33]. These methods aim to align source and target representation spaces in a shared coordinate system, pushing feature vectors of equivalent semantic concepts close to each other in the embedding space, which may happen at the expense of the representation power of individual domain-specific spaces. For instance, bright colors and rounded shapes might be important to encode for representing a clipart, while nuanced shadows, reflections, and textures might be important to represent a real image. As a result, the aligned space may become overly generic, correctly encoding only a subset of the data (see Figure 1(top)). Recent work [46] showed that semantically equivariant representation spaces of similarly trained neural networks exhibit distinct representation spaces with matching geometrical structures. For instance, two data points $(x_1,x_2)$ may be mapped to distinct vector pairs in the two representation spaces $(v_1, v_2)$ and $(v_1', v_2')$ (e.g., $||v_1 - v_1'||_2 >> 0$ and $||v_2 - v_2'||_2 >> 0$), while angles between the two pairs will be similar in their own representation spaces (e.g., $\\angle(v_1, v_2) \\sim \\angle(v_1', v_2')$). This suggests that pushing representation spaces to overlap in absolute coordinates, as done in current domain adaptation approaches, is not necessary to obtain equivariant representation spaces.\nBased on this observation, we introduce LAGUNA - LAnguage Guided UNsupervised Adaptation with structured spaces, a novel approach to domain adaptation which guides source and target spaces to develop semantic-geometric inter-relationships reflecting the structure of a reference space (Figure 1(bottom)). As shown in recent works, language can provide a semantic space agnostic to the nuances of visual observations enabling robust zero-shot generalization [10, 54] and supporting domain robustness [14, 23, 27, 45, 66], hence we choose language to build our reference structured space in LAGUNA. This assumes the availability for both source and target samples of natural language descriptions, which can be generated from available captioning models [32] or collected from the web at a fraction of the cost of typical human labeling procedures [27]. Specifically, LAGUNA employs a 3-stage approach to structurally align the source and target representation spaces while allowing each domain to preserve its typical patterns. In Stage 1, textual class labels are mapped to a domain-agnostic reference space representing their relationships at the conceptual level. In Stage 2, a language model is trained to map textual captions to the reference latent space in order to provide pseudo-labels for target samples Finally, in Stage 3, a cross-domain classifier is trained to encourage domain-specific representations to follow the reference structure.\nExperiments demonstrate the superiority of LAGUNA over existing state-of-the-art methods across 18 different domain adaptation scenarios sourced from four diverse image and video datasets, with gains of +3.32% on DomainNet [50], +5.75% on GeoPlaces [26], +4.77% on GeoImnet [26] and +1.94% mean per class accuracy on EgoExo4D [20]. We further report ablations to analyze the specific contributions of our design choices."}, {"title": "2. Related Work", "content": "In sum, our main contributions are as follows: 1) we investigate the use of relative representations for UDA, showing its advantages with respect to the current absolute alignment paradigm; 2) we propose LAGUNA, a three-stage method which learns a cross-domain classifier where individual source and target spaces are distinct yet aligned to a reference structure derived from language; 3) through extensive ablations and comparisons with state-of-the-art models, we show the superiority of the proposed approach. Code available at: https://github.com/ADiko1997/LAGUNA.git"}, {"title": "2.1. UDA in Computer Vision", "content": "UDA seeks to transfer knowledge from a labeled source domain to an unlabeled target domain [2, 16, 40]. This is tackled through various approaches, prominently, discrepancy-based methods that minimize distribution differences using techniques like MMD [28, 31, 39, 58, 61, 63], and adversarial learning [4, 6, 16, 38, 40, 57, 64, 64, 67, 68]. Other approaches also investigated class-conditional distributions alignment [29, 43, 49, 68, 72], clustering similar instances across domains [9, 24, 36, 48], instance-specific adaptation[25, 60, 65], and self-training leveraging pseudo-labeling to improve target domain performance [15, 27, 36, 52, 62]. More recently, transformer-based approaches utilize patch composition for domain-invariant representations through intermediate domains [77]. Video domain adaptation tackles the unique challenges of temporal dynamics and consistency in videos [8, 56, 69] with a particular focus on adapting models across the exocentric and egocentric domains [22, 34, 53, 74]. Existing approaches sought to align source and target representation spaces in absolute coordinates, often falling short in bridging complicated forms of domain shift [26, 52]. Contrarily, LAGUNA aligns representations in relative terms, encouraging source and target visual spaces to share a similar structure as a reference space derived from language while allowing them to encode domain-specific peculiarities."}, {"title": "2.2. Language Guidance in Vision UDA", "content": "Vision-language models like CLIP [54] have shown promising zero-shot transfer capabilities [10]. However, when fine-tuned to a specific scenario [1, 51], they often lose the ability to generalize to new domains [30, 71]. To address this issue, recent works leveraged textual information to bridge the gap between domains [14, 18, 19, 23, 45, 66]. In particular, the seminal work of [27] proposed to use rich textual captions to provide pseudo-labels for the target domain and addressed UDA by training a joint classifier on source and target domains. Similarly to [27], we generate pseudo-labels from textual captions, but, rather than seek-"}, {"title": "2.3. Relative Encodings", "content": "Recent work [46] has shown that equivalent latent spaces of similarly trained networks tend to be misaligned in absolute terms but share similar internal geometric relationships. As a result, representing data points with relative encodings, obtained as vectors of cosine similarity values of a data point with a predefined set of anchors, allows to perform zero-shot model stitching [46], translate representation spaces across models [44] or modalities [47]. As shown in [5], predefined invariances can be incorporated into the learned representation to enable specific forms of relative representations. In [11] relative encodings were used to tackle the downstream task of action anticipation. LAGUNA builds on relative encodings to represent the semantic inter-relations between classes in a domain-agnostic language-based reference space and support the development of aligned yet specialized source and target domains."}, {"title": "3. Method", "content": "In Stage 1, a language-based reference structure space is defined. We represent this structure as a set of anchors, which we will use as a reference to guide representation learning. In Stage 2, a text-supervisor language model is trained to map image/video captions to class categories in order to provide pseudo-labels for the unlabeled target domain. The language model is also trained to keep the internal text embeddings aligned to the domain-agnostic structure defined in Stage 1. Finally, in Stage 3, a cross-domain visual classifier is trained to categorize source and target samples by learning domain-specific action anchors bound to be structurally similar to the domain-agnostic structure defined in Stage 1."}, {"title": "3.1. Problem Setup", "content": "We follow the formulation of [27] and assume a labeled source domain $\\mathcal{X}_s: \\{(x_i, y_i, l_i)\\}_{i=1}^{N_s}$, where samples $x_i$ are paired both with labels $y_i \\in \\mathcal{Y}$ and language descriptions $l_i$, whereas the target domain $\\mathcal{X}_t: \\{x_i, l_i\\}_{i=1}^{N_t}$ contains unlabeled samples $x_i$ paired with language descriptions $l_i$. These textual descriptions can be readily obtained from associated metadata or generated using image-to-text models [32]. $N_s$ and $N_t$ represent the number of source and target samples, respectively, and the two domains share the same high-level semantics and number of classes $N_c$."}, {"title": "3.2. Stage 1 - Reference Structure Definition", "content": "Assuming shared classes across domains, in Stage 1 (Fig. 2), we construct a language-based, domain-agnostic reference structure in the form of a set of vectors $A \\in \\mathbb{R}^{N_c \\times D_l}$. Each vector is obtained by encoding the $N_c$ class label names through a pre-trained SentenceTransformer model with output dimensionality $D_l$ trained for semantic similarity [55]. Following literature on relative representations [46], we treat the vectors in $A$ as a reference set of anchors. Given a vector $v \\in \\mathbb{R}^{D_l}$, we define its relative encoding with respect to anchors $A$ as $r^v = rel(v, A) = [cos(v, A[1]), ..., cos(v, A[N_c])]$, where $cos(\\cdot,\\cdot)$ is the cosine similarity (i.e., the cosine of the angle $\\theta$ between two vectors), and $A[i]$ is the anchor in $A$ associated to class i. Intuitively, vector $r^v$ encodes the geometrical/semantic relationships between vector $v$ and the reference language anchors $A$. Following this logic, the set of all reference affinities is defined as $r^A = [rel(A[1], A), ..., rel(A[N_c], A)]$ where each class anchors $A[i]$ is represented according to its relationship with respect to the other anchors in $A$. During the training of the cross-domain visual classifier (Stage 3), these encodings are used to enforce the learned latent space to follow a similar structure as the one induced by $A$."}, {"title": "3.3. Stage 2: Training of the Language Supervisor", "content": "Similar to [27], we train a language model to map image captions to semantic labels, with the aim to provide pseudo-labels for target samples, which are associated with captions but not class labels. Differently from [27], we use the language supervisor both to provide pseudo-labels for target samples, useful to train the classifier, and textual representations semantically structured as the domain-agnostic anchors $A$, useful to encourage alignment to the reference structure. Hence, rather than training a regular classifier, we directly supervise a language model $G$ to provide representations that are 1) geometrically aligned to $A$ and 2) suitable for predicting class labels. Specifically, given a pair of source caption and corresponding label $(l, y)$, $G$ processes $l$ to produce a vector representation $z$:\n$$z = G(l).$$"}, {"title": "3.4.1. Structure Learning", "content": "Next, we encourage the vector representation $z$ to be geometrically aligned to the anchor $A[y]$ corresponding to ground truth action $y$. To do so, we computer $r_z = rel(z, A)$, the relative encoding of $z$, and supervise it to be similar to $r_{A[y]} = rel(A[y], A)$, the relative encoding of the anchor $A[y]$, with the following structure-preserving loss:\n$$L_s = ||r_{A[y]} - r_z||_1.$$\nThis loss encourages the encodings $z$ of text descriptions $l$ associated with labels $y$ to preserve the same geometrical associations as their corresponding anchor, encouraging the latent space learned by $G$ to mirror the structure defined by $A$. To further favor alignment to $A$, rather than employing a classification head, we predict class probabilities directly by Softmax-normalizing relative encodings:\n$$p_j = \\frac{e^{r^z_j}}{\\Sigma_k e^{r^z_k}}.$$\nFinally, we train the model using a combined loss $L$:\n$$L = \\lambda_1 L_{CE}(p^z, y) + \\lambda_2 L_s(r^z, r_{A[y]}),$$"}, {"title": "3.4. Stage 3: Cross-Domain Visual Classifier", "content": "where $L_{CE}$ is the cross-entropy loss, while $\\lambda_1$ and $\\lambda_2$ are hyperparameter weights to calibrate the magnitude of each loss. We refer to the label predicted by $G$ from $l$ as $\\hat{y}$.\nStage 3 aims to train a cross-domain visual classifier aligning representations extracted through a visual encoder $V$ to the structure imposed by the domain-agnostic language anchors $A$. We allow each domain to develop its own latent space, but we encourage both spaces to be aligned to two sets of learnable anchors $A_t \\in \\mathbb{R}^{N_c \\times D_v}$ and $A_s \\in \\mathbb{R}^{N_c \\times D_v}$ specific to the target and source domain respectively. We further ensure that the structures of $A_t$ and $A_s$ are aligned to the domain-agnostic, fixed anchors $A$ through the supervision provided by ground truth labels $y$ in the source domain and the text supervision provided by $G$ in the target domain in the form of the predicted vector $\\hat{z}$ and the predicted pseudo-label $\\hat{y}$. The source and target datasets are merged ($\\mathcal{X} = \\mathcal{X}_s + \\mathcal{X}_t$) and used for training with a total of $M = N_s + N_t$ samples. Source samples comprise an image, caption, and label triplet $(x_i, l_i, y_i)$, whereas target samples $(x_i, l_i, \\hat{y}_i)$ replace label $y_i$ with the pseudo-label $\\hat{y}_i$. Target and source images are processed by $V$ to obtain latent representations $g_t$ and $g_s$, while target captions $l$ are processed by $G$ to obtain latent representations $\\hat{z}$ as follows:\n$$g_t = V(x_t), \\hat{z} = G(l_t), g_s = V(x_s).$$"}, {"title": "3.4.1. Structure Learning", "content": "These representations and the anchors are used to achieve two main training objectives: 1) structure learning and 2) classification training.\nTo ensure the learned visual representations adhere to the structure defined by $A$, LAGUNA employs a supervised learning approach based on relative encodings. For target domain samples (without ground truth labels), we first compute the relative encoding $\\hat{r}_{z} = rel(\\hat{z}, A)$. For source domain samples, we compute the relative encoding of the anchor associated to the ground truth class $A[y]: r_{A[y]} = rel(A[y], A)$. These encodings represent the relative positions of textual representations with respect to the reference space $A$ and are not trainable as $G$ is fixed. On the visual side, we compute the relative encodings of $g_t$ and $g_s$ with respect to the learnable anchors $A_t$ and $A_s$, i.e., $r^{g_t} = rel(g_t, A_t)$ and $r^{g_s} = rel(g_s, A_s)$. We encourage the visual relative encodings ($r^{g_t}, r^{g_s}$) to match the text relative encodings ($\\hat{r}_{z}, r_{A[y]})$ with an L1 loss:\n$$L_s = \\begin{cases} ||r^{g_t} - \\hat{r}_{z}||_1, & \\text{if Target Domain} \\\\ ||r^{g_s} - r_{A[y]}||_1, & \\text{otherwise} \\end{cases}$$\nThis loss encourages the visual representations to maintain relationships with their respective domain-specific visual anchors that mirror the relationships expressed by the domain-invariant anchors $A$. This process supervises both the learnable anchors $A_s, A_t$ and the visual encoder $V$. However, since this loss alone might lead to the collapse of anchor representations into a small region, hindering effective classification, we introduce a volume (or spread) regularization loss. This loss treats each set of anchors as a multidimensional parallelotope whose spread or volume occupied in the latent space can be measured by the determinant of its Gram matrix. Given the Gram matrices for the three sets of anchors:\n$$\\Gamma = AA^T, \\Gamma_s = A_s A_s^T, \\Gamma_t = A_t A_t^T,$$"}, {"title": "3.4.2. Classification Training", "content": "we encourage the volume of each domain-specific parallelotope to be approximately equal to the volume of the reference anchors $A$ with the following regularization loss:\n$$L_{Reg} = |\\logDet(\\Gamma_t) - \\logDet(\\Gamma)| + |\\logDet(\\Gamma_s) - \\logDet(\\Gamma)|,$$\nThe network undergoes classification training concurrently with structure learning. To ensure that source and target visual representations are grounded in the structure imposed by $A$, while still being free to capture domain-specific nuances, we propose a novel cross-domain attention layer that aims to ground domain-specific visual representations $g_t$ and $g_s$ to the structure of source anchors $A_s$. We include two Cross-Domain attention layers sharing the same weights for the target and source branches of the architecture (See Fig. 2). In the target branch, we use $g_t$ as queries, target anchors $A_t$ as keys, and source anchors $A_s$ as values. The output is summed to $g_t$ with a residual connection to include both domain-specific and cross-domain information in the final representation:\n$$f_t = Attention(Q = g_t, K = A_t, V = A_s) + g_t$$\nA similar processing is applied to the source encoding $g_s$:\n$$f_s = Attention(Q = g_s, K = A_s, V = A_s) + g_s$$\nCrucially, the cross-domain attention layer constructs an attention map by leveraging the domain-specific relations between the encoded representations and the anchors (i.e., $Q^TK$). These relations are expected to be similar across domains due to the influence of the structural loss $L_s$. This process results in a unified representation since the output of the attention layer always depends on values coming from the source anchors $A_s$. The residual connection included in the attention layer is introduced to account for domain-dependent class characteristics, which results in final feature vectors $f_t$ and $f_s$. Finally, an MLP classification head processes $f_t$ and $f_s$ to output predictions $\\hat{y}_t$ and $\\hat{y}_s$. The MLP has shared weights across domains and is optimized with a cross-entropy loss using the ground truth label $y_i$ for source examples and the pseudo-label $\\hat{y}$ predicted by $G$ in the case of target samples. The overall training objective of the classifier is the following:\n$$L = \\lambda_1 L_{CE}(y_i, \\hat{y}) + \\lambda_2 L_s(\\hat{r}_z^*, r^{g^*}) + \\lambda_3 L_{Reg}(\\Gamma, \\Gamma^*),$$\nwhere $\\lambda_1$, $\\lambda_2$, and $\\lambda_3$ are hyperparameter weights to calibrate the magnitude of each loss value, and '*' signifies that the argument is domain-dependent."}, {"title": "4. Experiments", "content": "where $\\lambda_1$, $\\lambda_2$, and $\\lambda_3$ are hyperparameter weights to calibrate the magnitude of each loss value, and '*' signifies that the argument is domain-dependent."}, {"title": "4.1. Experimental Setup", "content": "Datasets. We evaluate LAGUNA on four comprehensive UDA benchmarks: DomainNet [50], GeoPlaces [26] GeoImnet [26], and Ego2Exo [20, 27]. DomainNet consists of 400K images across 345 classes and is used to evaluate performance in 12 domain transfer settings [27, 68]. GeoImnet and GeoPlaces are subsets of the larger GeoNet dataset [26], contain over 750K images and focus on geographic disparities in image classification (GeoImNet with 600 classes) and scene recognition (GeoPlaces with 205 classes). Finally, Ego2Exo is a subset of EgoExo-4D curated for domain adaptation. It involves a total of 9086 videos and 24 action categories. DomainNet and GeoNet include per-image captions derived from metadata and BLIP-2 [32], respectively, while Ego2Exo provides video descriptions from the original EgoExo-4D dataset.\nModel and training details. Our method comprises three base components: a language model for structure definition (Stage 1), a text supervisor model (Stage 2), and a visual backbone (Stage 3). We use a pre-trained SentenceTransformer [55] for structure definition and a BERT base model [59] as text supervisor. The latter is trained on the source domain of each scenario for 5 epochs with a batch size of 64, a learning rate of 1e-4, and the AdamW [41] optimizer. The visual encoders are tailored to each dataset. For DomainNet, we adapt the Swin-B backbone [37], following prior work [25, 73, 77]. For GeoImnet and GeoPLaces, we utilize the ViT-B backbone [12] following [68, 77]. For Ego2Exo, we employ pre-extracted Omnivore-L features [20] as in [27]. All vision models undergo joint training with the other network components for 10 epochs with a batch size of 32, an initial learning rate of 1e - 4, and cosine scheduling. We set the loss weights to $ \\lambda_1$=1.0 $ \\lambda_2$=0.1 and $ \\lambda_3$=0.001 to normalize the relative magnitude of the related loss functions."}, {"title": "4.2. Results", "content": "We benchmark our model against various domain adaptation methods that have reported results on the considered datasets [7, 25, 68, 73, 75, 77]. Besides classic UDA methods, we also compare with recent approaches using language guidance [27]. Following the evaluation protocols of each benchmark, we report accuracy for DomainNet, GeoImNet, and GeoPlaces, while for Ego2Exo, we use per-class mean accuracy as reported in [27].\nDomainNet. Table 1 presents the results on the DomainNet dataset, encompassing 12 domain adaptation scenarios across four distinct domains: Real (R), Clipart (C), Sketch (S), and Painting (P). LAGUNA is compared against previous state-of-the-art methods utilizing the Swin-B backbone to ensure a fair comparison. Notably, LAGUNA surpasses all prior work in all 12 scenarios, achieving an average accuracy improvement of +3.32%.\nGeoImnet & GeoPlaces. Table 2 presents the results on GeoImnet and GeoPlaces covering adaptation between USA (U) and Asia (A) geographical domains. LAGUNA consistently outperforms previous methods, achieving improvements of +3.72% (U\u2192A) and +5.81% (A\u2192U) on GeoImnet, and +5.01% (U\u2192A) and +6.49% (A\u2192U) on GeoPlaces. This translates to an average improvement of +5.26% over prior art across all scenarios.\nEgo2Exo. LAGUNA demonstrates consistent performance gains also on the challenging recent Ego2Exo action recognition benchmark based on EgoExo4D [20], as introduced in [27] (Table 3). Following the definition of this benchmark [27], we report per-class average accuracy due to the dataset's imbalanced class distribution. LAGUNA surpasses all prior work, achieving improvements of +1.18% (Ego\u2192Exo) and +2.69% (Exo\u2192Ego), resulting in an overall average gain of +1.94%.\nThese consistent improvements across all tested datasets, with a total of 18 scenarios, underscore the effectiveness of our approach both when considering classic UDA algorithms and recent methods exploring the use of language guidance to assist domain adaptation."}, {"title": "4.3. Ablation Study", "content": "In this section, we analyze the effect of the individual components of LAGUNA to provide an in-depth assessment of model performance and justify our design choices. Specifically, we investigate (a) the impact of our methodological elements, (b) the effects of different reference structures, (c) the behavior of LAGUNA with different ratios of pseudo-labels, and (d) the qualitative illustration of the alignment of relative and absolute spaces. More ablations are reported in the Supplementary Material.\nMethodological Elements. In Table 4, we ablate the main methodological elements of LAGUNA on GeoImnet. We set up this ablation by removing the core elements of LAGUNA, namely the structure loss ($L_s$ - Eq. (6)), the use of domain-agnostic reference anchors $A$, the learnable anchors $A_{t/s}$, the cross-domain attention (CD Attn. - Eq. (9), (10)), and the regularization loss ($L_{Reg}$ - Eq. (12)). We then progressively add the considered elements until we reach LAGUNA (settings (1)-(5)). For all settings, we report the relative improvement (w.r.t. previous row) and the absolute one (w.r.t. first row). In setting (1), the visual classifier is trained on source and target samples using labels and pseudo-labels predicted by the text supervisor, which leads to a baseline model akin to LaGTran [27]. In setting (2), we add the reference anchors $A$ and train the visual classifier as in setting (1), but we also add a loss to align visual representations with the related class anchors in absolute coordinates through cosine similarity. This yields an improvement of +0.87 with respect to (1), suggesting that imposing a reference structure beyond pseudo-labels is beneficial to performance but that absolute alignment is too restrictive. In setting (3), we extend (2) with a set of domain-independent learnable anchors (\u2713 * in Table 4, indicating that the weights of $A_t$ and $A_s$ are shared) and add the structure-preserving loss $L_s$ (Eq. 6). The introduction of learnable anchors enables the relative alignment of the visual domain with the reference language domain, leading to an improvement of +1.32 with respect to (2) and +2.10 over (1), which highlights the benefits of the proposed relative alignment over an absolute one. Considering domain-specific anchors in setting (4) allows source and target domains to focus on the individual characteristics of the respective domains, leading to a further improvement of +1.77 with respect to (3) and a robust +3.87 with respect to the baseline (1). Adding cross-domain attention in setting (5) allows to bridge the gap between target and source representations, leading to an additional improvement of +0.44 with respect to (4) and an overall improvement of +4.31 with respect to (1). We achieve the final configuration in LAGUNA, where we add the regularization loss, which prevents collapse in learning domain-specific anchors and leads to a further improvement of +1.16 with respect to (5) and an overall gain of +5.47 with respect to the baseline (1). Together, these improvements show the advantages of the careful design of LAGUNA, highlighting the benefits of learning domain-specific visual representations still aligned to a reference language structure.\nAblation on reference structures. Fig. 3 delves into the importance of defining a semantic structure for our method (Stage 1). We ablate three models: SentenceTransformer[55], CLIP [54], and BERT [59], each with varying capabilities of extracting semantic relationships. The figure shows semantic similarity maps for 100 randomly selected classes from GeoImnet and the average accuracy obtained by LAGUNA on the entire dataset when using the specified model. As can be seen, SentenceTransformer keeps low similarities for distinct classes (low values off-diagonal), BERT maintains high similarities for most classes but also presents some spurious correlations between distinct classes, while the behavior of CLIP is in-between. SentenceTransformer, which can better differentiate between semantic relations, leads to the best accuracy.\nAblation on the ratio of target samples. LAGUNA demonstrates strong generalization capabilities even with limited target domain data, as shown in Table 5. We progressively increase the amount of pseudo-labeled target data used for training (10%, 20%, 30%, 50%, 75%) and observe the impact on performance. Notably, LAGUNA achieves high accuracy even with only 10% of the target samples and attains state-of-the-art results with just 20%. This efficiency underscores the benefits of our structure-driven approach, which organizes sample projections within domain-specific representation spaces while leveraging a reference geometrical structure to guide their placement.\nQualitative illustration of embedding spaces. LAGUNA uniquely aligns representation structures without enforcing alignment in absolute coordinates but rather leveraging a reference structure defined by a language model and the class labels to guide the alignment of inter-class affinities. Figure 4 provides a qualitative validation of this approach. Using t-SNE, we visualize the projection of 1000 randomly selected samples from the GeoImnet validation set, where the source and the target domains belong to the USA and Asia, respectively. Notably, the features are well separated in absolute coordinates, reflecting their unique domain-related visual properties. However, when using relative encodings with domain-specific learnable anchors, the features blend together due to the structure-preserving loss $L_s$. This demonstrates that semantic alignment can be achieved without forcing representations into a shared space, allowing each domain to preserve its unique characteristics while adhering to a common structure."}, {"title": "5. Conclusions", "content": "This work introduces LAGUNA, a novel domain adaptation approach leveraging geometrical structures of semantically equivariant spaces to guide adaptation. LAGUNA defines a reference representation space structure based on domain-agnostic class semantic similarities encoded by a language model, ensuring the organization of sample projections reflects this structure while preserving domain-specific characteristics. By conditioning the classifier to adhere to this structure, LAGUNA encourages structural similarity across domain-specific latent spaces, retaining unique features for improved classification. This is achieved through pseudo-labeling and learnable domain-specific anchors, guided by a loss function that prioritizes mimicking geometrical associations over direct representation alignment. Extensive experiments demonstrate LAGUNA's superior performance compared to existing methods. This research highlights the importance of structural alignment and language-guided learning in domain adaptation. Future work could explore extending LAGUNA to more complex scenarios and multi-modal data."}, {"title": "A. Introduction", "content": "In this document, we report additional ablation studies regarding the choice of language models and the structural guidance of the target domain. Moreover, we provide a more detailed explanation of the motivation behind our regularization loss $L_{Reg}$ in Eq. (8). Finally, we attach a Python file, LAGUNA.py, demonstrating the implementation of LAGUNA in PyTorch. We will release the full code and configurations upon acceptance."}, {"title": "B. Ablation on Language Models", "content": "In addition to the language model used for defining the reference structure through $A$ in Stage 1", "55": "CLIP [54", "59": "on GeoImnet dataset in two settings: (1) using SentenceTransformer-defined $A$ (Table 6) as it can better model semantic relationships (recall Fig. 3 from the main manuscript) and (2) defining $A$ with the same language model as the one chosen for training (Table 7).\nIn setting (1), BERT outperforms CLIP and SentenceTransformer, motivating our choice for the language model. In setting (2), CLIP achieves the highest accuracy, but its overall performance remains lower than BERT's in setting (1). These results not only emphasize the best model and justify our choice but also demonstrate that using different models for structure definition and training can boost performance since the choices are tailored for their specific characteristics. Specifically, the Stage 1 model is selected for its ability to model meaningful relationships between classes, while Stage 2 is intended for building"}]}