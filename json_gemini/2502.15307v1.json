{"title": "Road Traffic Sign Recognition Method Using Siamese Network Combining Efficient-CNN-Based Encoder", "authors": ["Zhenghao Xi", "Yuchao Shao", "Yang Zheng", "Xiang Liu", "Yaqi Liu", "Yitong Cai"], "abstract": "Traffic signs recognition (TSR) plays an essential role in assistant driving and intelligent transportation system. However, the noise of complex environment may lead to motion-blur or occlusion problems, which raise the tough challenge to real-time recognition with high accuracy and robust. In this article, we propose IECES-network which with improved encoders and Siamese net. The three-stage approach of our method includes Efficient-CNN based encoders, Siamese backbone and the fully-connected layers. We firstly use convolutional encoders to extract and encode the traffic sign features of augmented training samples and standard images. Then, we design the Siamese neural network with Efficient-CNN based encoder and contrastive loss function, which can be trained to improve the robustness of TSR problem when facing the samples of motion-blur and occlusion by computing the distance between inputs and templates. Additionally, the template branch of the proposed network can be stopped when executing the recognition tasks after training to raise the process speed of our real-time model, and alleviate the computational resource and parameter scale. Finally, we recombined the feature code and a fully-connected layer with SoftMax function to classify the codes of samples and recognize the category of traffic signs. The results of experiments on the Tsinghua-Tencent 100K dataset and the German Traffic Sign Recognition Benchmark dataset demonstrate the performance of the proposed IECES-network. Compared with other state-of-the-art methods, in the case of motion-blur and occluded environment, the proposed method achieves competitive performance precision-recall and accuracy metric average is 88.1%, 86.43% and 86.1% with a 2.9M lightweight scale, respectively. Moreover, processing time of our model is 0.1s per frame, of which the speed is increased by 1.5 times compared with existing methods.", "sections": [{"title": "I. INTRODUCTION", "content": "WITH the development of computer technologies, image recognition has been widely applied in smart fields and scenarios such as security defense, imaging medicine and auto driving [1].\nTraffic sign recognition (TSR) is an important problem for image recognition. It plays a crucial part in driver assistance systems and autonomous driving systems nowadays [1], [2]. Following regulations with standard shapes and color, traffic signs should be easily detected and recognized by pattern recognition systems in theory. Nevertheless, when the vehicle is driving, images of traffic signs captured by camera on vehicles can be affected by the noise of complex factors from outside environment such as weather conditions, illumination, occluded by leaves or bars and motion artifacts [3], [4]. These non-ideal images hugely increase the difficulty level of the TSR problem, so researchers keep on raising new algorithms to improve the execution efficiency, recognition rate and robustness [5].\nIn order to solve the problems as above, a novel traffic sign recognition method of Siamese neural network with Convolutional encoder was proposed in this article, and this method is applied to significantly enhance improve recognition rate and robustness of traffic sign recognition. The remarkable ability of encoder and Siamese network have already been revealed by Liu [6], Zhang [7] and chen [8] recently. The main contributions of this paper include the following:\n1) Convolutional neural network to extract and encode the traffic sign features of training samples and reference samples.\n2) Siamese neural network is designed with Efficient-CNN based encoder and contrastive loss function, which can be trained to improve the robustness of TSR problem when facing the samples of motion-blur and occlusion.\n3) Recombined feature code and a fully-connected layer with SoftMax function to classify the codes of samples and recognize the category of traffic signs."}, {"title": "II. RELATED WORK", "content": "Traffic sign recognition technology is mainly composed of traffic sign detection and classification. As for traffic sign detection, traditional methods based on features such as particular color and shape of traffic signs have been widely researched. By converting RGB color space to other color spaces, [9] and [10] use the color threshold to extract focus areas and reduce the interference of illumination."}, {"title": "III. OUR APPROACH", "content": "The framework of our proposed IECES-network is shown in Fig. 1. When inputting an image captured from the on- vehicle camera, we first extract and encode its features with a convolutional encoder based on Efficient-CNN in the first branch. Then we compute the distance of pairs of codes with the input image and template signs with Siamese network to optimize the loss function during the training of encoder. In this way, recombined feature codes of input image can be produced by the improved encoder with closer distance with the templates, which augment the encoder and generate more similar input codes with the template. Finally, the classification step is executed by the improved encoder and a fully-connected layer to output the correct class of the input image."}, {"title": "A. Coding Sign Features: Convolutional Encoder Based on Efficient-CNN", "content": "It's also generally known that the traffic signs are always designed with clear rules and regulations, so that these signs usually presented with prescriptive samples. However, when the traffic signs are captured with the noise of motion-blur and occlusions under different conditions of surroundings, these traffic sign images do not correspond to the prescriptive samples, and very difficult to recognize them. Therefore, we extract the features of traffic signs with simple prior knowledge from these clear regulations. Following this route, we propose a new method to solve the TSR problem with convolutional encoder to encode traffic signs in this section, as shown in Fig. 2. The convolutional encoder has remarkable ability, which was revealed by Liu [26] and Zhang [7].\nAccording to the universal approximation theorem [27], neural networks can approximate any function that maps from any finite dimensional discrete space to another. We choose Efficient-Convolutional Neural Networks (Efficient-CNN) [28] as a mapping function \\(G_w\\), and use \\(G_w(.)\\) to establish a convolutional encoder to extract the feature codes \\(G_w(I)\\) of traffic sign images I, w is the parameter of convolutional encoder. This encoder is designed to provide feature codes for classification network in the following three parts: primary extraction layer, inception module [29], [30], [31] and convo- lutional module.\nWe first normalize the input traffic sign images as 3 x 48 x 48 samples, 3 is the number of channels, 48 \u00d7 48 is the resolution ratio of these images.\nPart 1: In the primary extraction layer, the sample gets through convolutional layers and a max pooling layer. We can get primary features maps contains of the basic information such as edge and color, etc. To ensure each primary features grids' scale is the same for the next layer or module's process, we fulfill the sample's surround with 0 to adjust these grids to the same scale as the input traffic sign image (3 \u00d7 48 \u00d7 48). In this way, we receive the 3 \u00d7 48 \u00d7 48 scale primary features maps.\nPart 2: We input the primary features maps into the Inception module to extract multi-scale features. This module consists of different scales of convolution kernels based on the Inception V3 network. With the design of asymmetric convolution, through the 3 x 1, 1 x 3, 7 x 1, 1 \u00d7 7 kernels, we can cascade input images and have the muti-scale features map.\nPart 3: In the convolutional module. We send the muti-scale features cascade map through the max pooling layers, con- volutional layers and a linear fully connected layer to output the 256-bit feature code \\(G_w(I)\\). \\(G_w(I)\\) contains of all crucial information filtered by previous modules.\nAs above, we introduce a Convolutional encoder module based on Efficient-CNN to extract and encode features. From the output of this part, the input traffic sign images have become codes with multi-scale information."}, {"title": "B. Computing the Distance: Siamese Parallel Network", "content": "To reduce the interference of noise on the feature encod- ing recognition of traffic signs, we propose a method that uses Siamese network to reduce the difference between the influenced image and templates in the IECES-network. The structure of our Siamese network as shown in Fig. 3. In this method, we compute the distance between the image pairs, and choose the pair of shortest distance as the result.\nWe add the encoder from Section III. A as a part of the Siamese network and double it, so that we can have two sub-networks with the same structure. We let there are n training samples, the set of training samples is \\(X = {x_1, x_2,..., x_i, ..., x_n}\\), \\(x_i (i = 1, 2, \u2026, n)\\) is any one from X and \\(G_w(x_i)\\) is the feature code of training sample \\(x_i\\). Similarly, we let C categories, the set of standard images (one standard image per category) as \\(Y = {y_1, y_2, \u2026\u2026\u2026, y_j, \u2026\u2026\u2026, y_c}\\), \\(y_j (j = 1,2,..., C)\\) is any one from Y and \\(G_w(y_j)\\) is the feature code of standard sample \\(y_i\\). Then, we can get a code pair of a training sample and a standard sample \\(G_w(x_i), G_w(y_j)\\).\nAlso, we have the sharing set of weight w. \\(G_w(x_i), G_w(y_j)\\) can be seen as the convolutional code in the comparison space from the encoders \\(G_w\\). Considering the possible blur or occlu- sion in X, we show sample's code pairs \\(G_w(x_i), G_w(y_j)\\) to the mapping function (1). By going through the Section III-A, the image pair should be transformed into code pairs.\n\\[D_w = ||G_w(x_i) \u2013 G_w(y_j) ||\\]\n(1)\nThe code pairs will be sent to compute the Euclidean distance \\(D_w\\) [32] to check the difference between the sample \\(x_i\\) from X and the sample \\(y_j\\) from Y. The contrastive loss function can be defined as follows:\n\\[L_{sim} = (1 - y) D_w^2 + y (max (0, m \u2013 D_w))^2\\]\n(2)\nwhere, y is a variable to judge whether the training sample X belongs to the same category of the standard traffic sign image, and m is the threshold to separate the different categories. When y = 0,\\(x_i\\) belongs to the category of \\(y_j\\), otherwise y = 1 it's not. The function can be defined as below in details:\n\\[L_{sim} = \\begin{cases}D_w^2 & \\gamma = 0 \\\\0 & y = 1, D_w^2 > m \\\\m - D_w^2 & y = 1, D_w^2 < m\\end{cases}\\]\n(3)\nWith function (3), we can regard the training process of using the same category as Expectation Maximization (EM) algorithm [8]. During the training, we optimize the encoder of X and Y alternately. Let the clear sample be shown to the"}, {"title": "C. Classification: Recombined Feature Code and a Fully-Connected Layer", "content": "To classify the codes from traffic signs, we need a classifi- cation module assembled with full connected layers based on SoftMax.\nOnly the code from \\(x_i\\) will be sent to the classification as it's the target of our mission. The classification code Z can be generated via function (11):\n\\[Z = w*G_w (x_i) + b\\]\n(11)\n@ is the parameter of transformation matrix and b is the bias. We then establish SoftMax function with Z and compute \\(p_c\\), the possibility of each category:\n\\[P_c = \\frac{e^{Z_c}}{\\sum_{k=1}^{C} e^{Z_k}}\\]\n(12)\n\\(Z_c\\) is the c bit of the classification code Z, while \\(z_k\\) is one exact bit between the first bit and c. By choosing the most possible category from function (12), we can put accurate recognition on the traffic signs.\nTo train the classification, we compute the cross-entropy loss \\(L_{class}\\) by function (13):\n\\[L_{class} = -\\sum_{i=1}^{C} x_{ic} ln (p_c)\\]\n(13)\nwhere \\(x_{ic}\\) is the category of the input training sample. We can generate the general loss function L as function (14):\n\\[L = \\alpha L_{sim} + L_{class}\\]\n(14)\nWe set the hyperparameter \\(\\alpha\\) = 0.1 to optimize the contrastive loss \\(L_{sim}\\)."}, {"title": "D. Convergence of the Contrastive Loss Function", "content": "In order to further analyze and compare the convergence of loss function L, we set the sample pair selected by training contains a pair of positive examples \\(x_i, y_{same}\\) and a pair of negative examples \\(x_i, y_{diff}\\). The contrastive loss of positive examples is \\(L_{same}\\), \\(L_{diff}\\) for the negative examples via function (3) (\\(L_{diff} \\neq 0\\)). When the codes from encoder of the Section III-A satisfy the Constraint 1, the codes are regarded as the valid codes.\nConstraint 1: \\(\\exists m_{sep} > 0, D_{same} + m_{sep} < D_{diff}\\).\nWhere,\\(m_{sep}\\) is the boundary for separation, \\(D_{same}\\) from the function (1) when the code pairs are positive examples, and \\(D_{diff}\\) also from the function (1) when the code pairs are negative examples."}, {"title": "IV. EXPERIMENT", "content": "In our experiment, we used the German Traffic Sign Recognition Benchmark dataset (GTSRB) [33] and Tsinghua- Tencent 100K dataset (TT100K) [34] such as Fig. 5 to validate the effectiveness of our proposed IECES-network method. GTSRB contains over 50,000 images of German road signs in 43 classes, where we randomly chose 32,087 images as training samples and 12,629 for the test samples. Then, we set the validation dataset with 20% of training and test samples.\nMoreover, we will continuously obtain new segmentation result data by deploying the segmentation method obtained from training the dataset on each terminal of the IoT network. The new segmentation result data continue to supplement our dataset to optimize the model training results. We achieve a closed-loop from \"data acquisition\", \"data processing\", to \"result sending\" to solve the problems of high time cost, slow inference speed, and low segmentation accuracy in the current method using deep learning models to process coal maceral group segmentation.\nTT100K is another public dataset contains 10,000 cropped images and 30,000 traffic sign instances from 162 cate- gories. These images cover large variations in illuminance and weather conditions. We used the 2016 annotation of TT100K (containing 42 categories of traffic signs) and randomly chose 19,187 images as training set, 10813 for testing and 20% of all for validation. Taking into account the network compatibility in the experiment, we uniformly cut the input images into the size 48 x 48."}, {"title": "A. Experimental Data", "content": "In our experiment, we used the German Traffic Sign Recognition Benchmark dataset (GTSRB) [33] and Tsinghua- Tencent 100K dataset (TT100K) [34] such as Fig. 5 to validate the effectiveness of our proposed IECES-network method. GTSRB contains over 50,000 images of German road signs in 43 classes, where we randomly chose 32,087 images as training samples and 12,629 for the test samples. Then, we set the validation dataset with 20% of training and test samples.\nMoreover, we will continuously obtain new segmentation result data by deploying the segmentation method obtained from training the dataset on each terminal of the IoT network. The new segmentation result data continue to supplement our dataset to optimize the model training results. We achieve a closed-loop from \"data acquisition\", \"data processing\", to \"result sending\" to solve the problems of high time cost, slow inference speed, and low segmentation accuracy in the current method using deep learning models to process coal maceral group segmentation.\nTT100K is another public dataset contains 10,000 cropped images and 30,000 traffic sign instances from 162 cate- gories. These images cover large variations in illuminance and weather conditions. We used the 2016 annotation of TT100K (containing 42 categories of traffic signs) and randomly chose 19,187 images as training set, 10813 for testing and 20% of all for validation. Taking into account the network compatibility in the experiment, we uniformly cut the input images into the size 48 x 48."}, {"title": "B. Experimental Augmentation", "content": "To simulate the influenced images from vehicle camera, we need to simulate the occlusion and motion-blur scene in the training process so that our proposed IECES-network model can gain the robustness facing these noised images.\nTo present the occlusion in pictures, according to the fruits from [35], we performed with \"Random Erasing\". Random Erasing is executed with a certain probability. For an image I in a mini-batch, the probability of it undergoing Random Erasing is p, and the probability of it being kept unchanged is I-p. In this way, training images with levels of occlusion are generated. This effective method randomly selects a rectangle region Ie in an image, and erases its pixels with random values. On the other words, we erase a random rectangle part of training image to stimulate the occlusion scene, shown in the Fig.6c and 6d.\nTo generate motion-blur scene in training images, we aug- ment both datasets by using the standard image for each class of traffic signs in training set X as shown in Fig. 5 and Fig. 6a, adding 5-10 pixels of motion blur to the training sets with an rotation angle in the range [0\u00b0, 180\u00b0] randomly as motion-blur interference as shown in Fig. 6b and Fig. 6e.\nAdditionally, for TT100k, we deployed further augmenta- tion [34] by rotating some of its images randomly by an amount in the range [-20\u00b0, 20\u00b0]. We also scaled it randomly"}, {"title": "C. Evaluation Metrics", "content": "To evaluate the recognition performance, we use the precision-recall metric and accuracy. The precision-recall is a curve that measures the trade-off between precision and recall [22]. The equations of these scores are shown as function (20)-(22).\n\\[Precision (P) = \\frac{TP}{TP+FP}\\]\n(20)\n\\[Recall (R) = \\frac{TP}{TP+FN}\\]\n(21)\n\\[Accuracy (A) = \\frac{TP+TN}{TP+FP+TN+FN}\\]\n(22)\nwhere, True Positive (TP) means that estimated sign region has at least 50% overlap with ground truth sign region with sign type being correctly classified. False Positive (FP) means that estimated sign region is incorrectly identified or has no over- lapping with ground truth sign region or type. True Negative (TN) means that no identification of traffic sign in non-sign regions. False Negative (FN) means that no identification of traffic sign in sign regions. With these metrics, typical scores are defined to summarize the performance of methods."}, {"title": "D. Experimental Setting", "content": "The experiments are conducted on a workstation with INTEL (R) XEON (R) E5-2680 v4 @2.40GHz and NVIDIA Tesla P40. We implement our IECES-network model by using the PyTorch v2.0 frameworks with CUDA 11.7.512 images from training set will be used as one batch, and all testing model is executed with Adaptive moment estimation optimize (Adam). In our experiment, we set the learning rate of Adam as 3 \u00d7 10-4, the weight decay as 2 \u00d7 10\u20137. Training results will be saved every 20 batches, and the validation of the model will be executed each epoch until the training loss achieved the convergence."}, {"title": "E. Performance on GTSRB", "content": "As our proposed IECES-network model is based on the codes of images, \\(D_w\\) between \\(G_w(x_i)\\) and \\(G_w (y_i)\\) from GTSRB and TT100k becomes essential to the encoder's valid- ity. By generating the heatmap of distance between test image convolutional code and template convolutional code as shown in Fig. 7-9, we can easily find that each distance of same classes is the shortest. These distance below 1.5 can be clearly separated from the distances of different classes, which were above 2.5. For the motion blur images and part-occlusion images, few distances come above 2.5 which as shown in Fig. 9 and 10, because the distances are related to the similarity"}, {"title": "F. Performance on TT100K", "content": "We evaluate the proposed IECES-network mode on the TT100K dataset to further validate its efficiency and robustness by the accuracy metric. The test image of TT100K dataset contains complex background and different sizes of traffic signs. A typical traffic sign may occupy only 0.2% of the area in images of 2048 x 2048 pixels. It becomes a great challenge to recognize small objects with the complex interference from environment.\nAccording to different traffic sign sizes in pixel, TT100K is divided into three categories: small [0, 32], medium [32, 96], and large [96, 400]. We compare the proposed IECES-network mode with other SOTA methods such as Min [22], Zhu [34], PVT [36], LNL [37] and Wang [23] to evaluate its performance on TT100K dataset. The typical recognition results on TT100K with our method in different case as shown in the Fig. 10.\n1) The Comparison on Original Images: Table II shows the proposed IECES-network mode's accuracy on small samples was 0.1% lower than Min, the precision and recall takes the second best was 1.6% and 0.9% lower than the best one, respectively. 1.6%, 3% and 1.6% lower with medium samples on accuracy, precision and recall as the runner-up. 2.1%, 0.3%, 2.9% lower with large samples on accuracy, precision and recall also as the second best. With the comparison of each class in Table VII of the Appendix, we find that Min takes the best performance on 40% of classes, while Wang occupies 22.2% and ours is 42.2%. However, Table II shows the proposed IECES-network mode had the smallest parameters which is nearly 45% of Wang and 0.6% of Zhu. Besides, we take the fastest FPS among these methods. Typical recognition results on original images are shown in Table III.\n2) The Comparison on Motion-Blured Images: Our IECES-network mode achieves the best accuracy on each size. Its accuracy performs 4.8% higher than Min in small samples, 2.8% higher in medium samples, 2.6% and 3.4% higher than Wang in large and overall samples. With the detail comparison, Min performs best on 11.1% of classes, Wang occupies 8.9% and ours takes 82.2% in Table VIII of the Appendix. The precision and recall also takes the best, we demonstrate the robustness from IECES-network mode method has when the input images become more motion noise.\n3) The Comparison on Occluded Images: The proposed IECES-network mode shows the remarkable result comparing to the others. The overall accuracy reaches 11.4% higher than the second-best method. The lead of the IECES-network mode can beat others in small, medium and large samples at"}, {"title": "G. Ablation Study and Robustness Analysis", "content": "1) Ablation Studies on Training Speed: To more intuitively demonstrated the better performance of the proposed method, we conducted the ablation study and the results are shown as Table V.\nIn order to highlight the effectiveness our proposed network, we conduct dataset testing of four kinds of trained models. As the parameter of our work is 2.9M, we pick a 14.6M param- eter model (Spatial Transformer Networks [38], STN [39]) and regard it as a large model to compare. Also, we choose the Committee of CNNs (Com-CNN) [40] as a small one because of its scale of 1.5M parameter. Efficient-CNN [28] of 2.8M parameter is trained as the similar size model of ours. With the same settings and input samples, the results come out and are shown below:\nAs observed from train accuracy line graph of all tested models, each method can achieve the accuracy above 98% with enough epochs except Com-CNN. STN reaches 98.05% with only 2000 iterations while the proposed method needs 4000. Efficent-CNN takes similar performance with 3000 iter- ations. It means the proposed method needs more positive and negative samples to seek suitable optimized parameters and ensure the encoder's performance.\nAs for the Loss functions, the proposed method uses contrastive loss function to enhance the encoder and the cross-entropy loss function to optimize the classification layer, presented a lower speed than Efficient-CNN, which includes"}, {"title": "V. CONCLUSION", "content": "In order to further increase the precision-recall and accuracy metric of traffic signs recognition in the motion-blurred and occluded images' scenes, this article proposed a novel traffic sign recognition approach by combining Efficient-CNN based encoders with Siamese network and fully-combined layers. With light Efficient-CNN based encoders which introducing primary extraction layer and Inception model at first, the more useful multi-scale feature cascade map was gained. Then, we designed the IECES-network with contrastive loss function, which could optimize the encoder of inputs images by computing the distance between inputs and templates. Next, the refinement classifier with recombined feature code was proposed, it retained the effectiveness of a simple structure.\nExperimental results indicated that the precision-recall and accuracy metric of the IECES-network proposed in this article increased obviously, and especially facing the environment with the noise of motion-blur and occlusion. Moreover, our proposed network costed less parameters and times than other SOTA methods. As above, the advantages of the IECES-network could be mainly concluded as three aspects: high recognition ability, less computation, and good robust- ness. Based on the important roles played by TSR in assistant driving and intelligent transportation research, it was assured that our research could promote the development of these areas, a light and fast model convenience to mobile and embedded devices. In the future, we would like to fuse more prior elements from other kinds of sensors into our system to achieve superior performance."}]}