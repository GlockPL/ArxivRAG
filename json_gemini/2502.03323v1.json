{"title": "Out-of-Distribution Detection using Synthetic Data Generation", "authors": ["Momin Abbas", "Muneeza Azmat", "Raya Horesh", "Mikhail Yurochkin"], "abstract": "Distinguishing in- and out-of-distribution (OOD) inputs is crucial for reliable deployment of classification systems. However, OOD data is typically unavailable or difficult to collect, posing a significant challenge for accurate OOD detection. In this work, we present a method that harnesses the generative capabilities of Large Language Models (LLMs) to create high-quality synthetic OOD proxies, eliminating the dependency on any external OOD data source. We study the efficacy of our method on classical text classification tasks such as toxicity detection and sentiment classification as well as classification tasks arising in LLM development and deployment, such as training a reward model for RLHF and detecting misaligned generations. Extensive experiments on nine InD-OOD dataset pairs and various model sizes show that our approach dramatically lowers false positive rates (achieving a perfect zero in some cases) while maintaining high accuracy on in-distribution tasks, outperforming baseline methods by a significant margin\u00b9.", "sections": [{"title": "1. Introduction", "content": "OOD detection is a critical challenge in machine learning, particularly for classification systems deployed in real-world applications. Identifying when a model encounters inputs that deviate significantly from its training distribution is crucial for ensuring reliability, safety, and alignment with intended use cases. However, effectively detecting OOD samples has proven difficult (Nguyen et al., 2015), largely due to the challenge of obtaining representative OOD data for training robust detectors."}, {"title": "2. Related Work", "content": "Detecting OOD data. In recent years, there has been a growing interest in OOD detection (Fort et al., 2021; Yang et al., 2024; Fang et al., 2022; Galil et al., 2023; Djurisic et al., 2023; Zheng et al., 2023; Wang et al., 2023b; Zhu et al., 2023b; Bai et al., 2023; Ming & Li, 2024; Ghosal et al., 2024). One approach to detect OOD data uses scoring functions to assess data distribution, including:\n\u2022 Distance-based methods (Lee et al., 2018; Tack et al., 2020; Ren et al., 2021; Du et al., 2022a; Ming et al., 2023): These methods compute distances (e.g., Mahalanobis distance or cosine similarity) between a sample and class prototypes in feature space to measure how far a sample is from in-distribution data.\n\u2022 Energy-based scores (Liu et al., 2020; Wu et al., 2023): These scores leverage the energy of a sample computed from the logits of a neural network to determine its likelihood of belonging to the in-distribution or OOD set.\n\u2022 Confidence-based approaches (Bendale & Boult, 2016; Hendrycks & Gimpel, 2017; Liang et al., 2018): These rely on model confidence scores (e.g., softmax probabilities) to identify OOD data, often enhanced by techniques like temperature scaling and input perturbation.\n\u2022 Bayesian methods (Gal & Ghahramani, 2016; Lakshminarayanan et al., 2017; Malinin & Gales, 2019; Wen et al., 2020): They use Bayesian models to quantify uncertainty in model predictions to identify inputs that are significantly different from the training data.\nAnother approach to OOD detection involves using regularization techniques during the training phase (Malinin & Gales, 2018; Geifman & El-Yaniv, 2019; Jeong & Kim, 2020; Yang et al., 2021; Wei et al., 2022; Du et al., 2022b; 2023; Wang et al., 2023a). For example, regularization techniques can be applied to the model to either reduce its confidence (Lee et al., 2017; Hendrycks et al., 2019) or increase its energy (Liu et al., 2020; Du et al., 2022c; Ming et al., 2022) on the OOD data. Most of these regularization methods assume the availability of an additional auxiliary OOD dataset. Several studies (Zhou et al., 2021; Katz-Samuels et al., 2022b; He et al., 2023) relaxed this assumption by either utilizing unlabeled wild data or employing positive-unlabeled learning, which trains classifiers using positive and/or unlabeled data (Letouzey et al., 2000; Hsieh et al., 2015; Niu et al., 2016; Gong et al., 2018; Chapel et al., 2020; Garg et al., 2021; Xu & Denil, 2021; Garg et al., 2022; Du et al., 2024). These approaches rely on the assumption that such external data is both sufficiently available and representative of real-world OOD scenarios. In practice, real-world OOD inputs are highly diverse and unpredictable, making it difficult to curate datasets that capture all potential distribution shifts; as Yang et al. (2024) highlight, \"...approaches impose a strong assumption on the availability of OOD training data, which can be infeasible in practice.\" Practical constraints have led to a shift in recent research toward settings where real OOD data is either unavailable or significantly limited. Unlike these approaches, our synthetic data generation approach completely removes the dependency on external data sources and allows us to create more controlled and flexible test conditions.\nSynthetic data. Recently, synthetic data has been used for OOD detection in the image domain; Kwon et al. (2023) leverage CLIP (Radford et al., 2021), a vision-language model, to erase InD regions from training images and then uses a latent diffusion model to replace them with realistic OOD features that blend seamlessly with the image background whereas Sun et al. (2024) generate synthetic image samples by using a variant of CLIP to mix InD features from different classes. In contrast, we focus on textual data and leverage LLMs to generate high-quality proxies for OOD data that capture the complexities of real-world OOD data. In our work, we explore the efficacy of LLM-generated OOD proxies for OOD detection, an area which remains largely unexplored."}, {"title": "3. Synthetic Data Generation", "content": "3.1. Synthetic data pipeline\nOur synthetic generation pipeline is illustrated in Figure 1. Unlike previous studies that leverage external OOD data sources or augment InD samples by mixing them together (see Section 1), our method completely removes the need for original OOD samples in training the OOD detector. Following the protocol in Liu et al. (2023); Yang et al. (2022); Winkens et al. (2020), we divide OOD data into two categories: near-OOD and far-OOD, far-OOD where InD and OOD data come from different domains and near-OOD where InD and OOD data come from the same domain but with different classes, as shown in Figure 2. Near-OOD samples are generally more challenging to identify.\nFor far-OOD, we employ a two-stage process, while for near-OOD, we use a single-stage process. This is because near-OOD data originates from the same domain as InD data, allowing us to use InD examples as in-context demonstrations within the prompt. In contrast, far-OOD data comes from a different domain, so we first generate a few seed demonstrations by prompting the LLM in the initial stage. These seed demonstrations are then used as in-context demonstrations in the second stage, guiding the LLM to generate the final responses, which helps enhance the diversity of the outputs. We generate all synthetic OOD data using the Llama 3 70B Instruct model, unless stated otherwise. The specific prompts used for generating the OOD data are detailed in Tables 10-14. After generating the final responses, following Wang et al. (2022), we filter out invalid entries, excessively long or short instructions, as well as low-quality or repetitive responses; this ensures a diverse and high-quality dataset for our subsequent analyses and model training. To better understand the effectiveness of our synthetic generation pipeline, we visualize the sentence representations of InD, original OOD, and synthetic OOD data in Figure 5; a detailed discussion of these visualizations follows in Section 4.2.\n3.2. Synthetic data model\nWe consider two strategies to train an OOD detector using the synthetic OOD data:\nRepurposing a pre-trained model. Suppose we have access to a model trained for the InD task. Let \u03c6 : X \u2192 Rh denote the feature extractor of the pre-trained InD model, where X is the input space and h is the dimensionality of the feature representation. We add a binary classification layer on top of the feature extractor to predict an OOD score Zood = W T\u03c6(x), where w \u2208 Rh. Then the probability that a sample is OOD is given by Pood(x) = \u03c3(zood), where \u03c3(\u00b7) is the sigmoid function. To fit the OOD detector weights w we can use a small amount of InD data and the synthetically generated OOD data and train with the binary classification loss. The main advantage of this approach is that it is guaranteed to preserve the in-distribution predictions of the pre-trained model while augmenting it with the ability to detect OOD samples. In addition, we don't require access to the exact InD data the model was trained on, which will be convenient in our RLHF reward modeling experiment in Section 4.2.2.\nEnd-to-end training. The second approach involves training a single (K + 1)-way model (e.g. Llama-2 13B), where the first K classes correspond to the InD classes and the (K + 1)-th class represents the OOD category. The classification layer is now parameterized by Wuniv \u2208 R(K+1)\u00d7h, enabling the model to output logits for K InD classes and one OOD class: zuniv = Wuniv\u03c6(x) where zuniv \u2208 RK+1 corresponds to the logits for the classes {1, . . ., K, K + 1}, with the (K + 1)-th class designated for OOD instances. This model is trained using the combined K-class InD dataset\u00b2 and the synthetic OOD dataset. The main advantage of this approach is the flexibility to simultaneously learn to accurately predict in-distribution and distinguish InD vs OOD, thus improving the overall performance. We use this method in all but the reward modeling experiments and conduct an ablation study in Section 4.2.4."}, {"title": "4. Experiments", "content": "In this section, we demonstrate how well our framework performs across various InD-OOD dataset pairs, encompassing a wide range of real-world scenarios. We identify four crucial scenarios where addressing the OOD detection problem is especially valuable: 1) toxicity detection, 2) harm detection, 3) RLHF reward modeling, and 4) selective classification."}, {"title": "4.1. Model, Datasets, and Prompt Details", "content": "For toxicity detection, harm detection, and selective classification tasks, we conduct experiments using Llama-2 (Touvron et al., 2023) with 7/13B parameters unless stated otherwise. For RLHF reward model filtering, we employ Starling-RM-7B-alpha (Zhu et al., 2023a), which is pre-trained from Llama2-7B-Chat (Touvron et al., 2023)3. We employ smaller 7B and 13B Llama variants as detector models to keep the system simple and computationally efficient, as larger models would add unnecessary complexity and computation. All experiments are performed on hardware equipped with NVIDIA A100-SXM4-80GB GPUs. We provide the necessary code to reproduce our results.\nDatasets. We evaluate the effectiveness of our method on nine InD-OOD dataset pairs. As InD datasets, we use Civil Comments (Borkan et al., 2019) (toxicity detection; we use CC for brevity), BeaverTails [Non-Violent Unethical Behavior] (NVUB) (Ji et al., 2024b) (harm detection; we use BT for brevity), and RewardBench Chat (Lambert et al., 2024) (RLHF reward model filtering). For toxicity and harm detection tasks, each InD dataset is paired with four OOD datasets; two are categorized as far-OOD4 and two as"}, {"title": "4.2. Experimental Setup and Results", "content": "4.2.1. \u03a4\u039f\u03a7ICITY AND HARM DETECTION\nToxicity detection is a classical text classification task with applications to moderation of online conversations to promote safe and inclusive conversations.\nHarm detection is essential for resolving critical misalignment issues in LLMs, where the LLM's outputs can diverge from desired ethical standards. The goal is to train a smaller specialized detector model (i.e. a fine-tuned classifier) to proactively identify when alignment methods should be applied (Ngweta et al., 2024; Ji et al., 2024a) to correct a harmful response from an LLM. By targeting alignment efforts only when necessary, this approach significantly mitigates the \"alignment tax\" the resource-intensive process of continuously aligning an LLM \u2013 ensuring more efficient and cost-effective alignment without compromising LLM's integrity (Ouyang et al., 2022).\nFor both tasks, we adopted LoRA (Hu et al., 2022), a parameter-efficient fine-tuning approach, to fine-tune Llama-2 13B. Our objective is twofold: first, to determine if an input, i.e. CC or BT prompt-response pair, is appropriate; second, to classify inputs as InD or OOD. To achieve this, we utilized a three-way model with labels Positive (i.e. non-toxic or aligned), Negative (i.e. toxic or not aligned), and Neutral (i.e. OOD). In all experiments, we maintained a consistent setup: a learning rate of 1.5e-4 and a batch size of 16. We configured the total number of epochs to 10 and applied early stopping. We employed LoRA with these configurations: an alpha of 16, dropout of 0.1, and a rank of 16. The LoRA target modules included \"q_proj,\" \"k_proj,\" \"v_proj,\" \"out_proj,\" \"fc_in,\" \"fc_out,\" and \"wte.\" For Civil Comments, we label samples with a toxicity score of 0 as Positive and those with a score above 0.6 as Negative. For BeaverTails, we select Negative samples based on the harm category and Positive samples when the 'is_safe' category is True. Each model was trained by randomly sampling 6000 data samples while ensuring a comparable number of samples per class, except for Mostly Basic Python Problems (MBPP), where only 374 training samples were available, all of which were used. The size of the synthetic and original data is kept similar in our experiments6. In cases where validation samples are not available, we sample them from the training data, ensuring the selected samples are mutually exclusive from the training set. The testing data is always disjoint from both the training and validation datasets.\nOur main results are shown in Table 1 for the eight"}, {"title": "4.2.2. RLHF REWARD MODELING", "content": "In the RLHF pipeline, a reward model serves as an automated system that learns human preferences and assigns scores to model outputs. It guides the fine-tuning process of LLMs, making the training more efficient, scalable, and consistent. By reducing the need for continuous human labeling, it significantly accelerates model development while maintaining alignment with human values. However, as evident from the RewardBench Leaderboard (Lambert et al., 2024),8 certain reward models excel in specific text categories (e.g., Chat), achieving high win percentages, yet perform miserably in others (e.g., Reasoning), yielding significantly lower win percentages. Therefore, we designed a dual-purpose reward model that not only evaluates the score of a given LLM response but also categorizes it based on whether it pertains to a high-performing category (i.e., InD) or a low-performing category (i.e., OOD) in terms of win percentage. Our redesigned reward model thus provides two outputs: 1) a score and 2) a classification label (i.e., InD vs OOD). Such a model can strengthen the RLHF pipeline. If the model encounters an input belonging to a low-performing category, the practitioner can choose to discard or ignore this output, thereby aiding in the training of a more robust RLHF model.\nTo model the aforementioned dual-purpose behavior, we applied a single layer classification head on top the last layer last token embedding of the Starling-RM-7B-alpha model while keeping the entire LLM frozen. We use the RewardBench (Chat) category as InD and the RewardBench (Reasoning) category as OOD. This decision was based on the performance of the Starling-RM-7B-alpha model, which achieves a high win percentage of 98.0% for Chat on the RewardBench Leaderboard, indicating strong performance. Conversely, its performance in the Reasoning category was notably poorer, with a win percentage of only 58.0%. As InD dataset (i.e. Chat), we used five subsets including alpacaeval-easy, alpacaeval-length, alpacaeval-hard, mt-bench-easy, mt-bench-medium. As OOD dataset (i.e. Reasoning), we used five code and math subsets including math-prm, hep-cpp, hep-java, hep-python, and hep-rust. The single layer classification head was trained using cross entropy loss for ten epochs with a batch size of 16, learning rate of 4e-5 with linear scheduling, and AdamW optimizer.\nResults for the RLHF reward modeling are shown in Table 2. We observe that our reward model accurately distinguishes"}, {"title": "4.2.3. SELECTIVE CLASSIFICATION", "content": "One way to improve the reliability and efficiency of a classifier model is to use selective classification (Geifman & El-Yaniv, 2017) under which the model abstains from making predictions when it is uncertain. This method has demonstrated promising results in classification tasks by minimizing the risk of incorrect predictions, making it well-suited for mission-critical applications where the impact of errors is significant. We investigate whether or not selective classification can be used to enhance classifier performance in the presence of OOD data. For example, given a binary detector trained to classify whether an input text is 'Negative' (i.e. toxic) or 'Positive' (i.e. non-toxic). At test time, we input samples from both InD (i.e. Negative/Positive) and OOD (e.g. math/code problems or toxicity data coming from a different data distribution) data. The model performance is enhanced by dropping samples on which the model is most uncertain based on a score (e.g. MSP/Energy/DICE scores; details in next section).\nFor selective classification experiments, we use four InD-OOD pairs: CC-SST-2, CC-ToxiGen, BT-BT (SEAC & DAWBS), and BT-BT (DSI & HSOL); abbreviations are detailed in Table 4. We opt for the more challenging near-OOD datasets because their strong semantic similarity to the InD data makes the classification task particularly difficult. We train a Llama-2 7B binary model, which is trained to"}, {"title": "4.2.4. ADDITIONAL STUDIES", "content": "Effect of data generation model size. Thus far, the Llama-3 70B-instruct model was used for data generation as larger models generally yield more diverse and high-quality generations (Chen et al., 2024). However, we also conducted an ablation using the Llama-3 8B-instruct model for data generation step. From Table 3 (for additional results, see Table 8), we observe that even the smaller 8B model achieves perfect zero FPR95 on the far-OOD CC-GSM8k InD-OOD pair. Additionally, on near-OOD datasets, its performance is second only to the ideal baseline (see Table 8), demonstrating that smaller models can still generate high-quality synthetic data for OOD detection tasks.\nThree-way vs binary model. Another natural question is to ask: Is it necessary to add a third class to the OOD detector, or would a repurposed binary model suffice? Here we fine-"}, {"title": "5. Conclusions", "content": "In this paper, we introduce a simple yet effective framework for OOD detection that leverages synthetic data generation powered by LLMs. Our method addresses the critical challenge of OOD data scarcity by leveraging LLMs to create high-quality OOD proxies, eliminating the need for external OOD data sources. Extensive experiments encompassing nine InD-OOD dataset pairs demonstrate that our method significantly outperforms baseline approaches across real-world text classification use cases, including tasks arising in LLM development and deployment lifecycle.\nIncorporating OOD detection capabilities into various classification systems used for training LLMs is a promising direction for future work. For example, OOD detection may help to identify when reward overoptimization (also known as reward hacking) starts to occur (Skalse et al., 2022; Gao et al., 2023b; Moskovitz et al., 2023). Another interesting application is pre-training data filtering, where various classifiers are often used to select data for pre-training (Penedo et al., 2024; Li et al., 2024) and are likely to benefit from OOD robustness due to the complexity and breadth of LLM pre-training text corpora."}, {"title": "A. Score-based Baseline Methods", "content": "A.1. Preliminaries and Problem Setup\nLet X = RN denote the input space, where d is the dimensionality of the input features. The output space is represented as Y = {1, 2, . . ., K}, where K is the number of classes. Given a training dataset D = {(xi, Yi)}Ni=1 sampled from the joint distribution P on X \u00d7 Y, the objective is to learn a mapping fo : X \u2192 Y. Assume that model fe is trained on a dataset drawn from the InD Pin.\nA.2. Formulation of OOD Detection\nDuring testing, inputs are sampled from a mixture of InD Pin and OOD Pout. The goal is to determine whether a given input x \u2208 X belongs to Pin. OOD detection is framed as a binary classification problem where the model fe must classify x as either:\n\u2022 InD: x belongs to the known distribution Pin.\n\u2022 OOD: x is from an unknown distribution Pout, with no overlap between the label set of Pout and Y.\nA.3. Decision Rule for OOD Detection\nThe decision rule for OOD detection is based on a score function S(x), which assigns a value to each input x indicating its likelihood of belonging to Pin. A threshold \u03bb is used for classification:\ngx(x) =\nSin if S(x) \u2265 \u03bb\nSout if S(x) < \u03bb\nThis mechanism ensures that inputs with scores above \u03bb are classified as InD, while those below are deemed OOD. The threshold \u03bb is chosen so that a high fraction of InD data (e.g. 95% in our case i.e. FPR95) is correctly classified.\nMaximum Softmax Probability (MSP) (Hendrycks & Gimpel, 2017). This method proposes to use the maximum softmax score as the OOD score S(x).\nEnergy (Wu et al., 2023). This approach leverages an energy score E(x) for OOD detection. The energy function maps the pre-softmax logits to a scalar E(x) \u2208 R, which is relatively lower for InD data. Importantly, (Wu et al., 2023) utilizes the negative energy score (i.e. S(x) = \u2212E(x)) for OOD detection, aligning with the convention that the score S(x) is higher for InD data and lower for OOD data. Furthermore, this method does not require hyperparameter tuning.\nDICE (Sun & Li, 2022). This method computes logits by applying sparsification to the penultimate layer of the model, using only a subset of important weights that significantly contribute to the prediction. After obtaining the logits, the final score S(x) is calculated using either the Energy score or MSP. An ablation study in the original paper demonstrates that the Energy score performs better, which is why we have selected this method. The approach includes a sparsity hyperparameter p\u2208 [0, 1]; a higher p indicates a greater fraction of weights are dropped, with p = 0 resulting in no weights being dropped. We set p = 0.5, as it performs effectively in our case and aligns with findings in the original paper.\nReAct (Sun et al., 2021). This method improves OOD detection by truncating the activations in the penultimate layer of the network. Activations are clipped to a threshold c, reducing the effect of noisy OOD data while preserving InD data. The truncated activations are used to compute the logits. After obtaining the logits, the final score S(x) is calculated using either the Energy score or MSP. An ablation study in the original paper demonstrates that the Energy score performs better, which is why we have selected this method. The rectification threshold c is set to 1.33 and is selected from a set of {0.85, 1.0, 1.33, 1.5, 2.0, 2.33}."}, {"title": "B. Datasets Details", "content": "In this section, we provide details about the different InD and OOD datasets that we used in our work."}]}