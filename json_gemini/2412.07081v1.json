{"title": "SEQUENTIAL CONTROLLED LANGEVIN DIFFUSIONS", "authors": ["Junhua Chen", "Lorenz Richter", "Julius Berner", "Denis Blessing", "Gerhard Neumann", "Anima Anandkumar"], "abstract": "An effective approach for sampling from unnormalized densities is based on the idea of gradually transporting samples from an easy prior to the complicated target distribution. Two popular methods are (1) Sequential Monte Carlo (SMC), where the transport is performed through successive annealed densities via prescribed Markov chains and resampling steps, and (2) recently developed diffusion-based sampling methods, where a learned dynamical transport is used. Despite the common goal, both approaches have different, often complementary, advantages and drawbacks. The resampling steps in SMC allow focusing on promising regions of the space, often leading to robust performance. While the algorithm enjoys asymptotic guarantees, the lack of flexible, learnable transitions can lead to slow convergence. On the other hand, diffusion-based samplers are learned and can potentially better adapt themselves to the target at hand, yet often suffer from training instabilities. In this work, we present a principled framework for combining SMC with diffusion-based samplers by viewing both methods in continuous time and considering measures on path space. This culminates in the new Sequential Controlled Langevin Diffusion (SCLD) sampling method, which is able to utilize the benefits of both methods and reaches improved performance on multiple benchmark problems, in many cases using only 10% of the training budget of previous diffusion-based samplers.", "sections": [{"title": "1 INTRODUCTION", "content": "We consider the task of sampling from densities of the form\n$\n\\begin{equation}\nP_{\\text{target}} = \\frac{\\rho_{\\text{target}}}{Z} \\quad \\text{with} \\quad Z := \\int_{\\mathbb{R}^d} \\rho_{\\text{target}}(x)dx,\n\\end{equation}\n$\nwhere $\\rho_{\\text{target}} \\in C'({\\mathbb{R}^d}, {\\mathbb{R}}_{\\geq 0})$ can be evaluated pointwise, but the normalizing constant $Z$ is typically intractable. This task is of great practical interest, with numerous applications in the natural sciences (Zhang et al., 2023b), for instance, for Boltzmann distributions in molecular dynamics or lattice field theory in quantum physics, as well as posterior sampling in Bayesian statistics (Gelman et al., 2013).\nSampling problems vs. generative modeling. The sampling problem poses unique challenges not found in other areas of probabilistic modeling. For instance, while both generative modeling and sampling involve approximating a target distribution $P_{\\text{target}}$, they differ fundamentally in terms of the information available. In generative modeling, one has access to samples $X \\sim P_{\\text{target}}$, whereas in sampling, we only have access to a pointwise oracle $\\rho_{\\text{target}}$ (and, potentially, its pointwise gradients) and no samples. This distinction introduces obstacles for the sampling problem that do not exist in generative modeling. For example, a key challenge in modeling a distribution is identify-ing its regions of high probability, or modes. When samples are available, they can directly reveal the locations of these modes. In their absence, however, the sampling algorithm must include an exploration strategy to discover them and identify their shape. This exploration becomes exponen-tially more difficult as the dimensionality of the state space increases, making the sampling problem challenging even in moderate dimensions (e.g., 10 \u2013 50)."}, {"title": "Sequential Monte Carlo methods and diffusion-based samplers.", "content": "A general idea to approach the sampling problem is to draw particles from an easy prior distribution and gradually move them toward the complicated target (sometimes termed dynamical measure transport). In this work, we focus on two popular paradigms:\n\u2022 In Annealed Importance Sampling (AIS) (Neal, 2001) and its extension Sequential Monte Carlo (SMC) (Chopin, 2002; Del Moral et al., 2006) particles are successively updated and reweighted, as to approach relevant regions in space, targeting an annealed sequence of intermediate distribu-tions. This procedure is typically formulated in discrete time and does not require learning.\n\u2022 In diffusion-based sampling (Richter & Berner, 2024; Vargas et al., 2024) the idea is to learn a drift of a stochastic differential equation (SDE) to transport the samples from the prior to the desired target, typically formulated in continuous time. The absence of samples means that data-driven approaches such as for generative modeling (Song et al., 2021) are not possible, and training is instead done via variational inference, gaining information through evaluations of $P_{\\text{target}}$.\nEach paradigm brings its own advantages and drawbacks. Traditional SMC methods rely on pre-defined rules for particle updates, such as Markov Chain Monte Carlo (MCMC) and resampling methods, which help to direct computational effort onto promising regions of the space and enjoy asymptotic guarantees. While they do not require learning, the employed MCMC methods can, in many cases, exhibit slow convergence to the target (Del Moral et al., 2006). Diffusion-based samplers, on the other hand, require a training phase, which enables them to automatically adapt to the given target. However, training can take significant time and often suffers from numerical instabilities as well as mode collapse (Richter & Berner, 2024)."}, {"title": "Sequential Controlled Langevin Diffusions.", "content": "In this work, we show that the two methods can complement each other. SMC can benefit from the flexible nature of the learnable transitions, and resampling and MCMC can help diffusion-based samplers converge faster and counteract numerical stability issues arising, for instance, from outlier particles. Motivated by this, we identify a prin-cipled and general framework to unify the two methods, culminating in our Sequential Controlled Langevin Diffusion (SCLD) algorithm, which alternates between SMC and diffusion steps as illus-trated in Figure 1. In addition, we devise a family of loss functions that enables end-to-end training (i.e., for which the algorithm used during inference can be directly optimized). This becomes possi-ble by viewing both methods in continuous time and considering measures of the underlying SDEs on the path space.\nOur contributions can be summarized as follows:\n\u2022 Taking the continuous-time perspective, we can rigorously connect and unify SMC and diffusion-based sampling by performing importance sampling in path space."}, {"title": "2 SEQUENTIAL CONTROLLED LANGEVIN DIFFUSIONS", "content": "We start by giving an introduction to Sequential Monte Carlo methods. However, different from previous work, our focus is on a continuous-time perspective that can be readily integrated with diffusion-based samplers."}, {"title": "2.1 A PRIMER ON SEQUENTIAL MONTE CARLO IN CONTINUOUS TIME", "content": "Importance sampling (IS). The idea of utilizing samples from a prior distribution in order to compute statistics relying on samples from a target can be motivated by importance sampling. In its simplest case, one can compute unbiased estimates w.r.t. the target distribution via\n$\n\\begin{equation}\n\\mathbb{E}_{X_T\\sim p_{\\text{target}}}[\\varphi(X_T)] = \\mathbb{E}_{X_0\\sim p_{\\text{prior}}}[\\varphi(X_0)w(X_0)] \\approx \\frac{1}{K} \\sum_{k=1}^K \\varphi(X_0^{(k)})w(X_0^{(k)}),\n\\end{equation}\n$\nwhere $\\varphi \\in C(\\mathbb{R}^d, \\mathbb{R})$ is a function of interest, the weight is defined\u00b9 as $w := \\frac{p_{\\text{target}}}{p_{\\text{prior}}}$, and $(X_0^{(k)})_{k=1}^K$ are i.i.d. samples from $p_{\\text{prior}}$. Since importance sampling becomes highly inefficient if the high-probability regions of the prior and target do not overlap substantially, a key idea is to gradually \"transport\" $X_0$ to $X_T$.\nAnnealed importance sampling (AIS). In particular, we may sequentially move particles from the prior to the target along a curve $(\\pi(\\cdot, t))_{t\\in[0,T]}$, chosen such that $\\pi(\\cdot, 0) = p_{\\text{prior}}$ and $\\pi(\\cdot,T) = p_{\\text{target}}$, e.g., by linear interpolation in log-space (Dai et al., 2022). To this end, we consider two (time-dependent, forward and backward) Markov kernels $p_{t_n|t_{n-1}}$ and $p_{t_{n-1}|t_n}$. Given a time grid $0 = t_0 < t_1 < \\dots < t_N = T$ (also referred to as annealing steps), we may now sample $X_{t_0} \\sim p_{\\text{prior}}$ and iterate for each $n = 1, ..., N$:\n1. Sample $X_{t_n} \\sim p_{t_n|t_{n-1}}(X_{t_{n-1}})$.\n2. Compute the weights $W_{t_{n-1},t_n}(X_{t_{n-1}}, X_{t_n}) = \\frac{\\pi(X_{t_n},t_n)p_{t_{n-1}|t_n}(X_{t_{n-1}}|X_{t_n})}{\\pi(X_{t_{n-1}},t_{n-1})p_{t_n|t_{n-1}}(X_{t_n}|X_{t_{n-1}})}$.\nWe can then perform importance sampling on an augmented target distribution via the weights\n$\n\\begin{equation}\nw(X_{t_0},..., X_{t_N}) := \\prod_{n=1}^N W_{t_{n-1},t_n}(X_{t_{n-1}}, X_{t_n}) = \\frac{p_{t_0,...,t_N}(X_{t_0},..., X_{t_N})}{p_{t_0,...,t_N}(X_{t_0},...,X_{t_N})},\n\\end{equation}\n$\nwhere $p_{t_0,...,t_n}$ and $p_{t_0,...,t_n}$ are the joint densities of the \u201cforward\u201d and a corresponding \u201cbackward\u201d operation. In particular, in analogy to (2), it holds that\n$\n\\begin{equation}\n\\mathbb{E}_{X_{t_0},..., X_{t_N}} [\\varphi(X_T)w(X_{t_0},..., X_{t_N})] = \\mathbb{E}_{X_T\\sim p_{\\text{target}}} [\\varphi(X_T)].\n\\end{equation}\n$\nIf the normalizing constant $Z$ is not available, we can compute unnormalized weights $\\tilde{w} := \\frac{\\rho_{\\text{target}}}{p_{\\text{prior}}}$ and normalize them by their sum, leveraging the identity $Z = \\mathbb{E}_{X_0\\sim p_{\\text{prior}}} [\\tilde{w}(X_0)]$ (self-normalized importance sampling). While this introduces bias, the estimator is still consistent as $K\\to\\infty$ (del Moral, 2013)."}, {"title": "Resampling.", "content": "In principle, any forward and backward Markov kernels lead to an unbiased estima-tor of the expectation of interest, as stated in (4). In practice, however, a notorious problem with importance sampling is its potentially high variance. Specifically, the variance might increase ex-ponentially with the dimension, sometimes termed curse of dimensionality, see, e.g., Chatterjee & Diaconis (2018); Hartmann & Richter (2024). To circumvent this issue, one idea is to sequentially \"update\" samples (also referred to as \"particles\") during the course of the simulation according to their weights, so as to refocus computational effort on promising particles\u2014a procedure referred to as resampling. For instance, we can select only certain (relevant) samples $X_T^{(k)}$ for the estimation of the expectation in (2). To this end, let $O(k)$ be a random variable with values in ${0, . . ., K}$ and $\\mathbb{E}[O(k)|X_0^{(1)},..., X_0^{(K)}] = K\\frac{W(X_0^{(k)})}{\\sum_{l=1}^K w(X_0^{(l)})}$, where $W(X_0^{(k)}) := w(X_0^{(k)})/\\Sigma_{l=1}^{K}w(X_0^{(l)})$, defining how many times we select the k-th sample. Due to the tower property, we can then also obtain a consistent estimator of the expectation in (2) via\n$\n\\begin{equation}\n\\mathbb{E}_{X_T\\sim p_{\\text{target}}}[\\varphi(X_T)] \\approx \\frac{1}{K} \\sum_{k=1}^K \\varphi(X_0^{(k)}) O(k).\n\\end{equation}\n$\nA common choice is to consider $O \\sim M_K(W(X_0^{(1)}),..., W(X_0^{(K)}))$ drawn from a multinomial distribution with K trials, where the normalized weights determine the event probabilities (Gordon et al., 1993). We note that with this resampling step, we introduce additional stochasticity. However, at the same time, it can bring statistical advantages by focusing on \"relevant\u201d samples, e.g., stabilizing effects and variance reduction (Dai et al., 2022).\nRemark 2.1 (SMC formulation in continuous vs. discrete time). We stress that, even though we evaluate our process X on N + 1 discrete time instances, the formalism above includes time-continuous processes $(X_t)_{t\\in[0,T]}$. While some transition kernels used in SMC, e.g., uncorrected Langevin kernels, can be interpreted in continuous time, SMC is typically stated for a fixed number of discrete steps. We will see in the sequel how the continuous-time formulation offers an ele-gant framework with certain advantages, in particular, allowing us to integrate learned SDE-based transition kernels and interleave them with resampling and MCMC steps at arbitrary times."}, {"title": "2.2 CONTROLLED SDES AND IMPORTANCE SAMPLING IN PATH SPACE", "content": "A central question in SMC is how to choose the forward and backward transition densities $p_{t_n|t_{n-1}}$ and $p_{t_{n-1}|t_n}$ defined above. Clearly, when the forward and backward joint densities stated in (3) agree, we achieve perfect sampling in the sense that no corrections with importance weights are necessary. However, it is typically not possible to obtain such transitions, and thus the choice of $p_{t_n|t_{n-1}}$ and $p_{t_{n-1}|t_n}$ to approximate this criterion is of critical importance to the success of SMC. Whereas, tradition-ally, MCMC steps have been employed as the transition kernel (Dai et al., 2022), they are known to require a large number of steps to achieve approximate transportation between densities. In recent years, there has been interest in employing learned transition densities to overcome the slow conver-gence times of fixed MCMC kernels (Matthews et al., 2022; Phillips et al., 2024). Advancing those attempts, we will show how transition densities corresponding to SDEs yield a principled solution that, moreover, allows us to leverage recent advancements in diffusion models.\nDiffusion bridges. To this end, let us consider the stochastic process $X^u = (X_t^u)_{t\\in[0,T]}$, defined by the SDE\n$\n\\begin{equation}\ndX_t = u(X_t, t)dt + \\sigma(t)dW_t, \\quad X_0 \\sim p_{\\text{prior}},\n\\end{equation}\n$\nwhere $u \\in C(\\mathbb{R}^d \\times [0,T], \\mathbb{R}^d)$ is a control function, $\\sigma\\in C'([0,T], \\mathbb{R})$ the diffusion coefficient, and $W$ a standard Brownian motion. This process uniquely defines a forward transition density $p_{s|t}$ and falls into the framework stated in Section 2.1 for any time steps $0 = t_0 < t_1 < \\dots < t_N = T$. In fact, we can leverage the ideas from CMCD (Vargas et al., 2024) and learn $u$ such that the transport happens along a prescribed density in time, i.e., such that the density $p_{X^u}(\\cdot, t)$ of $X^u$ is equal to a prescribed target density $\\pi(\\cdot, t)$, connecting the prior and the target, for every $t\\in [0,T]$; cf. Lemma 2.2 below. We will see that the knowledge of the marginals allows for a natural integration within SMC frameworks. Now, similar to the importance sampling framework from Section 2.1, the general idea is to exploit a time-reversed dynamics that starts in the desired target density. To be precise, we may further define a related reverse-time SDE\n$\n\\begin{equation}\ndY_t = v(Y_t, t)dt + \\sigma(t)dW_t, \\quad Y_T \\sim p_{\\text{target}},\n\\end{equation}\n$"}, {"title": "which depends on the control", "content": "$v \\in C(\\mathbb{R}^d \\times [0, T], \\mathbb{R}^d)$ and where $dW_t$ denotes backward\u00b2 integration of Brownian motion. Now, if $u$ and $v$ are learned such that $X^u$ and $Y^v$ are time-reversals of each other, then $p_{X^u} = p_{Y^v}$, i.e., the two processes transport the prior to the target and vice versa. However, in this general setting, there are infinitely many such bridging processes, all fulfilling Nelson's identity (Nelson, 1967), i.e.,\n$\n\\begin{equation}\nu - v = \\sigma^2 \\nabla \\log p_{X^u} = \\sigma^2 \\nabla \\log p_{Y^v}.\n\\end{equation}\n$\nSince our goal is to satisfy $p_{X^u} = p_{Y^v} = \\pi$, we can incorporate this constraint via the ansatz $v = u - \\sigma^2 \\nabla \\log \\pi$, leading to the SDE\n$\n\\begin{equation}\ndY_t = (u - \\sigma^2 \\nabla \\log \\pi)(Y_t, t)dt + \\sigma(t)dW_t, \\quad Y_T \\sim p_{\\text{target}},\n\\end{equation}\n$\nas suggested in Vargas et al. (2024), noting that the process now also depends on the control $u$. Consequently, under mild conditions, this constraint leads to a unique gradient field representing the solution $u^*$ to the time-reversal problem (Vargas et al., 2024, Proposition 3.2). We comment on more general, learnable density evolutions in Remark A.1.\nMeasures in path space. The task of learning the time-reversal can be approached via the per-spective of measures on the space of continuous trajectories $C([0, T], \\mathbb{R}^d)$, also called path space. Loosely speaking, a path space measure $\\mathbb{P} = \\mathbb{P}_{u,p_{\\text{prior}}}$ of the process (6) can be thought of as the joint density $p_{t_0,...,t_N}(X_0,..., X_{t_N})$ in (3) when $N \\to \\infty$, i.e., evaluated along infinitely many time instances (Baldi, 2017, Corollary 11.1).\nIn analogy to importance sampling described in Section 2.1, we may now consider a change of measure in path space, i.e.,\n$\n\\begin{equation}\n\\mathbb{E}_{X^u\\sim \\mathbb{P}_{u,p_{\\text{prior}}}}[\\varphi(X_T)w(X^u)] = \\mathbb{E}_{Y^v\\sim \\mathbb{P}_{v,p_{\\text{target}}}} [\\varphi(Y_T)] = \\mathbb{E}_{X\\sim p_{\\text{target}}} [\\varphi(X)],\n\\end{equation}\n$\nwhere $w = \\frac{d\\mathbb{P}_{v,p_{\\text{target}}}}{d\\mathbb{P}_{u,p_{\\text{prior}}}}$ and $\\mathbb{P} = \\mathbb{P}_{u,p_{\\text{target}}}$ is the path space measure associated to (9). Furthermore, we can formulate the time-reversal task as the minimization problem\n$\n\\begin{equation}\nu^* = \\arg \\min_{u \\in \\mathcal{U}} D \\Big(\\mathbb{P}_{u,p_{\\text{prior}}}, \\mathbb{P}_{u,p_{\\text{target}}}\\Big),\n\\end{equation}\n$\nwhere $D$ is a divergence and $\\mathcal{U} \\subset C(\\mathbb{R}^d \\times [0,T], \\mathbb{R}^d)$ the set of admissible controls, cf. Richter & Berner (2024). If we can bring the divergence to zero, we have indeed achieved time-reversal between the forward and backward transitions and, thus, perfect sampling. Both for (10) and typical divergences in (11), it is essential to have a tractable expression for the likelihood ratio $w$ between the measures of the forward and the reverse-time process, also called the Radon-Nikodym derivative (RND). This is given by the following lemma; see Vargas et al. (2024) for the proof.\nLemma 2.2 (Likelihood ratio between path measures). Let $\\mathbb{P}_{\\lbrack s,t\\rbrack}^u$ and $\\mathbb{P}_{\\lbrack s,t\\rbrack}^v$ be the path space measures of the solutions to the SDEs in (6) and (9) on the time interval $[s, t] \\subset [0, T]$, where we assume $X_s^u \\sim \\pi(\\cdot, s)$ and $Y_t^v \\sim \\pi(\\cdot,t)$. Then for a generic\u00b3 process $X$ it holds\n$\n\\begin{equation}\nw_{[s,t]}(X) = \\frac{d\\mathbb{P}_{\\lbrack s,t\\rbrack}^v(X)}{d\\mathbb{P}_{\\lbrack s,t\\rbrack}^u} = \\frac{\\pi(X_t, t)}{\\pi(X_s, s)} \\exp \\Big(\\frac{1}{2\\sigma^2} \\int_{s}^t (||u||^2 - ||u - \\sigma^2 \\nabla \\log \\pi||^2) - (\\nabla \\log \\pi(X_\\tau, \\tau)\\cdot dX_\\tau)\\Big).\n\\end{equation}\n$\nAs can be seen from Lemma 2.2, path space measures can be readily employed for sequential al-gorithms that operate on the time grid that we introduced before. In particular, we may divide our trajectories $X^u$ and $Y^v$ into subtrajectories and, thus, our path space measure into multiple chunks. To be precise, we may write\n$\n\\begin{equation}\nw = \\frac{d\\mathbb{P}}{d\\mathbb{P}} = \\frac{d\\mathbb{P}_{[t_0,t_1]}}{d\\mathbb{P}_{[t_0,t_1]}} \\dots \\frac{d\\mathbb{P}_{[t_{N-1},t_N]}}{d\\mathbb{P}_{[t_{N-1},t_N]}} = w_{[t_0,t_1]} \\dots w_{[t_{N-1},t_N]}.\n\\end{equation}\n$"}, {"title": "Algorithm 1 Sequential Controlled Langevin Diffusion (SCLD).", "content": "Require: Annealing path \ud835\udf0b, learned control \ud835\udc62, time grid 0 = \ud835\udc610 << \ud835\udc61\ud835\udc41 =\ud835\udc47\nRequire: Annealing path \ud835\udf0b, learned control \ud835\udc62, time grid 0 = \ud835\udc610 << \ud835\udc61\ud835\udc41 =\ud835\udc47\n1: Initialize: $X_0 := X^(1:\ud835\udc3e) ~ \ud835\udc5d_{prior}$ and \ud835\udc640 := \ud835\udc64^(1:\ud835\udc3e) = 1\n2: for \ud835\udc5b = 1 to \ud835\udc5b = \ud835\udc41 do\n3: Transport: $X_{[\ud835\udc61_{\ud835\udc5b\u22121},\ud835\udc61_\ud835\udc5b]} = simulate_SDE(\ud835\udc4b_{\ud835\udc61_{\ud835\udc5b\u22121}}, \ud835\udc62)$\n4: Compute RNDs: \ud835\udc4a\ud835\udc61_\ud835\udc5b\u22121,\ud835\udc61_\ud835\udc5b = \ud835\udc51\u2119[\ud835\udc61_\ud835\udc5b\u22121,\ud835\udc61_\ud835\udc5b] /\ud835\udc51\u2119[\ud835\udc61_\ud835\udc5b\u22121,\ud835\udc61_\ud835\udc5b] (\ud835\udc4b[\ud835\udc61_{\ud835\udc5b\u22121},\ud835\udc61_\ud835\udc5b])\n5: Update weights: \ud835\udc4a\ud835\udc5b = \ud835\udc4a\ud835\udc5b\u22121\ud835\udc4a[\ud835\udc61_{\ud835\udc5b\u22121},\ud835\udc61_\ud835\udc5b]\n6: Resample: $\ud835\udc4b_{\ud835\udc61_\ud835\udc5b}, \ud835\udc4a_\ud835\udc5b$ = resample($\ud835\udc4b_{\ud835\udc61_\ud835\udc5b}, \ud835\udc4a_\ud835\udc5b$)\n7: return Samples $\ud835\udc4b_\ud835\udc47 := X^(1:\ud835\udc3e)$ approximately from \ud835\udc5d\ud835\udc61\ud835\udc4e\ud835\udc5f\ud835\udc54\ud835\udc52\ud835\udc61\nDifferent from the framework in Section 2.1, we note that Lemma 2.2 offers an explicit formula for computing the weights \ud835\udc4a[\ud835\udc61_{\ud835\udc5b\u22121},\ud835\udc61_\ud835\udc5b] in continuous time. As can be seen in the importance sampling identity (10), the weights can be interpreted as correcting for a potentially imperfect time-reversal. For convenience, we state Algorithm 1 for a simplified, high-level overview of combining SMC with diffusion models and refer to Algorithm 3 in Appendix A.3 for a more detailed exposition. Further, we note that the suggested setting relates to the usual SMC algorithm (such as in Dai et al. (2022)) by taking a different forward transport step (where our Markov kernel is implemented by an SDE) and by adopting the weighting step (using the Radon-Nikodym derivative in place of the likelihood ratio). Using the target density \ud835\udf0b(\u22c5, \ud835\udc61\ud835\udc5b), we can also add MCMC refinements at each time \ud835\udc61\ud835\udc5b; see Section 2.4."}, {"title": "2.3 LOSS FUNCTIONS AND OFF-POLICY TRAINING", "content": "We can adapt the idea of learning the optimal control \ud835\udc62\u2217 to our sequential setting by considering divergences on each subinterval [\ud835\udc61\ud835\udc5b\u22121, \ud835\udc61\ud835\udc5b] separately, in consequence bringing losses of the form\n$\n\\begin{equation}\n\\mathcal{L}(u) = \\sum_{n=1}^N D\\Big(\\mathbb{P}_{u, \\pi_{n-1}}^{\\lbrack t_{n-1},t_n\\rbrack}, \\mathbb{P}_{u, \\pi_{n}}^{\\lbrack t_{n-1},t_n\\rbrack}\\Big),\n\\end{equation}\n$\nwhere $\u03c0_\ud835\udc5b := \u03c0(\u22c5, \ud835\udc61_\ud835\udc5b)$. We stress that with (14) optimization can in principle be conducted globally in spite of the resampling happening sequentially. However, depending on the choice of the divergence, this comes with additional challenges.\nKL divergence. A classical choice is the Kullback-Leibler (KL) divergence \ud835\udc37 = \ud835\udc37\ud835\udc3e\ud835\udc3f, i.e.,\n$\n\\begin{equation}\nD_{KL} \\Big(\\mathbb{P}_{u, \\pi_{n-1}}^{\\lbrack t_{n-1},t_n\\rbrack} \\Big| \\mathbb{P}_{u, \\pi_{n}}^{\\lbrack t_{n-1},t_n\\rbrack}\\Big) = -\\mathbb{E}_{X^u\\sim \\mathbb{P}_{u, \\pi_{n-1}}^{\\lbrack t_{n-1},t_n\\rbrack}} [\\log (W_{[t_{n-1},t_n]}(X^u))],\n\\end{equation}\n$\nwhere $W_{[\ud835\udc61_{\ud835\udc5b\u22121},\ud835\udc61_\ud835\udc5b]}$ is defined as in Lemma 2.2 and the minus originates from the reciprocal impor-tance weights in the logarithm. However, for computing the expectation we need $X_{\ud835\udc61_{\ud835\udc5b-1}}^\ud835\udc62 \u223c \ud835\udf0b_{\ud835\udc5b\u22121}$. If resampling has been employed in the previous iteration (at time \ud835\udc61\ud835\udc5b\u22121; see Algorithm 1), a potential mismatch in the expectation is automatically corrected. Alternatively, we may correct with impor-tance sampling in path space. To this end, let \ud835\udc61\ud835\udc5a (with \ud835\udc61\ud835\udc5a < \ud835\udc61\ud835\udc5b\u22121) be the last time resampling has been conducted, i.e., the last time the weights have been reset; see Algorithm 5. As suggested in Matthews et al. (2022), we can then consider the importance weight \ud835\udc4a[\ud835\udc61\ud835\udc5a,\ud835\udc61\ud835\udc5b\u22121] and compute\n$\n\\begin{equation}\nD_{KL} \\Big(\\mathbb{P}_{u, \\pi_{n-1}}^{\\lbrack t_{n-1},t_n\\rbrack} \\Big| \\mathbb{P}_{u, \\pi_{n}}^{\\lbrack t_{n-1},t_n\\rbrack}\\Big) = -\\mathbb{E}_{X^u\\sim \\mathbb{P}_{u, \\pi_{m}}^{\\lbrack t_{m},t_n\\rbrack}} [\\log (W_{[t_{n-1},t_n]}(X^u))W_{[t_m,t_{n-1}]}(X^u)],\n\\end{equation}\n$\nfor which $X_{\ud835\udc61\ud835\udc5a}^\ud835\udc62$ does not need to be distributed according to $\ud835\udf0b_{\ud835\udc5b\u22121}$ anymore. However, the im-portance weights potentially introduce additional variance into the loss, particularly in high dimen-sions. This observation is stated rigorously in the following proposition, cf. N\u00fcsken & Richter (2021, Proposition 5.7), and proved in Appendix A.2.\nProposition 2.3 (Relative error of KL divergence). Denote by \ud835\udc37\ud835\udc652 the \ud835\udf122-divergence and by \ud835\udc5f^(\ud835\udc3e) := \ud835\udc49\ud835\udc4e\ud835\udc5f(\ud835\udc37\ud835\udc3e\ud835\udc3f)1/2/\ud835\udc37\ud835\udc3e\ud835\udc3f the relative error of the Monte Carlo estimator \ud835\udc37^(\ud835\udc3e) of the KL divergence in (16) with sample size \ud835\udc3e. Moreover, let \ud835\udc61\ud835\udc5a be the last resampling time and let \u2119"}, {"title": "LV divergence and off-policy training.", "content": "An alternative divergence can be defined by\n$\n\\begin{equation}\nDiv\\Big(\\mathbb{P}_{u, \\pi_{n-1}}^{\\lbrack t_{n-1},t_n\\rbrack} \\Big| \\mathbb{P}_{u, \\pi_{n}}^{\\lbrack t_{n-1},t_n\\rbrack}\\Big) = Var_{X^u\\sim Q} [\\log (W_{[t_{n-1},t_n]}(X^u))],\n\\end{equation}\n$\nwhich, in fact, is a family of divergences parametrized by a reference measure $Q = \\mathbb{P}_{u, \\pi_{n-1}}^{\\lbrack t_{n-1},t_n\\rbrack}$ that can be chosen with arbitrary controls \ud835\udc62\u0304 and initial distributions \ud835\udf0b\ud835\udc5b\u22121 (also called off-policy training, see Remark A.2 for details and connections to reinforcement learning). In particular, we do not need $X_{\ud835\udc61_{\ud835\udc5b\u22121}}^\ud835\udc62 \u223c \ud835\udf0b_{\ud835\udc5b\u22121}$ anymore, and thus reweighting such as in (16) is not necessary, irrespective of the fact that resampling at time \ud835\udc61\ud835\udc5b might not have been conducted. We summarize the training procedure for both divergences in Algorithm 2 and present details in Appendix A.3."}, {"title": "2.4 ALGORITHMIC REFINEMENTS AND IMPLEMENTATIONAL DETAILS", "content": "In this section, we turn our theoretical considerations from Sections 2.1 to 2.3 into implementable algorithms. We collate these changes in Algorithm 3 in Appendix A.3, representing a practical version of Algorithm 1.\nLoss Function. We focus on the log-variance divergence in the sequel and refer to Ap-pendix A.6.10 for a comparison to the KL divergence. We choose \ud835\udc62\u0303 = \ud835\udc62 (or previous versions when using a buffer, see \"replay buffers\" below) and simulate X in (18) starting from the prior, so \ud835\udf0b\u0303\ud835\udc5b corresponds to the SDE marginal. However, since we do not take gradients w.r.t. the control \ud835\udc62\u0303 of the reference measures, we detach the trajectory \ud835\udc4b^\ud835\udc62, in line with Richter & Berner (2024). In particular, we do not need to differentiate through the SDE integrator.\nTime discretization. In practice, we choose N equidistant resampling times, i.e. \ud835\udc61\ud835\udc5b \u2212 \ud835\udc61\ud835\udc5b\u22121 = \ud835\udc47/\ud835\udc41, for every \ud835\udc5b \u2208 {1, . . ., \ud835\udc41}, where the number of subtrajectories N may change across applications. We discretize the SDE (7) via the Euler-Maruyama scheme, containing L evenly spaced steps per subtrajectory, i.e.,\n$\n\\begin{equation}\nX_i = X_{i-1} + u(X_{i-1},(i - 1)h)h + \\sigma((i - 1)h)\\sqrt{h}\\xi_i, \\quad \\xi_i \\sim \\mathcal{N}(0, I_d),\n\\end{equation}\n$"}, {"title": "for \ud835\udc56\u22081,", "content": "\u22ef", "\ud835\udc4a[\ud835\udc61\ud835\udc5b\u22121,\ud835\udc61\ud835\udc5b": ".", "\ud835\udefd": [0, "\ud835\udc47"], "1": "is a monotonically increasing function fulfilling \ud835\udefd(0) = 0 and \ud835\udefd(\ud835\udc47) = 1. We choose to learn the function \ud835\udefd to attain a smoother transition; see (34) and Appendix A.6.5.\nResampling. There is a wealth of literature (Webber, 2019; Doucet et al., 2001; Douc & Capp\u00e9, 2005) regarding designing SMC resampling schemes. However, for a fair comparison to CRAFT (Matthews et al., 2022), we utilize the common multinomial resampling scheme. Resampling can, however, reduce particle diversity by introducing identical particles in its output. As such, it is common to trigger resampling at a time \ud835\udc61\ud835\udc5b only when the Effective Sample Size (ESS), a measure of particle quality defined by ESS =(\u03a3\ud835\udc3e\ud835\udc58=1\ud835\udc64(\ud835\udc58))2 /\u03a3\ud835\udc3e\ud835\udc58=1(\ud835\udc64(\ud835\udc58))2, is below a certain threshold, where \ud835\udc64\ud835\udc5b are the importance weights at time \ud835\udc61\ud835\udc5b (as in Algorithm 3). In line with prior works (Matthews et al., 2022; Phillips et al"}]}