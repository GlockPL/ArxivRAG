{"title": "Semantic-Aware Adaptive Video Streaming Using Latent Diffusion Models for Wireless Networks", "authors": ["Zijiang Yan", "Jianhua Pei", "Hongda Wu", "Hina Tabassum", "Ping Wang"], "abstract": "This paper proposes a novel framework for real-time adaptive-bitrate video streaming by integrating latent diffusion models (LDMs) within the FFmpeg techniques. This solution addresses the challenges of high bandwidth usage, storage inefficiencies, and quality of experience (QoE) degradation associated with traditional constant bitrate streaming (CBS) and adaptive bitrate streaming (ABS). The proposed approach leverages LDMs to compress I-frames into a latent space, offering significant storage and semantic transmission savings without sacrificing high visual quality. While it keeps B-frames and P-frames as adjustment metadata to ensure efficient video reconstruction at the user side, the proposed framework is complemented with the most state-of-the-art denoising and video frame interpolation (VFI) techniques. These techniques mitigate semantic ambiguity and restore temporal coherence between frames, even in noisy wireless communication environments. Experimental results demonstrate the proposed method achieves high-quality video streaming with optimized bandwidth usage, outperforming state-of-the-art solutions in terms of QoE and resource efficiency. This work opens new possibilities for scalable real-time video streaming in 5G and future post-5G networks.", "sections": [{"title": "I. INTRODUCTION", "content": "In today's digital age, the demand for streaming services with high-quality video content has surged alongside the proliferation of Internet-enabled devices, bridging the gap between media products and edge users [1]. Streaming services, which cater to a vast audience with diverse preferences, face the challenge of delivering superior video quality while optimizing wireless network bandwidth and resource usage. Real-time adaptive bit-rate streaming (ABS) has emerged as a vital technology in this landscape, which enables dynamically adjusting video quality based on network conditions to provide a seamless viewing experience and improved quality of experience (QoE) [2]. One significant advancement in ABS is the integration of fast forward moving picture experts group (FFmpeg), advanced video coding (H.264), hypertext transfer protocol (HTTP), and dynamic adaptive streaming over HTTP (DASH). These open-source multimedia frameworks are capable of decoding, encoding, transmission, and streaming digital media files in various formats. In particular, FFmpeg's flexibility and efficiency make it an indispensable tool for real-time video processing and adaptive streaming solutions.\nDiffusion models (DMs) [3], a class of Artificial Intelligence generative content (AIGC) models, have shown promising results in generating high quality media content. These models can also intelligently refine video streams at the receiver side, significantly improving visual quality while minimizing bandwidth requirements. By integrating latent diffusion models (LDMs) [4] into the FFmpeg framework, high-quality real-time video streaming can be achieved while dynamically adapting to varying network conditions and diverse quality of experience (QoE) requirements based on different media content. Along another note, the advent of fifth-generation (5G) and beyond 5G (B5G) networks offer unprecedented speeds and low latency, providing an ideal infrastructure for deploying sophisticated video streaming technologies. Despite their advantages, 5G and B5G networks may introduce more semantic ambiguities due to encoding techniques that are more sensitive to wireless channel noises, large-scale high-quality 4K/6K video streaming delivery, and the increased utilization of machine learning models, all of which must be addressed to the quality of the transmitted video. As demonstrated by the excellent performance of LDM in wireless image transmission and video generation tasks, advanced data compression and channel denoising techniques can also ensure a flawless streaming experience with higher QoE and quality of service (QoS) in wireless video streaming.\nTo this end, this paper first highlights the key challenges in the domain of conventional video streaming and DM-enabled video streaming. Next, we propose a novel LDM-enabled semantic-aware adaptive video streaming framework that enables transfer of high-quality video contents over wireless channels, considering different frame types 1. Specifically, the proposed video streaming scheme integrates LDM and FFmpeg for efficient compression, reducing bandwidth by encoding keyframes as latent features and motion data as metadata. To enhance robustness, LDMs denoise I-frames and regenerate B/P-frames, ensuring better video quality over noisy wireless channels. Moreover, an adaptive bitrate mechanism using CNN-GRU optimizes streaming based on varying network conditions, media content, and user preferences for an improved viewing experience. The efficiency and reliability of the proposed system are quantified in terms of QoE that incorporates the trade-off between streaming video quality, quality fluctuation, and the risk of rebuffering events [6], providing concrete evidence of its superiority over the other ABS algorithms such as, BOLA, Comyco, MERINA [6]."}, {"title": "II. FUNDAMENTAL CHALLENGES IN SEMANTIC COMMUNICATION FOR VIDEO STREAMING", "content": "This section outlines several fundamental challenges in semantic-aware video streaming, including constant bit-rate streaming (CBS), adaptive bit-rate streaming (ABS), and DM-enabled bit-rate streaming. A comparative analysis of the various bit-rate streaming scheme is presented in Fig. 1. In CBS, parallel channels are utilized for transmission within the wireless zone to the user side, requiring extensive wireless resources. In ABS, a single channel is transmitted to the user side, and frames can be downscaled in the computing zone based on the user's request. This approach conserves wireless transmission resources while providing adaptive bi-trate options. LD-ABS introduces a novel mechanism where only latent frames are transmitted, eliminating the need for complete frames. This design significantly compresses the transmission, reducing bandwidth and storage requirements, and enabling ultra-efficient streaming."}, {"title": "A. High Bandwidth and Storage Utilization for CBS", "content": "Multi-bitrate streaming consumes a substantial amount of bandwidth and resources for transmission. To satisfy the diverse customers' demands of streaming video, traditionally providers adopt constant bitrate streaming (CBS), which involves providing parallel streaming channels to broadcast video in different resolutions and qualities.\n(i) Bandwidth Consumption: CBS requires a fixed amount of bandwidth for each resolution and quality level, irrespective of the content's complexity [7]. This can lead to inefficient use of available bandwidth, especially during periods of low network traffic or when streaming less complex scenes [8]. CBS necessitates maintaining multiple streams for different quality levels, which can overload the content delivery network (CDN) [6]. In the context of CBS, as illustrated in Fig. 1, let's assume we have 4 parallel routes to stream videos. We capture video at a 1080P resolution source. Each route streams video at 1080P, 720P, 360P, and 144P resolutions, respectively. FFMpeg decodes the video with different bitrates to their respective reference frames In and affiliated predictive frames Pn and Bn. Note that the size of predictive frames is significantly smaller than the reference frames [9].\nUsers manually select the resolution on the player side. This design requires all frames across all resolutions to be sent to the user side. Broadcasting video in different bitrates consumes considerable traffic and resources, overburdening the CDN."}, {"title": "B. Low QoE Multi-Bitrate Adaptive Streaming", "content": "ABS dynamically adjusts video quality based on a user's network conditions and device capabilities to optimize viewing experiences while efficiently utilizing bandwidth. Unlike CBS, which delivers pre-defined quality streams through parallel channels, ABS continuously monitors real-time factors such as network throughput, buffer occupancy, and device performance to make fine-grained adjustments. Achieving high-quality video streaming without excessive bandwidth consumption remains a challenging task. Traditional ABS algorithms often struggle to balance video quality and network efficiency, leading to either compromised video quality or excessive data usage [2]. Video streaming services require ABS to cater to varying network conditions and device capabilities.\n(i) Challenges of diverse wireless conditions: Compared to CBS, which does not automatically select chunks, ABS must dynamically adjust the video bitrate in response to varying network conditions. This adaptation introduces potential latency during bitrate switching, especially in dynamic wireless environments. Additionally, ABS aggressively reduces bitrate under poor network conditions, often resulting in compression artifacts. In contrast, CBS maintains a consistent quality, reducing the risk of rebuffering but lacking the flexibility to optimize for fluctuating bandwidth. [6].\n(ii) Cache and Buffering Issues: Managing cache and buffering of short clips in ABS streaming is complex. Storing multiple resolution video clips adds complexity and consumes significant storage and bandwidth [7].\n(iii) Downscaling to Ensure Users Watching Smooth and Continuous Video: Existing ABS methods attempt to maintain smooth play and playback by frequently switching bitrates. However, excessive bitrate switching negatively impacts QoE and QoS for viewers [7]."}, {"title": "C. VFI and Temporal Consistency", "content": "Streaming video is typically decomposed into Intra-coded frames (I-frames), Predictive-coded frames (P-frames) and Bidirectionally predictive-coded frames (B-frames) [5], which are defined, respectively, as follows:\n\u2022 I-frames are compressed using only the information within the frame itself. They contain a complete image, similar to a PNG image file.\n\u2022 P-frames use data from the previous I-frame or P-frame to predict and encode only the differences (motion vectors and residual data).\n\u2022 B-frames use data from both preceding and following I-frames or P-frames to predict the frame. They encode differences from both directions, often achieving higher compression.\nReconstructing frames based on these reference frames and adjustment frames is difficult [10]. In video compression, I-frames serve as key reference frames containing the full image data, while P- and B-frames store only the motion vectors2 and differences relative to I-frames. Motion vectors are light compared to frames, allowing for efficient reconstruction while maintaining high compression efficiency. However, estimating motion vectors between frames is complex. Ensuring temporal consistency across frames is challenging, especially when using generative models for frame reconstruction [10]. In"}, {"title": "D. High Latency and Low Resolution Restoration on Reverse Diffusion Frame Reconstruction", "content": "Traditional diffusion models require multiple steps to generate high-resolution frames, making them impractical for real-time video applications, which are restricted to 500ms [4].\n(i) Slow reconstruction speed: The reverse diffusion process involves gradually denoising the corrupted frames by iteratively applying the diffusion model which result in high latency, especially for long video sequences [10]-[12].\n(ii) Computational complexity: Reconstructing all the frames across multiple resolutions overburdens VAE.\n(iii) Limited frame generalization: DMs are trained on specific datasets and may not generalize well to unseen or diverse content. In addition, streaming video contains semantic errors. These difficulties lead to challenges on restoring low-resolution frames from different sources or with different characteristics [11]."}, {"title": "III. STATE-OF-THE-ART: DM-AIDED VIDEO STREAMING", "content": "Traditional video compression standards, such as H.264, present challenges in terms of bandwidth consumption, storage requirements, and computational overhead [9]. To achieve real-time performance, dedicated hardware acceleration (e.g., specialized GPUs or ASICs) is often required for multi-channel H.264 encoding to prevent frame drops and buffering delays on the producer side [13]. Additionally, packet loss in predicted frames (P- or B-frames) can degrade following frames until an I-frame refreshes the stream. To address the aforementioned issues, LDMs and DMs are becoming popular. In this section, we introduce the fundamental principles of LDMs and compare the key differences between DMs and LDMs followed by a review of existing literature in the domain of DM/LDM-enabled video streaming."}, {"title": "A. Overview of Latent Diffusion Models", "content": "LDMs specifically operate in a lower-dimensional latent space rather than pixel space, leading to more efficient computation and better scalability [4]. Conditional LDMs (CLDMs) further enhance this approach by conditioning the diffusion process on additional information, such as motion adjustment metadata. Among various implementations, stable diffusion [3] achieve high-quality outputs with improved efficiency and reduced computational requirements. Stable diffusion's architecture leverages both LDMs and CLDMs, allowing it to generate detailed and coherent frames while maintaining a smaller memory footprint compared to other models. This makes LDMs particularly suitable for compressing streaming video, as they can effectively balance the trade-offs between computational load, memory usage, and output quality, enabling efficient and scalable video compression. The key distinctions of LDMs compared to traditional DMs are:\n\u2022 Restoration of original data: LDMs learn to restore the original data by reversing the noising process through incremental denoising and reconstruction.\n\u2022 Systematic degradation of training data: Both LDMS and DMs systematically introduce Gaussian noise to degrade the original data in a step-by-step process known as diffusion. This transformation simplifies the data distribution.\n\u2022 Operating in latent space: Unlike traditional DMs that operate directly in pixel space, LDMs function in a lower-dimensional semantic latent space. This shift leads to more efficient computation and improved scalability.\n\u2022 Conditional diffusion: CLDMs enhance the standard diffusion process by incorporating additional information, such as motion adjustment metadata, to condition the model denoising processes. This results in higher quality and more relevant generated frames.\n\u2022 Denoising via Neural Network: LDMs utilize neural networks to capture complex spatiotemporal relationships across different frames in video streaming, facilitating the generation of high-fidelity frames and enhancing video synthesis."}, {"title": "B. Recent Advancements on DM-aided Video Streaming", "content": "1) DM-aided Video Streaming: Zhou et al. in [1] presented Codec-aware Diffusion Modeling (CaDM), a novel neural-enhanced video streaming paradigm. CaDM improves compression efficiency by reducing both the resolution and color bit-depth of video frames during encoding. At the decoder side, it employs a denoising diffusion process conditioned on the encoder's settings to restore high-quality frames, achieving significant bit-rate savings while maintaining superior visual quality. Li et al. in [14] proposed an extreme video compression approach leveraging the predictive power of the forward diffusion process (FDP) with a pre-trained model. However, this method lacks flexibility as it is not adaptable to different video sources. While DMs offer advancements in video streaming, high computational costs and iterative sampling processes can introduce latency, posing limitations for real-time applications.\n2) LDM-Aided Video Streaming: Danier et al. in [10] presented LDMVFI, leveraging high-fidelity image synthesis capabilities of DMs for video frame interpolation (VFI). LDMVFI generates dynamic frames based on I-frames and incorporates a vector-quantized auto-encoding model, VQ-FIGAN, to enhance VFI performance. Yu et al. [11] introduced Content-Motion LDM, which decomposes a video into a content frame (image-like) and a low-dimensional motion latent representation. Yu et al. [12] proposed the Projected Latent Video Diffusion Model (PVDM), consisting of an autoencoder stage and a diffusion model stage. The autoencoder employs one latent vector to capture common content (e.g., background) while using two additional vectors to encode motion. By utilizing a 2D image-like latent space, PVDM avoids the computational overhead of traditional 3D convolutional networks, relying instead on a 2D convolution-based diffusion model. Ma et al. in [15] presented DiffVC, a diffusion-based perceptual neural video compression framework that effectively integrates a foundational diffusion model with the video conditional coding paradigm. This framework leverages temporal context from previously decoded frames and the reconstructed latent representation of the current frame to guide LDMs in generating high-quality images. Despite their advancements, these approaches do not consider I-frames and B-frames, relying solely on consecutive frames. This design choice may lead to incorrect I-frame predictions due to improper conditioning, while also reducing training efficiency and increasing bandwidth usage, particularly under complex wireless transmission conditions."}, {"title": "IV. REAL TIME LATENT DIFFUSION ADAPTIVE-BITRATE VIDEO STREAMING FRAMEWORK", "content": "In this section, we will first propose a channel-aware bitrate selector that automatically decides the bitrate of a video stream for the next chunk according to the channel conditions, semantic performance requirements, and users' choices in Subsection IV-A. Then, we will introduce the remaining compression, denoising, and reconstruction parts of the latent diffusion adaptive bitrate video streaming (LD-ABS) framework with the selected video resolution to transmit the video frame-by-frame in Subsection IV-B."}, {"title": "A. Channel-Aware Video Streaming Bitrate Selector", "content": "In practical scenarios, an edge user typically sends an HTTP GET request to request a streaming service. Overall, ABS utilizes CNN-GRU model to select the bitrate as the output of neural networks (NN) for the next chunk and the proposed ABS selector diagram is depicted in Fig. 2. We divide ABS selector into the following three sub-units, each corresponding to rendering streaming video [6]:\n\u2022 Wireless Channel Resource Estimation: This sub-unit estimates the variations of the noisy physical wireless channel. A channel state detector within the wireless communication network identifies the channel status and forwards this information to the neural network.\n\u2022 Experience Buffer: Inspired by offline Reinforcement Learning (RL) techniques, we use a buffer to store past expert strategies, allowing the algorithm to sample randomly from the buffer pool during the training process.\n\u2022 Neural Network: For each episode e, the ABS learning agent determines a suitable bitrate for the next chunk via a neural network (NN). The input features of this NN in k-th chunk are ($S_k = \\{C_k, M_k, F_k\\}$) and are detailed as follows: 1) Past Wireless Channel Estimate: The learning agent uses the past t chunks' channel status vector $C_k = \\{C_{k-t},...,C_k\\}$ as input to the NN, where $C_i$ represents the channel throughput for video chunk i. 2) Latent Content: To detect the diversity of video content, the learning agent uses $M_k = \\{N_{k+1}, V_{k+1}\\}$, where $N_{k+1}$ and $V_{k+1}$ denote the size of each bitrate of the next chunk (k + 1) and the perceptual quality metrics (semantic ambiguities) for each bitrate of the next chunk, respectively. 3) Player Rollout Playback: Rollout information is collected by the player on the edge user's side and is defined as $F_k = \\{U_{k-1}, B_k, D_k, m_k\\}$. Here, $U_{k-1}$ represents the video quality of the last video chunk selected, while $B_k$, $D_k$, and $m_k$ represent the buffer utilization, loading time, and the normalized remaining chunks, respectively, for the past t chunks.\nUltimately, the learning agent then selects the next chunk bitrate. The NN architecture includes a 1-dimensional CNN (1D-CNN), a fully connected 128-dimensional layer (FC-128), and GRUs that output 128-dimensional vectors (GRU-128). We use ReLU as the activation function and softmax for the last layer. Consequently, as a part of LD-ABS, the ABS selector can mitigate the QoE loss challenges discussed in Section II by selecting the optimal bitrate, thereby rationalizing bandwidth and storage consumption based on available resources, channel conditions, and QoS requirements."}, {"title": "B. LDM-enabled Compression, Denoising, and Recontruction", "content": "For the remainder of LD-ABS, to achieve higher compression of streaming video, we consider using LDM to compress the original reference frames into latent reference I frames. Meanwhile, we only retain the adjustment metadata for the predictive B and P frames. On the user side, latent reference frames are denoised to restore reference frames, and predictive frames are restored using conditional LDM and VFI. Recognizing the challenges faced by LDM-enabled real-time adaptive multi-band streaming, we propose a collaborative end-to-end LDM-enabled streaming framework. This framework aims to deliver high-resolution, low-latency streaming video while conserving communication bandwidth and storage with adaptive time-varying appropriate bitrate. Specifically, the proposed framework involves the following key components:\n(i) Decode the video into I-frames, P-frames, and B-frames via FFmpeg\nIn the beginning, We utilize FFmpeg to decode the streaming videos into their respective frame types in Fig. 3, Step 1. It is crucial that each frame maintains accurate timestamps, which ensures that the streaming video can be reconstructed accurately in the final step. To achieve higher compression performance and reduce bandwidth usage, we do not compress each frame individually. Since I-frames are larger compared to other frames, we use the VAE in the LDM to compress I-frames into semantic latent features. B-frames and P-frames are left uncompressed, as compressing them would still consume significant computational resources. Moreover, these frame sizes are relatively small because they only store the differences from the previous or next frame.\n(ii) Semantic Encoding for I-Frames compression and B/P-Frames motion vector compression\nIn this phase, we utilize the VAE of LDM to compress I-frames, as depicted in Fig. 3, Step 2. Since I-frames are comparatively larger than B-frames and P-frames, we compress I-frames to reduce bandwidth and storage requirements, thereby reducing the load on the CDN. The process involves using a key-frame VAE to compress I-frames into semantic Latent I-frames (L-I-frames) through the LDM process (Red VAE in Fig. 3. On the other hand, B-frames and P-frames motion vectors and adjustments are encoded via the non-key-frame VAE (Blue VAE in Fig. 3. To this end, it is no longer necessary to transmit the complete B/P-frames; instead, only the compressed L-I-frames and compressed motion vectors need to be transmitted, overcoming the bottleneck of high bandwidth and cache storage requirements discussed in Section II.\n(iii) Transmitting semantic latent features through noisy wireless channel\nAs depicted in Fig. 3, streaming video is sent to the edge user side via ultra-reliable low-latency communications (URLLC). The fluctuating wireless environment provides an unstable communication bandwidth and channel degradation without prior awareness. ABS must first provide smooth, non-interruptive, low-latency service to satisfy QoS, and then aim to provide high bitrate streaming to satisfy QoE. With a constant channel-bandwidth-ratio (CBR), the transmitter selects the appropriate bitrate based on the wireless environment, as different bitrate streams require distinct latent spaces to store latent frames.\nFurthermore, the 5G environments may introduce random channel noises and semantic errors during latent feature transmission processes, which can lead to incomplete or corrupted information delivery to the edge user side. As a result, not all frames in a streaming video are successfully or perfectly transmitted. Specifically, the transmission of L-I-frames, as well as the encoding information from B-frames and P-frames, may be affected by channel noises, attenuation and other uncertainties, further impacting the quality and integrity of the received video stream. Base Stations (BSs) and the proposed ABS mechanism process and schedule these requests, ensuring efficient resource allocation and maintaining QoE, considering computation complexity and streaming video quality.\n(iv) Restoring key frames through LDM denoising processes\nAs highlighted in Fig. 3, Step 3, Upon receiving the L-I-frames $z_n^{r}$, which contain wireless channel noises and errors, we apply a short reverse process to fine-tune the compressed L-I-frames $z_n^{r}$ to restore the precise reference frames $\\hat{z_n}$. The fine-tuning step aims to improve the reconstruction quality by completing the channel denoising task. Finally, we pass $\\hat{z_n}$ through the Red VAE decoder to reconstruct the I-frames. Since the starting point r for channel denoising adapts to the variations in channel estimation results, the proposed LD-ABS approach enhances the robustness of video transmission against challenges such as cahnnel noises, gain attenuation, and semantic errors under time-varying wireless network conditions. Furthermore, since the LDM performs the denoising process in a low-dimensional semantic latent space with fewer steps, it achieves a relatively fast reconstruction speed. This helps mitigate the high computational complexity and slow processing typically associated with DMs.\n(v) Restoring non-key frames through conditional LDM\nWe follow this process to restore non-key frames. After recovering the I-frames $\\hat{z_n}$, we apply a fine-tuning process based on the recovered latent space for L-I-frames as illustrated in Fig. 3, Step \u2463. Specifically, a short forward noise n is added to the recovered latent space from the I-frames to determine $z_n^s$, which prepares the content for reconstructing the B-frames and P-frames from the I-frames. Next, we apply a reverse diffusion process to denoise and restore the non-key latent features. This process is conditioned on the motion vector encodings of the B-frames and P-frames. Finally, the fine-tuned non-key latent features are decoded using the Red VAE decoder to generate Latent B-frames (L-B-frames) and Latent P-frames (L-P-frames). Since the semantic latent vectors of B/P-frames are fine-tuned based on similar L-I-frames, challenges commonly encountered in FFmpeg technologies, such as frame adjustment and motion estimation, are effectively addressed. Consequently, only the encodings of motion vectors, which serve as the condition c for the CLDM, need to be transmitted.\n(vi) Merging new frames to a new streaming video\nThe latent spaces of I-frames, B-frames, and P-frames are decoded by the key-frame VAE decoder as shown in Fig. 3 Step 5, to reconstruct the corresponding I-frames, B-frames, and P-frames. In the final step, as depicted in Fig. 3, Step 6, we utilize FFmpeg to merge the newly constructed frames in chronological order to reconstruct the new streaming video."}, {"title": "V. NUMERICAL RESULT AND DISCUSSION", "content": "To assess performance of average chunk QoE, consistency, and fast adaptation across a wide range of wireless environments, we evaluate our proposed LD-ABS on a virtual player, which is widely assessing system performance of video streaming [6]. This player simulates the adaptive video streaming process using real-world network throughput datasets (3G/HSDPA, FCC, OBOE) and allows comparison with other ABS algorithms under various user and network conditions [6]."}, {"title": "C. QoE Performance Comparison", "content": "In ABS, the video is temporally divided into K chunks (i.e., segments) of fixed duration L. Each chunk is encoded into multiple quality versions at different bitrates, with the set of available bitrates denoted as A = {1, ..., aM}, where M is the total number of bitrate options. ak represents the bitrate selected for the k-th chunk. The Markov Decision Process (MDP) state sk \u2208 S for chunk Uk is characterized by six features: (1) previous chunk average throughput Ck-1, (2) corresponding download time dk-1, (3) chunk sizes for all available bitrate versions of the k-th chunk, (4) current buffer occupancy Bk-1, (5) selected bitrate ak-1 for the previous chunk, and (6) the remaining number of chunks yet to be downloaded. To evaluate user-side QoE, we adopt an objective metric that balances video quality (bitrate utility), quality fluctuations (smoothness penalty), and playback stalls (rebuffering penalty) through a linear combination [6]. The QoE for chunk k is defined as follows:\n$QoE(s_k, a_k) = \\mu(a_k) - \\alpha |\\mu(a_k) - \\mu(a_{k-1})| - \\beta max(0, d_k - B_{k-1})$         (1)\nwhere\n\u2022 Bitrate Utility: $\\mu(a_k) = log(a_k / min(A))$ quantifies video quality perception, where higher bitrates improve visual quality but consume more bandwidth, increasing rebuffering risk.\n\u2022 Smoothness Penalty: $\\alpha |\\mu(a_k) - \\mu(a_{k-1})|$ discourages large fluctuations in video quality between consecutive chunks, ensuring a consistent viewing experience.\n\u2022 Rebuffering Penalty: $\\beta max(0, d_k - B_{k-1})$ penalizes playback stalls when the buffer runs out before the next chunk is downloaded.\nThe penalty weights are set as $\\alpha$ = 1 and $\\beta$ = 2.66 to balance smooth transitions and minimize rebuffering delays, following [6]. The bitrate selection ak for the k-th chunk is optimized to maximize the overall QoE based on the tradeoffs above. The key observation is that LD-ABS consistently outperforms baseline algorithms in terms of average chunk QoE and bitrate utility. Moreover, the results demonstrate that LD-ABS maintains robust performance across all sessions, with the largest proportion of sessions achieving higher QoE values. As shown in Fig. 4(b), which illustrates the cumulative distribution functions (CDFs) of the average QoE across all sessions for different algorithms, at least 95% of LD-ABS sessions achieve an average QoE greater than zero.\nFig. 5 shows that our proposed LD-ABS framework outperforms the testing benchmarks in [6], in terms of chunk QoE, bitrate utility, rebuffering penalty, and smoothness penalty.\nFig. 6 illustrates a sample streaming simulation within a chunk. This chunk consists of one I-frame followed by a P-frame, another P-frame, and a B-frame. Instead of transmitting the entire frames through a noisy wireless channel, only lightweight adjustment metadata is transmitted. The LDM then reconstructs the respective latent frames based on the recovered latent I frame ($\\hat{z_n}$) and the received adjustment metadata, generating the corresponding adjusted frames accordingly."}, {"title": "VI. CONCLUSION AND FUTURE DIRECTION", "content": "This paper proposes a novel LDM-aided adaptive bitrate streaming framework to improve the efficiency and quality of real-time video streaming over wireless networks. The LDM-based approach could significantly reduce bandwidth consumption with high perceptual quality by semantic-aware video compression and reconstruction. It combines diffusion-based denoising with VFI to enhance temporal coherence and reduce the impact of noisy and attenuated wireless channels. Our experimental results confirm that the LD-ABS surpasses existing ABS methods in terms of QoE, robustness across sessions, and network adaptability.\nWhile the above results look promising, many challenges are still open for future research. The multi-step reverse diffusion process introduces some latency in reconstructing the video, in particular for longer video sequences. Investigating acceleration methods like knowledge distillation and model quantization is imperative. Second, since current LDMs are only trained within a specific dataset, they might lack the generalization ability across diverse video contents and network conditions. Future work should be devoted to domain adaptation and continual learning to enhance the robustness in real-world streaming scenarios, such as live news broadcasts and sports streaming, as well as vehicular networks."}]}