{"title": "PRIMO: Progressive Induction for Multi-hop Open Rule Generation", "authors": ["Jianyu Liu", "Sheng Bi", "Guilin Qi"], "abstract": "Open rule refer to the implication from premise atoms to hypothesis atoms, which captures various relations between instances in the real world. Injecting open rule knowledge into the machine helps to improve the performance of downstream tasks such as dialogue and relation extraction. Existing approaches focus on single-hop open rule generation, ignoring multi-hop scenarios, leading to logical inconsistencies between premise and hypothesis atoms, as well as semantic duplication of generated rule atoms. To address these issues, we propose a progressive multi-stage open rule generation method called PRIMO. We introduce ontology information during the rule generation stage to reduce ambiguity and improve rule accuracy. PRIMO constructs a multi-stage structure consisting of generation, extraction, and ranking modules to fully leverage the latent knowledge within the language model across multiple dimensions. Furthermore, we employ reinforcement learning from human feedback to further optimize model, enhancing the model's understanding of commonsense knowledge. Experiments show that compared to baseline models, PRIMO significantly improves rule quality and diversity while reducing the repetition rate of rule atoms.", "sections": [{"title": "1. Introduction", "content": "Rules usually refer to objective regularities or logical relationships of domain concepts, usually expressed in the form of \"if-then\u201d statement (Nov\u00e1k and Lehmke, 2006). Rules can describe most complex knowledge, while naturally incorporating domain-specific knowledge (Chi, 2010). When reasoning based on rule, users can intuitively understand the process logically (Wason, 1968). Therefore, rules are widely used in downstream applications, such as intelligent data analysis (Becquet et al., 2002) and knowledge discovery (Garc\u00eda-Vico et al., 2018). For example, Lin et al. (2001) suggested that rule-based reasoning can quickly narrow the search space in question answering.\nRule generation aims to discover rules that satisfy logical constraints from large amounts of data. Traditional research has been devoted to generating rules by observing data commonalities. For example, one of the core tasks of Inductive Logic Programming (ILP) (Muggleton, 1999) is to mine rules in the form of Horn clauses from data (Raedt and Kersting, 2004). Since the axioms of the rules are restricted to the entities and relations already present in the given context, this leads to a limited and fragile expression of such rules (Wrobel, 2001). Furthermore, these methods have weak generalization capabilities due to the scale of the knowledge source (Muggleton, 1999).\nIn recent years, some researchers proposed open rule generation, aiming to generalize more diverse rules from large-scale open KBs. Hwang et al. proposed COMET (2021), a pre-trained language model that can learn commonsense from natural language. Given any text, COMET can generate new rules of the form of If-Then statement. However, COMET's training dataset, ATOMIC2020 (Sap et al., 2019), contains only 23 manually defined relations, which limits the types of rules that can be generated. Orion (Cui and Chen, 2021) adopts an unsupervised approach to utilize the knowledge in the Pre-trained Language Models (PLM) to automatically mine open rules. However, since it does not consider the ontological information of the entities in the rules, Orion tends to generate rules that are not logically self-consistent. Existing approaches focus on single-hop open rule generation, i.e. the generation of multiple parallel hypothesis atoms based on given premise atoms. However, it is difficult to extend to some complex scenarios due to the short chain of rule reasoning and weak expression of complex logic capabilities, such as multi-round dialogues. In multi-hop open rule generation, the currently generated hypothesis atom must take into account all previously generated rule atoms, which places higher demands on the model's reasoning ability. In addition, multi-hop open rule generation requires the model to own global information awareness. Existing methods do not have long-term context awareness, which leads to logical incoherence between atoms.\nTo address these issues, we propose PRIMO a ProgRessive multi-stage Induction method for Multi-hop Open rule generation. By introducing ontological information of entities into the hypothetical atom generation procedure, the generation of incorrect rules can be effectively mitigated. Considering the reasoning challenges of multi-hop open rules, we introduce generation, extraction and ranking modules in each sub-rule generation phase. Multiple modules are connected through the designed prompt, and the modules collaborate with each other to progressively generate multi-hop open rules. To reduce the repeated generation of rule atoms, we update the prompt after each derivation to learn the prior information of the generated atoms. Finally, after fine-tuning the model, we construct reward signals based on human feedback, which further enhance reasoning with common sense through reinforcement learning.\nTo evaluate the effectiveness of multi-hop open rule generation, we constructed a benchmark dataset and evaluated various systems using a wide range of automated metrics and human judgement. The results show that PRIMO effectively improves performance by splitting rule generation into multiple stages. Moreover, thanks to the stage-wise updating strategy of prompts, our approach significantly reduces the generation of repetitive atoms. It outperforms a series of baseline models and achieves performance close to LLM, confirming the effectiveness and superiority of PRIMO."}, {"title": "2. Related Work", "content": "Text generation based on PLM Early research on text generation primarily relied on manual rules and predefined templates (Kale and Rastogi, 2020). These methods involved creating templates for text generation by manually extracting features and applying some simple syntactic and grammatical rules to organize the generated text. Rule-based and template-based approaches required a significant amount of manual effort and were limited in their application due to fixed generation patterns and narrow use cases. With the development of deep learning, models such as Recurrent Neural Networks (RNN) (Cho et al., 2014), Long Short-Term Memory Neural Networks (LSTM) (Greff et al., 2017), and Transformer (Vaswani et al., 2017) have brought significant performance improvements in text generation. Recently, text generation methods based on pre-trained language models have demonstrated unparalleled performance, capable of generating fluent text in few-shot and zero-shot scenarios (Iqbal and Qureshi, 2022).\nOpen rule generation Open rule generation involves summarizing and inducing rules from an open knowledge base (KB), which means deriving new information from known information. Previous work has focused on discovering rules within systems, and traditional rule generation methods typically rely on closed datasets, such as Inductive Logic Programming (ILP) (Raedt and Kersting, 2004), AMIE(Gal\u00e1rraga et al., 2013), AMIE+(Gal\u00e1rraga et al., 2015), and other methods. These methods have limited expressive power since they lack common sense, and their rules are restricted to existing entities and relations. John McCarthy (1984) suggested that these rules lack commonsense and are \"difficult to extend beyond the scope originally contemplated by their designers\". In recent years, researchers have discovered that PLM can serve as high-quality open KBs (Petroni et al., 2019; Wang et al., 2020)and commonsense KBs(Sap et al., 2019; Trinh and Le, 2018). COMET (Hwang et al., 2021) was trained on annotated If-Then rule sets, extending the knowledge representation from structured KBs to open natural language knowledge. However, COMET is limited by the type of rules for a given KB, which deviates from the principle of summarizing data commonalities to generate rules, resulting in a weaker expression. Cui et al. (2021) proposed an unsupervised rule generation method called Orion, indicating that PLM can be leveraged as a KB to discover commonalities in data. However, Orion ignores the ontological information associated with the rules. As a result, it may generate rules that are logically inconsistent."}, {"title": "3. Problem Definition", "content": "Rules employ logical symbols to describe implicit concepts or patterns in data. This paper references the definition of an open rule based on Horn clauses (Schoenmackers et al., 2010). In Horn clauses, atoms are facts that may contain variables in the subject and/or object (Schoenmackers et al., 2010). Cui et al. (2021) define an open rule as an implication from a premise atom to a hypothesis atom. We will follow the same definition in this paper. The formal definition of an open rule is given in the following form:\nDefinition 1 (Open rule). The open rule is a logical deduction from the premise atom (x, rp, y) to the hypothesis atom (x,rn, y):\n$\\newline$\n$\\ (x,r_p,y) \\rightarrow (x, r_h, y) $\\newline$\n$\\newline$\nwhere rp and r\u2081 represent relations described in natural language. This rule suggests that if entity x and y have rp, they are also likely to have rh. In contrast to rules with strict formal definitions, open rules have a more expressive form, which makes them better suited to capture the complexities of the real world. For example, the open rule (x, was born in,y) \u2192 (x,is citizen of,y) means that a person being born in a particular country (usually) is a citizen of that country. And (Kobe, was born in, USA) \u2192 (Kobe, is citizen of, USA) is an instance of the rule mentioned above.\nGiven a premise atom (x,rp,y), we aim to induce one most relevant hypothesis atom from LM. Specifically, we define the problem as follows:\nDefinition 2 (Open rule generation). For a given premise atom (x,rp,y), find the rh with the highest probability P(rh | rp) ,and then obtain the hypothesis atom (x, rh,y). In an open rule, the number of logical deductions from the premise atom is called its"}, {"title": "4. PRIMO", "content": "As shown in Figure 1, PRIMO consists of three stages, i.e., Generation, Extraction, and Ranking, connected sequentially. The Generation module creates descriptions of hypothesis atoms based on the premise atoms. The Extraction module retrieves hypothesis atoms implied in the text output of the generation stage, and the Ranking module evaluates the plausibility of candidate hypothesis atoms. The design reasons for the progressive framework can be summarized as follows:\n(1) We observed that adopting an end-to-end approach, where the model directly generates hypothesis atoms, makes it challenging to explore a logically coherent rule chain and tends to generate repetitive rule atoms. By combining three small-scale language models, each model serving a different purpose, and refining the reasoning process, we can achieve better generation performance. By dividing the process into distinct stages - Generation, Extraction, and Ranking - we believe that a multi-stage approach is more controllable and transparency in rule generation compared to a single-stage method, as we can directly observe the output of each stage and adjust the model for a particular stage individually.\n(2) Each module is independent and exhibits good transferability. PRIMO utilizes different base models in each stage, enabling these models to complement each other's strengths and weaknesses. At the same time, the flexibility of the framework allows the underlying model to be changed at any stage as needed to improve overall performance, rather than relying entirely on the performance of any singlone model. By fine-tuning the models separately for different domains of data, we enhance the overall network's transferability. We will discuss thprovide details of the architecture from Sec. 4.1 to Sec. 4.4."}, {"title": "4.1. Generation", "content": "Due to the substantial implicit common sense contained within PLM, this work aims to establish a high-performance open rule generation method that can be achieved without manual annotation. Cui et al (2021) suggests that the lack of ontology information about entities may lead to semantic conflicts between premise and hypothesis atoms. For instance, given [X] is a provincial capital of [Y], Orion may output some atoms like [X] is a river of [Y].Therefore, we attempt to introduce entity type information into the open rule generation process to improve the correctness and diversity of rule. Because adding type information can provide ontology-level constraints, we provide it to the Generation module for a given atomic entity pair.\nInitially, we input a prompt into the PLM. As shown in Figure 1, the prompt for the Generation module is constructed as follows: \"If A is typeA, B is typeB, premise atoms, then what other relationships can we derive between A and B?\" It contains three slots, typeA and typeB, which are filled with the type information of entity A and entity B. And premise atoms, which consists of the given premise atom. Importantly, during the subsequent multi-hop generation, in addition to the provided premise atom, premise atoms need to be updated to include the atoms generated in the previous generations.\nThe goal of the Generation module is to induce the model to \"speak out\" its internal implicit knowledge commonalities based on the initial information provided by the prompt, thereby generating rule atoms. Therefore, through a \u201cdialogue\" with the prompt, the Generation module is tasked with describing other potential relations that may exist between entity pairs. Taking into account the nature of the task, we choose GPT-2, which is well suited forexcels at generative tasks, as the base model for the Generation module."}, {"title": "4.2. Extraction", "content": "As previously stated, the Generation module performs reasoning on potential relations between pairs of entities based on the provided premise atoms. The output text from the Generation module contains a wealth of reasoning knowledge, including an analysis of entity pair type information, descriptions of the establishment of premise atoms, and scenarios of other possible relations that may exist between entity pairs. We need to summarize and extract the key information from the output of the Generation module. Therefore, we design the Extraction module to accomplish this task.\nAs shown in Figure 1, the Extraction module first fills the Text generated by the Generation module into predefined slots in a prompt and then feeds this prompt to the model. After fine-tuning, the model learns to extract hypothesis atoms from the given text. Therefore, the model outputs a set of candidate hypothesis atoms.\nSimilar to the Generation module, we choose GPT-2 as the base model for the Extraction module. Although the prompt aims to instruct GPT-2 to extract only hypothesis atoms from the given text, we found that the model may still output some atoms that do not exist in the given text. We attribute this to GPT-2's limitation in ensuring factual accuracy. To achieve better extraction performance, we further optimize GPT-2 using reinforcement learning from human feedback, as described in Sec. 4.5."}, {"title": "4.3. Ranking", "content": "As described above, we have obtained a set of candidate hypothesis atoms in the first two stages. However, these atoms may have problems such as logical inconsistency and semantic repetition with existing atoms. In the next stage, we need to evaluate the plausibility of these generated atoms.\nFirst, as shown in Figure 1, each candidate hypothesis atom is filled into a statement that is to be evaluated. This statement follows the format: \"If A is typeA, B is typeB, premise atoms, we can get hypothesis atom.\" A total of n statements are generated, where n is the number of hypothesis atoms output by the Extraction module. Next, these statements are fed into a well-trained Bert. Bert encodes and scores these n statements, and the hypothesis atom with the highest score is considered the most reliable and is added to the open rule chain. The goal of Bert is to map an input text sequence to a reward value, which numerically corresponds to human preferences.\nFor Bert's training, we use ranked sequences of open-rule statements, rather than artificially scoring the statements directly, in order to mitigate the potential noise caused by variations in annotators' perspectives and to reduce the bias introduced by subjective human scoring. For example, if there are four hypothesis atoms and their order is A > B > C > D, and we need to train a scoring model so that it can assign appropriate scores to the four atoms based on the order, i.e., r(A) > r(B) > r(C) > r(D). Here r() represents the score of the atom. Therefore, the loss function is designed as follows:\n$\\newline$\nloss(\u03b8) = \u2212 \u2211 [log (\u03c3(r\u03b8(yw) \u2212 r\u03b8(yl)))]\n$\\newline$\nwhere yw represents all the atoms ranked above ye. To better normalize the differences, we applied a sigmoid function to each pairwise difference to bring the values into the range of 0-1. After training, the model's scoring of hypothesis atoms reflects human value judgements, allowing the plausibility of atoms to be assessed."}, {"title": "4.4. Multi-hop Open Rule Generation", "content": "The multiple-hop generation of the open rule can be viewed as consisting of n successive single-hop generations. Through the Generation-Extraction-Ranking process, we can obtain the hypothesis atom that the model deems most reasonable given the current information. Then, as shown in Figure 1, we record the top-ranked generated atom and add it to the end of the open rule chain. For the next hop of rule generation, the hypothesis atom generated in the previous hop should serve as the premise atom for the current hop's rule generation. So the Generation module needs to consider all the premise atoms in the chain. We update the Generation module's prompt with the premise atoms along the chain, starting with the initial atom and concatenating all the atoms in the order they were generated. The prompt of the Extraction module remains unchanged and continues to output the candidate hypothesis atoms extracted from the text generated by the Generation module. The statements inputted into Ranking must also update the contents of the premise atoms. Hence, the open rule chain is continuously updated, repeating the single-hop generation incrementally to generate multi-hop open rules until the chain reaches the predefined length. Since both the Generation and Ranking modules have global information updates, PRIMO combines the information from all known atoms when completing the next hop generation. This helps ensure logical consistency between atoms and reduces semantic repetition."}, {"title": "4.5. RLHF", "content": "ChatGPT proposed by OpenAl (OpenAI, 2022) breaks the boundaries between machines and humans. This innovative model excels in various domains of tasks. The underlying work is based on a novel training paradigm in the field of Large Language Models (LLMs), namely Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022). Over the past few years, the ability of various LLMs to generate diverse text based on prompts has been quite impressive. However, the evaluation of the generated results is subjective and context dependent, making it challenging to measure these results using existing text generation metrics such as BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004).\nFor instance, the goal of open rule generation is to produce genuine information. Existing methods typically rely on next-word prediction and simple loss functions, such as cross-entropy. However, these methods lack the explicit incorporation of human preferences and subjective opinions. RLHF, on the other hand, employs reinforcement learning to optimize the model directly based on human feedback, encouraging the model's output to align with complex human values. To further enhance the rationality of the rule generated by PRIMO and to improve the performance of each stage in the model, we use RLHF for additional optimization.\nFor the Generation module, our objective is to ensure that the generated text adheres to factual correctness as much as possible while avoiding redundancy with known information. For the Extraction module, our goal is to ensure that the extracted results are faithful to the original text, matching with the descriptions in the text generated by the Generation module, without introducing unrelated information. Since the Ranking module is pre-trained with annotated data, its scoring results are to some extent a reflection of human value preferences. Therefore, it serves as the reward model for the Generation module in RLHF. Considering different optimization objectives, for the Extraction module, we employ human judgment to directly score the results in RLHF.\nWe utilize Proximal Policy Optimization (PPO) (Schulman et al., 2017) to optimize the initial PLM parameters. During the training of the Generation module, the input and output are aligned with the configuration in Figure 1. Subsequently, the Extraction module performs extraction and the reward value of the Generation module is the maximum score among all the hypothesis atoms evaluated by the Ranking module. In the training of the Extraction module, we fill the text generated by the Generation module into the prompt, and then judge the output result by human scoring. Based on the score, we use PPO to optimize the model and complete the training for one data point. The updated the Extraction module then proceeds to the training of the next data point, continuously optimizing the model. To prevent the optimization from getting out of control, we introduce KL divergence (van Erven and Harremo\u00ebs, 2014) as a constraint on the objective function for the optimization:\n$\\newline$\nr = re - ArkL\n$\\newline$\nwhere re is the reward, and rkl calculates the KL divergence between the model's outputs before and after the update, which serves as a penalty term. In this way, we enable PRIMO to better adapt to human preferences, resulting in rules that more closely resemble common-sense cognition."}, {"title": "5. Experiment", "content": "In order to evaluate the effectiveness of multi-hop open rule generation, we construct our benchmark dataset which contains 495 premise atoms. To build premise atoms that describe x and y, we"}, {"title": "5.4. Semantics Repetition", "content": "In order to investigate the effect of PRIMO on the generation of repeated rule atoms, we conduct an experiment on rule repetition rates. We compute the Jaccard similarity (Hamers et al., 1989) between the strings of each hypothesis atom in the open rule chain. If the similarity exceeds a certain threshold, the two atoms are considered duplicates, and the count of duplicate atoms is increased by one. The final repetition rate is calculated by dividing the number of duplicate atoms by the total number of rule atoms generated.\nThe experimental results are shown in Table 3. We find that PRIMO outperforms the baselines overall, leading baselines by at least 16.2% for different threshold settings, which demonstrates that updating information about generated atoms in the prompt when generating the rule atom for the next hop can significantly reduce the semantic repetition of atoms. Moreover, when the similarity threshold is increased from 80% to 90%, there is a significant improvement in the performance of PRIMO (8.9% improvement) compared to COMET and Orion, where there is no significant difference in performance (change of less than 2.5%). The Prompt-based method shows a smaller improvement (an improvement of less than 8%). We argue that PRIMO's atom repetition rate decreases dramatically as the similarity threshold is increased, suggesting that atoms generated by PRIMO are rarely nearly identical to previously generated atom representations, and that the semantic richness is significantly higher than the baselines."}, {"title": "5.5. The Length of Rule Chain", "content": "In our experiment, PRIMO has 65 data points resulting in no hypothesis atoms generated (rule chain length of 0). And there are 112 data points categorized as partial failure, meaning that PRIMO could generate hypothesis atoms but didn't reach the preset rule chain length. Except for the chains with a length of 0, we analyze the length of the other generated rule chains, as shown in the Figure 3. We observe that PRIMO performs well in generating shorter rule chains (hop \u2264 3), but experiences a slight performance drop when trying to generate longer rule chains. We analyze that one reason is that it's genuinely impossible to infer more unknown relations between entity pairs. However, it also suggests that there is room for improvement in PRIMO's ability to generate longer rule chains."}, {"title": "5.6. Case Study", "content": "To visualize the generation quality of PRIMO, we provide some examples in Table 4. When given the atom <A> is stop of <B>, although Orion generates some reasonable rule atoms, such as <MASK> is a major part of <MASK>, <MASK> is served by <MASK>, there are also some incorrect atoms. For instance, the first generated atom classified the entity as a subway station, but in reality, the correct answer should be a bus station. It is evident that Orion lacks modeling of entity ontology information related to rules, which leads to generation of wrong atoms. In comparison, PRIMO accurately describes the rule atom to the Transit Stop class, by introducing entity type information, such as <MASK> provides transit connections to <MASK>. Note that the rules generated by PRIMO excel in semantic expression, with each atom having corresponding synonymous expressions to the ground truth atoms, which indicates that PRIMO effectively improves the correctness of the final results by using the Generation module to generate descriptive text related to the premise atoms. When the given atom is <A> is political party of <B>, the generated atoms of Orion are restricted to those related to \"is the legislature of\", resulting in semantic repetition. On the other hand, PRIMO generates rule atoms that are both non-repetitive and reasonable. These cases show that PRIMO reduces the repetition rate of rule atoms by updating the information of previously generated atoms. Additionally, the preset rule chain length is 4, but Orion generates three identical atoms, indicating that Orion cannot generate rule atom after two hops. In contrast, PRIMO successfully reaches the preset length."}, {"title": "6. Conclusion", "content": "We propose a progressive multi-stage open rule generation method called PRIMO, i.e., generation-extraction-ranking. The latter module further refines and validates the results of the former module, which significantly enhances the semantic consistency of rule generation and mitigates duplicate generation. We optimize PRIMO with human feedback to further improve multi-hop open rule generation accuracy. Experiments show that PRIMO outperforms the PLM-based baseline model, and achieves nearly the same performance of LLMs using less than a tenth of parameters of a LLM."}]}