{"title": "BRIDGING CONTEXT GAPS: LEVERAGING COREFERENCE RESOLUTION FOR LONG CONTEXTUAL UNDERSTANDING", "authors": ["Yanming Liu", "Xinyue Peng", "Jiannan Cao", "Shi Bo", "Yanxin Shen", "Xuhong Zhang", "Sheng Cheng", "Xun Wang", "Jianwei Yin", "Tianyu Du"], "abstract": "Large language models (LLMs) have shown remarkable capabilities in natural language processing; however, they still face difficulties when tasked with understanding lengthy contexts and executing effective question answering. These challenges often arise due to the complexity and ambiguity present in longer texts. To enhance the performance of LLMs in such scenarios, we introduce the Long Question Coreference Adaptation (LQCA) method. This innovative framework focuses on coreference resolution tailored to long contexts, allowing the model to identify and manage references effectively. The LQCA method encompasses four key steps: resolving coreferences within sub-documents, computing the distances between mentions, defining a representative mention for coreference, and answering questions through mention replacement. By processing information systematically, the framework provides easier-to-handle partitions for LLMs, promoting better understanding. Experimental evaluations on a range of LLMs and datasets have yielded positive results, with a notable improvements on OpenAI-01-mini and GPT-40 models, highlighting the effectiveness of leveraging coreference resolution to bridge context gaps in question answering.", "sections": [{"title": "INTRODUCTION", "content": "Large language models (LLMs) (Brown, 2020; Chowdhery et al., 2023; Ouyang et al., 2022) have demonstrated exceptional competitiveness across various tasks, including question answering, summarization, and generation (Wang et al., 2023). Recently, many new versions of LLMs have begun to extend their capability to handle longer contexts (Yang et al., 2024; Dubey et al., 2024). Both open-source and proprietary models show some proficiency in understanding long texts and support extended reading (Zhang et al., 2024a). However, LLMs still struggle to accurately identify key passages from the middle of long contexts (Liu et al., 2024) and generate effective responses or expected content based on these passages when overwhelmed by excessive information (Shi et al., 2023). This suggests that despite having larger context windows, many LLMs still face challenges in handling long-text tasks (Li et al., 2024b)."}, {"title": "PRELIMINARIES", "content": "Coreference Resolution and Mentions. In information extraction and dialogue systems, traditional methods involve dealing with entities, relationships between entities, and semantic slot filling when processing context. However, in certain contexts and dialogue scenarios, such as speaker recognition and long context reading, the context often contains a large number of pronouns and modifiers, which significantly affects the model's understanding of the original information. To address this situation, the context can be optimized through two steps. The first step is mentions extraction: Given the input text x, extract all pronouns, nouns, noun phrases, and modifiers in the context to obtain the mention set $M = {m_1, m_2, ..., m_k}$. For a specific mention $m_i$, its reference information is represented as $R(m_i) \\rightarrow m_i$. In the input text, the reference information is likely to have a large number of"}, {"title": "METHODOLOGY", "content": "We employ a four-step Long Question Coreference Adaptation (LQCA) method. Each step involves the integration and extraction of information from the text. By partitioning long documents and resolving references within each sub-document, we use clustering techniques to merge the results from different sub-documents into the same cluster, thereby achieving overall reference resolution for the entire document. Finally, we replace the references and provide the cleaned text to the large model for contextual understanding and response."}, {"title": "COREFERENCE RESOLUTION ON SUB-DOCUMENT", "content": "The state-of-the-art reference resolution model, Maverick (Martinelli et al., 2024) is based on the DeBERTa-v3 (He et al., 2021) architecture, which has specific requirements regarding the length of the input tokens. We cannot directly perform reference resolution on the entire long input. Given an long input context X, we initiate the process by partitioning the context into sub-documents ${S_i}_{i=1}^{N}$, each constrained to a maximum length of L. N is the total number of partitioning documents. Our partitioning is based on a sliding window approach, where each partitioning starts from the beginning of a sentence and extends to the position of the last sentence that does not exceed the length of L. If the last sentence exceeds this limit, truncation is applied for the whole sentence.\nFor each sub-document $S_i$, we utilize the maverick-mes-ontonotes\u00b9 model to perform mentions extraction and coreference resolution. $M = {m_1, m_2, ..., m_k }$ represent the set of detected mentions in the input sequence, where each mention $m_i$ is characterized by its position $p_i$. In the coreference resolution of sub-documents, $c_i^j$ represents the j-th coreference set in the i-th sub-document and $C_i = {c_i^1, c_i^2,...}$ is the total coreferences in single sub-document. The mentions under this coreference set belong to this reference. After providing the sub-documents to the Maverick model through batch inference, we obtained all the coreferences $C = {C_i}_{i=1}^{N}$ and mention M corresponding to the long text. The results help us in the subsequent processing of recognizing and merging the same coreferences and mentions, allowing us to combine the information from these sub-documents."}, {"title": "MENTIONS DISTANCES COMPUTATION", "content": "To address the issue of co-referent mentions being distributed across multiple partitions, we construct a sparse matrix representing mention relationships. For any two mentions $m_a$ and $m_b$ occurring in the same sub-document, we define their co-reference score $s_i$ under a particular sub-document and a specific reference j as follows:\n$s_i(m_a, m_b) = \\begin{cases}\n1 & \\text{if } m_a, m_b \\in c_i^j \\\\\n0 & \\text{otherwise}\n\\end{cases}$\nSimilarly, the score for non-coreference $t_i$ is defined as:\n$t_i(m_a, m_b) = \\begin{cases}\n1 & \\text{if } m_a, m_b \\notin c_i^j \\\\\n0 & \\text{otherwise}\n\\end{cases}$\nThus, the co-reference score for these two mentions within the sub-document is $s_i(m_a, m_b) = \\sum_j s_i^j$, and the non-co reference score is $t_i(m_a, m_b) = \\sum_j t_i^j$. Since two mentions can only belong to the same reference in a single sub-document, the scores $s_i(m_a, m_b)$ and $t_i(m_a, m_b)$ satisfy $s_i(m_a, m_b) \\oplus t_i(m_a, m_b) = 1$, indicating whether the two mentions are in the same reference.\nThrough this process, we can determine the co-reference relationship of any two mentions within a sub-document. Only when both mentions are present in the sub-document can they receive a mention score. For cross-sub-document mention relationships, we integrate information across multiple sub-documents based on the distance between the two mentions.\nFor the entire long context, we define d(ma, m\u044c) as the distance between two mentions. Since mentions that appear in the same sub-document have already had their co-reference relationship evaluated by the Maverick model, we can assess their distance to determine whether they refer to the same entity in the context of the long document. The score between mentions is computed as:\n$d(m_a, m_b) = \\frac{\\sum_{i=1}^{N} s_i(m_a, m_b)}{\\sum_{i=1}^{N} s_i(m_a, m_b) + \\sum_{i=1}^{N} t_i(m_a, m_b)}$\nFor mentions not in the same sub-document, since they lack coreference and coreference scores, merging their relationships requires evaluation based on distance information. For any mention $m_c$, if the distance between mentions $m_a$ and $m_c$ is $d(m_a, m_c)$, and the distance between mentions $m_b$ and $m_c$ is $d(m_b, m_c)$, we apply the multiplication principle:\n$d(m_a, m_b) = max\\{ d(m_a, m_c) \\times d(m_b, m_c) \\}$\n$m_c \\in M$\nUsing this distance evaluation, the distance information between any two mentions in the mention cluster can be calculated. This form can be easily computed using the variations of the shortest path problem to obtain the corresponding distance information. The algorithm could be refered in Appendix D.\nAfter computing the longest dot product path, we compare the longest path score to a predetermined threshold k, and based on this information, we construct a mention relationship graph G(M, E):\n$(m_a, m_b) \\in E, if d(m_a, m_b) > k,$\nIn the mention relationship graph, each strongly connected component $C_i = \\{ u \\in M : \\forall v \\in C_i, u \\leftrightarrow v \\}$ represents a set of coreferent mentions.\nThis classification helps bridge the gaps in context, ensuring that relevant mentions are identified as referring to the same entity, thereby improving the model's understanding of the document content."}, {"title": "DEFINING COREFERENCE REPRESENTIVE MENTIONS", "content": "For the strongly connected components we obtained, each strong connection serves as a coreference, and when inputting into the LLM later, all mentions under the same coreference need to be replaced, especially those mentions that are pronouns, which should be transformed into specific, meaningful content.\nWe first use the lightweight spaCy model en_core_web_sm to perform part-of-speech tagging on all words in the article. In the tagging results, if a token in the mention span belongs to PRON, it is marked as $p(m_i) = PRON$. Additionally, for a coreference, we want the transformed text to select the mention that contains the most meaningful equivalent. Let $m_{c_i}$ be the representative mention of reference $C_i$, and $f(m_k, c_i)$ be the number of times mention $m_k$ appears under coreference $c_i$. The process could formulated as:\n$m_{c_i} = argmax_{m_k} \\{ f(m_k, C_i) \\times [p(m_i) \\neq PRON] \\}$\nIf there are multiple mentions that satisfy this condition, we select the earliest mention based on the principle of first selection. This representative mention serves as the normalized text for the entire coreference. By effectively replacing vague mentions with their n the text, enabling LLMs to maintain a coherent understanding of the context."}, {"title": "QUESTION ANSWERING WITH MENTIONS REPLACEMENT", "content": "We need to modify the original text into the target text by replacing all mentions in the text based on each coreferent representative mention. When handling overlapping cases, we prioritize replacing the mention with the largest overlapping range. If no suitable replacement is available (for example, only pronouns), we retain the original text.\nTo execute the Question Answering (QA) task, we leverage the reasoning capabilities of the LLM, formalized in the following process:\n$R = arg \\underset{R}{min} (-log P(R|C', Q))$\nwhere $P(R|C', Q)$ represents the probability of generating a response R given the modified context C' and the question Q. By implementing these steps, our approach effectively addresses the coreference issues in long texts, enabling subsequent tasks (such as QA) to interact more accurately with the content and enhance contextual awareness. This integrated method not only tackles the challenges posed by long texts but also effectively bridges contextual gaps, improving LLM performance in understanding long contexts."}, {"title": "EXPERIMENTAL SETUP", "content": "To evaluate the performance of LQCA in long text contexts, we primarily conduct assessments on question-answering data across three datasets, targeting different task categories including summarization tasks, question-answering tasks, and multiple-choice classification tasks. Specifically, these three datasets are LooGLE (Li et al., 2024a), L-Eval (An et al., 2024), and LongBench (Bai et al., 2024). For the summarization task, we select the arXiv paper abstract category from the LooGLE dataset for evaluation. For the question-answering task, we focus on closed-ended tasks in L-Eval, emphasizing multiple-choice and true/false question-answering tasks. For the dataset requiring information extraction from texts, we utilize the multi-document question-answering dataset under LongBench, primarily selecting three English-based datasets: HotpotQA (Yang et al., 2018), 2WikiMultihopQA (Ho et al., 2020), and MuSiQue (Trivedi et al., 2022). This diverse selection enables us to better understand the performance of our approach across different tasks and datasets, providing a comprehensive perspective on its strengths and weaknesses in multi-task scenarios. For more details on the datasets, please refer to Appendix B."}, {"title": "BASELINES", "content": "Currently, methods for addressing long-context issues mainly involve guiding models through chain-of-thought techniques for specific step reasoning. One method is the compression strategy, which condenses key information from long texts into concise paragraphs, allowing questions to be answered and processed based on this. Another commonly used method involves slicing text segments for retrieval. Our comparison benchmarks include the following methods:\nVanilla LM: Guides the model to generate responses through simple text input, testing its basic ability to handle long contexts.\nZero-shot Chain-of-Thought (Kojima et al., 2022): Utilizes prompt templates, designing specific prompts to guide the model's reasoning, leveraging the model's inference capabilities without the need for additional training data.\nRAG (Lewis et al., 2020; Ram et al., 2023) with Self-Corpus: Unlike traditional RAG, we do not introduce additional knowledge but instead use slices of long text segments as the corpus. When questioning relevant issues, we retrieve from the corpus and combine it with context for reasoning, aiming to improve the model's understanding and response quality to long texts.\nRECOMP (Xu et al., 2024) with Self-Corpus: Builds upon RAG with an added compression strategy, using the same long text segment slice corpus. This method improves processing efficiency by compressing key information from long texts, aiming to optimize the model's reasoning process without losing important information."}, {"title": "MODELS", "content": "We utilize five models, including three from OpenAI's GPT series (Brown, 2020): 01-mini-2024-09-12, gpt-4o-2024-08-06, and gpt-4o-mini-2024-07-18, as well as two open-source large models from Llama-3 (Dubey et al., 2024) and Qwen-2 (Yang et al., 2024): llama3-gradient-8b and qwen2-7b. The five tested large models all have a 128k context window, which meets our requirements for evaluating and testing long context datasets, reducing the information loss caused by additional text segmentation. Comparing different models helps us understand whether their core reference performance in long contexts is similar."}, {"title": "IMPLEMENTATION", "content": "Our experiments were primarily evaluated in a zero-shot setting. For inference predictions of general models, we used default settings and adopted greedy search as the inference benchmark. For zero-shot reasoning chains, we referred to the prompt templates as the default chain-of-thought scheme. The evaluated models have a long context window, allowing us to retain almost all contextual information for the questions without truncation.\nNonetheless, we still establish a safety mechanism for ultra-long texts. When the input length L exceeds the model's maximum context length (indicated by the name suffix), we truncate from the middle of the input sequence S to avoid losing the beginning and end portions, which may contain key information. During the generation process, we employed greedy decoding to ensure the reproducibility of results. For LQCA, our default experimental setting is a coreference score threshold of 0.9, meaning if the distance between two mentions exceeds 0.9, we consider them within the same coreference. For the spaCy model, we used en_core_web_sm model for tokenization and part-of-speech tagging.\nIn terms of benchmark comparison, we use a retrieval method, with Contriever-msmarco (Izacard et al., 2022) as the retrieval model responsible for slicing articles in a sliding, non-overlapping manner, with each segment limited to 512 tokens. Additionally, in the RECOMP experiments, we choose the abstract compression tool provided by RECOMP as the compression tool to help the model effectively extract important information from text segments. The comparison of LQCA with other baselines is based on the differences in the comparison datasets, evaluated using Rouge-L, accuracy and F1 scores."}, {"title": "EXPERIMENTS", "content": "We evaluate the performances of five LLMs on multiple datasets with LQCA and baselines, and the results are shown in the Table 1."}, {"title": "MAIN RESULTS", "content": "Context with coreference resolution. LQCA has demonstrated highly competitive performance among other long context processing methods. It achieves the best results on almost all datasets when applied to LLMs with a higher number of parameters and strong contextual understanding, consistently outperforming other baseline performances on GPT-4o. Meanwhile, the other two models also achieve the best performance on 7-8 out of 9 baselines methods, respectively. Compared to directly providing the question to the LLM, LQCA achieves an average improvement of +3.61% on GPT-4o. On the latest 01-mini model, the coreference resolution method also significantly improves by an average of +3.18% across various metrics compared to other methods. At the same time, LQCA has also achieved good performance on open-source large models with relatively small parameter counts, showing improvements in metrics compared to direct prompting and zero-shot chain-of-thought methods. However, due to differences in context understanding capabilities, retrieval methods can sometimes find corresponding answers more accurately for these types of large models. Overall, texts that have undergone coreference resolution can help models understand contextual dependencies to varying degrees, thereby assisting models in solving various problems.\nLQCA in Long dependency question. When addressing tasks involving long-dependency issues, such as summarization, long dependency QA, and strategy-related question-answering (HotpotQA, 2WikiMHQA) tasks, coreference resolution methods demonstrate a clear advantage over other approaches, especially when the model possesses stronger contextual understanding capabilities. For example, in summarization tasks, the O1-mini model improved performance by +5.05% compared to other methods. Even more notably, in datasets like HotpotQA, which contain extensive text passages and dependencies, the impact of coreference resolution is even more pronounced. For instance, on this dataset, GPT4o showed a +7.59% improvement in results after incorporating coreference resolution, outperforming other methods by about +6.5%. The most challenging aspect of long-dependency tasks is determining various coreference relations and the specific entities being referred to. When a question is directly presented to the model, it may confuse these coreferences, leading to incorrect"}, {"title": "IMPACT OF DIFFERENT COREFERENCE REALIZATIONS", "content": "To compare the performance of different coreference resolution designs and the LQCA framework in long document comprehension, as well as to explore the specific roles of each component, We adopt three variations of LQCA as follows:\n\u2022 LQCA-LLM: We use a LLM to perform coreference resolution on each document slice, while other steps remain unchanged. The prompts used for the LLM are provided in Appendix C.\n\u2022 LQCA-w/o overlap: This slicing method segments the document into non-overlapping slices, each no longer than 512 tokens. The coreference resolution results for each slice are used to replace information in the original text, following the same replacement methods as in Defining Representation and Mention Replacement.\n\u2022 LQCA-w/RAG: After replacing the text, we introduce a Contriever-msmarco retriever during the question-answering phase. The retriever uses the corpus provided by the document slices, similar to the setup of the baseline method. Compared to the original LQCA method, this approach reduces the model's dependency on extended context length.\nAs shown in Figure 3, we evaluate different methods. The coreference resolution method using the LLM shows significant shortcomings in answering questions, likely because the model is currently unable to effectively handle specific downstream tasks. In complex contextual environments with extensive coreference information, expert-level models are often required to handle and annotate mentions accurately. The non-overlapping slicing replacement method performs similarly to LQCA in most tests, but the inability to merge coreference information between different sub-documents slightly limits the question-answering effectiveness for long documents.\nAdditionally, the LQCA-w/ RAG method helps smaller models extract relevant information and even outperforms the LQCA framework on certain metrics. This suggests that retrieval augmentation is more suitable for models with limited capabilities and computational resources in long-document QA tasks. By gathering more relevant information related to the question, it helps the model provide"}, {"title": "SETTING IN SUB-DOCUMENTS", "content": "In LQCA, two critical parameters have a significant impact on the coreference resolution performance of the framework. The first is the length of the text segmentation. We ensure that the segmented text length stays within the token limit of the model's input. The second is the preset threshold parameter K, which largely determines whether two mentions refer to the same entity. Due to variations between different texts, the number of sub-documents generated by segmentation may differ, which influences the model's inference performance. To address this, we conduct a grid search on these two variables to analyze their impact on performance, with the experimental results shown in the Figure 4.\nLonger sub-documents help capture contextual information, improving coreference resolution performance. For example, in the figure, when the sub-document length approaches the upper limit of 512, the model shows good performance with most F1 scores above 70 when the k-value is less than 0.8. Since we resolve coreferences across multiple documents and establish connections between mentions, longer contexts provide more precise information.\nCoreference scores threhold is length affected. The impact of the score threshold between mentions across the entire document on the framework's performance is strongly correlated with the subdocument length. When the segmented sub-documents are shorter, a lower k value helps maintain consistency between related mentions. However, if the threshold is set too high, some pronouns may not find the correct referent. On the other hand, an excessively high"}, {"title": "EVALUATION OF KEY INFORMATION POSITIONS", "content": "Since current large language models tend to focus more on the beginning and the end of a document in long-text scenarios, they often overlook the middle sections where answers may be located. To better assess the effectiveness of our framework, we conducted inference evaluations using the GPT-4o model on the Coursera dataset, marking the percentage position of answers within the text. We divided the positions of the answers into five intervals, using the percentage value at the end of each interval as a marker.\nMiddle position performace is highly improved by LQCA. As shown in the Figure 5, the model with coreference resolution significantly alleviates the issue of ignoring mid-document"}, {"title": "RELATED WORK", "content": "Recent advancements in LLMs highlight the significance of long-context understanding across diverse applications (Xiong et al., 2024; Pan et al., 2024). Zhu et al. (2024a) explores strategies to extend the context window of embedding models to 32k tokens without additional training, enabling their application in tasks with long inputs such as legal contracts. LongBench, L-eval and LooGLE serve as multitask benchmarks, providing a comprehensive evaluation of long-context understanding (Bai et al., 2024; An et al., 2024; Li et al., 2024a). a new context understanding benchmark has been proposed, validating the need for improved generative models in understanding context amidst varying training conditions (Zhu et al., 2024b). The study of temporal complex events through LLMs sheds light on the ability to analyze event chains effectively, particularly when utilizing suitable retrieval mechanisms (Zhang et al., 2024b). However, challenges persist; current LLMs struggle with contextual relevance based on information positioning, as indicated by performance degradation in identifying important details (Liu et al., 2024). Our LQCA method improves text quality through coreference resolution, thereby enhancing the long context understanding of tasks by LLMs."}, {"title": "LONG CONTEXT UNDERSTANDING IN LLM", "content": "Coreference resolution is an important task in the field of information extraction (Lee et al., 2017; Dobrovolskii, 2021). Techniques such as finetuning pretrained seq2seq transformers have proven effective, where document inputs are mapped to coreference-tagged sequences, emphasizing the importance of model size and supervision levels (Zhang et al., 2023). Furthermore, a novel approach focusing on event coreference emphasizes learning from events rather than entities, integrating multiple representations for improved resolution (Yao et al., 2023a;b). The complexity of coreference evaluation is addressed by highlighting the need for standardized measurement methodologies across different datasets (Porada et al., 2023). Meanwhile, prompt-based methods like CorefPrompt allow for the modeling of events and coreference simultaneously through a masked language model setup (Xu et al., 2023). Linguistic insights contribute to performance enhancements by categorizing mention-pairs into distinct decision types (Otmazgin et al., 2023). We combine coreference resolution with long context, improving text quality by replacing referents in long context texts, which aids in enhancing downstream tasks."}, {"title": "COREFERENCE RESOLUTION", "content": "This paper presents the LQCA method, a framework aims at enhancing long-context understanding in LLMs by leveraging coreference resolution. The framework operates through four systematic steps: resolving coreferences within sub-documents, calculating mention distances, defining a representative mention for coreferences, and performing question answering with mention replacement. By processing long contexts in this manner, the method simplifies the information, making it more comprehensible for the language models. Experiments conducted on five large models and nine long-context question datasets show a notable improvement during inference, with a recorded 3.61% enhancement on GPT-40. The findings illustrate the effectiveness of integrating coreference resolution with information extraction to improve comprehension of lengthy texts. Text quality remains a critical factor influencing model inference performance. We believe this framework will contribute to the long-term development of the long context understanding of LLMs."}, {"title": "CONCLUSIONS", "content": ""}, {"title": "BROADER IMPACT AND LIMITATIONS", "content": "Broader Impact. Using coreference resolution methods to optimize long contexts inference is another significant improvement in information extraction within dialogue systems powered by large language models. We believe that by introducing more techniques, such as semantic recognition and entity extraction, large models can achieve better performance when handling long texts. Additionally, we can further investigate the integration of current compression and retrieval-augmented methods with coreference resolution techniques, exploring more potential solutions to enhance text quality. Higher-quality question texts help us better address various downstream tasks and provide appropriate solutions.\nLimitation. LQCA presents innovative advancements in addressing long-context understanding and question answering. However, certain limitations warrant consideration. Firstly, the coreference resolution might struggle in dealing with ambiguous references or contexts with high complexity, possibly leading to inaccurate mention replacements. Secondly, while the method improves performance on long-context questions, its effectiveness may diminish when applied to shorter contexts, where the overhead of processing could outweigh the benefits. Furthermore, the reliance on pre-defined mention distances may limit adaptability to varied linguistic structures and usages across different domains. Future endeavors may focus on enhancing coreference resolution techniques and exploring adaptive approaches to better manage diverse contexts and improve robustness in various settings."}, {"title": "DATASET DETAILS", "content": ""}, {"title": "LOOGLE", "content": "LooGLE (Li et al., 2024a) is a benchmark specifically designed to evaluate large language models (LLMs) in long-context understanding tasks. This benchmark emphasizes tasks that rely on both short-term and long-term dependencies in text inputs, such as question answering (QA), summarization, and cloze tasks. LooGLE supports automated evaluation metrics, such as BLEU, ROUGE, METEOR, and BERTScore, to assess model performance. Notable baseline models include GPT-4-32K, GPT-3.5-16K, and ChatGLM2-6B-32K, which have been evaluated on short-term dependency tasks like cloze tests and long-term dependency tasks like document summarization.\nThe benchmark provides detailed configurations on how to optimize large language models for long-context tasks, with a particular focus on retrieval-based tasks and long-form generation tasks."}, {"title": "L-EVAL", "content": "L-Eval An et al. (2024) is part of a growing suite of long-text understanding evaluation tools. It is designed for multilingual evaluation, testing the performance of large language models on diverse tasks across language families. These tasks include document-level question answering, summarization, and cloze tasks, with a particular focus on the models' ability to handle large amounts of contextual information. The datasets are derived from real-world domains, such as scientific papers, narratives, and technical reports, making L-Eval highly valuable for tasks that require extensive context retention. L-Eval is an ideal testing platform for evaluating models in scenarios like multi-document retrieval, cross-document summarization, and multi-hop question answering, where retaining long and diverse information is crucial."}, {"title": "LONGBENCH", "content": "LongBench (Bai et al., 2024) is a bilingual, multi-task benchmark designed to test large language models' ability to handle long contexts, covering both English and Chinese. It evaluates model performance on tasks such as narrative understanding, multi-domain question answering, and summarization, where the datasets require handling large amounts of complex input, such as legal documents, scientific reports, and news articles.\nThis benchmark includes datasets like NarrativeQA (Ko\u010disk\u1ef3 et al., 2018), Qasper (Dasigi et al., 2021), HotpotQA (Yang et al., 2018), and DuReader(He et al., 2018), offering diverse application scenarios from multi-document retrieval to entity tracking in long narratives. The models are evaluated not only on single-document tasks but also on multi-document tasks and zero-shot performance tests. LongBench also incorporates Chinese-specific datasets, further extending its applicability in multilingual scenarios."}, {"title": "PROMPT FOR VARIATION OF LCQA", "content": ""}, {"title": "LCQA-LLM", "content": "Please analyze the following context, do coreference resoulution. identify the mentions, and replace them with their corresponding golden mentions which have their actual reference and meaning. Mentions could be Pronouns, Nouns, Noun Phrases or Modifiers. Ensure that the revised text maintains the original meaning and reads naturally. Please only output result only contains revised context, don't output any other information. Here is the text to be processed:\nContext: [Sentence S]\nResult:"}, {"title": "LONGEST DOT PRODUCT PATH ALGORITHM", "content": "We use this algorithm to compute the distance of mentions that calculates the distance across the entire graph. The corresponding algorithm updates paths in a manner similar to Dijkstra's algorithm. By extracting the node with the smallest distance from the priority queue, the algorithm traverses its neighboring nodes, with the selection of these neighbors constrained by a range parameter to ensure that only nodes within a given distance are considered. For each neighbor, the algorithm determines whether to update the optimal path from the current node to that neighbor by calculating the possible path lengths. If the newly calculated path length is greater than the currently recorded optimal path, an update is made accordingly.\nAlgorithm 1 All-Pairs Longest Path for Restricted Graph\nInput: Graph: G = (V, E); Weight function: w : E \u2192 R+; Range parameter: k\nOutput: Longest dot product path distances: $d_{i,j}$ for all i, j \u2208 V\nfor each node n \u2208 V do \u25b7 // Initialize distances for node n\nfor each node m\u2208 V do\nif m = n then\n$d_{n,m}$ \u2190 1 \u25b7 // Distance to itself is 1\nelse\n$d_{n,m}$\u2190 0 \u25b7 // Initial distance set to 0\nend if\nend for\nQ\u2190V \u25b7 // Priority queue to store unvisited nodes\nwhile Q is not empty do \u25b7 // Dijkstra's algorithm\nu \u2190 Extract-Max(Q) \u25b7 // Node with the smallest distance\nfor v \u2208 {m \u2208 V | |m - u| \u2264 L, (u,m) \u2208 E} do \u25b7 // Neighbors within range\nalt \u2190 $d_{n,u}$ \u00d7 w(u, v) \u25b7 // Calculate potential distance\nif alt > $d_{n,v}$ then\n$d_{n,v}$\u2190 alt \u25b7 // Update distance if longer\nend if\nend for\nend while\nend for\nreturn $d_{i,j}$ for all i, j \u2208 V \u25b7 // Return all-pairs shortest path distances"}]}