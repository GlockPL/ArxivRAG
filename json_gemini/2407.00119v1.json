{"title": "Efficient Long-distance Latent Relation-aware Graph Neural Network for Multi-modal Emotion Recognition in Conversations", "authors": ["Yuntao Shou", "Wei Ai", "Jiayi Du", "Tao Meng", "Haiyan Liu"], "abstract": "The task of multi-modal emotion recognition in conversation (MERC) aims to analyze the genuine emotional state of each utterance based on the multi-modal information in the conversation, which is crucial for conversation understanding. Existing methods focus on using graph neural networks (GNN) to model conversational relationships and capture contextual latent semantic relationships. However, due to the complexity of GNN, existing methods cannot efficiently capture the potential dependencies between long-distance utterances, which limits the performance of MERC. In this paper, we propose an Efficient Long-distance Latent Relation-aware Graph Neural Network (ELR-GNN) for multi-modal emotion recognition in conversations. Specifically, we first use pre-extracted text, video and audio features as input to Bi-LSTM to capture contextual semantic information and obtain low-level utterance features. Then, we use low-level utterance features to construct a conversational emotion interaction graph. To efficiently capture the potential dependencies between long-distance utterances, we use the dilated generalized forward push algorithm to precompute the emotional propagation between global utterances and design an emotional relation-aware operator to capture the potential semantic associations between different utterances. Furthermore, we combine early fusion and adaptive late fusion mechanisms to fuse latent dependency information between speaker relationship information and context. Finally, we obtain high-level discourse features and feed them into MLP for emotion prediction. Extensive experimental results show that ELR-GNN achieves state-of-the-art performance on the benchmark datasets IEMOCAP and MELD, with running times reduced by 52% and 35%, respectively. In addition, ELR-GNN can effectively improve the accuracy of the MERC task by capturing and fusing the latent semantic relationships between utterances.", "sections": [{"title": "1. Introduction", "content": "Multi-modal emotion recognition in conversations (MERC) has received research attention [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] due to its wide application in the fields of intelligent customer service and emotion analysis [11], human-computer interaction (HCI) [12], and security monitoring [13]. For instance, in HCI, MERC can help computers better understand the emotional state of human users, thereby enabling more intelligent interactions and improving user experience. Unlike traditional non-conversational or unimodal emotion recognition tasks [14], MERC requires identifying the speaker's genuine emotions based on textual, auditory, and visual information in the conversation utterances [15].\nThe current mainstream research methods mainly use RNN, Transformer, and GCN to model conversation context and multi-modal information in MERC. For example, DialogueRNN [16] uses a sequential approach to track conversation context and captures the most important emotional features through a memory mechanism. Although RNN-based methods can model the speaker's contextual information, they have limited memory ability for long-distance conversations, which limits the application of RNN in MERC tasks [17]. To solve the above problems, the Transformer architecture [18] is proposed to model long-distance context dependencies in MERC. For instance, CT-Net [19] builds a Single Transformer and Cross Trans- former to capture long-distance context dependencies and realize intra-module and inter-module information interaction for emotion recognition. However, methods based on Transformer architecture ignore conversational relationship information between speakers, which limits the model's emotion recognition performance [20, 21]. To tackle this limitation, many GCN methods have been proposed to model interaction information between speakers. For example, DialogueGCN [22] uses a graph structure to model conversation context and uses GCN to learn conversation graphs to achieve semantic understanding and emotional recognition of conversations. In addition, LR-GCN [23] believes that the context latent dependencies of utterences should also be considered. LR-GCN uses multi-head attention to construct multiple full association graphs to model potential conversational relationships, and then uses GCN to learn latent relationships to achieve emotion recognition. However, limited by the complexity of GCN, these methods usually adopt a fixed window size strategy and then fully connect the utterances within the window to construct a conversation graph, which significantly limits the ability to obtain long-distance contextual information. Inspired by LR-GCN, we also use GCN to model dialogue relationship information between speakers for MERC. Furthermore, long-distance context potential dependencies can provide more information for emotion classification and help reveal the genuine emotion of utterances. Therefore, how to comprehensively consider long-distance contextual dependencies while ensuring that the number of model parameters does not increase dramatically remains a challenge.\nIn this paper, we propose an Efficient Long-distance Latent Relation-aware Graph Neural Network (ELR-GNN) for multi-modal emotion recognition in conversation. Specifically, we first use ROBERTa, 3D-CNN, and openSMILE to perform pre-feature extraction of text, video, and audio features, respectively. Next, we use Bi-LSTM to capture contextual semantic information and obtain low-level utterence features. We then use low-level utterence features to construct a speaker graph. In the constructed speaker relationship graph, low-level utterence features are used as node features, while dialogue relationship information between speakers is used for edge construction. To capture the latent dependency information between long-distance contexts, we use the graph random neural network algorithm to randomly sample top-k nodes for information extraction. In addition, we combine early fusion and adaptive late fusion mechanisms to simultaneously fuse speaker relationship information and latent dependency information between contexts. Finally, we fine-grained obtained high-level utterance features and fed them into the MLP and softmax function for emotion prediction.\n\u2022 We propose a novel Efficient Long-distance Latent Relation-aware Graph Neural Network (ELR-GNN) for MER. ELR-GCN not only considers conversational relationship information between speakers, but also captures long-distance context latent dependency information."}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Multi-modal Emotion Recognition", "content": "Single-modality emotion recognition may be limited. For example, text-based emotion recognition alone may not capture the emotional cues in speech and facial expressions [24]. Multimodal emotion recognition (MER) can integrate multiple information sources to improve the accuracy and robustness of emotion recognition.\nCurrent mainstream MER research mainly focuses on RNN, Transformer and GCN. For instance, DialogueRNN [16] modeled individual speakers and uses three different GRUs to achieve more effective correlations between speakers. DialogueGCN [22] was proposed to solve the problem that RNN-based methods cannot consider the dialogue relationship between speakers. DialogueGCN improves the performance of MER by modeling the interactive relationship between speakers through the inherent properties of the graph structure and using graph convolution operations to transfer contextual semantic information. TL-ERC [25] used transfer learning methods to solve problems in supervised learning that require large amounts of high-quality annotated data. CTNet [19] proposed a multi-modal learning framework, which achieves cross-modal contextual semantic information interaction by building a single Transformer and a cross Transformer.\nHowever, current mainstream methods only consider contextual semantic information, latent dependencies of local utterances, and conversational relationships between speakers, and their focus is on exploring the semantic information between utterances and the correlation between speakers. The above approach ignores latent dependencies of the global context, which limits the performance of MER."}, {"title": "2.2. Scalable Graph Neural Network", "content": "The current mainstream scalable GNN methods include three types of methods: 1) Node sampling strategy: Accelerate the aggregation process of node features by sampling nodes. The representative methods include GraphSAGE [26], FastGCN [27] and LADIES [28]. The main idea of GraphSage is to update the representation of each node through multiple rounds of neighbor sampling and aggregation of neighbor node information, thereby capturing the structure and relationships between nodes in graph data. FastGCN proposes a structured graph node sampling strategy, which selects sampling nodes by considering the structural information of the graph to retain important structural features of the graph. This sampling strategy can preserve the graph information as much as possible while ensuring sampling efficiency. LADIES adopts an adaptive density modeling method to capture local and global information by learning the density distribution of neighbors around a node. LADIES can effectively update the representation of nodes to one that takes into account both local and global information. 2) Graph partitioning method: Divide the original large graph into several small subgraphs and run GNN on the subgraphs. The mainstream graph partitioning methods include Cluster-GCN [29] and GraphSAINT [30]. The mainstream graph partitioning methods include Cluster-GCN and GraphSAINT. Cluster-GCN divides the original large-scale graph data into multiple subgraphs, each subgraph contains a part of nodes and corresponding edges, thereby reducing computational and memory overhead. GraphSAINT processes large-scale graph data through graph sampling and iterative coarsening. 3) Matrix approximation method: Accelerate feature propagation by decoupling feature propagation and nonlinear transformation. SGC simplifies the nonlinear activation function in traditional graph convolutional networks, retaining only graph convolution operations."}, {"title": "2.3. Multi-head Attention", "content": "The multi-head attention in MER can help the model effectively capture the correlation information between different modalities and adaptively focus on the most important parts for the emotion classification task. For example, TEMMA [31] proposesd a multi-modal multi-head attention for MER to comprehensively consider the complementarity and redundancy between modalities. TE-MMA can realize the semantic information interaction between modalities and capture the temporal dependence within the modalities. GA2MIF [32] constructed a multi-head directed graph attention network and a multi-head pairwise cross-modal attention network respectively to achieve contextual semantic information extraction and cross-modal information fusion. EEANet [33] used a multi-head self-attention mechanism to capture the discriminative features in contextual semantic information that are most suitable for emotion classification.\nWe apply an attention mechanism to the utterance features obtained through graph convolution operations to calculate the correlation between contexts and capture the utterances with the strongest emotional features among the global context latent dependencies. Our method ELR-GNN can simultaneously consider contextual semantic information, interaction information between speakers, and latent dependency information of the global context."}, {"title": "3. Proposed Method", "content": "The overall framework of the ELR-GNN proposed in this paper is shown in Fig. 1. ELR-GNN consists of four stages, including sequential contextual feature extraction, graph construction, long-distance contextual latent relationship exploration, and information fusion. In the following subsections, we describe these four key parts in detail."}, {"title": "3.1. Sequential context information extraction", "content": "The speaker's emotional state is not only related to the textual semantic information at the current moment, but also related to the previous contextual semantic information. Therefore, we use Bi-LSTM to capture contextual semantic information in multi-modal features to more accurately understand the speaker's emotional changes. The formula of LSTM is defined as follows:\n$C_t = r_t \\odot C_{t-1} + i_t \\odot \\tilde{C}_t$\n$h_t = o_t \\odot tanh(C_t)$ (1)\nwhere $u$ represents the concatenated multi-modal features, $h_t$ represents the hidden layer state, $r_t$ represents the input gate, $i_t$ represents the forgetting gate, $C_t$ represents the cell state, and $\\odot$ represents Hadamard product, $W$ is a learnable network parameter.\nBi-LSTM is composed of forward and reverse LSTM, and its formula is defined as follows:\n$\\overrightarrow{h_t}, \\overleftarrow{h_t} $ (2)\nwhere $h_t$ is obtained by concatenating the contextual semantic features extracted by forward and reverse LSTM."}, {"title": "3.2. Graph Construction", "content": "We use the inherent properties of the graph structure to construct a speaker relationship graph, in which the contextual semantic features extracted through Bi-LSTM are used as node features of the graph, and the dialogue relationships between speakers are used as edges. Specifically, given a speaker dialogue graph $G = {W,V,E, R}$, where the node $v_i$ ($v_i \\in V$) is composed of contextual semantic features (i.e., $h_t$), the edge $e_{ij} = 1$($e_{ij} \\in R$) indicates that there is a conversation relationship between node $v_i$ and node $v_j$, otherwise $e_{ij} = 0$, $W_{ij}$ ($W_{ij} \\in W$, $0 \\leq W_{ij} \\leq 1$) represents the weight of edge $e_{ij}$, and $r\\in R$ represents the edge relationship."}, {"title": "3.3. Long-distance Latent Context Relationship Extraction", "content": "Unlike previous work that set the context window size to 10 (i.e., the number of nodes), to capture long-distance latent dependencies of contexts, we adopt a larger context window to explore potential correlations between contexts. Specifically, we first construct an original graph $G$ with a larger context window, the generalized forward push algorithm is then used to calculate the propagation matrix of the row vectors, and top-k sparsification is used to further reduce the training time of the network, so as to comprehensively consider the latent correlation of the context."}, {"title": "3.3.1. Propagation Matrix", "content": "We use a mixed-order matrix of feature propagation to aggregate neighbor node information of different orders in the graph to obtain long-distance contextual latent dependency information. The formula of the propagation matrix is defined as follows:\n$\\Pi = \\sum_{n=0}^{N} w_n (D^{-1}A)^n$ (3)\nwhere $w_n \\geq 0$ and $\\sum_{n=0}^{N} w_n = 1$, $A$ is the adjacency matrix, and $D$ is the degree matrix. The propagation matrix can fuse different orders of neighbor node information and capture important contextual potential dependency information by adjusting the weights.\nThen we aggregate the node features and update the node features, defined as follows:\n$X_s = \\sum_{v \\in N} I_{s,v} \\cdot \\Pi(s,v) \\cdot h_s$ (4)\nwhere $z \\sim Bernoulli (1 \\textendash \\delta)$, $I$ is the row vectors of the node $s$, $N$ is the indices of non-zero value of $I_s$. Through Eq. 4, we can solve the problem of slow inference speed caused by the high computational complexity of GCN and achieve rapid training of the model. Therefore, we can construct larger graphs to capture long-distance context latent dependencies.\nHowever, $\\Pi$, is actually a difficult estimation problem. To address the problem, We use a two-stage estimation step for calculation, which includes Generalized Forward Push (GFP) and Top-k sparsification. First, GFP gives the error bound of $I_s$, and then Top-k sparsification only retains top-k elements to achieve faster calculation speed."}, {"title": "3.3.2. Generalized Forward Push", "content": "Since the row-normalized adjacency matrix $D^{-1}\u00c3$ is also an inverse random walk transition probability matrix on G, we design an efficient GFP estimation algorithm to estimate $I_s$. The key step of GFP is to accelerate the random walk probability diffusion process through pruning operation. Specifically, we first give two initial vectors $q^{(n)} \\in R^{|V|}$ and $r^{(n)} \\in R^{|V|}$, and both $q^{(0)}$ and $r^{(0)}$ are initialized to $e(s)$, where $e(s) = 1$ and $e(v) = 0$ for $s \\neq v$. Furthermore, $q^{(n)} = 0,r^{(n)} = 0,1 \\leq n \\leq N$. Then, the GFP algorithm begins to iteratively update the $q^{(n)}$ and $r^{(n)}$ vectors through $r^{(n)} \\leftarrow r^{(n)} + r^{(n-1)}/d_v$ and $q^{(n)} \\leftarrow r^{(n)}$ until node $v$ satisfies $r^{(n-1)} > d_v rmax$, where $d_v = D(v,v)$. When the GFP iteration is complete, we get $\\Pi \\leftarrow \\sum_{n=0}^{N} w_nq^{(n)}$."}, {"title": "3.3.3. Top-k Sparsification", "content": "To reduce the computational complexity of GCN, we perform top-k sparsification on $I_s$ to accelerate model training. The core idea of Top-k sparsification is to retain only the top-k largest elements of $\\Pi$, and set other elements to 0. Therefore, $\\Pi^{(k)}$ has only k non-zero elements, which preserves the most important emotion features in the latent dependencies of the context."}, {"title": "3.3.4. Learnable Information Propagation", "content": "Therefore, we introduce a learnable parameter $W$ to achieve dimensionality reduction of multi-modal features while improving the learning ability of the model. The formula is defined as follows:\n$X_s = \\sum_{v\\in N^{(k)}} I_v \\cdot \\Pi^{(k)}(s,v) \\cdot h_v \\cdot W$ (5)"}, {"title": "3.4. Auxiliary Information Module", "content": "Graph random neural networks can effectively extract dialogue relationship information between speakers and long-distance context potential dependency information, but it is easy to ignore some discriminative original full-emotion features. Therefore, we use AIM to extract and fuse higher-level emotional features, adaptively aggregating original emotional features, speaker relationship information, and long-distance context potential dependency information."}, {"title": "3.4.1. Feature Extractor (AIM-FE)", "content": "Multimodal data are characterized by noise and high dimensionality. To achieve denoising and capture discriminative emotional features in multi-modal data, we introduce gated convolutional networks to capture auxiliary information. In the gated convolutional network, we use sigmoid and tanh functions, which can retain the most important emotional feature information and improve the nonlinear fitting ability of the model. The formula of the gated convolutional network is defined as follows:\n$Z_c = tanh (Conv1D (h)) \\odot sigmoid (Conv1D (h))$ (6)\nwhere Conv1D represents 1D convolution operations, $\\odot$ represents Hadama product."}, {"title": "3.4.2. Late Adaptive Fusion (AIM-LAF)", "content": "To capture finer-grained semantic information in multimodal data, early and late adaptive fusion mechanisms are combined to capture auxiliary information with fine-grained emotional features. Specifically, late fusion fuses highly abstract time and space information, ignoring detailed information. Therefore, the combination of early and late adaptive fusion mechanisms proposed in this paper can more effectively capture more discriminative emotional features adaptively from multi-modal data.\nIn the early fusion process, we map the contextual features $Z_c$ through the gated convolutional network and the latent features $Z_g$ through the graph random neural network to the same dimension, obtain $z_g$ and $z_c$ and fuse them. The formula is defined as follows:\n$Z_f = \\Omega (z_g, Z_c)$ (7)\nwhere $\\Omega ()$ represents summation average operation. Then we use a FCN to achieve feature dimensionality reduction and obtain $z_g$, $z_c$, and $z_f$. Then we use the attention mechanism to obtain the corresponding attention score as follows:\n$e_g = q^T tanh(Wz+b)$ (8)\nwhere $q$ represents the query matrix, and $W$ and $b$ are the learnable parameters. Likewise, $e_c$ and $e_f$ are calculated using Eq. 8. Then we use the softmax function to normalize the attention coefficient as follows:\n$\\alpha_g = \\frac{exp(e_g)}{exp(e_g) + exp(e_c) + exp(e_f)}$ (9)\nFinally, we perform a weighted sum of $z_g$, $Z_c$ and $z_f$ to obtain the final emotional feature vector representation. The formula is defined as follows:\n$Z = \\alpha_g \\cdot Z_g + \\alpha_c \\cdot Z_c + \\alpha_f \\cdot Z_f$ (10)"}, {"title": "3.5. Model Training", "content": "The final emotional feature vector $z$ with contextual semantic information, dialogue relationship information between speakers, and long-distance latent dependency information is fed into the MLP with residual connections for feature conversion, and then use the softmax layer to get the probability of C-class emotion category:\n$Z = z + ReLU(zW_z+b_z)$ (11)\n$P = softmax(zW_z+b_z)$\nwhere $W_z$, $b_z$, $W_z$, $b_z$ is the learnable parameters. We then obtain the index of the maximum emotion probability by using the argmax function.\n$\\hat{y}(i) = argmax(P(i))$ (12)\nFinally, we use cross-entropy loss to complete the optimization of the model:\n$L=-\\sum_{i=1}^{M} \\frac{1}{L_i} \\sum_{j=1}^{L_i} \\sum_{c=1}^{C} Y_{ic}^{(j)} log_2(Y_{ic}^{(j)})$ (13)\nwhere M represents the number of dialogues, and Li represents the number of utterances in the i-th dialogue."}, {"title": "4. Experiments", "content": ""}, {"title": "4.1. Datasets", "content": "We evaluate the ELR-GNN model proposed in this paper on two benchmark datasets, IEMOCAP [34] and MELD [35]. All these data sets contain three modal data sets: text, video, and audio."}, {"title": "4.2. Baselines and Evaluation Metrics", "content": "bc-LSTM [36] performs final emotion recognition by extracting the sequential context information of the utterance, which is context-sensitive.\nText-CNN [37] uses convolution filters to extract local semantic information from utterances, which is context-independent.\nMFN [38] designs a multi-view learning mechanism to capture view-specific and cross-view semantic information, but MFN does not consider contextual information.\nCMN [39] achieves the fusion of speaker information and multi-modal features by introducing an attention mechanism.\nICON [40] uses GRU to extract contextual information of multi-modal features and uses attention layers to achieve the fusion of multi-modal semantic information.\nDialogueRNN [16] constructs three different gating units to achieve the extraction and fusion of speaker information, emotional information and global information.\nDialogueGCN [22] DialogueGCN constructs a speaker relationship graph by using contextual semantic features, and utilizes contextual semantic information and speaker relationship information to achieve emotion classification.\nConGCN [41] treats multimodal features as node features in the graph and utilizes heterogeneous graphs to model conversational relationship information between speakers.\nLR-GCN [23] captures the latent dependencies between contexts by constructing multiple graphs and constructs densely connected layers to extract speaker relationship information and structural information of the graph.\nAGHMN [42] uses BiGRU to fuse the correlation information between historical contexts and uses the attention mechanism to give higher weight to important context information.\nBIERU [43] uses emotion recurrent units and emotion feature extractors to extract contextual semantic information respectively. and refine contextual emotion feature vectors.\nEmoBERTa [44] uses ROBERTa to extract sequential contextual semantic information from text. This method does not use multi-modal data.\nLFM [45] uses low-rank decomposition to effectively reduce the dimensionality disaster problem that occurs during the fusion process of multi-modal features.\nRGAT [46] integrates position encoding information into the graph attention network to improve the model's context understanding ability.\nCOMPM [47] uses a pre-trained model to extract pre-trained context memory information and combines it with the context model to understand the global contextual emotional features in a fine-grained manner.\nCOGMEN [48] improves the representation ability of emotional feature vectors by building context GCN to extract global and local context information and fuse them.\nDER-GCN [49] improves the model's emotional representation capabilities by constructing speaker relationship graphs and event graphs.\nA-DMN [50] A-DMN comprehensively considers the intra-speaker and inter-speaker contextual information, and uses GRU to achieve cross-modal feature fusion.\nCTNet [19] realizes semantic information interaction within and between modalities by building Single Transformer and Cross Transformer."}, {"title": "4.3. Comparison with the State-of-the-Art Methods", "content": "To verify the superiority of the ELR-GNN method proposed in this paper, we report the experimental results of ELR-GNN and other comparative methods on the IEMOCAP and MELD data sets. Experimental results are presented in Tables 1 and 2.\nIEMOCAP: As shown in Table 1, the multi-modal emotion recognition method proposed in this paper achieved the best emotion recognition effect on the IEMOCAP data set, with an average accuracy of 70.6% and an average F1 value of 70.9%. ELR-GCN proposes an effective modeling method of long-distance context latent dependencies for multi-modal emotion recognition. In addition, ELR-GNN also combines early and adaptive late fusion methods to achieve the capture of fine-grained emotional features. Among other comparison methods, the emotion recognition effect of DER-GCN is slightly lower than that of ELR-GNN, with an average accuracy of 69.7% and an average F1 value of 69.4%. Although DER-GCN comprehensively considers event relationships and dialogue relationships between speakers to enhance the model's emotional understanding, it ignores latent context dependencies. The emotion recognition effect of LR-GCN is lower than ELR-GNN and DER-GCN, with an average accuracy of 68.5% and an average F1 value of 68.3%. Although LR-GCN considers latent dependencies between contexts, due to the high computational complexity of GCN, LR-GCN can only capture local latent dependencies. The emotion recognition effects of other comparison methods are lower than ELR-GNN. Likewise, none of them take into account potential dependencies on context. Overall, the accuracy of ELR-GNN on the happy emotion analogy is much higher than that of other comparison algorithms, while the accuracy of other emotion categories is also relatively close to that of other comparison algorithms. In addition, the F1 value of ELR-GNN on the happy and excited emotional analogies is much higher than that of other comparison algorithms. At the same time, the F1 value of ELR-GNN on other emotional categories is also relatively close to other comparison algorithms. The experimental results prove the superiority of the ELR-GNN method proposed in this paper.\nMELD: As shown in Table 2, The ELR-GNN method proposed in this article has the best emotion recognition effect on the MELD data set, with an average accuracy of 68.7% and an average F1 value of 69.9%. The emotion recognition effect of DER-GCN is second, with an average accuracy of 69.7% and an average F1 value of 69.4%. The emotion recognition effect of LR-GCN is lower than that of ELR-GNN and DER-GCN, with an average accuracy of 68.5% and an average F1 value of 68.3%. The emotion recognition effects of other comparison methods are relatively poor, and the average accuracy and F1 value are lower than ELR-GNN. The performance improvement may be attributed to ELR-GNN's ability to capture long-distance contextual latent dependencies and fine-grained fusion of dialogue relationships between speakers, contextual latent dependencies and contextual semantic information. Overall, the accuracy of ELR-GNN on the neutral, fear, sadness, joy, and disgust emotion analogy is much higher than that of other comparison algorithms, while the accuracy of other emotion categories is also relatively close to that of other comparison algorithms. In addition, the F1 value of ELR-GNN on the neutral, fear, sadness, joy, and anger emotional analogies is much higher than that of other comparison algorithms. At the same time, the F1 value of ELR-GNN on other emotional categories is also relatively close to other comparison algorithms. In addition, we find that ELR-GNN has better emotion recognition effects on the minority emotions fear and disgust, with relatively high accuracy and F1 value. The experimental results prove the superiority of the ELR-GNN method proposed in this paper.\nIn addition, to intuitively illustrate that the running time of the ELR-GNN method proposed in this paper is better than other comparative methods, we statistics in Table 3 the running time of other comparative methods of the ELR-GNN method on the IEMOCAP and MELD data sets. As shown in Table 3, the running time of the ELR-GNN method proposed in this paper on the IEMOCAP and MELD data sets is 41s and 91s respectively, which is significantly better than other comparison methods. The running times of DialogueGCN are 58s and 127s respectively, which are lower than LR-GCN and DER-GCN, but the emotion recognition effect is relatively poor. The running times of LR-GCN are 87s and 142s respectively. The running times of DER-GCN are 125s and 189s respectively. The experimental results prove the efficiency and effectiveness of the ELR-GNN method proposed in this paper."}, {"title": "4.4. Analysis of the Experimental Results", "content": "To intuitively understand the ability of the feeling model for each emotion category, we analyzed the emotion classification of ELR-GNN and LR-GCN on the test set. Fig. 2 shows the confusion matrix of ELR-GNN and LR-GCN for emotion classification on IEMOCAP and MELD data sets.\nOverall, on the IEMOCAP data set, ELR-GCN has a higher number of correct classifications for each emotion category than LR-GCN. On the MELD dataset, ELR-GCN has a higher number of correct classifications than LR-GCN in most emotional categories. The performance improvement may be attributed to ELR-GNN's ability to understand the semantic representation of each emotion category in a fine-grained manner.\nOn the IEMOCAP dataset, the confusion matrix shows that ELR-GNN easily misclassifies happy emotions into excited emotions. Similarly, LR-GCN also easily misclassifies happy emotions into excited emotions, and even the number of misclassifications is greater than that of ELR-GNN. We speculate that this is because the semantics of happy emotions and excited emotions are relatively similar, and the model cannot differentiate between these two types of emotions in a fine-grained manner. In addition, we also find that ELR-GNN easily misclassifies neutral emotions into frustated emotions.\nOn the MELD data set, the confusion matrix shows that ELR-GNN has a very poor classification effect on disgust and fear emotions, and can only correctly classify a few samples. This is because the number of disgust and fear emotion categories is relatively small, and the data set has a serious imbalance problem, which leads to deviations in the model's emotional understanding ability. The number of correct classifications of ELR-GNN on neutral emotions is very large, and there are very few misclassified samples. Experimental results prove that ELR-GNN has a relatively strong ability to understand neutral emotional categories."}, {"title": "4.5. Ablation Study", "content": ""}, {"title": "4.5.1. Importance of the Modalities", "content": "To verify the importance of the three modal features of text, video and audio for ELR-GNN, we conducted ablation experiments on the IEMOCAP and MELD data sets to compare the performance of the combination of different modal features. The experimental results are shown in Table 4. In single-modal experiments, ELR-GNN with text modality features has the best emotion recognition effect. The average accuracy on the IEMOCAP and MELD data sets are 64.1% and 63.5%, respectively, and the average F1 value is 63.9% and 62.4%, respectively. The emotion recognition effect of ELR-GNN with audio modal features is second, with average accuracy rates of 61.1% and 62.7% on the IEMOCAP and MELD data sets, and average F1 values of 60.8% and 62.0% respectively. ELR-GNN with video modality features has the worst emotion recognition effect, with average accuracy rates of 59.4% and 60.1% on the IEMOCAP and MELD data sets, and average F1 values of 59.7% and 61.4% respectively. Experimental results show that text features contain the most emotional semantic information. In the dual-modal experiment, ELR-GNN with text and audio modal features has the best emotion recognition effect. The average accuracy on the IEMOCAP and MELD data sets are 65.0% and 64.1%, respectively, and the average F1 values are are 64.4% and 63.2%, respectively. Experimental results demonstrate the effectiveness of multimodal features."}, {"title": "4.5.2. Parameter Analysis", "content": "We tested the impact of the maximum neighborhood size and parameter rmax in ELR-GNN on the accuracy and running time of emotion recognition. As shown in Figs. 3(a), and 3(b), we tested the impact of different neighborhood sizes and rmax on emotion recognition accuracy on the IEMOCAP and MELD datasets. Experimental results show that when r = 10-5, ELR-GNN has the best emotion recognition effect. When r = 10-4, the emotion recognition effect of ELR-GNN is second. When r = 10-3, ELR-GNN has the worst emotion recognition effect. Furthermore, as the size of the neighborhood continues to increase, the model's emotion recognition performance also improves. Experimental results demonstrate the necessity of capturing long-range latent context dependencies.\nAs shown in Figs. 3(c), and 3(d), We also calculated the impact of different neighborhood sizes on running time and emotion recognition accuracy. Experimental results show that as the neighborhood size increases, the running time of the model also increases, but it is lower than the running time of LR-GCN and DER-GCN. In addition, as the neighborhood size increases, the emotion recognition effect of the model also improves."}, {"title": "5. Conclusions", "content": "In this paper, we propose a novel Efficient Long-distance Latent Relation-aware Graph Neural Network (ELR-GNN) for multi-modal emotion recognition. Specifically, we first use ROBERTa, 3D-CNN and openSMILE to perform pre-feature extraction of text, video and audio features respectively. Next, we use Bi-LSTM to capture contextual semantic information and obtain low-level utterence features. We then use low-level utterence features to construct a speaker graph. In the constructed speaker relationship graph, low-level utterence features are used as node features, while dialogue relationship information between speakers is used for edge construction. To capture the latent dependency information between long-distance contexts, we use the graph random neural network algorithm to randomly sample top-k nodes for information extraction. In addition, we combine early fusion and adaptive late fusion mechanisms to simultaneously fuse speaker relationship information and latent dependency information between contexts. On the IEMOCAP and MELD data sets, the ELR-GNN method proposed in this paper is better than other comparative methods, and the experimental results prove the superiority of the ELR-GNN method."}, {"title": "CRediT authorship contribution statement", "content": "Yuntao Shou: Conceptualization, Methodology, Software, Data curation, Visualization, Validation, Writing - original draft, Writing review & editing. Wei"}]}