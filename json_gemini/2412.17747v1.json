{"title": "Deliberation in Latent Space via Differentiable Cache Augmentation", "authors": ["Luyang Liu", "Jonas Pfeiffer", "Jiaxing Wu", "Jun Xie", "Arthur Szlam"], "abstract": "Techniques enabling large language models (LLMs) to \u201cthink more\" by generating and attending to intermediate reasoning steps have shown promise in solving complex problems. However, the standard approaches generate sequences of discrete tokens immediately before responding, and so they can incur significant latency costs and be challenging to optimize. In this work, we demonstrate that a frozen LLM can be augmented with an offline coprocessor that operates on the model's key-value (kv) cache. This coprocessor augments the cache with a set of latent embeddings designed to improve the fidelity of subsequent decoding. We train this coprocessor using the language modeling loss from the decoder on standard pretraining data, while keeping the decoder itself frozen. This approach enables the model to learn, in an end-to-end differentiable fashion, how to distill additional computation into its kv-cache. Because the decoder remains unchanged, the coprocessor can operate offline and asynchronously, and the language model can function normally if the coprocessor is unavailable or if a given cache is deemed not to require extra computation. We show experimentally that when a cache is augmented, the decoder achieves lower perplexity on numerous subsequent tokens. Furthermore, even without any task-specific training, our experiments demonstrate that cache augmentation consistently reduces perplexity and improves performance across a range of reasoning-intensive tasks.", "sections": [{"title": "1. Introduction", "content": "Recent research (Kojima et al., 2022; Wei et al., 2022; Wu et al., 2024) has shown that enabling large language models (LLMs) to generate, or even search over, intermediate sequences of steps before producing a final answer can significantly improve performance on reasoning tasks. More broadly, providing LLMs with the ability to allocate compute adaptively during generation can lead to more effective generation within a fixed compute budget (Schuster et al., 2022). However, at a high level, many of these \u201cextra thinking\u201d approaches are similar in that their sequences of intermediate outputs are discrete, making them difficult to train in an end-to-end fashion, and in that their extra \u201cthinking\u201d (i.e. computation) is performed just-in-time, as part of the output generating process.\nIn this work, we introduce a fundamentally different approach, inspired by the literature on kv-cache compression (Ge et al., 2024; Mu et al., 2024). Our approach takes a step towards LLMs that can deliberate on their memories (encoded in the kv-cache), and distill these deliberations into a form usable for subsequent tasks. Specifically, our method processes the transformer's cache and augments it with a set of soft tokens produced in a single forward pass\u2014not sequentially. This extra processing is performed by a separate model, which we refer to as a \u201ccoprocessor\u201d, while the base transformer remains frozen. Once the kv-cache is augmented with the coprocessor's output (which we term \u201clatent embeddings\u201d), decoding proceeds as normal until the coprocessor is called again. This approach offers the following key advantages:\nEnd-to-end Differentiability: Our framework enables end-to-end backpropagation during coprocessor training, facilitating efficient optimization without the need for reinforcement learning techniques.\nWe leverage the standard language-modeling loss on pre-training data, making the method scalable.\nAsynchronous Operation: Because cache augmentation improves results many tokens beyond the augmentation point, and because the base transformer remains frozen during coprocessor training, asynchronous coprocessor operation becomes feasible. This contrasts with existing methods where additional computation occurs sequentially, and online. Our approach opens the door to models that can strategically bank computation by deliberating and refining their internal memory, independent of composing a response to a query.\nWe evaluate our method using Gemma-2 (Team-Gemma et al., 2024) models pretrained on a diverse dataset mixture. Our experiments demonstrate that without any fine-tuning on specific downstream tasks, our approach consistently improves performance across a range of reasoning- intensive tasks. We observed that increasing the number of injected latent embeddings generally leads to better performance. For example, we observe a 10.05% improvement on GSM8K and a 4.70% improvement on MMLU, when augmenting the Gemma-2 2B model with 64 latent embeddings. These results highlight the potential of enhancing LLMs with kv-cache coprocessing for augmenting model capabilities."}, {"title": "2. Methodology", "content": "We enhance a frozen LLM by training a coprocessor that inputs a key-value (kv) cache, and augments it with a set of soft tokens. This section details the architecture and training process of our approach."}, {"title": "2.1. Problem statement", "content": "Given an input x and a desired target output y, and a pretrained, frozen LLM parameterized by \u03b8, we seek to learn a coprocessor, denoted by f. This coprocessor takes the kv-cache (ke,x, U\u03b8,x) generated by the frozen LLM when processing the input x as input, and outputs a sequence of latent representations z:\n\\begin{equation}\nf(k\u0473,x, U\u0473,x) \\rightarrow z\n\\end{equation}\nThe objective of learning f is to produce latent embeddings z that, when combined with the input x, improve the frozen LLM's ability to generate the correct target y. Specifically, we aim to maximize the expected log-likelihood of the target y given the input x and the learned latent embeddings z, as predicted by the frozen LLM:\n\\begin{equation}\nmax Ex [log pe(y|x, z)]\n\\end{equation}"}, {"title": "2.2. Model architecture", "content": "Our proposed architecture enhances a frozen, pretrained LLM with a dedicated coprocessor module operating on the kv-cache. As illustrated in Figure 1, the interaction between these components unfolds in three stages:\n\u2022 KV-cache Generation: The input sequence, x, is first processed by the frozen LLM to generate its corresponding kv-cache (k\u0473,x, v\u0473,x). This cache encapsulates the LLM's internal representations of the input. Crucially, the LLM\u2019s weights remain frozen throughout the entire process.\n\u2022 Augmentation: The kv-cache is then passed to the coprocessor module, which adopts the same model architecture as the pretrained LLM. The coprocessor also receives a sequence of distinct extra soft tokens with trainable embeddings. These tokens do not correspond to actual words or sub-words but serve as abstract prompts for the coprocessor. The coprocessor ingests the kv-cache and these tokens to produce a sequence of latent embeddings, z.\n\u2022 LLM Generation with Augmented Context: Finally, z is appended to the original kv-cache. This augmented cache is then fed back into the frozen LLM, providing it with enriched contextual information derived from the coprocessor. The LLM then proceeds to generate the output sequence, y, conditioned on both the original input x and the coprocessor's output z. This allows the LLM to leverage the coprocessor's latent inferences without requiring it to explicitly verbalize intermediate steps.\nTraining focuses solely on optimizing the coprocessor and trainable embeddings' weights. The coprocessor shares the same model architecture as the pretrained LLM, and its weights are initialized with the pretrained weights of the LLM. The loss is calculated on the final output y, and backpropagation is used to update only the coprocessor's parameters. This targeted training approach allows for efficient fine-tuning without altering the pretrained LLM. In practice, the coprocessor's augmentation can potentially be performed offline and asynchronously, in parallel with the LLM\u2019s decoding process. This could enable continuous refinement of the LLM's contextual memory, leading to improved efficiency and faster response times."}, {"title": "2.3. Pretraining setup", "content": "We employ a pretraining strategy designed to encourage the coprocessor to learn augmentations that will be useful for predicting larger segments of text beyond the next token after the augmentation."}, {"title": "3. Experiments", "content": "We validate our approach using the frozen Gemma-2 2B model. Our augmented Gemma-2 models, with only the coprocessor being trained and the decoder-only LLM kept frozen, are trained on the same 2 trillion token, primarily-English dataset used for Gemma-2 pretraining (Team-Gemma et al., 2024), following the setup described in Section 2.2. This dataset includes a variety of sources, such as web documents, code, and scientific articles. We trained the model for 100,000 steps using a batch size of 1024, packed sequences of length 2048, 16 ahead tokens (NA), and 128 randomly sampled augmentation positions (\u201ctraces\u201d) for all training experiments. Importantly, no task-specific training is performed for any of the experiments; all training is done on the pretraining dataset."}, {"title": "3.1. Perplexity Evaluation", "content": "Our augmented Gemma model is able to achieve lower perplexity on the validation dataset compared to the pretrained Gemma model on many tokens ahead, even beyond the ahead token NA we defined during training. We evaluate this using a proprietary validation dataset (Same as the one used in\nGemma (Team-Gemma et al., 2024)) and evaluate the effect of augmenting the frozen Gemma-2 2B LLM's kv-cache on future token prediction. For each sequence in the validation set, we generate N\u2081 latent embeddings after each token using our coprocessor. These embeddings are then used to augment the cache at each token position. We then measure the model's ability to predict the n-th future token. Specifically, the \"1st token\" perplexity measures the model's performance predicting the token immediately following the inserted latent embeddings. The \"32nd token\" perplexity measures the model's performance predicting the token 32 positions ahead, given the context preceding the latent embeddings, the embeddings themselves, and the following 31 tokens. This demonstrates that even though we train with NA = 16, the benefits of cache augmentation extend beyond this range, improving predictions even at position 32. Figure 3 presents perplexity curves during training for the baseline frozen Gemma-2 2B model and our augmented models using N\u2081=8, 16, 32, and 64 latent embeddings. Across all latent sizes, our approach consistently reduces perplexity, with the improvement scaling with the number of latent embeddings. This demonstrates that augmenting the cache with the coprocessor improves both short-range and longer-range prediction accuracy."}, {"title": "3.2. Public Benchmark Evaluation", "content": "We evaluated cache augmentation on a range of public benchmarks spanning natural language understanding and reasoning tasks (Table 2). In this setting, we only call the coprocessor once, at the end of the prompt. Our method consistently improves performance compared to the baseline frozen Gemma-2 2B model, with particularly substantial gains on reasoning-intensive benchmarks. Several tasks, including MMLU, GSM8K, TriviaQA, NQ, and MATH, exhibit a strong correlation between the number of latent embeddings and performance improvement. For example, on GSM8K, accuracy steadily climbs from a +1.29% gain with 4 latent embeddings to a notable +10.05% with 64. Similarly, MATH improves from -0.12% with 4 to +2.06% with 64, and MMLU shows a jump from +0.45% with 4 to +4.70% with 64. This trend suggests that for certain challenging reasoning tasks, providing more latent embeddings allows the model to perform more extensive \u201cthinking\u201d in the latent space, significantly enhancing its reasoning capabilities.\nOther reasoning tasks, including ARC-e/c, Winogrande, and Boolq, also show improvements with increasing latent counts. While some tasks, such as AGIEval, BBH, and HumanEval, show less pronounced improvements or occasional performance dips with higher latent embedding counts, our method still frequently provides a benefit. This broad improvement across diverse benchmarks underscores the effectiveness and general applicability of cache augmentation for enhancing frozen language models."}, {"title": "3.3. Comparison with other baselines and variations", "content": "We compare our approach with a closely related baseline: the Pause Token method (Goyal et al., 2023). Pause Token introduces trainable embeddings inserted between the input (x) and output (y) sequences, encouraging the LLM to perform latent \"thinking\" before generating the output. The crucial distinction between our approach and Pause Token lies in how these latent embeddings are generated. While Pause Token utilizes fixed and pretrained embeddings that do not condition on the input x, our method employs a coprocessor that generates context-dependent, dynamic embeddings based on the input. This allows our approach to tailor the latent representations to the specific input, potentially leading to more effective reasoning."}, {"title": "3.3.1. Pause Token", "content": "Table 3 directly compares the performance of the baseline Gemma-2 2B model against both Pause Token and our approach, with the latter two using 32 embeddings. Notably, the Pause Token model was trained using the same training data and under the same experimental setup as our method. On the validation set, our method achieves a perplexity of 10.60 on the first token prediction, significantly lower than both the baseline (10.96) and Pause Token (11.63). Furthermore, our method achieves an accuracy of 26.76% on the GSM8K dataset, outperforming both the baseline (21.38%) and Pause Token (22.37%). These improvements underscore the effectiveness of our dynamic, contextually- informed embeddings, which provide a richer representation compared to the fixed embeddings in\nPause Token, leading to better next token prediction and improved performance on reasoning tasks."}, {"title": "3.3.2. Zero-shot CoT", "content": "Our technique can be viewed as a form of latent Chain-of-Thought (CoT) prompting. Therefore, we compare our approach to standard zero-shot CoT (Kojima et al., 2022), which involves appending \u201cLet's think step by step\u201d to the input prompt. While zero-shot CoT can be effective, it relies on the LLM to generate intermediate reasoning steps token by token, which can be computationally expensive during inference. Our method, on the other hand, generates latent embeddings in a single forward pass, potentially offering a more efficient approach to guiding reasoning.\nTable 4 presents the accuracy on GSM8K for the baseline Gemma-2 2B model, zero-shot CoT, and our approach with 16 and 32 latent embeddings. Our method shows clear improvements. With 16 latent embeddings, we achieve an accuracy of 24.72%, surpassing both the baseline (21.38%) and zero-shot CoT (23.20%). Performance further improves to 26.76% with 32 embeddings. This suggests that our learned, context-dependent latent embeddings provide a more efficient and effective mechanism for guiding reasoning compared to the generic prompt and sequential token generation of zero-shot CoT."}, {"title": "3.3.3. Alternative Coprocessor Configurations", "content": "We explored alternative configurations for our coprocessor to assess the importance of design choices. Our default setup involves finetuning the pretrained LLM to serve as the coprocessor (i.e., the coprocessor's weights are initialized with the pretrained weights of the LLM), which serves as the primary comparison point for the following experiments.\nTraining the Coprocessor from scratch: We also investigated training the coprocessor from scratch- randomly initializing its weights rather than finetuning from the pretrained weights of Gemma-2 2B. While training from scratch improves performance on all downstream tasks compared to the baseline, finetuning from pretrained weights yields even better results. This suggests that the coprocessor benefits from the foundational knowledge encoded in the pretrained LLM. Figure 4 illustrates this improvement in GSM8K accuracy as the number of latent embeddings increases, with the finetuned model consistently outperforming the model trained from scratch."}, {"title": "LORA Finetuning the pretrained LLM as the Coprocessor", "content": "In addition to full finetuning the pretrained LLM and from-scratch training, we explored the efficacy of Low-Rank Adaptation (LoRA) (Hu et al., 2021) for tuning the coprocessor from the pretrained LLM's weights. LoRA freezes the pretrained model weights and introduces trainable rank-decomposition matrices, significantly reducing the number of trainable parameters. This approach offers substantial memory benefits, as only the relatively small LoRA weights need to be stored in addition to the base model. We experimented with LoRA using ranks of 64 and 128, comparing their performance on GSM8K to the baseline Gemma-2 2B model, the from-scratch training approach discussed above, and our fully finetuned coprocessor. As shown in Table 5, which presents results using 32 latent embeddings for all methods, LoRA finetuning achieves reasonable improvements over the baseline, demonstrating that even a parameter-efficient approach can effectively train the coprocessor for improved reasoning. Specifically, LoRA with rank 64 achieved an accuracy of 23.35%, while LoRA with rank 128 reached 24.03%. These results fall between the baseline performance (21.38%) and the performance achieved by from-scratch training (25.78%), indicating that while the LoRA-tuned coprocessor benefits from the pretrained weights, generating high-quality latent embeddings for effective reasoning appears to require more substantial parameter updates than those provided by parameter-efficient methods like LoRA. While these results are not as strong as the 26.76% achieved by full finetuning, they represent a notable improvement over the baseline and highlight the potential of LoRA for efficient training/inference of our coprocessor, especially in memory-constrained environments."}, {"title": "Augmentation using Last Layer's Activations", "content": "Instead of using the kv-cache as input to the coprocessor, we experimented with providing the last layer's activations from the frozen LLM, concatenated with the soft token embeddings. This approach, using 32 latent embeddings, yielded a perplexity of 10.81 on the validation set and an accuracy of 23.20% on the GSM8K benchmark under the same training setup. Both metrics are notably worse than those achieved with kv-cache augmentation, which resulted in a perplexity of 10.69 and a GSM8K accuracy of 26.76% (also with 32 latent embeddings). We hypothesize that the last layer's activations alone do not provide as rich a representation for the coprocessor as the information aggregated across multiple layers in the kv-cache, hindering both next-token prediction (reflected in the higher perplexity) and reasoning ability (reflected in the lower GSM8K accuracy)."}, {"title": "3.4. Impact of the number of ahead token in training", "content": "We investigated the impact of varying the number of ahead tokens\u2014the number of future tokens the model is trained to predict\u2014during coprocessor training. While larger lookahead improves perplexity on later tokens, it often leads to higher perplexity on earlier tokens. Though learning rate scaling might mitigate this, we empirically chose 16 ahead tokens for most experiments in this paper, given its strong performance on GSM8K, as shown in the Table 6."}, {"title": "4. Related Work", "content": ""}, {"title": "4.1. Chain-of-Thought Reasoning in LLMs", "content": "Limitations in eliciting complex reasoning from LLMs through standard prompting have motivated research into prompting strategies that encourage intermediate reasoning steps. Chain-of-Thought (CoT) prompting (Wei et al., 2022) significantly improved reasoning performance by prompting LLMs to \"think step by step\". Subsequent work explored zero-shot CoT (Kojima et al., 2022; Zhou et al., 2023), aggregating multiple reasoning paths (Wang et al., 2022), internalizing the intermediate reasoning steps (Deng et al., 2024), verifying generation steps (Lightman et al., 2023), and broader search spaces for reasoning trajectories, such as Tree-of-Thought (Wang and Zhou, 2024; Yao et al., 2024). Other approaches leverage reinforcement learning (RL) to optimize the reasoning process based on final answer accuracy or target text likelihood (e.g., StaR (Zelikman et al., 2022), TRICE (Hoffman et al., 2024), Quiet-STaR (Zelikman et al., 2024)). While effective, these methods are often constrained by the expressiveness of natural language and can be computationally expensive due to the sequential generation of reasoning steps, both during training and inference."}, {"title": "4.2. Latent Space Reasoning", "content": "Previous research has investigated the role of latent transformer computations in LLMs' reasoning abilities. As demonstrated in (Biran et al., 2024), a sequential latent reasoning pathway in LLMs is identified for multi-hop reasoning problems. (Shalev et al., 2024) revealed that the middle layers of\nLLMs produce highly interpretable embeddings, representing a set of potential intermediate answers for multi-hop queries. To improve LLMs' latent reasoning ability, researchers have proposed to augment LLMs with meta tokens. The Pause Token method (Goyal et al., 2023), closely related to our work, introduces trainable embeddings inserted between input and output sequences to encourage latent \u201cthinking\u201d. Similarly, a recent work (Pfau et al., 2024) also studied the circumstances under which causal transformers are able to learn to utilize intermediate dummy tokens (e.g. the filler token used in this work). Unlike these studies which employ pretrained embeddings, our work generates latent tokens dynamically based on the input. More recently, COCONUT (Hao et al., 2024) introduced a new reasoning paradigm by utilizing LLMs' hidden states as input embeddings in latent space. In contrast to COCONUT, which requires multi-stage training to internalize reasoning, our approach only trains the coprocessor, thereby avoiding limitations on broader applicability."}, {"title": "4.3. KV-Cache Compression", "content": "KV-cache compression is a technique used to reduce the size of the transformer's kv-cache, the memory storing past activations, for efficient storage and faster computation. Ge et al. (2024) propose an in-context autoencoder (ICAE) to compress the context into concise embeddings for the kv-cache. Mu et al. (2024) introduce the concept of \"gist tokens\" to learn compressed representations of the kv-cache. Alternatively, prior work (Li et al., 2023) also improves the efficiency of LLMs by identifying and removing redundant information from the input, making it more compact and easier to process.\nWhile our approach also leverages the kv-cache, our motivation is fundamentally different. Rather than focusing on compression, we aim to augment the kv-cache with latent embeddings produced by an offline coprocessor, thereby enhancing the transformer's reasoning capabilities without modifying its architecture. This allows us to improve the fidelity of further decoding and boost performance on reasoning-intensive tasks. Our work is inspired by the idea of deliberation, where the coprocessor can \"think\" in the latent space by processing the kv-cache and generating meaningful embeddings that guide the transformer's subsequent generation."}, {"title": "4.4. Augmenting LLMs with External Modules", "content": "Extensive research has focused on augmenting pretrained LLMs with external modules for improved efficiency and performance (Pfeiffer et al., 2023). Parameter-efficient fine-tuning methods like prompt tuning (Lester et al., 2021), prefix tuning (Li and Liang, 2021), and adapters have also been explored. Adapters, first introduced for computer vision by Rebuffi et al. (2017, 2018) and later popularized in NLP by Houlsby et al. (2019), insert small, trainable modules into the LLM. LoRA (Hu et al., 2021) further improves adapter efficiency by decomposing weight updates into low-rank matrices. Furthermore, multimodal models like Flamingo (Alayrac et al., 2022), CoCa (Yu et al., 2022), PaLI (Chen et al., 2022), and PaLM-E (Driess et al., 2023) leverage cross-attention or soft prompts to incorporate information from other modalities. Building upon these techniques, recent work has explored augmenting LLMs with modules specifically designed for reasoning. For example, CALM (Bansal et al., 2024) employs cross-attention between specialized and general models to enhance the general model's capabilities."}, {"title": "4.5. Hypernetworks for Parameter Generation", "content": "Our work shares conceptual similarities with the concept of hypernetworks, where a separate network (the hypernetwork) generates the parameters of another network. In the context of LLM augmentation, rather than learning a fixed set of parameters for a module, a hypernetwork could generate these parameters conditioned on embeddings representing different tasks, inputs, or contexts (Ha et al.,2017; Platanios et al., 2018). This allows for a form of parameter sharing and \"entanglement\" between modules that are otherwise disjoint in their parameters (Goyal et al., 2021). Hypernetworks have been used to condition parameter generation on inputs as well. Examples include conditional batch normalization (de Vries et al., 2017), feature-wise linear modulation (FiLM) for text-and-vision tasks (Perez et al., 2018), and self-modulation in GANs (Chen et al., 2019). Bertinetto et al. (2016) even conditioned parameter generation on individual examples for one-shot learning. In the context of LLMs, hypernetworks have generated diverse module parameters, such as classifier heads (Ponti et al., 2021), continuous prompts (He et al., 2022), and adapter layers (Ansell et al., 2021; Mahabadi et al., 2021; \u00dcst\u00fcn et al., 2020), conditioned on task or language embeddings. These embeddings can be learned or fixed, incorporating side information about task or language relationships.\nImportantly, our approach can be viewed through the lens of hypernetworks, with the coprocessor itself acting as a hypernetwork that is conditioned on the kv-cache of the frozen LLM. Instead of generating parameters for a separate module, the coprocessor generates latent embeddings that augment the kv-cache. However, the core principle remains similar: a network dynamically generating outputs based on a rich contextual input. In our case, the kv-cache serves as a highly informative representation of the input sequence, allowing the coprocessor (hypernetwork) to generate augmentations tailored to the specific context."}, {"title": "5. Conclusion", "content": "This paper introduces differentiable cache augmentation, a novel method for enhancing frozen decoder- only language models by incorporating a learned coprocessor that operates on the model's kv-cache. This coprocessor generates latent embeddings that enrich the context provided to the LLM, improving its ability to reason and predict future tokens without requiring any modifications to the original model architecture. Our experiments demonstrate that this approach consistently reduces perplexity and significantly improves performance on a variety of reasoning-intensive tasks, even in zero/few-shot settings. These improvements are particularly notable on tasks requiring complex reasoning, such as MMLU and GSM8K. Importantly, because the coprocessor operates offline and asynchronously, it opens up exciting possibilities for future research into models that can perform more deliberate and computationally intensive reasoning processes, including deliberation not necessarily conditioned on a responding to a particular prompt. Future work will explore scaling the coprocessor to larger models or using many modular coprocessors, investigating different coprocessor architectures, and applying this method to more diverse downstream tasks."}, {"title": "A. Appendix", "content": ""}, {"title": "A.1. Scaling with Training Data", "content": "The scaling of performance with increasing training data is a crucial aspect of evaluating the effectiveness of our approach. Figure 5 demonstrates the impact of training duration on both GSM8K accuracy and validation perplexity for our method. The x-axis represents the total number of training steps for the coprocessor. The baseline performance, representing the frozen Gemma-2 2B model, is shown for reference at corresponding intervals along this axis. As shown, we observe a clear trend of improved performance for our method with increased training data (i.e., more tokens seen during training of the coprocessor). Specifically, our method (\"Ours\") demonstrates a clear benefit from increased training exposure, with GSM8K accuracy exhibiting a consistent upward trend and validation perplexity showing a decreasing trend. This indicates that the coprocessor learns to generate more useful latent embeddings and better integrate with the frozen LLM as it is exposed to more data, improving next token prediction. This trend highlights the importance of scaling with training data for our approach."}, {"title": "A.2. Adaptation to Downstream Tasks", "content": "All experiments described thus far have focused on training the coprocessor using the pretraining dataset. To assess the adaptability of our approach to downstream tasks, we conducted experiments using a data mixture containing the training sets of the GSM8K and MATH (Hendrycks et al., 2021) datasets. We employed LoRA finetuning (with a rank of 128) on both the baseline model and our augmented model. For the baseline, LoRA was applied directly to the base LLM, while for our augmented model, LoRA was applied specifically to the coprocessor, leaving the base LLM frozen.\nFigure 6 presents the results of this downstream adaptation. We observe a substantial improvement in performance for our augmented model compared to the baseline after LoRA finetuning. This improvement is likely attributable to the strong regularization imposed by keeping the base LLM frozen during coprocessor training. This freezing prevents overfitting to the relatively small downstream datasets, allowing the coprocessor to effectively learn task-specific reasoning patterns without disrupting the general knowledge encoded in the pretrained LLM. The baseline model, with LORA applied directly to the LLM, likely suffers from overfitting to the downstream data, limiting its performance gains. These results demonstrate the effectiveness of our approach in adapting to\ndownstream tasks while maintaining the robustness of the pretrained LLM."}, {"title": "A.3. Training Coprocessor from Scratch", "content": "We observed performance gains across most benchmarks when training the coprocessor from scratch (with randomly initialized weights), but finetuning from the pretrained LLM consistently yielded better results."}]}