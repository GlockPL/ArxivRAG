{"title": "Fairness-aware Multiobjective Evolutionary Learning", "authors": ["Qingquan Zhang", "Jialin Liu", "Xin Yao"], "abstract": "Multiobjective evolutionary learning (MOEL) has demonstrated its advantages of training fairer machine learning models considering a predefined set of conflicting objectives, including accuracy and different fairness measures. Recent works propose to construct a representative subset of fairness measures as optimisation objectives of MOEL throughout model training. However, the determination of a representative measure set relies on dataset, prior knowledge and requires substantial computational costs. What's more, those representative measures may differ across different model training processes. Instead of using a static predefined set determined before model training, this paper proposes to dynamically and adaptively determine a representative measure set online during model training. The dynamically determined representative set is then used as optimising objectives of the MOEL framework and can vary with time. Extensive experimental results on 12 well-known benchmark datasets demonstrate that our proposed framework achieves outstanding performance compared to state-of-the-art approaches for mitigating unfairness in terms of accuracy as well as 25 fairness measures although only a few of them were dynamically selected and used as optimisation objectives. The results indicate the importance of setting optimisation objectives dynamically during training.", "sections": [{"title": "I. INTRODUCTION", "content": "FAIRNESS is a critical concern in artificial intelli- gence [1]\u2013[4]. Over the years, at least 20 different measures to quantify (un)fairness have been proposed [5]. Different fairness measures often exhibit complex relationships among them [2]\u2013[8], such as conflicts, inconsistencies or even unknown patterns. Additionally, fairness is often conflicting with the accuracy of learning models [2]\u2013[4]. Thus, the intricate relationships among accuracy and multiple fairness measures pose great challenges in fair machine learning and fair artificial intelligence in general.\nVarious techniques have been developed to optimise fairness measures of learning models, which can be mainly divided into two categories [2]\u2013[4]. The core idea of the first category converts accuracy and multiple fairness measures into one combined objective to be optimised. One such technique is Multi-FR [9], which calculates the losses of all the measures and uses a weighted sum to update a learning model. Multiobjective evolutionary learning (MOEL) [10], as the second category, demonstrates significant advantages in training fairer machine learning models [11]\u2013[15]. In this category, a learning algorithm operates by explicitly defining a set of measures, including accuracy and multiple fairness measures, and si- multaneously optimising these measures during the training process, where each measure is viewed as an objective [2]- [4]. MOEL can generate a diverse set of learning models, with each model representing a tradeoff among different measures.\nOptimising all the fairness measures may not always be necessary. Recent studies [4], [7], [16] show that a compre- hensive set of fairness measures can be represented by a subset because of the positive correlations among some measures. Such a subset is denoted as a representative measure subset. In light of this, the work of [12] proposed to optimise a representative subset of measures [16] throughout the model training. The findings of [12] show that by optimising this carefully selected subset, improvements can be achieved across all the fairness measures, even including those that were not used as optimisation objectives during model training.\nHowever, using a static predefined measure subset [12] still has limitations for three reasons. First, prior knowledge or significant computational costs were involved in finding a subset that can comprehensively represent all the measures [7], [16]. Second, the proper representative subset may vary across different datasets. Carefully determining the representative subset of measures for specific datasets requires extra com- putational cost before model training. Third, the correlation among accuracy and multiple fairness measures is changing along with the model training stages. The optimal represen- tative subset of measures at one optimisation stage may not necessarily remain optimal at another stage.\nBecause of the aforementioned issues, instead of using a static predefined one, an adaptively online-determined repre- sentative subset that does not need any prior knowledge is more promising as the optimising objectives during model training. Novel contributions of this paper are as follows:\n1) We introduce a Fairness-aware strategy using MultiObjective Evolutionary Learning (FaMOEL) framework to optimise an objective set including accuracy and multiple fairness measures, as shown in"}, {"title": "II. BACKGROUND", "content": "This section presents an overview of fairness measures and their relationship. Then, the multiobjective evolutionary learning framework for fairer machine learning is introduced.\nNumerous measures have been proposed to evaluate (un)fairness from ethical standpoints in the context of fair- ness [2]\u2013[8]. There is no consensus on a universally agreed"}, {"title": "A. Fairness Measures in Machine Learning", "content": null}, {"title": "B. Mitigating Unfairness through Multiobjective Evolutionary Learning", "content": "Multiobjective evolutionary learning (MOEL) [10] has been proposed to optimise accuracy and multiple fairness measures for fairer machine learning [8], [11]\u2013[13], aiming to evolve a population of learning models, e.g., artificial neural nets (ANNs), by utilising multiobjective evolutionary algorithms"}, {"title": "III. FAIRNESS-AWARE MULTIOBJECTIVE EVOLUTIONARY LEARNING", "content": "Section III-A presents our proposed framework, namely FaMOEL, which dynamically and adaptively determines a rep- resentative subset of fairness measures during model training without any prior knowledge. The determined set are used as objectives of MOEL to guide the evolution of learning models. Then, an instantiation algorithm based on FaMOEL is imple- mented in Section III-B, where three enhancement strategies are designed to improve the fairness-awareness ability of our method."}, {"title": "A. Fairness-aware Multiobjective Evolutionary Learning Framework for Mitigating Unfairness", "content": "First, in our study, we formulate the task of improving the learning model's accuracy and fairness as a multi-objective learning task [8], [11]\u2013[13]\n$\\textrm{minimise}_{x \\in \\Omega} F(x) = \\{f_1(x), f_2(x), ..., f_M(x)\\},$ (1)\nwhere $x$ represents the parameters of a learning model within the decision space $\\Omega$. $F(x)$ is a set of $M$ objective functions that assess the accuracy and fairness of the model parametrised by $x$ on the given task.\nAlgorithm 1 outlines our proposed FaMOEL with six in- puts, including an initial population of models $M$, a set of model evaluation objectives $E$, a fairness-aware strategy $FA$, training data $D_{\\textrm{train}}$, validation data $D_{\\textrm{validation}}$, and a multiobjective optimiser $\\pi$. The objectives in $E$ are used to calculate optimised objective values, such as accuracy and fairness measures, based on the predictions of the models in $M$ on the validation data $D_{\\textrm{validation}}$. Fairness-aware strategy $FA$ is used to find a representative subset from the entire objectives $E$ during model training. Note that compared with the previous work [11], as clearly illustrated in Fig. 1, the core difference is the fairness-aware strategy $FA$. The training data $D_{\\textrm{train}}$ is utilised for local search strategies, such as partial training [23], [24], to update the parameters of the models in $M$."}, {"title": "B. Instantiation Algorithm based on Our Framework", "content": "To verify the effectiveness of our framework FaMOEL, an instantiation algorithm based on FaMOEL is developed and the key components of FaMOEL are introduced as follows, including the model set, evaluation objectives, fairness-aware method and multiobjective optimisation algorithm. Noted that our proposed framework allows for flexibility in selecting these components based on specific prediction tasks and preferences.\n1) Model Set: A range of machine learning (ML) models can be utilised within our framework. In our study, a collection of artificial neural networks (ANNs) with the same architecture is employed as individuals. Each ANN's weights and biases are encoded as a real-value vector and represented as an individual [12], [23].\n2) Evaluation objectives: In this study, a total of 26 fairness measures, including accuracy and Fair1 to Fair25 (as listed in Table II), are considered. The accuracy is evaluated using the cross-entropy (CE) measure commonly employed for classi- fiers [12], and it is minimised. Following [12], the absolute val- ues of Fair1-Fair6, Fair12, Fair13 and Fair15 are minimised. For Fair7-Fair11 and Fair14 using ratios, we construct the objective functions to be minimised with the transformation following the work of [12]. Taking Fair7 ($\\frac{\\textrm{FPR}(g_u)}{\\textrm{FPR}(g_p)}$) as an example, its corresponding objective function is calculated as 1-$\\textrm{min} \\{\\frac{\\textrm{FPR}(g_u)}{\\textrm{FPR}(g_p)}, \\frac{\\textrm{FPR}(g_p)}{\\textrm{FPR}(g_u)}\\}$. Fair16-Fair25 are directly used as objective values to be minimised since their values are always positive. The transformed objectives corresponding to Fair1-Fair25 are denoted as $f_1$-$f_{25}$, respectively. The optimal values of $f_1$-$f_{25}$ are all zeros."}, {"title": "3) Multiobjective Optimiser", "content": "Parent selection, survival se- lection and reproduction strategy of $\\pi$ can be implemented by any multiobjective evolutionary algorithm.\nIn our instantiation algorithm, we utilise Two_Arch2 [25] for both parent selection and survival selection. The sur- vey [26] demonstrates the efficacy of Two_Arch2 to address many-objective optimisation. Two_Arch2 is popular and effi- cient [21], [26]\u2013[28], exhibiting competitive performance in handling many-objective optimisation problems, which main- tains two archives, each focusing on convergence and diversity of individuals, respectively.\nIn the reproduction strategy, isotropic Gaussian perturbation and the variant of weight crossover are applied as mutation and crossover operators [12], respectively. A set of new $M'$ (line 8 in Algorithm 1) can be obtained by applying the mutation operator to the model set resulting from the crossover between the convergence archive and the diversity archive [25].\nSpecifically, isotropic Gaussian perturbation [12] is per- formed as the mutation operator, formulated as $r_i = r_i + \\delta \\epsilon$, where the $i$-th weight of an ANN, denoted as $r_i$, undergoes isotropic Gaussian perturbation, with $\\delta_i$ sampled from a nor- mal distribution $N(0, \\sigma^2)$. $\\sigma$ represents the mutation strength. Given parents $p$ and $q$, the weight crossover [12] is applied as $r_i^{o_1} = u_i r_i^p + (1 - u_i) r_i^q$, $r_i^{o_2} = u_i r_i^q + (1 - u_i) r_i^p$, where $u_i$ is sampled from (0,1) uniformaly at random. Meanwhile, $r_i^p$, $r_i^q$, $r_i^{o_1}$, and $r_i^{o_2}$ denote the $i$-th weight of the parent $p$, parent $q$, offspring $o_1$, and offspring $o_2$, respectively.\nRegarding the partial training [12], [23], [24], model pa- rameters are updated by the stochastic gradient descent (SGD) optimiser [29]."}, {"title": "4) Fairness-aware Strategy", "content": "The fairness-aware strategy is to adaptively construct a representative subset from all the objectives to be considered according to the current evolution status of each generation. In the literature, objective subset selection methods can be generally divided into three cate- gories [30]: dominance-based, model-based and correlation- based methods. Dominance-based methods [31], [32] aim to identify representative objectives that preserve the dominance structure as much as possible. In contrast, model-based meth- ods [30], [33] build a model to approximate the obtained non-dominated front and select representative objectives based on the model's coefficients. However, both dominance-based and model-based methods are less effective for problems with many objectives (up to 15) [30]. Dominance-based methods struggle to maintain the informative dominance structure when there is a high proportion of non-dominated solutions [32]. The models constructed by model-based methods often lack accu- racy due to the relatively limited number of non-dominated solutions in a highly approximated space with many objec- tives [30]. In contrast, correlation-based methods [17], [34] select representative objectives by leveraging the correlation relationships among objectives. Among these, ORNCIEE [17] has proven to be effective even in problems with up to 50 objectives. Therefore, we have chosen ORNCIE as the fairness-aware strategy in our study.\nInspired by ORNCIE [17], we propose our fairness-aware strategy, shown in Algorithm 2, by adding three novel en- hancement strategies based on ORNCIE. ORNCIE calculates a modified nonlinear correlation information entropy (mN- CIE) [17] to analyse the interrelationships among objectives according to the current population information and subse- quently identify a representative subset. The obtained subset is directly optimised by a multiobjective optimiser, such as Two_Arch2 [25]."}, {"title": "IV. EXPERIMENTAL STUDIES", "content": "In this section, Section IV-A introduces the aims of our experimental studies. Then, Section IV-B presents the experi- mental settings. Four experiments are presented and discussed in Section IV-C to Section IV-G, respectively."}, {"title": "A. Overview of Experimental Studies", "content": "Four experiments are conducted to achieve a comprehensive analysis of our proposed fairness-aware multiobjective evo- lutionary learning framework and its instantiation algorithm. Experimental setting is detailed in Section IV-B. First, we verify the effectiveness of our fairness-aware framework in Section IV-C by comparing it with two state-of-the-art meth- ods in multi-objective optimisation for fair machine learning: one that optimises the entire set of objectives [11] and another"}, {"title": "B. Experimental Setting", "content": "1) Compared Methods: A total of 26 objectives, includ- ing accuracy CE and 25 fairness measures ($f_1$-$f_{25}$, de- scribed in Table II), are considered for all methods. Our proposed method, denoted as FaMOEL, is described in Algorithm 1 and Algorithm 2. We compare FaMOEL with two state-of-the-art algorithms, namely MOEL [11] and MOELRep [12], respectively. MOEL directly optimises 26 objectives through the multiobjective evolutionary learning framework. MOELRep focuses on optimising a static rep- resentative subset. The work of [7] clusters $f_1$-$f_{25}$ into six groups. The static representative subset can be identified by selecting a measure from each group. Our study selects $f_4$, $f_7$, $f_{10}$, $f_{16}$, $f_{17}$ and $f_{25}$ as the representative fairness measure subset. Thus, MOELRep optimises CE and these six fairness objectives.\n2) Datasets: 12 well-known benchmark datasets widely used in the literature of fairness [7], [35] are considered, namely Heart health [36], Titanic [37], German [38], Student performance [39], COMPAS [40], Bank [41], Adult [42], Drug [43], Patient [44], LSAT [45], Default [46] and Dutch [18]. Table III summarises these datasets used. Note that the analysis of the static representative objective subset for MOELRep is based on the first seven datasets [7], while the remaining five datasets are not included. This can help to further verify the effectiveness of our framework on the new benchmark datasets. The pre-processing steps for Heart health, Titanic, German, Student performance, COMPAS, Bank, and Adult datasets follow the same procedure as described in [7]."}, {"title": "3) Parameter Settings", "content": "All the methods, including FaMOEL, MOELRep and MOEL, use the same settings for a fair comparison, introduced as follows. Each individual in the population is designed with a fully connected architecture, consisting of one hidden layer [11], [12]. The values of the learning rate, mutation strength and the number of hidden nodes are determined using grid search and are summarised in Table IV. The multiobjective optimiser $\\pi$ is Two_Arch2 [25]. The size of the convergence and diversity archives are all set as 100. We set the termination condition to a maximum of 100 generations. In our fairness-aware method FaMOEL, the selection threshold $\\tau$ is set to 0.22. Five-fold cross-validation is used in our experiments, where 10 trials are independently run for each fold. Thus, 50 trials in total are performed for each benchmark dataset."}, {"title": "4) Performance Measures", "content": "The quality of a model set can be assessed from four aspects, including convergence, spread, uniformity, and cardinality [22]. For a more compre- hensive analysis, we adopt widely used generational distance (GD) [47] for convergence, pure diversity (PD) [48] for spread, and spacing (SP) [49] for uniformity, as suggested in [22], [50], and summarise them in Table V. PD and SP can collectively depict the diversity of a solution set. A solution set with diverse performance not only provides decision-makers with a better understanding of the task at hand but also offers flexibility, allowing them to choose the most suitable ones according to varying requirements. Moreover, hypervolume (HV) [51], also known as the only indicator with Pareto compliance [52], is applied to measure the overall performance. All the objective values are normalised before computing HV values The reference point (1.2, 1.2, ..., 1.2) is used for HV. A larger PD or HV value indicates better performance with respect to its corresponding property, while a smaller GD or SP value indicates better performance."}, {"title": "C. Effectiveness of Our Framework", "content": "To verify the effectiveness of our fairness-aware MOEL framework, three perspectives are considered, (i) convergence"}, {"title": "D. Comparison with Optimising Frequently Selected objectives", "content": "As depicted in the last column of Fig. 3, the selection frequencies of different objectives among CE and $f_1$-$f_{25}$ vary across different datasets. It's intriguing to analyse the comparison between FaMOEL and the method optimising only the frequently selected, denoted as MOEL Rep. In this study, we specifically compare the performance of FaMOEL with that of MOEL Rep, where MOELRep optimises differ- ent objectives while maintaining the same settings for the remaining factors as in MOEL Rep. For each dataset, the"}, {"title": "E. Effectiveness of Our Fairness-aware Strategy", "content": "Our fairness-aware enhancement strategy, as described in Section III-B4, is designed to improve the work [17]. In order to evaluate the effectiveness of our enhancement, we compare our approach FaMOEL with the method that utilises the original strategy proposed [17], denoted as FaMOEL . The difference between FaMOEL and FaMOEL was discussed in Section III-B4. FaMOEL is implemented with the same settings as ones of FaMOEL except for the fairness-aware strategy."}, {"title": "F. Computational Cost Analysis", "content": "To analyse the efficiency of FaMOEL, we report the average runtime of MOEL, MOEL Rep, and FaMOEL in Fig. 7. Overall, FaMOEL have a similar computation runtime to MOEL and MOEL Rep, as indicated in Fig. 7. Nonetheless, it's worth noting that MOEL Rep which involves two issues relies on a set of pre-defined fairness measures. First, determining suitable measures requires considerable computational cost as different algorithms should be run across various datasets to identify the correlation among the measures. Secondly, those pre-defined measures may show different correlation on a new dataset. Therefore, the results demonstrate the effectiveness of our framework."}, {"title": "G. Parameter Sensitivity Analysis", "content": "In this study, we aim to analyse the sensitivity of the unique hyperparameter in our algorithms and recommend a value. The parameter $\\tau$ is introduced in our enhanced fairness-aware strategy and serves as a selection threshold for determining the objective set $D_{el}$ to be removed. Each objective in $D_{el}$ is viewed to be highly positively correlated"}]}