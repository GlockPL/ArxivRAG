{"title": "Identifying and Clustering Counter Relationships of Team Compositions in PvP Games for Efficient Balance Analysis", "authors": ["Chiu-Chou Lin", "Yu-Wei Shih", "Kuei-Ting Kuo", "Yu-Cheng Chen", "Chien-Hua Chen", "Wei-Chen Chiu", "I-Chen Wu"], "abstract": "How can balance be quantified in game settings? This question is crucial for game designers, especially in player-versus-player (PvP) games, where analyzing the strength relations among predefined team compositions\u2014such as hero combinations in multiplayer online battle arena (MOBA) games or decks in card games\u2014is essential for enhancing gameplay and achieving balance. We have developed two advanced measures that extend beyond the simplistic win rate to quantify balance in zero-sum competitive scenarios. These measures are derived from win value estimations, which employ strength rating approximations via the Bradley-Terry model and counter relationship approximations via vector quantization, significantly reducing the computational complexity associated with traditional win value estimations. Throughout the learning process of these models, we identify useful categories of compositions and pinpoint their counter relationships, aligning with the experiences of human players without requiring specific game knowledge. Our methodology hinges on a simple technique to enhance codebook utilization in discrete representation with a deterministic vector quantization process for an extremely small state space. Our framework has been validated in popular online games, including Age of Empires II, Hearthstone, Brawl Stars, and League of Legends. The accuracy of the observed strength relations in these games is comparable to traditional pairwise win value predictions, while also offering a more manageable complexity for analysis. Ultimately, our findings contribute to a deeper understanding of PvP game dynamics and present a methodology that significantly improves game balance evaluation and design.", "sections": [{"title": "1 Introduction", "content": "In the dynamic landscape of player-versus-player (PvP) games, team compositions, or \"comps,\" such as hero combinations or decks formed before matches commence, are pivotal (Costa et al., 2019; de Mesentier Silva et al., 2019; Reis et al., 2021). The gaming industry, now approximately a 200 billion US dollar market (Kristianto, 2023), thrives on the diversity and engagement offered by these compositions, reflecting players' individuality and sustaining market competitiveness (Figueira et al., 2018; Fontaine et al., 2019). However,\n\nthe key to optimizing player engagement and competitive fairness lies in maintaining reasonable strength relations among diverse team compositions a challenge for both players aiming for victory and designers striving for balance (Levkoff, 2014; Bakkes et al., 2014; Beyer et al., 2016).\n\nA quantitative measure for game balancing is thus essential for addressing this challenge. Currently, win or success rate, use rate, or even the entropy of strategy distributions are available measures across various game genres, targeting optimizations from detailed game parameters to skill-based matchmaking among players (Morosan & Poli, 2017; Hunicke, 2005; Rupp et al., 2023; Nikolakaki et al., 2020; Pendurkar et al., 2023). However, the prevailing reliance on these measures for balance assessment overlooks critical factors such as player skill variability and the counter relationships between compositions, rendering evaluations imprecise. Traditional player skill ratings, including Elo rating, TrueSkill, and Matchmaking Rating, predominantly focus on individual prowess, leaving a gap in the strength assessment of team compositions (Elo, 1966; Herbrich et al., 2006; Pramono et al., 2018).\n\nTo better understand strength relations in compositions and analyze game balance, we pose the question: \"How many compositions are not dominated?\" If a composition shows no advantage over others, it could be considered redundant. Hence, our goal in this paper is to define measures that answer this question. We first integrate the Bradley-Terry model with Siamese neural networks (Bromley et al., 1993) to predict the strengths of team compositions from game outcomes under the competitive scenario (Bradley & Terry, 1952; Li et al., 2021). This scalar strength rating helps us identify the strongest or dominating composition more effectively with the numerical max operation compared to the comparison operation over all compositions. However, a single scalar strength often fails to provide precise predictions due to players potentially altering their playstyle under different states (Lin et al., 2021) or the inherent intransitivity present in competitive scenarios (Chen & Joachims, 2016; Balduzzi et al., 2018). Accurate predictions often consider cyclic dominance, such as the Rock-Paper-Scissors dynamic, which the Bradley-Terry model does not capture. By analyzing discrepancies between actual outcomes and Bradley-Terry model predictions, we learn a counter table through neural discrete representation learning (van den Oord et al., 2017), thereby enhancing prediction accuracy and offering insights into counter dynamics without specific game knowledge. During the learning of counter tables, we found that vanilla vector quantization (VQ) training leads to poor codebook utilization (Zhang et al., 2023), especially in small codebook sizes; hence, we proposed a new VQ Mean Loss to improve codebook utilization for this new use case. Leveraging these methods, we define new measures of game balance for counting non-dominated compositions that simple win rates face challenges in computation due to high time complexity.\n\nOur contributions are threefold: First, we establish two measures for balance by counting the non-dominated compositions: Top-D Diversity, which counts playable compositions given a tolerant win value gap, where the tolerant gap is defined by game designers and can be due to factors like skill or luck that make players willing to play those compositions; and Top-B Balance, which considers counter relationships in counting non-dominated compositions, i.e., how many meaningful counter relationships exist in the game. Next, we introduce the learning of composition strength and counter relationships, reducing the space complexity of"}, {"title": "2 Game Balance", "content": "Game designers are tasked with devising engaging mechanisms and numerical frameworks that enhance player experiences (Schell, 2008). Developing an immersive game loop not only encourages participation but also assists players in forming a mental model of the game's mechanics (Sellers, 2017). Designers often apply the Yerkes-Dodson law to optimize player satisfaction, suggesting an optimal arousal level for peak performance that aligns in-game challenges with player skill progression (Dodson, 1915). This dynamic interaction is crucial for maintaining players in a state of mental flow (Cs\u00edkszentmih\u00e1lyi, 1990), where game balance plays a pivotal role in sustaining appropriate levels of difficulty and challenge.\n\nAs a critical research field within game design and operations (Schell, 2008; Novak et al., 2012; Sellers, 2017), game balance significantly influences player engagement through diverse strategies and playstyles. It extends beyond mere difficulty adjustments to encompass strategy, matchmaking, and game parameter tuning (Becker & G\u00f6rlich, 2020). Understanding balance definitions and metrics is vital for effectively addressing these components. Traditional metrics such as win rate, win value difference, and game scores have driven the evolution of game balancing techniques, refining the interplay between game mechanics and player satisfaction (Jaffe et al., 2012; Budijono et al., 2022; Mahlmann et al., 2012).\n\nIn PvP scenarios, win value estimation is a common approach, with values often normalized to scales like [0,1] or [-1,1] to simplify payoff calculations between competitors (Budijono et al., 2022). However, calibrating the strength of a composition with win values typically requires comparisons against multiple opponents (Fontaine et al., 2019). While strength rating systems like Elo, TrueSkill, and Matchmaking Rating can identify strength from a single scalar rating and suggest greater strength with higher ratings (Elo, 1966; Herbrich et al., 2006; Pramono et al., 2018), capturing intransitivity or cyclic dominance in scalar ratings is challenging. This necessitates multi-dimensional ratings (Chen & Joachims, 2016; Balduzzi et al., 2018), which reintroduce complexity into balance analysis. Thus, this paper aims to propose a solution that considers intransitivity while maintaining feasible complexity in balance analysis.\n\nAcquiring accurate game data is also crucial for balance analysis, often involving the deployment of rule-based agents during early development phases and integrating human testers later to capture realistic gameplay data. Advances in artificial intelligence, demonstrated by AlphaZero's performance in board games, have enabled learning-based agents to contribute to game balance data collection (Toma\u0161ev et al., 2022). Community discussions about strategies also provide valuable insights, often grounded in game theory principles or defining some empirical relationship graphs by humans to explain the game scenarios (Schmitz, 2022; Hern\u00e1ndez et al., 2020). Although the entropy of a strategy reaching Nash equilibrium can serve as a measure of strategic balance (Pendurkar et al., 2023), computing Nash equilibrium policies at the game action level in complex games is resource-intensive, posing a challenge for practical application in the game design"}, {"title": "3 Learning Rating Table and Counter Table", "content": "For understanding the strength relations between compositions (comps) and performing efficient balance analysis, we need to quantify the strength and counter relationships first. Our methodology begins with the application of the Bradley-Terry model to allocate a scalar value representing the strength of each comp based on win estimations. This process is elaborated upon in Section 3.1. To tackle the issue of cyclic dominance or intransitivity of win values efficiently, epitomized by the Rock-Paper-Scissors dynamic, we devise a counter table. This involves examining the variances between actual win outcomes from specific comps and the predictions made by the Bradley-Terry model, a process detailed in Section 3.2. Furthermore, the overarching framework that integrates these components into our learning process is delineated in Section 3.3."}, {"title": "3.1 Neural Rating Table", "content": "Win rates in PvP games, while useful as a conventional metric, do not fully encapsulate the actual strengths of individual players or team compositions. A player's or comp's true prowess is better reflected in their ability to triumph over comparable opponents, as victories against both weaker and stronger opponents contribute equally to the win rate but signify different levels of strength. The Elo rating system, commonly utilized in chess and similar two-player zero-sum games, offers a scalar strength rating for entities, aligning with the principles of the Bradley-Terry model (Elo, 1966; Bradley & Terry, 1952). This model predicts the probability of player i defeating player j, as delineated in Equation 1:\n\n$$P(i > j) = \\frac{\\gamma_i}{\\gamma_i + \\gamma_j}$$\n\nwhere $\\gamma_x$ represents the positive real-valued strength of player x. To manage the scale of $\\gamma$, it is often reparameterized using a rating value $\\lambda$ in an exponential function, as shown in Equation 2:\n\n$$P(i > j) = \\frac{e^{\\lambda_i}}{e^{\\lambda_i}+e^{\\lambda_j}}$$\n\nAdopting this model, we treat comps analogously to individual players, estimating each comp's strength to predict win probabilities. Given the impracticality of analyzing an extensive N\u00d7N win rate table for a large number of comps, we harness the Bradley-Terry model in conjunction with neural networks to overcome this challenge. Our approach employs a Siamese neural network architecture to deduce the ratings $e^\\lambda$ for each comp, utilizing mean square error (MSE) as a regression loss function D for model approximations. In our early experiments, we tried using binary cross entropy as the loss function for its probabilistic nature. However, this approach encouraged the rating values to become very large and prevented the model from converging, similar to using hinge loss. Therefore, we focused on using MSE for stable training.\n\nThis integration allows our neural network to learn the rating table $R_\\theta$ from match outcomes, assigning a rating to each comp $c$ through $R_\\theta(c)$. The ratings are computed using an exponential activation function to ensure appropriate scaling. The loss function, focused on match outcome $W_m$, is formalized as follows:\n\n$$L_R = E[D(W_m, \\frac{e^{R_\\theta(c_m^i)}}{e^{R_\\theta(c_m^i)} + e^{R_\\theta(c_m^j)}})] = E[D(W_m, \\frac{e^{R_\\theta(c_m^i)}}{e^{R_\\theta(c_m^i)} + e^{R_\\theta(c_m^j)}})]$$\n\nBy adopting this methodology, our network efficiently processes diverse comp combinations, offering a robust and scalable solution for predicting team composition strengths. We can efficiently identify the strongest composition by tracing the ratings over all compositions with a time complexity of O(N)."}, {"title": "3.2 Neural Counter Table", "content": "Within the framework of adapting the Bradley-Terry model through neural networks, we can list the strength of all N team compositions with a space complexity of O(N). However, the precision of strength relations provided by this method may not match the precision afforded by direct pairwise comparisons for each composition, a process that inherently bears a space complexity of O(N2). While theoretically feasible via neural networks, such direct prediction incurs a high space complexity, making it challenging to check these ratings and analyze balance, especially with a large N.\n\nThe phenomenon of cyclic dominance or intransitivity of win values, a common challenge in analyzing game balance, introduces further complications. An N \u00d7 N counter table, which would record adjustments from rating predictions to enhance accuracy, becomes impractical due to its high space complexity and cognitive load. In practice, players intuitively grasp counter relationships without the need for exhaustive memorization of large tables. To navigate this, we propose a more manageable M \u00d7 M counter table that serves as an approximation of the full N \u00d7 N relationships, where M represents a manageable number of discrete categories. Beginning with a minimum of 3 to capture basic cyclic dominance patterns, M can be adjusted to strike a balance between prediction accuracy and table interpretability.\n\nFor the task of learning discrete categories, we employ Vector Quantization (VQ), a technique of neural discrete representation learning celebrated for its effectiveness (van den Oord et al., 2017). It acts as an end-to-end analog to K-means clustering within neural networks, primarily introduced in the context of VQ-VAE (van den Oord et al., 2017; Baevski et al., 2020). Our goal diverges from traditional autoencoder objectives; rather than reconstructing inputs, our focus is on developing a counter table that learns from the residuals between direct win predictions and those derived from the Bradley-Terry model."}, {"title": "3.2.1 Neural Discrete Representation Learning", "content": "Before introducing our design for learning the counter table, we first discuss a popular discrete representation learning method with neural networks, VQ-VAE (van den Oord et al., 2017). In scenarios that require discrete representation, clustering is a common approach, with k-means clustering being a widely used method for several decades (MacQueen et al., 1967). This clustering idea is based on finding k reference points in the feature space to represent corresponding groups of actual features using the nearest neighbor method. However, obtaining an effective feature space from raw observations for this clustering process is a critical problem. With the growth of deep learning, variational autoencoder (VAE) (Kingma & Welling, 2014) was proposed to learn effective latent feature spaces for several tasks.\n\nBuilding on the ideas of autoencoders and k-means clustering, VQ-VAE uses the concept of preparing an embedding codebook and employing the nearest neighbor method to convert continuous latent features into the discrete indexes of the codebook, thereby obtaining the discrete representation. Afterward, we can restore the discrete representations back to continuous for decoding tasks. This discretization process can be formulated with the following notations: Given an observation o and its latent features $z_\\theta$ through encoder layers and an embedding space E = $e_1,\u2026\u2026,e_K$ with size K (codebook), we can define the following probability function for mapping o to discrete space:\n\n$$q(s=k_0) = \\begin{cases}1 & \\text{for } k_0 = \\text{arg} \\min_j||z(o) \u2013 e_j||^2, \\\\0 & \\text{otherwise.} \\end{cases}$$\n\nThrough this mapping, we can train a discrete encoder in an end-to-end manner without the need for feature engineering first as in k-means clustering.\n\nFor training this neural network, we use the following loss functions:\n\n$$L_{rec} = E[D(o,o')], L_{vq} = E[D(\\text{sg}[z_\\theta], z_q)], L_{commit} = E[D(z_\\theta, \\text{sg}[z_q])]$$\n\nHere, D is a distance function, with mean square error (MSE) being a common choice. $z_\\theta$ and $z_q$ are the latent codes before and after nearest neighbor replacement, respectively. The $\\text{sg}[.]$ denotes the stop-gradient operator. The standard VQ-VAE minimizes the combined loss function L = $L_{rec}+ L_{vq} + \\beta \u00d7 L_{commit}$, where $ \\beta$ is a weight term that encourages the encoder to produce latent codes closer to discrete representations.\n\nFor applying this loss to the encoder, we can use the gradient copy trick with the chain rule, as follows:\n\n$$\\bigtriangledown_{\\theta encoder} = \\frac{\\partial L_{rec}}{\\partial z_q} \u00d7 \\frac{\\partial z_q}{\\partial z_\\theta} \u00d7 \\frac{\\partial z_\\theta}{\\partial \\theta_{encoder}} + \\beta \u00d7 \\frac{\\partial L_{commit}}{\\partial \\theta_{encoder}}$$\n\nFor applying this loss to the codebook, it is treated as a simple regression optimization problem."}, {"title": "3.2.2 Applying Vector Quantization to the Counter Table", "content": "After a brief understanding of vector quantization with neural networks, we extend this idea of discrete representation to our counter table application. Given the symmetrical nature of residual win values and our aim to classify compositions into M discrete categories, we utilize Siamese network architectures for both the learning of discrete representations and the prediction of residual win values, as illustrated in Figure 2. The residual win value, $W_{res}$, is defined as:\n\n$$W_{res}(c_{m_i}, c_{m_j}|R_\\theta) = W_m \u2013 \\frac{R_\\theta(c_{m_i})}{R_\\theta(c_{m_i}) + R_\\theta(c_{m_j})}$$\n\nThe counter table, denoted as $C_\\phi$, comprises a discrete encoder $C_{\\phi e}$ and a residual win value decoder $C_{\\phi d}$, functioning as follows:\n\n$$C_\\phi(c_{m_i}, c_{m_j}) = C_{\\phi d}(C_{\\phi e}(c_{m_i}), C_{\\phi e}(c_{m_j})).$$ \n\nHere, every output of $C_{\\phi e}(x)$ belongs to $C_{ke}$, the embedding space optimized for vector quantization.\n\nThe core loss function focuses on the residual win values:\n\n$$L_{res} = E[D(W_{res}(c_{m_i}, c_{m_j}|R_\\theta), C_\\phi(c_{m_i}, c_{m_j}))]$$\n\ncomplemented by a vector quantization loss:\n\n$$L_{vq} = \\frac{E [D(z_e(c_{m_i}), z_q(c_{m_i})) + D(z_e(c_{m_j}), z_q(c_{m_j}))]}{2}$$\n\nwhere $z_e$ represents the latent code before vector quantization, and $z_q$ denotes the code post-quantization.\n\nIn our application, we require a minimal discrete state space to learn the counter table effectively. We observed low utilization of vectors within the embedding space $C_{ke}$, leading to unselected vectors during training, which cannot construct a comprehensive M \u00d7 M counter table. This low codebook utilization problem is common in VQ, and there are many techniques to improve it (van den Oord et al., 2017; Yu et al., 2022; Shin et al., 2023). Reg-VQ Zhang et al. (2023) specifically discusses this codebook utilization problem and suggests adopting KL divergence in stochastic VQ and leveraging Gumbel sampling over convolutional feature blocks. However, this raises the question of whether there is a simple and easy way to guarantee codebook utilization improvement conceptually and can be easily implemented for the vanilla VQ process for our simple usage. To handle this, we propose a new loss term for embedding vectors, termed VQ Mean Loss, which calculates the distance from the mean vector in the embedding space to the continuous latent code $z_\\theta$. This mechanism can be seen as another K-means clustering, encouraging the vectors in $C_{ke}$ to gravitate towards $z_\\theta$, thus increasing their likelihood of being selected by the nearest neighbor in subsequent iterations. For a more concrete explanation, we provide an example in Appendix A.1. We define this additional loss as:\n\n$$L_{mean} = E [\\frac{D(z_e(c_{m_i}),\\bar{e}_k) + D(z_e(c_{m_j}),\\bar{e}_k)}{2}], \\text{ where } \\bar{e}_k = \\frac{1}{M} \\sum_{e_k \\in C_{ke}} e_k.$$\n\nThe gradients for each component in the counter table learning process are calculated with the hyperparameters $\\beta_N$ and $\\beta_M$. $\\beta_N$ is utilized in VQ-VAE to ensure the continuous latent codes $z_e$ closely align with their quantized versions $z_q$, and we use $\\beta_M$ to activate $L_{mean}$, respectively:\n\n$$\\bigtriangledown_{C_{\\phi e}} = \\frac{\\partial L_{res}}{\\partial z_q} \u00d7 \\frac{\\partial z_q}{\\partial z_e} \u00d7 \\frac{\\partial z_e}{\\partial C_{\\phi e}} + \\beta_N \u00d7 \\frac{\\partial L_{vq}}{\\partial C_{ke}} + \\beta_M \u00d7 \\frac{\\partial L_{mean}}{\\partial C_{ke}}, \\\\ \\bigtriangledown_{C_{\\phi d}} = \\frac{\\partial L_{res}}{\\partial C_{\\phi d}}$$\n\nBy employing this $M \u00d7 M$ counter table, win values $W_\\theta$ that consider counter relationships can be computed via Equation 13:\n\n$$W_\\theta(c_{m_i}, c_{m_j}) = \\frac{R_\\theta(c_{m_i})}{R_\\theta(c_{m_i}) + R_\\theta(c_{m_j})} + W_{res}(c_{m_i}, c_{m_j}).$$\n\nThis approach reduces the space complexity of analyzing strength relations for N compositions from O(N2) to O(N + M\u00b2). When M is a small, constant value, the complexity can simplify further to O(N)."}, {"title": "3.3 Learning Procedure", "content": "The methodology underlying the construction of the rating and counter tables is encapsulated in the learning framework depicted in Figure 3. This framework requires a dataset consisting of match results, including the team compositions of the competing sides alongside the ultimate win-lose outcomes. The representation of these compositions is adaptable, ranging from simple binary encodings to more nuanced feature descriptions, according to the preferences and requirements set by game designers.\n\nCrucially, the derivation of the Neural Counter Table $C_\\phi$ is predicated on the prior establishment of the Neural Rating Table $R_\\theta$. This sequential approach ensures that the foundational ratings of team compositions are accurately determined before their interrelations and counter dynamics are analyzed. The development and refinement of these tables pave the way for the introduction of novel measures aimed at enhancing diversity and balance within the gaming environment. A comprehensive discussion of these newly introduced balance measures is forthcoming in Section 5, while the effectiveness and precision of the rating and counter tables will be evaluated in Section 4."}, {"title": "4 Accuracy of Strength Relations", "content": "With our rating table and counter table, we can approximate the win value of a match given two compositions and identify the strength relations for balance. In this section, we examine the accuracy of strength relations using these tables across different games and investigate the impact of the hyperparameters $ \\beta_N$ and $ \\beta_M$ on counter table training. There are 5 models for each method in our experiments, each trained from a different random seed. The results in the tables are the average values of these models."}, {"title": "4.1 PvP Games", "content": "To assess the accuracy of strength relations with our tables, we constructed simple games that emulate practical game scenarios for experiments. Additionally, we applied our methods to several open-access e-sports game records to confirm their real-world applicability."}, {"title": "4.1.1 Simple Combination Game", "content": "A simple combination game was designed with 20 elements, each assigned a score equal to its index from 1 to 20. A comp consists of three distinct elements. The score $s_c$ of a comp c is the sum of its elements'\n\nscores, and the win-lose outcome is binary, sampled via the probability function $P(c_1 > c_2) = \\frac{(s_{c_1})^2}{(s_{c_1})^2+($s_{c_2})^2}$.\n\nThere are $C^{3}_{20}$ = 1140 possible compositions in this game. The dataset, comprising 100,000 matches, was generated by uniformly sampling two comps."}, {"title": "4.1.2 Rock-Paper-Scissors", "content": "We adhered to the Rock-Paper-Scissors rules, only 3 compositions with win values of 0/0.5/1 for lose/tie/win, respectively. The dataset, consisting of 100,000 matches, generated by uniformly sampling."}, {"title": "4.1.3 Advanced Combination Game", "content": "This game combines the simple combination game with Rock-Paper-Scissors rules. The primary rule mirrors the simple combination game, with an additional rule: $T = s_c \\mod 3$, assigning T as the counter category of the comp, with 0/1/2 corresponding to Rock/Paper/Scissors. A winning Rock-Paper-Scissors comp receives a +60 score bonus during comp score calculation. There are still $C^{3}_{20}$ = 1140 possible compositions in this setting. The dataset consists of 100,000 matches generated uniformly."}, {"title": "4.1.4 Age of Empires II (AoE2)", "content": "Age of Empires II is a popular real-time strategy game. We utilized the statistics (as of January 2024) from aoestats\u00b9, an open-access statistics website. The game features 45 civilizations (comps) in 1v1 random map mode across all Elo ranges. Thus, there are 45 compositions in this game. Further combinations of civilizations in team mode or specific maps are not discussed in this paper. The dataset contains 1,261,288 matches."}, {"title": "4.1.5 Hearthstone", "content": "For Hearthstone, a popular collectible card game, we accessed the statistics (as of January 2024) from HSReplay\u00b2, an open-access statistics website. We considered 91 named decks as compositions for standard ranking at the Gold level. Therefore, only up to 91 compositions were used, and the detailed hero or card selections within these compositions were not considered for simplicity. The dataset comprises 10,154,929 matches."}, {"title": "4.1.6 Brawl Stars", "content": "Brawl Stars is a popular Multiplayer Online Battle Arena (MOBA) game. We focused on the Trio Modes of Brawl Stars, where teams of three compete for victory. Data were sourced from the \"Brawl Stars Logs & Metadata 2023\" on Kaggle, initially collected via the public API. With 64 characters, 43 maps, and 6 modes, the composition count could reach $C^{3}_{64} \u00d7 43 \u00d7 6$, i.e., the maximum number of compositions can reach 10,749,312. The dataset includes 179,995 matches, with 94,235 unique compositions observed."}, {"title": "4.1.7 League of Legends", "content": "For League of Legends, a renowned MOBA game with 5-on-5 team competition, we used the \"League of Legends Ranked Matches\" dataset from Kaggle, which features 136 champions. Thus, the maximum number of compositions is $C^{5}_{136} = 359,933,112$. The dataset covers 182,527 ranked solo games with 348,498 unique compositions observed, which implies that almost all compositions are different."}, {"title": "4.2 Comparisons of Strength Relation Prediction", "content": "To better understand whether win value predictions can provide accurate strength relations, we examine the accuracy of the strength relation classification task (weaker/same/stronger) rather than the prediction error of win values. For example, if there are two compositions, A and B, and the oracle win value is Win(A,B)=0.55, the actual outcome we care about is whether A is stronger than B. Now, consider two approximations: Win'(A,B)=0.49 and Win\"(A,B)=0.62. It is clear that Win' has a smaller absolute error (0.06) compared to Win\" (0.07), but Win' suggests the wrong strength relation.\n\nIn this strength relation classification task, if the win value falls within the range of [0.499, 0.501], we designate the prediction as the same; a value below 0.499 indicates weaker, and a value above 0.501 indicates stronger. The classification label is calculated based on the average pairwise win value in the dataset. For example, if CA has a 60% average win value against CB in the dataset, CA is deemed stronger when calculating accuracy. All models are approximated with neural networks and trained for 100 epochs on datasets using 5-fold cross-validation. A linear decay learning rate from 0.00025 to 0 over 100 epochs with the Adam optimizer is employed. We then compare the following five methods of win value prediction and provide their definitions with formulations in Section A.4.1:"}, {"title": "4.3 Counter Table Utilization", "content": "Given the need for a counter table for strength relation analysis, we adopt a vector quantization process in our NCT training. There is an issue with low codebook utilization when the state space is extremely small. We introduced a VQ Mean Loss to maximize the utilized M. For vector quantization, as described in Section 3.2, the standard hyperparameters are set to $ \\beta_N$ = 0.01 and $ \\beta_M$ = 0.25. We explore different configurations of these hyperparameters in Age of Empires II using NCT M=27 since it is a game requiring a large counter table for better strength relation accuracy, as shown in Tables 3 and 4. We found that the commonly suggested $ \\beta_N$ = 0.25 (van den Oord et al., 2017) leads to low utilization. Thus, we selected a nearly zero coefficient $ \\beta_N$ = 0.01 to perform regular VQ training. However, even with $ \\beta_N$ = 0.01, if we do not introduce the VQ Mean Loss (set $ \\beta_M$ = 0), the utilized M is still far from the upper bound of 27. We suggest using $ \\beta_M$ = 0.25 for better accuracy and codebook utilization. Also, a coefficient greater than 1 for VQ Mean Loss is not reasonable since it suggests gravitating the mean embedding more than the nearest embedding, which breaks the original idea of VQ and results in worse performance."}, {"title": "4.4 Tabular Version of Baselines", "content": "In Section 4.2, all baselines we used are trained from neural networks for better generalizing unseen compo- sitions. One may ask why not conduct a simple tabular approach like common rating or statistical analysis; thus, we also report the tabular version of WinValue, PairWin, and also Elo rating and a multidimensional variant, mElo2 (Balduzzi et al., 2018).\n\nWe test the following five types of methods:"}, {"title": "5 New Balance Measures", "content": "The creation of rating and counter tables allows us to devise new ways to measure balance in games, going beyond simple win rates to consider domination relations as described in Proposition 2.2. In games where two players compete against each other, a common goal is to equalize win rates, aiming for each player to have a win rate near 50%. This is easier to achieve in real-time games with symmetric settings for each player, but it is harder in turn-based games because the player who goes first often has an advantage (Beau & Bakkes, 2016). The main challenge in balancing is determining which player has the upper hand and the extent of their advantage. We propose two new ways to measure balance based on estimated win values and explain how to calculate these measures using our approximations to reduce computational complexity.\n\nNext, we will examine the diversity of comps players might choose in Section 5.1, and identify how many comps might give players an advantage in Section 5.2.\n\nFirst, let us define some important concepts:\n\nAssumption 5.1. The Bradley-Terry rating function R_\\theta(c) provides an estimate of $Win = \\frac{R_\\theta(c_1)}{R_\\theta(c_1)+R_\\theta(c_2)}$.\n\nProposition 5.2. The composition $c_{top}$ with the highest rating $R_\\theta(c_{top})$ over a rating function R_\\theta is considered to dominate all others with lower ratings.\n\nConsidering Definition 2.1, Proposition 2.2, and Assumption 5.1, we can conclude that Proposition 5.2 is true because $\\frac{x}{x+y} > \\frac{x'}{x'+y}$ when x > x' > 0. According to Proposition 5.2, if there is only one composition $c_{top}$ with the highest rating, it is considered the best choice before considering counter strategies. This information is often sufficient for some balancing methods, such as identifying and adjusting the strongest comp (Fontaine et al., 2019). The time complexity of identifying $c_{top}$ can be done in O(N) over N comps,"}, {"title": "5.1 Top-D Diversity Measure", "content": "We are examining how many different game compositions (comps) players might prefer to play. More choices can enrich the game content for fun and also help designers generate revenue by selling these comps. We want to know which comps players will pick"}]}