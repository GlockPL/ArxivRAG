{"title": "Aggregation Schemes for Single-Vector WSI Representation Learning in Digital Pathology", "authors": ["Sobhan Hemati", "Ghazal Alabtah", "Saghir Alfasly", "H.R. Tizhoosh"], "abstract": "A crucial step to efficiently integrate Whole Slide Images (WSIs) in computational pathology is assigning a single high-quality feature vector, i.e., one embedding, to each WSI. With the existence of many pre-trained deep neural networks and the emergence of foundation models, extracting embeddings for sub-images (i.e., tiles or patches) is straightforward. However, for WSIs, given their high resolution and gigapixel nature, inputting them into existing GPUs as a single image is not feasible. As a result, WSIs are usually split into many patches. Feeding each patch to a pre-trained model, each WSI can then be represented by a set of patches, hence, a set of embeddings. Hence, in such a setup, WSI representation learning reduces to set representation learning where for each WSI we have access to a set of patch embeddings. To obtain a single embedding from a set of patch embeddings for each WSI, multiple set- based learning schemes have been proposed in the literature. In this paper, we evaluate the WSI search performance of multiple recently developed aggregation techniques (mainly set representation learning techniques) including simple average or max pooling operations, Deep Sets, Memory networks, Focal attention, Gaussian Mixture Model (GMM) Fisher Vector, and deep sparse and binary Fisher Vector on four different primary sites including bladder, breast, kidney, and Colon from TCGA. Further, we benchmark the search performance of these methods against the median of minimum distances of patch embeddings, a non-aggregating approach used for WSI retrieval.", "sections": [{"title": "I. INTRODUCTION", "content": "The image analysis field has witnessed remarkable advance- ments in recent years, thanks to the integration of deep learning models and the availability of large-scale data. However, analysis of histopathology images using deep learning is not as straightforward as ordinary-sized images. Especially, to build whole slide image (WSI) search engines, the most critical step is to be able to measure the similarity between WSIs efficiently and accurately [Tizhoosh and Pantanowitz, 2024], [Lahr et al., 2024]. The state-of-the-art approach to this step is to extract high-quality deep embeddings that capture tissue morphology from each WSI using a well-trained deep model. To this end, it is necessary to feed WSIs to build such models. However, given the gigapixel nature of WSIs, feeding a WSI at high magnification into a model to GPU memory is infeasible. To overcome this challenge, a WSI is usually split into a set of much smaller sub-images called \"patches\" or tiles. Following this procedure, we end up with a set of deep embeddings per WSI which makes it challenging to employ them for downstream tasks like WSI retrieval (a set of pacthes that could be very large if a smart patch selection is not available). In this situation, having one single-vector WSI embedding can mitigate such challenges. With single- vector WSI representation much less memory is required to build a WSI search system. As well, using distance measures calculating similarity between WSIs becomes trivial. Finally, the search speed will increased and storage requirements will decrease resulting in wider deployment of WSI image classification and retrieval.\n\nA practical and established approach to calculate the sim- ilarity between two sets of embeddings is the median-of- minimums, a scheme that was introduced as part of the Yottixel search engine [Kalra et al., 2020b]; one calculates the minimum distance of any given input patch to all other patches (of the other WSI) and takes the median of all minimum distances overall (see Fig. 1). This approach is quite practical and fast (as Yottixel used binary feature vectors) but it is not able to provide a single vector to represent WSI. A more desirable solution would be to obtain one single embedding per WSI for more efficient storage and more targeted WSI-to- WSI matching, or for WSI classification (Fig. 2).\n\nTo this end, different aggregation algorithms to derive a single vector of deep features from a set of patch embeddings have been proposed including simple average or max pooling operations, Deep Sets [Zaheer et al., 2017], memory networks [Kalra et al., 2020a], focal attention [Kalra et al., 2021], graph convolution neural networks [Adnan et al., 2020], Gaussian mixture model (GMM) Fisher Vector [Jaakkola and Haussler, 1999], [Perronnin and Dance, 2007], [Hemati et al., 2023], and Deep Fisher Vector and its variations (binary and sparse Fisher Vector) [Hemati et al., 2023].\n\nAlthough multiple set representation learning techniques have been proposed and investigated in the literature, to the best of our knowledge, there is no study that evaluates these algorithms against each other for WSI retrieval as a challenging task. To address this shortcoming, in this paper, we briefly introduce each set representation learning technique and discuss our benchmarking scheme. Subsequently, we eval- uate each method over four different datasets namely bladder, breast, kidney, and colon through the k-Nearest Neighbour (k-NN) search using WSI embeddings obtained from each method. Finally, in the result section we present and discuss the performance of each technique to generate a single vector of deep features for a WSI."}, {"title": "II. BACKGROUND", "content": "In this section, we briefly review set representation learning algorithms applied to the WSI representation learning problem.\n\nA. GMM Fisher Vector (1999-2007)\nFisher Vector can be seen as an extension to bag of visual words (BoVW) [Csurka et al., 2004] that was initially developed in computer vision research for encoding a set of local image descriptors into one embedding. Theoreti- cally, Fisher Vector can be developed on top of any gen- erative models [Jaakkola and Haussler, 1999]. Some works [Perronnin and Dance, 2007] have introduced Gaussian mix- ture model (GMM)-based Fisher Vector which can be calcu- lated using the gradient of the log-likelihood of the GMMs with respect to its parameters given a bag of data points (e.g., means, variances, and mixing coefficients). Theoretically, while BoVW only employs mean statistics to obtain the set representation, in the GMM-based Fisher Vector, the variance as a higher-order statistic is also used.\n\nB. Deep Sets (2017-2021)\nThe Deep Set approach [Zaheer et al., 2017] focuses on permutation invariant representation learning and establish a standard definition of the permutation invariant functions. This approach shows such a family of functions can be implemented as neural networks to learn permutation invariant representa- tions. Other works employ Deep Sets-like architecture along with CNN to learn end-to-end WSI representations in WSI search tasks [Hemati et al., 2021]. These methods showed the obtained representations achieve better search performance both in terms of search speed and accuracy compared with Yottixel search engine [Kalra et al., 2020b].\n\nC. Memory Networks (2020)\nOther works have presented Memory-based Exchangeable Model (MEM) to learn the set functions [Kalra et al., 2020a]. MEM employs memory units and self-attention mechanisms to capture inter-dependencies between instances by mapping the input sets to high-level features. MEM was evaluated on point cloud and lung WSIs classification.\n\nD. Focal Attention (2021)\nAnother set representation scheme developed for WSI clas- sification/search is Focal attention [Kalra et al., 2021]. This approach was inspired by two recent developments in rep- resentation learning literature, namely focal loss and attention mechanism. More precisely, the focal attention approach maps embeddings for all patches of a mosaic into a single embed- ding using an attention-weighted averaging modulated by a trainable focal factor.\n\nE. Deep Fisher Vector and its variations (2023)\nMotivated by the success of GMM-based Fisher Vector, Fisher Vector theory has been proposed for more recent deep generative models including Variational Autoencoders (VAEs) [Qiu et al., 2017] and GANs [Zhai et al., 2019]. Recently, other works have extended VAE-based Fisher Vectors for the WSI representation learning task [Hemati et al., 2023]. More specifically, this approach offered two variations of Deep Fisher Vectors, namely \"deep sparse Fisher Vector\" and \"deep binary Fisher Vector\", to obtain binary and sparse permutation-invariant WSI embeddings suitable for down- stream tasks such as efficient WSI search. To obtain the deep sparse and binary Fisher Vectors, one can employ a simple VAE where its latent space is also connected to a dense layer with a softmax activation function that outputs a probability distribution representing the predicted WSI class. This classifier injects class information (i.e., diagnosis) into the final WSI embeddings. Considering that the Fisher Vector is derived from the gradient space of the VAE, to achieve sparse or binary embeddings, it is desired to have sparse or minimum quantization loss gradients. This step has been performed with a gradient regularization loss which is calculated using double backpropagation [Hemati et al., 2023]. Finally, the total loss for training the VAE consists of a weighted summation of reconstruction loss, KL divergence loss, classification loss, and gradient regularization loss. The contribution of gradient regularization loss to the total loss is controlled through the a regularization parameter. Fig. 3 shows the training architecture."}, {"title": "III. METHODOLOGY", "content": "This paper aims to compare the performance of the multiple set aggregation techniques for WSI representation learning including simple average or max pooling operations, Deep Sets, Memory networks, Focal attention, Gaussian Mixture Model (GMM) Fisher Vector, and deep sparse and binary Fisher Vector. We validate these methods for the task of WSI retrieval. Hence, we must construct an evaluation framework involving different datasets, and baselines. In this section, first, we discuss the datasets, the benchmarking task and baselines.\n\nA. Datasets\nThe datasets presented in this study contain a collection of four histopathology cases, including bladder, breast, kid- ney, and colon primary sites disease cases. All datasets are drawn from The Cancer Genome Atlas (TCGA). For each WSI, multiple patches were extracted using Yottixel's mo- saic algorithm (a two-staged clustering approach that em- ploys stain histograms and patch proximity to group patches) [Kalra et al., 2020b]. Each patch is represented by an embed- ding extracted from DenseNet [Huang et al., 2017] with ima- geNet pre-trained weights encoding the structural information and unique characteristics of the corresponding histological region. Patch embeddings enable quantitative analysis, facil- itating comparisons among different WSI regions providing a valuable resource for exploring morphology matching, de-veloping image analysis algorithms, and advancing diagnostic and prognostic research in oncology.\n\nBladder Dataset The skin data contains 457 cases with 6 different subtypes as classes. The subtypes include 'Transitional cell carcinoma', 'Papillary transitional cell car- cinoma', 'Cholangiocarcinoma', 'Squamous cell carcinoma- NOS', 'Papillary adenocarcinoma-NOS', \u2018Carcinoma-NOS'.\nBreast Dataset\nThe breast dataset contains 1,133 WSIs, with 23 subtypes including 'Lobular carcinoma-NOS', 'Infiltrating duct carcinoma-NOS', 'Infiltrating duct mixed with other types of carcinoma', 'Adenoid cystic carcinoma', 'Infiltrating duct and lobular carcinoma', 'Intraductal mi- cropapillary carcinoma', 'Large cell neuroendocrine carci- noma', 'Apocrine adenocarcinoma', 'Mucinous adenocarci- noma', 'Metaplastic carcinoma-NOS',\u2018Infiltrating lobular mixed with other types of carcinoma', 'Carcinoma-NOS', 'Paget disease and infiltrating duct carcinoma of breast', 'Pleomorphic carcinoma', 'Papillary carcinoma-NOS', 'Phyl- lodes tumor-malignant', 'Secretory carcinoma of breast', 'In- traductal papillary adenocarcinoma with invasion', 'Cribri- form carcinoma-NOS', \u2018Medullary carcinoma-NOS', \u2018Tubular adenocarcinoma', 'Basal cell carcinoma-NOS', 'Malignant lymphoma-large B-cell-diffuse-NOS'.\nKidney Dataset the kidney contains 6 subtypes and 940 cases fro TCGA. The subtypes include 'Papillary adenocarcinoma-NOS', 'Clear cell adenocarcinoma-NOS', 'Renal cell carcinoma-NOS', 'Giant cell sarcoma', 'Renal cell carcinoma-chromophobe type', 'Synovial sarcoma-spindle cell'.\nColon Dataset The colon dataset has 459 cases from TCGA that contain 9 classes, 'Adenocarcinoma-NOS', 'Mu- cinous adenocarcinoma', 'Adenosquamous carcinoma', 'Ade- nocarcinoma with neuroendocrine differentiation', 'Adenocar- cinoma with mixed subtypes', \u2018Carcinoma-NOS', 'Papillary adenocarcinoma-NOS', 'Dedifferentiated liposarcoma', 'Ma- lignant lymphoma-large B-cell-diffuse-NOS'.\n\nB. Benchmarking Scheme and Baselines\nWSI search and retrieval offers many benefits including teleconsultation, reduced workload, improved diagnostic quality, and expedited adoption and democratization of digital pathology through more efficient indexing of tissue images [Tizhoosh and Pantanowitz, 2018], [Janowczyk and Madabhushi, 2016], [Lahr et al., 2024], [Tizhoosh and Pantanowitz, 2024]. Considering these benefits, we are motivated to investigate the quality of WSI embeddings through the lens of k-Nearest Neighbors (k-NN) WSI search and retrieval."}, {"title": "IV. RESULTS", "content": "The accuracy, Macro F1 score, and weighted F1 score performance measures are used as the evaluation metrics. We benchmark multiple techniques against Yottixel's \"median of minimums\" as a non-aggregation approach [Kalra et al., 2020b]. We test max and mean poolings, Gaussian Mixture Model-based Fisher Vector [S\u00e1nchez et al., 2013], Deep Sets [Zaheer et al., 2017], memory networks [Kalra et al., 2020a], focal attention [Kalra et al., 2021], and deep sparse Fisher Vector and deep binary Fisher Vector by [Hemati et al., 2023].\n\nExcept for the median of minimums, all methods lead to a single embedding per WSI. Further, the deep Fisher Vector provides both binary and sparse WSI embeddings which are ideal for fast and efficient WSI search. These representations are memory-efficient, ensuring that the embeddings can be economically stored and efficiently processed for subsequent retrieval and analysis tasks. To ensure the reliability of the re- sults, a rigorous 5-fold cross-validation approach is employed. We compute the average and standard deviation from the five splits and use these as performance indicators.\nClearly, in most cases, Deep Fisher Vector and its variations (sparse and binary) achieve the best overall performance compared with all other baselines. Here we should emphasize sparse embeddings can reduce the memory required to store embeddings while binary WSI embeddings provide both memory efficiency and extremely fast search speed by employing Hamming distance. As a sample case, we also measured the time required to conduct the colon dataset search experiment for both Deep Sparse and Binary Fisher Vectors and also the median of minimums. For the Sparse Fisher Vector, Euclidean distance is used while for the Binary Fisher Vector Hamming distance is employed. The results for this experiment are presented in Table V. Clearly, the Deep Binary Fisher Vector achieves the fastest search speed. Given that for the Sparse Fisher Vector the Euclidean distance is used, its search speed is the same as other methods that output one real-valued embedding (with the same dimension) per WSI.\n\nA. Ablation study\nTo study the effect of the gradient sparsity and quantization losses for Deep Sparse and Binary Fisher vector embeddings respectively, we performed an ablation study on a for skin and lung datasets. To this end, we train the VAE for different values"}, {"title": "V. CONCLUSION", "content": "In this paper, we evaluated multiple WSI representation learning techniques including simple average or max pool- ing operations, deep sets, memory networks, focal attention, Gaussian Mixture Model (GMM) Fisher Vector, and deep sparse and binary Fisher Vector embeddings in the WSI search task for different datasets and two different deep models. Our evaluations show that the deep sparse and binary Fisher Vector may achieve the best overall performance compared with multiple set aggregation schemes. This result suggests that the Fisher Vector method has a robust ability to capture relevant patterns and features within the data, regardless of the specific characteristics of each dataset. This is an impor- tant indication of the versatility and generalizability of the Fisher Vector approach. Apart from their superior WSI search performance, deep sparse and binary Fisher Vectors also offer additional advantages that are suitable for fast and efficient search and retrieval. More precisely, the sparsity in deep sparse Fisher Vector embeddings makes them occupy less storage for indexing WSIs, a crucial factor for the wide adoption and also democratization of digital pathology. Also, the deep binary Fisher Vector is significantly faster than any real-valued counterpart as the calculation of Hamming distance between binary embeddings can be calculated using the XOR gate at the CPU level which is extremely faster than Euclidean distance."}]}