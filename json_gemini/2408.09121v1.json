{"title": "Selective Prompt Anchoring for Code Generation", "authors": ["Yuan Tian", "Tianyi Zhang"], "abstract": "Recent advances in large language models (LLMs) such as Copilot and ChatGPT\nhave transformed software development by automating coding tasks. Despite these\nadvancements, challenges remain in reducing error rates and fully meeting user\nexpectations. Our empirical study reveals LLMs tend to dilute their self-attention\non the initial prompt as more code tokens are generated. We hypothesize this self-\nattention dilution issue is one of the root causes of inaccuracies in LLM-generated\ncode. To mitigate this issue, we propose Selective Prompt Anchoring (SPA). SPA\namplifies the influence of the selected parts in the initial prompt, which we refer\nto as \"anchored text\", during code generation. Specifically, SPA calculates the\nlogit distribution difference with and without the anchored text. We prove this\ndifference approximates the anchored text's contextual contribution to the output\nlogits. SPA creates an augmented logit distribution by linearly combining the\noriginal logit distribution and the logit difference. We evaluate SPA with five LLMs\non four benchmarks. Our results demonstrate that using SPA can consistently\nimprove Pass@1 rates by up to 9.7% in all settings. Notably, with selective\ntext anchoring, a small version of DeepSeek-Coder (6.7B) can achieve better\nperformance than an original much larger version (33B). Our code is available at\nhttps://github.com/magic-YuanTian/Selective-Prompt-Anchoring.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) have emerged as powerful tools that assist programmers by automat-\ning parts of the coding process. These models leverage the vast capabilities of LLMs to interpret\ntask descriptions in natural language and generate corresponding code. While LLMs have achieved\nunprecedented performance on coding tasks, they still produce incorrect solutions to many tasks or\ngenerate code that does not fully meet user expectations. The prevalence of such generation errors\nundermines their reliability and limits their utility in real-world software development.\nTo improve the performance of LLMs on coding tasks, many efforts have been made to develop\nhigh-quality training data [45, 24, 82] and design new domain-specific training objectives [60, 4].\nHowever, these approaches require tremendous computational resources. Training-free approaches\nhave been explored to address this challenge by enhancing the prompting method or incorporating\nexternal knowledge, such as retrieval-augmented generation [18], chain-of-thoughts [38, 71, 56],\nself debugging and planning [32, 8], etc. While they have been proven to be effective in improving\nmodel performance, there exist limitations such as being sensitive to the quality of prompt design and\nretrieved data [88]. On top of these great attempts, this work aims to study and improve LLMs in an\northogonal direction-adjusting LLMs' attention.\nThe impressive performance of transformer-based LLMs is largely attributed to its self-attention\nmechanism [77], which enables models to focus dynamically on crucial parts of the given prompt.\nDespite the success of the self-attention mechanism, prior works found Language models exhibit"}, {"title": "An Empirical Analysis of Attention Dilution", "content": "We first conduct an empirical study to analyze the attention dilution issue in LLMs during code\ngeneration. To improve the generalizability of our findings, we experimented with two different\nmethods to compute the attention scores over input tokens. First, we used a self-attention-based\nmethod [87, 22, 44, 76, 78] to obtain scores from the last self-attention layer of LLMs. Second, we\nused a gradient-based method [66, 69, 70] that treats the entire LLM as a function and measures to\nwhat extent each input token contributes to the output. Based on these two methods, we calculate the\npercentage of attention on the initial prompt. Calculation details are provided in Appendix A.1.\nWe experiment on five LLMs [58, 65, 24] with various numbers of parameters on HumanEval [7],\na widely-used benchmark for code generation. Figure 2 and Figure 3 show the evolution of the\ndensity of LLMs' attention on the initial prompt when generating the first 400 tokens. The results\ndemonstrate that as the model generates more tokens, model attention on the initial prompt gradually\nbecomes smaller, which we refer to as attention dilution. As a result, the code generation process\nbecomes more influenced by the code tokens generated in previous time steps, rather than the prompt\nfrom users as the generated code sequence becomes longer. This can be problematic in two ways.\nFirst, generation errors in the previous time steps are very likely to propagate to the following steps as\nthe model pays more attention to the preceding code tokens. Second, for complex tasks that require\nthe generation of a long code sequence (e.g., multiple if statements), the model is likely to miss\ncritical descriptions as it pays little attention to the user prompt deep in the code generation process."}, {"title": "Approach", "content": "Given a transformer-based large language model (LLM) denoted as $f_\\theta$ and an initial prompt x, the\nmodel generates tokens autoregressively by calculating conditional probabilities for the next token $t_i$\nat step i. Let $t_1, t_2, ..., t_{i-1}$ be the sequence of tokens already generated by the model.\nThe input to the model $f_\\theta$ is an $n \\times m$ embedding matrix, where n stands for the dimension of each\nembedding (column) and m represents the fixed context window size. Each embedding is mapped\nfrom the corresponding token in the prompt. Formally,\n$E_i = embedding(x, t_1, t_2,..., t_{i-1}) = [E^x, C_1, C_2, . . ., C_{i-1}, PAD].$\\hfill(1)\nHere, $E_i$ is the input embedding matrix to the model $f_\\theta$ at step i. $E^x$ is the collection of embeddings\nfor the tokens in the prompt x, serving as a submatrix of $E_i$. $C_1,..., C_{i-1}$ are the embeddings"}, {"title": "Semantic Adjustment", "content": "To mitigate the attention dilution issue, we propose Selective Prompt Anchoring (SPA) to augment\nthe output logits by amplifying the contextual contribution of the selective tokens within the prompt,\nwhich we refer to as \"anchored text\".\nSPA introduces the mechanism of adjusting the semantic impact of arbitrary groups of token embed-\ndings in the input matrix $E_i$ towards the output logits $f_\\theta(E_i)$. For simplicity, here we make the entire\ninitial prompt x as the anchored text.\n$E_i$ is an $n \\times m$ input embedding matrix at step i, and $E^x$ represents an $n \\times k$ submatrix within $E_i$\ncovering the first k columns (corresponding to the prompt x). They are visualized below:\nWe construct two $n \\times m$ matrices, X and $G_i$, which add up to $E_i$. Matrix X is created by preserving\nthe first k columns of $E_i$ corresponding to $E^x$ and setting all other columns to zero (note that $E^x$ and\nX remain unchanged during generating new tokens). Matrix $G_i$ is constructed by setting the first k\ncolumns of $E_i$ that correspond to $E^x$ to zero, and retaining all other elements from the remaining\ncolumns. They are visualized as follows:\nThe sum of X and $G_i$ reconstructs the original matrix $E_i$:\n$E_i = X + G_i.$\nSuppose we want to amplify the semantic impact of the submatrix X by a weighting value $\\omega$. $\\omega > 1$\nindicates amplification of semantics, while $\\omega < 1$ indicates diminishing the semantics. We define\na semantic adjustment function $\\Phi(X, \\omega)$ that scales the influence of X by $\\omega$. Here we assume the\nsemantic consistency [64]: the scaled embeddings maintain their semantic interpretations in a manner"}, {"title": "Augmented Logits by Approximation", "content": "Given the computational complexities of LLMs, directly solving $\\int_0^1 \\frac{dF_{\\theta,i,x}(t)}{dt} dt$ is impractical.\nTherefore, we approximate it by employing the Taylor expansion:\n$F_{\\theta,i,x}(\\omega) = F_{\\theta,i,x}(0) + \\omega \\cdot F_{\\theta,i,x}'(0) + \\frac{\\omega^2}{2!} F_{\\theta,i,x}''(0) + ...$\\hfill(12)\nSince LLMs are inherently non-linear, higher-order derivatives of the logits function are non-zero.\nHowever, for computational efficiency, we truncate the series after the first derivative, yielding:\n$F_{\\theta,i,x}(\\omega) \\approx F_{\\theta,i,x}(0) + \\omega \\cdot F_{\\theta,i,x}'(0),$\\hfill(13)\nwhere the the integral part $\\int_0^1 \\frac{dF_{\\theta,i,x}(t)}{dt} dt$ in equation 11 is approximated by $\\omega \\cdot F_{\\theta,i,x}'(0)$. Here we\nassume the logit function $F_{\\theta,i,x}$ is smooth and differentiable so it is possible to compute the first-order\nderivative.\nTo calculate $F_{\\theta,i,x}(0)$, we mask tokens in the anchored text x using masked embeddings. Each\nLLM provides at least one special token reserved for text masking, which almost has no semantic\ninfluence, e.g., <unk> for Code Llama [65] and <pad> for DeepSeek-Coder [24]. Each special token\ncorresponds to a masked embedding. By replacing embeddings of x with masked embeddings, we\nget a masked input matrix $E^{mask}$. It ablates the semantic influence of the anchored text x while the\npositional encoding is not affected. Thus, we can get\n$F_{\\theta,i,x}(0) = f_{\\theta} (E^{mask}).$\\hfill(14)\nTo calculate $F_{\\theta,i,x}'(0)$, we use finite-difference methods to get an approximation. Assuming the\ninterval of 1 0 is sufficiently small for $F_{\\theta,i,x}$, we get:\n$F_{\\theta,i,x}' (0) \\approx \\frac{F_{\\theta,i,x} (1) - F_{\\theta,i,x} (0)}{1-0}$\\hfill(15)\nCombining equation 13, 14, and 15, we get the augmented logits by first-order approximation:\n$F_{\\theta,i,x}(\\omega) \\approx F_{\\theta,i,x}(0) + \\omega \\cdot (F_{\\theta,i,x}(1) - F_{\\theta,i,x}(0))$\n$= \\omega \\cdot f_{\\theta} (\\Phi(X, 1) + G_i) + (1 - \\omega) \\cdot f_{\\theta} (\\Phi(X, 0) + G_i)$\n$= \\omega \\cdot f_{\\theta} (E_i) + (1 - \\omega) \\cdot f_{\\theta} (E^{mask}).$\\hfill(18)\nBased on the augmented logits $F_{\\theta,i,x}(\\omega)$ where the impact of the anchored text is adjusted by a\ngiven value $\\omega$, a certain sampling algorithm is applied to select the particular token. SPA can be used\nto augment different existing sampling methods, including greedy sampling, beam search, nucleus\nsampling [29], and more."}, {"title": "Higher Order Approximation", "content": "It is feasible to calculate higher-order approximation. For example, if we want to keep the term\nwith the second-order derivative $\\frac{\\omega^2}{2!} F_{\\theta,i,x}''(0)$ in Equation 12, it can still be computed using finite-\ndifference methods:\n$F_{\\theta,i,x}''(0) \\approx \\frac{F_{\\theta,i,x}(1) - 2F_{\\theta,i,x}(0) + F_{\\theta,i,x}(-1)}{(1-0)^2}$\\hfill(19)\n$F_{\\theta,i,x}(-1)$ can be solved by Equation 16 where $F_{\\theta,i,x}(0)$ and $F_{\\theta,i,x}(1)$ are the logits generated from\nthe original input and the logits generated from the masked input. Similarly, whatever how many\nterms we keep in the Taylor expansion, it is essentially a linear combination of $F_{\\theta,i,x}(0)$ and $F_{\\theta,i,x}(1)$.\nFor simplicity, SPA calculates augmented logits using the first-order approximation in this work."}, {"title": "Tuning Weighting Values", "content": "The weighting value $\\omega$ serves as a hyperparameter. Our experiments demonstrate an unimodal\nrelationship between $\\omega$ and the performance. As the weight $\\omega$ increases, the performance first\nimproves, reaching an optimum, and then declines with further increases of $\\omega$. It is simple to tune\nthis single parameter. More details are discussed in Section 5.2."}, {"title": "Selective Anchored Text", "content": "While SPA can anchor the entire prompt, the initial prompt can significantly vary due to task\ndifferences. In some scenarios, the prompts can be lengthy and not all information is important.\nAccording to our observation, we find narrowing down anchored text can lead to better performance.\nThis is intuitive since anchoring too many tokens may decrease their impact on differentiating the\nlogit distribution, e.g., anchoring all the tokens will make no difference. Our goal is to identify and\nanchor the most informative tokens, which LLMs should consistently focus on, while excluding\ntrivial details in the prompt. For code generation tasks, the prompt commonly comprises four possible\ncomponents: (1) Natural language instruction or docstring; (2) Starting code snippet; (3) A list of test\ncases; (4) Few-shot examples. Intuitively, natural language instruction provides high-level guidance\nthat LLM should continually consider. Our ablation study in Section 5.3 confirms it and shows\nanchoring the natural language instruction alone yields the best performance."}, {"title": "Experiments", "content": "SPA requires access to the full logits generated by the large language models (LLMs), so we are\nunable to evaluate closed-source models, such as ChatGPT. We select five representative open-\nsource code LLMs: CodeGen-Mono [58] (350M), CodeLlama [65] (7B), and three different-sized DeepSeek-Coder-Instruct [24] (1.3B, 6.7B, 33B) models. These models have been fine-tuned\nfor code generation tasks. Notably, the DeepSeek-Coder-Instruct models have been fine-tuned by\ninstruction-tuning [81], while CodeGen-Mono and CodeLlama are standard text completion models.\nThis selection aims to cover diverse SOTA code LLMs of different types and sizes."}, {"title": "Benchmarks", "content": "HumanEval [7]. It includes 164 Python tasks designed by OpenAI developers. It was initially\ndesigned to evaluate Codex [5] and has since become a common benchmark for code generation.\nMBPP [1]. It includes 974 crowd-sourced Python tasks. However, due to the crowd workers'\nambiguous or insufficient task descriptions, the MBPP authors created a sanitized version containing\n427 tasks with clearer descriptions. We evaluate SPA on the sanitized version.\nHumanEval+ and MBPP+. Although HumanEval and MBPP are considered de facto standards for\nassessing code LLMs, a recent study [51] found they lack sufficient test cases and precise problem\ndescriptions. This has been demonstrated as an issue that can lead to an unreliable assessment of\nLLM-generated code [52]. Liu et al. [51] subsequently released HumanEval+ and MBPP+, which"}, {"title": "Experiment Setup and Metrics", "content": "Model Deployment. We download and deploy LLMs from Huggingface. To speed up evaluations,\nwe apply 8-bit quantization [19, 14] to all the models. Prior studies [46, 30] show 8-bit quantization\nhas very little impact on LLM performances. All the experiments are conducted on eight A5500\nGPUs (each with 24GB VRAM) with 192GB memory for five weeks.\nPrompt Design. We use the original task descriptions from the datasets as prompts for the text-\ncompletion models, CodeLlama and CodeGen-Mono. For the three DeepSeek-Coder-Instruct models,\nwe format the prompts using the official chat template from HuggingFace. All experiments are\nconducted in a zero-shot setting.\nEvaluation Metric. Following prior works [6, 36, 5], we measure model performance using the\nPass@k metric, which measures whether any of k candidates can pass all the test cases. As a common\nsetting, we set k to 1 in our experiment. For each task, LLMs only generate one code snippet using\ngreedy sampling. The generated code is considered correct when it passes all the test cases."}, {"title": "Results", "content": "We compare each LLM's performance with and without using SPA in Table 1. The results indicate\nthat using SPA can consistently improve the Pass@1 rate across all benchmarks and all LLMs. The\nimprovement is up to 9.7% on Humaneval for DeepSeek-Coder (6.7B). It's worth noting that through\nselective text anchoring, the small version of DeepSeek-Coder (6.7B) surpasses a much larger version\n(33B). This supports our hypothesis that the inaccuracies in LLMs mainly stem from poor attention,\nnot their generative capabilities. To demonstrate SPA can effectively anchor LLM's attention on the\ninitial prompt, we include two examples in Appendix A.3.\nWhile SPA achieved a consistent improvement on LLMs with different sizes, we do not observe a\nmonotonic relationship between model size and the improvement. Furthermore, there is no obvious\ncorrelation between the original model performance and the improvement. It is an interesting future\ndirection to investigate how different model attributes affect the improvement achieved by SPA."}, {"title": "Analysis of Weighting Values", "content": "SPA introduces a single weighting hyperparameter $\\omega$, which is used to adjust the magnitude of the\nanchoring effect of SPA. Figure 4 reports Pass@1 rates given various values of $\\omega$ for each model\nand benchmark ($\\omega = 1$ means the original model). We observe that the optimal $\\omega$ slightly varies\nacross different models and benchmarks. However, there roughly exists an unimodal relationship"}, {"title": "Analysis of Anchored Test Selection", "content": "As mentioned in Section 3.6, we analyzed how anchored text selection impacts the performance of\nSPA. Prompts in HumanEval and HumanEval+ include the function signature (referred to as Code),\ntask descriptions in natural language (NL), and test cases (Test). Prompts in MBPP and MBPP+\nconsist of task descriptions (NL) followed by test cases (Test). For HumanEval and HumanEval+,\nwe create four conditions by removing test cases and source code. For MBPP and MBPP+, we create\ntwo conditions by removing test cases. Since task descriptions serve as the core of a prompt, we\nchoose to anchor it in all conditions.\nFor each condition and benchmark, we calculate the average Pass@1 rate improvement across all five\nLLMs. Table 2 shows that anchoring the task description alone yields the best performance. This\nimplies that anchoring more tokens in the prompt may not necessarily be helpful. Narrowing down\nthe anchored text to fewer but critical, informative tokens can lead to better performance."}, {"title": "Related work", "content": "Code Generation. In recent years, there has been rapid progress in the development of code\ngeneration approaches [15, 31, 85, 72] and benchmarks [86, 43, 7, 1, 51, 27, 3, 37, 26]. With the\nadvent of large language models (LLMs), such as GPT-4 [61], Gemini [73], and the LLaMA family\n[74, 75], code generation has become a standard capability. Subsequent research [5, 65, 2, 24, 20,\n45, 53, 90, 55] has focused on fine-tuning these pre-trained LLMs on coding datasets, achieving\nstate-of-the-art performance across a variety of coding tasks.\nDespite their remarkable ability to follow natural language instructions, LLMs still face challenges\nwhen generating long and complex code. To enhance the code generation capabilities of LLMs, recent\nstudies have explored train-free approaches such as prompt engineering [13, 83], in-context learning\n[16, 41, 42], and retrieval-augmented generation [40, 18]. Additionally, self-debugging techniques"}, {"title": "Limitations", "content": "We employed 8-bit quantized LLMs to expedite all experiments. Although this method has been\nshown to have minimal impact on performance, we did notice some degradation. Furthermore, we did\nnot evaluate very large LLMs (e.g., more than 100B) due to computational constraints. Despite the\nunimodal feature, it is infeasible to enumerate all the weighting hyperparameter $\\omega$ on the continuous\ndistribution. The real optimal $\\omega$ should perform slightly better than the values reported in Section 4.\nWhile our experimented benchmarks have become standards for evaluating code generation models,\nwe did not evaluate SPA in real-world scenarios. More evaluations can be performed in future studies."}, {"title": "Conclusion & Future Work", "content": "In this paper, we propose SPA, a training-free approach designed to enhance the quality of code\ngenerated by large language models (LLMs) by mitigating the attention dilution issue. SPA employs\na novel technique to adjust the influence of arbitrary groups of input tokens, based on a mathemat-\nical approximation. Our empirical studies indicate that LLMs may overlook the initial prompt as\ngenerating more new tokens. To mitigate this issue, SPA amplifies the impact of the initial prompt\nthroughout code generation. Our evaluations demonstrate that SPA consistently and significantly\nenhances the performance of code LLMs of various sizes on multiple benchmarks.\nThe effectiveness of SPA highlights its potential in other domains, especially for generation tasks.\nBesides, the underlying principle of SPA is not confined to transformer-based LLMs and could be\nadapted for use in other model architectures (e.g., RNN). In this work, we pre-define the method\nto select anchored tokens and make the weighting hyperparameter fixed when generating code. We\nbelieve this only serves as a baseline. In future work, both of anchored text and the weighting value\ncan be dynamically determined according to different contexts and sampling stages."}, {"title": "Attention Calculation", "content": "Self-attention. Most LLMs are based on the decoder of transformer [77] which has multiple self-\nattention layers. Roughly speaking, given an LLM $f_\\theta$ and an input sequence of tokens $t_0, t_1,..., t_n$\nwhere $t_i$ represents the ith token. The transformer calculates relevance scores between every pair of\ntokens. The self-attention score for a token $t_i$ in the sequence can be roughly formulated as:\n$attention(t_i) \\approx \\frac{\\Sigma_{j=1}^{n} relevance(t_i, t_j)}{\\Sigma_{j=1}^{n} \\Sigma_{j=1}^{n} relevance(t_i, t_j)},$\\hfill(20)\nwhere the relevance function approximates the computation among Q, K, V in transformers [77].\nHowever, different layers have different attention distributions. According to a study [80], deeper\nself-attention layers can better capture long-distance dependencies and program structure, so we\ncalculate the attention by aggregating attention from multiple heads at the last layer. Nevertheless,\nthis still excludes the influence from the last forward layer.\nGradient-based Attention. Compared to using self-attention layers in transformers, the gradient-\nbased method can be generalized to different model architectures and consider the entire model as a\nwhole. It computes the model's attention by calculating the gradients relative to the input. Intuitively,\na token that induces a larger gradient is considered more influential, suggesting that the model pays\ngreater attention to it. Formally, the attention over the token $t_i$ is calculated by\n$attention(t_i) = \\frac{\\partial f_\\theta(t_0,t_1,...,t_n)}{\\partial t_i}$\\hfill(21)\nAttention Percentage to the Prompt. Based on these two methods, we analyze how the attention of\nLLMs to the initial prompt shifts. Formally, given the prompt x and the following generated tokens\n$t_0, t_1,..., t_{i-1}$, we calculate the percentage of attention $\\alpha(x)$ over the initial prompt\n$\\alpha(x) = \\frac{attention(x)}{attention(x) + \\Sigma_{j=1}^{i-1} attention(t_i)}$\\hfill(22)\nGiven attention analysis requires open sourcing, we select five SOTA code LLMs with various sizes.\nWe run the experiments on HumanEval [7], one of the most popular benchmarks for evaluating code\ngeneration models. We run five LLMs [58, 65, 24] on all 164 Humaneval tasks. Figure 2 shows the\nself-attention shift and Figure 3 shows the gradient-based attention shift when generating the first 400\ntokens. The value gradually becomes noisy due to the lack of generated sequence with enough length.\nThe results demonstrate that there indeed exists such attention dilution issue. Due to the autoregressive\nnature, LLMs' attention to the initial prompt is gradually diluted as generating more code. LLMs tend\nto attend to code generated by itself. Our finding is supported by another study [9] which investigates\nthe self-attention dilution of transformers in a more general scenario."}, {"title": "Optimal Weighting Values", "content": ""}, {"title": "Examples", "content": "Figure 5 presents two examples comparing the code generated by models alone and the models\naugmented using SPA.\nIn the first example, CodeLlama (7B) overlooks the specified condition \"upper vowels.\" In contrast,\nSPA enhances the model's focus on the intended purpose. The code initializes all the upper vowels in\nthe first line and correctly refers to it later.\nIn the second example, DeepSeek-Coder (1.3B) erroneously sorts the list by string names instead of\nintegers. When using SPA, the model demonstrates improved recognition of the required procedures,\naligning more closely with the task description. The code correctly sorts and reverses the list. Then\nthe integer list is mapped to the string list."}]}