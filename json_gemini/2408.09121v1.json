{"title": "Selective Prompt Anchoring for Code Generation", "authors": ["Yuan Tian", "Tianyi Zhang"], "abstract": "Recent advances in large language models (LLMs) such as Copilot and ChatGPT have transformed software development by automating coding tasks. Despite these advancements, challenges remain in reducing error rates and fully meeting user expectations. Our empirical study reveals LLMs tend to dilute their self-attention on the initial prompt as more code tokens are generated. We hypothesize this self-attention dilution issue is one of the root causes of inaccuracies in LLM-generated code. To mitigate this issue, we propose Selective Prompt Anchoring (SPA). SPA amplifies the influence of the selected parts in the initial prompt, which we refer to as \"anchored text\", during code generation. Specifically, SPA calculates the logit distribution difference with and without the anchored text. We prove this difference approximates the anchored text's contextual contribution to the output logits. SPA creates an augmented logit distribution by linearly combining the original logit distribution and the logit difference. We evaluate SPA with five LLMs on four benchmarks. Our results demonstrate that using SPA can consistently improve Pass@1 rates by up to 9.7% in all settings. Notably, with selective text anchoring, a small version of DeepSeek-Coder (6.7B) can achieve better performance than an original much larger version (33B). Our code is available at https://github.com/magic-YuanTian/Selective-Prompt-Anchoring.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have emerged as powerful tools that assist programmers by automating parts of the coding process. These models leverage the vast capabilities of LLMs to interpret task descriptions in natural language and generate corresponding code. While LLMs have achieved unprecedented performance on coding tasks, they still produce incorrect solutions to many tasks or generate code that does not fully meet user expectations. The prevalence of such generation errors undermines their reliability and limits their utility in real-world software development.\nTo improve the performance of LLMs on coding tasks, many efforts have been made to develop high-quality training data [45, 24, 82] and design new domain-specific training objectives [60, 4]. However, these approaches require tremendous computational resources. Training-free approaches have been explored to address this challenge by enhancing the prompting method or incorporating external knowledge, such as retrieval-augmented generation [18], chain-of-thoughts [38, 71, 56], self debugging and planning [32, 8], etc. While they have been proven to be effective in improving model performance, there exist limitations such as being sensitive to the quality of prompt design and retrieved data [88]. On top of these great attempts, this work aims to study and improve LLMs in an orthogonal direction-adjusting LLMs' attention.\nThe impressive performance of transformer-based LLMs is largely attributed to its self-attention mechanism [77], which enables models to focus dynamically on crucial parts of the given prompt. Despite the success of the self-attention mechanism, prior works found Language models exhibit"}, {"title": "2 An Empirical Analysis of Attention Dilution", "content": "We first conduct an empirical study to analyze the attention dilution issue in LLMs during code generation. To improve the generalizability of our findings, we experimented with two different methods to compute the attention scores over input tokens. First, we used a self-attention-based method [87, 22, 44, 76, 78] to obtain scores from the last self-attention layer of LLMs. Second, we used a gradient-based method [66, 69, 70] that treats the entire LLM as a function and measures to what extent each input token contributes to the output. Based on these two methods, we calculate the percentage of attention on the initial prompt. Calculation details are provided in Appendix A.1."}, {"title": "3 Approach", "content": "3.1 Limitations of Standard Inference\nGiven a transformer-based large language model (LLM) denoted as $f_\\theta$ and an initial prompt x, the model generates tokens autoregressively by calculating conditional probabilities for the next token $t_i$ at step i. Let $t_1, t_2, ..., t_{i-1}$ be the sequence of tokens already generated by the model.\nThe input to the model $f_\\theta$ is an n \u00d7 m embedding matrix, where n stands for the dimension of each embedding (column) and m represents the fixed context window size. Each embedding is mapped from the corresponding token in the prompt. Formally,\n$E_i = embedding(x, t_1, t_2,..., t_{i-1}) = [E^x, C_1, C_2, . . . , C_{i-1, PAD]$.\nHere, $E_i$ is the input embedding matrix to the model $f_\\theta$ at step i. $E^x$ is the collection of embeddings for the tokens in the prompt x, serving as a submatrix of Ei. $C_1,..., C_{i-1}$ are the embeddings"}, {"title": "3.2 Semantic Adjustment", "content": "To mitigate the attention dilution issue, we propose Selective Prompt Anchoring (SPA) to augment the output logits by amplifying the contextual contribution of the selective tokens within the prompt, which we refer to as \"anchored text\".\nSPA introduces the mechanism of adjusting the semantic impact of arbitrary groups of token embeddings in the input matrix Ei towards the output logits $f_\\theta (E_i)$. For simplicity, here we make the entire initial prompt x as the anchored text.\n$E_i$ is an n\u00d7m input embedding matrix at step i, and $E_x$ represents an \u00d7 k submatrix within $E_i$ covering the first k columns (corresponding to the prompt x). They are visualized below:\n$E_i = \\begin{bmatrix}\n e_{11} & ... & e_{1k} & e_{1,k+1} & ... & e_{1m} \\\\\n e_{21} & ... & e_{2k} & e_{2,k+1} & ... & e_{2m} \\\\\n : & & : & : & & : \\\\\n e_{n1} & ... & e_{nk} & e_{n,k+1} & ... & e_{nm}\n\\end{bmatrix}$", "formula": "E = \\begin{bmatrix}\n e_{11} & ... & e_{1k} & e_{1,k+1} & ... & e_{1m} \\\\\n e_{21} & ... & e_{2k} & e_{2,k+1} & ... & e_{2m} \\\\\n : & & : & : & & : \\\\\n e_{n1} & ... & e_{nk} & e_{n,k+1} & ... & e_{nm}\n\\end{bmatrix}"}, {"title": "3.3 Augmented Logits by Approximation", "content": "Given the computational complexities of LLMs, directly solving $\\int_{0}^{1} \\frac{dF_{\\theta, i, x}(t)}{dt} dt$ is impractical. Therefore, we approximate it by employing the Taylor expansion:\n$F_{\\theta, i, x}(\\omega) = F_{\\theta, i, x}(0) + \\omega \\cdot F_{\\theta, i, x}'(0) + \\frac{\\omega^2}{2!} F_{\\theta, i, x}''(0) + ...$\nSince LLMs are inherently non-linear, higher-order derivatives of the logits function are non-zero. However, for computational efficiency, we truncate the series after the first derivative, yielding:\n$F_{\\theta, i, x}(W) \\approx F_{\\theta, i, x}(0) + \\omega \\cdot F_{\\theta, i, x}' (0)$", "formulae": ["\\int_{0}^{1} \\frac{dF_{\\theta, i, x}(t)}{dt} dt", "F_{\\theta, i, x}(\\omega) = F_{\\theta, i, x}(0) + \\omega \\cdot F_{\\theta, i, x}'(0) + \\frac{\\omega^2}{2!} F_{\\theta, i, x}''(0) + ...", "F_{\\theta, i, x}(W) \\approx F_{\\theta, i, x}(0) + \\omega \\cdot F_{\\theta, i, x}' (0)"]}, {"title": "3.4 Higher Order Approximation", "content": "It is feasible to calculate higher-order approximation. For example, if we want to keep the term with the second-order derivative $\\frac{\\omega^2}{2!} F_{\\theta, i, x}''(0)$ in Equation 12, it can still be computed using finite-difference methods:\n$F_{\\theta, i, x}''(0) \\approx \\frac{F_{\\theta, i, x}(1) - 2F_{\\theta, i, x}(0) + F_{\\theta, i, x}(-1)}{(1-0)^2}$", "formula": "F_{\\theta, i, x}''(0) \\approx \\frac{F_{\\theta, i, x}(1) - 2F_{\\theta, i, x}(0) + F_{\\theta, i, x}(-1)}{(1-0)^2}"}, {"title": "3.5 Tuning Weighting Values", "content": "The weighting value w serves as a hyperparameter. Our experiments demonstrate an unimodal relationship between w and the performance. As the weight w increases, the performance first improves, reaching an optimum, and then declines with further increases of w. It is simple to tune this single parameter. More details are discussed in Section 5.2."}, {"title": "3.6 Selective Anchored Text", "content": "While SPA can anchor the entire prompt, the initial prompt can significantly vary due to task differences. In some scenarios, the prompts can be lengthy and not all information is important. According to our observation, we find narrowing down anchored text can lead to better performance. This is intuitive since anchoring too many tokens may decrease their impact on differentiating the logit distribution, e.g., anchoring all the tokens will make no difference. Our goal is to identify and anchor the most informative tokens, which LLMs should consistently focus on, while excluding trivial details in the prompt. For code generation tasks, the prompt commonly comprises four possible components: (1) Natural language instruction or docstring; (2) Starting code snippet; (3) A list of test cases; (4) Few-shot examples. Intuitively, natural language instruction provides high-level guidance that LLM should continually consider. Our ablation study in Section 5.3 confirms it and shows anchoring the natural language instruction alone yields the best performance."}, {"title": "4 Experiments", "content": "4.1 Comparison Baselines\nSPA requires access to the full logits generated by the large language models (LLMs), so we are unable to evaluate closed-source models, such as ChatGPT. We select five representative open-source code LLMs: CodeGen-Mono [58] (350M), CodeLlama [65] (7B), and three different-sized DeepSeek-Coder-Instruct [24] (1.3B, 6.7B, 33B) models. These models have been fine-tuned for code generation tasks. Notably, the DeepSeek-Coder-Instruct models have been fine-tuned by instruction-tuning [81], while CodeGen-Mono and CodeLlama are standard text completion models. This selection aims to cover diverse SOTA code LLMs of different types and sizes."}, {"title": "4.2 Benchmarks", "content": "HumanEval [7]. It includes 164 Python tasks designed by OpenAI developers. It was initially designed to evaluate Codex [5] and has since become a common benchmark for code generation.\nMBPP [1]. It includes 974 crowd-sourced Python tasks. However, due to the crowd workers' ambiguous or insufficient task descriptions, the MBPP authors created a sanitized version containing 427 tasks with clearer descriptions. We evaluate SPA on the sanitized version.\nHumanEval+ and MBPP+. Although HumanEval and MBPP are considered de facto standards for assessing code LLMs, a recent study [51] found they lack sufficient test cases and precise problem descriptions. This has been demonstrated as an issue that can lead to an unreliable assessment of LLM-generated code [52]. Liu et al. [51] subsequently released HumanEval+ and MBPP+, which"}, {"title": "4.3 Experiment Setup and Metrics", "content": "Model Deployment. We download and deploy LLMs from Huggingface. To speed up evaluations, we apply 8-bit quantization [19, 14] to all the models. Prior studies [46, 30] show 8-bit quantization has very little impact on LLM performances. All the experiments are conducted on eight A5500 GPUs (each with 24GB VRAM) with 192GB memory for five weeks.\nPrompt Design. We use the original task descriptions from the datasets as prompts for the text-completion models, CodeLlama and CodeGen-Mono. For the three DeepSeek-Coder-Instruct models, we format the prompts using the official chat template from HuggingFace. All experiments are conducted in a zero-shot setting.\nEvaluation Metric. Following prior works [6, 36, 5], we measure model performance using the Pass@k metric, which measures whether any of k candidates can pass all the test cases. As a common setting, we set k to 1 in our experiment. For each task, LLMs only generate one code snippet using greedy sampling. The generated code is considered correct when it passes all the test cases."}, {"title": "5 Results", "content": "5.1 Main Results\nWe compare each LLM's performance with and without using SPA in Table 1. The results indicate that using SPA can consistently improve the Pass@1 rate across all benchmarks and all LLMs. The improvement is up to 9.7% on Humaneval for DeepSeek-Coder (6.7B). It's worth noting that through selective text anchoring, the small version of DeepSeek-Coder (6.7B) surpasses a much larger version (33B). This supports our hypothesis that the inaccuracies in LLMs mainly stem from poor attention, not their generative capabilities. To demonstrate SPA can effectively anchor LLM's attention on the initial prompt, we include two examples in Appendix A.3.\nWhile SPA achieved a consistent improvement on LLMs with different sizes, we do not observe a monotonic relationship between model size and the improvement. Furthermore, there is no obvious correlation between the original model performance and the improvement. It is an interesting future direction to investigate how different model attributes affect the improvement achieved by SPA."}, {"title": "5.2 Analysis of Weighting Values", "content": "SPA introduces a single weighting hyperparameter w, which is used to adjust the magnitude of the anchoring effect of SPA. Figure 4 reports Pass@1 rates given various values of w for each model and benchmark (w = 1 means the original model). We observe that the optimal w slightly varies across different models and benchmarks. However, there roughly exists an unimodal relationship"}, {"title": "5.3 Analysis of Anchored Test Selection", "content": "As mentioned in Section 3.6, we analyzed how anchored text selection impacts the performance of SPA. Prompts in HumanEval and HumanEval+ include the function signature (referred to as Code), task descriptions in natural language (NL), and test cases (Test). Prompts in MBPP and MBPP+ consist of task descriptions (NL) followed by test cases (Test). For HumanEval and HumanEval+, we create four conditions by removing test cases and source code. For MBPP and MBPP+, we create two conditions by removing test cases. Since task descriptions serve as the core of a prompt, we choose to anchor it in all conditions.\nFor each condition and benchmark, we calculate the average Pass@1 rate improvement across all five LLMs. Table 2 shows that anchoring the task description alone yields the best performance. This implies that anchoring more tokens in the prompt may not necessarily be helpful. Narrowing down the anchored text to fewer but critical, informative tokens can lead to better performance."}, {"title": "6 Related work", "content": "Code Generation. In recent years, there has been rapid progress in the development of code generation approaches [15, 31, 85, 72] and benchmarks [86, 43, 7, 1, 51, 27, 3, 37, 26]. With the advent of large language models (LLMs), such as GPT-4 [61], Gemini [73], and the LLaMA family [74, 75], code generation has become a standard capability. Subsequent research [5, 65, 2, 24, 20, 45, 53, 90, 55] has focused on fine-tuning these pre-trained LLMs on coding datasets, achieving state-of-the-art performance across a variety of coding tasks.\nDespite their remarkable ability to follow natural language instructions, LLMs still face challenges when generating long and complex code. To enhance the code generation capabilities of LLMs, recent studies have explored train-free approaches such as prompt engineering [13, 83], in-context learning [16, 41, 42], and retrieval-augmented generation [40, 18]. Additionally, self-debugging techniques"}, {"title": "7 Limitations", "content": "We employed 8-bit quantized LLMs to expedite all experiments. Although this method has been shown to have minimal impact on performance, we did notice some degradation. Furthermore, we did not evaluate very large LLMs (e.g., more than 100B) due to computational constraints. Despite the unimodal feature, it is infeasible to enumerate all the weighting hyperparameter w on the continuous distribution. The real optimal w should perform slightly better than the values reported in Section 4. While our experimented benchmarks have become standards for evaluating code generation models, we did not evaluate SPA in real-world scenarios. More evaluations can be performed in future studies."}, {"title": "8 Conclusion & Future Work", "content": "In this paper, we propose SPA, a training-free approach designed to enhance the quality of code generated by large language models (LLMs) by mitigating the attention dilution issue. SPA employs a novel technique to adjust the influence of arbitrary groups of input tokens, based on a mathematical approximation. Our empirical studies indicate that LLMs may overlook the initial prompt as generating more new tokens. To mitigate this issue, SPA amplifies the impact of the initial prompt throughout code generation. Our evaluations demonstrate that SPA consistently and significantly enhances the performance of code LLMs of various sizes on multiple benchmarks.\nThe effectiveness of SPA highlights its potential in other domains, especially for generation tasks. Besides, the underlying principle of SPA is not confined to transformer-based LLMs and could be adapted for use in other model architectures (e.g., RNN). In this work, we pre-define the method to select anchored tokens and make the weighting hyperparameter fixed when generating code. We believe this only serves as a baseline. In future work, both of anchored text and the weighting value can be dynamically determined according to different contexts and sampling stages."}, {"title": "A.1 Attention Calculation", "content": "Self-attention. Most LLMs are based on the decoder of transformer [77] which has multiple self-attention layers. Roughly speaking, given an LLM fe and an input sequence of tokens to, t1,..., tn where ti represents the ith token. The transformer calculates relevance scores between every pair of tokens. The self-attention score for a token ti in the sequence can be roughly formulated as:\nattention(ti) \u2248\n\nwhere the relevance function approximates the computation among Q, K, V in transformers [77]. However, different layers have different attention distributions. According to a study [80], deeper self-attention layers can better capture long-distance dependencies and program structure, so we calculate the attention by aggregating attention from multiple heads at the last layer. Nevertheless, this still excludes the influence from the last forward layer.\nGradient-based Attention. Compared to using self-attention layers in transformers, the gradient-based method can be generalized to different model architectures and consider the entire model as a whole. It computes the model's attention by calculating the gradients relative to the input. Intuitively, a token that induces a larger gradient is considered more influential, suggesting that the model pays greater attention to it. Formally, the attention over the token t\u2081 is calculated by\nattention(ti) =\nAttention Percentage to the Prompt. Based on these two methods, we analyze how the attention of LLMs to the initial prompt shifts. Formally, given the prompt x and the following generated tokens t0, t1,..., ti\u22121, we calculate the percentage of attention \u03b1(x) over the initial prompt\n\u03b1(x) =\nGiven attention analysis requires open sourcing, we select five SOTA code LLMs with various sizes. We run the experiments on HumanEval [7], one of the most popular benchmarks for evaluating code generation models. We run five LLMs [58, 65, 24] on all 164 Humaneval tasks. Figure 2 shows the self-attention shift and Figure 3 shows the gradient-based attention shift when generating the first 400 tokens. The value gradually becomes noisy due to the lack of generated sequence with enough length.\nThe results demonstrate that there indeed exists such attention dilution issue. Due to the autoregressive nature, LLMs' attention to the initial prompt is gradually diluted as generating more code. LLMs tend to attend to code generated by itself. Our finding is supported by another study [9] which investigates the self-attention dilution of transformers in a more general scenario.", "formulae": ["attention(ti) \u2248\\frac{\\Sigma_{j=1}^{n} relevance(t_i, t_j)}{\\Sigma_{i=1}^{n} \\Sigma_{j=1}^{n} relevance(t_i, t_j)}", "attention(ti) =\\frac{\\frac{d fo(t_0, t_1, ..., t_n)}{d t_i}}{\\Sigma_i \\frac{d fo(t_0, t_1, ..., t_n)}{d t_i}}", "\u03b1(x) = \\frac{attention(x)}{attention(x) + \\Sigma_{i=1}^{i-1} attention(t_i)}", "\\Sigma_{j=1}^{n} relevance(t_i, t_j)", "\\Sigma_{i=1}^{n} \\Sigma_{j=1}^{n} relevance(t_i, t_j)", "\\frac{d fo(t_0, t_1, ..., t_n)}{d t_i}", "\\Sigma_i \\frac{d fo(t_0, t_1, ..., t_n)}{d t_i}", "\\Sigma_{i=1}^{i-1} attention(t_i)"]}, {"title": "A.2 Optimal Weighting Values", "content": "Table 3 reports optimal weighting values w that are used in our main results (Table 1). We observe the average value of 0.28 can be used to effectively improve performance across all benchmarks for all LLMs."}, {"title": "A.3 Examples", "content": "Figure 5 presents two examples comparing the code generated by models alone and the models augmented using SPA.\nIn the first example, CodeLlama (7B) overlooks the specified condition \"upper vowels.\" In contrast, SPA enhances the model's focus on the intended purpose. The code initializes all the upper vowels in the first line and correctly refers to it later.\nIn the second example, DeepSeek-Coder (1.3B) erroneously sorts the list by string names instead of integers. When using SPA, the model demonstrates improved recognition of the required procedures, aligning more closely with the task description. The code correctly sorts and reverses the list. Then the integer list is mapped to the string list."}]}