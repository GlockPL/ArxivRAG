{"title": "Evaluating the Effectiveness of LLMs in Fixing Maintainability Issues in Real-World Projects", "authors": ["Henrique Nunes", "Eduardo Figueiredo", "Larissa Rocha", "Sarah Nadi", "Fischer Ferreira", "Geanderson Esteves"], "abstract": "Large Language Models (LLMs) have gained attention for addressing coding problems, but their effectiveness in fixing code maintainability remains unclear. This study evaluates LLMs capability to resolve 127 maintainability issues from 10 GitHub repositories. We use zero-shot prompting for Copilot Chat and Llama 3.1, and few-shot prompting with Llama only. The LLM-generated solutions are assessed for compilation errors, test failures, and new maintainability problems. Llama with few-shot prompting successfully fixed 44.9% of the methods, while Copilot Chat and Llama zero-shot fixed 32.29% and 30%, respectively. However, most solutions introduced errors or new maintainability issues. We also conducted a human study with 45 participants to evaluate the readability of 51 LLM-generated solutions. The human study showed that 68.63% of participants observed improved readability. Overall, while LLMs show potential for fixing maintainability issues, their introduction of errors highlights their current limitations.", "sections": [{"title": "I. INTRODUCTION", "content": "Code maintainability is important because it affects how easily the code can be understood, changed, and improved [1]. Poor maintainability can increase development costs, reduce software quality [2], and lead to slower delivery of new features [3]. Addressing maintainability issues involves refactoring the code to improve its structure, readability, and adherence to best practices. However, fixing maintainability issues can be challenging, because a specialist review is expensive and slow, while automated tools are imprecise and require human interpretation [3]. Code smells are a common type of maintainability issue, with examples including methods that become complex and take on too many responsibilities, code that is no longer used, or instances where the same code snippets are repeated twice or more [4, 5, 6].\nRecent advances in large language models (LLMs) have generated much interest in their use for coding problems [7, 8, 9, 10, 11]. These models have shown remarkable capabilities in generating code, repairing bugs, and conducting software tests, but using LLMs to refactor maintainability issues is under- explored and lacks relevant data [12]. In contrast to traditional automated tools that adhere to rigid rules, LLMs can provide an innovative approach to addressing maintainability issues. Their ability to understand complex contexts allows them to generate flexible and adaptable solutions.\nFurthermore, LLMs early investigations typically employ controlled scenarios to measure LLM capabilities [13, 14, 15, 16]. Despite these initial results, it is essential to understand the effectiveness of LLMs in addressing issues within real- world software projects [17, 18]. This type of software projects introduces numerous challenges for LLMs, including the need to understand and navigate codebases, adhere to various coding standards, and maintain compatibility with existing systems. Therefore, it is crucial to investigate how LLMs can resolve issues without introducing new errors or unintended behavior [8, 10, 14, 19].\nIn this paper, we evaluate the effectiveness of using LLMS to fix maintainability issues within Java methods. Our goal is to understand which maintainability issues LLMs can fix and where they fail. We aim to provide a comprehensive assessment of the utility of LLMs in maintenance tasks.\nTo conduct our empirical study, we use SonarQube to collect 127 instances of maintainability issues out of 10 GitHub Java projects, which have recent development activity, strong community, and a test suite available. The instances of the detected issues correspond to violations of 10 unique SonarQube rules. Then, we experiment with a proprietary LLM, Copilot Chat (version 0.15.2), and an open- source LLM, Llama 3.1 70B Instruct, to fix the issues. We employ a zero-shot approach for both LLMs and a few-shot prompting approach for Llama to evaluate their performance across different prompt configurations. We also conduct a"}, {"title": "II. RELATED WORK", "content": "Correctness Evaluation of Copilot. Several previous empirical studies [8, 10, 20, 21] have aimed to evaluate Copilot's performance in software development tasks. For instance, Nguyen and Nadi [5] empirically evaluated the correctness and understandability of Copilot's suggested code in four different languages (Python, Java, JavaScript, and C) for 33 LeetCode questions. Their results showed that Java suggestions had the highest likelihood of being correct and that Copilot's sugges- tions have low cyclomatic and cognitive complexity (median 5 and 6, respectively). To repay self-admitted technical debts (SATD), O'Brien et al. [8] relied on Copilot to automatically generate 1,140 code bodies for TODO comments. From a dif- ferent perspective, Pearce et al. [10] investigated the security of Copilot's code contributions and the conditions that cause GitHub Copilot to recommend insecure code. In their context, they found that up to 40% of Copilot-generated code could be vulnerable. Unlike these published studies [10, 21], our paper focuses on whether and how Copilot solves maintainability issues in actual software projects from GitHub.\nCorrectness Evaluation of Llama. Several studies have compared the effectiveness of Llama with proprietary LLMs [22, 23]. Jensen et al. [22] evaluated the effectiveness of proprietary and open-source LLMs for identifying code with security vulnerabilities. The study used zero-shot prompting to assess Llama 2's effectiveness in providing detailed de- scriptions of security vulnerabilities. Zhu et al. [23] assessed the capabilities of Code Llama and other LLMs for code summarization. In most cases, GPT-3.5 and GPT-4 produced better results for Java, but sometimes CodeLlama outperforms other LLMs for Python. When comparing different prompting techniques, CodeLlama, when using few-shot prompting, had better results compared to zero-shot learning in most cases. Like these studies, our work compares proprietary and open-source LLMS, including different prompting approaches, but in the software maintainability context.\nEvaluation of LLMs for Code Refactoring. Initial studies evaluated the effectiveness of LLMs for code refactoring [18, 24, 25]. Choi et al. [18] propose an iterative project-level code refactoring process to reduce complexity by identifying the methods with the highest Cyclomatic Complexity and performing refactoring on them using ChatGPT 3.5. The results show that the average Cyclomatic Complexity is re- duced with several iterations. Pomian et al. [24] propose a tool named EM-Assist that utilizes ChatGPT 3.5 to generate refactoring suggestions for methods that require the Extract Method refactoring technique. The tool ranks the solutions to provide developers with high-quality options. The results demonstrate that EM-Assist achieves a recall rate of 53% to fix complex methods. Shirafuji et al. [25] propose a method to select the best-suited code refactoring examples used for few-shot prompts to reduce Cyclomatic Complexity, using ChatGPT 3.5. The results show that their method can reduce the complexity. These studies are very focused on Cyclomatic Complexity issues and Extract Method techniques. They also focus on ChatGPT 3.5. Our study evaluates different types of maintainability issues, refactoring techniques, and LLMs."}, {"title": "III. STUDY DESIGN", "content": "In this section, we present the design of our study, which aims to evaluate the effectiveness of LLMs in addressing maintainability issues. To identify these issues, we utilize SonarQube, a tool that detects violations of predefined rules, such as \"String literals should not be duplicated\". When a rule is violated, SonarQube reports it as an issue. Maintainability issues are categorized as a type of code smell by SonarQube [26]. Our evaluation focuses on assessing the capability of LLMs to fix different types of maintainability issues by addressing the following research questions:\nRQ1. To what extent can LLMs fix maintainability issues? For each Java method that has maintainability issues detected by SonarQube, we assess how many of these the LLMs can fix. We assess not only whether the LLM fixed the original issue, but also if the code passed the build process and if no new maintainability issues were introduced.\nRQ2. What are the main errors made by LLMs when attempting to fix maintainability issues? We evaluate the be- havior of LLMs in addressing different types of maintainability"}, {"title": "IV. EVALUATION", "content": "A. Effectiveness of an LLM in fixing maintainability issues\nFigure 3 shows the results for the three LLM configurations. The X-axis represents the status of the LLM solution: (1) fixed: methods that were fixed by the LLMs without intro- ducing any errors or failures; (2) not fixed: methods that were neither fixed nor introduced errors or failures; (3) compilation error: methods that caused compilation errors after the LLMs changes; (4) test failure: methods that caused test failures after the LLMs changes; (5) degraded: methods where new maintainability issues were introduced after the LLMs changes (6) no suggestion: methods for which the LLMs did not suggest any fix.\nFrom the 127 samples in the study, the best effectiveness in fixing maintainability issues was achieved by the Llama few- shot approach, which fixed 57 (44.9%) methods, followed by Copilot Chat with 41 (32.29%) methods, and Llama zero-shot with 38 (30%) methods. The worst result of not fixed comes from Copilot Chat with 23 (18.11%) methods, followed by Llama zero-shot with 22 (17.22%) methods and Llama few-shot with 12 (9.44%) methods. Concerning the cases with compilation errors, the worst case is the Llama zero-shot approach with 42 (33%) methods, followed by Llama few- shot with 37 (29.13%) methods, and Copilot Chat with 32 (25.2%) methods. Regarding LLM-generated solutions with test failures, Llama zero-shot has 19 (15%) methods, Llama few-shot has 18 (14.17%) methods, and Copilot Chat has 9 (7.08%) methods. Only Copilot Chat and Llama zero-shot have degraded methods, 6 (4.72%) and 5 (4%) respectively. Finally, in cases where LLMs did not suggest any solution, Copilot Chat has 16 (12.6%) methods, Llama zero-shot (0.78%) has 1 method, and Llama few-shot has 3 (2.36%) methods.\nAnswer of RQ1: Our evaluation reveals that Llama few- shot demonstrated the highest effectiveness in addressing maintainability issues, successfully fixing 57 out of 127 methods (44.9%). In comparison, Copilot Chat and Llama zero-shot fixed 41 (32.29%) and 38 (30%) methods, respec- tively. Notably, in addition to the Llama few-shot yielding the highest fixing rate, it also introduces fewer new main- tainability issues compared to the other methods, indicating its superior performance in both fixing existing problems and maintaining code quality.", "B. Refactoring Techniques and common LLM errors in maintainability fixes": ""}, {"content": "Table III shows the results categorized by the 10 SonarQube rules. The first column shows the status of the LLM solution. The second column lists the LLMs and the approaches used (zero-shot or few-shot). The remaining columns display the acronyms of the rules, as shown in Table II. Each numeric value represents the number of methods. We highlight some results in each status in bold. We want to show a higher number in the fixed status and a lower number in the other statuses.\nWe analyzed 127 LLM-generated solutions out of the 3 strategies to evaluate the effectiveness of LLMs and to un- derstand what types of refactoring techniques they use.\nHallucination in LLMs occurs when the model generates incorrect or irrelevant information, creating responses that appear plausible but are not grounded in real data [37]. This concept is crucial for this section because we provide some code examples to exemplify common errors when LLMs fail to fix maintainability issues. Due to space limitations in the paper, we use [...] to omit parts of the code that are not important for the discussion. We present the snippet of the original code and the refactored code immediately after the comments '// original' and '// LLM-generated solution', respectively. We now discuss our results.\nThe cognitive complexity of methods should not be too high (CCM). This issue occurs when the method control flow is hard to understand. We evaluated 26 methods that violated the CCM rule. Copilot Chat shows the best effectiveness, fixing 7 methods (26.9%), followed by Llama, which fixes 6 methods (23%) in both zero-shot and few-shot approaches. The effectiveness rate is low. Only 2 (7.7%) methods are fixed by all strategies, and at least one LLM fixed 10 (38.46%) methods. Regarding the refactoring strategies used to fix the issue, Copilot Chat uses the Extract Method [4] 6 times and Split Conditional [4] once, while Llama zero-shot and few- shot use the Extract Method every time. Concerning failed cases, the most common type is compilation error, with 15 occurrences in Llama few-shot and 11 in the other approaches. The most frequent compilation errors are: Cannot find symbol, Incompatible types, and Element is already defined.\nshows an example where the Extract Method refactoring tech- nique was applied to the put method, creating new methods, including overwriteEntry. Note that in the original method, the variable prev is declared as a DirectoryEntry, while in the LLM-generated solution, prev has become an attribute of the DirectoryEntry object. This example illustrates a common type of wrong logic interpretation caused by LLMs when attempting to fix CCM issues. Another important observation is the high number of LLM-generated solutions that change the method behavior for CCM: 7, 5, and 4 for Llama zero-shot, Llama few-shot, and Copilot zero-shot, respectively. These errors suggest that LLMs struggle to maintain the context and dependencies when moving code elements.\nGeneric exceptions should never be thrown (GET). This issue occurs when the method throws generic exceptions instead of specific ones. We evaluated 13 methods that violated the GET rule. Llama has the best effectiveness for zero-shot and few-shot approaches, both fixed 4 (30.7%) methods, while Copilot Chat fixed 3 (23%) methods. Like in CCM, the effec- tiveness rate is still low. Only 2 (15.3%) methods are fixed by all strategies, and at least one LLM fixes 6 (46.1%) methods. All method fixes use the Change Thrown Exception Type [38] refactoring technique, with one exception that uses Add Thrown Exception Type [39] by creating two exception classes. Cases that use Change Thrown Exception Type change Run- timeException for IllegalStateException. Most failed cases are caused by compilation errors, specifically when LLM changes the signatures with the throws command (most common in Llama few-shot) and when using exceptions without importing their classes. Llama few-shot, Llama zero-shot, and Copilot Chat have 7 (53.8%), 4 (30.7%) and 3 (23%) LLM-generated solutions classified as compilation error, respectively. To use LLMs to fix GET issues, developers should specify their exceptions, indicate what type of refactoring they want to use"}, {"title": "V. THREATS TO VALIDITY", "content": "Internal Validity. Internal validity concerns factors that could affect the results of our study without our knowledge. One primary concern is the use of two LLM tools, Copilot Chat and Llama 3.1 70B Instruct, for our analysis. Specific limitations or inherent biases in Copilot Chat and Llama 3.1 may influence our findings. Furthermore, the evaluation criteria and the method of assessing code maintainability might introduce subjective biases. Although we employed Sonar- Qube for an objective measure of code quality and included multiple evaluators in the assessment process, differences in evaluators' experience and interpretation could still impact the results. Additionally, it is important to note that we lack visibility into the inner workings of Copilot Chat and Llama 3.1, adding an element of uncertainty to our analysis.\nAdditionally, the experimental setup, including the selection of maintainability issues and the criteria for success, may affect the outcomes. For instance, the decision to focus on specific SonarQube rules could introduce bias if these rules do not represent a comprehensive view of code maintainability. The potential for human error in interpreting and applying these rules during the evaluation process is another factor that could influence the internal validity of our study. Also, the prompt build for this study might introduce bias, as it attempted to emulate a developer's interaction with the tool, typically not employing more advanced techniques in its input.\nExternal Validity. External validity addresses the extent to which our findings can be generalized beyond the specific context of our study. Our dataset comprised a selection of Java projects, which may limit the applicability of our results to other programming languages or types of software projects. To address this, we chose a diverse set of Java projects, aiming to cover different application domains and coding styles. The Java language also demonstrated good performance in studies using LLM [21]. However, future studies should include other programming languages and project types to enhance generalizability.\nMoreover, the context in which the LLM was used (in- tegrated within an IDE and prompted to fix specific issues) may not reflect other potential use cases of LLMs in software development. We attempted to replicate a realistic development environment but acknowledged that different settings and user interactions could lead to different outcomes. We selected Copilot Chat and Llama 3.1 due to their prominence in previous research and extensive training by OpenAI, Meta, and GitHub. These LLMs represent a state-of-the-art tool widely used in practice. To mitigate the impact of this choice, we carried out a study with human evaluations and maintained a critical perspective on Copilot and Llama performance throughout the study. Encouraging further studies in various environments and with different developer expertise levels will help validate our findings more broadly.\nReliability. Reliability concerns the consistency of our results. If the study were repeated under similar conditions, it should yield comparable results. The variability in Copilot Chat and Llama suggestions and the subjective nature of some evaluations could affect reliability. We mitigated this by stan- dardizing the evaluation process, using a consistent dataset, and documenting our methodology. This ensures that future researchers can replicate our setup and obtain similar results. Additionally, the reproducibility of our results is dependent on the specific version of the tools and datasets used. To address this, we archived the versions of Copilot Chat, Llama, SonarQube, and the datasets used in our study, providing a reference for future studies. This approach helps maintain consistency even as tools and environments evolve."}, {"title": "VI. CONCLUSIONS AND FUTURE WORK", "content": "This paper evaluated the effectiveness of LLMs in fixing maintainability issues by mining 127 methods with issues, corresponding to 10 SonarQube rules, from 10 GitHub repos- itories. Llama few-shot fixed 57 (44.9%) out of 127 meth- ods, the Copilot Chat 41 (32.29%) and Llama zero-shot 38 (30%). Furthermore, in our human evaluation study, 68.6% of developers considered the LLM solutions more readable compared to the original methods. We conclude that although LLMs show potential in improving code readability and fixing maintainability issues, their effectiveness is limited, and they often introduce new errors or fail to fix maintainability issues. In our study, LLM performance in addressing maintainability issues fell short, but recent studies highlight fine-tuning as a promising optimization strategy [46, 47, 48, 49], which we aim to explore in our research. We also plan to explore other languages and different types of LLMs as part of these efforts."}]}