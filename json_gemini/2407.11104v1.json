{"title": "Exploring the Potentials and Challenges of Deep Generative Models in Product Design Conception", "authors": ["Phillip Mueller", "Lars Mikelsons"], "abstract": "The synthesis of product design concepts stands at the crux of early-phase development processes for technical products, traditionally posing an intricate interdisciplinary challenge. The application of deep learning methods, particularly Deep Generative Models (DGMs), holds the promise of automating and streamlining manual iterations and therefore introducing heightened levels of innovation and efficiency. However, DGMs have yet to be widely adopted into the synthesis of product design concepts. This paper aims to explore the reasons behind this limited application and derive the requirements for successful integration of these technologies. We systematically analyze DGM-families (VAE, GAN, Diffusion, Transformer, Radiance Field), assessing their strengths, weaknesses, and general applicability for product design conception.\nOur objective is to provide insights that simplify the decision-making process for engineers, helping them determine which method might be most effective for their specific challenges. Recognizing the rapid evolution of this field, we hope that our analysis contributes to a fundamental understanding and guides practitioners towards the most promising approaches. This work seeks not only to illuminate current challenges but also to propose potential solutions, thereby offering a clear roadmap for leveraging DGMs in the realm of product design conception.", "sections": [{"title": "INTRODUCTION", "content": "Product design conception (PDC) is an intricate and multifaceted process, demanding considerable effort and investment. During the early-phase, foundational concepts translate functional and feature requirements into preliminary visual and geometric representations [1]. These visualizations are crucial, shaping both the aesthetic appeal and engineering functionality of the final product. The challenge intensifies in consumer products such as passenger vehicles, which must harmonize appealing design with robust functionality to meet both aesthetic desires and practical necessities.\nTraditionally, this process heavily relies on the domain expertise of engineers and designers, necessitating extensive manual iteration across various design modalities. Such iterative and manual processes are not only time-consuming but also require substantial investments, often depending on limited and compartmentalized information that impede innovation and efficiency.\nRecent advances in deep learning, particularly in generative methods, offer a promising path to the goal of reducing manual, time-consuming iterations. Additionally, they bear the potential to elevate human creativity by democratizing the engineering design process through lowering the skill-barrier [2].\nDeep Generative Models (DGMs) are specifically tailored to learn complex data distributions and generate novel samples from the learned distributions [3], [4]. These models, including Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), Diffusion Models, and Transformer-based architectures, have demonstrated significant potential in various fields, including natural language processing [5] and image generation [6], [7] [8]\u2013[10].\nThe landscape of DGM-applications is currently dominated by text-based tasks and the corresponding Large-Language-Models like LLAMA [11] and GPT [12]. Despite their undeniable potential, DGMs have not yet been widely adopted in engineering design and more specifically in PDC. This is due to several challenges that complicate their integration into existing workflows. Firstly, product and engineering design tasks often require precise domain-specific knowledge, which is difficult to encapsulate within the frameworks of DGMs. These tasks rely on specific modalities, like requirement tables, hand-drawn sketches, technical drawings and low-fidelity images to translate ideas and constraints into technical product representations. Many of the modalities are challenging for DGMs to accommodate. As a result, the outputs from DGMs frequently lack the robustness, reliability, interpretability, and replicability necessary for critical design applications [13], [14].\nSecondly, the inherent complexity of DGMs poses a significant barrier to their adoption by non-expert users, such as engineers, who may find the advanced machine learning concepts and operations daunting. This challenge is compounded by the rapid evolution and diversity of models within the field, which can overwhelm even dedicated specialists trying to keep pace with the latest developments [15]. Selecting a feasible method for a given task typically requires elaborate experimentation on top of existing in-depth knowledge by an expert.\nAdditionally, efficient application and deployment of DGMs within the PDC-process presuppose substantial data and computational resources, further restricting their accessibility and scalability. These models must also effectively integrate multifaceted data that captures human inputs and interactions, a requirement essential for enhancing design quality and tailoring solutions to specific domain challenges. The convergence of these factors contributes to the limited penetration of DGMs in the realm of product design conception.\nThis study aims to provide a comprehensive analysis of the potentials and challenges associated with the application of DGMs in PDC. In an effort to lower the barrier for the application of these models, we will derive fundamental requirements that DGMs must fulfill to be effectively integrated into PDC-processes. These requirements will serve as a basis for evaluating important model families, assessing their suitability for tasks within product design. By examining how these approaches align with the defined requirements, we seek to enhance understanding and assist practitioners in three key areas:\n1. Analyzing Model Suitability: Practitioners will gain a better and more realistic understanding of which applications are realistic with DGMs and where their limitations lie. This will facilitate more informed decisions regarding the selection of appropriate models for solving specific product design problems.\n2. Evaluating Application Cases: By outlining prerequisites and limitations of DGMs concerning PDC-application, we hope to guide practitioners in analyzing their application cases to determine the suitability for incorporating DGMs. This analysis aims to ensure that the deployment of these technologies is both practical and effective, taking into consideration the specific conditions and constraints of the design task at hand.\n3. Outlining Research Directions: As a result of our analysis, we aim to identify areas where further research could enhance the applicability and effectiveness of DGMs.\nBy setting a baseline for the comparison of DGM- capabilities with the needs of the concept design process, we hope to assist in selecting the most appropriate models, setting realistic expectations, and effectively utilizing these technologies in product design workflows.\nWe summarize relevant background about DGM- families in Chapter 2 and provide further references for the interested reader. Chapter 3 discusses the PDC- process, existing potentials and open challenges, from which the key requirements for DGM application are derived. The outlined requirements form the basis for the technical analysis of DGM-families in Chapter 4. To lower the knowledge-barrier and make our work more accessible for non-DGM-experts, we summarize our application recommendations in Chapter 5. Our conclusions in Chapter 6 aim to briefly discuss the three objectives we outlined throughout this chapter."}, {"title": "2 BACKGROUND", "content": "2.1 Deep Generative Models\nGenerative Adversarial Networks (GANs) are a class of models that consist of two competing neural networks \u2013 the generator and the discriminator. GANs have found success in a variety of applications since their initial introduction [16], including image generation [17], [18] and manipulation [19], as well as text-to-image synthesis [20]. While the generator learns to map a vector of latent variables to a desired distribution, thus generating new content, the discriminator learns to distinguish between real and generated content. The weights of both networks are improved independently during training. As the discriminator improves, the generator also improves as it learns to generate content that fools the discriminator. The training of GANs is often considered challenging due to instabilities from vanishing gradients in the generator and the failure of the generator to capture all modes in the data distribution (mode collapse). Nevertheless, GAN- architectures have found great success in a variety of applications, as they allow for the important possibility of conditioning the generative process, meaning the generative process can be guided by user-provided constraints. This significantly increases the potential for application in product design. [21] provide a discretely conditioned GAN (\u2018cGAN'), where the conditioning vector is fed into the generator and the discriminator. There are numerous other approaches on conditioning GANs, we direct the interested reader to refer to 'InfoGAN' [22] and continuous conditional GANS [23].\nVariational Autoencoder (VAEs) belong to the family of probabilistic machine learning models. For Autoencoders, an encoder maps the input data into a lower-dimensional latent representation while the decoder reconstructs the original content as accurately as possible. Variational Autoencoders, introduced by Kingma and Welling [24], add probabilistic sampling in the latent space, regularizing the latent distribution and creating a more continuous mapping of the data distribution to the latent space that allows for sampling of realistic latent vectors. The Kullback-Leibler (KL) divergence loss between the latent distribution and a standard Gaussian is added to obtain a predictable latent space distribution. Interested readers may refer to [25] and [26] for detailed information on VAE- foundations and to [27], [28] for conditional VAEs.\nTransformer models are a deep learning architecture popular in the fields of natural language processing and computer vision. The modern transformer- architecture was initially proposed in the widely known work 'Attention Is All You Need' [5], to which we refer the reader for more detailed information. Transformers rely on the 'attention mechanism', that computes a weighted sum of input values (V) based on their 'attention scores' (relevance) to a specific query (Q) and key (K) pair. This allows to capture global dependencies between input and output in parallel and dynamically prioritize specific parts of the input data. Transformers excel at capturing long-range dependencies and intricate patterns in data. Transformers have been widely adopted for training large language models [29]\u2013[31]. They also are a key element in vision-centric models, to learn visual concepts from natural language, such as in 'ViT' [32] and 'CLIP' [33].\nDiffusion models are a class of generative models that simulate the data generation as a reverse diffusion process. The approach, routed in non-equilibrium thermodynamics, was initially proposed in 2015 [4]. Data structures, like images, are gradually destroyed through the addition of Gaussian noise until there is no information left from the original data. The reverse process is learned by a deep neural network, that transforms a simple distribution (like Gaussian noise) into the complex distribution of the original data. Although requiring large amounts of data and compute, diffusion models have recently found immense success in image generation. We direct the interested reader to the following works: [6], [34]\u2013[36] for fundamentals and [37] as well as [38] for large- scale image generation applications."}, {"title": "2.2 Development of early-phase product design concepts", "content": "2.2.1 Process\nThe early-phase process of PDC is characterized by the definition and subsequent translation of main engineering requirements into functional representations of the product design. In the product- design-process defined by [1], the PDC is localized in the early-phases of \u201cIdeation and Planning\" as well as \"Concept Development\". The phase of planning and ideation is crucial for brainstorming and comparing diverse ideas, facilitating a creative exploration of potential solutions before formalizing the product concept. It serves as the foundation for generating a broad array of ideas, which are then critically evaluated to translate them into an initial set of requirements for the product and its design. It is essential for exploring the realm of physical and technical possibilities, identifying domain-specific prerequisites, and setting the stage for defining a core set of product requirements. As the process transitions into subsequent conceptualization in Phase 1, these initial ideas are further refined.\nTransitioning into \u201cConcept Development\u201d, the focus shifts to the design conception, where the visualization and iteration of product design concepts become paramount. The objective is to converge opposing technical and design demands into a meaningful design. This requires numerous iterations and adjustments of the concept representation and therefore creates a significant bottleneck in the overall product development process.\nThe phase involves a meticulous process of iterating on the basic characteristics of the product design, considering various requirements including customer needs, technological capabilities, and physical constraints. The objective of the concept development is not merely to refine the ideas generated during the ideation phase but to explore and compare alternative solutions systematically. Each iteration serves as an opportunity to reassess and refine the product concept against the backdrop of evolving requirements and constraints. During the design conception phase, the visualization techniques play a crucial role in materializing abstract ideas into tangible representations. Whether through sketches, digital mock-ups, or physical prototypes, visualization aids in communicating ideas, facilitating discussions, and identifying potential design issues early in the development process. The exploration and comparison of solutions are instrumental in identifying the most viable path forward.\nWhile block illustrations of functional structures, circuit diagrams and flow charts provide helpful illustrations, conceptual design relies on the two primary modalities text descriptions and visual representations. Visualizations of the product concept through sketches, images, and 3D models are vital as they relate to the products aesthetics and performance [39], [1]. In our work, we therefore focus on representations depicting the exterior design of product concepts.\n2.2.2 Product-Design Representations\n2D-Shapes play a vital role in the concept development of a variety of product designs as they are often used as initial, low-level parametric representations for finding feasible solutions. Through rapid ideation, they allow for experimentation with different shapes and are often part of the brainstorming process as they are easy to understand and manipulate. Furthermore, 2D-shapes are key in communication between designers, engineers, and stakeholders to share ideas and feedback. They additionally provide important insights about the feasibility of a geometry in early-stage simulations like aerodynamics, refer to [40], [41]. Albeit their versatility, 2D-shape representations do not provide details about the semantics of the product design or about 3D characteristics, resulting in limited information about geometry and aesthetics.\nImages serve as vital representations for product concepts in many domains. While they are used for structural and topology representations, they provide even more potential when used as natural image representations of the product design concepts, embodying intricate details and significantly contributing to the visual interpretation of product concepts. Key geometric and design aspects can be symbolized in a semantically meaningful manner. However, there are disadvantages to using images in design. For low-level design concepts, the pixel-based representation may lead to designs that are unsuitable or infeasible for further application. The consideration of 3D-features, both for semantic perception and performance assessment, demands intermediary transitions to 3D models since they are not provided in images. The disconnection between images and usable models, such as meshes or CAD models, remains a significant obstacle.\n3D-Objects and geometries are often used as the go-to representation of product design concepts as they are the most information-rich approximation. Meshes are the most common method to represent objects in 3D space. They represent the geometry as a collection of connected vertices, edges and faces either as a surface or as a whole-body. They are utilized for downstream tasks like simulations but are challenging to synthesize, requiring specialized tools and software. Voxelizations depict the 3D equivalent to pixelizations in images and offer a volumetric representation. They can depict high-level and low-level features of the product design. For downstream tasks conversion into meshes is often required. Point-clouds are a collection of points in space representing the spatial distribution of the depicted object. Like Voxels, they often require conversion for downstream tasks."}, {"title": "3 DGM-DRIVEN PRODUCT DESIGN CONCEPTION", "content": "In recent years generative machine learning (ML) methods have made substantial progress in synthesizing complex data representations like text (e.g. ChatGPT [12]), images (e.g. Stable Diffusion [37] and Midjourney [42]) or 3D objects (e.g. [43], [44]). Fueled by these advancements, DGMs are met with increasing interest in the domain of engineering design [13], [14], [45]\u2013[48].\nThe application of DGMs is especially promising in concept development [48] where text and visual representations are the primary modalities [46]. We focus on PDC and therefore are especially interested in visual representations. A DGM-driven process to synthesize product design concepts is represented in FIGURE 2.\n3.1 Potentials of DGMs in Product Design Conception\nThe potential for Al-driven design is not just a theoretical possibility but a practical reality [45]. DGMs are particularly interesting for their ability to rapidly generate a multitude of solutions for a single problem. This marks a paradigm shift from the traditional iterative approach that typically seeks a singular, optimal solution. This attribute of DGMs is especially beneficial in the realms of engineering- and PDC-processes, where the diversity of solutions can significantly enhance creativity and innovation [47].\nThe potential applications and benefits of DGMs in PDC are manifold. [13] highlight several key advantages, including the reduction of costly late-stage design changes and providing critical information to designers and engineers by identifying suitable design spaces. Traditionally, iterative design workflows heavily rely on the domain expertise of skilled engineers, necessitating a substantial investment of time and effort. These workflows are often fractured across multiple specialized teams, leading to inefficiencies through iterative hand-offs. Constraints need to be communicated back and forth among various experts. In contrast, DGMs offer the promise of streamlining this process by producing designs that satisfy constraints earlier in the design lifecycle, thereby facilitating a more cohesive and efficient approach to product development.\nMoreover, DGMs allow for the rapid visualization of concepts and ideas, which can lead to better representations of customer needs and an improved understanding of how basic performance requirements impact the product design. This shift towards a constraint-driven design process enables designers to focus more on product performance and constraints [49], fostering a more exploratory approach to considering a wider range of possible product solutions [50].\n3.2 Open Challenges for DGM-Application in PDC\nWhile DGMs herald a new era of potential in product design generation, their successful integration into user-centric processes reveals fundamental challenges. Despite the evident potential of DGMs to revolutionize conceptual design, their widespread adoption has been relatively slow for various reasons. While there is a general willingness to incorporate and adapt these methods, particularly for text-based applications, hesitations persist. Concerns stem from uncertainty about the outcomes, a lack of knowledge among potential users, and questions regarding the precision of the generated representations [14].\nThe gap between the theoretical promise of DGMs and their practical application in PDC is facilitated by the struggle of current DGMs with tasks requiring advanced spatial reasoning and the solving of complex design problems. This limitation not only undermines the consistency and feasibility of the generated outputs but also diminishes their utility in practical design settings [46]. The prevailing shortcomings of DGMs in this context include their inability to generate robust, reliable, and replicable outputs. This is compounded by a lack of relevant domain knowledge, an unawareness of industry standards, difficulties in integrating with existing workflows, and challenges in interpreting data from diverse sources and formats [13]. Further compounding these issues are the challenges associated with ensuring that generated designs adhere to explicit design constraints, consider design performance, ensure physical feasibility, promote design novelty, and navigate data sparsity [47]."}, {"title": "3.3 Requirements for DGM-Application in PDC", "content": "Promises of DGMs in PDC are underwhelming unless the limitations, outlined in the previous sections, are addressed. Addressing the existing gaps presupposes understanding and acknowledgment of the inherent requirements and obstacles that currently curtail the full exploitation of DGM-capabilities in this domain. As identified by [13], key requirements for DGMs in engineering design include reliability, with models delivering consistent performance; stability, ensuring predictable functioning under a variety of conditions; accuracy, providing outputs that are both precise and controllable; adaptability, allowing for seamless adjustment to new or evolving requirements; and a strict adherence to predefined product specifications and design constraints.\n[51] emphasize that the incorporation of DGMs into complex design problems yields tangible benefits only when the context and interactions between human designers and AI-models are made intuitively accessible. Moreover, [52] advocate for a collaborative approach wherein both humans and autonomous agents explore a diverse set of designs that align with human preferences. This approach seeks to strike a balance between the performance and novelty of solutions, suggesting that the synergy between human intuition and machine efficiency is crucial for early design exploration.\nDrawing from the existing body of research on the capabilities and shortcomings of DGMs and informed by profound theoretical models on the product development process [1], [39], we propose a set of core requirements for the effective incorporation of DGMs into the development of product concepts. These requirements aim to address the highlighted challenges by fostering a more seamless integration of DGMs into the design process, ensuring that the generated outputs are not only innovative and diverse but also practical, feasible, and aligned with the intricate web of constraints and standards that define the field of product design. The focus of DGMs in the PDC should be to enhance human creativity by integrating existing design methods with Gen-AI.\n3.3.1 Conditioning and Controllability\nTo be applied in the generation of product design concepts, DGMs must allow for conditional controllability of the generative process by the user. User input for generative tasks has to be intuitive, easy to edit interactively, and commonly used in the traditional creative process providing modalities to incorporate specifications about the products purpose, distinct visual, functional, and technical features, and performance requirements [53]. Relevant modalities to formulate the objective of the generative task come in the form of text, visual inputs, and performance parameters.\nNatural Language Text is the most intuitive way of human communication and allows users to articulate specific requirements and demands in simple or more complex formulations. It allows for flexible articulation of complex ideas, requirements, and preferences. While natural language can capture high- level concepts and abstract ideas (\"futuristic\", \"inspired by nature\") to assist the synthesis of novel and unconventional content, it requires sophisticated models for processing and is inherently ambiguous, as the same text-prompt can have a different meaning depending on the observer.\nFor visual conditionings, Reference Images provide the possibility of using existing image representations as guidance for the generative process. They allow for the utilization of existing designs and concepts as visual references for type, geometry, style or features of the product design concept and therefore create a starting point for creativity and design exploration.\nSketches and Drawings, depicting contours and cross-sections of the product design, are a crucial part of early-phase design exploration and therefore relevant modality for conditioning. They allow to represent fundamental ideas and features in a low- fidelity format that contains relevant visual or geometric details and is easy to understand and manipulate. Structural Inputs (Edge-, Depth-, Spatial- and Segmentation-Maps) depict spatial and geometrical characteristics like edges, segmented objects, depth fields and bounding boxes to describe the spatial composition of the target representation. They allow for explicit control over the spatial composition and characteristics of the design concept, facilitating the inclusion of spatial information product features.\nPerformance parameters ensure that specific performance criteria are met and help to ensure technical feasibility of the design. They come in the form of numerical values or dimensionless ratios (e.g.: lift coefficient of airfoils, drag coefficient of vehicle silhouettes). Performance parameters may limit the creative exploration if too narrowly defined.\n3.3.2 Consistency\nInconsistencies between the generated output and the users intentions remain an open challenge in DGMs and headwind their application in product and engineering design [13]. They originate from various sources; a prominent example are ambiguities in the conditioning Ensuring consistency in the output generated by DGMs in relation to user inputs and conditions is a critical requirement for their application in PDC.\nFor the generation process to be considered truly controllable, it is imperative that the design concepts depict the main geometric features and design characteristics essential for the concept at hand. User- provided inputs and conditionings, whether they are specific design parameters, aesthetic preferences, or functional criteria, need to be respected by the DGM. Ultimately, the sophistication of the model and the conditioning mechanisms employed are rendered moot if the outputs fail to align with the user's intentions.\nThis alignment is what enables DGMs to serve as effective and efficient tools in the hands of designers, facilitating the creative process by ensuring that the conceptual outputs are consistently in tune with the initial design vision.\n3.3.3 Coherence\nThe probabilistic nature of most current-generation DGMs often contradicts the general requirement of real-world coherence, as the models have learned the probability distribution of the data, not physical laws, as depicted in Figure 4.\nCoherence ensures that the generated outputs not only adhere to basic physical principles, showcasing an elementary level of world knowledge, but also align with domain-specific functional requirements. This dual alignment imbues the generated designs with a sense of realism and practical applicability, making them more than mere imaginative explorations, which is essential to be considered seriously within the engineering process and to contribute effectively to the innovation pipeline. To bridge the creativity gap by offering variations and novel designs that are desirable and valuable, it is not enough for generated solutions to merely replicate training data. DGMs need to produce outputs that are both innovative and aligned with real-world constraints and possibilities.\n3.3.4 Customization\nFor DGMs to be truly effective, they must not only generate outputs that are coherent with general physical and technical principles but also embody the nuanced attributes unique to their specific domain. This requirement for customization necessitates the efficient induction of further domain-specific data and knowledge into the models, either through fine-tuning mechanisms or as conditional inputs. This process allows for the better utilization of existing information, ensuring that the generated designs are not just plausible but also aligned with domain-specific requirements. This requirement can be viewed as an additional layer of detail to the need for coherence as it aims to ensure that the models produce results that are both universally plausible and tailored to the unique demands of the domain.\nIn PDC, the ability to use domain-specific inputs and understand their implications is critical as specifications are often subject to specific sets of conventions [13], [46]. Generative models must be adaptable enough to interpret these inputs correctly and generate outputs that conform to these specialized communication forms.\n3.3.5 Data and Computational Costs\nThe proprietary nature of most engineering design data, coupled with strict intellectual property rules [13], poses a significant challenge in aggregating the amounts of data typically required for training DGMs. The question of computational cost and data efficiency is not just a technical hurdle but a critical factor in the broader applicability of DGMs in domain-specific PDC. In many instances, the feasibility of training a DGM on millions of data points, utilizing hundreds of hours on high-end GPUs, is simply not practical within the constraints of industry-specific applications. Domain-specific applications often grapple with limitations not just in terms of the volume of training data\u2014frequently only a few thousand data points\u2014but also in the available computational resources, which may extend only to consumer-grade GPUs. This scenario underscores a critical challenge: achieving computational efficiency and managing the substantial costs associated with deploying DGMs that demand extensive data and compute for training.\nAddressing these challenges necessitates solutions, such as the potential to fine-tune pre-trained models or the development of small-scale adapters capable of learning from modest amounts of highly specific data. Such approaches enable the customization of large, general-purpose models to generate high-quality, domain-specific representations with a fraction of the data and computational power typically required."}, {"title": "4 ANALYSIS OF DGM-APPLICABILITY IN PRODUCT DESIGN CONCEPTION", "content": "In this chapter, we conduct a technical analysis of DGMs to assess their applicability and potential utility in PDC. We direct readers more interested in the implications of this analysis on the applicability in PDC to Chapter 5.\nFocusing on Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs), Diffusion Models, Transformers, and Radiance Field Methods, we scrutinize these models through the lens of the five requirements identified in the preceding chapter: Controllability, Consistency, Coherence, Customization, and Cost (Data and Compute). Our objective is twofold: firstly, to delineate the strengths and weaknesses of each model family in meeting these requirements, thereby guiding engineers and designers in selecting the most suitable model family for their specific design tasks; secondly, to identify areas where further research could enhance the applicability and effectiveness of DGMs in engineering and PDC.\n4.1 VAE's\nVAES are characterized by their capability of embedding input data into an interpretable, lower- dimensional latent space [3], facilitating smooth interpolation between points in the learned distribution. Their architecture balances reconstruction accuracy with adherence to the latent space distribution [25]. This capability is particularly beneficial in domains requiring a grasp of the underlying data distribution. They are generally less prone to training instabilities, making them user- friendly for non-experts. Relevant representations where VAE's have been applied are 2D-Shapes [54], [55] and 3D-Objects [56]\u2013[60].\n1. Controllability\nBy nature, VAEs operate by sampling randomly from a learned probability distribution, an approach that is inherently unconditional and thus does not provide direct control over the specifics of the generated content. To imbue VAEs with a degree of controllability, conditional sampling must be explicitly incorporated during the training phase [28]. Achieving highly detailed control over specific features or high- resolution outputs often falls beyond their current capabilities.\nPossible conditioning modalities span a range of inputs, including text, which can be effectively utilized through the integration of models like CLIP [33] as demonstrated in [61] and [60]; visual inputs, such as sketches and images, [59], [56], [62] and performance parameters [63].\n2. Consistency\nThe requirement for consistency between user inputs and the generated outputs in VAEs presents a complex challenge, due to the intrinsic probabilistic nature of these models. This inherent characteristic often stands in contrast to the precise expectations set by conditioning inputs. Incorporating conditioning into the learned probability distribution aims to guide the generative process towards outputs that align more closely with specified parameters. However, the decoding process, which translates latent representations back into comprehensible outputs, is notoriously difficult to control. This difficulty underscores the challenge in enforcing strict constraints within the generative process, often resulting in outputs that diverge from the intended specifications.\n3. Coherence\nThe continuous representation of data in the latent space, while facilitating smooth interpolations, may inadvertently encompass areas that yield designs and interpolations deemed physically infeasible or technically implausible. Classical-architecture VAEs have tendency to produce blurry outputs in tasks demanding high-resolution detail [55]. Such blurrier results directly impact the model's ability to accurately reflect detailed physical and technical nuances of designs. Some VAE-based approaches aim to generate high-quality variations of product and object designs, but are limited due to the lack of explicit user- conditioning capabilities [64], [65]. To achieve higher quality and more coherent results, adaptations to the model's architecture, such as incorporating adversarial training elements, have been explored with promising outcomes [66].\nTo increase the generalization and coherence of VAE outputs with real-world conditions, novel approaches continue to be investigated [67]. Attempts to address these issues have seen success through the adoption of discrete representations of the data in the latent space [68]. This approach not only scales better but also enhance the model's capacity to manage complex, high-level data such as images, thereby improving coherence with real-world expectations.\n4. Customization\nVAEs' ability to handle medium sizes of data relatively well makes them generally feasible for training from scratch on domain-specific datasets. This feasibility translates to less computational expense compared to training more data-intensive models, offering a practical advantage for customization to specific engineering or design tasks. However, this customization comes with a notable trade-off: a model highly customized for specific domain tasks often experiences a loss in its generalization capabilities. This trade-off is contingent upon several factors, including the complexity of the model itself and the size and format of the data used for training. Tailoring a VAE too closely to a particular set of data or task requirements may limit its applicability across broader or slightly different tasks within the same domain.\n5. Cost (Data and Compute)\nVAES are known for their relatively modest computational requirements compared to other DGMs This efficiency makes VAEs particularly attractive for scenarios with limited computational budgets or where rapid prototyping is necessary. The feasibility of training VAEs on consumer-level hardware underscores their practicality. [64] demonstrated that training a VAE-model to generate domain-specific 3D-Objects could be completed within 3 hours using a Geforce 1080 GPU.\nHowever, it's essential to balance the expectations regarding the quality of the output and the complexity of the task with the computational resources available. While VAEs can be trained on consumer-grade hardware, the resolution and complexity of the generated designs might be constrained by the computational power and the size and quality of the dataset used for training.\n4.2 GAN's\nGenerative Adversarial Network have established themselves as powerful tools to generate complex data structures like images. Their proficiency in producing realistic images surpasses that of VAEs. GANS leverage an adversarial training process, characterized by the dynamic competition between the generator and discriminator [16] that significantly enhances the fidelity of generated designs [17]. This process fosters a continual improvement in the quality of generated outputs and has been extended to general-purpose image generation tasks [18], [20], [69],. GANs have also found various applications in product design. They are used to generate 2D-aerodynamic shapes [70], [41], [71], [72] and structural designs of vehicle wheels [73].\nDue to the adversarial training process, GANs are notoriously difficult to train. Issues such as mode collapse (the model overfits to a subspace of the solution space) and the potential for either the generator or discriminator to become disproportionately powerful, lead to instabilities and high sensitivity on training conditions.\n1. Controllability\nGANs have been adapted to incorporate various conditioning modalities, since a general conditioning methodology was introduced [21]. The modalities range from performance parameters like Mach number and lift coefficient [41] to reference images [73] and textual descriptions [20], [74]\u2013[76]. Image editing tasks have also been covered, allowing for inpainting [19], image synthesis from segmentation and edge maps [77], and point-dragging mechanisms [78].\nNotably, the conditioning in GANs must be incorporated from the onset of the training process, which inevitably introduces increased complexity and potentially higher computational costs.\n2. Consistency\nThe adversarial training process inherent to GANs plays a pivotal role in ensuring consistency between user-provided inputs and the generated content, a feature that often positions GANs favorably in comparison to VAEs. This inherently incentivizes the production of outputs that closely match the conditioning inputs. Mismatches between conditioning inputs and generated images can be further addressed by incorporating a semantic comparison module [75], which maps both the input texts and the generated images into a shared semantic latent space, allowing for a direct comparison to ensure alignment. The introduction of a hybrid attention mechanism in the generator serves to direct the focus of the model on relevant aspects of the input, thereby enhancing the fidelity of the generated content to the specified conditions.\n3. Coherence\nAt their core, GANs are incentivized to remain close to the distribution of their training data. While GANS can produce outputs that exhibit a high degree of coherence within the realms they have been trained on, their ability to generalize and maintain this coherence in out-of-distribution scenarios is inherently constrained. The generation of content that accurately reflects real-world knowledge and feasibility necessitates vast amounts of training data, covering a broad spectrum of scenarios and contexts. The introduction of dynamic Gaussian mixture latents into the GAN generator is designed to increase the diversity and realism of the generated results and allow for a richer variety of outcomes [79]. Vector quantization has also shown to increase quality and realism in GANs [69], especially in combination with text encoding [74].\n4. Customization\nThe necessity to tailor GAN models to particular domains can be addressed with several approaches. Few-shot generation utilizes a small set of reference images as conditioning, enabling the model to generate new images that adhere to the domain-specific characteristics reflected in the limited dataset [80]. Fine-tuning pre-trained GAN models offers another avenue for customization [81], adapting the model to new domains using relatively small amounts of data. In general, the topic of directing the generative capabilities of GANs towards specific domains using limited amounts of data has already been covered by numerous studies. We therefore direct the interested reader to the comprehensive overview by [82].\n5. Cost (Data and Compute)\nIn domain-specific applications, GANs have shown efficiency. [71] achieved training times between 6 and 13 minutes on an NVIDIA Titan X, a consumer-level GPU, with data requirements ranging from 5000 to 10000 datapoints. Similarly, [73] managed to train their model with just 1728 datapoints for a few hours on an Nvidia GTX 1080, and [41] completed their training between 2 and 4 hours on a Nvidia Tesla V100. Contrastingly, the training of general-purpose models demands significantly higher computational resources and data. StyleGAN-T reported training durations of four weeks on 64 Nvidia A100 GPUs- although notably still only a quarter of the resources used by models like Stable Diffusion and required an extensive dataset of 250 million datapoints [20].\nTraining instabilities remain a hurdle across the board, impacting both the efficiency and effectiveness of GANs in practice. We direct those interested in delving deeper into the nuances of GAN training efficiency to the study of [82]."}, {"title": "5.3 Diffusion Models", "content": "Diffusion models have rapidly ascended to the forefront of DGM-based image synthesis"}, {"title": "Exploring the Potentials and Challenges of Deep Generative Models in Product Design Conception", "authors": ["Phillip Mueller", "Lars Mikelsons"], "abstract": "The synthesis of product design concepts stands at the crux of early-phase development processes for technical products, traditionally posing an intricate interdisciplinary challenge. The application of deep learning methods, particularly Deep Generative Models (DGMs), holds the promise of automating and streamlining manual iterations and therefore introducing heightened levels of innovation and efficiency. However, DGMs have yet to be widely adopted into the synthesis of product design concepts. This paper aims to explore the reasons behind this limited application and derive the requirements for successful integration of these technologies. We systematically analyze DGM-families (VAE, GAN, Diffusion, Transformer, Radiance Field), assessing their strengths, weaknesses, and general applicability for product design conception.\nOur objective is to provide insights that simplify the decision-making process for engineers, helping them determine which method might be most effective for their specific challenges. Recognizing the rapid evolution of this field, we hope that our analysis contributes to a fundamental understanding and guides practitioners towards the most promising approaches. This work seeks not only to illuminate current challenges but also to propose potential solutions, thereby offering a clear roadmap for leveraging DGMs in the realm of product design conception.", "sections": [{"title": "INTRODUCTION", "content": "Product design conception (PDC) is an intricate and multifaceted process, demanding considerable effort and investment. During the early-phase, foundational concepts translate functional and feature requirements into preliminary visual and geometric representations [1]. These visualizations are crucial, shaping both the aesthetic appeal and engineering functionality of the final product. The challenge intensifies in consumer products such as passenger vehicles, which must harmonize appealing design with robust functionality to meet both aesthetic desires and practical necessities.\nTraditionally, this process heavily relies on the domain expertise of engineers and designers, necessitating extensive manual iteration across various design modalities. Such iterative and manual processes are not only time-consuming but also require substantial investments, often depending on limited and compartmentalized information that impede innovation and efficiency.\nRecent advances in deep learning, particularly in generative methods, offer a promising path to the goal of reducing manual, time-consuming iterations. Additionally, they bear the potential to elevate human creativity by democratizing the engineering design process through lowering the skill-barrier [2].\nDeep Generative Models (DGMs) are specifically tailored to learn complex data distributions and generate novel samples from the learned distributions [3], [4]. These models, including Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), Diffusion Models, and Transformer-based architectures, have demonstrated significant potential in various fields, including natural language processing [5] and image generation [6], [7] [8]\u2013[10].\nThe landscape of DGM-applications is currently dominated by text-based tasks and the corresponding Large-Language-Models like LLAMA [11] and GPT [12]. Despite their undeniable potential, DGMs have not yet been widely adopted in engineering design and more specifically in PDC. This is due to several challenges that complicate their integration into existing workflows. Firstly, product and engineering design tasks often require precise domain-specific knowledge, which is difficult to encapsulate within the frameworks of DGMs. These tasks rely on specific modalities, like requirement tables, hand-drawn sketches, technical drawings and low-fidelity images to translate ideas and constraints into technical product representations. Many of the modalities are challenging for DGMs to accommodate. As a result, the outputs from DGMs frequently lack the robustness, reliability, interpretability, and replicability necessary for critical design applications [13], [14].\nSecondly, the inherent complexity of DGMs poses a significant barrier to their adoption by non-expert users, such as engineers, who may find the advanced machine learning concepts and operations daunting. This challenge is compounded by the rapid evolution and diversity of models within the field, which can overwhelm even dedicated specialists trying to keep pace with the latest developments [15]. Selecting a feasible method for a given task typically requires elaborate experimentation on top of existing in-depth knowledge by an expert.\nAdditionally, efficient application and deployment of DGMs within the PDC-process presuppose substantial data and computational resources, further restricting their accessibility and scalability. These models must also effectively integrate multifaceted data that captures human inputs and interactions, a requirement essential for enhancing design quality and tailoring solutions to specific domain challenges. The convergence of these factors contributes to the limited penetration of DGMs in the realm of product design conception.\nThis study aims to provide a comprehensive analysis of the potentials and challenges associated with the application of DGMs in PDC. In an effort to lower the barrier for the application of these models, we will derive fundamental requirements that DGMs must fulfill to be effectively integrated into PDC-processes. These requirements will serve as a basis for evaluating important model families, assessing their suitability for tasks within product design. By examining how these approaches align with the defined requirements, we seek to enhance understanding and assist practitioners in three key areas:\n1. Analyzing Model Suitability: Practitioners will gain a better and more realistic understanding of which applications are realistic with DGMs and where their limitations lie. This will facilitate more informed decisions regarding the selection of appropriate models for solving specific product design problems.\n2. Evaluating Application Cases: By outlining prerequisites and limitations of DGMs concerning PDC-application, we hope to guide practitioners in analyzing their application cases to determine the suitability for incorporating DGMs. This analysis aims to ensure that the deployment of these technologies is both practical and effective, taking into consideration the specific conditions and constraints of the design task at hand.\n3. Outlining Research Directions: As a result of our analysis, we aim to identify areas where further research could enhance the applicability and effectiveness of DGMs.\nBy setting a baseline for the comparison of DGM- capabilities with the needs of the concept design process, we hope to assist in selecting the most appropriate models, setting realistic expectations, and effectively utilizing these technologies in product design workflows.\nWe summarize relevant background about DGM- families in Chapter 2 and provide further references for the interested reader. Chapter 3 discusses the PDC- process, existing potentials and open challenges, from which the key requirements for DGM application are derived. The outlined requirements form the basis for the technical analysis of DGM-families in Chapter 4. To lower the knowledge-barrier and make our work more accessible for non-DGM-experts, we summarize our application recommendations in Chapter 5. Our conclusions in Chapter 6 aim to briefly discuss the three objectives we outlined throughout this chapter."}, {"title": "2 BACKGROUND", "content": "2.1 Deep Generative Models\nGenerative Adversarial Networks (GANs) are a class of models that consist of two competing neural networks \u2013 the generator and the discriminator. GANs have found success in a variety of applications since their initial introduction [16], including image generation [17], [18] and manipulation [19], as well as text-to-image synthesis [20]. While the generator learns to map a vector of latent variables to a desired distribution, thus generating new content, the discriminator learns to distinguish between real and generated content. The weights of both networks are improved independently during training. As the discriminator improves, the generator also improves as it learns to generate content that fools the discriminator. The training of GANs is often considered challenging due to instabilities from vanishing gradients in the generator and the failure of the generator to capture all modes in the data distribution (mode collapse). Nevertheless, GAN- architectures have found great success in a variety of applications, as they allow for the important possibility of conditioning the generative process, meaning the generative process can be guided by user-provided constraints. This significantly increases the potential for application in product design. [21] provide a discretely conditioned GAN (\u2018cGAN'), where the conditioning vector is fed into the generator and the discriminator. There are numerous other approaches on conditioning GANs, we direct the interested reader to refer to 'InfoGAN' [22] and continuous conditional GANS [23].\nVariational Autoencoder (VAEs) belong to the family of probabilistic machine learning models. For Autoencoders, an encoder maps the input data into a lower-dimensional latent representation while the decoder reconstructs the original content as accurately as possible. Variational Autoencoders, introduced by Kingma and Welling [24], add probabilistic sampling in the latent space, regularizing the latent distribution and creating a more continuous mapping of the data distribution to the latent space that allows for sampling of realistic latent vectors. The Kullback-Leibler (KL) divergence loss between the latent distribution and a standard Gaussian is added to obtain a predictable latent space distribution. Interested readers may refer to [25] and [26] for detailed information on VAE- foundations and to [27], [28] for conditional VAEs.\nTransformer models are a deep learning architecture popular in the fields of natural language processing and computer vision. The modern transformer- architecture was initially proposed in the widely known work 'Attention Is All You Need' [5], to which we refer the reader for more detailed information. Transformers rely on the 'attention mechanism', that computes a weighted sum of input values (V) based on their 'attention scores' (relevance) to a specific query (Q) and key (K) pair. This allows to capture global dependencies between input and output in parallel and dynamically prioritize specific parts of the input data. Transformers excel at capturing long-range dependencies and intricate patterns in data. Transformers have been widely adopted for training large language models [29]\u2013[31]. They also are a key element in vision-centric models, to learn visual concepts from natural language, such as in 'ViT' [32] and 'CLIP' [33].\nDiffusion models are a class of generative models that simulate the data generation as a reverse diffusion process. The approach, routed in non-equilibrium thermodynamics, was initially proposed in 2015 [4]. Data structures, like images, are gradually destroyed through the addition of Gaussian noise until there is no information left from the original data. The reverse process is learned by a deep neural network, that transforms a simple distribution (like Gaussian noise) into the complex distribution of the original data. Although requiring large amounts of data and compute, diffusion models have recently found immense success in image generation. We direct the interested reader to the following works: [6], [34]\u2013[36] for fundamentals and [37] as well as [38] for large- scale image generation applications."}, {"title": "2.2 Development of early-phase product design concepts", "content": "2.2.1 Process\nThe early-phase process of PDC is characterized by the definition and subsequent translation of main engineering requirements into functional representations of the product design. In the product- design-process defined by [1], the PDC is localized in the early-phases of \u201cIdeation and Planning\" as well as \"Concept Development\". The phase of planning and ideation is crucial for brainstorming and comparing diverse ideas, facilitating a creative exploration of potential solutions before formalizing the product concept. It serves as the foundation for generating a broad array of ideas, which are then critically evaluated to translate them into an initial set of requirements for the product and its design. It is essential for exploring the realm of physical and technical possibilities, identifying domain-specific prerequisites, and setting the stage for defining a core set of product requirements. As the process transitions into subsequent conceptualization in Phase 1, these initial ideas are further refined.\nTransitioning into \u201cConcept Development\u201d, the focus shifts to the design conception, where the visualization and iteration of product design concepts become paramount. The objective is to converge opposing technical and design demands into a meaningful design. This requires numerous iterations and adjustments of the concept representation and therefore creates a significant bottleneck in the overall product development process.\nThe phase involves a meticulous process of iterating on the basic characteristics of the product design, considering various requirements including customer needs, technological capabilities, and physical constraints. The objective of the concept development is not merely to refine the ideas generated during the ideation phase but to explore and compare alternative solutions systematically. Each iteration serves as an opportunity to reassess and refine the product concept against the backdrop of evolving requirements and constraints. During the design conception phase, the visualization techniques play a crucial role in materializing abstract ideas into tangible representations. Whether through sketches, digital mock-ups, or physical prototypes, visualization aids in communicating ideas, facilitating discussions, and identifying potential design issues early in the development process. The exploration and comparison of solutions are instrumental in identifying the most viable path forward.\nWhile block illustrations of functional structures, circuit diagrams and flow charts provide helpful illustrations, conceptual design relies on the two primary modalities text descriptions and visual representations. Visualizations of the product concept through sketches, images, and 3D models are vital as they relate to the products aesthetics and performance [39], [1]. In our work, we therefore focus on representations depicting the exterior design of product concepts.\n2.2.2 Product-Design Representations\n2D-Shapes play a vital role in the concept development of a variety of product designs as they are often used as initial, low-level parametric representations for finding feasible solutions. Through rapid ideation, they allow for experimentation with different shapes and are often part of the brainstorming process as they are easy to understand and manipulate. Furthermore, 2D-shapes are key in communication between designers, engineers, and stakeholders to share ideas and feedback. They additionally provide important insights about the feasibility of a geometry in early-stage simulations like aerodynamics, refer to [40], [41]. Albeit their versatility, 2D-shape representations do not provide details about the semantics of the product design or about 3D characteristics, resulting in limited information about geometry and aesthetics.\nImages serve as vital representations for product concepts in many domains. While they are used for structural and topology representations, they provide even more potential when used as natural image representations of the product design concepts, embodying intricate details and significantly contributing to the visual interpretation of product concepts. Key geometric and design aspects can be symbolized in a semantically meaningful manner. However, there are disadvantages to using images in design. For low-level design concepts, the pixel-based representation may lead to designs that are unsuitable or infeasible for further application. The consideration of 3D-features, both for semantic perception and performance assessment, demands intermediary transitions to 3D models since they are not provided in images. The disconnection between images and usable models, such as meshes or CAD models, remains a significant obstacle.\n3D-Objects and geometries are often used as the go-to representation of product design concepts as they are the most information-rich approximation. Meshes are the most common method to represent objects in 3D space. They represent the geometry as a collection of connected vertices, edges and faces either as a surface or as a whole-body. They are utilized for downstream tasks like simulations but are challenging to synthesize, requiring specialized tools and software. Voxelizations depict the 3D equivalent to pixelizations in images and offer a volumetric representation. They can depict high-level and low-level features of the product design. For downstream tasks conversion into meshes is often required. Point-clouds are a collection of points in space representing the spatial distribution of the depicted object. Like Voxels, they often require conversion for downstream tasks."}, {"title": "3 DGM-DRIVEN PRODUCT DESIGN CONCEPTION", "content": "In recent years generative machine learning (ML) methods have made substantial progress in synthesizing complex data representations like text (e.g. ChatGPT [12]), images (e.g. Stable Diffusion [37] and Midjourney [42]) or 3D objects (e.g. [43], [44]). Fueled by these advancements, DGMs are met with increasing interest in the domain of engineering design [13], [14], [45]\u2013[48].\nThe application of DGMs is especially promising in concept development [48] where text and visual representations are the primary modalities [46]. We focus on PDC and therefore are especially interested in visual representations. A DGM-driven process to synthesize product design concepts is represented in FIGURE 2.\n3.1 Potentials of DGMs in Product Design Conception\nThe potential for Al-driven design is not just a theoretical possibility but a practical reality [45]. DGMs are particularly interesting for their ability to rapidly generate a multitude of solutions for a single problem. This marks a paradigm shift from the traditional iterative approach that typically seeks a singular, optimal solution. This attribute of DGMs is especially beneficial in the realms of engineering- and PDC-processes, where the diversity of solutions can significantly enhance creativity and innovation [47].\nThe potential applications and benefits of DGMs in PDC are manifold. [13] highlight several key advantages, including the reduction of costly late-stage design changes and providing critical information to designers and engineers by identifying suitable design spaces. Traditionally, iterative design workflows heavily rely on the domain expertise of skilled engineers, necessitating a substantial investment of time and effort. These workflows are often fractured across multiple specialized teams, leading to inefficiencies through iterative hand-offs. Constraints need to be communicated back and forth among various experts. In contrast, DGMs offer the promise of streamlining this process by producing designs that satisfy constraints earlier in the design lifecycle, thereby facilitating a more cohesive and efficient approach to product development.\nMoreover, DGMs allow for the rapid visualization of concepts and ideas, which can lead to better representations of customer needs and an improved understanding of how basic performance requirements impact the product design. This shift towards a constraint-driven design process enables designers to focus more on product performance and constraints [49], fostering a more exploratory approach to considering a wider range of possible product solutions [50].\n3.2 Open Challenges for DGM-Application in PDC\nWhile DGMs herald a new era of potential in product design generation, their successful integration into user-centric processes reveals fundamental challenges. Despite the evident potential of DGMs to revolutionize conceptual design, their widespread adoption has been relatively slow for various reasons. While there is a general willingness to incorporate and adapt these methods, particularly for text-based applications, hesitations persist. Concerns stem from uncertainty about the outcomes, a lack of knowledge among potential users, and questions regarding the precision of the generated representations [14].\nThe gap between the theoretical promise of DGMs and their practical application in PDC is facilitated by the struggle of current DGMs with tasks requiring advanced spatial reasoning and the solving of complex design problems. This limitation not only undermines the consistency and feasibility of the generated outputs but also diminishes their utility in practical design settings [46]. The prevailing shortcomings of DGMs in this context include their inability to generate robust, reliable, and replicable outputs. This is compounded by a lack of relevant domain knowledge, an unawareness of industry standards, difficulties in integrating with existing workflows, and challenges in interpreting data from diverse sources and formats [13]. Further compounding these issues are the challenges associated with ensuring that generated designs adhere to explicit design constraints, consider design performance, ensure physical feasibility, promote design novelty, and navigate data sparsity [47]."}, {"title": "3.3 Requirements for DGM-Application in PDC", "content": "Promises of DGMs in PDC are underwhelming unless the limitations, outlined in the previous sections, are addressed. Addressing the existing gaps presupposes understanding and acknowledgment of the inherent requirements and obstacles that currently curtail the full exploitation of DGM-capabilities in this domain. As identified by [13], key requirements for DGMs in engineering design include reliability, with models delivering consistent performance; stability, ensuring predictable functioning under a variety of conditions; accuracy, providing outputs that are both precise and controllable; adaptability, allowing for seamless adjustment to new or evolving requirements; and a strict adherence to predefined product specifications and design constraints.\n[51] emphasize that the incorporation of DGMs into complex design problems yields tangible benefits only when the context and interactions between human designers and AI-models are made intuitively accessible. Moreover, [52] advocate for a collaborative approach wherein both humans and autonomous agents explore a diverse set of designs that align with human preferences. This approach seeks to strike a balance between the performance and novelty of solutions, suggesting that the synergy between human intuition and machine efficiency is crucial for early design exploration.\nDrawing from the existing body of research on the capabilities and shortcomings of DGMs and informed by profound theoretical models on the product development process [1], [39], we propose a set of core requirements for the effective incorporation of DGMs into the development of product concepts. These requirements aim to address the highlighted challenges by fostering a more seamless integration of DGMs into the design process, ensuring that the generated outputs are not only innovative and diverse but also practical, feasible, and aligned with the intricate web of constraints and standards that define the field of product design. The focus of DGMs in the PDC should be to enhance human creativity by integrating existing design methods with Gen-AI.\n3.3.1 Conditioning and Controllability\nTo be applied in the generation of product design concepts, DGMs must allow for conditional controllability of the generative process by the user. User input for generative tasks has to be intuitive, easy to edit interactively, and commonly used in the traditional creative process providing modalities to incorporate specifications about the products purpose, distinct visual, functional, and technical features, and performance requirements [53]. Relevant modalities to formulate the objective of the generative task come in the form of text, visual inputs, and performance parameters.\nNatural Language Text is the most intuitive way of human communication and allows users to articulate specific requirements and demands in simple or more complex formulations. It allows for flexible articulation of complex ideas, requirements, and preferences. While natural language can capture high- level concepts and abstract ideas (\"futuristic\", \"inspired by nature\") to assist the synthesis of novel and unconventional content, it requires sophisticated models for processing and is inherently ambiguous, as the same text-prompt can have a different meaning depending on the observer.\nFor visual conditionings, Reference Images provide the possibility of using existing image representations as guidance for the generative process. They allow for the utilization of existing designs and concepts as visual references for type, geometry, style or features of the product design concept and therefore create a starting point for creativity and design exploration.\nSketches and Drawings, depicting contours and cross-sections of the product design, are a crucial part of early-phase design exploration and therefore relevant modality for conditioning. They allow to represent fundamental ideas and features in a low- fidelity format that contains relevant visual or geometric details and is easy to understand and manipulate. Structural Inputs (Edge-, Depth-, Spatial- and Segmentation-Maps) depict spatial and geometrical characteristics like edges, segmented objects, depth fields and bounding boxes to describe the spatial composition of the target representation. They allow for explicit control over the spatial composition and characteristics of the design concept, facilitating the inclusion of spatial information product features.\nPerformance parameters ensure that specific performance criteria are met and help to ensure technical feasibility of the design. They come in the form of numerical values or dimensionless ratios (e.g.: lift coefficient of airfoils, drag coefficient of vehicle silhouettes). Performance parameters may limit the creative exploration if too narrowly defined.\n3.3.2 Consistency\nInconsistencies between the generated output and the users intentions remain an open challenge in DGMs and headwind their application in product and engineering design [13]. They originate from various sources; a prominent example are ambiguities in the conditioning Ensuring consistency in the output generated by DGMs in relation to user inputs and conditions is a critical requirement for their application in PDC.\nFor the generation process to be considered truly controllable, it is imperative that the design concepts depict the main geometric features and design characteristics essential for the concept at hand. User- provided inputs and conditionings, whether they are specific design parameters, aesthetic preferences, or functional criteria, need to be respected by the DGM. Ultimately, the sophistication of the model and the conditioning mechanisms employed are rendered moot if the outputs fail to align with the user's intentions.\nThis alignment is what enables DGMs to serve as effective and efficient tools in the hands of designers, facilitating the creative process by ensuring that the conceptual outputs are consistently in tune with the initial design vision.\n3.3.3 Coherence\nThe probabilistic nature of most current-generation DGMs often contradicts the general requirement of real-world coherence, as the models have learned the probability distribution of the data, not physical laws, as depicted in Figure 4.\nCoherence ensures that the generated outputs not only adhere to basic physical principles, showcasing an elementary level of world knowledge, but also align with domain-specific functional requirements. This dual alignment imbues the generated designs with a sense of realism and practical applicability, making them more than mere imaginative explorations, which is essential to be considered seriously within the engineering process and to contribute effectively to the innovation pipeline. To bridge the creativity gap by offering variations and novel designs that are desirable and valuable, it is not enough for generated solutions to merely replicate training data. DGMs need to produce outputs that are both innovative and aligned with real-world constraints and possibilities.\n3.3.4 Customization\nFor DGMs to be truly effective, they must not only generate outputs that are coherent with general physical and technical principles but also embody the nuanced attributes unique to their specific domain. This requirement for customization necessitates the efficient induction of further domain-specific data and knowledge into the models, either through fine-tuning mechanisms or as conditional inputs. This process allows for the better utilization of existing information, ensuring that the generated designs are not just plausible but also aligned with domain-specific requirements. This requirement can be viewed as an additional layer of detail to the need for coherence as it aims to ensure that the models produce results that are both universally plausible and tailored to the unique demands of the domain.\nIn PDC, the ability to use domain-specific inputs and understand their implications is critical as specifications are often subject to specific sets of conventions [13], [46]. Generative models must be adaptable enough to interpret these inputs correctly and generate outputs that conform to these specialized communication forms.\n3.3.5 Data and Computational Costs\nThe proprietary nature of most engineering design data, coupled with strict intellectual property rules [13], poses a significant challenge in aggregating the amounts of data typically required for training DGMs. The question of computational cost and data efficiency is not just a technical hurdle but a critical factor in the broader applicability of DGMs in domain-specific PDC. In many instances, the feasibility of training a DGM on millions of data points, utilizing hundreds of hours on high-end GPUs, is simply not practical within the constraints of industry-specific applications. Domain-specific applications often grapple with limitations not just in terms of the volume of training data\u2014frequently only a few thousand data points\u2014but also in the available computational resources, which may extend only to consumer-grade GPUs. This scenario underscores a critical challenge: achieving computational efficiency and managing the substantial costs associated with deploying DGMs that demand extensive data and compute for training.\nAddressing these challenges necessitates solutions, such as the potential to fine-tune pre-trained models or the development of small-scale adapters capable of learning from modest amounts of highly specific data. Such approaches enable the customization of large, general-purpose models to generate high-quality, domain-specific representations with a fraction of the data and computational power typically required."}, {"title": "4 ANALYSIS OF DGM-APPLICABILITY IN PRODUCT DESIGN CONCEPTION", "content": "In this chapter, we conduct a technical analysis of DGMs to assess their applicability and potential utility in PDC. We direct readers more interested in the implications of this analysis on the applicability in PDC to Chapter 5.\nFocusing on Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs), Diffusion Models, Transformers, and Radiance Field Methods, we scrutinize these models through the lens of the five requirements identified in the preceding chapter: Controllability, Consistency, Coherence, Customization, and Cost (Data and Compute). Our objective is twofold: firstly, to delineate the strengths and weaknesses of each model family in meeting these requirements, thereby guiding engineers and designers in selecting the most suitable model family for their specific design tasks; secondly, to identify areas where further research could enhance the applicability and effectiveness of DGMs in engineering and PDC.\n4.1 VAE's\nVAES are characterized by their capability of embedding input data into an interpretable, lower- dimensional latent space [3], facilitating smooth interpolation between points in the learned distribution. Their architecture balances reconstruction accuracy with adherence to the latent space distribution [25]. This capability is particularly beneficial in domains requiring a grasp of the underlying data distribution. They are generally less prone to training instabilities, making them user- friendly for non-experts. Relevant representations where VAE's have been applied are 2D-Shapes [54], [55] and 3D-Objects [56]\u2013[60].\n1. Controllability\nBy nature, VAEs operate by sampling randomly from a learned probability distribution, an approach that is inherently unconditional and thus does not provide direct control over the specifics of the generated content. To imbue VAEs with a degree of controllability, conditional sampling must be explicitly incorporated during the training phase [28]. Achieving highly detailed control over specific features or high- resolution outputs often falls beyond their current capabilities.\nPossible conditioning modalities span a range of inputs, including text, which can be effectively utilized through the integration of models like CLIP [33] as demonstrated in [61] and [60]; visual inputs, such as sketches and images, [59], [56], [62] and performance parameters [63].\n2. Consistency\nThe requirement for consistency between user inputs and the generated outputs in VAEs presents a complex challenge, due to the intrinsic probabilistic nature of these models. This inherent characteristic often stands in contrast to the precise expectations set by conditioning inputs. Incorporating conditioning into the learned probability distribution aims to guide the generative process towards outputs that align more closely with specified parameters. However, the decoding process, which translates latent representations back into comprehensible outputs, is notoriously difficult to control. This difficulty underscores the challenge in enforcing strict constraints within the generative process, often resulting in outputs that diverge from the intended specifications.\n3. Coherence\nThe continuous representation of data in the latent space, while facilitating smooth interpolations, may inadvertently encompass areas that yield designs and interpolations deemed physically infeasible or technically implausible. Classical-architecture VAEs have tendency to produce blurry outputs in tasks demanding high-resolution detail [55]. Such blurrier results directly impact the model's ability to accurately reflect detailed physical and technical nuances of designs. Some VAE-based approaches aim to generate high-quality variations of product and object designs, but are limited due to the lack of explicit user- conditioning capabilities [64], [65]. To achieve higher quality and more coherent results, adaptations to the model's architecture, such as incorporating adversarial training elements, have been explored with promising outcomes [66].\nTo increase the generalization and coherence of VAE outputs with real-world conditions, novel approaches continue to be investigated [67]. Attempts to address these issues have seen success through the adoption of discrete representations of the data in the latent space [68]. This approach not only scales better but also enhance the model's capacity to manage complex, high-level data such as images, thereby improving coherence with real-world expectations.\n4. Customization\nVAEs' ability to handle medium sizes of data relatively well makes them generally feasible for training from scratch on domain-specific datasets. This feasibility translates to less computational expense compared to training more data-intensive models, offering a practical advantage for customization to specific engineering or design tasks. However, this customization comes with a notable trade-off: a model highly customized for specific domain tasks often experiences a loss in its generalization capabilities. This trade-off is contingent upon several factors, including the complexity of the model itself and the size and format of the data used for training. Tailoring a VAE too closely to a particular set of data or task requirements may limit its applicability across broader or slightly different tasks within the same domain.\n5. Cost (Data and Compute)\nVAES are known for their relatively modest computational requirements compared to other DGMs This efficiency makes VAEs particularly attractive for scenarios with limited computational budgets or where rapid prototyping is necessary. The feasibility of training VAEs on consumer-level hardware underscores their practicality. [64] demonstrated that training a VAE-model to generate domain-specific 3D-Objects could be completed within 3 hours using a Geforce 1080 GPU.\nHowever, it's essential to balance the expectations regarding the quality of the output and the complexity of the task with the computational resources available. While VAEs can be trained on consumer-grade hardware, the resolution and complexity of the generated designs might be constrained by the computational power and the size and quality of the dataset used for training.\n4.2 GAN's\nGenerative Adversarial Network have established themselves as powerful tools to generate complex data structures like images. Their proficiency in producing realistic images surpasses that of VAEs. GANS leverage an adversarial training process, characterized by the dynamic competition between the generator and discriminator [16] that significantly enhances the fidelity of generated designs [17]. This process fosters a continual improvement in the quality of generated outputs and has been extended to general-purpose image generation tasks [18], [20], [69],. GANs have also found various applications in product design. They are used to generate 2D-aerodynamic shapes [70], [41], [71], [72] and structural designs of vehicle wheels [73].\nDue to the adversarial training process, GANs are notoriously difficult to train. Issues such as mode collapse (the model overfits to a subspace of the solution space) and the potential for either the generator or discriminator to become disproportionately powerful, lead to instabilities and high sensitivity on training conditions.\n1. Controllability\nGANs have been adapted to incorporate various conditioning modalities, since a general conditioning methodology was introduced [21]. The modalities range from performance parameters like Mach number and lift coefficient [41] to reference images [73] and textual descriptions [20], [74]\u2013[76]. Image editing tasks have also been covered, allowing for inpainting [19], image synthesis from segmentation and edge maps [77], and point-dragging mechanisms [78].\nNotably, the conditioning in GANs must be incorporated from the onset of the training process, which inevitably introduces increased complexity and potentially higher computational costs.\n2. Consistency\nThe adversarial training process inherent to GANs plays a pivotal role in ensuring consistency between user-provided inputs and the generated content, a feature that often positions GANs favorably in comparison to VAEs. This inherently incentivizes the production of outputs that closely match the conditioning inputs. Mismatches between conditioning inputs and generated images can be further addressed by incorporating a semantic comparison module [75], which maps both the input texts and the generated images into a shared semantic latent space, allowing for a direct comparison to ensure alignment. The introduction of a hybrid attention mechanism in the generator serves to direct the focus of the model on relevant aspects of the input, thereby enhancing the fidelity of the generated content to the specified conditions.\n3. Coherence\nAt their core, GANs are incentivized to remain close to the distribution of their training data. While GANS can produce outputs that exhibit a high degree of coherence within the realms they have been trained on, their ability to generalize and maintain this coherence in out-of-distribution scenarios is inherently constrained. The generation of content that accurately reflects real-world knowledge and feasibility necessitates vast amounts of training data, covering a broad spectrum of scenarios and contexts. The introduction of dynamic Gaussian mixture latents into the GAN generator is designed to increase the diversity and realism of the generated results and allow for a richer variety of outcomes [79]. Vector quantization has also shown to increase quality and realism in GANs [69], especially in combination with text encoding [74].\n4. Customization\nThe necessity to tailor GAN models to particular domains can be addressed with several approaches. Few-shot generation utilizes a small set of reference images as conditioning, enabling the model to generate new images that adhere to the domain-specific characteristics reflected in the limited dataset [80]. Fine-tuning pre-trained GAN models offers another avenue for customization [81], adapting the model to new domains using relatively small amounts of data. In general, the topic of directing the generative capabilities of GANs towards specific domains using limited amounts of data has already been covered by numerous studies. We therefore direct the interested reader to the comprehensive overview by [82].\n5. Cost (Data and Compute)\nIn domain-specific applications, GANs have shown efficiency. [71] achieved training times between 6 and 13 minutes on an NVIDIA Titan X, a consumer-level GPU, with data requirements ranging from 5000 to 10000 datapoints. Similarly, [73] managed to train their model with just 1728 datapoints for a few hours on an Nvidia GTX 1080, and [41] completed their training between 2 and 4 hours on a Nvidia Tesla V100. Contrastingly, the training of general-purpose models demands significantly higher computational resources and data. StyleGAN-T reported training durations of four weeks on 64 Nvidia A100 GPUs- although notably still only a quarter of the resources used by models like Stable Diffusion and required an extensive dataset of 250 million datapoints [20].\nTraining instabilities remain a hurdle across the board, impacting both the efficiency and effectiveness of GANs in practice. We direct those interested in delving deeper into the nuances of GAN training efficiency to the study of [82]."}, {"title": "5.3 Diffusion Models", "content": "Diffusion models have rapidly ascended to the forefront of DGM-based image synthesis, achieving unparalleled success in producing high-quality results that outperform GANs across numerous benchmarks [83", "42": "Stable Diffusion [37", "84": ".", "85": [87], "44": [88], "89": ".", "90": "which we strongly"}]}]}