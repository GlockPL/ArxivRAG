{"title": "SegXAL: Explainable Active Learning for Semantic Segmentation in Driving Scene Scenarios", "authors": ["Sriram Mandalika", "Athira Nambiar"], "abstract": "Most of the sophisticated AI models utilize huge amounts of annotated data and heavy training to achieve high-end performance. However, there are certain challenges that hinders the deployment of AI models \"in-the-wild\" scenarios i.e. inefficient use of unlabeled data, lack of incorporation of human expertise and lack of interpretation of the results. To mitigate these challenges, we propose a novel Explainable Active Learning (XAL) model viz. 'XAL-based semantic segmentation model \"SegXAL\", that can (i) effectively utilize the unlabeled data, (ii) facilitate the \"Human-in-the-loop\" paradigm and (iii) augment the model decisions in an interpretable way. In particular, we investigate the application of the SegXAL model for semantic segmentation in driving scene scenarios. The SegXAL model proposes the image regions that require labelling assistance from Oracle by dint of explainable AI (XA\u0399) and uncertainty measures in a weakly-supervised manner. Specifically, we propose a novel Proximity-aware Explainable-AI (PAE) module and Entropy-based Uncertainty (EBU) module to get an Explainable Error Mask, which enables the machine teachers/human experts to provide intuitive reasoning behind the results and to solicit feedback to the AI system, via an active learning strategy. Such a mechanism bridges the semantic gap between man and machine through collaborative intelligence, where humans and AI actively enhance each other's complementary strengths. A novel high-confidence sample selection technique based on the DICE similarity coefficient is also presented within the SegXAL framework. Extensive quantitative and qualitative analyses are carried out in the benchmarking Cityscape dataset. Results show the outperformance of our proposed SegXAL against other state-of-the-art models.", "sections": [{"title": "Introduction", "content": "Over the past decade, the world has witnessed an unprecedented technological revolution with the help of Artificial Intelligence (AI) towards accelerating automation, improving decision-making processes, and extracting insights from vast datasets. Despite these advancements, deep learning models commonly encounter substantial challenges while deploying in real-world or \u201cin-the-wild\" settings, such as limitation of well-annotated data, contextual & prior information and interpretability of the results [1].\nAnnotation of new data points is an expensive and laborious task, yet crucial for enriching training datasets with valuable information. In tasks like image semantic segmentation, manually labelling each pixel with its class label is arduous. Supervised algorithms provide efficient solutions for this task, whereas in unsupervised scenarios, automatic labelling poses a significant challenge for machines. Furthermore, integrating prior and contextual information can significantly enhance AI model performance, especially in high-risk scenarios e.g. medical and defence. Domain experts can contribute valuable knowledge to AI systems in such situations, enabling a \"Human-in-the-loop\" paradigm for more rational analysis and informative results. However, most existing AI systems lack mechanisms to incorporate additional human-collected information or domain expertise. In real-world scenarios, the inverse situation also exists, wherein the operators often have to rely on visual inspection to make decisions due to the lack of explainability in machine decisions. Despite the advancements in deep neural networks, the integration of AI tools in various fields is hindered by the opacity of these \"black-box\" models, which fail to provide explanations for their actions. All of these scenarios highlight the semantic gap between human and machine analysis, emphasizing the need for human involvement in decision-making as well as the development of Explainable AI tools towards better interpretability of the model.\nTo mitigate the aforementioned challenges, we propose a novel Explainable Active Learning (XAL) model that combines domain expert assistance and explainable AI (XAI) support within the active learning (AL) paradigm.\nIn particular, we propose a novel XAL based semantic segmentation model \"SegXAL\" for the driving scene scenarios. Active learning facilitates effective training set by iteratively curating the most informative unlabeled data for annotation with the help of human intervention (oracle) accentuating the \"human-in-the-loop\" paradigm [2], [3]. This \"domain expert teaching\" emphasizes productivity and enhances trust in Al systems, especially in low-resource as well as high-risk scenarios. Similarly, the explainability aspect of the SegXAL model enables the \"machine teachers\" (human experts) to obtain intuitive reasoning behind the results and to give solicit feedback to the system [4]. This is inspired by the rationale that humans' cognizance leverages causal and interpretable information to make decisions [5], [6]. Both of these AL and XAI notions within the SegXAL model bridge the semantic gap between man and machine through collaborative intelligence, wherein humans and AI actively enhance each other's complementary strengths.\nThe key component of the SegXAL framework is the Explainable Error Mask (EEM) module that provides intuitive reasoning as well as uncertainty measures for the sample selection. The EEM module internally contains two components"}, {"title": "Methodology: SegXAL - Explainable Active Learning for semantic segmentation", "content": "The Active Learning (AL) protocol ensures that by intelligently selecting instances for labelling, a learning algorithm can achieve good performance with significantly less training data. Formally, it can be expressed as follows: Let $(x, y)$ be an annotated sample from the original labelled dataset $D_L$ and $x_u$\nrepresent an unannotaed sample from a significantly larger pool of unlabeled\ndata, $D_U$. The goal of AL is to iteratively query a subset $D_S$, that contains the\nmost informative $n$ samples $x_1, x_2,...,x_n$ from $D_U$ in an iterative manner, given\n$n$ is the fixed labelling budget.\nIn this work, we present a novel Explainable Active Learning paradigm for\nsemantic segmentation (SegXAL) in driving scene imagery. Refer to Fig. 1 for the\noverall architecture of the SegXAL framework. It contains training of the model,\nprediction of semantic maps, \"Explainable Error Mask\" (EEM) computation,\nannotation, selection mechanism and retraining steps. Each of these steps is\nexplained in detail in the forthcoming subsections:"}, {"title": "Step 1: Semantic Segmentation - Training & Prediction", "content": "We leverage U-Net [32] as the semantic segmentation network for the model training. Typically, any segmentation model such as FCN [31] or DeepLab [33], among others, could also be utilized. U-Net is employed in this pilot study, due to its ability for the precise localization of objects while maintaining a high level of contextual information as well as lower memory consumption. The U-Net model embodies an encoder-decoder framework. The encoder is responsible for the initial feature extraction and dimensionality reduction, by utilizing successive convolutional and pooling layers followed by nonlinear activation functions"}, {"title": "Step 2: Explainable Error mask Module", "content": "The Explainable Error Mask (EEM) module is the key component of our SegXAL framework. In contrast to the vanilla Active learning models that provide uncertainty/ representativeness insights for the annotation, this novel EEM module presents an explainable error mask for the interactive annotation by the oracle. It consists of the following components: i) Entropy-based Uncertainty (EBU), ii) Proximity-aware XAI (PAE) and iii) fusion of PAE and EBU.\nOne of the most important postulations in active learning strategy is to guide\nthe user towards the most relevant areas to annotate, to fix errors. To this\nend, some standard uncertainty measuring techniques such as entropy [34], or\nODIN [35] are exploited in the literature. Following many of the popular AL\npipelines, our EBU module leverages entropy metric to measure the uncertainty/\ndisagreement for the unlabeled data, to obtain the most uncertain data which is\ninformative and worthful ones to be annotated by the oracle.\nEntropy is a measure of uncertainty or information content in a probability\ndistribution [34]. In the context of image segmentation, it is commonly used to\nquantify the uncertainty of pixel-wise predictions across different classes within\na batch of segmented images. Let us denote a batch of segmented images as X\nwith dimensions $[B, C, H, W]$, where $B$ is the batch size, $C$ is the number of\nclasses, $H$ is the height and $W$ is the width of images. Each image in the batch\nconsists of pixel-wise predictions across $C$ classes. The entropy $H(x_{i,j})$ for each\npixel $x_{i,j}$ can be calculated as:\n$H(x_{i,j}) = - \\sum_{c=1}^{C} P(c|x_{i,j}) log_2(P(c|x_{i,j}))$"}, {"title": "ii) Proximity-aware Explainable-AI (PAE) module:", "content": "The high entropy pixels generated by the Entropy-based Uncertainty (EBU) module can be spread across the entire image, making it challenging from an Oracle perspective to determine where to prioritize attention. Consequently, this may lead to missing out of some of the vital regions to be annotated first. For instance, in driving scene imagery with high entropy scores in the sky, vegetation, and vehicles, annotation priority should be given to nearby classes i.e. vehicles, considering safety concerns. We hypothesise that such a proximity awareness can improve the oracle annotation. In addition, uncertainty techniques often lack human interpretability, hindering an intuitive understanding of why certain regions are crucial for annotation.\nBased on the aforesaid rationale, we propose a novel Proximity-aware Explainable-AI (PAE) module to mitigate the priority and interpretability concerns. The PAE module is capable of focusing on the key objects and regions of interest in the proximity regions with the help of an explainability heatmap.\nEither MiDaS or DINOv2 model is leveraged to obtain the given image's relative depth map. MiDaS [10] is a robust monocular depth estimation technique that employs mixed-dataset training to create a robust and generalizable depth es-timation model. Whereas, DINOv2 [11] is a self-supervised vision transformer model that uses a teacher-student architecture to provide object-level feature extraction. Both of the models are capable of providing monocular depth map outputs. By integrating the MiDaS/DINO-v2 patchwise depth map with the raw image using a thresholding mechanism, the proximity coverage will be estimated. This results in a depth-informed or soft attention image. Note that the threshold for generating a depth-informed image varies with each image based on the proximity of the nearest objects. Upon this image, a Gradient-weighted Class Activation Mapping (GradCAM) [12] explainability map is applied to visualize the important objects and regions. GradCAM is a technique for visualizing CNN decisions, highlighting regions crucial for predictions. The mathematical equation for GradCAM activation at spatial position (i, j) for class c i.e. Grad \u2013 CAM; can be summarized as:\n$GradCAM^{c}_{ij} = ReLU(\\frac{1}{Z} \\sum_{k} \\sum_{h} \\sum_{i}  \\frac{\\partial y^c}{\\partial f_k(i, j)})$"}, {"title": "iii) Fusion of PAE and EBU modules:", "content": "The PAE heatmap ProxGradCAM is further fused with EBU uncertainty\nheatmap $H(x_{i,j})$, to obtain the Explainable Error mask $EEM_{i,j}$. Formally,\n$EEM_{i,j} = \u03b1  ProxGradCAM_{i,j} + \u03b2 . H(x_{i,j})$\nwhere \u03b1 and \u03b2 are the weights for the ProxGrad - CAM and H($x_{i,j}$), re-spectively. Albeit we used equal contribution for the weights in this work, it can be made learnable."}, {"title": "Step 3: Oracle for annotation", "content": "Next, we acquire labels for the superpixels/Region of Interest (ROI) selected by\nEEM module, with the help of oracle. In particular, two modes of oracle annota-\ntions are envisaged in this work: machine and human oracle. In the former mode\n(Machine oracle), automatic pixel annotations are simulated by the machine\nitself. We term these annotations as 'pseudolabels'. In the latter mode (Hu-\nman oracle), the reannotations are carried out manually by a domain expert. By\nkeeping the interpretable information of the potential error map obtained from\nEEM as a reference, the annotation process is carried out using tools like La-\nbel Studio2. Specifically, two manual annotation schemes are devised within the\nActive learning framework viz. Manual-M and Manual-D, leveraging MiDaS\nand DINOv2-based explainable error masks, respectively."}, {"title": "Step 4: Thresholding Mechanism for Sample Selection", "content": "After the oracle, the labeled images are fed into the Ranking & Selection module.\nAnalogous to the high-confidence sample selection techniques as in [19], we use\na novel thresholding mechanism to select high-confidence samples to be incorpo-\nrated into the labeled data pool. In particular, a standard evaluation metric i.e.\n'DICE predictor' is utilized to compute the quantitative measure of performance\nof the segmented images. Mathematically, DICE computation can be written as:\nDICE = $\\frac{2 \\times A \\cap B}{A+B}$"}, {"title": "Step 5: Iterative Active Loop for Semantic Segmentation Improvement", "content": "After the Ranking & Selection module, high-confidence segmentation images are added to the labelled data pool DL, as shown in Fig. 1. Based on this updated dataset, the semantic segmentation model retraining will be carried out. This concludes a complete active learning cycle. Further, a new AL cycle will start based on the updated model weights and the unlabeled dataset DU. All the series of steps Semantic map prediction from unlabelled data, EEM computation, Annotation, Ranking & Selection and Retraining are repeated until the labelling budget is reached or all the data is labelled. This iterative AL cycle optimally selects the most informative samples via EEM information and Oracle annotation, enhancing model performance with minimal labelling costs."}, {"title": "Experimental Setup", "content": "Dataset: We evaluate our proposed SegXAL framework on the Cityscapes dataset for semantic segmentation [30]. Cityscape is a large-scale benchmark for urban street scene understanding, at 1024 \u00d7 2048 pixel resolution with 30 classes including road, car, pedestrian, bicycle, traffic sign, and more. The dataset is divided into three subsets: train (2975 images), validation (500 images), and"}, {"title": "Experimental Results", "content": "To verify the effectiveness of our proposed SegXAL framework, various quantita-tive and qualitative analyses are carried out in the Cityscape dataset. The mean Intersection over Union (mIoU) at each AL stage i.e. 10%, 15%, 20%, 25%, 30%, 35%, 40% of the full training set are adopted as the evaluation metric. Every method is run 5 times and the average mIoUs are reported.\nRefer to Table 1 for the per-class IoU and mIoU for each method at the fifth AL cycle, using 40% training data in the Cityscapes dataset. Compared to other popular approaches such as DEAL [7] and Core-set [36], SegXAL is found to be outperforming in overall mIoU (Pseudolabels-63.56; Manual-M -64.37; Manual-D -65.11), as well as on various classes, such as road, building, wall, traffic light, traffic sign, vegetation, terrain, sky, rider, car and truck. Furthermore, between the two modes of oracle annotation i.e. Pseudolabel vs Manual, we observe that the manual mode outperforms with a 0.8% increase against the former, and has a significant boost in class-wise IoUs. We also provide a statistical measure of standard deviation (STD) to give an insight into the variability of the model"}, {"title": "Visualisation results", "content": "To demonstrate the efficacy of our proposed EEM module, we visualize the qualitative results. Referring to Fig 4, the visualization of 5 AL cycles of a sample raw image shown in Fig. 2 are depicted column-wise. The pixel entropy, Explainable Error Mask (EEM) output and the machine annotated pseudolabel-based segmentation results are shown along the first, second and third rows respectively.\nReferring to Fig. 4 (a)-(e), high entropy areas represented in red or orange patches indicate a high degree of variability in pixel values. Conversely, low en-tropy regions, in blue, signify homogenous or less complex segments, where pixel intensities are similar and are in their class boundaries. This entropy map thus serves as a useful visualisation tool to analyse the complexity of the scenes over the loops. Further, Proximity-aware GradCAM-XAI is fused with this entropy mask to obtain an Explainable Error Mask as shown in Fig. 4 (f)-(j)(Refer Sec."}, {"title": "Ablation Study", "content": "i) Impact of Machine based pseudo label annotation vs Manual An-notation/Impact of Pixel level strategy and object level strategy:\nIn this ablation study, we analyse the effect of Machine-based pseudo label reannotation and Manual reannotation. As mentioned earlier, the machine oracle mode leverages pixel-level pseudolabel values for annotation whereas the human oracle employs object-level annotation via Label Studio. We could ob-serve from Table 1, Table 2 and Fig. 5 that both approaches provide superior performance in semantic segmentation. Specifically, the manual annotation out-performs the Pseudolabel annotation (Refer Table 1) and smooth segmentation masks (See Fig. 5). Nevertheless, Machine-based auto labelling is faster and be-stows a promising automated AL solution from a practical perspective compared to manual annotation, wherein a human expert reviews every image and rean-notates."}, {"title": "ii) Impact of Proximity-aware \u03a7\u0391\u0399 EBU and PAE modules :", "content": "To understand the impact of EBU and PAE modules, quantitative ablation stud-ies are carried out. Referring to Table 3, it can be observed that the lack of EBU sub-module within the EEM block results in a mIoU drop of 3.69, 3.94 and 4.02 in Pseudolabel, Manual-M and Manual-D cases, respectively. Its counterpart re-sults in the absence of PAE sub-modules are 3.47, 2.69 and 3.4 respectively. Ad-ditionally, a qualitative study is also conducted to comprehend the visual inter-pretation of Proximity-aware XAI, as depicted in Fig.6. It is observed that PAE outperforms the Vanilla GradCAM[12], which provides insights of the scene by localizing on the key areas semantic classes via saliency heat maps (Refer Fig.6(a, b)). Built on top of this Grad-CAM concept, our Proximity-aware XAI module refines the attention further onto the nearby objects in the proximity regions e.g. nearby vehicles and sidewalks, as shown in Fig.6(c). This PAE enhancement notably fosters safety and transparency in autonomous driving scenarios."}, {"title": "iii) Impact of change in % of data split", "content": "In this ablation study, we investigate the effect of data split on the SegXAL performance. In particular, we perform various splits of 10%, 15%, 20%, 25%, 30%, 35%, and 40% of the dataset for the initial model training. Referring to Fig. 7 showing the fifth AL cycle mIoU result, it can be observed that based on the increase of labelled data from 10% to 40%, there is a significant increase in mIoU for Pseudolabel 51.02 to 63.56, Manual-M 52.29 to 64.37 and Manual-D 52.83 to 65.11."}, {"title": "State-of-the-art Comparison", "content": "We compare SegXAL with other Active Learning-based semantic segmentation approaches that are deployed on the Cityscapes dataset under similar conditions (with 40% training data over 5 AL cycles) i.e. DEAL[7], core-set approach [36], random, entropy [7,27] and QBC [7]. Although another recent study S4AL [27] achieves a competitive result of mIoU 64.80, it is not included in the comparison due to its different setting of 16% training data. Referring to the results as shown in Table 1 and Fig. 7, it can be observed that SegXAL outperforms the state-of-the-art approaches with a significant margin, achieving the best result of 65.11 mIoU with human annotations with DINOv2 depth map (blue-dotted line). It is also observed from Table 1 that, the segmentation performance on the nearby classes such as road (96.98), sidewalk (73.43), wall (73.48) and vehicles such as truck (59.47), rider (39.34) are better or on par with the previously proposed methods. This superior performance could be accredited to the Explainable Er-ror Mask module that facilitates object-level proximity mechanism using \u03a7\u0391\u0399 attention and Entropy metric, which prioritizes the highly informative nearby objects' annotations compared to far away objects such as train, vegetation etc."}, {"title": "Conclusions and Future work", "content": "In this work, we proposed a novel Explainable Active Learning framework viz. SegXAL for semantic segmentation. A pilot study on the application of the SegXAL model for driving scene semantic segmentation is presented in this pa-per. In contrast to most of the existing Active learning methods that annotate using uncertainty information, the proposed model additionally \"explains\" the proximity region of interests and key objects to be prioritized while annotating by the oracle, with the help of a newly proposed Explainable Error Mask (EEM) module. Such XAI heatmap explanations not only improve the segmentation ac-curacy but also bridge the semantic gap that exists between human and machine interpretation. Our SegXAL model outperforms state-of-the-art results. Future improvements can be made by introducing better attention mechanisms such as Vision transformers and extending the applications to other driving datasets and other domains."}]}