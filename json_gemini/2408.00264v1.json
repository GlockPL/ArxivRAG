{"title": "Clover-2: Accurate Inference for Regressive Lightweight Speculative Decoding", "authors": ["Bin Xiao", "Lujun Gui", "Lei Su", "Weipeng Chen"], "abstract": "Large Language Models (LLMs) frequently suffer from inefficiencies, largely attributable to the discord between the requirements of auto-regressive decoding and the architecture of contemporary GPUs. Recently, regressive lightweight speculative decoding has garnered attention for its notable efficiency improvements in text generation tasks. This approach utilizes a lightweight regressive draft model, like a Recurrent Neural Network (RNN) or a single transformer decoder layer, leveraging sequential information to iteratively predict potential tokens. Specifically, RNN draft models are computationally economical but tend to deliver lower accuracy, while attention decoder layer models exhibit the opposite traits. This paper presents Clover-2, an advanced iteration of Clover, an RNN-based draft model designed to achieve comparable accuracy to that of attention decoder layer models while maintaining minimal computational overhead. Clover-2 enhances the model architecture and incorporates knowledge distillation to increase Clover's accuracy and improve overall efficiency. We conducted experiments using the open-source Vicuna 7B and LLaMA3-Instruct 8B models. The results demonstrate that Clover-2 surpasses existing methods across various model architectures, showcasing its efficacy and robustness.", "sections": [{"title": "Introduction", "content": "Generative Large Language Models (LLMs) [25, 1, 7], exemplified by models such as GPT, have significantly transformed the field of artificial intelligence. These models showcase exceptional adaptability, extending their applications from creative writing to engaging in human-like chatbot conversations. Their profound understanding of natural language has enhanced human-computer interactions by automating tasks that require contextual sensitivity. Nonetheless, LLMs encounter efficiency challenges when deployed on GPUs, primarily due to their sequential text generation mech- anism, which involves two distinct phases: prefilling and decoding. The prefilling phase processes the entire input sequence to produce the initial token, whereas the decoding phase generates subsequent tokens iteratively, leveraging the input and previously generated tokens. The decoding phase, characterized by its repeated small-batch token processing cycles, leads to suboptimal utilization of GPU resources. This inefficiency in the decoding process represents a significant bottleneck in leveraging the full potential of these high-capacity models.\nSpeculative decoding [19, 9] is an acceleration technique devised to address the performance con- straints associated with sequential text generation. This approach enhances computational efficiency by generating multiple tokens per step, while maintaining output consistency. The technique involves employing one or more lightweight draft models to predict several subsequent tokens with minimal"}, {"title": "Background", "content": "2.1 Speculative Decoding and Tree Attention\nSpeculative decoding [19, 9] represents a sophisticated technique aimed at expediting the inference process of large language models (LLMs) through the enhanced utilization of hardware computational resources. This method differentiates itself from conventional auto-regressive decoding by concur- rently calculating and generating multiple tokens within each iteration. At the core of speculative decoding resides a speculator component, typically a lightweight model often referred to as the draft model, tasked with predicting multiple subsequent candidate tokens (commonly structured as a tree). In the context of speculative decoding, the principal LLM, known as the target LLM, ingests all candidate tokens concurrently. This critical process is designated as the verification phase, during which the target LLM meticulously filters out any incorrect tokens from the set of speculative predictions. Consequently, speculative inference generates equivalent outputs with a reduced number of decoding steps, thereby significantly enhancing latency efficiency.\nTree Attention [22] is utilized to calculate attention scores for tree-structured candidate tokens in parallel. By applying prefix matching to various speculated sequences, the speculation results are organized into a Token Tree, which is represented as a 2-D matrix (Figure 2). It is important to note that the attention block is the only component within the modern LLM architecture that requires knowledge of sequential dependency. The scoring of tree-structured tokens is a relatively straightforward task and can be achieved by configuring the attention's Causal-Mask to align with the topological matrix. Tree Attention facilitates the integration of multiple speculations with minimal computational overhead, a feature widely implemented in many speculative decoding systems such as [14, 30, 28].\n2.2 Clover Decoding\nClover, a lightweight speculative sampling method to address large batch sizes, introduces three incremental components to leverage sequential knowledge: Regressive Connection, Attention Decoder and Augmenting Block. The Regressive Connection enables sequential dependency from preceding speculated tokens to be considered when a speculator generates the next token. The Attention Decoder is the factual regressive block in Clover, combining the hidden states from the last transformer block and previously speculated token, merging sequential knowledge between pre- speculated tokens and the entire input sentence. The Augmentation Block is an additional transformer or self-attention block appended to the target model and is used to enhance sequence features to improve speculator accuracy."}, {"title": "EAGLE Decoding", "content": "2.3 EAGLE Decoding\nEAGLE (Extrapolation Algorithm for Greater Language-model Efficiency) [20], a state-of-the-art speculative sampling method, is grounded in two key observations: first, autoregression at the feature level is simpler than at the token level, and second, feature sequences exhibit more regularity compared to token sequences. By autoregressively processing features and then deriving tokens using the LM head of the original LLM, EAGLE achieves better performance, as evidenced by a higher speedup ratio. EAGLE incorporates a single transformer decoder layer to the target LLM, ensuring easy deployment in production environments. Experimental evaluations on various models (Vicuna and LLaMA2-chat series) and tasks (multi-turn dialogue, code generation, mathematical reasoning, instruction following) demonstrate that EAGLE significantly enhances generation speed while maintaining output quality. EAGLE's innovative approach of autoregressively processing features and incorporating tokens from one time step ahead effectively mitigates sampling uncertainty, resulting in a substantial acceleration effect."}, {"title": "Clover-2", "content": "3 Clover-2\nFigure 3 illustrates how Clover-2 is seamlessly integrated into existing LLMs as the speculator. Like Clover, Clover-2 incorporates three key functional modules: Regressive Connection Attention Decoder Augmenting Block. However, there are four notable differences: (1) The initial head information extraction is predefined. Clover-2 employs an independent Attention Decoder prior to the Augmenting Block to pre-integrate the hidden states and the output token information of the LLM; (2) An output projector replaces the ResBlock of Medusa with a fully connected layer, the input to this fully connected layer (FC) encompasses the hidden states and previous token embeddings; (3) Clover-2 utilizes a more sophisticated Augment Block to enhance model performance; (4) Clover-2 adopts a knowledge distillation strategy, learning not only the classification output of the LLM but also the hidden states of the LLM output.\n3.1 Information extraction order\nThe Augmenting Block serves as an excellent sequence information extractor. However, in Clover, the input to the Augmenting Block lacks the information from the last token output by the LLM, potentially undermining its effectiveness. To address this, we introduced an Attention Decoder prior to the Augmenting Block to pre-summarize the hidden states and token information. Consequently, the output projection of the first head bypasses the Attention Decoder, instead directly connecting to a fully connected layer."}, {"title": "Attention Decoder output projector", "content": "3.2 Attention Decoder output projector\nIn Clover-2, the Attention Decoder output projector, previously a Medusa [8] ResBlock, is replaced with a fully connected layer. This layer accounts for both the hidden states of the Attention Decoder and the token embeddings, thereby mitigating confusion caused by the inherent uncertainty of the hidden states.\nA minor adjustment to the Attention Decoder involves the removal of the SiLU activation function. Experiments have indicated that this modification does not result in performance improvements, and it is also deemed anomalous for the residual to only accumulate positive values.\nThe pseudocode for the Attention Decoder will be presented in Appendix A.3."}, {"title": "Augmenting Block", "content": "3.3 Augmenting Block\nTo condense the information from the preceding sequence into a hidden state, Clover-2 appends n additional transformer blocks following the first Attention Decoder, thereby augmenting features from the entire input sentence. Incorporating such a comprehensive layer incurs a minimal computational overhead (e.g. approximately 1/Nlayer of inference time), while the accuracy gains from the augmenting block far outweigh the time it consumes. The more layers a model possesses, the smaller the proportion of computational consumption becomes.\nIn EAGLE [20], employing an attention decoder layer as the draft model necessitates running an additional number of head layers of attention decoder layers for each decode process. Clover-2 utilizes a lightweight Attention Decoder, with a computational load approximately 2.5 times lighter than a single layer of EAGLE, enabling the use of a more computationally intensive Augmenting Block. Such an approach is not feasible in EAGLE[20], where any additional operations incur costs that must be multiplied by the number of heads. Clover-2 adopts the simplest method, increasing the number of decoder layers in the Augmenting Block to 2."}, {"title": "Knowledge distillation", "content": "3.4 Knowledge distillation\nDuring the comparative training between Clover and Eagle[20], Clover displayed severe overfitting. Various strategies were tested without any improvement. Eventually, we observed Eagle's regression loss, which was only mentioned in the paper for auxiliary intermediate result learning. Through analysis and experimentation, we discovered that regression loss enables the draft model to focus not only on the probability of output tokens but also to more closely align with the distribution of the LLM. This represents a more profound knowledge distillation strategy, effectively suppressing overfitting and enhancing model performance. Regression loss calculates the L1 loss using the LLM's output hidden states (after normalization) and the hidden states (after normalization) output by the draft model. In Clover-2, we refer to it as regularization loss. Consequently, our loss function was updated as follows:\n$L_{regi} = Smooth\\_L1(LLM hidden states_{i+1}, Draft hidden states_i).$\n$L_{clsi} = CrossEntropy(LLM prob_{i+1}, Draft prob_i).$\n$L = \\sum_{i=0}^{n-1}(L_{cisi} + w\\_reg * L_{regi}) * decay\\_coe f ficient'.$ , where n denotes the number of draft model heads, which is 5 in Clover-2. In the optimal practices of Clover-2, w_reg is set to 10.0 and decay_coe f ficient is set to 0.7."}, {"title": "Other Details", "content": "3.5 Other Details\nFirstly, in Clover, a layer normalization is incorporated prior to the Attention Decoder, whereas in Clover-2, a layer normalization is introduced before the second Attention Decoder. Additionally, a layer normalization is applied before the llm_head, mirroring the configuration of the LLM. Similar to Clover, token embeddings are derived from the transposed matrix of the Ilm head weight.\nSecondly, during the design process of Clover-2, it was observed that the actual training data is not necessarily SFT data for LLM, and even SFT data exhibits distribution differences compared to"}, {"title": "Evaluation", "content": "4 Evaluation\n4.1 Experiment Settings\nModels and baselines Both the EAGLE and Clover-2 approaches are employed on the Vicuna 7B v1.5 [2] and LLAMA3-Instruct 8B models [3] with the number of speculative head is 5. To ensure the fairness of the comparison, the inference engine, tree construction and tree sampling algorithm of EAGLE are used for all scenarios. We also evaluate auto-regressive decoding under the same circumstances.\nDataset and Metrics We employ the SharedGPT dataset, containing 68,000 dialogue iterations, to train both EAGLE and Clover-2. We then evaluate inference performance on Spec-Bench[27], which includes data from MT-bench [34], WMT14 DE-EN (WMT14) [6], CNN/Daily Mail (CN-N/DM) [24], Natural Questions (NQ) [18], GSM8K [11], and DPR [16], representing the tasks of multi-turn conversation, translation, summarization, question answering, mathematical reasoning, and retrieval-augmented generation, respectively. We choose extra generated tokens (i.e. tokens/step) and throughput (i.e. tokens/second) as our main metrics, followed by prior speculative decoding works.\nTraining Both models are trained with all weights frozen in the target model. For EAGLE, the initial weight settings correspond to the configuration given in [20]. While for Clover-2, the initial weight settings correspond to the configuration given in Appendix A.2. We train Clover-2 for 20 epochs (about 4000 steps per epoch), with (\u03b2\u2081 = 0.9, \u03b22 = 0.95) for the AdamW optimizer. The learning rate\u00b9 is set to 1e-3 with linear schedule(warmup-steps=1000, final-min-lr=5e-4).\n4.2 End-to-end Results"}, {"title": "Ablation Study", "content": "4.3 Ablation Study\nIn the ablation study, we gradually add modules according to the experimental timeline to measure the effectiveness of each module compared to Clover. The main metric is the extra generated tokens (i.e., tokens/step). Clover2 has an average improvement of about 30% compared to Clover, with relevant data presented in Table 2. The benefits brought by each module are as follows:\nKnowledge distillation In the comparative experiment between Clover and EAGLE, severe overfitting was observed in Clover. To address this issue, we introduced a regularization loss based on knowledge distillation, which contributed to a 9% performance improvement. The main improvement comes from the later epochs, which continuously enhance the metrics."}, {"title": "Related Works", "content": "5 Related Works\nSince the introduction of speculative decoding for LLMs as outlined in [19, 9], numerous optimization techniques have been developed. The concept of tree attention, as explored in [22], has been widely implemented for the efficient verification of multiple speculations in a single step. Initial research efforts [17, 21, 23, 26, 35, 33, 15, 10] concentrated on enhancing independent draft models. In contrast, later studies [29, 14, 13] focused on draft model architectures that do not require additional training. More contemporary research has delved into the potential advantages of regressive speculators. Zhang et al. [32] employ a Multilayer Perceptron (MLP) layer as a regression block, Hydra [4] and ReDrafter [32] introduce a regressive component based on a Recurrent Neural Network (RNN), Eagle [20] incorporates a transformer decoder layer for speculation, Chimera [31] proposes the utilization of a Trigram Encoder and a Full Context Encoder as sophisticated regressive speculation mechanisms. The main difference in Clover-2 is the use of the Attention Decoder and Augment Block to capture sequential context information, followed by an RNN architecture to output multiple candidate tokens."}, {"title": "Conclusion", "content": "6 Conclusion\nWe present an upgraded version of Clover, named Clover-2, which incorporates four enhancement points(Section 3.1, 3.2, 3.3, 3.4). In tests conducted against the original Clover and the current state-of-the-art EAGLE, Clover-2 not only significantly boosts the performance of Clover but also surpasses EAGLE in terms of hit rate and speed. Relative to Clover, Clover-2 achieves at least 19% increase in speculative tokens per step and an 18% improvement in speed. When compared to EAGLE, a state-of-the-art method, Clover-2 shows a maximum 7.7% more speculative tokens per step and a maximum 9.3% faster speed. These results demonstrate the effectiveness of the implemented improvements."}, {"title": "Appendix", "content": "A Appendix\nA.1 Compressed tree mask"}, {"title": "Model parameter initialization methods", "content": "A.2 Model parameter initialization methods"}, {"title": "The pseudocode of Attention Decoder", "content": "A.3 The pseudocode of Attention Decoder"}]}