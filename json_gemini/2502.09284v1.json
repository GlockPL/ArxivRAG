{"title": "SparQLe: Speech Queries to Text Translation Through LLMs", "authors": ["Amirbek Djanibekov", "Hanan Aldarmaki"], "abstract": "With the growing influence of Large Language Models (LLMs), there is increasing interest in integrating speech representations with them to enable more seamless multi-modal processing and speech understanding. This study introduces a novel approach that leverages self-supervised speech representations in combination with instruction-tuned LLMs for speech-to-text translation. The proposed approach leverages a modality adapter to align extracted speech features with instruction-tuned LLMS using English-language data. Our experiments demonstrate that this method effectively preserves the semantic content of the input speech and serves as an effective bridge between self-supervised speech models and instruction-tuned LLMs, offering a promising solution for various speech understanding applications.", "sections": [{"title": "1 Introduction", "content": "Progress in speech processing has been accelerated by introducing self-supervised learning (SSL) methods that utilize large unlabeled speech data, which established new benchmarks in the field (Xu et al., 2021; Hsu et al., 2021; Zeghidour et al., 2021). Continuous representations and/or discrete units derived from self-supervised models have been used to compresses the size of speech data and improve performance in downstream tasks, including speech recognition (Baevski et al., 2020), speech synthesis (Ren et al., 2020; Wang et al., 2023b), speech translation (Inaguma et al., 2020) and speech understanding (Wang et al., 2020). Progress in text processing has also been accelerated by the emergence of pre-trained Large Language Models (LLMs), which enabled new applications such as few-shot/zero-shot language processing (Radford et al., 2019; Brown et al., 2020; Touvron et al., 2023; Bai et al., 2023) and multi-modal processing (Tsimpoukelli et al., 2021; Radford et al., 2021). Recent efforts in speech understanding explored the possibility of incorporating speech representations directly into LLMs (Zhang et al., 2023; Wang et al., 2023c; Das et al., 2024; Fang et al., 2024). By incorporating speech data, these models can greatly enhance their contextual grasp, providing a deeper and more thorough perspective of their environment. Multi-modal speech-language models signify a shift in both speech and natural language processing, merging both modalities for a more encompassing and integrated representation of human communication.\nIn this short paper, we propose a novel efficient model for aligning speech features with LLMs, leveraging only self-supervised learning (SSL) features and a modality adapter. We align the features with LLM input space using only English data and a a small portion of translated text, then demonstrate the generalization of translation performance across both seen and unseen target languages. We call our approach SparQLe 1. SparQLe is inspired by Querying Transformer modules used in vision language models to bootstrap vision-language representations from frozen image encoders (Li et al., 2023). We demonstrate through speech translation that SparQLe enables the integration of existing pre-trained speech encoders and LLMs without the need for updating the parameters of either speech encoder or LLM. To summarize our contribution, we develop a novel efficient method to query instruction-tuned LLMs using speech input. In contrast to previously explored speech-LLM integration approaches, our method is the first to utilize frozen SSL speech representations, without relying on large pre-trained ASR models like Whisper. We experimentally demonstrate the effectiveness of this relatively simple approach and release both the pre-trained and fine-tuned models 2."}, {"title": "2 Related Works", "content": "The availability of instruction-tuned LLMs (AI@Meta, 2024; Touvron et al., 2023; Jiang et al., 2023) open a new research direction for speech processing by connecting speech directly to these multi-task models. Chen et al. (2023b) proposed multitask speech-language modeling with unified LLM framework that shows in-context learning ability. Yu et al. (2023) utilized three approaches for adapting speech to text modality: Fully Connected Linear Layers following (Houlsby et al., 2019) adapter method, multi-head cross attention mechanism described in (Vaswani et al., 2017), and query transformer (Li et al., 2023). For processing speech input, they utilized two models: Whisper Large-v2 (Radford et al., 2023) and BEATS (Chen et al., 2023a). The SpeechVerse (Das et al., 2024) framework used WavLM-based speech encoder interfaced with a Flan-T5-XL (Chung et al., 2024) language model. In a another study, Ma et al. (2024) demonstrated the sufficiency of a single linear layer for speech-LLM integration in ASR, albeit with limited exploration beyond this task. Speech as language modeling was also studied in SpeechGPT (Zhang et al., 2023) which integrates both speech and text modalities. The model incorporates a speech tokenizer that converts raw audio waveforms into discrete speech tokens, enabling efficient processing within the transformer architecture. Through multi-task fine-tuning on downstream tasks such as ASR, translation, and generation, the model demonstrates remarkable versatility. Qwen2-Audio (Chu et al., 2024), designed as a general-purpose audio understanding model, exhibits broad applicability across various audio-related tasks. The model employs self-supervised learning techniques, such as masked audio modeling and contrastive learning, to capture rich audio representations."}, {"title": "3 Model", "content": "SparQLe is a parameter efficient model designed to extract information from speech representation and route them to query pre-trained open-sourced LLMs, without modifications to the underlying speech encoder or LLM. Motivated by the success of multi-modal representations in vision language modeling (Li et al., 2023), we propose the adoption of speech representations to LLMs for generative tasks, specifically Automatic Speech Translation (AST). We pre-trained our model using English data first and fine-tuned with mix of English and French.\nFor the first version of our SprQLe project, we used HuBERT (Hsu et al., 2021) as the speech encoder. The output from its final hidden layer is fed into the query adapter.  Figure 1 shows the overall system structure, which consists of three main parts: a pre-trained speech encoder, a bridging mechanism (SparQLe), and a text generator. In"}, {"title": "3.1 Pre-Training", "content": "This stage is akin to ASR training, where we utilize transcribed speech for supervised training. However, we do not introduce additional parameters and instead use the same modality adapter as an auto-regressive language model: each output vector from the Q-Former is successively fed into a modality adapter to predict the next token. We only update the parameters of the adapter, and keep the underlying speech encoder frozen. The process is depicted in Figure 2. In addition to the text generation task, we use various modality alignment objectives to account for speech in the input and aligned text-like features in the output, similar to image-text alignment done in Li et al. (2023): Speech-text contrastive learning aligns speech and text representation such that mutual information is maximized. This is achieved through contrasting speech-text cosine similarity of positive against negative pairs. Speech-text matching loss aligns representations of speech and text via a binary classification task. Speech text generation loss trains the model to produce text based on the given audio."}, {"title": "3.2 Fine-tuning", "content": "After pre-training, we fine-tune the adapter on downstream tasks using a instruction-tuned LLM, specifically LLama3. We utilize the extracted query tokens as the input to the LLM and update the adapter parameter using cross entropy loss derived from the LLM's objective. For instruction tuning, a frozen Large Language Model was fed with a randomly selected pool of prompts, which were designed to define the translation task. Subsequently, the instruction-tuned model was employed in a chat-based format to collect predictions."}, {"title": "4 Experiments", "content": "To train and evaluate the model on Automatic Speech Translation (AST) task, MuST-C (Di Gangi et al., 2019) and LibriSpeech (Panayotov et al., 2015) datasets are employed. Specifically, French and German languages are selected from MuST-C. We normalized the text from across datasets by converting all letters to lowercase and eliminating punctuation marks. Furthermore, the MuST-C includes action descriptions within the audio samples, such as \"<|speech|> (applause) <|speech|>'. It signifies an auditory sequence where spoken content is interspersed with audience applause. We opted to remove these actions from the translation text."}, {"title": "4.1 Datasets", "content": "To train and evaluate the model on Automatic Speech Translation (AST) task, MuST-C (Di Gangi et al., 2019) and LibriSpeech (Panayotov et al., 2015) datasets are employed. Specifically, French and German languages are selected from MuST-C. We normalized the text from across datasets by converting all letters to lowercase and eliminating punctuation marks. Furthermore, the MuST-C includes action descriptions within the audio samples, such as \"<|speech|> (applause) <|speech|>'. It signifies an auditory sequence where spoken content is interspersed with audience applause. We opted to remove these actions from the translation text."}, {"title": "4.2 Pre-Training", "content": "For feed-forward networks and self-attention of the modality adapter, we employ a 12-layer transformer-based UniLM (Dong et al., 2019); cross-attention is initiated randomly. For pre-training experiment, we trained the model using Adam optimizer, coupled with a cosine annealing learning rate scheduler during the pre-training. The learning rate was initiated at 1 \u00d7 10-4 and gradually reduced to 1 \u00d7 10-5, incorporating a warm-up phase at 1 \u00d7 10-6. The maximum length for speech samples was capped at 480K frames, which is equivalent to 30 s of audio. We used 100 learnable query tokens for the Q-Former.\nOur experiments were conducted using open-source library for language-vision intelligence, LAVIS5. The training processes were executed on one RTX4090 GPU with 24G memory being used over a period of two-three weeks with batch size equal to 8."}, {"title": "4.2.1 Experimental settings", "content": "For feed-forward networks and self-attention of the modality adapter, we employ a 12-layer transformer-based UniLM (Dong et al., 2019); cross-attention is initiated randomly. For pre-training experiment, we trained the model using Adam optimizer, coupled with a cosine annealing learning rate scheduler during the pre-training. The learning rate was initiated at 1 \u00d7 10-4 and gradually reduced to 1 \u00d7 10-5, incorporating a warm-up phase at 1 \u00d7 10-6. The maximum length for speech samples was capped at 480K frames, which is equivalent to 30 s of audio. We used 100 learnable query tokens for the Q-Former.\nOur experiments were conducted using open-source library for language-vision intelligence, LAVIS5. The training processes were executed on one RTX4090 GPU with 24G memory being used over a period of two-three weeks with batch size equal to 8."}, {"title": "4.3 Fine-Tuning", "content": "We used 960 hours of audio from the LibriSpeech dataset, along with an additional 457 \u00d7 2 hours of audio samples from MuST-C that included both translation and transcription tasks: 70% of the speech samples for fine-tuning were used for recognition, while the remaining 30% involve English-to-French translation. We deliberately restricted"}, {"title": "4.3.1 Experimental Settings", "content": "We used 960 hours of audio from the LibriSpeech dataset, along with an additional 457 \u00d7 2 hours of audio samples from MuST-C that included both translation and transcription tasks: 70% of the speech samples for fine-tuning were used for recognition, while the remaining 30% involve English-to-French translation. We deliberately restricted"}, {"title": "4.3.2 Results & Analysis", "content": "We benchmarked translation against the IWSLT challenge baselines for speech-to-text translation using BERTScore as reported in (Anastasopoulos et al., 2022). The results for English-German translation are zero-shot since the model is only fine-tuned with English-French speech translation data. Evaluating LLM answers for speech translation is a challenging task, primarily due to the presence of chat-specific artifacts in the output, such as prompt repetition, follow-up comments, and connecting phrases (e.g., \"here is the transcribed text: \"). To address this issue, we implemented a post-hoc approach in which we endeavored to eliminate instances of prompt recurrence (chat artifacts) in the final text. The results in  highlight the generalization potential of the model in translation. Specifically, the BERTScore for the tst-COMMON split in the French language demonstrates that our system has surpassed both the WEAKBASELINE and STRONGBASELINE in terms of semantic similarity. Furthermore, evaluations on the tst-COMMON split for the German language show that the performance quality extends to languages not included in the training set. This success can be attributed to the inherent translation performance"}, {"title": "5 Discussion", "content": "We introduced a framework for efficient routing of SSL speech features to query LLMs, and demonstrated its effectiveness in speech translation tasks. The results indicate that the proposed model and training paradigm result in generalized performance and avoid instruction over-fitting; the model was able to adhere to instructions for translating speech into multiple target languages (see Figure 3). SparQLe demonstrates ability to translate speech input into diverse languages not encountered during our fine-tuning stage, such as German, Russian, Arabic, etc. Finally, with the appropriate prompts, the instruction-tuned model is capable of performing multiple tasks in a single run, See Figure 4 in Appendix B. The performance in speech translation show promising results, where the proposed approach outperformed both weak and strong baselines from Anastasopoulos et al. (2022) in both French and German."}, {"title": "6 Conclusion & Future Work", "content": "In this short paper, we demonstrate the performance of the proposed SparQLe model, an aligned speech-to-text model based on SSL features, for speech translation applications. What we have demonstrated in this study is only a subset of potential applications of this method. The SparQLe model can conceptually handle both text and speech modalities, and potentially can be applied for any speech-to-text applications. As demonstrated, our model outperforms existing speech translation baselines from IWSLT challenge, which demonstrates the potential of transferring the inherent capacities of LLMs into speech tasks using a parameter-efficient approach. Future work will explore the generalization of the model to other languages and speech understanding tasks and analyze the characteristics of the resulting queries."}, {"title": "Limitations", "content": "Our model was initially pre-trained to align specifically with English speech samples, disregarding other rich languages that present unique challenges. While we believe SparQLe has the potential to handle various tasks beyond its original training scope, we have not yet carried out a formal assessment to verify this capability. Although our model is adaptable to multiple LLM we have not explored it yet.\nApart from HuBERT as speech encoder other encoders were not used. For translation evaluation we used BERTScore, which measures semantic similarity for generation tasks, but all automatic translation metrics have limitations. For example, sentences \"never had any act seemed so impossible\" and \"always had any act seemed so impossible\" convey different information but are similar in words. BERTScore outputs that these two sentences have a high similarity score, which is, in fact, not true (99.7% in F1 score)."}, {"title": "A Instruction Tuning Prompts", "content": "We derived instruction prompts from SALMONN (Tang et al., 2023). As demonstrated, each prompt includes a placeholder for speech <Speech><SpeechQuery></Speech>, into which we insert query-extracted embeddings as inputs to the LLM. Please note that the query embeddings are placed inside the placeholder denoted by <SpeechQuery>. Here is the prompts that we used for training:\n\u2022 <Speech><SpeechQuery></Speech> Can you translate the speech into Language?\n\u2022 <Speech><SpeechQuery></Speech> Please translate the speech you heard into Language.\n\u2022 <Speech><SpeechQuery></Speech> Listen to the speech and translate it into Language.\n\u2022 <Speech><SpeechQuery></Speech> Give me the Language translation of this Language."}, {"title": "B Multi task Discussion", "content": "As mentioned before with the appropriate prompts, the instruction-tuned model is capable of performing multiple tasks in a single run, See Figure 4. While we have not conducted an exhaustive analysis of this particular aspect in the current study, its potential implications for efficiency and versatility in language model applications are substantial."}]}