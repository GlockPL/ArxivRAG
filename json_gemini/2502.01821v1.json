{"title": "Agentic Bug Reproduction for Effective Automated Program Repair at Google", "authors": ["Runxiang Cheng", "Michele Tufano", "J\u00fcrgen Cito", "Jos\u00e9 Cambronero", "Pat Rondon", "Renyao Wei", "Aaron Sun", "Satish Chandra"], "abstract": "Bug reports often lack sufficient detail for developers to reproduce\nand fix the underlying defects. Bug Reproduction Tests (BRTs),\ntests that fail when the bug is present and pass when it has been\nresolved, are crucial for debugging, but they are rarely included in\nbug reports, both in open-source and in industrial settings. Thus,\nautomatically generating BRTs from bug reports has the potential\nto accelerate the debugging process and lower time to repair. This\npaper investigates automated BRT generation within an industry\nsetting, specifically at Google, focusing on the challenges of a large-\nscale, proprietary codebase and considering real-world industry\nbugs extracted from Google's internal issue tracker. We adapt and\nevaluate a state-of-the-art BRT generation technique, LIBRO, and\npresent our agent-based approach, BRT Agent, which makes use of\na fine-tuned Large Language Model (LLM) for code editing. Our BRT\nAgent significantly outperforms LIBRO, achieving a 28% plausible\nBRT generation rate, compared to 10% by LIBRO, on 80 human-\nreported bugs from Google's internal issue tracker. We further\ninvestigate the practical value of generated BRTs by integrating\nthem with an Automated Program Repair (APR) system at Google.\nOur results show that providing BRTs to the APR system results\nin 30% more bugs with plausible fixes. Additionally, we introduce\nEnsemble Pass Rate (EPR), a metric which leverages the generated\nBRTs to select the most promising fixes from all fixes generated by\nAPR system. Our evaluation on EPR for Top-K and threshold-based\nfix selections demonstrates promising results and trade-offs. For\nexample, EPR correctly selects a plausible fix from a pool of 20\ncandidates in 70% of cases, based on its top-1 ranking.", "sections": [{"title": "1 INTRODUCTION", "content": "Bug reproduction is a critical part of the software debugging pro-\ncess [6, 16, 20, 22, 23, 26, 38, 40]. The bug reproduction steps of a\nreported bug can often be encapsulated into test(s), which automat-\nically reproduce the bug by demonstrating test failure(s). Such a\ntest is referred to as a Bug Reproduction Test (BRT). More formally,\na BRT should (1) fail in the presence of the bug, demonstrating the\nfaulty behavior; and (2) pass once the bug has been resolved, con-\nfirming the effectiveness of the applied fix patch. For both human\nusers and Automated Program Repair (APR) systems, BRTs can\noffer insights on the root cause of the bug [17, 20, 21, 28], guide the\ndevelopment of a fix patch [27, 30, 40], and is essential for validating\nthe effectiveness of the proposed fix patch [16, 21, 23, 31].\nHowever, despite their importance, BRTs are often absent when\nthe debugging or repairing process starts. Bug reports from hu-\nman users/developers, written in natural language, are often vague,\nambiguous, and lacking crucial details for reproducing and subse-\nquently addressing the reported software defects [3, 5, 46]. Studies\nshow that bug reports in open-source projects often do not have\nBRTs, e.g., Koyuncu et al. [22] found that only 4% of the bug reports\nin Defects4J [16] include a BRT; Kang et al. [20] found that 28% of\nthe tests among the 300 most-starred GitHub projects are added\ndue to bug reports; more recently, M\u00fcndler et al. [27] found that\nthere are no corresponding BRTs for bug-reporting pull requests\nprior to the creation of those pull requests in SWE-Bench projects.\nIn industrial setting, similarly, BRTs are commonly expected to be\ndeveloped alongside the fix patch, after the bug is reported. Through\nour exploration of BRT creation and BRT availability in Google's\ninternal issue tracking system, we observed that BRT development\ncan be deferred to fix development stage because developers often\ndo not prioritize the task of, or possess enough information of,\ndeveloping a BRT at the time of bug reporting (\u00a72). For example,\ndevelopers may not attend to BRT development when actively\ndealing with issues reported by production monitoring. Moreover,\ndeveloping a BRT requires knowledge of the bug's root cause, and"}, {"title": "2 BACKGROUND AND MOTIVATION", "content": "In this section, we describe the definition of BRT (\u00a72.1), the avail-\nability of BRT in the industrial setting (\u00a72.2), and the need for\nautomated BRT generation (\u00a72.3).\n2.1 BRT Definition\nA BRT is a test that exhibits a Fail-to-Pass (F\u2192 P) behavior: the\ntest fails when executed against a buggy codebase, but passes when\nexecuted against the fixed codebase [27]. The implementation of\nthe BRT is useful for understanding and localizing the reproduced\nbug, aiding the development of an effective fix patch to the bug,\nwhile the F \u2192 P behavior of BRT provides necessary evidence that\nthe proposed fix patch to the reproduced bug is correct [16, 20-\n23, 27, 28, 30, 31, 40]. Program repair literature often denotes \"fix\npatch\" as \"patch\"; however, to avoid ambiguity with the test patch\nin BRT development, we denote fix patch for a bug as fix.\n2.2 BRT Availability\nPrior studies have shown that BRTs are scarce at time of bug report-\ning in open-source projects [22, 27] (\u00a71). In the industrial setting,\nthe common expectation is that BRT(s) are developed alongside the\nfix\u00b9. The practice of deferring BRT development to the fix devel-\nopment stage stems from the fact that bug reports originate from\nsources that often do not readily prioritize the task of, or possess\nthe knowledge of, developing BRTs at the time of bug reporting.\nSpecifically, typical bug report sources are team-internal de-\nvelopers, team-external developers, and end users. Team-internal\ndevelopers understand the codebase but may not prioritize writing\na BRT immediately when encountering an issue, particularly during\nactive development or when dealing with issues surfaced by produc-\ntion monitoring systems. Team-external developers understand the\nsystem's expected functionality but may not have the expertise to\nwrite a bug-targeted test. End users have limited knowledge of the\nunderlying code, and focus primarily on describing the observed\nissue. It is also important to note that issues reported by end users\nare typically not classified as internal bugs in our context.\nRegardless of the bug report source's exposure to the codebase,\nwriting an effective BRT is challenging and time-consuming. It\nrequires sufficient understanding of the targeted bug's root cause\nand code segment, as well as elaborated engineering effort to create\na test failure that captures the buggy behaviors, all of which could\nnot be immediately carried out as soon as the bug report is filed. As\na result, bug report sources often prioritize to describing the buggy\nbehaviors over developing a BRT. Manual effort of developing BRTs\ncan also substantially delay fix implementation.\n2.3\nAutomated BRT Generation\nThe importance of BRTs in software debugging (\u00a72.1) and the chal-\nlenge in BRT development (\u00a72.2) motivate the need for automated"}, {"title": "3 RELATED WORK", "content": "We first introduce related work in LLM for test generation. Next, we\ndiscuss recent automated BRT generation techniques, specifically\nLIBRO and SWE-Agent+ from SWT-Bench. We then highlight the\nsimilarities and differences between these works and this paper.\n3.1 LLM for Test Generation\nThe application of LLMs to automated test generation has garnered\nsignificant attention recently, showcasing the ability of LLMs to\nsupport developers during testing activities. Yuan et al. [43] em-\nploys a conversational approach, iteratively generating unit tests\nthrough interaction with ChatGPT. This method leverages the con-\nversational abilities of LLMs to refine test cases based on ongoing\nfeedback. Similarly, Xie et al. [37] adopts a feedback-based gener-\nation process with ChatGPT, focusing on iterative improvements\nthrough continuous interaction. Lemieux et al. [24] takes a hybrid\napproach that combines evolutionary search with the code under-\nstanding capabilities of Codex [8] to overcome the prior limitation\nin generating tests that improve coverage. Sch\u00e4fer et al. [32] in-\ncorporates API documentation alongside GPT-3.5 to generate unit\ntests, highlighting the importance of providing additional context\nto LLM for improved test generation. Further, Li et al. [25] demon-\nstrated that guiding ChatGPT with subtle differences between fixed"}, {"title": "3.2 LLM for BRT Generation", "content": "3.2.1 LIBRO. LIBRO [19, 20] was one of the first approaches to\nuse LLMs for general bug reproduction. It frames the task of BRT\ngeneration as a few-shot code generation problem, where an LLM is\nprompted with a few (bug report, test) examples before being asked\nto generate a test for the targeting bug report. The key assumption\nof LIBRO is that LLMs, due to their extensive pre-training on vast\namounts of code and natural language data, can understand the\nrelationship between a bug description and the test code required\nto reproduce it. LIBRO has the following components [20]:\n(1) Prompt Engineering. Given a bug report, construct the input\nprompt with the bug report title and description, and a few\n(bug report, test) examples.\n(2) Querying an LLM. Query an LLM multiple times with the\nsame input prompt and non-zero temperature to allow vari-\nation in the LLM's generated test code each time.\n(3) Test Postprocessing. Find a test file F that is the most textually\nsimilar to each generated test t, inject t into F and resolve\nany new dependencies. LIBRO considers t a candidate BRT\nif it compiles and fails on the buggy code.\n(4) Selection and Ranking. Candidate BRTs are clustered and\nde-duplicated by their failure traces and lines of code. LIBRO\npresents one test per cluster in a round-robin fashion to\ndevelopers, prioritizing tests from the larger clusters.\nLIBRO shows promising results on Defects4J 2.0, a benchmark\nconsisting of Java projects, demonstrating the ability of LLMs to\nunderstand bug descriptions and generate corresponding BRTs.\nAhmed et al. [1] proposed an approach on top of LIBRO, composed\nof three LLM calls to find a test file, get function names from bug\nreport, and then generate tests. Our work, developed independently\nand concurrently to Ahmed et al. [1], also found that selecting test\nfile(s) as input to BRT generation is more practical (\u00a74.1).\n3.2.2 SWE-Agent+. M\u00fcndler et al. [27] recently proposed SWT-\nBench, a benchmark built on top of SWE-Bench specifically for\nBRT generation in Python. With SWT-Bench, M\u00fcndler et al. [27]\nevaluated LIBRO. They also evaluated APR agents that can interact\nwith the codebase, i.e., AIDER [2], AutoCodeRover [44], and SWE-\nAgent [40]-these agents were adapted to the BRT generation task\nvia changes in their system and instruction prompts. On SWT-\nBench, the adapted SWE-Agent and its proposed variant SWE-\nAgent+ perform the best, successfully generating F \u2192 P tests on\n15.9% and 18.5% of the instances, respectively. They are also the\nmost related to our work."}, {"title": "3.3 Similarities and Differences with Our Work", "content": "Like LIBRO and SWT-Bench, we focus on LLM for BRT generation.\nWe adapted and evaluated LIBRO in our context (\u00a74.1). We also\ndeveloped an agent-based approach for automated BRT genera-\ntion (\u00a74.2). While our approach is developed independently and\nconcurrently to SWE-Agent+, they both share the same high-level\nphilosophy in its use of an LLM agent interacting with a codebase\nvia a set of commands to generate BRTs.\nOur work has the following key differences from prior work:\nIndustrial Context. LIBRO and SWT-Bench focus on a set of open-\nsource Java and Python projects that are relatively well-studied [16,\n40], while we focus on BRT generation for production bugs within\nGoogle's internal ecosystem [30]. These production bugs are recent,\nfrom diverse projects, and span multiple languages (\u00a75.1.1).\nDeveloping effective BRT generation for Google's internal en-\nvironment also poses unique challenges and opportunities, such\nas dealing with a massive proprietary codebase, internal-specific\ntooling and infrastructure. For example, when developing our BRT\ngeneration agent, we use an LLM fine-tuned on Google's code for\nthe agent's code editing command-this design addresses the con-\nfidential nature of Google's code and substantially improves the\nagent's performance and the effectiveness of its generated BRTs.\nUsefulness of the Generated BRTs. In addition to studying the\neffectiveness of standalone LLM or LLM agent for automated BRT\ngeneration, we investigate the practical impact of the generated\nBRTs on an industrial-scale APR system-a dimension not explored\nin detail by LIBRO or SWT-Bench.\nSpecifically, we study the generated BRTs on: (1) their effec-\ntiveness on improving the performance of APR system; (2) their\neffectiveness on selecting the most plausible APR-generated fixes.\nOur study adds another layer of practical application not explicitly\naddressed in the related work in LLM-based BRT generation."}, {"title": "4 APPROACHES", "content": "In this section, we first describe our adaptation of LIBRO to Google's\ninternal development environment. We then detail the design and\nimplementation of our agent-based approach for BRT generation,\nwhich we will refer to as BRT Agent. Figure 1 illustrates both our\nadaptation of LIBRO (1a) and BRT Agent (1b)."}, {"title": "4.1 Adaptation of LIBRO", "content": "LIBRO was originally built on open-source projects. We make\nchanges to LIBRO's LLM, input data, and prompt to make it com-\npatible with Google's internal development environment.\nLLM. We replaced the open-source LLM that LIBRO uses with a\nGemini model fine-tuned on Google's internal code. This LLM is"}, {"title": "4.2 Our Approach: BRT Agent", "content": "Recent research on LLM agents for APR have shown promising\nresults [7, 13, 30, 40, 44], while LLM agents for BRT generation\nremained rather under-explored. Moreover, we find that state-of-\nthe-art technique using standalone LLM, i.e., LIBRO, faces perfor-\nmance challenges even after adaptation (\u00a76). Thusly motivated, we\ndevelop an agent-based approach for BRT generation: BRT Agent.\nWe now describe BRT Agent's workflow and components.\n4.2.1 Initialization. BRT Agent receives a bug report and identified\nbuggy file content as initial input. These files can be provided by\nusers or upstream fault localization services [18, 29, 35, 39].\n4.2.2 Reasoning. A Gemini model serves as the primary reasoning\nLLM, responsible for understanding the bug report, planning overall\nexecution of the BRT generation task, and making decisions about\nwhich action to take in each ReAct step [42]. At each step, it analyzes\nthe current agent state, which includes the bug report, the displayed\ncode context, and the history of past actions and observations, to\neventually determine the next action.\n4.2.3 Action Selection and Execution. Table 1 shows a list of pos-\nsible actions BRT Agent can select and execute. For action cat,\ncode_search, bazel test, and finish, the agent executes the action\ndirectly. For action edit, the agent initiates a specialized code edit-\ning process that will be handled by a code-editing LLM. Specifically:\nChange Description. As part of the edit action invocation, BRT\nAgent, via its reasoning LLM, generates a natural language descrip-\ntion of the desired code change to a test file the agent specifies. The\nagent often chooses a test file from results of code_search action(s)\nfrom prior steps (\u00a76.1.2). The description captures the intent of the\nedit; one example description can be: \"Add a test case that asserts\nthe function returns null when given an empty input.\"\nPrompt Construction. The action edit constructs a prompt for the\ncode-editing LLM, which includes (1) the bug report description; (2)\nthe content of the specified test file; and (3) a clear task description"}, {"title": "5 EMPIRICAL STUDY DESIGN", "content": "This section outlines the design of our empirical study. We de-\nscribe the evaluation dataset, experiment configurations, research\nquestions (RQs), and evaluation metrics for each RQ."}, {"title": "5.1 Dataset and Experiment Configurations", "content": "5.1.1 Dataset. Our evaluation is conducted on a dataset of 802\nproduction bugs in Google's internal issue tracking system (GITS).\nAll the bugs were reported by human developers, and were fixed\nby human developers from corresponding teams. This dataset was\nconstructed via automated extraction and filtering phases as well\nas manual curation [30]. The bugs are recent (since June 2024),\nfrom a wide array of Google projects, and span seven programming\nlanguages: Java, C++, Go, Python, Kotlin, Dart, and TypeScript. In\nthe manual curation process, each bug and its corresponding fix\nwere carefully reviewed to ensure that the fix genuinely addresses\nthe underlying issue reported in the bug report, and not merely a\nworkaround or masking the bug.\nEach bug sample in the evaluation dataset has:\n\u2022 GITS Issue: A bug report with a title and a description,\nreported by a human developer.\n\u2022 Ground Truth Fix: A code change (CL) that resolves the\nreported bug. This fix includes an \"oracle\" BRT-a manually\nwritten test that reproduces the bug and validates the fix [10]."}, {"title": "5.2 Research Questions", "content": "Our study aims to answer the following research questions:\n\u2022 RQ1: BRT Generation Effectiveness: How effective is\nour BRT Agent in generating BRTs for Google's internal\nhuman-reported bugs?\n\u2022 RQ2: Impact of BRT on Fix Generation: How effective\nare the generated BRTs for improving the bug-fixing perfor-\nmance of an APR system?\n\u2022 RQ3: Impact of BRT on Fix Selection: How effective are\nthe generated BRTs for selecting plausible fixes generated\nby an APR system?"}, {"title": "5.2.1 RQ1: BRT Generation Effectiveness.", "content": "A high success rate in\ngenerating BRTs in an industrial context is crucial, because it pro-\nvides developers with a valuable tool for understanding, debugging,\nand ultimately fixing software defects, thereby reducing the ef-\nfort required in the debugging process and accelerating the overall\nsoftware development lifecycle. Successfully developing BRT gener-\nation techniques for an industrial environment like Google's would\ndemonstrate their practical applicability and potential for broader\nimpact beyond open-source projects."}, {"title": "5.2.2 RQ2: Impact of BRT on Fix Generation.", "content": "This research ques-\ntion investigates the practical impact of the automatically generated\nBRTs on fix generation. The hypothesis is that BRTs provide addi-\ntional context to better understand the buggy behaviors", "performance": "n\u2022 Number of Bugs with Plausible Fixes: Number of bugs\nfor which Passerine generates at least one plausible fix (a fix\nthat passes the oracle BRT which Passerine cannot access).\n\u2022 Steps to Plausible Fix: The number of steps taken by Passer-"}, {"title": "5.2.3 RQ3: Impact of BRT on Fix Selection.", "content": "This research question\nexplores the utility of generated BRTs as a mechanism for selecting\nplausible fixes from a pool of possible fixes generated by an APR\nsystem. LLM-based APR systems can generate a large number of\npotential fixes per bug [9, 14, 15, 30, 36], efficiently identifying\na correct fix from all potential fixes is a crucial challenge when\nno existing test had failed because of the bug. We thus examine\nwhether generated BRTs can be used to effectively discriminate\nbetween correct and incorrect fixes, and to rank fixes based on their\nlikelihood of being correct.\nWe introduce Ensemble Pass Rate (EPR), defined as the percentage\nof candidate BRTs that pass when executed against a given fix. The\nintuition is to use the generated tests (i.e., candidate BRTs with F \u2192\nX behavior) as a \"synthetic test suite\" to assess the effectiveness for\nall potential fixes, and leverage the pass/fail test results to inform fix\nselection. We omit Pass-to-Pass tests [27] because they do not fail\non the bug. If successful, this approach would be a useful tool for\nimproving the accuracy of APR systems and reducing the manual\neffort required to validate their outputs.\nSetup. Different from \u00a75.2.2, in this research question, Passerine\nis not provided a BRT for its bug-fixing run-we only evaluate the\nusefulness of the generated BRTs for fix selection, after Passerine\nhas generated fixes. We use standard Information Retrieval metrics\nto measure the efficacy of EPR in selecting plausible fixes:\n\u2022 Precision, Recall, F1-score, and Mean Reciprocal Rank\n(MRR). We define a true positive as a selected fix that passes\nthe oracle BRT, a false positive as a selected fix that fails the\noracle BRT, and a false negative as an unselected fix that\npasses the oracle BRT.\nWe compute values of these metrics for EPR-based fix selection\nunder two settings: (1) Top-K Selection, where we select the top-K\nfixes with the highest EPR, and (2) Threshold-Based Selection, where\nwe select all fixes with an EPR above a certain threshold."}, {"title": "6 EMPIRICAL RESULTS", "content": "We now present results of our research questions defined in \u00a75.2:\n\u2022 RQ1: BRT Generation Effectiveness: How effective is\nour BRT Agent in generating BRTs for Google's internal\nhuman-reported bugs?\n\u2022 RQ2: Impact of BRT on Fix Generation: How effective\nare the generated BRTs for improving the bug-fixing perfor-\nmance of an APR system?\n\u2022 RQ3: Impact of BRT on Fix Selection: How effective are\nthe generated BRTs for selecting plausible fixes generated\nby an APR system?"}, {"title": "6.1 BRT Generation Effectiveness", "content": "6.1.1 Overall Effectiveness. Table 2 presents the BRT generation ef-\nfectiveness of the evaluated techniques. The adapted LIBRO achieves\na candidate BRT generation rate of 41%. Its plausible BRT generation\nrate is 10%, which is lower than the 33% reported on Defects4J [20],\npotentially due to the increased complexity of Google's internal"}, {"title": "6.1.2 Agent Behaviors.", "content": "We also study the statistics on agent behav-\niors, specifically the action distribution and termination reasons.\nFigure 2 shows the action distribution of each step for all 1600\n(80* 20) runs of BRT Agent. In each run, the agent can take at most\n25 steps (\u00a75.1). The \"other\" category includes actions output by the\nagent that are not part of the provided action set-in a few cases,\nthe agent hallucinated and output actions, such as grep or find, that\nare not listed in its system prompt. Figure 2 also shows BRT Agent\nfrequently starts by finding related files via code_search, inspecting\ntheir content via cat, and proceeding to edit and run tests.\nTable 4 shows the top-5 most frequent action bigrams from BRT\nAgent. The frequency column indicates the occupancy of a specific\nbigram among all bigrams. The top-3 bigrams mainly represent\nthe agent's core task of BRT generation: editing, analyzing, and\nrunning test files. The two remaining bigrams mainly represent the\nagent's file localization and debugging activities.\nTable 5 shows the category of reasons that the evaluated BRT\ngeneration techniques terminated. BRT Agent terminates gracefully\nby invoking the finish action in 72% of the cases, while exhaust-\ning the configured total number of steps (i.e., 25) in 21% of the\ncases. There are also cases where BRT Agent and LIBRO throw\nframework-level exceptions; common reasons for framework-level\nexceptions are the LLM input exceeding the input context window\nsize, server rate limit exceptions, and arbitrary exceptions thrown\nfrom imported libraries outside the framework (BRT Agent only)."}, {"title": "6.2 Impact of BRT on Fix Generation", "content": "We now present results of Passerine's bug-fixing effectiveness with\nversus without providing generated plausible BRTs (\u00a75.2.2)."}, {"title": "6.3 Impact of BRT on Fix Selection", "content": "6.3.1 Top-K Selection. Figure 5 shows precision@K and recall@K\nachieved with the Ensemble Pass Rate (EPR) computed on the gen-\nerated BRTs, K increments from 1 to 5. We can see from Figure 5\nthat EPR allows for different fix selection strategies.\nWhen prioritizing maximum precision, selecting the top-ranked\nfix (K = 1) correctly identifies a plausible fix from a pool of 20\ncandidates in 70% of cases, achieving a precision of 0.7. This means\nthat, on average, the correct fix is ranked first in 70% of the cases\n(an MRR of 0.7). However, this approach yields a lower recall of 0.3.\nAlternatively, a more balanced approach, considering the top-3\nranked fixes (K = 3), achieves a precision of 0.6 and a recall of\n0.5 obtaining the highest F1-score among the different K values."}, {"title": "7 THREATS TO VALIDITY", "content": "While our study provides promising results for automated BRT\ngeneration in an industrial setting, several factors could potentially\ninfluence the validity of our findings.\nInternal Validity. One potential threat to internal validity is the\nevaluation dataset size. We study BRT generation on a relatively\nsmall dataset of 80 bugs from Google's internal issue tracking sys-\ntem (\u00a75.1). Although this dataset has been carefully curated and\nstudied for prior work on APR [30], its size could limit the general-\nizability of our results to the broader spectrum of bugs encountered\nwithin Google. Another potential threat is implementation bias.\nOur adaptation of LIBRO may have introduced subtle differences\nthat could affect its performance. To mitigate such threat, we ensure"}, {"title": "8 CONCLUSION", "content": "This paper investigates the potential for automated Bug Reproduc-\ntion Test (BRT) generation within a large-scale industrial setting,\nspecifically at Google. We adapt and evaluate an existing LLM-\nbased approach for BRT generation, specifically LIBRO [20], to\nfunction within Google's complex internal development environ-\nment. We also introduce our LLM-agent-based BRT generation\napproach, designed to work with Google's development environ-\nment. BRT Agent, our BRT generation technique that is built on\ntop of a Gemini model fine-tuned on Google's codebase, signifi-\ncantly outperforms the adapted LIBRO built on top of the same\nGemini model, demonstrating the efficacy of agent-based system\nfor effective BRT generation.\nOur empirical results show that BRT Agent can generate plausi-\nble BRTs for more bugs (28% compared to 10% by LIBRO). These\ngenerated plausible BRTs can improve the performance of Passer-\nine, Google's industrial-scale APR system, in generating plausible\nfixes for more bugs more efficiently. Finally, we show that candidate\nBRTs with F \u2192 X behavior can also be used to effectively rank and\nselect plausible fixes generated by APR leveraging our proposed\nEnsemble Pass Rate metric.\nThese findings underscore the practical value of BRTs and high-\nlight the importance of developing robust, industry-aware BRT gen-\neration techniques. We hope this research contributes to bridging\nthe gap between academic research and industrial needs, offering a\npractical solution for improving software quality and accelerating\nbug resolution in large-scale, complex software ecosystems."}]}