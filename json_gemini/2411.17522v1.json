{"title": "On Statistical Rates of Conditional Diffusion Transformers: Approximation, Estimation and Minimax Optimality", "authors": ["Jerry Yao-Chieh Hu", "Weimin Wu", "Yi-Chen Lee", "Yu-Chao Huang", "Minshuo Chen", "Han Liu"], "abstract": "We investigate the approximation and estimation rates of conditional diffusion transformers (DiTs) with classifier-free guidance. We present a comprehensive analysis for \u201cin-context\u201d conditional DiTs under four common data assumptions. We show that both conditional DiTs and their latent variants lead to the minimax optimality of unconditional DiTs under identified settings. Specifically, we discretize the input domains into infinitesimal grids and then perform a term-by-term Taylor expansion on the conditional diffusion score function under H\u00f6lder smooth data assumption. This enables fine-grained use of transformers' universal approximation through a more detailed piecewise constant approximation and hence obtains tighter bounds. Additionally, we extend our analysis to the latent setting under the linear latent subspace assumption. We not only show that latent conditional DiTs achieve lower bounds than conditional DiTs both in approximation and estimation, but also show the minimax optimality of latent unconditional DiTs. Our findings establish statistical limits for conditional and unconditional DiTs, and offer practical guidance toward developing more efficient and accurate DiT models.", "sections": [{"title": "1 Introduction", "content": "We investigate the approximation and estimation rates of conditional diffusion transformers (DiTs) with classifier-free guidance. Specifically, we derive score approximation, score estima- tion, and distribution estimation guarantees for both conditional DiTs and their latent variants. We provide a comprehensive analysis under various data conditions. Moreover, we show that both conditional DiTs and their latent variants lead to the minimax optimality of unconditional DiTs under identified settings. This analysis is not only practical but also timely. Transformer-based conditional diffusion models are at the forefront of generative AI due to their success as scal- able and flexible backbones for image [Wu et al., 2024a, Bao et al., 2023, Batzolis et al., 2021] and video generation [Liu et al., 2024, Ni et al., 2023, Saharia et al., 2022, Voleti et al., 2022]. However, the theoretical understanding of conditional DiTs remains limited. On the one hand, while prior work by Hu et al. [2024b] reports approximation and estimation rates of DiTs using the established universality of transformers [Yun et al., 2020], their results are not tight and are limited to unconditional diffusion. On the other hand, existing theoretical works on conditional diffusion models only focus on ReLU networks [Fu et al., 2024a, Yuan et al., 2023], model-free settings [Ye et al., 2024, Guo et al., 2024] or generative sampling process [Dinh et al., 2023], without considering the transformer architectures. This work addresses this gap by providing a timely analysis of the statistical limits of conditional DiTs.\nIn this work, we present a comprehensive analysis of conditional DiT and its latent setting under four common data assumptions. We also establish the minimax optimality of unconditional DiT and its latent version by deriving the tight distribution estimation error bounds. Our techniques include two key parts: (i) Discretizing the input domains into infinitesimal grids. (ii) On each grid, performing a term-by-term Taylor expansion on the conditional diffusion score function under generic and stronger H\u00f6lder smooth data assumptions, motivated by the local diffused polynomial analysis [Fu et al., 2024a, Oko et al., 2023]. These techniques leverage the nice regularity of the score function imposed by the H\u00f6lder smoothness data assumptions and hence enable fine-grained use of transformers' universal approximation [Kajitsuka and Sato, 2024, Yun et al., 2020] through"}, {"title": "2 Background and Preliminaries", "content": "In this section, we provide a high-level overview of the conditional diffusion model with classifier- free guidance in Section 2.1 and conditional Diffusion Transformer (DiT) networks in Section 2.2."}, {"title": "2.1 Conditional Diffusion Model with Classifier-free Guidance", "content": "Forward and Backward Conditional Diffusion Process. In the forward process, conditional diffusion models gradually add noise to the original data $x_0 \\in \\mathbb{R}^{d_x}$. Give a condition $y \\in \\mathbb{R}^{d_y}$, and $x_0 \\sim P_0(\\cdot|y)$. Let $x_t$ denote the noisy data at the timestamp $t$, with marginal distribution and density as $P_t(\\cdot|y)$ and $p_t(\\cdot|y)$. The conditional distribution $P_t(x_t|y)$ follows $\\mathcal{N}(x_0, \\sigma_t^2 I_{d_x})$, where $a_t = e^{-t/2}$, $\\sigma_t^2 = 1 - e^{-t}$, and $w(t) > 0$ is a nondecreasing weighting function. In practice, the forward process terminates at a large enough $T$ such that $P_T$ is close to $\\mathcal{N}(0, I_{d_x})$. In the backward process, we obtain $x_t$ by reversing the forward process. The generation of $x_t$ depends on the score function $\\nabla \\log p_t(\\cdot|y)$. See Appendix E.1 for the details. In below, when the context is clear, we suppress the notation dependence of $x_t$ on the time step $t$.\nClassifier-Free Guidance. Classifier-free guidance [Ho and Salimans, 2022] is the standard workhorse for training condition diffusion models. It approximates both conditional and uncon- ditional score functions using neural networks $s_w$ with parameters $W$. It uses the following loss function:\n\n\nwhere $\\nabla_{x_t} \\log p_t (x_t|x_0) = -(x_t - a_tx_0)/\\sigma_t^2$, $t_0$ is a small cutoff to stabilize training 1. $\\tau = 0$ denotes the unconditional version, $\\tau = id$ denotes the conditional version, and $P(\\tau = 0) = P(\\tau = id) = 0.5$. To train $s_w$, we select $n$ i.i.d. samples $\\{x_{0,i}, y_i\\}_{i=1}^n$, where $x_{0,i} \\sim P_0(\\cdot|y_i)$. We"}, {"title": "3 Statistical Limits of Conditional DiTs", "content": "In this section, we present a refined decomposition scheme for the fine-grained analysis of score approximation, score estimation, and distribution estimation in conditional DiT. Our analysis con- siders two assumptions on initial data distributions:"}, {"title": "3.1 Score Approximation: Generic H\u00f6lder Smooth Data Distributions", "content": "We present a fine-grained piecewise approximation using transformers to approximate the condi- tional score function under the H\u00f6lder smoothness assumption on the initial data [Fu et al., 2024b]. At its core, we introduce a score function decomposition scheme with term-by-term tractability.\nWe first introduce the definition of H\u00f6lder space and H\u00f6lder ball following [Fu et al., 2024b].\n\n\nWe also define the H\u00f6lder ball of radius $B$: $H^\\beta(\\mathbb{R}^d, B) := \\{f : \\mathbb{R}^d \\rightarrow \\mathbb{R} ~|~ ||f||_{H^\\beta(\\mathbb{R}^d)} < B\\} $.\nLet $x_0 \\in \\mathbb{R}^{d_x}$ denote the initial data, and $y \\in [0, 1]^{d_y}$ the conditional label. With Definition 3.1, we state the first assumption on the conditional distribution of initial data $x_0$.\n\nRemark 3.1. The H\u00f6lder continuity assumption captures various smoothness levels in the condi- tional density function. The light-tail condition relaxes the bounded support assumption in [Oko et al., 2023]. Moreover, Assumption 3.1 only applies to the initial conditional distribution and imposes no constraints on the induced conditional score function. This is far less restrictive than the Lipschitz score condition in prior works [Yuan et al., 2024, Lee et al., 2023, Chen et al., 2022].\nIn our work, we aim to approximate the conditional score function $\\nabla \\log p_t(x_t|y)$ using trans- former architectures. Hu et al. [2024b] analyze the unconditional DiTs based on the established"}, {"title": "3.2 Score Approximation: Stronger H\u00f6lder Smooth Data Distributions", "content": "Next, we study the conditional DiT score approximation problem using our score decomposition scheme under the stronger H\u00f6lder smoothness assumption from Fu et al. [2024b, Assumption 3.3].\n\nAssumption 3.2 imposes stronger assumption than Assumption 3.1 and induces a refined condi- tional score function decomposition. Explicitly, by Lemma H.1, $\\nabla \\log p_t(x|y)$ becomes:\n$\\nabla \\log p_t(x|y) = \\frac{-C_2x}{a_t^2 + C_2 \\sigma_t^2} + \\frac{\\nabla h(x, y, t)}{h(x,y,t)},$"}, {"title": "4 Latent Conditional DiTs", "content": "In this section, we extend the results from Section 3 by considering the latent conditional DiTs. Specifically, we assume the raw input $x \\in \\mathbb{R}^{d_x}$ has an intrinsic lower-dimensional representation.\n\nRemark 4.1. \u201cLinear Latent Space\" means that each entry of a given latent vector is a linear com- bination of the corresponding input, i.e., $x = Uh$. This is also known as the \u201clow-dimensional data\" assumption in literature [Hu et al., 2024b, Chen et al., 2023c]. This assumption is fundamen- tal in dimensionality reduction techniques for capturing the intrinsic lower-dimensional structure of data.\nScore Decomposition and Model Architecture. To derive approximation and estimation results, we extend the techniques and network architecture presented in Section 3 to latent diffusion by considering the \u201clow-dimensional linear subpace\". Under Assumption 4.1, we decompose the score:\n$\\nabla \\log p_t(x|y) = U(\\sigma_t^2 \\nabla \\log p_t^r(U^\\top x|y) + U^\\top x)/\\sigma_t^2 - x/\\sigma_t^2,$"}, {"title": "5 Discussion and Conclusion", "content": "We investigate the approximation and estimation rates of conditional DiT and its latent setting. We focus on the \u201cin-context\u201d conditional DiT setting presented by Peebles and Xie [2023], and conduct a comprehensive analysis under various common data conditions (Section 3 for generic and strong H\u00f6lder smooth data, Section 4 for data with intrinsic latent subspace).\nInterestingly, we establish the minimax optimality of the unconditional DiTs' estimation by re- ducing our analysis of conditional DiTs to the unconditional setting (Section 3.4 and Remark 4.3). Our key techniques include a well-designed score decomposition scheme (Section 3.1). These en- able a finer use of transformers' universal approximation, compared to the prior statistical rates of DiTs derived from the universal approximation results in [Yun et al., 2020] by Hu et al. [2024b].\nConsequently, we provide two extensions in the appendix:"}, {"title": "A Notation Table", "content": null}, {"title": "B Related Works, Broader Impact and Limitations", "content": null}, {"title": "B.1 Related Works", "content": "In the following, we discuss the recent success of the techniques used in our work. We first give the universality (universal approximation) of the transformer. Then, we discuss recent theoretical developments (approximation and estimation) in diffusion generative models.\nUniversality of Transformers. The universality of transformers refers to their ability to ap- proximate any sequence-to-sequence function with arbitrary precision. Yun et al. [2020] establish this by showing that transformers is capable of universally approximate sequence-to-sequence functions using deep stacks of feed-forward and self-attention layers. Additionally, Alberti et al. [2023] demonstrate universal approximation for architectures employing non-standard attention mechanisms. Recently, Kajitsuka and Sato [2024] show that even a single-layer transformer with self-attention suffices for universal approximation assuming all attention weights are rank-1. Moreover, Hu et al. [2024b] leverage Yun et al. [2020] universality results to analyze the approx- imation and estimation capabilities of DiT.\nOur paper is motivated by and builds upon the works of Hu et al. [2024b], Kajitsuka and Sato [2024], Yun et al. [2020]. Specifically, we utilize and extend the transformer universality result from Kajitsuka and Sato [2024]. We employ a relaxed contextual mapping property in Kajitsuka and Sato [2024] (see Appendix F.1). This generalization allows us to avoid the \u201cdouble expo- nential\u201d sample complexity bounds in previous DiT analyses [Hu et al., 2024b, Remark 3.4] and establish transformer approximation in the simplest configuration a single-layer, single-head attention model.\nApproximation and Estimation Theories of Diffusion Models. The theories of DiTs revolve around two main frontiers: score function approximation and statistical estimation [Chen et al., 2024a, Tang and Zhao, 2024]. Score function approximation refers to the ability of the score network to approximate the score function. It leverages the universal approximation ability of the neural network in $L^\\infty$ norms [Hayakawa and Suzuki, 2020], the approximation characterized as Taylor polynomial [Fu et al., 2024a] or B-Spline [Oko et al., 2023]. Chen et al. [2023c] and Fu et al. [2024a] investigate score approximation under specific conditions, such as low-dimensional linear subspaces and H\u00f6lder smooth data assumptions, using ReLU-based models. Furthermore, Hu et al. [2024b] presents the first characterization of score approximation in diffusion transform- ers (DiTs).\nThe statistical estimation includes score function and distribution estimation [Wu et al., 2024b, Dou et al., 2024a, Guo et al., 2024, Chen et al., 2023c]. Under a $L_2$ accurate score estimation, several works have provided the convergence bounds under either smoothness assumptions [Ben- ton et al., 2024, Chen et al., 2022] or bounded second-order moment assumptions [Chen et al.,"}, {"title": "C Latent Conditional DiT with H\u00f6lder Assumption", "content": "In this section, we extend the results on approximation and estimation of DiT from Section 3 by considering the latent conditional DiTs. Latent DiTs enables efficient data generation from latent space and therefore scales better in terms of spatial dimensionality [Rombach et al., 2022]. Specifically, we assume the raw input $x \\in \\mathbb{R}^{d_x}$ has an intrinsic lower-dimensional representation in a $d_o$-dimensional subspace, where $d_o \\le d_x$. This setting is common in both empirical [Peebles and Xie, 2022, Rombach et al., 2022] and theoretical studies [Hu et al., 2024b, Chen et al., 2023c].\nOrganization. We present the statistical results under H\u00f6lder data smooth Assumptions 3.1 and 3.2 and state the results in Theorem C.1, Theorem C.2, Theorem C.3, and Theorem C.4, respectively. Appendix C.1 discusses score approximation. Appendix C.2 discusses score estima- tion. Appendix C.3 discusses distribution estimation. The proofs in this section primarily follow Appendices G and H.\nLet $d_o$ denote the latent dimension. We summarize the key points of this section as follows:\nK1. Low-Dimensional Subspace Space Data Assumption. We consider the setting that latent representation lives in a \u201cLow-Dimensional Subspace\" under Assumption 4.1, following [Hu et al., 2024b, Chen et al., 2023c].\n\nFor raw data $x \\in \\mathbb{R}^{d_x}$, we utilize linear encoder $W_J \\in \\mathbb{R}^{d_o \\times d_x}$ and decoder $W_U \\in \\mathbb{R}^{d_x \\times d_o}$ to convert the raw $x \\in \\mathbb{R}^{d_x}$ and latent $h \\in \\mathbb{R}^{d_o}$ data representations. Importantly, $x = Uh$ with $U \\in \\mathbb{R}^{d_x \\times d_o}$ by Assumption 4.1.\nFor each input $x \\in \\mathbb{R}^{d_x}$ and corresponding label $y \\in \\mathbb{R}^{d_y}$, we use a transformer network to obtain a score estimator $s_w \\in \\mathbb{R}^{d_x}$. To utilize the transformer network as the score estimator, we introduce reshape layer to convert vector input $h \\in \\mathbb{R}^{d_o}$ to matrix (sequence) input $H \\in \\mathbb{R}^{\\tilde{d} \\times \\tilde{L}}$. Specifically, the reshape layer in the network Figure 3 is defined as $R(\\cdot) : \\mathbb{R}^{d_o} \\rightarrow \\mathbb{R}^{\\tilde{d} \\times \\tilde{L}}$ and its reverse $R^{-1}(\\cdot) : \\mathbb{R}^{\\tilde{d} \\times \\tilde{L}} \\rightarrow \\mathbb{R}^{d_o}$, where $d_o \\le \\tilde{d} \\tilde{L}$, $\\tilde{d} \\le d$, and $\\tilde{L} \\le L$.\nWe remark that the \u201clow-dimensional data\u201d assumption leads to tighter approximation rates than those of Sections 3.1 and 3.2 and estimation errors due to $d_o \\le d_x$ (Theorems C.1 and C.2).\nK2. H\u00f6lder Smooth Assumption. For approximation and estimation results for latent condi- tional DiTs (Theorems C.1 to C.4), we study the cases under both the generic and strong H\u00f6lder smoothness assumptions on latent representation $h \\in \\mathbb{R}^{d_o}$. Specifically, we assume"}, {"title": "D Latent Conditional DiT with Lipschitz Assumption", "content": "In this section, we apply our techniques to the setting of [Hu et al., 2024b] on DiT approximation and estimation theory. Specifically, we extend their work by using the one-layer self-attention transformer universal approximation framework introduced in Appendix F.1.\nCompared to [Hu et al., 2024b], we consider classifier-free conditional DiTs, providing a holistic view of the theoretical guarantees under various assumptions. In particular, our sample complexity bounds avoid the gigantic double exponential term $2^{(1/\\epsilon)^{21}}$ reported in [Hu et al., 2024b]. We adopt the following three assumptions considered by Hu et al. [2024b]:\n(A1) Low-Dimensional Linear Latent Space Data Assumption.\n\nUnder this data assumption, Chen et al. [2023a] show that the latent score function endows a neat decomposition into on-support $s_+$ and orthogonal $s_-$ terms (see Lemma C.1).\n\n(A2) Lipschitz Score Assumption. We assume the on-support score function $s_+(h, y, t)$ to be $L_{s_+}$-Lipschitz for any $h$ and $y$.\n\n(A3) Light Tail Data Assumption."}, {"title": "E Supplementary Theoretical Background", "content": "In this section, we provide an overview of the conditional diffusion model and classifier guidance in Appendix E.1 and classifier-free guidance in Appendix E.2."}, {"title": "E.1 Conditional Diffusion Process", "content": "Conditional diffusion models use the conditional information (guidance) $y$ to generate samples from conditional data distribution $P(\\cdot|y = guidance)$. Depending on the model's objective, the guidance is either a label for generating categorical images, a text prompt for generating images from input sentences, or an image region for tasks like image editing and restoration. Through- out this paper, we coin diffusion models with label guidance $y$ as conditional diffusion mod- els (CDMs). Practically, implement a conditional diffusion model characterized as classifier and classifier-free guidance. The classifier guidance diffusion model combines the unconditional score function with the gradient of an external classifier trained on corrupted data. On the other hand, classifier-free guidance integrates the conditional and unconditional score function by randomly ignoring $y$ with mask signal (see (E.6)). In this paper, we focus on the latter approach.\nSpecifically, we consider data $x \\in \\mathbb{R}^{d_x}$ and label $y \\in \\mathbb{R}^{d_y}$ with initial conditional distribution $P(x|y)$. The diffusion process (forward Ornstein-Uhlenbeck process) is characterized by:\n$dX_t = -\\frac{1}{2} X_t dt + dW_t$ with $X_0 \\sim P(x|y),$\nwhere $W_t$ is a Wiener process. The distribution at any finite time $t$ is denoted by $P_t(x|y)$, and $X_\\infty$ follows standard Gaussian distribution. Up to a sufficiently large terminating time $T$, we generate samples by the reverse process:\n$dX_t = [-\\frac{1}{2} X_t + \\nabla \\log P_{T-t}(X_t|y)] dt + dW^{\\leftarrow}$ with $X_T \\sim P_T(x|y),$\nwhere the term $\\nabla \\log P_{T-t}(X_t|y)$ represents the conditional score function. We have $X_t|X_0 \\sim \\mathcal{N}(a_tX_0, \\sigma_t^2 I)$ with $a_t = e^{-t/2}$ and $\\sigma_t^2 = 1 - e^{-t}$.\nWe use a score networks $s$ to estimate the conditional score function $\\nabla \\log P_t(x|y)$, and the quadratic loss of the conditional diffusion model is given by\n$\\argmin_{s \\in \\mathcal{T}_{h,s,r}} \\mathbb{E}_{(x_0, y)} \\Big[ \\mathbb{E}_{T, X_t|x_0} \\big[ \\mathbb{E}_{(x', y')|x_0} [|| s(x', y', t) - \\nabla_{x'} \\log P_t(x'|x_0) ||^2] \\big] \\Big],$"}, {"title": "F Universal Approximation of Transformers", "content": "In this section, we discuss the universal approximation theory of transformers.\nIn Appendix F.1, we present the universal approximation results of transformers for score approx- imation in Section 3. We emphasize that most of the material in Appendix F.1 is not original and is drawn from [Hu et al., 2024a, Kajitsuka and Sato, 2024, Yun et al., 2020].\nIn Appendix F.2, we compute the parameter norm bounds of the transformers used for score approximation. These bounds are crucial for calculating the covering number of the transformers and are essential for score and distribution estimation in Section 3.3."}, {"title": "F.1 Transformers as Universal Approximators", "content": "The key idea for demonstrating the transformers' ability to capture the entire sequence lies in the concept of contextual mapping [Hu et al., 2024a, Kajitsuka and Sato, 2024, Yun et al., 2020]. We first restate the background of a $(\\gamma, \\delta)$-contextual mapping in Definition F.3, using the definition of vocabulary (Definition F.1) and token separation (Definition F.2) in the input sequences.\n\n\nIn line with prior works [Hu et al., 2024a, Kajitsuka and Sato, 2024, Kim et al., 2022, Yun et al., 2020], we assume the embeddings separateness to be $(\\gamma_{\\min}, \\gamma_{\\max}, \\delta)$-separated, as defined in Def- inition F.2."}, {"title": "H Proof of Theorem 3.2", "content": "We provide the formal version of Theorem 3.2 at the end of Appendix H.2.\nNext, similar to the proof of Theorem 3.1, we need the truncation of $x$ due to the unboundedness as well.\n\n1\nWe utilize the condition assumed in Assumption 3.2 to achieve the decomposition."}, {"title": "I Proof of the Estimation Results for Conditional DiTs", "content": "Overview of Our Proof Strategy of Theorem 3.3.\nStep 0. Preliminaries. We introduce the mixed risk that accounts for risk with the distribution of the mask signal in Definition I.1. We restate the loss function and the score matching technique in Definition I.2.\nStep 1. Truncate the Domain of the Risk. We truncate the domain of the loss function in order to obtain finite covering number of transformer network class. Precise definition of the truncated loss function class is in Definition I.4. We bound the error from the truncation from the assumed light tail condition in Lemma I.1.\nStep 2. Derive the Covering Number of Transformer Network. We introduce the covering number of a given function class in Definition I.5. We provide lemma detailing the cal- culation of the covering number for transformer architecture in Lemma I.2. We derive the covering numbers under the respective parameter configurations for our two previous main results in Lemma I.3.\nStep 3. Bound the True Risk on Truncated Domain. With the previous steps, we present the upper-bound of the mixed risk in Lemma I.4.\nOverview of Our Proof Strategy of Theorem 3.4. We decompose the total variation into three components and we bound the separately.\nStep 1. We bound the total variation distance between the true distributions evaluated at $t = 0$ and early-stopping time $t = t_0$.\nStep 2. We bound the total variation between the true distribution at $t_0$ and the reverse process distribution using the true score function.\nStep 3. We bound the total variation between the reverse process distributions using the true and estimated score functions at $t_0$.\nOrganization. Appendix I.1 includes auxiliary lemmas for supporting our proof of Theo- rem 3.3. Appendix I.2 includes the main proof of Theorem 3.3. Appendix I.5 includes auxil- iary lemmas for supporting our proof of Theorem 3.4. Appendix I.6 includes the main proof of Theorem 3.4."}, {"title": "I.1 Auxiliary Lemmas for Theorem 3.3", "content": "Step 0: Preliminary Framework. We evaluate the quality of the estimator sw through the risk:\n$R(s_W) := \\frac{1}{T-T_0} \\int_{T_0}^T \\mathbb{E}_{x_t,y} [|| s_W (x_t, y, t) - \\nabla \\log p_t(x_t|y) ||^2] dt.$"}, {"title": "1.2 Proof of Theorem 3.3", "content": "Proof of Theorem 3.3. For simplicity, we use $k = 1/t_0$ for the case in Theorem 3.1 and $k = log(1/t_0)$ for the case in Theorem H.1. The proof proceeds through the following three steps.\n* Step A: Decompose the mixed risk.\nWe denote the ground truth by $s^\\circ(x,y,t) = \\nabla log p_t(x|y)$, and if $y = \\emptyset$ we set $s^\\circ(x,y,t) = p_t(x)$.\nFrom Definition I.3 and Lemma I.4, by introducing a different set of i.i.ds samples $\\{x'_i, y_i\\}_{i=1}^n$ from the initial data distribution $P(x, y)$ independent of the training samples, we rewrite the mixed risk:"}]}