{"title": "ASAP: Learning Generalizable Online Bin Packing via Adaptive Selection After Pruning", "authors": ["Han Fang", "Paul Weng", "Yutong Ban"], "abstract": "Recently, deep reinforcement learning (DRL) has achieved promising results in solving online 3D Bin Packing Problems (3D-BPP). However, these DRL-based policies may perform poorly on new instances due to distribution shift. Besides generalization, we also consider adaptation, completely overlooked by previous work, which aims at rapidly finetuning these policies to a new test distribution. To tackle both generalization and adaptation issues, we propose Adaptive Selection After Pruning (ASAP), which decomposes a solver's decision-making into two policies, one for pruning and one for selection. The role of the pruning policy is to remove inherently bad actions, which allows the selection policy to choose among the remaining most valuable actions. To learn these policies, we propose a training scheme based on a meta-learning phase of both policies followed by a finetuning phase of the sole selection policy to rapidly adapt it to a test distribution. Our experiments demonstrate that ASAP exhibits excellent generalization and adaptation capabilities on in-distribution and out-of-distribution instances under both discrete and continuous setup.", "sections": [{"title": "I. INTRODUCTION", "content": "The 3D Bin Packing Problem (3D-BPP) is a classic combinatorial optimization problem, where the goal is to pack items of various shapes into a container such that space utilization is maximized. Thanks to its practical applications, e.g., robotics or warehousing [1] and the recent promising achievements of machine learning-based heuristic solvers for combinatorial optimization problems [2, 3], 3B-BPP starts to be actively studied in the machine learning community.\nIn traditional 3D-BPP, all the items are known at solution time and the solver can decide to pack items in any arbitrary order [4]. This so-called offline setting leads to an NP-hard problem [5]. To solve it, various machine learning approaches [6-10] have been proposed. However, in many real-world scenarios, knowing all incoming items in advance is difficult or even impossible, leading to the online setting [11]. In this setting, the solver is required to generate the solution as the items arrive, without accessing any information about future items. To tackle this problem, methods [12, 13] leveraging deep reinforcement learning (DRL) have recently been shown to perform well, especially on online regular 3D-BPP, where the item shapes are cuboid. However, these DRL-based solvers (policy) are still inadequate for practical deployment. Firstly, these learned policies may have weak cross-distribution generalization capabilities: their performance may degrade quickly when tested on new instances that differ from training instances. This distribution shift is common in practice: for instance, in logistics, items to be packed evolve over time due to change of products or purchase patterns. In addition, in such online scenarios, a natural new goal needs to be considered, which is to make the trained policy quickly adapt to the test instances.\nTo address the generalization and adaptation issues in online regular 3D-BPP, we propose an approach called ASAP. More specifically, to improve the generalization ability of a DRL-based solver, we propose to decompose its decision-making into two steps: pruning then selection. In a first step, a pruning policy removes inherently bad actions. In a second step, a selection policy can then choose among the remaining actions. This two-step approach is justified by our empirical observation (see Section III B) that suggests some actions should be ignored whatever the instance distribution. Doing so, the task of the selection policy is facilitated since it can directly focus on the most promising actions.\nTo address the adaptation issue, we propose a two-phase training method. Firstly, both policies are trained using MAML (Model-Agnostic Meta-Learning) [14], which also promotes the generalizabil-ity of the resulting solver. Then, on test instances, the selection policy is finetuned to quickly adapt to the new test distribution. Note that finetuning the selection policy alone is more effective and faster than finetuning a whole DRL-based solver (without pruning) for two reasons: (1) Our selection policy has been trained via meta-learning to adapt quickly to new dis-tributions, avoiding potential issues such as plasticity"}, {"title": "II. RELATED WORK", "content": "Online 3D-BPP has been addressed primarily through two approaches: methods based on traditional heuristics [4, 16, 17] and learning-based methods [12, 13]. For space reasons, we focus our discussion on the most related work for online regular 3D-BPP and do not discuss the studies focused on irregular 3D-BPP [18-22].\nHeuristics-based Methods. Various traditional heuristic-based solvers [17, 23-25] have been proposed for online 3D-BPP. Their design often relied on human practical experience, although some of them [23, 24] have been analyzed to provide worse-case performance guarantees. In addition, to identify promising positions for incoming items, general heuristic-based placement rules have also been developed, e.g., corner points [4], extreme point [16], heightmap-minimization [1], or empty maximal space [26]. However, due to their reliance on hand-crafted rules, these methods struggle with complex shapes or constraints.\nDRL-based Methods. To learn a solver for online 3D-BPP, the first DRL-based methods [12, 27] were formulated to use the assistance of heuristic-based placement rules. Though heuristic-based methods do not provide optimal packing under complex constraints, they can suggest potential placement candidates. Since this approach only works in discrete environments (i.e, the item sizes can only take discrete values), Zhao et al. [13] developed the Packing Configuration Tree (PCT) by integrating various heuristics. By using PCT to generate candidate placements and applying Deep Reinforcement Learning (DRL) for decision-making in continuous environments, this approach outperforms heuristic-based methods and encourages further study. [28-30] further extends this approach by applying novel network architectures and updating heuristics. Building on earlier DRL approaches, Puche and Lee [31] integrate Monte Carlo Tree Search (MCTS) with the assumption of knowing the next n items in the buffer.\nGeneralization in Online 3D-BPP. Although Zhao et al. [13] show that their DRL-based approach using PCT can achieve good generalization under a few limited cross-distribution evaluation scenarios, the effectiveness of DRL-based methods can drop when the test distribution deviates from the training distribution (e.g., more frequent outlier shapes or ocur-rence of novel shapes), as we observed in our experiments (see Section V B). To improve the performance of the DRL-based policy in worst-case scenarios, Pan et al. [32] introduce AR2L, a method that uses an attacker to change the permutations of coming items in training, thus balancing average and worst-case performance. Xiong et al. [33] introduce GOPT, a solver that can generalize across containers of various sizes. Given that altering the container size is akin to normalizing the container size and altering the incoming item size, we concentrate on the generalization across different shapes of incoming items. In contrast to our work, none of these previous propositions directly address the problem of generalizing and adapting to new test distributions."}, {"title": "III. BACKGROUND AND MOTIVATION", "content": "We first recall the basic formulation of online regular 3D-BPP, then present some carefully-designed preliminary experiments. These empirical results suggest a key factor that can help promote generalization (and adaptation) of DRL-based solvers, which motivates the design of ASAP.\nProblem Formulation. An online regular 3D-BPP instance can be defined by specifying two elements: a container of size (L,W,H) and a se-quence $(l_i, w_i, h_i)_{i=1}^n$ of n cuboid-shaped items of size $(l_i, w_i, h_i)$. In this problem, the solver needs to place each item in this sequence without knowing any information about the subsequent items under two constraints [4]: (1) non-overlapping constraint (i.e., placed items cannot intersect in the 3D space) and (2) containment constraint (i.e., placed items should fit inside the container) (details in ??). Once placed, an item cannot be moved. The objective is to place a maximum number of items in order to maximize the space utilization of the container, which represents the proportion of the volume used in the container. As such, it is upperbounded by 1. If the first T items fit in the container, it is defined as follows:\n$Uti = \\frac{\\sum_{i=1}^T l_iw_ih_i}{LWH}$"}, {"title": "B. Motivation", "content": "After a distribution shift, the performance of a trained DRL-solver may noticeably decrease, as shown in Section V B. Although this weak cross-distribution generalization implies that the greedy actions chosen by such DRL-based solver may not be good anymore, one question we may ask is whether it still ranks good actions high enough. To answer this question, we design the following first set of experiments assuming the test item distribution is known in order to compare the performance of Monte-Carlo Tree Search (MCTS) [34, 35] in two settings: either with the full action set At or with a pruned action set At, which is obtained by removing the actions with the lowest probabilities of being taken by the DRL-based solver. Recall that MCTS is an online method to select an approximately optimal action computed over many adaptively-sampled rollouts (possible futures). Although its main drawbacks, which are (i) its high computational costs and (ii) it needs to know the item distribution, make it unsuitable for directly solving online 3D-BPP, MCTS can approximate an optimal policy (with sufficiently-many rollouts).\nThe rationale for these experiments is that if the two versions of MCTS yield similar results then the pruned actions were indeed bad. On the contrary, if the results differ significantly, then good actions have been pruned. Our preliminary experiments (see in ??) only indicate a slight performance drop after pruning, which corroborates the first possibility, i.e., the DRL-based solver can identify bad actions even after distribution shift, although its greedy policy may not generalize well in cross-distribution settings.\nTo further explore the issue of distribution shift, we perform another set of experiments, which are designed to understand how good actions are ranked by a trained DRL-based solver. In these experiments, the greedy DRL-based solver is used to solve many randomly-sampled online 3D-BPP instances. More specifically, it is applied at every time step, except in a randomly-selected timestep i, where all the top-k actions (i.e., with highest probability) of this trained DRL-based solver are tried (see Figure 2 (a) for an illustration on one instance). For each of these top-k actions (or choice), we can then observe a posteriori if they lead to the highest space utilization. We can then compute the empirical frequency for each action rank to reach a highest reward. We call it Optimal Frequency, since it can be obtained if good actions were"}, {"title": "IV. METHOD", "content": "As shown in Section IIIB, the trained DRL-based policy could prune inherently bad actions but still fail to predict optimal probabilities of the remaining actions when facing cross-distribution instances. Motivated by this observation, we propose Adaptive Selection After Pruning (ASAP), which decouples pruning policy and selection policy \u03c0S. The pruning pol-icy \u03c0P removes inherently bad actions and outputs a pruned action set $A_t = \\{a \\sim \\pi^P(a | S_t)\\}$. After that, the selection policy \u03c0S predicts the proba-bilities of actions in At and samples the action by $a_t \\sim \\pi^S(\\cdot| S_t) \\in A_t$. Although the pruning policy's goal differs from that of the selection policy, both output a probability distribution over actions. This implies they can utilize the same network architecture. Following previous work [13], we adopt non-spectral Graph Attention Networks (GATs) [36] and the ACKTR algorithm [37] as the network and backbone training algorithm of our policy. Figure 3 illus-trates the overall scheme of our method during training and inference.\nTraining of Pruning Policy. We follow the statistical definition for the potential errors. In our problem setup, type I error refers to the persistence of poor actions (e.g. the policy cannot find the op-timal actions) while type II error indicates the policy has pruned the valuable actions. As illustrated in Figure 3, pruning policy prunes bad actions from the action set. However, sometimes it may prune valuable actions, causing the type II error. Due to the decoupled policy framework, type I errors can be mitigated by the selection policy after the pruning. Whereas, type II error may directly cause a performance drop.\nIn such cases, randomly sampling and evaluating ac-tions from a pruned set is inefficient. By leveraging the selection policy to sample actions, we can speed up convergence, even though this may increase type I errors. The pruning policy can still be trained with a policy gradient approach as follows:\n$L(\\theta) = \\sum_t log \\pi(a_t|s_t)A_t,$\nwhere $a_t \\sim \\pi^S(\\cdot|s_t) \\in A_t$ denotes the action sampled by selection policy \u03c0S at timestep t and At denotes the advantage estimates.\nTraining of Selection Policy. The selection policy works similarly to the DRL-based policies [12, 13]. The only difference is that it samples ac-tions from the pruned action set At instead of At. Consequently, the training process for the selection policy follows existing DRL-based methodologies, ex-cept that its exploration is confined to actions within At.\nTwo-phase Training. As mentioned previously, the pruning and selection policies assist each other during training, potentially leading to mutual interference. With DRL promoting exploration of various actions, even poor ones, a fault in one policy can degrade overall performance and corrupt the gradient of the other. Hence, starting training from scratch often results in a local optimum. To tackle this is-sue, we propose a two-phase training method, which involves an initialization phase and a finetune phase. By initializing the two policies, we could address the interference issue and reduce the type I and type II errors of the pruning policy."}, {"title": "B. Policy Online Adaptation", "content": "The policy online adaptation also follows the MAML framework. As observed in Section IIIB, the pruning policy generalizes to other distributions better than the selection policy. The reason is that the pruning policy aims to propose top-K candidates whose strategy is more universal across different dis-tributions. Meanwhile, the selection policy aims to find the optimal action, where the decision is more domain-specific. Therefore, when encountering new instances with unknown distributions, we finetune only the selection policy while fixing the pruning pol-icy. The finetuning step follows the line 6 of Algo-rithm 1. This approach enables the selection policy to adapt more quickly by concentrating on actions within the pruned action set, rather than using a single policy for both pruning and selecting. A thorough analysis of the impacts resulting from this decomposition is provided in Section V D."}, {"title": "V. EXPERIMENTAL RESULTS", "content": "We designed a series of experiments in order to demonstrate the performance of ASAP in different aspects. To illustrate the generalization capability of ASAP, we show the results on a set of datasets of different distributions. To further show the capability of the proposed method, we conducted experiments in both discrete and continuous solution space.\nFollowing Zhao et al. [13], we use the space utilization shown in Equation (1) as our evaluation metric. We measure the performance with and without adaptation to reflect the generalization and adaptation capability. The improvement in space utilization with adaptation is reported to high-light the method's adaptation capabilities.\nEnvironmental Setting and Dataset. We evaluate in the most common online 3D-BPP setting [4, 13] where both non-overlapping and containment constraints are enforced. The container sizes are equal for each dimension, i.e., L = W = H = 20. We prepare In-distribution (ID) and Out-of-distribution (OOD) datasets for both discrete and continuous environments. The ID dataset contains 4 subsets, which is Default, ID-Large, ID-Medium, and ID-Small, while the OOD dataset contains 3 sub-sets: OOD, OOD-Large and OOD-Small. Following Zhao et al. [13], in the discrete environment, we define an item set for each subset as follows: De-fault (l, w, h\u2208 {2,4,6,8,10}), ID-Large (l,w,h \u2208 {6,8,10}), ID-Medium (l, w, h \u2208 {4,6,8}), ID-Small (l,w,h\u2208 {2,4,6}). Meanwhile we have out-of-distribution datasets as OOD (l, w, h \u2208 [1,11]), OOD-Large (l,w,h \u2208 [6,11]) and OOD-Small (l,w,h \u2208 [1,6]). For each subset, 100 random distributions are sampled from its item set, and each distribution gener-ates 64 instances. In the continuous environment, this process is followed by an additional step that augment with random noises in the range of [-0.5,0.5] to the length, width, and height of the generated items.\nTraining and Adaptation Setups. To ensure a fair comparison, we also train and adapt each com-parison method as follows. For the policy training, we set the total number of training epochs to 300. For our proposed ASAP, which involves two-phase of training, we allocate 250 epochs for the policy ini-tialization and 50 epochs for the policy finetuning. Within each epoch, the policy solves 200 batches of instances, which are online generated using the item set of the Default dataset. During adaptation, each comparison method finetunes its trained policy using 200 batches of instances generated from the same dis-tribution used to create the test subset. Note that the number of items used for adaptation is 1/300 of the training set to mimic the real-world application of quick online adaptation. The batch size we use is 64 for both training and adaptation.\nComparison Methods. We choose to compare with SOTA methods which are (i) strictly adhere to the online setting, i.e., policy can only observe the current item being packed, and (ii) can learn to adapt to new distributions. Hence, we set the following SOTA"}, {"title": "B. Performance on Discrete Environments", "content": "We first conducted experiments on discrete environments, where we evaluated both the In-distribution generalization and OOD-generalization performance, the results are shown in Table I and Table II.\nIn-distribution Datasets. As illustrated in Ta-ble Table I, the performance without adaptation (marked as w/o adaptation) demonstrates the gen-eralization capabilities of the policy. As shown in Table, PCT achieves the best results on the Default and ID-Medium datasets, when reas GOPT performs best on the ID-Large and ID-Small datasets among the baseline methods. This reveals the generaliza-tion challenges faced by SOTA DRL-based methods, which motivates us to perform adaptation on trained DRL policies. Compared to baseline methods, ASAP achieves a maximum (resp. minimum) increase of 2.9% (resp. 2.0%) on the Medium (resp. Small) dataset. This highlights the generalization capabili-ties of the proposed method.\nTo further illustrate the adaptation capability of different methods, we use improvements to measure the performance increase between w/o and w/ adaptation. From Table I, it is evident that the baseline policy has limited adaptation improvement, with a maximum of 0.5% across all in-distribution datasets. In some cases, such as AR2L's adaptation to the ID-Large dataset, there were even negative effects. This indicates that baseline methods need extensive data to adapt effectively to cross-distribution scenarios. Conversely, ASAP achieves the highest adaptation improvement across all datasets, with a peak improvement of 0.9% and a minimum of 0.3%. The limited improvement on the Default dataset is mainly due to ASAP uses its item set to generate training instances.\nOOD Datasets. The experiments on the OOD dataset are more challenging since the domain shift is even more significant. Such domain shift significantly reduces the generalization performance. Nevertheless, as shown in Table II, ASAP manages to achieve a performance increase ranging from 1.3% to 1.8% over the best baseline methods. The heightened complexity provides more opportunities for adaptation improve-ments. For instance, AR2L achieves the best adaptation improvement with a maximum of 0.9% on the OOD-Small dataset. Meanwhile, ASAP demonstrates a superior adaptation, ranging from 0.9% to 2.2%."}, {"title": "C. Performance on Continuous Environments", "content": "3D-BPP in the continuous environment is consid-ered a much harder problem [13] since it has a larger solution space compared to discrete environments. We also divide the experiments into In-distribution generalization and OOD generalization, shown in Table I and Table II.\nIn-distribution Datasets. The continuous en-vironment involves more diversity in terms of item shapes, therefore increased significantly the problem's complexity. In such setup, PCT shows better average performance than AR2L across datasets, except for the In-distribution Small dataset. This is because AR2L emphasizes worst-case scenarios, enhancing its minimum performance. Compared to the best baseline methods, ASAP achieves a performance increase ranging from 1.9% to 3.0%. In terms of adapta-tion, ASAP attains improvements between 0.2% and 0.8%, surpassing the baseline methods on all cross-distribution datasets.\nOOD Datasets. In the continuous environ-ment, the performance drop from in-distribution to OOD datasets is less significant. The reason is that the environment is diverse enough, and the trained policy has already adapted to handle items of higher complexity. Compared to the best baseline methods, ASAP achieves a performance increase of up to 3.3% at maximum and 2.2% at minimum. This further il-lustrates the robust generalizability of ASAP. Regard-ing the performance gain of online adaptation, ASAP w/ adaptation shows a clear improvement with a max-imum enhancement of 1.3% and a minimum of 2.1%, indicating robust performance increase across differ-ent OOD distributions."}, {"title": "D. Ablation Study", "content": "To further show the advantage of each proposed module, we did experiments with two additional variants of the proposed methods, namely ASAP w/o MAML and ASAP w/o Decouple, also shown in Table I and Table II. ASAP w/o Decouple removes the decoupled-policy design to show the benefit the decoupled policies, while ASAP w/o MAML removes the MAML-based initialization.\nFrom the results it is evident that ASAP w/o Decouple, achieves superior generalization compared to the best baseline methods across all in-distribution and OOD datasets in both discrete and continuous environments. However, though MAML aims at rapid adaptation, its adaptation improvement is not so sig-nificant as ASAP.\nMeanwhile, the performance of the ASAP w/o MAML without adaptation demonstrates that the decoupled-policy design also enhances generalization. It benefits from the decoupled functionality of the two policies, where we have the pruning policy fo-cusing on the domain-free proposal and the selec-"}, {"title": "VI. CONCLUSION", "content": "We present Adaptive Selection After Pruning (ASAP), a DRL-based 3D-BPP solver that can rapidly adapt to cross-distribution instances by de-coupling the pruning and selection policy. Experiments demonstrate that ASAP outperforms SOTA DRL-based solvers with excellent generalization and adaptation capabilities on in-distribution and out-of-distribution instances. For future work, We will extend this method to other decision-making scenarios."}]}