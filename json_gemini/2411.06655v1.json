{"title": "Explore the Reasoning Capability of LLMs in the Chess Testbed", "authors": ["Shu Wang", "Lei Ji", "Renxi Wang", "Wenxiao Zhao", "Haokun Liu", "Yifan Hou", "Ying Nian Wu"], "abstract": "Reasoning is a central capability of human intelligence. In recent years, with the advent of large-scale datasets, pretrained large language models have emerged with new capabilities, including reasoning. However, these models still struggle with long-term, complex reasoning tasks, such as playing chess. Based on the observation that expert chess players employ a dual approach combining long-term strategic play with short-term tactical play along with language explanation, we propose improving the reasoning capability of large language models in chess by integrating annotated strategy and tactic. Specifically, we collect a dataset named MATE, which consists of 1 million chess positions with candidate moves annotated by chess experts\u00b9 for strategy and tactics. We finetune the LLaMA-3-8B model and compare it against state-of-the-art commercial language models in the task of selecting better chess moves. Our experiments show that our models perform better than GPT, Claude, and Gemini models. We find that language explanations can enhance the reasoning capability of large language models.", "sections": [{"title": "1 Introduction", "content": "Rational thought and deliberate cognition rely heavily on reasoning, a core component of human intelligence(Garnham and Oakhill, 1994). Given sufficient information, people can logically progress through a sequence of steps. In the field of artificial intelligence(Russell and Norvig, 2016), it has been a persistent objective to study the reasoning capability, as it is essential for both problem-solving and decision-making processes."}, {"title": "2 Related Work", "content": "Chess has historically been esteemed as a challenging intellectual pursuit(Thrun, 1994). With all the rules and the chess board provided, it is a pure reasoning task without any uncertainty or randomness. In 1997, Deep Blue, created by IBM, defeated the chess world champion\u2014Russian player Garry Kasparov\u2014in a match that astonished the world. Modern chess engines such as Stockfish, AlphaZero(Silver et al., 2017), Leela Chess Zero, which integrate search algorithms, deep neural networks, and reinforcement learning, play significantly better than the strongest human players. Recent work(Ruoss et al., 2024) trains a transformer model on millions of annotated chess games, enabling it to play precise and beautiful chess.\nThough chess is a \u201csolved problem\" in the field of artificial intelligence, many researchers used it as"}, {"title": "3 MATE", "content": "We propose the MATE(Move on strAtegy and Tactics datasEt) for exploring the reasoning capability of large language models in chess. In chess, mate is known as checkmate, which occurs when a king is placed in check and has no legal moves to escape. Checkmating the opponent wins the game.\nWe collect around 1 million chess positions from the open source chess server Lichess. The positions are either selected from chess games or chess puzzles. These specific board positions ask players to play moves to achieve a particular goal, such as checkmating or gaining a material advan-"}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Experiment Setup", "content": "We train our models using the pretrained Llama-3-8B model(Dubey et al., 2024) as the foundation. The models are finetuned with llamafactory(Zheng et al., 2024), employing a cosine learning rate scheduler with 3% warm-up steps. We set the maximum learning rate to $5 \\times 10^{\u20136}$. We use DeepSpeed ZERO Stage 3 (Rajbhandari et al., 2020) across 4\u00d7H100 GPUs. We train the models for 5 epochs.\nWe incorporate specific tokens in FEN format to enhance the foundation model's understanding of chessboard positions. We add the <line> token to separate each row of the board and the <color> token to indicate which side is to move next. Our experiments show no significant difference in performance with or without these special tokens.\nWe train four models with MATE-No-Explanation(MATE-N), MATE-Strategy(MATE-S), MATE-Tactic(MATE-T), and MATE-Strategy&Tactic(MATE-ST), respectively.\nWe compare our models with the following base-lines:"}, {"title": "4.2 Results", "content": "Our experimental results in Table 1 shows: (i) MATE proves sufficiently complex to differentiate among commercial LLMs. Our results demonstrate that the o1-preview model leads in performance by a substantial margin. (ii)Interestingly, prompting strategies do not significantly impact performance in our task. We observe no substantial improvement in performance when adopting a few-shot setting compared to a zero-shot setting. (iii)Our models exhibit superior reasoning capabilities compared to commercial models, as demonstrated by their performance across various test sets.\nLanguage enhances chess-reasoning in language models. While some researchers argue that language is not used for reasoning(Fedorenko et al., 2024), our findings lead us to a contradictory"}, {"title": "5 Conclusion", "content": "We propose a method to enhance LLMs' chess-reasoning capabilities by incorporating strategy and tactic annotations. We craft the MATE, train our models and compare them against state-of-the-art commercial language models. Our models outperform others in the chess-reasoning task. We find language helps language models' reasoning. We demonstrate combining long-term intuition with"}, {"title": "Limitation", "content": "Although the idea of combining strategy and tactics is prevalent in all games, we only study chess. A comprehensive study of multiple game types should demonstrate this approach's effect better.\nWe use chess puzzles to test the models' ability, asking the model to choose between two plausible moves. This is a common way for professional players to exercise. However, the ideal scenario would require running a complete game on the chess engine to test a model's full strength and ability to carry out strategy and tactics.\nOur dataset is annotated by chess experts. However, we acknowledge that potential biases may exist in determining appropriate strategies for various positions and in evaluating post-tactical situations. Furthermore, the limited number of chess experts may only capture the thought processes of a subset of all players.\nOur experiment only uses LLaMA-3-8B for fine-tuning, so we don't understand how the improvement changes to model sizes and base model quality."}, {"title": "A Appendix", "content": ""}, {"title": "A.1 Chess Notation", "content": "FEN Forsyth-Edwards Notation, abbreviated as FEN, is the standard method for describing chess positions. This system was developed by Steven J. Edwards, a computer programmer, who adapted an earlier notation created by journalist David Forsyth. Edwards' modifications made the notation compatible with chess software, enhancing its utility in the digital age.\nFEN encodes chess positions using the following elements:(1) Piece positions: Capital letters for white pieces, lowercase for black. Numbers indicate empty squares. (2) Active color: w for white's turn, b for black's. (3) Castling rights: K means white kingside, Q means white queenside, k means black kingside, q means black queenside. (4) En passant target square: If a pawn has just moved two squares, this is the square behind it. (5) Half-move clock: Moves since the last pawn advance"}, {"title": "A.2 Chess Strategy", "content": "We elaborate on the details of each strategy, including the criteria we use to identify them.\nMaterial Count It is a fundamental strategy, particularly for beginners. While the game ultimately aims for checkmate, having a material advantage often influences the result more frequently. Each piece is assigned a specific value, and understanding these values helps players assess their position. When other elements are relatively equal, prioritizing material acquisition can lead to a decisive advantage in the game. This strategy is most relevant when there is an imbalance in material comparison and both kings are safe. It generally applies to most types of positions, though king safety may occasionally take precedence.\nPiece Activity It is an advanced strategy, focuses on the placement and effectiveness of pieces rather than just their assigned value. In some situations, players may have an equal material count, but the effectiveness of their pieces can vary significantly. Pieces positioned centrally are typically more powerful, allowing for greater control and flexibility. This strategy is especially relevant in dynamic positions where the mobility of pieces can lead to tactical opportunities. Focus on piece activity when there is a marked difference in piece positioning, such as when some pieces occupy central squares while others remain in the corners. This is especially crucial in dynamic positions, particularly when one side is attacking.\nSpace Gaining a spatial advantage is closely related to piece activity and can greatly impact a player's effectiveness. When one side controls"}, {"title": "A.3 Chess Tactic", "content": "Here we list several common tactics in chess:\nPin Pin tactics occur when an attacked piece cannot move without exposing an even more valuable piece (or target) behind it.\nFork A fork is a type of double attack whereby a single piece makes multiple threats.\nBattery In chess, a battery refers to lining up two or more pieces on the same diagonal, rank or file. Only queens, rooks and bishops can form a battery."}, {"title": "A.4 Difficulty Levels of Sub-Datasets", "content": "Our MATE consists of 4 sub-datasets: MATE-N, MATE-S, MATE-T, and MATE-ST. We conduct two experiments to study the difficulty levels of chess board positions across all these sub-datasets through both human and automatic assessment."}, {"title": "A.5 Case Study", "content": "We pick a sample case with both strategy and tactic annotated, and show the responses from three language models. See Figure4, Figure5, and Figure6."}]}