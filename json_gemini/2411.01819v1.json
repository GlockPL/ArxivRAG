{"title": "DiffuMask-Editor: A Novel Paradigm of Integration\nBetween the Segmentation Diffusion Model and Image\nEditing to Improve Segmentation Ability", "authors": ["Bo Gao", "Fangxu Xing", "Daniel Tang"], "abstract": "Semantic segmentation models, like mask2former, often demand a substantial\namount of manually annotated data, which is time-consuming and inefficient to\nacquire. Leveraging state-of-the-art text-to-image models like Midjourney and\nStable Diffusion has emerged as an effective strategy for automatically generating\nsynthetic data instead of human annotations. However, prior approaches have been\nconstrained to synthesizing single-instance images due to the instability inherent\nin generating multiple instances with Stable Diffusion. To expand the domains\nand diversity of synthetic datasets, this paper introduces a novel paradigm named\nDiffuMask-Editor, which combines the Diffusion Model for Segmentation with\nImage Editing. By integrating multiple objects into images using Text2Image\nmodels, our method facilitates the creation of more realistic datasets that closely\nresemble open-world settings while simultaneously generating accurate masks.\nOur approach significantly reduces the laborious effort associated with manual an-\nnotation while ensuring precise mask generation. Experimental results demonstrate\nthat synthetic data generated by DiffuMask-Editor enable segmentation methods\nto achieve superior performance compared to real data. Particularly in zero-shot\nbackgrounds, DiffuMask-Editor achieves new state-of-the-art results on Unseen\nclasses of VOC 2012. The code and models will be publicly available soon.", "sections": [{"title": "Introduction", "content": "Semantic segmentation [1, 2] is a critical computer vision component with profound implications\nacross diverse applications. It is the cornerstone for achieving nuanced scene understanding, enabling\nadvancements in autonomous driving, intelligent surveillance, and robotic navigation by deciphering\nintricate visual environments. Additionally, the precision of semantic segmentation significantly\nenhances object recognition and localization, thereby improving tasks like image retrieval and object\nidentification. However, existing semantic segmentation models largely depend on robust, manually\nannotated data, which can be prohibitively expensive. For example, precise pixel-level annotations\nare required in medical imaging to delineate the boundaries of organs and structures, demanding\nexpertise and meticulous attention.\nTo address this issue, some previous works have shifted focus to weak supervision, which involves\ntraining models with a smaller scale of data, such as image-level labels [3\u20137] and bounding boxes [8]."}, {"title": "Related Work", "content": "Text-to-image diffusion models have become a significant trend in image generation. Following\nthe publication of \"Diffusion Models Beat GANs on Image Synthesis\" [20], an increasing number\nof diffusion model variants have emerged, with applications spanning image editing, image super-\nresolution, and more. For instance, Imagic [13] introduces a text-to-image diffusion model that\naligns a text embedding with both the input image and target text, then fine-tunes the model to capture\nimage-specific appearances for image editing purposes. IDM [14] proposes a denoising diffusion\nmodel with an implicit neural representation featuring a scale-controllable conditioning mechanism\nto address common issues such as over-smoothing and artifacts in continuous image super-resolution.\nAccording to DiffuMask [15], attention maps can be used as mask annotations roughly since the\ncross-attention layer is the only way through which text during the diffusion process can influence\nthe image denoising, thus the images in the attention layer will exhibit 'attention' towards the objects\ndescribed in the text."}, {"title": "Strongly and Weakly Supervised Semantic Segmentation", "content": "Due to the time-consuming and labor-intensive nature of strong supervision involving manual\nannotation, many researchers have shifted focus toward weakly supervised training approaches.\nThese methods do not rely on pixel-wise labels but use simpler annotations such as points, bounding\nboxes [8], and image classification tasks [3\u20137], but they also provide the lowest precision. On the\nother hand, bounding boxes offer more effective results than fully supervised training, though they still\nrequire significant annotation. Another limitation of these approaches is their confinement to closed-\nset object categories. To overcome these challenges, models like DiffuMask [15], DiffusionSeg\n[16], ODISE [21], DiffSegmenter [22], and DifFSS [23] leverage powerful diffusion models\nin weakly-supervised semantic segmentation. They utilize attention maps within text-conditional\ndiffusion models to generate rich data in image-mask pairs. These methods achieve promising results\nefficiently and effectively by correlating text prompts with foreground objects."}, {"title": "Methodology", "content": "In this paper, we explore the integration of semantic segmentation and image editing using existing\npretrained diffusion models. We efficiently train existing segmentation modules using images that\nare first generated and then edited, along with their precise masks, and finally applied in open-world\nscenarios.\nIn generating datasets, the key shift in our approach is from acquiring precise masks to editing images,\nfacilitated by a precise and imposed locating manner for mask annotation. In an open world with\nnumerous objects, we face three primary challenges. First, it is essential to determine which objects\ncan be appropriately added to generated images. For example, in an image of an airport produced\nby a diffusion model, it is logical to add airplanes but not giraffes. Second, it is crucial to decide\nwhere these objects should be placed within the image to ensure they fit the scene appropriately. For\ninstance, a tree should be rooted on the ground rather than appearing to float in the air. Third, we must\naddress discrepancies in physical conditions, such as lighting differences between the foreground\nobjects and the background, to enhance the overall harmony.\nTo address these challenges, we propose a two-step strategy. Initially, we produce images with a\nsingle object per image and their corresponding masks, similar to the approaches used by DiffuMask\n[15] and DiffusionSeg [16] (Sec. 3.1). Subsequently, we proceed to the second phase of image\nediting to tackle the aforementioned issues(Sec. 3.2)."}, {"title": "Single-object and Mask Generation", "content": "Text-to-Image models(e.g., Stable Diffusion [18], Imagen [24], DALLE.2 [12]) are conditional\ndiffusion models, with text prompt $C_{text}$ to guide and influence the denoising process from the input,\na Gaussian noise $z_t$ to the latent image $z_0$. Specifically, Stable Diffusion consists of a text encoder $S_0$,"}, {"title": "Cross Attention Map in the Diffusion Model", "content": "a variational auto-encoder (VAE), and a conditional UNet $\\epsilon_\\theta$. During the progress, the text prompt\n$C_{text}$ is input into the cross-attention layers to be fused with the visual latent embeddings. This\nadvanced fusion mode allows us to use the cross-attention map to help the model concentrate on the\nobjects referred to in the texts. For a given step t, assuming $z_t \\in R^{H \\times W \\times C}$, where H is the height,\nW is the width, and C reflects the number of channels, it is linearly projected into the Query matrix\n$Q = Z_t W_q$. The text prompt $C_{text}$ is processed by the text encoder $S_0$, producing the text embeddings\n$X = S_0(C_{text}) \\in R^{L \\times d}$, which are linearly transformed into a Key matrix $K = XW_k$ and Value\nmatrix $V = XW_v$. Here, $W_q, W_k$, and $W_v$ are linear transformations applied to the input features to\nobtain the query, key, and value. The cross attention map at the step t and the $n_{th}$ layer of UNet is\ncalculated by the below formula:\n$M_{n,t}^c = \\text{Softmax}(\\frac{QK^T}{\\sqrt{d}})$\nwhere $M_{n,t}^c \\in R^{H \\times W \\times L}$ after projection and d reflects the features' dimension."}, {"title": "Mask Generation and Refinement", "content": "Observations from [17] indicate that using different ranges of timesteps only marginally affects the\nfinal result. Therefore, averaging these cross-attention maps over layers and timesteps is a practical\nmethod for achieving feature fusion, applied to four different resolutions: 8 \u00d7 8, 16 \u00d7 16, 32 \u00d7 32,\nand 64 \u00d7 64 assuming the input size (width, height) is (512, 512).\n$M_c = \\frac{1}{N \\times T} \\sum_{n=1}^{N} \\sum_{t=0}^{T} M_{n,t}^c$\nAlthough the cross-attention maps $M_c$ provide an indication of the location of the target classes in\nthe image, they still appear vague and inaccurate. Following the approach of DiffuMask [15], which\nassociates different classes of objects with different segmentation thresholds, we refine the vague\nmasks. By utilizing AffinityNet [3], which helps identify object boundaries or perform pixel-level"}, {"title": "Image Editing", "content": "By innovatively transforming segmentation tasks into image editing tasks, precise segmentation\nmasks for foreground objects can be easily obtained through the second step of localization. Firstly,\nwe construct an adaptive matching lexicon by leveraging the vast array of terms in text-image pairs\nfound on the internet, aiming to collect foreground objects that semantically fit the background.\nSecondly, we employ a novel discriminative network to localize the selected objects, thereby ensuring\ntheir geometric coherence. Lastly, we achieve harmonization between foreground and background\ntasks in terms of physics through image harmonization. Sec. 3.2.1 will introduce how to construct\na matching lexicon to select semantically appropriate foreground objects. Sec. 3.2.2 will discuss\nthe application of a fast discriminative network for foreground object localization and dimension\nstandardization. Sec. 3.2.3 will explain the final step of image harmonization, aiming to normalize\nand unify the physical conditions, such as the lighting of foreground objects with the background."}, {"title": "Adaptive Matching Thesaurus", "content": "The training process of Stable Diffusion [18]leverages the extensive LAION dataset [25], which\nprovides a rich array of text-image pairs and deriving semantic alignment between foreground\nobjects and backgrounds directly from image feature analysis can be overly complex and may lead\nto overfitting in network design. Therefore, beginning with text-based analysis is deemed more\nreasonable and efficient. In this paper, we construct a comprehensive matching lexicon to establish\ncorrespondences in an open-world scenario, measuring the semantic similarity between foreground\nobjects and backgrounds from a language understanding perspective. To quantitatively assess this\nmatching, we calculate probability values based on the occurrence positions and frequencies of\nforeground object terms within a large text-image-pair database. These probabilities serve as a\nmeasure of similarity. The following formula is used to calculate the semantic similarity between\nobject i and background j:\n$\\text{Similarity} <o_i, b_j> = \\frac{\\sum_{\\text{text}_k \\in \\text{Texts}_j} (o_i \\in \\text{text}_k)}{\\sum_{\\text{text}_k \\in \\text{Texts}_j} \\text{text}_k}$\nwhere $\\text{Texts}_j$ represents the set of all text prompts containing the background bj, and $\\text{text}_k$ denotes\neach element within this set..\nUsing this formula, we can establish a semantic summary table for foreground and background, as\nshown in 2, thus enabling the collection of appropriate object classes for embedding."}, {"title": "Foreground Object Location", "content": "Due to the fast speed, efficiency, and time-saving characteristics, the Fast Object Placement Assess-\nment (FOPA) [26] directly generates a heatmap to identify the optimal position, using the foreground\ntarget image, its segmentation binary mask, and the background image as inputs. The goal of this\nmodel is to evaluate all positions in a single pass, akin to pixel-level classification tasks in semantic\nsegmentation. FOPA [26] utilizes a U-Net-like [27] structure (Encoder and Decoder) to encode the\nfeatures of foreground targets and backgrounds. As the SOPA model [28] has already demonstrated\nthe capability to learn background and foreground features, it directly utilizes the SOPA decoder\nwithin the FOPA model's architecture, as mentioned in the \"Background Prior Transfer\" section. Ad-\nditionally, following the feature loss approach introduced in literature [29, 30], FOPA [26] leverages\neach pixel-wise output feature to mimic the image-level features of corresponding composite images\nfrom the SOPA model [28], where each pixel on the output feature map corresponds to a composite\nimage obtained by pasting the foreground at this location.\nAfter this stage, we generate heatmaps of newly added objects. To determine the final location\n(accurate masks), we also need to identify the center point of the heatmap as our objects' center"}, {"title": "Image Harmonization", "content": "Image harmonization is more like a style transfer task, but how to choose the normalization layer in\nthe model is a question worth considering. There are several normalization methods, such as Batch\nNormalization(BN) [31], Layer Normalization(LN) [32], Instance Normalization (IN) [33], and\nGroup Normalization(GN) [34]. The Batch Normalization (BN) [31] or Instance Normalization (IN)\n[32], using the same mean and variance as those of the background features, results in the background\nfeatures being influenced by statistics from the foreground, thereby limiting the ability to learn style\nconsistency in subsequent layers. Layer Normalization (LN) [32] and Group Normalization (GN)\n[34], based on channel-wise normalization, can significantly disrupt the features of each channel,\nthus undermining the effective transfer of the background features to the foreground objects.\nTherefore, we adopt RAIN [35] as our starting point and utilize the efficient and effective AdaIn\nplugin for our image harmonization component. The new plugin avoids the shortcomings of the\nnormalization methods mentioned above by separately extracting features from the background and\nforeground objects, which exclusively transfers statistics from background features to normalized\nforeground features, independent of the inconsistent foreground objects. Through the combination of\nthe UNet-decoder and AdaIn plugin, we could gain the harmonious images as displayed in 1."}, {"title": "Experiments and Analysis", "content": "The VOC2012 dataset comprises hundreds of images annotated with\nbounding boxes and pixel-wise segmentation masks for 20 object classes, facilitating tasks like object"}, {"title": "Implementation Details", "content": "Synthetic data for training. We follow the details from DiffuMask [15]. Specifically, for Pascal-\nVOC 2012 [36], we augment 10k images per category and filter out 7k images. Consequently, we\nassemble a final training set of 60k synthetic images across 20 classes, all with a spatial resolution of\n512x 512 pixels. Regarding Cityscapes [40], we focus on evaluating two significant classes, namely\n'Human' and 'Vehicle,' encompassing six sub-classes: person, rider, car, bus, truck, and train.\nWe generate 30k images for each sub-category, with a final selection of 10k images per class.\nThe basic tools. We leverage foundational components such as pre-trained Stable Diffusion [18],\nthe text encoder from CLIP [41], object placement from FOPA [26], and image harmonization\ntechniques [35] as foundational components. And we utilize Mask2Former [19] as the baseline model\nfor dataset evaluation. Without finetuning Stable Diffusion or training any module for individual\ncategories, we maintain parameter optimization and settings consistent with the original papers,\nincluding initialization, data augmentation, batch size, and learning rate. All experiments are\nconducted using 8 Tesla A100 GPUs."}, {"title": "Comparison with State-of-the-art Methods", "content": "For the VOC2012 dataset [36], as shown in Table 1, our model outperforms the competition in nearly\nall of the 20 classes. Compared to DiffuMask [15], when trained with purely synthetic data, our\nDiffuMask-Editor enhances the mIOU by nearly 8% (from 57.4% to 62.5%) using Resnet50 [42] and\nfrom 70.6% to 72.0% using Swin-B [42]. In the \"Finetune on Real Data\" segment, which combines\n60,000 synthetic images with 5,000 real images, the mIOU also increases by 9%. Most notably, for\ncategories such as \"bird,\" \"boat,\" \"cat,\" \"chair,\" and \"sofa,\" our model demonstrates exceptionally\nstrong performance, exceeding that achieved through training on real data by a significant margin,\nwith an average gap exceeding 2%."}, {"title": "Semantic Segmentation", "content": "Regarding the Cityscapes dataset [40], our model continues to show robust capabilities. Compared\nto DiffuMask [15], we have significantly reduced the performance gap with training on actual data,\ndecreasing the difference from 10% to single digits. Moreover, relying solely on generated data, we\nhave improved our previous method from 70% to over 80%, surpassing the 80% mark."}, {"title": "Zero-Shot Segmentation", "content": "Despite our model not being fine-tuned, it retains the capability for open-world segmentation. As\nindicated in Table 3, compared to many previous methods trained exclusively on real images and\nmanually annotated masks, it achieves state-of-the-art results in zero-shot scenarios. All data auto-\nmatically generated by DiffuMask-Editor are synthetic, eliminating the laborious manual annotation\nand dataset construction processes. We have also expanded the scope from single objects to multiple\ninstances, enriching the dataset without introducing significant additional effort. Even when using the\nCOCO dataset [47]to predict pseudo labels, which incurs higher computational costs, our approach,\nrelying only on synthetic data, achieves a promising result of 66.6% on unseen classes, representing\nan almost 3% improvement over previous methods. More visualization results are in (A)."}, {"title": "Ablation Study", "content": "Table 4 displays that there is a 3% gap between 'without Mask Refinement'\nand our method. It means that at the first stage, the cross-attention maps are so vague and inaccurate\nthat the noisy data could influence the performance and robustness of the segmentation models."}, {"title": "Analysis", "content": "In our experiments, we recognize the limitations of Stable Diffusion in generating multiple instances,\nas previously demonstrated in multi-category generation [48]. Therefore, DiffuMask [15] is\nintentionally limited to single foreground objects, potentially introducing dataset bias. In real-world\nscenarios, backgrounds rarely consist of solitary objects. Thus, the dataset curated by DiffuMask [15]\nmay predispose models to such scenarios, complicating performance evaluation on diverse datasets.\nFurthermore, our proposed DiffuMask-Editor serves as a data augmentation technique that not\nonly enriches the dataset but also broadens its scope, thereby enhancing the segmentation model's\nlearning capabilities, especially in real-world scenarios. Moreover, by transforming the task of\nobtaining precise masks into an image editing task, we can effortlessly obtain mask annotations\nsimultaneously while localizing the targets. Since we specify the positions ourselves through the\nmodel, it is straightforward to have accurate mask annotations. Compared to the traditional approach\nof deriving masks from images, our reverse thinking easily addresses the issue of inaccurate masks,\nand precise masks will naturally significantly improve the performance of segmentation models."}, {"title": "Why DiffuMask-Editor is superior to DiffuMask?", "content": "As illustrated in Table 3, we thoroughly examined the model's performance in real-world scenarios.\nOur model exhibits state-of-the-art performance across previously unseen categories, thus affirming\nits dominance in the realm of open-world data analysis. Further analysis revealed that the model's\nexceptional ability to handle zero-shot scenarios stems not only from its architectural design but also\nfrom the diversity and richness of the training data. Unlike earlier approaches that struggled with\ndata scarcity and domain-specific biases, we proactively enhanced our training dataset with a broad\narray of foreground objects from diverse domains. This strategic enhancement not only expands\nthe model's exposure to a variety of visual stimuli but also ensures the seamless integration of new,\nunseen elements into its predictive framework."}, {"title": "Bridge the gap between the close-set and open-world.", "content": "This paper introduces a novel approach for generating segmentation masks by integrating semantic\nsegmentation with image editing, thereby eliminating the need for manual annotation through the\nuse of powerful generative models. Distinct from previous methods, our model is capable of"}, {"title": "Conclusion", "content": "centerx = argmax Fh (x, y), w = centerx \u03b3\u03c9 * \u03c9\u03bf/2\n(x,y)\ncentery = argmax Fh (x, y),\nh = centery\n(x,y)\nYh * ho/2\nwhere Fh (x, y) indicating the heatmap, Yw and yn respectively representing the scale of width and\nheight, wo and ho respectively noting the width and height of object image."}]}