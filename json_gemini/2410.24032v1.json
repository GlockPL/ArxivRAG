{"title": "Navigating the Unknown: A Chat-Based Collaborative Interface for Personalized Exploratory Tasks", "authors": ["YINGZHE PENG", "XIAOTING QIN", "ZHIYANG ZHANG", "JUE ZHANG", "QINGWEI LIN", "XU YANG", "DONGMEI ZHANG", "SARAVAN RAJMOHAN", "QI ZHANG"], "abstract": "The rise of large language models (LLMs) has revolutionized user interactions with knowledge-based systems, enabling chatbots to synthesize vast amounts of information and assist with complex, exploratory tasks. However, LLM-based chatbots often struggle to provide personalized support, particularly when users start with vague queries or lack sufficient contextual information. This paper introduces the Collaborative Assistant for Personalized Exploration (CARE), a system designed to enhance personalization in exploratory tasks by combining a multi-agent LLM framework with a structured user interface. CARE's interface consists of a Chat Panel, Solution Panel, and Needs Panel, enabling iterative query refinement and dynamic solution generation. The multi-agent framework collaborates to identify both explicit and implicit user needs, delivering tailored, actionable solutions. In a within-subject user study with 22 participants, CARE was consistently preferred over a baseline LLM chatbot, with users praising its ability to reduce cognitive load, inspire creativity, and provide more tailored solutions. Our findings highlight CARE's potential to transform LLM-based systems from passive information retrievers to proactive partners in personalized problem-solving and exploration.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in large language models (LLMs) [1, 10, 32] have transformed user interactions with knowledge-based systems by enabling chatbots to synthesize vast amounts of information, surpassing human cognitive limits. While traditional search engines like Bing [4] and Google [16] efficiently address straightforward queries like fact-checking or retrieving specific information, LLM-based chatbots truly shine in guiding users through more open-ended and exploratory tasks [19, 23, 28, 41, 46]. For instance, they can support users in researching emerging scientific fields [26], or planning intricate projects that require synthesizing information [40] from diverse sources and close user engagement. This capability has made LLM-based chatbots invaluable in helping users navigate unfamiliar areas.\nDespite their extensive capabilities, LLM-based chatbots face challenges in delivering personalized assistance during exploratory tasks [20, 24], particularly when they lack access to user-specific data, such as past interactions and personal preferences. This limitation forces them to rely heavily on user-provided inputs for personalization. In exploratory tasks, where users often begin with vague queries, generating answers based solely on limited information typically results in generic and impractical recommendations. For example, in travel planning, a query like \"Plan a 5-day trip to Hawaii\" without additional details (e.g., budget, preferred activities, or accommodation preferences) leads to an overly generic response, as shown in the bottom part of Figure 1. This lack of personalization arises either because users may not be aware of the essential details to include or because providing detailed information imposes cognitive load on users. Consequently, the travel plan generated from such a vague query would be insufficiently tailored to the user's specific needs. While users can refine their requests after receiving an immediate solution from the initial vague query, limitations remain, such as bias toward existing solutions and reduced motivation to explore alternative dimensions, and dissatisfaction is not always resolved despite using specification tactics [24].\nIn this study, we introduce the Collaborative Assistant for Personalized Exploration (CARE) system to tackle the challenges associated with generating personalized solutions in exploratory tasks. CARE facilitates tailored exploration"}, {"title": "2 Related Work", "content": "2.1 Large Language Models for Exploratory Tasks\nLLM-based human-computer interaction (HCI) systems, such as ChatGPT [31], Copilot [29], and Gemini [15], have expanded the role of AI in supporting users with exploratory tasks. Unlike traditional search engines that excel at handling well-defined queries, LLMs demonstrate notable strengths in assisting with open-ended, complex tasks. Recent work in HCI has explored the integration of LLMs into various domains, including creative workflows [41, 43, 46, 47], help-seeking [21, 36], planning [28, 50], and data exploration [23], revealing the potential of LLMs to facilitate adaptive interactions in complex contexts. Much of this research focuses on advancing prompt-based methods to enhance LLM performance, unlocking reasoning, problem-solving, and planning capabilities [6, 44]. While effective, prompt-based techniques encounter challenges such as inconsistent instruction-following [17, 48, 49], leading to reliability issues in chatbot behavior. These limitations underscore the need for systems that can more effectively manage the complexity and ambiguity inherent in exploratory tasks.\nOur work builds on this body of research by introducing a multi-agent collaboration system specifically designed for exploratory tasks. Task decomposition, a method effective in managing complexity, plays a central role in our approach. While some systems attempt to address exploratory tasks by prompting a single LLM agent, we found that a multi-agent architecture offers more rigorous task management and better alignment with user needs. Single-agent system often struggle to maintain focus across multiple responsibilities or lose track of ongoing tasks [24, 46], which can disrupt user progress. By assigning distinct roles to each agent, our system maintains greater control over task decomposition, milestone tracking, and user intent discovery, resulting in a more structured and personalized exploratory experience."}, {"title": "2.2 Personalization and Inspiration in LLM-Based Systems", "content": "Personalization is a critical factor in enhancing user experience across Al-driven systems [25, 42]. Many interactive platforms, such as recommendation engines [2, 38] and adaptive learning systems [27, 30], achieve personalization by leveraging user data, including past behavior, preferences, and explicit feedback. However, in the context of LLM-based systems, personalization presents greater challenges, as these models typically lack access to user-specific data unless explicitly provided within the conversation (e.g., via prompts) [20]. Consequently, LLMs often generate responses based only on the limited context of the current interaction, which can result in generic or irrelevant outputs, particularly in exploratory tasks. This passive approach to personalization-where LLMs react solely to the user's inputs and user follows up with new prompt-often leads to incomplete articulations of the user's needs, resulting in less comprehensive and relevant responses [22]. This is especially problematic in open-ended, complex tasks where users may not fully express or even realize their own requirements.\nIt is important to note that while adding a memory module to LLM-based chatbots can improve personalization by enabling the system to reference prior interactions, it may take a significant number of exchanges to gather enough data to meaningfully enhance this personalization. In the early stages of interaction, the chatbot may lack relevant insights, limiting its ability to provide tailored responses. Additionally, for new or unfamiliar tasks, previously stored information may not be useful, as exploratory tasks often require fresh context that past exchanges cannot fully provide.\nInspiration is another critical component, particularly in open-ended tasks. Prior research has shown that LLMs can stimulate creativity, assisting users in generating new ideas or perspectives [12, 33]. Unlike studies focused on sparking creativity or employing Socratic questioning in educational contexts [7, 9], our work emphasizes the discovery of users' implicit needs and requirements. Rather than merely responding to explicit queries, we employ proactive inquiry techniques that encourage users to think expansively. By actively probing for underlying goals and unarticulated needs, our system helps users consider aspects they may not have initially thought of, ultimately leading to more comprehensive and personalized outcomes."}, {"title": "2.3 User Interface Support for Thinking and Task Organization", "content": "Traditional chatbots and most LLM-based conversational systems rely on linear, dialogue-based interfaces, generating responses in a text stream without exploiting graphical user interfaces (GUIs). However, recent work has begun to explore the potential of graphical interfaces to better support users in complex tasks. For example, Sensecape integrates LLMs into an interactive system to facilitate information foraging and sensemaking [40], while Graphologue converts text responses into interactive diagrams to enhance information-seeking and question-answering tasks [19]. ExploreLLM employs a card-based schema to help users structure their thoughts and explore alternative solutions in a more organized manner [28]. A key challenge with linear text interfaces is the difficulty in locating and recalling relevant information, as important details can become buried within lengthy dialogue streams [14, 24, 34]. Our work contributes to this area by introducing a novel conversational interface, which separates the solution display from the dialogue interactions, while incorporating a dedicated section for managing user requirements. This structured design reduces cognitive load, enabling users to focus more easily on their tasks and better manage information flow during extended interactions."}, {"title": "3 Motivating Insights from Empirical Studies on Chat-Based Interfaces", "content": "Our proposed system is motivated by challenges users face when interacting with conventional LLM-based chatbots. A review of existing research on systems like ChatGPT [3, 5, 11, 13, 17, 24, 37], highlights key issues that persist in chat-based systems, especially regarding personalization [37], relevance [17], and sustained contextual understanding [3] in complex, open-ended scenarios.\nSeveral studies [8, 20, 24, 35] have consistently identified user dissatisfaction stemming from LLMs' frequent inability to capture the nuanced intent behind vague or incomplete queries. Users often encounter responses that are generic, irrelevant, or redundant, which diminishes trust in the system, particularly in tasks requiring deeper exploration or personalization. For example, research shows that users seeking localized information about a service may struggle when systems fail to interpret location-specific terms or understand personal preferences [50], leading to frustration and ineffective recommendations. This problem is further exacerbated in exploratory tasks, where users themselves may not have well-formed goals and rely on the system to guide them through ambiguity.\nMoreover, existing work [18, 24, 46, 49] has revealed that many users feel ill-equipped to manage the iterative, multi-turn interactions necessary to refine system outputs in such scenarios. The cognitive load imposed by these systems can be high, particularly when users are required to repeatedly clarify or reframe their inputs. Even with prompt engineering and optimization techniques designed to enhance LLM performance, users still frequently experience dissatisfaction when systems cannot offer personalized assistance based on minimal input or evolving needs.\nFrom these insights, several design goals emerge as essential for creating more collaborative chat-based interfaces:\n* Improved Contextual Understanding: System must be capable of maintaining and interpreting user intent over multiple turns, adapting to ambiguous or evolving user inputs to provide more relevant guidance.\n* Enhanced Personalization: System should be designed to tailor responses dynamically, accommodating user preferences and contextual needs, even when the initial input lacks specificity.\n* Facilitation of Inspired Exploration: In addition to providing personalized solutions, the system should inspire users to think implicit needs or aspects, promoting comprehensiveness in open-ended tasks.\n* Reduction of Cognitive Load: To mitigate the frustration associated with managing complex tasks, the interface should provide structure by organizing information effectively and helping users refine their queries.\nThese design goals are informed by extensive literature on user interaction with LLMs, providing a clear direction for improving the CARE system's capabilities in supporting personalized, exploratory tasks."}, {"title": "4 CARE: Collaborative Assistant for Personalized Exploration", "content": "In this section, we describe the proposed system, Collaborative Assistant for Personalized Exploration (CARE), which aims to achieve the design goals derived in the previous section. The overall system architecture of CARE is illustrated in Figure 2, and the subsequent sections elaborate on the user interface and back-end components.\n4.1 User Interface\nAs illustrated in Figure 2, the CARE's user interface consists of three main panels: the Chat Panel, Solution Panel, and Needs Panel. Next, we illustrate their functions using a travel planning scenario.\nThe Chat Panel functions as the primary interface where users interact with CARE, entering initial queries such as \"Plan a 5-day trip to Hawaii\". Through iterative feedback and conversation with the chatbot, users refine their requests, resulting in a dynamic multi-turn dialogue."}, {"title": "4.2 LLM-Powered Multi-Agent Collaboration System", "content": "The back-end of the CARE system is powered by an LLM-driven multi-agent collaboration framework, as depicted in the bottom part of Figure 2. It comprises five specialized LLM-based agents: the Inquiry Agent, Milestone Agent, Needs Discovery Agent, Ranking Agent, and Solution Craft Agent. These agents collaborate with the user to progressively manage the solution development across three panels in the UI, discovering implicit needs and generating personalized solutions. While similar systems can be created by prompting a single LLM-based agent, we found that a multi-agent approach ensures more rigorous task management and better alignment with our design goals. Single-agent models can lose track of tasks or become overwhelmed by managing too many responsibilities. By assigning distinct roles to each agent, we maintain better control over individual components, ensuring consistency in task decomposition, milestone tracking, and user need discovery. Next, we introduce the overall interaction workflow of this multi-agent system, using the same example case of \"Plan a 5-day trip to Hawaii.\"\n4.2.1 Interaction Workflow among Agents. The workflow, shown in Figure 2, is triggered by the query initiated by the user in the Chat Panel. Subsequently, the Inquiry Agent receives the user's request and passes it to the Milestone Agent, who assesses if enough user needs are collected to proceed. If more details are needed, a new milestone is set, such as \"understand user's hotel preferences.\" The Needs Discovery Agent then extracts needs from the user's input and generates follow-up questions to uncover additional needs that not mentioned by user.\nThe Ranking Agent organizes these questions, grouping and prioritizing them before they are presented to the user. These questions are then passed back to the Inquiry Agent, who presents them to the user in an intuitive manner. As the user responds, their answers are populated into the Needs Panel. This process continues iteratively until the Milestone Agent confirms that the collected needs are sufficient, prompting the Solution Craft Agent to generate a personalized solution, which is then presented in the Solution Panel.\nUpon receiving the solution, the user can choose to refine the solution in two ways. They can directly communicate new preferences to the Inquiry Agent, which may trigger additional needs discovery steps, or if the preference is straightforward, the Milestone Agent will pass the new feedback directly to the Solution Craft Agent for immediate integration into a revised solution. Alternatively, the user can manually modify, add, or delete items in the Needs Panel. Any such updates will prompt the Solution Craft Agent to generate a new solutions based on the latest needs.\nThe above workflow illustrates how the agents collaborate to deliver solutions tailored to users' preferences while facilitating a comprehensive needs discovery process. The following sections describe each agent in detail.\n4.2.2 Inquiry Agent. The Inquiry Agent is designed to facilitate smooth user interactions by gathering and clarifying user needs in an intuitive manner. One of its key responsibilities is refining the questions generated by the Needs Discovery Agent before they are presented to the user. This process involves simplifying the language of questions and providing default options, which significantly reduces the user's cognitive load. For instance, instead of presenting a broad question like, \u201cWhat kind of accommodation do you prefer?", "Hotel or Airbnb?\", thereby streamlining the decision-making process for users.\"\n        },\n        {\n            \"title\": \"4.2.3 Milestone Agent\",\n            \"content\": \"The Milestone Agent plays the strategic coordinator role in CARE by determining the next critical steps (i.e., milestones) based on the existing user's input and collected needs, ensuring that all necessary user needs are gathered before moving forward with solution generation.\nThe Milestone Agent has two primary responsibilities. First, if it determines that more specific user needs are required or the user requests an improvement on the existing solution, it sets the next milestone. This milestone typically involves collecting missing or incomplete information necessary for building a comprehensive solution. The Milestone Agent uses the current Needs Panel and input from the user to establish these milestones, ensuring that they are specific, actionable, and contribute directly to solving the user's problem. It references previously established milestones to avoid redundancy and ensures each new milestone is unique and focused on specific, measurable outcomes. Additionally, the agent carefully considers dependencies between tasks, breaking down complex problems into manageable milestones that the CARE system can address step by step.\nSecond, if the recorded user needs are sufficient, the Milestone Agent notifies the Solution Craft Agent to begin generating a solution. At this stage, no new milestones are created, and the process transitions to solution generation. This decision is based on whether the current Needs Panel fully addresses the user's query.\nIn cases where the user manually updates their needs via the user interface, the Milestone Agent immediately passes this information to the Solution Craft Agent, triggering a plan update without setting a new milestone. This ensures flexibility in how the CARE system adapts to user input and allows for rapid response to direct feedback.\"\n        },\n        {\n            \"title\": \"4.2.4 Needs Discovery Agent\",\n            \"content\": \"The Needs Discovery Agent serves a vital function in identifying and documenting user needs, both explicit and implicit, throughout the interaction with the system. This process involves two critical tasks that ensure comprehensive coverage of user Needs.\nFirst, the agent is responsible for extracting explicit needs directly from the user's input. Explicit needs are those that the user clearly articulates. For instance, if a user expresses a desire to \\\"Plan a 5-day trip to Hawaii,\\\" the agent will identify and extract specific needs, such as \\\"the destination is Hawaii": "nd \u201cthe trip duration is 5 days\". These explicit needs are systematically recorded in the Needs Panel to ensure they are fully documented, making it simple for users to review their own needs, while also allowing the CARE system to further develop personalized solutions.\nSecond, the agent plays an essential role in identifying implicit needs, which are often unspoken but critical to achieving the current milestone. Implicit needs are inferred based on the chat history and the specific demands of the milestone. For example, when the task involves selecting a hotel, the agent may infer preferences such as whether the hotel needs to be close to popular attractions or if the user has specific needs for certain amenities, such as a gym, swimming pool, or free Wi-Fi. By anticipating these needs, the agent ensures a more complete understanding of the user's needs. Once the agent has completed its inference, it generates clarification questions to further refine the user's preferences. These questions may include, for example, \"whether the user has specific requirements for the accommodation's location\u201d or \u201cany preferences regarding hotel amenities.\u201d The generated questions are then added to the Needs Panel, although invisible on the UI to avoid user distraction."}, {"title": "4.2.5 Ranking Agent", "content": "The Ranking Agent brings structure and order to the system by organizing and prioritizing the clarification questions identified during the user needs discovery process. Instead of overwhelming users with a disjointed list of queries, the agent groups related questions thematically, such as accommodation preferences or budget constraints, allowing the user to focus on one topic at a time. This approach not only minimizes cognitive overload but also keeps the interaction flowing smoothly.\nOnce the questions are grouped, the Ranking Agent further arranges them in a logical sequence. It starts with simpler, more general inquiries, then gradually progresses to more detailed, specific questions. By controlling the pacing and order of the interaction, the agent ensures that users can answer with confidence and clarity, without being rushed or overwhelmed. This results in a seamless experience where the user is led through the interaction with ease, while all necessary information is gathered efficiently."}, {"title": "4.2.6 Solution Craft Agent", "content": "The Solution Craft Agent is central to transforming user needs into concrete, personalized solutions. This agent excels at turning complex sets of needs into clear, actionable outcomes. After analyzing the user's preferences and constraints, it organizes the solution in a way that is both transparent and user-friendly. Each solution is neatly structured, with headings and concise explanations, making it easy for users to follow and understand how their specific needs are being addressed. By linking every recommendation to a particular need, the agent ensures that users can see exactly how their input shapes the proposed solution.\nPersonalization is another key strength of the Solution Craft Agent. Rather than offering generic solutions, it tailors each recommendation to align with the user's context, preferences, and limitations. Whether suggesting accommodations that fit within a specific budget or transportation options that align with the user's schedule, the agent ensures that the solution is relevant and practical. This emphasis on customization guarantees that each user receives a solution that is uniquely suited to their situation, making the final outcome both useful and highly personalized."}, {"title": "4.3 Implementation Details", "content": "The CARE system is a web-based application featuring a UI developed with Streamlit [39] and a back-end multi-agent system constructed using AutoGen [45]. Additionally, we implemented a conventional LLM-based chat interface by directly prompting the base LLM, which serves as the baseline system for our user study. Both CARE and the baseline systems utilize GPT-40 as the underlying LLM, with all corresponding prompts detailed in Supplementary Material."}, {"title": "5 User Study", "content": "To minimize the effects of participant variability, we conducted a within-subject user study with 22 participants, comparing CARE to a baseline LLM-based chatbot as described above. Each participant completed two exploratory tasks, travel planning and skill learning, using both systems. To control for order effects, we counterbalanced the order of the systems and tasks, resulting in four settings. The study aimed to address the following research questions (RQs):\n* RQ1. How does the CARE system compare to the baseline system in supporting users in exploratory tasks in terms of interaction experience and user satisfaction?\n* RQ2. How does the CARE system provide personalized and inspired solutions compared to the baseline system?\n* RQ3. How does the CARE system's thoughtful interface design help reduce cognitive load during complex, open-ended tasks, compared to the baseline system?"}, {"title": "5.1 Procedure", "content": "Each study session began with an introduction to both systems, during which participants received basic instructions on how to interact with the systems (e.g., explanation on UI layout, reminder on no web search function). Participants then completed a demographic questionnaire (Section A.2) detailing their background and prior experience with LLM-based chatbots. Each participant completed two exploratory tasks using both systems. We encouraged participants to spend 5-10 minutes on each task. After interacting with each system, participants filled out a post-task questionnaire (Table 1, questionnaire format in Section A.3) to evaluate their experience. Throughout the session, participants were encouraged to think aloud. Follow-up interviews (Section A.1) were conducted at the end of each session to gather additional qualitative insights, particularly focusing on any notable highlights or frustrations with each system.\nEach session lasted approximately one hour. The study was conducted in person, and participants were compensated with cash equivalent to two hours of the local minimum wage in appreciation of their time."}, {"title": "5.2 Data Collection and Analysis", "content": "To evaluate how effectively CARE supports users in exploratory tasks, we designed a comprehensive questionnaire (Table 1) focusing on several key aspects of the user experience: Interaction, Cognitive Load, Inspiration, Compre-hensiveness, and Personalization. During each study session, we recorded participants' screen activities and audio to capture their think-aloud verbalizations, allowing us to gather qualitative insights alongside quantitative data.\nFor the quantitative analysis, we employed a combination of statistical methods, including paired t-tests and Pearson's Chi-Square tests, to assess the significance of our findings. The paired t-test allowed us to determine whether there was a statistically significant difference in user ratings between CARE and the baseline system. The Chi-Square test was used to examine categorical differences in participant responses across various dimensions of the user experience. To provide further context for the quantitative results, we analyzed the think-aloud data and post-task interviews using an inductive qualitative approach. This allowed us to probe into the reasons behind the participants' ratings and explore their subjective experiences with the two systems. The qualitative feedback was instrumental in corroborating the experimental findings and providing deeper insight into how CARE facilitated user exploration, reduced cognitive load, and supported creativity."}, {"title": "5.3\nParticipants", "content": "We recruited 22 participants (4 female, 18 male) through a call for participation distributed via the university's mailing list, as well as through word-of-mouth referrals. Participants represented a diverse range of academic disciplines,"}, {"title": "6 Quantitative Results", "content": "The binary choice results, collected through post-task interviews, reveal a clear preference for CARE over the baseline system, with 16 out of 22 participants favoring CARE. This finding aligns with the quantitative data from the five survey questions listed in Table 1, as depicted in Figure 3. When averaging scores across all five questions, CARE was rated significantly higher (t = 3.07, p = 0.0058), with an average score of 4.20 (\u03c3 = 0.78) compared to the baseline's 3.25 (\u03c3 = 0.91). Cohen's d = 1.12 also illustrates the considerable difference in perceived effectiveness, highlighting the participants' clear preference for CARE. We now turn to the detailed statistical analysis of each survey question.\nOverall Interaction Experience. The Pearson Chi-Square analysis (x\u00b2(4) = 5.17, p = 0.27) indicates no significant difference in overall satisfaction between the two systems in Q1. However, a larger proportion of participants rated CARE favorably in the \"Agree\" and \"Strongly Agree\" categories, with 17 out of 22 participants finding CARE engaging, compared to only 10 for the baseline. This pattern, though not statistically significant, suggests that CARE might provide a better interaction experience for the majority of users.\nReducing Cognitive Load. According to the results in Q2, users perceived that CARE significantly reduced cognitive load in organizing and managing complex tasks compared to the baseline (x\u00b2(4) = 19.04, p = 0.001). Notably, 13 participants strongly agreed that CARE was more systematic in presenting and organizing information, compared to only one participant in the baseline system."}, {"title": "7 Findings", "content": "The findings from our user study are organized around addressing the three research questions listed in Section 5, providing insights into the strengths of the CARE system compared to the baseline. First, we explore how CARE's inquiry process supports exploratory tasks by fostering deeper exploration, ensuring comprehensiveness, and delivering personalized outputs, aligning with RQ1 and RQ2. Next, we examine how CARE's collaborative workspace effectively organizes user needs, generated solutions, and chat interactions, contributing to reduced cognitive load and improved task performance, directly addressing RQ3. These findings collectively highlight the key benefits of CARE's design in enhancing user experience and productivity."}, {"title": "7.1 CARE's Inquiry Process Inspires Exploration, Ensures Comprehensiveness, and Personalizes Outputs", "content": "7.1.1 CARE's Inquiry Process Inspires Users by Uncovering Implicit Needs. Through our investigation, we found that CARE effectively uncovers users' implicit needs by encouraging them to think broadly and respond to proactive inquiries. During the interviews, we further identified that users' implicit needs can be divided into two distinct categories: hidden needs and latent needs. Hidden needs refer to those needs that users are aware of but do not explicitly express, while latent needs are those that users do not recognize until they are prompted by CARE's exploratory questions.\nCARE effectively inspires users to consider both hidden and latent needs. Specifically, most participants (17/22) indicated that CARE's questioning strategy encouraged them to think about aspects they would not have otherwise considered. For instance, P7 highlighted how CARE helped him uncover hidden needs while planning a trip: \"For example, when planning a trip, it (CARE) would ask me if I prioritize sightseeing, food, or shopping, and if I have a preference, it can better tailor the itinerary for me.\" This kind of questioning reveals needs that the user might be aware of but did not communicate explicitly. On the other hand, P19 emphasized how CARE brought to light latent needs during the process of creating a baseball training plan: \"I think System A (CARE) inspired me a lot. For example, when I wanted to start playing baseball, it asked about the coach's style and how much time I wanted to train-especially questions about the"}, {"title": "7.1.2 CARE's Milestone-Driven Approach Improves User Focus And Personalizes Task Progression", "content": "To ensure that CARE continuously focuses on the user's current needs, tasks are organized into sequential milestones by Milestone Agent, guiding other agents in the system step by step through the planning process. Each milestone represents a distinct phase of progress, enabling agents to focus on one task at a time while maintaining a coherent long-term strategy. As users provide feedback or refine their goals based on the current milestone, the system dynamically updates the plan or generates the next milestone, ensuring continuous alignment with the user's evolving needs.\nThis milestone-driven approach was noticed and appreciated by participants (10/22). P20 described this as, \u201cIt (CARE) asked questions that helped me break down a big problem into smaller, more systematic tasks. This way, I could understand my needs better, and the system could offer more personalized settings\". Similarly, P8 mentioned, \"The system A (CARE) would update after each step, and I could see how my decisions affected the plan, which helped me stay on track.\"\nThe baseline system, however, lacked this granular task breakdown. Without the ability to iteratively refine and adjust based on continuous feedback, users found it difficult to maintain focus on evolving goals. P15 remarked, \"With the another system (Baseline), I often felt lost trying to manage everything at once.\""}, {"title": "7.1.3 Comprehensive Inquiry Process Ensures More Detailed, Structured, and Complete Solution", "content": "CARE ensures that users' needs are comprehensively covered through a systematic inquiry process during interactions. This comprehensiveness is reflected in two dimensions: the comprehensiveness of the inquiry process and the comprehensiveness of the generated solution. The former refers to the system's ability to cover multiple aspects of the user's needs through its multi-dimensional questions, while the latter pertains to the final solution being more structured and complete.\nSome participants (13/22) expressed a preference for this proactive, multi-dimensional inquiry-driven interaction style. For example, P12 noted: \"A (CARE) asks follow-up questions based on your answers and helps fill in things you might not have thought about. I think it's very comprehensive.\u201d Similarly, P4 highlighted CARE's ability to expand a single question into multiple dimensions, noting: \"A (CARE) is suitable for general users because many times you only have one question in your mind, but it can help you break it down into various aspects that need to be considered.\"\nMoreover, CARE incorporates a Ranking Agent, which prioritizes questions to allow users to first answer simpler, foundational questions, thereby reducing cognitive load during interactions. This design ensures that questions are presented in a structured sequence, avoiding users being overwhelmed by excessive information at once. As P8 stated: \"It's (CARE's) interactive, and it asks simple questions, like multiple-choice or short questions. I felt relaxed while interacting with it, and it was really good at recognizing my intent and adding that to the list of needs.\"\nThis level of comprehensiveness during interaction was lacking in the baseline system. Users had to think through the task and provide as much information as possible upfront by themselves, and the system did not proactively ask about additional dimensions of the task. Some participants (8/22) expressed frustration with this limitation. P21 explained: \"With chatbot B (Baseline), I had to ask more follow-up questions myself to complete the plan. But sometimes, I might forget to ask something important or miss a key detail.\""}, {"title": "7.1.4 Personalization Through Tailored Inquiry Results in More User-Specific Solutions", "content": "CARE generates highly per-sonalized plans by continuously adjusting its output based on users' specific needs through dynamic questioning. Specifically, CARE's inquiries focus on clarifying vague requests by asking about personalized preferences or needs, such as preferred activity types or priorities. This not only helps users articulate their preferences more clearly but also ensures that the system tailors its output accordingly. The flexibility and specificity of the generated solution made users feel that the system was responsive to their input, enhancing their satisfaction. P10 highlighted this by explaining, \"A (CARE) kept asking questions that I hadn't even thought about, like whether I wanted to prioritize certain skills or what time of day I preferred for training. This really helped make the final plan feel like it was made just for me.\"\nThe baseline system often fell short in this area as it relied on users to provide all necessary input upfront without prompting further clarification. As a result, the solutions generated by the baseline system were often less personalized. P9 commented, \u201cB (Baseline) didn't ask any follow-up questions after I gave my preferences. It just generated a plan that was okay, but it didn't really reflect everything I wanted.\" This feedback underscores how baseline's limited engagement led to less tailored solutions, resulting in solutions felt generic. Even though occasionally the baseline system encouraged users to follow up with additional inputs, long-form answers could still lead to missing crucial personalization elements. For instance, P13 noted: \"For my fitness training task, the plan from B (Baseline) didn't match what I wanted at first. I had to follow up and ask for revisions, but even after updating, it still missed some details that A (CARE) had caught because A asked very specific questions from the start.\" This illustrates that even when users try to supplement their preferences with follow-up inquiries in the baseline system, the lack of thorough questioning from the outset could result in oversights.\nCARE's adaptability during the interaction process further enhanced its ability to personalize plans. P8 appreciated how the system dynamically adjusted based on real-time inputs: \"With A (CARE), I could tweak my needs as we went along, and it would adjust the plan. That made me feel like I had control over the outcome and that the plan was really made for me.\" This iterative refinement process ensured that the final plan was more accurately tailored to the user's evolving preferences, providing a clear advantage over the more static baseline system."}, {"title": "7.1.5 CARE's Inquiry Process May Occasionally Cause Friction for Some Users", "content": "While some participants (9/22) explicitly expressed that the CARE system's structure made it better suited for guiding users through complex tasks, with P20"}, {"title": "7.2 CARE's Collaborative Workspace Effectively Organizes User Needs, Generated Solutions, and Chat Interactions, thereby Reducing Cognitive Load", "content": "One of the most frequently mentioned advantages of the CARE system during interviews was its well-organized UI. Many participants (14/22) noted that compared to the baseline, CARE's UI significantly improved their overall experience by reducing cognitive load and making interactions more intuitive. Specifically, users appreciated the structured layout, which clearly separated their needs from generated solutions and system interactions. This organization helped them stay focused on their tasks and reduced the effort needed to manage the flow of information during extended interactions. In the following sections, we will delve deeper into how specific features of CARE's UI, such as the Needs Panel and Solution Panel, contributed to these advantages."}, {"title": "7.2.1 Panel Separation Improved Clarity and Reduced Cognitive Load", "content": "The division of the Chat Panel", "remarked": "It (CARE) asks questions on the right side and outputs what I need on the left, so I don't need to search through the conversation to find the result.\" This clear division between the chat interactions and the solution outputs allowed users to locate relevant information effortlessly, minimizing the cognitive effort associated with scrolling back through chat histories.\nThe Needs Panel, in particular, proved to be a critical feature in reducing mental load. It enabled users to track and update their requirements in real time without revisiting previous conversations. P8 noted, \"A's (CARE's) needs list is clearly displayed, making it easy to see and update my requirements.\u201d This visualization of needs provided users with a constant, structured overview, reducing the effort required to recall or modify their requests. Similarly, P21 highlighted how the separation of the Needs Panel from the chat log made it easier"}]}