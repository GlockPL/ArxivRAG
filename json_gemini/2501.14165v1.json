{"title": "LoCoML: A Framework for Real-World ML Inference Pipelines", "authors": ["Kritin Maddireddy", "Santhosh Kotekal Methukula", "Chandrasekar Sridhar", "Karthik Vaidhyanathan"], "abstract": "The widespread adoption of machine learning (ML) has brought forth diverse models with varying architectures, data requirements, introducing new challenges in integrating these systems into real-world applications. Traditional solutions often struggle to manage the complexities of connecting heterogeneous models, especially when dealing with varied technical specifications. These limitations are amplified in large-scale, collaborative projects where stakeholders contribute models with different technical specifications. To address these challenges, we developed LoCoML, a low-code framework designed to simplify the integration of diverse ML models within the context of the Bhashini Project - a large-scale initiative aimed at integrating AI-driven language technologies such as automatic speech recognition, machine translation, text-to-speech, and optical character recognition to support seamless communication across more than 20 languages. Initial evaluations show that LoCoML adds only a small amount of computational load, making it efficient and effective for large-scale ML integration. Our practical insights show that a low-code approach can be a practical solution for connecting multiple ML models in a collaborative environment.", "sections": [{"title": "I. INTRODUCTION", "content": "Integrating machine learning (ML) systems into complex, real-world applications brings engineering challenges that go beyond traditional software engineering challenges [1], [2]. ML-based systems require continuous data management, frequent model updates, and robust workflows to link various ML components, which are often sourced from different providers with varying technical compatibility [3]\u2013[6]. These challenges are especially significant in large-scale, collaborative projects. We were faced with one such challenge in the Bhashini Project 1, which had multiple academic/industrial partners contribute models with distinct architectures and formats towards building a nationwide AI platform. Ensuring that different models work smoothly as part of a unified, high-quality system demands considerable engineering effort [7]\u2013[9].\nWhile some ML platforms provide pipeline management and model deployment tools, they often lack the flexibility required for projects with the scale and diversity of Bhashini Project. Many existing platforms also require high levels of coding expertise, creating a barrier for teams with varied technical backgrounds [9]. This restricts accessibility and slows development, as users must rely on expert developers for even minor adjustments. In the Bhashini Project, the need to coordinate multiple, heterogeneous models to achieve accurate, aligned inferences has highlighted gaps in current frameworks, necessitating a customized solution. Many existing solutions impose limitations in compatibility and scalability, making it challenging to support workflows where independently developed models must work together within a single pipeline [5], [6], [10].\nThe need for such a flexible, accessible solution is particularly evident in workflows involving a series of interdependent models, such as speech-to-text processing. In this domain, one model might transcribe speech to text, another handles language translation, and further models manage additional language-specific nuances. Integrating these models into a single, reliable pipeline that ensures high-quality output at each stage is a complex engineering task, further complicated by the lack of frameworks that support seamless integration across diverse models with specialized functions. To address these real-world demands, LoCoML was developed as a low-code ML engineering framework, providing flexibility and modularity in creating and managing ML inference pipelines. To address the challenges of managing and orchestrating ML models in complex workflows, LoCoML (As in Figure 1) is organized around two main components: the Model Hub and the Pipeline Orchestrator. Together, these parts handle everything from storing and retrieving models to executing pipelines, with different user roles involved to keep things running smoothly.\nInspired by model-driven engineering (MDE) [11] practices and supported by low-code principles, LoCoML abstracts technical complexities, allowing users to focus on the application logic rather than the underlying engineering details. Additionally, the framework's low-code design invites users with diverse technical backgrounds to contribute to ML workflows, improving"}, {"title": "II. BHASHINI PROJECT", "content": "The Bhashini Project is a large-scale initiative focused on breaking down language barriers by enabling digital services in multiple languages. The project integrates various AI-driven language technologies, including Text-to-Speech (TTS), Automatic Speech Recognition (ASR), Machine Translation (MT), and Optical Character Recognition (OCR). By combining these tools, Bhashini Project facilitates seamless communication across more than 20 languages, providing a unified platform that allows people to access and interact with content in their preferred language. A range of stakeholders support this initiative, contributing resources, expertise, and data to expand its capabilities. Small and medium enterprises (SMEs) and private organizations with substantial digital reach offer technical support and data to enhance the project's language resources. Additionally, local language organizations and individual users contribute through a crowdsourcing platform, enriching the language data and making the platform more representative. Together, these efforts create a collaborative system that addresses the diverse linguistic needs of the public and supports the integration of advanced language models into real-world applications.\nThe diversity and scale of the Bhashini Project created unique challenges in integrating multiple AI-driven language models into a single cohesive system. The need to coordinate between different model architectures, manage dynamic data flows, and ensure high-quality results across various languages required a flexible and robust framework that could adapt to evolving requirements. Motivated by these challenges, we developed LoCoML to streamline the integration and management of these diverse models. LoCoML's low-code design and modular approach address the complexity of combining technologies like TTS, ASR, MT, and OCR, making it possible for the Bhashini Project to deliver consistent, reliable, and scalable language services. This framework enables seamless collaboration across stakeholders, allowing them to contribute and refine models effectively while enhancing the platform's accessibility and usability for a broader audience."}, {"title": "III. LOCOML APPROACH", "content": "ed in Figure 1, LoCoML co themprises system two primary subsystems: the Model Hub and the Pipeline Orchestrator. Within this system, we have identified three distinct user roles. The first is the Model Developer, responsible for creating the models utilized within the pipelines, which are stored in and retrieved from the Model Hub. The other two roles, the Pipeline Designer and the Pipeline User, engage directly with the Pipeline Orchestrator."}, {"title": "A. The Model Hub", "content": "The Model Hub supplies the system with all the necessary ML models. The Model Developer uses this subsystem in one of two ways: either by training an ML model from scratch via the Model Trainer and saving the trained model in the Model Repository, or by providing an API to an externally deployed model along with a usage mechanism, stored as part of Model APIs. Together, the Model Repository and Model APIs form the Saved Models database, which is essential for providing models to the Pipeline Orchestrator. In the Bhashini Project, all the necessary models have already been developed and have been made available for use via APIs. We are leveraging these APIs to build our inference pipelines."}, {"title": "B. The Pipeline Orchestrator", "content": "The Pipeline Orchestrator is responsible for managing all inference-related processes within the system. At the core of this subsystem is the Pipeline Designer, who uses the Pipeline Builder component to construct pipelines. A pipeline is essentially a sequence of processing steps, each of which is represented by a node $n_i \\in N \\forall i \\in \\{1,...,k\\}$ for a pipeline with k steps. These nodes may include data pre-processing components, various types of machine learning (ML) models, and specialized components known as adapters. The sequence in which steps are to be executed is determined by the set of directed edges E, where each edge $e_{n_i \\rightarrow n_j i\\neq j}$ denotes that a directed edge from node ni to node nj exists in the pipeline.\nAn adapter is a specialized node in the pipeline designed to ensure compatibility between nodes, especially when the output format of one model doesn't align with the input requirements of the next. They play a critical role when models handle different data types or structures. For instance, an OCR model might output raw text that contains unrecognized or misinterpreted characters when processing an image, whereas a downstream MT model requires clean, structured text as input. The adapter acts as an intermediary, cleaning and reformatting the OCR output to meet the MT model's requirements. This bridging function of adapters is essential for maintaining smooth data flow and reliable integration among various models within the pipeline, ensuring that each model receives compatible, usable data."}, {"title": "Algorithm 1 Rule evaluation procedure", "content": "Require: Source Node ni, Destination Node nj\nEnsure: Boolean indicating whether this rule has been satisfied by nodes ni and nj\n1: procedure EVALUATE(ni,nj)\n2: if property px \u2208 Pn\u2081 and property py \u2208 Pn; then\n3: return validateRuleConstraint(ni \u2192Px, nj\u2192Py)\n4: else\n5: return false\n6: end if\n7: end procedure\nAs defined in Algorithm 2, the Pipeline Validator takes the set of all these rules r\u2208 R as the RuleSet, along with the source node ni and the destination node nj, and verifies whether all the rules present in the RuleSet are satisfied for a potential edge between these nodes. If they are, then an edge en\u2081\u2192n; can exist between them. Else, the user is notified about the invalid edge."}, {"title": "Algorithm 2 Pipeline Validator's edge validation procedure", "content": "Require: Source Node ni, Destination Node nj, RuleSet R\nEnsure: Boolean indicating if an edge eni\u2192n; can exist\n1: procedure CANEDGEEXIST(ni,nj, R)\n2: for each rule r\u2208 R do\n3: if not r.evaluate(ni,nj) then\n4: return false\n5: end if\n6: end for\n7: return true\n8: end procedure\nIn the Bhashini Project, the Pipeline Validator enforces compatibility rules between model connections to ensure a logical workflow. Specifically, it verifies whether the output of an ASR model is linked to an MT or TTS model's input, an OCR model's output is connected to an MT or TTS model's input, the output of an MT model is connected to another MT or TTS model's input, and the output of a TTS model is connected to an ASR model's input. Furthermore, it also checks"}, {"title": "IV. PRELIMINARY EVALUATION", "content": "As a preliminary step, experiments were conducted across two distinct machine learning pipeline scenarios. We aimed to measure the additional overhead in running the pipeline that was introduced by the LoCoML platform\u00b2, on top of the model execution time, in order to demonstrate the overhead's relative impact."}, {"title": "A. Experimental Setup", "content": "We deploy the backend of the LoCoML framework locally in a Docker container, using the Python:3.12-slim base image for the execution environment, on a laptop with the Ryzen 7 5800H CPU that runs at 3.2GHz base speed, and 16GB of DDR4 RAM clocked at 3200MHz. Further, we evaluate LoCoML using two different scenarios:\n1) Machine Translation Pipeline: We tested pipelines containing 1 to 16 MT models.\n2) Speech Processing Pipeline: Pipelines cannot consist of only ASR or only TTS nodes because of input-output mismatches-ASR models take audio as input and produce text, while TTS models take text as input and produce audio. Therefore, we tested pipelines with 1 to 8 pairs of ASR and TTS models chained together."}, {"title": "V. RELATED WORK", "content": "Recent advances in MDE have highlighted the role of low-code platforms in simplifying ML development and deployment. Naveed et al. [5] recommend researchers and practitioners to develop low-code platforms for systems with ML components to make ML capabilities more accessible to non-experts, as these platforms can significantly reduce development complexity and time to deployment. Similarly, Iyer et al. [9] introduced Trinity, a no-code platform specifically designed to handle complex spatial datasets, highlighting the versatility and scalability low-code solutions bring to ML applications. In addition, Esposito et al. [13] emphasize the importance of user-configurable controls within Al systems, proposing that a balance between automation and manual adjustments can address diverse user needs effectively. Sahay et al. [14] provide a detailed survey of various low-code development platforms, identifying key features such as graphical interfaces, interoperability, and scalability as critical for decision-makers evaluating such platforms.\nLoCoML builds on these principles, offering a flexible, user-friendly platform that enables non-expert users to construct and customize ML pipelines, facilitating tasks like data preprocessing, model training, and inference without extensive coding knowledge [15]. This approach empowers users to iteratively reconfigure workflows, bridging adaptability gaps noted in previous studies and enhancing both accessibility and control [16] [17]. Unlike traditional ML systems, LoCoML allows users to adjust pipelines dynamically, aligning with recommendations for integrating user-centric features and configurable controls within ML platforms [18] [19].\nExisting platforms like Azure Machine Learning Designer [20] and AWS SageMaker Pipelines [21] provide robust solutions for building and managing machine learning workflows. Azure Machine Learning Designer enables users to create pipelines via a drag-and-drop interface, integrating seamlessly within Azure's ecosystem. Similarly, AWS SageMaker Pipelines offers a complete suite for creating, deploying, and managing workflows, tightly coupled with AWS-native services. However, these platforms often face challenges when dealing with custom models from external sources, primarily because they are largely designed to operate within their respective ecosystems [22] [23]. It is too tedious to have to port custom models into the specific input-output constraints as prescribed by these platforms. In contrast, our framework addresses this gap by providing a unified and adaptable solution capable of accommodating diverse, partner-contributed models, ensuring compatibility and seamless integration-capabilities that are currently absent in these existing platforms."}, {"title": "VI. CONCLUSION AND FUTURE WORK", "content": "To conclude, we have introduced LoCoML, a low code framework designed to streamline the development of ML inference pipelines. LoCoML has been successfully integrated into the Bhashini Project with a drag-and-drop interface to create pipelines, where it operates in a real-world production environment, supporting users in building and managing inference pipelines efficiently. The framework's simple interface allows users, including those without extensive coding skills, to connect and control various ML models seamlessly. Our evaluation across multiple scenarios, including TTS, MT, and ASR, indicate that LoCoML has significantly simplified the process of constructing complex, multimodel workflows, making ML pipeline development more accessible and practical for a diverse range of users.\nLooking ahead, we aim to expand LoCoML's capabilities in response to evolving requirements within the Bhashini Project. Stakeholders of this project are exploring the potential of extending the framework to support model training, thus creating an end-to-end solution covering both training and inference within the same pipeline. Additionally, we plan to conduct further studies to assess LoCoML's effectiveness in terms of user experience, usability, and performance. These future enhancements will ensure that LoCoML continues to evolve as a versatile and robust framework, meeting the growing demands of ML practitioners and researchers within the Bhashini Project and beyond."}]}