{"title": "DevEval: Evaluating Code Generation in Practical Software Projects", "authors": ["Jia Li", "Ge Li", "Yunfei Zhao", "Yongmin Li", "Zhi Jin", "Hao Zhu", "Huanyu Liu", "Kaibo Liu", "Lecheng Wang", "Zheng Fang", "Lanshen Wang", "Jiazheng Ding", "Xuanming Zhang", "Yihong Dong", "Yuqi Zhu", "Bin Gu", "Mengfei Yang"], "abstract": "How to evaluate Large Language Models (LLMs) in code generation is an open question. Many benchmarks have been proposed but are inconsistent with practical software projects, e.g., unreal program distributions, insufficient dependencies, and small-scale project contexts. Thus, the capabilities of LLMs in practical projects are still unclear. In this paper, we propose a new benchmark named DevEval, aligned with Developers' experiences in practical projects. DevEval is collected through a rigorous pipeline, containing 2,690 samples from 119 practical projects and covering 10 domains. Compared to previous benchmarks, DevEval aligns to practical projects in multiple dimensions, e.g., real program distributions, sufficient dependencies, and enough-scale project contexts. We assess five popular LLMs on DevEval (e.g., gpt-4, gpt-3.5-turbo, CodeLLaMa, and StarCoder) and reveal their actual abilities in code generation. For instance, the highest Pass@1 of gpt-3.5-turbo only is 42 in our experiments. We also discuss the challenges and future directions of code generation in practical projects. We open-source DevEval and hope it can facilitate the development of code generation in practical projects.", "sections": [{"title": "1 Introduction", "content": "Code generation with Large Language Models (LLMs) has attracted lots of researchers' attention (Li et al., 2023c,a; Zhang et al., 2023b), and some commercial products have even been produced, such as GitHub Copilot (GitHub, 2023). How to evaluate LLMs on code generation is an open question. Many code generation benchmarks have been proposed, but there are gaps between them and practical software projects. The gaps result in the development of code generation technologies being inconsistent with the experience of developers. To clarify the gaps, we analyzed over 1 million functions from 500 practical projects and summarized the gaps as follows.\nGap 1: Existing benchmarks differ from real program distributions, especially the proportion of non-standalone programs. As shown in Figure 1, a standalone function solely uses built-in elements, while a non-standalone one contains dependencies. A dependency refers to an invocation of elements defined in projects, like parse_response in Figure 1. Out of 500 practical projects, 73.8% of functions are non-standalone, and 26.2% are standalone. However, existing benchmarks focus on standalone programs, with few or no non-standalone programs. For example, the latest benchmark, CoderEval (Yu et al., 2023), only includes 76 (32.4%) non-standalone programs.\nGap 2: Dependencies within existing bench-"}, {"title": "2.1 Overview", "content": "DevEval contains 2,690 samples derived from 119 real-world open-source projects. As illustrated in Figure 2, each sample consists of six components.\nFunction Signature: The signature of the code intended for generation. Requirement: An English description detailing the functionality of the code to be generated. Project Contexts: Existing programs (e.g., hundreds of Python files) in the current project. Reference Code: A reference implementation of the code to be generated, crafted by human developers. This code invokes dependencies defined within the project contexts. Reference Dependency: The dependencies invoked in the reference code, include intra-class, intra-file, and cross-file dependencies. Test Cases: Test"}, {"title": "2.2 Benchmark Characteristics", "content": "Compared to existing benchmarks (e.g., CoderEval (Yu et al., 2023)), DevEval aligns practical projects due to three key advances.\nReal program distributions. DevEval shows a real distribution of standalone and non-standalone programs. As shown in Table 1, it contains 1,984 (73.8%) non-standalone programs and 706 (26.2%) standalone programs, aligning the observed ratio in 500 practical projects.\nSufficient dependencies. DevEval covers three dependency types and contain sufficient dependencies. As shown in Table 1, each sample in DevEval averages 2.17 dependencies, surpassing the averages in previous benchmarks (e.g., CoderEval: 1.18) and closely approaching that of 500 practical projects (i.e., 2.32).\nTable 2 further shows the distribution of dependency types, i.e., intra-class, intra-file, and cross-file dependencies. DevEval outperforms previous benchmarks in all types, showing a more real distribution that is close to the distribution in 500 practical projects. For instance, cross-file dependencies constitute 30% in DevEval compared to the meager 19% in CoderEval.\nEnough-scale project contexts. As shown in Table 1, previous benchmarks' project contexts are notably small-scale, with CoderEval at 14k lines versus 46k lines in practical projects. In contrast, DevEval introduces more large-scale project contexts, averaging 45k lines.\nMoreover, DevEval has advantages in other aspects compared to existing benchmarks, such as requirements and test cases.\nRequirements. We engaged 11 developers to manually write requirements, costing approximately 674 person-hours. As depicted in Figure 2, each requirement encapsulates the code's purpose and input-output parameters. The average length of requirements in DevEval (91.5 tokens) more than"}, {"title": "2.3 Task Definition", "content": "We define the Context-based Code Generation task upon DevEval. It aims to generate code based on a function signature, a requirement, and the project contexts. We also design a baseline setting, which generates code based on the signature and requirement. The baseline is used to evaluate LLMs' coding ability without project contexts."}, {"title": "2.4 Evaluation Metrics", "content": "Pass@k (Functional Correctness). Following previous studies (Chen et al., 2021; Austin et al., 2021; Yu et al., 2023), we assess the functional correctness of programs by executing test cases. Subsequently, we compute the unbiased Pass@k. Specifically, we generate n \u2265 k programs per requirement, count the number of correct programs c \u2264 n that pass test cases, and calculate the Pass@k:\nPass@k := E_{Requirements} \\left[ 1- \\frac{\\binom{n-c}{k}}{\\binom{n}{k}} \\right]  \\tag{1}\nRecall@k (Recall of Reference Dependency). Besides the functional correctness, we expect LLMs to appropriately invoke dependencies defined in contexts. Hence, we propose Recall@k, which gauges the recall of reference dependencies in the generated programs.\nSpecifically, LLMs generate k programs per requirement. For the i-th program, we employ a parser2 to extract its dependencies as Pi. Subsequently, we compare P_i with reference dependencies R and compute the Recall@k:\nRecall@k:= E_{Requirements} \\left[ \\max_{i \\in [1,k]} \\frac{|R \\cap P_i|}{|R|} \\right] \\tag{2}\nwhere |.| means the number of elements of a set."}, {"title": "3 Benchmark Collection", "content": "As shown in Figure 3, the collection of DevEval consists of four steps. This section briefly describes the collection process.\nProject Selection. We crawl high-quality projects from an open-source community - PyPI (PyPI). To ensure a broad diversity, we identify the top 10 popular programming topics on PyPI and download the top 50 projects within each topic based on their stars on GitHub. We download the latest released version of each project in November 2023. This step results in 500 distinct open-source projects (10 topics * 50 projects).\nFunction Parse. We extract all functions within projects and parse their signatures and bodies. Trivial functions (i.e., empty functions and initialization functions) are excluded from consideration. The function bodies, crafted by developers and subjected to rigorous code reviews, are deemed as the reference code. Subsequently, we extract other programs within the current project as project contexts. This step results in the extraction of 590,365 functions from the projects. For each function, we meticulously capture its signature, the reference code, and the corresponding contexts.\nTests Construction. For each function, we extract test cases invoking it from its project. We use a popular testing framework named Pytest to organize these test cases. Next, we build the running"}, {"title": "4 Experiments", "content": "We select six popular LLMs on code generation as base models, i.e., OpenAI gpt-4-1106 (OpenAI, 2023b), gpt-3.5-turbo-{0613, 1106} (OpenAI, 2023a), CodeLLaMa-{13B, 7B} (Rozi\u00e8re et al., 2023), and StarCoder (Li et al., 2023d). Table 4 shows the basic characteristics of these LLMs."}, {"title": "4.2 Experimental Setup", "content": "The prompt template in our experiments is shown as follows."}, {"title": "4.3 Main Results", "content": "Baseline. We first show the results of LLMs in the baseline setting (i.e., without the project contexts). As the gpt-4-1106 is expensive, we provide the results in the greedy search. We observe that LLMs exhibit relatively low Pass@k and Recall@k values, particularly when compared to their performance on previous benchmarks. For instance, gpt-4 achieves a Pass@1 score of 67 on HumanEval, whereas it scores 21.6 on Pass@1 in this setting. The results validate our motivation that existing benchmarks can not comprehensively assess the capabilities of LLMs in practical projects. Furthermore, the results emphasize that the project contexts play a crucial role in practical projects.\nInterestingly, we observe that LLMs can successfully generate several dependencies without project contexts. A manual inspection of these dependencies reveals that they are mainly relatively simple dependencies that can be reasoned from the requirements, e.g., initialization functions of returned objects. It is hard for LLMs to generate more intricate dependencies without project contexts.\nContext-based Code Generation. We further take the project contexts into considerations. The project contexts are typically very long, surpassing the context window of existing LLMs. Inspired by related works (Shrivastava et al., 2023), we compress contexts by extracting parts of contexts as inputs.\nLocal file: The code file where the reference code is in. We only take programs above the reference code. We consider the local file as a fundamental context and progressively add other contexts. Imported files: Files imported by the local file. Similar files: Files with names similar to the local file. We split the names based on underscore or camelcase formatting and then match the tokens of names. If one or more parts match, two files are considered to have similar names. Sibling files: Files within the same sub-folder as the local file. Oracle: Implementations corresponding to reference dependencies. It consists of many code snippets from different files. While there are additional approaches to compressing contexts, they fall beyond the scope of this paper and are deferred to future work. We apply different contexts to gpt-3.5-turbo-1106, and the results are presented in Table 5.\nAfter introducing the contexts, both Pass@k and Recall@k values increased significantly. We inspect a few successful cases and attribute the improvements to the synergy of contexts and our"}, {"title": "4.4 Discussion", "content": "Results on different program types. Figure 5 shows the Pass@1 of gpt-3.5-turbo-1106 on different program types (i.e., standalone and non-standalone). The results reveal three observations. Firstly, project contexts are crucial to generating non-standalone functions. For example, adding a local file improves the Pass@1 on non-standalone functions from 10.84 to 36.24 points. Secondly, contexts also benefit standalone functions. This is attributed to the project-specific knowledge within contexts, aiding LLMs in understanding requirements. Thirdly, there exists considerable room for improving the Pass@1 of LLMs on both types of programs. How to effectively utilize contexts is a key problem.\nResults on different dependency types. Figure 6 shows the Recall@1 of gpt-3.5-turbo-1106 on different dependency types (i.e., intra-class, intra-file, and cross-file). The results yield two insights. Firstly, without project contexts, LLMs exhibit low Recall@1 values across three dependency types. However, LLMs demonstrate the ability to infer reason about easy dependencies based on requirements, e.g., initialization functions of returned objects. Secondly, after introducing contexts, LLMs exhibit an improvement in generating dependencies. Nevertheless, LLMs have yet to grapple with generating dependencies, especially cross-file dependencies.\nData leakage. Typically, the training data for LLMs is crawled from the Internet and is closed-source. Consequently, there is a risk of data leakage where several projects used to build DevEval may be included in the training data. We think this risk is negligible due to three reasons. First, we manually write the requirements for all samples, which are the primary inputs for code generation and are never seen by any LLMs. Second, LLMs achieve similar Pass@1 values in unseen projects and potentially seen projects. For example, we divide DevEval into two groups: Group A (77%): projects released after September 20213, and Group B (23%): projects released before September 2021. The average difference in Pass@1 of gpt-3.5-turbo-1106 on the two groups is around 1.2 points. Compared to the results in Table 3 and 5, the difference is slight. Third, we opt for high-quality projects covering general topics (e.g., Database, Text Processing), excluding rare topics that are easy to overfit. We also release the links to our selected projects and encourage practitioners to omit these projects when collecting the training data. Fourth, LLMs exhibit low Pass@k and Recall@k values on DevEval (see Table 3 and 5). This suggests that existing LLMs do not demonstrate overfitting tendencies to DevEval.\nThe bias of Recall@k. As stated in Section 2.4, we develop a parser to automatically extract dependencies in generated programs. Given that Python is a dynamically typed language, certain dependencies can only be identified at runtime and may elude static analysis technologies. Consequently, our parser may miss a few generated dependencies, potentially leading to a lower Recall@k than the actual values.\nTo gauge the bias introduced by our parser, we"}, {"title": "5 Conclusion and Future Work", "content": "In this paper, we propose a new code generation benchmark named DevEval. Collected through a meticulous pipeline, DevEval aligns practical software projects in multiple dimensions, e.g., real program distributions, sufficient dependencies, and enough-scale project contexts. We evaluate five popular LLMs on DevEval. The results reveal the strengths and weaknesses of LLMs in practical projects.\nDevEval, by presenting a more challenging and practical scenario, aims to facilitate advancements in code generation. Based on our findings, we outline potential future directions for code generation:\nCompressing Contexts. As depicted in Table 6, the project contexts are lengthy. Thus, we plan to explore new techniques to compress contexts, e.g., adaptively retrieving contexts relevant to current requirements.\nUnderstadning Contexts. As discussed in Section 4.3, LLMs struggle with understanding the long and heterogeneous contexts. We will explore new pre-training objectives or prompting techniques to help LLMs understand contexts. Besides, accelerating code generation with long contexts is a noteworthy direction."}]}