{"title": "Visual Reasoning Evaluation of Grok, Deepseek's Janus, Gemini, Qwen, Mistral, and ChatGPT", "authors": ["Nidhal Jegham", "Marwan Abdelatti", "Abdeltawab Hendawi"], "abstract": "Traditional evaluations of multimodal large language models (LLMs) have been limited by their focus on single-image reasoning, failing to assess crucial aspects like contextual understanding, reasoning stability, and uncertainty calibration. This study addresses these limitations by introducing a novel benchmark that uniquely integrates multi-image reasoning tasks with rejection-based evaluation and positional bias detection. To rigorously evaluate these dimensions, we further introduce entropy as a novel metric for quantifying reasoning consistency across reordered answer variants. We applied this innovative benchmark to assess Grok 3, ChatGPT-4o, ChatGPT-01, Gemini 2.0 Flash Experimental, DeepSeek's Janus models, Qwen2.5-VL-72B-Instruct, QVQ-72B-Preview, and Pixtral 12B across eight visual reasoning tasks, including difference spotting and diagram interpretation. Our findings reveal ChatGPT-01 leading in overall accuracy (82.5%) and rejection accuracy (70.0%), closely followed by Gemini 2.0 Flash Experimental (70.8%). QVQ-72B-Preview demonstrated superior rejection accuracy (85.5%). Notably, Pixtral 12B (51.7%) showed promise in specific domains, while Janus models exhibited challenges in bias and uncertainty calibration, reflected in low rejection accuracies and high entropy scores. High entropy scores in Janus models (Janus 7B: 0.8392, Janus 1B: 0.787) underscore their susceptibility to positional bias and unstable reasoning, contrasting with the low entropy and robust reasoning of ChatGPT models. The study further demonstrates that model size is not the sole determinant of performance, as evidenced by Grok 3's underperformance despite its substantial parameter count. By employing multi-image contexts, rejection mechanisms, and entropy-based consistency metrics, this benchmark sets a new standard for evaluating multimodal LLMs, enabling a more robust and reliable assessment of next-generation AI systems.", "sections": [{"title": "I. INTRODUCTION", "content": "The rapid advancement of large language models (LLMs) has transformed artificial intelligence, enabling state-of-the-art performance in natural language processing (NLP), reasoning, and generative tasks [4], [8], [33]. While early LLMs were primarily trained on text-based data, recent developments have led to the emergence of multimodal LLMs, capable of processing and reasoning over diverse data modalities, including text, images, videos, and structured information [22] [6], [29]. These models integrate vision and language understanding, significantly enhancing their applications across various domains, such as visual question answering (VQA) [2], [24], document comprehension [17], [27], medical image interpretation [28], [35], and multimodal dialogue systems [31], [44].\nSeveral multimodal models have demonstrated impressive capabilities in bridging vision and language understanding. XAI's Grok 3 [40] introduces advanced multimodal reasoning with a large parameter count, aiming to enhance contextual understanding and consistency. OpenAI's GPT-4o [29] extends its predecessor's capabilities by incorporating image processing and reasoning over complex visual inputs. Google Deep-Mind's Gemini 2.0 [9] also advances multimodal interactions by integrating video and spatial reasoning. Meta's LLaVA [22] aligns language and vision for improved visual grounding and generative capabilities, while Mistral's Pixtral 12B [1] introduces high-resolution image reasoning. In addition, open-source models such as DeepSeek-VL Janus [11] and Qwen-VL [3] are pushing multimodal AI research forward, democratizing access to powerful vision-language models.\nEvaluating multimodal LLMs remains a significant challenge, as existing benchmarks often assess isolated perception tasks rather than the complex reasoning required for real-world applications. While early datasets such as VQAv2 [14] and AI2D [19] focused on single-image understanding, recent benchmarks like NLVR2 [32], MMMU [42], and MathVista [23] have introduced multi-image tasks, logical comparisons, and mathematical reasoning. Moreover, MUIRBench [34] further advanced the field by integrating unanswerable question variants into multimodal visual reasoning. However, these efforts still fall short in systematically evaluating reasoning consistency, uncertainty calibration, and bias susceptibility. Addressing these gaps is crucial to ensuring that multimodal"}, {"title": "II. RELATED WORK", "content": "models move beyond pattern recognition and heuristic shortcuts to demonstrate genuine comprehension.\nUnlike previous benchmarks, this study extends on the MUIRBench benchmark [34] and systematically evaluates overall reasoning capabilities, reasoning stability, rejection-based reasoning, and bias susceptibility by: (i) Reordering answer choices to assess whether models rely on heuristic-driven shortcuts rather than content understanding; (ii) introducing entropy as a novel metric to quantify reasoning stability and consistency across reordered variants, allowing for the detection of positional biases and randomness in answer selection; and (iii) testing rejection-based reasoning and abstention rate to measure whether models correctly abstain from answering when no valid option is provided.\nThe study evaluates multiple state-of-the-art multimodal LLMs, including Grok 3 [40], ChatGPT-01 and ChatGPT-40 [30], Gemini 2.0 Flash Experimental [10], DeepSeek's Janus models [7], Pixtral 12B [1], and Qwen-VL models [3] to analyze how well-trending models generalize across these reasoning challenges. The results reveal notable discrepancies in performance. ChatGPT-40 and ChatGPT-01 consistently achieve higher accuracy and reasoning stability, while Janus 7B and Janus 1B exhibit poor accuracy and high entropy scores, indicating significant reasoning variability and susceptibility to positional biases. This suggests that Janus models rely more on surface-level patterns rather than genuine comprehension, highlighting the importance of entropy as a consistency metric in identifying unstable reasoning.\nThe remainder of this paper is structured as follows. Section II reviews related work on multimodal benchmarks and evaluation methodologies. Section III describes the dataset used, the evaluated models, the experimental setup, and the evaluation framework, including the integration of new evaluation metrics based on reordered answers and entropy for reasoning consistency. Section V presents the results of the evaluation, highlighting key trends in accuracy, reasoning consistency, and rejection-based decision-making. Section VI explores the broader implications of the findings, model-specific weaknesses, and improvements for multimodal AI evaluation. Finally, Section VII provides the conclusion and outlines future directions for refining multimodal benchmarking and reasoning assessment.\nTo provide a more comprehensive assessment, our benchmark builds on MUIRBench by incorporating four key criteria: (i) Multi-image reasoning, (ii) unanswerable question recognition, (iii) reordered answer variations, and (iv) entropy-based reasoning consistency. In particular, we introduce entropy as a novel metric to measure consistency in responses across reordered versions of the same question. By quantifying variability in answer distributions, entropy identifies models that exhibit instability in reasoning, revealing reliance on positional heuristics or superficial patterns rather than genuine content understanding. Moreover, our benchmark evaluates rejection accuracy, determining whether models correctly abstain from answering when no valid option exists.\nBy integrating multi-image reasoning, reordered answer variations, entropy-based reasoning consistency, and rejection-based evaluation, this benchmark contributes a novel frame"}, {"title": "III. BENCHMARK SETUP", "content": "To evaluate the multi-image reasoning capabilities of multimodal LLMs, we curated a subset of 120 questions and 376 images from the MUIRBench [34] dataset, ensuring a balanced assessment across diverse reasoning tasks. Unlike traditional benchmarks that focus on single-image tasks, this dataset challenges models to process and integrate multiple visual inputs, making it a more rigorous evaluation framework for contextual, spatial, and logical reasoning.\nThe dataset maintains diversity in image types, as illustrated in Figure 1, spanning real-world photographs, medical imagery, scientific diagrams, and satellite views. By integrating both curated and synthetic data, the benchmark mitigates data contamination risks, ensuring models are tested on truly novel inputs.\nAdditionally, the dataset includes a range of multi-image relations, depicted in Figure 1, challenging models to process various dependencies such as temporal sequences, complementary perspectives, and multi-view representations. This structured approach ensures that models are tested beyond simple pattern recognition, requiring advanced spatial, logical, and comparative reasoning.\nThe selection of questions from MUIRBench was guided by the need for a balanced and diverse evaluation while maintaining computational feasibility. Given that MUIRBench consists of 2,600 multiple-choice questions across 12 multi-image reasoning tasks, we curated a representative subset of 120 questions and 376 images, averaging 3.13 images per question. This selection ensures an efficient assessment of multimodal LLMs while focusing on key reasoning abilities.\nWe prioritized coverage of core multi-image tasks, selecting 8 diverse tasks that span both low-level perception (e.g., difference spotting) and high-level reasoning (e.g., image-text matching) as seen in Fig 1. Tasks with high significance in multimodal evaluation, such as Image-Text Matching and Difference Spotting, were given higher representation, each comprising 28 questions. Additionally, we ensured diversity in multi-image relationships, including temporal sequences, complementary perspectives, and multi-view representations, to evaluate models' ability to integrate spatial, logical, and comparative reasoning across multiple images.\nOur selection also maintained MUIRBench's answerable-unanswerable pairing strategy by including 40 unanswerable questions. This prevents models from exploiting shortcut biases and lucky guesses and encourages a deeper understanding of visual content.\nAdditionally, we incorporated alternate versions of each question by reordering the answer choices. This ensures that models do not rely on positional biases or heuristic patterns but genuinely understand and reason the multi-image context. By shuffling the answer choices, we can verify whether a model is consistently selecting the correct response rather than guessing based on positional tendencies.\nThis question refinement strategy improves model robustness by ensuring they rely on reasoning rather than heuristics. Our study offers a rigorous and efficient way to test how well next-generation vision-language models can reason about multiple images."}, {"title": "B. Models", "content": "In our evaluation, we utilized a diverse set of multimodal language models, as seen in Table I, each designed with unique capabilities to process and reason over visual inputs. Below is a brief overview of each model:\n1) Janus 7B: Developed by DeepSeek and was released on January 27, 2025, Janus 7B [7] is a 7-billion parameter open-source multimodal model built for advanced image understanding and text-to-image generation. It supports multiple-image processing, making it suitable for complex visual reasoning tasks.\n2) Janus 1B: developed by DeepSeek and was Released alongside Janus 7B on January 27, 2025. Janus 1B [7] is a lightweight 1-billion parameter open-source variant, designed for efficient multi-image processing while maintaining a lower computational footprint.\n3) Grok 3: Grok 3 is a multimodal large language model developed by XAI. Released on February 17, 2025, Grok 3 boasts 2.7 trillion parameters, making it one of the most advanced Al models to date.\n4) Gemini 2.0 Flash Experimental: Launched on February 10, 2025, Gemini 2.0 Flash Experimental [10] is an advanced multimodal model developed by DeepMind that processes both multiple images and videos. It is optimized for rapid inference and efficient memory usage, making it well-suited for real-time visual reasoning applications.\n5) QVQ-72B-Preview: Released on December 25, 2024, QVQ-72B-Preview [3] is a 72-billion parameter open-source vision-language model developed by Alibaba that introduces novel techniques in visual question answering. It supports multi-image reasoning, allowing for better contextual understanding across images.\n6) Qwen2.5-VL-72B-Instruct: Developed by Alibaba and released on September 19, 2024, Qwen2.5-VL-72B-Instruct [3] is a 72-billion parameter open-source instruction-tuned vision-language model. It incorporates dynamic resolution handling and multimodal rotary position embedding (M-ROPE), enabling multi-image and video comprehension.\n7) Pixtral 12B: Released on September 17, 2024, Pixtral 12B [1] is a 12-billion parameter open-source multimodal model developed by Mistral AI, specializing in high-resolution image analysis and text generation.\n8) ChatGPT-01: Introduced on December 5, 2024, ChatGPT-01 [30] is an advanced iteration of OpenAI's multimodal models, supporting multiple images and video inputs. It enhances contextual multimodal understanding, making it effective for vision-language interactions."}, {"title": "IV. METHODOLOGY", "content": "To ensure a standardized and fair assessment of all multimodal LLMs, we established a strict evaluation protocol that handles varying model constraints, answer formats, and consistency measures.\n1) Handling of Image Restrictions: Some models have restrictive policies that prevent them from processing images, either due to safety mechanisms or inherent limitations. If a model fails to accept an image input, the response is automatically marked as incorrect, as it indicates an inability to engage with the core visual reasoning task.\n2) Answer Parsing and Validity: To eliminate ambiguity in model responses, we adhere to a strict answer validation process:\n3) Ensuring Answer Consistency: To detect inconsistencies in model reasoning, we incorporate reordered answer choices across repeated questions. If a model guesses correctly on one instance but incorrectly on another reordered variant, this indicates reliance on positional heuristics rather than genuine understanding. This methodology allows us to identify unreliable answering patterns and assess whether a model is robust to answer order changes.\nThis evaluation setup ensures that models are judged on their actual reasoning abilities, eliminating potential issues or biases caused by format variations or restrictive policies.\n4) Overall Accuracy Evaluation: Each correctly answered question is assigned a score of one point, with all questions weighed equally regardless of their difficulty or complexity. The evaluation metric is based on accuracy, defined as:\nAccuracy = $\\frac{\\text{Number of Correct Answers}}{\\text{Total Number of Questions}} \\times 100\\%$ (1)\nAccuracy is a fundamental metric in evaluating multimodal large language models (MLLMs), as it provides a straightforward measure of a model's performance across various tasks. For instance, the MME benchmark employs accuracy to assess both perception and cognition abilities of MLLMs across multiple subtasks [39]. Similarly, the MM-BigBench framework utilizes accuracy to evaluate model performance on a wide range of multimodal content comprehension tasks [13].\n5) Rejection Accuracy Evaluation: In addition to overall accuracy, we evaluate each model's ability to recognize when no correct answer is available. Rejection accuracy measures how well a model correctly selects the \"None of the provided options\" choice when no valid answer is present. This is a critical aspect of uncertainty calibration, ensuring that models"}, {"title": "V. RESULTS", "content": "As illustrated in Tables II and III, and Figures 5 and 6, ChatGPT-01 was of the highest performance, achieving an accuracy of 0.825, followed by Gemini 2.0 Flash Experimental (0.717) and ChatGPT-40 (0.691). The lowest-performing models were Janus 1B and Janus 7B, with accuracy scores of 0.383 and 0.433, respectively. Grok 3, despite its large parameter count of 2.7 trillion, showed disappointing results with an accuracy of 0.580, underperforming in tasks requiring complex reasoning and consistency.\nAdditionally, QVQ-72B-Preview demonstrated high performance in geographic understanding, but struggled with image-text matching and difference spotting, which highlights a domain-specific strength but overall inconsistency. Qwen2.5-VL-72B-Instruct displayed moderate accuracy (0.633) but encountered challenges in cartoon understanding, likely due to content restrictions preventing the processing of specific images.\nThe difference in accuracy between the highest- and lowest-performing models is 0.442, highlighting a substantial performance gap across evaluated models. ChatGPT-01 leads by a margin of 13.3 percentage points over the second-best model, Gemini 2.0 Flash Experimental.\nAdditionally, as seen in Figure 6, ChatGPT-01 maintains the most balanced high performance across tasks, exhibiting consistently strong accuracy without significant weaknesses in any category. In contrast, models like Pixtral 12B and Grok 3 display more uneven performance, excelling in certain tasks while struggling in others. Pixtral 12B demonstrates relatively strong results in tasks such as cartoon understanding and diagram interpretation but underperforms in difference spotting and image-text matching."}, {"title": "B. Rejection Accuracy and Abstention Rate", "content": "As seen in Table IV and Fig 7, QVQ-72B-Preview recorded the highest rejection accuracy at 0.850, followed by ChatGPT-o1 at 0.700. Janus 7B (0.150) and Janus 1B (0.050) had the lowest rejection accuracies, showing a strong bias toward selecting an option."}, {"title": "C. Overall Accuracy vs. Rejection Accuracy and Abstention Rate", "content": "ChatGPT-01 achieved both high total accuracy (0.825) and balanced rejection accuracy (0.700) with an abstention rate of 0.267, showing well-calibrated uncertainty handling. QVQ-72B-Preview, despite high rejection accuracy (0.850), had an abstention rate (0.425) above the 0.33 threshold, suggesting overrejection. Grok 3 also showed an abstention rate of 0.375, indicating a tendency to overreject despite its high parameter count.\nJanus 1B displayed the most extreme behavior with the lowest rejection accuracy (0.050) and abstention rate (0.042), reflecting an overconfident approach. ChatGPT-40 and Gemini 2.0 Flash Experimental both showed moderate abstention rates below 0.33, maintaining a balanced decision strategy."}, {"title": "D. Reasoning Stability: Entropy-Based Consistency", "content": "To assess reasoning consistency across reordered answer variations, we compute entropy scores for each model. As shown in Table V, models with higher entropy scores, such as Janus 7B (0.8392), Janus 1B (0.787), and Pixtral 12B (0.557), exhibited greater response variability, suggesting fluctuations in answer selection across question variants.\nIn contrast, ChatGPT-01 (0.1352), ChatGPT-40 (0.216), and Grok 3 (0.256) achieved the lowest entropy values, indicating"}, {"title": "VI. DISCUSSION", "content": "The evaluation of multimodal language models on this dataset reveals critical insights into their reasoning capabilities, robustness, and inherent biases. This section examines whether model size correlates with performance, the effectiveness of open-source models compared to proprietary ones, the restrictive nature of Qwen models, and the impact of reordered answers in detecting biases. Additionally, we discuss the performance of Grok 3 and DeepSeek's Janus. While formal scientific validation of Grok 3 and DeepSeek's Janus's advanced capabilities is still emerging, online discussions suggest significant anticipation for their performance. Our analysis reveals that Grok 3 and DeepSeek's Janus exhibited suboptimal performance."}, {"title": "A. Does Model Size Correlate with Performance?", "content": "A common misconception in the research community is larger models perform better due to increased capacity for processing and learning complex relationships. While this trend holds in many cases, our results suggest that size alone does not guarantee superior performance.\nIn this benchmark, the smallest models, Janus 7B and Janus 1B, significantly underperformed against their larger counterparts. Given their lower parameter count, this is expected, yet their frequent positional biases, and inconsistent reasoning indicate that their training or optimization was insufficient to compensate for their size limitations.\nHowever, Grok 3, despite having the largest parameter count of 2.7 trillion, showcased low performance relative to its size, underperforming in tasks requiring complex reasoning and consistency. Its moderate rejection accuracy (0.525) indicate inconsistent reasoning stability, suggesting that sheer model size does not necessarily lead to more robust performance or consistent decision-making.\nThese findings underscore that while larger models generally have greater capacity, effective optimization and fine-tuning are critical for maximizing performance and consistency."}, {"title": "B. Are Open-Source Models Competitive Against Proprietary Models?", "content": "While open-source models offer transparency and adaptability, this benchmark demonstrates that proprietary models continue to hold a significant advantage in complex multimodal reasoning tasks. ChatGPT-01 and Gemini 2.0 Flash outperformed all open-source models across most reasoning tasks, underscoring the benefits of high-quality fine-tuning and diverse pretraining data. In contrast, open-source models like Janus 1B and Janus 7B exhibited significant reasoning inconsistencies (i.e., hallucination) and biases, suggesting that limited access to high-quality data remains a major constraint"}, {"title": "C. The Restrictive Nature of Qwen Models", "content": "One of the most unexpected findings was the high rate of unanswered or incorrectly answered questions from Qwen2.5-VL-72B-Instruct and QVQ-72B-Preview due to potential content restrictions. This issue was particularly evident in Cartoon Understanding, where the model struggled with half of the test cases despite the images being non-explicit or hazardous.\nThis suggests that Qwen's content filtering mechanisms are overly aggressive, preventing it from engaging with conventional content. This restrictive nature severely limits Qwen's applicability in real-world multimodal reasoning, particularly in cases where understanding humor, memes, or non-literal content is necessary. A more balanced content moderation approach could improve its usability in diverse applications without compromising ethical safeguards."}, {"title": "D. Grok 3's Unmet Expectations", "content": "Grok 3, currently in its Beta version, was positioned as a powerful contender to ChatGPT-01, boasting an impressive 2.7 trillion parameters, the largest in this benchmark. Despite its scale and claims of advanced reasoning, Grok 3 fell short of expectations, achieving underwhelming results taking its size into consideration.\nAdditionally, Grok 3 exhibited an unusually high abstention rate, indicating a tendency to reject answers more frequently than average, even when correct options were available. This suggests an overly conservative approach to uncertainty, which undermined its decision-making effectiveness.\nWhile it showed moderate success in specific tasks, Grok 3 failed to outperform ChatGPT-01 or leverage its scale for superior reasoning stability. Overall, Grok 3, although it is still in its beta version, demonstrates that model size alone does not equate to improved accuracy or reasoning consistency."}, {"title": "E. The Underperformance of DeepSeek Janus Models", "content": "The release of DeepSeek's Janus models was widely anticipated, particularly following the success of DeepSeek's R1 model, which made significant strides in competing with ChatGPT-01 despite utilizing far fewer resources [15]. This led to growing expectations that DeepSeek could emerge as a serious competitor to OpenAI across multiple AI domains. However, this benchmark demonstrates that such expectations do not hold up in visual reasoning tasks.\nJanus 7B and Janus 1B, the smallest models in this benchmark, also rank among the weakest performers, struggling significantly in numerous tasks, where they exhibit severe positional biases and incosistent reasoning. The accuracy evaluation shows that Janus models fail to generalize well across answer variations, reinforcing that their reasoning ability is not robust to minor changes in answer presentation."}, {"title": "F. The Continued Dominance of ChatGPT Models", "content": "While there has been significant development in multimodal models across different organizations, ChatGPT models continue to demonstrate an overwhelming advantage in visual reasoning tasks. ChatGPT-40 and ChatGPT-01 outperformed every other model in almost all reasoning tasks, particularly excelling in Diagram Understanding, Image-Text Matching, and Visual Retrieval. The accuracy table further highlights that these models maintained stable performance across different variations of the same questions, indicating their robustness in reasoning and comprehension.\nThese models consistently provided the most stable and accurate responses, exhibiting the lowest rates of hallucination and positional bias. While some open-source models, such as Pixtral 12B, showed promising results in specific areas, none were able to compete across the board with OpenAI's Chat-GPT models. This result reinforces the idea that proprietary models still maintain a significant edge in multimodal visual reasoning, likely due to access to better training datasets, fine-tuning techniques, and alignment strategies that remain unavailable to the public. Future developments in open-source multimodal models will need to focus on improving robustness and reducing bias to close the gap with proprietary alternatives."}, {"title": "G. The Contribution of Reordered Answers and Entropy in Detecting Biases", "content": "One of the key contributions of this study was the inclusion of reordered answer variants and the application of entropy to detect positional biases and randomness in model responses. This combined approach exposed significant differences in reasoning stability across models, highlighting the extent to which certain models rely on positional heuristics and arbitrary answers rather than genuine comprehension.\nBy using entropy to quantify variability in answer distributions across reordered variants, we effectively measured reasoning consistency and stability. Models with high entropy scores, such as Janus 7B (0.8392) and Janus 1B (0.787), exhibited significant inconsistencies across reordered variants, suggesting reliance on positional heuristics or randomness rather than content-based reasoning. Their fluctuations indicate that answer selection was influenced by surface-level patterns rather than a stable understanding of the question, particularly in tasks requiring multi-image integration.\nIn contrast, ChatGPT-01 (0.1352) and ChatGPT-40 (0.216) demonstrated the lowest entropy scores, indicating strong reasoning stability and resistance to positional biases. These models consistently selected the same answer across reordered"}, {"title": "H. Avoidance of \"None of the Choices Provided\"", "content": "The majority of models demonstrated a consistent tendency to avoid selecting \"None of the choices provided,\u201d even when it was the correct answer. This pattern was most pronounced in Janus 7B and Janus 1B, which exhibited extremely low rejection accuracy and abstention rates. Their strong bias towards selecting an option, regardless of correctness, suggests an overcommitment to answers, likely influenced by pretraining datasets that emphasize choosing the best available choice.\nIn contrast, QVQ-72B-Preview and Grok 3 displayed the highest abstention rates, exceeding the 0.33 threshold for the proportion of questions where \u201cNone of the provided options\" was correct. This indicates a more conservative approach to decision-making, with a tendency to over-reject. QVQ-72B-Preview consistently identified unanswerable questions, while Grok 3 exhibited more variability, reflecting inconsistent uncertainty calibration.\nChatGPT-01 and ChatGPT-40 maintained balanced rejection reasoning, selectively abstaining without excessive avoidance. Their abstention rates remained close to the 0.33 threshold, indicating well-calibrated uncertainty recognition and strategic decision-making.\nThese findings reveal distinct patterns in rejection behavior, highlighting that while some models consistently avoid selecting \"None of the choices provided,\" others exhibit over-rejection tendencies. The balanced approach observed in ChatGPT models underscores the importance of effective uncertainty calibration for strategic rejection-based reasoning."}, {"title": "VII. CONCLUSION", "content": "This study provides a comprehensive evaluation of multimodal LLMs, revealing significant differences in reasoning stability, bias susceptibility, and uncertainty handling. ChatGPT-01 and ChatGPT-40 consistently outperformed other models, demonstrating superior consistency, balanced rejection reasoning, and effective uncertainty calibration. These results highlight the advantages of extensive fine-tuning and high-quality training data in proprietary models.\nGrok 3, despite its massive parameter count of 2.7 trillion, failed to meet expectations, showcasing inconsistent reasoning stability, excessive rejection behavior, and moderate overall accuracy. Its high abstention rate (0.375) indicates an overly conservative approach, emphasizing that scale alone does not guarantee better performance. Similarly, Janus 7B and Janus 1B displayed the lowest rejection accuracy and reluctance to abstain, reflecting a bias towards overcommitting to answers, likely due to insufficient exposure to rejection-based reasoning.\nThis study also highlights the impact of reordered answer variations in detecting positional biases. Models with high entropy scores, such as Janus 7B (0.8392) and Janus 1B (0.787), exhibited greater variability and susceptibility to positional heuristics, whereas ChatGPT-01 (0.1352) and ChatGPT-40 (0.216) maintained consistent reasoning patterns.\nThe introduction of entropy as a reasoning consistency metric provides a novel, quantitative measure of stability across reordered variants, revealing limitations in traditional VQA metrics that focus solely on correctness.\nRejection accuracy and abstention rates further exposed weaknesses in uncertainty calibration. QVQ-72B-Preview displayed the highest rejection accuracy but also over-rejected, exceeding the 0.33 threshold, reflecting risk-averse decision-making. Conversely, Janus models consistently avoided rejection, highlighting poor uncertainty recognition. The balanced rejection strategies of ChatGPT-01 and ChatGPT-40 illustrate the importance of strategic abstention for reliable decision-making.\nOverall, this study underscores the need for advanced benchmarks that incorporate reordered answers, entropy-based consistency metrics, and rejection accuracy to more effectively evaluate reasoning stability and uncertainty calibration. Addressing positional biases, refining rejection strategies, and enhancing generalization are crucial for advancing multimodal LLMs' real-world applicability."}]}