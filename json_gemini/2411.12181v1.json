{"title": "ENHANCING LOW DOSE COMPUTED TOMOGRAPHY IMAGES USING CONSISTENCY TRAINING TECHNIQUES", "authors": ["Mahmut S. Gokmen", "Cody Bumgardner", "Jie Zhang", "Ge Wang", "Jin Chen"], "abstract": "Diffusion models have significant impact on wide range of generative tasks, especially on image inpainting and restoration. Although the improvements on aiming for decreasing number of function evaluations (NFE), the iterative results are still computationally expensive. Consistency models are as a new family of generative models, enable single-step sampling of high quality data without the need for adversarial training. In this paper, we introduce the beta noise distribution, which provides flexibility in adjusting noise levels. This is combined with a sinusoidal curriculum that enhances the learning of the trajectory between the noise distribution and the posterior distribution of interest, allowing High Noise Improved Consistency Training (HN-iCT) to be trained in a supervised fashion. Additionally, High Noise Improved Consistency Training with Image Condition (HN-iCT-CN) architecture is introduced, enables to take Low Dose images as a condition for extracting significant features by Weighted Attention Gates (WAG).Our results indicate that unconditional image generation using HN-iCT significantly outperforms basic CT and iCT training techniques with NFE=1 on the CIFAR10 and CelebA datasets. Moreover, our image-conditioned model demonstrates exceptional performance in enhancing low-dose (LD) CT scans.", "sections": [{"title": "1 Introduction", "content": "X-ray computed tomography (CT) is essential in both diagnosis and treatment, with applications ranging from detecting internal injuries and tumors to surgical planning. To minimize the harmful effects of low-dose ionizing radiation, many studies focus on achieving high-quality denoising while keeping the dose as low as reasonably possible. Recent studies reveals that generative tasks has a remarkable success to increase quality of low dose CT scans. The most commonly used techniques due to their ease of application are Non-local Means (NLM) and Block-Matching 3D (BM3D), both of which can enhance low-dose CT (LDCT) performance. However, despite their utility, these post-processing methods often fall short of meeting clinical requirements [9, 3].\nRecent advancements in deep learning showed that generative models are highly capable of meeting clinical requirements for LDCT denoising [19, 5, 1, 21]. Especially, Diffusion Probabilistic Models (DDPM) models outperform other techniques, especially GANs, upon the task of image denoising by iteratively recovering data [2]. The core working"}, {"title": "2 Background", "content": "Consistency models [16], relies on continuous-time definition of diffusion models which generate data by gradually transforming the data distribution,pdata, into a noise distribution using stochastic differential equation (SDE). These models then learn to reverse this process to generate data from noise. Notably, the SDE has a related ordinary differential equation (ODE) known as the probability flow (PF) ODE [17].According to definition of score based generative models, PF ODE shapes into"}, {"title": "2.1 Consistency Models", "content": "Consistency models [16], relies on continuous-time definition of diffusion models which generate data by gradually transforming the data distribution,pdata, into a noise distribution using stochastic differential equation (SDE). These models then learn to reverse this process to generate data from noise. Notably, the SDE has a related ordinary differential equation (ODE) known as the probability flow (PF) ODE [17].According to definition of score based generative models, PF ODE shapes into\n$\\displaystyle dx = -\\sigma\\nabla_x \\log p_\\sigma(x) dt, \\sigma\\in [\\sigma_{min}, \\sigma_{max}]$ \nfor $\\nabla_x \\log p_\\sigma(x)$ is score function for perturbed data distribution $p_\\sigma(x) \\approx P_{\\sigma_{min}}(x)$. $\\sigma_{min}$ is defined as 0.002 to maintain stability and $\\sigma_{max} = 80$ is chosen as reasonable by [6] to provide $p_\\sigma(x) \\approx N(0, \\sigma_{max}I)$. Solving the probability flow ODE from $\\sigma(t + 1)$ to $\\sigma(t)$ allows the model to transform a sample $X_{\\sigma_{t+1}} \\approx P_{\\sigma_{t+1}}$ to $X_{\\sigma_{t}} \\approx P_{\\sigma_{t}}$.\nThe provided bijective mapping between $x_\\sigma \\approx p_\\sigma(x)$ and $X_{\\sigma_{min}} \\sim P_{\\sigma_{min}}(x) \\approx P_{data}(x)$ maintains consistency and which is denoted as $f^* : (x, \\sigma) - X_{\\sigma_{min}}$.\nA consistency model which denoted as $f_\\theta(x, \\sigma)$ is parameterized to meet boundary condition and transforming it into a differentiable form, it is parameterized as it is defined in [16].\n$\\displaystyle f_\\theta(x, \\sigma) = C_{skip}(\\sigma)x + C_{out}(\\sigma)F_\\theta(x, \\sigma)$, \nWhere $F_\\theta(x, \\sigma)$ is a free-from network. To train consistency models, probability flow ODE is discretized using noise sequences are ranging from $\\sigma_{min}$ to $\\sigma_{max} = \\sigma_N$. Discretization of these noise sequences is denoted as\n$\\displaystyle \\sigma_i = \\sigma_{min} + (\\sigma_{max}^{1/\\rho} - \\sigma_{min}^{1/\\rho}) (\\frac{i-1}{N-1})^{1/\\rho} for i \\in [1, N], \\rho = 7$ in [6].\nConsistency models are trained using the consistency matching loss\n$\\displaystyle \\mathbb{E} [\\lambda(\\sigma_i)d(f_\\theta(\\~{x}_{\\sigma_{i+1}}, \\sigma_{i+1}), f_{\\theta^-}(\\~{x}_{\\sigma_i}, \\sigma_i))],$"}, {"title": "2.2 Improved Training Techniques for Consistency Models", "content": "Improved training techniques for CT moves one step forward isolation training for consistency models [15]. The modifications are utilized for improved consistency models (iCM) consist of curriculum, elimination of EMA and teacher model, replacing LPIPS loss function with pseudo huber loss and the noise distribution changed from uniform to lognormal noise distribution. The first modification, setting N to 1281 in N(k), provides a good balance between bias and variance compared to CM training. Experimental results show that changing so from 2 to 10 and 81 from 150 to 1280 yields the best generative performance for iCT.\n$\\displaystyle N(k) = min(802^{\\lfloor \\frac{k}{K'}\\rfloor}, 81) +1, K' = \\frac{K}{\\log_2 (\\frac{8_1}{8_0})+1} where K = 1280$\niCT utilizes the same noise scheduling as it is described in [6] and Equation 4 which emphasizes high weighted low noise levels and corresponds to $p(log \\sigma) = \\sigma^{p-1} \\sigma_{max}^{1/\\rho -1}$ as N$\\rightarrow \\infty$.\n$\\displaystyle \\sigma_i = ((\\sigma_{min}^{1/\\rho} + (\\sigma_{max}^{1/\\rho} - \\sigma_{min}^{1/\\rho}) (\\frac{i-1}{N-1}) )^\\rho )$ for i$\\in$ [1, N], and $\\rho$ = 7,\nBesides the exponential curriculum and Karras noise scheduling, to emphasize lower noise levels in the noise distribution during training, the modification employed for iCM includes a log-normal noise distribution on image batches, which significantly assigns low weights to high noise levels.\n$\\displaystyle p(\\sigma_i) \\propto erf (\\frac{log(\\sigma_{i+1}) - \\mu}{\\sqrt{2}\\sigma_{std}}) - erf(\\frac{log(\\sigma_{i}) - \\mu}{\\sqrt{2}\\sigma_{std}})$ where $\\mu=-1.1$ and $\\sigma_{std} = 2.0$. This log-normal noise schedule leverages sampling quality and significantly decrease FID scores. Addition to lognormal noise distribution, to increase the emphasize on lower noise levels which is provided by lognormal noise distribution, the loss weighting is adjusted as\n$\\displaystyle \\lambda(\\sigma_i) = \\frac{\\sigma_{i+1}}{\\sigma_i}$\nThe refined weighting function notably improves sample quality in consistency training by assigning smaller weights to higher noise levels. This approach addresses the issue of the default uniform weighting function, which assigns equal weights to all noise levels and is found to be suboptimal. By reducing the weighting as noise levels increase, the new method ensures that smaller noise levels, which can influence larger ones, are weighted more heavily.\nImproved training techniques for CT eliminates teacher model and EMA update during training. The underlying reason for elimination of EMA and teacher model is unbiased trajectory mapping between student and teacher model and the decay rate is updated as $\\mu(k) = 0$. The loss function LPIPS employed for CT is replaced with the pseudo-Huber loss due to undesirable bias in evaluation.\n$\\displaystyle d(x, y) = \\sqrt{ ||x - y||_2^2 + c^2 } - c$"}, {"title": "3 Methods", "content": "The proposed approach, High Noise Improved Consistency Training (HN-iCT), shares the same backbone as the consistency models used for image generation. We categorized this approach into two sections: the first focuses on unconditional image generation, which involves modifications to the curriculum and noise distribution, while the second is conditional image generation (HN-iCT-CN), which requires a different architecture in addition to the proposed curriculum and beta noise distribution, as shown in Figure 1.\nFor unconditional image generation, HN-iCT relies on the same U-Net architecture as implemented for consistency models [16]. For image conditional generation, the most well-known technique is label embedding or class embedding. Generally, this type of embedding is implemented by summing encoded features irrespective of the dimensions of the input size. In other cases, such as when an image is given as condition to a model, it is important to choose what type of embedding will be employed while training. In this study, AG (Attention Gate) modules are utilized to extract common spatial features between the image chosen as condition and input image [13]. Further more, AG modules are modified as Weigted Attention Gate (WAG) to prevent incompatibility between other U-NET components and adapted according to HN-iCT model. Figure 1 represents general architecture used for image conditional training, comprehends iAG modules.\nWeighted Attention Gate (WAG): During the encoding process, gradually downsampled features retain essential structural information about the image that will be denoised in the decoding process. While it is possible to reconstruct the denoised image from the latent space, the output may include noisy pixels due to the loss of structural details. To achieve a clean image at the end of the decoding process and preserve structural information, it is necessary to evaluate the skip connections and decoded features. The attention gates are designed to assess both global structural information and pixel-wise details to reconstruct denoised images effectively. In our implementation, the Weighted Attention Gate (WAG) incorporates a learnable weighting mechanism, where the attention map is squared to sharpen common features between the skip connection and the conditioned input. The gate ensures that the skip connections do not dominate the reconstruction while preserving the spatial-temporal features. Additionally, extracted features from the conditioned input are scaled by a weight parameter (defaulting to 0.8) and combined with the skip connection output to achieve a balanced reconstruction. The architecture of WAG is represented in Figure 2."}, {"title": "3.2 Training Technique", "content": "High Noise Level Effect on Performance. Noise distribution in mini-batches plays a crucial role in teaching the model the trajectories between noise levels. To provide a high-confidence trajectory, it is important to have a noise distribution that includes a wide variety of noise levels. A noise distribution with a high variety of noise levels increases the number of high noise levels in the distribution, in contrast to the log-normal noise distribution used in iCT training techniques [15].\nIn this section, we evaluate the experimental results conducted within the scope of adding extra high noise levels manually to scheduled noises with a log-normal distribution. As it is claimed 'High Noises is a Must', the experiments begin by adding high-noise levels gradually on the mini-batches based on percentage ratio of the mini-batch size. This"}, {"title": "4 Experiments", "content": "For unconditional image generation, we consider CIFAR10 [8], which includes 50K training images; CelebA 64x64 [11], which contains 162,770 training images of individual human faces; and Butterflies [18] 256x256 dataset, which provides 1K images with different species of butterflies.\nFor image conditional learning, the dataset sourced from the Mayo Clinic, as used in the AAPM low-dose CT grand challenge, was utilized [12]. The data is reconstructed on a 512x512 pixel grid with a slice thickness of 1 mm and a medium (B30) reconstruction kernel. The first eight patients provided training data, resulting in a total of 4800 slices, while the remaining two patients were used for validation, contributing a total of 1136 slices. This demonstrates the model's effectiveness on medical data and validates that the proposed training technique yields good results across different datasets.\nImage Conditioning. The image conditioning technique employed in our model takes Low dose 512x512 reconstructed CT slices as a condition for denoising process. No additional pre-processing is applied on low dose slices before submitting it as a condition. It is a similar approach such as label embedding."}, {"title": "4.1 Experimental Setup", "content": "Datasets. For unconditional image generation, we consider CIFAR10 [8], which includes 50K training images; CelebA 64x64 [11], which contains 162,770 training images of individual human faces; and Butterflies [18] 256x256 dataset, which provides 1K images with different species of butterflies.\nFor image conditional learning, the dataset sourced from the Mayo Clinic, as used in the AAPM low-dose CT grand challenge, was utilized [12]. The data is reconstructed on a 512x512 pixel grid with a slice thickness of 1 mm and a medium (B30) reconstruction kernel. The first eight patients provided training data, resulting in a total of 4800 slices, while the remaining two patients were used for validation, contributing a total of 1136 slices. This demonstrates the model's effectiveness on medical data and validates that the proposed training technique yields good results across different datasets.\nImage Conditioning. The image conditioning technique employed in our model takes Low dose 512x512 reconstructed CT slices as a condition for denoising process. No additional pre-processing is applied on low dose slices before submitting it as a condition. It is a similar approach such as label embedding."}, {"title": "Beta Distribution", "content": "The beta distribution is a continuous probability distribution defined on the interval [0, 1], commonly used to model the behavior of random variables that are constrained within this range. It is parameterized by two positive shape parameters, denoted as a (alpha) and \u03b2 (beta), which determine the distribution's shape. The probability density function (PDF) of the beta distribution is given by:\n$\\displaystyle f(x; \\alpha, \\beta) = \\frac{x^{\\alpha-1}(1 - x)^{\\beta-1}}{B(\\alpha, \\beta)} = \\frac{1}{B(\\alpha, \\beta)} where B(\\alpha, \\beta) = \\int_0^1 t^{\\alpha-1}(1-t)^{\\beta-1} dt$\nfor 0 \u2264 x \u2264 1. The parameters a and \u03b2 influence the skewness and kurtosis of the distribution, allowing for a wide range of shapes including uniform, U-shaped, and J-shaped distributions. This flexibility makes the beta distribution particularly useful in Bayesian statistics, where it is often employed as a prior distribution for probabilities and proportions.\nThe flexibility of the beta distribution makes it highly adaptable for various noise distributions, particularly for those aiming to adjust the weight of noise levels within a mini-batch [22]. Particularly, it is possible increasing the weight of high level noises in distribution up to 4% by adjusting a = 0.5 and \u03b2 = 5. CLAIM and modify: The set of parameters were empirically determined by using CIFAR10."}, {"title": "Sinusoidal Curriculum", "content": "The proposed sinusoidal curriculum, inspired by the sinus function, offers a significant advancement in the training of consistency models by introducing a smooth, continuous progression of the number of timesteps for each noise distribution on mini-batch. Unlike improved curriculum that rely on abrupt or stepwise changes, the sinusoidal curriculum leverages the natural oscillation of the sine function to modulate the variety of noise levels encountered during training. This gradual adjustment ensures that the model experiences a consistent and well-distributed range of noise levels, enhancing the stability and robustness of the learning process.\n$\\displaystyle N(k) = min(81 sin( {\\pi*.3k \\over 2*K} )) + so  +1,81+1)$\n$\\displaystyle \\sigma(i) = sin( {\\pi*i \\over 2*N} ) * \\Delta t + t_o$\nThe sinusoidal curriculum is governed by key parameters. The initial timestep so sets the starting point, while the difference between final number of time steps and initial the number of time steps $\\Delta t = s_1 - s_o$ controls the amplitude of the sinusoidal curve. The total number of time steps N defines the schedule length, with the sine function scaled by \u03c0 to ensure a smooth, gradual adjustment across timesteps. To prevent marginal increases, so and s\u2081 are set to 20 and 250, respectively, providing broad yet stable coverage of noise levels and enhancing training stability."}, {"title": "4.2 Unconditional Image Generation", "content": "We compare HN-iCT model with prior models are trained with CT, iCT technique and diffusion models. Considering lack of studies based on CT and iCT technique, we evaluate publicly available sources on iCT and CT technique for comparing with our approach. The models are gathered from public resources are referred as number of versions from v1 to v3 in table 2. Additionally, to recover fairness of comparison between models, all models compared in table 2 are trained locally. As shown in Table 2, HN-iCT performs better FID results when it is compared the other models which are publicly available and official models. We employed 2 number of residual blocks instead of 4 to prove robustness of our proposed training technique."}, {"title": "4.3 Image Conditioned Generation", "content": "In this section, our proposed image conditioned consistency model (HN-iCT-CN Small) which employs Weighted Attention Gate (WAG) modules is utilized for enhancing low-dose CT scans. Experimental results are shown in Table 3 and compared with related studies utilizing same LDCT dataset [12]."}, {"title": "4.4 Ablation Study", "content": "In this section, we present an ablation study conducted on the improved curriculum, sinusoidal curriculum, log-normal noise distribution, and beta noise distribution. The model configuration used for the ablations is designated as HN-iCT-Small, and the training steps are kept the same at 400K for each configuration, as shown in Table 4. Each training configuration for the ablations is represented by version numbers from HN \u2013 iCT \u2013 Sv1 to HN \u2013 iCT - Sv4, with two different options configured within the maximum possible transformation configurations."}, {"title": "5 Discussion and Conclusion", "content": "Our enhancements to noise scheduling and curriculum address the need for a balanced noise distribution and controlled progression of noise steps within the curriculum. We examined the impact of the high noise levels in a noise distribution"}, {"title": "A High Noise Level Experimental Details", "content": "The experiments reveals that adding minor weighted high noise levels on mini-batches increase denoising performance. As it is represented in table 5, while adding high level noise levels with lower percentage ratios can enhance denoising performance, adding high level noise at 10% of the mini-batch length has effects on denoising performance conversely."}]}