{"title": "Grid and Road Expressions Are Complementary for Trajectory Representation Learning", "authors": ["Silin Zhou", "Shuo Shang", "Lisi Chen", "Peng Han", "Christian S. Jensen"], "abstract": "Trajectory representation learning (TRL) maps trajectories to vectors that can be used for many downstream tasks. Existing TRL methods use either grid trajectories, capturing movement in free space, or road trajectories, capturing movement in a road network, as input. We observe that the two types of trajectories are complementary, providing either region and location information or providing road structure and movement regularity. Therefore, we propose a novel multimodal TRL method, dubbed GREEN, to jointly utilize Grid and Road trajectory Expressions for Effective representatioN learning. In particular, we transform raw GPS trajectories into both grid and road trajectories and tailor two encoders to capture their respective information. To align the two encoders such that they complement each other, we adopt a contrastive loss to encourage them to produce similar embeddings for the same raw trajectory and design a mask language model (MLM) loss to use grid trajectories to help reconstruct masked road trajectories. To learn the final trajectory representation, a dual-modal interactor is used to fuse the outputs of the two encoders via cross-attention. We compare GREEN with 7 state-of-the-art TRL methods for 3 downstream tasks, finding that GREEN consistently outperforms all baselines and improves the accuracy of the best-performing baseline by an average of 15.99%. Our code and data are available online\u00b9.", "sections": [{"title": "1 Introduction", "content": "Trajectories record the movements of objects (e.g., vehicles or pedestrians). Analyzing trajectory data is crucial for understanding movement patterns and helps to solve many real-world problems, including transportation optimization [37], traffic management [21], and human mobility analysis [36]. These problems rely on a set of basic trajectory operations such as trajectory classification [26], travel time estimation [15], and trajectory similarity computation [16].\nTrajectory representation learning (TRL) is the task of learning vector representations of variable-length trajectories to facilitate these operations [7]. For example, to compute trajectory similarity, traditional methods have a quadratic cost w.r.t. trajectory length [2, 5, 6] as they adopt dynamic programming, instead using fixed-length vectors is much more efficient.\nA trajectory can be expressed in three different forms: as a GPS trajectory, a grid trajectory, or a road trajectory, as shown in Figure 1.\nIn particular, a GPS trajectory is a sequence of GPS points sampled at regular time intervals, with the points being timestamped coordinates. A GPS trajectory is often noisy due to positioning inaccuracies (e.g., P2 in Figure 1), and this noise is harmful to downstream processing. Thus, only early TRL methods (e.g., Traj2vec [41] and CSTRM [27]) use raw GPS trajectories as inputs. Grid trajectories are obtained by covering the space with a grid and mapping GPS points to the grid cell that contains them. Thus, a grid trajectory is a sequence of timestamped grid cells. Compared with a GPS trajectory, a grid trajectory preserves location and time information but is more robust to noise as multiple GPS points can be mapped to the same cell. Moreover, grid trajectories can also encode information about urban regions (e.g., downtown or residential) since a grid cell covers a fixed region so that properties such as trajectory density and average movement speed can be associated with such regions. Therefore, methods like t2vec [24], E2DTC [11], and TrajCL [4] use grid trajectories. Next road trajectories are obtained by map matching [29, 38]) GPS trajectories to an underlying road network, so that road trajectories are timestamped sequences of road segments. Like grid trajectories, road trajectories can reduce noise due to the map matching. By incorporating a road network, road trajectories model movement regularities, as vehicles can only travel along the road segments. The majority of recent TRL methods, such as START [17], JGRM [30], and JCLRNT [31], use road trajectories.\nWe observe that existing TRL methods only use a single type of trajectory representation, although different trajectory expressions capture complementary information. Specifically, grid trajectories reduce noise in raw GPS points and excel at capturing location and time information. They also capture information about urban regions. In contrast, road trajectories capture movement regularity along road segments. These trajectories also capture information about the road network. For example, deriving properties, traffic volumes, travel speeds of road segments, and transition probabilities between road segments. Such information can be used to enhance the trajectory representation in turn.\nWe propose GREEN to jointly utilize grid and road trajectory expressions for TRL. By exploiting both expressions, GREEN aims to improve over existing TRL methods. GREEN resembles multi-modal learning [23, 25, 32], as it treats the two trajectory expressions as different modalities. Like multi-modal learning, GREEN addresses two challenges to make its idea work.\nHow to effectively capture the information in grid and road trajectory expressions? GREEN includes an encoder for each trajectory expression to capture its particular information. Thus, the grid encoder uses a convolution neural network (CNN) [22] and treats the grid cells as pixels in an image and uses the traffic flow and geographical locations of the cells as pixel values. To encode more information, we augment the cell representations learned by the CNN with spatial-temporal information unique to each GPS trajectory. The road encoder uses a graph neural network (GNN) [20] to learn representations with structure information from the road segments in the road network graph. We also enhance the road representations with road type and trajectory-specific timings to capture more information. Both the grid encoder and road encoder adopt Transformer [34] as the backbone by modeling trajectories as sequences of grid and road representations. The outputs of the two encoders are the grid and road representations of trajectories.\nHow to integrate the grid and road trajectory representations? We need to first ensure that the grid and road representations align with each other and then fuse them into a final trajectory representation. We design two loss functions for alignment, i.e., contrastive loss [32] and masked language model (MLM) loss [10]. The contrastive loss encourages the grid and road encoder to produce similar representations for the same trajectory such that the representations belong to the same space and can be fused. The MLM loss reconstructs a complete trajectory from a masked road trajectory and uses the grid representation to facilitate the reconstruction. As such, it encourages the grid representation to complement the road representation. Since the contrastive loss uses complete road trajectories while the MLM loss uses masked road trajectories, we perform two forward operations over the road encoder with different inputs. Finally, we use an interactor to fuse the grid and road representations via cross-attention.\nTo evaluate GREEN, we compare with 7 state-of-the-art TRL methods and experiment on 2 real-world datasets with 3 important downstream tasks: travel time estimation, trajectory classification, and most similar trajectory search. The results show that GREEN is able to consistently outperform all baselines in terms of accuracy across all tasks and datasets. In particular, compared with the best-performing baseline, the average accuracy improvement of GREEN for travel time estimation, trajectory classification, and most similar trajectory search are 19.55%, 2.21%, and 23.90%, respectively. An ablation study shows that all elements of GREEN contribute to improving accuracy. We also observe that GREEN is comparable to the baselines in terms of training and inference efficiency.\nTo summarize, we make the following contributions:\n\u2022 We observe that existing TRL methods use only one trajectory expression and propose to jointly utilize grid and road trajectory expressions to capture more information for enhanced accuracy.\n\u2022 We design GREEN as a self-supervised learning model for TRL, featuring encoders tailored to extract information from both grid and road trajectory expressions, and we propose specialized loss terms to ensure that the two expressions complement each other.\n\u2022 We evaluate GREEN experimentally, by comparing it with 7 state-of-the-art TRL methods and on 3 downstream tasks, validating our designs in ablation study, and quantifying its efficiency."}, {"title": "2 Related Work", "content": "Existing solutions to the TRL problem can be classified into GPS-based TRL, grid-based TRL, and road-based TRL.\nGPS-based TRL. The TRL problem can be addressed by learning representations from GPS trajectories in free space. For example, traj2vec [41] transforms a GPS trajectory to a trajectory feature sequence and applies an RNN-based seq2seq [33] model to learn trajectory representations. CSTRM [27] learns trajectory representations by distinguishing trajectory-level and point-level differences between GPS trajectories. However, GPS points carry limited spatial-temporal information and are noisy [7], e.g., due to drift points, inversion points, and redundant points, which can lead to inaccurate trajectory modeling. Thus, only early methods use GPS trajectories directly.\nGrid-based TRL. To simplify or enhance GPS trajectories in free space, some methods [4, 11, 24, 40] use grids to simplify GPS trajectories and learn grid trajectory representations. For example, t2vec [24] first applies an encoder-decoder model [33] to learn robust grid trajectory representations from low-quality data to support trajectory similarity computation and search. E2DTC aims to capture hidden spatial dependencies in trajectories and proposes self-training to learn grid trajectory representations for trajectory clustering. TrajCL [4] introduces various trajectory augmentations and uses contrastive learning to jointly learn grid trajectory representations from spatial and structural perspectives. Unlike the above methods, TrajGAT [40] abandons the sequential model in favor of a graph-based model to learn trajectory representations. In our study, we employ a CNN to capture regional properties of grid cells, while incorporating GPS features to improve spatial-temporal information capture and enhance accuracy of grid representations.\nRoad-based TRL. Road trajectories, obtained using map matching [29, 38], capture the transportation structure of a region [7]. Some studies learn road trajectory representations for specific tasks, e.g., similarity computation [12, 16, 42], trajectory anomalous detection [28], and trajectory generation [9]. However, the representations obtained by these methods are difficult to transfer to other spatial-temporal tasks. To learn generic road trajectory representations, Trembr [13] uses an RNN-based seq2seq model [33] and uses spatial-temporal information of road segments to learn representations. PIM [39] and Toast [8] first use node2vec [14] to learn road embeddings on the road network. Then PIM utilizes mutual information maximization, and Toast introduces route recovery tasks to reconstruct masked road segments to learn trajectory representations, respectively. JCLRNT [31] creates specialized augmentations for road-road contrast using contextual neighbors and for trajectory-trajectory contrast by replacing detours and removing alternatives. START [17] integrates travel semantics and temporal regularities into the BERT [10], utilizing various data augmentations such as trimming and temporal shifting of trajectories to enhance the accuracy and robustness of trajectory representations. JGRM [30] enriches the spatial-temporal information of roads and models road trajectories by aligning GPS points to corresponding road segments. Our study considers the information of grid and road trajectories, and we integrate them to obtain the final representation."}, {"title": "3 Preliminaries", "content": "3.1 Definitions\nDefinition 1 (GPS Trajectory). A GPS trajectory \\(T^{GPS}\\) is a sequence of points collected at a fixed sampling time interval by a GPS-enabled device. Each point is of the form \\(p_i = (x_i, y_i, t_i)\\), where \\(x_i, y_i\\), and \\(t_i\\) denote longitude, latitude, and timestamp, respectively.\nDefinition 2 (Grid Trajectory). A grid trajectory \\(T^g\\) is a sequence of timestamped grid cells obtained by transforming GPS points in \\(T^{GPS}\\). Each cell is of the form \\(\\tau = (g_i, t)\\), where \\(g_i\\) denotes the ID of a grid and \\(t\\) denotes the time.\nDefinition 3 (Road Network). A road network is modeled as a directed graph \\(G = (V, A)\\), where \\(V\\) denotes the road segment set in the road network, and \\(A \\in R^{|V|\\times|V|}\\) is the adjacency matrix that represents the connectivity between road segments. \\(A[i, j] = 1\\) if and only if road segments \\(v_i\\) and \\(v_j\\) are directly connected, otherwise \\(A[i, j] = 0\\). Under this definition, a trajectory can be extracted as a sequence of road segments that it passes through.\nDefinition 4 (Road Trajectory). A road trajectory \\(T^r\\) is a timestamped sequence of road segments that is generated from GPS trajectory \\(T^{GPS}\\) using map matching. Each road segment is of the form \\(\\tau = (v_i, t)\\), where \\(v_i\\) denotes the ID of a road segment in the road network \\(G\\) and \\(t\\) denotes the time.\n3.2 Problem Statement\nWe are given a set of GPS trajectories and a road network. Then we aim to learn a d-dimensional vector representation of each trajectory in the set by using the two expressions, i.e., grid trajectories and road trajectories. We expect the learned representation to achieve high accuracy for downstream tasks such as:\n\u2022 Travel time estimation: Given a trajectory \\(T_a\\) without time information, this task predicts the travel time of \\(T_a\\).\n\u2022 Trajectory classification: Given a trajectory \\(T_a\\), this task assigns \\(T_a\\) to a category, e.g., the type of vehicle.\n\u2022 Most similar trajectory search: Given a query trajectory \\(T_a\\) and a trajectory database D, this task finds the trajectory \\(T_b \\in D\\) that is most similar to \\(T_a\\)."}, {"title": "4 The GREEN Model", "content": "We proceed to present GREEN. Figure 2 provides an overview of GREEN, which consists of three modules, i.e., a grid encoder (Section 4.1), a road encoder (Section 4.2), and a dual-modal interactor (Section 4.3). We use two self-supervised tasks (Section 4.4) to train GREEN, i.e., contrastive loss and MLM loss. For conciseness, we ignore multi-head attention and the [CLS] token in formulas related to the Transformer architecture.\n4.1 Grid Encoder\nThe grid encoder is designed to learn grid trajectory representations in free space. To obtain grid trajectories, the underlying space is first partitioned using a regular grid, and then GPS points are mapped to the ID of the grid cells they fall into. Compared to GPS trajectories, grid trajectories retain location information and capture rich regional information. We design a grid encoder to learn region and spatial-temporal information of grid trajectories.\nCNN-based Grid Learning. We first learn regional information from a global view. In particular, a grid map of H-height and W-width can be transformed into a 2D-image \\(M \\in R^{H\\times W\\times C}\\), where (H, W) is the resolution of the image and C is the number of channels. The channels represent the features of the grid cells, and we use three types of region information: \\((x_i, y_i, c_i)\\). Thus, C = 3, and \\((x_i, y_i)\\) denotes the central coordinate of the grid, while \\(c_i\\) captures the traffic flow (the number of times trajectories in the training dataset visits a grid cell) of cell \\(g_i\\). Then, we use a CNN to learn global region representations for the grids in the 2D-image:\n\\begin{equation}\n\\begin{array}{l}\nX_{1}=\\sigma(\\text { Conv }_{1}(M)), \\\\\nX_{2}=\\sigma(\\text { Conv }_{2}(X_{1})),\n\\end{array}\n\\end{equation}\nwhere \\(\\sigma\\) is the relu [1] activation function, \\(Conv_1\\) and \\(Conv_2\\) are convolution kernels of size 3 \u00d7 3. Note that \\(X_2 \\in R^{H\\times W\\times C'}\\), where \\(C'\\) is the number of output channels. To learn grid embeddings with hidden dimension h, we reshape \\(X_2\\) into a 2D-tensor, and use a 2-layer MLP to map it:\n\\begin{equation}\nX^{g} = MLP(Reshape(X_{2})),\n\\end{equation}\nwhere \\(X^g \\in R^{(HW)\\times h}\\). We take \\(X^g\\) as the grid embedding table and transform each grid ID into an embedding by means of a lookup in \\(X^g\\). Thus, we can transform a grid trajectory \\(T^{g} = (\\tau_{1}^{g}, \\tau_{2}^{g},..., \\tau_{|T^{g}|}^{g})\\) into a grid embedding sequence \\(E^{g} = (e_{1}^{g}, e_{2}^{g}, ...e_{|T^{g}|}^{g})\\).\nFeature-enhanced Grid Representation. Although the grid embedding contains rich region information, it is the same for different trajectories. However, the same grid ID has different semantics for different trajectories. To address this issue, we incorporate GPS information for a grid trajectory \\(T^g\\). For each grid \\(\\tau = (g_i, t_i) \\in T^{g}\\), we choose a GPS point as the anchor, and if there are multiple consecutive GPS points of a trajectory in \\(g_i\\), we choose the first GPS point (The statistical results show that less than 1% of the grids under 100m\u00d7100m in the dataset have more than one GPS point). For the spatial information, we compute a 4-dimensional vector \\(\\hat{e_i}^{g} = (x, y, d, r)\\) for each grid \\(\\tau \\in T^{g}\\), where \\(x\\) and \\(y\\) are GPS coordinates, \\(d\\) is the distance to the previous GPS point and \\(r\\) is the azimuth w.r.t. the previous GPS point. Then we use a linear layer to fuse the general \\(e_i^{g}\\) and the trajectory specific \\(\\hat{e_i}^{g}\\):\n\\begin{equation}\ne_{i} = Linear(e_{i}^{g} || \\hat{e}_{i}^{g}),\n\\end{equation}\nwhere || denotes vector concatenation. For the temporal information, we use pre-trained Time2vec [18] to encode time embedding for the fine-grained time \\(t\\), i.e., timestamp, as follows:\n\\begin{equation}\nt_{i} = f_1(t) || sin(f_2(t)),\n\\end{equation}\nwhere \\(f_1()\\) and \\(f_2 (.)\\) are linear layers to learn time embedding, and \\(sin()\\) function helps capture time periodic behaviors. Then we use a Transformer to learn the grid trajectory representation as follows:\n\\begin{equation}\n\\begin{aligned}\nh_{i} &= e_{i} + t_{i} + p_{i}, \\\\\nH^{g} &= [h_{1}, h_{2}, ..., h_{|T^{g}|}] \\in R^{|T^{g}|\\times h}, \\\\\nZ^{g} &= TransformerEncoder(H^{g}),\n\\end{aligned}\n\\end{equation}\nwhere \\(p_{i}\\) is the position encoding of grid \\(\\tau_i\\), \\(h_i\\) is the hidden embedding of the Transformer, and \\(Z^{g} \\in R^{|T^{g}|\\times h}\\). Next, we use a linear layer to map \\(Z^g\\) into a new embedding space with dimension d:\n\\begin{equation}\nZ^{g} = Linear(Z^{g}) \\in R^{|T^{g}|\\times d}\n\\end{equation}\nThe final grid trajectory representation \\(v^g\\) is obtained by the embedding of the [CLS] token into \\(Z^{g}\\).\n4.2 Road Encoder\nThe road encoder aims to learn road trajectory representations on the road network. A road trajectory, which is obtained by map matching, captures the actual route of the object. Compared with a GPS trajectory, a road trajectory is continuous and includes road segment transfer information. The road network also introduces information on urban transportation structures. We design a road encoder to learn urban structure and continuity of road trajectories.\nGNN-based Road Structure Learning. We first capture the topological attributes of the road network using a graph neural network (GNN) at the global level. In particular, we concatenate multiple attributes\u00b2 as features of road segments, including maximum speed limit, average travel time, road direction, out-degree, in-degree, road length, and road type (eight class for one-hot encoding, {living street, motorway, primary, residential, secondary, tertiary, trunk, unclassified}), which is denoted as \\(F \\in R^{|V|\\times 14}\\). We learn the road embeddings with topological structure as follows:\n\\begin{equation}\n\\begin{array}{l}\nX_{1}=\\operatorname{Linear}(F), \\\\\nX_{2}=\\operatorname{GAT}(G, X_{1}),\n\\end{array}\n\\end{equation}\nwhere \\(X_1, X_2 \\in R^{|V|\\times h}\\), and GAT is the graph attention network [35]. Like the grid trajectory, we can transform a road trajectory \\(T^r = (\\tau_{1}^{r}, \\tau_{2}^{r}, ..., \\tau_{|T^{r}|}^{r})\\) into a road embedding sequence \\(E^{r} = (e_{1}^{r}, e_{2}^{r}, ...e_{|T^{r}|}^{r})\\) by looking up road embeddings in \\(X_2\\) using road IDs.\nContinuity-enhanced Road Representation. We observe that some road segments are aligned many GPS points with many timestamps or are generated completely by the map matching with no corresponding GPS points, which makes it difficult to assign a precisely timestamp to them (The statistical results show that more than 20% of the road segments in the dataset have multiple GPS points or are completed by map matching). To solve this problem, we use a coarse-grained time encoding scheme. For roads with many GPS points or without GPS points, the number of minutes can be estimated accurately based on the context. Therefore, we use two learnable matrices for time encoding on road segments, \\(E^{day} \\in R^{1440\\times h}\\) to represent the minute of the day and \\(E^{week} \\in R^{7\\times h}\\) to represent the day of the week. This approach solves the road temporal misalignment problem and also introduces the second modal information into the temporal encoding. Thus, the input embedding to the Transformer of a road trajectory \\(T^r\\) is as follows:\n\\begin{equation}\n\\begin{aligned}\nh_{i} &= e_{i}^{r} + e^{d a y} + e^{w e e k} + p_{i}, \\\\\nH^{r} &= [h_{1}, h_{2}, ..., h_{|T^{r}|}] \\in R^{|T^{r}|\\times h},\n\\end{aligned}\n\\end{equation}\nwhere \\(e^{day} \\in E^{day}\\) and \\(e^{week} \\in E^{week}\\) are time embeddings corresponding to \\(t_i\\) in \\(\\tau \\in T^{r}\\), and \\(p_i\\) is the position encoding.\nHowever, the vanilla Transformer cannot capture the local continuity of road trajectories. When vehicles are traveling in a road network, the types of adjacent road segments are usually the same\u00b3, as shown in Figure 1. To model such continuity, we explicitly inject road type information to dynamically adjust the self-attention coefficient. We first use a learnable road type matrix \\(E^{type} \\in R^{|type|\\times h}\\) to transform the road type sequence \\(O = (o_1, o_2, ..., o_{|T^{r}|})\\) of \\(T^r\\) into a road type embedding \\(E^{o} = (e_{1}^{o}, e_{2}^{o}, ...e_{|T^{r}|}^{o})\\), and then we incorporate road type information into the Transformer as follows:\n\\begin{equation}\n\\begin{array}{l}\nH^{o}=\\left[h_{1}^{o}, h_{2}^{o}, ..., h_{|T^{r}|}^{o}\\right] \\in R^{|T *| \\times h}, h_{i}^{o}=e_{i}^{o}+p_{i}, \\\\\nQ^{o}=H^{o} W_{q}^{o}, K^{o}=H^{o} W_{k}^{o}, \\\\\nA^{o}=\\frac{Q^{o} K^{o T}}{\\sqrt{h}}, \\\\\nQ^{r}=H^{r} W_{q}^{r}, K^{r}=H^{r} W_{k}^{r}, V^{r}=H^{r} W_{v}^{r}, \\\\\nA^{r}=\\frac{Q^{r} K^{r T}}{\\sqrt{h}}, \\\\\nZ^{r}=F F N(\\operatorname{Softmax}(A^{r}+A^{o}) V^{r}),\n\\end{array}\n\\end{equation}\nwhere \\(p_i\\) is the position encoding, \\(W_{q}^{o}, W_{k}^{o}, W_{q}^{r}, W_{k}^{r}, W_{v}^{r} \\in R^{h \\times h}\\) are learnable matrices, FFN is the feed-forward network corresponding to the vanilla Transformer, and \\(Z^{r} \\in R^{|T^{r}| \\times h}\\). Then, we map \\(Z^r\\) into a new embedding space with dimension d:\n\\begin{equation}\nZ^{r} = Linear(Z^{r}) \\in R^{|T^{r}| \\times d}\n\\end{equation}\nThe final road trajectory representation \\(v^r\\) is obtained by the embedding of the [CLS] token into \\(Z^r\\).\n4.3 Dual-modal Interactor\nThe dual-modal interactor fuses grid and road representations into the final trajectory representation. Grid and road trajectories are two expressions of GPS trajectories in geographic free space and road network space, respectively, each containing unique spatial-temporal information, e.g., region and location information for grid trajectory, road network, and movement continuity for road trajectory. To enable the final trajectory representation to learn from the grid and road expressions of trajectories, we use cross-attention as the dual-modal interactor as follows:\n\\begin{equation}\n\\begin{array}{l}\nQ^{m}=Z^{r} W_{q}^{m}, K^{m}=Z^{g} W_{k}^{m}, V^{m}=Z^{g} W_{v}^{m}, \\\\\nZ^{m}=F F N(\\operatorname{Softmax}(\\frac{Q^{m} K^{m T}}{\\sqrt{d}}) V^{m}),\n\\end{array}\n\\end{equation}\nwhere \\(W_{q}^{m}, W_{k}^{m}, W_{v}^{m} \\in R^{d \\times d}\\) are learnable matrices, and \\(Z^{m} \\in R^{|T^{r}| \\times d}\\). Note that the query matrix \\(Q^m\\) comes from road representation, while the key matrix \\(K^m\\) and value matrix \\(V^m\\) come from grid representations. This is because we use grid trajectories to help reconstruct masked road trajectories in the MLM loss during training, which is covered in Section 4.4. The final trajectory representation is the embedding of the [CLS] token into \\(Z^m\\).\n4.4 Loss Functions\nWe use two self-supervised loss functions for training, i.e., contrastive loss and MLM loss.\nContrastive Loss. The contrastive loss is applied to the grid and road representations such that they are aligned to prepare for the subsequent fusion by the dual-modal interactor. Given a similarity function \\(s(.,.)\\) that measures how similar a grid trajectory representation and a road trajectory representation are, the contrastive loss encourages the grid and road encoders to produce similar representations for the same raw GPS trajectory and dissimilar representations for different raw GPS trajectories. Given a batch of training trajectories, we compute the softmax-normalized grid-to-road trajectory loss as follows:\n\\begin{equation}\n\\begin{array}{l}\n\\mathcal{L}_{c l}^{g} =\\frac{1}{N} \\sum_{i=1}^{N} \\log \\frac{\\exp (s(\\mathcal{T}_{i}^{g}, \\mathcal{T}_{i}^{r}) / \\delta)}{\\sum_{j=1}^{N} \\exp (s(\\mathcal{T}_{i}^{g}, \\mathcal{T}_{j}^{r}) / \\delta)}, \\\\\ns(\\mathcal{T}^{g}, \\mathcal{T}^{r}) =\\frac{v^{g} \\cdot v^{r}}{\\left|v^{g}\\right|\\left|v^{r}\\right|},\n\\end{array}\n\\end{equation}\nwhere N is the batch size and \\(\\delta\\) is a learnable temperature parameter. We compute a road-to-grid trajectory loss \\(\\mathcal{L}_{c l}^{r}\\) in the same way, and the final contrastive loss in a batch is \\(\\mathcal{L}_{c l} = \\frac{\\mathcal{L}_{c l}^{g} + \\mathcal{L}_{c l}^{r}}{2}\\).\nMLM Loss. The MLM loss predicts masked tokens in a sequence and is used widely for self-supervised learning in natural language processing (NLP) [10]. As trajectories are sequences, so we can use the MLM loss. In particular, we apply the mask operation to a road trajectory by randomly removing some road segments. One issue is that if we mask each road segment independently, the model can easily infer masked segments because it needs to connect with its previous and subsequent segments. To avoid this issue, we mask continuous road segments with a mask length l \u2265 2. We use grid trajectories to help reconstruct masked road trajectories, which encourages grid trajectories to provide information that complements masked road trajectories. This also makes our MLM loss different from the MLM loss in NLP and other MLM-based TRL methods [17, 30] in that these methods use the masked sequences themselves for reconstruction while we introduce other sequences (i.e., grid trajectories) to facilitate reconstruction. Thus, MLM loss in our model not only provides the self-supervised information but also contributes to the fusion of two modal representations.\nWe do not use road trajectories to help reconstruct grid trajectories. This is because grids are in a discrete free space and thus are difficult to recover, making such learning difficult. In contrast, road trajectories are continuous in the road network space, and the transitions between road segments resemble the transition between words in sentences. A road segment can pass several grids but a grid trajectory may not contain all of them (due to GPS sampling rate, some grids of these may have no GPS point). E.g., a road trajectory is [R1, R2, R3], R1 passes grids [G1, G2], R2 passes [G3, G4, G5], R3 passes [G6, G7, G8], and the grid trajectory can be [G2, G5, G6, G8]. After masking (denoted as M), the grid trajectory is [G2, M1, M2, G8], and it's difficult to predict missing grids (e.g., candidates for M\u2081 are G3, G4, G5 of R2, and G6, G7, G8 of R3). Conversely, when reconstructing masked road trajectory [R1, M, R3], it's easy to find that M is R2 because R2 connects with adjacent roads, and G5 of this grid trajectory is on R2. The experimental results of Figure 3 show that the accuracy of mask recovery of masked road trajectory reconstruction (blue line) helped by grid trajectories is much higher than that of masked grid trajectory reconstruction (red line) helped by road trajectories.\nTo compute MLM loss, we first use a linear layer to transform the final trajectory embedding fused by the dual-modal interactor into the predicted values for the masked road segments:\n\\begin{equation}\n\\hat{Z}^{m} = Linear(Z^{m}) \\in R^{|T^{r}| \\times |V|},\n\\end{equation}\nThen, we use cross-entropy to compute the MLM loss for \\(T^r\\):\n\\begin{equation}\n\\mathcal{L}_{m l m}^{i} = -\\frac{1}{\\left|T^{r}\\right|} \\sum_{\\tau \\in \\mathcal{T} r} \\log \\frac{\\exp (\\hat{z} m)}{\\sum_{j=1}^{|V|} \\exp (\\hat{z} j)}\n\\end{equation}\nWe average the loss over the N trajectories in a mini-batch to obtain the MLM loss \\(\\mathcal{L}_{m l m}\\). The overall loss in a batch is computed as:\n\\begin{equation}\n\\mathcal{L} = \\mathcal{L}_{c l} + \\mathcal{L}_{m l m}\n\\end{equation}\n4.5 Training and Inference\nThe contrastive loss and MLM loss act on different modules in GREEN, and thus GREEN has different processing flows during training and inference.\nTraining. Since the mask operation makes road trajectories incomplete, masked trajectories cannot be used to effectively compute the contrastive loss. To address this issue, we use a two forward operation for the road encoder during training. In particular, we first feed grid trajectories and the complete road trajectories to the grid encoder and the road encoder in the first-forward process to compute the contrastive loss. Then, we feed masked road trajectories to the road encoder for the second-forward process and feed the grid embeddings and masked road embeddings to the dual-modal interactor to compute the MLM loss. Overall, GREEN executes the grid encoder once and the road encoder twice.\nInference. We feed grid trajectories and the complete road trajectories as the inputs to the encoders, and the outputs from the encoders are used as the inputs to the dual-modal interactor to obtain the final trajectory representation. During inference, GREEN executes the grid encoder and the road encoder once.\nWe show that GREEN is comparable to state-of-the-art TRL methods in terms of training and inference efficiency in Section 5.3."}, {"title": "5 Experimental Evaluation", "content": "We experiment extensively to answer the following questions:\n\u2022 RQ1: How does GREEN's accuracy compare with state-of-the-art TRL methods for various downstream tasks?\n\u2022 RQ2: How do GREEN's designs contribute to model accuracy?\n\u2022 RQ3: How effective is GREEN's pre-trained model compared with training a model from scratch for each downstream task?\n\u2022 RQ4: How efficient is GREEN at training and inference?\n\u2022 RQ5: How effective is GREEN on transferability?\n5.1 Experiment Settings\nDatasets. We experiment on two real-world trajectory datasets", "17": ".", "3": "from OpenStreetMap. The proportions of training", "0.2": "for both datasets. The statistics of the datasets are shown in Table 1. Road network data includes road IDs", "vehicles": 1}]}