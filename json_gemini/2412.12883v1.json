{"title": "A Comparative Study of Pruning Methods in Transformer-based Time Series Forecasting", "authors": ["Nicholas Kiefer", "Arvid Weyrauch", "Muhammed \u00d6z", "Achim Streit", "Markus G\u00f6tz", "Charlotte Debus"], "abstract": "The current landscape in time-series forecasting is dominated by Transformer-based models. Their high parameter count and corresponding demand in computational resources pose a challenge to real-world deployment, especially for commercial and scientific applications with low-power embedded devices. Pruning is an established approach to reduce neural network parameter count and save compute. However, the implications and benefits of pruning Transformer-based models for time series forecasting are largely unknown. To close this gap, we provide a comparative benchmark study by evaluating unstructured and structured pruning on various state-of-the-art multivariate time series models. We study the effects of these pruning strategies on model predictive performance and computational aspects like model size, operations, and inference time. Our results show that certain models can be pruned even up to high sparsity levels, outperforming their dense counterpart. However, fine-tuning pruned models is necessary. Furthermore, we demonstrate that even with corresponding hardware and software support, structured pruning is unable to provide significant time savings.", "sections": [{"title": "1 INTRODUCTION", "content": "Time series analysis and forecasting is a central task in many commercial and scientific applications, such as meteorology, particle physics, life sciences, and energy research. Examples include the forecasting of electricity generation and consumption [38, 44], weather phenomena [29] or technical components in particle detectors [8]. While deep learning techniques have an established position next to classical statistical methods for quite some time, the introduction of the Transformer architecture [39] for natural language processing (NLP) and its adaptation to time series data have truly revolutionized the field [27, 45, 48, 49]. Transformers are, however, known to be computationally demanding, both in memory and training time, due to their large parameter count, the quadratic complexity with respect to the sequence length, and their heavy use of dense layers with large hidden dimensions [11]. Indeed, experiments on scaling laws have demonstrated that linear increases in predictive capabilities of Transformer models require exponential increases in trainable weights [20], which results in a growing need for computational resources.\nSimilarly, Transformer-based models for time series forecasting problems are growing with respect to their parameter counts, as illustrated in Table 1. Additionally, these models quickly suffer from an overfitting problem, due to an imbalance between parameter count and training dataset size [19, 46].\nThis high demand in memory and computation time of Transformers stands in sharp contrast to their prospective deployment on low-resource devices, as required in real-world applications [4]. Meanwhile pruning, i.e., removal of neural network weights estimated to be unnecessary for the predictive performance, has become an established method to reduce computational demand, in particular for model deployment [28]. Pruning is based on the observation that the majority of weights in a neural network are close to zero after training, and thus, do not contribute to the output [23]. Consequently, these weights can be removed from the neural network weight matrices, thereby reducing the number of numerical operations performed in every pass of the model. The corresponding networks are termed sparse.\nSince the inception of sparse neural networks, research interest in network pruning has spiked, and numerous methods and algorithms have been proposed in the last five years [18]. While the natural occurrence of sparsity and the phenomenon of an a-priori better sparse networks, so-called lottery tickets [12], has only been observed in selected models, it is generally assumed that these findings translate across network architectures and learning tasks [25].\nPruning of Transformer-based models for time series forecasting is a promising approach to reduce computational resource demand in real-world application, but has not been studied to date to the best of the authors' knowledge. In this work, we aim to close this gap by providing a corresponding benchmark study. We empirically study the behavior of previously proposed time"}, {"title": "2 RELATED WORK", "content": "Transformer-based Time Series Forecasting. Multivariate time series (MTS) forecasting is the task of forecasting the next $T_{pred}$ time steps of a given times series x with length $T_{inp}$, where each time step has a real-valued feature vectors with dimension D. These D univariate series are assumed to be dependent and relevant to the task. Models for MTS forecasting have to tackle multiple problems: dependencies and patterns can emerge inside a single dimensional long time series, and so the models have to be adequate to handle long sequences without forgetting; events may be correlated across time and across features.\nDeep learning-based MTS forecasting has received increasing attention since the introduction of the Transformer [39]. Originally introduced for NLP, the ability to process and forecast sequential data makes it a natural fit for temporally resolved data. However, while the capability of the self-attention-mechanism to relate individual sequence elements to one another provides impressive predictive skills, the $O(L_{inp}L_{out})$ complexity in sequence length L poses computational challenges, especially for longer time series. Hence, researchers have dedicated special attention to reducing this computational complexity with respect to memory and computational load [11].\nOne of the first Transformer-based models specifically designed for time series forecasting was the Informer model [48]. It introduced the ProbSparse attention mechanism, which attends to the most important of a random subset of past time series tokens, thereby achieving a theoretical complexity of O(nlog n). The Autoformer [45] achieves the same complexity as the Informer by introducing a decomposition architecture with an auto-correlation mechanism. This feature specifically addresses the periodicity of time series data. The FEDformer model [49] is a frequency-enhanced Transformer with a complexity of O(n). It expands upon the frequency spectrum idea, performing attention in the Fourier domain. While the above mentioned models are designed for multivariate time series, the Crossformer [47] was proposed specifically to capture cross-dimension dependencies with an $O(DS^2)$ memory complexity. The complexity is a function of data dimensions D and the segment size S, instead of the whole sequence length L. The sequence is split up in a pre-defined number of segments. Besides architectural advances, other approaches have focused on improving tasks like classification or anomaly detection in time series [42].\nPruning. Neural network pruning has a long-standing history ever since its inception. For a general survey with a broad overview about pruning techniques and taxonomy, see [5].\nPruning has various purposes, such as reducing the model size and thereby computational demands, increasing inference speed or as a form of regularization. Existing techniques can be largely categorized into unstructured and structured pruning. Unstructured pruning of parameters is the method of zeroing out entries that are estimated to be unimportant to the networks performance."}, {"title": "3 EXPERIMENTAL SETUP", "content": "In the following, we provide an in-depth experimental benchmark on pruning Transformer-based models for multivariate time series forecasting. We train several state-of-the-art models on a number of commonly used datasets. These trained models are then pruned to increasing levels of sparsity, using unstructured magnitude pruning as well as structured pruning with DepGraph [10]. The pruned models are evaluated with respect to their density, predictive performance and, in case of structured pruning, computational speedup and FLOPs. We further fine-tune the best pruned model in each category, to make-up for any loss in predictive performance originating from pruning, and evaluate the corresponding models as well."}, {"title": "3.1 Models and Training", "content": "For our benchmark study, we use five state-of-the art Transformer-based models for MTS forecasting: the vanilla Transformer [39], Informer [48], Autoformer [45], FEDformer [49] and Crossformer [47]. We train these models on different datasets, described in section 3.2, with forecasting horizons of $T_{pred}$ = 96, 192, 336, 720. Hyperparameters for all model trainings are set following the publications of FEDformer and Crossformer: all models are trained on all datasets and forecast lengths using the"}, {"title": "3.2 Datasets", "content": "We perform experiments on well-known time series datasets [9, 26, 43] including the following: ETT, ECL, Exchange, Traffic, and Weather, made available by Autoformer [45] and Informer [48]. ETT includes four temperature datasets collected from two electricity transformers, with two different time resolutions, consequently named ETTm1, ETTm2, ETTh1, ETTh2. Due to brevity we will show results for ETTm1 only. ECL contains the electricity consumption of 321 different costumers from 2012 to 2014. The Exchange dataset is a record of the daily exchange rates of eight countries from 1990 to 2016. Traffic is a dataset comprising the occupation rate of freeways in the San Francisco Bay area. The Weather dataset is the set of 21 meteorological indicators recorded by stations in Germany, covering one year. For experiments on training and pruning models on large dataset sizes (c.f. section 3.7), we compile a dataset from the Transparency Platform [32], operated by the European Network of Transmission System Operators for Electricity (ENTSO-E). We will name this the ENTSO-E dataset. It contains hourly electricity grid data of 56 zones in Europe, like load, generation power flow, etc. starting from 2015.\nDetails on the number of features and timesteps (samples) are listed in Table 2. Datasets are each split into training, validation and testing set by a ratio of 7:1:2."}, {"title": "3.3 Pruning", "content": "The unstructured weight pruning sets the smallest parameters to zero, resulting in a sparsity s, a fractional number denoting the number of zeros in relation to full parameter count, defined as\n$S = \\frac{\\text{number of zeros}}{\\text{number of trainable parameters}}$\n. The parameter density d is then defined as the complement of the sparsity, d = 1 \u2212 s. We define ten target sparsity levels, spread uniformly in log-space\n$s = 1 - 0.8^i, i = 0, ..., 10$\n. A fixed pruning ratio allows us to compare how models fare with an equal ratio of their original parameters. However, this does not imply that all models have an equal parameter budget.\nCurrent frameworks will only mask pruned elements and do not support unstructured sparsity any further, for example by employing specialized kernels for sparse matrix-vector-multiplication (SpMV) [13, 36]. This mask is calculated with the PyTorch library [35]. It has the same shape as all weight matrices, and its binary elements indicate pruned weights. For inference, this mask can be applied to the original matrix, setting corresponding elements to zero.\nOn the other hand, structured weight pruning is able to yield performance gains by removing rows of the weight matrix, which reduces the computational load in the matrix-vector-multiplication. For structured weight pruning, we utilize the pruner of the torch-pruning framework [10]. A group of parameters is defined as in DepGraph [10]. We set the pruner to the same pruning ratios as for unstructured pruning. Note that in this case, a set pruning ratio is not necessarily achieved due to dependencies in the graph."}, {"title": "3.4 Fine-tuning", "content": "To make up for any losses in predictive accuracy incurred during the pruning phase, it is common to add a second training phase after pruning, following the scheme train-prune-fine-tune. For the cost of additional compute resources, one can attain a higher sparsity rate with equal predictive performance. We performed fine-tuning on each of the pruned models trained on the ETTm1 dataset with a forecast length of 192, as this is representative of a medium-length forecast. We load the pruned model and continue training with the same setup as in the training phase, either for ten epochs or early stopping with a patience of three. In the unstructured pruning setup, we retain the sparsity level by applying the corresponding mask in every update step."}, {"title": "3.5 Evaluation Metrics", "content": "After pruning, the models are evaluated on the hold-out test data and compared to their unpruned version with respect to the following metrics.\nFor quantifying predictive performance of the models, the MSE training loss function is equally used for evaluation on the validation and testing dataset. It is defined as\n$\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2$\nwith $y_i$ the ground truth and $\\hat{y_i}$ the output of the model.\nFurther, the count of a models parameters is the sum of all trainable weight matrices. Runtime measurements are done by measuring the time between CUDA events before and after an example batch of data is put through the model 500 times, with 50 warm-up steps.\nIn case of structured pruning, inference time speedup is compared to the unpruned model. We compute speedup by dividing two measurements of a model, unpruned and pruned. The"}, {"title": "3.6 Reducing model size to rule-out overfitting", "content": "When training models in the previous experiments, we observe very fast model convergence, hinting that the models in their originally published version are too large, and thus run into overfitting. Overfitting models on small datasets with little to no regularization is a valid concern, especially when smaller models could yield the same performance, at a lower computational cost. To investigate the effects of pruning in smaller model sizes and rule-out the effect of overfitting and loss of predictive performance, we additionally train model instances with linear embedding layer sizes scaled down by a factor of 10 on the ETTm2 dataset (corresponding to our highest pruning rate), thereby significantly reducing the number of trainable parameters (see table 5). Apart from the model size, we keep an equal training setting compared to the big models."}, {"title": "3.7 Increasing dataset size to rule-out overfitting", "content": "Overfitting can be induced either if the model is too large or if the dataset is too small. Hence, orthogonal to the previous experiment, we examine the performance of all models on a significantly larger dataset in terms of number of samples, features, diversity and general periodicity. For this, we utilize the ENTSO-E dataset. We train and evaluate the models on a representative slice of the load subset, zeroing out missing values. We perform 1.6 million gradient update steps, equivalent to two epochs. The models are then pruned through unstructured pruning as described above and evaluated on the test dataset. For comparison, appropriate smaller model sizes are chosen, trained and tested. Their size is determined by the pruning results, choosing the minimal size where performance is not impacted by parameter loss."}, {"title": "4 RESULTS", "content": "In this section we present the results of our experiments, starting with the unstructured pruning experiments. This is followed by the structured pruning results, the measured inference speedup from it, and fine-tuning results for unstructured pruning. We then present results on reducing model size and increasing dataset size."}, {"title": "4.1 Unstructured weight magnitude pruning", "content": "Figure 1 shows the results of unstructured weight magnitude pruning. We report the average loss on the test dataset given the models pruned to predetermined sparsities. Missing datapoints in the plot are induced by models either diverging in training and producing exploding gradients, or out-of-memory errors during training, especially on the longest forecasting horizon. We observe the FEDformer to be the most unstable. As assumed, all models perform worse with a vanishing number of parameters, as they lose predictive performance, indicated by the higher loss on the test dataset. The strength of this effect, however, varies between models, datasets and, on a lesser note, forecast length.\nWe observe that nearly all models can be pruned to around 50% sparsity without any significant performance loss. The pruned versions of the Informer retain the least performance through pruning; in many cases they regain a lower loss through further pruning, on ETTm1, ECL, Traffic and Weather. This is likely due to regularization effects, which are lost again at high sparsities. The Autoformer achieves the lowest loss throughout, i.e., the best predictive skill, together with the FEDformer. Even pruning the Auto- and FEDformer models to 1% of their original parameter"}, {"title": "4.2 Structured node pruning", "content": "We present the results of pruning the models with the DepGraph pruner from torch-pruning in Figure 2. We observe, that while the vanilla Transformer and the Informer can be pruned to 1% of their original parameter count, the pruning algorithm fails to prune Autoformer and FEDformer to"}, {"title": "4.3 Inference time speedup", "content": "Models trained on the ETTm1 dataset, for a forecast length of 192, and structurally pruned to a density of d = 0.85 \u2248 0.33 are evaluated for their actual density, speedup compared to the dense counterpart and MSE in Table 3. We do not find a significant speedup except for the Informer model, which is also the most pruned model with a remaining density of 11%. Contrarily, The FEDformer is slower than unpruned, even though it is pruned to a density of 74%, indicating that the CUDA kernels can not take advantage of the smaller weight matrices.\nCompared to the unpruned version, every model has a reduced FLOP count, up to seven times (for the Informer) smaller. Both the FEDformer and Autoformer have the least reduction with a factor of < 2."}, {"title": "4.4 Fine-tune after pruning", "content": "The models trained on the ETTm1 dataset, for a forecast length of 192, and weight magnitude pruned to a density of 33% are trained again. The resulting predictive performance on the testing dataset is presented in table Table 4. The vanilla Transformer has better performance on the test set after pruning, which it loses again with fine-tuning. This discrepancy can likely be explained through overfitting on the training dataset and a regularizing effect of pruning. The Informer and Autoformer models have an approximately 20% higher loss, which is not restored through fine-tuning. The FEDformer model has a lower loss after pruning and after fine-tuning. The Crossformer loses much of its predictive power with pruning, but is able to recover most of it through fine-tuning, to within 1% of its original loss, making it the only model where fine-tuning seems necessary.\nExcept for the Transformer, all models that were specifically proposed for time series forecasting have a greater or equal performance after fine-tuning."}, {"title": "4.5 Reducing model size to rule-out overfitting", "content": "Cutting down on initial parameter size, models with reduced linear embedding layer size are trained on ETTm2, prediction length 192, to compare with their large counterparts. With a ten-fold reduction in total parameter count, all smaller models are able to deliver a lower loss score on the test dataset than their large counterparts, with precise results shown in table Table 5. A notable exception is the Crossformer, which additionally experiences no change in time per epoch. We find in our experiments that the larger models yield lower training losses than the smaller ones, indicating overfitting on the training dataset, directly impacting performance on the test dataset"}, {"title": "4.6 Increasing dataset size to rule-out overfitting", "content": "Training all models on the ENTSO-E dataset produces the averaged training loss curve shown on the left in figure 4. While not fully converged in training, the models work adequately, with a sample prediction shown on the right in figure 4. From pruning and testing on the ENTSO-E test dataset we compile the relationship between test error and density in figure 3. We observe that for all models, except Crossformer, a pruning to at least 50% without significant performance loss is possible. Around this mark, test error rises steeply with decreasing density.\nSimilar to the previous experiment, we compare against smaller models. The reduced model size is defined by the largest pruning ratio without performance loss, based on results in figure 3 we choose a reduction of 50%. These smaller models are trained and tested on the ENTSO-E dataset, their results are shown in table 5. In all cases, the models with a higher parameter count have a lower test error, indicating that the surplus of parameters is needed to accurately learn and generalize from the training data. This higher parameter count necessarily leads to an increase in training time. Table 5 demonstrates the necessity of large parameter counts during training"}, {"title": "5 CONCLUSION AND FUTURE WORK", "content": "With Transformer-based models dominating the current landscape in time-series forecasting, the growing computational demand of these models urgently needs to be reined in. This is particularly relevant as demand for their deployment in real-world applications e.g., on low-resource embedded devices grows. Pruning methods provide a viable approach to address this issue. Our study provides valuable insights into potential and pitfalls of pruning Transformer-based model for time series forecasting. We find that while it is possible to prune models with unstructured pruning to a sparsity of at least 50%, only models containing Fourier series decomposition retain sufficient predictive performance up to 90% sparsity. While structured pruning can provide actual savings in terms of operations performed, the complex, interwoven architecture of current state-of-the-art time series forecasting Transformers pose a hindering factor. Our experiments utilizing DepGraph demonstrate this, as the algorithm fails to prune these models as desired.\nWe observe that overall the performance of pruned models is strongly coupled to the dataset at hand. Although dataset size does not play a crucial role, as seen with the small Exchange dataset, the feature dimension size seems to be the relevant factor, e.g. in the Traffic and ECL dataset. While unstructured pruning amounts to a purely academic exercise with theoretical insights due to the lack of hardware and software support, structured pruning with DepGraph can practically reduce overall model size. This is demonstrated by our experiments, where we observe a reduction in model parameters, and consequently, in FLOPs. Unfortunately this does not amount to any notable gains in inference runtime. This result implies that for the selected Transformer-based models most of the compute time is not spent on prunable linear layer calculations, but on the time series specific parts of time series Transformers.\nOne major concern when it comes to Transformer-architectures for time series forecasting is that models are simply over-parameterized for most common time-series forecasting problems, resulting in overfitting on the comparably small datasets. To account for this aspect, we specifically conducted experiments with reduced model sizes compared to the originally published versions. Our results show, that indeed smaller models yield competitive prediction results on small datasets. Likewise, we investigate model performance and pruning potential on larger datasets. Here, we observe that the larger models with more free parameters yield better performance in terms of lower MSE, even after pruning. A large parameter count is therefore necessary during training, but can be pruned afterwards.\nWhen it comes to inference and model deployment, pruning is typically performed in conjunction with kernel compilation for corresponding hardware, to achieve actual speed-up. TensorRT [31] is an open-source library developed by NVIDIA, providing CUDA kernels to speed up deployed models. Existing models and code can be compiled to make use of TensorRT. To take such an inference compilation into account, we applied TensorRT to the investigated pruned models. However, the published implementations failed to compile for all models, because of specialized code that may be too complex to be retracable by the compiler. To test the speedup possibilities of TensorRT for model deployment would require completely new re-implementations of the models. The focus of our work lies on studying the effects of pruning in existing models, rather than optimized custom implementations. Hence, we refrained from this step, given that it would go way beyond the scope of this study and leave it for future work.\nThe insights of our work highlight the complexity and variability in the behavior of pruned models, underscoring the need for further research into time series specific models. Future work should hence explore alternative approaches, like dynamic sparse training and lottery ticket pruning, or other compression and reduction schemes, such as low-rank factorization and quantization. It is of interest how these fare in conjunction with one another. Understanding these dynamics will be"}]}