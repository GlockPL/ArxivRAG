{"title": "Multivariate Time-Series Anomaly Detection based on Enhancing Graph Attention Networks with Topological Analysis", "authors": ["Zhe Liu", "Xiang Huang", "Jingyun Zhang", "Zhifeng Hao", "Li Sun", "Hao Peng"], "abstract": "Unsupervised anomaly detection in time series is essential in industrial applications, as it significantly reduces the need for manual intervention. Multivariate time series pose a complex challenge due to their feature and temporal dimensions. Traditional methods use Graph Neural Networks (GNNs) or Transformers to analyze spatial while RNNs to model temporal dependencies. These methods focus narrowly on one dimension or engage in coarse-grained feature extraction, which can be inadequate for large datasets characterized by intricate relationships and dynamic changes. This paper introduces a novel temporal model built on an enhanced Graph Attention Network (GAT) for multivariate time series anomaly detection called TopoGDN. Our model analyzes both time and feature dimensions from a fine-grained perspective. First, we introduce a multi-scale temporal convolution module to extract detailed temporal features. Additionally, we present an augmented GAT to manage complex inter-feature dependencies, which incorporates graph topology into node features across multiple scales, a versatile, plug-and-play enhancement that significantly boosts the performance of GAT. Our experimental results confirm that our approach surpasses the baseline models on four datasets, demonstrating its potential for widespread application in fields requiring robust anomaly detection.", "sections": [{"title": "1 INTRODUCTION", "content": "Time-series data, characterized by observations sampled at discrete time intervals, exhibits strong temporal correlations and often displays periodic patterns. These data are prevalent in various sectors, including industrial [2, 25], medical, transportation [1], and network environments [31]. In these applications, time-series data may appear as industrial sensor readings, network traffic, traffic flows, or medical records. Analyzing these time-series data includes regression and classification, typically focusing on predicting future values [35] and detecting anomalies [29], respectively. The latter is particularly crucial, as timely anomaly detection enables prompt intervention, essential in scenarios like network security breaches or traffic disruptions due to accidents or special events. For example, Figure 1 shows anomalous data patterns in an industrial scenario.\nWith the advancements in computing power, large language models have gained prominence, setting new standards across numerous tasks [27]. However, these models often struggle to effectively handle sparse features from data with distributional imbalances, such as those with long-tailed distributions characteristic of temporal anomalies. The increasing complexity of time series data, spurred by the proliferation of industrial Internet of Things platforms, adds complexity by introducing dependencies among features in multivariate time series [12, 40]. Additionally, the need for labeled anomaly data complicates the detection of various anomaly patterns in an unsupervised manner. Temporal anomalies can be divided into two categories: global anomalies, which significantly deviate from overall statistical properties, and contextual anomalies [23], which, although within a normal range, show significant deviations from expected patterns, thereby disrupting periodicity. Hence, effectively managing the relationship between local and global [44] anomalies over an extended period presents a significant challenge in temporal tasks."}, {"title": "2 RELATED WORK", "content": "This section briefly reviews earlier anomaly detection methods for multivariate time series data. Then, we introduce two techniques in our approach: graph neural network and topological analysis."}, {"title": "2.1 Multivariate Time Series Anomaly Detection", "content": "Multivariate time series anomaly detection involves classifying anomalies across feature and temporal dimensions. Existing anomaly detection methods are classified into two categories based on how the data is processed in the model: prediction and reconstruction-based. The prediction-based approach generates future data by learning from historical data and comparing the generated and actual data to determine anomalies. GDN [11] first utilizes similarity to model graph structures, employing graph-based techniques to map dependencies among features. However, this approach suffers from inadequate utilization of temporal dependencies. Anomaly Transformer [37] leverages the Transformer to model dependencies between features and employs a variable Gaussian kernel to model temporal dependencies. However, the Transformer struggles to cope with anomaly-sparse scenarios. Our method also adopts a prediction-based approach, separately modeling both dependencies.\nReconstruction-based methods, on the other hand, use generative models such as Variational Auto-Encoder (VAE) [4] and Generative Adversarial Network (GAN) [13] to model data representations. Lin et al. [22] leverages the VAE to create robust local features in short windows and utilizes the LSTM to estimate long-term"}, {"title": "2.2 Graph Neural Network", "content": "In previous studies, Graph Neural Networks (GNNs) are often employed to address graph structure-related tasks, such as node classification [36] and edge prediction [41]. Among these, Graph Convolutional Networks (GCNs) [10], which leverage spectral domain transformations, are at the forefront. GCN integrates convolution operations from traditional Convolutional Neural Networks (CNNs) with the Laplacian graph matrix that facilitates message diffusion. Subsequently, as the understanding of graph structures has evolved beyond mere predefined knowledge, graph structure learning emerges to model similarities between variables. This has led to an expansion in the application areas of graph neural networks, such as multivariate time series analysis. GAT [34] introduces an attention mechanism to define relationships between nodes, assigning learnable coefficients to each edge. Unlike GCNs, GATs can dynamically adjust these attention coefficients to accommodate changes in the graph structure. Research has explored the connections between GATs and Transformers [38], demonstrating that the attention coefficient matrix computed by GAT tends to be sparser than those in traditional attention maps, an attribute that helps explain the sparsity observed in anomalous data."}, {"title": "2.3 Topological Analysis", "content": "Topological data analysis comprises methods that encapsulate structural information about topological features, including connectivity components, rings, cavities, and more [6]. Using a persistence diagram, the statistical properties of the above topological features can be mapped onto a two-dimensional plane and finally vectorized. Persistence Homology (PH) is a topological analysis method demonstrating that graph filtering functions, which remain invariant under graph isomorphism, can assimilate information about edges and nodes into a near-simplicial complex [19]. Topological features serve as higher-order structural elements to address the over-smoothing issue in GNN [43]. Pho-GAD [39] enhances the distinction between normal and abnormal node behaviors by optimizing edge attributes using persistent homology. Chen et al. [7] leverage the persistent homology of a node's immediate neighborhood to rewire the original graph, incorporating the resultant topological summaries as auxiliary data into local algorithms. Meanwhile, Zhou et al. [42] employ topological analysis to refine the understanding of complex human joint dynamics using the graph's adjacency matrix."}, {"title": "3 METHODOLOGY", "content": "In this section, we provide a detailed overview of the proposed TopoGDN framework. We begin by outlining the problem formulation for multivariate time series anomaly detection, followed by describing our data preprocessing methodology. Next, we introduce and discuss the modules integrated into our approach. Specifically, our method comprises the following four modules:\n(1) Multi-scale Temporal Convolution Module: This module extracts temporal features at multiple scales and incorporates residual connections to enhance the learning from data within the time window.\n(2) Graph Structure Learning Module: It constructs dynamic graph structures based on the distributional similarity among variables, continuously adapting as the window progresses.\n(3) Topological Feature Attention Module: This module aggregates embeddings from individual nodes, their neighboring nodes, and higher-order topological features, fusing inter-feature dependencies at multiple scales.\n(4) Anomaly Score Calculator: This module computes an anomaly score for the predicted time step, aiding in identifying potential outliers."}, {"title": "3.1 Problem Formulation", "content": "Given a dataset $X \\in \\mathbb{R}^{N \\times T}$, where $N$ represents the number of features and $T$ denotes the number of time steps. We distinguish between a training set $X_{train}$, which consists entirely of regular data, and a test set $X_{test}$ that includes both normal and abnormal data. The model aims to predict anomaly scores $s_t$ for each time step $t$ in the test set. These scores are then converted into labels $Y_{test}$ based on the proportion of anomalies present in the dataset."}, {"title": "3.2 Data Preprocessing", "content": "The values across different columns vary significantly in magnitude in the original dataset. For example, in the MSL dataset, most data are binary, with values being either 0 or 1. To mitigate the impact of different scales on the final prediction results, we employ min-max normalization to map all values to the interval [0, 1]. The formula is given by:\n$X' = \\frac{X - min(X^i)}{max(X^i) - min(X^i) + \\epsilon},$ (1)\nwhere $X^i$ is the feature vector with all time steps, and $\\epsilon$ is a small vector added to avoid division by zero in cases where $X^i$ contains identical values. Our model employs a prediction-based anomaly detection approach using a sliding time window to divide the dataset into training and test sets. We define the window as a stride length of $s$ and a size of $w$. This configuration allows the window to be shifted by $s$ units at each step, providing a mechanism for sequential data processing across the dataset. The Slide Window operation is defined such that for any given time step $t$, the window $S = [X_{t-w}, X_{t-w+1}, ..., X_{t-1}]$ is constructed, where $S \\in \\mathbb{R}^{N \\times w}$. The window predicts the next data point $X_t$. The model learns from the data of the previous $w$ steps to predict the data at the next time step $t$, denoted as $\\hat{X}_t$. By comparing $\\hat{X}_t$ with $X_t$, it determines whether the time step $t$ is anomalous. The sliding window continuously moves forward. If the final data segment is shorter than the step length, we use replication padding with the last step's data. The model is solely trained on its predictive capabilities in the training set, while scores are predicted only on the test set."}, {"title": "3.3 Multi-scale Time Convolution", "content": "To address the conflict between local and global features over a long period, we design a multi-scale temporal convolution module to capture fine-grained temporal dependencies. This module features one-dimensional convolution kernels of various sizes, which capture temporal features at different scales in the time series. These convolutions may be dilated to increase their receptive field without increasing the kernel size [44]. Given a time series $S_i \\in R^w$, we employ P different sizes of convolution kernels. Each kernel size $w_p$ contributes to extracting features at a particular scale. The convolution operation for the p-th kernel is defined as follows:\n$y^{(p)}[t] = \\sum_{j=0}^{W_p - 1} S_i[t + j] \\cdot f_p[j]$, (2)\nwhere $y^{(p)}[t]$ represents the output at time t using the p-th convolution kernel, $f_p [i]$ is the weight of the p-th kernel at position i, and $w_p$ is the width of the p-th kernel. After applying the convolution kernels, the module performs a pooling operation across the outputs from all kernels to synthesize and enhance the representation of temporal dynamics. This step typically uses average pooling to merge the diverse features extracted by each kernel into a unified output. The final output $Y^i$ from the pooling layer is then used in subsequent processing:\n$Y^i[t] = pool(y^{(1)}[t], y^{(2)}[t], ..., y^{(p)}[t]),$ (3)\nwhere pool() is the pooling function employed to combine the outputs from each convolution scale into a single vector, effectively capturing temporal features across different scales and maintaining the same dimensionality as the input $S_i$. The convolution kernels used in this implementation include sizes 1 \u00d7 2, 1\u00d73, 1\u00d75, and 1\u00d77, designed to accommodate a broad range of temporal resolutions."}, {"title": "3.4 Graph Structure Learning", "content": "Drawing on insights from GDN, it is crucial to model a graph structure through similarity computations when dealing with multivariate data that lacks inherent structure. In the proposed graph model, we represent each sensor within the dataset as a node. We construct edges between nodes based on the similarity of the data corresponding to the features they represent. Specifically, the probability of an edge between two nodes increases proportionally with the similarity observed in their associated data.\nConsider a learned graph denoted as G = (V, E), where V represents the set of nodes with their embeddings given by $Y \\in \\mathbb{R}^{N \\times w}$, capturing feature representations for each node. Using an adjacency matrix, we depict the relationships between nodes, which encodes their connectivity and interaction details. We assess the"}, {"title": "3.5 Topological Feature Attention Module", "content": "As shown in Figure 2 (b), the Topological Feature Aggregation module consists of two key components: the graph attention mechanism and topological graph pooling. Initially, node aggregation occurs through the graph attention mechanism, facilitating the integration of local and global information across multiple layers. Subsequently, the module extracts high-order topological structures from the graph, converting them into a Persistence barcode. The structural information is then globally vectorized and fused, resulting in the final feature graph.\n3.5.1 Graph Attention Mechanism. We have adopted the Graph Attention Mechanism from GATs, which aggregates information from neighboring nodes by computing attention coefficients for pairs of adjacent nodes. We divide the output of the Multi-Scale Temporal Convolution module Y such that the feature vector corresponding to each sensor becomes node embedding $Y_i \\in \\mathbb{R}^{W}$. Before calculating the attention coefficients $a_{ij}$, the following operations are performed:\n$h_i = W \\cdot Y_i,$ (6)\n$g_i = c_i || h_i,$ (7)\n$s_{ij} = FeedForward([g_i || g_j]).$ (8)\nNotably, each node's embedding is transformed to obtain the hidden embedding $h_i \\in \\mathbb{R}^{d}$. Here, W is a shared parameter matrix incorporating node features. The intermediate vectors $g_i$ and $g_j$, resulting from the fused sensor embeddings, are combined and then transformed into an actual number using a feed-forward neural network. This process serves as a typical feature enhancement technique. This attention mechanism bears a resemblance to the self-attention mechanism found in Transformers [33]. Additionally, optimizing it using a multi-head mechanism allows for more nuanced and"}, {"title": "3.5.2 High-order Topological Graph Pooling", "content": "In algebraic topology, persistence homology tools aim to identify connected components (0-dimensional homology) and cycles (1-dimensional homology) within graph homology. This method has proven effective in optimizing graph representation learning [17, 18].\nWe apply High-order Topological Graph Pooling to the node embeddings $Z \\in \\mathbb{R}^{N \\times d}$ and the graph structure information, which the graph attention mechanism has refined. In this process, N represents the number of vertices, and d the dimensionality of each vertex's hidden embedding. This pooling operation aggregates and abstracts the enhanced feature representations, succinctly capturing the topological characteristics of the graph. Initially, the nodes undergo dimensionality increase using a multi-layer perceptron. After the dimension transformation, we describe the graph from k different perspectives, resulting in k transformed graphs $K = {K_1, K_2, ..., K_k} \\in \\mathbb{R}^{k \\times N \\times d}$. This process allows for the extraction of richer topological features at multiple scales. We employ a set of k filtering functions to perform graph filtering from k distinct viewpoints. Specifically, we apply graph filtering to each transformed graph $K_i$ (for i = 1, ..., k), which processes the information of nodes and edges for each viewpoint. We filter the graph according to this form:\n$K_i^{(n)} = K_i,$ (11)\n$K_i^{(0)} \\subseteq K_i^{(1)} \\subseteq ... \\subseteq K_i^{(n)},$ (12)\n$K_i^{(0)} = (V^{(0)}, E^{(0)}).$\nThe subscript i in the formula represents the graph of the i-th view; the superscript (j) represents the degree of filtration, where the closer j is to 0, the more extensive the filtration. The $z_v$ represents the embedding of the node v, and $f_i () represents the pooling function done on the node embedding in the graph of the i-th viewpoint. Our designed graph filtering method uses threshold filtering, where nodes above the threshold $a_i^{(j)}$ and some of their connecting edge information are removed in this form:\n$V^{(j)} = {v \\in V^{(j+1)} | f_i(z_v) \\leq a_i^{(j)} },$ (13)\n$E^{(j)} = {(u, w) \\in E^{(j+1)} | max{f_i(z_u), f_i(z_w)} = a_i^{(j)} }.$ (14)\nSince Vietoris-Rips complexes better represent higher-order topological features, we apply Vietoris-Rips complexes to construct simple complexes from graphs as higher-order topological features of the graphs, and only 0- and 1-dimensional complexes, i.e., connected components and voids, are considered in our implementation."}, {"title": "3.6 Anomaly Score Calculator", "content": "Suppose the node embeddings of the graph after the Topological Feature Attention Module are denoted as $p(t) \\in \\mathbb{R}^{N \\times d}$. After the dimensional transformation, the predicted time step data $\\hat{X}_t \\in \\mathbb{R}^{N}$ is obtained. This process is formulated as:\n$\\hat{X}_t = FeedForward(p(t)),$ (17)\nwhere a feed-forward neural network is employed to achieve this transformation. This dimensional transformation is equivalent to performing a pooled dimensionality reduction. On the training set, we update the weights of the sensor embedding and other neural networks based on the average mean square error of $X_t$ and $\\hat{X}_t$ at each step as the backward loss without computing anomaly scores. On the test set, we similarly compare the prediction result $\\hat{X}_t$ with the ground truth $X_t$, compute its L1-Loss, and normalize the difference. After normalization, we use the maximum value in the sensor dimension as the anomaly score, calculated as:\n$Anomaly Score = max_{i=1,...,N} (Normalize(|X_t^i - \\hat{X}_t^i|)),$ (18)\nwhere $X_t^i$ is the value taken by the ith sensor at time t. Finally, if the anomaly score exceeds the preset threshold, the time step t is marked as an anomaly. In our implementation, we adopted the method from GDN, setting the threshold as the maximum anomaly score observed on the validation set. The ratio of the training set to the validation set closely matches the proportion of anomalies in the dataset."}, {"title": "4 EXPERIMENTS", "content": "This section elaborates on the experiment and the details of the model implementation."}, {"title": "4.1 Datasets", "content": "We select four of the most commonly used datasets for time series anomaly detection, which are: Mars Science Laboratory rover (MSL) [20], Secure Water Treatment (SWaT) [25], Server Machine Dataset (SMD) [31], Water Distribution (WADI) [2]. Among them, the WADI dataset has the largest volume of data to detect and is consequently the most challenging. The MSL dataset has the smallest feature dimension, making detection less complicated. Table 1 details our four benchmark datasets, including their sizes and Anomaly Rates (%)."}, {"title": "4.2 Baselines", "content": "To assess the performance of TopoGDN, we compare it to seven methods that belong to three categories: traditional machine learning approaches, reconstruction-based methods, and prediction-based methods.\n\u2022 PCA [24]: PCA is a traditional machine learning technique that finds a dimensionality reduction mapping to extract features from data, using reconstruction error as the Anomaly Score.\n\u2022 LSTM-VAE [28]: LSTM-VAE utilizes the VAE for data reconstruction combined with the LSTM to extract temporal features.\n\u2022 MAD-GAN [21]: MAD-GAN employs the GAN for data reconstruction, using the generator to learn implicit features of time series and the discriminator to directly identify genuine and fake sequences, enabling it to detect anomalies.\n\u2022 OmniAnomaly [31]: OmniAnomaly utilizes stochastic variables linkage and regularization flow techniques from probabilistic graphical models to capture the typical patterns of data, determining anomalies by reconstructing probability distributions.\n\u2022 GDN [11]: GDN uses graph structure learning to model dependencies among features and employs the GAT to extract temporal features, representing a typical prediction-based method for anomaly detection.\n\u2022 TranAD [32]: TranAD utilizes a conventional Transformer as a sequence encoder, designing an adversarial-based training process to amplify reconstruction errors and employing model-independent meta-learning to accelerate inference.\n\u2022 ImDiffusion [9]: ImDiffusion utilizes an interpolation-based approach for data reconstruction, leveraging information about"}, {"title": "4.3 Experimental Setup", "content": "We conduct all experiments on an NVIDIA GeForce 3090 with 24GB of memory. The programming language and framework used for the implementation are Python 3.7, PyTorch 1.13.1, CUDA 11.7, and PyTorch-Geometric 1.7.1. Default generic hyperparameters are presented in Table 2. When computing topological features, we adopt graph batch processing to merge multiple batches of graph data into one large graph. This approach solves the problem of constructed graph structures that need to be sparse due to fewer variables in the dataset. Consequently, batch size becomes a hyperparameter that balances efficiency and accuracy. The calculation of topological features employs a multi-view approach, utilizing four different embedding functions for the Persistence Barcode. Each function is applied three times, corresponding to the number of graph viewpoints."}, {"title": "5 EXPERIMENTAL RESULTS", "content": "In this section, we analyze the experimental results and present the visualization of graphs and their topologies."}, {"title": "5.1 Results Analysis", "content": "We compare traditional and state-of-the-art models from the past five years. The experiment results are shown in Table 4. TopoGDN optimally balances accuracy and recall, achieving the highest F1-Score on all four datasets. While some methods outperform TopoGDN in terms of precision or recall for specific datasets, the overall performance of TopoGDN is impressive. TopoGDN outperforms the second-ranked model by about 2-3% in the F1-Score on the SWaT and SMD datasets. This performance showcases its robustness and effectiveness in handling complex data scenarios. The WADI dataset, characterized by a significant period and double to triple the number of multivariate features compared to other datasets, shows that all models' effects are less pronounced. ImDiffusion runs on the WADI dataset for over 8 hours without producing results due to the long runtime. Nevertheless, TopoGDN outperforms all baseline models in accuracy and F1-Score, confirming that our model is suitable for large-scale datasets with some generalization ability.\nThe efficiency comparative analysis shown in Table 3 focuses on two models, TranAD and ImDiffusion, which rank just behind"}, {"title": "5.2 Ablation Study", "content": "Our model refers to the overall framework of the GDN, exhibiting significant improvement over the original model across all four datasets shown in Table 5. Specifically, integrating all modules improves the average F1-Score on all datasets by 7% to 19%, respectively. TopoGDN integrates a Multi-Scale Temporal Convolution (MSTCN) and a Topological Analysis module (TA) into the standard GDN framework. It also performs the Softmax normalization before the element-wise multiplication in the output layer. We also compare the effects of single-scale Temporal Convolution (TCN) and Multi-Scale Temporal Convolution (MSTCN). Adding TCN and MSTCN modules improves the model's overall effectiveness, with an average F1-Score improvement of about 6%, which can reach 12% on the SMD dataset. This indicates that the SMD dataset exhibits"}, {"title": "5.3 Hyperparameter Sensitivity", "content": "We conduct experiments to evaluate the effects of different hyperparameters, as illustrated in Figure 3. The batch size, depicted in Figure 3a, is crucial since we use one large fused graph instead of multiple small graphs for batch processing. For larger datasets, a smaller batch size, typically around 16, ensures a moderately dense graph structure within the batch. If the batch size is too large, the topological features after graph filtering become less distinct. The Top-K parameter, shown in Figure 3b, representing the number of edges per node, also impacts topological feature extraction. Connecting too many edges makes it difficult to extract distinct topological features and values of 15 or 20, which are commonly used. Excessive graph filtering operations can destroy structural"}, {"title": "5.4 Topological Feature Visualization", "content": "The topological analysis module extracts higher-order features from the learned graph structure information. Figure 4 illustrates the graph structure obtained from training on the SMD dataset with different Top-K values. The results show that when Top-K = 5, the graph tends to be sparser. The nodes of the graph form clusters of multiple types along the training, corresponding to the two data types in the SMD dataset: binary data with values 0 and 1, and flow data, which shows different data distributions in [0,1] after normalization.\nFigure 5 displays a visualization of the Persistence Barcode computed for the graph structures obtained from different training processes at Top-K = 5. Each horizontal line (a bar) represents a topological feature (e.g., a connectivity component or a ring) across a specific range of scales. The beginning of the bar indicates the"}, {"title": "6 CONCLUSION", "content": "We propose a prediction-based multivariate time series anomaly detection method called TopoGDN. This method extracts temporal features using multi-scale temporal convolution with kernels of different sizes. Concurrently, the model captures cross-feature dependencies through the graph structure learning and GAT. Furthermore, the method incorporates a plug-and-play topological analysis module to integrate higher-order structural information at multiple scales. This process significantly enhances GAT's feature extraction capabilities. Experimental results demonstrate that our method outperforms other baseline models on four datasets."}]}