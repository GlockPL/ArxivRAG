{"title": "Advanced Persistent Threats (APT) Attribution Using Deep Reinforcement Learning", "authors": ["ANIMESH SINGH BASNET", "MOHAMED CHAHINE GHANEM", "DIPO DUNSIN", "WIKTOR SOWINSKI-MYDLARZ"], "abstract": "This paper investigates the application of Deep Reinforcement Learning (DRL) for attributing malware to specific Advanced Persistent Threat (APT) groups through detailed behavioural analysis. By analysing over 3,500 malware samples from 12 distinct APT groups, the study utilises sophisticated tools like Cuckoo Sandbox to extract behavioural data, providing a deep insight into the operational patterns of malware. The research demonstrates that the DRL model significantly outperforms traditional machine learning approaches such as SGD, SVC, KNN, MLP, and Decision Tree Classifiers, achieving an impressive test accuracy of 89.27%. It highlights the model's capability to adeptly manage complex, variable, and elusive malware attributes. Furthermore, the paper discusses the considerable computational resources and extensive data dependencies required for deploying these advanced AI models in cybersecurity frameworks. Future research is directed towards enhancing the efficiency of DRL models, expanding the diversity of the datasets, addressing ethical concerns, and leveraging Large Language Models (LLMs) to refine reward mechanisms and optimise the DRL framework. By showcasing the transformative potential of DRL in malware attribution, this research advocates for a responsible and balanced approach to Al integration, with the goal of advancing cybersecurity through more adaptable, accurate, and robust systems.", "sections": [{"title": "1 Introduction", "content": "In recent years, cyber attacks have evolved from isolated incidents into sophisticated operations conducted by well-resourced Advanced Persistent Threats (APTs), which are characterised by their strategic, long-term approaches compared to more opportunistic cyberattacks [17]. Like traditional cyberattacks, APTs utilise malware as their primary tool, but they stand out due to their complexity, higher number of network events, and intricate behavioural activities [15]. APTs are meticulously orchestrated, employing advanced techniques to remain hidden while they extract data, disrupt operations, or create entry points for future attacks [38]. Often backed by nation-states or large organisations with political or economic motives, APTs pose significant threats to critical resources [17]. A notable example is the Stuxnet worm, which, although discovered in 2010, had been operating covertly since at least 2005, specifically targeting Iran's nuclear facilities at the Natanz uranium enrichment plant [4]. Developed by the USA, Stuxnet utilised advanced evasion techniques like zero-day exploits and rootkits to infiltrate and compromise its target while remaining undetected for years [4].\nAccording to Statista, the global revenue from the APT protection market is worked to reach $12.5 billion by 2025, driven by the urgent need to defend against these evolving threats [18]. Despite significant investments in security solutions, APT incidents, including ransomware attacks, continue to rise across industries, military sectors, and government institutions, with a 55.5% increase in ransomware cases in 2023 alone, reaching 4,368 incidents worldwide [26, 31]. The use of advanced technologies like large language models (LLMs) has further intensified the threat landscape, enabling more sophisticated cyberattacks [14]. This escalation underscores the critical need for innovative defence strategies, encouraging organisations and governments to continuously invest in advanced security measures to stay ahead of these persistent adversaries [5].\nThe increasing sophistication and frequency of APTs highlight the critical challenge of precise attribution in the cybersecurity landscape [15]. Accurate attribution is essential for developing targeted defensive strategies, as understanding an adversary's tactics, techniques, and procedures (TTPs) allows for tailored responses to specific threats [29]. It also plays a key role in holding perpetrators accountable, which can act as a deterrent through legal and diplomatic consequences, thereby maintaining global cyber stability [24]. However, attribution is complicated by the obfuscation methods used by APTs, including routing attacks through proxies and deceptive indicators [33]. These sophisticated tactics require extensive technical expertise and collaboration across sectors to analyse threat profiles that reveal attackers' motives and strategies [33]. Building on this complexity, each APT group possesses a distinct signature, merging specific malware applications with strategic objectives, whether for financial gain or disrupting critical infrastructure [18]. The intricacies of these profiles underscore the importance of attribution, pinpointing the perpetrator not only aids in defence but also in shaping cybersecurity policies and measures to pre-empt future attacks [9].\nTo address the growing challenge of attributing APTs, this report suggests leveraging machine learning algorithms that focus on analysing malware behaviour within sandbox environments. Machine learning's ability to process vast datasets and detect subtle patterns offers a promising solution to understanding the complex and often hidden techniques used by APT groups [29]. By training models on behavioural data obtained from executing malware in virtual systems, these systems can be developed to automatically detect and classify patterns, leading to more precise attribution of cyberattacks. Within this context, Deep Reinforcement Learning (DRL) emerges as particularly effective for attributing APT malware. DRL combines deep learning's pattern recognition capabilities with reinforcement learning's adaptive decision-making through trial and error, enabling it to detect and enhance its response to evolving malware behaviours [25]. Techniques like the Markov Decision Process (MDP) and model-free learning allow DRL to structure decision-making and adapt without relying on predefined models. Unlike traditional machine learning models that may struggle with the dynamic nature of cyber threats, DRL continuously learns and refines its strategies, making it highly effective against sophisticated APT tactics. Its ability to operate in environments with incomplete information, simulate diverse attack scenarios, and evolve through interaction underscores its potential as a powerful tool in crafting robust cyber defence strategies [27]."}, {"title": "2 Related Work", "content": "The adoption of Deep Reinforcement Learning (DRL) for malware detection is a relatively recent and promising development in the wider field of cybersecurity [3]. Grasping the related work in this area requires an initial understanding of the behavioural patterns, origins, and classifications of malware. Since malware plays a crucial role in APT attacks, analysing the characteristics of these malware attacks can reveal essential attributes of the attacking APT group [42]. The chosen papers examine current research and methodologies in malware behavioural analysis, attribution, and family classification, critically assessing the effect of these elements to enhance the precision and efficiency of cybersecurity defences [12]."}, {"title": "2.1 Behavioural Analysis", "content": "Malware behavioural analysis is a cornerstone technique in cybersecurity that involves observing and under-standing the actions performed by malware within a controlled environment, typically a sandbox [44]. This technique allows for the identification of malicious patterns and behaviours including networks and operations within the system. Recent studies emphasise the evolution of this analysis to include automated systems that leverage machine learning to predict and react to malware behaviour dynamically [13]. Such systems can discern between benign and malicious processes by examining changes made by the software to the system's state or its network behaviour [41]. These analyses often involve the extraction of features such as API calls, file-system operations, and network activity which are then processed using advanced algorithms to detect anomalous patterns that suggest malicious intent [41]. By comprehensively understanding the behaviour exhibited by the malware during execution and examining its underlying code and structure, we can gain valuable insights that aid in accurately attributing the malware to specific APT groups or threat actors."}, {"title": "2.2 Malware Attribution", "content": "The attribution of malware, identifying the probable origin or actor behind an attack, is a complex yet crucial task within cybersecurity. Traditional approaches in malware attribution have relied heavily on manual, domain-specific feature engineering and pre-processing to isolate attributes indicative of a malware's lineage or family ties [30]. The incorporation of neural networks has significantly advanced malware attribution capabilities, particularly using machine learning techniques such as Random Forest and Extreme Gradient Boosting (XGBoost), which have strengthened efforts in this area [16]. These algorithms refine neural network models by improving accuracy and handling overfitting, essential for distinguishing between benign and malicious activities in vast and complex datasets [16]. This synergy optimises feature selection and enhances predictive capabilities, effectively supporting the identification of malware origins and behaviours.\nRecent studies have introduced novel approaches in malware attribution that significantly improve upon traditional methods. For instance, the work by Binhui Tang and colleagues transforms APT malware samples into RGB images rather than relying on standard grayscale feature extraction [37]. This approach allows for deeper and more nuanced feature mining, using an enhanced Convolutional Neural Network (CNN) model that incorporates Self-Attention mechanisms and Spatial Pyramid Pooling (SPP-net) [37]. This novel framework aids in not only detecting APT malware but also in facilitating the identification of malware origins and attack methodologies through sophisticated visual data representations. Another innovative approach is presented by Elijah Snow and his team, who utilised an end-to-end multimodal learning strategy [36]. This method integrates three distinct neural network architectures-dense networks, CNNs, and Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) cells\u2014to automatically extract and learn features from diverse malware data attributes [36]. By combining these architectures, their model effectively classifies malware into respective groups, enhancing the granularity and accuracy of malware attribution. Further, Gil Shenderovitz and Nir Nissim introduced a dynamic analysis technique for segmenting Multivariate Time Series Data (MTSD) derived from API calls [35]. Their approach uses temporal segmentation to provide a detailed behavioural profile of APT malware, facilitating the detection and attribution to specific cyber-groups or nations with enhanced explainability."}, {"title": "3 Research Questions and Contribution", "content": "As previous sections have detailed the complexity and threat posed by APTs, this study leverages the sophisticated capabilities of DRL to analyse and interpret intricate malware data from controlled tests. The main goal is to refine a DRL model that effectively attributes APTs by analysing behavioural patterns, thus advancing cybersecurity defence mechanisms. This initiative to apply DRL seeks to harness its superior pattern recognition and strategic decision-making properties to enhance the detection and mitigation of advanced cyber threats."}, {"title": "3.1 Research Questions", "content": "The guiding questions of this research aim to critically evaluate the effectiveness of DRL in the cybersecurity landscape, particularly in attributing APTs. These questions explore: the identification of unique behavioural patterns of APTs within sandbox-analysed malware, the capability of DRL to precisely differentiate between malware behaviours from diverse APT groups, and the influence of the Markov Decision Process in boosting the strategic decision-making of DRL models within the context of cyber threat attribution. These inquiries are designed to assess whether DRL can offer a sophisticated and adaptive approach to understanding and countering APTs."}, {"title": "3.2 Contribution", "content": "This study makes impactful contributions to the domain of cybersecurity by pioneering the use of Deep Rein-forcement Learning (DRL) for the specific purpose of APT attribution, benchmarking its effectiveness against traditional machine learning models, and exploring its adaptability to varied APT scenarios. It constructs a DRL model that not only processes and understands detailed behavioural data from malware but also empirically demonstrates its enhanced effectiveness over existing techniques. Additionally, by probing the model's ability to adapt to new threats, the research highlights DRL's potential to evolve and maintain relevance in a rapidly changing threat environment. The findings and methodologies of this research expand the practical and theoreti-cal frameworks for deploying advanced AI in active cybersecurity defences, potentially setting new standards for the integration of machine learning in threat intelligence and response strategies."}, {"title": "4 Methodology", "content": "This section outlines the methodology adopted during the design, implementation and testing of the system. it provides details and justification on tools, approaches and methods employed as well as providing background information necessary for understanding the methodology."}, {"title": "4.1 Proposed System Design", "content": "The research adopts an experimental and simulation-based design, focusing on developing and evaluating a Deep Reinforcement Learning model for malware attribution to APT groups. The study begins by preparing a dataset of malware samples, followed by data preprocessing and feature extraction to ensure accuracy and relevance. The DRL model is then trained and tested in a simulated environment designed to mimic real-world conditions. This allows for controlled experimentation, where the model's ability to handle complex and evasive malware behaviours can be systematically assessed using metrics such as accuracy, robustness, and computational efficiency."}, {"title": "4.2 Data Collection", "content": "Data collection is strategically executed from two specialised sources to capture a broad spectrum of malware behaviours and characteristics, ensuring the depth and breadth of data necessary for effective DRL modelling."}, {"title": "4.2.1 Cuckoo Report", "content": "The Cuckoo Sandbox is an advanced open-source malware analysis system designed to analyse and report on the behaviour of potentially malicious files in a secure, isolated environment [6, 39]. It is widely used for malware detection by providing a controlled setting where files can be executed to observe their actions without risking the integrity of the host system [39]. In this work, the Cuckoo Sandbox plays an integral role in the hybrid process of collecting malware behavioural data, combining both manual and automated tasks to efficiently analyse large datasets of malware samples.\nSpecifically, because this analysis utilises the web version of Cuckoo Sandbox, mallicious files are manually uploaded via the Cuckoo Sandbox interface, which provides detailed system specifications and analytics, then prioritise and monitor these files in a secure environment to ensure the host system's integrity. Process IDs are then generated during the analysis phase, which allow tracking and retrieval of detailed reports on each malware sample's activity, including system changes and network traffic. An automated script further facilitates the extraction of these IDs, organising them into a structured dataset and performing data cleanup to avoid duplicates, ensuring the integrity and accuracy of the analysis."}, {"title": "4.2.2 VirusTotal Report", "content": "VirusTotal is a comprehensive online service that analyses files and URLs to detect viruses, worms, trojans, and other kinds of malicious content [32, 40]. Leveraged by security professionals and researchers, VirusTotal aggregates information from over 70 antivirus scanners and URL/domain blacklisting services, along with a plethora of tools for the analysis of files, which makes it an indispensable resource for the real-time detection of emerging threats [32].\nIn this work, VirusTotal complements the Cuckoo Sandbox by offering an extensive database of antivirus scan results and behaviour reports for more in-depth analysis of malware samples. Automated scripts access specific API endpoints using the SHA-256 hash of each sample to fetch detailed file reports and behaviour summaries. The file report encapsulates antivirus detection results, file type, size, and associated detection names, while the behaviour report sheds light on the malware's activities within an operating system, such as registry modifications and network actions. These reports are critical for understanding the malware's potential impact and aid in training deep reinforcement learning models to recognise and predict similar behaviours in future security threats."}, {"title": "4.3 Data Understanding", "content": "Understanding the data collected from sources like Cuckoo Sandbox and VirusTotal is essential before diving into deeper analyses or model development, as it establishes the groundwork for recognising patterns, anomalies, and intrinsic properties of malware behaviours. This preliminary step ensures that subsequent processes, such as data cleaning, preprocessing, and detailed exploratory analysis, are effectively tailored to the characteristics of the data. For instance, the \"reports.json\" file from Cuckoo Sandbox provides a wealth of information on malware activities through detailed logs of file creation, registry changes, and network connections. By parsing these entries, it is possible to discern the common tactics used by malware, such as communication strategies and system infiltration methods, which are crucial for identifying threat behaviours.\nSimilarly, VirusTotal's file and behaviour reports complement the data by providing insights into the malware's detectable characteristics and operational tactics within infected systems. These reports include critical metadata on the malware's type, the extent of its recognition across different security platforms (unique_sources), and its evasion techniques (packers). Additionally, behavioural data like registry modifications and network traffic from these reports help in understanding how malware interacts with and affects systems, highlighting potential persistence mechanisms or damage attempts. Through a comprehensive understanding of these datasets, it is possible to understand that the data used in modelling is accurate, reliable, and robust enough to develop effective machine-learning models that can attribute malware to specific APT groups, enhancing cybersecurity measures and threat intelligence."}, {"title": "4.4 Data Preparation", "content": "Data preparation is crucial for transforming raw data from Cuckoo Sandbox and VirusTotal into a structured format suitable for in-depth analysis and modelling. The process begins with data cleaning, which involves refining the datasets to highlight essential malware characteristics. For file reports, this includes isolating key attributes like \"file_name\", \"apt_group\", and \"unique_sources\", and quantifying the threat level by analysing entries classified as \"malicious\". Additionally, the \"import_list\" is parsed to assess the complexity of malware interactions. For behaviour reports, the focus is on dynamic interactions, such as the number of files written and registry keys manipulated, which provide insights into the malware's impact on system operations. Cuckoo reports are also processed to extract API call statistics from \"api_stats\", giving a detailed view of system interactions at the API level.\nFollowing data cleaning, the process moves to data integration, where the cleaned datasets are merged into a cohesive framework for unified analysis. The file and behaviour datasets are merged with the Cuckoo reports, with missing entries filled with zeros to maintain numerical data integrity. This step is essential for creating a comprehensive dataset that aligns all aspects of the malware's behaviour, enabling more effective modelling and analysis."}, {"title": "4.5 Data Modelling", "content": "In the data modelling phase, several critical steps are undertaken to prepare the dataset for effective machine learning applications. The process begins with Variable Transformation, where the dataset variables are identified and categorised based on their data types. Numerical columns are separated from categorical columns to facilitate different preprocessing techniques suitable for each type. The \"apt_group\" column, serving as the target variable for the models, is meticulously handled to ensure it is excluded from the feature sets when present in numerical columns, preventing data leakage. Categorical variables are then transformed into integer codes using techniques like Label Encoding, converting nominal data into a format that is digestible for machine learning algorithms. This transformation is essential for preparing the data for accurate and efficient modelling, ensuring that all features are in a machine-readable form.\nFollowing the transformation, the dataset undergoes Data Partition and Class Imbalance treatment and normalisation to optimise it for model training and evaluation. SMOTE (Synthetic Minority Over-sampling Technique) is employed to address class imbalances within the dataset, synthesising new examples in the minority class to prevent model bias towards the majority class. This ensures a balanced representation across classes, which is crucial for generalising the model effectively. The data is then split into training and testing sets, with a significant portion reserved for testing to assess model performance. Subsequently, normalisation is performed using MinMaxScaler, scaling all features to a uniform range to prevent any single variable from dominating due to its scale. This step is vital as it allows the machine learning model to converge more rapidly during training. The normalised data is then carefully reformatted back into DataFrames, retaining the original column names for better traceability and clarity during model training and evaluation phases."}, {"title": "4.6 Model Building", "content": "In the model-building phase, a bespoke environment is crafted using the Gymnasium framework, tailored to the complexities of Advanced Persistent Threat (APT) data. This setup precisely defines the observation space based on the feature set derived from malware samples and the action space aligned with unique labels constructed using the number of APT groups. The environment facilitates the simulation of interaction sequences, rewarding the model for accurate predictions and resetting for new episodes as data points are iteratively processed.\nDuring the training of the model, a Deep Q-Network (DQN) is utilised, configured with adjustable learning rates and buffer sizes to optimise the learning curve. The model's performance is periodically assessed using key metrics such as accuracy and the F1 score, which aid in fine-tuning the training regimen. This dynamic approach ensures a balance between exploration of new strategies and exploitation of known effective tactics, enhancing the model's ability to make progressively more accurate malware classifications. Subsequent development includes hyperparameter tuning-adjusting the discount factor, exploration rate (epsilon), and mini-batch sizes-to enhance the learning process's efficiency and effectiveness. Training episodes are varied in length to reflect the complex nature of real-world APT scenarios better, preventing overfitting and improving generalisation. Moreover, regularization techniques like dropout and batch normalisation are integrated within the neural network architecture to mitigate the risk of overfitting by moderating less predictive features' influence and stabilising learning across different batches. Detailed performance analysis and error metrics are continuously collected and reviewed to identify the model's strengths and weaknesses, providing a clear direction for its ability to capture the APT groups."}, {"title": "4.7 Dataset", "content": "The APT Malware Dataset utilised in this work is a comprehensive collection of over 3,500 malware samples (https://github.com/cyber-research/APTMalware), categorised into 12 distinct Advanced Persistent Threat (APT) groups obtained from. These groups are believed to be state-sponsored by five different countries, including China, Russia, North Korea, the USA, and Pakistan. The dataset serves as a critical resource for benchmarking various machine-learning techniques aimed at authorship attribution of cyberattacks.[7]\nEach APT group in the APT Malware Dataset represents a unique threat actor with a specific set of malware samples attributed to their cyber activities. The dataset's diversity is evident in both the quantity of samples per group and the variety of file types, ranging from executable files like .dll and .exe to documents such as .doc, .xlsx, and .ppt. This assortment adds complexity to the analysis, enabling robust evaluations of the various attack vectors and infection methods used by these groups. However, the dataset also shows a significant class imbalance, with some groups having as few as 32 samples and others as many as 961, presenting a challenge for reinforcement learning models to achieve unbiased behavioural representations. To manage this, the samples are meticulously labeled with their SHA-256 hashes for precise identification and stored in separate, password-protected compressed folders to ensure security and data integrity, with the universal password \"infected\" providing controlled access. The analysis leverages tools like Cuckoo for dynamic analysis, where malware files are extracted and executed, and VirusTotal, which uses the hashes to fetch pre-generated reports for deeper insights into the malware behaviour."}, {"title": "4.8 MDP Model", "content": "The Markov Decision Process (MDP) provides a structured framework for understanding how an agent makes decisions while interacting with its environment [22]. In this work, MDP framework is utilised to design and develop the DRL model for attributing malware to APT groups. The primary data sources for the model come from detailed reports generated by Cuckoo Sandbox and VirusTotal, which offer comprehensive behavioural analyses of malware samples. These reports provide a multi-dimensional view of each malware's characteristics and behaviour, which are crucial for defining the states, actions, and rewards in the MDP framework as listed below:"}, {"title": "4.8.1 States Space", "content": "The state represents the current understanding of a malware sample based on its observed behaviours and characteristics. Each state is derived from a feature dataset that encapsulates various aspects of malware behaviour, such as file operations, registry changes, network activities, and other dynamic interactions recorded during the malware's execution. This dataset is constructed using key data points extracted from the Cuckoo and VirusTotal report. These features collectively form a comprehensive behavioural profile of the malware, encapsulating its operational tactics and techniques, which are used to define the current state in the MDP. This state representation serves as the foundation for the reinforcement learning model's decision-making process, enabling accurate attribution and classification of malware to specific APT groups."}, {"title": "4.8.2 Actions Space", "content": "In the MDP model, an action refers to the transition from analysing one malware sample to another within the dataset. Each action involves selecting a new malware sample from the dataset and performing the analysis to obtain its behavioural profile, thus transitioning the state of the MDP from the current malware profile to the next. This action reflects the decision-making process in identifying and comparing malware attributes across different samples, which is central to attributing them to specific APT groups."}, {"title": "4.8.3 Rewards", "content": "The reward in our proposed MDP model is defined by the accuracy of the attribution. When the DRL model correctly attributes a malware sample to its respective APT group based on the analysed behaviours, a positive reward is assigned. Conversely, incorrect attributions yield a negative reward. The magnitude of the reward is scaled based on the confidence level of the attribution and the criticality of correctly identifying specific APT-related malware, reflecting the importance of precision in cybersecurity measures.\nIn summary, the dataset features, such as process call count, registry access patterns, file operations, and network behaviours, represent the various states of malware samples within the MDP framework. The DRL agent continuously monitors these states and takes actions to attribute the malware to a specific APT group. At each time step t, the agent is in the state st and selects an action at, transitioning to a new state st+1, which corresponds to the analysis of another set of behavioural features. The agent is rewarded based on the accuracy of its attributions, receiving a positive reward (+1) for correct classifications and no reward (0) for misclassifications. Through this process, the agent refines its policy, improving its ability to attribute malware samples to the correct APT group as it progresses through the dataset."}, {"title": "5 Implementation and Testing", "content": ""}, {"title": "5.1 Simulation Environment", "content": "The proposed DRL model for Advanced Persistent Threat (APT) attribution utilises a structured approach incorporating an environment for sequential decision-making, a Q-network for estimating the quality of actions, and a replay memory for learning from past experiences. Here, the agent's states are derived from comprehensive behavioural data extracted from malware reports, while actions represent decisions to attribute malware to specific APT groups. An action represented by \\(a_{t} = n\\) indicates the model's prediction, where n corresponds to the malware associated with an APT group. The agent operates within this environment, aiming to optimise the cumulative rewards over time, where the rewards are aligned with the accuracy of the attribution to the correct APT group. This structure is designed to refine the agent's decision-making process and improve its policy through continuous learning and adaptation based on detailed malware behaviour analysis."}, {"title": "5.1.1 Environment", "content": "This is the simulated setting where the DRL agent is deployed, designed for making informed attributions of malware to specific Advanced Persistent Threat (APT) groups based on behavioural analysis. This environment is an adaptation of the OpenAI Gym interface, featuring a discrete action space that corresponds to different APT groups identified in the dataset [21]. The observation space is constructed from detailed features such as API calls, file-system operations, and network activities, which are crucial for defining the states of the malware being analysed [21]."}, {"title": "5.1.2 Q-Network", "content": "At the core of the decision-making process, the Q-Network includes a policy network and a target network, each configured as a multilayer perceptron with two hidden layers leading to an output layer that represents each potential APT group. The networks use Leaky ReLU activation functions to maintain gradient flow during training, helping to prevent the vanishing gradient problem that can occur with standard ReLU functions if negative values are present in the inputs [28]. The output layers of the networks apply a MinMaxScaler to normalise the outputs, ensuring that the classification probabilities for the APT groups are scaled between 0 and 1 [34]. This normalisation helps stabilize the learning process by keeping the network's predictions within a consistent range."}, {"title": "5.1.3 Replay Memory", "content": "Essential for robust learning, Replay Memory archives tuples of the agent's experiences, including states, actions, rewards, and subsequent states. These experiences are accumulated as the agent processes the behavioural data, employing an epsilon-greedy strategy \\( \\epsilon \\) to balance the exploration of new strategies with the exploitation of known patterns. Each action-representing an attribution decision-transitions the agent from one state to another (st to st+1), with rewards assigned based on the accuracy of these attributions."}, {"title": "5.1.4 Policy Training", "content": "The training of the DRL agent's policy operates over a series of episodes, with each episode consisting of numerous time steps, labelled as T. Each time step t involves the sampling of a feature vector representing the current state st from the replay buffer B, which is then fed into the policy network. The policy network processes this input to output Q-values, Q(st, at), for potential actions aimed at matching these values with the target or optimal Q-value, Q(s, a)."}, {"title": "DRL - Agent Policy Training Algorithm", "content": "Preconditions: \\( 0 \\leq \\gamma \\leq 1\\); \\(0.1 \\leq \\epsilon \\leq 1\\)\n1: Set \\(X_{p} \\leftarrow s_{t}\\)\n2: Set \\(X_{t} \\leftarrow s_{t+1}\\)\n3: For each episode, repeat:\n4: While t T do:\n5: If \\( \\epsilon \\geq 0.1 \\) then:\n6: Select random \\(a_{t}\\)\n7: Else if \\( \\epsilon = 0.1 \\) then:\n8: \\(Q(s_{t},a_{t}) \u2013 X_{p}w_{p} + b\\)\n9: Select \\(a_{t}: a_{t} \\leftarrow index(max Q(s_{t}, a_{t}))\\)\n10: End If\n11: Observe \\(r_{t}, s_{t+1}\\)\n12: Store experiences: \\(B_{n} \\leftarrow \\{(s_{t}, a_{t}, r_{t}, s_{t+1})_{n} \\}\\)\n13: Select randomly \\(B \\subset B_{n}\\)\n14: \\(Q(s_{t}, a_{t}) \\leftarrow X_{p}w_{p} + b\\)\n15: \\(Q(s_{t+1}, a_{t+1}) \\leftarrow X_{t}w_{t} + b\\)\n16: \\(Q*(s, a) \\leftarrow r_{t} + \\gamma \\cdot max(Q(s_{t+1}, a_{t+1}))\\)\n17: \\(L(f_{x}) \\leftarrow Q*(s, a) \u2013 Q(s, a)\\)\n18: \\(w_{p} \\leftarrow \\{w_{p} \u2013 \\alpha \\}\\)\n19: If T = f then:\n20: \\(X_{t}w_{t} + b - X_{p}w_{p} + b\\)\n21: End If\n22: t-t+1\n23: End While\n24: End For"}, {"title": "5.2 Experimental Specifications", "content": "The experimental setup for the DRL-based APT detection model involves finely tuned parameters for the Deep Q-Network (DQN) model to optimise its performance in learning and adapting to detect advanced persistent threats (APTs) effectively. This section details the specific arguments and configurations passed to the DQN model, which are crucial in defining its learning behaviour and operational dynamics in the simulated environment."}, {"title": "5.3 Software Environment", "content": "In order to implement and evaluate the DRL-based APT attribution model, several key software tools and techniques are utilised. These are essential for creating a robust environment that can simulate real-world scenarios and evaluate the performance of the model under controlled conditions."}, {"title": "5.3.1 Cuckoo Sandbox", "content": "Cuckoo Sandbox is an open-source automated malware analysis system that acts as a vital tool in the environment. It allows for the isolation and analysis of suspicious files in a safe, contained environment. This sandboxing technique enables the collection of detailed analysis about the behaviour of the file while running in an operating system, which is vital for training the DRL model to recognise threat behaviours. The outputs provided by Cuckoo Sandbox include API calls, network traffic, file system changes, and memory dumps, which serve as critical inputs for the model's learning process. [11]"}, {"title": "5.3.2 Stable Baselines 3", "content": "Stable Baselines 3, an enhancement over the original OpenAI Baselines, offers refined implementations of reinforcement learning algorithms. The Deep Q-Network (DQN) model from Stable Baselines 3 is specifically utilised for the agent's training process. This model efficiently estimates the optimal action-value function, which is central to making informed decisions in the simulated network environments. The DQN supports the development of a robust policy that can effectively identify and differentiate between benign and malicious network traffic. [1]"}, {"title": "5.3.3 Gymnasium", "content": "Gymnasium, formerly known as Gym, is a tool from OpenAI that provides standardised interfaces for a diverse array of environments. These environments serve as testbeds for reinforcement learning algorithms. In this work, Gymnasium offers the foundational framework necessary for designing and managing the interaction between the DRL agent and the simulated network environment, which is vital for both the training and evaluation phases. It enables the DRL model to adapt and learn efficiently from dynamic scenarios that mimic real APT attacks. [10]"}, {"title": "6 Results and Discussions", "content": ""}, {"title": "6.1 Results Evaluation", "content": "The development of the DRL model for malware attribution involved extensive research, iterative coding, and numerous adjustments based on the insights gathered from predecessor models and contemporary research papers. This preparatory work was essential to establish a robust foundation for the model, ensuring it could adapt and respond effectively to the dynamic nature of malware threats. Initially, the model struggled with low accuracy levels, but through persistent adjustments to its architecture and learning algorithms, accuracy improved dramatically-from about 7% to over 73% in early iterations. By the end of the training, the model consistently reached accuracy levels near 98%, demonstrating its strong capability to accurately recognise and attribute malware activities. This upward trajectory in training accuracy is graphically represented in the Figure, which vividly illustrates the model's maturation and increasing proficiency over time.\nFollowing the graph for training accuracy, a detailed heatmap was generated to gain insight into the model's performance across each of the APT groups, highlighting precision, recall, and F1-scores. Notably, the model demonstrated exceptional performance with Equation Group', achieving perfect scores across all metrics, show-casing its capability to precisely attribute actions to this well-documented APT. Similarly, 'APT 30', 'Gorgon Group', and 'Winnti' show remarkable precision and near-perfect F1-scores, reflecting the model's strength in handling sophisticated malware profiles. In contrast, 'APT 21' presents a lower recall of 93.44%, indicating a slight challenge in capturing all activities associated with this group. This variance in performance underscores areas for potential refinement, providing valuable feedback for further enhancing the model's accuracy and adaptability to diverse malware behaviours. This heatmap serves as a crucial tool for visualising the model's specific strengths and areas for improvement in malware attribution.\nIn parallel with the training dataset, the DRL model's accuracy on the test dataset was thoroughly evaluated, demonstrating significant improvements throughout the training process. Starting from a modest 19.30% at the initial evaluation (step 500), the model's accuracy steadily climbed to an impressive 89.27% by the final testing step (step 20000). This progression highlights the model's increasing proficiency in adapting its predictive strategies to the complexities inherent in cybersecurity data.\nThe analysis of the DRL model's performance on the test dataset across various APT groups reveals its accuracy through precision, recall, and F1-scores. The model excelled with 'APT 19' and 'Equation Group', achieving F1-scores of 95.8% and 99.1% respectively, underscoring its proficiency in accurately identifying and attributing their activities. In contrast, the model faced challenges with 'APT 28', where it recorded a lower precision of 62.4% and an F1-score of 68.1%, indicating difficulties in correctly detecting this group's actions. Notably, 'Energetic Bear' and 'Equation Group' displayed almost perfect precision, highlighting the model's strength in pinpointing these groups with high accuracy. Generally, the model demonstrated relatively high precision, recall, and F1-scores across most groups, reflecting its overall effectiveness in accurately attributing a diverse array of APT activities.\nThe overall performance of the DRL model on the test dataset can be quantified through key metrics"}]}