{"title": "RoarGraph: A Projected Bipartite Graph for Efficient Cross-Modal Approximate Nearest Neighbor Search", "authors": ["Meng Chen", "Kai Zhang", "Zhenying He", "Yinan Jing", "X.Sean Wang"], "abstract": "Approximate Nearest Neighbor Search (ANNS) is a fundamental and critical component in many applications, including recommendation systems and large language model-based applications. With the advancement of multimodal neural models, which transform data from different modalities into a shared high-dimensional space as feature vectors, cross-modal ANNS aims to use the data vector from one modality (e.g., texts) as the query to retrieve the most similar items from another (e.g., images or videos). However, there is an inherent distribution gap between embeddings from different modalities, and cross-modal queries become Out-of-Distribution (OOD) to the base data. Consequently, state-of-the-art ANNS approaches suffer poor performance for OOD workloads.\nIn this paper, we quantitatively analyze the properties of the OOD workloads to gain an understanding of their ANNS efficiency. Unlike single-modal workloads, we reveal OOD queries spatially deviate from base data, and the k-nearest neighbors of an OOD query are distant from each other in the embedding space. The property breaks the assumptions of existing ANNS approaches and mismatches their design for efficient search. With the insights from the OOD workloads, we propose pRojected bipartite Graph (RoarGraph), an efficient ANNS graph index that is built under the guidance of query distribution. Extensive experiments show that RoarGraph significantly outperforms state-of-the-art approaches on modern cross-modal datasets, achieving up to 3.56\u00d7 faster search speed at a 90% recall rate for OOD queries.", "sections": [{"title": "1 INTRODUCTION", "content": "Approximate nearest neighbor search (ANNS) is a fundamental and performance-critical component in various application domains such as large-scale information retrieval [49, 54, 78], recommendation [11, 55], and question answering [42, 62]. More recent applications of retrieval-augmented generation (RAG) in large language models (LLMs) also utilize vector databases as external knowledge libraries, employing ANNS to enhance search efficiency [2, 46, 73]. These applications demand fast and accurate responses to similarity vector search, where ANNS can be performed to efficiently retrieve the approximate nearest neighbors from the database for a given query, rather than conducting impractically exact k-nearest neighbor searches [9, 19, 85]. To improve the ANNS performance, a spectrum of studies have been carried out to design efficient data structures, including partition-based approaches [15, 65, 67, 81], quantization-based methods [4, 24, 26, 36, 77], and hashing-based methods [13, 21, 31, 31, 86], where graph-based approaches [20, 52, 56] represent the state-of-the-art performance on many datasets.\nRecently, cross-modal retrieval has drawn much attention with the advancement of multimodal data representation techniques. Deep learning models, such as CLIP [59], trained for multimodal tasks embed unstructured data from different modalities like vision and natural language into a shared high-dimensional space with semantics preserved, say embeddings. In cross-modal vector search, data from one modal (e.g., texts) is used as the query to retrieve the most semantically similar data from another modal (e.g., images or videos) [32, 41, 47]. With the diverse and critical application scenarios, efficient ANNS are widely demanded to enhance the performance of cross-modal retrieval [7, 63, 66, 68, 74, 80, 83]. However, existing ANNS indexes are designed for single-modal scenarios, and they suffer poor performance with cross-modal queries. For example, in the modern cross-modal dataset LAION [61], an HNSW (Hierarchical Navigable Small World) [52] index needs to visit 14374 nodes to ensure recall@10 to reach 0.95 in text-image search, while only 1568 nodes are required to traverse if using an image to search images, indicating nearly 10 times efficiency degradation.\nThe main characteristic of cross-modal retrieval is the dramatically different data distributions between vectors from two modalities. Even though multimodal neural embedding models enable similarity measurements on vectors from different modalities, a consistent and inherent distribution gap, recognized as the modality gap, persists between embeddings of two modalities in cross-modal representation learning [45]. Accordingly, in cross-modal ANNS, query vectors are Out-of-Distribution (OOD) with respect to vectors in the database (base data) [34]. This stands in stark contrast to single-modal workloads where queries are In-Distribution (ID) with the base data. The Mahalanobis distance [51] shows that queries from another modality deviate 10 ~ 100\u00d7 far away from the base data than that between ID queries and the base data in cross-modal datasets, e.g. Text-to-Image [14], LAION [61], and WebVid [5]. Further, through in-depth experiments and analysis, we find that an out-of-distribution query is far from the base data, and the k-nearest neighbors of such a query tend to be distant from each other, indicating that queries deviate from the base data and the nearest neighbors (ground truths) to OOD queries are more widely distributed than that to ID queries.\nHowever, state-of-the-art ANNS indexes are designed for ID queries [20, 52, 56]. They presume that queries appear near the base data and nearest neighbors for a query are in close proximity to each other. Under the assumption, graph-based ANNS approaches employ beam search (n-greedy search) during the index-building phase to construct an approximate KNN graph [20, 35, 43, 50, 52], where vectors with a smaller distance tend to be connected. Besides, the search phase also uses beam search, which anticipates rapid convergence with the closely connected base data. This indexing convention suffers from OOD queries in cross-modal ANNS. Since cross-modal queries and the base data follow different distributions and the ground truths for OOD queries are scattered, the critical assumption on the distribution of queries and the base data is broken. As a result, the cross-modal search on such a graph cannot converge efficiently but incurs more hops in graph traversals. That constitutes the primary reason why existing ANNS approaches are incompetent in handling OOD workloads.\nWe propose projected bipartite Graph (RoarGraph), an efficient graph index with the knowledge from query distribution for cross-modal ANNS. Our key idea is to map distributed vectors that are nearest neighbors to queries into closely connected neighbors within a graph index. The indexing process of RoarGraph is as follows. Firstly, with elaborate edge selection, a bipartite graph is built by mapping the relationship of similarity between queries and base data into the unified graph structure. Secondly, the bipartite graph is projected onto base data, incorporating neighborhood-aware projection to create pathways for spatially distant nodes, recognized as proximate from the perspective of queries. Finally, a connectivity enhancement scheme is performed to ensure the graph's connectivity and the reachability of all base data vectors. The RoarGraph index exclusively consists of base data yet effectively preserves the neighboring relationship derived from the query distribution. The main contributions of this paper are summarized as follows:\n\u2022 We identify the inefficiency of cross-modal ANNS through in-depth experiments and present an insightful analysis that reveals underlying reasons causing performance degradation on state-of-the-art approaches in cross-modal ANNS.\n\u2022 We propose RoarGraph, a novel graph index for efficient cross-modal ANNS, which effectively utilizes query distribution to guide graph index construction.\n\u2022 We performed extensive experiments on three cross-modal datasets comprising text, images, and video frames. Our results show that RoarGraph significantly improves the performance of cross-modal vector search.\nRoarGraph speeds up cross-modal vector search on a graph by minimizing detours and reducing the number of hops during the search phase. This leads to a significant performance improvement over existing graph indexes, ranging from 1.84x to 3.56\u00d7 faster on three cross-modal datasets with recall@k\u22650.9, where k=1,10, and 100. In particular, RoarGraph also achieves an exceptional level of recall (recall@k \u2265 0.99) that is unattainable by existing methods. A variant of our approach won the championship in the OOD track of NeurIPS' Practical Vector Search (Big ANN) Challenge 2023."}, {"title": "2 BACKGROUND AND MOTIVATION", "content": "This section introduces the background of approximate nearest neighbor search (ANNS) and out-of-distribution (OOD) ANNS. We also quantitatively evaluate and analyze the performance of existing ANNS approaches on cross-modal datasets."}, {"title": "2.1 Background on ANNS", "content": ""}, {"title": "2.1.1 The ANNS Definition", "content": "ANNS stems from the k-nearest neighbors search (KNNS), which aims to find k vectors in the database (base data) that are closest to a given query vector. The measurement of closeness typically involves utilizing cosine distance, $l_2$-distance, inner product, etc. Contemporary applications involve working with large-scale datasets in which the dimension of vectors grows to hundreds [66], the pursuit of the exact KNNS becomes both costly and impractical due to the challenges imposed by the curse of dimensionality [33]. ANNS approaches create specific indexes for the base data to achieve a tradeoff between search speed and accuracy.\nDEFINITION 1. Given N vectors in the database $X = \\{x_1, ..., x_N\\} \\in \\mathbb{R}^D$, $q \\in \\mathbb{R}^D$ and a function for computing the distance between two vectors $\\delta(.,.)$. Top-k $(k \\le N)$ ANNS aims to find\n$$\nS = k-arg \\min_{x_i \\in X} \\delta(q, x_i)\n$$\n(1)\nS satisfies that $|S| = k$, $\\delta(q, x) \\le (1+e)\\delta(x', q)$ for $x \\in S$, $x' \\in X \\backslash S$ and $e \\ge 0$, where $e$ is a small constant and not used directly, only denoting the approximation property hold by ANNS.\nThe search performance of an ANN index will be evaluated by search speed-vs-recall tradeoff. Recall is calculated using the formula recall@k = $|S \\cap KNN(q)|/k$, where KNN(q) represents the exact k-nearest neighbors (ground truths) of a query q, and S is the result set with $|S| = k$."}, {"title": "2.1.2 State-of-the-Art ANNS Approaches", "content": "Graph-based methods and the inverted file index are two prevalent ANNS approaches. Graph-based methods are the most high-performance ANNS approach family, providing a better search speed-recall tradeoff than other methods [20, 44, 50, 52, 56, 75]. These methods index base data into a graph structure that each node represents one data vector. HNSW [52] is a well-known multi-layer graph in the hierarchical structure. During construction, all base vectors are inserted into the base layer (layer 0), and only subsets of layer i will be inserted into the i + 1 layer, with decreasing probability from bottom to top (the top layer contains only 1 point). Nodes within each layer are connected to their approximate nearest neighbors. During the search phase, HNSW employs greedy search on higher layers for a given query. The closest point obtained from higher layers becomes the entry point for the next layer. At the base layer, a beam search is performed on the graph. Beam search is a variant of greedy search that explores a graph by expanding the most promising element in a limited queue [57], converging towards the nearest neighbors for a given query. The capacity of the queue, termed the beam width, controls the trade-off between accuracy and search speed.\nThe inverted file index (IVF)-based methods are another popular index type in ANNS for its convenience and superior performance for range nearest neighbors search, including IVF [67], IMI [3], etc. IVF first applies K-means to cluster base data and gets n centroids, then assigns each vector in base data to its nearest cluster. During the search phase, IVF selects nprobe closest centroids to the query, then scans all vectors belonging to the corresponding nprobe clusters and obtains the top-k results."}, {"title": "2.2 Out-of-Distribution ANNS", "content": "Out-of-distribution Approximate Nearest Neighbor Search (OOD-ANNS) indicates the distribution of queries differs from that of the base data. A distribution gap is consistently present and inherent between vectors from two modalities in modern cross-modal applications [45]. For instance, when using texts as queries to retrieve relevant visual data, a distribution gap occurs, resulting in queries becoming out-of-distribution. Unfortunately, the majority of ANNS indexing algorithms are primarily designed for single-modal tasks, such as image-image search.\nThe OOD property of cross-modal vector search can be mathematically quantified using two mathematical distances: the Wasserstein distance [38, 72] that measures two distributions and the Mahalanobis distance [51] that measures the distance from a vector to a distribution. We use two metrics to evaluate data distributions on three modern real-world multimodal datasets: Text-to-Image [14], LAION [61], and WebVid [5]. The characteristics of these datasets are shown in Table 1.\nTo quantify the OOD characteristic by Wasserstein distance, we sample two non-intersecting sets $(B_1, B_2)$ from the base data and one query set (Q) from the query vectors, each containing 100,000 vectors. As shown in Table 2, two samples from the base data demonstrate proximity with $W_2(B_1, B_2)$. In contrast, the query distribution diverged from the base data distribution, being 1.67 times, 1.89 times, and 2.89 times more distant across three datasets. In addition to the entire distribution difference, a query is considered out-of-distribution if its Mahalanobis distance to the base data significantly differs from the distances between base vectors [34]. For each $q_{id}$ in the ID query set and $q_{ood} \\in Q$, we compute the Mahalanobis distance to estimate $d_m(q, P)$, where P is the base data distribution. As depicted in Figure 1, it is evident that OOD queries significantly deviate from the base data distribution. In particular, queries from LAION and WebVid exhibit a more pronounced out-of-distribution property compared to those in Text-to-Image."}, {"title": "2.3 The Inefficiency of Current Approaches for OOD-ANNS", "content": ""}, {"title": "2.3.1 Performance of Current Approaches on OOD Workloads", "content": "Evaluations of IVF and HNSW are conducted on three multimodal datasets shown in Table 1. In-distribution (ID) queries are sourced from original large-scale datasets that follow the same empirical distribution of base data, and OOD queries use the textual query set for each dataset. We build IVF indexes by the Faiss library [37] and HNSW indexes by the official implementations [52] with recommended parameters.\nCritical performance degrading on OOD workloads is observed in Figure 2. When using the IVF index, OOD queries demonstrate a noticeable need to search a significantly larger number of clusters to achieve high recalls, in contrast to ID queries. Recall@10 for ID queries exceeded 0.97 when searching the closest 50 clusters on all three datasets, whereas OOD queries achieved recall@10 of only 0.91, 0.20, and 0.52, respectively.\nSimilarly, with the HNSW index, OOD queries also require visiting a much larger number of nodes during beam search on the graph, resulting in poor search efficiency among the three datasets. On the LAION dataset, OOD queries necessitate more than 500 hops to achieve recall@10\u22650.93, whereas only 48 hops are required for ID queries. This highlights inefficient performance attributed to approximately 10 times the length of the search path caused by OOD queries. The substantial performance decay proves that existing indexes perform poorly on cross-modal ANNS tasks, emphasizing the urgent need for an efficient index."}, {"title": "2.3.2 Limitations of Previous Solution for OOD-ANNS", "content": "The pioneering graph index designed to tackle the OOD-ANNS problem is RobustVamana introduced in OOD-DiskANN [34]. The primary objective of RobustVamana is to use query vectors to add edges in the Vamana graph [35]. After linking the base data, queries are also inserted into the Vamana graph. Then, it launches an interconnecting procedure named RobustStitch to create a full connection among the closest nodes associated with inserted queries.\nFigure 3 compares the performance of RobustVamana to its original design, Vamana. Searching with OOD workloads, RobustVamana offers 13% ~ 67% improvements over Vamana when reaching recall@10=0.9, though it becomes marginal with the increasing recall. However, OOD queries on RobustVamana are, on average, 3.9 times, 5.3 times, and 10.0 times slower than ID queries on Vamana for the three datasets.\nThe results underscore the inefficiency in OOD-ANNS and highlight the substantial potential to design a novel index for improvement in performance concerning OOD workloads."}, {"title": "3 ANALYSIS OF OOD WORKLOADS IN CROSS-MODAL ANNS", "content": "In this section, we analyze out-of-distribution workloads to gain insights into the reasons existing approaches are ineffective in achieving high performance in cross-modal ANNS."}, {"title": "3.1 Underlying Key Differences", "content": "Through in-depth experiments and analysis, we find there are two critical differences between out-of-distribution ANNS (OOD-ANNS) and in-distribution ANNS (ID-ANNS).\nOOD queries are distant from their nearest neighbors. In Figure 4, we calculate the distances between queries and their k-nearest neighbors (k=1), denoted as $\\delta(q, i^{th}NN)$. Let q denote the query, and $\\delta(.,)$ represents the distance measurement function of each dataset. As depicted in this figure, we observe that the distances from OOD queries to their nearest neighbors are significantly greater than those from in-distribution queries to the ground truth of the same base data, i.e. $\\delta(q_{ood}, i^{th}NN_{ood}) \\gg \\delta(q_{id}, i^{th}NN_{id})$ in the context of ANNS, with only a small intersection observed in the histograms of Text-to-Image. Despite the presence of extreme values, focusing on the median, OOD queries are 2.1 times, 5.3 times, and 11.3 times farther from their nearest neighbors than ID queries. We also find that the deviated OOD query leads to the k-nearest neighbors of an OOD query are distant from each other in the high-dimensional space, as opposed to the neighboring nature of the nearest neighbors of an ID query. To confirm that the k-nearest neighbors of a given OOD query exhibit considerable spatial separation, in the scenario with k=100 for a given query q, we calculate the average distance between $i^{th}NN$ (i = 1...100) and the other 99 nearest neighbors of q, resulting in 100 values that represent the degree of separation between nearest neighbors of q. Subsequently, the values for $i^{th}NN$ across all queries are averaged to obtain the mean, reflecting this general property. Figure 5 presents the phenomenon, distances in neighbors of OOD queries are evidently larger than the neighbors of ID queries, for about 1.29 times, 1.45 times, and 2.11 times on the three datasets, respectively. The finding suggests the presence of numerous noise data vectors between the top-k answers to OOD queries."}, {"title": "3.2 Why Previous Methods Fail on OOD-ANNS", "content": "The main reason for the inefficiency of existing approaches is that the OOD query breaks the assumptions held by conventional ANNS approaches. The primary assumptions behind state-of-the-art ANNS indexes include 1) queries exhibit proximity to vectors in the base data, assuming a shared distribution for both queries and base data [20, 44, 56], and 2) the k-nearest neighbors of a query are close to each other in space $\\mathbb{R}^D$, or a neighbor's neighbor is likely also to be a neighbor [16, 20, 44, 56, 57, 64].\nBased on the assumptions, the graph-based methods generally utilize beam search both for constructing an approximate k-nearest neighbor graph and executing searches [19, 20, 43]. Thus, these methods transform spatial proximity vectors into nodes that are closely connected in a graph and further presume that beam search can efficiently navigate into a sphere containing ground truths by shrinking the search space at each step in greedy routing [20, 56, 57].\nHowever, the search space expands significantly for OOD queries. Considering a high-dimensional sphere denoted as $B^k(1^{st}NN, R)$ centered at $1^{st}NN$ of a given query, with the radius R defined as the maximum of $\\delta(i^{th}NN, j^{th}NN)$ for a given query (i \u2260 j). The data nodes inside a sphere $B^q(x)$ enclosing the currently visiting node x and $B^k$ constitute the recognized search space on the graph [20, 56]. In Figure 5, it is observed that $R^{ood}$ is significantly larger than $R^{id}$, ranging from 1.29 \u00d7 to 2.11 x. Expressing the volume of a sphere as $C_B \\times R^D$, where $C_B$ is a constant for a fixed dimension D, the ratio $R^{D_{ood}}/R^{D_{id}}$ increases substantially in the high-dimensional space, leading to significant enlargement of $B^k$ compared to $B^k$. Meanwhile, the search space $B^q(x)$ undergoes a vast expansion and encounters challenges in efficiently shrinking due to the inflation of $B^k$. Each of the separated nearest neighbors to an OOD query becomes a local optimum trap in greedy routing, posing difficulties for search convergence on existing graph indexes. This indicates that a massive number of nodes are required to be visited for an OOD query.\nTo illustrate the difficulty of convergence when searching an OOD query, assuming a prevailing graph index is constructed based on the data in Figure 7, where the close vectors tend to be connected within the graph index. For an ID query, BS (x) is easily reduced along the search path due to the close proximity of the nearest neighbors (blue hexagons). With each progression in greedy routing, the search space contracts to a small sphere containing the nearest neighbors to the ID query [20, 56, 57]. However, for an OOD query, beam search struggles to converge in a single direction. The search space cannot be recognized as shrunk even when routed to one of the k-nearest neighbors (red diamonds). This is because the ground truths for an OOD query are distributed within a huge sphere compared to an ID query. To achieve a high recall, the search process for OOD queries requires a larger beam width, an extended search path, and increased computations and memory accesses along the detour to escape from the local optimum and find dispersed answers, leading to performance degradation.\nThe space partition-based methods such as IVF also suffer from OOD queries. Clusters are obtained by running K-means algorithm on the base data, where close points are assigned to the same cluster, and the nearest neighbors for OOD queries can be distributed in separated far-away clusters. A cluster containing the closest centroid to an OOD query may not contain ground truths since the close neighbors recognized by an OOD query can be dispersed. In the real-world example Figure 7, the obvious performance impact for partition-based methods is the necessity to scan 5 clusters for OOD queries to achieve recall@5=1.0, compared to only 2 clusters needed for ID queries, resulting in about 2.5 times lower performance. This negative impact becomes worse for million-scale datasets, as described in Section 2.3.1."}, {"title": "4 ROARGRAPH: A GRAPH INDEX FOR EFFICIENT OOD-ANNS", "content": "With the revelation of inefficiencies in OOD-ANNS and insightful analyses, we introduce RoarGraph, a graph index that is built under the guidance of query distribution to provide efficient ANNS in cross-modal retrieval."}, {"title": "4.1 Query Guided Index for ANNS: Challenges", "content": "As the particular characteristics of the OOD workloads break the assumptions made in the design of existing ANNS approaches, we propose to leverage the query distribution to guide the construction of a graph index. In contrast to connecting base nodes with smaller distances, we propose to transform spatially distant vectors, perceived as close from queries' perspectives, into closely connected neighboring nodes within the graph index. Given the goal, our approach utilizes queries to guide the RoarGraph construction. For example, we use the embedded vectors of image captions sampled from the original billion-scale LAION dataset to build RoarGraph supporting text-image search for LAION.\nTo effectively utilize the query distribution in guiding the graph construction, a bipartite graph emerges as a reasonable candidate for modeling the relationship of closeness between embeddings from two modalities. Widely utilized in recommendation systems [17, 27, 76], a bipartite graph comprises two sets of nodes with edges connecting nodes of each set, while nodes within the same set remain unconnected.\nIn OOD-ANNS, base data and queries can be treated as two distinct sets (base nodes and query nodes) within a bipartite graph.\nHere, query nodes play a crucial role in serving as bridges to identify and connect the nearest neighbors in base data to query vectors. Besides, a greedy search on such a bipartite graph can be executed by visiting neighbors' neighbors. Given an online query, starting from a base node, moving to its out-neighbors (queries) to check out-neighbors (base nodes) of these queries, and then selecting a closer base node to proceed. While it seems to be a natural structure to model the closeness relationship of two distributions and support search, the following challenges remain to build a competent graph index for efficient ANNS.\n(1) Effective establishment of edges between base nodes and query nodes in the bipartite graph for modeling the neighboring relationship between two modalities and enabling practical greedy routing in ANNS is a non-trivial task. Naively interconnecting nearest neighbors for both types of nodes can only cause excessively high degrees, harming search efficiency.\n(2) Searching on a bipartite graph entails traversing through query nodes, and maintaining high out-degrees (e.g., \u2265 100) of query nodes is essential for base data coverage. This leads to heavy overheads at each hop and an increased number of visited nodes along the routing path, slowing the search. Because each base node visiting involves fetching vector data from the main memory to the CPU and calculating its distance to the query, both of which are costly in the node-visiting operation [10]. The more nodes are visited, the more substantial memory access and computational burden are imposed. Therefore, node degrees need to be further reduced while preserving navigable paths for search routing. Additionally, the memory consumption of a bipartite graph is increased due to the inclusion of two node types and edges.\n(3) Limitations of reachability and connectivity among base nodes arise when utilizing queries to guide the graph construction. Solely depending on the out-neighbors of query nodes in the bipartite graph to cover the entire base data becomes challenging, resulting in isolated nodes or components and impacting search efficiency."}, {"title": "4.2 Design and Implementation", "content": ""}, {"title": "4.2.1 Overview", "content": "RoarGraph is proposed to address the aforementioned challenges and deliver superior performance in cross-modal ANNS. The construction of RoarGraph can be outlined in three steps, as presented in Figure 8(c-e).\nIn the initial step, we utilize the Query-Base Bipartite Graph (Figure 8(c)) to unify queries (query nodes) and base data (base nodes) within the same data structure. Prior to the bipartite graph, ground truths of queries vectors are computed during preprocessing (Figure 8(a)). After that, We establish edges from each query to its $N_q$ nearest neighbors and then edges from base nodes to queries are added under a restrictive constraint that only the closest base node of its in-neighbor can link to the query (blue arrow in Figure 8(c)). This approach achieves two primary goals: 1) creating a closeness mapping between queries and the base data and 2) reducing out-degrees of base nodes to enhance search efficiency on the bipartite graph, thereby addressing challenge 1.\nSecondly, we propose a technique named Neighborhood-Aware Projection to project the bipartite graph onto base data effectively. Before projecting, a degree limitation is imposed on every node. For each query node, we choose a connected base node as the pivot to select neighbors from the out-neighbors of the corresponding query node. Following the selection of the closest node to the pivot, we select farther nodes than the selected ones iteratively. Through Neighborhood-Aware Projection, we remove the query nodes but keep the neighboring relationship obtained from the query distribution in the projected graph (Figure 8(d)). It lowers the average degree of the graph and makes the projected graph become navigable [20, 52]. Consequently, the number of visited nodes during the search is reduced and thus resolves challenge 2.\nIn the final step, we apply Connectivity Enhancement to the projected graph to address the challenges of collecting isolated nodes, handling separated graph components, and introducing more alternative paths between nodes (solving challenge 3). We traverse every node in the projected graph using beam search, incorporating proximate nodes as diverse supplementary neighbors to each node with an additional degree budget. This process enhances the connectivity and reachability of the graph, completing the RoarGraph index construction (Figure 8(e)).\nSubsequently, we will provide a detailed introduction to the design and each technical optimization."}, {"title": "4.2.2 Query-Base Bipartite Graph", "content": "We build the query-base bipartite graph, which functions as a unified container, to establish a neighboring map between the distributions of base data and queries. Both queries (T) and base data (X) contribute to the formation of the bipartite graph as two distinct node types: query nodes and base nodes. Two kinds of directed edges need to be established: 1) edges from base nodes to query nodes and 2) edges from query nodes to base nodes.\nFirst of all, to build the bipartite graph that can recognize the proximity of the base data from the view of queries, we establish edges from query nodes to base nodes. We add directed edges from each query node to their $N_q$ nearest neighbors (base nodes) in the base data. It is essential to maintain out-degrees of query nodes ($N_q$ = 3 in Figure 8(b)) at a larger value to 1) enlarge coverage of base data and sufficiently model the neighboring relationship within the base data by queries and 2) ensure the overlapping of queries' out-neighbors, making a majority of base nodes within the bipartite graph reachable during search.\nSecond, to connect base nodes to query nodes, we tried a simplistic strategy that turns existing directed edges into bidirectional ones, assigning base nodes a degree d = $N_q$. However, this approach would require checking neighbors' neighbors ($N_q$ nodes) at each step of the search in the bipartite graph, as explained in Section 4.1, which is inefficient. Instead, we propose maintaining $N_q$ links from each query node to its nearest neighbors (see Algorithm 1, line 3) and reducing d, making the process more practical. Aligning with our goal to minimize d and adhere to the design goal of modeling closeness relationship, we choose x as the nearest base node among $N_q$ out-neighbors for each query node and add an edge from x to its corresponding query node. This strategy reduces d to 1 and forms the bipartite graph. Concurrently, we remove the link from the query node to x, i.e., $t_c \\rightarrow x$ in Algorithm 1, lines 4-6.\nFigure 8(b-c) illustrates the query-base bipartite graph construction. In this example, with $N_q$ = 3, each query node in the built bipartite graph has two out-neighbors, while the out-degrees of the other base nodes are one. Figure 8(c), node C serves as x in Algorithm 1 for query node $Q_1$. Node G is isolated due to degree limit and its insufficient closeness to query nodes, a phenomenon commonly observed in real-world datasets [5, 14, 61]."}, {"title": "4.2.3 Neighborhood-Aware Projection", "content": "Despite the bipartite graph's high memory consumption, we find that searching on the query-base bipartite graph is inefficient because routing through query nodes needs a long search path, and there are too many nodes visited along the search path (about $N_q$ base nodes at each hop). To address the challenge, we propose to project the bipartite graph onto the base nodes. Although a naive bipartite graph projection approach that fully connects nodes sharing common neighbors [87] could exclude query nodes, it is undesirable for a graph index due to failing the objective of reducing degrees. Therefore, we propose Neighborhood-Aware Projection to eliminate query nodes from the bipartite graph while preserving the neighborhood relationships of base nodes identified by the query nodes. In Figure 8(d), the graph is projected, and the out-neighbors of query nodes are linked with pivots C, D, and E. The edges D \u2192 Band F \u2192 D are not established due to degree bound.\nWe illustrate the projection with Figure 9. Let the query node function as a bridge (green node), with the incoming neighbor of a query node designated as the pivot (Node P) responsible for selecting its neighbors during projection. Numerous proximal yet irrelevant grey nodes are filtered in the current projection. For each pivot, out-neighbors of its bridges become potential neighbors of the pivot (note that neighbors' neighbors of base nodes are also base nodes). These potential candidates, representing the nearest neighbors to a query but distanced from each other (demonstrated in Section 3), are placed into a Candidates queue with a capacity of L and ranked by distances to the pivot (Algorithm 2 lines 5-6).\nNext, the closest node in the queue is selected as an out-neighbor for the pivot (Algorithm 3 line 2, Figure 9(b)), followed by acquiring up to degree limit (M) neighbors from the Candidates queue for each pivot. The essence of acquiring neighbors is that a candidate is excluded from the pivot's out-neighbor list if it is closer to any existing neighbor than to the pivot (Algorithm 3 line 4). This strategy includes more distant nodes among candidates with the goal of establishing pathways for spatially scattered base nodes that are in close proximity from the queries' perspective. In Figure 9(c), we perceive node Y is likely to be reached through node X and Z is harder to find because the Y is relatively closer to X, which is already connected to the pivot P. So X and Z become neighbors of P within the degree limit.\nTo maximize the knowledge from query distribution, we will fulfill out-neighbors within the M degree limitation during projection (Algorithm 3 line 8). For example, although node Y was previously filtered, including it in the fulfill operation with degree limit = 3 ensures no degree budget is wasted. After acquiring neighbors for each pivot, we also check if the pivot can establish reverse links to its in-neighbors (Algorithm 2 line 9)."}, {"title": "4.2.4 Connectivity Enhancement", "content": "The projected graph preserves information from the query distribution. Nevertheless, it inadequately provides appropriate reachability and connectivity by only relying on the coverage from the bipartite graph, which is crucial for greedy routing [20, 57"}]}