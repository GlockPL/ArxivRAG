{"title": "Neural Cover Selection for Image Steganography", "authors": ["Karl Chahine", "Hyeji Kim"], "abstract": "In steganography, selecting an optimal cover image-referred to as cover selec- tion-is pivotal for effective message concealment. Traditional methods have typically employed exhaustive searches to identify images that conform to specific perceptual or complexity metrics. However, the relationship between these metrics and the actual message hiding efficacy of an image is unclear, often yielding less-than-ideal steganographic outcomes. Inspired by recent advancements in generative models, we introduce a novel cover selection framework, which involves optimizing within the latent space of pretrained generative models to identify the most suitable cover images, distinguishing itself from traditional exhaustive search methods. Our method shows significant advantages in message recovery and image quality. We also conduct an information-theoretic analysis of the generated cover images, revealing that message hiding predominantly occurs in low-variance pixels, reflecting the waterfilling algorithm's principles in parallel Gaussian channels. Our code can be found at https://github.com/karlchahine/Neural-Cover-Selection-for- Image-Steganography.", "sections": [{"title": "1 Introduction", "content": "Image steganography embeds secret bit strings within typical cover images, making them imperceptible to the naked eye yet retrievable through specific decoding techniques. This method is widely applied in various domains, including digital watermarking (Cox et al. [2007]), copyright certification (Bilal et al. [2014]), e-commerce (Cheddad et al. [2010]), cloud computing (Zhou et al. [2015]), and secure information storage (Srinivasan et al. [2004]).\nTraditionally, hiding techniques such as modifying the least significant bits have been effective for embedding small data volumes up to 0.5 bits per pixel (bpp) (Fridrich et al. [2001]). Leveraging advancements in deep learning, recent approaches employ deep encoder-decoder networks to embed and extract up to 6 bpp, demonstrating significant enhancements in capacity (Chen et al. [2022], Baluja [2017], Zhang et al. [2019]). The encoder takes as input a cover image x and a secret message m, outputting a steganographic image s that appears visually similar to the original x. The decoder then estimates the message m from s. The setup is illustrated in Fig. 1 (left).\nThe effectiveness of steganography is significantly influenced by the choice of the cover image x, a process known as cover selection. Different images have varying capacities to conceal data without detectable alterations, making cover selection a critical factor in maintaining the reliability of the steganographic process (Baluja [2017], Yaghmaee and Jamzad [2010]).\nFrom a theoretical standpoint, numerous studies have employed information-theoretic analyses to investigate cover selection and determine the capacity limits of information-hiding systems, thereby identifying the maximum number of bits that can be embedded (Moulin et al. [2000], Cox et al. [1999], Moulin and O'Sullivan [2003]). For instance, in Moulin and O'Sullivan [2003], the steganographic setup is conceptualized as a communication channel where the cover image x acts as side information. However, such models are based on impractical assumptions: firstly, the steganographic process is additive-where the message m is simply added to the cover x; and secondly, it presupposes that the cover elements adhere to a Gaussian distribution.\nFrom a practical standpoint, existing techniques for cover selection predominantly rely on exhaustive searches to identify the most suitable cover image. These methods evaluate a variety of image metrics to determine the best candidate from a database. Some strategies include counting modifiable discrete cosine transform (DCT) coefficients to select images with a higher coefficient count for covers (Kharrazi et al. [2006]), assessing visual quality to determine embedding suitability (Evsutin et al. [2018]), and estimating the embedding capacity based on image complexity metrics (Yaghmaee and Jamzad [2010], Wang and Zhang [2019]).\nTraditional methods for selecting cover images have three key limitations: (i) They rely on heuristic image metrics that lack a clear connection to steganographic effectiveness, often leading to suboptimal message hiding. (ii) These methods ignore the influence of the encoder-decoder pair on the cover image choice, focusing solely on image quality metrics. (iii) They are restricted to selecting from a fixed set of images, rather than generating one tailored to the steganographic task, limiting their ability to find the most suitable cover.\nRecent progress in generative models, such as Generative Adversarial Networks (GANs) (Goodfellow et al. [2020]) and diffusion models (Song et al. [2020], Ho et al. [2020]), have ignited significant interest in the area of guided image generation (Shen et al. [2020], Avrahami et al. [2022], Brooks et al. [2023], Gafni et al. [2022], Kim et al. [2022]). Inspired by these innovations, we propose a novel approach that addresses the aforementioned limitations by treating cover selection as an optimization problem.\nIn our proposed framework, a cover image x is first inverted into a latent vector, which is then passed through a pretrained generative model to reconstruct the cover image. This image is processed by a neural steganographic encoder to embed a secret message, followed by a decoder to recover the message. We optimize the latent vector to generate an enhanced cover image x*, minimizing message recovery errors while preserving the visual and semantic integrity of the image. Fig. 1 (right) presents message recovery errors for randomly selected images before and after optimization. Our approach of optimizing the cover image uncovers a novel way to analyze the transformation from x to x*, revealing that the encoder embeds messages in low-variance pixels, analogous to the water-filling algorithm in parallel Gaussian channels. To the best of our knowledge, this is the first work that examines neural steganographic encoders by framing cover selection as a guided image reconstruction problem."}, {"title": "Our contributions are outlined as follows:", "content": "Framework. We describe the limitations of current cover selection methods and introduce a novel, optimization-driven framework that combines pretrained generative models with steganographic encoder-decoder pairs. Our method guides the image generation process by incorporating a message recovery loss, thereby producing cover images that are optimally tailored for specific secret messages (Section 3).\nExperiments. We validate our methodology through comprehensive experimentation on public datasets such as CelebA-HQ, ImageNet, and AFHQ. Our results demonstrate that the error rates of the optimized images are an order of magnitude lower than those of the original images under specific conditions. Impressively, this optimization not only reduces error rates but also enhances the overall image quality, as evidenced by established visual quality metrics. We explore this intriguing phenomenon by examining the correlation between image quality metrics and error rates (Section 3.3).\nInterpretation. We investigate the workings of the neural encoder and find it hides messages within low variance pixels, akin to the water-filling algorithm in parallel Gaussian channels. Interestingly, we observe that our cover selection framework increases these low variance spots, thus improving message concealment (Section 4).\nPractical considerations. We extend our guided image generation process to practical applications, demonstrating its robustness against steganalysis and resilience to JPEG compression, as detailed in Section 5."}, {"title": "2 Preliminaries", "content": "Image steganography aims to hide a secret bit string \\(m \\in \\{0,1\\}^{H\\times W \\times B}\\) into a cover image \\(x \\in [0,1]^{H\\times W \\times 3}\\) where the payload B denotes the number of encoded bits per pixel (bpp) and H, W denote the image dimensions. As depicted in Fig. 1 (left), the hiding process is done using a steganographic encoder Enc, which takes as input x and m and outputs the steganographic image s which looks visually identical to x. A decoder Dec recovers the message, \\(m = Dec(s)\\) with minimal error rate \\(||m-m||_0\\over{HXWXB}\\).\nCover selection involves generating the ideal cover image x, to achieve three primary objectives: (i) minimize the error rate as defined above, (ii) ensure that the steganographic image s visually resembles x as closely as possible, and (iii) maintain the integrity of the cover image x using established perceptual quality metrics.\nDenoising Diffusion Implicit Models (DDIMs) (Song et al. [2020]) are a class of generative models that learn the data distribution by adopting a two-phase mechanism. The forward phase incorporates noise into a clean image, while the backward phase incrementally removes the noise. The formulation for the forward diffusion in DDIM is presented as:\n\\(x_t = \\sqrt{a_t} x_{t-1} + \\sqrt{1 - a_t} \\epsilon, \\ \\epsilon \\sim \\mathcal{N}(0, 1),\\)\nwhere \\(x_t\\) is the noisy image at the t-th step, \\(a_t\\) is a predefined variance schedule, and t spans the discrete time steps from 1 to T. The DDIM's backward sampling equation is:"}, {"title": "3 Methodology", "content": "We propose two cover selection methodologies using pretrained Denoising Diffusion Implicit Models (DDIM) and pretrained Generative Adversarial Networks (GAN) (Sections 3.1, 3.2), and compare the performances of the two approaches (Section 3.3). Detailed descriptions of the training procedures are in Appendix B. Broadly speaking, starting with a cover image x randomly selected from the dataset, we gradually optimize this image to minimize the loss \\(||m \u2013 m||\\). Intriguingly, while our primary focus is on reducing the error rate, we observe that all three objectives of cover selection outlined in Section 2 are concurrently achieved. We investigate this phenomenon in Section 3.3."}, {"title": "3.1 DDIM-based cover selection", "content": "As depicted in Fig. 2, our DDIM approach consists of two steps. We get inspired from DDIM inversion, which refers to the process of using DDIM to achieve the conversion from an image to a latent noise and back to the original image (Kim et al. [2022]).\nStep 1: latent computation. The initial cover image \\(x_0\\) (where the subscript denotes the diffusion step) goes through the forward diffusion process described in Eq. 3 to get the latent \\(x_T\\)."}, {"title": "4 Analysis", "content": "In this section, we explore the reasons behind the enhanced performance achieved by our framework. Initially, we analyze the behavior of the pretrained steganographic encoder (Section 4.1). Our observations indicate that the encoder preferentially embeds messages within pixels of low variance. To validate these findings, we compare the encoder's behavior with the waterfilling technique applied to parallel Gaussian channels (Section 4.2). Lastly, we demonstrate that the cover selection optimization effectively increases the presence of low variance pixels. This adjustment equips the encoder with greater flexibility to hide messages, thereby improving overall performance (Section 4.3). We present the results for the ImageNet Robin class with a payload of B = 4 bpp. Additional results for various classes and datasets are presented in Appendix D."}, {"title": "4.1 Encoding in low-variance pixels", "content": "We begin by investigating the underlying mechanism of the pretrained steganographic encoder (Chen et al. [2022]). We hypothesize that the encoder preferentially hides messages in regions of low pixel variance. To test this hypothesis, we structure our analysis into two steps.\nStep 1: variance analysis. In Fig. 3 (top), we illustrate the variance of each pixel position for the three color channels, calculated across a batch of images and normalized to a range between 0 and 1, as detailed in Appendix D. The plot reveals significant disparities in variance, with certain regions displaying notably lower variance compared to others.\nStep 2: residual computation. Using the same batch of images, we pass them through the steganographic encoder to obtain the corresponding steganographic images. We then compute the residuals by calculating the absolute difference between the cover and steganographic images and averaging these differences across the batch. This process yields three maps, one for each color channel, which are subsequently normalized to a range between 0 and 1. Those maps are plotted in Fig. 3 (bottom).\nAs shown in Fig. 3, we observe correlations between the variance and the magnitude of the residual values; where pixels with lower-variance tends to have higher residual magnitudes. To quantify this observation, we introduced a threshold value of 0.5. In the residual maps (from Step 2), locations exceeding this threshold are classified as \u201chigh-message regions\" and assigned a value of 1. Conversely, locations in the variance maps (from Step 1) falling below this threshold are defined as \"low-variance regions\", also set to 1. We discovered that 81.6% of the high-message regions coincide with low-variance pixels. This substantial overlap confirms our hypothesis and underscores the encoder's tactic of utilizing low-variance areas to embed messages.\nThe encoder's strategy of selectively embedding message bits in low-variance pixel locations is akin to the waterfilling technique employed in parallel Gaussian channels, a fundamental concept in communication theory (Cover [1999]). This method optimizes the allocation of power across channels to maximize channel capacity under power constraints. In the subsequent section, we delve deeper into this analogy and further demonstrate the relationship between these two processes."}, {"title": "4.2 Analogy to waterfilling", "content": "To validate the findings presented in Section 4.1, we draw parallels between our analysis and the waterfilling problem for Gaussian channels. We consider a simple additive steganography scheme: \\(s_i = x_i + \\gamma_i m_i\\), for \\(i = 1, 2, ..., N\\), where \\(N = H \\times W \\times 3\\) is the image dimension, \\(m_i = \\{-1,1\\}\\) indicates the i-th message to be embedded, \\(\\gamma_i\\) its corresponding power, \\(x_i\\) and \\(s_i\\) represent the i-th element of the cover and steganographic images respectively. We assume a power constraint P that restricts the deviation between the cover and steganographic images: \\(E[\\sum_{i=1}^{N} (s_i - x_i)^2] \\le P\\).\nThis formulation is similar to the waterfilling solution for N parallel Gaussian channels (Cover [1999]), where the objective is to distribute the total power P among the N channels so as to maximize the capacity C, which is maximum rate at which information can be reliably transmitted over a channel, defined as: \\(C = \\sum_{i=1}^{N} \\log_2(1 + \\frac{\\gamma_i^2}{\\sigma_i^2})\\), where \\(\\sigma_i^2\\) is the variance of \\(x_i\\). The problem can be formulated as a constrained optimization problem, where the optimal power allocation is given by \\(\\gamma_i^2 = (\\lambda - \\sigma_i^2)^+\\), where \\((x)^+ = max(x, 0)\\) and \\(\\lambda\\) is chosen to satisfy the power constraint.\nWe calculate \\({\\sigma_i^2}\\)^{3\\times H \\times W}\\) using a batch of images, and find the optimized \\({\\gamma_i}\\)^{3\\times H \\times W}\\) using the approach described above. We plot the \\(\\gamma_i\\)'s for each color channel in Fig. 4.\nWe observe a degree of similarity when comparing with Fig. 3 (bottom). To quantitatively assess this resemblance across color channels, we quantize the three matrices by setting values greater than 0.5 to 1 and values less than 0.5 to 0. For each channel, the similarity is calculated using the equation"}, {"title": "4.3 Impact of cover selection", "content": "A natural question becomes: what is the cover selection optimization doing? We plot the variance maps of the optimized cover images in Fig. 5.\nWe notice that the number of low variance spots significantly increased as compared to Fig. 3 (top), meaning that the encoder has more freedom in encoding the secret message. Quantitatively, we find that 92.4% of the identified high-message positions are encoded in low-variance pixels, as compared to 81.6% before optimization. Given that the encoder preferentially embeds data in these low variance areas, this increase provides greater flexibility for data embedding, thereby explaining the performance gains observed in our framework."}, {"title": "5 Practical settings", "content": "In this section, we adapt our framework for practical considerations. We evaluate its performance across different payloads (Section 5.1), adapt it for JPEG compression (Section 5.2), and confirm security against steganalysis (Section 5.3). Computational times are detailed in Appendix I. We use two datasets, CelebA-HQ (Karras et al. [2017]) and AFHQ-Dog (Choi et al. [2020]), using the same settings described in Section 3.3."}, {"title": "5.1 Payload impact on performance", "content": "We explore different payload capacities B, highlighted in Table 2. We show the results for B = 1, 2, 3, 4 bits per pixel (bpp). DDIM-optimized images show error rates significantly lower than originals, with image quality metrics like BRISQUE, SSIM, and PSNR largely preserved, though some quality decline was noted at lower bpp levels in CelebA-HQ and AFHQ-Dog. We include sample generared cover images generated using the DDIM framework in Appendix E. Despite experimenting with various regularization techniques aimed at maintaining image quality, no noticeable improvement was observed (Appendix C). Considering this, extending our framework to explore novel regularization techniques for such payload capacities is an interesting future direction. We also provide example cover and steganographic images generated by the LISO framework under different payload values in Appendix F."}, {"title": "5.2 JPEG compression", "content": "Robustness against lossy image compression is crucial for steganography. We extend our framework to accommodate JPEG compression (Wallace [1991]). Following Athalye et al. [2018], we implement an approximate JPEG layer where the forward pass executes standard JPEG compression, while the backward pass operates as an identity function. Once the encoder-decoder pair is trained, we generate a JPEG-compliant cover image following the framework described in Section 3.1, augmented by adding a JPEG layer post-encoding. In Table 3, we demonstrate that our framework achieves improved error rates for B = 1 bpp, thereby validating our approach's capability to optimize cover images under JPEG compression constraints. In addition, we show robustness results to Gaussian noise in Appendix K."}, {"title": "5.3 Steganalysis", "content": "Steganalysis systems are designed to detect whether there is hidden information within images. As these tools evolve, neural steganography techniques now integrate these systems into their end-to-end pipelines to create images that can bypass detection (Chen et al. [2022], Shang et al. [2020]). We show our results in Table 4 on the AFHQ-Dog dataset. Following the approach in Chen et al. [2022], we evaluate the security of our optimized images by measuring the detection rate using the steganalysis tool XuNet (Xu et al. [2016]) and also recorded message recovery error rates. The image quality metrics, such as BRISQUE, SSIM, and PSNR, are comparable to those listed in Table 2 and have therefore been omitted for brevity. We explore two different scenarios:\nScenario 1: In this scenario, the experimental setup remains the same as described in Section 3.1 and illustrated in Fig. 2. The steganographic encoder-decoder pair is trained without regularizers to evade steganalysis detection. The DDIM-optimized images exhibit comparable detection rates at payloads of B = 1 and B = 4, superior performance at B = 2, and inferior performance at B = 3, all while achieving significantly lower error rates. While it is puzzling that detection rates do not consistently decrease with lower payload size, this phenomenon is also observed in LISO Chen et al. [2022], on which our framework is built. We provide a more detailed discussion in Appendix J.\nScenario 2: We leverage the differentiability of XuNet as described in Chen et al. [2022]. During the optimization of the steganographic encoder-decoder pair, we introduce an additional loss term to account for steganalysis. This adjustment leads to a notable reduction in detection rates across all payload sizes, while maintaining consistently low error rates for both original and DDIM-optimized images. Notably, DDIM-optimized images exhibit even lower detection and error rates compared to the original images, demonstrating superior performance."}, {"title": "6 Conclusion", "content": "We propose a novel cover selection framework for steganography leveraging pretrained generative models. We demonstrate that by carefully optimizing the latent space of these models, we generate steganographic images that exhibit high visual quality and embedding capacity. Additionally, our information-theoretic analysis shows that message hiding predominantly occurs in low-variance pixels, reflecting the waterfilling algorithm's approach to parallel Gaussian channels. Our framework is versatile, allowing for the incorporation of further constraints to produce JPEG-resistant steganographic images or to evade detection by particular steganalysis systems. For future work, we aim to expand our analysis (Section 4.2) to draw similarities with correlated Gaussian channels, moving beyond the independent channels considered in this work."}, {"title": "A Learned Iterative Steganography Optimization (LISO)", "content": "LISO (Chen et al. [2022]) advances the method established in Kishore et al. [2021], which is centered around Optimization-based Image Steganography. Leveraging a differentiable decoder equipped with either randomly initialized or pretrained weights (as referenced in the preceding paragraph), Kishore et al. [2021] formulates the steganography encoding as an optimization task for each sample. This approach is similar to the generation of adversarial perturbations as discussed in Szegedy et al. [2013]. Specifically, the technique described in Kishore et al. [2021] seeks to compute a steganographic image by addressing a constrained optimization problem that ensures the perturbed image remains within the bounds of the \\([0, 1]^{H\\times W\\times 3}\\) hypercube.\nwhere\nwhere m represents the secret message, \\(m\\) is the decoded message, x is the cover image, and s is the steganographic image. The operation \\(\\langle\\cdot\\rangle\\) signifies the dot product, \\(\\lambda\\) is a scaling factor, and \\(N = H\\times W\\times 3\\) represents the total number of pixels in the image, with H and W being the height and width of the image, respectively. The accuracy loss, \\(L_{acc}(m, \\hat{m})\\), is calculated using binary cross entropy to minimize the distance between the estimated and actual messages, while the quality loss, \\(L_{qua}(s, x)\\), employs mean squared error to ensure the steganographic image closely resembles the cover image. This objective function is represented as l(x, m). To solve the optimization problem outlined above, various solvers can be utilized and as shown in Algorithm 1, with iterative, gradient- based algorithms. In Algorithm 1, \\(\\eta > 0\\) is the step size, and g(\\(\\cdot\\)) describes the update function specific to the optimization method used. The perturbation \\(\\delta\\) is iteratively adjusted to minimize the loss l while adhering to the pixel constraints of the image.\nIn LISO, The function g(\\(\\cdot\\)) in Algorithm 1 is approximated using a fully convolutional network designed around a gated recurrent unit (GRU). The complete LISO framework, which includes the iterative encoder, decoder, and critic, undergoes end-to-end training on a diverse image dataset. Similar to the training process of Generative Adversarial Networks (GANs), the training of LISO alternates between optimizing the critic and the encoder-decoder networks. Throughout this training phase, losses for all intermediate updates are calculated with exponentially increasing weights (\\(\\gamma^{T-t}\\) at step t). With intermediate predictions denoted as \\(\\hat{m}_1, . . ., \\hat{m}_T\\) the loss is:\nwhere \\(\\gamma \\in (0, 1)\\) is a decay factor and \\(L_{crit}\\) denotes the critic loss to generate real-looking images (with weight \\(\\mu > 0\\))."}, {"title": "B Training details", "content": "B.1 GAN-based cover selection\nIn our GAN-based cover selection method, we utilize the BigGAN generator (Brock et al. [2018]) and a LISO encoder-decoder pair (Chen et al. [2022]), both pretrained on the ImageNet dataset"}, {"title": "B.2 DDIM-based cover selection", "content": "In our cover selection method based on Denoising Diffusion Implicit Models (DDIM), we employ three DDIM models alongside LISO encoder-decoder pairs, each pretrained on different datasets: ImageNet (Russakovsky et al. [2015]), AFHQ-Dog (Choi et al. [2020]), and CelebA-HQ (Karras et al. [2017]).\nFollowing the procedure outlined in Section 3.1, we initiate the process by sampling a random image \\(x_0\\). We then execute the deterministic forward DDIM process over T steps, with each step defined as follows:\nAfter obtaining the latent representation \\(x_T\\), we initiate the stochastic reverse DDIM process, which spans E epochs. Within each epoch, we perform the reverse DDIM process on the acquired latent for N iterations. Each iteration proceeds as follows:\nWhere \\(\\epsilon_{\\theta}\\) is a pretrained network, \\(f_{\\theta}\\) is a function of \\(\\epsilon_{\\theta}\\), \\(\\sigma_t = \\eta \\sqrt{\\frac{(1 - \\alpha_{t-1})}{(1 - \\bar{\\alpha}_t)}} \\cdot \\epsilon_{\\theta}(x_t, t)\\), and \\(\\epsilon \\sim \\mathcal{N}(0, 1)\\).\nWe configure our model with the following parameters: E 50 epochs, T = 40 time steps, and N = 6 iterations per epoch. For optimization, we employ the Adam optimizer with a learning rate of \\(2E^{-06}\\). The variance schedule that determines \\(\\bar{\\alpha_t}\\) and \\(\\bar{\\alpha_{t-1}}\\), as well as the DDIM architectures, are consistent with those described in Kim et al. [2022]."}, {"title": "C Regularization effect", "content": "Despite testing several regularization methods intended to preserve image quality\u2014including total variation (Rudin et al. [1992]), edge preservation (Perona and Malik [1990]), feature matching with a pre-trained VGG network (Gatys et al. [2015]), and a classic l\u2081 distance between the original and updated cover images-we observed no significant enhancements. These results are shown in Table 5."}, {"title": "D Encoding operation analysis: additional results", "content": "In this section, we further describe the encoder's strategy of embedding messages in regions with low pixel variance, as described in Section 4.1."}, {"title": "J Steganalysis: detailed settings and additional experiments", "content": "We adopt the simulation settings outlined in Chen et al. [2022] for our experiments. In Scenario 1, the steganography model M is trained without specific techniques to avoid detection by steganalysis. We assume the attacker, who performs steganalysis, knows the architecture of M but has no access to its weights, training data, or hyperparameters. However, the attacker can train a surrogate model M' to generate their own steganographic images. To simulate this scenario, we trained a steganalysis model on the CelebA dataset and used it to detect steganographic images generated from the AFHQ- Dog dataset. Interestingly, detection rates did not consistently decrease with lower payload sizes, a phenomenon also noticed in LISO Chen et al. [2022], on which our framework is based. We hypothesize this behavior arises from the distributional mismatch between training and testing data, as discussed earlier. In scenario 2, We leverage the fact that neural steganalysis methods are entirely differentiable, and that LISO uses gradient-based optimization. This allows us to reduce security risk by incorporating an additional loss term from the steganalysis system into the LISO optimization process. Specifically, during evaluation, if an image is identified as steganographic, we add the logit value of the steganographic class to the loss function.\nIn addition to XuNet (Xu et al. [2016]), we compute the steganalysis results of SRNet, another state- of-the-art steganalysis system (Boroumand et al. [2018]). The results of both schemes are compared in Table 7. Our observations indicate that the images generated by our framework effectively resist steganalysis by SRNet. This is evidenced by the significant drop in detection rate when transitioning from scenario 1 to scenario 2. As a reminder, in scenario 2, we exploit the differentiability of the steganalyzer (SRNet) and incorporate an additional loss term to account for steganalysis."}, {"title": "K Robustness to Gaussian noise", "content": "In this section, we evaluate the robustness of our DDIM-based approach to Gaussian noise. The experimental setup remains the same as described in Section 3.1 and illustrated in Fig. 2, with the only modification being the injection of Gaussian noise, distributed as \\(\\mathcal{N}(0, \\beta)\\), into the output of the steganographic encoder. The decoder subsequently processes the noisy steganographic image to estimate the embedded message. Results of this experiment are presented in Table 8. Our findings demonstrate that the proposed framework produces cover images resilient to Gaussian noise,"}]}