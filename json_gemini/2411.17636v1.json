{"title": "MALMM: Multi-Agent Large Language Models\nfor Zero-Shot Robotics Manipulation", "authors": ["Harsh Singh", "Rocktim Jyoti Das", "Mingfei Han", "Preslav Nakov", "Ivan Laptev"], "abstract": "Large Language Models (LLMs) have demonstrated remarkable planning abilities across various domains, including robotics manipulation and navigation. While recent efforts in robotics have leveraged LLMs both for high-level and low-level planning, these approaches often face significant challenges, such as hallucinations in long-horizon tasks and limited adaptability due to the generation of plans in a single pass without real-time feedback. To address these limitations, we propose a novel multi-agent LLM framework, Multi-Agent Large Language Model for Manipulation (MALMM) that distributes high-level planning and low-level control code generation across specialized LLM agents, supervised by an additional agent that dynamically manages transitions. By incorporating observations from the environment after each step, our framework effectively handles intermediate failures and enables adaptive re-planning. Unlike existing methods, our approach does not rely on pre-trained skill policies or in-context learning examples and generalizes to a variety of new tasks. We evaluate our approach on nine RLBench tasks, including long-horizon tasks, and demonstrate its ability to solve robotics manipulation in a zero-shot setting, thereby overcoming key limitations of existing LLM-based manipulation methods.", "sections": [{"title": "I. INTRODUCTION", "content": "Robotic manipulation has seen impressive advancements, enabling agents to handle increasingly complex tasks with greater precision and efficiency [1], [2]. Current solutions, however, often struggle with generalization, particularly when trained using imitation learning [3], [4]. Agents trained in this manner typically excel at specific tasks but lack the adaptability to handle new tasks. One major drawback of imitation learning is the labor-intensive and time-consuming process for data collection [5], which limits the scalability of these systems. Moreover, training agents using policy-based methods for even a single task requires thousands of episodes [6], making the approach computationally expensive and inefficient. Effective manipulation agents, however, should demonstrate a deeper understanding of their environment [7]. This involves not only recognizing and grounding relevant objects but also understanding the relationships between them [8]. Equipped with this knowledge, agents can then plan and execute actions more efficiently, adapting to changes in their environment or task requirements.\nRecent advancements in Large Language Models (LLMs) have demonstrated remarkable generalization and reasoning capabilities across diverse domains such as common-sense [9], mathematical [10], and symbolic reasoning [11]. These models, particularly when scaled to billions of parameters [12], exhibit emergent abilities to break down complex tasks into simpler steps through structured reasoning techniques like \u201cchain of thought\u201d [13] and prompting [14]. LLMs have already shown promise in high-level planning tasks across fields, suggesting their potential for solving similar challenges in robotics manipulation by offering more flexible, generalizable solutions. This opens new possibilities for improving manipulation systems beyond their current limitations.\nRecent LLM-based approaches to robotics planning have shown promise but also face challenges. A major issue is the tendency of LLMs [15] to produce hallucinations, especially in long-horizon tasks where precision and adaptability are essential. This issue arises from their difficulty in managing lengthy conversations which is common in long-horizon manipulation tasks [16]. As a result, LLMs may disregard geometric constraints, parameters of predefined functions, and can even forget the goal. Additionally, most LLM-based robotics manipulation methods [17], [18] generate entire plans in a single pass without incorporating environmental feedback during execution. This limitation prevents methods to dynamically adjust or replann in response to unexpected outcomes or intermediate failures.\nTo mitigate these issues, we here propose a Multi-Agent Large Language Model for Manipulation (MALMM) that leverage the collective intelligence and the specialized skills of multiple agents for complex manipulation tasks. Our approach incorporates agents dedicated to high-level planning, low-level code generation and a supervisor that oversees transitions between other agents and tools. We show that through the use of multiple specialized agents in a multi-"}, {"title": "II. RELATED WORK", "content": "Language instructions for robotics serve as a tool for compositional goal specification [19] and allow composition-ally scalable generalization to new tasks [20]. The literature around language grounding is vast, ranging from classical tools such as lexical analysis, formal logic, and graphical models to interpreting language instructions [19], [21], [22], [23]. Recently, there has been a lot of effort to utilize the impressive compositional understanding capability of Large Language Models to perform language grounding for robotics [20], [24], [25], [26]. Additionally, recent advancements use pre-trained large language models (LLMs) for their open-world knowledge, tackling more challenging"}, {"title": "B. LLM for Robotics", "content": "Language Models have been used for diverse applications in the field of robotics ranging from a reward function [27], [28] and data augmentation [29] for policy training, for task planning [17], [30], [31], [32], failure summarization to guide language based planner [33], and policy program synthesis [18]. VoxPoser [34] and Language to Rewards [35] uses LLM for generating reward regions for assisting external trajectory optimizers to compute trajectory. Our work relates to studies where LLM is used for generating plans for completing manipulation tasks. Most of these prior works [36], [32], [30], [34] relied on pre-trained skills, motion primitives and trajectory optimizers and focused primarily on high-level planning. The approach in [17] is closest to our work, it deploys a language model to generate high-level plans and uses its code generation capabilities to convert those high-level plans into low-level control. However, LLMs suffer from hallucinations affecting long-horizon task planning. In addition, [17] assumes correct execution of each step and does not account for occasional failures or unforseen changes of the environment. Our approach mitigates these issues using multiple specialized agents and intermediate environment feedback."}, {"title": "C. LLM-Based Multi-Agents in Robotics", "content": "Recently, numerous studies have focused on using LLMs for sequential decision-making [37], [38], [39] and complex reasoning tasks [40]. There is also an emerging field of using multiple agents driven by LLMs to solve complex tasks including robotics in a collaborative manner [41], [42], [43]. For most of the work in robotics, the LLM is used in multi-robot collaboration and communication. Additionally, all these work use RL policy agents which causes limits their generalization to new tasks and environments [44]. In contrast, we propose a multi-agent framework, MALMM, which uses three LLM agents in a zero-shot manner to perform robotics manipulation tasks."}, {"title": "III. METHOD", "content": "Large Language Models have recently emerged as a universal tool for a variety of tasks including robotic navigation and manipulation [45], [36], [34], [18], [17]. Such models exhibit surprising generalization abilities and support zero-shot solutions for new tasks. While several recent methods explored LLMs for high-level task planning [32], [31], [30], [36], other methods attempt to bring the power of LLMs all the way to the low-level trajectory planning [18], [17]. We follow this work and leverage LLMs for precise control of the end effector to solve complex and previously unseen manipulation tasks.\nTrajectory planning is a complex problem that requires reasoning about the precise shape, position and semantics of the objects as well as understanding object interactions. To solve this problem, LLM agents can be instructed to handle information about the objects and the robot by text prompts. To harness the complexity of the problem, we here propose to address the manipulation problem with multiple LLM agents. Inspired by recent work on multi-agent LLMs [46], we design agents specialized on different aspects of the manipulation problem and connect them into a network. Below, we describe our proposed Multi-Agent Large Language Model for Manipulation (MALMM) framework (Fig. 2) in detail."}, {"title": "A. Preliminaries", "content": "Our goal is to design a robotics system capable of solving previously unseen manipulation tasks defined by natural language instructions. We assume access to a robotics environment supporting manipulation actions of closing the gripper, opening the gripper, and changing the gripper's position and orientation. Observations of the environment are given either in the form of object and gripper 3D poses obtained from the simulator, or a 3D point cloud obtained from a depth camera. In the latter case, we estimate object poses and potential grasping poses from the point cloud. We also assume to have access to a modern LLM that can process language instructions together with environment observations in the form of text."}, {"title": "B. MALMM: Multi-Agent Manipulation", "content": "The core motivation of our work is to investigate how a multi-agent framework can leverage the collaborative and role-specific reasoning capabilities of multiple LLM agents to complete complex manipulation tasks. Below, we first introduce a single-agent architecture and then propose its multi-agent extensions. Single- and multi-agent architectures considered in our work are illustrated in Fig. 3.\nSingle Agent (SA). Prompting LLMs to interpret natural language instructions in the form of executable primitive actions has been shown successful for a variety of tasks including image understanding [47] and numerical reasoning [48]. In a similar fashion LLMs can be prompted to interpret embodied tasks, e.g., open a wine bottle and convert them into a sequence of primitive manipulation actions. A version of such a system with a single LLM agent is outlined in Fig. 3(a). Here a LLM is first prompted to break down the language instruction into a sequence of steps. It then uses its code generation capabilities [49] to translate each step into executable Python code, using predefined functions to control the end-effector. This code is then sent to a Python interpreter that executes the steps in the environment. After each step, LLM receives new observations from the environment and proceeds in a loop with planning and code generation until meeting termination criteria.\nMulti-Agent (MA). Manipulation tasks require prolonged interactions with the environment. LLMs, however, often struggle to manage long context conversations [16], leading to hallucinations. To address this issue, we propose two specialized LLM agents with shorter role-specific contexts: the Planner and the Coder, see Fig. 3(b). The Planner breaks down the language instructions into a sequence of manipulation steps, while the Coder iteratively translates these steps into the executable Python code. After each intermediate step, the Planner detects potential failures and re-plans according to new observations of the environment.\nMulti-Agent with Supervisor (MALMM). Our final multi-agent architecture (MALMM) extends MA with a Supervisor agent that coordinates the Planner, the Coder and the Code Executor, see Fig. 3(c). The Supervisor LLM decides which agent or tool to activate next based on input instructions, the roles of individual agents, environmental observations and the entire chat history of active agents."}, {"title": "C. Multi-Agent Prompting", "content": "Each agent is provided with a task-agnostic system prompt. The agents rely solely on their internal world knowledge for reasoning and decision-making. For prompt construction, we draw inspiration from [17] and its study of LLM based trajectory generation for robotics manipulation. Note, that in contrast to other recent work [18], [34] we do not provide agents with any examples for in-context learning and apply MALMM with no changes, i.e. in a zero-shot mode, to new tasks. Complete prompts used for LLM agents in our experiments are available from [50].\nEach agent's prompt is specifically designed to suit its role. Since LLMs require step-by-step reasoning to solve tasks, the Planner is prompted to generate steps that define the intermediate goals needed to complete the task. MALMM perceives the 3D environment using object poses provided either by the internal state of the simulator or by the analysis of visual observations, see Sec. III-D. Therefore, the Planner is given a detailed description of the environment's coordinate system, enabling it to interpret directions from the gripper's perspective. Given the limited exposure of LLMs to grounded physical interaction data in their training, LLM agents often fail to account for potential collisions. To address this, we include generic collision-avoidance rules in the Planner's prompts. Additionally, to handle intermediate failures, primarily due to collisions or missed grasping, the Planner is prompted to evaluate outcomes after each intermediate step and, if necessary, replan based on updated observations. The prompt used for the Planner agent is shown in Figure 8.\nThe prompt for the Coder, as shown in Figure 9, includes information about the expected input and output for all available functions-execute_trajectory(), open_gripper(), close_gripper(), and check_task_completion()\u2014as well as guidelines to avoid syntactic and semantic errors, which are common in code generated by LLMs. Finally, the Supervisor agent, as presented in Figure 7, is prompted to manage the workflow, coordinating the transitions between the LLM agents, the Planner the Coder, and the Code Executor to ensure successful task completion."}, {"title": "D. Environmental Observations", "content": "LLMs trained on textual inputs cannot directly perceive or interpret 3D environments. Our agents receive information about the environment either from the internal state space of a simulator or from visual observations.\nState space observations. In this setup LLM agents have direct access to the simulator state information. The observations are provided in the form of 3D bounding boxes, orientations and colors of scene objects as well as the 3D position, orientation and the open/close state of the gripper. The execution logs for empty container task presented in Appendix-E show input observation format as CURRENT ENVIRONMENT STATE for MALMM. The same format is also used for Single Agent and Multi-Agent presented in Section III-B.\nVisual observations. To approach real-world settings, we restrict observations to images and 3D point clouds obtained from RGB-D sensor and then use pretrained vision models to extract information about scene objects. To this end, we deploy LangSAM [51] to generate image segmentation masks for objects described by text, e.g. \"block\" or \"red jar\" in task instruction. We then segment 3D object point clouds by projecting 2D segmentation masks into 3D. To compute accurate object-centric grasping poses, we apply M2T2 [52] model to predict grasp poses for 3D object point clouds for the object identified for grasping, see details in Appendix-D. We use obtained gripper poses to control the robotics gripper during grasping. To facilitate object placement, we estimate the 3D bounding box of the target object in the environment based on the task instructions. For example, in the task close the red jar, the target object would be the red jar. We extract the 3D bounding box directly from the object's point cloud and use it to guide the placement process. Please refer to Appendix-D for details on obtaining visual observations."}, {"title": "IV. EXPERIMENTS", "content": "Our experiments are designed to assess whether MALMM can accurately solve a diverse set of manipulation tasks in a zero-shot manner."}, {"title": "A. Implementation Details.", "content": "We use gpt-4-turbo\u00b9 [54] to drive each of the three LLM agents in MALMM. For developing the multi-agent framework, we used AutoGen [55], which is an open-source"}, {"title": "E. Multi-Agent Ablation", "content": "We perform ablation for each of the components (agents) in MALMM to evaluate how each of them contributes to the overall performance. We consider two tasks, namely stack blocks and empty container and report results in Table II.\nWe first analyze the importance of intermediate environment feedback. To this end, we consider the Single Agent (SA) setting and remove the environmental feedback provided after each intermediate step. In this setup LLM generates the full manipulation plan at once and executes it without revisions. As shown in Table II, the Single Agent without environmental feedback exhibits 12% and 24% drop in performance for the 'stack blocks' and 'empty container' tasks respectively. By analyzing failure cases of both methods, we observed that the environmental feedback provided after each intermediate step crucially effects the agent's ability to detect unforeseen situations and to recover from failures such as collisions and inaccurate grasping.\nWe next validate the advantage of the Multi-Agent architecture with separate LLM agents for planning and code generation. As shown in Table II, the Multi-Agent system (MA), consisting of a dedicated Planner and Coder, demonstrates 16% and 12% performance improvement over SA for the two tasks respectively. This can be attributed to the inherent limitations of LLMs in managing very long context conversations [16]. In the SA setup, where a single LLM is responsible for both high-level planning and low-level code generation, the agent must handle an extensive context, particularly for long-horizon tasks. This often leads to errors such as failing to account for collisions with other objects, omitting input arguments for predefined functions, using variables before they are initialized, and even forgetting the specified goal. In contrast, the MA system mitigates these issues by dividing the workload among specialized agents. The Planner and Coder agents in the MA setting focus on specific roles through specialized prompts and communicate with each other reducing the likelihood of errors and hallucinations, in particular for longer tasks.\nOur initial Multi-Agent system pre-defines the cyclic sequence of the Planner, the Coder, and the Coder Executor, see Fig. 3(b), assuming each agent correctly completes its task. However, hallucinations may occur even within multi-agent systems [15]. For example, the Coder agent may miss the variable initialization resulting in compilation errors or incomplete sub-goal code generation, such as producing the code only for approaching the object without grasping it. In such situations, the Coder may need to be re-executed to correct for errors before passing control to the Planner. To automate this process, we introduce a Supervisor agent that dynamically re-routs the execution process to the next agent based on the input instruction, entire communication history of all active agents, and the role descriptions of all agents, rather than following a fixed sequence. This adaptive approach at the core of our MALMM method improves the performance of the dual-agent MA setup by 20% in the 'stack blocks' task and 16% in the 'empty container' task respectively, see Table II."}, {"title": "F. MALMM is Better at Long-Horizon Planning", "content": "To validate the effectiveness of MALMM in long-horizon tasks, we create three variations of the 'stack blocks' task, each with a different number of blocks, and compare the performance of MALMM to the Single Agent setup. Results in Fig. 5 indicate that while the Single Agent setup struggles with stacking 3 and 4 blocks, MALMM significantly outperforms SA, in particular for tasks that require longer planning."}, {"title": "G. Results for Vision-Based Observations", "content": "To approach real-world settings, where direct access to the environment states is not available, we next perform experiments restricted to vision-based observations in the form of 3D point clouds. We evaluate the performance of MALMM and compare it to the Single Agent setup on three tasks: 'put block', 'rubbish in bin', and 'close jar'. Consistently with our previous state-based experiments, results of MALMM in Table III significantly outperform the Single-Agent baseline across all three tasks and confirm the advantage of our proposed Multi-Agent framework.\nBy comparing results in Tables I and III we observe degradation of performance when switching to vision-based observations. This can be attributed to inaccuracies of vision-based estimators such as 3D bounding box detection and grasp estimation. Note that our current vision pipeline makes use of single-view scene observations. A parallel work, Manipulate Anything [57] showed that a relatively straight-forward extension to multi-view settings can reduce the impact of occlusions and result in higher accuracy."}, {"title": "V. DISCUSSION", "content": "Despite its significant advantages over a Single LLM Agent, MALMM has several limitations. First, MALMM relies on three gpt-4-turbo agents, making it costly to operate. Using open-source LLMs such as LLAMA [58] could help reduce costs, but running these models locally also incurs expenses. Second, like other LLM-based planners, MALMM depends on manual prompt engineering, which impacts its performance. However, advancements in alignment [59] and prompting [13], [14] reduce reduced efforts. Finally, MALMM requires accurate bounding box estimation to determine correct grasp positions, but as the complexity of objects increases, bounding boxes alone may not provide enough information for precise grasping. Our experiments with vision pipeline suggest that using pretrained grasping and placement models, such as M2T2 [52], could improve performance for complex manipulation tasks."}, {"title": "B. Conclusions", "content": "This work explored LLM agents for solving previously un-seen manipulation tasks. In particular, we have proposed the first multi-agent LLM framework for robotics manipulation MALMM and demonstrated its advantages over the single agent baselines. Our method uses task-agnostic prompts and requires no in-context learning examples for solving new tasks. Extensive evaluation demonstrated excellent results of MALMM for a variety of manipulation tasks, especially for tasks that require longer horizon planning. Future work will consider tasks with richer object interactions and will extend our experiments to real robotics scenes."}]}