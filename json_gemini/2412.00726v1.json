{"title": "FREE AND CUSTOMIZABLE CODE DOCUMENTATION WITH LLMs: A FINE-TUNING APPROACH", "authors": ["Sayak Chakrabarty", "Souradip Pal"], "abstract": "Automated documentation of programming source code is a challenging task with significant practical and scientific implications for the developer community. We present a large language model (LLM)-based application that developers can use as a support tool to generate basic documentation for any publicly available repository. Over the last decade, several papers have been written on generating documentation for source code using neural network architectures. With the recent advancements in LLM technology, some open-source applications have been developed to address this problem. However, these applications typically rely on the OpenAI APIs, which incur substantial financial costs, particularly for large repositories. Moreover, none of these open-source applications offer a fine-tuned model or features to enable users to fine-tune. Additionally, finding suitable data for fine-tuning is often challenging. Our application addresses these issues which is available at https://pypi.org/project/readme-ready/.", "sections": [{"title": "1 Introduction", "content": "The integration of natural and programming languages is a research area that addresses tasks such as automatic documentation of source code, code generation from natural language descriptions, and searching for code using natural language queries. These tasks are highly practical, as they can significantly enhance programmer efficiency, and they are scientifically intriguing due to their complexity and the proposed relationships between natural language, computation, and reasoning [1, 2, 3]."}, {"title": "1.1 Background and Related Work", "content": "Significant progress in machine learning [4, 5, 6] and other challenging natural language processing tasks has been achieved using neural networks, such as sequence-to-sequence transducers [7]. Neural networks require training on extensive and diverse datasets[8] for effective generalization. These methods have been applied to code documentation [9, 10] and code generation [11, 12], often using small or domain-specific datasets, sometimes confined to single software projects. Datasets like DJANGO and Project Euler [13] were developed by human annotators, ensuring accuracy but at a high cost and resulting in limited data sizes. Others, such as those referenced in [14, 9] and IFTTT [15], are larger but contain more noise.\nRecently, large language models (LLMs) have become increasingly significant, demonstrating human-like abilities across various fields [16, 17, 18]. LLMs typically employ transformer architecture variants and are trained on massive data volumes to detect patterns [19]."}, {"title": "1.2 Our Contributions", "content": "\u2022 We propose a software package\u00b9 that generates documentation for an open-source repository and serves as a documentation assistant for developers. This application creates a README file in Markdown format and allows generating a GitHub page from the documentation.\n\u2022 Instead of relying on the OpenAI APIs for handling calls, our model allows users to choose from various open-source models before generating the README, thereby avoiding the costs associated with existing applications.\n\u2022 As an example for our application, we fine-tune one of the models using LoRA. We allow users to fine-tune any of the models on a dataset we have created. Users' fine-tuning results may surpass ours due to our limited GPU resources."}, {"title": "2 Methodology", "content": "The application prompts the user to enter the project's name, and GitHub URL, and select the desired model from the following options:\n\u2022 gpt-3.5-turbo [23]\n\u2022 gpt-4 [24]\n\u2022 gpt-4-32k [25]\n\u2022 TheBloke/Llama-2-7B-Chat-GPTQ (quantized) [26]\n\u2022 TheBloke/CodeLlama-7B-Instruct-GPTQ (quantized) [27]\n\u2022 meta-llama/Llama-2-7b-chat-hf [28]\n\u2022 meta-llama/CodeLlama-7b-Instruct-hf [29]\n\u2022 google/gemma-2b-it [30]\n\u2022 google/codegemma-2b-it [31]\nNote that the first two options will incur a cost for each call, and users need to provide an OpenAI API key. For large projects, the cost can reach several hundred dollars. Detailed OpenAI pricing can be found at https://openai.com/api/pricing/.\nDocument Retrieval: Our application indexes the codebase through a depth-first traversal of all repository contents and utilizes an LLM to generate documentation. All files are converted into text, tokenized, and then chunked, with each chunk containing 1000 tokens. The application employs the sentence-transformers/all-mpnet-base-v2 [32] sentence encoder to convert each chunk into a 768-dimensional embedding vector, which is stored in an in-memory vector store. When a query is provided, it is converted into a similar vector using the same sentence encoder. The neighbor nearest to the query embedding vector is searched using KNN (k=4) from the vector store, utilizing cosine similarity as the distance metric. For the KNN search, we use the HNSWLib library, which implements an approximate nearest-neighbor search based on hierarchical navigable small-world graphs [33]. This methodology provides the relevant sections of the source code, aiding in answering the prompted question. The entire methodology for Retrieval Augmented Generation(RAG) and fine-tuning is illustrated in Figure 1.\nPrompt Configuration: Prompt engineering is accomplished using the Langchain API. For our purpose, a prompt template has been used as provided in Appendix C. This template includes placeholders for questions, which users can edit and modify as needed. This flexibility allows the README to be generated according to the user's specific requirements. Our default README structure includes sections on description, requirements, installation, usage, contributing methods, and licensing, which align with standard documentation practices. The temperature for text generation is kept at the default value of 0.2. The current prompts are developer-focused and assume that the repository is code-centric."}, {"title": "2.1 Fine Tuning", "content": "Parameter-efficient fine-tuning (PEFT) [34] is a technique in natural language processing that enhances pre-trained language models for specific tasks by fine-tuning only a subset of their parameters. This method involves freezing most of the model's layers and adjusting only the last few, thus conserving computational resources and time. Several parameter-efficient fine-tuning (PEFT) methods exist, such as Adapters, LoRA [35], etc. We chose to fine-tune with QLoRA [36] due to its significant reduction in the number of trainable parameters while maintaining performance. Given our limited resources, QLoRA is highly efficient as it adapts models for specific tasks with minimal computational overhead.\nIn our work, we fine-tune only one model, TheBloke/Llama-2-7B-Chat-GPTQ [26], which is a 4-bit quantized model with 1.13 billion parameters. It supports a maximum sequence length of 4096 tokens and requires 3.9 GB of memory. We utilized GPU clusters provided by Northwestern University for fine-tuning our model. The configuration used is 1\u00d7 NVIDIA Tesla V100 with 16GB of GPU memory. With this resource, training on a large dataset (12,803 data points) takes more than 15 hours, while training on a small dataset (339 data points) takes approximately 30 minutes for 3 epochs.\nThese resources are substantially limited compared to typical LLM fine-tuning requirements. Due to these constraints, we could only train the model for 3 epochs on a small dataset. As a result, we have made fine-tuning an optional feature, giving users the choice to fine-tune the model using their own GPU resources."}, {"title": "3 Experiments and Results", "content": "Our contribution is an application, not a new language generation model, making it challenging to establish an experimental methodology. We conducted the finetuning experiment on a small dataset consisting of randomly selected 190 README files, which may not address our default documentation questions. For each README, we examine its sections and subsections, frame relevant questions, and use the answers generated by our tool for training. A few samples of the fine-tuning dataset can be found in Appendix B. For evaluation, we selected the rest 10 repositories and compared the original answers with the autogenerated documentation (sample documentation shown in Appendix D) using BLEU and BERT scores to assess our model's performance."}, {"title": "3.1 Before Fine-tuning", "content": "We conducted a series of experiments utilizing the TheBloke/Llama-2-7B-Chat-GPTQ model [26] to demonstrate the functionality and efficacy of our proposed pipeline. The accompanying codebase is designed to be flexible, allowing the user to easily switch between different large language models (LLMs) by simply modifying the configuration file. Given the characteristics of LLMs, models with a greater number of parameters are generally expected to deliver enhanced performance. However, we lack the GPU resources to run a non-quantized version. The BLEU and BERT scores for the TheBloke/Llama-2-7B-Chat-GPTQ model are reported in Table la and Table 1b respectively, under the \"W/O FT\" or \"W/O Finetuning\" columns."}, {"title": "3.2 After Fine-tuning", "content": "We utilized the PEFT library from Hugging Face, which supports several Parameter Efficient Fine-Tuning (PEFT) methods. This approach is cost-effective for fine-tuning large language models (LLMs), particularly on lightweight hardware. The training configuration and hyperparameters are detailed in Table 2a and Table 2b respectively. The results are reported in Table 1b and Table 1a, under the \"With FT\" or \"With Finetuning\" columns. it is observed that BLEU scores range from 15 to 30, averaging 20, indicating that the generated text is understandable but requires substantial editing to be acceptable. Conversely, BERT scores reveal a high semantic similarity to the original README content, with an average F1 score of 85%. The slightly lower scores for fine-tuned models compared to their original counterparts can be attributed to heavy quantization and the use of very low-rank configuration in QLoRA to manage memory constraints, leading to a noticeable reduction in quality. The effectiveness of the tool in generating accurate documentation relies significantly not only on the fine-tuned model for text generation but also on the comprehensiveness of the code context included in the prompt. Consequently, when relevant context is captured by the HNSW algorithm using text embeddings, the generated outputs remain satisfactory even when employing quantized fine-tuned models. However, the performance of these models tends to plateau if sufficient contextual information is not adequately captured."}, {"title": "4 Conclusion", "content": "Our application addresses the critical need for generating documentation for code repositories by utilizing multiple LLM models and allowing users to fine-tune these models using LoRa on their own GPUs. While our approach is not designed to surpass state-of-the-art benchmarks, its significance lies in the application of NLP techniques to solve a pressing issue faced by the developer community. The tool provides initial documentation suggestions based on the source code, assisting developers in initiating the documentation process and enabling them to modify the generated README files to meet their specific requirements, thereby reducing manual effort. Additionally, the generated README files can be seamlessly converted into PyPI-compliant standard documentation websites using tools such as MkDocs or Sphinx. This application can also be adapted as a plugin for integration with code editors like Visual Studio Code, thus enhancing the development workflow by minimizing the need for manual documentation creation."}, {"title": "A Ethical Limitations", "content": "Like any study, ours has its limitations. It is important to note that we propose our application as a developer support tool. While it assists developers in documenting unseen repositories or undocumented code bases and quickly understanding how to use them, developers should not blindly follow the instructions provided by the application. This caution is necessary because the application relies on pre-trained models like Meta's Llama2 and OpenAI's GPT, which are known to hallucinate and provide incorrect instructions with confidence. We have provided a fine-tuning feature and plan to launch the fine-tuned model, which is expected to hallucinate less than the raw model. Another limitation is that the application will not address more complex questions that developers might have; it will only provide answers to basic questions. However, developers can modify our source code to add more prompts to obtain specific answers. In such cases, fine-tuning must be performed again with a different dataset. Lastly, no AI assistants were used for this research or the writing of this paper.\nThe primary potential risk of this application is that it might misguide developers if they uncritically accept every instruction generated in the README file. Our experiments indicate that the generated README files produce close to correct content (as measured by various evaluation methods; see Table 1b and Table 1a), but it is still crucial not to take every piece of text generated by the LLM backend at face value. The application should be regarded as a support tool for developers, rather than a definitive source of truth."}, {"title": "B Data Samples", "content": "Table 3 shows samples of data that were used in fine-tuning the pretrained language models based on the prompt template. For our experiment, some of the inputs to the prompt were kept fixed but may be varied based on the use-case. Here, we fixed the content_type as \u201cdocs\", target_audience as \"smart developer\" and did not pass any additional_instructions. In Table 3, shortened versions of the context has been provided to give an idea on the contextual information obtained using the HNSW algorithm on the document chunks.\""}, {"title": "C Prompt Template", "content": "Instruction:\nYou are an AI assistant for a software project called {project_name}. You are trained on all the {content_type} that makes up this project. The {content_type} for the project is located at {repository_url}. You are given a repository which might contain several modules and each module will contain a set of files. Look at the source code in the repository and you have to generate content for the section of a README.md file following the heading given below. If you use any hyperlinks, they should link back to the github repository shared with you. You should only use hyperlinks that are explicitly listed in the context. Do NOT make up a hyperlink that is not listed. Assume the reader is a {target_audience} but is not deeply familiar with {project_name}. Assume the reader knows nothing about how the project is structured or which folders/files do what and what functions are written in"}, {"title": "D Sample Autogenerated Readme", "content": "# python-fire\n<pre><code>**Description**</code></pre>\n<p>The <code>python-fire</code> library is a powerful tool for building web applications.\nIt provides a wide range of features and functionalities that can help you build complex\nweb applications quickly and efficiently. In this section, we will provide an overview of\nthe main features and functionalities of <code>python-fire</code>.</p>\n<pre><code>**Installation**\nTo install the latest version of 'python-fire', follow these steps:\nClone the repository from GitHub:\n</code></pre><p><code>bash\ngit clone https://github.com/google/python-fire.git\n</code></p>\n<pre><code>Change into the cloned repository:</code>\n</pre><p>'''bash\ncd python-fire\n</code></p>\n<pre><code>Run the following command to install the dependencies:\n</code></pre><p>'''bash\n<pre><code>pip install -r requirements.txt</code></pre>\n<p>'''</p><pre><code>\nThis will install all the necessary dependencies for python-fire`. Once the installation\nis complete, you can run 'python-fire' by running the following command:\n</code></pre><p>``'bash\n<pre>\n<code>python fire</code></pre><p>'''</p><pre><code>\nThis will launch 'python-fire' in your default Python interpreter.\n**How to contribute**\nWe'd love to accept your patches and contributions to this project. There are just a\nfew small guidelines you need to follow. Before you begin making changes, state your\nintent to do so in an Issue. Then, fork the project. Make changes in your copy of the\nrepository. Then open a pull request once your changes are ready. If this is your\nfirst contribution, sign the Contributor License Agreement. A discussion about your\nchange will follow, and if accepted your contribution will be incorporated into the\nPython Fire codebase."}]}