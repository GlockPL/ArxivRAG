{"title": "MAKE THE PERTINENT SALIENT:\nTASK-RELEVANT RECONSTRUCTION\nFOR VISUAL CONTROL WITH DISTRACTIONS", "authors": ["Kyungmin Kim", "JB Lanier", "Pierre Baldi", "Charless Fowlkes", "Roy Fox"], "abstract": "Recent advancements in Model-Based Reinforcement Learning (MBRL) have\nmade it a powerful tool for visual control tasks. Despite improved data efficiency,\nit remains challenging to train MBRL agents with generalizable perception. Train-\ning in the presence of visual distractions is particularly difficult due to the high\nvariation they introduce to representation learning. Building on DREAMER, a\npopular MBRL method, we propose a simple yet effective auxiliary task to facil-\nitate representation learning in distracting environments. Under the assumption\nthat task-relevant components of image observations are straightforward to iden-\ntify with prior knowledge in a given task, we use a segmentation mask on image\nobservations to only reconstruct task-relevant components. In doing so, we greatly\nreduce the complexity of representation learning by removing the need to encode\ntask-irrelevant objects in the latent representation. Our method, Segmentation\nDreamer (SD), can be used either with ground-truth masks easily accessible in\nsimulation or by leveraging potentially imperfect segmentation foundation mod-\nels. The latter is further improved by selectively applying the reconstruction loss\nto avoid providing misleading learning signals due to mask prediction errors. In\nmodified DeepMind Control suite (DMC) and Meta-World tasks with added vi-\nsual distractions, SD achieves significantly better sample efficiency and greater\nfinal performance than prior work. We find that SD is especially helpful in sparse\nreward tasks otherwise unsolvable by prior work, enabling the training of visually\nrobust agents without the need for extensive reward engineering.", "sections": [{"title": "1 INTRODUCTION", "content": "Recently, model-based reinforcement learning (MBRL) (Sutton, 1991; Ha & Schmidhuber, 2018;\nHafner et al., 2019; 2020; Hansen et al., 2022; 2023) has shown great promise in learning control\npolicies, achieving high sample efficiency. Among recent advances, the DREAMER family (Hafner\net al., 2020; 2021; 2023) is considered a seminal work, showing strong performance in diverse visual\ncontrol environments. This is accomplished by a close cooperation between a world model and an\nactor-critic agent. The world model learns to emulate the environment's forward dynamics and\nreward function in a latent state space, and the agent is trained by interacting with this world model\nin place of the original environment.\nUnder this framework, accurate reward prediction is all we should sufficiently require for agent\ntraining. However, learning representations solely from reward signals is known to be challenging\ndue to their lack of expressiveness and high variance (Hafner et al., 2020; Jaderberg et al., 2017). \u03a4\u03bf\naddress this, DREAMER employs image reconstruction as an auxiliary task in world model training\nto facilitate representation learning. In environments with little distraction, image reconstruction\nworks effectively by delivering rich feature-learning signals derived from pixels. However, in the\npresence of distractions, the image reconstruction task pushes the encoder to retain all image in-\nformation, regardless of its task relevance. For instance, moving backgrounds in observations in\nFig. 1 are considered distractions. Including this information in the latent space complicates dynam-\nics modeling and degrades sample efficiency by wasting model capacity and drowning the relevant\nsignal in noise (Fu et al., 2021)."}, {"title": "2 RELATED WORK", "content": "Model-Based RL for Distracting Visual Environments. Recent advances in MBRL have facil-\nitated the learning of agents from image observations (Finn & Levine, 2017; Ha & Schmidhuber,\n2018; Hafner et al., 2019; 2020; 2021; 2023; Schrittwieser et al., 2020; Hansen et al., 2022; 2023).\nNevertheless, learning perceptual representations in the presence of distractions remains challeng-\ning in these models (Zhu et al., 2023). For effective representation learning, some works apply\nnon-reconstructive representation learning methods (Nguyen et al., 2021; Deng et al., 2022), such as\ncontrastive learning (Chen et al., 2020) and prototypical representation learning (Caron et al., 2020).\nHowever, features learned with these methods do not necessarily involve task-related content since\nthey do not explicitly consider task-relevance in feature learning. Some other works design auxil-\niary objectives to explicitly use downstream task information (Zhang et al., 2021; Fu et al., 2021).\nFor example, DBC (Zhang et al., 2021) uses a bisimulation metric (Ferns et al., 2011) to encourage\ntwo trajectories of similar behaviors to become closer in a latent space. Perhaps most relevant to\nour method is TIA (Fu et al., 2021) which explicitly separates task-relevant and irrelevant branches\nto distinguish reward-correlated visual features from distractions. Features from each branch are\ncombined later to reconstruct the original, distracting observation. Recently, a few approaches pro-\nposed to leverage inductive biases such as predictability (Zhu et al., 2023) and controllability (Wang\net al., 2022; Bharadhwaj et al., 2022) to learn useful features for visual control tasks. These methods\nhave shown to be more effective than using the reward signal alone, but many of them suffer from\nsample inefficiency, requiring many samples to implicitly identify what is task-relevant from data.\nIn contrast, our work proposes to leverage domain knowledge in the form of image masks to pro-\nvide an explicit signal for identifying task-relevant information. Notably, training in sparse reward\nenvironments with distraction has remained unsolved in the literature. Several methods for robust\nrepresentation learning have also been proposed for model-free RL (Laskin et al., 2020; Kostrikov\net al., 2021; Yarats et al., 2021; Hansen et al., 2021; Hansen & Wang, 2021; Nair et al., 2022; Zhang\net al., 2019). However, the results suggest that MBRL is more powerful and sample efficient for\nvisual control tasks, thus we focus on comparison with methods in the MBRL framework.\nSegmentation for RL. Segmentation models (He et al., 2017; Redmon et al., 2016) have been\nused in many downstream tasks, including RL, to assist in pre-processing inputs (Kirillov et al.,\n2023; Anantharaman et al., 2018; Yuan et al., 2018; James et al., 2019; So et al., 2022). Recently,"}, {"title": "3 PRELIMINARIES", "content": "We consider a partially observable Markov decision process (POMDP) formalized as a tuple\n($S, \u03a9, \u0391,\u03a4, \u039f, p_0, R, \u03b3$), consisting of states $s \u2208 S$, observations $\u03bf \u2208 \u03a9$, actions $a \u2208 A$, state\ntransition function $T : S \u00d7 A \u2192 \u25b3(S)$, observation function $O : S \u2192 \u03a9$, initial state distribution\n$p_0$, reward function $R : S \u00d7 A \u2192 R$, and discount factor \u03b3. At time t, the agent does not have\naccess to actual world state $s_t$, but to the observation $o_t = O(s_t)$, which in this paper we consider\nto be a high-dimensional image. Our objective is to learn a policy \u03c0($a_t$|$0_{t, a_{s + 1} ~ T(s_{t + 1}|s_t, a_t)$ (Zhu et al., 2023; Fu et al., 2021; Bharadhwaj et al., 2022). Note that\nobservations $O_t$ are a function of both $s_t^+$ and $s_t^-$, thus we have $O : S^+ \u00d7 S^- \u2192 \u03a9$.\nOur goal is to learn effective latent representations [$h_t; z_t$] for task control. Ideally, this would mean\nthat the world model will only encode and simulate task-relevant state components $s_t^+$ in its latent\nspace without modeling unnecessary information in $s_t^-$. To learn features pertaining to $s_t^+$, image\nreconstruction can provide a rich and direct learning signal, but only when observation information\nabout $s_t^+$ is not drowned out by other information from $s_t^-$. To overcome this pitfall, we propose\nto apply a heuristic filter to reconstruction targets $o_t$ with the criteria that it minimizes irrelevant\ninformation pertaining to $s_t^-$ while keeping task-relevant information about $s_t^+$."}, {"title": "4 METHOD", "content": "We build on DREAMER-V3 (Hafner et al., 2023) to explicitly model $s_t^+$ while attempting to avoid\nencoding information about $s_t^-$. In Section 4.1, we describe how we accomplish this by using do-\nmain knowledge to apply a task-relevance mask to observation reconstruction targets. In Section 4.2\nwe describe how we leverage segmentation mask foundation models to provide approximate masks\nover task-relevant observation components. Finally, in Section 4.3, we propose a modified decoder\narchitecture and objective to mitigate noisy learning signals from incorrect mask predictions.\n4.1 USING SEGMENTATION MASKS TO FILTER IMAGE TARGETS\nWe first introduce our main assumption, that the task-relevant components of image observations are\neasily identifiable with domain knowledge. In many real scenarios, it is often straightforward for a\npractitioner to know what the task-related parts of an image are, e.g. objects necessary for achieving\na goal in object manipulation tasks. With this assumption, we propose a new reconstruction-based\nauxiliary task that leverages domain knowledge of task-relevant regions. Instead of reconstruct-\ning the raw image observations (Fig. 1b) which may contain task-irrelevant distractions, we apply\na heuristic task-relevance segmentation mask over the image observation (Fig. 1c) to exclusively\nreconstruct components of the image that are pertinent to control.\nSince our new masked reconstruction target should contain only image regions that are relevant for\nachieving the downstream task, our world model should learn latent representations where a larger\nportion of the features are useful to the RL agent. By explicitly avoiding modeling task-irrelevant\nobservation components, the latent dynamics should also become simpler and more sample-efficient\nto learn than the original (more complex, higher variance) dynamics on unfiltered observations. In\nsimulations, ground-truth masks of relevant observation components are often easily accessible, for\nexample, in MuJoCo (Todorov et al., 2012), through added calls to the simulator API. We term the\nmethod trained with our proposed replacement auxiliary task as Segmentation Dreamer (SD) and\ncall the version trained with ground-truth masks SDGT."}, {"title": "4.2 LEVERAGING APPROXIMATE SEGMENTATION MASKS", "content": "A simulator capable of providing ground-truth masks for task-relevant regions is not always avail-\nable. For such cases where only RGB images are available from the environment, we propose to\nfine-tune a segmentation mask foundation model to our domain and integrate its predictions into the"}, {"title": "4.3 LEARNING IN THE PRESENCE OF MASKING ERRORS", "content": "Although foundation segmentation models generalize well to new scenarios (e.g., different poses,\nocclusions), prediction errors are inevitable (Fig. 1d). Since each frame is processed independently,\nsegmentation predictions can flicker along trajectories. False negatives in task relevance are particu-\nlarly detrimental when using naive L2 loss on image reconstruction. Missing relevant scene elements\nin reconstruction targets can lead the encoder to learn incomplete representations, dropping essen-\ntial task-related information. This variability disrupts the learning of accurate representations and\ndynamics in the world model.\nDespite noisy targets, neural networks can self-correct if most labels are accurate (Han et al., 2018).\nAdditionally, DREAMER's use of GRUs (Cho et al., 2014) provides temporal consistency even with\nflickering targets. However, as illustrated in Fig. 2 (b)&(c), it's undesirable to propagate gradients\nfrom regions where the original image has been incorrectly masked out. Allowing gradients from\nthese regions provides misleading signals. If we could identify the incorrect regions in the recon-\nstruction target, we could nullify the decoder's $L_2$ loss there-a technique we call selective $L_2$ loss.\nSince we cannot directly identify regions where the RGB target is incorrectly masked due to false\nnegatives, we estimate them. Preliminary experiments show that a binary mask decoder from world\nmodel states (as an added auxiliary task) can be less prone to transient false negatives, unlike RGB\nprediction, which tends to memorize noisy labels. Therefore, we propose training a world model\nwith two reconstruction tasks (Fig. 2, right): one decoding masked RGB images and the other\npredicting task-relevance binary masks. Both use the foundation model's binary mask, maskFM, to\nconstruct targets. The RGB branch decodes masked RGB images, while the binary branch predicts"}, {"title": "5 EXPERIMENTS", "content": "We evaluate our method on a variety of visual robotic control tasks from the DeepMind Control Suite\n(DMC) (Tassa et al., 2018) and Meta-World (Yu et al., 2019). Since the standard environments in\nthese benchmarks have simple backgrounds with minimal distractions, we introduce visual distrac-\ntions by replacing the backgrounds with random videos from the 'driving car' class in the Kinetics\n400 dataset (Kay et al., 2017), following prior work (Zhang et al., 2021; Nguyen et al., 2021; Deng\net al., 2022). Details about the environment setup and task visualizations are provided in Appen-\ndices F and B. In evaluation, we roll out policies over 10 episodes and compute the average episode\nreturn. Unless otherwise specified, we report the mean and standard error of the mean (SEM) of\nfour independent runs with different random seeds. We use default DREAMER-V3 hyperparameters\nin all experiments.\n5.1 DMC EXPERIMENTS\nWe evaluate SD on six tasks from DMC featuring different forms of contact dynamics, degrees\nof freedom, and reward sparsities. For each task, models are trained for 1M environment steps\ngenerated by 500K policy decision steps with an action repeat of 2."}, {"title": "5.1.1 COMPARISON WITH DREAMER", "content": "We compare our methods, SDGT and SDapprox., to the base DREAMER (Hafner et al., 2023) method.\nHere, SDapprox. is denoted as SDFM, specifying the segmentation model used (FM) and the number\nof fine-tuning examples (N). All methods are trained in distracting environments, except for the\nDREAMER* baseline, which is trained in the original environment without visual distractions. In\nmost cases, we consider DREAMER* as an upper bound for methods trained with distractions. Sim-\nilarly, SDGT serves as an upper bound for SDapprox., with the performance gap expected to decrease\nin the future as segmentation quality improves.\nAs shown in Fig. 3a, DREAMER fails across all tasks due to task-irrelevant information in RGB\nreconstruction targets, which wastes latent capacity and complicates dynamics learning. In contrast,\nSDGT achieves test returns comparable to DREAMER* by focusing on reconstructing essential fea-\ntures and ignoring irrelevant components. Interestingly, SDGT outperforms DREAMER* in Cartpole\nSwingup, possibly because the original environment still contains small distractions (e.g., moving\ndots) that DREAMER* has to model.\nA limitation of SD is its reliance on acccurate and correct prior knowledge to select task-relevant\ncomponents. In Cheetah Run, SDGT underperforms compared to DREAMER*, likely because we"}, {"title": "5.1.2 \u0421\u043eMPARISON WITH BASELINES", "content": "We compare SDapprox. with state-of-the-art methods, including DreamerPro (Deng et al., 2022),\nRePo (Zhu et al., 2023), TIA (Fu et al., 2021), and TD-MPC2 (Hansen et al., 2023). DreamerPro\nincorporates prototypical representation learning in the DREAMER framework; RePo minimizes\nmutual information between observations and latent states while maximizing it between states and\nfuture rewards; TIA learns separate task-relevant and task-irrelevant representations which can be\ncombined to decode observations; and TD-MPC2 decodes a terminal value function. Among these\nbaselines, only TIA relies on observation reconstruction. Further details are in Appendix K.\nOur results in Fig. 3a show that our method consistently outperforms the baselines in performance\nand sample efficiency. TIA underperforms in many tasks, requiring many samples to infer task-\nrelevant observations from rewards and needing exhaustive hyperparameter tuning. Even with opti-\nmal settings, it may lead to degenerate solutions where a single branch captures all information. In\ncontrast, our method focuses on task-relevant parts without additional tuning by effectively inject-\ning prior knowledge. RePo performs comparably to ours in Cartpole Swingup but underperforms in\nother tasks and converges more slowly.\nTD-MPC2 struggles significantly in distracting environments. We speculate that spurious corre-\nlations from distractions introduce noise to value-function credit assignment that hinders repre-\nsentation learning. Our method mitigates this by directly supervising task-relevant features using\nsegmentation models, leading to more consistent and lower-variance targets.\nAmong these methods, DreamerPro is the most competitive, demonstrating the effectiveness of\nprototypical representation learning for control. However, it often requires more environment inter-\nactions and converges to lower performance.\nIn the Cartpole Swingup with sparse rewards, none of the prior works successfully solved the task,\nhighlighting the challenge of inferring task relevance from weak signals. Our method achieves near-\noracle performance, being the only one to train an agent with sparse rewards amidst distractions.\nThis suggests the potential to train agents in real-world, distraction-rich environments without ex-\ntensive reward engineering."}, {"title": "5.1.3 ABLATION STUDY", "content": "We investigate the effects of the components in SDapprox. by addressing: (1) the benefits of using\nsegmentation models for targets vs. input preprocessing; (2) the effectiveness of the selective $L_2$ loss\ncompared to the naive $L_2$ loss; and (3) the impact of the segmentation quality on RL performance. In\nthese experiments, we fine-tune PerSAM with a single data point for segmentation mask prediction.\nUsing segmentation masks for an auxiliary\ntask vs. input preprocessing. We create a\nvariant of SDPerSAM that uses masked obser-\nvations for both inputs and targets, denoted in\nTab. 1 by As Input. These results suggest that\nSD PerSAM, in addition to not requiring mask\nprediction at test-time, also achieves better test\nperformance and lower variance. Using pre-\ndicted masks as input is more prone to segmen-\ntation errors, restricting the agent's perception\nwhen masks are incorrect and making training\nmore challenging. In contrast, SDapprox. receives intact observations, with task-relevant filtering at\nthe encoder level, leading to better state abstraction. Further analysis on test-time segmentation\nquality's impact is in Appendix D.\nSelective $L_2$ loss vs. naive $L_2$ loss. As shown in Tab. 1, SDPerSAM consistently outperforms the\nNaive $L_2$ variant, especially in complex tasks like Cheetah Run and Walker Run. Segmentation\nmodels often miss embodiment components (Fig. 4, third row). With the naive $L_2$ loss, the model\nreplicates these errors, leading to incomplete latent representations and harming dynamics learn-\ning (Fig. 4a, fourth row). In contrast, SDapprox. self-corrects by skipping the $L_2$ computation where\nPerSAM targets are likely wrong (Fig. 4b, fourth row). Fig. 4(c)&(d) show that the naive $L_2$ loss\nfollows PerSAM's trends, while the selective $L_2$ loss recovers from poor recall with only a moderate\nprecision decrease."}, {"title": "5.2 \u039c\u0395\u03a4\u0391-WORLD EXPERIMENTS", "content": "Object manipulation is a natural application\nfor our method where prior knowledge can be\napplied straightforwardly by identifying and\nmasking task-relevant objects and robot em-\nbodiments. We evaluate SD on six tasks from\nMeta-World (Yu et al., 2019), a popular bench-\nmark for robotic manipulation. Depending on\nthe difficulty of each task, we conduct exper-\niments for 30K, 100K, and 1M environment\nsteps, with an action repeat of 2 (details in Ap-\npendix G). Preliminary tests showed that Seg-\nFormer performs well with few-shot learning\non small objects. We fine-tune SegFormer with\n10 data points to estimate masks in these exper-\niments.\nFig. 5 suggests that our approach outperforms the baselines overall, with a more pronounced advan-\ntage in tasks involving small objects like Coffee-Button. Our method excels because it focuses on\nsmall, task-relevant objects, avoiding the reconstruction of unnecessary regions that occupy much\nof the input. In contrast, the baselines struggle as they often underestimate the significance of these\nsmall yet highly task-relevant objects. Among the baselines, RePo (Zhu et al., 2023) is the most\ncompetitive. However, RePo performs poorly in a sparse reward setup (see Appendix H)."}, {"title": "6 CONCLUSION", "content": "In this paper, we propose SD, a simple yet effective method for learning task-relevant features in\nMBRL frameworks like DREAMER by using segmentation masks informed by domain knowledge.\nUsing ground-truth masks, SDGT achieves performance comparable with undistracted DREAMER\nwith high sample efficiency in distracting environments when provided with accurate prior knowl-\nedge. Our main method, SDapprox., uses mask estimates from off-the-shelf one-shot or few-shot\nsegmentation models and employs a selective $L_2$ loss. It learns effective world models that produce\nstrong agents outperforming baselines.\nTo the best of our knowledge, our approach appears to be the first model-based approach to success-\nfully train an agent in a sparse reward environment under visual distractions, enabling robust agent\ntraining without extensive reward engineering. This work also advances the integration of computer\nvision and RL by presenting a novel way to leverage recent advances in segmentation to address\nchallenges in visual control tasks. The proposed method achieves strong performance on diverse\ntasks with distractions and effectively incorporates human input to indicate task relevance. This\nenables practitioners to readily train an agent for their own purposes without extensive reward engi-\nneering. However, SD has some limitations to consider in future work, which we further explore in\nAppendix M."}, {"title": "A CODE RELEASE", "content": "We plan to make the code for Segmentation Dreamer publicly available upon acceptance."}, {"title": "B VISUALIZATION OF TASKS", "content": "B.1 DEEPMIND CONTROL SUITE (DMC)\nFig. 6 visualizes the six tasks in DMC (Tassa et al., 2018) used in our experiments. Each row\npresents the observation from the standard environment, the corresponding observation with added\ndistractions, the ground-truth segmentation mask, and the RGB target with the ground-truth mask\napplied. Cartpole Swingup Sparse and Cartpole Swingup share the same embodiment and dynam-\nics. Cartpole Swingup Sparse only provides a reward when the pole is upright, whereas Cartpole\nSwingup continuously provides dense rewards weighted by the proximity of the pole to the upright\nposition. Reacher Easy entails two objects marked with different colors in the segmentation mask,\nas shown in Fig. 6e 3rd column. Before passing the mask to SD, the mask is converted to a binary\nformat where both objects are marked as true as task-relevant."}, {"title": "C THE IMPACT OF PRIOR KNOWLEDGE", "content": "We investigate the impact of accurate prior knowledge of task-relevant objects. Specifically, we\nconduct additional experiments on Cheetah Run-the task showing the largest disparity between\nDREAMER* and SDGT in Fig. 3a. In our primary experiment, we designated only the cheetah's\nbody as the task-relevant object. However, since the cheetah's dynamics are influenced by ground\ncontact, the ground plate should have also been considered task-relevant.\nFig. 8 (a-c) illustrates the observation with distractions, the auxiliary target without the ground plate,\nand with the ground plate included, respectively. Fig. 8d compares SDGT trained with different\nselections of task-relevant objects included in the masked RGB reconstruction targets. We show\nthat including the ground plate leads to faster learning and performance closer to that of the oracle.\nThis highlights the significant influence of prior knowledge on downstream tasks, suggesting that\ncomprehensively including task-relevant objects yields greater benefits."}, {"title": "D THE IMPACT OF TEST-TIME SEGMENTATION QUALITY ON PERFORMANCE", "content": "We investigate how test-time segmentation quality affects SDapprox. as well as the As Input variation\nthat applies mask predictions to RGB inputs in addition to reconstruction targets. For this analysis,\nwe use PerSAM fine-tuned with a single data point for segmentation prediction. To measure seg-\nmentation quality, we compute episodic segmentation quality by averaging over frame-level IoU.\nIn Fig. 9 we plot episode segmentation quality versus test-time reward on the evaluation episodes\nduring the last 10% of training time.\nFig. 9 illustrates that SDapprox. exhibits greater robustness to test-time segmentation quality com-\npared to the As Input variation, with the discrepancy increasing as the IoU decreases. This dispar-\nity primarily arises because As Input relies on observations restricted by segmentation predictions,\nand thus its performance deteriorates quickly as the segmentation quality decreases. In contrast,\nSDapprox. takes the original observation as input and all feature extraction is handled by the obser-\nvation encoder, informed by our masked RGB reconstruction objective. Consequently, SDapprox.\nmaintains resilience to test-time segmentation quality.\nAn intriguing observation is that a poorly trained agent can lead to poor test-time segmentation\nquality. For instance, Cartpole Swingup (Sparse) exhibits different segmentation quality distribu-\ntions between SDapprox. and As Input. This discrepancy occurs because the sub-optimal agent often\npositions the pole at the cart track edge, causing occlusion and hindering accurate segmentation\nprediction by PerSAM."}, {"title": "E ABLATION WITHOUT STOP GRADIENT", "content": "Should the SDapprox. world model be shielded from gradients of the binary mask decoder head?\nTo estimate potential regions on RGB targets where task-relevant regions are incorrectly masked\nout, we train a binary mask prediction head on the world model to help detect false negatives in\nmasks provided by the foundation model. We see better performance when gradients from this bi-\nnary mask decoder objective are not propagated to the rest of the world model. Thus, the default\nSDapprox. architecture is trained with the gradients of the binary mask branch stopped at its [$h_t; z_t$]\ninputs, and the latent representations in the world model are trained only by the task-relevant RGB\nbranch in addition to the standard DREAMER reward/continue prediction and KL-divergence be-\ntween the dynamics prior and observation encoder posterior. Tab. 2 shows that the performance\ndrops significantly when training without stopping these gradients.\nWe also examine masks predicted by the binary mask decoder head in Fig. 10. Predictions are\ncoarser grained than their RGB counterparts, lacking details important for predicting intricate\nforward dynamics. Overall, reconstructing RGB observations with task-relevance masks applied\ndemonstrates itself as a superior inductive bias to learn useful features for downstream tasks com-\npared to binary masks or raw unfiltered RGB observations."}, {"title": "F DISTRACTING DMC SETUP", "content": "We follow the DBC (Zhang et al., 2021) implementation to replace the background with color\nvideos. The ground plate is also presented in the distracting environment. We used hold-out videos\nas background for testing. We sampled 100 videos for training from the Kinetics 400 training set of\nthe 'driving car' class, and test-time videos were sampled from the validation set of the same class."}, {"title": "G DISTRACTING META-WORLD SETUP", "content": "We test on six tasks from Meta-World-V2. For all tasks, we use the corner3 camera viewpoint.\nThe maximum episode length for Meta-World tasks is 500 environment steps, with the action re-\npeat of 2 (making 250 policy decision steps). We classify these tasks into easy, medium, and\ndifficult categories based on the training curve of DREAMER* (DREAMER trained in the stan-\ndard environments). Coffee Button, Drawer Close, and Handle Press are classified as easy, and we\ntrain baselines on these for 30K environment steps. Button Press Topdown (medium) is trained for\n100K steps, and Door Open and Drawer Open (difficult) are trained for 1M environment steps."}, {"title": "H RESULTS ON META-WORLD WITH SPARSE REWARDS", "content": "We also evaluate on sparse reward variations of the distracting Meta-World environments where\na reward of 1 is only provided on timesteps when a success signal is given by the environment\n(e.g. objects are at their goal configuration). Rewards are 0 in all other timesteps. The maximum\nattainable episode reward is 250.\nThe sparse reward setting is more challenging because the less informative reward signal makes\ncredit assignment more difficult for the RL agent. Fig. 11 shows that our method consistently\nachieves higher sample efficiency and better performance, showing promise for training agents\nrobust to visual distractions without extensive reward engineering. In Meta-World experiments,\nTIA (Fu et al., 2021) is not included as it requires exhaustive hyperparameter tuning for new do-\nmain"}]}