{"title": "InstructOCR: Instruction Boosting Scene Text Spotting", "authors": ["Chen Duan", "Qianyi Jiang", "Pei Fu", "Jiamin Chen", "Shengxi Li", "Zining Wang", "Shan Guo", "Junfeng Luo"], "abstract": "In the field of scene text spotting, previous OCR methods primarily relied on image encoders and pre-trained text information, but they often overlooked the advantages of incorporating human language instructions. To address this gap, we propose InstructOCR, an innovative instruction-based scene text spotting model that leverages human language instructions to enhance the understanding of text within images. Our framework employs both text and image encoders during training and inference, along with instructions meticulously designed based on text attributes. This approach enables the model to interpret text more accurately and flexibly. Extensive experiments demonstrate the effectiveness of our model and we achieve state-of-the-art results on widely used benchmarks. Furthermore, the proposed framework can be seamlessly applied to scene text VQA tasks. By leveraging instruction strategies during pre-training, the performance on downstream VQA tasks can be significantly improved, with a 2.6% increase on the TextVQA dataset and a 2.1% increase on the ST-VQA dataset. These experimental results provide insights into the benefits of incorporating human language instructions for OCR-related tasks.", "sections": [{"title": "Introduction", "content": "Scene text spotting technology aims to detect and recognize characters directly within natural scene images. Recently, significant advancements in integrating vision and text have been made across various visual-language tasks, leading to the emergence of innovative instruction-based models. Some studies (Liu, Zeng et al. 2023; Geng et al. 2023; Brooks, Holynski, and Efros 2023; Kirillov, Mintun et al. 2023) have validated that incorporating human language instructions can enable models to comprehend the content of images more accurately. Drawing on the insights gained from these studies, we pose the following question: For scene text images, which inherently involve visual text, wouldn't the incorporation of human language instructions be more beneficial?"}, {"title": "Related Work", "content": ""}, {"title": "None Sequence-based Method", "content": "Most previous scene text spotting methods have treated detection and recognition as two separate tasks (Borisyuk, Gordo et al. 2018; Liao, Shi et al. 2017; Huang, Liu et al. 2022; Liu, Shen et al. 2021). MANGO (Qiao, Chen et al. 2021) proposes a one-stage text spotting framework with Mask Attention Guidance, allowing for the direct recognition without the need for RoI operations. The Mask TextSpotter series (Lyu, Liao et al. 2018, 2021; Liao, Pang et al. 2020) leverages the advantages of character-level annotations to achieve character segmentation and can recognize scene text of arbitrary shapes. PGNet (Wang, Zhang et al. 2021) introduces a graph refinement module to optimize coarse recognition and enhance end-to-end performance. Additionally, there is also a parallel mode of detection and recognition. TTS (Kittenplon, Lavi et al. 2022) proposes a weakly supervised learning method and employs a shared query mechanism for its detector and recognizer. Estextspotter (Huang, Zhang et al. 2023) and SRSTS (Wu, Lyu et al. 2022) process text detection and recognition in parallel, thereby decoupling text recognition from dependency on detection. Inspired by the DETR (Carion, Massa et al. 2020) family models. TESTR (Zhang, Su et al. 2022) proposes a framework free from Region-of-Interest operations and heuristics-driven post-processing procedures. DeepSolo (Ye, Zhang et al. 2023) is a DETR-like model that employs a query form with explicit points sampled from the Bezier center curve of text instance lines to efficiently encode the text's position, shape, and semantics.\nSimilar to scene text spotting methods, early VQA approaches also followed a two-stage task solutions (Singh, Natarajan et al. 2019; Hu, Singh et al. 2020; Yang, Lu et al. 2021), OCR results and pre-computed features are fed into a vision-and-language model. These approaches lack an interaction between text being recognized and the representation of its context."}, {"title": "Sequence-based Method", "content": "Inspired by the immense success in natural language processing, computer vision tasks are converging to Transformers. These approaches extract the image feature along with the transformer decoder to handle various tasks by predicting sequence. Pix2seq V1 and V2 (Chen, Saxena et al. 2021, 2022) show that it is possible to output boxes and labels as a"}, {"title": "Method", "content": "We introduce InstructOCR, an end-to-end scene text spotting method that leverages instructions to guide the model's output and enhance its understanding of the text. This enables the model to generate recognition results based on the provided instructions. The complete process is illustrated in Figure 2, the instructions are input into the model alongside the image. In the following sections, we will detail the structure of our model."}, {"title": "InstructOCR Architecture", "content": "InstructOCR is an encoder-decoder architecture that encompasses a text encoder, an image encoder, and a decoder, designed to synergistically process and interpret textual information within visual contexts.\nImage Encoder. The image encoder utilizes the ResNet50 architecture (He, Zhang et al. 2016) to extract features from the input image.\nText Encoder. The text encoder adopts BERT (Devlin, Chang et al. 2018), a 12-layer transformer specifically designed to extract features from instructions. By applying cross-attention between the extracted visual and textual features, we obtain instruction-based encoded features. These features are further processed by the decoder to generate the output sequence.\nDecoder. The decoder of InstructOCR following the approach of SPTS (Peng, Wang et al. 2022), utilizes an auto-regressive Transformer to generate long sequences for all text instances. Each text instance is represented by a sequence composed of three parts: [x, y, t], where (x, y) represents the center point coordinates and t represents the transcription text. To simplify the representation, the coordinates are uniformly discretized into integers ranging from 1 to 1000. The text is either padded or truncated to a fixed length of 25, and the maximum number of text instances in an image is set to 60. In the sequence representation, the < PAD > token is used to fill in the gaps for shorter text instances. Additionally, < SOS > and < EOS > tokens are inserted at the beginning and end of the sequence, respectively, to indicate the start and end of the sequence.\nFor the VQA task, the question is encoded by the text encoder, while the answer is treated as a direct sequence, as illustrated in Figure 3. The entire sequence length is set to 256."}, {"title": "Instructions Generation", "content": "Existing OCR annotations typically consist of locations and recognition results. To eliminate the need for human-"}, {"title": "Loss Function", "content": "In InstructOCR, the training objective is to predict tokens, and we utilize the standard cross-entropy loss for model training. This loss function aims to maximize the likelihood of the correct tokens during training. The mathematical expression of the cross-entropy loss is as follows:\n\n$L_{seq} = \\text{maximize} \\sum_{i=1}^{L} w_i \\log P(\\hat{s}_i | I, s_{1:i})$ (1)"}, {"title": "Experiments", "content": ""}, {"title": "Scene Text Spotting", "content": "In our experiments, we evaluate our method on Total-Text (Ch'ng and Chan 2017), ICDAR2015 (Karatzas et al. 2015), and ICDAR2013 (Karatzas, Shafait et al. 2013). Total-Text is an arbitrarily shaped word-level scene text benchmark, with 1,255 training images and 300 testing images. ICDAR2015 contains 1,000 training images and 500 testing images for quadrilateral scene text. ICDAR2013 contains 229 training images and 233 testing images with horizontal text."}, {"title": "VQA for Scene Text", "content": "Scene text VQA involves answering questions about the natural scene images or reasoning about the scene text. TextVQA (Singh, Natarajan et al. 2019) contains 45,336 questions on 28,408 images that require reasoning about text to answer. ST-VQA comprises 23, 038 images sourced from a combination of public datasets (Biten, Tito et al. 2019)"}, {"title": "Implementation Details", "content": "The Transformer encoder and decoder consist of 6 layers with 8 heads. The max length of recognition queries is 25 and the maximum number of objects is 60. The entire model is distributively trained on 32 NVIDIA A100-80G GPUs. We pretrain the model on a combination dataset that includes ICDAR2013, ICDAR2015, Total-Text, Curved Synthetic Dataset 150k (Liu, Chen et al. 2020), and ICDAR2017 MLT (Nayef, Yin et al. 2017). And the input for the text encoder is a fixed instruction: \u201c<Recognize all text>\u201d.\nWe use a batch size of 320, and the pretrain model is trained for 200 epochs, with an initial 5-epoch warm-up phase. We use AdamW optimizer with a learning rate of 4 x 10-4. The input image's short size is randomly resized to a range from 704 to 1024 (intervals of 32), the maximum length of image is set as 1024. Subsequently, the model is trained for another 40 epochs, with a fixed learning rate of 1 \u00d7 10-4, and the maximum length of image is set as 1920. Then, instructions are added, and the model is further trained for another 50 epochs. For the scene text spotting task, the model is fine-tuned on the corresponding real datasets for another 140 epochs, with a fixed learning rate of 1 \u00d7 10\u22125. For the scene text VQA task, the model is fine-tuned on the TextVQA and ST-VQA datasets for another 120 epochs. At the inference stage, we resize the image's maximum length shorter than 1920 pixels."}, {"title": "Comparison with Scene Text Spotting Methods", "content": "We evaluate the model using the point-based metric proposed in SPTS (Peng, Wang et al. 2022). Notably, our model adeptly outputs the coordinates of single-point and has been compared with other point-based methods. We have also listed methods based on bounding boxes for comparison. However, some of these methods use additional datasets,"}, {"title": "Total-Text: Arbitrarily-Shaped Text", "content": "To validate the generalization ability of our method for arbitrarily shaped scene text spotting, we tested our approach on Total-Text. The scene text spotting results are shown in Table 2, where InstructOCR significantly surpasses SPTS-V2 on TotalText, by 1.6% without a dictionary and by 0.1% with a \"full\" dictionary."}, {"title": "ICDAR2015: Multi-oriented Text", "content": "To evaluate the robustness of our method for multi-oriented text, we conduct experiments on ICDAR2015, with the results shown in Table 2. Our method outperforms the previous single-point methods across all dictionary settings. Notably, in the strong dictionary setting, InstructOCR achieves an Hmean of 82.5%, and after adding more training data, it reaches 87.5%."}, {"title": "ICDAR2013: Horizontal text", "content": "To further compare with point-based methods, we conduct experimental comparisons on ICDAR2013, which already has high baseline metrics, resulting in relatively smaller improvements. In the weak dictionary setting, InstructOCR achieves an Hmean of 92.4%, which is 0.6% higher than SPTS-v2."}, {"title": "Applicability to Scene-Text VQA", "content": "In this section, we further explore other scene-text related domains (Table 3). We show that InstructOCR is also applicable on VQA tasks with a considerable accuracy of 42.0% on TextVQA and 45.8% on ST-VQA. While apples-to-apples comparison is difficult due to different data and parameter sizes, we emphasize the applicability to the VQA task. Specifically, most of the recent works utilize strong backbones such as ViT (Dosovitskiy, Beyer et al. 2020) and large language models such as T5large (Raffel, Shazeer et al. 2020), while ours adopt ResNet-50 and BERT. Our model has, to the best of our knowledge, the least number of parameters (78M) among the similar levels of VQA performance, whereas other approaches range from hundreds of millions to even billions of parameters. It is well known that the VQA performance goes up as more pre-training data is included. The aforementioned methods employ up to millions of text-image pairs, not to mention the multimodal large language models that utilize various forms of large-scale data. Our model is not specifically tailored for VQA tasks and is trained only on 0.2M scene text images. These promising results show the utility and applicability of InstructOCR on image understanding. Our research further extends the boundaries of small-scale models in the VQA task beyond previous limits."}, {"title": "Ablation Studies", "content": "We conduct a thorough analysis to understand the individual contributions of each component in our framework, with a particular focus on the effectiveness of the VQA tasks."}, {"title": "Impact of Module Integration", "content": "In this section, we perform a comparative analysis to evaluate the effects of several proposed modules. Prior to validating the effectiveness of instructions, we assess the impact of incorporating a text encoder into the model. We compare the model's performance without a text encoder to its performance with an included and frozen text encoder. We then evaluate the performance metrics when the text encoder is integrated and trained throughout the entire process. The results indicate that the inclusion and active training of the text encoder significantly enhance the capabilities of InstructOCR. Finally, we incorporate instructions, observing further improvements in performance metrics. As shown in Table 4, in the strong dictionary setting, the inclusion of text encoder results in a 1.0% improvement, while the addition of instructions leads to a further increase of 0.4%. The experimental results demonstrate that incorporating a text encoder enables the model to more effectively comprehend text within images, and the addition of instructions can further facilitate this. Figure 5 presents some examples where the results became correct after incorporating human language instructions."}, {"title": "The performance of InstructOCR on the VQA task", "content": "In this section, we conduct experiments to verify the effectiveness of our model in VQA tasks. Since our model is pre-trained for scene text spotting, we choose the ST-VQA and TextVQA datasets, which relate to natural scene images. We hypothesize that our framework can be transferred to VQA tasks and that pretraining on text spotting is beneficial for downstream VQA tasks. Table 5 supports this argument."}, {"title": "Limitation", "content": "Owing to the VQA task requiring a large amount of data for pre-training and instruction fine-tuning, and different methods using inconsistent data amounts that make it impossible to align data volume. We only use data from the scene text spotting task for pre-training and only use the ST-VQA and TextVQA datasets for instruction fine-tuning. We do not add a large amount of data to demonstrate the best performance of InstructOCR on the VQA task."}, {"title": "Conclusion", "content": "In this paper, we propose InstructOCR, a novel instruction-based scene text spotting model that leverages instructions to enhance the understanding of text within images. The model integrates a text encoder that processes instructions and a visual-text fusion module that combines image features with these instructions to guide the decoding process. With the introduction of human language instructions and our meticulously designed instruction set based on text attributes, InstructOCR demonstrates an extraordinary ability to comprehend and process textual information within natural scenes, indicating the benefits of aligning human language instructions with visual text for OCR tasks. Future research will explore increasing training data and incorporating more instructions, thereby enabling the model to achieve higher metrics in VQA tasks and address a broader range of OCR challenges."}]}