{"title": "Leveraging Large Language Models to Generate Course-specific Semantically Annotated Learning Objects", "authors": ["Dominic Lohr", "Marc Berges", "Abhishek Chugh", "Michael Kohlhase", "Dennis M\u00fcller"], "abstract": "Background: Over the past few decades, the process and methodology of automated question generation (AQG) have undergone significant transformations. Recent progress in generative natural language models has opened up new potential in the generation of educational content.\nObjectives: This paper explores the potential of large language models (LLMs) for generating computer science questions that are sufficiently annotated for automatic learner model updates, are fully situated in the context of a particular course, and address the cognitive dimension understand.\nMethods: Unlike previous attempts that might use basic methods like ChatGPT, our approach involves more targeted strategies such as retrieval-augmented generation (RAG) to produce contextually relevant and pedagogically meaningful learning objects.\nResults and Conclusions: Our results show that generating structural, semantic annotations works well. However, this success was not reflected in the case of relational annotations. The quality of the generated questions often did not meet educational standards, highlighting that although LLMs can contribute to the pool of learning materials, their current level of performance requires significant human intervention to refine and validate the generated content.", "sections": [{"title": "1 | INTRODUCTION", "content": "The determinants of learning success have been extensively researched in various disciplines [1, 2]. Numerous empirical studies support the hypothesis that tailoring learning materials to learner's needs significantly increases the effectiveness of learning outcomes [3, 4]. The mastery learning theory by Bloom [5] or the personalized system of instruction theory by Keller [6] postulate teaching methodologies that highly build on individualized learning materials and assessment on a very fine-grained level. However, resource limitations and an increasingly diverse educational landscape challenge teachers and content creators in general.\nAdaptive learning systems like ALEA [7] promise to address this need by delivering Learning Objects (LOs), such as definitions, examples, or questions tailored to individual learners' specific prior knowledge, competencies, and preferences. One way of realizing this is by using semantically annotated LOs that allow for determining their pre-requisites, which concepts they address, and which competencies they intend to foster. In conjunction with a model of a learner's current knowledge and competencies, this allows for many valuable services, such as the automatic generation of flashcards or so-called guided tours: individually selected sequences of LOs (\"learning paths\") that can be automatically created by learners on demand by a simple click, to open up a specific concept in a specific cognitive dimension. To make this possible, the pool of semantically annotated LOs must be just as diverse as the educational biographies of the learners. However, developing sufficiently annotated LOs requires huge amounts of manual labor and expertise. This bottleneck underlines the need for innovative solutions that can generate customized learning materials automatically, thus becoming a focus of current research in computer science education. Attempts to (partially) automate the creation of quiz questions can be found in the literature as early as the 1970s [8].\nStudies on the role of artificial intelligence in education point out that a significant hurdle is the deficiency of suitable learning materials for individualized and adaptable learning [9], a problem that may be mitigated by the capabilities of large language models (LLMs). Research in computing education is exploring the capabilities of these models to generate educational content that is both contextually appropriate and educationally demanding, offering a promising solution to the constraints associated with manually creating content. However, the effectiveness of LLM-generated educational content is often limited by their lack of focus on specific course content and the learner's current understanding. The generated LOs often do not fit within the intended educational framework or meet the diverse needs of individual learners.\nThis paper presents the results of experiments on generating semantically annotated quiz questions using a state-of-the-art LLM. In particular, we investigate the question of to what extent LLMs can be used to generate questions in the domain of university-level computer science (CS) that are didactically valuable, are sufficiently annotated to allow for the above selection process, can ideally be graded by a software system (e.g. multiple choice or fill-in-the-blanks questions) to automatically and immediately increase the accuracy of the system's learner models, and are entirely situated in the context of a particular university-level course concerning terminology and notations used. Unlike earlier similar studies [10, 11], our task requires extensive context, rendering it unsuitable for naive approaches, e.g., using ChatGPT. Instead, we use more targeted techniques beyond \"prompt engineering\", such as retrieval-augmented generation (RAG)."}, {"title": "2 | RELATED WORK", "content": "Over the past few decades, the process and methodology of automated question generation (AQG) have undergone significant transformations, driven primarily by advancements in computational linguistics and artificial intelligence (AI). In the earlier stages, AQG relied heavily on rule-based systems that applied predefined templates and linguistic patterns to generate questions from text, requiring extensive manual crafting and domain-specific adjustments. First studies can be found by Wolfe [8].\nWith the development of deep learning and neural network models, a transition to more sophisticated, context-aware systems can be recognized. These models, particularly sequence-to-sequence and transformer-based architectures, have enabled the generation of more nuanced, relevant, and diverse questions by \"understanding\" deeper semantic relationships within the text. Kurdi et al. conducted a systematic review of empirical research focused on addressing the issue of AQG within educational contexts [12]. They thoroughly outlined various methods of generation, tasks, and evaluation techniques found in studies between 2015 and early 2019. A standard method involves identifying sentences in the text sources with high information content using topic words or key phrases. The system then selects a keyword as the answer key, removes it to form a question stem, and generates incorrect choices (distractors) using a clustering method without external data. These systems mainly produce questions testing remember factual knowledge, not understanding, a key focus of the work presented in this paper, and the results show that large language models (LLMs) can potentially benefit from semantics-based approaches to generate meaningful questions that are closely related to the source content.\nRecent progress on generative natural language models has opened up new potentials in the generation of educational content [13, 14, 15, 16]. In recent years, more and more approaches have been found to generate tasks in a single step using LLMs like GPT-3 instead of dividing the AQG process into several sub-tasks. Yan et al. did a systematic scoping review of articles published since 2017 to pinpoint the current state of research on using LLMs [17]. They identified content generation, including multiple-choice questions and feedback generation, as primary educational tasks that research aims to automate. McNichols et al. tested the effectiveness of LLM-based methods for automatic distractors and feedback generation using a real-world dataset of authentic student responses [18]. Their findings show that fine-tuning was ineffective and that other approaches than LLM-prompting need to be explored. Dijkstra et al. [19] developed a quiz generator based on OpenAl's GPT-3 model, fine-tuned on text-quiz pairs to generate complete multiple-choice questions, with correct answers and distractors. They noted that while most of the generated questions were of acceptable quality, creating high-quality distractors was more challenging than generating question-and-answer pairs.\nMore specifically, LLM-based approaches have recently been applied in programming education (for an intensive literature review, see [13]). Sarsa et al. [11] explored the capabilities of OpenAl's LLM Codex to generate programming exercises and code explanations. Their results show that the generated questions and explanations were novel,"}, {"title": "3 | REQUIREMENTS FOR THE GENERATED QUESTIONS", "content": "We posit that automated question generation (AQG) in the context of university courses poses additional challenges barely (or not at all) covered by the existing literature, especially in a domain like computer science:\nFirstly, we claim that in more abstract domains like mathematics and math-related subfields of computer science, questions solely focusing on remembering facts, or rudimentary application exercises are less suitable for the outcomes of a university-level class. This makes designing appropriate questions a more challenging task because doing so requires a level of understanding of and experience with the learning material.\nSecondly, although the topics covered in a particular course are common across different universities, the details are much less standardized (e.g., precise definitions, terminology, and notational conventions). This entails the additional requirement that questions (whether automatically generated or not) need to be formulated and situated in line with the conventions in a particular course, which are also more frequently subject to change. This makes approaches based on large amounts of dedicated training data, which necessarily do not generalize beyond a specific course and instructor's preferences, unsuitable. Instead, we should be able to provide the generator with the relevant learning materials (course notes, slides, etc.) whenever we want to generate a new batch of questions.\nAdditionally, our goal is to utilize the generated questions in an adaptive learning assistant that is capable of automatically selecting suitable questions based on the associated learning objective and a particular student's (estimated) prior knowledge in the form of a learner model. Therefore, we need to annotate the questions with the relevant information for this selection process, namely:\n1. The concepts the question is intended to test,\n2. the cognitive level the question targets (modeled as levels in Bloom's revised taxonomy [21]), and\n3. the prerequisite concepts occurring in the question and the associated competencies that a user should have mastered for the question to be suitable."}, {"title": "4 | METHODOLOGY", "content": ""}, {"title": "4.1 | Semantic Annotations in STEX", "content": "To semantically annotate learning objects - including questions - we use the STEX package [31] for ATEX. STEX uses an ontology based on OMDoc [32]: Concepts are represented as symbols that can be introduced via the \\symdecl-macro and can be related to each other in various ways. Symbols are always declared in modules (via the smodule-environment), which can import (the symbols of) other modules, adding another layer of relations and allowing for sharing concepts among large collections of disparate documents, thus enabling the collaborative and modular development of domains of knowledge as highly interrelated knowledge graphs independent of, and across, presentational context: symbols can have arbitrarily many definitions, formulations thereof, and different notations, to accommodate author's preferences without duplicating the domain knowledge itself.\nSymbols can be referenced in various ways; most importantly, the \\symref-macro allows annotating arbitrary text as representing a particular symbol, and formal notations can be produced via dedicated semantic macros that additionally associate the notation with the corresponding symbol. Since the details are largely irrelevant to the topic of this paper, we refer to [33] for details and Section 4.4 for examples.\nMore importantly, for our purposes, STEX also bundles the problem-package, providing dedicated markup macros and environments for various variants of quiz questions. Most importantly, it provides the sproblem-environment to annotate questions, within which we can use the mcb and scb environments for multiple and single choice blocks, respectively. As a third (autogradable) question type, the \\fillinsol macro can be used to generate a blank box for fill-in-the-blanks questions. In all three cases, we can mark (or provide) the correct answer(s), add feedback text to be displayed if a particular answer (correct or wrong) is chosen, and specify grading actions (i.e., set, add or deduct points) depending on the answers given. Additionally, we can specify the learning objectives and prerequisites as pairs of a symbol and one of the keywords remember, understand, apply, analyze, evaluate, create. Symbols referenced in the body of the question are automatically determined to be prerequisites with the cognitive dimension remember. For example, if a module declares the symbol plus for addition on integers, then the question \"what is 2 \\symref{plus}{added to} 2?\" automatically has the prerequisite (remember,plus). We refer to such annotations as relational annotations and to those that do not relate text to some symbol (e.g. fillinsol or sproblem) as structural annotations.\nBesides being ATEX compilable to pdf, we can use the RusTEX system [34] to convert the STEX documents to HTML, preserving the semantic annotations in the form of attributes on the HTML nodes. Additionally, this assigns a globally unique URI to the document itself and every section, module, symbol, and learning object therein, enabling searching, querying, and referencing all of these afterward across documents.\nSubsequently, we can embed various services that act on the semantic annotations directly into the HTML documents via JavaScript. Our learning assistant utilizes this to render quiz questions as interactive components, evaluate answers provided by students, display the appropriate feedback provided, and update a student's learner model accordingly (see Figure 1).\nWhile our usage of STEX is primarily motivated by our learning assistant, for the purposes of this paper, it offers additional advantages: Its underlying ontology, being designed around representing the semantics of mathematical statements formally, is consequently sufficiently expressive to subsume most semantic annotation systems, in particular those tailored specifically for quiz questions. Our results should, therefore, (all else being equal) generalize to other annotation schemes. Furthermore, since STEX inherits its syntax from ATEX, the LLM used does not need to be finetuned or explicitly prompted on the usage of a new or esoteric language - the huge amount of publicly available"}, {"title": "4.2 | Model Selection", "content": "Our goals put notable constraints on the methodology for generating questions:\n1. Targeting understanding rather than remembering factual knowledge implies that the model we use should be capable of \"synthesizing\" complex knowledge, excluding, e.g., smaller language models less capable of \"reasoning\".\n2. Since the knowledge domain and preferred conventions should not be fixed, deliberately training a model on dedicated course materials is largely not feasible, which calls for few-shot approaches. And\n3. since the latter point also entails providing possibly large amounts of course materials online during generation, the model used needs to be able to process large amounts of text at once. More precisely, it should allow for a large enough context window \u2013 the maximum number of tokens the model can consider in a single prompt/reply step.\nOpenAl's GPT-4 [35] shows consistently better results across all tasks compared to alternative large language models\u00b9. In particular, we initially experimented with the free GPT-3.5 model (that powers the free version of Chat-GPT), which quickly demonstrated very poor performance both concerning the questions generated and, more broadly, the ability to follow the instructions provided - meaning more prompt design was unlikely to improve the results significantly.\nSimilarly, most models (including almost all open-source models) primarily focus on short conversations and, therefore, only support a relatively small context window. For example, one of the currently most popular open source language models, LLaMA [36], can process a total input of 2048 tokens, whereas GPT-3.5 and GPT-4-Turbo have context windows of up to 16,385 and 128,000 tokens, respectively.2\nWe consequently opted for the (unfortunately commercial, closed source, and proprietary) GTP-4-Turbo model as the one holding the most promise with respect to our criteria at the time of the experiments. Unfortunately, this also means we can not provide access to a public instance of our pipeline since the API used to access the model is monitored and billed on a per-token basis.3"}, {"title": "4.3 | Overview of the Generation Pipeline", "content": "To allow for the model to generate questions for a specific course regarding terminology, definitions, and notations, we opted for a technique called retrieval-augmented generation (RAG). This technique gives models access to additional information beyond their training data by querying an external knowledge base (such as a database or a web search engine) for results relevant to the specific prompt. It concatenates them to the prompt itself before passing it into the model."}, {"title": "4.4 | Prompt Design", "content": "Despite ongoing attempts to declare \"prompt engineering\" a marketable skill, few hard principles for designing LLM prompts can be consistently well supported empirically. The most important rules of thumb can be briefly summarized thusly [37, 38]:\n1. Be specific in what the output should look like,\n2. be detailed by including any and all information and requirements relevant to the expected output,\n3. provide examples in the prompt, effectively transforming the task from a zero-shot to a few-shot approach, and, most importantly,\n4. iterate by repeatedly testing, observing problems with the outputs, and modifying the prompt to discourage these problems in subsequent iterations."}, {"title": "5 | RESULTS AND DISCUSSION", "content": ""}, {"title": "5.1 | Question Quality", "content": "The question types of the GQs were dominated by single choice questions (12 out of 30), followed by multiple choice questions (18 out of 30). The first consistently had three possible answers, while the latter almost always offered four options, usually with three distractors. The constant number of answers may be because the examples in the prompt also contain precisely this number of answers. With the MCQ, explicit care was taken to ensure that the example provided contained more than one correct answer. Nevertheless, the generated MCQs almost exclusively contained exactly three distractors. Notably, no fill-in-the-blank (FIB) questions appear in the generated questions. This is likely because designing meaningful FIB questions is significantly more challenging for the understand dimension, especially considering the necessary constraint that the answer be unambiguous.\nThe FIT of the GQs to the provided teaching material was evaluated by the experts as consistently given (28 out of 30), and most of them were assessed as solvable using the available teaching material (27 out of 30). In most cases,"}, {"title": "5.2 | Content Errors", "content": "Eleven out of 30 GQs contained errors, mainly in the answer options and the feedback. These errors occurred particularly frequently in the topics arc consistency and semantics of propositional logic. We can only speculate on the reasons for this. Still, it seems likely that it correlates with the conceptual complexity of the topics or how well the topic is represented in the model's original training data - semantics requires a much better understanding of the meaning of logical statements and the formal/mathematical mechanisms that provide statements with meaning (interpretation functions, models and their formal definitions, etc), and there are multiple ways to formulate these mathematically. Similarly, to understand arc consistency requires a solid intuition about how various constraints in a constraint satisfaction problem interact with respect to the variables in a particular problem.\nOne crucial result to consider is that the model may produce superficially plausible questions that align with the \"aesthetic\" of a good question but are false in not necessarily trivial ways.\nOne particularly striking example is the following question on the topic of propositional logic (generated in an earlier experiment),\nIf we have already established \u00acB, how can we use the Natural Deduction rule for implication (\u21d2) elimination to infer a new formula?\nGiven B \u21d2 C, deduce \u00acC\nwhere the answer above was considered the intended and only correct one. This is the common fallacy of denying the antecedent, but without understanding the semantics of propositional logic (or the natural deduction calculus) already, this is not immediately obvious.\nNotable here is that the question is not only wrong, but that it actively reinforces a common misconception that may not necessarily be caught even by, e.g., student employees who assume they are sufficiently knowledgeable in the domain to evaluate questions, which is arguably even more dangerous than a merely wrong question."}, {"title": "5.3 | Semantic Annotations", "content": "We can distinguish between two kinds of annotations in STEX: Structural annotations and relational annotations.\nStructural annotations, such as \\begin{sproblem}, \\begin{mcb} (for MCQ-blocks), \\mcc[<T|F>,feedback={...}]{<answer>}(for an answer in a MCQ-block), \\objective{<competency>}{<symbol>} etc. should be present in all questions, or all questions of a particular question type, and are very schematic in the sense that they are not particular to a specific question, topic or concept (other than the one explicitly provided as a parameter to the prompt). These annotations seemingly pose no significant challenge to the model - the few examples in the prompt are enough for the model to pick up on them immediately and use them consistently and correctly in (almost) all generated questions. Rarely does it hallucinate plausible but non-existent symbol names in the objectives."}, {"title": "5.4 | Limitations", "content": "We used a single model (GPT-4-turbo) to generate the questions for the evaluation. At the time of the study (March 2024), this model was \u2013 in our opinion \u2013 the most suitable model (see Section 4.2 for a detailed explanation). However, we explicitly do not rule out the possibility that there will be ongoing \"better\" models that are more suitable and can lead to better results. A further limitation of this study is that we did not involve students in evaluating the GQ. \u03a4\u03bf obtain objective evidence of the quality of the tasks, a survey of students on the tasks would be helpful and is planned for the future. Especially when evaluating multiple choice questions, for example, an assessment by (only) experts is not sufficient. Whether a distractor is good or bad usually only becomes apparent when the exercise is presented to"}, {"title": "6 | CONCLUSION", "content": "In this experiment, we investigated the extent to which large language models are suitable for generating semantically annotated quiz questions for higher education from semantically annotated course materials that are (1) suitable for a specific course and (2) address understanding a concept.\nOur research reveals significant limitations in the application of LLMs for this task. While generating questions that address remembering factual knowledge works well, creating questions that address understanding remains a notable challenge - which would need questions aiming for conceptual knowledge, or asking for explanations rather than recall of a simple concept. Despite LLMs' ability to generate a range of questions, domain experts' need for extensive filtering underscores the models' inadequacies in autonomously generating educationally valuable content. The quality of questions generated by LLMs often does not meet educational standards, primarily due to the simplistic nature of the questions and the lack of valuable (and correct) feedback. Thus, the required careful review process by experts is both time-consuming and counter to the goal of automated content creation. In addition, errors in the content of the questions are challenging to detect for non-experts, as semantically incorrect output from LLMs is known to be very close to the \"truth\" [41].\nOur results show that generating structural, semantic annotations works well. However, this success was not reflected in the case of relational annotations, which exhibited poor integration, indicating a limitation in the LLM's \"ability\" to contextualize and link concepts within the generated content effectively.\nThe results of the present work make clear that while LLMs can contribute to the pool of learning materials, their current state requires significant human intervention to refine and validate the generated content. So, despite the promises systems using LLMs give, the human-in-the-loop remains crucial. Although the prompt was carefully designed and we provided a lot of static and dynamic context, the results still show weaknesses in quality and semantic annotation. Nevertheless, further studies with a high number of generated tasks and more experts might show that the automatic generation of questions following our approach could lead to more efficiency when used as an assistive system for the preparation process."}]}