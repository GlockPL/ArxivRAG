{"title": "Stochastic interior-point methods for smooth conic optimization with applications", "authors": ["Chuan He", "Zhanwang Deng"], "abstract": "Conic optimization plays a crucial role in many machine learning (ML) problems. However, practical algorithms for conic constrained ML problems with large datasets are often limited to specific use cases, as stochastic algorithms for general conic optimization remain underdeveloped. To fill this gap, we introduce a stochastic interior-point method (SIPM) framework for general conic optimization, along with four novel SIPM variants leveraging distinct stochastic gradient estimators. Under mild assumptions, we establish the global convergence rates of our proposed SIPMs, which, up to a logarithmic factor, match the best-known rates in stochastic unconstrained optimization. Finally, our numerical experiments on robust linear regression, multi-task relationship learning, and clustering data streams demonstrate the effectiveness and efficiency of our approach.", "sections": [{"title": "1 Introduction", "content": "Conic optimization covers a broad class of optimization problems with constraints represented by convex cones, including common forms such as linear constraints, second-order cone constraints, and semidefinite constraints. Over the years, this class of optimization problems has found applications across various fields, including control [21], energy systems [55], combinatorial optimization [49], and machine learning (ML) [40]. In practice, the efficient handling of traditional conic optimization problems has been extensively studied for decades, with interior-point methods (IPMs) standing out due to their ability to effectively and elegantly solve a wide range of conic constrained problems within a unified framework (see the monograph by [35])."}, {"title": "1.1 Stochastic optimization with conic constraints for machine learning", "content": "Existing studies on IPMs primarily focus on the deterministic regime, despite the recent widespread applications of stochastic conic optimization in ML. These applications include multi-task relationship learning [4, 54], robust learning with chance constraints [39, 51], kernel learning [5], and clustering data streams [10, 36, 42]. Next, we briefly highlight two examples.\nMulti-task relationship learning Multi-task learning is an ML paradigm where related tasks are learned together to improve generalization by sharing information (see [53]). In many applications, task correlations are not explicitly available. To learn these correlations from data, a regularization framework is proposed, where the relationships between models for different tasks are controlled by a regularizer defined using the covariance matrix \u03a9 [4, 54]:\n$\\min_{\\tiny{W\\in \\mathbb{R}^{p\\times d},\\\\Omega\\in \\mathbb{R}^{p\\times p}}} \\frac{1}{m} \\sum_{i=1}^p \\sum_{j=1}^m l(W_i, A_{ij}) + \\lambda \\text{tr}(W^T P(\\Sigma) W) \\quad \\text{s.t.} \\quad \\Sigma \\in \\mathbb{S}_+^p, \\; \\text{tr}(\\Sigma) = 1,$\nwhere W = [w\u2081,..., wp]\u1d40, SP denotes the positive semidefinite cone, l(\u00b7,\u00b7) is the loss function, wi and {aij}=1 are respectively the model weight and the training set for the ith task, 1 \u2264 i \u2264 p, \u03bb > 0 is a tuning parameter, P : RP\u00d7P \u2192 RPXP is a given map that controls the interaction between Wand \u03a9, and tr(.) denotes the trace of a matrix.\nRobust learning with chance constraints Consider supervised learning with feature-label pairs {(ai, bi)}=1, where the a\u00bf's are assumed to be generated from a distribution D with expected value \u0101 and covariance matrix \u03a3. \u03a4\u03bf mitigate the uncertainty in the ai's, a chance constraint Pa\u223cD(|w\u00b2 (a\u2212\u0101)| \u2265 0) \u2264 \u03b7 is proposed to be incorporated when preforming robust linear regression [39, 51], where \u03b8 and \u03b7 are the confidence level and desired probability, respectively. By approximating this chance constraint with a second-order cone constraint, [39] propose the following robust regression problem with conic constraints:\n$\\min_{w\\in \\mathbb{R}^d,\\;\\theta,v \\geq 0} \\frac{1}{p} \\sum_{i=1}^p (w^T a_i - b_i)^2 + \\lambda_1 \\theta + \\lambda_2 v \\quad \\text{s.t.} \\quad (v, \\theta) \\in \\mathcal{Q}^{d+1}, \\; (\\Sigma^{1/2} w, \\sqrt{\\eta} \\theta) \\in \\mathcal{Q}^{d+1},$\nwhere (\u00b7) is the loss, \u03bb\u2081, \u03bb2 > 0 are tuning parameters, and Qd+1 = {(u,t) \u2208 Rd \u00d7 R+ : ||u|| \u2264 t} denotes the second-order cone.\nBoth examples in (1) and (2) involve nonlinear conic constraints and are typically coupled with large datasets in real applications. However, stochastic algorithms for addressing general conic optimization problems in ML remains largely underdeveloped. Existing algorithmic developments for conic constrained ML problems are limited to specific use cases: for instance, alternating minimization is widely employed in the literature [4, 54] to solve (1), while [39] formulates (2) as second-order cone programming, assuming that is convex. Nevertheless, these developments do not unify the treatment of conic constraints and often lack convergence guarantees for problems with nonconvex objective functions.\nIn this paper we aim for proposing a stochastic interior-point method (SIPM) framework for smooth conic optimization problems, including (1) and (2) as special cases. This class of problems takes the following form:\n$\\min_X f(x) \\quad \\text{s.t.} \\quad x \\in \\Omega = \\{ x : Ax = b, x \\in K \\},$\nwhere f is continuously differentiable and possibly nonconvex on \u03a9, but its derivatives are not accessible. Instead, we rely on stochastic estimators G(\u00b7;\u00a3) of \u2207f(\u00b7) (see Assumptions 1(c) and 3(b) for assumptions on G(\u00b7; \u00a7)). Here, A \u2208 Rm\u00d7n is of full row rank, b \u2208 Rm, and K \u2286 R\u2033 is a closed and pointed convex cone with a nonempty interior. Assume that (3) has at least one optimal solution."}, {"title": "1.2 Our contributions", "content": "In this paper we propose an SIPM framework (Algorithm 1) for solving problem (3), which, to the best of our knowledge, is the first stochastic algorithmic framework for this problem. Building on Algorithm 1, we introduce four novel SIPM variants by incorporating different stochastic gradient estimators: mini-batch estimators, Polyak momentum, extrapolated Polyak momentum, and recursive momentum. Under mild assumptions, we establish the global convergence rates for these variants.\nOur main contributions are highlighted below.\n\u2022 We propose an SIPM framework (Algorithm 1) for solving problem (3). To the best of our knowledge, this is the first stochastic algorithmic framework for ML problems with general conic constraints. Building upon this framework, we introduce four novel SIPM variants that incorporate different stochastic gradient estimators.\n\u2022 We establish the global convergence rates of our SIPMs using mini-batch estimations, Polyak momentum, and extrapolated Polyak momentum under locally Lipschitz assumptions (Assumptions 1(b) and 2(b)), which relax the globally Lipschitz assumptions commonly used in the literature (e.g., [23, 16]). In addition, we introduce a novel local-norm-truncated recursive momentum scheme into an SIPM and establish its global convergence rate.\n\u2022 We conduct numerical experiments (Section 4) to compare our SIPMs with existing methods on robust linear regression, multi-task relationship learning, and clustering data streams. The results demonstrate that our SIPMs achieve solution quality comparable to or better than existing methods."}, {"title": "1.3 Related work", "content": "Interior-point methods In the deterministic regime, IPMs are recognized as a fundamental and widely used algorithmic approach for solving constrained optimization problems, having been extensively studied for decades [2, 11, 26, 35, 37, 46, 47, 50, 22]. In particular, primal-dual IPMs are a popular class of methods that iterate towards to an optimal solution by applying Newton's method to solve a system of equations derived from the perturbed first-order necessary conditions of the optimization problem [47, 50]. Another early and classical family of IPMs is the affine scaling method, which iteratively improves a feasible point within the relative interior of the feasible region by scaling the search direction to prevent the solution from exiting the boundary [45, 18]. The SIPMs developed in this paper are more closely aligned with the algorithmic ideas of affine scaling methods. Moreover, IPM-based solvers are widely adopted for large-scale constrained optimization, including Ipopt [47], Knitro [11], and LOQO [46] for functional constrained problems, and SDPT3 [43] and SeDuMi [41] for conic constrained problems.\nStudies on SIPMs have only emerged recently. In particular, [6] and [33] propose randomized IPMs for minimizing a linear function over a convex set. In addition, [14] introduces an SIPM for bound-constrained optimization by augmenting the objective function with a log-barrier function, and [13] generalizes this approach to solve inequality constrained optimization problems."}, {"title": "2 Notation and preliminaries", "content": "Throughout this paper, let Rn denote the n-dimensional Euclidean space and \u3008\u00b7,\u00b7) denote the standard inner product. We use ||\u00b7 || to denote the Euclidean norm of a vector or the spectral norm of a matrix. For the closed convex cone K, its interior and dual cone are denoted by intK and K*, respectively. We define the barrier function of problem (3) as:\n$\\varphi_{\\mu}(x) = f(x) + \\mu B(x) \\quad \\forall x \\in \\Omega^\\circ = \\{ x \\in \\text{int} K : Ax = b \\}.$\nFor a finite set B, let |B| denote its cardinality. For any t \u2208 R, let [t]+ and [t]- denote its nonnegative and nonpositive parts, respectively (i.e., set to zero if t is positive or negative, respectively). We use the standard big-O notation O(.) to present convergence rate, and \u00d5(\u00b7) to represent the order with a logarithmic factor omitted.\nFor the rest of this section, we review some background on logarithmically homogeneous self-concordant barriers and introduce the approximate optimality conditions for our SIPMs."}, {"title": "2.1 Logarithmically homogeneous self-concordant barrier", "content": "Logarithmically homogeneous self-concordant (LHSC) barrier functions play a crucial role in the development of IPMs for conic optimization (see [35]). In this paper the design and analysis of SIPMs also heavily rely on the LHSC barrier function. Throughout this paper, assume that Kis equipped with a\n-LHSC barrier function B for some v \u2265 1. Specifically, B : intK \u2192 R satisfies the following conditions: (i) B is convex and three times continuously differentiable in intK, and moreover, |'''(0)| \u2264 2(6\" (0))3/2 holds for all x \u2208 intK and u \u2208 R\", where y(t) = B(x+tu); (ii) B is a barrier function for K, meaning that B(x) goes to infinity as x approaches the boundary of K; (iii) B satisfies the logarithmically homogeneous property: B(tx) = B(x) \u2013 9 lnt for all x \u2208 intK, t > 0.\nFor any x \u2208 intK, the function B induces the following local norms for vectors:\n$||v||_x = \\langle v, \\nabla^2 B(x) v \\rangle^{1/2}, \\quad ||v||_* = \\langle v, \\nabla^2 B(x)^{-1} v \\rangle^{1/2} \\quad \\forall v \\in \\mathbb{R}^n.$"}, {"title": "2.2 Approximate optimality conditions", "content": "Since f is nonconvex, finding a global solution to (3) is generally impossible. Instead, we aim to find a point that satisfies approximate optimality conditions, as is common in nonconvex optimization. For deterministic IPMs developed to solve (3), the following approximate optimality conditions are proposed in [25, 24]:\nx \u2208 intK, Ax = b, \u2207 f(x) + A\u03bb\u2208K*, ||\u2207f(x) + AT ||* < \u20ac\nwith a given tolerance \u20ac > 0. In addition, stochastic algorithms typically produce solutions that satisfy approximate optimality conditions only in expectation. To facilitate our developments of SIPMs, we next derive an alternative approximate optimality condition for (3) using B, which is a sufficient condition for (7). Its proof is deferred to Section 5.1.\nLemma 1. Let \u00b5 > 0. Suppose that (x, \u03bb) \u2208 \u03a9\u00b0 \u00d7 Rm satisfies ||\u2207\u03c6\u03bc(x) + \u0391\u03bb||* \u2264 \u03bc, where \u03c6\u03bc and \u03a9\u00b0 are given in (4). Then, (x, 1) also satisfies (7) with \u0454 = (1 + \u221a\u03b8)\u03bc.\nThe above lemma offers an alternative approximate optimality condition for (3). We extend this condition to an expectation form and define a \u00b5-stochastic stationary point for problem (3), which our SIPMs aim to achieve.\nDefinition 1. Let \u03bc > 0. We say that x \u2208 \u03a9\u00b0 is a \u03bc-stochastic stationary point of problem (3) if it, together with some \u03bb\u2208 Rm, satisfies E[||\u2207\u03c6\u03bc(x) + ATX||*] \u2264 \u03bc, where the expectation is taken over the random variables involved in the algorithm."}, {"title": "3 Stochastic interior-point methods", "content": "In this section we propose an SIPM framework for solving (3) and then analyze the global convergence rates of four SIPM variants. We now make the following additional assumptions that will be used throughout this section.\nAssumption 1. (a) The feasible region \u03a9 is compact, and the Slater's condition holds; that is, \u03a9\u00b0 \u2260 \u00d8, where \u03a9 and \u03a9\u00b0are given in (3) and (4), respectively.\n(b) There exists L\u2081 > 0 such that\n$||\\nabla f(y) - \\nabla f(x)||_* \\leq L_1 ||y - x||_x \\quad \\forall x,y \\in \\Omega^\\circ \\text{ with } ||y - x||_x < \\delta_{\\eta},$\nwhere s\u03b7 \u2208 (0,1) is a user-defined input of Algorithm 1 to control the step sizes.\n(c) The stochastic gradient estimator G : \u03a9\u00b0\u00d7\u039e \u2192 R\u2033 satisfies\n$\\mathbb{E}_{\\xi}[G(x; \\xi)] = \\nabla f(x), \\quad \\mathbb{E}_{\\xi}[(||G(x; \\xi) - \\nabla f(x)||^2)] \\leq \\sigma^2 \\quad \\forall x \\in \\Omega^\\circ$\nfor some \u03c3 > 0.\nRemark 1. (i) Since f is continuous on \u03a9 and B is convex, we see from Assumption 1(a) that f and B are bounded below on \u03a9 and \u03a9\u00b0, respectively. For convenience, define\n$\\Delta(x) = f(x) + [B(x)]_+ - f_{low} - [B_{low}]_- \\quad \\forall x \\in \\Omega^\\circ,$\nwhere flow = infz\u2208n f(x) and_Blow = infz\u2208N\u00b0 \u0392(x).\n(ii) Assumption 1(b) implies that \u2207f is locally Lipschitz continuous on \u03a9\u00b0 with respect to local norms. Since N\u00ba is bounded under Assumption 1(a), it follows from Theorem 1 in [25] that \u22072B(x)-1 is bounded above for all x \u2208 \u03a9\u00b0. Consequently, using (5), we see that Assumption 1(b) holds if \u2207f is globally Lipschitz continuous on \u03a9\u00b0. For convenience, we define\n$L_{\\phi} = L_1 + 1/(1 - \\delta_{\\eta}),$\nwhich denotes the Lipschitz constant of \u2207\u03c6\u03bc for any \u03bc\u2208 (0,1] (see Lemma 11 below).\n(iii) Assumption 1(c) implies that G(\u00b7; \u00a7) is an unbiased estimator of \u2207f(\u00b7) and that its variance, with respect to the local norm, is bounded above. As noted in Remark 1(ii), \u22072B(x)\u2212\u00b9 is bounded for all x \u2208 \u03a9\u00b0. By this and (5), we have that the second relation in (9) holds if the variance of G(\u00b7; \u00a7) with respect to the Euclidean norm is bounded.\nIn what follows, we propose an SIPM framework in Algorithm 1 for solving problem (3). Subsequently, we will employ four distinct stochastic estimators to construct {mk}k>0, with specific schemes provided in (13), (18), (24), and (34), respectively.\nWe remark that computations involving \u22072B(xk)-1 can be performed efficiently for common nonlinear cones K. For example, when Kis the second-order cone, \u2207B(xk)\u22121 can be computed analytically (e.g., see [3]), and (AV2B(xk)-1AT)-1v for any v \u2208 Rm can be efficiently evaluated using Cholesky factorization. When K is the semidefinite cone, we have that \u22072B(Xk)\u22121[V] = XkVXk holds for any n \u00d7 n symmetric matrix V, and that AV2B(Xk)-1AT can be efficiently computed by exploiting the sparsity of A (see [43] for details).\nAlgorithm 1 An SIPM framework\nInput: starting point x\u00ba \u2208 \u03a9\u00b0, maximum step length s\u03b7 \u2208 (0,1), nonincreasing barrier parameters {k}k\u22650 C (0,1].\nLet {nk}k>o be a sequence of predefined or adaptive step sizes satisfying \u03b7\u03ba \u2208 (0, s\u014b] for all k \u2265 0. for k = 0, 1, 2, . . . do Construct a stochastic estimator mk for \u2207f(xk), and set mk = m* + \u03bc\u03ba\u2207B(xk). Update dual and primal variables as follows:\n$ \\lambda^{k} = -(AH_k A^T)^{-1} AH_k m^{k}, \\quad x^{k+1} = x^{k} - \\eta_k H_k (m^{k} + A^T \\lambda^{k})/|| m^{k} + A^T \\lambda^{k} ||_*^{k},$\nwhere Hk = \u22072B(xk)\u22121. end for\nOur next lemma shows that all iterates {xk}k>o generated by Algorithm 1 are strictly feasible, i.e., xk \u2208 \u03a9\u00b0 holds for all k \u2265 0. Its proof is deferred to Section 5.1.\nLemma 2. Suppose that Assumption 1 holds. Let {xk}k>0 be generated by Algorithm 1. Then, ||xk+1\n- xk||xk = Nk and xk \u2208 N\u00b0 for all k \u2265 0, where N\u00b0is defined in (4).\nIn the remainder of this section, we propose four variants of SIPMs in Sections 3.1 to 3.4 and study their global convergence rates. The developments and guarantees of these four SIPM variants are independent of each other."}, {"title": "3.1 An SIPM with mini-batch estimators", "content": "We now describe a variant of Algorithm 1, where the step size sequence {k}k>0 is predefined and nonincreasing, and {m}k>0 is constructed using mini-batch estimators:\n$m^{k} = \\frac{1}{|B_k|} \\sum_{i \\in B_k} G(x^{k}; \\xi^{k}), \\quad \\forall k \\geq 0,$\nwhere G(,) satisfies Assumption 1(c), and the sequence {Bk}k>o denotes the sets of sample indices. We refer this variant as SIPM with mini-batch estimators (SIPM-\u039c\u0395).\nThe following lemma establishes an upper bound for the estimation error of the mini-batch estimators defined in (13), and its proof is deferred to Section 5.3.\nLemma 3. Suppose that Assumption 1 holds. Let {xk}k>0 be generated by Algorithm 1 with {m}k\u22650 constructed as in (13), and let {(\u014bk, Bk)}k\u22650 be predefined algorithm inputs. Then,\n$\\mathbb{E}_{\\{\\xi\\}_{i\\in B_k}} [ (||m^{k} - \\nabla f(x^{k})||_{*}^{k})^2 ] \\leq \\sigma^2 / |B_k|, \\quad \\forall k \\geq 0,$\nwhere o is given in Assumption 1(c).\nWe next provide an upper bound for the average expected error of the stationary condition across all iterates generated by SIPM-ME. Its proof is relegated to Section 5.3.\nTheorem 1. Suppose that Assumption 1 holds. Let {(xk, k)}k>o be a sequence generated by Algorithm 1 with {m}k\u22650 constructed as in (13), and let {(\u03b7\u03ba, \u0392k, \u03bc\u03ba)}k\u22650 be predefined algorithm inputs. Assume that {k}k\u22650 is nonincreasing. Then, for any K \u2265 1,\n$\\frac{1}{K} \\sum_{k=0}^{K-1} \\mathbb{E}[|| \\nabla \\Phi_{\\mu_k} (x^{k}) + A^T \\lambda^{k} ||_*^{k}] \\leq \\frac{\\Delta(x^0)}{\\eta_{K-1}} + \\sum_{k=0}^{K-1} \\eta_k (\\frac{2 \\sigma}{\\sqrt{|B_k|}} + \\frac{L_{\\phi}}{2} \\eta_k),$\nwhere \u2206(\u00b7), L\u00a2, and \u03c3 are given in (10), (11), and Assumption 1(c), respectively."}, {"title": "3.1.1 Hyperparameters and convergence rate", "content": "To establish the convergence rate of SIPM-ME, we specify its inputs {(\u014bk, Bk, \u03bck)}k\u22650 as\n$\\eta_k = \\delta_{\\eta} / (k + 1)^{1/2}, \\quad |B_k| = k + 1, \\quad \\mu_k = \\frac{\\text{ln}^2 (k + 7)}{4 (k+1)^{1/2}} \\quad \\forall k \\geq 0,$\nwhere s\u03b7 \u2208 (0,1) is a user-defined maximum length for step sizes in Algorithm 1. It follows that {k}k\u22650 C (0, S\u014b] and {\u00b5k}k\u22650 C (0, 1], with both sequences decreasing.\nThe following theorem presents the global convergence rate of SIPM-ME with inputs specified in (16). Its proof is deferred to Section 5.3.\nTheorem 2. Suppose that Assumption 1 holds. Consider Algorithm 1 with {m}k\u22650 constructed as in (13) and {(\u03b7\u03ba, Bk,\u00b5k)}k>0 specified as in (16). Let \u043a(k) be uniformly drawn from {k,...,2k \u2212 1}, and define\n$K_{me} = \\text{max} \\{5, \\text{exp} (32 \\Delta (x^0) / \\delta_{\\eta} + 64 \\sigma + 16 \\delta_{\\eta} L_{\\phi})\\},$\nwhere (\u00b7) is defined in (10), L\u00f8 and o are given in (11) and Assumption 1(c), respectively, and sn is an input of Algorithm 1. Then, E[||\u2207\u03c6\u03bc\u03ba(k) (xk(k)) + ATXk(k) ||*\u3093(k)] \u2264 \u03bc\u03ba(k) holds for all k > Kme\u00b7\nRemark 2. From Theorem 2, we see that when k > Kme, SIPM-ME can return a \u03bc\u03ba(k)-stochastic stationary point of problem (3) within 2k iterations. In addition, using the fact that \u03ba(k) \u2208 {k, ...,2k-1} and the definition of {k}k\u22650 in (16), we deduce that \u03bc\u03ba(k) = \u00d5(k-1/2). Therefore, SIPM-ME achieves a global convergence rate of O(k-1/2)."}, {"title": "3.2 An SIPM with Polyak momentum", "content": "We now propose a variant of Algorithm 1, in which the step size sequence {nk}k>0 is predefined and nonincreasing, and {m}k>0 is constructed using Polyak momentum [16] as follows:\n$\\gamma_{-1} = 1, \\quad m^{-1} = 0, \\quad m^{k} = (1 - \\gamma_{k-1})m^{k-1} + \\gamma_{k-1} G(x^{k},\\xi^{k}) \\quad \\forall k \\geq 0,$\nwhere G(,) satisfies Assumption 1(c), and {k}k>0 denotes the sequence of momentum parameters. We refer this variant as SIPM with Polyak momentum (SIPM-PM).\nThe next lemma provides the recurrence relation for the estimation error of the gradient estimators based on Polyak momentum defined in (18). Its proof is deferred to Section 5.4.\nLemma 4. Suppose that Assumption 1 holds. Let {xk}k>0 be generated by Algorithm 1 with {m}k\u22650 constructed as in (18), and {(7k,k)}k\u22650 be predefined algorithm inputs. For all k \u2265 0, assume that Yk > \u014bk holds, and define \u0430\u043a ; = 1 \u2212 (1 \u2212 \u03b3\u03ba)/(1 \u2013 \u03b7\u03ba). Then,\n$\\mathbb{E}_{\\xi_{k+1}}[(|| m^{k+1} - \\nabla f(x^{k+1}) ||_{*}^{k+1})^2] \\leq (1 - \\alpha_k) (|| m^{k} - \\nabla f(x^{k}) ||_{*}^{k})^2 + \\frac{L_1^2 \\eta_k^2}{\\alpha_k} + \\sigma^2 \\gamma_k^2 \\quad \\forall k \\geq 0,$\nwhere L\u2081 and o are given in Assumption 1.\nWe now derive an upper bound for the average expected error of the stationary condition across all iterates generated by SIPM-PM. Its proof is deferred to Section 5.4.\nTheorem 3. Suppose that Assumption 1 holds. Let {(xk, k)}k>o be a sequence generated by Algorithm 1 with {m}k\u22650 constructed as in (18), and let {(nk, Yk,\u00b5k)}k\u22650 be predefined algorithm inputs. Assume that {k}k\u22650 is nonincreasing and also that Yk > \u014bk holds for all k \u2265 0. Define ak = 1 \u2212 (1 \u2212 \u03b3\u03ba)/(1 \u2212 \u03b7\u03ba) for all k \u2265 0. Then,\n$\\frac{1}{K} \\sum_{k=0}^{K-1} \\mathbb{E}[|| \\nabla \\Phi_{\\mu_k} (x^{k}) + A^T \\lambda^{k} ||_{*}^{k}] \\leq \\frac{\\Delta(x^0) + \\frac{\\sigma^2}{L_1}}{\\eta_{K-1}} + \\frac{L_1}{\\eta_{K-1}} \\sum_{k=0}^{K-1} (\\frac{L_1 \\eta_k^2}{\\alpha_k} + \\frac{2 L_1 \\eta_k^2}{\\alpha_k} + \\sigma^2 \\gamma_k^2),$\nwhere \u2206(\u00b7) is defined in (10), L\u00f8 is given in (11), and L\u2081 and o are given in Assumption 1."}, {"title": "3.2.1 Hyperparameters and convergence rate", "content": "To analyze the convergence rate of SIPM-PM, we specify its inputs {(\u014bk, \u221ak, \u03bck)}k\u22650 as:\n$\\eta_k = \\delta_{\\eta} / (k + 2)^{3/4}, \\quad \\gamma_k = 1/ (k + 2)^{1/2}, \\quad \\mu_k = \\frac{\\text{ln}^2 (k + 150)}{22 (k+2)^{1/4}} \\quad \\forall k \\geq 0,$\nwhere s\u03b7 \u2208 (0,1) is a user-defined input of Algorithm 1. It can be verified that {k}k\u22650 C (0,8\u03b7] and {k}k\u22650 C (0, 1], with both sequences decreasing. From (21) and s\u03b7 \u2208 (0,1), we observe that the sequence {ak}k>0 defined in Lemma 4 and Theorem 3 satisfies:\n$\\alpha_k \\geq \\frac{(k + 2)^{1/4} - \\delta_{\\eta}}{(k + 2)^{3/4} - \\delta_{\\eta}} > \\frac{(k + 2)^{1/4} - 1}{(k + 2)^{3/4}} = \\frac{1}{(k + 2)^{1/2}} - \\frac{1}{7 (k+2)^{1/2}} \\quad \\forall k \\geq 0.$\nThe following theorem presents the global convergence rate of SIPM-PM with its inputs specified in (21). Its proof is relegated to Section 5.4.\nTheorem 4. Suppose that Assumption 1 holds. Consider Algorithm 1 with {mk}k>0 constructed as in (18) and {(\u03b7\u03ba, /k,\u00b5k)}k>0 specified as in (21). Let \u043a(k) be uniformly drawn from {k, ..., 2k \u2013 1}, and define\n$K_{pm} = \\text{max}\\{3, \\text{exp} (176 (\\Delta (x^0) / \\delta_{\\eta} + 2 \\sigma^2 / (\\delta_{\\eta} L_1) + \\delta_{\\eta} L_{\\phi} + 14 \\delta_{\\eta} L_1))\\},$\nwhere \u2206(\u00b7) is defined in (10), L\u00f8 is given in (11), L\u2081 and \u03c3 are given in Assumption 1, and sn is an input of Algorithm 1. Then, E[||\u2207\u03c6\u03bc\u03ba(k) (xk(k)) + AT\u03bb\u03ba(k) ||*\u03ba(k)] \u2264 \u03bc\u03ba(k) holds for all k > Kpm.\nRemark 3. From Theorem 4, we see that when k > Kpm, SIPM-PM can return a \u03bc\u03ba(k)-stochastic stationary point of problem (3) within 2k iterations. Moreover, using the fact that \u03ba(k) \u2208 {k, . . ., 2k \u2212 1} and the definition of {\u00b5k}k\u22650 in (21), we find that \u03bc\u03ba(k) = O(k-1/4). Therefore, SIPM-PM achieves a global convergence rate of \u00d5(k-1/4). This rate matches the rate for the unconstrained case [16], up to a logarithmic factor."}, {"title": "3.3 An SIPM with extrapolated Polyak momentum", "content": "We now propose a variant of Algorithm 1, where the step size sequence {nk}k\u22650 is predefined and nonincreasing, and {m}k>0 is constructed based on extrapolated Polyak momentum [16] as follows:\n$\\gamma_{-1} = 1, \\; x_{-1} = x_0, \\; m_{-1} = 0,$\n$z^k = x^k + \\frac{1-\\gamma_{k-1}}{\\gamma_{k-1}} (x^k - x^{k-1}), \\quad m^{k} = (1 - \\gamma_{k-1})m^{k-1} + \\gamma_{k-1} G(z^{k},\\xi^{k}) \\quad \\forall k > 0,$\nwhere G(,) satisfies Assumption 1(c), and {k}k\u22650 C (0, +\u221e) denotes momentum parameters. We refer to this variant as SIPM with extrapolated Polyak momentum (SIPM-EM).\nTo analyze SIPM-EM, we make the following additional assumption regarding the local Lipschitz continuity of \u22072f.\nAssumption 2. (a) The function f is twice continuously differentiable on \u03a9\u00b0.\n(b) There exists L2 > 0 such that\n$|| \\nabla^2 f (y) - \\nabla^2 f(x)||_* \\leq L_2||y-x||_x \\quad \\forall x,y \\in \\Omega^\\circ \\text{ with } ||y - x||_x < \\delta_{\\eta},$\nwhere s\u03b7 \u2208 (0,1) is a user-defined input of Algorithm 1.\nThe following lemma shows that the iterates {zk}k>0 generated by SIPM-EM lie in \u03a9\u00b0. Its proof is deferred to Section 5.5.\nLemma 5. Suppose that Assumptions 1 and 2 hold. Let {zk}k\u22650 be generated by Algorithm 1 with {mk}k\u22650 constructed as in (24), and let {(nk, k)}k\u22650 be predefined algorithm inputs. Assume Nk/Yk \u2264 \u03b4\u03b7 for all k \u2265 0, where sn is an input of Algorithm 1. Then, zk \u2208 N\u00b0 for all k \u2265 0, where N\u00b0 is defined in (4).\nThe next lemma provides the recurrence relation for the estimation error of the gradient estimators based on extrapolated Polyak momentum defined in (24). Its proof is deferred to Section 5.5.\nLemma 6. Suppose that Assumptions 1 and 2 hold. Let {xk}k\u22650 be generated by Algorithm 1 with {mk}k\u22650 constructed as in (24), and let {(\u014bk, Yk)}k>0 be predefined algorithm inputs. Assume that {k}k\u22650 is nonincreasing, and that 7k/Yk \u2264 sn holds for all k \u2265 0, where sn is an input of Algorithm 1. Define \u0430\u043a = 1 \u2212 (1 \u2212 \u03b3\u03ba)/(1 \u2013 \u014bk) for all k \u2265 0. Then,\n$\\mathbb{E}_{\\xi_{k+1}}[(|| m^{k+1} - \\nabla f(x^{k+1}) ||_{*}^{k+1})^2] \\leq (1 - \\alpha_k) (|| m^{k} - \\nabla f(x^{k}) ||_{*}^k)^2 + \\frac{L_2^2 \\eta_k^4}{(1 - \\eta_0)^2 \\text{min}\\{2, 1\\}\\alpha_k} + \\frac{\\sigma^2 \\gamma_k^2}{(1 - \\eta_0)^2(1 - \\eta_k / \\gamma_k)^2} \\quad \\forall k \\geq 0,$\nwhere \u03c3 and L2 are given in Assumptions 1(c) and 2(b), respectively."}, {"title": "3.3.1 Hyperparameters and convergence rate", "content": "To establish the convergence rate of SIPM-EM, we specify its inputs {(\u014bk, \u221ak, \u00b5k)}k\u22650 as\n$\\eta_k = \\delta_{\\eta} / (k + 4)^{5/7}, \\quad \\gamma_k = 1/ (k+4)^{4/7}, \\quad \\mu_k = \\frac{\\text{ln}^2 (k + 70)}{13 (k+4)^{2/7}} \\quad \\forall k \\geq 0,$\nwhere s\u03b7 \u2208 (0,1)"}]}