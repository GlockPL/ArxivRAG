{"title": "Combining Physics-based and Data-driven Modeling for Building Energy Systems", "authors": ["Leandro Von Krannichfeldt", "Kristina Orehounig", "Olga Fink"], "abstract": "Building energy modeling plays a vital role in optimizing the operation of building energy systems by providing accurate predictions of the building's real-world conditions. In this context, various techniques have been explored, ranging from traditional physics-based models to data-driven models. Recently, researchers are combining physics-based and data-driven models into hybrid approaches. This includes using the physics-based model output as additional data-driven input, learning the residual between physics-based model and real data, learning a surrogate of the physics-based model, or fine-tuning a surrogate model with real data. However, a comprehensive comparison of the inherent advantages of these hybrid approaches is still missing. The primary objective of this work is to evaluate four predominant hybrid approaches in building energy modeling through a real-world case study, with focus on indoor temperature dynamics. To achieve this, we devise three scenarios reflecting common levels of building documentation and sensor availability, assess their performance, and analyse their explainability using hierarchical Shapley values. The real-world study reveals three notable findings. First, greater building documentation and sensor availability lead to higher prediction accuracy for hybrid approaches. Second, the performance of hybrid approaches depend on the type of building room, but the residual approach using a Feedforward Neural Network as data-driven sub-model performs best on average across all rooms. This hybrid approach also demonstrates a superior ability to leverage the physics-based simulation from the physics-based sub-model. Third, hierarchical Shapley values prove to be an effective tool for explaining and improving hybrid models while accounting for input correlations.", "sections": [{"title": "1. Introduction", "content": "In the last decade, the operation of buildings accounted for approximately 30% of global energy consumption and 26% of CO2 emissions [18]. Over half of this consumption stems from the operation of building energy systems such as Heating, Ventilation and Air Conditioning (HVAC), as well as the electrical systems. This situation underscores the urgent need for measures to reduce energy consumption in the operation these systems. Fortunately, advancements in digitalization and sensor deployment provide a foundation for using analytical and machine learning tools to optimize the performance of building energy systems. On this basis, Building Energy Models (BEM) can be developed to simulate a building's real-world condition and predict future behaviour thereby enabling the recommendation of optimal control actions. However, a major challenge remains: the availability of data, particularly in terms of comprehensive building documentation, sensor coverage, and the length of recorded data. This lack of data complicates the reliable and accurate construction of BEMs but also hinders the widespread adoption of energy optimization strategies in buildings.\nThe Building Energy Model is a digital representation of a building designed for simulating energy and temperature dynamics. It is constructed based on various characteristics of the building, including building geometry, material properties, installed energy systems, and operational inputs such as weather conditions, HVAC operation, and occupancy schedules [11]. To accurately reflect real-world conditions, the BEM is initially calibrated with sensor data to ensure faithful representation of the building and its energy systems. Once calibrated, the BEM can predict building operational performance and indoor temperature evolution across different time scales such as quarter-hourly or hourly resolution. Its comprehensive predictive capabilities make the BEM a versatile tool for various applications in building planning and operations. In retrofitting studies, it serves as an evaluation tool to assess potential measures for improving energy efficiency [36]. In building operations, it provides detailed temperature predictions as basis for control algorithms, enabling the optimization of energy system operations [20].\nClassical BEMs are physics-based models that use a system of differential equations to represent all building subsystems and their interactions. This includes equations for indoor heat balance, HVAC dynamics, and incident solar radiation. These models, often referred to as physics-based models, provide insights into the underlying physical phenomena governing building performance [11]. However, the accuracy of these simulations can be compromised by the unavailability of detailed building information. Additionally, setting up a BEM can be a time-consuming process that requires significant expertise. A widely used physics-based modeling software in both academia and industry is EnergyPlus [39], which enables dynamic thermal simulations and supports the energy-efficient design and operation of buildings. In case of simplified building descriptions or the requirement to lower computational complexity, reduced-order physics-based models may be employed. These models consist of simplified physical descriptions in the form of differential equations paired with data-driven identification of model coefficients, such as thermal Resistance-Capacitance (RC) models [11].\nMore recent approaches of building energy modeling focus on data-driven methods, which utilize statistical and machine learning techniques such as Autoregressive Integrated Moving Average (ARIMA), Feedforward Neural Network (FFNN), Long-Short Term Memory (LSTM) and Convolutional Neural Network (CNN) [11]. These methods rely on sensor measurements to establish relationships between defined input and output variables. Commonly modeled relationships include those between building operation data \u2013 such as energy consumption or temperature \u2013 and the corresponding sensor readings. Because these models are directly applied to data, the underlying mechanisms are not easily accessible or interpretable, leading them to be classified as data-driven models. Unlike physics-based models, data-driven approaches do not require detailed physical building information or the calibration of physical parameters, making them easier to set up as digital twins. However, their performance is highly dependent on the quantity and quality of the available measurement data. Despite this dependency, data-driven models have demonstrated high accuracy in numerous studies and are particularly well-suited for modelling buildings at an urban scale, thanks to their reduced configuration time [29].\nAn emerging category of approaches for building energy modeling is known as hybrid or physics-induced modeling, which combines elements from physics-based and data-driven methods. These approaches offer significant advantages, particularly in scenarios where sensor data or detailed documentation is lacking.\nSeveral research studies aim to combine reduced-order physics-based models with data-driven models through two general strategies. The first strategy involves formulating a loss function based on the reduced-order model to train the data-driven model. A prominent example is the use of Physics-informed Neural Network (PINN), where a physics-informed loss function is devised based on RC model temperature dynamics in order to train a FFNN [17, 26, 8]. The second strategy consists of incorporating reduced-order model elements into a data-driven model architecture. A number of works model building dynamics with a State-Space model framework and parameterize state and/or observation equation with FFNN [14], Neural Ordinary Differential Equations [38] or Graph Neural Networks [47]. However, ensuring physical consistency still remains a challenge in aforementioned strategies [12].\nAnother line of work researches the combination of high-fidelity physics-based models with data-driven models. We identify four distinct approaches in the context of our research: assistant, residual, surrogate, and augmentation. In the assistant strategy, the output from a physics-based model is used as an additional input to the data-driven model [6, 2] or conversely, the data-driven model can provide inputs or corrections to the physics-based model [42, 4, 21]. This additional input may provide valuable context information, but also increases the number of input features. The residual strategy involves using a data-driven model to learn the residuals between the output of a physics-based model and actual observed data [37, 10, 13, 25]. This approach aims to capture unmodeled physical phenomena and variations in the data that the physics-based model may not fully account for. It is particularly useful when certain inputs are not reliably represented in the data or when incorporating domain knowledge with a physical reference input is necessary. In the surrogate strategy, a data-driven model is trained to replace the physics-based model by using the same inputs, with the physics-based model's simulations serving as the target outputs [45, 44, 15, 41, 35]. The primary motivation for this approach is to reduce computation time, allowing the data-driven model to perform simulations that would otherwise be time-consuming with the physics-based model. Nevertheless, it relies on a sufficiently accurate simulator. The augmentation strategy augments the real data with simulated data from a physics-based model, training the data-driven model on this augmented dataset and subsequently adapting it to real-world situations [27, 9, 1, 40].\nThis augmentation strategy is particularly advantageous when little to no real-world data is available, as it allows the model to leverage simulated data to improve its accuracy and generalization capabilities. However, this approach may suffer from performance degradation if there is a significant disparity between simulated and real data.\nSeveral studies investigate the dependency of hybrid models on documentation and sensor measurements. For example, various levels of building documentation are explored in [6] using an assistant methodology that combines IDA-ICE software and Gradient Boosting Regression Trees (GBRT). In scenarios with limited sensor data, [24] employs a Surrogate-FFNN model based on EnergyPlus simulations to assess performance with varying data availability \u2013 100%, 30% and 20%, or even less [7]. Additionally, [19] explores the impact of seasonally limited data using an Augmentation-FFNN-CNN model implemented through the MATLAB toolbox CARNOT [46]. Few research studies address the explainability of hybrid models. For example, [22] evaluates a Surrogate-Random Forest model trained on simulated data from Energyplus using Pearson correlation and Gini importance scores to assess feature relevance. Similarly, [5] enhances the interpretability of a Surrogate-GBRT model trained on simulated data from Energyplus by incorporating a causal inference framework, thereby making the hybrid model inherently more transparent.\nAlthough hybrid approaches in BEM are gaining increasing attention, three significant research gaps remain. First, most recent studies focus on developing new hybrid methods within specific data contexts, yet there is a lack of a comprehensive research comparing the advantages and disadvantages of different hybrid approaches. A systematic evaluation of these methods is still missing, which limits our understanding of their relative strengths and weaknesses. Second, there are only few works that compare scenarios with limited building documentation or sensor data. Third, the majority of existing literature primarily evaluates hybrid models in terms of accuracy, with less attention given to the explainability of these approaches. Greater emphasis on explainability is crucial for understanding the hybrid models general behaviour, uncovering model biases of the physics-based sub-model and building trust for real-world application.\nThese research gaps highlight the need for more comparative studies and a deeper exploration of how hybrid BEMS perform under varying levels of data availability and quantity. In our study, we focus on the thermal modeling aspect of the BEM to predict indoor temperature dynamics, as understanding these dynamics forms the foundation for further analysis. Furthermore, we concentrate on a high-fidelity physics-based model for the construction of hybrid models. This paper tries to address the previously mentioned research gaps by making the following three main contributions:\n\u2022 We enhance the understanding of hybrid building energy models by investigating and comparing four predominant hybrid approaches across three challenging real-world scenarios, each characterized by varying levels of building documentation and sensor data availability.\n\u2022 We apply a hierarchical Shapley value framework to an agglomerative clustering analysis using Pearson's distance metric, providing valuable insights into the nature of hybrid models while accounting for the correlations. This also allows to investigate potential model biases of the physics-based part such as a bias at higher outdoor temperatures.\n\u2022 We examine and compare performance of the four hybrid approaches in a limited training data setting, offering a detailed analysis of their dependency on data quantity and their robustness under constrained conditions.\nThe remainder of this paper is organized as follows: Section 2 outlines the methodology used in this study. Section 3 introduces the data set-up and implementation details of the hybrid approaches. Section 4 conducts case studies across various documentation and sensor scenarios and analyzes the results. Finally, Section 5 draws conclusions and provides an outlook on future research directions."}, {"title": "2. Methodology", "content": "Our methodology consists of combining a physics-based EnergyPlus model with a data-driven model in four prevalent combinations. We then proceed to investigate these combinations in terms of accuracy with standard accuracy metric, interpretability with hierarchical Shapley values and data dependency with different documentation and sensor availability scenarios."}, {"title": "2.1. Hybrid model", "content": "In our research, we evaluate the four most prevalent forms of combining physics-based and data-driven models in building energy modeling, shown in Figure 1. The assistant approach incorporates the indoor temperature predictions from the physics-based model as an additional input to the data-driven model. The residual approach involves using a data-driven model to learn the indoor temperature residuals \u2013 the differences between the physics-based model's predictions and the actual indoor temperatures. The surrogate approach aims to train a data-driven model to fully replace the physics-based model for indoor temperature prediction, taking the physics-based simulation as training label. Finally, the augmentation approach takes this step further by fine-tuning the data-driven model with real data following its initial pre-training as a surrogate."}, {"title": "2.2. Physics-based models", "content": "As our physics-based model, we utilize the widely recognized open-access building simulation software EnergyPlus (EP) [39] to construct a comprehensive building energy model, as depicted in Figure 2. EnergyPlus simulates the building and its subsystems using a system of differential equations. These modules include equations for the building envelope, indoor heat balance, mass balance, HVAC fluid dynamics, lighting system, window performance, and weather conditions. To set-up the EP model, the building geometry and physical parameters are curated from the building documentation and provided in an Intermediate Data Format file. Alongside weather data formatted in the EnergyPlus weather file, these inputs allow the internal simulation manager to accurately simulate the building's dynamics behavior. Once the multi-zone building model is set up, its parameters are calibrated by aligning the real target variable with the simulation output using the sensor data from the building and individual rooms. This procedure ensures accurate simulation of temperature dynamics within the building. In case of less detailed building information, an archetypal EnergyPlus model can be constructed. For this purpose, we use the archetype framework Cesar-P [43] for automated model construction."}, {"title": "2.3. Data-driven models", "content": "In the case of data-driven models we investigate Linear Regression (LR), Feedfoward Neural Network (FFNN) and Random Forest (RF). We choose the LR due to its simplicity as well as strong baseline performance. The FFNN is chosen for its popularity and adequacy for regression problems. It also has computational advantages compared to kernel-based methods such as Support Vector Regression. The RF is selected as a representative of the tree ensemble models that are widely used in regression problems for their accuracy. Moreover, both FFNN and RF are able to represent the building temperature dynamics of all rooms within a single model. A Recurrent Network architecture is not considered since no time-lagged features are used as input. This enables a direct comparison to the pure physics-based EnergyPlus simulation, which doesn't allow explicit time lag integration."}, {"title": "2.4. Explainability", "content": "In a combination of physics-based and data-driven models, the explainability of the data-driven model as well as of the interdependencies between the two sub-models is crucial for the overall understanding. In particular, we want to examine these two aspects through the lens of feature contributions for model predictions on a global level. In this way, we can assign feature importance to get a better understanding of the model's general behavior, uncover model biases and potentially give suggestions whether to record particular sensor variables. To comprehensively explain the hybrid models, we employ a Shapley value framework [34] with the SHAP package [23]. Given the challenge of correlated input features, we opt for a hierarchical Shapley value calculation [28]. In this approach, hierarchical Shapley values are recursively computed based on a predefined hierarchy. We establish this hierarchy using agglomerative clustering [33], with Pearson's distance [16] as the distance metric. Consequently, the distance matrix D can be expressed as follows:\n$D = 1 - R_{XX}, R^{(i,j)}_{xx} = |cov(X_i, X_j) (\\sigma_i \\sigma_j)^{-1}|$\nwhere each entry of the absolute correlation matrix Rxx represents the absolute value of the Pearson correlation between two feature columns. Here, cov denotes covariance and \\sigma represents standard deviation. Subsequently, the hierarchical Shapley value \\phi_i of a specific feature i is computed as follows:\n$\\phi_i(v, B) = \\sum_{R \\subseteq M\\\\{k\\}} \\sum_{T \\subseteq B_K\\\\{i\\}} \\frac{1}{\\binom{|M|-1}{|R|}\\binom{|B_k|-1}{|T|}} \\frac{1}{|M||B_k|} [v(Q \\cup T \\cup \\{i\\}) - v(Q \\cup T)]$\nwhere the norm || in this context denotes the size of a set, M is the set of clusters, B = {B1, ..., Bm} represents the partition of features into clusters, Br is the cluster containing feature i and k is the index of cluster containing feature i. R is a subset of clusters and the first binomial coefficient calculates the corresponding number of ways that to choose |R| subsets out of |M| \u2013 1 clusters. T is a subset of features and the second binomial coefficient calculates the corresponding number of ways that to choose |T| features within cluster Bk from |BK|\u2013 1 features. Q = \\bigcup_{r\\in R} B_r is a union of features in the clusters defined by R, and v(\u00b7) = E[f(X)|X(.)] is the expected prediction of a subset. Therefore, the term v(QUTU {i}) \u2013 v(QUT) describes the marginal contribution of feature i when added to the union QUT."}, {"title": "2.5. Evaluation metrics", "content": "For evaluating the accuracy of temperature predictions, we utilize the widely used metrics: Mean Absolute Error (MAE) for its insensitivity to outliers, Mean Absolute Percentage Error (MAPE) for its comparability across rooms as well as Root Mean Squared Error (RMSE) for its outlier penalization. These metrics are defined as follows:\n$MAE = \\frac{1}{T} \\sum_{t=1}^{T} |y_t - \\hat{y}_t|$\n$MAPE = \\frac{1}{T} \\sum_{t=1}^{T} |\\frac{y_t - \\hat{y}_t}{y_t}|$\n$RMSE = \\sqrt{\\frac{1}{T} \\sum_{t=1}^{T} (y_t - \\hat{y}_t)^2}$\nwhere y, and \u0177, represent the actual and predicted room temperatures at time t, across T time steps."}, {"title": "3. Case study", "content": "The main subject of investigation is the inhabited experimental unit Urban Mining and Recycling (UMAR), located at the Swiss Federal Laboratories for Materials Science and Technology (Empa) in D\u00fcbendorf [32], as shown in Figure 3. This unit comprises several rooms equipped with various indoor and outdoor sensors, which have been recording data over several years. Our study concentrates on the five most frequently used rooms: two bedrooms (R272, R274), a living room (R273), and two bathrooms (R275, R276). The utility room is not regarded for our study. A comprehensive overview of all weather, building, and room sensors utilized in this study is provided in Table 2. The dataset for our case study is recorder at a 1-minute resolution for all sensor variables spanning the two years 2020 and 2021. For the whole dataset, the percentage of missing values is below 1%. After linearly interpolating the missing values, the dataset is aggregated to 15-minutes resolution. The dataset is split into a 50%/50% training/test set, with the year 2020 used for training and the year 2021 used for testing."}, {"title": "3.2. Documentation/sensor scenarios", "content": "The case study aims to investigate the performance of hybrid models in common real-world scenarios. To achieve this, we define commonly encountered levels of documentation (floor plan, construction information, ...) and sensor availability, as outlined in Table 1. The documentation level is categorized as either archetypal, using general information such as building layout, type, glazing ratio, and year of construction, or detailed, relying on comprehensive building documentation. Sensor availability is classified into three levels: weather, building and room, also detailed in Table 2. The weather-level includes only weather-related variables such as outdoor temperature and solar radiation. The building-level adds measurements for entire building, including total heating and cooling power. The room-level further includes room-specific variables such as temperature setpoints, occupancy, and shading. Each specific combination of documentation and sensor levels is referred to as a scenario. In our study, we focus on three typical scenarios: W-, WB- and WBR-scenario. In the W-scenario, we assume that only weather-related sensors are available as well as only basic building information. On this basis, we utilize the archetype framework Cesar-P [43] construct a simplified EnergyPlus model as physics-based model. In the WB-scenario, we consider having weather and general building related sensors as well as detailed construction information. In this case, we use an uncalibrated detailed EnergyPlus model, since a precise calibration without room sensors is not possible. In the WBR-scenario, we presume having access to weather, building and room sensors as well as detailed construction information. Here, we take a calibrated detailed EnergyPlus model to serve as the physics-based component in the hybrid models."}, {"title": "3.3. Model architecture", "content": "In our time series regression framework, we use all exogenous variables to predict the indoor temperatures of all rooms at the same time step. No time-lagged data is used as input to enable a direct comparison to the pure physics-based EnergyPlus simulation. Regarding the physics-based model, we use a Cesar-P model for the W-scenario and a detailed EnergyPlus model for the WB- and WBR-scenarios. The Cesar-P model is configured using parameters such as building footprint, type, age, heating system type, and glazing ratio and set up as a simplified EnergyPlus model. For the detailed EnergyPlus model, we employ the same model as described in [20], using an uncalibrated version for the WB-scenario and a fully calibrated version according to the authors's calibration cycle for the WBR-scenario. In both instances, each room is treated as a single temperature zone.\nFor the data-driven part, we compare three models of differing complexity: LR, FFNN and RF. While LR and RF implementations are used from Scikit-learn [31], the FFNN is implemented in Pytorch [30]. All three models are trained on standardized feature inputs. The LR model used is a multiple ordinary least squares regression with an intercept. The FFNN consists of two layers with 128 neurons activated by the sigmoid function. It is trained using a batch size of 32, with a maximum of 1000 epochs, early stopping with a patience of 10, and a validation split of 20%. The model is optimized using the Adam optimizer and the mean squared error loss function. In case of the RF, the number of forest trees is 300, the splitting criterion squared error, the minimum samples split 2 and minimum samples leaf 1. The specific FFNN and RF architectures are determined through hyperparameter grid search for the data-driven case. The chosen hyperarameters are maintained consistent across all hybrid approaches as well as all documentation/sensor scenario in order to isolate the effects of each experimental condition.\nIn integrating the physics-based and data-driven components, we explore four approaches: residual, assistant, surrogate and augmentation. For the augmentation approach, fine-tuning is performed with early stopping set to a patience of 3. Fine-tuning of the Augmentation-LR model is achieved by updating the LR weights using the Adam optimizer. The fine-tuning in case of the Augmentation-RF is achieved by using the previously fitted trees on the simulated data as warm start and adding 100 more trees for learning on the real data."}, {"title": "4. Results and analysis", "content": "In order to comprehensively evaluate the the hybrid models' prediction performance in real-world settings, we explore their performance in three different documentation/sensor scenarios. The results for the three documentation/sensor scenarios employing the time series regression framework are illustrated in Figure 4, presented as a MAPE boxplot across all rooms. Across all methods, the average MAPE (indicated by the green triangles) decreases as the level of documentation and sensor data increases from W-scenario to WBR-scenario. Notably, for the best performing method, Residual-FFNN, the MAPE improves by 0.88%, decreasing from 4.55 % in the W-scenario to 3.67 % in the WBR-scenario. This indicates that while the weather features and archetypal building characteristics provide a robust foundation for prediction, significant gains are achieved through the inclusion of more more sensors and documentation data. Furthermore, all hybrid methods,except for the surrogate models, outperform the EP simulator in terms of MAPE. Across all scenarios, the hybrid models exhibit performance comparable to the purely data-driven LR and FFNN. Nevertheless, we can see a slight performance advantage for the Residual-FFNN in the WBR-scenario.\nFigure 5 presents the performance of the WBR scenario, broken down by room. While the Residual-FFNN demonstrates the best overall performance (indicated by the grey-dotted line), there are notable variations across different room types. Specifically, the Residual-FFNN achieves the lowest MAPE in bedroom 272, whereas the Residual-RF performs better in the living room 273 and bedroom 274. For the two bathrooms, the Augmentation-FFNN yields the lowest MAPE for bathroom 275 and the data-driven RF the best MAPE performance. An example of the Residual-FFNN's superior performance is highlighted in Figure 6. During extended window openings from March 4th to 7th and March 11th to 15th (shaded in grey), the Residual-FFNN accurately captures the extreme temperature fluctuations by leveraging the EP model. In contrast, the other hybrid approaches fail to predict this behavior effectively. A similar effect can be observed in the residual approaches of LR and RF.\nFigure 7 presents the average MAPE across rooms, grouped by month, for the hybrid models. Two notable observations can be made. Firstly, March and April exhibit a higher error rate, exceeding 4% MAPE for all models. This is due to extended periods of open windows in the bedrooms and living room, which cause a drop in room temperature and trigger the heating system to compensate. These extreme fluctuations are challenging for all models to accurately capture. Secondly, in November and December, the EP model performs competitively with the other models compared to the rest of the year. An explanation for this is the rental situation of UMAR, as the unit was uninhabited in winter 2021. Consequently, the temperature patterns in November and December 2021 were more regular, making it easier for the EP model to simulate. In contrast, the other models were trained on data from 2020 when the unit was inhabited during winter. This makes it harder for them to generalize to an uninhabited period."}, {"title": "4.2. Explainability within model", "content": "In order to improve the understanding of the data-driven model as well as of the interdependencies between the physics-based and data-driven model, we investigate the models by using hierarchical Shapley Value framework to explain their predictions. Our analysis focuses on the WBR-scenario due the highest documentation and sensor availability level. Moreover, we concentrate on living room 273 because it's the largest room and on the Residual-FFNN given its best average performance across all rooms. Figure 8 presents the SHAP beeswarm plot for the hierarchical Shapley values, highlighting the 10 most influential features for living room 273 as identified by the Residual-FFNN model. Among the top features are the simulated EP input, weather variables (solar radiation, relative humidity, dew-point temperature), as well as room-specific variables like occupancy and shading. It is intuitive that the simulated EP inputs play a key role in a residual approach. Additionally, the significant contribution of room-level variables underscores their importance in model performance. The high importance of weather variables suggests that the residual approach compensates for weather variations not fully captured by the EP model.\nFigure 9 presents the SHAP dependence plot for living room 273, highlighting the second most important feature (Simulated air temperature) according to Figure 8. Note that the Shapley value represents the average contribution to the output and in this case the contribution in temperature degrees to the residual correction. The plot reveals that when the simulated temperature input exceeds a threshold of 24.6\u00b0C, the contribution to the residual correction is generally negative. Below this threshold, the contribution to the correction is positive. Examining the dry-bulb temperature coloring, we observe that contributions to residual corrections for higher temperatures tend to be negative and up to a correction contribution of -2 degrees, which may suggest a bias in the EP model at higher temperatures. This insight could serve as a basis for improving the EP model.\nFigure 10 displays the hierarchical Shapley dendrogram for the inputs of the best-performing method, Residual-FFNN. With a few minor exceptions, four distinct groups can be identified. The orange cluster is composed mainly of outdoor temperature and room mass flow variables. The green cluster includes wind, window, and occupancy variables. The red and violet clusters are dominated by EP simulated inputs, solar radiation, and relative humidity, while the brown group consists of setpoints and shading variables. Notably, Table 3 exposes that the red cluster holds the highest relative importance among all clusters for room 273. This indicates that solar radiation, simulated air temperatures and relative humidity are the most impactful input group. The strong influence of solar radiation is consistent with the observation that the living room has three large windows. At the same time, the importance of the simulated air temperatures can be attributed to the nature of the residual model. If we cut the dendogram at the second level, the red, violet and brown groups together have an aggregated hierarchical Shapley value that is more than double that of the combined orange and green clusters for room 273. We can interpret features directly related to indoor temperature (simulated air temperatures and temperature set points) as well as irradiation related features (solar radiation and shading) contribute significantly more on average than outside temperature, wind features, window openings and occupancy. A similar effect is observed for the two bedrooms, except that the violet group has gained importance due to the simulated temperature of the respective bedrooms being in this group. In case of the bathrooms, we notice a decrease of importance in all groups besides the red cluster, of which the simulated temperatures of bathrooms are part of. Such an effect can be attributed to the more isolated dynamics of the bathrooms having no windows."}, {"title": "4.3. Explainability comparison between models", "content": "In order to compare the influences of the input cluster groups across the different hybrid approaches, Figure 11 shows the averaged absolute hierarchical Shapley values across rooms for LR, FFNN and RF. Note that the cluster groups of the assistant and residual approaches follow the same form shown in Figure 10 due to the additional simulated input, and are henceforth denoted as AR-clustering. The data-driven, augmentation and surrogate approach follow the same form given in Figure 16 having no additional simulated input and are denoted as DAS-clustering. In princile, the DAS-clustering resembles the AR-clustering for most parts, except for the absence of simulated inputs, the violet group being dissolved into the orange and green one as well as two set point input being merged into the green cluster. Moreover, note that the values between LR, FFNN, and RF hybrid approaches should not be compared directly but only in relative terms, since the Shapley value calculation is highly dependent on the model's inner workings. The most informative observation in Figure 11 is the high importance of the red cluster relative to the other clusters in case of the assistant and residual approaches for RL, FFNN and RF. In contrast, the DAS-clustering bar plots show a more balanced picture with the orange group having the highest importance. This suggests that the assistant and residual approach are provided with most parts of the dynamics through the simulated inputs in the red group, while the data-driven, augmentation and surrogate approach have to learn the temperature dynamics primarily through the sensor data."}, {"title": "4.4. Impact of data quantity on performance", "content": "In practice, building sensor data may not abundantly available due to cost and infrastructure challenges or privacy concerns. In this light, we investigate the data quantity dependence of the hybrid models with limited data. In these experiments, we reduce the amount of training data and evaluate the hybrid models on the same test set (year 2021) as before. The MAPE results for these experiments, using limited training data, are displayed in Figure 12. The models are trained with a progressively decreasing number of months, ranging from January 15th to July 15th. The results for 12 months of training data are shown as a reference on the left side of the figure. For all experiments, we keep the same calibration of the EnergyPlus model as in the 12-months set-up, assuming ideal conditions.\nOverall, we observe a decline in performance as the amount of training data decreases, with two notable exceptions at 3 months and 1 month. The 3-months performance is heavily influenced by the validation set, which predominantly consists of data collected in April. Since April has the highest indoor mean temperature and standard deviation, it does not serve as a representative validation set. This results in more biased models and poorer performance on the test set. The 1-month performance, however, is an evaluation artifact. Although the predictions cover less variance in the true room temperature, they are closer to the true temperature mean, resulting in a slightly better MAPE than when training with 2 months of data.\nAmong the models, the Residual-FFNN performs best, while the Surrogate-FFNN consistently shows the highest MAPE across all training data sizes. Additionally, the performance gap between the FFNN and Residual-FFNN widens as the amount of training data increases. Notably, from three months of training data, the limited data significantly impacts the FFNN's performance. In contrast, the Residual-FFNN effectively leverages the physical information from the EP simulation to generate more accurate predictions. An illustration of this can be seen in Figure 13, which shows predictions over the entire test set using only 1 month of training data. Similar effects can be observed in cases of LR and RF, depicted in Appendix Figure 19 and Figure 13. In summary, we see that the performance declines with decreasing training data while the residual approach maintains the best ability to cover the temperature variations."}, {"title": "5. Discussion", "content": "Our results indicate that more detailed documentation for the physics-based model and increased sensor availability significantly improve prediction accuracy. In scenarios with limited documentation and sensor data, hybrid approaches do not show substantial improvements over purely data-driven models. However, in scenarios with comprehensive documentation and abundant sensor data, hybrid methods demonstrate considerable performance enhancements. This suggests that hybrid approaches reach their full potential only when sufficient sensor measurements related to temperature dynamics are available. Additionally, we observe that the most influential inputs vary depending on the hybrid approach used. For hybrid methods incorporating physics-based simulations as additional inputs (assistant and residual approaches), the feature group containing simulation data exhibits the highest relevance, indicating that the physics-based simulations provide the majority of information about temperature dynamics. In contrast, for hybrid approaches like augmentation and surrogate, the importance is more evenly distributed across feature groups, indicating that these methods learn the dynamics directly from sensor data.\nIn scenarios with the highest documentation and sensor availability, different hybrid models perform best for different room types. However, the Residual-FFNN consistently achieves the best average performance across all rooms compared to the other hybrid models. We also find that the residual approach is most effective at capturing out of distribution behaviour, such as extended periods of window openings. Even in scenarios with limited data, the residual approach successfully captures temperature variations, whereas purely data-driven model and other hybrid approaches struggle. We attribute these advantages to the robust foundation provided by the physics-based model, combined with data-driven corrections. In stark contrast, the surrogate approach consistently performs the worst across all experiments, likely due to the physics-based model's performance setting an upper bound.\nWhen applying the hierarchical Shapley value with the Residual-FFNN, we showcase the potential to detect biases in the physics-based component and identify areas for correction. Specifically, the feature importances in the residual approach show increased contributions to residual corrections at high outside temperatures, indicating that the physics-based model exhibits significant bias under these conditions."}, {"title": "6. Conclusion", "content": "In this study, we assess the performance of four predominant hybrid approaches along with purely data-driven and physics-based models for predicting building temperatures. To mimic challenging real-world conditions, we evaluate these hybrid methods across three common scenarios that vary in documentation detail and sensor availability. Furthermore, we explore the explainability of the"}]}