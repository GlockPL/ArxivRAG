{"title": "RL2: Reinforce Large Language Model to Assist Safe Reinforcement Learning for Energy Management of Active Distribution Networks", "authors": ["Xu Yang", "Chenhui Lin", "Haotian Liu", "Wenchuan Wu"], "abstract": "As large-scale distributed energy resources are integrated into the active distribution networks (ADNs), effective energy management in ADNs becomes increasingly prominent compared to traditional distribution networks. Although advanced reinforcement learning (RL) methods, which alleviate the burden of complicated modelling and optimization, have greatly improved the efficiency of energy management in ADNs, safety becomes a critical concern for RL applications in real-world problems. Since the design and adjustment of penalty functions, which correspond to operational safety constraints, requires extensive domain knowledge in RL and power system operation, the emerging ADN operators call for a more flexible and customized approach to address the penalty functions so that the operational safety and efficiency can be further enhanced. Empowered with strong comprehension, reasoning, and in-context learning capabilities, large language models (LLMs) provide a promising way to assist safe RL for energy management in ADNs. In this paper, we introduce the LLM to comprehend operational safety requirements in ADNs and generate corresponding penalty functions. In addition, we propose an RL2 mechanism to refine the generated functions iteratively and adaptively through multi-round dialogues, in which the LLM agent adjusts the functions' pattern and parameters based on training and test performance of the downstream RL agent. The proposed method significantly reduces the intervention of the ADN operators. Comprehensive test results demonstrate the effectiveness of the proposed method.", "sections": [{"title": "I. INTRODUCTION", "content": "As significant distributed energy resources (DERs), such as diesel generators (DGs), photovoltaics (PVs), and battery energy storage systems (BESSs), are integrated into the distribution networks, traditional distribution networks are gradually transforming into active distribution networks (ADNs) [1], [2], which also involves massive emerging ADN operators. In order to fully exploit the potential of these DERs, energy management, which adjusts the active/reactive power generation of controllable devices [3], [4], is essential compared to passive control strategies. Effective energy management not only reduces operational costs but also improves the safety of the ADNs.\nTraditionally, energy management is formulated as a coordinated active/reactive power optimization problem [5], [6], which can be successfully solved by existing well-developed optimization algorithms. However, these methods involve complicated modelling and optimization of the entire ADN and the controllable devices within it, which is usually unaffordable for the ADN operators. In addition, solving the optimization problem involves a heavy communication and computational burden, which makes it difficult to cope with the fluctuations in loads and PV power generation [7], [8]. Therefore, reinforcement learning (RL) based energy management methods recently received considerable attention [9]\u2013[13]. Unlike the above optimization algorithms, RL-based methods construct the energy management problem as a Markov decision process (MDP) [14], in which the RL agent interacts with the environment and automatically updates its control policy based on the feedback reward signals. After the offline training, a trained RL agent is able to generate control strategies of controllable devices using only ADN online measurements and without continuously calculating optimization problems, which greatly improves the efficiency for ADN operators.\nAs can be seen from the definition of RL, the performance of the RL agent depends heavily on the feedback reward signal from the MDP, which requires careful design of the reward function. Fortunately, the design of the reward function is relatively straightforward for the energy management problem, which aligns with the objective function, i.e., minimizing operational costs of the ADN. When applying RL-based methods to real-world problems, safety becomes a critical concern. As for the operational safety constraints in ADNs, a common practice is to include a penalty term in the reward, providing the RL agent with appropriate feedback when a violation occurs. To meet the operational safety requirements in ADNs, researchers have developed several types of penalty functions, such as violation times, L1-norm function, L2-norm"}, {"title": "II. PRELIMINARIES", "content": "In this section, we first formulate the energy management problem in ADNs. Then, the basic concepts about MDP, safe RL, and LLM used in this paper are introduced.\nA. Energy Management Problem Formulation\nConsidering an ADN equipped with DGs, PVs, and BESSs which can be described as an undirected graph G(N,E), where the Nand & are nodes and branches in the ADN, its active and reactive power injection of node i at time step t can be calculated as:\n$P_{i,t} = P_{i,t}^{DG} + P_{i,t}^{PV} + P_{i,t}^{BESS} \u2013 P_{li,t}$ (1)\n$Q_{i,t} = Q_{i,t}^{DG} + Q_{i,t}^{PV} - Q_{li,t}$ (2)\nwhere $P_{i,t}^{DG}, P_{i,t}^{PV}, P_{i,t}^{BESS}$ represent active power generated by DG, PV, and BESS, respectively; $Q_{i,t}^{DG}, Q_{i,t}^{PV}$ represent reactive power generated by DG and PV, respectively; $P_{li,t}$ and $Q_{li,t}$ are the active and reactive load of node i at time step t. Then the power flow functions in the ADN are expressed as:\n$P_{i,t} = V_{i,t} \\sum_{j \\in N} V_{j,t}(G_{ij} cos \\delta_{ij,t} + B_{ij} sin \\delta_{ij,t})$ (3)\n$Q_{i,t} = V_{i,t} \\sum_{j \\in N} V_{j,t}(-B_{ij} cos \\delta_{ij,t} + G_{ij} sin \\delta_{ij,t})$ (4)\nwhere $V_{i,t}$ is the voltage magnitude at node i at time step t; $\\delta_{ij,t}$ is the voltage phase difference between node i and j at time step t; $G_{ij}$ and $B_{ij}$ are the real and imaginary parts of the corresponding element $Y_{ij} = G_{ij} + jB_{ij}$ in the admittance matrix of the ADN.\nBy setting energy management control signals for $P_{i,t}^{DG}, P_{i,t}^{BESS}, Q_{i,t}^{DG}, Q_{i,t}^{PV}$, the ADN operator aims to minimize the total operational cost:\n$\\min \\sum_{t=0}^{T} [\\sum_{i \\in N} C_{PG} (t) + \\sum_{i \\in N} C_{BESS}(t) + C_{O}(t)]$ (5)\nwhere T is the length of the control process; $C_{PG} (t)$ is the generation cost of the DG at node i at time step t; $C_{BESS} (t)$ is the charge or discharge cost of the BESS at node i at time step t; $C_{O}(t)$ is the cost of buying electricity from the upper grid."}, {"title": "III. METHODS", "content": "In this section, we first formulate the MPD for the energy management problem in ADNs so that it can be effectively optimized by SAC algorithm. Then the overall RL2 mechanism is described in detail. Within the RL2 mechanism, design of the RL agent as well as the LLM agent are then introduced.\nA. Markov Decision Process Formulation for Energy Management\nDefinitions of state space, action space, reward function, and penalty functions for the energy management problem are designed as follows.\n1) State Space: The state s ES of the MDP for energy management problem is based on the measurements and states of the equipment in the ADN. In this paper, we assume all the"}, {"title": "IV. NUMERICAL STUDY", "content": "In this section, to validate the effectiveness of the proposed method, several numerical simulations are conducted on IEEE 33-bus [32] and 69-bus [33] distribution networks. Control interval is 15 minutes and the length of the control process is 96 steps. A steady state ADN RL training and test environment is built under the scheme of toolkit Gym [34] using power flow functions.\nIn 33-bus system, 2 DGs are located at bus-18 and bus-33, 2 PVs are located at bus-22 and bus-25, and 2 BESSs are located at bus-21 and bus-24. In 69-bus system, 2 DGs are located at bus-18 and bus-58, 2 PVs are located at bus-35 and bus-46, and 2 BESSs are located at bus-34 and bus-45. The branch capacities of these two cases are 5.0 MVA and 4.7 MVA, respectively. Voltage limitations are [0.95,1.05]\u0440. \u0438.."}, {"title": "V. CONCLUSION", "content": "Utilizing safe RL method to optimize energy management problems in ADNs requires extensive domain knowledge in RL and power system operation, which heavily depends on the ADN operators' experiences. To improve the efficiency of automatically safe RL, we introduce LLMs to comprehend the energy management problem and design penalty functions for corresponding operational safety constraints. In addition, an RL2 mechanism is also proposed to adaptively refine the designed functions, in which the penalty functions are iteratively adjusted through multi-round dialogues based on feedback from the RL agent. Comprehensive results demonstrate effectiveness and safety of the proposed method.\nIn future work, other approaches to combining LLMs with RL will be an interesting topic. With the accelerated development of LLMs, other applications in the power system can also be investigated."}]}