{"title": "A LLM-Powered Automatic Grading Framework with Human-Level Guidelines Optimization", "authors": ["YUCHENG CHU", "HANG LI", "KAIQI YANG", "HARRY SHOMER", "HUI LIU", "YASEMIN COPUR-GENCTURK", "JILIANG TANG"], "abstract": "Open-ended short-answer questions (SAGs) have been widely recognized as a powerful tool for providing deeper insights into learners' responses in the context of learning analytics (LA). However, SAGs often present challenges in practice due to the high grading workload and concerns about inconsistent assessments. With recent advancements in natural language processing (NLP), automatic short-answer grading (ASAG) offers a promising solution to these challenges. Despite this, current ASAG algorithms are often limited in generalizability and tend to be tailored to specific questions. In this paper, we propose a unified multi-agent ASAG framework, GradeOpt, which leverages large language models (LLMs) as graders for SAGs. More importantly, GradeOpt incorporates two additional LLM-based agents-the reflector and the refiner-into the multi-agent system. This enables GradeOpt to automatically optimize the original grading guidelines by performing self-reflection on its errors. Through experiments on a challenging ASAG task, namely the grading of pedagogical content knowledge (PCK) and content knowledge (CK) questions, GradeOpt demonstrates superior performance in grading accuracy and behavior alignment with human graders compared to representative baselines. Finally, comprehensive ablation studies confirm the effectiveness of the individual components designed in GradeOpt.", "sections": [{"title": "1 Introduction", "content": "Learning analytics (LA), supported by data processing and performance prediction, presents enormous educational opportunities to enhance learning quality through the provision of effective feedback and actions [2]. Meanwhile, providing timely and precise assessments to student assignments and examinations is vital to LA due to the significance of performance measurement in the learning process [37]. Traditionally, close-ended multiple-choice questions (MCQs), which asks students to select correct answer from distracting options, dominated LA studies. This is due to their convenience in the grading process [3, 28]. However, open-ended short-answer questions (SAQs), which asks for not only final answer but explanations, often provide deeper insights into students' answering rationale and knowledge concepts. This is because they are known to elicit the thinking path that describes how a student arrives at their conclusion [22]. Unfortunately, grading open-ended textual answers is tedious as the workloads are significantly increased, and instructors need to examine each response and provide feedback [34]. More importantly, the inconsistent or unfair assessments, caused by diverged interpretations, biases, or mistakes create another challenge to SAQs grading in practice [37]. To mitigate these issues and provide timely and consistent evaluation, automatic short-answer grading (ASAG) [3] systems have become appealing.\nASAG, which can be traced back to the 1960s, has bloomed in recent years due to advancements in natural language processing (NLP) [22, 40]. Early ASAG systems often used pattern-matching techniques and hand-crafted features [22]. Thus, those systems required intensive human labor to build and were limited to a few specific grading tasks. The rise of deep learning (DL) has lessened the amount of burdensome feature designs needed for early ASAG systems. DL provides an end-to-end solution that automatically learns to output grading scores from a large number of graded answer samples [13]. Due to the strong data-fitting capability of DL models, DL-based ASAG systems are able to be extended to different tasks if a large number of annotated samples are available. However, when the annotated sample size is limited, DL-based ASAG systems often face serious over-fitting issues. Beyond that, as DL is a black-box model whose results lack interpretation, the application of DL-based ASAG systems is still limited [7].\nThe emergence of pre-trained language models (PLMs) and the more advanced Large Language Models (LLMs) have recently revolutionized the design of ASAG systems due to their human-like language ability and human-interpretable intermediate textual results. Therefore, many recent studies have attempted to build ASAG systems with LLMs. Promising results have been demonstrated that using fine-tuning [21] and prompting techniques such as Chain-of-Thought (CoT) [5] and in-context learning [23]. Yet these recent techniques are still limited due to LLMs' inherent limitations such as sensitivity to prompts, context window restriction, etc., making the complex ASAG task challenging for the LLM grader. In reality, accurate, standardized, and unambiguous guidelines are critical to help human graders formulate a precise interpretation of scoring criteria. For LLM-based ASAG systems, those guidelines also serve as the principal instructions. They teach LLMs to perform the grading task following a similar standard as human graders. However, using guidelines composed by pedagogical experts directly for LLMs is sub-optimal since the general-purposed LLMs lack domain-specific knowledge and can misinterpret the guidelines [22]. Meanwhile, LLMs are often sensitive to various facets of prompts [18] where minor changes could lead to great differences in LLM's performance. Optimizing the guidelines manually for LLMs can further take a lot of trial and error. Thus, recent works propose to conduct guideline modification with LLMs to offload human burden [5]. While the modified guidelines yield performance improvement, the prompt search space in these methods is relatively limited. Because of this, the modified guidelines are not necessarily optimal. Additionally, abundant human efforts such as timely feedback or a large amount of labeling are required. Therefore, methods to optimize grading guidelines automatically and effectively are still desired.\nIn this paper, we propose a unified multi-agent ASAG framework that can automatically optimize grading guidelines. Specifically, it utilizes an iterative reflection mechanism to generate a task prompt (guideline) that can capture learners' thinking and knowledge from a small dataset of short answers. To do this, we innovatively propose to leverage prompt optimization in ASAG by viewing grading guideline refinement as an optimization problem that aims for maximal accuracy. To implement the backbone algorithm, we take inspiration from APO [32] and propose novel techniques such as misconfidence-based selection, iterative optimization, and log-probability-based robustness to enhance the ASAG system's stability in making accurate and trustable score predictions to unseen datasets. To minimize human labelling effort, our mechanism intelligently selects short answer samples that can lead to the optimal performance of guideline enhancement. Additionally, our framework serves as a solution to test questions of a variety of complexity, providing interpretable assessments to each learning goal that contributes to the overall scoring. To validate the effectiveness of the framework, we conducted experiments with data collected from teachers who answered a set of questions designed to capture their content knowledge (CK) and pedagogical content knowledge (PCK) of mathematics [8]. The experimental results demonstrate that both accuracy and alignment increase as compared to the representative baselines. In addition, our further analysis showcases steady improvement of test accuracy throughout iterations, demonstrating effective and continuous enhancement of the learned guidelines. To the best of our knowledge, we are the first to apply prompt optimization in ASAG by optimizing grading guidelines similar to how an optimal task prompt is generated. We believe that refining the grading guidelines through a multi-agent reflective mechanism can help the great potential of LLMs in LA by better assisting learners with detailed, accurate assessment while significantly reducing educators' scoring burden."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Automatic Short Answer Grading", "content": "Automatic Short Answer Grading (ASAG) is often treated as a text classification or regression problem in NLP studies. Here we mainly focus on classification due to its relevancy to our setting. Traditional ASAG models mainly rely on text similarity and employ classic ML classifiers. They use lexical features such as bag-of-words (BOW) [28] and TF-IDF [12], or syntactic features indicating the structure of sentences [22]. However, these methods require significant manual design, which makes them hard to be applied to new datasets. To reduce the burden of feature engineering, Deep Neural Networks (DNNs) such as Long-Short-Term-Memory (LSTM) are utilized [13], which produce superior results but suffer from limited generalizability. Pre-trained BERT-based models provide enhanced versatility through transfer learning including on ASAG datasets [4]. To further enhance grading accuracy, researchers have made attempts to ensemble BERT with statistics-based methods [10] and data augmentation [25]. LLMs are increasingly utilized in ASAG and similar assessment tasks [5, 40]. However, their prompts are mostly always manually-crafted and thus are unable to properly adapt to new datasets. To solve this issue, several works have shifted attention to assisting educators with guideline creation [5, 40]."}, {"title": "2.2 LLM Prompt Optimization and Reflection", "content": "Prompts are critical to the success of LLMs [43]. To tailor LLMs to challenging tasks, manually crafted prompts are adopted to enhance the performance [39]. To automate the generation and optimization of prompts, prompt optimization emerges as a promising method for input prompt refinement. Using these techniques, LLMs have demonstrated superior performance in many down-stream tasks, particularly in instruction following and reasoning [32, 42, 43]. However, such automatic methods are risky when directly applied to ASAG tasks considering the limitations of LLMs such as hallucination [17] and misalignment [20]. To enhance both accuracy and trustworthiness, we adopt the idea of state-of-the-art prompt optimization APO [32] and implement novel techniques for reliability.\nSimilar to how humans gather knowledge from failures, experience-and-reflect [30] is an important technique for improving LLMs' alignment with task specifications. By reflection, LLMs learn through failure, which enriches its knowledge base and provides valuable reference in similar scenarios. Self-reflection has demonstrated promising results in improving LLM reasoning [27, 36]. However, LLMs' reflection ability is relatively limited when it comes to self-correction without human feedback or true labels [16]. A recent work [38] divides the task of self-correction into two steps: mistake finding and output correction. They empirically show that while LLMs struggle to find errors, their correction ability is robust when given ground-truth labels. This provides grounding support for our proposed framework due to the similar use of true labels in guiding LLM reflection."}, {"title": "3 Problem Statement", "content": "We define ASAG as a text classification task, which grades the short answer text $x_i$ by classifying it into the discrete score categories ${\\hat{y_i} = F(x_i) = s_j|j = 1, .., C}$, where $F$ is a ASAG system, ${\\hat{y_i}}$ is the score prediction, $s_j$ is the score category, and $C$ is size of the score category set. When $F$ is a LLM, the grading guideline text $G$ will be concatenated at the front of $x_i$ as an instructional prompt, and the grading process can be expressed as ${\\hat{y_i} = F (G, x_i) = s_j}$. In this work, we focuses on leveraging the reflection and refining capabilities of LLMs to automatically generate an optimized grading guideline $G^*$ based on a small amount of graded short answer text $D = {\\{(x_i, Y_i)|i = 1, ..., N\\}}$, where $N$ is the number of graded samples. The goal of our framework can be expressed as: $G^* = \\underset{G}{\\text{argmax}} \\sum_{i=1}^{N} 1 \\cdot 1\\{y_i=\\hat{y_i}\\}$, where $1 \\cdot 1\\{\\cdot\\}$ is an indicator function that is 1 if $y_i = \\hat{y_i}$ and 0 otherwise. Once the optimization process is finished, our framework will concatenate the optimized guidelines $G^*$ at the front of unlabeled short answer text and generate the grading results, ${\\hat{y} = F(G^*, x_i)}$."}, {"title": "4 Method", "content": "In this section, we introduce our unified multi-agent ASAG framework GradeOpt. It can automatically optimize the grading guidelines and achieve better grading alignment with human experts. Next, we first give an overview of GradeOpt. Then, we detail the LLM-based agent design, and implementation details."}, {"title": "4.1 An Overview", "content": "As demonstrated in Figure 1, GradeOpt consists of two stages: training and test-time adaptation. The training stage is supported by three LLM-based agents: Grader, Reflector, and Refiner. They synergically enhance the grading guidelines by optimizing the score classification accuracy using the graded answers to the SAQs $D_{train}$ (i.e., the training data). In the test-time adaptation stage, the system first performs an out-of-distribution (OOD) test over a small amount of unlabeled answers sampled from the test data. To be specific, by checking the log likelihood score of the predicted grading results, GradeOpt decides whether the optimized guidelines $G_{opt}$ can be applied to the test data directly. If the test failed, the current guideline is not optimal for the test data. Therefore, our framework will improve $G_{opt}$ via test-time training. If the test successes, GradeOpt will perform the auto-grading over the whole test data automatically."}, {"title": "4.2 Training Stage", "content": "The training stage is to optimize the guideline for the Grader agent to achieve the optimal grading performance over the training dataset $D_{train}$. GradeOpt leverages a multi-agent framework powered by three agents which collaboratively predict scores for $D_{train}$, identify errors, and suggest rule modifications to mitigate errors.\nBefore diving into the details of this stage, we first give a brief introduction to the three key components of a common grading guideline: Question Stem ($G_{qs}$), Key Concept ($G_{kc}$) and Scoring Rubric ($G_{sr}$). Specifically, $G_{qs}$ contains the complete question contents, $G_{kc}$ describes the test knowledge concepts, and $G_{sr}$ is the operational guidance instructing human graders how to score responses. As we previously mentioned, directly using $G_0 = {\\{G_{qs}||G_{kc}||G_{sr}\\}}$ as grading guideline for Grader is sub-optimal since the human-based scoring rubrics commonly lack detailed explanations to some concepts. As a result, LLM-based grading methods could provide ambiguous judgments. To solve this issue, GradeOpt focuses on optimizing $G$ by appending new Adaption Rules ($G_{ar}$) that provides the detailed explanations regarding reflections from failed predictions and identified errors. Specifically, when given the expert-designed input containing \"Task Description\", \"Question Stem\", \"Key Concept\" and \"Scoring Rubrics\", GradeOpt automatically generates the additional descriptions in \"Adaptation Rules\". These new rules help describe how to assign a grade based on answer patterns and details.\nThe training procedure is shown on the left sub-figure of Figure 1. During training, the optimization is conducted in an iterative manner. In the $t$-th round, GradeOpt first draws a batch of samples $b$ from $D_{train}$ and sends them to the grader agent for grading. GradeOpt compares the grades outputted by LLMs with human-annotated scores, then identifies error samples. These samples are then sent to the reflector agent for error reflections. Based on the reflections generated from those error samples, the reflector agent proposes a series of suggestions for improving $G_{t-1}$, represented by $\\Delta G_t$. $\\Delta G_t$ is then sent to the refiner agent, which fuses $G_{t-1}$ with $\\Delta G_t$ and generates $G_t$ for the next iteration of optimization. Next, we will introduce detailed designs of the three agents in GradeOpt. Then, we will present the implementation details of the iterative optimization process."}, {"title": "4.2.1 Agent Configurations.", "content": "Grader. The Grader focuses on mapping $x_i$ to $s_j$ based on the given $G$. In GradeOpt, we leverage the exceptional instruction-following capability of LLMs by using a prompt to instruct LLMs to simulate the grading process of human graders. To fully exploit the potential of LLMs, we incorporate the prompt engineering strategy Chain-of-Thought [39]. This encourages LLMs to provide both judgment and intermediate reasoning steps in their outputs. With such design, the Grader becomes better aligned with the human-like grading process. Meanwhile, the intermediate reasoning steps provide support for the Reflector to discover the potential improvements to the given guideline. The prompt for the Grader agent is shown in Figure 3.\nReflector. The role of Reflector is to propose ways to improve the current guideline $G_{t-1}$ by reflecting over the error samples returned by Grader. To be specific, we design a two-step instruction prompt for LLMs to achieve this goal. In the first step, LLM is instructed to analyze the individual and shared failure reasons for a set of error samples. Then, in the second step, we ask LLMs to propose suggestions that can help resolve those issues. In general, the two-step improving process is analogous to the gradient descent algorithm used by parameter optimization for machine learning algorithm [35]. In our case, the guideline $G$ serves as the parameter of Grader and identifying the error reason is similar to the \"gradient\". Finally, proposing improving suggestions based on discovered reasons is similar to making a descent down the \"gradient\" and thus optimizing $G_{t-1}$. The prompt for the Reflector agent is shown in Figure 4.\nRefiner. The role of Refiner is to generate a new guideline $G_t$ based on the suggestions from Reflector. Specifically, Refiner is asked to make modifications to the examples and illustrations to the content in $G_{ar}$. Such edits include adding, removing, or editing. Note that we keep the other components, i.e., $G_{qs}$, $G_{kc}$, $G_{sc}$ unchanged since they are composed by human experts, and any small change may distort the scoring logic away from its original design. The refined guideline can be expressed as $G_t = {\\{G_{qs}||G_{kc}||G_{sc}||G_{ar}\\}}$, where $||$ is the text concatenation operator. The prompt for the Refiner agent is given in Figure 5."}, {"title": "4.2.2 Iterative Optimization Designs.", "content": "Nested Iteration. The high complexity of test questions and grading guidelines makes it nontrivial to implement the optimization directly. Beyond that, the constraint over the input context window size of LLMs forbids it to accept all examples in $D$ for processing at once. To resolve that, we propose a nested iterative optimization approach, i.e., inner and outer loop, in GradeOpt. Specifically, during the $t$-th outer loop, GradeOpt selects a batch of samples $b_{out}$ from $D_{train}$ and sends them with $G_{t-1}$ to Grader for grading. Then, the wrongly graded answers $e_t$ are filtered for reflections. However, due to the input context window size limitation, all errors in $e_t$ cannot be entirely processed by reflector and refiner simultaneously. Thus, we introduce the inner loop, which samples an inner batch $b_{in}$ from $e_t$, and updates $G_{t-1}$ with the iterative procedure.\nTo accelerate the optimization process and encourage a wider exploration of all possible combinations of error samples in $b_{out}$, we integrate the beam searching strategy [11] within both inner and outer loops. The algorithm of the nested iteration is shown in Algorithm 1. To be specific, in the $w$-th inner loop of the $t$-th outer iteration, GradeOpt accepts guidelines beam $G_{t, w-1} = {\\{g_{t,w-1}^{(k)} | 1 \\le k \\le K\\}}$ from $(w - 1)$-th inner iteration instead of a single guideline for refining (line 5). Then, during the inner iteration, each $g_{t,w-1}^{(k)}$ will be sent for reflection and refinement with $L$ independently sampled inner batches $b_{in}^{(l)}$ in a parallel manner (line 9). After all refined guidelines for the $w$-th inner loop are finished, $G_{t, w} = {\\{g_{t,w}^{(l,k)} | 1 \\le l \\le L, 1 \\le k \\le K\\}}$, each new guideline $g_{t,w}^{(l,k)}$ will be tested over a hold-out validation set $D_{val}$ (line 14). Meanwhile, the top-$K$ performing guidelines will be kept as $G_{t, w}$ and passed to the $(w + 1)$-th inner loop. Finally, the beam output of the last iteration of inner loop $G_{t,w}$ will be sent to the $(t + 1)$-th outer iteration (line 4).\nWhile this procedure helps increase the accuracy and reliability, blindly increasing the iteration could lead to over-fitting and higher computational overheads [19]. This is particularly true for smaller datasets. To help address these challenges, we introduce an early-stopping criteria. Specifically, during the selection for top-$K$ performed $G_{t, w}$ in the $w$-th inner loop, we record the performance metric $m_w$ of the best performed guideline. Then, in the next $(w + 1)$-th inner iteration, we check if $m_{w+1}$ is improved. If $m_w$ stops improving for two consecutive inner iterations, it indicates that the current guideline is facing risks to be over-fitted, thus following inner iterations are skipped. Similarly, during the $t$-th outer iterations, if its inner iteration is terminated due to the early-stopping and $(t - 1)$-th outer iteration's inner iteration is also terminated by early-stopping, the following outer iterations will also be skipped.\nBatch Sampling Strategies. Using self-reflective approaches of LLMs to refine grading guidelines requires the exposure of similar errors in consecutive optimization iterations due to LLMs' lack of ability in generating appropriate modifications with one attempt [26]. This is especially true for complicated cases involving nuanced differences between score categories. However, the randomness of batch sampling in the outer loop fails to guarantee this pre-requisite, which limits the performance of GradeOpt. To solve this, we develop a novel sampling strategy, which leverages the misconfidence metric ($\\psi$) [41] to find challenging examples in $D_{train}$. To be specific, given $x_i$ as an input to Grader and $y_i$ as its human grading result, we calculate $\\psi_i = \\frac{\\text{max}_{j | i \\neq y_i} \\text{log } P_{LLM}(y_i | G, x_i)}{\\text{log } P_{LLM}(y_i | G, x_i)}$, where $y_i$ is the prediction of Grader. The misconfidence quantifies the discrepancy between the highest log probability of Grader's incorrect prediction $y_i$ and the log probability of correct prediction $\\hat{y_i}$. Intuitively, the larger $\\psi$ indicates that the Grader is giving the wrong judgment with a relatively high confidence over the correct one, thereby implying that the sample is more challenging.\nHowever, calculating $\\psi$ over all $x_i \\in D_{train}$ is computationally expensive and cannot be directly done in each iteration. To avoid introducing the additional computing cost to the current algorithm, we only calculate $\\psi_i$ for samples in current iteration batch $x_i \\in b_{out}$ and select the top-$C$ samples as seeds to query similar samples from $D_{train}$ through embedding similarities. In this way, we simplify the selection process and ensure the consecutive appearance of the similar challenging examples between iterations. At last, to avoid the optimization being operated over the same portion of samples from $D_{train}$ all the time, we only select half batch based on misconfidence, and keep the another half as random samples."}, {"title": "4.3 Test-time Adaptation Stage", "content": "In this stage, GradeOpt begins to perform the automatic grading to the large scaled unlabeled responses in test data. However, due to the diversity of language expressions existing in open-ended answers and other influence factors such as geography and time that change users' expression styles, the performance of the auto-graded is not always guaranteed to be the same as during training. Such phenomenon is well-recognized as the out-of-distribution (OOD) issue in many machine learning problems [14]. Prior work [14] has shown that capturing prediction probability statistics about correct or in-sample examples is often sufficient for detecting whether an example is in error or abnormal. Inspired by this, we compose a confidence indicator $\\zeta$ as follows: $\\zeta = D_{test} E_{x_i \\in D_{test}} \\text{max}_j (\\text{log } P_{LLM}(s_j|G, x_i))$, where $\\text{log } P_{LLM}(\\cdot)$ denotes the log likelihood probability given by the LLM. Intuitively, the log probability reflects the confidence that Grader gives to its graded results. By comparing $\\zeta$ with the average LLM confidence scores $\\mu$ on samples in $D_{train}$, we can know how serious the OOD phenomenon is. Specifically, when $\\zeta > \\mu$, it indicates that $G$ is well-applicable to $D'$. When $\\zeta < \\mu$, it suggests that the guideline is facing serious OOD influences which suggests the current grader may struggle to produce reliable and accurate predictions for $D_{test}$.\nIf the test samples are deemed to be OOD, a common solution is to first compose an adaption dataset from the testing scenario. Using this adaption dataset, we then perform test-time training on the existing model. To be specific, test-time training leverages the annotation samples from $D_{test}$ and fine-tunes the optimized guideline $G_{opt}$ with the same training process introduced in Section 4.2. Unfortunately, in the ASAG scenario, the annotation is usually expensive. Besides, it is challenging to ask pedagogical experts to provide a large amount of annotation samples to help the existing system adapt to any changes in a timely manner. To solve this issue, we propose an incremental labeling approach which checks the marginal performance changes brought by gradually increasing the size of annotation samples. By selecting the size with highest marginal gains in metrics like accuracy and Cohen's Kappa, GradeOpt only asks pedagogical experts for necessary annotations. This not only reduces the annotation work loads but also increases the adaption efficiency of the framework. Finally, when the $G_{opt}$ passes the OOD test, GradeOpt will be leveraged to finish ASAG over all samples in $D_{test}$."}, {"title": "5 Experiment", "content": "In this section, we conduct experiments to validate the effectiveness of GradeOpt. Through the experiments, we aim to answer the following research questions. (1) RQ1: Whether the refined guidelines based on prompt optimization match or exceed the performance of human-crafted guidelines? (2) RQ2: Are the optimized guidelines applicable to new datasets of the same or similar questions? (3) RQ3: How does each component contribute to the overall effectiveness of the guideline optimization system? Before presenting our experimental results and observations, we first introduce our general experimental settings."}, {"title": "5.1 Dataset", "content": "To answer the research questions above, we conduct experiments with a dataset of SAQs grading. Different from existing ASAG works [9, 28], which focus on student work datasets, in this work we extend ASAG to a pedagogical answer grading task. These datasets collect answers from teachers over a set of questions designed to capture their content knowledge (CK) and pedagogical content knowledge (PCK) of mathematics [8]. As the pedagogical answer grading requires a more intricate interpretation of the answer to understand the thought process, it will be valuable to explore the performance of GradeOpt in solving challenging ASAG tasks. Specifically, we selected three questions targeting teachers' PCK and two questions targeting CK to test our framework. PCK helps test the working of our framework in capturing the thinking of responders, as PCK questions assess teachers' ability to elaborate content to students and to reflect on students' misconceptions. CK questions focus on examining teachers' subject-related content knowledge through short answers, which is also a nontrivial task.\nOverall, the pedagogical dataset is collected from two separate surveys finished by middle school mathematics teachers: one named pilot study denoted as $D$, and the other named nationwide survey denoted as $D'$. The major difference between $D$ and $D'$ is the scale and semantic complexity, as a large portion of $D'$ contains responses that only implicitly reflect the thought process, which makes them harder to distinguish. In our experiment, we select five questions that are shared by both $D$ and $D'$. The data from $D$ and $D'$ is used for the training stage and test-time adaptation stage, respectively. During the training stage, the answers to each question in $D$ are split into training:validation:test with the ratio of 7:1:2. In the test-time adaptation stage, 100 samples from $D'$ are spared for test-time training and the remaining ones are used for test. The initial grading guidelines for each question are different, including types of assessment objectives and learning goals, which we refer to as expectations. Based on the number of expectations, we divide test questions into two types, i.e., single-expectation and multi-expectation. All answers are labelled using a three-point scale by two human raters."}, {"title": "5.2 Baselines", "content": "We compared our model with several representative ASAG baselines. Firstly, we choose two popular non-LLM methods, i.e., SBERT [33] with Logistic Regression and RoBERTa [24] with Fine-tuning. Both of them have demonstrated strong performance in prior studies [6, 31]. In addition, we adopt GPT-40 with zero-shot prompting as another baseline. Compared with non-LLM methods, LLM's exceptional instruction and human-like reasoning capabilities make it a powerful method when facing complicated grading cases [15]. At last, to mitigate the manual burden of revising the guidelines in the GPT-40 zero-shot setting, we implement and compare GradeOpt with APO [32], which is a state-of-the-art method for automatic prompt optimization tasks."}, {"title": "5.3 Implementations", "content": "To implement the nested iterative optimization, we set the outer batch size $|b_{out}| = 64$ and inner batch size $|b_{in}| = 8$. The outer loop iteration number $T = 5$ and the inner loop iteration number $W = 3$. We implement the beam search selection mechanism with Upper Confidence Bound (UCB) [1], where the guideline beam size $K = 4$. The evaluation metric for UCB is Cohen's Kappa as it empirically works better than other metrics. The agents in our framework are all powered by GPT-40 [29] with zero-shot prompting. The temperature for Grader is set to 0.0 to decrease the randomness of the result. The temperatures for both Reflector and Refiner are set to 0.5, since we want to encourage the LLMs to be more open in exploring the error reasons and propose the improving suggestions. For each question, we run the algorithm 3 times and report the average results."}, {"title": "5.4 Evaluation Metrics", "content": "In this work, we use Accuracy (Acc) and Cohen's Kappa (Kappa) as the evaluation metrics to compare the performance of different models. To be specific, accuracy measures the percentage of correct predictions over all predictions and kappa measures the alignment between model's and expert's grading results."}, {"title": "5.5 Results on Pilot Dataset", "content": "In this section, we answer RQ1 with comparisons between baseline models and GradeOpt on the Pilot Dataset $D$. From Table 2, we can observe that GPT-40 achieves the similar grading accuracy and Cohen's Kappa comparing to traditional ML-based methods on the average result. This finding indicates that LLMs are a promising tool for ASAG task even without training on a domain dataset. Beyond that, it also reveals that LLMs have great potentials in providing generous solution to various ASAG tasks in the future. Additionally, using LLMs for ASAG provides the benefit of better comprehensibility as educators can easily see LLMs' scoring rationale, which can be even given on the expectation level to assess the answer's fulfillment of learning goals.\nBy comparing the results of APO and GradeOpt with GPT-40, we can find both models bring steady performance gain to the original LLMs. This fact demonstrates our claim that using guidelines composed by pedagogical experts directly for LLMs is usually sub-optimal, which suggests that it is necessary to leverage additional guidance optimization algorithms to fully exploit the potential of LLMs in ASAG tasks. At last, our proposed GradeOpt always yields the best performance when compared to all baseline models over all the questions. This observation supports that GradeOpt can assist LLMs to achieve its optimal performance on ASAG tasks."}, {"title": "5.6 Results on National Dataset", "content": "We answer RQ2 by applying the optimized guidelines $G_{opt}$ learned from $D$ in Section 5.5 to grade the test split of national dataset $D'_{test}$. To be specific, if the $G_{opt}$ fails to pass the OOD test, where the confidence indicator $\\zeta < \\mu$, we will implement the test-time training with the train split of national dataset $D'_{train}$. Then, the adapted guideline will be tested over the same $D'_{test}$ again. Overall, the results are reported in Table 3.\nFrom the table, we can find that Q1 is marked with OOD Flag as its confidence indicator $\\zeta = -0.22 < -0.2 = \\mu$. By comparing its performance between Table 2 and Tabel 3, we can confirm that it suffers great performance drops. Meanwhile, the confidence indicator of $Q2, \\zeta = -0.16 > -0.2 = \\mu$, and its performance gap between $D$ and $D'$ is relatively smaller. These two observations indicate that the proposed confidence indicator is a valid indicator for the OOD detection purpose. On the other hand, by comparing $G_{opt}$ with the raw guidelines provided by $G_0$, we find that $G_{opt}$ consistently outperforms $G_0$. This observation indicates that even the automatically optimized guideline suffers from the OOD issue, it is still better than the raw guideline. Finally, by calculating the performance change between the guideline before and after the test-time training, we can find that GradeOpt is able to get adapted to the new examples with only limited available annotated examples. In addition, the performance of guidelines after the test-time training is restored back to the acceptable grading range, e.g., Kappa> 0.6, which indicates that the test-time training is an effective solution to help a high-performed $G_{opt}$ to quickly get applied to different datasets."}, {"title": "5.7 Ablation Studies", "content": "To answer RQ3, we conduct ablation studies to the nested iteration introduced in Section 4.2. We choose to experiment with Q4 as it is the only PCK question that contains one single expectation, as shown in Table 1. Our framework emphasizes PCK answers due to its challenge over CK answers in capturing the thinking process, as introduced in Section 1. The relatively simple rubric design of Q4 makes the shortest guideline prompt, leaving room for GradeOpt to add in its reflective experience as it iteratively learns from D. Thus, experimenting with Q4 can better showcase GradeOpt's optimization power. With the experimental results shown in the following sections, we demonstrate the effectiveness of each component.\nFirst, we demonstrate the superiority of our misconfidence-based batch sampling strategy by comparing it with the random-based one. From Figure 6, we can observe that misconfidence-based batch sampling results are more consistent and accurate. While random selection generates optimal guidelines in 2 to 4 rounds, misconfidence-based selection consistently optimizes guidelines in 3 rounds. This, together with high predictive accuracy and alignment brought by misconfidence-based selection, makes the system reliable in practical educational scenarios as the required training round number is coherent. Then, we conduct experiments over the sizes of outer batch $|b_{out}| \\in \\{20, 32, 64\\}$ and inner batch $|b_{in}| \\in \\{4, 6, 8\\}$, targeting at exploring the influence of those two hyper-parameters on the performance of GradeOpt. From the results in Figure 7a, we observe an increasing trend in accuracy and kappa as its outer batch size increases. This observation suggests that increasing the number of examples is"}]}