{"title": "KernelBench: Can LLMs Write Efficient GPU Kernels?", "authors": ["Anne Ouyang", "Simon Guo", "Simran Arora", "Alex L. Zhang", "William Hu", "Christopher R\u00e9", "Azalia Mirhoseini"], "abstract": "Efficient GPU kernels are crucial for building performant machine learning architectures, but writing\nthem is a time-consuming challenge that requires significant expertise; therefore, we explore using language\nmodels (LMs) to automate kernel generation. We introduce KernelBench, an open-source framework\nfor evaluating LMs' ability to write fast and correct kernels on a suite of 250 carefully selected PyTorch\nML workloads. KernelBench represents a real-world engineering environment and making progress on the\nintroduced benchmark directly translates to faster practical kernels. We introduce a new evaluation metric\n$fast_p$, which measures the percentage of generated kernels that are functionally correct and offer a speedup\ngreater than an adjustable threshold p over baseline. Our experiments across various state-of-the-art\nmodels and test-time methods show that frontier reasoning models perform the best out of the box but still\nfall short overall, matching the PyTorch baseline in less than 20% of the cases. While we show that results\ncan improve by leveraging execution and profiling feedback during iterative refinement, KernelBench\nremains a challenging benchmark, with its difficulty increasing as we raise speedup threshold p.", "sections": [{"title": "1 Introduction", "content": "AI relies on efficient GPU kernels to achieve high performance and cost and energy savings; however, developing\nkernels remains challenging. There has been a Cambrian explosion of ML architectures [7, 29, 33], but their\navailable implementations routinely underperform their peak potential. We are seeing a proliferation of AI\nhardware [4, 10, 11, 14, 24, 25, 26], each with different specs and instruction sets, and porting algorithms\nacross platforms is a pain point. A key example is the FlashAttention kernel [8], which is crucial for running\nmodern Transformer models the initial kernel released in 2022, five years after the Transformer was\nproposed; it took two more years from the release of NVIDIA Hopper GPUs to transfer the algorithm to the\nnew hardware platform. We explore the question: Can language models help write correct and optimized\nkernels?\nAI engineers use a rich set of information when developing kernels and it is not clear whether language\nmodels (LMs) can mimic the workflow. They use compiler feedback, profiling metrics, hardware-specific\nspecs and instruction sets, and knowledge of hardware-efficiency techniques (e.g., tiling, fusion). They can\nuse programming tools ranging from assembly (e.g., PTX as in DeepSeek-AI [9]) to higher-level libraries\n(ThunderKittens [32], Triton [36]). Compared to existing LM code generation workloads [43], kernel writing\nrequires a massive amount and diversity of information. We first design an environment that reflects the\ntypical AI engineer's workflow and supports providing LMs with this rich information. The environment\nshould:\n\u2022 Automate the AI engineer's workflow. The model should have full flexibility to decide which operators to\noptimize and how to optimize them.\n\u2022 Support a diverse set of AI algorithms, programming languages, and hardware platforms."}, {"title": "2 Related Works", "content": "Kernel libraries and compilers. We evaluate existing approaches for kernel programming along the\ndimensions of automation, breadth, and performance. Mainstream kernel programming libraries like cuDNN\n[22], CUTLASS [23], and Apple MLX [1] are hardware-specific and demand substantial engineering effort from\nhuman experts. Other libraries, like ThunderKittens [32] and Triton [36], successfully help AI researchers\nwrite a breadth of fast and correct kernels [2, 45], but still require human programming effort. Compiler-based\ntools, like torch.compile [28] and FlexAttention [34], automatically provide a narrow slice of optimizations.\nIn contrast to these efforts, we ask if LMs can automatically generate performant kernels for a breadth of A\u0399\nworkloads.\nLLMs for performance-optimized code generation. In the past year, there have been several ef-\nforts to build LMs that can automate algorithmic coding [5, 19, 31], resolving GitHub issues [43, 44], and\ndomain-specific coding [17, 46]. While these works focus on producing correct and functional code, subsequent\nworks have explored LMs' ability to produce solutions with better algorithmic and asymptotic efficiency\n[21, 40]. KernelBench focuses on wall-clock efficiency. LMs generate high-performance computing (HPC)\ncode, which requires an understanding of the underlying hardware features and device instruction set, and\ncommon performance characteristics of parallel processors.\nExisting works in the space of HPC code generation have evaluated LM performance on translating\narbitrary code samples from C++ to CUDA [35, 41] or generating well-known, low-level kernels such as\nGEMMs [38, 42]. KernelBench instead curates a set of 250 diverse kernels from real-world, modern deep\nlearning workloads, many of which do not have existing human-written implementations in other words,\nsolving KernelBench tasks are immediately beneficial for real deep learning workloads."}, {"title": "3 KernelBench: A Framework for AI Kernel Generation", "content": "KernelBench is a new framework for evaluating the ability of language models to generate performant kernels\nfor a breadth of AI workloads. In this section, we describe the task format, contents, and evaluation metric."}, {"title": "3.1 KernelBench Task Format", "content": "KernelBench contains 250 tasks representing a range of AI workloads, and is easily extensible to new workloads.\nThe end-to-end specification for a task is illustrated in Figure 1 and described below.\nTask input: Given an Al workload, the input to the task is a reference implementation written in\nPyTorch. Mimicking an AI researcher's workflow, the PyTorch code contains a class named Model derived\nfrom torch.nn.Module(), where the standard __init__ and forward() functions (and any helper functions)\nare populated with the AI workload's PyTorch operations.\nAI algorithms generally operate on large tensors of data. The optimal kernel for a workload depends on\nthe size and data type (e.g., BF16, FP8) of the tensor. Therefore, each task additionally contains functions\nget_inputs() and get_init_inputs(), which specify the exact input tensors that the kernel needs to handle."}, {"title": "3.2 Task Selection", "content": "The 250 tasks in KernelBench are partitioned into three levels, based on the number of primitive operations,\nor PyTorch library functions, they contain:\n\u2022 Level 1 (100 tasks): Single primitive operation. This level includes the foundational building blocks\nof AI (e.g. convolutions, matrix-vector and matrix-matrix multiplications, losses, activations, and layer\nnormalizations).\nSince PyTorch makes calls to several well-optimized and often closed-source kernels under-the-hood, it\ncan be challenging for LMs to outperform the baseline for these primitive operations. However, if an LM\nsucceeds, the open-source kernels could be an impactful alternative to the closed-source (e.g., CuBLAS [27])\nkernels.\n\u2022 Level 2 (100 tasks): Operator sequences. This level includes AI workloads containing multiple\nprimitive operations, which can be fused into a single kernel for improved performance (e.g., a combination\nof a convolution, ReLU, and bias).\nSince compiler-based tools such as the PyTorch compiler are effective at fusion, it can be challenging for\nLMs to outperform them. However, LMs may propose more complex algorithms compared to compiler\nrules.\n\u2022 Level 3 (50 tasks): Full ML architectures. This level includes architectures that power popular AI\nmodels, such as AlexNet and MiniGPT, collected from popular PyTorch repositories on GitHub.\nGiven the scale of modern models, it is critical to use kernels when running training and inference.\nUnfortunately, it has been difficult for the AI community to generate performant kernels. For instance,\nit took 5 years from the release of the Transformer architecture [39] to obtain performant kernels [8], let\nalone today's many new architectures. Peak performance kernels for these architectures require algorithmic\nmodifications that are often beyond the scope of a compiler.\nWe reiterate that each task contains a meaningful set of AI primitive operations or architectures, such that\nLM success on the task can directly lead to real world impact."}, {"title": "3.3 Metric Design", "content": "We describe the evaluation approach for KernelBench and how we compare the success of different LMs.\nEvaluation approach KernelBench is an evaluation-only benchmark. We do not provide ground truth\nkernels for the tasks since we imagine users benchmarking on a variety of hardware platforms (including new\nplatforms), input types, and workloads. However, by design, KernelBench is automatically verifiable. Given\na task, we randomly generate input tensors of the prescribed shape and precision and collect the PyTorch\nModel output. We can evaluate whether LM generations are correct and fast as follows:\n1. Correctness We compare the Model output to the LM-generated ModelNew output. We evaluate on 5\nrandom inputs per problem (detailed in Appendix B).\n2. Performance We compare the wall-clock execution time of Model against ModelNew using repeated trials\nto account for timing variations."}, {"title": "4 KernelBench Baseline Evaluation", "content": "In this section, we investigate how a range of LMs perform when evaluated off-the-shelf on KernelBench and\nexplore their capabilities and failure modes."}, {"title": "4.1 One-shot Baseline", "content": "We evaluate LMs using a prompt that contains one example of a PyTorch Model input and ModelNew output,\nhighlighting the task format. The example is simple, containing only an add operator (See Appendix C.1).\nGiven this in-context example and the PyTorch task Model to optimize, the LM generates ModelNew via\ngreedy decoding. We profile the generated code on an NVIDIA L40S GPU, and measure the $fast_p$ metric\nacross all problems. Table 1 shows that the LM-generated kernels achieves a speedup over PyTorch Eager in\nfewer than 20% of tasks on average."}, {"title": "4.2 Correctness: Error Analysis", "content": "In Figure 2, we analyze the failure modes of LMs across problems. It can be seen that a large proportion of\nmodel-generated kernels are incorrect. To better understand where model-generated kernels fail, we break\ndown their correctness issues into execution failures (CUDA/nvcc / Python compile-time errors, CUDA\nmemory violations, and runtime errors) and correctness errors (output tensor shape and value mismatches).\nWe observe that the reasoning LMs (01, R1) produce fewer incorrect solutions (< 55%) than other models\n(> 70%). However, we find this is mainly because they make fewer execution failures. All LMs struggle with\nfunctional correctness to a similar degree."}, {"title": "4.3 Performance: Speedup Distribution", "content": "A key point of interest is whether the functionally correct LM-generated kernels outperform the PyTorch\nbaseline. Figure 3 shows the distribution of $fast_p$ as p varies, indicating the percentage of kernels that are\np-times faster than the PyTorch Eager baseline (the top right of the plot is better). At p = 1, fewer than 15%\nof LM-generated kernels outperform PyTorch across all KernelBench levels. Reasoning-based LMs generally\noutperform the other LMs in providing speedups."}, {"title": "4.4 Performance Variations across Hardware", "content": "Our one-shot baseline makes no assumptions about the underlying hardware, so a natural question is how\nour analysis of the LM-generated kernels generalizes across various GPU types. Table 14 and Figure 8 show\nthat kernels outperforming PyTorch Eager on NVIDIA L40S in Level 1 achieve similar speedups versus the\nbaselines on other GPUs. However, on problems in Level 2, LMs exhibit larger variations in speedups across\nGPUs (Figure 9): DeepSeek R1-generated kernels achieve a $fast_1$ of 36% on NVIDIA L40S but 47% on\nNVIDIA A10G for Level 2. This suggests that one-shot LM-generated kernels may not generalize well across\nhardware. To generate target-specific kernels, we explore in Section 5.2 whether providing hardware-specific\ndetails in-context could help.\nOur analysis reveals that the best models available today struggle to generate correct kernels that\noutperform the baseline PyTorch speeds. LM-generated kernels frequently fail due to simple compiler and\nrun-time errors. Furthermore, it is difficult for LMs to write kernels that perform well across hardware\nplatforms given simple instructions."}, {"title": "5 Analysis of Model Capabilities", "content": "In the last section, we found that KernelBench is a challenging benchmark for today's models. In this section,\nwe conduct case studies to explore opportunities for improvement in future models and AI systems."}, {"title": "5.1 Case Study: Leveraging the KernelBench Environment Feedback at Test- Time", "content": "As observed in Section 4.2, execution failures are the most frequent failure mode in LM-generated kernels. The\nenvironment provided by KernelBench allows us to collect rich signals, including compiler errors, correctness\nchecks, and runtime profiling metrics, all of which can be fed back in to the LM to help it resolve kernel failures.\nTo explore how well LMs can use this feedback, we evaluate and compare two baselines: (1) generating\nmultiple parallel samples from the LM per KernelBench task and (2) sequentially generating kernels per\nKernelBench task by allowing the LM to iteratively refine using the execution feedback."}, {"title": "5.1.1 Repeated Sampling", "content": "The KernelBench environment enables programmatic verification of LM-generated kernels, allowing us to\ncollect and evaluate multiple LM generations per task [3, 12, 19]. We evaluate this repeated sampling\napproach using fast@k, which measures the percentage of tasks where the model generated at least one\nfunctionally correct kernel that is p times faster than PyTorch Eager when drawing k samples."}, {"title": "5.1.2 Iterative Refinement of Generations", "content": "The KernelBench environment is well-suited for collecting compiler feedback, execution errors, and timing\nanalysis using tools like the PyTorch profiler as ground-truth signals. We investigate whether leveraging this\nfeedback can help LMs to iteratively refine their generations."}, {"title": "5.1.3 Comparing Repeated Sampling and Iterative Refinement", "content": "In Table 2, we compare repeated sampling and iterative refinement given a fixed budget of 10 inference calls.\nBoth methods provide meaningful improvements over the one-shot baseline, with iterative refinement being\nmore effective in 5 of the 6 cases. However, ultimately we find that the effectiveness of the test-time methods"}, {"title": "5.2 Case Study: Generating Hardware-Efficient Kernels via Hardware Knowl- edge", "content": "It is clear that LMs demonstrate limited success at generating hardware-efficient kernels. This is likely due\nto the scarcity of kernel code in the training data and the fact that the optimal kernel may need to change\ndepending on the hardware platform-specific properties, as discussed in Section 4.4. In this case study, we\nexplore providing 1) in-context examples of best-practices for kernel engineering and 2) in-context hardware\nspecification details."}, {"title": "5.2.1 Hardware-aware In-Context Examples", "content": "Well-written kernels often use techniques such as fusion, tiling, recompute, and asynchrony to maximize\nperformance. We find that most of the one-shot generated kernels evaluated in Section 4 often do not use these\ntechniques. Here, we explore whether providing explicit in-context examples that use these techniques can\nhelp the LMs improve their performance on KernelBench. Specifically, we include three in-context examples:\nGeLU [13] using operator fusion, matrix multiplication using tiling [20], and a minimal Flash-Attention [8, 15]\nkernel that demonstrates shared memory I/O management.\nIn-context examples degrade the LM's overall $fast_1$ score since LMs attempt more aggressive\noptimization strategies, but result in more execution failures. OpenAI ol's generations are 25%\nlonger on average using the few-shot examples, compared to the generations produced by Section 4 baseline.\nHowever, among the correct solutions, the LMs apply interesting optimizations: we find that on 77% of\nGEMM variants in KernelBench Level 1, 01 applies tiling and improves speed over the one-shot baseline\n(although remains slower than PyTorch Eager due to the lack of tensor core utilization). On Level 2, 01\napplies aggressive shared memory I/O management on 11 problems, and is able to outperform PyTorch Eager\n(See Appendix F)."}, {"title": "5.2.2 Specifying Hardware Information", "content": "As discussed in Section 4.4, kernel performance varies depending on the hardware platform. For instance,\nFlashAttention-2 [6] degrades 47% in hardware utilization going from the NVIDIA A100 to H100 GPU.\nFlashAttention-3 [30], an entirely different algorithm, was written for the H100. In this study, we explore\nwhether LMs can use (1) hardware specifications such as the GPU type (H100, A100, etc.), memory sizes,\nbandwidths, TFLOPS and (2) hardware knowledge (e.g. definitions of threads, warps, thread-blocks, stream-\ning multiprocessors) in-context to generate improved kernels (See Appendix G for more detail on the context).\nModels rarely generate kernels that are optimized for the underlying hardware, highlighting\nroom for improvement for future models. Certain generations of GPUs (e.g. H100) feature a variety\nof new hardware units and instructions from their predecessors. Providing hardware information does not\nsignificantly impact the outputs of Llama 3.1 70B or DeepSeek-V3.\nInterestingly, we find that a subset of OpenAI ol and DeepSeek-R1 generated kernels use hardware-specific\ninstructions and optimizations. R1 attempts to generate warp matrix multiply-accumulate (wmma) instructions\n(Figure 10) for approximately 50% of the Level 1 matrix multiplication problems, although most fail to\ncompile. Among the functionally correct generations, R1 and ol produce 1-3 outliers per level that are \u2265 2\u00d7\nfaster than the Section 4 baselines. Overall, we find that LMs are better at adjusting their approaches when\nprovided with few-shot examples in Section 5.2.1 than with hardware information."}, {"title": "6 Discussion", "content": ""}, {"title": "6.1 Deep Dive Into Interesting Kernels", "content": "Here, we discuss a few surprising LM-generated kernels that demonstrate significant speedups over the\nPyTorch baseline. See detailed examples in Appendix D.\nOperator fusion GPUs have small amounts of fast-access memory and large amounts of slow-access\nmemory. Fusion can help reduce slow-access I/O costs by performing multiple operations on data that has\nbeen loaded into fast-access memory. We find that LMs optimize the GELU (2.9x) and Softsign (1.3x)\noperators by fusing their computations into a single kernel. LMs generated a kernel that fuses multiple\nfoundational operators - matrix multiplication with division, summation, and scaling - giving a 2.6x speedup.\nOverall, LMs leave many fusion opportunities on the table.\nMemory hierarchy Effective kernels explicitly manage utilization of the limited amounts of shared and\nregister memory. In the generated kernels, we found kernels that uses GPU shared memory - cosine similarity\n(2.8x) and triplet margin loss (2.0x) to achieve speedups. We did not find successful usages of tensor core\ninstructions, which are crucial for AI performance.\nAlgorithmic optimizations Kernels can require algorithmic modifications to better utilize the hard-\nware features. We found one interesting generation for the problem of performing a multiplication between a\ndense and diagonal matrix, where the kernel scales each row (or column), rather than loading the zero-entries\nof the diagonal matrix, yielding a 13x speedup over PyTorch Eager."}, {"title": "6.2 Conclusion", "content": "Our contributions are: (1) We present KernelBench, a framework that lays the groundwork for LM-driven\nkernel optimization, and (2) We evaluate a diverse set of models and approaches, analyzing their strengths\nand limitations, and providing insights into opportunities for improvement.\nOverall, while most benchmarks eventually saturate, KernelBench is designed to dynamically evolve as\nnew AI workloads arise. Our $fast_p$ metric can be adapted over time to measure the speedup threshold (p)\nover increasingly advanced baselines (i.e., beyond the PyTorch baseline used in our work). Since PyTorch\nis cross-hardware platform compatible, the PyTorch-based tasks in KernelBench tasks can be evaluated on\nevery new hardware platform release. Finally, unlike many benchmarks, success on KernelBench directly\nmaps to production value and real-world impacts (lowering costs and reducing energy consumption at scale).\nThese properties ensure that KernelBench will remain valuable in the ever-evolving AI landscape."}, {"title": "6.3 Opportunities for Future Work", "content": "We show that there is significant room for improvement on KernelBench given the currently available\nmodels. First, future work can explore the development of advanced fine-tuning and reasoning techniques,\nincluding agentic workflows. Since CUDA is a low-resource language, it would be valuable for future work to\nopen-source more high quality data. Second, LMs generate raw CUDA code in our experiments. However,\nfuture work can explore whether generating code using alternative programming abstractions (e.g., provided\nin ThunderKittens, CUTLASS, Triton, and others) can simplify the generation problem, for instance by\nmaking it easier for LMs to leverage tensor core instructions. Third, our evaluation has also been limited to\nGPUs so far and future work can expand to other hardware accelerators."}, {"title": "Ethics Statement", "content": "Optimized GPU kernels can lead to significant energy savings in large-scale machine learning workloads,\nreducing both computational costs and environmental impact. By providing a framework for AI-assisted\nperformance tuning, KernelBench contributes to more energy-efficient AI systems, aligning with global efforts\nto reduce the carbon footprint of computing infrastructure."}, {"title": "A KernelBench Task Example", "content": "Here we provide an example task from KernelBench. Each task is wrapped in a class named Model. A\ntask contains two key functions in the Model class, __init__ and forward; helper functions are included if\nnecessary. We fix the shape of inputs and vary the numerical values through randomly generated tensors. We\nprovide two functions, get_inputs and get_init_inputs, for generating random parameters for initializing\nthe model and running a forward pass, respectively."}, {"title": "B Evaluation Methodology and Baselines", "content": "All evaluations are conducted on a bare-metal NVIDIA L40S GPU with Ada Lovelace architecture unless\notherwise stated (such as the device generalization experiments in Section 4.4 and the hardware case study in\n5.2). The NVIDIA L40S has 48 GB of HBM memory and operates at 300W. Our environment uses Python\n3.10, PyTorch 2.5.0+cu124, and CUDA 12.4, which is also where our PyTorch Eager and torch.compile\nbaselines are derived from."}, {"title": "B.1 Kernel Evaluation Setup", "content": "Recall the KernelBench task entails a PyTorch reference module Model as baseline, and model-generated\nPyTorch architecture ModelNew with custom inline CUDA kernel.\nFor correctness, we set num_correctness to 5, where we check equivalence of output between reference\narchitecture Model and generated architecture with custom kernel ModelNew with 5 randomized inputs. We\nelaborate on our choice in Appendix B.2.\nFor performance, we measure the wall-clock execution time of nn.module.forward for both Model and\nModelNew. We ensure only one kernel is being evaluated (no other CUDA process) on current GPU. We\nwarm up for 3 iterations and then set num_profile to 100 times which measures the elapsed execution time\nsignaled between CUDA events torch.cuda.Event. We take the mean of the 100 trials, and also note its\nmax, min, and standard deviation. While the wall clock time might vary for every trial, we note our coefficient\nof variation (CV): std/mean is consistently < 3%, we use the mean of both measured wall clock time for\ncomparisons.\nTo compute the speedup of generated architecture over baseline architecture for individual problems, we\nuse the mean for both $speedup = T_{Model}/T_{ModelNew}$. For example, if $T_{Model} = 2 ms$ and $T_{ModelNew} = 1 ms$,\nwe have a 2x speedup with the newly generated kernel. We compare this speedup with our speedup threshold\nparameter p (as explained in section 3.3) to compute fastp scores."}, {"title": "B.2 Correctness Analysis Varying Number of Randomly Generated Inputs", "content": "Checking equivalence of programs in a formal sense is undecidable. \"The Halting Problem\" [37] states that it\nis impossible to decide, in general, whether a given program will terminate for every possible input. This\nproblem naturally extends to checking equivalence because in order to check whether two programs are\nequivalent, it is necessary to check their behavior for all inputs, including cases where one or both programs\nmay not terminate. Since determining whether a program halts on a given input is undecidable (the Halting\nProblem), checking equivalence also becomes undecidable.\nApproximate or heuristic methods are often used in practice for checking program equivalence. Random\ntesting is the most common practical approach, where the program is run with sets of randomly chosen\ninputs, and their outputs are compared. Random testing is particularly effective for AI kernels, where control\nflow is simpler and the focus is primarily on numerical correctness. By using diverse inputs, it can uncover\nerrors in computations or memory handling with high probability. Evaluating correctness more systematically,\nespecially in the presence of subtle hardware-specific behavior, is an area for further exploration. Future\nwork could investigate formal verification tools to provide stronger guarantees of equivalence.\nWe use five sets of random inputs for correctness, which is a good tradeoff between the ability to catch\nerrors and efficiency. In an experiment with 100 generated kernels, the results were as follows: 50 kernels\nwere correct (all 5/5 and 100/100), 19 had output value mismatches (19 0/5 and 0/100), 4 had output shape\nmismatches, 10 encountered runtime errors, and 17 had compilation errors. Notably, the 0/5 and 0/100\nfailures indicate that no partial correctness was observed."}, {"title": "B.3 Distribution of Model Performance for One-Shot Baseline", "content": "Here we examine the quality of (functionally correct) kernel generations across a wide variety of models.\nFigure 7 shows the distribution of speedups for various kernels across different levels and models. The median\nspeedup for both Level 1 and Level 3 are less than 1, and the median speedup for Level 2 is only slightly"}, {"title": "B.4 PyTorch Baselines", "content": "PyTorch offers two common execution modes: Eager and torch.compile. Aside from the results shown in\nTable 1, all performance analysis is evaluated against PyTorch Eager.\nPyTorch Eager is the default execution mode of PyTorch, which dynamically executes computation\nby invoking calls to highly optimized closed-source kernels.\nPyTorch Compile or torch.compile uses rule-based heuristics over the underlying computation graph\nduring an initial compilation phase and invokes various backends to perform optimizations like kernel fusion\nand graph transformations. In Table 1, our performance baseline for torch.compile assumes the default\nconfiguration using PyTorch Inductor in default mode. Furthermore, we exclude the torch.compile compile"}, {"title": "C Experiment Prompting Details", "content": "We provide details for the prompting strategies and associated sampling strategies used in Section 4 and\nSection 5."}, {"title": "C.1 One-shot Baseline Prompt", "content": "For the one-shot baseline as shown in Section 4.1, we want to examine each model's out-of-the-box ability to\ngenerate kernels by providing the minimum set of information while ensuring the instructions and output\nformat are clear. We query each model with the following prompt and a pair of in-context add examples\n(the PyTorch reference add and its CUDA kernel counterpart using inline compilation) to provide the\noutput format. We sample the model with greedy decoding to ensure deterministic output, which is setting\ntemperature = 0."}, {"title": "C.2 Repeated Sampling Prompts", "content": "For repeated sampling, we use the same prompt that we used for the one-shot baseline in Appendix C.1. We\nused the same sampling temperature described in [3] as they allow sample diversity while ensuring quality.\nSpecifically we use temperature = 1.6 for Deepseek-V3 and temperature = 0.7 for Llama 3.1-70B."}, {"title": "C.3 Iterative Refinement Prompts", "content": "For iterative refinement, we start with the same initial prompt that we used for the one-shot baseline in\nAppendix C.1. A limitation of our experiments is that we sample with temperature= 0 to focus on the effect\nof iterating based on feedback rather than introducing variability. On subsequent generations, we prompt the\nmodel with the following template depending on the feedback it expects:"}, {"title": "C.4 Few-Shot in Context Prompts", "content": "For Few-Shot experiments as outlined in Section 5.2.1. We provide more details about the in-context example\nin Appendix F. We sampled these experiments with temperature 0."}, {"title": "C.5 Hardware Case Study Prompts", "content": "Here we provide hardware information. This is used in Section 4.4 and elaborated more in G, sampled with\ntemperature = 0."}, {"title": "D Kernels of Interest", "content": "In this section we provide examples of interesting or notable kernel generations. We first expand on the\ndiscussion in Section 6, where we defined the following categories of optimizations: algorithmic optimizations,\noperator fusion, and using hardware features."}, {"title": "D.1 Algorithmic Optimizations", "content": "13x Speedup on Level 1 Problem 11 by Claude-3.5 Sonnet\nThe original torch operator is torch.diag(A) @ B, multiplying a diagonal matrix formed from the vector A\nwith the matrix B. The model identifies an optimization in the special case of a diagonal matrix multiplication,\nwhere the diagonal matrix doesn't need to be explicitly constructed. Instead, each element of the vector A is\ndirectly multiplied with the corresponding row in matrix B, significantly improving performance:"}, {"title": "D.2 Kernel Fusion", "content": "2.9x Speedup on Level 1 Problem 87 by DeepSeek-V3\nGeLU reference in torch:\n1 $0.5 * x * (1.0 + torch. tanh (math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0)))$"}, {"title": "D.3 Hardware Features", "content": "2.8x Speedup on Level 1 Problem 96 by OpenAI-01\nTorch reference for Cosine Similarity Loss\n1 $cosine_sim = torch.nn.functional.cosine_similarity (predictions, targets, dim=1)$\n2 $return torch.mean (1 - cosine_sim)$"}, {"title": "D.4 Iterative Refinement Examples", "content": ""}, {"title": "D.4.1 Iteratively Trying new Optimizations", "content": "We provide an example of a kernel that iteratively improves on its existing generation. In the following\nexample, the model attempts new optimizations incorrectly, fixes them, and continue to attempt new opti-\nmizations, improving its kernel to faster than the torch.compile baseline (1.34ms) but short of the Torch\nEager baseline (0.47ms)."}, {"title": "D.4.2 Leveraging Feedback to Correct Kernel Code", "content": "Level 2, Problem 73: 2D Convolution with a BatchNorm and a scale factor. DeepSeek-R1 with\nExecution Feedback\nWe provide an example of a kernel that the model struggles to generate correctly, and produces a correct\nkernel after iterative refinement using execution feedback."}, {"title": "D.4.3 Iterative Refinement Never Fixes the Error", "content": "Level 1, Problem 54: 3D Convolution square input and square kernel. DeepSeek-R1 with\nExecution and Profiler Feedback\nThis problem is particularly interesting because no model is able to consistently produce functional code\nfor this kernel, even with different forms of feedback and profiling information. Interestingly, the example\nbefore is an arguably more difficult version of this kernel that fuses the 3D convolution with another operator,\nand the same model is able to generate functional code for this task. In the example above, the model\nconsistently makes the same mistake and continually generates a functionally incorrect kernel with the same\nvalue errors."}, {"title": "E Iterative Refinement on Correctness", "content": "Here we show that fast0 across iterative refinement (Section 5.1.2) configurations at a turn budget of N = 10\ncompared to one-shot baseline (Section 4.1). We find that models self-correct more effectively with execution\nfeedback E, fixing issues especially related to execution errors. Notably, DeepSeek-R1 on Level 1 and 2\ncan generate a functional kernel on >90% of the tasks given 10 turns of iterative refinement. However, the\nremaining incorrect kernels almost always fail due to functional incorrect"}]}