{"title": "Scaling laws for decoding images from brain activity", "authors": ["Hubert Banville", "Yohann Benchetrit", "St\u00e9phane d'Ascoli", "J\u00e9r\u00e9my Rapin", "Jean-R\u00e9mi King"], "abstract": "Generative AI has recently propelled the decoding of images from brain activity. How do these approaches scale with the amount and type of neural recordings? Here, we systematically compare image decoding from four types of non-invasive devices: electroencephalography (EEG), magnetoen-cephalography (MEG), high-field functional Magnetic Resonance Imaging (3T fMRI) and ultra-high field (7T) fMRI. For this, we evaluate decoding models on the largest benchmark to date, encompassing 8 public datasets, 84 volunteers, 498 hours of brain recording and 2.3 million brain responses to natural images. Unlike previous work, we focus on single-trial decoding performance to simulate real-time settings. This systematic comparison reveals three main findings. First, the most precise neuroimaging devices tend to yield the best decoding performances, when the size of the training sets are similar. However, the gain enabled by deep learning - in comparison to linear models - is obtained with the noisiest devices. Second, we do not observe any plateau of decoding performance as the amount of training data increases. Rather, decoding performance scales log-linearly with the amount of brain recording. Third, this scaling law primarily depends on the amount of data per subject. However, little decoding gain is observed by increasing the number of subjects. Overall, these findings delineate the path most suitable to scale the decoding of images from non-invasive brain recordings.", "sections": [{"title": "1 Introduction", "content": "Decoding natural images from brain activity originated in the 2000s (Kamitani and Tong, 2005; Miyawaki et al., 2008; Naselaris et al., 2009) but progressed rapidly over the past two years. Reconstructing images from functional Magnetic Resonance Imaging (fMRI) (Ozcelik and VanRullen, 2023; Mai and Zhang, 2023; Zeng et al., 2023; Ferrante et al., 2022; Scotti et al., 2024) and magnetoencephalography (MEG) (Benchetrit et al., 2024) can now be achieved by training a deep neural network to predict, from brain signals, the latent representation of an image, and then using this prediction to condition an image generation model (see Figure 1A).\nFour main factors are presumably responsible for this recent progress. First, recent studies train their decoders on a larger amount of data than in the past: it is now common to train models on several hours of brain recordings per subject (D\u00e9fossez et al., 2022; Gwilliams et al., 2023; Schoffelen et al., 2019; Tang et al., 2023; Armeni et al., 2022; Allen et al., 2022; Scotti et al., 2024; Benchetrit et al., 2024; Hebart et al., 2023; Chehab et al., 2022). Second, several approaches benefited from hardware improvements, such as the development of ultra high field (7T) fMRI (Allen et al., 2022). Third, modern deep learning has effectively provided neuroscience with powerful representations of images in the brain (Kriegeskorte and Diedrichsen, 2016; Yamins et al., 2014; Eickenberg et al., 2017; Schrimpf et al., 2018). Indeed, computer vision models like OpenAI CLIP (Radford et al., 2021; Ozcelik and VanRullen, 2023; Conwell et al., 2022) and DINOV2 (Oquab et al., 2023; Benchetrit et al., 2024; Adeli et al., 2023), have been shown to learn representations that linearly predict brain responses to natural images. Fourth, recent models based on diffusion effectively help reconstructing plausible images from the decoding of these latent image features.\nWhat are the best paths to improve brain-to-image decoding? In spite of a flourishing field, this issue is particularly difficult to address. First, most studies focus on a single neuroimaging device, i.e., either electroencephalography (EEG), MEG, fMRI at 3 Tesla (3T fMRI) or 7T fMRI. Second, existing datasets contain"}, {"title": "2 Methods", "content": "2.1 Problem statement\nGoal We aim to systematically compare brain-to-image decoding approaches, and identify potential scaling laws, i.e., how decoding improves with the type and amount of data. For this, we curate the largest public datasets into a unified benchmark and compare single-trial decoding performance across different experimental setups.\nFormalization Decoding images from brain signals at the pixel-level is challenging because the brain does not represent images in this feature space (Miyawaki et al., 2008). Over the years, it has thus become standard to learn to predict latent embeddings of images and to use these predictions to condition an image generation model (Lin et al., 2019; VanRullen and Reddy, 2019; Ozcelik and VanRullen, 2023; Chen et al., 2023). Formally, this approach involves three components:\n\u2022 Image module $f_e : \\mathbb{R}^{H\\times W\\times 3} \\rightarrow \\mathbb{R}^{F}$, to transform image $I_i$ into a latent embedding $z_i$,"}, {"title": "2.2 Denoising", "content": "Brain signals are often denoised before they are fed into the brain module $g_\\theta$. For example, it is common to average the K brain responses to the same image $(\\overline{X_i} = \\frac{1}{K}\\sum_{k=1}^{K} X_{i,k})$ or to average the K predicted embeddings $(\\overline{Z_i} = \\frac{1}{K}\\sum_{k=1}^{K} \\widehat{z}_{i,k})$. For fMRI, it is also common to first fit a Generalized Linear Model (GLM) (Friston et al., 1995) on the whole dataset (or on each of its recordings). Generally, GLMs estimate the fMRI response from the convolution of each image-presentation boxcar function with a parameterizable hemodynamic response function (HRF). However, as the resulting parameters $\\beta$ cannot be applied in real-time and are typically computed irrespective of train/test splits, we will here focus on predicting images from X directly. More generally, denoising strategies are often paradigm-dependent (e.g. the results will depend on the number of repetitions within and across subjects), which can hinder the comparison of decoding performances and limit their transferability to real-time applications. To address this, we primarily focus on single-trial performance without denoising."}, {"title": "2.3 Brain modules", "content": "We implement two state-of-the-art architectures to predict image embeddings from brain activity. A detailed description of the hyperparameter search procedure and architecture configurations is provided in Appendix D. For clarity, we compare these architectures to a simple ridge-regularized linear regression trained to predict image embeddings from either M/EEG or fMRI.\nLinear model baseline The linear model is a ridge regression (Hoerl and Kennard, 1970). In practice, we use scikit-learn's RidgeCV (Pedregosa et al., 2011), with \u03b1 selected log-linearly between 10-4 and 108 and otherwise default parameters. We train and evaluate on each subject separately.\nAs compared to linear models, deep learning architectures make it easier to learn on data from multiple individuals at once (e.g. with subject-specific layers or embeddings (D\u00e9fossez et al., 2022; Chehab et al., 2022)), leveraging cross-subject brain activity patterns into representations that maximally align with the pretrained image representation.\nM/EEG deep learning module We use the convolutional architecture of D\u00e9fossez et al. (2022); Benchetrit et al. (2024). This architecture includes a spatial attention layer, a subject-specific linear layer, a series of residual dilated convolutional blocks, a temporal aggregation layer, and two projection heads (one for each loss term described below in Equation (3)). In the largest configuration obtained with hyperparameter search, this yields a total of 20.8M parameters.\nfMRI deep learning module We adapt the convolutional architecture of Scotti et al. (2023) for (1) multi-subject training and (2) handling BOLD data with a time dimension 1. For this, we first apply a subject-specific"}, {"title": "Training objective", "content": "To learn to predict image embeddings from brain activity, we use a combined retrieval\u00b3 and reconstruction loss, as in Benchetrit et al. (2024):\n$\\mathcal{L}_{CLIP}(\\theta) = \\frac{1}{B}\\sum_{i=1}^{B} -\\log \\frac{\\exp(s(\\hat{z}_i, z_i) / \\tau)}{\\sum_{j=1}^{N} \\exp(s(\\hat{z}_i, z_j) / \\tau)}$   (1)\n$\\mathcal{L}_{MSE}(\\theta) = \\frac{1}{NF} \\sum_{i=1}^{N} ||z_i - \\widehat{z}_i||^2$   (2)\n$\\mathcal{L}_{Combined} = \\lambda \\mathcal{L}_{CLIP} + (1 - \\lambda) \\mathcal{L}_{MSE}$   (3)\nwhere $s$ is the cosine similarity and $\\tau$ is a temperature parameter that we set to 1 in all experiments. Based on early experiments, we fix $\\lambda = 0.25$ to balance out the contribution of the two loss terms."}, {"title": "Training details", "content": "We train M/EEG and fMRI modules using the Adam optimizer (Kingma and Ba, 2014) with default parameters (\u03b2\u2081=0.9, \u03b22=0.999) for up to 50 epochs. The learning rate and batch size were selected as part of the hyperparameter search procedure and differ per neuroimaging device and data regime (Appendix D). We use early stopping on a validation set obtained by randomly sampling 20% of the training data, with a patience of 10 epochs, and evaluate the performance of the selected model on a held-out test set. Models are trained on a single Volta GPU with 32 GB of memory. We repeat training using three different random seeds for the weight initialization of the brain module, with two exceptions in Section 3.7: first, when analyzing the impact of the number of subjects, we additionally repeat the sampling of subjects three times; second, when analyzing the impact of trial quantity, we instead use two random seeds for weight initialization and two random seeds for subsampling training trials."}, {"title": "2.4 Image and reconstruction modules", "content": "We focus our benchmark on the ability to predict, from brain activity, the latent embeddings of a state-of-the-art computer vision model. For this, we use DINOv2-giant (Oquab et al., 2023) and take the average output token as target for our embedding prediction task (F = 1536). DINOv2 image embeddings have shown great transferability and performance on multiple computer vision downstream tasks and yielded high brain-to-image retrieval performance in previous work (Benchetrit et al., 2024). We z-score-normalize the latent embeddings of the images of each dataset by using the training set statistics. Additionally, we compare images reconstructed from brain activity using the methodology of Ozcelik and VanRullen (2023), for four representative datasets (see description in Section 3.6)."}, {"title": "2.5 Data", "content": "We use eight publicly available datasets of brain activity recorded in response to image stimuli: Xu2024 (Alljoined) (Xu et al., 2024), Grootswagers2022 (THINGS-EEG1) (Grootswagers et al., 2022), Gifford2022 (THINGS-EEG2) (Gifford et al., 2022), Hebart2023meg (THINGS-MEG) (Hebart et al., 2023), Shen2019 (DeepRecon) (Shen et al., 2019), Hebart2023fmri (THINGS-fMRI) (Hebart et al., 2023), Chang2019 (BOLD5000) (Chang et al., 2019) and Allen2022 (Natural Scenes Dataset, or NSD) (Allen et al., 2022). The datasets are summarized in Table 1. A detailed description of each dataset is provided in Appendix A. The brain imaging data was collected and publicly shared by the authors of each dataset (Xu et al., 2024; Grootswagers et al., 2022; Gifford et al., 2022; Hebart et al., 2023; Shen et al., 2019; Chang et al., 2019; Allen et al., 2022).\nFor the THINGS-derived datasets (Grootswagers2022, Gifford2022, Hebart2023meg, Hebart2023fmri), we removed from the training set the images whose category was also in the test set to avoid categorical leakage between the train and test splits as in Benchetrit et al. (2024). On Allen2022, we follow previous image decoding work and use only the four (out of eight) subjects that completed all 40 recording sessions.\nWe subsample the test set of each dataset by randomly selecting 100 unique test images (except for Shen2019, which has only 50 test images available), and keeping all repetitions for these 100 images. When studying the impact of averaging over multiple repetitions at test time, we also vary the number of available repetitions in the test set, and evaluate decoding on averaged repetitions (either within- or across-subjects)."}, {"title": "2.6 Preprocessing", "content": "M/EEG We apply minimal preprocessing to M/EEG data following previous work (D\u00e9fossez et al., 2022; Benchetrit et al., 2024). First, raw data is highpass-filtered above 0.1 Hz and downsampled to 120 Hz. Each channel is independently normalized using a robust scaler and values outside [-20, 20] are clipped to minimize the impact of large outliers. Data is then epoched relative to stimulus onset and baseline-corrected by subtracting the channel-wise average value from the pre-stimulus interval. Epochs always extend to 1 s after stimulus onset, but have different start times to based on previous research: -0.1 for Grootswagers2022 (as in Grootswagers et al. (2022)), -0.2 for Gifford2022 (as in Gifford et al. (2022)) and -0.5 for Hebart2024meg (as in Benchetrit et al. (2024)). For Xu2024, we start epochs -0.3 s relative to stimulus onset to use as much of the previous interstimulus interval segments as possible, however, we use the same interval as in Xu et al. (2024) for baseline correction, i.e., (-0.05, 0.0).\nfMRI We use fMRIPrep 23.2.0 (Esteban et al., 2019) with default parameters to process the fMRI datasets into the standard space MNI152NLin2009aSym (Fonov et al., 2009). Each brain volume of the time series is then projected onto the fsaverage5 surface (Fischl et al., 1999). This yields, for each recording run, a time series of brain volumes of shape (20484, T) where T is the total number of TRs recorded for this run. Following this, we remove low-frequency noise in the fMRI signal using an additional detrending step: we fit a cosine-drift linear model to each voxel in the time series, and subtract it from the raw signal. Each time series"}, {"title": "2.7 Evaluation", "content": "We evaluate the ability of brain modules to predict $z_{i,k}$ given $X_{i,k}$ across different datasets, subjects and numbers of unique image presentations. To evaluate prediction performance, we compute the average feature-wise Pearson correlation $R = \\frac{1}{F} \\sum_{f=1}^{F} corr(z^{(f)}, \\hat{z}^{(f)})$. Whenever applicable, we also report the standard error of the mean computed across subjects. Of note, we use the output of the MSE head to evaluate performance as the reconstruction objective $\\mathcal{L}_{MSE}$ is conceptually more aligned with the feature-wise Pearson correlation evaluation metric."}, {"title": "2.8 Scaling laws", "content": "We evaluate the scaling behavior of image decoding models by varying data quantity along two axes: (1) number of training trials and (2) number of subjects.\nImage quantity analysis We focus on single-subject models and vary, in the training set, the number of unique images as well as the number of image repetitions, whenever available (i.e., in Allen2022, Gifford2022, Xu2024). We repeat this analysis for the first 10 subjects of every dataset.\nSubject quantity analysis We vary the number of subjects seen by the models from one to all subjects available in a given dataset. For this, we compare two additional configurations: first, we use all available data per dataset (all-trials) and second, we approximately match the number of trials across datasets (matched-trials, described in Appendix B)."}, {"title": "2.8.1 Recording time estimation", "content": "While the number of image presentations is a straightforward measure of data quantity, it does not reflect the longer stimulus presentation times and interstimulus intervals used in some datasets. For instance, fMRI datasets relied on much longer stimulus onset asynchrony (SOA) durations (i.e., the time elapsed between the start of one image presentation and the start of the following image presentation) than M/EEG to account for the slow hemodynamic response. For instance, the SOA is 10s for Chang2019 but only 100 ms for Grootswagers2022. Therefore, we additionally study scaling laws from the angle of recording duration, as computed by multiplying the number of training trials by the SOA."}, {"title": "2.8.2 Cost estimation", "content": "When building a new dataset, the choice of neuroimaging device and the targeted quantity of data is strongly influenced by the cost of data acquisition. As cost varies greatly between different neuroimaging devices, it is therefore useful to also study how it relates to decoding performance. To provide an approximate scaling cost for each device, we surveyed publicly available information about neuroimaging data collection services (Appendix F). Based on this information, hourly cost (in USD) is estimated at $263 for EEG, $550 for MEG, $935 for 3T fMRI and $1093 for 7T fMRI. The reader should bear in mind that these are rough estimates and that data acquisition costs can vary significantly between countries and institutions."}, {"title": "2.9 Image reconstruction", "content": "The analyses described above focus on the decoding of an image embedding given single-trial or test-time averaged brain signals. However, it is becoming increasingly common to evaluate decoding pipelines on their ability to reconstruct the original image rather than its latent representation only. Following this approach, we implement an additional generation step for all decoders."}, {"title": "3 Results", "content": "3.1 Encoding analysis\nTo validate the datasets, we first test a standard linear encoding pipeline (Naselaris et al., 2011). Here, encoding refers to the prediction of brain responses given the image features. For this, we build a time-lag concatenation of image features to predict, with a ridge regression, the amplitude of the neural time series at each time point relative to stimulus onset (see Appendix C). We use DINOv2 (Oquab et al., 2023) to extract image features, as this unsupervised model has been shown to capture representations similar to those of the brain (Benchetrit et al., 2024) (see Section 3.6 for analyses using other pre-trained image representations). These linear encoding models are trained for each subject separately. Finally, we use the Pearson correlation (R) to evaluate the similarity between the true and predicted brain responses held out from the same subject.\nThe encoding results confirm that EEG, MEG, 3T and 7T fMRI can be reliably predicted from the features of the images that subjects watched (see Figure 1B and S1). As expected, brain responses to visual stimuli are best predicted in the occipital lobe, host of the visual cortices, but these responses are visible in a distributed set of brain regions. Overall, these results confirm that the brain responses to visual stimuli can be modeled, for each of these datasets, from the pretrained embedding of a computer vision model."}, {"title": "3.2 Linear decoding", "content": "Encoding analyses make it difficult to compare different types of brain recordings, because the space in which they are evaluated (e.g. voxels or sensors) varies arbitrarily between datasets. Thus, we now turn to linear decoding. We train and evaluate linear ridge regression decoders at each time step relative to image onset, and evaluate how well image features can be predicted from brain activity patterns across time.\nFigure 2A and C show that image embeddings can be maximally decoded 110 ms and 380 ms after image onset for EEG and MEG, respectively. For fMRI, which captures the slow blood-oxygen-level-dependent (BOLD) response, decoding performance peaks around 4.5-5.2s after image onset. The decoding performance then decreases to chance-level around 750 ms (EEG), 1,500 ms (MEG), 7.5s (3T fMRI) and 16s (7T fMRI) after image onset."}, {"title": "3.3 Decoding with deep learning", "content": "To what extent can these linear decoding scores be improved with deep learning models? To address this question, we implemented and trained the deep learning pipelines described in Section 2.3 on sliding windows. To account for different temporal resolutions across devices, we used 100-ms windows for M/EEG, and a single TR per window for fMRI (1 TR corresponds to 1.5 to 2s in the curated datasets).\nThese deep learning models reveal a similar decoding dynamic (Figure 2B-D). Critically, we observe a significant improvement over linear baselines (two-sided Wilcoxon signed rank test (Wilcoxon, 1945) across subjects and datasets, p < 10-14; see Figure 2E). Device performance was ordered similarly to linear models, with EEG and 7T fMRI leading to the worst and best performances, respectively. Interestingly however, the gain in performance observed between linear and deep models varies across devices: 1.9-2.4x for EEG, 1.5x for MEG, 1.1-1.2x for 3T fMRI, and 1.2x for 7T fMRI. In other words, the devices that benefit the most from deep learning pipelines appear to be those that are typically associated with lower signal-to-noise ratios."}, {"title": "3.4 Impact of test-time averaging", "content": "In all datasets, test set images were shown multiple times to each subject. This allows improving signal-to-noise ratio and decoding performance, by e.g. learning to predict from an averaged response, or averaging single-trial predictions before evaluating metrics.\nTo evaluate whether this multiple-repetition approach effectively scales decoding performance, we systematically vary the number of test image repetitions used for averaging over the test set. Results for both sliding- and growing-window models are shown in Figure 3A-B. For all devices, adding more test repetitions improves decoding performance (Spearman rank correlation of 0.33-0.72, 0.51, 0.58-0.97 and 0.82 for EEG, MEG, 3T and 7T fMRI, respectively). However, this gain follows a moderate log-linear relationship: the gain in decoding performance rapidly decreases with the increasing number of repetitions. Further averaging test predictions across subjects (black lines and bars in Figure 3) leads to additional small improvements. Overall, we observe diminishing returns for all datasets, suggesting test-time averaging may not be ideal to scale decoding performance."}, {"title": "3.5 Image retrieval", "content": "Next, we evaluate the performance of our decoding pipeline in a retrieval setting: given the predicted DINOv2-giant embedding $\\hat{z}$ for an image in the test set, we identify the image, among the 100 (50 for Shen2019) unique images in the test set whose true image embedding is most similar to $\\hat{z}$. Of note, the prediction we use is the output of the CLIP head of our model, as it is specifically trained using a retrieval objective ($\\mathcal{L}_{CLIP}$).\nFigure 4A shows a sample of the best retrievals obtained across one representative dataset per device, to align with reconstruction analyses (Figure 5). For all devices, top-1 or top-2 predictions are often correct, and wrong predictions usually share categorical semantic information with the ground truth (e.g. animals, inanimate objects, etc.). Of note, the performance on Shen2019 can be partially attributed to the smaller size of its retrieval set (50 vs. 100)."}, {"title": "3.6 Image reconstruction", "content": "Image retrieval requires having access to the true image in the test set. To alleviate this constraint, we also evaluate image reconstructions from our decoders. For this, we conditioned the generation of images from the decoded image embeddings, as described in Section 2.9.\nFigure 5 shows a sample of the reconstructions obtained on the same images as in Figure 4. Overall, the images are never perfectly reconstructed, but nevertheless often share low-level as well as semantic features with the images seen by the subjects.\nTo quantify these qualitative observations, we evaluate reconstructions with pixel-, low- and semantic-level metrics, and compare them across decoding approaches (Section 3.4). Specifically, to evaluate the consistency between a stimulus image I and its reconstruction \u00ce, we select a representation method \u03c4 and compute the Pearson correlation between $\\tau(I)$ and $\\tau(\\widehat{I})$. In our study, we choose three different representations for \u03c4, covering both perceptual and semantics aspects of images: (1) the unmodified image itself as an array of pixels, (2) the AlexNet-2 embedding, and (3) the CLIP-final embedding of an image. Averaging these correlations across unique images for each representation \u03c4, we obtain three different metrics that we respectively denote"}, {"title": "3.7 Comparing decoding performance across data quantities", "content": "How does image decoding scale with the amount of brain signals? To address this question, we train our decoders on increasingly larger subsets of neuroimaging data and measure the corresponding single-trial decoding performance (i.e., without test-time averaging).\nScaling trials First, we consider the scaling of decoding performance within subjects. Figure 6A shows that different devices require different numbers of trials to reach the same decoding performance. A log-linear fit shows that, to reach R = 0.01, 7T fMRI only requires 57 trials, whereas 3T fMRI requires between 123 and 522 trials, MEG requires 2,3K trials, and EEG requires between 4,9K and 5K trials."}, {"title": "Scaling subjects", "content": "Second, we consider the scaling of decoding performance across subjects (Figure 6B). Results suggest that scaling the number of subjects yields limited improvement in most cases and may even lead to a decrease in performance (Shen2019, Hebart2023fmri). Although our models are trained and tested on the same subjects, inter-subject variability (see also Figure 2E) appears to harm overall model performance, especially in low-subject regimes, e.g. fewer than 10 subjects. Of note, the Grootswagers2022 dataset, with 48 subjects, has the largest number of subjects in our benchmark. Yet, the improvement of 0.004 obtained by doubling the number of subjects from 24 to 48 is not significant (two-sided t-test, p = 0.49). Scaling the number of EEG subjects does not seem to provide clear benefits for decoding performance."}, {"title": "Scaling both trials and subjects", "content": "Next, we compare data quantity across a mixture of data regimes (i.e., number of subjects and trials) by considering (1) the total number of training trials across subjects) and (2) the recording duration (i.e., the number of hours corresponding to a given number of trials). As shown in Figure 7, decoding performance increases with the amount of data across all datasets. This increase follows a log-linear trend, whose slope and intercept depends on the recording device. For example, focusing on models fitted on the number of recording hours (Figure 7B, middle column), EEG has a slope of 0.045 (\u00b10.003 standard error of the mean), MEG of 0.064 (\u00b10.011), 3T fMRI of 0.048 (\u00b10.009) and 7T fMRI of 0.075 (\u00b10.009).\nStrikingly, 7T fMRI (Allen2022) yields the best decoding performance across the board, both in terms of number of trials and hours of recording. 7T fMRI aside, the device that best scales decoding performance depends on what factor (trials, time, cost) is considered (Figure 7B). When considering the number of trials, 3T fMRI either outperforms (Chang2019) or works similarly to MEG with a similar number of trials, while EEG lags behind. When considering the amount of recording time, we find instead that fMRI 3T (Chang2019), MEG and EEG with trial scaling (Gifford2022) yield similar performance for the same number of hours. When considering the potential of scaling, MEG has the largest slope after 7T fMRI (Figure 7B), suggesting it may be a promising avenue to scale decoding performances. Finally, we do not currently find evidence of a saturation effect: decoding performance does not appear to plateau after a specific quantity of data."}, {"title": "3.8 Estimating cost of data acquisition", "content": "The possibility of scaling does not solely depend on performance, but also on the cost of data acquisition. To address this issue, we estimate the cost associated with each type of device, by using publicly available information (see Appendix F), and modeled the log-linear relationship between cost and decoding performance. Fitted models are shown in Figure 7B (last column). We can then estimate the cost or the gain associated with different hypothetical objectives. First, with these cost estimates, we retrospectively infer that the acquisition of the present datasets costs $1.5k for Xu2024, $6.9k for Grootswagers2022, $9.7k for Gifford2022, $19.4k"}, {"title": "4 Discussion", "content": "We aimed to characterize the factors which are critical to improving brain-to-image decoding performance. For this, we conducted a comprehensive comparison of decoding pipelines on the largest benchmark to date encompassing 84 subjects who watched 2.3M natural images over 498 h while their brain activity was recorded with EEG, MEG, 3T fMRI or 7T fMRI. To ensure meaningful comparisons, we focused on a unified preprocessing and modeling pipeline, evaluated decoding on single-trial and controlled for the amount and type of training data."}, {"title": "4.1 Contributions", "content": "This work provides three main contributions.\nDecoding performance and deep learning gain First, both linear and deep learning models highlight the importance of recording devices: as expected, when the size of the training sets are similar, 7T fMRI leads to the best results, followed by 3T fMRI, MEG and ultimately EEG. However, the decoding gain enabled by deep learning algorithms, as compared to linear models, unexpectedly appears to benefit the noisiest devices, namely EEG and MEG (Figure 2). We speculate that the noise associated with brain recordings may have specific spatial (sensors or voxels) and temporal structures that can only be separated in a latent space. If confirmed, this hypothesis would highlight the importance of developing, in the future, foundational models of brain activity, to automatically perform such separation between brain signals and recording noise (e.g. Thomas et al. (2022); Ortega Caro et al. (2023); Yang et al. (2024); Yuan et al. (2024)).\nScaling laws Second, and in spite of analyzing the largest amount of brain responses to images to date, we do not observe any plateau of decoding performance as the amount of training data increases. Rather, the log-linear scaling laws presently observed strengthens previous findings (Antonello et al., 2024; Bonnasse-Gahot and Pallier, 2024; D\u00e9fossez et al., 2022; Benchetrit et al., 2024). Together, these results suggest that the decoding of brain activity may be most simply improved by recording more data. Interestingly, not all data regimes are equivalent in that regard. In particular, our results show that the amount of within-subject recordings steadily improves decoding, whereas the amount of across-subject recordings lead to modest, if any, improvements. This result emphasizes the importance of inter-individual differences when it comes to neural representations and brain activity patterns. While additional research incorporating large databases across many more subjects (Van Essen et al., 2012; Nastase et al., 2021; Schoffelen et al., 2019) remains necessary, this result suggests that future efforts may benefit most from focusing on building datasets with a few subjects recorded over many sessions (Allen et al., 2022; Hebart et al., 2023; Armeni et al., 2022). Finally, the study of scaling laws often considers the impact of data size in relation to the size of the model. The systematic exploration of increasingly large architectures remains an open question (Kaplan et al., 2020; Hoffmann et al., 2024).\nBeyond performance: the importance of time and cost Third, the present comparisons highlight that decoding performance should not be the sole factor to consider when deciding which device to use for data collection. As illustrated in Figure 7, the temporal resolution and the costs associated with fMRI is such that, depending on the budget, and/or the targeted decoding performance, the most cost- and time-efficient route may not necessarily be in favor of MRI devices. For instance, when comparing performance for equivalent numbers of trials, 3T fMRI is better or equivalent to MEG, which itself is better than EEG. However, if we look instead"}, {}]}