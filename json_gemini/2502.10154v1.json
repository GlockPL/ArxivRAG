{"title": "Video Soundtrack Generation by Aligning Emotions and Temporal Boundaries", "authors": ["Serkan Sulun", "Paula Viana", "Matthew E. P. Davies"], "abstract": "We introduce EMSYNC, a video-based symbolic music generation model that aligns music with a video's emotional content and temporal boundaries. It follows a two-stage framework, where a pretrained video emotion classifier extracts emotional features, and a conditional music generator produces MIDI sequences guided by both emotional and temporal cues. We introduce boundary offsets, a novel temporal conditioning mechanism that enables the model to anticipate and align musical chords with scene cuts. Unlike existing models, our approach retains event-based encoding, ensuring fine-grained timing control and expressive musical nuances. We also propose a mapping scheme to bridge the video emotion classifier, which produces discrete emotion categories, with the emotion-conditioned MIDI generator, which operates on continuous-valued valence-arousal inputs. In subjective listening tests, EMSYNC outperforms state-of-the-art models across all subjective metrics, for music theory-aware participants as well as the general listeners.", "sections": [{"title": "Introduction", "content": "The online distribution of user-generated multimedia content is expanding at an exponential rate thanks to affordable, high-quality recording equipment and video editing software [Falkowski-Gilski and Uhl, 2020]. A key challenge in content creation is providing suitable soundtracks to enhance viewer engagement [Buhler, 2018]. However, unauthorized use of commercially published music infringes copyright, preventing monetization for creators on platforms like YouTube. Alternatives such as purchasing music, hiring composers, or searching for royalty-free tracks are often costly, time-consuming, or fail to ensure proper synchronization with video content. Automatic video-based music generation offers a promising solution to this problem [Di et al., 2021; Kang et al., 2024; Zhuo et al., 2023].\nExisting approaches that generate music as audio waveforms lack editability [Lam et al., 2024; Copet et al., 2024]. This method compresses all stages of music production-composition, performance, editing, mixing, and mastering into a single process, limiting creative control for professionals. By contrast, generating music in a symbolic format like MIDI (Musical Instrument Digital Interface) offers greater flexibility. MIDI functions as a digital music score, encoding instrument names, note pitches, durations, and velocities. Professionals can edit compositions, synthesize audio using digital audio workstations (DAWs), or record and refine performances.\nThis work focuses on generating music in MIDI format from arbitrary videos using deep neural networks (DNNs). We use \u201cMIDI\u201d and \u201cmusic\u201d interchangeably, as MIDI is our exclusive format. A major challenge in training video-to-MIDI DNNs is the absence of large-scale paired video-MIDI datasets. Existing datasets are either domain-specific, such as pianist hand videos [Koepke et al., 2020], or contain limited samples, around 1k [Di et al., 2021; Zhuo et al., 2023]. To address this, we develop a model that generates music for any video type, leveraging the Lakh MIDI Dataset the largest available MIDI dataset with 176,581 samples ensuring diverse and high-quality outputs [Raffel, 2016].\nSince the Lakh MIDI dataset lacks corresponding videos, we adopt a two-stage approach: extracting video features relevant to music generation and using them as conditioning inputs. We name our model EMSYNC, as it aligns music with video by matching their emotions, and synchronizing their temporal boundaries. While our method is applicable to any selection of temporal boundary and emotion representation, we define musical boundaries using long-duration chords, video boundaries using scene cuts, and represent emotions with the valence-arousal model [Russell, 1980]. Specifically, we extract scene cut locations from the input video and guide the music generator to produce long-duration chords near those locations that are rhythmically and harmonically compatible with the rest of the generated music. We also use a pretrained video emotion classifier to estimate discrete emotion probabilities [Sulun et al., 2024a], map them to valence-arousal values, and condition our music generator accordingly.\nTemporal conditioning in MIDI-generating models presents unique challenges. While deep transformer models can handle time-based data, such as videos, their sequence dimension correlates linearly with time due to a fixed frame rate [Liu et al., 2022; Arnab et al., 2021]. In contrast, MIDI is typically processed"}, {"title": "Related work", "content": "In this section, we introduce the video emotion classifier and the emotion-based MIDI generator used in our work. We then review existing video-based MIDI generation methods and discuss their evaluation approaches."}, {"title": "Video emotion classification", "content": "The VEMOCLAP model exploits publicly available pretrained models for a multimodal video analysis and uses the resulting pretrained features to classify the emotions of arbitrary videos [Sulun et al., 2024a; Sulun et al., 2024b]. It employs pretrained models for automatic speech recognition (ASR), optical character recognition (OCR), facial expression classification, audio classification, and image understanding. Textual features obtained from ASR and OCR are further processed using a text sentiment classifier. Cross-attention layers [Vaswani et al., 2017] integrate and process these multimodal features, and a linear layer produces the final emotion probabilities. The model is trained on the Ekman-6 dataset [Xu et al., 2016], consisting of 1637 videos labeled with the six basic emotions derived from the original work of Ekman: anger, disgust, fear, joy, sadness, and surprise [1971]."}, {"title": "Emotion-based MIDI generation", "content": "Sulun et al. [2022] collected valence and arousal values for songs in the Lakh Pianoroll Dataset [Dong et al., 2018] and trained a conditional transformer using the resulting labeled dataset. To integrate continuous-valued valence and arousal features with discrete musical note tokens, they project valence and arousal into a vector space using separate linear layers. Musical input tokens are projected into vectors of the same dimensionality using an embedding layer. These vectors are then concatenated with the projected valence and arousal vectors along the sequence dimension and fed into the transformer body with relative global attention [Huang et al., 2018]."}, {"title": "Video-based MIDI generation", "content": "While our method applies to arbitrary videos, several works focus on generating symbolic music for specific types of videos, such as those featuring human movements like dancing or instrumental performances. The Foley Music model [Gan et al., 2020] generates MIDI from videos of musicians by processing body keypoint movements using a Graph Convolutional Network [Kipf and Welling, 2017] and a Transformer [Vaswani et al., 2017]. Similarly, Koepke et al. [2020] and Su et al. [2020] utilize use deep neural networks with residual connections to generate symbolic music from videos of finger movements on piano keyboards. Due to their specialized nature, these approaches rely on datasets containing video-MIDI pairs, though these datasets typically contain fewer than 1k samples [Zhao et al., 2018; Li et al., 2018; Koepke et al., 2020; Su et al., 2020]. The RhythmicNet model employs a multi-stage process to generate music from dance videos by predicting beats and style, generating a drum track, and subsequently creating multitrack music [Su et al., 2021].\nSome studies explore the more general task of generating symbolic music for arbitrary videos. The most similar to our approach, the Controllable Music Transformer (CMT), generates music based on video features such as motion speed, motion saliency, and timing [Di et al., 2021]. CMT employs the Lakh Pianoroll Dataset [Dong et al., 2018], the same dataset we use, and processes music using an extended Compound Word representation, where each token encodes type, beat/bar"}, {"title": "Methodology", "content": "In this section, we present our video-based symbolic music generator. We match the output music to the input video based on emotions and temporal boundaries. Since there are no datasets where videos are paired with symbolic music, we employ a two-stage approach, i.e., use independent modules for video analysis and conditional music generation, and then combine them. Our pipeline is shown in Figure 1. We first highlight our music generator and how it is conditioned on emotions and temporal boundaries. We then describe how we incorporate temporal and emotional video features into our conditional music generator."}, {"title": "Conditional music generator", "content": "Our music generator is conditioned on emotions as valence-arousal values and temporal boundaries. Figure 2 illustrates the model, with the emotion-conditioning mechanism in the upper part and the boundary-conditioning mechanism in the lower part."}, {"title": "Training dataset and preprocessing", "content": "We train our music generator on the Lakh Pianoroll Dataset (LPD) [Dong et al., 2018], which contains 174,154 pianorolls derived from the Lakh MIDI Dataset [Raffel, 2016]. We tokenize the pianorolls using an event-based symbolic music representation [Oore et al., 2020]. Specifically, an ON (note on) token marks the start of a note, and an OFF (note off) token marks its end. These tokens also encode pitch and instrument information. For example, a piano note with a MIDI pitch of 60 (C4) is denoted as PIANO_ON_60. Since a larger number of instruments increases vocabulary size, we use the Lakh Pianoroll Dataset-5 variant, where all instrument tracks are merged into five predefined categories: bass, drums, guitar, piano, and strings [Dong et al., 2018]. However, our method is adaptable to datasets with different instrument groupings.\n tokens are used to move along the time axis, representing both note durations and the silences between them. Each token specifies a time increment in milliseconds. For example, an 800-millisecond shift is encoded as TIMESHIFT_800. We use a temporal resolution of 8 milliseconds with a maximum shift of 1000 milliseconds. Longer durations are represented using multiple consecutive  tokens. We also use the START tokens to mark the beginning of the songs, the BAR tokens to indicate the musical bars, and the PAD tokens to standardize input sequence lengths in minibatches. We use the CHORD token to mark long-duration chords as temporal boundaries, as they often serve as anchor points in musical compositions [Bharucha and Krumhansl, 1983]. However, this is a design choice, and our method can accommodate any selection of temporal boundaries."}, {"title": "Temporal boundary matching", "content": "In addition to matching emotions, we also temporally synchronize the input video and output music by aligning their temporal boundaries. We define temporal boundaries as scene cut locations in video and chord locations in music. Since we employ a hybrid approach, we first train a music generator capable of incorporating boundary locations as input and producing music with chords near these boundaries. To achieve this, we label the chords in the Lakh Pianoroll Dataset. During training, we consider only guitar and piano chords with at least three simultaneous notes that last for a minimum of two beats. These chords are labeled by inserting a CHORD token before the first ON token of each chord. To allow the model to"}, {"title": "Video scene detection", "content": "During video-based inference, we first extract the video's temporal boundaries, specifically its scene cut locations, using the FFmpeg software\u00b9. We then apply a difference filter to the extracted scene cuts and remove those occurring less than 4 seconds apart. The remaining timestamps serve as the input boundaries for the music generator."}, {"title": "Emotion matching", "content": "We use the VEMOCLAP emotion classifier to extract emotions from input videos as probability distributions over discrete emotion categories [Sulun et al., 2024a]. The music generator is conditioned on emotions represented as valence and arousal values ranging from -1 to 1, rather than discrete"}, {"title": "Experimental setup", "content": "As discussed in Section 2.3, conducting an objective evaluation is challenging when using unmatched video and MIDI datasets. We therefore focus on the subjective evaluation of our overall model. We conduct a user study to compare our results with two open-source state-of-the-art models that generate MIDI from arbitrary videos: Video2Music [Kang et al., 2024] and CMT [Di et al., 2021]."}, {"title": "Dataset", "content": "We train our music generator on the Lakh Pianoroll-5 dataset [Dong et al., 2018] with valence-arousal labels [Sulun et al., 2022]. For evaluation, we perform inference on videos from the Pittsburgh Advertisements Dataset (Ads) [Hussain et al., 2017]. We select this dataset because advertisements often rely on music to maximize viewer engagement [Zander, 2006]. Additionally, since none of the compared methods"}, {"title": "Implementation details", "content": "We use the Music Transformer model with relative global attention in our music generator [Huang et al., 2018]. The model consists of 11 layers, 8 attention heads, and a dimensionality of 512. We train it with a context length of 1216 and a batch size of 64. Training is performed using cross-entropy loss with a learning rate of 2e-4 for the first 300k steps, followed by 5e-5 for the next 300k steps. We use the Adam optimizer with gradient clipping to a norm of 1 [Kingma and Ba, 2015]. We do not apply any regularization methods and did not observe overfitting, as the model is trained on large-scale, densely labeled data where it predicts each token of its input sequence [Sulun, 2018].\nFor data augmentation, we transpose the pitches of all instruments, except drums, by a randomly chosen integer between -3 and 3, inclusive. We set our model size to be comparable to the baseline methods, with our model containing 37M parameters, compared to 39M in CMT [Di et al., 2021] and 33M in Video2Music [Kang et al., 2024]. We set the maximum absolute value of means across all emotion categories to 0.8. For all models, we synthesize MIDI files into audio waveforms using Fluidsynth\u00b2 and apply peak audio normalization up to -3 dB."}, {"title": "Subjective evaluation", "content": "Using the 24 test videos, we generate accompanying music with the three models and create 12 survey pages, each containing two videos. For each video, the three music versions, generated by the compared models, are presented side by side with anonymized model names, with each model's output appearing equally in the left, center, and right positions. We en-"}, {"title": "Results and conclusion", "content": "Table 2 compares EMSYNC, Video2Music [Kang et al., 2024], and CMT [Di et al., 2021] across all subjective criteria based on average rankings. A lower score represents a better ranking, with 1 being the best and 3 the worst possible.\nEMSYNC consistently outperforms both baselines across all metrics for both Group 1 (music theory-aware) and Group 2. Group 2 rated EMSYNC even higher, particularly in music richness and emotion match, indicating that its strengths are even more apparent to general listeners. EMSYNC is also the fastest, with a runtime of 1.417 minutes, compared to 1.607 minutes for Video2Music and 3.554 minutes for CMT.\nThese results confirm that EMSYNC produces the most diverse and high-quality music with the best emotional alignment, rhythmic synchronization, and overall video compatibility while also being the most computationally efficient model.\nBy automatically generating soundtracks for user-provided videos, our work has the potential to streamline video production and enhance viewer engagement while offering a valuable framework for both machine learning researchers and multimedia content creators."}]}