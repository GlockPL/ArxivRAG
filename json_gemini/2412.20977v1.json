{"title": "UNREALZOO: ENRICHING PHOTO-REALISTIC VIRTUAL WORLDS FOR EMBODIED AI", "authors": ["Fangwei Zhong", "Kui Wu", "Churan Wang", "Hao Chen", "Hai Ci", "Zhoujun Li", "Yizhou Wang"], "abstract": "We introduce UnrealZoo \u00b9, a rich collection of photo-realistic 3D virtual worlds built on Unreal Engine, designed to reflect the complexity and variability of the open worlds. Additionally, we offer a variety of playable entities for embodied AI agents. Based on UnrealCV, we provide a suite of easy-to-use Python APIs and tools for various potential applications, such as data collection, environment augmentation, distributed training, and benchmarking. We optimize the rendering and communication efficiency of UnrealCV to support advanced applications, such as multi-agent interaction. Our experiments benchmark agents in various complex scenes, focusing on visual navigation and tracking, which are fundamental capabilities for embodied visual intelligence. The results yield valuable insights into the advantages of diverse training environments for reinforcement learning (RL) agents and the challenges faced by current embodied vision agents, including those based on RL and large vision-language models (VLMs), in open worlds. These challenges involve latency in closed-loop control in dynamic scenes and reasoning about 3D spatial structures in unstructured terrain.", "sections": [{"title": "INTRODUCTION", "content": "Currently, embodied artificial intelligence (Embodied AI) agents are often homebodies, primarily confined to controlled indoor environments and rarely venturing outside to explore the diversity of the open world. While several simulators (Kolve et al., 2017; Dosovitskiy et al., 2017; Puig et al., 2018; Li et al., 2023; Puig et al., 2024) have advanced the field, they often focus on specific scenarios, such as daily activities in homes or autonomous driving on urban roads. This narrow focus hinders AI agents' adaptability and generalization in diverse open worlds, e.g., industrial areas, public spaces, and natural landscapes, required by a wide range of real-world applications.\nTo bridge this gap, there is an increasing demand for simulators that feature a diverse range of open worlds. First, the diversity of the 3D scenes and bodies of agents is crucial to develop spatial intelligence (Davison, 2018), enabling agents to actively perceive, reason, plan, and act when handling numerous tasks in complex 3D worlds with varying dynamics and styles. Second, the complexity of multi-agent interactions is essential for developing social intelligence (Du\u00e9\u00f1ez-Guzm\u00e1n et al., 2023), such as the theory of mind (Jin et al., 2024), negotiation (Guan et al., 2024), cooperation (Wang et al., 2022), and competition (Zhong et al., 2021), encouraging agents to behave more like humans. Third, virtual worlds that mimic the challenges in open-world scenarios can evaluate agents efficiently and effectively, identifying limitations and preventing hardware losses from real-world deployment failures (Kadian et al., 2020). Ultimately, these features will inspire researchers to explore new challenges previously overlooked in other simulators (Duan et al., 2022), facilitating seamless integration into real-world applications.\nIn this work, we introduce UnrealZoo, a comprehensive collection of photo-realistic virtual worlds, based on Unreal Engine 2 and UnrealCV (Qiu et al., 2017). UnrealZoo features a diverse range of complex open worlds and playable entities to advance research in embodied AI and related domains. This high-quality set includes 100 realistic scenes at varying scales, such as houses, supermarkets, train stations, industrial factories, urban cities, villages, temples, and natural landscapes. Each environment is expertly designed by artists to mimic realistic lighting, textures, and dynamics, closely resembling real-world experiences. Our collection also includes diverse playable entities, including humans, animals, robots, drones, motorbikes, and cars. This diversity enables researchers to investigate the generalization of agents across different embodiments or build complex 3D social worlds with numerous heterogeneous agents. To enhance usability, we further optimize UnrealCV and offer a suite of easy-to-use Python APIs and tools (UnrealCV+), including environment augmentation, demonstration collection, and distributed training/testing. These tools allow for customization and extension of the environments to meet various needs in future applications, ensuring UnrealZoo remains adaptable as the embodied AI agents evolve.\nWe conduct experiments to demonstrate the applicability of UnrealZoo for embodied AI. First, we benchmark frames per second (FPS) across various commands, highlighting the significant improvement in image rendering and multi-agent interactions with the UnrealCV+ API. We use embodied visual navigation (Zhu et al., 2017) and tracking (Luo et al., 2018; Zhong et al., 2021) as two example tasks to benchmark embodied vision agents in complex dynamic environments with moving objects and unstructured maps. We also introduce a set of simple yet effective baseline methods for developing embodied vision agents, including distributed online reinforcement learning algorithms (Mnih et al., 2016), offline reinforcement learning algorithms (Kumar et al., 2020), and a reasoning framework for large vision-language models (VLMs). Our evaluations across different settings emphasize the importance of diverse training environments for enhancing agent generalization and robustness, the necessity of low latency in closed-loop control to handle dynamic factors, and the potential of reinforcement learning for training agents to navigate complex scenes.\nOur contributions can be summarized in the following: 1) We build UnrealZoo, a collection of 100 high-quality photo-realistic scenes and a set of playable entities with diverse features, covering the most challenging to embodied AI agents in open worlds. 2) We optimize the communication efficiency of UnrealCV APIs and provide easy-to-use Gym interfaces with a toolkit for diverse requirements. 3) We conduct experiments to demonstrate the usability of UnrealZoo, showing the importance of the diversity of the environments to the embodied agents, and analyzing the limitations of the current RL-based and VLM-based agents in the open worlds."}, {"title": "RELATED WORKS", "content": "Realistic Simulators for Embodied AI. Realistic simulators are extensively utilized in embodied artificial intelligence due to their appealing benefits, including high-quality rendering, cost-effective ground truth generation, low-cost interaction, and environmental controllability. They are crucial for training and testing AI agents to handle increasingly complex tasks. Notable realistic 3D simulators have been created for specific applications, such as indoor navigation (Kolve et al., 2017; Puig et al., 2018; Xia et al., 2018; Wu et al., 2018), robot manipulation (Yu et al., 2020; Ehsani et al., 2021; Chen et al., 2024), and autonomous driving (Gaidon et al., 2016; Shah et al., 2018; Dosovitskiy et al., 2017). Recent advances in computer graphics have spurred interest in developing general-purpose virtual worlds with photo-realistic rendering, allowing agents to collect high-fidelity data and learn skills applicable across various tasks and scenes. ThreeDWorlds (TDW) (Gan et al., 2021) and LEGENT (Cheng et al., 2024) are notable simulators that offer photo-realistic, multi-modal platforms, based on Unity, for interactive physical simulation. However, their built-in scenes and playable entities are somewhat limited. Additionally, the performance of the simulator decreases significantly in large outdoor environments, a typical weakness of Unity. V-IRL (Yang et al., 2024) is a recent approach that leverages Google Maps' API to simulate agents with real-world street view images, significantly reducing the gap between virtual and real-world settings. However, since V-IRL is inherently composed of static images, it lacks the capability to simulate the dynamics of the physical worlds for agent-object interactions. Recently, the community has also begun to explore dynamic environments with social interactions and unexpected events. However, existing solutions like Habitat 3.0 (Puig et al., 2024) focus on a limited number of agent interactions in indoor scenes, while HAZARD (Zhou et al., 2024b) addresses only single-agent simulations in dynamic scenarios like fires, floods, and winds. In contrast, UnrealZoo offers a comprehensive collection of scenes that feature various scenes at different scales, situations, eras, and cultural backgrounds with diverse playable entities for embodied AI. With advancements in Unreal Engine and optimized UnrealCV, our environment achieves real-time performance in large-scale scenes with multiple agents (around 10) and photo-realistic rendering. A comprehensive comparison across the related photo-realistic simulators is shown in Table 1.\nEmbodied Vision Agents. Embodied vision agents, which perceive and interact with their environments through vision, are a key focus in artificial intelligence research. These agents perform tasks like navigation (Zhu et al., 2017; Gupta et al., 2017; Yokoyama et al., 2024; Long et al., 2024), active object tracking (Luo et al., 2018; Zhong et al., 2019; 2021; 2023; 2024), and other interactive tasks (Chaplot et al., 2020; Weihs et al., 2021; Ci et al., 2023; Wang et al., 2023), mimicking human behavior. Their development involves various methods, including state representation learning (Yadav et al., 2023; Yuan et al., 2022; Gadre et al., 2022; Yang et al., 2023), reinforcement learning (RL) (Schulman et al., 2017; Xu et al., 2024; Ma et al., 2024), and large vision-language models (VLMs) (Zhang et al., 2024; Zhou et al., 2024a). Despite significant progress, challenges remain. RL methods often require extensive trial-and-error interactions and computational resources for training, and they usually struggle to generalize to new environments. Conversely, VLM-based methods excel at interpreting language instructions and images but may lack the fine-grained control and adaptability necessary for real-time interactions. The computational demands and time needed for inference with"}, {"title": "UNREALZOO", "content": "UnrealZoo is a collection of photo-realistic, interactive open-world environments with diverse embodied characters, built on Unreal Engine and UnrealCV (Qiu et al., 2017). The environments are sourced from the Unreal Engine Marketplace \u00b3, which shares high-quality content from artists, and were accumulated over two years at a cost exceeding 10,000. UnrealZoo features a diverse array of scenes with varying sizes and styles. Among them, the largest scene, i.e., Medieval Nature Environment, covers more than 16km\u00b2 areas. The environments also include a wide range of embodiment, such as human avatars, vehicles, drones, animals, and virtual cameras, all of which can interact with the environment and equipped with ego-centric sensing systems. We offer easy-to-use Python APIs based on UnrealCV to facilitate interaction between Python programs and the game engine. Note that UnrealCV is optimized for rendering and communication, particularly in large-scale and multi-agent scenarios, namely UnrealCV+. Additionally, we provide OpenAI Gym interfaces to standardize agent-environment interactions. The gym-like interface also contains a set of toolkits, e.g., environment augmentation, population control, time dilation, and JSON-style task configurations to help the user customize the environments for various tasks with minimal effort."}, {"title": "SCENE COLLECTION", "content": "UnrealCV Zoo contains 100 scenes based on Unreal Engine 4 and 5. We select the scene based on the public reviews in the marketplace and the difference between the collected scenes, aiming at covering a wide range of styles from ancient to fictional, ensuring diversity. We provide an overview of the environments in the scene gallery.\nWe have tagged the collected scenes with several feature labels allowing researchers to select appropriate scenes for testing or training based on the tags associated with each scene. Our tags cover the following aspects:"}, {"title": "PLAYABLE ENTITIES", "content": "UnrealZoo includes seven types of playable entities: humans, animals, cars, motorbikes, drones, mobile robots, and flying cameras (See Figure 1). Specifically, it comprises 19 human entities, 27 animal entities (dog, horse, elephant, pig, bird, turtle, etc), 3 cars, 14 quadruped robots, 3 motorcycles, and 1 quad-copter drone. This diversity, with varying affordances like action space and viewpoint, allows us to explore new challenges in embodied AI, such as cross-embodiment generalization and heterogeneous multi-agent interactions.\nEach entity includes a skeleton with appropriate meshes and textures, a local motion system, and a navigation system. We offer a set of callable functions for each entity, enabling users to modify attributes like size, appearance, and camera positions, as well as control movements. Each entity can switch between different textures and appearances via UnrealCV API, enhancing visual diversity and adaptability for various scenarios. Each entity is equipped with an ego-centric camera, allowing the users to capture various types of image data such as RGB, depth, surface normal, and instance-level segmentation (object mask) from the agent's ego-centric view. Figure 3 shows examples of the"}, {"title": "PROGRAMMING INTERFACE", "content": "We provide UnrealCV+ as the basic application programming interface (API) on Python to capture data and control the entities and scenes, and provide an OpenAI Gym interface for general agent-environment interactions. The architectures of the programming interfaces are shown in Figure 2.\nUnrealCV+ is our improved version of the UnrealCV (Qiu et al., 2017) for high-throughput inter-actions. As the original version of UrnealCV primarily focuses on generating synthesis data for computer vision, the frame rates per second (FPS) are not optimized for real-time interactions. We optimize the rendering pipelines in the UnrealCV server and the communication protocols between the server and the client to improve the FPS. Specifically, we enable parallel processing while render-ing object masks and depth images, which can significantly improve the FPS in large-scale scenes. For multi-agent interactions, we further introduce the batch commands protocol. In this protocol, the client can simultaneously send a batch of commands to the server, processing all the received commands and returning a batch of results. In this way, we can reduce the time spent on server-client communication. Since reinforcement learning requires extensive trial-and-error interactions for train-ing, often running multiple environments on a computer, we introduce Inter-process communication (IPC) sockets instead of TCP sockets to improve the stability of the server-client communication under high loads. We benchmark the FPS performance in Table 2. To enhance user-friendliness, we have developed high-level Python APIs that are built upon the command systems of UnrealCV. These APIs encapsulate all the request commands and their corresponding data decoders into a callable Python function. This approach significantly simplifies the process for beginners, allowing them to interact with and customize the environment using UnrealCV+.\nGym Interface is used to define the interactive tasks and standardize the agent-environment interac-tion, following Gym-UnrealCV. Even though there are a lot of tasks for agents, they usually share common interaction protocols, i.e., the agent gets observations from the environment and returns actions. The main difference across different tasks usually is the reward functions, the modality of the"}, {"title": "EXPERIMENTS", "content": "In this section, we use a subset of UnrealZoo to demonstrate the usability of the collected environments. For visual navigation, we select two scenes with complex spatial structures to train and validate the RL-based and VLM-based agents. For active tracking, we select at most 8 scenes as training environments and validate the generalization of the learned policy in another 24 scenes, which are divided into four categories according to the scene types. The results demonstrate the importance of the diversity of the training environments to the cross-domain generalization. For social tracking, we analyze the robustness of the agent in social environments with different control frequencies, using the toolkit provided in the gym to generate crowds with varying populations and control frequencies."}, {"title": "VISUAL NAVIGATION", "content": "visual navigation in the wild introduces a new level of complexity compared to traditional navigation tasks for indoor scenes or autonomous driving, which often run on complex 3D spatial structures. Differently, we place the agent in open-world environments where it must take a set of locomotion, e.g., running, climbing, jumping, crouching, to go over the various obstacles in unstructured terrains to reach the target object. In this setting, the agent requires advanced spatial reasoning and actions to make real-time decisions about its path. The emphasis on such complex environments ensures the agent can operate effectively in a broad range of challenging scenarios, moving beyond the constraints of traditional navigation frameworks. The details of the task setting are introduced in Appendix B.1.\nEvaluation Metrics. We employ two key metrics to evaluate visual navigation agents: 1) Average Episode Length (EL), representing the average number of steps per episode over 50 episodes. 2) Success Rate (SR), measuring the percentage of episodes the agent successfully navigates to the target object out of 50 total episodes, which represents the navigation capability in the wild environment.\nBaselines for Navigation. We build simple baselines to demonstrate the applicability of our envi-ronments for training reinforcement learning agents and benchmark the agents based on pre-trained large models. 1) Online RL: We trained the RL-based navigation agents separately in the Roof and Factory environments using a distributed online reinforcement learning (RL) approach, e.g. A3C (Mnih et al., 2016). The training curve is shown in Figure 16. The model takes the first-person view segmentation mask and the relative position between the agent and target as input, and outputs direct control signals (from the predefined action space) to navigate. This setup allows the agent to learn and optimize navigation strategies during continuous interaction with the environment. Please refer to Appendix C.1 for the implementation details. 2) GPT-40: We employ the GPT-40 model to take action, leveraging its powerful multi-modal reasoning capabilities. The model takes first-person view images and the relative position between the agent and the fixed target as input. The GPT-40 model follows our prompt template (See Table 14) as guidance, reasoning appropriate actions from the predefined control space to guide the agent toward the target. 3) Human: We also have a human"}, {"title": "ACTIVE VISUAL TRACKING", "content": "We evaluate the generalization of the tracking agents across four environment categories: Interior Scenes, Palaces, Wilds, and Modern Scenes. Each category contains 4 individual environments, as shown in Figure 9. We aim to capture a broad range of features in our environment collection by selecting four distinct and representative scenes from each category, ensuring a comprehensive eval-uation of the agents' capabilities. The details of the tasks are introduced in Appendix B.2. We analyzed the effectiveness of the diversity of the training data by collecting demonstrations with different numbers of training environments.\nEvaluation Metrics. Our evaluation employs three key metrics: (1) Average Episodic Return (ER), which calculates the mean episodic return over 50 episodes, providing insights into overall tracking performance; (2) Average Episode Length (EL), representing the average number of steps per episode, which reflects long-term tracking effectiveness; and (3) Success Rate (SR), measuring the percentage of episodes that complete 500 steps out of 50 total episodes.\nBaselines for Active Visual Tracking. For the RL-based agents, we extend from the official implementation settings from the recent offline RL method (Zhong et al., 2024), collecting offline datasets and employing the original network architecture. To demonstrate the impact of data diversity on tracking performance, we collect three sets of offline datasets, each containing 100k steps. The key difference between these datasets is the number of environments used for data collection: one was collected in a single environment (denoted as 1 Env.), another in two environments(denoted as 2 Envs.), and the third in eight distinct environments (denoted as 8 Envs.). The offline training curve of each setting is shown in Figure 15. The environment distribution of each dataset setting is shown in Figure 11. It is worth noting that FlexibleRoom, one of the environments used for data collection, is a unique abstract environment, with all objects represented as geometric shapes covered by randomized patterns. This distinctive setup contrasts with the more realistic and diverse environments in the collection, offering a unique scenario for testing agent adaptability. For the VLM-based agents, we"}, {"title": "SOCIAL TRACKING", "content": "We further evaluate the tracking agents in a social scenario, where the agent needs to follow the target in crowds. Such a setting contains varying high-dynamics objects with similar ap-pearances, e.g., pedestrians. We can directly ap-ply the population control wrapper in the Gym toolkit to extend the environment used for active tracking to this setting.\nRobustness to Active Distractions. A key chal-lenge in active visual tracking tasks is managing active distractions, a critical issue for real-world deployment in crowds. Thus, we conducted an experiment in the DowntownWest and generated crowds with varying numbers of human charac-ters as distractors notated as 4D, 8D, and 10D. The number indicates the number of distractors. We compared the performance of the offline RL method, trained under three dataset configura-tions (1 Env., 2 Envs., 8 Envs.), against the VLM-based method, evaluating the agents' ability to maintain robust tracking under these different levels of active distractions."}, {"title": "LIMITATION ANALYSIS AND SUMMARY", "content": "The current RL method shows some capacity to learn spatial-temporal information and dynamically respond to target move-ment in most scenarios, but it struggles with executing advanced actions like bypassing obstacles. In compact Interior cate-gories and some special environments such as TerrainDemo, IndustrialArea, and ModularSciFiSeason1, which feature irreg-ular landscapes, narrow passageways, and maze-like structures, the agent often collides with casually placed low-level objects. While the agent can track targets, it's insufficient to handle unpredictable hindrances, especially in key moments like by-passing corners or tight spaces, which increases the likelihood of failure. This highlights a significant limitation: although the agent can learn and react to its environment, it lacks the higher-level reasoning to anticipate and avoid obstacles effec-tively. Advanced behaviors like bypassing obstacles are crucial for improving performance, especially in cluttered environments where basic reactive controls are insufficient. Incorporating such reasoning mechanisms would help reduce failure rates, particularly in critical scenarios, and improve overall tracking performance.\nFor the VLM-based method, one key factor contributing to GPT-40's notably poor performance, especially in comparison to the RL methods, is its susceptibility to time delays. From our experience, this issue becomes particularly evident when the target makes abrupt movements, such as turning around. Due to the API's response lag, the GPT-40 agent struggles to track the target in real time, often losing it before receiving updated instructions. This limitation highlights the difficulty of real-time processing in embodied tracking tasks using models that rely on slower external API communications, underscoring the need for more efficient integration methods for such systems."}, {"title": "CONCLUSIONS", "content": "In conclusion, we present UnrealZoo, a versatile platform designed to advance embodied AI research. The diversity and complexity of the collected realistic environments challenge agents with varying embodied interaction tasks, such as visual navigation, active tracking across various environments, and social tracking in crowds. The enhanced UnrealCV+ API supports efficient data collection and task customization, enabling seamless interaction for both single and multi-agent systems. These features will open up potential applications like developing spatial intelligence in the 3D world and social intelligence in human-AI society, making our platform a valuable tool for pushing the boundaries of embodied AI in real-world scenarios. Looking forward, we will continue to enrich the virtual worlds with diverse scenes, bodies, and interaction tasks, advancing agents from the virtual realm to reality for a harmonious human-AI society.\nLimitations. While our proposed environment provides diverse and complex scenarios for visual navigation, tracking, and other visual-based tasks, it currently lacks high-fidelity physical simulation, limiting the agent's ability to manipulate objects. Additionally, transferring learned behaviors to different embodied agents poses a challenge, as adapting models to various physical structures and control schemes is not yet seamless. These issues highlight areas for further research to enhance interaction dynamics and improve generalization across diverse agent embodiments."}, {"title": "UE ENVIRONMENTS", "content": "To better explain Table 1, we list the description of each symbol about the scene types and playable entities in Table 6. Since photorealism mainly relies on the engine used, we visualize the snapshots rendered by different engines in Figure 6. Note that Google Maps are images captured in the real world, but can not simulate the dynamic of the scenes and interactions between objects. By utilizing advanced rendering and physics engines, Unreal Engine simulates large-scale photorealistic environments that are not only visually appealing but also capable of complex interactions between agents and objects. So we choose to build environments on Unreal Engine."}, {"title": "ENVIRONMENTS USED IN VISUAL NAVIGATION", "content": "We carefully selected two photo-realistic environments (Roof and Factory) for training and evaluating navigation in the wild, shown in Figure 8. The Roof environment features multiple levels connected by staircases and large pipelines scattered on the ground, providing an ideal setting for the agent to learn complex action combinations for transitioning between levels, such as jumping, climbing, and navigating around obstacles. The Factory environment, on the other hand, is characterized by compact boxes and narrow pathways, challenging the agent to determine the appropriate moments to jump over obstacles or crouch to navigate under them. These two environments offer diverse spatial structures, enabling agents to develop an understanding of multi-level transitions and precise obstacle avoidance."}, {"title": "ENVIRONMENTS USED IN ACTIVE VISUAL TRACKING", "content": "For training agents via offline reinforcement learning, we selected 8 distinct environments to collect demonstrations, as is shown in Figure 11. To comprehensively evaluate the generalization of the active visual tracking agents, we selected 16 distinct environments, categorized into Interior Scenes, Palaces, Wilds, and Modern Scenes. Each category presents unique challenges: 1) Interior Scenes feature complex indoor structures with frequent obstacles; 2) Palaces include multi-level structures and narrow pathways; 3) Wilds encompass irregular terrain and varying illumination; 4) Modern"}, {"title": "NAVIGATION MESH", "content": "Based on NavMesh, we build an internal navigation system, allowing agents to autonomously navigate with the built-in AI controller in the Unreal Engine. This includes path-finding and obstacle-avoidance capabilities, ensuring smooth and realistic movement throughout diverse terrains and structures. Moreover, in our City style map, we manually construct road segmentation, we manually segment the roads to distinguish between pedestrian and vehicle pathways. When agents use the navigation system for autonomous control, they will navigate the shortest path based on the priority of the different areas. Figure 10 shows an example of the rendered semantic segmentation for NavMesh in an urban city."}, {"title": "EXEMPLAR TASKS", "content": null}, {"title": "VISUAL NAVIGATION", "content": "In this task, the agent is initialized at a random location in the environment at the beginning of each episode, while the target object's location and category remain fixed throughout. The agent must rely on its first-person view observations and the relative spatial position of the target as input. The ultimate objective is to locate the target object within 2000 steps. Success is defined by the agent reducing the relative distance to less than 3 meters and aligning its orientation such that the relative rotation between the target and the agent is smaller than 30 degrees (in the front of the agent). This setup challenges the agent to optimize its movements and decision-making while adapting to the randomized starting conditions and dynamic environment. All methods in the task share the same discrete action space to control the movement, consisting of moving forward (+1 meter/s), moving backward (-1 meter/s), turning left (-15 degrees/s), turning right (+15 degrees/s), jumping (two continuous jumping actions trigger the climbing action), crouching, and holding position. This action space enables the agent to navigate and interact with complex 3D environments, making strategic decisions in real-time to reach the target object efficiently. The step reward for the agent is defined as:\n$$r(t) = tanh(\\frac{dis2target(t \u2013 1) \u2013 dis2target(t)}{max(dis2target(t \u2212 1), 300)} - \\frac{|Ori|}{90\u00b0})$$\nwhere $dis2target(t)$ is the Euclidean distance between the agent and the target at a given timestep $t$ and $Ori$ is the absolute orientation error (in degrees) between the agent's current heading and the direction toward the target, normalized by $90\u00b0$"}, {"title": "ACTIVE VISUAL TRACKING", "content": "Referring to previous works (Zhong et al., 2024), we use human characters as an agent player and a continuous action space for agents. The action space contains two variables: the angular velocity and the linear velocity. Angular velocity varies between -30\u00b0/s and 30\u00b0/s, while linear velocity ranges from-1 m/s to 1 m/s. In the agent-centric coordinate system, the reward function is defined as:\n$$r=1-\\frac{|\u03c1 - \u03c1^*|}{\u03c1_{max}} - \\frac{|\u03b8 - \u03b8^*|}{\u03b8_{max}}$$"}, {"title": "IMPLEMENTATION DETAILS OF AGENTS", "content": null}, {"title": "RL-BASED AGENTS", "content": "Learning to navigate with online reinforcement learning. For navigation, we construct an RL-based end-to-end model, using A3C (Mnih et al., 2016) to accelerate online reinforcement learning in a distributed manner. The model's structure is as follows: a mask encoder extracts spatial visual features from the segmentation mask, which are then passed to a temporal encoder to capture latent temporal information. Finally, the spatiotemporal features, concatenated with the target's relative spatial position, are fed into the actor-critic network to optimize the actor layer for action prediction. The detailed network structure and parameters used in the experiment are listed in Table 7 and 8. Here, we provide the training curves in Roof and Factory environments, depicted in Figure 16. In the Factory, we set the number of workers to 4, while in the Roof, the number of workers is set to 6. It can be observed that, for Online RL, the number of workers and the complexity of environments have a significant impact on training efficiency. Looking forward, we anticipate that offline-based algorithms can effectively address the challenges of training efficiency and generalization.\nLearning to track with offline reinforcement learning. For the tracking task, we adopt an offline reinforcement learning (Offline RL) approach to enhance training efficiency and improve the agent's generalization to unknown environments. Specifically, we build an end-to-end model trained using offline data and the conservative Q-learning (CQL) strategy (Kumar et al., 2020). We adopt the same model structure from the latest visual tracking agent (Zhong et al., 2024), consisting of a Mask Encoder, a Temporal Encoder, and an Actor-Critic network. Detailed model structures and training parameters are summarized in Table 9 and 10. Additionally, we provide the model's loss curves under different dataset setups, as shown in Figure 15. The model achieves near-convergence within two hours across all dataset setups. To ensure the loss curves stabilize fully, we continued training for an additional three hours, during which no significant further decrease in the loss was observed. A comprehensive evaluation of the model's performance is presented in Tables 11 and 12, highlighting its strong generalization to unseen environments and robustness to dynamic disturbances. The training efficiency, generalization capability, and robustness achieved by offline RL further reinforce our belief that offline RL methods will become a mainstream approach for rapid prototyping and iteration in embodied intelligence systems."}, {"title": "VLM-BASED AGENTS", "content": "We built agents with a reasoning framework based on the Large Vision-Language Model. We employ OpenAI GPT-40 as the base model. System prompt used in the navigation task, as shown in Figure 14 and system prompt used in the tracking task, as shown in Figure 13."}, {"title": "HUMAN BENCHMARK FOR NAVIGATION", "content": "In the navigation task, we incorporated human evaluation as a baseline for comparison to demonstrate the existing gap between the current method and optimal navigation performance. Specifically, five male and five female evaluators participated in the assessment, performing the same navigation tasks under comparable conditions.\nBefore each human evaluator began their assessment, we provided a free-roaming perspective to familiarize them with the map structure and clearly conveyed the target's location and image. This ensured that human evaluators had a comprehensive understanding of the environment and the target's position. During the evaluation, the player was randomly initialized in the environment, and human evaluators used the keyboard to control the agent's movements. Each human evaluator repeated the experiment five times, providing multiple data points to ensure reliability and reduce variability in performance measurements. The termination conditions for the evaluation were identical to those applied to the RL-based agent, ensuring consistency in the comparison."}, {"title": "ADDITIONAL RESULTS", "content": null}, {"title": "LEARNING CURVE", "content": "We provide the CQL loss curve under the 1 Env., 4 Envs. and 8 Envs. training setup. As shown in Figure 15, the offline model approaches convergence after two hours and we continued training for another three hours after nearing convergence, observing no significant further decrease in the loss. Note that the offline training was conducted on a Nvidia RTX 4090 GPU."}, {"title": "EVALUATE TRACKING AGENTS ACROSS 16 UNSEEN ENVIRONMENTS", "content": "We provide the detailed quantitative evaluation results (episodic returns, episode length, success rate) of the RL-based embodied tracking agents across 16 environments, listed in Table 11. In each environment, we report the average results over 50 episodes. The results show that in the Palace Maze, which contains abundant structural obstacles, the agent's tracking performance was generally weaker compared to the other three categories. In contrast, the agent performed generally better in Lifelike Urbanity, characterized by its relatively regular and flat terrain. Additionally, we observed that as the diversity of the training environments increased, the agent's tracking performance improved across all four environment categories. This highlights the positive impact of diverse training data on enhancing the agent's overall tracking effectiveness. We also provide vivid demo videos in https://unrealzoo.notion.site/task-evt."}, {"title": "EVALUATE TRACKING AGENTS ACROSS UNSEEN SOCIAL ENVIRONMENTS", "content": "We select 4 environments from different categories as the testing environments, including Storage-House, DesertRuins, TerrainDemo, and SurburNeighborhoodDay. We test the distraction robustness of the social tracking agents by adding different numbers of distractors (4, 8, 10) in the environment. The distractors randomly walk around the environment, which may produce various unexpected"}]}