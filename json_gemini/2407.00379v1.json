{"title": "GraphArena: Benchmarking Large Language Models on Graph Computational Problems", "authors": ["Jianheng Tang", "Qifan Zhang", "Yuhan Li", "Jia Li"], "abstract": "The \"arms race\" of Large Language Models (LLMs) demands novel, challenging, and diverse benchmarks to faithfully examine their progresses. We introduce GraphArena, a benchmarking tool designed to evaluate LLMs on graph computational problems using million-scale real-world graphs from diverse scenarios such as knowledge graphs, social networks, and molecular structures. GraphArena offers a suite of 10 computational tasks, encompassing four polynomial-time (e.g., Shortest Distance) and six NP-complete challenges (e.g., Travelling Salesman Problem). It features a rigorous evaluation framework that classifies LLM outputs as correct, suboptimal (feasible but not optimal), or hallucinatory (properly formatted but infeasible). Evaluation of 10 leading LLMs, including GPT-40 and LLaMA3-70B-Instruct, reveals that even top-performing models struggle with larger, more complex graph problems and exhibit hallucination issues. Despite the application of strategies such as chain-of-thought prompting, these issues remain unresolved. GraphArena contributes a valuable supplement to the existing LLM benchmarks and is open-sourced at https://github.com/squareRoot3/GraphArena.", "sections": [{"title": "1 Introduction", "content": "As Large Language Models (LLMs) continue to evolve, accurately assessing their expanding capabilities becomes crucial but increasingly complex. Standardized tests, which are originally designed for humans, have become common LLM benchmarks in domains such as mathematics [30, 22, 50, 16], medicine [48, 15, 40], and multi-task collections [29, 65, 74, 89]. Despite their utility, concerns about data leakage in pretraining corpora [4, 82] cast doubt on whether LLMs are genuinely reasoning or merely memorizing. Crowd-sourced evaluations [88, 20], while reflective of real-world performance, are expensive and time-consuming [13]. Alternatively, evaluations using synthetic data like theorem proving [73, 80, 32], are rigorous but often lack real-world relevance.\nTo balance factors like cost, novelty, complexity, diversity, and practical applicability in LLM evaluation, the community is expanding its attention to modalities that go beyond text to include vision [50, 27], audio [84], tabular [66], and spatiotemporal [49] data. Among these, graphs stand out as a particularly promising avenue. Ubiquitous in many modern applications, graphs provide a robust framework for testing LLMs' capabilities on interpret relational information, process non-sequential data, and generalize across diverse structures[44, 36, 59, 75, 18, 19]. Specifically, graph computational problems\u2014which typically require systematic traversal or search algorithms to solve\u2014serve as an excellent testbed for LLMs [25, 75]. Consider the task of finding the shortest path: a model must understand the entire graph structure, identify relevant nodes, logically deduce multiple steps to trace paths between nodes, and perform calculations to arrive at a correct conclusion.\nOver the past year, several studies have assessed LLM performance on graph computational problems, including notable efforts like NLGraph [75], GraphQA [25], VisionGraph [45], GITA [79], and GraphInstruct [17]. While these studies have provided valuable insights, we identify three significant shortcomings in the existing benchmarks. First, they rely on synthetic graphs generated from models such as Erd\u0151s-R\u00e9nyi [24] and Barab\u00e1si-Albert [5], which may not accurately reflect real-world diversity. Second, these benchmarks focus predominantly on elementary graph tasks involving small-scale graphs, limiting tasks to simple queries and counts of basic elements like nodes, edges, triangles, and rings. More complex and challenging graph problems are largely overlooked. Third, they only require LLMs to provide straightforward answers\u2014such as yes, no, or a numerical value\u2014rather than detailed paths or reasoning processes. This format may be utilized by models through guesswork rather than through real comprehension of the underlying logic.\nIn this paper, we introduce GraphArena, a benchmarking tool designed to assess the reasoning capabilities of LLMs on addressing graph computational problems. Recognizing the limitations of existing benchmarks, GraphArena incorporates three core principles:\n\u2022 Realistic Graph Collection. GraphArena utilizes a rich assortment of million-scale real-world graphs, covering knowledge graphs, social networks, molecular structures, etc. We aim to maintain the integrity of the original graph topology and attributes throughout the sampling process.\n\u2022 Curated Task Selection. Our benchmark offers a diverse and challenging set of tasks that examine a broad spectrum of LLM reasoning skills. Illustrated in Figure 1, GraphArena includes 4 polynomial-time and 6 NP-complete challenges, which test both intuitive (System 1) and deliberative (System 2) cognitive processes [37]. These tasks require skills from simple computations to complex multi-step analytics, long-range location identification, and heuristic approximations.\n\u2022 Rigorous Evaluation Framework. GraphArena introduces a task-specific, fine-grained evaluation protocol to increase the rigor of assessments and reduce reliance on superficial pattern recognition. LLMs are required to identify the specific component or path in the graph that leads to the solution. Responses are then classified as correct, suboptimal (feasible but not optimal), hallucinatory (properly formatted but infeasible), or missing, enabling a nuanced comparison among LLMs.\nOur comprehensive assessment of ten popular LLMs across 10,000 problems in GraphArena reveals substantial insights. Consistent with common perception, GPT-40 [2] and Llama3-70b-Instruct [3] stand out as the best closed-source and open-source models, respectively. However, even these leading models face challenges with NP-complete tasks and exhibit a propensity for hallucination, in which they frequently generate structurally correct but contextually infeasible responses. The hallucination issue is more severe on larger graphs and models with fewer parameters. While chain-of-thought prompting has shown some improvements, it remains insufficient to fully resolve the hallucination issue. Additionally, graph-aware LLMs fine-tuned on relevant corpora also fail to resolve this problem. These findings indicate that graph computational problems is a valuable testbed for evaluating and advancing LLMs towards the artificial general intelligence with strong reasoning capacities."}, {"title": "2 Benchmark Construction", "content": "Figure 1 provides an overview of our proposed GraphArena benchmark. In this section, we first discuss the collection and sampling methodologies for real-world graphs in Section 2.1, then offers detailed descriptions of each task within GraphArena and outlines the problem generation process in Section 2.2. Finally, Section 2.3 details the evaluation framework used to score the responses."}, {"title": "2.1 Dataset Collection", "content": "GraphArena distinguishes itself from previous benchmarks by utilizing real-world graphs instead of synthetic ones, offering a richer diversity of data. We collect graphs from the following five sources:\nDBLP [39] is an academic database containing over 7 million publications under the CC0 1.0 Public Domain Dedication license. We use an undirected research collaboration graph for the period 2003-2018 from DBLP [72]. This graph consists of 1,354,852 nodes and 5,129,047 edges, with each node representing an author and each edge reflecting a collaboration that has occurred at least five times.\nSocial Network [63] is a repository comprising over 70 different social networks. We combine graphs from four large networks-Digg, Flixster, Lastfm, and Pokec-to form a dataset of 6,118,793 nodes and 40,647,227 edges. Each node symbolizes a user, and each edge denotes a friendship connection. All user names are pseudonymized for privacy. It is under the CC BY-SA 3.0 License.\nDBpedia [11] extracts structured content from Wikipedia to form a comprehensive knowledge graph. We use DBpedia1M [81], a subset of the English version, which comprises 1,160,306 nodes and 7,214,631 edges. Each node represents an entity, and each edge denotes a relationship, with all original textual content retained as attributes. It is under the CC BY-SA 3.0 License.\nOpenflights [55] provides data for over 10,000 airports and their connecting flight routes. We construct an undirected graph where nodes are airports and edges are flight routes, weighted by the geographical distance between airports. We preserve the largest connected component in this graph, which consists of 3,390 nodes and 19,166 edges. It is under the DbCL v1.0 License.\nPubChemQC [53] offers a quantum chemistry database detailing ground-state electronic structures. From the PCQM4Mv2 Dataset part of the OGB Large-Scale Challenge [31] collected from PubChemQC-we incorporate 3,746,620 individual graphs. Each graph represents a molecule, where nodes are atoms and edges are chemical bonds, with atom types preserved as node attributes. It is under the CC BY 4.0 License.\nWe employ a random walk with restart strategy initiating from randomly selected nodes to effectively sample subgraphs. This technique enables us to identify the top-k most frequently visited nodes, forming local dense subgraphs that closely maintain the topological features of the original graphs. For the PubChemQC dataset, we sample molecular graphs directly by specified sizes. All graphs are treated as unweighted and undirected, except for OpenFlights which is weighted."}, {"title": "2.2 Task Design", "content": "Wikipedia lists 73 graph computational problems in graph theory, not counting variants. Selecting an appropriate subset for benchmarking is a notable challenge. Previous benchmarks often focused on simpler tasks like querying the existence and counts of basic graph elements such as nodes, edges, triangles, and rings [75, 25]. In contrast, we aim at incorporating more complex problems that demand advanced reasoning abilities.\nTo establish a nuanced evaluation of responses, we decompose each task into two requirements: satisfying the first requirement indicates a feasible solution, whereas fulfilling both requirements denotes a correct solution. We have chosen four polynomial-time tasks that require increasingly thorough understandings of graph structures:\n\u2022 Common Neighbor: For a graph $G = \\{V, E\\}$ with vertices $v_1, v_2 \\in V$, the task involves identifying the subset $S \\subseteq V$ where each $u \\in S$ is connected to both $v_1$ and $v_2$. The objectives are to (1) identify these common neighbors, and (2) maximize their count. We utilize the DBLP academic collaboration graph for this task.\n\u2022 Shortest Distance: In a graph $G = \\{V, E\\}$, determine the shortest path from node $v_1$ to $v_2$. The goals are to (1) find a viable path $(v_1, u_1, ..., v_2)$, and (2) minimize its hop count. We apply this task to the DBpedia knowledge graph.\n\u2022 Connected Component: In this task, the model identifies one representative node from each connected component within a graph. For a given graph $G = \\{V,E\\}$, the objectives are to (1) identify a set of vertices such that each is from a distinct connected component, and (2) ensure that these vertices represent all possible connected components of the graph. This task is applied to the Social Network dataset.\n\u2022 Graph Diameter: The diameter of a graph is the maximum distance between any pair of nodes in the graph. Given a graph $G = \\{V, E\\}$, this task entails finding (1) a shortest path between two arbitrary vertices and (2) ensuring it is the maximum possible among all shortest paths. This is performed using the DBpedia knowledge graph.\nEach task is briefly described by a keyword (Neighbor, Distance, Component, Diameter) to facilitate easy reference. For the NP-complete challenges, we have selected six representative tasks:\n\u2022 Maximum Clique Problem (MCP): Identify the largest complete subgraph, or clique, in a given graph $G = \\{V,E\\}$. The task entails (1) finding a clique $C \\subseteq V$ that is feasible within the graph, and (2) ensuring that $C$ is the largest among all possible cliques. This is implemented using the DBLP dataset.\n\u2022 Maximum Independent Set (MIS): Select the largest set of mutually nonadjacent nodes from a graph $G = \\{V,E\\}$. The task is to (1) identify an independent set $S \\subseteq V$, and (2) ensure that $S$ is the largest among all feasible sets. This task is performed using the Social Network dataset.\n\u2022 Minimum Vertex Cover (MVC): Given a graph $G = \\{V, E\\}$, find a subset of nodes $S \\subseteq V$ such that each edge in $G$ has at least one endpoint in $S$. The Minimum Vertex Cover Problem is to (1) find the vertex cover $S$ and (2) ensure the size of $S$ is minimum among all vertex covers of $G$. We explore this using the Social Network dataset.\n\u2022 Maximum Common Subgraph (MCS): Compare two graphs $G$ and $H$ to find the largest common subgraph. The objectives are to (1) determine a node-induced common subgraph $S$ between $G$ and $H$, and (2) maximize the size of $S$. The PubChemQC dataset is used for this task.\n\u2022 Graph Edit Distance (GED): For two graphs $G$ and $H$, determine the minimum edit distance via node mappings. This involves (1) establishing a node mapping that aligns $G$ with $H$, and (2) minimizing the edit operations required. These operations include adding or deleting an edge or isolated node, or relabeling a node. The PubChemQC dataset is utilized here.\n\u2022 Traveling Salesman Problem (TSP): Solve the TSP in a weighted graph $G = \\{V,E\\}$, where $w: E \\rightarrow R^+$ assigns a positive weight to each edge, representing the travel distance. The objectives are to (1) find a route P that visits each node exactly once and returns to the starting point, and (2) minimize the total travel distance. We utilize the OpenFlights dataset for this challenge. To ensure every pair of nodes is connected, thereby guaranteeing a feasible solution, we convert the dataset into a complete graph by adding edges to represent the shortest possible distances between nodes that are not directly connected.\nProblem Generation: For each of the 10 tasks, we randomly sample 500 small and 500 large graphs to create two distinct subsets, yielding a total of 10,000 graphs. The size ranges for these graphs are tailored to the complexity of the tasks. For the Neighbor and Distance tasks, small graphs contain 4 to 19 nodes, while large graphs contain 20 to 50 nodes. For Component, Diameter, MCP, MIS, and MVC, small graphs have 4 to 14 nodes, and large graphs range from 15 to 30 nodes. For MCS, GED and TSP, small graphs have 4 to 9 nodes and large graphs range from 10 to 20 nodes.\nAfter sampling graphs with the given size, we encode graph problem into text based on templates, a common approach in previous benchmarks [75, 17]. An example of this can be seen in the bottom left of Figure 1. The process begins with an introduction and definition of the task, followed by a randomly selected example with the correct answer to demonstrate the problem-solving process. Subsequently, the actual problem to be solved is listed, including detailed graph information and task requirements. While the graphs used may not appear large, their conversion into text often results in problems containing up to 6,000 tokens, posing a significant long-range challenge. Detailed statistics on graph sizes and problem lengths, along with examples for each task, are provided in the appendix."}, {"title": "2.3 Evaluation Framework", "content": "Unlike previous benchmarks that only verify responses as yes, no, or a numeric value, we develop a more detailed evaluation framework to prevent pattern-based guesses. For each task, LLMs are required to output the relevant graph component or the path that contributes to the final answer, as indicated by the green sections in each graph in Figure 1. Responses are categorized into four types through a checker: (1) Correct: The solution satisfies both specified task requirements; (2) Suboptimal: The solution satisfies the first requirement but not the second; (3) Hallucinatory: The solution is in the correct format but does not satisfy either requirement; (4) Missing: The LLM fails to provide a solution in the correct format.\nFor example, in the bottom middle of Figure 1, Deepseek-V2 correctly identifies the maximum clique of four authors. In contrast, GPT-40 finds a feasible but suboptimal solution with three nodes, while Llama3-8b produces a hallucinatory response, erroneously linking Claudiu and Fabien\u2014a connection that does not exist. If we merely report numbers as in existing graph problem benchmarks, this error would be misclassified as a correct answer.\nMetrics. Given the rarity of missing results in our experiments (less than 1% in most cases), our primary metrics focus on Accuracy (the proportion of correct answers), Feasibility (the proportion of both correct and sub-optimal answers), and Hallucination (the proportion of hallucinatory responses). We also introduce ranking-based metrics for additional comparison, particularly useful when most models yield suboptimal solutions. These metrics include Mean Reciprocal Rank (MRR), Top-1 Probability, and Top-3 Probability."}, {"title": "3 Experiments", "content": "Experimental Setup. In our experiments, we evaluated ten popular LLMs in GraphArena, encompassing both closed-source and open-source models. This included single models and mix-of-experts with diverse scales. Among the closed-source models, we tested the latest GPT-40 (gpt-40-2024-05-13) [54], the well-known GPT-3.5 (gpt-3.5-turbo-0125) [56], and the cost-effective Claude3-haiku [6]. In the open-source category, our assessments included the Llama series (Llama3-70b-Instruct and Llama3-8b-Instruct) from Meta, the Qwen series (Qwen1.5-72b-Chat and Qwen1.5-8b-Chat) from AliCloud, and Gemma-7b from Google Cloud [1]. Additionally, we evaluated two mix-of-expert LLMs: Deepseek-V2 [10] with 230 billion parameters (21 billion active) and Mixtral-7x8b [35] featuring 47 billion parameters (13 billion active).\nFor closed-source LLMs and open-source LLMs larger than 10 billion parameters, we utilized services from cloud providers, including OpenAI, Claude, AliCloud [23], Deepseek [60], and AIMLAPI [7]. For smaller models with fewer than 10 billion parameters, inferences were conducted on our local machines equipped with four NVIDIA H800 PCIe 80GB GPUs. EGiven the high costs and time-intensive nature of evaluating LLMs, we limited our runs to a single instance per model across 10,000 problems in GraphArena. To encourage consistency in responses, we used a low temperature setting of 0.1 and imposed no constraints on the output length of any model. All responses were documented for public reference."}, {"title": "3.1 Main Results", "content": "Table 1: Average rankings and performance of 10 LLMs on 4 Polynomial-time and 6 NP-complete tasks across small and large graphs. All metrics excluding MRR are scaled to [0, 100]. Acc., Fea., and Hallu. represent Accuracy, Feasibility, and Hallucination probability, respectively. For all metrics except Hallucination, higher values indicate better performance. The best-performing open and closed-source models are highlighted in bold.\nIn Table 1, we present the average performance of ten LLMs across four polynomial-time and six NP-complete tasks, assessed on both small and large graphs. As expected, GPT-40 consistently ranks as the top performer across most metrics and settings, with Llama3-70b closely following as the premier open-source model. Performance generally declines for all LLMs as graph sizes increase and tasks transition from polynomial-time to NP-complete, expanding the search space significantly. GPT-40, despite its capabilities, struggles notably with NP-complete tasks on large graphs, achieving only a 5.93% accuracy rate. It indicates the most challenging scenario in GraphArena is still much larger than current LLMs' upper bound.\nGraphArena also reveals a significant performance gap between LLMs with different parameter numbers. For example, in polynomial-time tasks, the disparity between Llama3-70b and Llama3-8b is notable-61.2% vs. 28.6% on small graphs and 31.6% vs. 9.4% on large graphs. This gap is significantly wider than in other benchmarks, such as GSM8K (93.0% vs. 79.6%) and GPQA (39.5% vs. 34.2%). A similar trend is observed in the Qwen series. Moreover, models with fewer parameters are more prone to hallucination, with ratios of 78.2% for Llama3-8b, 91.2% for Qwen1.5-7b, and 84.0% for Gemma-7b, indicating that GraphArena's challenges necessitate more advanced reasoning capabilities that larger LLMs are better equipped to manage.\nTo closely examine model performance on individual tasks, we analyzed the feasibility and accuracy of five selected LLMs-Claude3-haiku, Deepseek-V2, GPT-40, Llama3-70b, and Mixtral-7x8b-as shown in Figure 2. Results for the remaining five LLMs are demonstrated in Figure 5 in the appendix. The general trends align with Table 1, showing decreased performance in larger graphs and more complex tasks. A detailed analysis shows that model performance varies substantially across different tasks. For instance, tasks such as Diameter, MCS, and MVC demonstrate notably low feasibility. This is attributed to their complex verification processes: verifying Diameter requires solving a shortest path problem, while verifying MCS involves graph isomorphism. Regarding accuracy, NP-complete tasks generally perform worse than polynomial-time tasks. Notably, the Diameter task shows the poorest performance among polynomial-time tasks due to its demand for a comprehensive global search. This indicates that task complexity significantly impacts accuracy."}, {"title": "3.2 In-depth Analysis of Hallucination", "content": "This section delves into the phenomenon of hallucination in LLMs, specifically aiming to address two critical questions: (1) What are the underlying causes of hallucination in GraphArena? (2) Are existing strategies able to mitigate the issue?\nOur primary findings suggest that the hallucination ratio is linked with the number of model parameters, the complexity of its feasibility checks, and the scale of the graphs. To further illustrate this, we have plotted the hallucination probability against varying node sizes for three selected tasks in Figure 3. Additional data for seven other tasks are detailed in Figures 6 and 7 in the appendix. These plots reveal a significant, almost linear increase in hallucination probability as node size grows from 5 to 30. Notably, for GPT-40 on the Diameter task, the hallucination probability rises from 18% with a node size of 5 to 80% with a node size of 30, underscoring that larger graph sizes and their associated challenges in long text and exploding search space are major contributors to hallucination.\nChain-of-Thought (CoT) Prompting. Several strategies have been proposed to mitigate hallucinations in LLMs [9, 34, 87, 77]. We explore the effectiveness of a representative approach, CoT prompting, which encourages the model to generate intermediate reasoning steps towards a conclusion [78, 26, 52]. We design manually crafted examples for the Graph Diameter and Connected Component tasks, assessing the impact of using zero to four CoT examples. As shown in figure 4, a general enhancement in model accuracy is observed. Specifically, the performance of Deepseek-V2 notably improves after incorporating just one CoT example for the Component task. Nonetheless, Claude3-haiku and Mixtral-7x8b exhibit only marginal improvements. This suggests that, while CoT prompting contributes to reducing errors, it remains insufficient to fully resolve hallucinations.\nGraph-aware LLMs. Several methods have been developed to fine-tune LLMs using graph-specific datasets, aiming to create graph-aware LLMs capable of understanding graph structures and solving basic graph tasks. To investigate whether this can help alleviate the issue of hallucinations, we test GraphArena on GraphWiz [17], an open-source model derived from Llama2-7b and fine-tuned using Direct Preference Optimization [61] across nine graph problems. While GraphWiz shows proficiency on the tasks it was initially trained on, it struggles on the GraphArena benchmark, with accuracies only ranging from 0.2% to 1.2% across four small-scale polynomial-time tasks. This can be attributed primarily to two factors: the absence of task overlap between GraphArena and the GraphWiz training corpus, and the model's tendency to rely on pattern recognition rather than problem understanding."}, {"title": "3.3 Comparison with Classic Solvers", "content": "To better understand the performance of LLMs on NP-complete tasks, we compare them with several classic solvers on the Travelling Salesman Problem (TSP) task. We consider the following three solvers: (1) Random: randomly generates a feasible solution, (2) Greedy: selects a node at each step that is not already in the cycle and whose connection to the previous node adds the least cost to the cycle, and (3) Christofides: computes a 3/2-approximation of the TSP in a complete undirected graph using the Christofides algorithm [21], which is considered a better heuristic algorithm compared to Greedy. For each solver, the percentage of problems where the LLM wins, ties, or loses against the classic solvers is reported in Table 2."}, {"title": "4 Related Work", "content": "LLM Evaluation. Early evaluations of text generation focused on fundamental skills, including similarity measures such as ROUGE [46], METEOR [8], and BLEU [58] for machine translation [47, 64]. They also included inherent scores like Diversity [41] and Perplexity [33] for open-domain conversations [86, 43], and factual measures such as F-score and exact match for machine reading comprehension [62, 14] and question answering [90, 38]. Recently, numerous benchmarks have employed more advanced tests to assess the high-level capabilities of LLMs [30, 22, 50, 48, 15, 40, 74, 16, 71]. For instance, the Massive Multitask Language Understanding benchmark [29] covers 57 subjects across STEM, the humanities, the social sciences, and more. The Beyond the Imitation Game Benchmark [65] includes over 200 tasks, ranging from language understanding to interaction and behavior. Human preference remains a critical aspect of LLM evaluation [88, 42, 20]. Chatbot Arena [20] employs a pairwise comparison approach and leverages input from a diverse user base on crowd-sourcing. Our proposed GraphArena benchmark aims to evaluate a wide range of reasoning capabilities of LLMs on polynomial-time and NP-complete graph Computational problems, offering a valuable supplement to the existing benchmarks.\nLLM on Graphs. The synergy between graphs and LLMs has sparked considerable interest due to their bi-directional benefits: integrating graph-based approaches can enhance the reasoning abilities of LLMs, enabling them to tackle complex logical tasks such as mathematical problem-solving [85], code generation [28], and knowledge reasoning [83, 57], among others. Conversely, LLMs offer powerful capabilities to augment graph analytics and learning, particularly for predictive tasks in text-attributed networks [25, 75, 18, 19, 70]. Compared with machine learning tasks on graphs such as node classification and anomaly detection [68, 69], graph computational problems pose more challenges for LLMs as they require a deeper understanding of structural information and long-range multi-step reasoning [51, 76, 67, 12]. GraphQA [25] and NLGraph [75] are two early benchmarks in this domain, focusing on basic graph problems using small-scale synthetic graphs. VisionGraph [45] and GITA [79] introduce multimodal graph reasoning tasks, which require LLMs to reason over both text and image modalities related to the graph. GraphWiz [17] introduces an instruction-tuning dataset designed to equip language models with the ability to tackle graph problems using explicit reasoning paths."}, {"title": "5 Conclusion", "content": "This paper introduces GraphArena, a benchmarking tool designed to evaluate the reasoning capabilities of LLMs through graph computational challenges. GraphArena features a realistic graph collection, carefully selected tasks, and a rigorous evaluation framework. Our evaluation of ten LLMs across 10,000 GraphArena problems reveals persistent hallucination issues, especially with larger graphs, more complex tasks, and models with fewer parameters. Despite implementing advanced strategies such as chain-of-thought prompting and domain-specific fine-tuning, these issues remain unresolved. Future efforts include investigating LLMs' ability to write code, integrating multi-modality to better address these challenges, and exploring new techniques to mitigate hallucinations and improve reasoning capabilities."}, {"title": "A Additional Experiments", "content": "We provide detailed statistics and examples to further explain the tasks and performance evaluations discussed in the main text. Table 3 presents comprehensive statistics for each task in GraphArena, including the average and maximum number of nodes edges, and text length counted by number of characters. Figure 5 offers a feasibility and accuracy comparison of the remaining five LLMs on each individual task, complementing the analysis in Figure 2. Figures 6 and 7 illustrate the influence of graph size on hallucination probability for the remaining seven tasks, as the complement of Figure 3. Tables 4 through 13 provide detailed examples of prompts for each task. For the Connected Component and Graph Diameter tasks, chain-of-thought (CoT) prompting with step-by-step solution demonstrations is provided."}]}