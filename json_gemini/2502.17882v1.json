{"title": "Science Across Languages: Assessing LLM Multilingual Translation of Scientific Papers", "authors": ["Hannah Calzi Kleidermacher", "James Zou"], "abstract": "Scientific research is inherently global. However, the vast majority of academic journals are published exclusively in English, creating barriers for non-native-English-speaking researchers. In this study, we leverage large language models (LLMs) to translate published scientific articles while preserving their native JATS XML formatting, thereby developing a practical, automated approach for implementation by academic journals. Using our approach, we translate articles across multiple scientific disciplines into 28 languages. To evaluate translation accuracy, we introduce a novel question-and-answer (QA) benchmarking method, in which an LLM generates comprehension-based questions from the original text and then answers them based on the translated text. Our benchmark results show an average performance of 95.9%, showing that the key scientific details are accurately conveyed. In a user study, we translate the scientific papers of 15 researchers into their native languages, finding that the authors consistently found the translations to accurately capture the original information in their articles. Interestingly, a third of the authors found many technical terms \"overtranslated,\" expressing a preference to keep terminology more familiar in English untranslated. Finally, we demonstrate how in-context learning techniques can be used to align translations with domain-specific preferences such as mitigating overtranslation, highlighting the adaptability and utility of LLM-driven scientific translation. The code and translated articles are available at hankleid.github.io/ProjectMundo.", "sections": [{"title": "Introduction", "content": "Around 98% of all peer-reviewed scientific articles are published in English, but only around 7% of the world's population speaks English as a first language (Liu, 2017). While having a common language among academic journals facilitates international scientific discourse, it also creates a significant barrier to access scientific knowledge for non-native English speakers. For instance, a large-scale survey found that 96% of respondents agree or strongly agree that English as the dominant academic language disproportionately advantages native speakers, among other similar studies (Ferguson et al., 2011) (Tardy, 2004) (Flowerdew, 1999). This linguistic dominance introduces challenges across multiple aspects of science, from biases in peer review against non-native English writers to global implications for science-informed policy (Steigerwald et al., 2022). At the heart of this issue is language accessibility in existing scientific literature. Academic journals, especially widely-read and open-access journals, cater to a global audience (Nature Index, 2024). The availability of scientific literature in a person's native language could play a crucial role in shaping their decision to pursue a career in science.\nMany studies have explored potential solutions and paradigm shifts to overcome the language barrier in academic publishing. Given the systemic role that academic journals play in disseminating scientific knowledge, the most direct and impactful solution is for journals themselves to support translations of their articles to other languages. Several challenges hinder the adoption of multilingual translations for journal articles, including cost, logistical complexity, and the question of how best to translate scientific text. The search for a feasible, accurate, and easily adoptable method for translating scientific articles remains elusive.\nMachine translation offers a cost-effective and scalable solution for translating text. With the rapid development of neural-based approaches and deep learning, machine translation improved enormously, and neural machine translation (NMT) systems like Google Translate and DeepL have been the gold standard for both general and professional translation tasks (Kalchbrenner & Blunsom, 2013) (Stahlberg, 2019). Recently, with the rise of transformer-based large language models (LLMs), the landscape is shifting. Recent studies show that LLMs match and often surpass NMT systems in performance across a wide variety of translation tasks, including scientific text (Hendy et al., 2023) (Jiao et al., 2023) (Mohsen, 2024). What truly sets LLMs apart, however, is their ability to process complex instructions and generate context-aware, customized outputs. By leveraging simple in-context learning techniques alone, LLMs can be trained to produce translations that are specifically tailored to the requirements of the scientific community, accommodating factors like formatting preferences and domain-specific vocabulary. This flexibility opens the door to incorporating a wide range of potential feedback from non-native English-speaking researchers, enabling more specialized and effective translations.\nIn this article, we develop LLM-backed automated translation solutions to support lowering the language barrier in the scientific community. We introduce a method for generating publisher-ready full-length article translations, propose a novel QA benchmarking strategy to evaluate translation quality, and demonstrate how LLM few-shot prompting can be used to integrate feedback from actual authors of research papers into the translation process. We assess the strengths and weaknesses of LLM translation through both automated evaluations and user studies."}, {"title": "1.1 Related works", "content": "Several studies have evaluated the performance of LLMs (e.g. GPT models) on various translation tasks, showing that many are competitive with previous state of the art NMT systems, especially more recent models such as GPT-4 (Hendy et al., 2023) (Jiao et al., 2023). Further developments in LLM-based translation include prompting techniques (Vilar et al., 2022) (Zhang et al., 2023), context aware and document-level translation (Wang et al., 2023), translations that adapt to user feedback in real time (Moslem et al., 2023a), non-English monolingual corpora fine-tuning (Xu et al., 2023), and fine-tuning to emulate professional human translation strategies such as analyzing specific parts of a sentence before translating (He et al., 2024).\nWhen it comes to assessing machine translation of scientific journal articles, the literature is more sparse. Zulfiqar et al. (2018) applied a variety of NMT systems, including Google Translate and DeepL, to translate excerpts of German scientific articles from the last century. Other studies focused on scientific abstracts (Tongpoon-Patanasorn & Griffith, 2020) (Wei, 2017). To the best of our knowledge, all other studies on scientific translation were specialized to the medical field (Soto et al., 2019) (Daniele, 2019) (Sebo & de Lucia, 2024). Although the topic of translating full-length academic journal articles has yet to be thoroughly investigated, many studies have introduced general LLM-backed translation strategies for technical and terminology-heavy text. Some of those strategies include term extraction and glossary creation (Kim et al., 2024), RAG-based dictionary retrieval (Zheng et al., 2024), and using LLM-generated synthetic data to train proper usage of domain terminology (Moslem et al., 2023b).\nThe most widely used and convenient methods for benchmarking machine translation are automated metrics such as BLEU, ChrF, TER, and COMET. These metrics are typically applied to source-target translation pairs from established datasets like the Workshop on Machine Translation (WMT) or FLoRes (for low resource languages). In addition to these automatic metrics, human evaluation is often employed to provide a more nuanced and reliable assessment of translation quality. Parallel datasets have a few drawbacks, primarily that they contain a limited number of language pairs and are restricted to specific topics. WMT offers parallel biomedical datasets (Neves et al., 2022), but none for scientific text at large.\nPengpun et al. (2024) implemented a No Language Left Behind (NLLB) model that supports code-switching (keeping some terminology in English) in Thai-English medical translation, constituting the only study to our knowledge that fine-tunes the translation to an established preference of end-users (in their case, medical physicians). Another study analyzed research abstracts from English and Chinese articles and found substantial differences in rhetorical conventions (Li, 2020). We were not able to find systematic studies on the preference of researchers on academic translation."}, {"title": "1.2 Our contributions", "content": "Journal-compatible translation. To the best of our knowledge, we develop the first pipeline using LLMs to translate scientific articles while preserving standard publishing formats (JATS XML).\nAutomated QA benchmarking. We have developed an automated benchmarking method specifically tailored for scientific documents (Section 3). Our approach requires only the original and translated documents to assess translation quality, making it language agnostic, article specific, and independent of parallel translation datasets.\nTranslation preferences for scientific text. We have gathered feedback on machine translations in a variety of languages directly from authors of research papers across multiple scientific disciplines. Subsequently, similar to Pengpun et al. (2024), we have also create code-switched translations; instead of masking, we implemented few-shot prompting using a scientist-curated example translation."}, {"title": "2 Journal-compatible translation", "content": "Journals have the power to change language barrier norms, as they serve as the primary forum for scientific knowledge. However, for multilingual translation to be widely adopted in scientific publishing, the process must be practical for journals to implement. In this section, we demonstrate how LLMs can preserve the formatting of journal articles during translation, offering an approach that is adaptable and easy to integrate.\nIn 2002, the NIH introduced the Journal Article Tag Suite (JATS), an XML protocol for structuring scientific journal articles. Since then, JATS has become part of the National Information Standards Organization (NISO) and is the global standard for academic publishing. Despite the distinct \"look and feel\" of articles across different publishers, they all share the same underlying JATS XML structure. For instance, academic journal articles universally include <front>, <body>, and <back> sections that contain the main text; <article-title> and <abstract> sections; a <contrib-group> section that stores author information, and many more (Needleman, 2012).\nWe employ an LLM (GPT-4o) to translate journal articles in their native JATS form, ensuring that the XML structure remains intact while translating the content. When tasked with translating this section (\"Sec5\"), GPT-4o successfully translates the text while preserving the surrounding <sec>, <title>, <p>, and <xref> tags. A full article is much more complex, however, consisting of multiple, heavily nested elements that include figures, tables, equations, and more. We translate each full article in a series of API calls to GPT-4o, processing memory-manageable chunks (typically around 5 paragraphs) at a time. To increase context awareness, we prepend the prompt with the contents of the full original document.\nEven for complex elements, we find that GPT-4o reliably maintains XML formatting without introducing errors. However, occasional issues arise with nesting, such as paragraph text incorrectly appearing inside a figure caption. To ensure structural accuracy, we translate tables and figures-typically the most complex elements in a JATS article independently before appending them to their respective sections. Additionally, we observe that special characters like '<' and '>' can sometimes cause truncation of the article text, even though the XML structure remains intact. To address these issues, we modify the prompt to include \"Do not cut sentences short and include all symbols,\" after which the model produces fully translated articles with no other structural issues. In the 348 translations we generated over the course of this study, we identified no truncation errors and only one nesting error, resulting in a 99.7% accuracy in preserving the original JATS structures.\nUsing our method, we successfully translate full articles into 28 different languages while fully preserving the JATS formatting. Because of its compatibility with native article formatting, this translation step can be applied at the final stage of publication or to articles that have already been published. This ensures maximum compatibility with the publication framework and paves the way for widespread translation of articles across different journals. Moreover, the translation is applicable to an arbitrary number of languages. While JATS is the ubiquitous standard, this approach is adaptable to other XML protocols as long as the tag suite is properly documented, ensuring broad compatibility across scientific publishing. A database of all translated articles, totaling to 348 translations, is available on our webpage: hankleid.github.io/ProjectMundo."}, {"title": "3 QA-style automated benchmarking", "content": "In this section, we evaluate the translation quality of our approach. Traditional machine translation evaluation relies on automated benchmarking metrics such as BLEU, which compare translations against parallel reference data. However, to our knowledge, no dataset exists that provides parallel, document-level scientific translations across the diverse range of languages and disciplines we have included here. Instead, we introduce a novel question-and-answer (QA) style benchmarking method. In this approach, an LLM generates a \"quiz\" with multiple-choice questions designed to capture key details from the original scientific article. The LLM then \"reads\" the translated article and attempts to answer these questions based solely on the translated content. The higher the accuracy, the better the translation conveys the scientific details of the original text.\nA key advantage of this benchmarking method is its automation and adaptability. Unlike traditional evaluation techniques, it does not require parallel translation data and is therefore applicable to any article, in any format, and in any language. This flexibility is particularly valuable for evaluating translations in \"low-resource\" languages, where high-quality parallel datasets are scarce."}, {"title": "3.1 Benchmarking procedure", "content": "The benchmarking procedure is illustrated in Figure 2. First, we prepare the quiz by providing GPT-4o with the original English text and prompting it to create 50 multiple-choice questions that encapsulate key details of the paper, along with a corresponding answer key. The exact prompt is as follows:\n\"Please read the following scientific journal article. Generate 50 detailed and specific questions to test a reader's understanding of the findings of the article. Each question should be unique. The questions should labeled 1-50. The questions should be multiple choice with 6 possible answers: 5 are labeled A-E, and the 6th option should say 'I don't know'. There should only be one correct answer from the options. The questions should cover the unique results, figures, and tables of the article as much as possible.\"\nTo execute the benchmark, we then prompt the model to read the translated article and answer the quiz questions. In this scenario, the model simulates a real person reading the translated text; if the translated article effectively conveys the core details and central findings, the model should perform well on the evaluation. The model's quiz accuracy, graded against the answer key, constitutes the benchmark result. For instance, if the LLM correctly answers 48 out of 50 questions for a given article in a particular language, the benchmark score for that translation would be 96%. To ensure that the quiz-taking LLM relies only on the translated article rather than its prior knowledge, we implement two safeguards. First, we exclude the quiz-generation exchange from the model's memory before administering the quiz. Second, we prevent pre-training contamination by filtering for articles that score 0% on the benchmark when the article is not provided. Full details on prompts and model parameters are provided in the Appendix."}, {"title": "3.2 Results", "content": "For this study, we apply the QA benchmark to six articles spanning a wide range of disciplines, from medicine to archaeology to quantum optics. Each article is evaluated across translations in 28 different languages, which were selected based on countries in Nature Index's Research Leaders list and further supplemented to include languages from more regions of the world. We also include an English baseline, which we perform by conducting the quiz on the original article. Figure 3 presents the results. The overall average performance across all 29 languages and all six articles is 95.9%, with the lowest average score at 91.7% (Tamil) and the highest average score at 98.0% (Swedish). Notably, no individual quiz score falls below 84%, and translations in 23 languages score 100% on at least one article. These high QA results indicate that our translation approach effectively conveys the key findings and essential details of scientific articles across diverse disciplines.\nThe English baseline score (97.3%) is higher than the overall average, but not a perfect 100%. We attribute this to two potential factors: (1) the quiz-taking LLM, like a human reader, may exhibit slight imperfections in reading comprehension, and (2) quiz questions or answer choices may be occasionally ambiguous (see Section A.1.3 for further analysis). While refining the quiz questions could further improve the benchmark, we believe the current methodology already provides a strong evaluation framework. Additionally, our results reveal that \"low-resource\" languages such as Urdu, Telugu, and Tamil perform slightly below high-resource languages, aligning with prior findings in both machine translation and multilingual LLM research (Nicholas & Bhatia, 2023) (Jiao et al., 2023). However, since even the lowest-performing languages achieve an average accuracy above 91%, this effect is minor, demonstrating that our benchmarking technique is applicable across a wide range of languages.\nComparisons with the same articles translated as plain text (without JATS formatting) by GPT-4o and Google Translate further support our finding that highly structured text is translated just as effectively as unstructured text. We observe no degradation in translation quality due to the additional task of preserving structured content. Specifically, the average benchmark score for GPT-4o's XML-based translations (95.9%) closely matches that of GPT-4o's plain text translations (96.0%) and Google Translate (95.8%) (Section A.1). While this study focuses on benchmarking JATS-formatted translations as a form of customization, our QA-based evaluation method is broadly applicable for evaluating translations across various formats, even when other types of translation customizations are applied."}, {"title": "4 Feedback from authors", "content": "In this section we complement the QA benchmarking results with evaluations from 15 human scientists across various languages and disciplines, including theoretical and experimental quantum optics, nanophotonics, biostatistics, materials science, magneto-electronics, machine learning, and more. In this study, each participant is provided with a translation of their own scientific paper in their native language, generated using our method. We then gather feedback on translation quality using the following questions:\n1. How effectively does the translation convey the original information of the article?\n2. How well do you think another speaker of this language would be able to understand the key ideas of this paper just from this translation?\n3. How satisfied are you with the translation of technical terms in the article?\n4. How well does the translation flow and maintain cohesion throughout the text?\n5. How well does the translation maintain the original tone and style of the article?\nFor each question, the three possible options are few or no issues, some issues, many issues, and other. Participants also have the opportunity to provide free-form comments with their observations and opinions. Questions 1 and 2 target the accuracy and main details, similar to the QA benchmark, while questions 3, 4, and 5 probe stylistic and subjective aspects of translation quality. Through this questionnaire we aim to gain deeper insights into the academic community's perspective on what defines effective scientific translation.\nAs expected, nearly all researchers in our study (93.3%) report that the translation of their paper contains few or no issues in conveying the original information, reinforcing the findings from our QA benchmarking. Participants also generally agree (86.7%) that other scientists reading their translated paper would understand the key ideas with few or no issues. Based on some participant comments, the most commonly cited issues in this area include minor misinterpretations and inconsistencies in vocabulary (e.g., a specific word being translated differently throughout the text).\nKey insights arise from the more subjective questions. As one might anticipate from machine translation, authors rate lower scores in the categories of tone and style, flow and cohesion, and technical terms. In particular, many participants (86.7%) describe an unnatural quality to the translation or dissatisfaction attributed to the handling of technical and domain-specific vocabulary. With regard to technical vocabulary, participants reported two kinds of issues:\n(i) Mistranslation: This technical term exists in their native language, but the model translated it awkwardly or incorrectly.\n(a) Example 1: The model translated edge coupling into French as couplage par bord, but the more commonly-used phrase is couplage par la tranche.\n(b) Example 2: The model translated switching (e.g. magnetic switching) into Chinese as \u5207\u63db, but \u8f49\u63db is a better fit.\n(ii) Overtranslation: This technical term does not exist in their native language, or is rarely used in practice, and the original English word is preferred.\n(a) Example 1: The model provided a literal translation of rigorous coupled-wave analysis into Korean (\uc5c4\ubc00 \uacb0\ud569 \ud30c\ub3d9 \ud574\uc11d), but using the English term is preferred.\n(b) Example 2: The model translated gap (e.g. Hamiltonian/energetic gap) into Spanish as brecha (breach). A better translation might be salto, as in salto de energ\u00eda (energy jump), but many scientists would simply use the English gap.\nWhether certain terms might be more appropriately left untranslated is not a typical factor in traditional machine translation. However, the feedback from scientists highlights the importance of this consideration in scientific translation. The frequency of overtranslation comments in our survey responses (33.3%) suggests the need for nuanced translation approaches that align with how technical terms are used in practice."}, {"title": "5 Feedback-adaptive translation", "content": "In this section, we leverage LLM output customization to incorporate the feedback from scientists. In particular, since so many scientists expressed a preference for retaining some technical terms in English, we apply a targeted prompting technique to preserve some English vocabulary during translation.\nTo translate text while maintaining the appropriate English terms, a key challenge is the inherently subjective nature of deciding which terms to keep and which to translate. To navigate this, we employ few-shot prompting, an in-context learning technique where GPT-4o is provided with verified examples to improve responses in scenarios where data is scarce (Brown et al., 2020). Specifically, we construct a one-shot prompt using a translated paragraph from a scientific article in which the author of the article has reviewed the translation and identified terms that should remain in English. This curated example then serves as a guide for translating other texts.\nUsing this prompting method, we generate new translations and seek feedback from five authors who previously expressed concerns about technical term translations. Each participant reviews two versions of an excerpt from their paper: one direct translation and one generated with the one-shot prompt that retains some English terms. They are then asked to indicate their preference between the two versions.\nThe results of this follow-up survey reveal a diverse range of preferences. As anticipated from the initial survey, three of five participants find that retaining some English terms produces a more natural and readable scientific text. Conversely, the other two participants are more inclined toward the complete translation, citing a preference for better-translated terms over English terms. From the responses, one interesting observation is that speakers of languages with a higher prevalence of English loanwords, such as Korean, tend to favor English technical terms compared to those from languages with fewer English loanwords, such as French, a phenomenon which might be influenced by historical linguistic reasons\u00b9 (Blackwood, 2013) (Tyson, 1993). Additionally, one participant proposes a balanced approach: to present the original English term in brackets alongside the translated word, rather than strictly choosing one over the other. The strength of LLM-based translation lies in its ability to integrate diverse customization and feedback, enabling tailored and therefore more effective translations. While this study focuses on the overtranslation phenomenon, the prompting technique we utilize in this section can be applied further to other vocabulary or stylistic preferences by incorporating additional examples."}, {"title": "6 Discussion", "content": "In this study, we utilized LLM-powered translation to go beyond traditional plain-text translation, resulting in scientific translations that are tailored with both publishers and authors in mind. Our method successfully translates scientific articles in JATS XML while maintaining the complex structure, opening a new avenue for academic journals to include translations for their articles. Through a novel automated QA benchmarking approach, we quantitatively evaluated full article translations across 28 languages and many scientific disciplines, finding that key scientific details are reliably conveyed even in low-resource languages. Further, our human evaluation study revealed valuable insights into the qualitative aspects of scientific translation. While the survey results confirmed the high overall translation accuracy, they also highlighted areas for improvement, such as the handling of technical vocabulary. By leveraging the few-shot prompting technique, we incorporated feedback and generate customized translations that align with the linguistic preferences of researchers across different fields and languages.\nUltimately, our findings emphasize that the flexibility of LLMs allows for nearly limitless degrees of customizability, making it possible to improve translations based on domain-specific requirements and preferences. This adaptability presents a significant step toward breaking language barriers in academic publishing, fostering broader accessibility and collaboration in global research."}, {"title": "6.1 Limitations and future directions", "content": "Two participants in our user study reported inconsistencies in the translation of certain terms throughout the article. This likely stems from our approach of translating articles in separate sections to mitigate memory and XML nesting issues (Section 2), leading to potential variations in the model's vocabulary choices between different API calls. One possible solution is to track all translations of the same term and standardize them at the end by replacing inconsistent terms with the most common translation. Furthermore, while our method includes the full original article in the prompt to provide context, further research could explore ways to enhance context-awareness in scientific translation, which may also help reduce vocabulary inconsistencies. Additionally, we informally evaluated Llama 3.1-405B for our translation approach and found it to be more prone to XML nesting issues compared to GPT-4o, although further testing with other LLMs could provide valuable insights into their suitability for the translation methods introduced in this study.\nFuture work could also expand on understanding researchers' preferences in scientific translation. A larger-scale survey may reveal additional patterns in translation preferences, particularly across different languages (as discussed in Section 5). Moreover, if our method or a similar approach is adopted for large-scale scientific translation, the resulting corpora could be used to fine-tune models, further improving consistency and overall translation quality."}, {"title": "A Appendix", "content": null}, {"title": "A.1 QA benchmark", "content": "The journal articles translated for this study are:\n\u2022 Article snippet in Figure 1: Lepczyk et al. (2023)\n\u2022 Article 1: Heo et al. (2024)\n\u2022 Article 2: Fern\u00e1ndez-Crespo et al. (2023)\n\u2022 Article 3: Rus et al. (2023)\n\u2022 Article 4: Sephton et al. (2023)\n\u2022 Article 5: Yang et al. (2023)\n\u2022 Article 6: Peng et al. (2023)\nModel information for generating and executing the QA benchmark:\n\u2022 Model: gpt-4o-2024-08-06"}, {"title": "A.1.1 Translating quiz questions", "content": "In generating the data in Figure 3 of the main text, we translate the quiz questions into the target language before executing the quiz benchmark. We translate the questions with the following prompt (temperature = 0):\n\u2022 \"The following JSON comprises a list of questions about an academic journal article. Please translate the questions and options into [lang]. Do not translate the keys of the JSON. Please return the translated JSON. Here is the JSON to translate: [questions]\"\nHere, we perform the quiz benchmark without translating the quiz questions (keeping them in English) and find an increase in overall accuracy from 95.6% to 97.2% (Fig. 6), suggesting that the language of the quiz questions may play a role in the model's general performance."}, {"title": "A.1.2 Comparisons with plain text translations", "content": "We translate the six articles using the same model (GPT-4o-2024-08-06) but processed the article as plain text, not JATS-formatted, as well as with Google Translate (GNMT) and compare the benchmarking results. The performance among all three translation methods were very similar (Fig. 7), indicating that our JATS-formatted translation method sees no degradation as a result of the LLM parsing XML at the same time as translating."}, {"title": "A.1.3 Quiz questions with high error rates", "content": "In Figure 8 we analyze the number of incorrect responses for specific quiz questions across all six articles and 29 languages. In particular, we note that question #8 on Article 3 was incorrect for all languages, including English. The quiz question is as follows: \"What percentage of patients reported dissociative symptoms that disappeared after SSRI treatment?\" with possible answers \"15%,\" \"25%,\" \"35%,\" \"45%,\" \"55%,\" and \"I don't know.\" Upon investigation of Article 3, the study reports that all patients who experienced dissociative symptoms before SSRI treatment had those symptoms alleviated with SSRIs. The quiz question is therefore malformed, and further research could be useful to determine methods for generating more robust quiz questions."}, {"title": "A.2 One-shot prompt for technical terms", "content": "Here we provide the full prompt used for translating excerpts while preserving some English terminology. First, we describe the one-shot example prompt:\nQ: \"Here is an excerpt of a scientific article: [original text]. Please take note of any highly domain specific words in this excerpt. Then, please translate the excerpt into Korean. But do not translate those highly domain specific words that you identified. For those words, keep the original English words in your translation instead. Everything else in the excerpt should be translated into Korean.\"\nA: [translated text with some technical terms preserved]\nWe use the following excerpt from Ahn et al. (2017) as our author-curated one-shot example:"}]}