{"title": "The Cognitive Revolution in Interpretability: From Explaining Behavior to Interpreting Representations and Algorithms", "authors": ["Adam Davies", "Ashkan Khakzar"], "abstract": "Artificial neural networks have long been understood as \"black boxes\": though we know their computation graphs and learned parameters, the knowledge encoded by these weights and functions they perform are not inherently interpretable. As such, from the early days of deep learning, there have been efforts to explain these models' behavior and understand them internally; and recently, mechanistic interpretability (MI) has emerged as a distinct research area studying the features and implicit algorithms learned by foundation models such as large language models. In this work, we aim to ground MI in the context of cognitive science, which has long struggled with analogous questions in studying and explaining the behavior of \"black box\" intelligent systems like the human brain. We leverage several important ideas and developments in the history of cognitive science to disentangle divergent objectives in MI and indicate a clear path forward. First, we argue that current methods are ripe to facilitate a transition in deep learning interpretation echoing the \"cognitive revolution\" in 20th-century psychology that shifted the study of human psychology from pure behaviorism toward mental representations and processing. Second, we propose a taxonomy mirroring key parallels in computational neuroscience to describe two broad categories of MI research, semantic interpretation (what latent representations are learned and used) and algorithmic interpretation (what operations are performed over representations) to elucidate their divergent goals and objects of study. Finally, we elaborate the parallels and distinctions between various approaches in both categories, analyze the respective strengths and weaknesses of representative works, clarify underlying assumptions, outline key challenges, and discuss the possibility of unifying these modes of interpretation under a common framework.", "sections": [{"title": "1 Introduction", "content": "How can we understand, interpret, and explain the behavior of complex intelligent biological systems like humans, chimpanzees, dolphins, or (at least seemingly) intelligent AI systems like large language models (LLMs)? This question lies at the heart of cognitive science, and different answers have defined entire paradigms in member disciplines such as psychology, neuroscience, and linguistics. For instance, in the 1950s and early 1960s, the dominant paradigm in academic psychology was behaviorism [1], resting on the fundamental assumption that human behavior can be fully understood and explained in terms of stimulus-and-response mechanisms [2, 3], with no need to consider humans' internal mental states, representations, or processing. However, behavior-ism failed to account for many key facets of human psychology, including language acquisition [3, 4], concept"}, {"title": "2 Background", "content": "2.1 A Brief History of Deep Learning Interpretation\nSince their introduction, neural networks have been considered \"black boxes,\u201d\u201d meaning they are not inherently interpretable. The research on deep learning interpretation is concerned with casting light on the \"black box\" problem in some form or another. In the era of deep learning preceding the development of foundation models (i.e., neural networks pre-trained on large-scale, self-supervised tasks such as LLMs [19]), the terms interpretation and explanation most often referred to saliency (feature attribution) methods. However, following the paradigm shift toward foundation models, these terms now more often refer to mechanistic interpretation. In this section, we provide a broad overview of both periods and associated paradigms in deep learning interpretation.\nBehaviorist Interpretation: The Rise of Post-hoc Explanations The paradigms of behaviorist and mechanistic interpretation of neural networks emerged concurrently, shortly following the modern study of deep learning. For instance, early work in deep learning interpretation discussed explaining the network behavior by identifying the importance of input features for the output, coining the term \"saliency maps\" [20]; and contemporaneous work studied representations by analyzing different neuron activations within convolutional neural networks [21]. However, the focus on post-hoc behavioral explanations \u2013 i.e., explaining why a given model produced a specific outputs [22-24] \u2013 gained more traction in the context of the then-dominant paradigm of classification and supervised learning, particularly in the forms of feature attribution methods and counterfactual explanations. Specifically, feature attribution [25-33] analyzes the behavior of the network by identifying input features that are relevant for that output; and counterfactual explanations [34\u201336] understand the behavior of the network by answering what needs to change in the input for the network to have a particular output. We highlight that"}, {"title": "3 Semantic Interpretation", "content": "We may define semantic interpretation as a matter of answering the following question: what latent properties are learned and represented by neural networks, and how do they contribute to observed model behaviors? For example, do vision models learn representations of semantically-related object categories or spatial relations; or do LLMs learn representations of syntactic dependencies or lexical relations?\n3.1 Optimization and Search\nWhat inputs maximize the activation of a given neuron? Given that individual neurons are the atomic level of representation for all neural networks, a natural question is, to what extent do single neurons code for distinct properties of interest? An analogous hypothesis in neuroscience is the notion of \u201cgnostic cells\u201d, cells (neurons) which fire only in the context of very specific concepts, such as one's grandmother (hence the name) [52, 53]. Do we find such neurons in contemporary deep learning systems? The most prominent approach to neuron-level semantic interpretation involves selecting an individual neuron and either searching through a pre-defined query set or generating an input (such as an image) that maximally activates the target neuron, and inspecting these optimized inputs for common features. Early work in this area focused on image recognition networks [21, 54\u201356], where some approaches operate by searching for patterns that maximize the output of neurons via image optimization [20, 21, 57, 58], and others searched through a pre-defined query set to find inputs that maximized neuron activations [54, 59\u201361]. For instance, Net2Vec [59] and Network Dissection [60, 61] systematically analyze the relationship between concepts and neurons by analyzing the activations of neurons in response to images in the dataset clustered by human-annotated visual concepts. Later work has investigated how the same paradigm can be applied to interpret individual neurons in language models [62-65] and multimodal vision-language models [66, 67].\n3.1.1 Assumptions and Challenges\nLevels of Representation Most \"optimization and search\"-based methods operate at the level of individual neurons, which comes with serious limitations: neural networks encode distributed representations where features are encoded by multiple neurons; and even small-scale, simplified \u201ctoy\" models have also been found to"}, {"title": "3.2 Structural Probing", "content": "Framework One of the most popular and well-studied approaches to has been structural probing [11, 12, 74]. The goal of structural probing is to train auxiliary classifiers (probes) to predict discrete latent properties of inputs from model embeddings. For example, one may train a probe to predict parts-of-speech from LLM token embeddings, so when given each token in the sequence (The, cat, meows, for, dinner), the probe predicts the corresponding parts of speech (determiner, noun, verb, preposition, noun), respectively. A strong form of the underlying assumption here is that a model is \"representing\" a property if and only if this property can be consistently predicted from embeddings (e.g., if probes can achieve high validation accuracy w.r.t. to the property in question). For example, an early and influential argument in structural probing, the pipeline hypothesis, uses probe accuracies over linguistic tasks across BERT [75] layers to argue that BERT processes linguistic properties in the same order as the \"classical NLP pipeline\", with surface-level features recognized first, followed by syntactic features, and semantic features recognized last [11, 76\u201378].\n3.2.1 Assumptions and Challenges\nLevels of Representation In order to carry out structural probing research, one must first define the class of representations to be probed \u2013 or equivalently, the architecture of the probe being trained (e.g., linear probes can only detect properties that are linearly-encoded). The question of which probing architecture should be utilized is a contentious one [12, 79, 80]; and below, we outline several choices of probing architecture as utilized in the literature, supporting arguments in favor of each architecture, and corresponding limitations.\nLinear Probes Perhaps the most well-studied probing architecture is the linear probe [16, 81\u201389]. Use of such probes assumes the linear subspace hypothesis: that property representations are encoded by linear embedding subspaces [90, 91]. For instance, Concept Activation Vectors (CAV) [82] are vectors in the representation space perpendicular to a linear classification boundary that classifies the representations of a concept versus other input representations. One argument in favor of linear probing is the intuition that neural classifiers must make class-discriminative information linearly separable in their final embedding layer, so probes (particularly over final-layer embeddings) should also be linear [81]. Another motivation is that, given enough training data, sufficiently expressive probes can memorize arbitrary probe tasks irrespective of the model being probed [79], so"}, {"title": "3.3 Causal Probing", "content": "Framework One proposed solution to structural probing's inability to distinguish correlation from causation problem is causal probing, where interventions are performed over representations detected by (structural) probes, and the resulting impact on model behaviors is measured in order to study how these properties are used by the model [16, 17, 102]. For instance, amnesic probing [16] uses the iterative nullspace projection (INLP) algorithm [84] to remove all information that is linearly-predictive of a given target property from LLM embeddings, then performs this intervention in the model's forward pass to measure the impact of the removal operation on language modeling performance across a large text corpus to broadly estimate and compare the model's use of various target properties.\n3.3.1 Assumptions and Challenges\nInherited from Structural Probing As in structural probing, causal probing requires one to pre-define the level of representation (neuron-level, linear, or nonlinear) and set of properties for which to probe before any probing experiment can begin. Most intervention methodologies are built to operate at only a single level of representation \u2013 typically linear [16, 84, 103, 104], with some work exploring kernelized linear representations [105, 106] - meaning that methods and results from one level cannot be directly adapted or compared to those from other levels. Alternative approaches have been defined using adversarial attacks against arbitrary probing architectures, allowing interventions to target whatever level of representation assumed by the probe [17, 107]; but these approaches come with fewer theoretical guarantees on potential collateral damage to non-targeted properties, as discussed below.\nCompleteness vs. Selectivity An important observation made by Elazar et al. [16] is that properties removed from earlier layers can be recoverable by later ones \u2013 i.e., when INLP is used to remove information about some target property from the embeddings of a given upstream layer, it is often still possible to train (linear) probes to predict the property from embeddings of a later downstream layer, meaning that these linear interventions are incomplete. For such properties, this finding may be taken as evidence against the linear subspace hypothesis discussed above: INLP removes all information that is linearly predictive of the target property, so if BERT only encoded these properties linearly, it would not be possible to recover them following an INLP intervention. Later works in causal probing have investigated nonlinear interventions [17, 105, 106], but as in structural probing, there is a tradeoff associated with expressivity: theoretically, the more powerful (and potentially more complete) an intervention is, the more \u201ccollateral damage", "110]": "when there are multiple explanations with equal causal efficacy (e.g., if removing information about property A and property B from the model leads to the same impact in its behavior), there is no way to determine which explanation is \u201ccorrect"}, {"title": "3.4 Dictionary Learning", "content": "As noted in Sections 3.2 and 3.3, one of the key limitations of both structural and causal probing is that they are fully supervised, meaning that one must pre-define a set of latent properties for which to probe. This means that, even for a perfect (causal) probing methodology, it is always possible that the most important properties leveraged by the model in the course of performing a particular task could be completely missed if one simply does not probe for these particular properties [11, 12, 97]. This is a serious concern for semantic interpretation, given that we cannot reasonably presume to know ahead of time what complete set of properties may be represented and leveraged by a model in any given context.\nDictionary Learning An alternative paradigm in semantic interpretation, dictionary learning, removes this presumption by inverting the traditional probing process: instead of directly training a supervised probe to predict some latent property of interest from a model's intermediate embedding representations, the goal of dictionary learning is to train an unsupervised probe to decompose embeddings into a sparse combination of features and"}, {"title": "4 Algorithmic Interpretation", "content": "We may define algorithmic interpretation as a matter of answering the following questions: what operations is a given neural network implicitly performing (over representations); what role do these operations play in observed behaviors; and, when these operations are taken in aggregate, what algorithm is being implicitly implemented by the model? Such operations and algorithms are typically understood in terms of circuits: \"sub-graphs of the network\" (weights) that implement a given operation or algorithm, which may be themselves composed to form larger circuits [120]. In principle, circuits can refer to any sub-graph of a neural network; but algorithmic interpretation studying Transformer-based models (the architecture of most modern LLMs and many other foundation models) typically studies circuits at the level of individual attention heads, or compositions of such [14, 46].\n4.1 Circuit Discovery\nCircuit Discovery The task of finding circuits that faithfully describe a given category of model behaviors is often referred to as circuit discovery [15, 46]. The simplest circuits to understand are end-to-end circuits, which describe a full path (composed of sub-circuits) from input to output, implementing a complete algorithm. For instance, one of the first circuits identified in a Transformer language model is the induction circuit [14, 121]: an end-to-end circuit which, given an input of the form \u201c[A] [B] ... [A]\u201d (where [A] and [B] are arbitrary token sequences), determines whether or not to predict that [B] once again follows the second [A] (as it did earlier in the sequence). For instance, given the input \u201cVernon Dursley and Petunia Durs", "Durs\" token with \"#ley\". (See Conmy et al. [15] for additional examples of end-to-end circuits that have been discovered in LLMs.)\nCausal Interventions As in causal probing and dictionary learning above, a popular strategy in circuit discovery is to explain a circuit's contribution to a model's behavior by intervening in its forward pass and study the effect on the model's predictions. There are two popular strategies for doing so, knockouts and patching, which we discuss in turn below.\nKnockout Knockouts \u201cturn off": "sub-)circuit by nullifying it in order to determine its contribution to LLM behaviors. Given a circuit, a few approaches to performing knockout have been proposed: zero ablation sets the output of the analyzed circuit to zero [121]; whereas mean ablation instead sets its output to the mean value of a given reference distribution [46]. The former approach is simpler, as it does not require one to define a reference distribution; but as subsequent circuits may \"rely on activation value[s] as an implicit bias term\" [46], it is theoretically more sound (and empirically less noisy) to perform knockout by setting to the mean value of a reference distribution where possible."}, {"title": "5 Towards Unifying Semantic and Algorithmic Interpretation", "content": "Finally, a few theoretical frameworks have been proposed to simultaneously account for the role of representations and algorithms in model behaviors [17, 18] \u2013 i.e., to unify semantic interpretation and algorithmic interpretation under a common framework. Geiger et al. [18] proposes a causal abstraction formalism to interpret neural network behaviors in terms of an underlying graphical causal model, where representations are nodes in the causal model and operations over these representations are edges, and measures the faithfulness of the causal model as an explanation of the neural network by computing the instance-level alignment between the network and the causal model under interchange interventions [125]; and Davies et al. [17] reformulates this framework at the level of concepts instead of individual instances, extending it to concept-level interventions such as those in causal probing or dictionary learning. Several algorithms have been proposed to empirically deploy causal abstraction frameworks to explain the behavior of large-scale neural networks such as LLMs by learning linear rotational interventions over embedding spaces to perform interchange interventions [126, 127] or substituting interchange interventions for gradient-based attacks against nonlinear causal probes [17]. However, to date, each of these works exhibit serious empirical limitations: they either (1) consider only small-scale, simplified \u201ctoy\u201d models and tasks [18, 126]; or (2) intervene only in a single neural network layer [17, 127]; meaning that their ability to describe how real-world models carry out operations that transform representations from layer to layer - i.e., to accomplish algorithmic interpretation \u2013 has not yet been empirically demonstrated."}, {"title": "6 Conclusion", "content": "In this work, we discussed the parallels between such work and research trends in the history of cognitive science, including Marr's levels of analysis and the cognitive revolution that birthed the modern cognitive sciences. We explored the relationship between two broad categories of deep learning interpretability research, semantic interpretation (what latent properties are represented) and algorithmic interpretation (what operations are performed over representations), highlighting the importance of causal analysis across both categories in delivering faithful explanations of model behavior. For each category, we surveyed salient works, analyzed their comparative strengths and weaknesses, clarified underlying assumptions, and outlined key challenges; and we discussed the relationships between these categories of work, focusing on the parallel challenges they each face and how they can compliment each other, concluding by outlining recent efforts to provide a unified framework that can account for both semantic and algorithmic interpretation. Our goal is to facilitate more open, productive conversation of deep learning interpretation by providing a common lexicon of goals, assumptions, and challenges associated with different modes of interpretation, stimulating further research toward more rigorous neural network interpretation, and informing current discourse by consulting lessons from the cognitive sciences."}, {"title": "A Supplementary Background", "content": "A.1 Interpretable Machine Learning\nMany definitions of \u201cinterpretability\" have been proposed to describe methods that fall outside our analysis in this work [22, 24, 128, 129] concerning either (1) interpretability as an inherent feature of human-understandable methods, or (2) input-output explanations (e.g., to what input features is a given output attributable) of supervised deep learning models. In contrast, in this work we are concerned with the study of internal mechanisms and latent representations learned by self-supervised (foundation) models that has come to characterize much of the inter-\"\n    }"}]}