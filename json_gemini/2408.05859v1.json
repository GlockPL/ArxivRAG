{"title": "The Cognitive Revolution in Interpretability: From Explaining Behavior to Interpreting Representations and Algorithms", "authors": ["Adam Davies", "Ashkan Khakzar"], "abstract": "Artificial neural networks have long been understood as \"black boxes\": though we know their computation graphs and learned parameters, the knowledge encoded by these weights and functions they perform are not inherently interpretable. As such, from the early days of deep learning, there have been efforts to explain these models' behavior and understand them inter-nally; and recently, mechanistic interpretability (MI) has emerged as a distinct research area studying the features and implicit algorithms learned by foundation models such as large language models. In this work, we aim to ground MI in the context of cognitive science, which has long struggled with analogous questions in studying and explaining the behavior of \"black box\" intelligent systems like the human brain. We leverage several important ideas and developments in the history of cognitive science to disentangle divergent objectives in MI and indicate a clear path forward. First, we argue that current methods are ripe to facilitate a transition in deep learning interpretation echoing the \"cognitive revolution\" in 20th-century psychology that shifted the study of human psychology from pure behaviorism toward mental representations and processing. Second, we propose a taxonomy mirroring key parallels in computational neuroscience to describe two broad categories of MI research, semantic interpretation (what latent representations are learned and used) and algorithmic interpretation (what operations are performed over representations) to elucidate their divergent goals and objects of study. Finally, we elaborate the parallels and distinctions between various approaches in both categories, analyze the respective strengths and weaknesses of representative works, clarify underlying assumptions, outline key challenges, and discuss the possibility of unifying these modes of interpretation under a common framework.", "sections": [{"title": "1 Introduction", "content": "How can we understand, interpret, and explain the behavior of complex intelligent biological systems like humans, chimpanzees, dolphins, or (at least seemingly) intelligent AI systems like large language models (LLMs)? This question lies at the heart of cognitive science, and different answers have defined entire paradigms in member disciplines such as psychology, neuroscience, and linguistics. For instance, in the 1950s and early 1960s, the dominant paradigm in academic psychology was behaviorism [1], resting on the fundamental assumption that human behavior can be fully understood and explained in terms of stimulus-and-response mechanisms [2, 3], with no need to consider humans' internal mental states, representations, or processing. However, behavior-ism failed to account for many key facets of human psychology, including language acquisition [3, 4], concept"}, {"title": "2 Background", "content": "2.1 A Brief History of Deep Learning Interpretation\nSince their introduction, neural networks have been considered \"black boxes,\u201d meaning they are not inherently interpretable. The research on deep learning interpretation is concerned with casting light on the \"black box\" problem in some form or another. In the era of deep learning preceding the development of foundation models (i.e., neural networks pre-trained on large-scale, self-supervised tasks such as LLMs [19]), the terms interpretation and explanation most often referred to saliency (feature attribution) methods. However, following the paradigm shift toward foundation models, these terms now more often refer to mechanistic interpretation. In this section, we provide a broad overview of both periods and associated paradigms in deep learning interpretation.\nBehaviorist Interpretation: The Rise of Post-hoc Explanations The paradigms of behaviorist and mechanis-tic interpretation of neural networks emerged concurrently, shortly following the modern study of deep learning. For instance, early work in deep learning interpretation discussed explaining the network behavior by identifying the importance of input features for the output, coining the term \"saliency maps\" [20]; and contemporaneous work studied representations by analyzing different neuron activations within convolutional neural networks [21]. However, the focus on post-hoc behavioral explanations \u2013 i.e., explaining why a given model produced a specific outputs [22\u201324] \u2013 gained more traction in the context of the then-dominant paradigm of classification and supervised learning, particularly in the forms of feature attribution methods and counterfactual explanations. Specifically, feature attribution [25\u201333] analyzes the behavior of the network by identifying input features that are relevant for that output; and counterfactual explanations [34\u201336] understand the behavior of the network by answering what needs to change in the input for the network to have a particular output. We highlight that"}, {"title": "Mechanistic Interpretation: The Cognitive Revolution in Interpretability", "content": "With the rise of generative and self-supervised learning paradigms that have enabled increasingly powerful and task-general foundation mod-els, the focus has shifted toward questions regarding what these models actually learn in pre-training and what implicit algorithms they perform internally, rather than why a model generated a particular output in a down-stream task. Such work is often referred to as mechanistic interpretability, which is generally defined as the subfield of interpretability research concerned with \u201creverse engineer[ing] neural networks, similar to how one might reverse engineer a compiled binary computer program\" [37]. This description clearly covers implicit algorithms and individual constituent operations, as studied in circuit discovery (see Section 4); but it is less clear precisely how mechanistic interpretability relates to the study of latent representations of human-interpretable concepts, particularly when specific concepts of interest are provided in advance of empirical analysis (as in probing; see Sections 3.2 and 3.3), rather than dynamically discovered from embedding representations (as in dictionary learning; see Section 3.4). In order to avoid conflating these related but fundamentally distinct notions of interpretability and clarify the specific object of analysis for each family of work, we define a taxonomy of interpretation methods according to the study of latent representations (semantic interpretation; see Section 3) and operations/algorithms (algorithmic interpretation; see Section 4), and discuss the possibility of a unified interpretation framework integrating both perspectives in Section 5.\nComplementary Paradigms It is important to recognize that mechanistic interpretation and behavioral expla-nation are complementary rather than competing. For instance, understanding why a given deep neural network produced a certain output remains crucial, especially in safety-critical domains such as healthcare [38\u201340]; and monitoring the latent knowledge and capabilities of frontier models has key implications for AI safety [41, 42]. Such directions are deeply intertwined and should continue to inform each other, as they have throughout the history of interpretability. For instance, internal representations have been leveraged for feature attribution ex-planations in Class Activation Mapping (CAM) [43], where neuron activations are used to explain which input features are relevant to the output; and subnetwork analysis (which is closely related to circuits; see Section 4) has worked to explain behaviors in terms of input features by finding and ablating sparse internal pathways through the network [33, 44]. Most recently, causal probing [16, 17] and dictionary learning [13, 45] have been leveraged for explaining the behavior of LLMs in terms of interpretable latent features (see Sections 3.3 and 3.4, respec-tively), and circuit discovery has been applied to uncover the interpretable subnetworks of LLMs that correspond to certain behaviors [15, 46] (see Section 4). This interplay mirrors parallel approaches to understanding human intelligence as observed between behavioral and cognitive modes of analysis: just as cognitive neuropsychology can be invaluable in explaining real-world behaviors and pathologies, mechanistic interpretations can inform our understanding of model outputs, and vice versa. Both perspectives are essential for a holistic understanding of neural networks."}, {"title": "2.2 Levels of Analysis", "content": "Marr's levels of analysis [47] have long been a workhorse foundational framework for scientific inquiry in neu-roscience and other disciplines of cognitive science [48]. They are as follows:\n\u2022 The level of computational theory provides a mathematical description of the goal of an information-processing system in terms of the desired transformation from inputs to outputs.\n\u2022 The level of representations and algorithms is concerned with how the system represents inputs and outputs, and the algorithm it employs to carry out the transformation described by the computational theory.\n\u2022 The level of implementation is concerned with how representations and algorithms are realized in a physical or software medium."}, {"title": "3 Semantic Interpretation", "content": "We may define semantic interpretation as a matter of answering the following question: what latent properties are learned and represented by neural networks, and how do they contribute to observed model behaviors? For example, do vision models learn representations of semantically-related object categories or spatial relations; or do LLMs learn representations of syntactic dependencies or lexical relations?\n3.1 Optimization and Search\nWhat inputs maximize the activation of a given neuron? Given that individual neurons are the atomic level of representation for all neural networks, a natural question is, to what extent do single neurons code for distinct properties of interest? An analogous hypothesis in neuroscience is the notion of \u201cgnostic cells\u201d, cells (neurons) which fire only in the context of very specific concepts, such as one's grandmother (hence the name) [52, 53]. Do we find such neurons in contemporary deep learning systems? The most prominent approach to neuron-level semantic interpretation involves selecting an individual neuron and either searching through a pre-defined query set or generating an input (such as an image) that maximally activates the target neuron, and inspecting these optimized inputs for common features. Early work in this area focused on image recognition networks [21, 54\u201356], where some approaches operate by searching for patterns that maximize the output of neurons via image optimization [20, 21, 57, 58], and others searched through a pre-defined query set to find inputs that maximized neuron activations [54, 59\u201361]. For instance, Net2Vec [59] and Network Dissection [60, 61] systematically analyze the relationship between concepts and neurons by analyzing the activations of neurons in response to images in the dataset clustered by human-annotated visual concepts. Later work has investigated how the same paradigm can be applied to interpret individual neurons in language models [62-65] and multimodal vision-language models [66, 67].\n3.1.1 Assumptions and Challenges\nLevels of Representation Most \"optimization and search\"-based methods operate at the level of individual neurons, which comes with serious limitations: neural networks encode distributed representations where fea-tures are encoded by multiple neurons; and even small-scale, simplified \u201ctoy\" models have also been found to"}, {"title": "3.2 Structural Probing", "content": "Framework One of the most popular and well-studied approaches to has been structural probing [11, 12, 74]. The goal of structural probing is to train auxiliary classifiers (probes) to predict discrete latent properties of inputs from model embeddings. For example, one may train a probe to predict parts-of-speech from LLM token embeddings, so when given each token in the sequence (The, cat, meows, for, dinner), the probe predicts the corresponding parts of speech (determiner, noun, verb, preposition, noun), respectively. A strong form of the underlying assumption here is that a model is \"representing\" a property if and only if this property can be consistently predicted from embeddings (e.g., if probes can achieve high validation accuracy w.r.t. to the property in question). For example, an early and influential argument in structural probing, the pipeline hypothesis, uses probe accuracies over linguistic tasks across BERT [75] layers to argue that BERT processes linguistic properties in the same order as the \"classical NLP pipeline\", with surface-level features recognized first, followed by syntactic features, and semantic features recognized last [11, 76\u201378].\n3.2.1 Assumptions and Challenges\nLevels of Representation In order to carry out structural probing research, one must first define the class of representations to be probed \u2013 or equivalently, the architecture of the probe being trained (e.g., linear probes can only detect properties that are linearly-encoded). The question of which probing architecture should be utilized is a contentious one [12, 79, 80]; and below, we outline several choices of probing architecture as utilized in the literature, supporting arguments in favor of each architecture, and corresponding limitations.\nLinear Probes Perhaps the most well-studied probing architecture is the linear probe [16, 81\u201389]. Use of such probes assumes the linear subspace hypothesis: that property representations are encoded by linear embedding subspaces [90, 91]. For instance, Concept Activation Vectors (CAV) [82] are vectors in the representation space perpendicular to a linear classification boundary that classifies the representations of a concept versus other input representations. One argument in favor of linear probing is the intuition that neural classifiers must make class-discriminative information linearly separable in their final embedding layer, so probes (particularly over final-layer embeddings) should also be linear [81]. Another motivation is that, given enough training data, sufficiently expressive probes can memorize arbitrary probe tasks irrespective of the model being probed [79], so"}, {"title": "3.3 Causal Probing", "content": "Framework One proposed solution to structural probing's inability to distinguish correlation from causa-tion problem is causal probing, where interventions are performed over representations detected by (structural) probes, and the resulting impact on model behaviors is measured in order to study how these properties are used by the model [16, 17, 102]. For instance, amnesic probing [16] uses the iterative nullspace projection (INLP) algorithm [84] to remove all information that is linearly-predictive of a given target property from LLM embed-dings, then performs this intervention in the model's forward pass to measure the impact of the removal operation on language modeling performance across a large text corpus to broadly estimate and compare the model's use of various target properties.\n3.3.1 Assumptions and Challenges\nInherited from Structural Probing As in structural probing, causal probing requires one to pre-define the level of representation (neuron-level, linear, or nonlinear) and set of properties for which to probe before any probing experiment can begin. Most intervention methodologies are built to operate at only a single level of representation \u2013 typically linear [16, 84, 103, 104], with some work exploring kernelized linear representations [105, 106] - meaning that methods and results from one level cannot be directly adapted or compared to those from other levels. Alternative approaches have been defined using adversarial attacks against arbitrary probing architectures, allowing interventions to target whatever level of representation assumed by the probe [17, 107]; but these approaches come with fewer theoretical guarantees on potential collateral damage to non-targeted properties, as discussed below.\nCompleteness vs. Selectivity An important observation made by Elazar et al. [16] is that properties removed from earlier layers can be recoverable by later ones \u2013 i.e., when INLP is used to remove information about some target property from the embeddings of a given upstream layer, it is often still possible to train (linear) probes to predict the property from embeddings of a later downstream layer, meaning that these linear interventions are incomplete. For such properties, this finding may be taken as evidence against the linear subspace hypothesis discussed above: INLP removes all information that is linearly predictive of the target property, so if BERT only encoded these properties linearly, it would not be possible to recover them following an INLP intervention. Later works in causal probing have investigated nonlinear interventions [17, 105, 106], but as in structural probing, there is a tradeoff associated with expressivity: theoretically, the more powerful (and potentially more complete) an intervention is, the more \u201ccollateral damage\" it may also cause to representations more generally [108], in which case the intervention is less selective in restricting damage to only the target property. Thus, for the most general intervention methodologies (such as those proposed by Davies et al. [17] and Tucker et al. [107], which can be used to manipulate representations detected by any differentiable probe), it is difficult to determine whether any observed changes to model behavior in the presence of interventions is attributable to the model's representation of the target property or simply to collateral damage (i.e., low selectivity).\nRashomon Effect Finally, as in most studies of causality, causal probing introduces the Rashomon effect [109, 110]: when there are multiple explanations with equal causal efficacy (e.g., if removing information about property A and property B from the model leads to the same impact in its behavior), there is no way to determine which explanation is \u201ccorrect\" (e.g., we cannot say whether the model's representation of A or B is responsible its behavior) [111]."}, {"title": "3.4 Dictionary Learning", "content": "As noted in Sections 3.2 and 3.3, one of the key limitations of both structural and causal probing is that they are fully supervised, meaning that one must pre-define a set of latent properties for which to probe. This means that, even for a perfect (causal) probing methodology, it is always possible that the most important properties leveraged by the model in the course of performing a particular task could be completely missed if one simply does not probe for these particular properties [11, 12, 97]. This is a serious concern for semantic interpretation, given that we cannot reasonably presume to know ahead of time what complete set of properties may be represented and leveraged by a model in any given context.\nDictionary Learning An alternative paradigm in semantic interpretation, dictionary learning, removes this presumption by inverting the traditional probing process: instead of directly training a supervised probe to predict some latent property of interest from a model's intermediate embedding representations, the goal of dictionary learning is to train an unsupervised probe to decompose embeddings into a sparse combination of features and"}, {"title": "3.4.1 Assumptions and Challenges", "content": "Levels of Representation: Superposition As with structural and causal probing above, it is necessary to define the level of representation (e.g., neuron-level, linear, or nonlinear) as the target of analysis. Formally, SAEs (as formulated above) are an unsupervised nonlinear probe that project embeddings onto an overcomplete linear basis, and thus fall somewhere in-between the linear and nonlinear levels of representation. Instead, SAEs follow the superposition hypothesis [68], which argues that models learn to represent more features than they have neurons in a given layer by encoding them via almost-orthogonal directions in the embedding space, expanding the number of features that can be represented in the embedding space at the cost of some noise due to interference between non-orthogonal features. According to this hypothesis, models leverage superposition because the benefits of representing (potentially many) more features with fewer neurons outweighs the cost of filtering out this noise (via nonlinear activation functions) so long as features are sufficiently sparse (leading to less interference on average) [68, 118].\nUnsupervised Feature Interpretation While SAEs (and dictionary learning more broadly) remove the need for labeled training data for supervised probes, they effectively transfer the burden of annotation from probe training data to unsupervised feature interpretation, as each feature vector in the dictionary cannot be directly interpreted any more easily than dense embeddings themselves. Rather, each feature must be retroactively in-terpreted, usually by asking an annotator (either a human or an LLM [119]) to inspect the input samples that maximally activate the feature and annotating it according to whatever feature these samples intuitively appear to have in common [13, 45, 116]. For instance, in the largest SAE study to date (including up to 34 million SAE features learned from embeddings of a frontier LLM), Templeton et al. [13] find that one feature learned by an unsupervised probe over a large multilingual, multimodal vision-language model corresponds to the Golden Gate Bridge, and that this feature is strongly activated in contexts discussing the bridge in many different languages or for images depicting the bridge, while being weakly activated in contexts discussing (or images depicting) other tourist landmarks in San Francisco or other famous bridges. While such features are indeed interesting, it is not clear what proportion of features have such clear interpretations \u2013 how many of these millions of features are as easily interpreted as \"Golden Gate Bridge\u201d? At such a large scale, it is not feasible for human annotators to manually interpret features, requiring automated interpretation \u2013 e.g., prompting another LLM to explain the relationship between multiple passages that highly activate the same feature, a scalable approach that has been shown to be reasonably well-aligned with human judgments [119]. However, it is important to note that, in contrast to supervised probing methods (where target properties are labeled in advance), unsupervised feature interpretations (whether human- or LLM-annotated) are not transferable in that this process must be performed each time one analyzes a new model, layer, or trains a new SAE \u2013 that is, where probing datasets only require that each input (or parts of inputs, such as individual tokens) be labeled once, dictionary learning requires that one (re-)interpret learned features every time a new dictionary is learned, significantly increasing the burden of annotation and preventing direct comparisons between different models, layers, or dictionary learning methods."}, {"title": "4 Algorithmic Interpretation", "content": "We may define algorithmic interpretation as a matter of answering the following questions: what operations is a given neural network implicitly performing (over representations); what role do these operations play in observed behaviors; and, when these operations are taken in aggregate, what algorithm is being implicitly implemented by the model? Such operations and algorithms are typically understood in terms of circuits: \"sub-graphs of the network\" (weights) that implement a given operation or algorithm, which may be themselves composed to form larger circuits [120]. In principle, circuits can refer to any sub-graph of a neural network; but algorithmic inter-pretation studying Transformer-based models (the architecture of most modern LLMs and many other foundation models) typically studies circuits at the level of individual attention heads, or compositions of such [14, 46].\n4.1 Circuit Discovery\nCircuit Discovery The task of finding circuits that faithfully describe a given category of model behaviors is often referred to as circuit discovery [15, 46]. The simplest circuits to understand are end-to-end circuits, which describe a full path (composed of sub-circuits) from input to output, implementing a complete algorithm. For instance, one of the first circuits identified in a Transformer language model is the induction circuit [14, 121]: an end-to-end circuit which, given an input of the form \u201c[A] [B] ... [A]\u201d (where [A] and [B] are arbitrary token sequences), determines whether or not to predict that [B] once again follows the second [A] (as it did earlier in the sequence). For instance, given the input \u201cVernon Dursley and Petunia Durs", "turn off\" a (sub-)circuit by nullifying it in order to determine its contribution to LLM behaviors. Given a circuit, a few approaches to performing knockout have been proposed: zero ablation sets the output of the analyzed circuit to zero [121]; whereas mean ablation instead sets its output to the mean value of a given reference distribution [46]. The former approach is simpler, as it does not require one to define a reference distribution; but as subsequent circuits may \"rely on activation value[s] as an implicit bias term\" [46], it is theoretically more sound (and empirically less noisy) to perform knockout by setting to the mean value of a reference distribution where possible.\"\n    },\n    {\n      \"title\": \"4.1.1 Assumptions and Challenges\",\n      \"content\": \"Circuit Architecture As in semantic interpretation, circuit discovery requires one to target a specific level at which to interpret models \u2013 for instance, in probing, this is determined by the expressivity of the selected probe architecture; but in circuit discovery, one must decide on the atomic (smallest-scale) sub-graphs considered as possible features or (sub-)circuits. For example, Olah et al. [120] starts at the level of individual neuron activations; but given the intractability of considering all neurons in larger models, more recent work targeting LLMs has begun at the level of attention heads [15, 46]. While such tradeoffs are necessary in order to study models at scale, it is important to remember that any operations or algorithms implemented by sub-graphs below or outside the scope of the atomic level \u2013 e.g., any analysis of Transformer models built exclusively on attention heads will miss sub-circuits implemented by MLPs [64, 72].\nOne-to-One Mapping Another important assumption in circuit discovery is that atomic sub-graphs are under-stood as performing one and only one operation. Given that even the simplest of models are known to often represent multiple properties using the same neuron [68], the assumption that single neurons of LLM architec-tures can be neatly discretized into individual, atomic operations is suspect; and larger sub-graphs can be difficult to precisely localize and may contain redundant elements [122]. This assumption can be somewhat attenuated with the interventions discussed above, as they can be used to test the extent to which knocking out or patching a given sub-graph leads to the behavior predicted by the hypothesized circuit, evaluating whether or not the sub-graph actually performs the indicated operation. However, this process only solves part of the problem: while it can test whether or not the sub-graph is indeed involved in implementing the observed behavior, it cannot determine whether there are other sub-graphs that may also have a similar effect [111, 123]; and in some cases, knocking out randomized circuits can have a similar effect as knocking out circuits that are precisely calibrated to the target behavior [122].\nPre-Specification Another important limitation of current circuit discovery work is that, to our knowledge, all circuits that have so far been identified in real-world LLMs have required pre-specifying a full algorithmic description before they can be found. While this significantly reduces computational complexity and thus allows discovery to scale to LLMs [15], it also means that such discovery can generally only move \u201ctop-down": "in this case, one cannot discover a circuit that implements an algorithm which has not been explicitly specified ahead of time, and will be less likely to discover intermediate sub-graphs that do not already fit into pre-specified algorithms (a more \u201cbottom-up\u201d approach that has been employed in smaller-scale \u201ctoy model\u201d investigations [14, 123, 124]).\nRashomon Effect Finally, perhaps the greatest challenge in circuit discovery is that it is possible to yield multiple distinct circuit descriptions for the same LLM behavior depending on how circuit analysis is carried out [111, 123]. (another instance of the Rashomon effect discussed above). For instance, Zhong et al. [123] explore an"}, {"title": "5 Towards Unifying Semantic and Algorithmic Interpretation", "content": "Finally, a few theoretical frameworks have been proposed to simultaneously account for the role of representa-tions and algorithms in model behaviors [17, 18] \u2013 i.e., to unify semantic interpretation and algorithmic inter-pretation under a common framework. Geiger et al. [18] proposes a causal abstraction formalism to interpret neural network behaviors in terms of an underlying graphical causal model, where representations are nodes in the causal model and operations over these representations are edges, and measures the faithfulness of the causal model as an explanation of the neural network by computing the instance-level alignment between the network and the causal model under interchange interventions [125]; and Davies et al. [17] reformulates this framework at the level of concepts instead of individual instances, extending it to concept-level interventions such as those in causal probing or dictionary learning. Several algorithms have been proposed to empirically deploy causal abstraction frameworks to explain the behavior of large-scale neural networks such as LLMs by learning linear rotational interventions over embedding spaces to perform interchange interventions [126, 127] or substituting interchange interventions for gradient-based attacks against nonlinear causal probes [17]. However, to date, each of these works exhibit serious empirical limitations: they either (1) consider only small-scale, simplified \u201ctoy\u201d models and tasks [18, 126]; or (2) intervene only in a single neural network layer [17, 127]; meaning that their ability to describe how real-world models carry out operations that transform representations from layer to layer - i.e., to accomplish algorithmic interpretation \u2013 has not yet been empirically demonstrated."}, {"title": "6 Conclusion", "content": "In this work, we discussed the parallels between such work and research trends in the history of cognitive sci-ence, including Marr's levels of analysis and the cognitive revolution that birthed the modern cognitive sciences. We explored the relationship between two broad categories of deep learning interpretability research, semantic interpretation (what latent properties are represented) and algorithmic interpretation (what operations are per-formed over representations), highlighting the importance of causal analysis across both categories in delivering faithful explanations of model behavior. For each category, we surveyed salient works, analyzed their compara-tive strengths and weaknesses, clarified underlying assumptions, and outlined key challenges; and we discussed the relationships between these categories of work, focusing on the parallel challenges they each face and how they can compliment each other, concluding by outlining recent efforts to provide a unified framework that can account for both semantic and algorithmic interpretation. Our goal is to facilitate more open, productive con-versation of deep learning interpretation by providing a common lexicon of goals, assumptions, and challenges associated with different modes of interpretation, stimulating further research toward more rigorous neural net-work interpretation, and informing current discourse by consulting lessons from the cognitive sciences."}, {"title": "A Supplementary Background", "content": "A.1 Interpretable Machine Learning\nMany definitions of \u201cinterpretability\" have been proposed to describe methods that fall outside our analysis in this work [22, 24, 128, 129] concerning either (1) interpretability as an inherent feature of human-understandable methods, or (2) input-output explanations (e.g., to what input features is a given output attributable) of supervised deep learning models. In contrast, in this work we are concerned with the study of internal mechanisms and latent representations learned by self-supervised (foundation) models that has come to characterize much of the inter-"}]}