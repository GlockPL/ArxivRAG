{"title": "Amortized In-Context Bayesian Posterior Estimation", "authors": ["Sarthak Mittal", "Niels Leif Bracher", "Guillaume Lajoie", "Priyank Jaini", "Marcus Brubaker"], "abstract": "Bayesian inference provides a natural way of incorporating prior beliefs and assigning a probability measure to the space of hypotheses. Current solutions rely on iterative routines like Markov Chain Monte Carlo (MCMC) sampling and Variational Inference (VI), which need to be re-run whenever new observations are available. Amortization, through conditional estimation, is a viable strategy to alleviate such difficulties and has been the guiding principle behind simulation-based inference, neural processes and in-context methods using pre-trained models. In this work, we conduct a thorough comparative analysis of amortized in-context Bayesian posterior estimation methods from the lens of different optimization objectives and architectural choices. Such methods train an amortized estimator to perform posterior parameter inference by conditioning on a set of data examples passed as context to a sequence model such as a transformer. In contrast to language models, we leverage permutation invariant architectures as the true posterior is invariant to the ordering of context examples. Our empirical study includes generalization to out-of-distribution tasks, cases where the assumed underlying model is misspecified, and transfer from simulated to real problems. Subsequently, it highlights the superiority of the reverse KL estimator for predictive problems, especially when combined with the transformer architecture and normalizing flows.", "sections": [{"title": "1. Introduction", "content": "Bayesian analysis of data has become increasingly popular and is widely used in numerous scientific disciplines. In politics, predictive models based on public polling and other factors play a crucial role in the discourse around the state of a campaign. Throughout the COVID-19 pandemic, models that estimate the infectiousness of the virus, the effi-cacy of public health measures, and the future course of the pandemic became critical to government planning and the public's understanding of the pandemic (Cooper et al., 2020). In cryogenic electron microscopy (cryo-EM), the posterior over an unknown 3D atomic-resolution molecular structure is explored given image observations (Glaeser et al., 2021).\nWhile recent years have made such methods more accessible (Bingham et al., 2019; Carpenter et al., 2017; \u0160trumbelj et al., 2023), they still remain computationally burdensome. Further, in practical contexts where new observations are continuously available, the analysis must be re-run every time new data becomes available, e.g., when new case counts become available, previous measurements are corrected, or when applied to different geographic regions. As a result practitioners adopt approximations (Welling & Teh, 2011; Gelfand, 2000; Brooks, 1998), simplify their models (Hoffman et al., 2013; Blei et al., 2017) or reduce the frequency with which they perform their analyses.\nA common thread is that the probabilistic model defining the relationship between its parameters and the observations is fixed. Poll aggregation models use hierarchical time series models (Athanasopoulos et al., 2023; Chen et al., 2023), infectious diseases are studied using variants on compartment models (Tang et al., 2020), and cryo-EM uses a linear image formation model (Glaeser et al., 2021). This makes these applications ideal candidates for amortized inference (Morris, 2013; Paige & Wood, 2016; Kingma & Welling, 2013; Rezende et al., 2014; Stuhlm\u00fcller et al., 2013).\nMultiple approaches leverage neural networks to learn functions that map an observed dataset directly to a posterior distribution (Garnelo et al., 2018b; Cranmer et al., 2020) or model the posterior predictive directly (Garnelo et al., 2018a; M\u00fcller et al., 2021; Garg et al., 2022; Hollmann et al., 2022). They sidestep the need for iterative procedures, e.g., Markov chain Monte Carlo (MCMC) sampling (Gelfand, 2000; Hoffman et al., 2014) or standard variational inference (VI) and efficiently handle permutation invariance stemming from iid observations using Transformers and DeepSets (Zaheer et al., 2017; Vaswani et al., 2017; Lee et al., 2019). If learned properly, this mapping allows generalization to new datasets passed in context in zero-shot.\nHowever, analysis into evaluating different in-context posterior estimation objectives is currently lacking. The goal"}, {"title": "2. Background", "content": "We first cover some of the important preliminaries below.\nBayesian Inference. Let x \u2208 Rd denote the outcome of an experiment observed through a set of independent and identically distributed (iid) samples D := {x1,..., XN} \u2286 Rd. Given these observations, we are interested in either quantifying the certainty of or generating potential future observations x*. Bayesian Inference provides a natural methodology of quantifying p(x*|D) by prescribing a space of hypotheses \u2208 Rk and a prior belief p(0) over it. These hypotheses define the likelihood of observing an outcome, i.e., p(x0). The likelihood and prior are then combined through Bayes rule to infer the posterior p(0|D), through which the quantity of interest can then be easily expressed as\np(x\u2217|D) = \u222b p(x\u2217|\u03b8)p(\u03b8|D)d\u03b8 (1)\nThis poses two challenges: (a) the posterior, often a quantity of interest in itself, is not known, and (b) the integration can be intractable which is often resolved through Monte Carlo estimation\np(x\u2217|D) = E\u03b8|D [p(x\u2217|\u03b8)] \u2248 1/M \u03a3 p(x\u2217|\u03b8(m)) (2)\nwhere \u03b8(m) ~ p(\u03b8|D). The quantity p(\u03b8|D) can be obtained through an application of Bayes rule\np(\u03b8|D) = p(D|\u03b8) p(\u03b8)/p(D) = p(\u03b8)/p(D) \u220fp(xn|\u03b8) (3)\nn=1\nGiven the form of the likelihood and prior, the above distribution is often difficult to sample from, especially with the added complexity of the marginal p(D) = \u222b \u03b8 p(D|\u03b8) p(\u03b8) being intractable. Additionally, the posterior itself is often of interest on its own, especially in cases where \u03b8 is interpretable, e.g. if we model the bias of a coin based on multiple tosses. We refer the readers to Bishop & Nasrabadi (2006) for additional applications of Bayesian Inference.\nApproximate Bayesian Inference. To bypass the in-tractability of the posterior distribution, or at least the difficulty to sample from it, approximate methods are used.\nSampling based methods provide ways of sampling from the true posterior distribution based on easy access to an un-normalized density function, e.g. rejection sampling. More advanced methods like MCMC construct a chain of updates 01, 02,... such that asymptotically the samples converge to samples from the true posterior. Such sampling methods rely on transition kernels T(0t+1|0t) and often some acceptance criteria A(0t+1, 0t), a key example of which is the Metropolis-Hastings algorithm. We refer the readers to (Hoffman et al., 2014; Welling & Teh, 2011) for a detailed analysis into different MCMC methods like Langevin and Hamiltonian Monte Carlo which rely on gradient of the log density as additional signal for better convergence.\nIn contrast, another class of methods approximate the true posterior with a parametric family q\u03c6(\u03b8) and convert the estimation problem into the following optimization problem\n\u03c6 \u2217 = arg min D (p(\u00b7|D), q\u03c6 (\u00b7)) (4)"}, {"title": "3. Posterior Estimation from Data in Context", "content": "As described earlier, standard in-context approaches are predominantly concerned with prediction and model the posterior predictive directly, taking D as input. However, they can be tweaked to perform posterior estimation instead. We discuss ways to train such an in-context estimator and showcase its connections to existing amortization methods.\nGiven any modeling assumption defined via a probabilistic model p(0) with parameters 0, we are interested in estimating the full Bayesian posterior over the parameters p(0|D) after obtaining some observations D, in a manner that allows fast and scalable approximation. Equations (5) and (7) showcase two different methodologies of performing posterior estimation, however, both the methods train a new q\u03c6 every time new observations D are obtained. However, such methods can be easily amortized by leveraging in-context learning with a goal towards posterior estimation instead of prediction, i.e. training a model to approximate the posterior distribution based on in-context examples. Mathematically, this is obtained by considering an approximate density q(D) which is explicitly conditioned on the set of observations D and trained over multiple such sets\n\u03c6 \u2217 = arg min ED\u223cx D (p(\u00b7|D), q\u03c6(\u00b7|D)) (9)\nwhere x denotes some distribution over observations D.\nIf the measure of divergence is the forward KL DF\u2212KL, it leads to the neural posterior estimation methodology of simulation-based inference (SBI-NPE) as long as an additional constraint is satisfied, i.e. x defines sampling from the assumed underlying model p, i.e.\nx(D) =\u222b [p(\u03b8) \u03a0 P(xn|\u03b8)d\u03b8 (10)\nn\u2208D\nThe importance of this constraint is that it leads to a simpler gradient-based optimization procedure as opposed to EP\n\u03c6F\u2212KL = arg min ED\u223cx E\u03b8\u223cp(\u00b7|D) [log p(\u03b8|D)/q\u03c6(\u03b8|D)] (11)\n= arg min ED\u223cx E\u03b8\u223cp(\u00b7|\u03b8) [\u2212 log q\u03c6(\u03b8|D)] (12)\nwhere we focus our attention to the change in expectations which is only possible when D is sampled according to p. This removes the requirement of sampling or evaluating the true posterior, a known hurdle with EP methods.\nIn contrast, one could also take motivation from VAEs and NP which predominantly work under the reverse KL divergence DR\u2212KL minimization. An amortized in-context learner in this setting can be mathematically formalized as\n\u03c6R\u2212KL = arg min ED\u223cx E\u03b8\u223cq\u03c6(\u00b7|D) [log q\u03c6(\u03b8|D)/p(\u03b8|D)] (13)\n= arg min ED\u223cx E\u03b8\u223cq\u03c6(\u00b7|D) [log p(D, \u03b8)/q\u03c6(\u03b8|D)] (14)\nIt is important to note that unlike forward KL, reverse KL provides the freedom of choosing any arbitrary x while still maintaining ease in training, i.e. the in-context estimator can be trained on datasets that come from a different distribution than p. Such flexibility is important because we"}, {"title": "4. Experiments", "content": "To provide a fair and comprehensive evaluation of the different estimators and modeling choices, we consider a variety of well-known probabilistic models encompassing supervised and unsupervised scenarios. In particular, we look at the problem of estimating the Bayesian posterior over the (a) mean of a Gaussian distribution (GM), (b) means of a Gaussian mixture model (GMM), (c) parameters of a (non-)linear regression model (NLR/LR), and (d) parameters of a (non-)linear classification model (NLC/LC). We refer the readers to Appendix C for particulars about the probabilistic models, including their likelihoods and priors considered.\nBaselines. We consider dataset-specific baselines to compare different amortized in-context posterior estimators with. In particular, we use the prior (Random), perform maximum likelihood estimation using gradient-based optimization (Optimization) as well as an approximate Bayesian inference procedure through Langevin and Hamiltonian based MCMC sampling. Such baselines rely on iterative proce-"}, {"title": "4.1. Zero-Shot Posterior Approximation", "content": "We first test the in-context estimators' ability to succeed at novel tasks solely at inference over q(\u00b7|D). To do so, we train the estimators on datasets being generated as Dtrain ~ p, and are then evaluated on new Dtest ~ p. Mathematically this is equivalent to setting x according to"}, {"title": "4.2. Generalizing to Variable Feature Dimensions", "content": "So far, we only considered amortization over datasets for the same underlying likelihood model, which fixes the dimensionality of the problem. For example, a different in-context estimator has to be trained for a 2-dimensional and 5-dimensional Bayesian linear regression model since the dimensionality of e changes. It is important to note that a deep learning-based approach leaves hopes of generalizing to new datasets of different dimensionalities since the underlying functional form of the solution remains constant across different datasets, irrespective of the number of features, and is given by the solution obtained from Equation 3.\nAlternatively, we can see that a low-dimensional problem can just be embedded into a high-dimensional space, with the extra features and parameters set to 0, akin to the proce-"}, {"title": "4.3. Model Misspecification", "content": "The true likelihood model underlying a data-generating process is often unknown. Practitioners address this by assuming a likelihood model and fitting its parameters to best explain the data. For example, while the true model for classifying emails as spam or not is unknown, one can assume a linear model to approximate the problem. This introduces model misspecification\u2014a mismatch between the assumed and true model.\nAs discussed in Section 3, forward KL methods train only on simulated data from the assumed model, whereas reverse KL methods can use real-world data. Consequently, forward KL approaches struggle with sim-to-real transfer because they cannot incorporate real data during training. In contrast, reverse KL methods leverage real data, leading to more robust predictions in practical settings.\nMathematically, let Xsim from Equation (10) denote simulated data and Xreal the actual target data."}, {"title": "4.4. Application to Tabular Benchmarks", "content": "To evaluate the efficacy of the trained in-context estimators and their ability to generalize out-of-distribution, we test the models trained in Section 4.2 on a suite of regression and classification problems chosen from the OpenML platform. These tasks have varying number of feature dimensions and inherently have different data statistics than the ones obtained from x during training. Shows the zero-shot performance of the parameters inferred from the in-context estimators, and highlights that they perform considerably better than chance, with transformer models and reverse KL methods outperforming other modeling choices.\nWe also look at finetuning the inferred parameters from the in-context estimators with a maximum-a-posteriori (MAP) objective and compare its performance with a corresponding model initialized from the prior."}, {"title": "4.5. Evaluating Posterior Quality", "content": "While comparing with the true posterior is hard due to its intractability, it is still available when estimating the mean of a Gaussian distribution or performing Bayesian Linear Regression. Figure 3 (Right) shows the kernel density estimate of the samples from the true posterior, amortized forward, and reverse KL model, showing that both estimators efficiently capture the true posterior. We further quantify it through the symmetric KL divergence in Table 5.\nFor more complex problems, we compute the squared Wasserstein metric W between samples from the amortized posterior and multiple chains of Langevin MCMC in Results indicate that reverse KL approaches do slightly better in high-dimensional setups, while for low-dimensional multi-modal scenarios (eg. GMM), forward KL approaches fare better. Importantly, we note that this metric only provides a crude proxy to the quality of the posterior, since MCMC methods only provide asymptotic guarantees."}, {"title": "5. Discussion and Conclusion", "content": "We show that Bayesian posterior inference can be amortized for a broad class of probabilistic models and explore a variety of design decisions associated with it. Some key conclusions from our analysis are described below."}, {"title": "Appendix", "content": "A. Related Work\nIn this section, we draw parallels of our work to various approaches that have been proposed to tackle the problem of either providing a good initialization for different tasks, performing implicit optimization to model predictive distributions for new tasks, or estimating the posterior through a different objective."}, {"title": "A.1. Variational Autoencoders", "content": "VAEs (Kingma & Welling, 2013; Rezende et al., 2014; Rezende & Mohamed, 2015; Kingma et al., 2019) are latent variable models which model observations \u00e6 conditioned on latent variables z through the joint distribution p\u03b8(x, z) = p\u03b8(x|z)p(z) where p(z) is generally chosen as N(0, I). Training the model is done through VI where q\u03c6(z) is obtained by explicit amortization over the data point, that is, q\u03c6(z|x) = \u039d (\u03bc\u03c6(x), \u03a3\u03c6(x)). Training this system on a dataset D is done by similarly optimizing the Evidence Lower-Bound, which boils down to the following optimization problem\narg max Ex\u223cD Ez\u223cq(\u00b7|x) [log p\u03b8 (x, z)] (17)\nThis objective can easily be optimized using gradient-based learning and the reparameterization trick. While typically, a diagonal Gaussian distribution is considered for q\u03c6, more complex distributions utilizing normalizing flows can also be used."}, {"title": "A.2. Hypernetworks", "content": "Hypernetworks are neural networks that generate weights for another neural network, used in tasks such as uncertainty quantification, zero-shot learning, etc. We refer for a comprehensive overview to (Chauhan et al., 2023). Based on experiments on predicting the weights of a compact MLP (section 4), our work shows similarities with studies in this area but also has significant differences. Regarding uncertainty quantification, hypernetworks are instrumental in creating an ensemble of models by generating multiple weight vectors for the primary network. Each model within this ensemble possesses distinct parameter configurations, enabling robust estimation of uncertainty in model predictions. This feature is precious in safety-critical domains like healthcare, where confidence in predictions is essential. Multiple weight sets can be generated through techniques like dropout within hypernetworks or sampling from a noise distribution. The latter (Krueger et al., 2017) is based on a Bayesian framework where weights can be sampled using invertible network architecture, such as normalizing flows. However, while we amortize posterior inference, the weights sampled from the hypernetwork are not conditioned on information from the currently observed input data during inference time but indirectly solely on the dataset available during training, and retraining would need to be done given a new dataset. Departing from the Bayesian framework, (Sun et al., 2017) have shown data-specific discriminative weight prediction, which aligns well with their specific objective of defending a convolutional neural network against adversarial attacks. Combining the ability to sample a new set of weights dataset-specifically but also handling dataset exchangeability, even in the more realistic case of missing information, our work has a distinctly different focus but also can be seen as an extension to hypernetwork research."}, {"title": "A.3. In-Context Learning", "content": "Amortized inference has close links to in-context learning (ICL), which has been gaining popularity, especially in natural language modeling. Various works show how in-context learning can be seen as performing implicit optimization based on the context examples, with some constructions showing exact equivalence with gradient descent in linear regression (Von Oswald et al., 2023; von Oswald et al., 2023). Other works have shown how such systems can be seen as implicitly modeling the Bayesian posterior predictive distribution (M\u00fcller et al., 2021). In a similar vein, there have been additional works aimed at directly modeling the posterior predictive distribution by providing the training data as \u201ccontext\u201d to a Transformer model and training it based on the maximum log-likelihood principle (Hollmann et al., 2022). While such approaches have been seeing tremendous success, they cannot be directly applied to cases where we care about and want to analyze the solution space as the solution space is only modeled implicitly, and thus, recovering it is not possible. For example, if our goal is to learn a linear regression model, an ICL model could end up learning a nonlinear model and would provide no information about the actual parameters used for prediction. As opposed to this, we obtain parameters explicitly. We thus can answer questions like the relevance of a particular feature (which corresponds to its weight in the output, and we know the weight vector explicitly). Even further, many systems grounded in physics and economics only admit a constrained solution space; for example, the movement of a human arm lies on a particular manifold, or the configuration of molecules and proteins"}, {"title": "A.4. Meta Learning", "content": "Meta-learning (Hospedales et al., 2022) aims to equip models with the ability to quickly learn from different tasks or data sets to generalize to new tasks in resource-constrained domains. This attribute is precious in practical scenarios where obtaining large amounts of task-specific data is impractical or costly. A simple way of obtaining this is through nonparametric or similarity-based models like k-Nearest Neighbours, where no training is involved. Thus, new tasks can be solved quickly based on a few examples by computing a similarity metric with these examples (Koch et al., 2015; Vinyals et al., 2016; Sung et al., 2018). Another way of achieving this is through optimization-based setups, which use a nested optimization procedure. An inner step learns individual tasks from a shared initialization, whereas the outer loop computes the gradient of the whole inner process and moves the initialization in a way that allows for better generalization. Here, by relying on only a few iterations in the inner loop, the outer loop has the incentive to move the initialization to a point from which solutions to multiple tasks are reachable (Finn et al., 2017). Given the similarities between meta-learning and hierarchical Bayesian inference (Grant et al., 2018), our approach can be considered as a kind of meta-learning framework; however, the line between meta-learning and Bayesian posterior inference is quite blurry as any amortized approach for the latter can be seen as a case of the former."}, {"title": "A.5. Neural Processes", "content": "A notable approach in meta-learning related to our research is neural processes (NP), which excel in learning scenarios with few examples. NPs (Garnelo et al., 2018a;b; Kim et al., 2019; Pakman et al., 2020; Gordon et al., 2019) can be seen as a more flexible and powerful extension of Gaussian processes that leverage a neural network-based encoder-decoder architecture for learning to model a distribution over functions that approximate a stochastic process. However, while we are interested in approximating the posterior distribution over the parameters, NPs are used to approximate the posterior predictive distribution to make predictions based on observed data. Similar to our setup, NPs rely on amortized VI for"}, {"title": "A.6. Simulation-Based Inference", "content": "In the case of simulation-based inference (Cranmer et al., 2020), when the likelihood p(x|0) is intractable, BayesFlow (Radev et al., 2020) and similar methods (Lorch et al., 2022) provide a solution framework to amortize Bayesian inference of parameters in complex models. Starting from the forward KL divergence between the true and approximate posteriors, the resulting objective is to optimize for parameters of the approximate posterior distribution that maximize the posterior probability of data-generating parameters @ given observed data D for all 0 and D. Density estimation of the approximate posterior can then be done using the change-of-variables formula and a conditional invertible neural network that"}, {"title": "A.7. Amortization in Gaussian Processes", "content": "Gaussian Processes (GPs) define a class of probabilistic models that do enjoy tractable likelihood. However, inference in such systems is slow and sensitive to the choice of kernel function that defines the covariance matrix. Similar to meta learning and neural processes, current research also focuses on estimating the kernel function in GPs by leveraging permutation invariant architectures like transformers (Liu et al., 2020; Simpson et al., 2021; Bitzer et al., 2023). Additionally, often these approaches amortize based on point estimates and are leveraged when considering GPs for regression problems, and it is not straightforward to extend them to classification or unsupervised learning. In contrast, our approach is more general and can work for all problems that define a differentiable likelihood function. Additionally, our approach also approximates the Bayesian posterior distribution over the parameters of interest, as opposed to point estimates."}, {"title": "A.8. Mode Collapse in Variational Inference", "content": "Reverse KL based methods have been widely known to suffer from mode collapse due to the nature of the optimization objective (Bishop & Nasrabadi, 2006), which implies that even if the approximate distribution possesses the ability to represent multiple modes, optimization is often sub-optimal and the distribution ends up covering only a small handful of"}, {"title": "B. Architectures respecting Exchangeability", "content": "In this section, we highlight how DeepSets and Transformer models satisfy the dataset exchangeability criteria, which is essential in modeling the posterior distribution over the parameters of any probabilistic model relying on iid data."}, {"title": "B.1. DeepSets", "content": "DeepSets (Zaheer et al., 2017) operate on arbitrary sets X = {x1,...XN} C Rd of fixed dimensionality d by first mapping each individual element xi \u2208 X to some high-dimensional space using a nonlinear transform, which is parameterized as a multi-layered neural network with parameters \u03c61\nzi = f\u03c61 (xi) (20)\nAfter having obtained this high-dimensional embedding of each element of the set, it applies an aggregation function a(\u00b7), which is a permutation invariant function that maps a set of elements Z = {z1, ..., zN} \u2208 R\u02dc to an element h \u2208 R\u02dc,\nh = a(Z) (21)\nThus, the outcome does not change under permutations of Z. Finally, another nonlinear transform, parameterized by a multi-layered neural network with parameters \u03c62, is applied to the outcome h to provide the final output.\no = g\u03c62 (h) (22)\nFor our experiments, we then use the vector o to predict the parameters of a parametric family of distributions (e.g., Gaussian or Flows) using an additional nonlinear neural network. As an example, for the Gaussian case, we consider the distribution"}, {"title": "B.2. Transformers", "content": "Similarly, we can look at Transformers (Vaswani et al., 2017) as candidates for respecting the exchangeability conditions in the data. In particular, we consider transformer systems without positional encodings and consider an additional [CLS] token, denoted by c\u2208 Rd, to drive the prediction. If we look at the application of a layer of transformer model, it can be broken down into two components.\nMulti-Head Attention. Given a query vector obtained from c and keys and values coming from our input set X C Rd, we can model the update of the context c as\n\u0109(X) = Softmax (cW Q WT X) XWV (27)"}, {"title": "C. Probabilistic Models", "content": "This section details the various candidate probabilistic models used in our experiments for amortized computation of Bayesian posteriors over the parameters. Here, we explain the parameters associated with the probabilistic model over which we want to estimate the posterior and the likelihood and prior that we use for experimentation.\nMean of Gaussian (GM): As a proof of concept, we consider the simple setup of estimating the posterior distribution over the mean of a Gaussian distribution p(\u00b5|D) given some observed data. In this case, prior and likelihood defining the"}, {"title": "D. Metrics", "content": "In this section, we provide details about the metrics considered for the different tasks. We generally look at two main metrics for benchmarking performance: L2 loss and Accuracy. For estimating the mean of a Gaussian distribution, the L2 loss is defined as\nGM L2 = ED\u223cx E\u00b5\u223cq\u03c6(.|D) \u03a3(xi - \u00b5)2 (45)\ni=1\nwhere D = {xi}. Intuitively, this captures the quality of the estimation of the mean parameter by measuring how far the observations are from it. Lower value implies better estimation of the mean parameter. Similarly, for estimating the means of a Gaussian Mixture Model, we rely on a similar metric but we also find the cluster closest to the observation, which can be defined as\nGMM L2 = ED\u223cx E\u00b5k\u223cq\u03c6(.|D) \u03a3(Xi - \u00b5Match(xi,{\u00b51,...,\u00b5K }))2 (46)\ni=1\nMatch(x, {\u00b51, \u2026, \u00b5K } = arg min(x \u2212 \u00b5k )2 (47)\nk\nwhich intuitively captures the distance of observations from the cluster closest to them. Next, we define the metric for evaluating (non-)linear regression models as\n(N-)LRL2 = ED\u223cx E\u03b8\u223cq\u03c6(.|D) \u03a3(Yi - Mode [p(yi|xi, \u03b8)])2 (48)\ni=1\nFinally, for the (non-)linear classification setups, we define the accuracy metric as\n(N-)LC Accuracy = ED\u223cx E\u03b8\u223cq\u03c6(.|D) /ND \u03a3 \u03b4(Yi, Mode [p(yi|xi, \u03b8)]) (49)"}, {"title": "E. Architecture Details", "content": "In this section, we outline the two candidate architectures that we consider for the backbone of our amortized variational inference model. We discuss the specifics of the architectures and the hyperparameters used for our experiments."}, {"title": "E.1. Transformer", "content": "We use a transformer model (Vaswani et al., 2017) as a permutation invariant architecture by removing positional encodings from the setup and using multiple layers of the encoder model. We append the set of observations with a [CLS] token before passing it to the model and use its output embedding to predict the parameters of the variational distribution. Since no positional encodings or causal masking is used in the whole setup, the final embedding of the [CLS] token becomes invariant to permutations in the set of observations, thereby leading to permutation invariance in the parameters of q\u03c6.\nWe use 4 encoder layers with a 256 dimensional attention block and 1024 feed-forward dimensions, with 4 heads in each attention block for our Transformer models to make the number of parameters comparative to the one of the DeepSets model."}, {"title": "E.2. DeepSets", "content": "Another framework that can process set-based input is Deep Sets (Zaheer et al., 2017). In our experiments, we used an embedding network that encodes the input into representation space, a mean aggregation operation, which ensures that the representation learned is invariant concerning the set ordering, and a regression network. The latter's output is either used to directly parameterize a diagonal Gaussian or as conditional input to a normalizing flow, representing a summary statistics of the set input."}, {"title": "E.3. RNN", "content": "For the recurrent neural network setup, we use the Gated Recurrent Unit (GRU). Similar to the above setups, we use a 4-layered GRU model with 256 hidden dimensions. While such an architecture is not permutation invariant, by training on tasks that require such invariance could encourage learning of solution structure that respects this invariance."}, {"title": "E.4. Normalizing Flows", "content": "Assuming a Gaussian posterior distribution as the approximate often leads to poor results as the true posterior distribution can be far from the Gaussian shape. To allow for more flexible posterior distributions, we use normalizing flows (Kingma & Dhariwal, 2018; Kobyzev et al., 2020; Papamakarios et al., 2021; Rezende & Mohamed, 2015) for approximating q\u03c6(0|D) conditioned on the output of the summary network hay. Specifically, let gv : z 0 be a diffeomorphism parameterized by a conditional invertible neural network (cINN) with network parameters v such that 0 gv (z; hy(D)). With the change-of-variables formula it follows that p(0) = p(z) | det gv (z; hy(D))|\u00af\u00b9 = p(z)| det Jv(z; hy(D))|\u22121, where J, is the Jacobian matrix of gr. Further, integration by substitution gives us de = | det J, (z; hy(D)|dz to rewrite the objective from eq. 14 as:\narg min ED\u223cX KL[qq(0|D)||p(0|D)] = arg min ED\u223cxE\u03b8\u223cq\u03c6(0|D) [logq\u03c6(0|D) \u2212 log p(0, D)] =arg min ED\u223cxEz\u223cp(z) log [|p(z/det( Jv (z; hy (DD) )| - log p(gv (z; hy (D)), D)] (51),(52),(53)\nAs shown in BayesFlow (Radev et al., 2020), the normalizing flow gv and the summary network hay can be trained simultaneously. The AllInOneBlock coupling block architecture of the FrEIA Python package (Ardizzone et al., 2018),"}]}