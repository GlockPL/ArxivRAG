{"title": "Counterfactual Evaluation of Ads Ranking Models through Domain Adaptation", "authors": ["Mohamed A. Radwan", "Himaghna Bhattacharjee", "Quinn Lanners", "Jiasheng Zhang", "Serkan Karakulak", "Houssam Nassif", "Murat Ali Bayir"], "abstract": "We propose a domain-adapted reward model that works alongside an Offline A/B testing system for evaluating ranking models. This approach effectively measures reward for ranking model changes in large-scale Ads recommender systems, where model-free methods like IPS are not feasible. Our experiments demonstrate that the proposed technique outperforms both the vanilla IPS method and approaches using non-generalized reward models.", "sections": [{"title": "1 INTRODUCTION", "content": "Over the past decade, online advertising has undergone significant transformations, particularly with the integration of Artificial Intelligence and Deep Learning technologies. Leveraging the vast amount of traffic managed by popular recommender services, even minor adjustments to such recommendation models can significantly affect business objectives. Understanding and predicting the impact of such model changes in advance highlights the importance of Offline Evaluation in large scale recommender systems [8]."}, {"title": "2 METHODOLOGY", "content": "The high level setup for an offline counterfactual evaluation system is shown in Figure 1. It employs an offline serving simulator that operates a separate instance for each target domain (i.e ads recommended by a given ranking model). The simulator runs recommendation request under each domain. Following simulation, the recommended ads for each corresponding target domain are collected. Subsequently, we leverage the reward model to estimate the expected reward for each domain. In the following sections, we detail how to train such a generalized reward model to effectively handle multiple domains."}, {"title": "2.1 Overview", "content": ""}, {"title": "2.2 Notation and set-up", "content": "Let $X \\subset R^d$ be a d-dimensional context space of user and request features, $A \\subset R^q$ be a q-dimensional ad space, and $Y \\subset R$ be a one-dimensional reward space (which may be a discrete set or continuous interval depending on the use case).\nEach recommendation model defines a policy which induces a domain in the $X \\times A$ space. We use $S$ and $T_k \\in \\{T_1,..., T_K\\}$ to refer to source and target policies. Finally, we let $p(y | x, a)$ be the conditional distribution of the reward given a particular context and ad combination, which is assumed to be independent of the choice of $T_k$. The objective is to evaluate a set of K target recommendation models denoted by $T_1, ..., T_K$ in an offline setting, using data generated by source policy S."}, {"title": "2.3 Metric", "content": "Our goal is to correctly rank target policies, and therefore we need the reward model to perform fairly across different domains. To achieve this, we choose the coefficient of variance of recovery (Recco) as the main performance indicator for the reward model:\n$Recco = \\frac{Rec_{dev}}{Rec_{avg}}$, $Rec(T_k, S) = \\frac{Lift(T_k, S)}{Lift(T_k, S)},$"}, {"title": "2.4 Estimating Lifts Between Domains", "content": "We introduce a model based approach to estimate the lift between two domains. Our approach focuses on modeling the non-overlapping regions between source and target domains and then using the trained reward model to calculate the expected lift of the target domain $T_k$ over the source domain S (for each target domain k).\nWe use $D_s = \\{(x_i, a_i, y_i)\\}_{i=1}^{n}$, a set of n labeled source domain samples, and $D_{T_k} = \\{(x_i, a_i)\\}_{i=1}^{n}$, unlabeled target domain data for the same n samples, to estimate the lift as\n$\\begin{aligned}\nLift (T_k, S) = \\frac{1}{n} \\sum_{(x_i, a_i) \\in D_{T_k, n}} h(x_i, a_i) - \\frac{1}{n} \\sum_{(x_i, a_i) \\in D_{S, n}} h(x_i, a_i)\n\\end{aligned}$"}, {"title": "2.5 Reward Model Training", "content": "Let $p_c (a | x)$ be the probability of observing ad a under context x with policy c. We define a weight\n$w_k = \\frac{p_{T_k} (a|x)}{p_S (a|x)}$\n$w_k$ is used in the per sample weights for training a reward model for all K target domains by minimizing the following loss function on the labeled $D_s$:\n$\\sum L[h(x_i, a_i, \\theta), y_i]$\n$(x_i,a_i, y_i) \\in D_s$\n$+\\beta\\begin{bmatrix}\n\\sum_{k=1}^K \\Big| w_k \\sum_{i=1} w_k \\Big| + \\sum_{k' > k} |w - w'|\n\\end{bmatrix}$"}, {"title": "3 EXPERIMENTAL RESULTS", "content": "We report our findings on both synthetic and online experiment results for a CTR prediction model. For the synthetic environment, we generated domain policies, where each target domain or test variant represents an incremental improvement over the source domain or control variant. The reward function is a linear model of the context and ad covariates, where the coefficients are drawn from a normal distribution based on the ad context vector values. We train the baseline solely on the source domain data. We train our proposed reward model using Section 2.5 weighted target domain information."}, {"title": "Appendix A METRIC DETAILS", "content": "The metric of interest in our off policy evaluation for ad recommendation models is the performance difference of the offline model with respect to the online source model. We define a lift metric for a target policy $T_k$:\n$Lift(T_k, S) = E_{p_{T_k}} [Y] \u2013 E_{p_S} [y].$"}, {"title": "Appendix B DERIVATION OF RECOVERY LOSS", "content": ""}, {"title": "B.1 Single-Domain Recovery Optimization", "content": "Let us look at the Recovery metric when there is one target domain Tk. Using the definition of recovery from Eq 1 and the definition for lift from Eq 5, we have $Rec(T_k, S) = 1 - rdiff-rdiff$ where\n$r_{diff}^{true} = E_{T_k}[Y] - E_{S}[Y]$\nand\n$r_{diff}^{est} = E_{T_k}[Y] - E_{S}[Y]$\nwith domain distributions, the distance can be further expanded as:\n$\\begin{aligned}\n|r_{diff} - r_{diff}| &= (E_{T_k} [y] \u2013 E_S [y]) \u2013 (E_{T_k} [y] \u2013 E_S [y])\n&= E_{T_k} [y(a, x) \u2013 y(a, x)] \u2013 E_S [y(a, x) \u2013 y(a, x)]\n&= E_{p_s} (\\frac{p_{T_k} (a | x)}{p_S (a|x)} - 1 ) (y(a, x) - y(a,x))\n&= \\frac{1}{N} \\sum \\Big[ ( \\frac{p_{T_k} (a | x)}{p_S (a|x)} - 1 ) (y(a, x) - y(a,x))\\Big]\n(x, y) \\in D_s\n\\end{aligned}$"}, {"title": "B.2 Multi-Domain Optimization", "content": "For the multi-domain use-case, the metric to be optimized is Recco (.) defined in Eq 6. The Recdev defined in Eq 8 can be minimized by"}]}