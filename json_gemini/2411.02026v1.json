{"title": "CTEFM-VC: Zero-Shot Voice Conversion Based on Content-Aware Timbre Ensemble Modeling and Flow Matching", "authors": ["Yu Pan", "Yuguang Yang", "Jixun Yao", "Jianhao Ye", "Hongbin Zhou", "Lei Ma", "Jianjun Zhao"], "abstract": "Zero-shot voice conversion (VC) aims to transform the timbre of a source speaker into any previously unseen target speaker, while preserving the original linguistic content. Despite notable progress, attaining a degree of speaker similarity and naturalness on par with ground truth recordings continues to pose great challenge. In this paper, we propose CTEFM-VC, a zero-shot VC framework that leverages Content-aware Timbre Ensemble modeling and Flow Matching. Specifically, CTEFM-VC disentangles utterances into linguistic content and timbre representations, subsequently utilizing a conditional flow matching model and a vocoder to reconstruct the mel-spectrogram and waveform. To enhance its timbre modeling capability and the naturalness of generated speech, we propose a context-aware timbre ensemble modeling approach that adaptively integrates diverse speaker verification embeddings and enables the joint utilization of linguistic and timbre features through a cross-attention module. Experiments show that our CTEFM-VC system surpasses state-of-the-art VC methods in both speaker similarity and naturalness by at least 18.5% and 7.0%.", "sections": [{"title": "1. Introduction", "content": "As a pivotal task within the field of speech signal processing, zero-shot voice conversion (VC) seeks to transfer the timbre of a source utterance to an unseen target speaker while maintaining the original phonetic content, with applications spanning various practical domains such as voice anonymization [1] and audiobook production [2].\nOverall, the core difficulties in zero-shot VC lies in effectively modeling, decoupling, and utilizing various speech attributes, including content, timbre, and so forth. Previous zero-shot VC approaches [3, 4, 5] often use pre-trained automatic speech recognition (ASR) [6, 7] and speaker verification (SV) models [8, 9] to extract content and timbre information from the source and target speech, respectively. Nevertheless, due to factors such as the inherent complexity of speech signals [10, 11] and the limitations in timbre and content modeling [12, 13], these methods present significant opportunities for performance enhancement. With recent progressions of self-supervised learning (SSL) based speech models [14, 15], many works [16, 12] have sought to use them to extract semantic features from utterances. However, due to limitations of the SSL mechanism, the extracted features inevitably contain certain source timbre characteristics, ultimately affecting their VC quality. To this end, [17, 13, 18] incorporated the model quantization technique to minimize non-content elements. Nonetheless, this operation incurs additional training overhead, and variations in training objectives may lead to token format inconsistencies across different models, thereby limiting broader applicability. Moreover, existing VC methods [3, 5, 12, 18] normally employ a single pre-trained SV model to capture target timbre embeddings. Although speaker embedding techniques have advanced significantly [9, 19, 20], relying exclusively on a single model is insufficient to deliver high-performance VC, resulting in subpar speaker similarity compared to authentic recordings.\nInspired by the powerful zero-shot capabilities of large-scale language models (LLMs) [21], recent studies [22, 23] attempted to discretize utterances with neural speech codecs [24, 25], so as to leverage LLMs to generate target waveform in an auto-regressive manner. For instance, [22] proposed a two-stage language model that first generates coarse acoustic tokens to capture source linguistic content and target timbre, then refines acoustic details for VC. Afterwards, [23] implemented a single-stage VC framework based on context-aware language model and acoustic predictor, facilitating zero-shot voice conversion in a streamable way. Despite these advancements, such methods commonly encounter stability problems due to their auto-regressive fashion and may experience error accumulation, leading to a gradual decline in VC performance.\nTo address aforementioned problems, we propose CTEFM-VC, an effective zero-shot VC framework based on content-aware timbre ensemble modeling and flow matching. To be specific, we first use a pretrained ASR model HybridFormer [7] to capture the precise linguistic content of the source speech. Subsequently, to improve timbre modeling capabilities, we introduce an ensemble strategy that leverages multiple pre-trained SV models to extract timbre embeddings from the target speech, concatenating these embeddings as conditional inputs for a conditional flow matching (CFM) module. Additionally, to further enhance the speaker similarity and naturalness of the entire system, we propose an effective and easily scalable content-aware timbre ensemble modeling (CTE) approach that incorporates learnable hyperparameters for each SV embedding and utilizes a cross-attention module to achieve adaptive fusion between timbre embeddings and linguistic content features, thereby generating higher-quality representations to serve as inputs for the CFM module. Finally, we incorporate the CFM and pre-trained vocoder [26] to reconstruct the mel-spectrogram and generate the converted speech. Experimental results demonstrate that the proposed CTEFM-VC approach outperforms several state-of-the-art (SOTA) zero-shot VC systems in terms of both speaker similarity and naturalness, achieving relative improvements of at least 18.5% and 7.0%, respectively."}, {"title": "2. METHODOLOGY", "content": "As shown in Fig. 1, the proposed CTEFM-VC is an end-to-end zero-shot VC framework. Assume the input speech signal is represented as $X = [x_1, x_2, ..., x_T] \\in R$, our CTEFM-VC method initially employs the pre-trained ASR model, HybridFormer, to extract the linguistic content $f_c \\in R^{T_1\\times D}$. Detailed, the used HybridFormer consists of 12 blocks, each with a convolution kernel size of 31 and 4 attention heads. The hidden dimensions of the attention and feed-forward network layers are set to 256 and 1024, respectively. For timbre modeling, unlike prior works, we use multiple pretrained SV models [9, 19, 20] to extract their corresponding timbre embeddings $f_{r_i} \\in R^{d_i}$ of the reference waveform. To further enhance its speaker similarity and naturalness, we propose CTE, a context-aware timbre ensemble modeling approach that uses a straightforward yet effective AdaFusion method to fuse $f_{r_i}$ as a global representations $f_r$ and employ a cross-attention module to facilitate the joint utilization of the source linguistic content and target timbre representations. Last, the outputs $F$ of CTE and ensembled timbre features $f_r$ are fed into the CFM model to reconstruct the Mel-spectrograms, followed by a pretrained Bigvgan vocoder to generate the desired target waveform."}, {"title": "2.2. CTE: Context-aware Timbre Ensemble Modeling", "content": "Essentially, zero-shot VC is an inherently challenging task, as it requires the model to generalize effectively to arbitrary unseen speakers without the need for any supplementary training or fine-tuning. This imposes exceptionally high demands on the timbre modeling capabilities of the proposed method.\nConsequently, to enhance the overall timbre modeling capacity of the proposed method, a model ensemble strategy that utilizes multiple pretrained speaker verification (SV) models to extract timbre features is first presented. We hypothesize that integrating diverse timbre embeddings enables our approach to capture a richer array of timbre characteristics, thereby enhancing its adaptability to variations in speaker identity. To better exploit these features, we propose a straightforward yet effective AdaFusion method that applies learnable hyperparameters to weight the SV embeddings before concatenating them into"}, {"title": "2.3. Conditional Flow Matching", "content": "To achieve an optimal balance between the generation quality and real-time performance, we incorporate an optimal transport (OT) based CFM module to reconstruct the target mel-spectrograms $x_1=p_1(x)$ from a standard Gaussian noise $x_0 = p_0(x)=N(x; 0, I)$. Concretely, an OT flow $\\psi_t : [0, 1] \\times R^d \\rightarrow R^d$ is employed to train our proposed OT-CFM that is composed of multiple UNet blocks with timestep fusion. By leveraging the ordinary differential equation to estimate a learnable time-dependent vector field $v_t: [0, 1] \\times R^d \\rightarrow R^d$, it can approximate the optimal transport probability path from $p_0(x)$ to the target distribution $p_1(x)$:\n$\\frac{d}{dt} \\psi_t(x) = v_t(\\psi_t(x), t)$  (1)\nwhere $\\psi_0(x) = x$, and $t \\in [0,1]$. Additionally, inspired by [27, 18] that recommend straighter trajectories, we simplify the formula of OT flow as follow:\n$\\psi_{t,z}(x) = \\mu_t(z) + \\sigma_t(z)x$ (2)\nwhere $\\mu_t(z) = tz$, $\\sigma_t(z) = (1-(1-\\sigma_{min})t)$, $z$ represents the random conditioned input, $\\sigma_{min}$ signifies the minimum standard deviation of the white noise introduced to perturb the individual samples, empirically set to 0.0001. As a result, the training loss of our OT-CFM module is defined as:\n$L_{CFM}=E_{t,p(x_0),q(x_1)} || (x_1-(1-\\sigma_t)x_0)-v_t (\\psi_{t,x_1}(x_0)| \\theta) ||^2$ (3)\nwhere $t \\sim U[0, 1]$, $x_0 \\sim p(x_0)$, $x_1 \\sim q(x_1)$, with $q(x_1)$ representing the true but potentially non-Gaussian distribution of the data, and $\\theta$ denotes the parameters of the CFM module."}, {"title": "2.4. Training Objectives", "content": "In addition to the $L_{CFM}$, we incorporate a structural similarity-based loss function [25] to minimize the disparity between the timbre embeddings of the source and converted speech, i.e., $T_s$ and $T_t$, thereby further enhancing the speaker similarity performance of the proposed system:\n$L_{spk}=SSIM(T_s, T_t)=\\frac{(2\\mu_{T_s} \\mu_{T_t} + C_1) (2\\sigma_{T_sT_t} + C_2)}{(\\mu_{T_s}^2 + \\mu_{T_t}^2 + C_1) (\\sigma_{T_s}^2 + \\sigma_{T_t}^2 + C_2)}$ (4)\nHere, $\\mu_{T_s}$ and $\\mu_{T_t}$ denote the means of $T_s$ and $T_t$, while $\\sigma_{T_s}^2$ and $\\sigma_{T_t}^2$ represent their respective variances. Besides, $\\sigma_{T_sT_t}$ indicates the covariance between $T_s$ and $T_t$. The constants $c_1$ and $c_2$ are employed to ensure numerical stability during division, with values set to 0.01 and 0.03, respectively.\nConsequently, the overall training objective of the proposed CTEFM-VC can be expressed as:\n$L_{Total} = L_{CFM} + \\lambda L_{spk}$ (5)\nwhere $\\lambda$ is a tuning hyperparameter empirically set to 0.1."}, {"title": "3. EXPERIMENTS", "content": ""}, {"title": "3.1. Experimental Setups", "content": ""}, {"title": "3.1.1. Datasets", "content": "We conduct all experiments on LibriTTS [28], which consists of 585 hours of English recordings from 2,456 speakers. We downsample all speech to 16kHz and use all training subsets for model training, while the dev-clean subset is employed for validation. To assess their zero-shot VC performance, we use the VCTK [29] and ESD [30] corpora. Besides, for each corpus, we randomly select 100 samples from 10 unseen speakers, ensuring that there is no overlap with the training data."}, {"title": "3.1.2. Implementation Details", "content": "We employ the AdamW optimizer to train the proposed CTEFM-VC approach over 600K iterations using four NVIDIA A10 GPUs, with an initial learning rate of 1e-4 and a batch size of 64. Additionally, during the inference stage, we sample the reference mel-spectrograms using 30 Euler steps within the CFM model, applying a guidance scale of 1.0."}, {"title": "3.1.3. Comparative Baseline Systems", "content": "We compare CTEFM-VC with four SOTA VC systems to evaluate its zero-shot VC performance. The first baseline, DiffVC [31], is a zero-shot VC approach employing diffusion models alongside a fast maximum likelihood sampling scheme. The second, NS2VC, is a modified variant of NaturalSpeech2 [32] that integrates diffusion models and neural codecs. The third, VALLE-VC [33], is adapted to perform voice conversion by replacing the original phoneme input with semantic tokens extracted from a supervised model. The final baseline, SEFVC [13], utilizes a position-agnostic cross-attention mechanism to incorporate the target timbre from reference speech. To ensure a fair comparison, all methods are trained on the same dataset."}, {"title": "3.1.4. Evaluation Metric", "content": "To thoroughly assess our CTEFM-VC method, we perform both objective and subjective evaluations.\nIn the objective evaluation, we compute the speaker embedding cosine similarity (SECS), character error rate (CER), and UTMOS between the converted speech and the reference speech using a pre-trained WavLM-TDCNN model\u00b9, a CTC-based ASR system\u00b2, and a mean opinion score (MOS) prediction model\u00b3, respectively. For the subjective evaluation, we invite 15 professional raters to assign MOS scores for both naturalness (NMOS) and similarity (SMOS) on a scale ranging from 1 to 5. This scoring reflects the naturalness of the generated speech and the speaker similarity between the converted and source waveforms. Specifically, a score of '5' indicates excellent quality, '4' denotes good quality, '3' represents fair quality, '2' indicates poor quality, and '1' signifies bad quality."}, {"title": "3.2. Main Results", "content": "To examine the performance of the proposed CTEFM-VC, we compare it with SOTA zero-shot VC baselines, with the results reported in Table 1."}, {"title": "3.3. Ablation Study", "content": "To evaluate the contributions and validity of each component of the proposed system, we conduct ablation studies. All results are summarized in Table 2.\nInitially, we assess the effectiveness of the timbre ensemble modeling strategy. As indicated in Table 2, the omission of any SV model leads to a marked decrease in performance across all metrics, particularly evident in the SECS score. This phenomena supports our hypothesis that the integration of diverse timbre embeddings facilitates the capture of a broader range of timbre characteristics, thereby enhancing the timbre modeling capabilities of the proposed method. Additionally, the experimental results indicate that within the proposed CTEFM-VC framework, CAM++ demonstrates the most effective timbre modeling capabilities, followed closely by ERes2Net, whereas ReDimNet exhibits the least effectiveness.\nNext, we examine the influence of removing the AdaFusion method. From Table 2, we can easily observe that in the absence of AdaFusion, both subjective and objective metrics display varying degrees of decline. Specifically, the subjective scores decrease by 5.1% to 5.6%, while the objective scores drop by 6.5% to 11.2%. This reduction proves the effectiveness of the proposed AdaFusion method in adaptively fusing different timbre embeddings. By learning the importance of individual SV model, AdaFusion facilitates the joint utilization of linguistic content and timbre features, thereby enhancing overall performance of CTEFM-VC.\nLast, we study the effect of the SSIM-based speaker similarity loss $L_{spk}$. From the above table, we notice that without the $L_{spk}$, the SECS and SMOS scores drop significantly, while the UTMOS and NMOS exhibits a slightly improvements. This observation suggests that the $L_{spk}$ is crucial for capturing speaker identity. However, the slight increase in UTMOS and NMOS scores without $L_{spk}$ indicates that the model may slightly enhance naturalness and intelligibility at the cost of speaker similarity. This trade-off highlights the importance of carefully balancing the loss components to achieve optimal performance across multiple evaluation metrics."}, {"title": "4. CONCLUSIONS", "content": "In this work, we propose CTEFM-VC, an effective and scalable zero-shot VC workflow based on content-aware timbre ensemble modeling and conditional flow matching. To be concrete, CTEFM-VC utilizes a pretrained ASR model and multiple SV models to extract the linguistic content and timbre features. Subsequently, a content-aware timbre ensemble modeling approach is proposed to integrate diverse timbre embeddings and facilitate the adaptive utilization of the source content and target timbre representations. Finally, we incorporate a CFM module to reconstruct the mel-spectrogram, followed by a pretrained vocoder to generate the converted speech. Extensive experiments conducted on the LibriTTS corpus demonstrate that our proposed CTEFM-VC approach significantly outperforms recent SOTA zero-shot VC methods by at least 18.5% and 7.0% in terms of speaker similarity and naturalness, highlighting its effectiveness and superiority."}]}