{"title": "Cannot or Should Not? Automatic Analysis of Refusal Composition in IFT/RLHF Datasets and Refusal Behavior of Black-Box LLMS", "authors": ["Alexander von Recum", "Christoph Schnabl", "Gabor Hollbeck", "Silas Alberti", "Philip Blinde", "Marvin von Hagen"], "abstract": "Refusals - instances where large language models (LLMs) decline or fail to fully execute user instructions are crucial for both AI safety and AI capabilities and the reduction of hallucinations in particular. These behaviors are learned during post-training, especially in instruction fine-tuning (IFT) and reinforcement learning from human feedback (RLHF). However, existing taxonomies and evaluation datasets for refusals are inadequate, often focusing solely on should-not-related (instead of cannot-related) categories, and lacking tools for auditing refusal content in black-box LLM outputs.\nWe present a comprehensive framework for classifying LLM refusals: (a) a taxonomy of 16 refusal categories, (b) a human-annotated dataset of over 8,600 instances from publicly available IFT and RLHF datasets, (c) a synthetic dataset with 8,000 examples for each refusal category, and (d) classifiers trained for refusal classification.\nOur work enables precise auditing of refusal behaviors in black-box LLMs and automatic analyses of refusal patterns in large IFT and RLHF datasets. This facilitates the strategic adjustment of LLM refusals, contributing to the development of more safe and reliable LLMs.", "sections": [{"title": "Introduction and Related Work", "content": "Fine-tuning language models [Wei et al., 2022], particularly instruction fine-tuning (IFT) [Ouyang et al., 2022], along with reinforcement learning from human feedback (RLHF) [Christiano et al., 2017, Ziegler et al., 2020, Stiennon et al., 2022] and reinforcement learning from AI feedback (RLAIF) [Bai et al., 2022b], collectively referred to as the reward model (RM) training phase, have emerged as popular techniques for enhancing the capabilities [Wang et al., 2022, Muennighoff et al., 2023] and safety [Bai et al., 2022a] of LLMs [Naveed et al., 2024, Zhang et al., 2024b]. During multiple iterations of IFT and RM, jointly referred to as the post-training phase, pairs of instructions and outputs are used, which are usually written by human annotators or generated by LLMs [Wang et al., 2023a, Dubey et al., 2024], to either directly compute the supervised loss on the output and update the model parameters, or use methods such as proximal policy optimization [Schulman et al., 2017] or direct preference optimization [Rafailov et al., 2024] with relative preference labels for pairs of outputs. Despite the significant impact of these IFT and RM datasets on model behavior, little is known about their composition, since they remain largely proprietary. In particular, instances of refusals within these datasets, which are pairs of user inputs and model outputs where the model partially or completely declines to comply with the instruction, have a great impact on safety behavior and hallucination rates. [Zhang et al., 2024a]"}, {"title": "Existing Refusal Taxonomies", "content": null}, {"title": "Alignment efforts by frontier labs", "content": "Askell et al. [2021] introduce the HHH framework, which states that an assistant must be \"helpful, honest, and harmless\". An assistant should not engage in \"offensive\" or \"discriminatory\" behavior. It should also refuse to \"aid in a dangerous act[s] (e.g. building a bomb)\", and recognize when \"it may be providing very sensitive or consequential advice and act with appropriate modesty and care\". The authors also acknowledge that \"behaviors are considered harmful and to what degree will vary across people and cultures. It will also be context-dependent, i.e. it will depend on the nature of the user query, who is using the AI assistant, and the time and place in which the assistant is being used\". A more precise definition of what is considered \"harmful\" is not provided.\nBai et al. [2022a] use this framework to train a \"helpful and harmless\" assistant, by using human annotators to choose the less harmful and more helpful response at each conversation turn, therefore letting these annotators implicitly define \"helpfulness\" and \"harmlessness\" without providing a clear taxonomy of unsafe or unsupported requests.\nIn 2022, Google DeepMind released the Sparrow chatbot [Glaese et al., 2022], which also builds upon the principles of Askell et al. [2021], borrowing the HHH framework, substituting \"honest\" for \"correct\" and defining a more detailed set of rules related to each of the three principles. These include \"Stereotypes (harm)\", \"Hate and harrassment (harm)\", \"Self-anthropomorphism (harm)\", \"Misinformation (correct)\", \"Other (harm)\", and \"Other (helpful)\". Each of these categories includes several subcategories, such as \"no stereotypes\" and \"no microagressions\" within the \"Stereotypes (harm)\" categories. In total, the paper outlines 23 subcategories, with \"Other (harm)\" and \"Other (helpful)\" serving as catch-all categories. The paper also references Banko et al. [2020], who define \"A Unified Typology of Harmful Content\" consisting of 13 subcategories, with several specific examples for each subcategory. However, the specific dataset used to align the Sparrow model remains proprietary, including the ratings of annotators.\nBai et al. [2022b] build on this work by defining a \"Constitution\", a set of principles, adherence to which is verified by an AI assistant. This allows them to train a model preferred by human evaluators without collecting manually written feedback or human labels. The specific judgments made by the AI assistant relating to adherence to the \"Constitution\" on concrete examples are, again, not released publicly, with only a handful of examples presented in the paper. The \"Constitution\" is specified in the paper, along with a few example instructions given to the AI assistant to describe principles of the constitution and verify adherence to them. The prompts used for verification include vague adjectives such as \"toxic\", \"dangerous\" or \"harmful\", and appeal to the model's own judgment by asking it to identify cases where instructions fit these descriptions, instead of offering a concrete and exhaustive categorization of behaviors matching these descriptions and deciding whether the instruction falls into one of these categories."}, {"title": "Existing should not-related taxonomies", "content": "Several other works have proposed taxonomies of both harmful and unsupported requests, along with datasets including safety-related content which LLMs are not allowed to generate.\nRealToxicityPrompts [Gehman et al., 2020] contains 100K prompts and continuations, categorized into 8 kinds of toxicity: sexual content, toxicity, severe toxicity, profanity, insults, flirtation, identity attacks, and threats. The dataset does not consist of user-assistant interactions, but rather contains 100K prompts and continuations of sentences generated by a language model. ToxiGen [Hartvigsen et al., 2022] contains 274K statements about 13 minority groups with binary labels of either toxic or benign. Like RealToxicityPrompts, the dataset does not consist of user-assistant interactions, but rather contains standalone statements. Wang et al. [2023b] introduce the Do-Not-Answer dataset, which contains 939 instructions that responsible LLMs should refuse to respond to. The dataset is organized into a hierarchical taxonomy covering five risk areas, 12 harm types, and 61 total harmful scenarios. The risk areas and harm types include: Information Hazards (with harm types Risks from leaking sensitive information and Compromise privacy), Malicious Uses (with Assisting illegal activities, Nudging users to perform unethical actions, and Reducing cost of disinformation), Discrimination, Exclusion, Toxicity (with Social stereotypes and discrimination, Toxic language, and Adult content), Misinformation Harms (with Disseminating false information and Causing material harm through misinformation), and Human-chatbot Interaction Harms (with Mental health crisis and Treating chatbot as human). AdvBench [Huang et al., 2023] includes a dataset designed to evaluate LLM robustness against adversarial attacks, focusing on their ability to resist generating harmful or toxic content. The dataset consists of two components: (1) 500 harmful strings reflecting toxic behavior like profanity, graphic depictions, threats, misinformation, discrimination, cybercrime, and dangerous suggestions, and (2) 500 harmful behaviors formulated as instructions covering similar themes. The dataset does not provide a formal categorization of these harmful behaviors and categories, as they were generated using an uncensored Vicuna model (Wizard-Vicuna-30B-Uncensored) through few-shot learning from author-written examples. ToxicChat [Lin et al., 2023] contains 10,166 examples of real user-AI conversations collected from interactions with the Vicuna chatbot, with binary toxicity labels (toxic/non-toxic) and annotations for jailbreaking attempts. The dataset was annotated through a hybrid human-AI process where moderation APIs first filtered likely non-toxic content (reducing annotation workload by 60%), followed by manual annotation by researchers focusing on edge cases. The final dataset has a toxicity rate of 7.10% and a jailbreaking rate of 1.75%, with annotations determined by majority vote among four annotators. MaliciousInstruct [Zou et al., 2023] contains 100 malicious instructions categorized into 10 distinct malicious intents, with 10 instructions for each intent. The malicious intent categories are psychological manipulation, sabotage, theft, defamation, cyberbullying, false accusation, tax fraud, hacking, fraud, and illegal drug use. BeaverTails [Ji et al., 2023] includes 330K human-labeled question-answer pairs, annotated by 70 human annotators across 14 distinct harm categories: Animal Abuse, Child Abuse, Controversial Topics/Politics, Discrimination/Stereotype/Injustice, Drug Abuse/Weapons/Banned Substance, Financial Crime/Property Crime/Theft, Hate Speech/Offensive Language, Misinformation Regarding Ethics/Laws/Safety, Non-Violent Unethical Behavior, Privacy Violation, Self-Harm, Sexually Ex-"}, {"title": "Existing cannot-related taxonomies", "content": "Jiang et al. [2021] show that models tend to perform poorly on questions that are unanswerable, also finding that prediction confidence in early transformer-based language models was not a reliable indicator of the model's uncertainty. Agarwal et al. [2023] create a dataset of five types (Incomplete, Future, Incorrect, Ambiguous, and Unmeasurable) of unanswerable questions and also find that SOTA LLMs underperform the human baseline on this task. Liu et al. [2024] create a similar dataset of unanswerable or unknown questions (UnknownBench), and reach a comparable conclusion, additionally confirming previous findings showing that model prediction confidence is not a reliable indicator of model uncertainty. Xiong et al. [2024] find that LLMs tend to be overconfident when asked to express their own uncertainty, highlighting the need for more research on this topic. Deng et al. [2024] explore methods to perform synthetic data augmentation to improve model performance on such questions, showing promising results. Zhang et al. [2024a] attempt to construct a refusal-aware dataset by identifying gaps between the model's training corpus and knowledge encoded in model parameters, showing that augmentation of the dataset with refusal instances of this kind improves model performance on such tasks, and that ability to refuse is a meta-skill that can generalize to other tasks."}, {"title": "Contributions", "content": "We present a unified taxonomy of 16 refusal categories, a dataset of 8,600 real instances annotated by a single annotator, 500 refusals annotated by four independent annotators for each instance, over 100,000 synthetic refusals, and linguistic mutations resulting in over 7 million synthetic refusals. We also release classifiers for these datasets to automatically audit refusal behaviors of models and adjust IFT and RLHF datasets to improve the safety and reliability of LLMs."}, {"title": "Problem Statement", "content": "To establish a clear understanding of the problem, this section formalizes the recurring terminologies, entities, and concepts we frequently discuss."}, {"title": "Definitions", "content": "We define a dataset D as a set of tuples (S, I, O), where:\n\u2022 S represents the system prompt, an instruction or statement provided by a developer.\n\u2022 I represents the input messages, an ordered sequence of messages, where each message is a tuple (role, content), indicating the role (e.g., user or assistant) and the message content.\n\u2022 O represents the output message, a tuple (role, content).\nAn AI assistant is defined as a function:\n$f : (S, I) \\rightarrow O$ , which maps a system prompt and input messages to an output message."}, {"title": "Refusals", "content": "A refusal occurs when the output O indicates that a refusal to comply with the instruction supplied in the input I did occur. We define the refusal identification function r as:\n$r: (S,I,O) \\rightarrow {0,1}$, where r(S, I, O) = 1 if O is a refusal, and 0 otherwise. We also define the estimated refusal identification function \u00ee as:\n$\\hat{r}: (S, I) \\rightarrow {0,1}$, which predicts whether a refusal should occur based only on the system prompt S and input I, without access to the output O, as opposed to whether it actually occurred in the output O. This function is, for example, implicitly learned during post-training. Although the datasets published in this work could be used as a starting point to train a classifier that learns this function, this modified problem implicitly necessitates the use of judgment over which requests should and should not be refused. Such judgments often involve complex ethical considerations and can vary based on cultural, legal, and personal perspectives. Although this is an important aspect of refusal behavior in AI systems, it is beyond the scope of our current work. In this paper, we focus primarily on the technical aspects of identifying and classifying refusals, rather than making normative judgments about which refusals are appropriate or necessary.\nWe do not focus on learning to predict the refusal behavior of a particular model, as this is covered in other works [Reuter and Schulze, 2023]."}, {"title": "Human Ratings", "content": "Due to inherent ambiguity and noise in refusal identification, we introduce a set of human annotators H and define a dataset of human ratings H.\nEach human annotator h \u2208 H provides a binary rating for each (S, I, O) tuple:\n$H(S, I, O,h) \\in {0,1}$, where H(S, I, O, h) = 1 if annotator h judges O to be a refusal, and 0 otherwise."}, {"title": "Refusal Decision Function", "content": "Given the human ratings H, we define the refusal decision function d as:\n$d(S,I,O) =\\begin{cases} 1 & \\text{if } \\frac{1}{|H|} \\sum_{h\\in H} H(S, I, O, h) \\geq \\tau, \\\\ 0 & \\text{otherwise,} \\end{cases}$, where \u03c4\u2208 [0, 1] is a threshold representing the minimum proportion of annotators who must agree that O is a refusal. If the proportion of annotators exceeds the given threshold, the human annotators have decided that (S, I, O) is a refusal."}, {"title": "Refusal Classification", "content": "To capture more granular refusal behaviors, we define a set of refusal categories C = {C1, C2, ..., Cn}, where each ci represents a specific type of refusal. We define the refusal classification function c as:\n$c: (S,I,O) \\rightarrow 2^C$, where 2C denotes the power set of C, allowing multiple distinct categories to be assigned to a single instance. If c(S,I,O) = \u00d8, the output O is regarded as not a refusal. Similarly, we define the estimated refusal classification function \u0109 as:\n$\\hat{c} : (S, I) \\rightarrow 2^C$, which predicts refusal categories based only on S and I, without access to \u039f.\nNote that the refusal categories are not mutually exclusive; multiple categories can apply to a single output O."}, {"title": "Human Classification Ratings", "content": "We extend the human ratings to include category assignments. Each annotator h\u2208 H assigns a subset of categories to each (S, I, O) tuple:\n$H_c(S, I, O,h) \\subseteq C$."}, {"title": "Learning Refusal Prediction Functions", "content": "Given the human ratings H and category assignments He, we aim to learn two prediction functions:\n1. The refusal likelihood function r:\n$r : (S,I,O) \\rightarrow [0, 1]$, which predicts the probability with which O would be identified as a refusal by human annotators.\n2. The category assignment likelihood function \u0113: We extend the set of categories to include a \"not a refusal\" category co, so C' = {Co, C1, C2, . . ., Cn}, where co represents the absence of a refusal. We define \u010das:\n$\\bar{c} : (S, I, O) \\rightarrow \\mathbb{R}^{[C']}$, where $\\bar{c}(S, I, O) = (\\hat{c}_0, \\hat{c}_1, ..., \\hat{c}_n)$, with \u0109j \u2208 [0, 1] representing the probability that human annotators would assign category cj to (S, I, O)."}, {"title": "Category Validity Function", "content": "For each category cj \u2208 C' and instance (S, I, O), we define the category validity function yj(S, I, O) based on the proportion of annotator who assigned category cj to the instance and a threshold \u03a4\u03bf \u2208 [0, 1]:\n$\\begin{aligned} p_j(S,I,O) &= \\frac{1}{|H|} \\sum_{h\\in H} \\mathbb{I}(c_j \\in H_c (S, I, O,h)), \\\\ y_j(S,I,O) &= \\begin{cases} 1 & \\text{if } p_j (S, I, O) \\geq \\tau_c \\\\ 0 & \\text{otherwise}, \\end{cases} \\end{aligned}$ where:\n\u2022 pj(S, I, O) is the proportion of annotator who assigned category cj to (S, I, O).\n\u2022 $(cdot)$ is the indicator function\n\u2022 Te is the threshold determining the minimum proportion of annotators required for a category to be considered valid."}, {"title": "Methodology", "content": "Our approach consists of seven main components:\n1. Data Collection: Creation of a dataset of refusals from publicly available IFT and RM datasets.\n2. Taxonomy Development: Development of a comprehensive taxonomy comprising 16 refusal categories.\n3. Human-Labeled Dataset Creation: Creation of a human-annotated dataset with over 8,600 instances from publicly available IFT and RM datasets.\n4. Synthetic Dataset Creation: Generation of a synthetic dataset containing 8,000 examples for each refusal category.\n5. Classifier Training: Training a refusal classifier using both human-annotated and synthetic data.\n6. Dataset Evaluation: Investigation of composition, diversity, confidence and ambiguity metrics of synthetic and human dataset.\n7. Classifier Evaluation: Evaluation and comparison of different classifiers and LLM classifiers."}, {"title": "Data Collection", "content": "We employed an iterative approach to collect a diverse set of refusal instances. Initially, we collect a set of refusals from publicly available IFT and RM datasets by searching through a small subset of the data both manually and using an LLM. We then iteratively gather new refusals by embedding all instances in the dataset, identifying the embeddings of the seed refusals, and searching for new refusals with high similarity scores to a representative vector of the seed refusals, such as the mean or weighted mean of their embeddings. We subsequently sample the top n candidates and verify them using an LLM to ensure their quality, adding them to the refusal dataset if they pass the verification step. We repeat this process for a fixed number of iterations.\nThe process can be summarized as follows:"}, {"title": "Taxonomy Development", "content": "We developed a comprehensive taxonomy tree of refusal behaviors through an extensive literature review of existing work on LLM refusals and safety behaviors, combined with a bottom-up analysis of the refusal instances in our collected dataset R. This dual approach allowed us to capture both theoretical categorizations from prior research and empirically observed refusal patterns.\nThrough review of refusals found in public datasets and the existing literature, we identified two fundamental categories of refusals: Should not-related and Cannot-related. We then performed a bottom-up analysis of the refusals in R combined with the categories defined in previous works listed in 1 to identify additional categories that better capture the nuances of refusal behavior and arrived at the taxonomy tree shown in Figure 15."}, {"title": "Refusal Taxonomy", "content": "We arrived at the following taxonomy:"}, {"title": "Mutual Exclusivity and Exhaustiveness", "content": "During taxonomy development, it became evident that it is impossible to define a set of exhaustive and mutually exclusive categories and still adhere to an intuitive notion of distinct reasons for request rejection. This is because one can always construct mixed refusal instructions that belong to multiple categories."}, {"title": "Human Annotation", "content": "For evaluation and classifier training using this dataset, as well as for synthetic data generation, we filter out some instances of refusals collected in this dataset. We remove instructions that are not refusals because binary classification of refusals has already been addressed in previous works. We also remove the \"Unclear\" labels used during the labeling process, since it does not signify a category, and \"[Should Not Do] Chain of Command\" because most public instruction fine-tuning datasets do not contain system messages and the category does not yield itself to synthetic data generation as any possible instruction could be specified, making it not well-constrained enough."}, {"title": "Single Human Annotator", "content": "For each refusal in R, we randomly sample a single human annotator from a pool of 8 available annotators to classify the refusal into one or more categories c \u2208 C'.\nDue to the scale of the dataset and to increase the speed of the labeling process, we pre-label all refusals in R with an LLM (GPT-40) using a prompt that contains the taxonomy path until each 21-level category and examples of refusals for each v\u2081-level category. The human annotator then"}, {"title": "Multiple Human Annotators", "content": "To assess inter-annotator agreement and capture the nuances in refusal categorization, we select a representative subset of refusals from those classified by single human annotators to undergo a multi-labeling process where multiple annotators independently classify the same instructions, without knowledge of the other annotators' classifications or LLM suggestions. This allows us to assess inter-annotator agreement and later evaluate the agreement of our refusal classifier with human judgments. To select a diverse subset of refusals, we embed all refusals in R and subsequently perform dimensionality reduction using UMAP [McInnes et al., 2020]. We then select a subset of the refusals that are well-separated in the reduced embedding space by overlaying a 2D grid over the feature space and sampling from each cell. We continue picking samples for each category until we obtain an even distribution of categories. All labels previously obtained are then discarded. This subset of instances is then annotated by multiple human annotators to assess inter-annotator agreement. In total, 4 annotators labeled 500 instructions, resulting in the dataset $D_{human}^{multi}$"}, {"title": "Synthetic Dataset Generation", "content": "To improve the generalization of our classifier and reduce dataset bias, we create a synthetic dataset Ds for each leaf node l \u2208 L containing examples that correspond to the specific refusal pattern represented by the path P\u2081 from root to leaf. We first generate a set of synthetic input examples Is that correspond to the refusal pattern in P\u2081. We then generate a set of synthetic output examples Os that correspond to the same refusal pattern in Pr.\nFor generating the input examples, we use the following algorithm:"}, {"title": "Classifier Training", "content": "Our goal is to learn the refusal likelihood function (see Equation 9) and \u0113 (see Equation 10) that best predict human judgments. For that, we train a BERT-based and a Logistic Regression-based classifier."}, {"title": "BERT-based Classifier", "content": "To learn r, we minimize the binary cross-entropy loss:\n$L_r = - \\sum_{(S,I,O) \\in D_{train}} [d(S, I, O) \\log \\bar{r}(S, I, O) + (1 - d(S, I, O)) \\log (1 - \\bar{r}(S, I, O))]$, where d(S, I, O) is the aggregated human decision defined as:\n$d(S,I,O) = \\begin{cases} 1 & \\text{if } p_r(S,I,O) \\geq \\tau_r, \\\\ 0 & \\text{otherwise}, \\end{cases}$ with pr(S, I, O) being the proportion of annotators who identified O as a refusal:\n$p_r(S,I,O) = \\frac{1}{|H|} \\sum_{h\\in H} H(S, I, O,h)$, and \u03c4\u03b7 \u2208 [0, 1] being the refusal identification threshold. For the category assignment likelihood function \u0113, we use a multi-label binary cross-entropy loss:\n$L_c = - \\sum_{(S,I,O) \\in D_{train}} \\sum_{j=0}^{n} [y_j (S,I,O) \\log \\bar{c}_j (S, I, O) + (1 - y_j (S, I, O)) \\log (1 - \\bar{c}_j (S, I, O))]$, where yj(S, I, O) is defined using the category validity function from equation 12. The classifier is trained by attaching a linear classification head to the output vector of the \"[CLS]\" token. A softmax is applied to the resulting logits, and the highest value is taken as the prediction."}, {"title": "Embedding - Logistic Regression Classifier", "content": "We also train a multinomial logistic regression classifier to predict the refusal category based on the output embeddings of a state-of-the-art (SOTA) embedding model, NV-Embed-V2 [Lee et al., 2024], with embedding dimensions of 4096. The classifier models the probability of assigning category ci to an output with embedding x as:\n$P(y = c_i | x) = \\frac{\\exp(w_{c_i}^T x + b_{c_i})}{\\sum_{j=1}^{|C|} \\exp(w_{c_j}^T x + b_{c_j})}$, where $w_{c_i} \\in \\mathbb{R}^d$ is the weight vector, $b_{c_i} \\in \\mathbb{R}$ is the bias term for category j, d is the dimensionality of the embeddings, and |C| is the total number of categories. The model parameters {Wk,bk}|C|k=1 are optimized by minimizing the cross-entropy loss over the training dataset Dtrain:\n$L_c = - \\sum_{i=1}^{N_{train}} \\log P(y_i|x_i)$, where Ntrain is the number of training samples, and (xi, Yi) are the embeddings and labels of the training outputs. The classifier predicts a set of logits, which are normalized using a softmax, and the highest value is taken as the prediction."}, {"title": "Dataset and Classification Evaluation", "content": "To assess the quality and characteristics of the datasets, our taxonomy, and various classification methods, we conduct several analyses using the metrics outlined in this section. We craft our analysis methods with the goal to answer the following questions:\n\u2022 What is the general composition of the data we collected and synthetically generated? How diverse is it?\n\u2022 How robust is our categorization when stress-tested on a diverse set of refusals $D_{human}^{multi}$ which spans the embedding space of refusals evenly?\n\u2022 How well do different SOTA LLMs perform on the task of refusal classification on $D_{human}^{multi}$?\n\u2022 How well do our classifiers perform on $D_{human}^{multi}$?\n\u2022 How do our classifiers compare to LLM classification methods cost-wise?"}, {"title": "Dataset Evaluation", "content": "\u2022 Human Dataset Evaluation:\nComposition Analysis: We analyze the label distribution normalized by the number of labels annotators assigned for a particular refusal instance.\n\u2022 Synthetic Dataset Evaluation:\nComposition Analysis: We analyze the composition of the synthetically generated dataset, such as mean length and standard deviation of lengths, and most common bi-grams."}, {"title": "Classification Evaluation", "content": "Correlation of Classifications: To measure the reliability of the human annotations and classifier, we employ the following statistical metrics.\n\u2022 Cohen's Kappa (\u03ba): Cohen's Kappa measures the pairwise agreement between two annotators, correcting for agreement that could occur by chance. It is defined as:\n$\\kappa = \\frac{P_o - P_e}{1 - P_e}$, where Po is the observed agreement proportion and Pe is the expected agreement by chance. Cohen's Kappa values range from -1 (complete disagreement) to 1 (complete agreement), with 0 indicating no agreement beyond chance.\n\u2022 Krippendorff's Alpha (\u03b1): Krippendorff's Alpha measures the agreement among multiple annotators and is suitable for data where items may belong to multiple categories (non-mutually exclusive). It accounts for varying sample sizes and missing data. It is defined as:\n$\\alpha = 1 - \\frac{D_o}{D_e}$, where Do is the observed disagreement and De is the expected disagreement by chance.\n\u2022 Intersection Ratio: Intersection ratio measures the agreement of a annotator with all other annotators. It is calculated by comparing the cardinality of the set of classifications provided by an annotator r intersected with the classification of all other annotators -r, normalized by the number of labels given by all other annotators:\n$K_r = \\frac{|C_r \\cap C_{-r}|}{|C_{-r}|}$, where:\n\u2013 Cr: Set of classifications made by annotator r."}, {"title": "Correlation Between Classifiers and Majority Votes", "content": "We analyze how correlated the classifier's single classification is correlated with the majority vote of all four human annotators. This gives an indication of how well the classifier's notion of the most likely category is correlated with humans."}, {"title": "Results", "content": "In this section we describe the outcomes of the aforementioned research objectives. Each subsection corresponds the respective subsection in the methodology section."}, {"title": "Refusal Dataset Composition", "content": null}, {"title": "Human-Labeled Datasets", "content": "The first human-labeled dataset $D_{human}^{single}$ consists of 8,650 input-output pairs labeled by human annotators. Each sample was annotated once by one of eight annotators. Reviewers could assign"}, {"title": "Synthetic Datasets", "content": "The two synthetic datasets provide larger volumes and higher diversity in refusal data. We first created one dataset of synthetic input prompts. Subsequently, we generated synthetic refusal outputs based on these inputs. This combined constitutes the first dataset $D_{synth}^{100K}$\nThe second dataset $D_{synth}^{ultra}$ contains variations of the input and outputs from $D_{synth}^{100K}$ and all of their combinations. As explained in the methodology section, we reduced the number of categories from 16 to 13. The $D_{synth}^{100K}$ dataset includes 8,000 input-output pairs for each of the 13 categories, resulting in 104,000 samples. To enhance linguistic and contextual diversity, a varied version of the dataset $D_{synth}^{100K}$, $D_{synth}^{ultra}$ was also created. Details about the 14 input and 5 output variations are summarized in table 2 and 3. By combining these variations, excluding one infeasible combination (shortened input with expanded output), the dataset contains 7.17 million samples. These details are summarized in table 6."}, {"title": "Classification Evaluation", "content": "We now analyze the classification correlation between humans, agreement between humans and LLMs and the performance of our classifiers."}, {"title": "Classifactions by Human Annotators", "content": "This section will analyze inter-annotator agreement patterns across our 13 refusal categories using the $D_{human}^{multi}$ dataset. We will present quantitative measures of agreement, examine the distribution of agreement levels across annotators, and identify key patterns in category disagreements.\nWe first analyze the correlation between different human annotators. Correlation scores range from 0.42 to 0.59, with Reviewer 3 being a clear outlier."}, {"title": "Classification performance of LLMs", "content": "We next examine the classification accuracy of various LLMs (Figure 9). For this, we analyze various metrics, such as \"at-least-one agreement\" and \"majority agreement\". When evaluating LLMs, we originally experimented with giving each model the ability to name multiple categories. However, when given the choice to assign many labels per category, we observed that models would either confidently assign just only one single category, or, in rare cases, name a lot more categories than one or two, which did not make much sense. We also observed that different LLMs were generally more or less likely to provide a list of categories when asked to do so. Thus, we changed the evaluation process of LLMs to allow them to predict only a single category, with the intention of getting more signal from such predictions. For \"at-least-one agreement\", we observe that models with generally higher capabilities also tend to perform better on the task of refusal classification. Unfortunately, because learnable parameter counts for many of the models are not publicized, we are not able to conduct an exhaustive correlation analysis between them and accuracy in refusal classification. We did, however, observe some fluctuations in model ability to follow our classification instructions, which consisted of a general description of the categories and one few-shot example per category. For instance, \"Llama 3.1 70B\" consistently underperformed compared to other models of similar size, such as \"Qwen 2 72B\", which performed surprisingly well for their parameter count. Agreement with the majority of human labelers for each model was 51.10% for GPT-40, 49.90% for Gemini 1.5 Pro, 52.10% for Mistral Large, 47.31% for Qwen 2.5 72B, 38.92% for Llama 3.1 405B"}]}