{"title": "Task-Informed Anti-Curriculum by Masking Improves Downstream Performance on Text", "authors": ["Andrei Jarca", "Florinel Alin Croitoru", "Radu Tudor Ionescu"], "abstract": "Masked language modeling has become a widely adopted unsupervised technique to pre-train language models. However, the process of selecting tokens for masking is random, and the percentage of masked tokens is typically fixed for the entire training process. In this paper, we propose to adjust the masking ratio and to decide which tokens to mask based on a novel task-informed anti-curriculum learning scheme. First, we harness task-specific knowledge about useful and harmful tokens in order to determine which tokens to mask. Second, we propose a cyclic decaying masking ratio, which corresponds to an anti-curriculum schedule (from hard to easy). We exemplify our novel task-informed anti-curriculum by masking (TIACBM) approach across three diverse downstream tasks: sentiment analysis, text classification by topic, and authorship attribution. Our findings suggest that TIACBM enhances the ability of the model to focus on key task-relevant features, contributing to statistically significant performance gains across tasks. We release our code at https://github.com/JarcaAndrei/TIACBM.", "sections": [{"title": "1 Introduction", "content": "Nowadays, masked language modeling (MLM) (Devlin et al., 2019) is one of the most popular frameworks used to pre-train language models, as it enables the use of vast amounts of unlabeled data. However, the process of selecting tokens for masking is generally based on random selection, while the percentage of masked tokens is typically fixed for the entire training process (Wettig et al., 2023). To the best of our knowledge, there are only two studies attempting to dynamically adapt the masking ratio (Ankner et al., 2024; Yang et al., 2023). These studies concur that the optimal schedule is to use a decaying masking ratio during training. Interestingly, we find that this observation is deeply connected to the curriculum learning paradigm. Curriculum learning is a training strategy formulated by Bengio et al. (2009), where neural models learn the data in a systematic manner, starting with easy samples and gradually adding more difficult samples as the learning progresses. Intuitively, using a higher masking ratio makes the learning task more difficult. Hence, employing a decaying masking ratio corresponds to an anti-curriculum strategy (Liu et al., 2022; Soviany et al., 2022). We discuss additional related works in Appendix A.\nIn this paper, we further develop and explore anti-curriculum learning based on MLM to fine-tune a pre-trained model on downstream tasks. We propose a novel task-informed anti-curriculum by masking (TIACBM) scheme, which employs a cyclic decaying masking ratio and relies on task-specific knowledge to decide which tokens to mask. Our most important contribution is to harness task-specific knowledge about useful and harmful tokens in order to select tokens for masking. For example, in sentiment analysis, the masking probability of a token is determined based on its polarity scores recorded in SentiWordNet 3.0 (Baccianella et al., 2010). In text categorization by topic and authorship attribution, masking a token is conditioned by its part-of-speech tag. While content words receive higher masking probabilities for text categorization by topic, function words and punctuation tokens are more likely to be masked in authorship attribution.\nWe conduct fine-tuning experiments on four datasets: SST-2 (Socher et al., 2013), 20 Newsgroups (Lang, 1995), Reuters-21578 (Lewis, 1987), and PAN 2019 Cross-Domain Authorship Attribution (Kestemont et al., 2019). Our experiments cover a diverse set of downstream tasks, including sentiment analysis, text classification by topic, and authorship attribution. We compare with several baselines, including conventional fine-tuning and standard MLM, and state-of-the-art training strate-"}, {"title": "2 Method", "content": "We propose a novel task-informed anti-curriculum by masking to fine-tune pre-trained language models. Specifically, we employ a cyclic decaying masking ratio which encourages a progressive adaptation of the model over time. In addition, we harness task-specific knowledge to determine which tokens need to be masked, ensuring that the model focuses on the most relevant words in the sentence. By combining a dynamic masking ratio with selective token masking, our strategy can boost performance on complex downstream tasks.\nPrior to the start of the training, we create a vector $r = {r_1 \\ge ... \\ge r_K} \\in [0,1]^K$ containing K masking ratios, which represents the anti-curriculum schedule. Note that $K \\ll T$, where T is the total number of training iterations. Thus, after every K iterations, we reuse the masking ratios starting with $r_1$. In Algorithm 1, we formally present how the masking is performed for a training text sample, given as a sequence of tokens x. In the first step, we compute the number of tokens to be masked N, based on the sequence length and the masking ratio $r_t$ of the current training iteration t. In the second step, for each token $x_i$, we call a task-specific function to compute its importance, taking into account the token and the surrounding text. Additionally, in this step, we also normalize the importance scores to obtain probability values. Finally, in the third and last step, we build a categorical distribution from these probabilities and sample N tokens to mask. We further describe and motivate the task_relevance function for each task.\nSentiment analysis. For polarity classification, we hypothesize that the most subjective words represent the most important features, and masking them will result in a hard-to-easy curriculum. The core foundation of this approach lies in using the SentiWordNet 3.0 sentiment lexicon (Baccianella et al., 2010). For each sentence, we analyze the most probable synset of each word, using a generic Lesk algorithm, and search it in the lexicon. This process aims to determine the most likely positive ($s_{pos}$) and negative scores ($s_{neg}$) for the current word. Both scores range from 0 to 1, with higher values indicating stronger positive or negative connotations, respectively. We emphasize that these scores are linked together and their sum is lower or equal to 1. Based on these values, Baccianella et al. (2010) determine the objectivity score for each word as:\n$o = 1 - (s_{pos} + s_{neg}). \\qquad (1)$\nIn contrast, we leverage the subjectivity score as an importance measure to form the vector s for a given input x in Algorithm 1. Accordingly, we compute the importance score $s_i$ for each token $x_i$ as follows:\n$s_i = (s_{pos} + s_{neg}), \\forall i = 1, ..., |x|. \\qquad (2)$"}, {"title": "3 Experiments and Results", "content": "We evaluate TIACBM on the three tasks, namely sentiment analysis, text categorization and authorship attribution.\nReuters-21578 (Lewis, 1987) is a multi-label text categorization dataset containing 12,449 training and 5,458 test instances. The dataset gathers documents from 90 categories.\n20 Newsgroups (Lang, 1995) is a multi-class dataset for text categorization, which comprises 11,314 training instances and 7,532 test instances belonging to 20 classes.\nThe SST2 dataset (Socher et al., 2013) is a popular benchmark for sentiment analysis, comprising 67,349 training and 872 validation samples, which are labeled either as positive or negative.\nFor authorship attribution, we use the PAN19 (Kestemont et al., 2019) dataset. We report results for Problem 0001 (P1) and Problem 0005 (P5). We discard the unknown files when reporting the metrics in our experiments. Both problems have 9 authors, with 7 training files each. There are 561 test files for P1, and 264 test files for P5."}, {"title": "Cyclic Decaying Masking Ratio", "content": "This is an ablated version of our approach, which simply discards the task-specific information."}, {"title": "Text categorization.", "content": "For text categorization, we hypothesize that content words (nouns, verbs, adjectives, adverbs, and proper names) are more relevant. Consequently, we assign an importance score of 0 to every other part of speech. To compute the importance scores for content words of a given sequence, we also draw upon the knowledge of the pre-trained language model. Before fine-tuning, we extract the attention weights from each attention block and each attention head, given by:\n$A^b_h = softmax\\left(\\frac{Q^b_h(K^b_h)^T}{\\sqrt{d}}\\right), \\qquad (3)$\nwhere b iterates over the self-attention blocks and h iterates over the attention heads, $Q^b_h \\in \\mathbb{R}^{|x| \\times |x| \\times d}$ and $K^b_h \\in \\mathbb{R}^{|x| \\times |x| \\times d}$ are the query and key matrices of the attention block b and head h. The result, $A^b_h \\in \\mathbb{R}^{|x| \\times |x| \\times |x|}$, is a square matrix containing the similarity between each token found in the input sequence x and all the other tokens. We further compute an importance vector a by averaging the attention matrices, as follows:\n$a = \\frac{1}{B \\cdot H} \\sum^{B}_{b=1} \\sum^{H}_{h=1} \\sum^{|x|}_{j=1} A^{b,j}_h, \\qquad (4)$\nwhere B is the number of attention blocks and H is the number of heads. The final importance scores s employed in Algorithm 1 are computed as:\n$s_i=\\begin{cases} a_i, & \\text{if } x_i \\text{ is a content word} \\\\ 0, & \\text{otherwise} \\end{cases}, \\forall i=1,...,|x|. \\qquad (5)$"}, {"title": "Authorship attribution.", "content": "For the authorship attribution task, we compute the importance score vector s using a procedure similar to the one described for text classification. However, instead of using the content words for masking, we mask functional words, such as adpositions, determiners, conjunctions, symbols, particles, and punctuation. Authors exhibit consistent writing style patterns, which are reflected in their use of these functional words (Kestemont, 2014). Consequently, Eq. (5) is modified as follows:\n$s_i=\\begin{cases} a_i, & \\text{if } x_i \\text{ is a functional word} \\\\ 0, & \\text{otherwise} \\end{cases}, , \\forall i=1, ...,|x|, (6)$\nwhere a\u017c is computed as in Eq. (4)."}, {"title": "3.2 Baselines", "content": "We compare TIACBM with five fine-tuning strategies, which are described in detail below.\nConventional. This is the standard fine-tuning approach, which does not involve MLM. It uses the CB-NTR loss (Huang et al., 2021) for Reuters-21578, due to its long-tail distribution.\nConstant. This fine-tuning strategy uses a constant masking ratio to mask input tokens. The masking ratio is set to 15%, following Devlin et al. (2019).\nCart-Stra-CL++. This is a state-of-the-art easy-to-hard curriculum approach introduced by Poesina et al. (2024). This method needs to perform data cartography for the baseline fine-tuned with the conventional regime, before employing the curriculum. This essentially doubles the training time.\nDecaying Masking Ratio. The decaying masking ratio, a.k.a. anti-curriculum by masking, is proposed by Ankner et al. (2024). This training strategy can be seen as an ablated version of our approach, which is obtained by dropping the cyclical regime (K = T) and by discarding task-specific information."}, {"title": "4 Conclusion", "content": "We proposed a novel task-informed anti-curriculum by masking approach (TIACBM), and we evaluated its effectiveness on three tasks: sentiment analysis, text categorization by topic, and authorship attribution. The proposed method leverages information about the downstream tasks to decide which tokens to select for masking in a novel anti-curriculum by masking framework. On all the three tasks, our method achieved better results across all experiments, outperforming both baselines and state-of-the-art methods. Moreover, our method performed well on both multi-label and multi-class classification, while also proving resilience against imbalanced datasets, such as Reuters-21578. Additionally, we also showed that TIACBM is effective in scenarios with a low number of training samples, as it is the case of PAN19."}, {"title": "5 Limitations", "content": "We present a novel method to consistently improve the performance of language models on downstream tasks. However, there is no universal anti-curriculum (masking ratio) schedule that can work for all models or data sets, representing an important parameter to be optimized by the user. Still, a general empirical statement proven in our work is that cycling anti-curriculum schedulers are superior in NLP, when it comes to curriculum by masking. Additionally, our method computes token relevance using a task-specific approach and this can be challenging to design for some tasks. We show on two tasks that language model attention weights can effectively serve this purpose."}, {"title": "6 Ethics Statement", "content": "To our knowledge, the proposed method poses no immediate risk. However, it can be adapted for generative modeling, which raises concerns about its potential misuse for malicious purposes, such as fake content generation."}, {"title": "A Related Work", "content": "Our framework is mostly related to work on curriculum learning (Bengio et al., 2009). Soviany et al. (2022) divide curriculum learning methods into data-level (Chang et al., 2021; Gong et al., 2021; Kocmi and Bojar, 2017; Liu et al., 2018; Nagatsuka et al., 2023), model-level (Croitoru et al., 2025; Sinha et al., 2020), task-level (Liu et al., 2020a; Narvekar et al., 2016), and objective-level (Pathak and Paffenroth, 2019) strategies. Most of existing studies focus on computer vision (Croitoru et al., 2025; Huang et al., 2020; Sinha et al., 2020) and reinforcement learning (Fang et al., 2019; Florensa et al., 2017). Methods in these domains are vaguely related to our approach, with some exceptions that employ curriculum based on masked image modeling (MIM) (Jarca et al., 2024; Madan et al., 2024). In the image domain, the MIM approach was explored from multiple perspectives, which led to the development of adaptive masking strategies based on curriculum learning (Madan et al., 2024), that can produce more robust representations. A notable finding is that an easy-to-hard curriculum works generally well for image masking (Jarca et al., 2024). In contrast, analogous studies focusing on text (Ankner et al., 2024; Yang et al., 2023) suggest that a hard-to-easy curriculum, i.e. using a decaying masking ratio, is more appropriate for text. Our results confirm the observations of Ankner et al. (2024) and Yang et al. (2023), although we reset the masking ratio during training, resulting in a cyclic decaying masking ratio."}, {"title": "B Masking Ratio Schedules", "content": "We fix the decaying masking ratio schedules r = {r1,...,rk}, that are employed in Algorithm 1, through validation. For both BERT and RoBERTa, we mention the maximum and minimum masking ratios in Table 2, as well as the length K. We cycle through the schedules every K epochs."}, {"title": "C Computational Resources", "content": "The experiments are carried out on a machine with 64GB of RAM, an AMD Ryzen 7 7800x3d CPU, and an Nvidia GeForce RTX 4090 GPU. Our most expensive experiments are performed on 20 Newsgroups, with 112 mins (3 mins per epoch) for TIACBM, and about 180 mins for decaying runs, due to the need of masking at every epoch. The masking step takes between 2-3 mins. In the case of SST2, the experiments require 6 mins per epoch, while the masking step requires 0.5 mins. The timetable for PAN19 varies from 0.5 mins per epoch to 1 mins per epoch, regardless of the approach, while the masking takes roughly 3-4 secs. Finally, the masking step on Reuters-21578 takes"}, {"title": "D Curriculum vs Anti-Curriculum", "content": "In Table 3, we compare our anti-curriculum method (TIACBM) with a reversed masking ratio schedule, which implements an easy-to-hard curriculum learning. Both approaches benefit from a cyclic schedule and task-specific information. The anti-curriculum approach consistently outperforms its counterpart, validating our choice based on hard-to-easy curriculum."}]}