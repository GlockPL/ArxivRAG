{"title": "Unbiased GNN Learning via\nFairness-Aware Subgraph Diffusion", "authors": ["Abdullah Alchihabi", "Yuhong Guo"], "abstract": "Graph Neural Networks (GNNs) have demonstrated remarkable efficacy in tackling\na wide array of graph-related tasks across diverse domains. However, a significant\nchallenge lies in their propensity to generate biased predictions, particularly with\nrespect to sensitive node attributes such as age and gender. These biases, inherent\nin many machine learning models, are amplified in GNNs due to the message-\npassing mechanism, which allows nodes to influence each other, rendering the\ntask of making fair predictions notably challenging. This issue is particularly\npertinent in critical domains where model fairness holds paramount importance.\nIn this paper, we propose a novel generative Fairness-Aware Subgraph Diffusion\n(FASD) method for unbiased GNN learning. The method initiates by strategically\nsampling small subgraphs from the original large input graph, and then proceeds to\nconduct subgraph debiasing via generative fairness-aware graph diffusion processes\nbased on stochastic differential equations (SDEs). To effectively diffuse unfairness\nin the input data, we introduce additional adversary bias perturbations to the\nsubgraphs during the forward diffusion process, and train score-based models to\npredict these applied perturbations, enabling them to learn the underlying dynamics\nof the biases present in the data. Subsequently, the trained score-based models\nare utilized to further debias the original subgraph samples through the reverse\ndiffusion process. Finally, FASD induces fair node predictions on the input graph\nby performing standard GNN learning on the debiased subgraphs. Experimental\nresults demonstrate the superior performance of the proposed method over state-of-\nthe-art Fair GNN baselines across multiple benchmark datasets.", "sections": [{"title": "1 Introduction", "content": "Graph Neural Networks (GNNs) have exhibited remarkable efficacy in addressing a multitude of\ngraph-related tasks across various domains, ranging from recommendation systems [34] to drug\ndiscovery [35]. Despite their successes, GNNs have been show to be susceptible to inheriting biases\npresent in the training data [8], potentially exhibiting more pronounced biases than traditional machine\nlearning models. This increased vulnerability arises from GNNs' utilization of message passing\nmechanisms to propagate information across the graph, facilitating mutual influence among nodes\nand thereby amplifying existing biases in the data [8]. The prevalence of biased patterns in collected\ndatasets [4, 7, 11], coupled with standard GNNs' inability to effectively mitigate these biases [8],\nrestricts their applicability in domains where fairness across sensitive attributes such as age, gender,\nand ethnicity is paramount, such as healthcare [28] and job applicant evaluations [24].\nTraditional fair learning methods [17, 36] fail to consider the underlying graph structure and the\ninterplay between nodes through message propagation, making them unsuitable for direct application\nto graph data. Therefore, it is important to develop methodologies capable of generating both accurate\nand fair predictions on graphs. Several endeavors have emerged to address this need, focusing on"}, {"title": "2 Related Works", "content": "Existing fair GNN learning methods can be classified into three primary categories: pre-processing,\nin-processing, and post-processing techniques. Pre-processing methods aim to mitigate bias within\nthe input graph data prior to training standard GNN models for downstream tasks. For instance,\nFairWalk [27] modifies the random walk component in node2vec [13] to sample more diverse\nneighborhood information, resulting in less biased node embeddings. FairDrop [31] employs a\nheuristic edge-dropout strategy to debias the graph structure by removing edges between nodes with\nsimilar sensitive attribute values. Graphair [22] filters out sensitive attribute information from graph\nstructure and node features to learn fair graph representations. EDITS [10] introduces a Wasserstein\ndistance approximator that alternates between debiasing node features and graph structure. However,\nmany existing pre-processing methods lack sufficient robustness, as they are specifically designed\nto address particular forms of bias presumed to exist in the data, which restricts their applicability\nacross diverse domains. In-processing methods adapt the learning process to achieve a balance\nbetween accuracy and fairness through the incorporation of fairness-aware regularization techniques.\nIn particular, NIFTY [1] introduces graph contrastive learning with fairness-aware augmentations and\na novel loss function to balance fairness and stability. FairAug [21] proposes several fairness-aware\nadaptive graph augmentation strategies, including feature masking, node sampling, and edge deletion,\nto debias the graph data. REDRESS [9] introduces a ranking-based objective to promote individual\nfairness. Adversarial learning has also been utilized to eliminate sensitive attribute information\nfrom the learned node embeddings [12, 8, 5]. However, in-processing methods often require careful\ntraining to balance fairness and informativeness objectives, which can significantly compromise their\nstability. Post-processing methods on the other hand focus on modifying the predictions of GNN\nmodels to promote fairness and alleviate bias [15, 26]. Further details on Fair GNN Learning methods\ncan be found in the survey paper [6]."}, {"title": "2.1 Fair GNN Learning", "content": "developing fairness-aware models tailored specifically to GNNs [1, 8, 21] and balancing fairness\nand informativeness objectives in GNN training. In particular, adversarial learning has been utilized\nto debias learned representations and predictions by employing adversary discriminator networks\nto remove sensitive attributes from learned embeddings [8, 5, 12]. Various strategies for fairness-\naware graph data augmentation have been explored, including augmenting node features [21], graph\nstructures [31], or both [22] to debias the input data and promote fairness in downstream predictions\nwith GNN models. Nevertheless, existing fairness-aware graph augmentation methods are often\ntailored to address specific forms of bias assumed to be present in the data [21, 1, 31], lacking\nsufficient and broad applicability across the diverse application domains of GNNs [22], highlighting\nthe necessity for developing data-adaptive fairness-aware graph augmentation and learning methods."}, {"title": "2.2 Graph Diffusion Methods", "content": "Graph diffusion methods have exhibited notable efficacy in capturing the underlying distribution of\ngraph data, facilitating the synthesis of high-fidelity graph samples closely aligned with the input\ndata distribution. These methods are broadly categorized into two main groups: continuous diffusion\nmethods and discrete diffusion methods. Discrete methods often model the diffusion process using\nMarkov noise processes, while stochastic differential equations commonly model the diffusion\nprocess in continuous methods. For instance, DiGress [33] employs a discrete diffusion process to\ngenerate graph instances with categorical node features and edge attributes. GDSS [16], on the other\nhand, models the joint distribution of node features and graph structure using stochastic differential\nequations. Kong et al. [20] accelerate the training and sampling processes of diffusion models through\nan autoregressive diffusion model, while Haefeli et al. [14] leverage discrete noise in the forward\nMarkov process to ensure the discreteness of the graph structure throughout the forward diffusion\nprocess, thereby enhancing sample quality and speeding up sampling. Additionally, PRODIGY [29]\nadopts projected diffusion to enforce explicit constraints on synthesized graph samples in terms of\nedge count and valency. Further details on graph diffusion methods can be found in [23]. However,\nprior graph diffusion methods typically synthesize new samples from the distribution of input graph\ndata, rendering them susceptible to inheriting biases present in the input data. To our knowledge, our\nFairness-Aware Subgraph Diffusion method represents the first endeavor towards a fairness-aware\ngraph diffusion approach, harnessing graph diffusion to promote fairness in downstream tasks."}, {"title": "3 Method", "content": "We consider the following fairness-sensitive semi-supervised node classification problem. The input\ngraph is given as $G = (V, E)$, where $V$ is the set of nodes with size $N = |V|$ and $E$ denotes the set\nof edges of the graph. Typically, $E$ is represented by an adjacency matrix $A$ of size $N \\times N$. Each\nnode in the graph is associated with a feature vector of size $D$, and the features of all nodes in the\ngraph are represented by an input feature matrix $X \\in \\mathbb{R}^{N \\times D}$. Additionally, each node in the graph is\nassociated with a binary sensitive attribute $s \\in \\{0,1\\}$, and the sensitive attributes of all nodes are\nrepresented by a sensitive attribute vector $S$ of size $N$. The nodes are split into two distinct subsets:\n$V_e$ and $V_u$ comprising the labeled and unlabeled nodes, respectively. The labels of nodes in $V_e$ are\nrepresented by a label indicator matrix $Y^e \\in \\{0,1\\}^{N_e \\times C}$, where $C$ is the number of classes and\n$N_e$ is the number of labeled nodes in the graph. The objective is to learn a GNN model capable of\naccurately and fairly predicting the labels of the nodes. Here, we focus on a widely studied fairness\ncriterion, group fairness [15, 37], which stipulates that a model should not exhibit bias in favor of or\nagainst demographic groups in the data, as defined by their sensitive attributes. To assess the fairness\nof a model, we employ two standard metrics: statistical parity [11] and equal opportunity [15]."}, {"title": "3.1 Problem Setup", "content": "We consider the following fairness-sensitive semi-supervised node classification problem. The input\ngraph is given as $G = (V, E)$, where $V$ is the set of nodes with size $N = |V|$ and $E$ denotes the set\nof edges of the graph. Typically, $E$ is represented by an adjacency matrix $A$ of size $N \\times N$. Each\nnode in the graph is associated with a feature vector of size $D$, and the features of all nodes in the\ngraph are represented by an input feature matrix $X \\in \\mathbb{R}^{N \\times D}$. Additionally, each node in the graph is\nassociated with a binary sensitive attribute $s \\in \\{0,1\\}$, and the sensitive attributes of all nodes are\nrepresented by a sensitive attribute vector $S$ of size $N$. The nodes are split into two distinct subsets:\n$V_e$ and $V_u$ comprising the labeled and unlabeled nodes, respectively. The labels of nodes in $V_e$ are\nrepresented by a label indicator matrix $Y^e \\in \\{0,1\\}^{N_e \\times C}$, where $C$ is the number of classes and\n$N_e$ is the number of labeled nodes in the graph. The objective is to learn a GNN model capable of\naccurately and fairly predicting the labels of the nodes. Here, we focus on a widely studied fairness\ncriterion, group fairness [15, 37], which stipulates that a model should not exhibit bias in favor of or\nagainst demographic groups in the data, as defined by their sensitive attributes. To assess the fairness\nof a model, we employ two standard metrics: statistical parity [11] and equal opportunity [15]."}, {"title": "3.2 Fairness-Aware Subgraph Diffusion", "content": "In this section, we present the proposed Fairness-Aware Subgraph Diffusion (FASD) method de-\nsigned to address the intricate challenge of fair semi-supervised node classification. The generative\ncontinuous diffusion processes for FASD are modeled on a set of subgraph instances sampled from\nthe input graph via Stochastic Differential Equations (SDEs). During the forward diffusion process,\nfairness-based adversary perturbations, derived from a sensitive attribute prediction model, are in-\ntroduced onto the subgraphs. These perturbations serve to amplify existing biases within subgraph\ninstances and furnish learnable debiasing targets for score-based bias prediction models. The trained\nscore-based models are then employed to mitigate biases on the input subgraph instances via the\nreverse-diffusion process, resulting in their debiased counterparts. Finally, standard graph neural\nnetwork (GNN) learning is applied to the debiased subgraphs, facilitating the generation of node\npredictions that are both fair and accurate. An overview of the FASD framework is depicted in Figure\n1. In the remainder of this section, we elaborate on the details of the proposed FASD method."}, {"title": "3.2.1 Subgraph-level Instance Sampling", "content": "In the context of fair semi-supervised node classification, the input comprises a single graph $G$.\nHowever, generative graph diffusion methods require a multitude of graph samples to effectively"}, {"title": "3.2.2 Fairness-Aware Forward Diffusion", "content": "The forward graph diffusion process is modeled via a system of Stochastic Differential Equations\n(SDEs) [16, 30], as follows:\n$dG^{(i)}_t = f_t(G^{(i)}_t)dt + g_t(G^{(i)}_t) dw, G^{(i)} \\in G$.\nHere, $G^{(i)} \\in G$ denotes the input subgraph, and $G^{(i)}_t$ represents the perturbed subgraph at time-step $t$,\nwhere $t \\in [0, T]$. $f_t(.)$ denotes the linear drift coefficient, $g_t$ represents the diffusion coefficient, and\n$w$ signifies the Wiener process. Previous graph diffusion methods are fairness-agnostic, perturbing\ninput samples solely with stochastic noise during the forward diffusion process. With the goal\nof inducing a debiasing reverse diffusion process for fair graph generation, we propose to further\nincorporate fairness-adversary perturbations in the forward diffusion process.\nAs the key aspect of fairness is reflected in whether a prediction model exhibits bias in favor of or\nagainst demographic groups defined by the sensitive attributes of the data, we attempt to capture the"}, {"title": "3.2.3 Estimating Diffusion Perturbations via Score-Based Models", "content": "To estimate the perturbations in the fairness-aware forward diffusion process, we introduce two score-\nbased models: $s_{\\theta,t}(G^{(i)})$ and $s_{\\phi,t}(G^{(i)})$. These models are designed to approximate the perturbation\nscores applied to obtain $X^{(i)}_t$ and $A^{(i)}_t$, respectively. These models leverage permutation-equivariant\nGNN-based architectures [16], given their effectiveness at capturing the complex interdependencies\nbetween $X^{(i)}_t$ and $A^{(i)}_t$ over time. Specifically, the score-based model $s_{\\theta,t}(G^{(i)})$ is defined as follows:\n$s_{\\theta,t}(G^{(i)}) = MLP_{\\theta}([{H_j}^{(i)}_{j=0}])$\n$H_{j+1} = GNN_X(H_j, A^{(i)}_t), H_0 = X^{(i)}_t$\nwhere $L$ denotes the number of GNN-based layers, $MLP$ signifies a multi-layer perceptron (fully-\nconnected layers), and $[.]$ represents the concatenation operation. The output of $s_{\\theta,t}(G^{(i)}) \\in \\mathbb{R}^{(N^{(i)},D)}$"}, {"title": "3.2.4 Subgraph Debiasing via Reverse Diffusion", "content": "Prior graph diffusion methods synthesize new graph samples from random noise through reverse\ndiffusion via reverse-time SDEs [2]. In contrast, we utilize the score-based models, $s_{\\theta,t}$ and $s_{\\phi,t}$,\nlearned from the forward diffusion process to estimate perturbations and further reduce the bias\npatterns in the input subgraph instances through a fairness-aware reverse diffusion process, thereby\ngenerating their debiased counterparts. This reverse graph diffusion process over a subgraph $G^{(i)} =$\n$(X^{(i)}_0, A^{(i)}_0)$ can be modeled via the following reverse-time SDEs:\n$dX^{(i)}_t = [f_{1,t}(X^{(i)}_t) - g_{1,t}s_{\\theta,t}(G^{(i)})]dt + g_{1,t}dW_1$\n$dA^{(i)}_t = [f_{2,t}(A^{(i)}_t) - g_{2,t}s_{\\phi,t}(G^{(i)})]dt + g_{2,t}dW_2$\nHere $w_1$ and $w_2$ represent the reverse-time Wiener processes, while $dt$ denotes the infinitesimal\nnegative time step. $f_{1,t}$ and $f_{2,t}$ are linear drift coefficients, and $g_{1,t}$ and $g_{2,t}$ are diffusion coefficients.\nSpecifically, instead of starting from random noise, we initialize the reverse diffusion process with the\nsubgraphs $G$ sampled from the input data distribution\u2014the original input graph $G$. The score-based\nmodels are then deployed to iteratively debias the subgraph node features and connectivity structure\nat each step of the reverse diffusion process, thereby approximating an unbiased distribution of the\ninput data. This reverse diffusion process can be denoted as follows:\n$\\tilde{G}^{(i)} = reverse\\_diffusion(G^{(i)}, N_{steps})$\nwhere $\\tilde{G}^{(i)}$ represents the debiased subgraph for the input subgraph $G^{(i)}$, and $N_{steps}$ denotes the\nnumber of steps of the reverse-diffusion process. As the reverse-diffusion process is modeled as a\nsystem of two SDEs interconnected through score functions, we utilize the Predictor-Corrector (PC)\nSampler [30] to approximate this process. The PC sampler comprises four components: two Predictors\n(Predictor$_X$, Predictor$_A$) and two Correctors (Corrector$_X$, Corrector$_A$). These components iteratively\ndebias the input subgraph instances. At each reverse-diffusion step, the Predictors employ current\nestimates of subgraph node features and adjacency matrix, leveraging the trained score-based models\n$s_{\\theta,t}$ and $s_{\\phi,t}$, to generate initial debiased estimates. Subsequently, the Correctors refine these estimates.\nAfter $N_{steps}$ reverse-diffusion steps, we obtain the debiased subgraph node input features $X^{(i)}$ and"}, {"title": "3.2.5 Fair Node Classification with Debiased Subgraphs", "content": "The debiased subgraph set $\\tilde{G}$, obtained through our proposed FASD, represents the debiased input\ngraph and serves as the training data for a standard GNN-based node classification model $f$, such that\n$P^{(i)} = f(\\tilde{X}^{(i)}, \\tilde{A}^{(i)})$.\nHere $\\tilde{X}^{(i)}$ and $\\tilde{A}^{(i)}$ denote the debiased node feature matrix and debiased adjacency matrix of\nsubgraph $\\tilde{G}^{(i)} \\in \\tilde{G}$, respectively. $P^{(i)} \\in \\mathbb{R}^{N^{(i)} \\times C}$ represents the predicted class probability matrix\nfor all nodes in the subgraph. The GNN model $f$ is trained by minimizing the node classification loss\nacross all the debiased subgraphs:\n$L = \\sum_{\\tilde{G}^{(i)} \\in \\tilde{G}} \\sum_{u \\in V^{(i)}} lce (P^{(i)}_u, Y_u)$\nwhere $lce$ denotes the standard cross-entropy loss, while $P^{(i)}_u$ and $Y_u$ are the predicted class probability\nvector and ground-truth label indicator vector of node $u$, respectively. $V^{(i)}$ denotes the set of labeled\nnodes in subgraph $\\tilde{G}^{(i)}$. For inference on unlabeled nodes, the predicted class probability vector $P_u$\nfor a given node $u$ is obtained by averaging the prediction vectors of model $f$ over all the subgraphs\nin $\\tilde{G}$ containing node $u$:\n$P_u = \\frac{1}{|G_u|} \\sum_{\\tilde{G}^{(i)} \\in G_u} P^{(i)}_u$\nwhere $P^{(i)}_u$ is predicted class probability vector of node $u$ using subgraph $\\tilde{G}^{(i)}$, and $G_u$ signifies the\nset of subgraphs from $G$ containing node $u$."}, {"title": "4 Experiments", "content": "We conduct experiments on three benchmark graph datasets: NBA [8],\nPokec-z and Pokec-n [32]. Across all three datasets, the sensitive attributes and class labels take\nbinary values. To ensure a fair comparison, we adhere to the same train/validation/test splits provided\nby [22]. Comprehensive details regarding the benchmark datasets are provided in Appendix E. We\ncompare our proposed method with five fair GNN learning methods, namely FairWalk [27], FairDrop\n[31], NIFTY [1], FairAug [21] and Graphair [22], along with two fairness-agnostic graph contrastive\nlearning methods, GRACE [38], and GCA [39]."}, {"title": "4.1 Experimental Setup", "content": "Datasets & Baselines We conduct experiments on three benchmark graph datasets: NBA [8],\nPokec-z and Pokec-n [32]. Across all three datasets, the sensitive attributes and class labels take\nbinary values. To ensure a fair comparison, we adhere to the same train/validation/test splits provided\nby [22]. Comprehensive details regarding the benchmark datasets are provided in Appendix E. We\ncompare our proposed method with five fair GNN learning methods, namely FairWalk [27], FairDrop\n[31], NIFTY [1], FairAug [21] and Graphair [22], along with two fairness-agnostic graph contrastive\nlearning methods, GRACE [38], and GCA [39].\nEvaluation Criteria We evaluate the performance of our proposed FASD and all comparison base-\nlines using three distinct metrics: accuracy, demographic parity, and equal opportunity. Demographic\nparity is computed as follows: $\\Delta DP = |P(\\hat{Y} = 1|S = 0) - P(\\hat{Y} = 1|S = 1)|$ where $\\hat{Y}$ denotes\nthe predicted class label and $S$ denotes the sensitive attribute value. Meanwhile, equal opportunity\nis calculated as follows: $\\Delta EO = |P(\\hat{Y} = 1|S = 0, Y = 1) - P(\\hat{Y} = 1|S = 1, Y = 1)|$ where\n$Y$ denotes the ground-truth class label. All three metrics are measured over the test nodes of each\ndataset. It is important to note that smaller values for $\\Delta DP$ and $\\Delta EO$ indicate enhanced fairness."}, {"title": "4.2 Comparison Results", "content": "We evaluate the efficacy of our proposed FASD method and compare it with the other comparison\nmethods in the context of fair semi-supervised node classification. The mean and standard deviation\nof all three performance metrics\u2014Accuracy (Acc.%), Demographic Parity ($\\Delta_{DP}$%), and Equal\nOpportunity ($\\Delta EO$%)\n The table clearly shows the trade-off between accuracy and fairness metrics, where methods achieving\nthe highest accuracy tend to exhibit pronounced bias (low fairness). Conversely, approaches prioritiz-\ning fairness often experience a decline in accuracy. This underscores the formidable challenge in fair\nnode classification, necessitating the enhancement of fairness without significantly compromising\naccuracy. Our proposed method achieves the most notable results in terms of equal opportunity across\nall three datasets, significantly surpassing all other methods. Notably, it yields performance gains\nof 29% (0.8) and 43% (0.71) over the second-best method on the Pokec-z and Pokec-n datasets,\nrespectively. Similarly, our FASD method outperforms all baselines in demographic parity on NBA\nand Pokec-n datasets, while securing the second-best results on Pokec-z dataset. The substantial\nimprovement in demographic parity over the second-best baseline exceeds 64% (1.64) and 60%\n(1.23) on NBA and Pokec-n datasets, respectively. Furthermore, FASD demonstrates only a minimal\ndecrease in accuracy, underscoring its ability to strike a great balance between fairness and accuracy.\nThese findings highlight the superior performance of our proposed method compared to existing\nstate-of-the-art fair GNN learning methods."}, {"title": "4.3 Ablation Study", "content": "We conducted an ablation study to investigate our proposed methodology, considering two variants:\n(1) Drop Fairness-Aware Subgraph Diffusion (w/o Diffusion): the node classification model $f$ is\ntrained directly on the sampled subgraphs without incorporating Fairness-Aware Subgraph Diffusion.\n(2) Drop Fairness-Based Perturbations in the forward diffusion (w/o Fairness): only stochastic\nperturbations are applied in the forward diffusion process, thereby discarding the fairness-aware\ncomponent of subgraph forward diffusion. The ablation study results are presented in Table 2.\nTable 2 illustrates the substantial improvement in fairness achieved by our full proposed method\ncompared the two variants, across both equal opportunity and demographic parity metrics on all the\nthree datasets. Furthermore, the full proposed method achieves the highest accuracy on both NBA\nand Pokec-n datasets. The Drop Fairness-Aware Subgraph Diffusion variant (w/o Diffusion) yields\nlower fairness results in both fairness evaluation metrics across all the three datasets, underscoring\nthe tendency for sampled subgraphs to inherit biases present in the data. Similarly, the Drop Fairness\nBased Perturbations variant (w/o Fairness) exhibits a decline in performance regarding fairness\nmetrics on all the three datasets, highlighting the limitations of standard graph diffusion methods in\nenhancing fairness within the context of Fair GNN learning."}, {"title": "4.4 Hyper-Parameter Sensitivity", "content": "To investigate the impact of the hyper-parameters in our proposed FASD method, we present the\nresults of several sensitivity analyses on NBA, Pokec-z and Pokec-n datasets in Figure 2. Specifically,\nFigures 2a, 2b, 2c illustrate the impact of varying the number of reverse-diffusion steps $N_{steps}$ on both\naccuracy and demographic parity. Additionally, Figures 2d, 2e, and 2f depict the effects of adjusting\nthe hyperparameters $\\lambda_X$ and $\\lambda_A$, which control the contribution of fairness-based perturbations in\nthe fairness-aware forward diffusion process.\nIn the case of $N_{steps}$ hyper-parameter, the top-left corner point represents the $N_{steps}$ value that\noptimally balances high accuracy with low bias (high fairness). Across all datasets, our findings\nindicate that optimal performance is achieved with a relatively small number of reverse-diffusion\nsteps ($\\left\\{2, 3, 4, 5\\right\\}$). Excessive steps ($N_{steps} > 5$) result in diminished model accuracy and fairness,\nwhile a single step ($N_{steps} = 1$) yields high accuracy but with pronounced bias. This underscores the\nnecessity of multiple reverse-diffusion steps to effectively debias the input subgraphs. Similarly, for\n$\\lambda_X$ and $\\lambda_A$ hyper-parameters, the top-left corner point represents the $\\lambda_X$ and $\\lambda_A$ values that strike the\nideal balance between accuracy and fairness. Our results reveal that very small values ($\\lambda_X, \\lambda_A < 0.1$)\noffer marginal improvement in fairness, while excessively large values ($\\lambda_X, \\lambda_A > 10$) significantly\nimpair accuracy. Values within the range of $[0.1, 10.0]$ achieve the optimal balance, with 0.1 yielding\nthe best results for NBA dataset and 10.0 yielding optimal performance for Pokec-z and Pokec-n."}, {"title": "5 Conclusion", "content": "In this paper, we introduced a novel Fairness-Aware Subgraph Diffusion (FASD) method to tackle fair\nGNN learning for graph node classification. FASD generates fairness-based subgraph perturbations\nusing an adversary sensitive attribute prediction model. These perturbations are then integrated into\nthe fairness-aware forward diffusion process. Through the training of score-based models to predict\nthe applied perturbation scores, we enable the learning of the underlying dynamics of fairness and\nbias within the subgraph data. Subsequently, the trained score-based models are employed to debias\nthe subgraph samples via the reverse-diffusion process. Finally, standard node classification models\nare trained on the debiased subgraphs to yield fair and accurate predictions. Experimental evaluations\nconducted on three benchmark datasets validate the effectiveness of our proposed method. The\nempirical results demonstrate the capacity of the proposed FASD for successfully mitigating bias\ninherent in the data, outperforming state-of-the-art Fair GNN methods."}, {"title": "A Subgraph Sampling Procedure", "content": "The details of the subgraph sampling procedure are presented in Algorithm 1."}, {"title": "B Perturbation Kernels", "content": "We utilize the perturbation kernels $\\psi_{1,t}(.)$ and $\\psi_{2,t}(.)$ provided in [30]. The mean and standard\ndeviation of $X^{(i)}_0$ at time t are calculated using perturbation kernel $\\psi_{1,t}(.)$ as follows:\n$\\mu_t(X^{(i)}) = exp(\\frac{-t^2 (\\beta_{1,max} - \\beta_{1,min})}{4} - \\frac{t \\beta_{1,min}}{2} ) X^{(i)}_0$\n$\\sigma_t(X^{(i)}) = \\sqrt{1 - exp(\\frac{-t^2 (\\beta_{1,max} - \\beta_{1,min})}{4} - \\frac{t \\beta_{1,min}}{2} )}$\nwhere $\\beta_{1,max}$ and $\\beta_{1,min}$ are hyper-parameters of perturbation kernel $\\psi_{1,t}(.)$ that govern the rate at\nwhich perturbations are applied to the data over time, and $exp(.)$ is the exponential function. Similarly,\nthe mean and standard deviation of $A^{(i)}_0$ at time t are calculated using perturbation kernel $\\psi_{2,t}(.)$ as\nfollows:\n$\\mu_t(A^{(i)}) = exp(\\frac{-t^2 (\\beta_{2,max} - \\beta_{2,min})}{4} - \\frac{t \\beta_{2,min}}{2} ) A^{(i)}_0$\n$\\sigma_t(A^{(i)}) = \\sqrt{1 - exp(\\frac{-t^2 (\\beta_{2,max} - \\beta_{2,min})}{4} - \\frac{t \\beta_{2,min}}{2} )}$\nwhere $\\beta_{2,max}$ and $\\beta_{2,min}$ are hyper-parameters of perturbation kernel $\\psi_{2,t}(.)$. The standard deviations\n$\\sigma_t(X^{(i)})$ and $\\sigma_t(A^{(i)})$ depend solely on time t and increase asymptotically towards $\\sigma_t \\rightarrow  \\sigma_t(X^{(i)}) = $\n$\\sigma_t(A^{(i)}) = 1$ as t increases. Conversely, the means $\\mu_t(X^{(i)})$ and $\\mu_t(A^{(i)})$ decrease as t increases."}, {"title": "C Score-based Models Training Procedure", "content": "The details of the training procedure of the score-based models are provided in Algorithm 2."}, {"title": "D PC Sampler", "content": "We adopt Euler-Maruyama numerical solver [19] for the two Predictors (Predictor$_X$, Predictor$_A$) and\nLangevin MCMC [25] for the two Correctors (Corrector$_X$, Corrector$_A$). In the case of the Euler-"}, {"title": "E Benchmark Datasets", "content": "In the NBA dataset, nodes represent NBA players, with node features comprising player information\nand performance statistics. The sensitive attribute pertains to nationality, while labels indicate\nwhether a player's salary surpasses the median. Regarding the Pokec-z and Pokec-n datasets, both\nare derived from the Pokec dataset, a prominent social network in Slovakia. Each node corresponds\nto a user profile, with features encompassing profile details such as hobbies, interests, and age. The\nsensitive attribute denotes the user's region, while labels reflect the user's field of work. For the\ntrain/validation/test splits, we allocate 20%/35%/45%, 10%/10%/80%, and 10%/10%/80% of the\ngraph nodes in the NBA, Pokec-z, and Pokec-n datasets, respectively."}, {"title": "F Implementation Details", "content": "For each dataset", "Diffusion": "The sensitive attribute prediction model consists of two\nGCN layers [18", "Models": "In the case of the score-based\nmodel $s_{\\theta,t}$, GNN$_X$ consists of 3 GCN layers followed by hyperbolic tangent (tanh) activation\nfunctions, while"}]}