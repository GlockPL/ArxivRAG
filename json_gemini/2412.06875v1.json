{"title": "VQ4ALL: Efficient Neural Network Representation via a Universal Codebook", "authors": ["Juncan Deng", "Shuaiting Li", "Zeyu Wang", "Hong Gu", "Kedong Xu", "Kejie Huang"], "abstract": "The rapid growth of the big neural network models puts forward new requirements for lightweight network representation methods. The traditional methods based on model compression have achieved great success, especially VQ technology which realizes the high compression ratio of models by sharing code words. However, because each layer of the network needs to build a code table, the traditional top-down compression technology lacks attention to the underlying commonalities, resulting in limited compression rate and frequent memory access. In this paper, we propose a bottom-up method to share the universal code-book among multiple neural networks, which not only effectively reduces the number of codebooks but also further reduces the memory access and chip area by storing static code tables in the built-in ROM. Specifically, we introduce VQ4ALL, a VQ-based method that utilizes codewords to enable the construction of various neural networks and achieve efficient representations. The core idea of our method is to adopt a kernel density estimation approach to extract a universal codebook and then progressively construct different low-bit networks by updating differentiable assignments. Experimental results demonstrate that VQ4ALL achieves compression rates exceeding 16 \u00d7 while preserving high accuracy across multiple network architectures, highlighting its effectiveness and versatility.", "sections": [{"title": "1. Introduction", "content": "Neural networks have become indispensable tools in modern computer vision tasks due to their exceptional performance. However, these models are often resource-intensive, requiring substantial memory and computational power. Deploying them on resource-constrained platforms, where multiple networks may need to run simultaneously, remains a significant challenge. Addressing this limitation requires two critical advancements: enabling more universal network representation and more efficient network compression.\nTraditional methods for reducing model size and enabling hardware acceleration typically employ techniques such as pruning and quantization. While these methods can achieve acceleration to some extent, they often suffer from significant performance degradation when aiming for extreme model compression. This is primarily because low-bit quantization introduces large quantization errors, often requiring mixed precision to maintain performance. Meanwhile, pruning leads to the loss of important weights, causing substantial structural differences in the pruned model. Both methods result in low hardware compatibility, making it challenging to design a unified accelerator architecture.\nVector quantization is a more hardware-friendly method of model compression that can achieve a higher compression ratio. However, traditional vector quantization requires training a separate codebook for each neural network, leading to long training times. Additionally, when achieving higher compression ratios, larger codebooks are often needed, resulting in longer loading times when switching between tasks. To address these challenges, we propose that it is essential to explore the underlying commonalities across neural networks. By leveraging these shared logical patterns, we can construct multiple neural network models in a bottom-up fashion. This approach not only effectively reduces the model size but also facilitates the design of unified low-level hardware, enabling general-purpose acceleration capabilities. Furthermore, it significantly reduces model compression time and holds the potential for enhancing the interpretability of neural networks.\nIn this paper, we introduce VQ4ALL, a VQ-based method that enables universal network representation and efficient network compression. VQ4ALL adopts a kernel density estimation approach to extract a universal codebook and then learn the differentiable assignments to gradually construct the low-bit neural network. The universal code-"}, {"title": "2. Related Work", "content": "Quantization is a commonly used method in network compression, which is mainly divided into two types: uniform quantization and vector quantization.\nUniform Quantization. Uniform quantization converts floating-point weights into integer values using scale coefficients, applied either tensor-wise, channel-wise, or group-wise. Most current methods focus on 8-bit or lower precision, often leveraging second-order information or quantization errors to enhance the process. To improve the accuracy of quantized models, training-time quantization techniques have been proposed. For instance, EWGS [14] adjusts gradients by scaling them based on Hessian approximations for each layer, while PROFIT [24] employs an iterative process and freezes layers depending on activation instability. Quant-Noise [6], a structured dropout approach, applies quantization to a random subset of weights, allowing any quantization technique to be used. This approach enhances the predictive power of the compressed model. However, uniform quantization incurs a larger error at extremely low bit-width quantization due to its limitation of reconstructing weights in equidistant distributions, and the quantization coefficients cannot be shared across different DNNs.\nVector Quantization. Vector quantization (VQ) in neural networks was first introduced by Gong et al. [8], who explore scalar, vector, and product quantization (PQ) for fully connected (FC) layers. Wu et al. [35] extend PQ to compress both FC and convolutional layers of CNNs by sequentially quantizing layers to reduce error accumulation. DeepCompression [9] introduces k-means clustering for model compression by clustering weights and assigning the same value to all weights within a cluster. Son et al. [31] apply vector quantization to 3\u00d73 convolutions and"}, {"title": "3. Motivation", "content": "We provide a concise overview of uniform quantization and vector quantization.\nUniform Quantization: Let $W \\in \\mathbb{R}^{o \\times i}$ represent the weight matrix, where o denotes the number of output channels and i is the number of input channels. A standard symmetric uniform quantizer approximates the original floating-point weight W as $W \\approx sW_{\\text{int}}$, where $W_{\\text{int}}$ is a b-bit integer representation and s is a high-precision scaling factor shared across all elements of W. However, uniform quantization incurs a larger error at very low bit widths due to its inherent limitation in reconstructing weights with equidistant distributions.\nVector Quantization: A more flexible quantization approach is vector quantization (VQ), which expresses W in terms of assignments A and a codebook C. W is divided"}, {"title": "3.1. Types of quantization", "content": "book can be stored in ROM, which significantly reduces the silicon area and eliminates the need for repeated codebook loading during fast task switching. Experimental results demonstrate that our approach not only achieves accuracy comparable to the original models across various networks but also significantly reduces storage requirements.\nOur main contributions can be summarized as follows:\n\u2022 We propose VQ4ALL, a novel VQ-based approach that enables universal network representation and efficient network compression.\n\u2022 Our method introduces a kernel density estimation method to generate a universal codebook and a differentiable assignment learning strategy to progressively build low-bit neural networks.\n\u2022 Our method achieves high compression rates (exceeding 16x) while maintaining accuracy comparable to original models across multiple architectures, demonstrating its effectiveness and versatility."}, {"title": "3.2. Benefits of a universal codebook", "content": "As illustrated in Table 1, we apply the classic uniform quantization (UQ) and vector quantization (VQ) to the multi-"}, {"title": "4. VQ4ALL", "content": "To address the identified challenges, we introduce VQ4ALL, a VQ-based method that utilizes a universal codebook to enable the construction of various low-bit neural networks and achieve efficient representations. The pipeline of VQ4ALL is visualized in Figure 1. In Section 4.1, we describe the initialization of the universal codebook, which applies to most deep neural networks. In Section 4.2, we introduce the definition of the objective function. In Section 4.3, we explain how the network is progressively constructed through the learning of differentiable candidate assignments."}, {"title": "4.1. Initialization of the Universal Codebook", "content": "To generate a universal shared codebook, the normal approach is to concatenate the weight sub-vectors of multiple models and apply the K-means clustering method to obtain the codebook. However, this approach is time-consuming and the resulting codebook may only be effective for a few models with a large number of parameters. Therefore, we randomly sample an equal number of weight sub-vectors from each network and concatenate them, ensuring that the codebook remains unbiased. We then use the kernel density estimation (KDE) algorithm to analyze the distribution of these sub-vectors:\n$f(w) = \\frac{1}{nh} \\sum_{i=1}^{n} K(\\frac{w - w_{o,i/d}}{h}), K(u) = \\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{u^2}{2}},$ \nwhere $f (w)$ represents the estimated density function, n is the number of sub-vectors, $w_{o,i/d}$ are the sub-vectors being analyzed, h is the bandwidth that controls the smoothing of the estimation, and $K(u)$ is the Gaussian kernel function. The computation process of KDE is highly efficient, and $f(w)$ can be used to randomly sample a frozen universal codebook of size k \u00d7 d:\n$C = {c(1), ..., c(k)}, c(k) \\sim f(w).$"}, {"title": "4.2. Definition of Objective Function", "content": "The next step is to determine which codeword to use for representing each weight sub-vector. The most straightforward approach is to select the codeword with the smallest distance. However, this method can introduce significant quantization errors, which may result in a considerable loss of network accuracy. For each weight sub-vector, we calculate its Euclidean distance to all codewords, obtaining the indices of the top n closest codewords:\n$A_{c}^{n} = {a_{o,i/d}}^{n} = arg min min ||w_{o,i/d} - c(k)||^{2},$ \nwhere $A_c$ is the candidate assignments of the sub-vectors and n is the number of each $A_c$. We need to select the most appropriate assignments from the candidate assignments to effectively represent the information. To achieve this, we assign softmax ratios R to all candidate assignments:\n$R= {r_{o,i/d}}^{n} = {\\frac{e^{\\{z_{o,i/d}\\}^n}}{\\sum_{j=1}^{n} e^{z_{o,i/d}}}} = {\\frac{e^{\\{z_{o,i/d}\\}^n}}{\\{n \\atop j=1\\}e^{z_{o,i/d}}}}^{n} = 1,$ \nwhere ${z_{o,i/d}}^n$ is the actual value of each ratio, which is initialized to be inversely proportional to the Euclidean distance:\n${z_{o,i/d}}^m = ln \\frac{||W_{o,i/d}- c({\\{a_{o,i/d}\\}}^{\\{m-1\\}})||^{2}}{||W_{o,i/d} - c({\\{a_{o,i/d}\\}m})||^{2}}$\nTherefore, $W = RC[A_c]$ can be reconstructed based on the differentiable weighted average function:\n$\\begin{bmatrix} {r_{1,1}}n \\times c(a_{1,1}n) & ... & {r_{1,i/d}}n \\times c(a_{1,i/d}n) \\\\ {r_{2,1}}n \\times c(a_{2,1}n) & ... & {r_{2,i/d}}n \\times c(a_{2,i/d}n) \\\\ : & ... & : \\\\ {r_{o,1}}n \\times c(a_{o,1}n) & ... & {r_{o,i/d}}n \\times C(a_{o,i/d}n) \\end{bmatrix}$ \nOur method aims to construct multiple low-bit deep neural networks that exhibit the same behavior as the floating-point networks. Notably, the universal codebook remains"}, {"title": "4.3. Progressive Network Construction Strategy", "content": "unchanged throughout. We combine three common training strategies to effectively update candidate assignments and other network parameters (e.g., bias and normalization layers). Specifically, given the input x and the target y from the calibration dataset for the quantized network eq, the task objective function is computed as:\n$L_t = E_{x,y,ca} [||y - g(x)||^{2}].$ \nTo ensure that the output of each network block in eq is similar to that of the floating-point network efp, we also apply block-wise knowledge distillation (KD) as a constraint. Given the same input x to both efp and eq, the KD objective function is computed as:\n$L_{kd} = E_{x,bfp,bq} [||b^{fp}_{l}(x) - b^{q}_{l}(x)||^{2}]$ \nwhere $b^{fp}_{l}$ and $b^{q}_{l}$ represent the features of l-th main block from $E_{fp}$ and $E_q$, respectively.\nTo accelerate the update of R towards values close to 0 or 1, rather than getting stuck in local optima, we introduce a regularization function for R:\n$L_r = nx \\sum_{o,i/d,n} ({r_{o,i/d}}^n\\times(1-{r_{o,i/d}}^n)/(x).$ \nThe final objective function L is represented as:\n$L = L_t + L_{kd} + L_r.$ \nDuring the above process, we update the ratios through gradients: $R \\leftarrow R - u \\frac{\\partial L}{\\partial R}$, where u is the optimizer with hyperparameters \u03b8.\nAs shown in Figure 3 (Down), most of the largest ratios oscillate around 1, while a small number of them fluctuate within certain ranges in the later stages of the updating process. Therefore, selecting the optimal assignments to construct low-bit networks becomes challenging. DKM addresses this issue by enforcing a transition from soft assignments to hard assignments. We replicate this approach by selecting the candidate assignments with the highest ratios as the optimal assignments. However, as shown in Figure 3 (Up), this approach significantly reduces the accuracy of the low-bit network due to discrepancies introduced by the reconstructed weights:\n$\\sum ||RC[A_C] - C[A_C[R_{max}]]||^{2} > 0.$ \nSuch discrepancies cause corresponding deviations in the features of each layer of the deep neural network, which eventually accumulate in the output."}, {"title": "5. Experiments", "content": "Therefore, we propose the Progressive Network Construction (PNC) strategy, a gradual approach that prevents network collapse. When updating the parameters, once the ratio of a candidate assignment becomes very close to 1, PNC adopts a frozen one-hot mask to set it as the optimal assignment with its ratio fixed at 1 and other ratios fixed at 0. The formula can be expressed as:\n${r_{o,i/d}}^n = one-hot(m), {r_{o,i/d}}^m > \u03b1,$ \nwhere \u03b1 is a specific value (e.g. 0.9999). Additionally, $L_r$ is computed only for the unset ratios. Compared to selecting all optimal assignments at once, the accuracy loss from the PNC is much smaller, allowing the remaining candidate assignments larger space to restore the model's accuracy. In this way, the network is gradually constructed.\nWe evaluate our method on ResNet-18/50 [10] and MobileNet-V2 [29] for image classification, Mask R-CNN R-50 FPN [11] for object detection, and Stable Diffusion v1-4 [27] for image generation. The network weights and datasets are obtained through official sources. Our primary objective is to explore the trade-off between model compression and precision; therefore, activation quantization is disabled in all experiments.\nHyperparameters: All experiments are conducted on a single NVIDIA A6000 GPU with 48GB memory. The ratios of candidate assignments are fine-tuned by the Adamax optimizer with a learning rate of 3 \u00d7 10-1 (without individual hyper-parameter tuning). The universal codebook sizes k \u00d7 d used for 3/2/1/0.5 bit quantization are 212 \u00d7 4, 216 \u00d7 8, 216 \u00d7 16, and 216 \u00d7 32, respectively. To align with the configurations of other compression methods, the per-layer codebook sizes used for 2/1 bit quantization of special layer (e.g., the last output layer of image classification networks) are 28 \u00d7 4, 28 \u00d7 8, respectively. The number of candidate assignments in all cases of our method is set to 64.\nUniversal Codebook: We randomly extract 10 \u00d7 k \u00d7 d weight sub-vectors from each of the networks mentioned above and concatenate them. A kernel density estimation with a bandwidth of 0.01 is then applied efficiently to these concatenated sub-vectors. The resulting estimation is used to sample a single frozen universal codebook, which is used to construct most layers across different networks in our method."}, {"title": "5.1. Image Classification", "content": "We benchmark our method on the task of image classification by compressing the popular ResNet-18/50 and MobileNet-V2 networks using the ImageNet dataset [3]. We compare our VQ4ALL method with previous works:\nTrained Ternary Quantization (TTQ) [37], Binary Weight"}, {"title": "5.2. Object Detection and Segmentation", "content": "We benchmark our method on the task of object detection and segmentation by compressing the popular Mask-RCNN R-50 FPN architecture using the MS COCO 2017 dataset [20]. We also compare with other object de-"}, {"title": "5.3. Image Generation", "content": "We benchmark our method on the task of image generation by compressing the Stable Diffusion v1-4 using the MS COCO 2017 dataset [20]. We compare our VQ4ALL with state-of-the-art quantization methods for diffusion models,"}, {"title": "5.4. Ablation Study", "content": "In Table 5, we show an ablation study on image classification results of 2-bit ResNet-18. We first perform an ablation on the number of candidate assignments (n) for constructing optimal low-bit networks with minimal training time and memory usage. When n = 1, VQ4ALL degenerates to a standard VQ-based method without codebook updates, achieving only 65.9% accuracy. When n = 8, accuracy sharply increases by 3.3%, indicating that VQ4ALL effectively identifies optimal assignments and constructs optimal low-bit networks through different combinations of codewords within the same universal codebook. As n continues to increase, the improvements in accuracy diminish, with n = 64 nearly reaching the optimal performance of our VQ4ALL.\nWe also evaluate the importance of each component in the VQ4ALL pipeline by measuring the accuracy after disabling each component. When block-wise distillation between the floating-point and low-bit networks is disabled,"}, {"title": "6. Conclusion", "content": "Therefore, we propose the Progressive Network Construction (PNC) strategy, a gradual approach that prevents network collapse. When updating the parameters, once the ratio of a candidate assignment becomes very close to 1, PNC adopts a frozen one-hot mask to set it as the optimal assignment with its ratio fixed at 1 and other ratios fixed at 0. The formula can be expressed as:\nIn this work, we propose a novel bottom-up approach for sharing a universal codebook across multiple neural networks. This method not only effectively reduces the number of required codebooks, but also minimizes memory access and chip area by storing static code tables in the built-in ROM. Specifically, we introduce VQ4ALL, a VQ-based technique that leverages codewords to construct various neural networks and facilitate efficient representations. VQ4ALL-constructed low-bit networks achieve state-of-the-art compression quality on popular tasks such as image classification, object detection, and image generation, particularly demonstrating robustness even at extremely high compression ratios. Experimental results show that VQ4ALL achieves compression rates greater than 16x, while maintaining high accuracy across various network architectures, showcasing its advantages in efficient network compression and universal network representation."}]}