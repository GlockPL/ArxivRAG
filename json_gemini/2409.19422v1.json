{"title": "Identifiable Shared Component Analysis of Unpaired Multimodal Mixtures", "authors": ["Subash Timilsina", "Sagar Shrestha", "Xiao Fu"], "abstract": "A core task in multi-modal learning is to integrate information from multiple feature spaces (e.g., text and audio), offering modality-invariant essential representations of data. Recent research showed that, classical tools such as canonical correlation analysis (CCA) provably identify the shared components up to minor ambiguities, when samples in each modality are generated from a linear mixture of shared and private components. Such identifiability results were obtained under the condition that the cross-modality samples are aligned/paired according to their shared information. This work takes a step further, investigating shared component identifiability from multi-modal linear mixtures where cross-modality samples are unaligned. A distribution divergence minimization-based loss is proposed, under which a suite of sufficient conditions ensuring identifiability of the shared components are derived. Our conditions are based on cross-modality distribution discrepancy characterization and density-preserving transform removal, which are much milder than existing studies relying on independent component analysis. More relaxed conditions are also provided via adding reasonable structural constraints, motivated by available side information in various applications. The identifiability claims are thoroughly validated using synthetic and real-world data.", "sections": [{"title": "Introduction", "content": "The same data entities can often be represented in different feature spaces (e.g., audio, text and image), due to the variety of sensing modalities. Learning common latent components of data from multiple modalities is well-motivated in representation learning. The shared components are considered modality-invariant essential representations of data, which can often enhance performance of downstream tasks by shedding modality-specific noise [1-4] and avoiding over-fitting [5\u20137].\nA prominent theoretical aspect of shared component learning lies in identifiability of the components of interest. The literature posed an intriguing theoretical question [1, 2, 8]: If every modality of data is represented by a linear mixture of shared and private components with an unknown mixing system, are the shared components identifiable (up to acceptable ambiguities)? Such component identification problems are often nontrivial due to the ill-posed nature of any linear mixture model (see, e.g., [9-14]). Interestingly, the work [1] showed that using the classical canonical correlation"}, {"title": "Background", "content": "Generative Model of Interest. Following the classical settings in [1, 2, 15, 16, 18], we consider modeling the multi-modal data as linear mixtures. More specifically, we adopt the model in [1, 2]that splits the latent representation of data into shared components and private components:\n$x^{(q)} = A^{(q)} z^{(q)}, \\quad z^{(q)} = [c^T, (p^{(q)})^T]^T, \\quad q = 1,2,$\nwhere $x^{(q)} \\in \\mathbb{R}^{d^{(q)}}$ represents the data from the $q$th modality, $z^{(q)} \\in \\mathbb{R}^{d_c+d_f^{(q)}}$ represents the corresponding latent code, $c \\in \\mathbb{R}^{d_c}$ and $p^{(q)} \\in \\mathbb{R}^{d_f^{(q)}}$ stand for the shared components and the private components, respectively. The data \u00e6(q)'s are assumed to be zero-mean, which can be enforced by"}, {"title": "Proposed Approach", "content": "Unaligned SCA: Problem Formulation We assume that \u00e6(q)'s are zero-mean. We use the notation from CCA in (3a). However, since no aligned samples are available, we replace the sample-level matching objective with a distribution matching (DM) module, as DM can be carried out without sample level alignment:\n$\\begin{aligned}\n&\\text { find }\\quad Q^{(q)} \\in \\mathbb{R}^{d_c \\times d^{(q)}}, q = 1,2,\n\\\\\n&\\text { subject to }\\quad Q^{(1)} x^{(1)} \\sim Q^{(2)} x^{(2)},\n\\\\\n&\\qquad Q^{(q)}E [x^{(q)} (x^{(q)})^T] (Q^{(q)}) ^T = I \\quad q = 1, 2.\n\\end{aligned}$\nwhere \u201c$u \\sim v$\u201d means the distributions of u and v are the same.\nThe formulation in (6) can be realized using various distribution matching tools, e.g., maximum mean discrepancy (MMD) [36] and Wasserstein distance [37]. We use the adversarial loss:\n$\\underset{Q^{(1)},Q^{(2)}, f}{\\min } \\underset{f}{\\max }\\quad E_{x^{(1)}} \\log (f(Q^{(1)}x^{(1)})) + E_{x^{(2)}} \\log (1-f(Q^{(2)}x^{(2)})) + \\sum_{q=1}^{2} \\lambda R \\left(Q^{(q)}\\right).$\nThe first and second terms comprise the adversarial loss from GAN [38]. It finds $Q^{(q)}$ to confuse the best-possible discriminator $f : \\mathbb{R}^{d_c} \\rightarrow \\mathbb{R}$, where f is represented by a neural network in practice. It is well known that the minimax optimal point of the first two terms is attained when (6b) is met [38]. We use $R(Q^{(q)}) = ||Q^{(q)} [E[x^{(q)} (x^{(q)})^T] (Q^{(q)})^T - I||$ to \u201clift\u201d the constraints. This way, the learning criterion in (7) can be readily handled by any off-the-shelf adverserial learning tools.\nIdentifiability of Unaligned SCA As we saw in Theorem 4, CCA identifies $Q^{(q)}x^{(q)} = \\Theta c$ where $\\Theta \\in \\mathbb{R}^{d_c \\times d_c}$ under the settings of aligned SCA. Establishing a similar result for unaligned SCA is much more challenging. First, it is unclear if (6b) could disentangle c from p(q). In general, $Q^{(q)}x^{(q)}$ could still be a mixture of c and p(q) yet (6b) still holds (e.g., when both c and p(q) are Gaussian.)\nSecond, even when the disentanglement is attained via enforcing (6b) and we have $Q^{(q)}x^{(q)} = \\Theta^{(q)}c$, in general it does not hold that $\\Theta^{(1)} = \\Theta^{(2)}$. This is because $\\Theta^{(1)} c \\sim \\Theta^{(2)} c$ where $\\Theta^{(1)} \\neq \\Theta^{(2)}$ can still be perfectly met (e.g., when $P_{\\Theta^{(q)}c}$is symmetric Gaussian in Fig. 1 ). However, $\\Theta^{(1)} \\neq \\Theta^{(2)}$ means that the extracted representations from the two modalities are not matched. This creates challenges for applications like cross-domain information retrieval, language translation, or domain adaptation.\nOur intuition is as follows: If the two distributions $P_{c,p^{(1)}}$ and $P_{c,p^{(2)}}$ are very different, then $Q^{(1)} A^{(1)} x^{(1)} \\sim Q^{(2)} A^{(2)} x^{(2)}$ cannot hold unless $Q^{(q)} A^{(q)} = [\\Theta^{(q)}, 0]$. We use the following to charac-terize such difference between the joint distributions:\nAssumption 1 (Modality Variability). For any two linear subspaces $P^{(q)} \\subset \\mathbb{R}^{d_c+d_f^{(q)}}, q = 1, 2$, with dim$(P^{(q)}) = d_f^{(q)}, P^{(q)} \\neq \\{0\\} \\times \\mathbb{R}^{d_f^{(q)}}$ and linearly independent vectors {$y_i^{(q)} \\in \\mathbb{R}^{d_c+d_f^{(q)}}$}$_{i=1}^{d_c}$, $q = 1,2$, the sets $A^{(q)} = conv\\{0, y_1^{(q)},...,y_{d_c}^{(q)}\\} + P^{(q)}$, $q = 1,2$, are such that if $P_{c,p^{(q)}} [A^{(q)}] > 0$ for $q = 1$ or $q = 2$, then there exists a $k \\in \\mathbb{R}$ such that the joint distributions $[P_{c,p^{(1)}} [kA^{(1)}] \\neq P_{c,p^{(2)}} [kA^{(2)}]]$, where $kA^{(q)} = \\{ka | a \\in A^{(q)}\\}$.\nThe condition in Assumption 1 is a geometric way to characterize the difference between $P_{c,p^{(1)}}$ and $P_{c,p^{(2)}}$ -if the joint distributions have different measures for all possible \u201cstripes\u201d, each being a direct sum of a subspace and a convex hull (see Fig. 2), then $P_{c,p^{(1)}}$ and $P_{c,p^{(2)}}$ must be very different. Note that the difference is contributed by the modality-specific term p(q), and thus we call this condition \u201cmodality variability\u201d. Modality variability is similar to the \u201cdomain variablity\u201d used in [32, 39]\u2014both characterize the discrepancy of the joint probabilities $P_{c,p^{(1)}}$ and $P_{c,p^{(2)}}$. However, there are key differences: The domain variability was defined in a unified latent domain over arbitrary sets A, which could be stringent. Instead, we use the fact that (6) relies on linear operations to construct $A^{(q)}$, which makes the condition defined over a much smaller class of sets\u2014thereby largely"}, {"title": "Enhanced Identifiability via Structural Constraints", "content": "Theorem 1 was well-supported by the synthetic data experiments. However, our experiments found that the learning criterion (6) often struggles to produce sensible results in some applications. Our conjecture is that the Assumptions in Theorem 1 (a) and (b) might not have been satisfied by the real data under our tests. Although they are not necessary conditions for identifiability, these conditions do indicate that the requirements to guarantee identifiability of unaligned SCA using (6) are nontrivial to meet. In this section, we explore a couple of structural constraints arising from side information in applications to remove the need for the relatively stringent assumptions on c.\nHomogeneous Domains. The first structural constraint that we consider is $A^{(q)} = A$ for $q = 1,2$. This model is motivated by the fact that advanced representation learning tools, e.g., self-supervised learning tools (e.g., SimCLR [40]) and foundation models (e.g., CLIP [34]), are already capable of mapping the data clusters to a shared linearly separable space-which indicates that the representations share a subspace, i.e., $x^{(q)} \\approx Az^{(q)}$. Under such circumstances, the proposed model and method can be used to further process the data by discarding the private components in the latent representation.\nHere, we consider the special case of generative process in (1) where,\n$x^{(q)} = A[c^T, (p^{(q)})^T]^T.$\nUnder this model, we look for the shared components by solving (6) with a single $Q = Q^{(1)} = Q^{(2)}$. We use the following version of the modality variability condition:\nAssumption 2. For any linear subspace $P \\subset \\mathbb{R}^{d_c+d_p}, d_p = d_f^{(q)} = d_f^{(q)}$, with dim$(P) = d_p, P \\neq \\{0\\} \\times \\mathbb{R}^{d_p}$ and linearly independent vectors {$Y_i \\in \\mathbb{R}^{d_c+d_p}$}$_{i=1}^{d_c}$, $q = 1,2$, the sets $A = conv\\{0, Y_1,\\cdots, Y_{d_c}\\} + P, q = 1, 2$. are such that if $P_{c,p^{(q)}} [A] > 0$ for $q = 1$ or $q = 2$, then the joint distributions $[P_{c,p^{(1)}} [kA] \\neq P_{c,p^{(2)}} [kA]]$ for some $k \\in \\mathbb{R}$.\nTheorem 2. Consider the mixture model in (8). Assume that rank(A) = dc + dp and rank(E[cc]) = dc, and that Assumption 2 holds. Denote Q as any solution of (6) by constraining Q = Q(1) = Q(2). Then, we have $Qx^{(q)} = \\Theta c$.\nOne can see that the conditions (a) and (b) in Theorem 1 are completely removed, if the structure $A^{(1)} = A^{(2)}$ is imposed. In fact, the result in Theorem 2 is expected and readily seen from the proof of Theorem 1, as the cause for $\\Theta^{(1)} \\neq \\Theta^{(2)}$ is the use of two different $Q^{(q)}$'s. Nonetheless, this simple variation will prove useful in a series of real-data experiments.\nThe Weakly Supervised Case. Another way to add structural constraints is to use available auxiliary information. For example, some datasets have weak annotations and selected pairs; see, e.g., [41, 42].\nAssumption 3 (Weak Supervision). There exist a set set of available aligned samples $(x_l^{(1)}, x_l^{(2)})$ for l \u2208 L such that $x_l^{(q)} = A^{(q)} z_l^{(q)}, z_l^{(q)} = [c_l^T, (p_l^{(q)})^T]^T; i.e., (x_l^{(1)}, x_l^{(2)})$ share the same $c_l$."}, {"title": "Conclusion", "content": "In this work, we considered the problem of identifying shared components from unaligned multi-domain mixtures. We proposed a learning loss that matches the distributions of linearly transformed data. Based on this loss, we came up with a suite of sufficient conditions to ensure the identifiability of shared components. Furthermore, we proposed modified models and losses that enjoy more relaxed conditions for shared component identifiability. This was achieved via introducing structural con-straints, namely, the homogeneity of the mixing systems and the existence of weak supervision. Our"}, {"title": "Limitations", "content": "First, our conditions for shared component identification are sufficient. The necessary conditions are not underpinned, but necessary conditions assist understanding the limitations of the models and algorithms. Second, our methods were developed under the linear mixture model, which has limited expressiveness, and thus often requires pre-processing to approximately meet the model specification. We expect that results with similar flavors to be derived for nonlinear models in the future. Third, the results were derived under an unlimited data assumption. It would be interesting have a finite sample analysis."}, {"title": "Notation", "content": "The notations used throughout the paper are summarized in the Table 3.:"}]}