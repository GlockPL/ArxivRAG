{"title": "MITracker: Multi-View Integration for Visual Object Tracking", "authors": ["Mengjie Xu", "Yitao Zhu", "Haotian Jiang", "Jiaming Li", "Zhenrong Shen", "Sheng Wang", "Haolin Huang", "Xinyu Wang", "Qing Yang", "Han Zhang", "Qian Wang"], "abstract": "Multi-view object tracking (MVOT) offers promising solutions to challenges such as occlusion and target loss, which are common in traditional single-view tracking. However, progress has been limited by the lack of comprehensive multi-view datasets and effective cross-view integration methods. To overcome these limitations, we compiled a Multi-View object Tracking (MVTrack) dataset of 234K high-quality annotated frames featuring 27 distinct objects across various scenes. In conjunction with this dataset, we introduce a novel MVOT method, Multi-View Integration Tracker (MITracker), to efficiently integrate multi-view object features and provide stable tracking outcomes. MITracker can track any object in video frames of arbitrary length from arbitrary viewpoints. The key advancements of our method over traditional single-view approaches come from two aspects: (1) MITracker transforms 2D image features into a 3D feature volume and compresses it into a bird's eye view (BEV) plane, facilitating inter-view information fusion; (2) we propose an attention mechanism that leverages geometric information from fused 3D feature volume to refine the tracking results at each view. MITracker outperforms existing methods on the MVTrack and GMTD datasets, achieving state-of-the-art performance. The code and the new dataset will be available at mii-laboratory.github.io/MITracker.", "sections": [{"title": "1. Introduction", "content": "Visual object tracking, a core computer vision task, involves estimating class-agnostic target positions across video se- quences. This technique is crucial for applications such as augmented reality and autonomous driving, where it is essential to continuously monitor and predict the trajectories of various objects within dynamic environments. Despite notable advances in single-view tracking through Siamese networks [10, 25] and transformers [3, 9, 46], significant challenges persist \u2013 particularly occlusions, appearance changes, and target loss. While approaches like RTracker [22] attempt to address these challenges by determining target loss and detection mechanisms, the inherent limitations of single viewpoint information remain a fundamental constraint.\nMulti-camera systems offer a promising solution by leveraging complementary viewpoints to maintain continuous tracking, particularly for handling occlusions through camera overlap [48]. However, the development of effective"}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Visual Object Tracking", "content": "Visual object tracking has garnered significant research interest, leading to many breakthroughs. Numerous single-view datasets [13, 20, 21, 23, 24, 27, 29, 36, 39] span a wide range of categories, aimed at enhancing models' ability to track arbitrary objects. With the expansion of these datasets, single-view tracking methods have also advanced rapidly. Early approaches based on Siamese networks [10, 25] use CNNs to extract features from reference and search regions, establishing a linear relationship between them. More recent works have incorporated transformers for enhanced feature extraction [9, 43], while others introduce attention modules to enable nonlinear relationships [5]. However, these methods lack temporal continuity as they process each frame independently. Algorithms like dynamic template updating [6] and spatio-temporal trajectory tracking [37, 46] have shown promising results in addressing this issue. Despite these advancements, recovering from target loss remains a significant challenge.\nTo re-track the target after a tracking failure, RTracker [22] leverages a tree-structured memory system to detect target loss and a dedicated detector for self-recovery. However, this approach is constrained by its complex design and the detector's reliance on specific categories. Single-view tracking suffers from inherent limitations due to its restricted field of view, which is an inevitable challenge. In contrast, GMT [38] incorporates multi-view tracking within a single-view training framework. This limits its capacity to effectively model the intricate relationships between multi-view appearances and background contexts in the real world."}, {"title": "2.2. Multi-View Object Tracking", "content": "MVOT provides more comprehensive information about the target, effectively addressing issues such as occlusion. To leverage multi-view information, various fusion strategies have been developed for target association across viewpoints. Some approaches establish multi-view relationships by projecting detection results onto a BEV plane [42]. However, this method is prone to detection errors, especially"}, {"title": "3. MVTrack Dataset", "content": "MVTrack dataset is designed to fill the gaps in the field of MVOT and has received approval for data collection from an Institutional Review Board. As shown in Table 1, compared to single-view datasets, we maintain competitive class diversity while adding multi-view capabilities. Compared to MVOT datasets, we provide significantly richer object categories (27 vs 1-8 classes) and more videos (260) with practical camera setups (3-4 views). MVTrack dataset is the only dataset that combines multi-view tracking, rich object categories, absent label annotations, and calibration information.\nData Collection. We employ a multi-camera system"}, {"title": "4. MITracker", "content": "We propose MITracker, a novel multi-view tracking framework that robustly tracks class-agnostic objects across multiple camera views. As illustrated in Figure 3, MITracker consists of two main components: (1) a view-specific feature extraction module (Sec. 4.1) that encodes frame features and generates single-view tracking results in a stream-"}, {"title": "4.1. View-Specific Feature Extraction", "content": "As shown in Figure 3a, this module processes the video stream from a specific viewpoint k, and extracts target-aware features in the search frame at a timepoint t based on the reference frame that indicates the target object.\nView-Specific Encoder. We employ ViT as the backbone of our view-specific encoder. The visual inputs of the view-specific encoder consist of a search frame $S \\in \\mathbb{R}^{3 \\times H \\times W_s}$ and a reference frame $R \\in \\mathbb{R}^{3 \\times H \\times W_r}$. As the transformer block processes a series of tokens, we segment the frames into non-overlapping patches with $p \\times p$ resolution. The search and reference frames are individually embedded into a token sequence, represented by $I_S \\in \\mathbb{R}^{N_s \\times D}$ and $I_R \\in \\mathbb{R}^{N_r \\times D}$, where D is the hidden dimension, $N_s = \\frac{H_sW_s}{p^2}$ is the number of search tokens, and $N_r = \\frac{H_rW_r}{p^2}$ is the number of reference tokens.\nTo ensure temporal continuity between frames, akin to the method utilized in ODTrack [46], two specialized temporal tokens are also included in the inputs of the view-specific encoder to facilitate the propagation of temporal information. Specifically, at any given time t, a learnable token $T_t$ is randomly initialized, which is designed to capture temporal information of the current frame. Concurrently, we incorporate a token $T_{t-1}$ that carries temporal information from the preceding frame, which leverages historical features to enhance tracking accuracy and continuity. The input token sequence of our view-specific encoder can be formulated as the composition of the visual and temporal tokens $f = [T_t, T_{t-1}, I_R, I_S]$, while the output token sequence is denoted as $f' = [T'_t, T'_{t-1}, I'_R, I'_S]$.\nAfter obtaining $f'$, $T'_t$ is used to compute attention weights in conjunction with $I'_S$ to utilize temporal information for adjustments, which can be described as follows:\n$I_V = I'_S \\cdot (I'_S \\times (T'_t)^T)$,\nwhere $I_V$ represents the extracted feature that encapsulates attention focused on the target object in the search frame.\nSingle-View Tracking Result. We employ a BBox head based on the CenterNet architecture [47] to output tracking results from the extracted feature $I_V$. This head comprises three distinct sub-networks, each designed to compute the classification score map, BBox dimensions, and offset sizes, respectively. The highest-scoring position on the classification score map is identified as the target location. This configuration establishes a robust framework capable of effectively handling single-view visual object tracking tasks.\nTo facilitate further multi-view integration, we also apply convolutional layers to map $I_V$ to a 2D feature map"}, {"title": "4.2. Multi-View Integration", "content": "To effectively integrate 2D feature maps $F_{2D}^1, F_{2D}^2, ..., F_{2D}^K$ from K viewpoints, we project them into a 3D feature space and then aggregate them under the supervision of BEV guidance. Finally, we embed the aggregated feature to a 3D-aware token to refine all view-specific features $I_V^1, I_V^2, ..., I_V^K$ via spatial-enhanced attention, thus producing stable tracking results across different viewpoints.\n3D Feature Projection. As illustrated in Figure 3b, we construct a 3D feature volume of size $X \\times Y \\times Z$, where (X, Y) represents the horizontal plane and Z axis denotes the vertical direction following [17, 44]. For a viewpoint k, we project the (u, v) coordinates in $F_{2D}^k$ to (x, y, z) coordinates in the 3D feature volume by the formula below:\n$\\begin{pmatrix}\nu \\ 1\\x \\ y \\ z \\ 1\\end{pmatrix} = C_K[C_RC_t] \\begin{pmatrix}\nu \\ 1\\end{pmatrix}$,\nwhere $C_K$ represents the camera's intrinsic matrix, $C_R$ denotes the rotation matrix describing the camera's orientation, and $C_t$ is the translation vector specifying the camera's position in space. Upon establishing the mapping matrix, we implement bilinear sampling to populate the 3D feature volume. In scenarios that involve multiple viewpoints, we compute the average of the mapped values from each view to ensure consistency. Consequently, we derive a 3D feature volume represented as $F_{3D} \\in \\mathbb{R}^{32 \\times X \\times Y \\times Z}$.\n3D Feature Aggregation. To better integrate multi-view spatial information, we apply 1D convolutional layers to aggregate features along the Z-axis of $F_{3D}$, resulting in $F'_{3D} \\in \\mathbb{R}^{32 \\times X \\times Y}$, thereby consolidating spatial information within the (X, Y) plane. Subsequently, a classification head (i.e., BEV head) is employed to generate a BEV score map from $F'_{3D}$. This BEV map delineates the object positions on the horizontal plane, thereby imposing supervision constraints on information fusion across multiple viewpoints. This integrative approach allows for precise localization and mapping within multi-view scenarios.\nSpatial-Enhanced Attention. BEV guidance for the aggregated 3D feature $F'_{3D}$ only implicitly constrains the original single-view output, but it is insufficient to address the potential target loss issue due to the lack of direct supervision on tracking results. To remedy this, we introduce spatial-enhanced attention to explicitly incorporate $F'_{3D}$ into the tracking process as shown in Figure 3c.\nWe first use convolutional layers to embed $F'_{3D}$ into a 3D-aware token $T_{3D} \\in \\mathbb{R}^{1 \\times D}$, which inherits multi-view spatial information. For all the K viewpoints, we then individually concatenate $T_{3D}$ with their unrefined features $I_V^1, I_V^2, ..., I_V^K$ produced by the view-specific encoder. For a viewpoint k, a series of transformer blocks take in its composite token sequence ($T_{3D}, I_V^k$) and refine them using attention mechanisms that leverage fused 3D spatial information. A final BBox head outputs the refined tracking results, where potential errors such as target loss are corrected."}, {"title": "5. Experiments", "content": ""}, {"title": "5.1. Dataset", "content": "In addition to MVTrack dataset, we use two external datasets for training and evaluation, which are detailed as follows.\n\u2022 GOT10K. GOT-10K [21] is a large and diverse dataset with a wide range of object categories. Its training set contains 9,335 videos across 480 moving object categories.\n\u2022 GMTD. GMTD [38] is a multi-view tracking test set with 10 scenes, captured by 2-3 uncalibrated cameras in indoor and outdoor settings. It includes 6 target types and various tracking challenges."}, {"title": "5.2. Implementation Details", "content": "Loss Function. For the BBox head, we employ a weighted focal loss [26] $L_{cls}$ for classification, along with the generalized intersection over union loss [31] $L_{giou}$ and $L_1$ loss for BBox regression. Additionally, a focal loss $L_{bev}$ is utilized for BEV map supervision. The overall loss function of the model is formulated as follows:\n$L_{track} = L_{cls} + \\lambda_{giou} L_{giou} + \\lambda_{L_1} L_1 + \\lambda_{bev} L_{bev}$,\nwhere $\\lambda_{giou} = 5$, $\\lambda_{L_1} = 2$, and $\\lambda_{bev} = 0.1$ are the coefficients that balance the contributions from each loss.\nTraining Setup. We initialize our view-specific encoder with pre-trained DINOv2 [28] parameters using the ViT-base model [12]. For the visual inputs, we set the reference frame with 182 \u00d7 182 pixels, and the search frame"}, {"title": "5.3. Evaluation Metrics", "content": "We evaluate our method using three standard performance measures from the single-view tracking benchmark [13, 27, 39]: Area Under Curve (AUC), Precision (P), and Normalized Precision (PNorm):\n\u2022 AUC: The Intersection over Union (IoU) measures the overlap between predicted and ground truth BBoxes in each frame. The AUC metric is calculated by varying the IoU threshold to evaluate the area error in the tracking region."}, {"title": "5.4. Comparison with Existing Methods", "content": "SOTA Performance on Benchmark. We evaluate tracking performance with single-view visual object tracking methods, training all models (except SAM2 and SAM2Long) on the GOT10K and MVTrack datasets. The models are tested on both the MVTrack and GMTD datasets under single-view and multi-view settings.\nHowever, single-view methods cannot handle multi-view inputs or generate multi-view predictions. To address this, we employ a post-fusion strategy to obtain multi-view results. Specifically, single-view predictions are first projected into the 3D world coordinate system. The region with maximum overlap is identified as the target position, which is then reprojected onto the 2D image plane of each viewpoint to generate the fused multi-view tracking results.\nAs shown in Table 2, MITracker achieves superior performance in both multi- and single-view tracking across different datasets. In multi-view scenarios with 3-4 cameras, MITracker outperforms other methods that rely on post-processing for multi-view fusion, surpassing the second-best method OSTrack by approximately 26% in PNorm. In single-view settings, MITracker surpasses SOTA methods on the MVTrack dataset, achieving an AUC of 68.57%, which outperforms ODTrack by approximately 5%.\nNotably, MITracker exhibits strong generalization capabilities by achieving exceptional performance on the GMTD, despite it not being included in the training data. This demonstrates the robustness of our multi-view approach even in single-view scenarios. We attribute these improvements to our multi-view training strategy, which enables the model to better understand spatial relationships crucial for precise tracking. It is also noteworthy that post-processing degrades the performance of all single-view methods. This indicates a substantial distribution gap in view-independent feature detection across models, making effective fusion through geometric projections challenging.\nStable Continuous Tracking Capability. To further evaluate tracking robustness, we conducted three comparative experiments on the MVTrack dataset. Only MITracker utilizes multi-view inputs, other methods use single-view inputs and generate BBoxes independently.\nFirst, we analyzed tracking success rates across various IoU thresholds, as shown in Figure 4a. MITracker consistently outperforms competing methods regardless of the threshold value.\nSecond, we evaluated the recovery capability after the target was invisible by measuring the proportion of successful tracking resumption within given frame intervals [22]. As illustrated in Figure 4b, with a 10-frame interval, MITracker achieves a high success rate of 79.2% in these recovery tests. In comparison, SAM2Long only achieves a 56.7% recovery rate under the same setting, highlighting our method's exceptional ability to quickly reestablish tracking after the target dissapears.\nIn practical applications, users can manually intervene to restart the model's tracking by providing an accurate initial position. In this experimental setup, we measured the maximum continuous tracking length of video frames and the average number of restarts (triggered when target loss exceeds 10 frames, using ground truth for repositioning) [45]. As shown in Figure 4c, MITracker achieves nearly 100 frames longer tracking duration than ODTrack while"}, {"title": "5.5. Ablation Study", "content": "Results in Table 3 demonstrate that BEV Loss, which provides implicit multi-view information feedback, significantly enhances model performance. This improvement is attributed to its ability to augment spatial awareness during single-view feature extraction. The Spatial attention, which utilizes fused information to adjust outputs from single-view perspectives, also contributes to notable performance improvements in the model."}, {"title": "5.6. Visualization Comparison", "content": "Our qualitative evaluation focuses on the influence of occlusion and fast motion. In the upper part of Figure 5, we select two viewpoints from the MVTrack dataset and evaluate them on MITracker and ODTrack, which has the second-best performance on this dataset. The gray areas in the graph represent periods when the object is out of view or fully occluded by other objects. We can easily observe that MITracker is able to re-track the object shortly after it reappears, whereas ODTrack tends to continue in a lost state. For instances #405 and #515 in V2, even when the object reappears in the frame, ODTrack still mistakenly locks onto the wrong object. The bottom of Figure 5 presents tests conducted on the GMTD, where we also selected the second-best method, EVPTrack, for comparison with MITracker. When a pedestrian reappears after being obscured by a pillar, EVPTrack mistakenly locks onto the wrong target, whereas MITracker is able to maintain stable and continuous tracking.\nWe also visualize the predicted BEV trajectories from MITracker in Figure 6. Referencing the ground-truth trajectories, MITracker effectively integrates multi-view features and provides accurate 3D spatial information."}, {"title": "6. Discussion", "content": "In previous sections, we provide a detailed introduction to the MVTrack dataset and demonstrate the outstanding performance of MITracker. However, there are some areas that could be improved in future work.\nLimitations. Although MVTrack dataset includes a diverse set of scenes, it currently consists of indoor environments only, potentially limiting the generalization of methods trained on it to outdoor settings. Additionally, MITracker relies on camera calibration for multi-view fusion, which may restrict its applicability in scenarios where calibration is challenging or infeasible.\nFuture work. We plan to extend MVTrack dataset by including outdoor scenes and a wider range of tracking objects to enable the development of more generalizable"}, {"title": "7. Conclusion", "content": "In this study, we address key challenges such as occlusion and target loss in MVOT by making two significant contributions: (1) MVTrack, a comprehensive dataset with 234K high-quality annotations across diverse scenes and object categories, and (2) MITracker, a novel visual tracking method that effectively integrates multi-view object features. MITracker achieves SOTA results on MVTrack and GMTD datasets, demonstrating its ability to provide stable and reliable tracking across different viewpoints and video durations. Our contributions lay the foundation for future advancements in MVOT, enabling the development of more robust and accurate tracking systems for real-world scenarios."}]}