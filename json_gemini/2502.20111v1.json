{"title": "MITracker: Multi-View Integration for Visual Object Tracking", "authors": ["Mengjie Xu", "Yitao Zhu", "Haotian Jiang", "Jiaming Li", "Zhenrong Shen", "Sheng Wang", "Haolin Huang", "Xinyu Wang", "Qing Yang", "Han Zhang", "Qian Wang"], "abstract": "Multi-view object tracking (MVOT) offers promising solutions to challenges such as occlusion and target loss, which are common in traditional single-view tracking. However, progress has been limited by the lack of comprehensive multi-view datasets and effective cross-view integration methods. To overcome these limitations, we compiled a Multi-View object Tracking (MVTrack) dataset of 234K high-quality annotated frames featuring 27 distinct objects across various scenes. In conjunction with this dataset, we introduce a novel MVOT method, Multi-View Integration Tracker (MITracker), to efficiently integrate multi-view object features and provide stable tracking outcomes. MITracker can track any object in video frames of arbitrary length from arbitrary viewpoints. The key advancements of our method over traditional single-view approaches come from two aspects: (1) MITracker transforms 2D image features into a 3D feature volume and compresses it into a bird's eye view (BEV) plane, facilitating inter-view information fusion; (2) we propose an attention mechanism that leverages geometric information from fused 3D feature volume to refine the tracking results at each view. MITracker outperforms existing methods on the MVTrack and GMTD datasets, achieving state-of-the-art performance. The code and the new dataset will be available at mii-laboratory.github.io/MITracker.", "sections": [{"title": "1. Introduction", "content": "Visual object tracking, a core computer vision task, involves estimating class-agnostic target positions across video se- quences. This technique is crucial for applications such as augmented reality and autonomous driving, where it is essential to continuously monitor and predict the trajectories of various objects within dynamic environments. Despite notable advances in single-view tracking through Siamese networks [10, 25] and transformers [3, 9, 46], significant challenges persist \u2013 particularly occlusions, appearance changes, and target loss. While approaches like RTracker [22] attempt to address these challenges by determining target loss and detection mechanisms, the inherent limitations of single viewpoint information remain a fundamental constraint.\nMulti-camera systems offer a promising solution by leveraging complementary viewpoints to maintain continuous tracking, particularly for handling occlusions through camera overlap [48]. However, the development of effective"}, {"title": "2. Related Work", "content": "Visual object tracking has garnered significant research interest, leading to many breakthroughs. Numerous single-view datasets [13, 20, 21, 23, 24, 27, 29, 36, 39] span a wide range of categories, aimed at enhancing models' ability to track arbitrary objects. With the expansion of these datasets, single-view tracking methods have also advanced rapidly. Early approaches based on Siamese networks [10, 25] use CNNs to extract features from reference and search regions, establishing a linear relationship between them. More recent works have incorporated transformers for enhanced feature extraction [9, 43], while others introduce attention modules to enable nonlinear relationships [5]. However, these methods lack temporal continuity as they process each frame independently. Algorithms like dynamic template updating [6] and spatio-temporal trajectory tracking [37, 46] have shown promising results in addressing this issue. Despite these advancements, recovering from target loss remains a significant challenge.\nTo re-track the target after a tracking failure, RTracker [22] leverages a tree-structured memory system to detect target loss and a dedicated detector for self-recovery. However, this approach is constrained by its complex design and the detector's reliance on specific categories. Single-view tracking suffers from inherent limitations due to its restricted field of view, which is an inevitable challenge. In contrast, GMT [38] incorporates multi-view tracking within a single-view training framework. This limits its capacity to effectively model the intricate relationships between multi-view appearances and background contexts in the real world."}, {"title": "2.2. Multi-View Object Tracking", "content": "MVOT provides more comprehensive information about the target, effectively addressing issues such as occlusion. To leverage multi-view information, various fusion strategies have been developed for target association across viewpoints. Some approaches establish multi-view relationships by projecting detection results onto a BEV plane [42]. However, this method is prone to detection errors, especially multi-view object tracking (MVOT) faces several critical challenges. First, existing multi-view datasets are largely restricted to specific object categories like humans or birds [15, 40], limiting their applicability for generic object tracking. Second, current MVOT approaches [17, 19, 42] primarily focus on tracking specific categories of objects using detection and re-identification methods, which are not suitable for class-agnostic object tracking. Even when attempting to track generic objects across multiple views, researchers have to rely on single-view datasets for training due to the absence of comprehensive multi-view data [38]. This limitation severely restricts models' ability to understand complex spatial relationships and appearance variations across different viewpoints.\nTo address these challenges, we first construct a Multi-View object Tracking (MVTrack) dataset. MVTrack dataset contains 234K frames captured from 3-4 cameras, with precise bounding box (BBox) annotations covering 27 distinct objects across 9 challenging tracking attributes such as occlusion and deformation. Unlike existing datasets such as GMTD [38] which only provides testing data, MVTrack dataset offers both training and evaluation sets, enabling development and validation of MVOT models.\nTo effectively utilize MVTrack dataset, we propose a novel MVOT method named Multi-View Integration Tracker (MITracker) for tracking any object in video frames of arbitrary length from arbitrary viewpoints. As illustrated in Figure 1, MITracker can integrate multi-view features into a unified 3D feature volume and further refine tracking in occluded views, thus producing robust tracking outcomes. The framework of MITracker consists of two important modules: View-Specific Feature Extraction and Multi-View Integration. The first module employs a Vision Transformer (ViT) [12] to extract view-specific features of the target object from the current search frame in a streaming manner, where the target object is indicated by a reference frame. The second module constructs a 3D feature volume by fusing 2D features from multiple views and leveraging bird's eye View (BEV) guidance, which significantly enhances the model's spatial understanding. This 3D feature volume is then deployed in spatial-enhanced attention to improve tracking accuracy. MITracker allows for the maintenance of stable tracking results and demonstrates strong recovery capabilities in challenging cases such as occlusions and out of view objects.\nIn summary, our main contributions are as follows:\n\u2022 We introduce MVTrack, a large-scale multi-view tracking dataset containing 234K frames from 3-4 calibrated cameras. It has precise BBox annotations of 27 object categories across 9 challenging tracking attributes, which provides the first comprehensive benchmark for training class-agnostic MVOT methods and enriches the approaches for evaluating these methods.\n\u2022 We propose MITracker, a novel multi-view tracking method that constructs BEV-guided 3D feature volumes to enhance spatial understanding and utilize a spatial-enhanced attention mechanism to enable robust recovery from target loss in specific views.\n\u2022 Our extensive experiments demonstrate that MITracker achieves state-of-the-art (SOTA) performance on both MVTrack and GMTD datasets, improving recovery rate from 56.7% to 79.2% to reduce target loss in challenging scenarios."}, {"title": "3. MVTrack Dataset", "content": "MVTrack dataset is designed to fill the gaps in the field of MVOT and has received approval for data collection from an Institutional Review Board. As shown in Table 1, compared to single-view datasets, we maintain competitive class diversity while adding multi-view capabilities. Compared to MVOT datasets, we provide significantly richer object categories (27 vs 1-8 classes) and more videos (260) with practical camera setups (3-4 views). MVTrack dataset is the only dataset that combines multi-view tracking, rich object categories, absent label annotations, and calibration information.\nWe employ a multi-camera system"}, {"title": "Data Collection.", "content": "for data collection, consisting of 3 or 4 time-synchronized Azure Kinect cameras. All video sequences are recorded at a resolution of 1920\u00d71080 with 30 FPS. These cameras are positioned to ensure multiple overlapping views, and their intrinsic parameters are provided by the manufacturer. The extrinsic parameters are obtained through calibration and finely adjusted using MeshLab [7, 8]. With this calibration information, we set the central point of the scene as the origin of the world coordinate system, aligning all viewpoints to this unified coordinate system."}, {"title": "Data Annotation.", "content": "MVTrack dataset provides frame- level annotations, including 2D object BBoxes and ground coordinate annotations in a unified coordinate system (i.e., BEV annotations). Following an annotation strategy similar to LaSOT [13], where for each visible frame, an axis-aligned BBox tightly encloses the target, and an 'invisible' label is assigned for the invisible target. The BBox annotations are generated semi-automatically, with trackers [6, 41, 46] used for initial labeling. The machine-generated annotations are then manually adjusted and double-checked for accuracy. Subsequently, using camera calibration parameters, the 2D object BBoxes from multiple viewpoints are projected into the unified coordinate system to compute the BEV coordinates."}, {"title": "Challenging Attributes.", "content": "In our dataset, we particularly focus on 9 common tracking challenges to better assess tracker performance: Background Clutter, Motion Blur, Partial Occlusion, Full Occlusion, Out of View, Deformation, Low Resolution, Aspect Ratio Change, and Scale Variation."}, {"title": "Statistical Analysis.", "content": "MVTrack dataset consists of five indoor scenes, captured with a total of ten sets of calibration parameters. It covers 27 everyday objects, ranging from small objects like pens to larger objects such as umbrellas. The dataset includes 68 sets of multi-view data, comprising 260 videos and a total of 234,430 frames.\nWe divide the dataset into training, validation, and testing sets. The training set consists of 196 videos and 180K frames, while the validation set contains 30 videos and 28K frames. The testing set comprises 34 videos and 26K frames. We include an unseen scene in the validation and testing sets that are distinct from the scenes in the training set. Furthermore, the testing set includes both object categories that appear in the training set and new object categories not present during training. This enables evaluation of the model's performance across various targets and settings."}, {"title": "4. MITracker", "content": "We propose MITracker, a novel multi-view tracking framework that robustly tracks class-agnostic objects across multiple camera views. As illustrated in Figure 3, MITracker consists of two main components: (1) a view-specific feature extraction module (Sec. 4.1) that encodes frame features and generates single-view tracking results in a streaming fashion, and (2) a multi-view integration module (Sec. 4.2) that fuses multi-view features with BEV guidance and refines view-specific feature with a spatial-enhanced attention mechanism."}, {"title": "4.1. View-Specific Feature Extraction", "content": "As shown in Figure 3a, this module processes the video stream from a specific viewpoint k, and extracts target-aware features in the search frame at a timepoint t based on the reference frame that indicates the target object.\nWe employ ViT as the backbone of our view-specific encoder. The visual inputs of the view-specific encoder consist of a search frame $S \u2208 R^{3\u00d7H_s\u00d7W_s}$ and a reference frame $R\u2208 R^{3\u00d7H_r\u00d7W_r}$. As the transformer block processes a series of tokens, we segment the frames into non-overlapping patches with $p \u00d7 p$ resolution. The search and reference frames are individually embedded into a token sequence, represented by $I_S \u2208 R^{N_s \u00d7 D}$ and $I_R \u2208 R^{N_r \u00d7 D}$, where D is the hidden dimension, $N_s = \\frac{H_sW_s}{p^2}$ is the number of search tokens, and $N_r = \\frac{H_rW_r}{p^2}$ is the number of reference tokens.\nTo ensure temporal continuity between frames, akin to the method utilized in ODTrack [46], two specialized temporal tokens are also included in the inputs of the view-specific encoder to facilitate the propagation of temporal information. Specifically, at any given time t, a learnable token $T_t$ is randomly initialized, which is designed to capture temporal information of the current frame. Concurrently, we incorporate a token $T_{t\u22121}$ that carries temporal information from the preceding frame, which leverages historical features to enhance tracking accuracy and continuity. The input token sequence of our view-specific encoder can be formulated as the composition of the visual and temporal tokens $f = [T_t, T_{t\u22121}, I_R, I_S]$, while the output token sequence is denoted as $f' = [T'_t, T'_{t\u22121}, I'_R, I'_S]$.\nAfter obtaining $f'$, $T'_t$ is used to compute attention weights in conjunction with $I'_S$ to utilize temporal information for adjustments, which can be described as follows:\n$I_v = I'_S \u00b7 (I'_S \u00d7 (T'_t)^T)$,(1)\nwhere $I_v$ represents the extracted feature that encapsulates attention focused on the target object in the search frame.\nWe employ a BBox head based on the CenterNet architecture [47] to output tracking results from the extracted feature $I_v$. This head comprises three distinct sub-networks, each designed to compute the classification score map, BBox dimensions, and offset sizes, respectively. The highest-scoring position on the classification score map is identified as the target location. This configuration establishes a robust framework capable of effectively handling single-view visual object tracking tasks.\nTo facilitate further multi-view integration, we also apply convolutional layers to map $I_v$ to a 2D feature map - View-Specific Encoder."}, {"title": "4.2. Multi-View Integration", "content": "To effectively integrate 2D feature maps $F^{1}_{2D}$, $F^{2}_{2D}$, ..., $F^{K}_{2D}$ from K viewpoints, we project them into a 3D feature space and then aggregate them under the supervision of BEV guidance. Finally, we embed the aggregated feature to a 3D-aware token to refine all view-specific features $I^1_v$, $I^2_v$, ..., $I^K_v$ via spatial-enhanced attention, thus producing stable tracking results across different viewpoints.\nAs illustrated in Figure 3b, we construct a 3D feature volume of size X X Y X Z, where (X, Y) represents the horizontal plane and Z axis denotes the vertical direction following [17, 44]. For a viewpoint k, we project the (u, v) coordinates in $F^{k}_{2D}$ to (x, y, z) coordinates in the 3D feature volume by the formula below:\n$\\begin{bmatrix}x\\\\y\\\\z\\\\1\\end{bmatrix} = C_K[C_R C_t] \\begin{bmatrix}u\\\\v\\\\1\\end{bmatrix}$ (2)\nwhere $C_K$ represents the camera's intrinsic matrix, $C_R$ denotes the rotation matrix describing the camera's orientation, and $C_t$ is the translation vector specifying the camera's position in space. Upon establishing the mapping matrix, we implement bilinear sampling to populate the 3D feature volume. In scenarios that involve multiple viewpoints, we compute the average of the mapped values from each view to ensure consistency. Consequently, we derive a 3D feature volume represented as $F_{3D} \u2208 R^{32\u00d7X\u00d7Y\u00d7Z}$.\nTo better integrate multi-view spatial information, we apply 1D convolutional layers to aggregate features along the Z-axis of $F_{3D}$, resulting in $F'_{3D} \u2208 R^{32\u00d7X\u00d7Y}$, thereby consolidating spatial information within the (X, Y) plane. Subsequently, a classification head (i.e., BEV head) is employed to generate a BEV score map from $F'_{3D}$. This BEV map delineates the object positions on the horizontal plane, thereby imposing supervision constraints on information fusion across multiple viewpoints. This integrative approach allows for precise localization and mapping within multi-view scenarios.\nBEV guidance for the aggregated 3D feature $F'_{3D}$ only implicitly constrains the original single-view output, but it is insufficient to address the potential target loss issue due to the lack of direct supervision on tracking results. To remedy this, we introduce spatial-enhanced attention to explicitly incorporate $F'_{3D}$ into the tracking process as shown in Figure 3c.\nWe first use convolutional layers to embed $F'_{3D}$ into a 3D-aware token $T_{3D} \u2208 R^{1\u00d7D}$, which inherits multi-view spatial information. For all the K viewpoints, we then individually concatenate $T_{3D}$ with their unrefined features $I^1_v$, $I^2_v$, ..., $I^K_v$ produced by the view-specific encoder. For a viewpoint k, a series of transformer blocks take in its composite token sequence (T3D, $I^k_v$) and refine them using attention mechanisms that leverage fused 3D spatial information. A final BBox head outputs the refined tracking results, where potential errors such as target loss are corrected. - Multi-View Integration."}, {"title": "5. Experiments", "content": "In addition to MVTrack dataset, we use two external datasets for training and evaluation, which are detailed as follows."}, {"title": "Dataset", "content": "\u2022 GOT10K. GOT-10K [21] is a large and diverse dataset with a wide range of object categories. Its training set contains 9,335 videos across 480 moving object categories.\n\u2022 GMTD. GMTD [38] is a multi-view tracking test set with 10 scenes, captured by 2-3 uncalibrated cameras in indoor and outdoor settings. It includes 6 target types and various tracking challenges."}, {"title": "5.2. Implementation Details", "content": "For the BBox head, we employ a weighted focal loss [26] $L_{cls}$ for classification, along with the generalized intersection over union loss [31] $L_{giou}$ and $L_1$ loss for BBox regression. Additionally, a focal loss $L_{bev}$ is utilized for BEV map supervision. The overall loss function of the model is formulated as follows:\n$L_{track} = L_{cls} + \u03bb_{giou}L_{giou} + \u03bb_{L_1} L_1 + \u03bb_{bev} L_{bev}$,(3)\nwhere $\u03bb_{giou}$ = 5, $\u03bb_{L_1}$ = 2, and $\u03bb_{bev}$ = 0.1 are the coefficients that balance the contributions from each loss."}, {"title": "Training Setup.", "content": "We initialize our view-specific encoder with pre-trained DINOv2 [28] parameters using the ViT-base model [12]. For the visual inputs, we set the reference frame with 182 \u00d7 182 pixels, and the search frame with 364 \u00d7 364 pixels. We utilize the camera parameters from the dataset for projection, with the 3D feature volume having dimensions X = 200, Y = 200, and Z = 3.\nOur training process consists of two stages. In the first stage, we only train the view-specific feature extraction module. Specifically, we train the view-specific encoder and BBox head using single-view inputs from GOT-10K and MVTrack datasets until convergence. For each viewpoint, we include one reference frame and two random search frames from 200 frame interval in each iteration, thus promoting temporal information propagation between frames. In the second stage, we fine-tune the view-specific encoder and train the entire framework using multi-view data from the MVTrack dataset. For each training sample, we randomly select 2 to 4 viewpoints with one reference and two search frames in each iteration. All the training procedures are conducted on 2 NVIDIA A100 80GB GPUs."}, {"title": "5.3. Evaluation Metrics", "content": "We evaluate our method using three standard performance measures from the single-view tracking benchmark [13, 27, 39]: Area Under Curve (AUC), Precision (P), and Normalized Precision ($P_{Norm}$):\n\u2022 AUC: The Intersection over Union (IoU) measures the overlap between predicted and ground truth BBoxes in each frame. The AUC metric is calculated by varying the IoU threshold to evaluate the area error in the tracking region."}, {"title": "5.4. Comparison with Existing Methods", "content": "We evaluate tracking performance with single-view visual object tracking methods, training all models (except SAM2 and SAM2Long) on the GOT10K and MVTrack datasets. The models are tested on both the MVTrack and GMTD datasets under single-view and multi-view settings.\nHowever, single-view methods cannot handle multi-view inputs or generate multi-view predictions. To address this, we employ a post-fusion strategy to obtain multi-view results. Specifically, single-view predictions are first projected into the 3D world coordinate system. The region with maximum overlap is identified as the target position, which is then reprojected onto the 2D image plane of each viewpoint to generate the fused multi-view tracking results.\nAs shown in Table 2, MITracker achieves superior performance in both multi- and single-view tracking across different datasets. In multi-view scenarios with 3-4 cameras, MITracker outperforms other methods that rely on post-processing for multi-view fusion, surpassing the second-best method OSTrack by approximately 26% in $P_{Norm}$. In single-view settings, MITracker surpasses SOTA methods on the MVTrack dataset, achieving an AUC of 68.57%, which outperforms ODTrack by approximately 5%.\nNotably, MITracker exhibits strong generalization capabilities by achieving exceptional performance on the GMTD, despite it not being included in the training data. This demonstrates the robustness of our multi-view approach even in single-view scenarios. We attribute these improvements to our multi-view training strategy, which enables the model to better understand spatial relationships crucial for precise tracking. It is also noteworthy that post-processing degrades the performance of all single-view methods. This indicates a substantial distribution gap in view-independent feature detection across models, making effective fusion through geometric projections challenging."}, {"title": "Continuous Tracking Capability.", "content": "To further evaluate tracking robustness, we conducted three comparative experiments on the MVTrack dataset. Only MITracker utilizes multi-view inputs, other methods use single-view inputs and generate BBoxes independently.\nFirst, we analyzed tracking success rates across various IoU thresholds, as shown in Figure 4a. MITracker consistently outperforms competing methods regardless of the threshold value.\nSecond, we evaluated the recovery capability after the target was invisible by measuring the proportion of successful tracking resumption within given frame intervals [22]. As illustrated in Figure 4b, with a 10-frame interval, MITracker achieves a high success rate of 79.2% in these recovery tests. In comparison, SAM2Long only achieves a 56.7% recovery rate under the same setting, highlighting our method's exceptional ability to quickly reestablish tracking after the target dissapears.\nIn practical applications, users can manually intervene to restart the model's tracking by providing an accurate initial position. In this experimental setup, we measured the maximum continuous tracking length of video frames and the average number of restarts (triggered when target loss exceeds 10 frames, using ground truth for repositioning) [45]. As shown in Figure 4c, MITracker achieves nearly 100 frames longer tracking duration than ODTrack while"}, {"title": "5.5. Ablation Study", "content": "Results demonstrate that BEV Loss, which provides implicit multi-view information feedback, significantly enhances model performance. This improvement is attributed to its ability to augment spatial awareness during single-view feature extraction. The Spatial attention, which utilizes fused information to adjust outputs from single-view perspectives, also contributes to notable performance improvements in the model."}, {"title": "5.6. Visualization Comparison", "content": "Our qualitative evaluation focuses on the influence of occlusion and fast motion. In the upper part of Figure 5, we select two viewpoints from the MVTrack dataset and evaluate them on MITracker and ODTrack, which has the second-best performance on this dataset. The gray areas in the graph represent periods when the object is out of view or fully occluded by other objects. We can easily observe that MITracker is able to re-track the object shortly after it reappears, whereas ODTrack tends to continue in a lost state. For instances #405 and #515 in V2, even when the object reappears in the frame, ODTrack still mistakenly locks onto the wrong object. The bottom of Figure 5 presents tests conducted on the GMTD, where we also selected the second-best method, EVPTrack, for comparison with MITracker. When a pedestrian reappears after being obscured by a pillar, EVPTrack mistakenly locks onto the wrong target, whereas MITracker is able to maintain stable and continuous tracking.\nWe also visualize the predicted BEV trajectories from MITracker in Figure 6. Referencing the ground-truth trajectories, MITracker effectively integrates multi-view features and provides accurate 3D spatial information."}, {"title": "6. Discussion", "content": "In previous sections, we provide a detailed introduction to the MVTrack dataset and demonstrate the outstanding performance of MITracker. However, there are some areas that could be improved in future work.\nAlthough MVTrack dataset includes a diverse set of scenes, it currently consists of indoor environments only, potentially limiting the generalization of methods trained on it to outdoor settings. Additionally, MITracker relies on camera calibration for multi-view fusion, which may restrict its applicability in scenarios where calibration is challenging or infeasible."}, {"title": "Future work.", "content": "We plan to extend MVTrack dataset by including outdoor scenes and a wider range of tracking objects to enable the development of more generalizable multi-view tracking algorithms. Furthermore, we aim to enhance MITracker by reducing its dependency on precise camera calibration, making it more adaptable to scenarios where accurate calibration is challenging."}, {"title": "7. Conclusion", "content": "In this study, we address key challenges such as occlusion and target loss in MVOT by making two significant contributions: (1) MVTrack, a comprehensive dataset with 234K high-quality annotations across diverse scenes and object categories, and (2) MITracker, a novel visual tracking method that effectively integrates multi-view object features. MITracker achieves SOTA results on MVTrack and GMTD datasets, demonstrating its ability to provide stable and reliable tracking across different viewpoints and video durations. Our contributions lay the foundation for future advancements in MVOT, enabling the development of more robust and accurate tracking systems for real-world scenarios."}]}