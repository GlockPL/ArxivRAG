{"title": "PUTNAMBENCH: Evaluating Neural Theorem-Provers on the Putnam Mathematical Competition", "authors": ["George Tsoukalas", "Jasper Lee", "John Jennings", "Jimmy Xin", "Michelle Ding", "Michael Jennings", "Amitayush Thakur", "Swarat Chaudhuri"], "abstract": "We present PUTNAMBENCH, a new multilingual benchmark for evaluating the ability of neural theorem-provers to solve competition mathematics problems. PUTNAMBENCH consists of 1697 hand-constructed formalizations of 640 theorems sourced from the William Lowell Putnam Mathematical Competition, the premier undergraduate-level mathematics competition in North America. All the theorems have formalizations in Lean 4 and Isabelle; a substantial subset also has Coq formalizations. Proving the theorems requires significant problem-solving ability and proficiency in a broad range of topics taught in undergraduate mathematics courses. We use PUTNAMBENCH to evaluate several established neural and symbolic theorem-provers. These approaches can only solve a handful of the PUTNAMBENCH problems, establishing the benchmark as a difficult open challenge for research on neural theorem-proving. PUTNAMBENCH is available at https://github.com/trishullab/PutnamBench.", "sections": [{"title": "1 Introduction", "content": "Automating mathematical reasoning is a longstanding goal in artificial intelligence (Newell et al., 1957). A prominent line of work on the problem (Li et al., 2024) uses neural models to direct theorem-proving in formal frameworks like Lean 4 (Moura and Ullrich, 2021), Isabelle (Wenzel et al., 2008), and Coq (Huet et al., 1997). These frameworks can \"execute\" proofs like code and offer execution feedback, which simplifies the search for correct proofs.\nThe design of quality benchmarks is a key challenge in this research area. The two most prominent benchmarks for neural theorem-proving are MINIF2F (Zheng et al., 2021) and FIMO (Liu et al., 2023). The former formalizes a mix of problems from high-school level courses and mathematics competitions such as AIME, AMC, and IMO; the latter consists of a collection of IMO problems. Both benchmarks have limitations. For example, MINIF2F contains many problems that can be immediately solved using an SMT solver, and FIMO only targets the Lean 3 framework, which is no longer actively maintained.\nMore generally, as large language models (LLMs) grow in importance as a tool for neural theorem-proving (Li et al., 2024), preventing leakage between pretraining sets and evaluation sets is more important than ever. This makes the continued supply of new benchmarks an important goal.\nIn this paper, we respond to this challenge with PUTNAMBENCH, a new hand-curated, multilingual benchmark for neural theorem-provers. PUTNAMBENCH includes 1697 formalizations of 640 problems from the William Lowell Putnam Mathematical Competition, the premier college-level"}, {"title": "2 Background", "content": "Formal Theorem-Proving. Formal proof frameworks like Lean 4 (Moura and Ullrich,\n2021), Coq (Huet et al., 1997), and Isabelle (Wenzel et al., 2008) allow users to write machine-verifiable proofs of mathematical theorems. To create such a proof, one first uses a framework-specific language to formally state the target theorem. The mathematical objects referenced in the theorem can be imported from an existing repository or defined by the user. During the proof process, the proof framework maintains a state that includes information about the parts of the proof that remain to be completed. One can change this state by executing a proof step. The user's goal is to write a sequence of proof steps (in the framework's language) that changes the proof state to a special state \"QED\" in which there are no unmet proof obligations.\nThe Putnam Competition. The William Lowell Putnam Mathematical (Competition, 2024), organized by the Mathematical Associ-"}, {"title": "3 PUTNAMBENCH", "content": "PUTNAMBENCH is a multilingual evaluation benchmark consisting of formalized problems from the Putnam competition. PUTNAMBENCH is a manually produced benchmark, including 640 formalizations in Lean 4 and Isabelle, and 417 formalizations in Coq. In aggregate, PUTNAMBENCH contains 1697 formalizations of Putnam competition problems. We also incorporate the informal statements and numerical solutions where applicable.\nNow we elaborate on the main features of PUTNAMBENCH.\nDiversity and Breadth. Compared to MINIF2F (Zheng et al., 2021) and FIMO (Liu et al., 2023), which generally rely on high-school mathematics, PUTNAMBENCH incorporates a wider variety of problems which require definitions of the standard undergraduate mathematics curriculum. The PROOFNET benchmark (Azerbayev et al., 2023) also sources problems from the undergraduate curriculum, but these problems are generally from standard textbooks as opposed to mathematical competitions. Putnam problems often require definitions from multiple fields, which standard textbooks do not necessarily target. Formalizations in PUTNAMBENCH include concepts from a wide range of mathematical fields, including: (i) Analysis: Limits, integrals, derivatives, continuity; (ii) Linear Algebra: Matrices, determinants, fields; (iii) Abstract Algebra: Rings, groups, magmas, permutations; (iv) Algebra: Polynomials, inequalities, algebraic expressions; (v) Number Theory: Primes, irrationality, base representations, divisors, palindromes; (vi) Geometry: Polygons, point sets, line intersections, Euclidean distance; (vii) Set Theory & Combinatorics: Countability, power sets, discrete structures, counting.\nMultilinguality. PUTNAMBENCH contains formalizations of Putnam problems in Lean 4, Isabelle, and Coq. The formalizations also include concepts defined in each proof assistant's mathematical repositories - notably, Mathlib, the HOL standard library, and Coquelicot (among various Coq repositories). To the best of our knowledge, PUTNAMBENCH is the first undergraduate-level competition benchmark for each of these languages. Furthermore, we are the first to produce a human mathematics competition-style evaluation benchmark for Coq.\nWe hope that this contribution can enable Coq practitioners access to the rapidly-growing field of machine learning for mathematics."}, {"title": "4 Experimental Evaluation", "content": "To understand the challenges that PUTNAMBENCH poses for state-of-the-art theorem-proving approaches, we attempt to solve its problems using a suite of such approaches. Given the relative lack of tailored systems for multilingual theorem-proving, we run evaluations for each language separately. Any method that is evaluated on multiple languages is based on off-the-shelf foundation models.\nMetrics. Our evaluation is based on the pass@n (Lample et al., 2022) metric. This metric measures a prover's ability to produce a successful proof, as determined by the formal proof environment, given a budget of n proof attempts. In search-based methods (Thakur et al., 2024), each proof attempt involves a distinct search that can query a neural model multiple times.\nModels. For each of the languages, we perform evaluations using GPT-4 (OpenAI, 2023) \u2021, a highly capable foundation model. We run evaluations using in-context learning, appending several examples of successful proofs of simple theorems in each language. For evaluations with Lean 4 approaches, we note that many approaches have targeted Lean 3, which is not backward-compatible and no longer actively maintained. We evaluate COPRA (Thakur et al., 2024) on PUTNAMBENCH, modifying the prompt examples of COPRA to enable search in Lean 4. Furthermore, we run evaluations LeanDojo's retrieval-augmented prover REPROVER, a finetuned model designed to utilize incorporate retrieved lemmas as part of the proof search. We also include evaluate with the retrieval component held out.\nFor our Isabelle experiments, we run evaluations of Draft, Sketch, and Prove (DSP) (Jiang et al., 2022b) using GPT-4 as the underlying foundation model, noting that many further works for theorem-proving in Isabelle have extended on the DSP pipeline as we mention in Section 5. We also run evaluations using stand-alone invocations to Sledgehammer, a powerful symbolic automation tool in Isabelle that relies on calls to external SMT solvers.\nAs for our Coq experiments, prior neural approaches for Coq have mostly targeted software verification tasks, as opposed to competition mathematics. As a result, our Coq experiments use COPRA, which also supports theorem-proving in Coq. We evaluate using the Tactician (Blaauwbroek et al., 2020) platform with the locality sensitive hashing model configuration. We also run evaluations using CoqHammer (Czajka and Kaliszyk, 2018), a tool similar to Isabelle's Sledgehammer, which makes calls to external constraint solvers."}, {"title": "4.1 Results", "content": "Lean 4. We prompt GPT-4 in a pass@10, setting temperature T = 0.7 and using several examples of simple theorems and proofs, to generate a proof for each problem. The result of this experiment yields a single successful proof across all 640 Lean formalizations. The problem (Putnam 1988 B1) and the generated proof are given in Figure 1. In particular, Putnam 1988 B1 is solved on the first of 10 attempts. An example of a failure mode of GPT-4 is given in Figure 18."}, {"title": "4.2 General Analysis", "content": "Aggregating over all experiments performed in all languages, we find that a total of 6 problems in PUTNAMBENCH are successfully proven. A majority of these come from evaluations in Isabelle, particularly with strong contributions from Sledgehammer. Sledgehammer can solve all three problems involving magmas which appear in our benchmark but fails to produce successful proofs for any other formalization. DSP solves an additional two problems and relies heavily on Sledgehammer to fill in the proofs of intermediate steps. The single problem solved in Lean and Coq also makes use of automated tactics like linarith and lia, and requires only a single crucial step.\nHence, we find that a few PUTNAMBENCH problems are not entirely intractable using current methods. However, anecdotally, these problems are among the easiest ever included in the Putnam competition. All admit a very short natural language proof and do not require reasoning about particularly complicated objects. We believe that significant advancements in automated mathematical reasoning are required to make progress on PUTNAMBENCH."}, {"title": "5 Related Work", "content": "Formal Benchmarks. Several evaluation benchmarks for formal mathematics have been developed in recent years. MINIF2F (Zheng et al., 2021) is a formal-to-formal benchmark of competition problems, sourced from high school competitions such as the AMC, AIME, and IMO. MINIF2F is a multilingual benchmark, comprising of 488 problems each formalized in Lean 3, Metamath, Isabelle and HOL Light. We chose not to include formalizations in Metamath and HOL Light as they have not been the focus of attention for neural theorem-proving. A similar competition-style benchmark is FIMO (Liu et al., 2023), which contains 149 Lean 3 formalizations of IMO shortlist problems produced using a back-translation procedure with GPT-4. The automatically-generated formalizations are then manually verified. Both benchmarks are designed to measure certifying the solution to the informal problem statement when one exists. Compfiles (2024) is a collection of 171 Lean 4 formalizations of competition problems, predominantly from the IMO and USAMO, often accompanied by a formal proof, which has not seen use in benchmarking automated theorem-provers. ProofNet (Azerbayev et al., 2023) introduced a benchmark of 371 exercises, formalized in Lean 3, from standard textbooks in the undergraduate mathematics curriculum. While largely not competition-based, problems in ProofNet draw from a broader library of concepts than miniF2F and FIMO, which rely only on high-school mathematics. LeanDojo (Yang et al., 2023) introduces a dataset of formal mathematics and proofs derived from Lean's mathlib library (mathlib Community, 2020), and trains a retrieval-augmented model towards generating proofs on their held-out test set. ProverBot9001 (Sanchez-Stern et al., 2020) introduced a dataset for theorems and proofs written in"}, {"title": "6 Conclusion", "content": "We presented PUTNAMBENCH, a benchmark for neural theorem-proving consisting of formalizations of Putnam competition problems. A distinctive feature of PUTNAMBENCH is that it spans a broad range of undergraduate-level mathematical topics, including algebra, analysis, and number theory. Another unique benefit is that it includes problems in Lean 4, Isabelle, and Coq, the three most popular formal proof frameworks.\nAs our experiments show, PUTNAMBENCH is a challenging benchmark: all current theorem-proving approaches fail to solve more than a handful of its problems. We believe that these failures include two root causes: (i) While current theorem-provers can effectively stitch together standard proof steps well-represented in the training corpus, they often fail at synthesizing new lemmas and or-chestrating these lemmas into intricate proofs. (ii) Current methods often fail to leverage the deep knowledge available in mathematics repositories. Developing a new generation of neural theorem-provers in which these weaknesses are at least partly addressed is an exciting direction of future research."}]}