{"title": "Spatiotemporal-aware Trend-Seasonality Decomposition Network for Traffic Flow Forecasting", "authors": ["Lingxiao Cao", "Bin Wang", "Guiyuan Jiang", "Yanwei Yu", "Junyu Dong"], "abstract": "Traffic prediction is critical for optimizing travel scheduling and enhancing public safety, yet the complex spatial and temporal dynamics within traffic data present significant challenges for accurate forecasting. In this paper, we introduce a novel model, the Spatiotemporal-aware Trend-Seasonality Decomposition Network (STDN). This model begins by constructing a dynamic graph structure to represent traffic flow and incorporates novel spatio-temporal embeddings to jointly capture global traffic dynamics. The representations learned are further refined by a specially designed trend-seasonality decomposition module, which disentangles the trend-cyclical component and seasonal component for each traffic node at different times within the graph. These components are subsequently processed through an encoder-decoder network to generate the final predictions. Extensive experiments conducted on real-world traffic datasets demonstrate that STDN achieves superior performance with remarkable computation cost. Furthermore, we have released a new traffic dataset named JiNan, which features unique inner-city dynamics, thereby enriching the scenario comprehensiveness in traffic prediction evaluation.", "sections": [{"title": "Introduction", "content": "With technological advancements, a diverse array of sensors has been increasingly integrated into monitoring systems to bolster modern intelligent transportation systems (ITS) (Cirstea et al. 2021; Ji et al. 2022; Dai et al. 2023). Transportation authorities deploy a variety of sensors, such as electronic police cameras and bayonet detectors, across road networks to continuously collect essential traffic data, including flow and speed. Utilizing historical traffic flow data and road network topology, traffic forecasting aims to predict future flow variations, thereby improving daily travel and traffic management (Dai et al. 2021; Li et al. 2023).\nIn the realm of traffic forecasting, considerable efforts are devoted to modeling traffic dynamics. Methods based on deep learning, especially the Spatio-Temporal Graph Neural Networks (STGNNs), have proven more effective than statistical time series analysis and shallow machine learning techniques in addressing traffic forecasting challenges.\nTo address temporal dynamics, sequential models like RNN-based variants (Graves and Graves 2012; Deng et al. 2022, 2024a) and non-sequential Transformers (Vaswani et al. 2017) have been profoundly studied. For spatial dynamics, recent progress has been made with the Graph Neural Networks (GNNs) (Yin et al. 2021), which represent sensors as nodes within a graph, leveraging graph structure to capture traffic patterns. Despite these substantial advancements, our study suggests that current methods still exhibit considerable potential for improvement in two critical aspects.\nFirstly, the prominent spatio-temporal characteristics in traffic flow can be more effectively modeled with the appropriate inductive bias. Although previous studies have demonstrated (Zhang, Zheng, and Qi 2017; Yu et al. 2019; Guo et al. 2019), traffic patterns are strongly influenced by the specific temporal periodicities, most efforts have roughly incorporated these temporal features into the models without explicitly modeling the synergy between the long and short periodicities (Chen et al. 2018; Deng et al. 2024b). Regarding spatial aspects, while different locations exhibit unique spatial characteristics, most GNN methods construct static graphs based solely on the distances between two nodes, fail to consider the global interactions among all nodes in the graph (Yin et al. 2021).\nSecondly, effective trend-seasonality decomposition of traffic flow can greatly enhance the representation learning of traffic nodes (Wu et al. 2021; Fang et al. 2023). Utilizing this methodology improves the prediction of traffic flow by distinguishing systematic patterns and noise components. However, the application of trend-seasonality decomposition predominantly to individual nodes in a traffic network overlooks the interactions among global nodes, thereby diminishing the quality of node representations learned by GNNS.\nTo bridge these research gaps, we introduce a Spatiotemporal-aware Trend-Seasonality Decomposition Network (STDN), which enhances global node representations through a novel trend-seasonality decomposition incorporating spatio-temporal embeddings. It features three key modules: (1) Module of Spatio-Temporal Embedding Learning models the temporal periodicity by learning the temporal embedding including specific weeks and minutes, and acquires an initial spatial location embedding from the eigenvalues and eigenvectors of the graph Laplacian matrix."}, {"title": "Problem Definition", "content": "In this section, we define the key components and objectives of our study, focusing on the structure of traffic networks and the goals of traffic forecasting.\nDefinition 1 (Traffic Network) Given the real-world traffic scenarios, we define the traffic network as a directed graph G = {V,E, A}, where V denotes a set of N nodes, each corresponding to a different sensor within the road network. E represents a set of edges that denote the connectivity among the nodes. $A \\in R^{N \\times N}$ is the adjacency matrix that models the connectivity between nodes.\nDefinition 2 (Traffic Forecasting) Given historical traffic time series and the road topology, the objective of traffic flow forecasting is to predict future values of traffic time series. Specially, we represent the historical time series as a signal tensor X = [X1, X2, \u2026\u2026\u2026, XT] $\\in R^{T \\times N \\times C}$, where T is the length of historical traffic time series and C is the number of dimensions of node attributes. We aim to construct a function f(\u00b7) that maps the historical time series over T time steps to predict the subsequent T' time steps:\n$[X_1, X_2,..., X_T;G] \\rightarrow [X_{T+1}, X_{T+2},..., X_{T+T'}].$ \\(\\)\n\\((1)\\)"}, {"title": "Methodology", "content": "STDN consists of three principal modules, which are introduced as follows.\nModule 1: Dynamic Relationship Graph Learning\nTo address the limitations of simple distance-based connectivity, which overlooks the high-order relationships between each node, we construct a dynamic relationship graph that considers different time steps and nodes. This approach allows us to model the complex higher-order spatio-temporal relationships among all traffic nodes effectively.\nInspired by Han et al. (2021), we design three learnable matrices and a learnable core tensor to streamline the constructing of the dynamic graph. These components include: a time slot embedding $E_t \\in R^{N_t \\times D}$, a starting"}, {"title": "Module 2: Spatio-Temporal Embeddings Learning", "content": "To more effectively capture the temporal correlations and periodicities in traffic flow, we design a temporal context embedding learning module. Given the time of day, represented as $z_a \\in R^T$ and the day of the week, represented as $z_w \\in R^T$, we extract temporal features and encode them by one-hot. By concatenating these encoded features (denoted by [,]), we generate an initial temporal embedding as below:\n$Z = \\sigma(W[onehot(z_a), onehot(z_w)]),$ \\((4)\\)\nwhere $Z \\in R^{T \\times D}$, $\\sigma$ denotes the ReLU activation function. W comprises the trainable parameters. D specifies the dimensionality of the temporal embedding.\nTo enhance the ability of the temporal embedding to learn multi-resolution temporal features and to facilitate its integration with traffic time series, we refine the initial temporal embedding using a bias-free MLP:\n$M = \\sigma_2(W_2\\sigma_1(W_1Z)),$ \\((5)\\)\nwhere $\\sigma_1$ denotes the ReLU activation function, $\\sigma_2$ is the sigmoid activation function, and $W_1, W_2$ are the trainable parameters. The resulting temporal embedding $M \\in R^{T \\times D}$ is utilized in subsequent stages of the model.\nTo effectively model the structure of the road network, we utilize the normalized Laplacian matrix, defined as:\n$\\Delta = I \u2013 D^{-1/2}AD^{-1/2},$ \\((6)\\)"}, {"title": "Module 3: Trend-Seasonality Decomposition", "content": "Given the strong correlation between the trend-seasonality of a traffic node and its spatio-temporal context, the Trend-Seasonality Decomposition module is designed based on spatio-temporal embeddings learning. This module effectively disentangles the traffic flow of each node into distinct trend and seasonal components, adjusting the representations to better suit the forecasting task.\nWhen processing the traffic sequence data with trend-seasonality decomposition, since the dimensionality of the traffic hidden states and the spatio-temporal embeddings has been aligned, we initially derive the trend component by through multiplication-wise interaction between the hidden states of the nodes $H_L$ and the spatio-temporal embeddings M. The residual part constitutes the seasonal component. The calculations are as follows:\n$X_t = H_L \\odot M,$ \n$X_s = H_L - X_t,$ \\((10)\\)\nwhere $X_s, X_t \\in R^{T \\times N \\times D}$ denote the seasonal and the extracted trend-cyclical part respectively. $\\odot$ is the Hadamard product. The trend component of traffic flow is generally smooth and strongly influenced by the time and location of the traffic node. Our approach is based on the mild assumption that nearby times and locations yield similar trends, while distinct times and locations exhibit unique traffic patterns."}, {"title": "Encoder-Decoder Architecture", "content": "In this module, we utilize an encoder-decoder network to extract deeper spatio-temporal features. For the encoding process, we employ Gated Recurrent Unit (GRU) due to its effectiveness in capturing temporal dependencies. For the decoding process, we opt for the Transformer architecture, primarily because of its unique multi-head attention mechanism, which generates predictions with superior performance. We denote use the GRU(.) to denote the Gated Recurrent Unit as below:\n$Y_t = GRU(X_t),$ \n$Y_s = GRU (X_s),$ \\((11)\\)\nwhere $Y_s, Y_t \\in R^{T \\times N \\times D}$ represent the outputs of the GRU encoder for the seasonal and trend components, respectively. These outputs are then combined through element-wise summation to produce the final output $Y \\in R^{T \\times N \\times D}$.\nThe Transformer architecture utilizes multi-head attention mechanisms, which first projects the queries, keys and values into h different d-dimensional subspaces, and then execute the attention function in parallel:\n$MHSA(Q, K, V) = \\oplus(head_1, . . ., head_h)W^O,$ \n$head_i = softmax(\\frac{(QW_i^Q)(KW_i^K)^T}{\\sqrt{d}})(VW_i^V),$ \\((12)\\)\nwhere $W$ is the training parameter.\nInspired by Guo et al. (2023), we incorporate a component known as the Bottleneck Transformer Block (BT block) into our architecture, strategically designed to reduce both temporal and spatial complexity.\nFor the predicted time of day and day of the week, we obtain the predicted time embedding $M^P \\in R^{T'\\times D}$ according to equation 4 and equation 5. To align with the default settings, we maintain $T = T'$. Additionally, we extend the temporal embedding $M^P$ across the dimension N, resulting in $M^P \\in R^{T' \\times N \\times D}$.\nTo effectively prompt the decoder to focus on capturing the high-order dynamics between time and space, we concatenate the predicted temporal embedding and spatial embedding with the output from $(l\u22121)^{th}$ BT block as $Z^{(l\u22121)} \\in$"}, {"title": "Performance Comparison", "content": "Table 2 presents the results from graph-based baselines and grid-based baselines. The best results are highlighted in bold, and the second-best results are underlined. Based on these results, several key conclusions can be drawn:\n\u2022 STDN achieves state-of-the-art performance, particularly evident in the PeMS04 and JiNan datasets. Traditional machine learning methods such as ARIMA typically perform poorly, as they are unable to capture the non-linear correlations present in the spatio-temporal traffic data.\n\u2022 Among the GCN-based models, AGCRN demonstrates strong performance. Compared to other models, STDN excels in capturing the structure of the road network by effectively integrating eigenvalues from the Laplacian matrix with traffic flow data.\n\u2022 Attention-based models generally perform near optimally among all baselines. Notably, STDN distinguishes itself by integrating spatio-temporal embeddings with traffic flow data and the decoder, significantly enhancing traffic forecasting accuracy.\n\u2022 Compared to the baseline models, we incorporate multi-resolution temporal features, such as \"time of day\" and \"day of week\", for temporal embeddings, alongside a geospatial directed graph for spatial embeddings. Based on these spatiotemporal embeddings, traffic flow is disentangled into trend and seasonality parts. This novel dis-"}, {"title": "Ablation Study", "content": "To evaluate the effectiveness of different components in STDN, we conducted the ablation study with several variants of the STDN:\n\u2022 w/o TE: This variant removes the temporal embedding modeling, meaning the decoder operates solely with spatial embedding cues.\n\u2022 w/o SE: This variant removes the spatial embedding modeling, meaning the decoder operates solely with temporal embedding cues.\n\u2022 w/o STE: This variant eliminates spatio-temporal embedding, thus the traffic flow is not decomposed into trend-seasonality components. As a result, the decoder does not incorporate any spatio-temporal cues.\n\u2022 w/o DRG: This variant eliminates the dynamic relationship graph learning.\n\u2022 w/o STD: Instead of using spatiotemporal-aware decomposition to disentangle the traffic sequence data, this vari-"}, {"title": "Parameter Sensitivity Study", "content": "Figure 2 illustrates the results of hyper-parameter sensitivity analysis for our STDN on PeMS04 and PeMS07 datasets. This study involved varying the number of decoder layers and the number of features in STDN, exploring options within the ranges of [1, 2, 3, 4] for layers and [32, 64, 96, 128, 160] for features. From this analysis, we can draw several conclusions: (1) The performance of our model improves with an increasing in the number of decoder layers but stabilizes at 2 layers. (2) Optimal performance is achieved with 128 features on the PeMS04 dataset and 96 features on the PeMS07 dataset. This finding highlights that while increasing the number of features generally enhances the model's capability to represent complex traffic patterns, but excessive features may introduce noise and degrade model performance."}, {"title": "Model Efficiency Study", "content": "To demonstrate the efficiency of our model, we benchmark STDN against DMSTGCN, SSTBAN, and STWave, which have achieved suboptimal results at the PeMS04 and JiNan datasets. Figure 3 displays the average training time per epoch and inference time for each model. Figure 4 illustrates the MAE curves on the validation part of PeMS04 dataset during the training process. The following observations can be made: (1) STDN not only trains faster but also infers quicker than the compared models. (2) STDN demonstrates a faster convergence rate, achieving better performance in fewer epochs. In our experiments, while STWave reaches its best performance at epoch 100, its MAE is still higher than the lowest MAE achieved by our STDN at just epoch 19.\nThe computational complexity of our STDN encoder-decoder module is O(TD+LND), with the encoder and decoder contributing complexities of O(TD) and O(LND), respectively. Here, L denotes the number of bottleneck transformer blocks. The complexity of the spatio-temporal embedding module is given by O((T + N)D + N\u00b3). Although calculating the eigenvectors and eigenvalues of the graph Laplacian is computationally intensive, marked by a complexity of O(N\u00b3), this process can be efficiently handled through preprocessing prior to training. Therefore, STDN maintains comparable time and memory complexity during training, ensuring efficiency without compromising performance."}, {"title": "Conclusion", "content": "In this paper, we introduce a novel spatiotemporal-aware trend-seasonality decomposition network (STDN), which marks a pioneering approach in employing spatio-temporal embeddings to learn disentangled representations of traffic flow. The empirical evaluations conducted across three real-world datasets demonstrate the superior performance of STDN over existing models. The release of the new inner-city dataset JiNan can also enrich the scenario comprehensiveness in traffic forecasting evaluations."}]}