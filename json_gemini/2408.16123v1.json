{"title": "ChartEye: A Deep Learning Framework for Chart\nInformation Extraction", "authors": ["Osama Mustafa", "Muhammad Khizer Ali", "Imran Siddiqi", "Momina Moetesum"], "abstract": "The widespread use of charts and infographics as\na means of data visualization in various domains has inspired\nrecent research in automated chart understanding. However,\ninformation extraction from chart images is a complex multi-\ntasked process due to style variations and, as a consequence,\nit is challenging to design an end-to-end system. In this study,\nwe propose a deep learning-based framework that provides a\nsolution for key steps in the chart information extraction pipeline.\nThe proposed framework utilizes hierarchal vision transformers\nfor the tasks of chart-type and text-role classification, while\nYOLOv7 for text detection. The detected text is then enhanced\nusing Super Resolution Generative Adversarial Networks to\nimprove the recognition output of the OCR. Experimental results\non a benchmark dataset show that our proposed framework\nachieves excellent performance at every stage with F1-scores of\n0.97 for chart-type classification, 0.91 for text-role classification,\nand a mean Average Precision of 0.95 for text detection.", "sections": [{"title": "I. INTRODUCTION", "content": "Data visualization in the form of infographics such as charts,\ngraphs, and plots has been widely adopted for summarization\nand analytical purposes in various domains. Charts also play\na vital role as an integrated component of interactive visual-\nization dashboards due to effective and efficient interpretation\nof complex data patterns. With the increased use of charts\nand plots, the need for information extraction from these\nhas also gained popularity. Automatic information extraction\nfrom chart images is an emerging area of research that seeks\nto develop computer vision and natural language processing\n(NLP) algorithms to identify and extract data points from\nvisual charts such as bar graphs, line plots, and pie charts and\ninterpret their semantics. Primarily two types of information\ncan be extracted from charts i.e. explicit and implicit. Explicit\ninformation includes the graphical and textual components\nthat constitute a chart, while implicit information determines\ntheir relation with each other and their semantics. Explicit\ninformation extraction is the pre-requisite for implicit un-\nderstanding and is the prime focus of this research as well.\nDue to the complexity of the problem, it is divided into\nseveral sub-modules or tasks [1]\u2013[3]. The first task involves\nthe identification of the chart type since the information layout\nof each type is different and may require separate processing.\nThe second step is to detect and recognize the text and graphic\ncomponents of the chart. This step is vital to determine the\nsemantics of the chart. Another important step is the text-role\nclassification task which identifies the labels and values of the\nvarious types of textual information present. For instance, X-\naxis, Y-axis, legend, titles, etc. The final step is to determine\nthe association between the identified text roles and their\nvalues. All these tasks are characterized by challenges like\nvariations in component position, layout, structure, text size,\nfont, and orientation. Incorrect\nextraction of explicit information can adversely impact the\nimplicit inference of chart images.\nDue to the challenging nature of the problem, information\nextraction from chart images has remained an active area of\nresearch for the document analysis and recognition (DAR)\ncommunity. It has also been a regular challenge at popular\nDAR events like ICDAR 2019 [1], ICPR 2020 [2], and ICPR\n2022 [3]. In earlier studies [8], traditional statistical methods\n(i.e. area, mean, median, and variance) were employed to\nclassify various chart types. High level shape descriptors\nlike Hough transform and histograms of oriented gradients\n(HOG) were also used for similar purposes by authors in [9]-\n[11]. These hand-crafted features were then used to train\nmachine learning models like Hidden Markov Model [12],\nMultiple Instance Learning [13], Support Vector Machines\n(SVM) [14], [15], and decision trees [16] to identify the\ninput chart type. The main limitation of this approach was\nthe lack of generalization across various types of charts.\nWith the paradigm shift towards deep representation learning,\nstudies like [17]\u2013[20] utilized various deep learning models"}, {"title": "II. METHODOLOGY", "content": "In this section, we discuss the proposed framework in\ndetail. Figure 2 illustrates the main steps in the pipeline\ni.e. chart-type classification, text detection, detected-text up-\nscaling, text recognition, and text-role classification. The\nframework utilizes a combination of deep convolutional and\nvision transformer-based approaches to effectively extract in-\nformation from chart images. For instance, for tasks such\nas text detection, a one-stage object detector is employed,\nwhile for chart-type classification and text-role classification,\na hierarchical vision transformer is used. In order to enhance\nthe text, an enhanced super-resolution generative adversarial"}, {"title": "A. Chart-Type Classification", "content": "Classification of chart type is a challenging task for any clas-\nsifier, as different charts have increasingly different structures\nand semantics. Although convolutional neural networks have\nshown considerable success however, there is still room for\nimprovement in terms of performance and generalization [1].\nIn our case, we employed the state-of-the-art Swin transformer\narchitecture [33], pre-trained on ImageNet. It is a hierarchical\nvision transformer that uses shifted windows to generate\nhierarchical feature maps. Due to contextual learning, it is\nmore robust in the classification of chart types i.e. 15 in\nour case as shown in Table II. An input image is\npassed through a patch partitioning layer, the output of which\nenters stage 1 where a linear embedding layer and two Swin\nblocks are present. In stage 2, patch merging is performed\nand the output is passed onto the next two Swin blocks. In\nstage 3, patch merging with six Swin blocks is done. Finally,\npatch merging with two Swin blocks is repeated. A Swin\ntransformer block consists of a shifted window-based multi-\nhead self-attention MSA (SW-MSA) module, followed by a\nmulti-layered perceptron (MLP), with GELU non-linearity at\nin between. Layer normalization (LN) is applied before each\nMSA module and each MLP. A skip connection is applied after\neach module. A detailed architecture of two Swin transformer\nblocks is illustrated in the Figure 4. With the shifted window\npartitioning approach, consecutive Swin transformer blocks are\ncomputed as shown in Equations 1, 2, 3, 4.\n$z^l = W \u2013 MSA(LN(z^{l-1})) + z^{l-1}$ (1)\n$z\u2019^l = MLP(LN(z^l)) + z^l$ (2)\n$z^{l+1} = SW \u2013 MSA(LN(z\u2019^l)) + z\u2019^l$ (3)\n$z^{l+1} = MLP(LN(z^{l+1})) + z^{l+1}$ (4)\nW-MSA and SW-MSA denote window-based multi-head\nself-attention using regular and shifted window partitioning\nconfigurations, respectively.  and z\u00b9 denote the output fea-\ntures of the SW-MSA module and the MLP module for block\nl, respectively."}, {"title": "B. Text Detection", "content": "Text detection is performed on the chart image, focused on\nlogical blocks with a single role. This means that a multi-\nline axis-title should be detected as a single block. This\nstep performs dual role as it not only detects text for the\npurpose of recognition but the detected text from this step\nis also passed on to the last step in pipeline for text-role\nclassification. All types of text such as axis title, legend text,\nchart title, and tick values are detected. It is a challenging\ntask where there is sometimes increasingly small-scale text\npresent in the image. We performed transfer learning using\na state-of-the-art YOLOv7 [34] pre-trained on MS-COCO. It\nis a single-stage detector that performs object detection by\nfirst separating the image into N grids of equal size. Each of\nthese regions is used to detect and localize any objects they\nmay contain. Although recently, transformer architectures have"}, {"title": "C. Detected Text Upscaling", "content": "In most chart samples, the resolution of the text is very low\nresulting in poor recognition even by a mature text recognizer.\nThus, we propose an additional step utilizing enhanced super\nresolution generative adversarial network (ESRGAN) [35] to\nupscale the image without losing the pixel information as\ncompared to conventional image upscaling techniques. Im-\nage super-resolution (SR) techniques reconstruct a higher-\nresolution (HR) image or sequence from the observed lower-\nresolution (LR) images. ESRGAN\nis an enhanced version of super resolution GAN (SRGAN),\nwhich introduces the residual-in-residual dense block (RRDB)\nwithout batch normalization as the basic network building unit.\nThe perceptual loss has been improved by using the features\nbefore activation, which could provide stronger supervision for\nbrightness consistency and texture recovery. The total loss for\nthe generator is given by Equation 5.\n$L_G = L_{percep} + \\lambda L^{Ra}_{adv} + \\eta L_1,$ (5)\nWhere $L_{percep}$ is the perceptual loss, $L^{Ra}_{adv}$ is the adversarial\nloss for generator part in the GAN, $L_1 = E_{x_i} ||G (x_i) - Y||_1$\nis the content loss that evaluates the 1-norm distance between\nrecovered image $G (x_i)$ and the ground-truth y, and \u03bb, \u03b7 are\nthe coefficients to balance different loss terms. This proposed\nstep helps improve the recognition accuracy in the next step\nsignificantly."}, {"title": "D. Text Recognition", "content": "Text recognition is performed on the up-scaled detected text\nfrom the previous step. As the text in charts is in English, thus\nwe have utilized a TPS-ResNet-BiLSTM-Attn [36] architec-\nture to perform text recognition. The model applies a thin-\nplate spline (TPS) spatial transformation. ResNet backbone\nis used as a feature extractor and bi-directional long short\nterm memory network (BiLSTM) [37] for sequence modeling.\nBiLSTM is a type of RNN architecture that processes input\nsequences in both forward and backward directions, handling\nvariable-length input sequences and learning context from both\npast and future inputs making it good for sequence modeling"}, {"title": "E. Text Role Classification", "content": "Text on a chart image has a specific role. The purpose\nof this task is to classify the role of text. Nine roles are\nbeing considered in this study. These include chart title, mark\nlabel, legend title, legend label, axis title, tick label, tick\ngrouping, value label, and others. It is a challenging task due\nto the position, orientation, and size variations of text in a\nchart [1], [2]. We attempt to address\nthese challenges by fine-tuning a Swin transformer [33] as a\nseparate classifier for text-role classification instead of using\nYOLOv7's recognizer. Therefore, the text detected by the\nYOLOv7 model is fed to the Swin transformer for role\nclassification after upscaling. This enhances the performance\nof the text-role classification step in different chart types. Also\nSwin Transformer's ability to capture multi-scale features,\npositional encoding, and self-attention mechanism makes it\nan effective technique for handling orientation issues in chart\nimages. We also performed experimentation using DEtection\nTRansformer (DETR) [38] and YOLOv7. However, Swin\ntrasformer outperforms both of these techniques significantly."}, {"title": "III. EXPERIMENTAL PROTOCOL", "content": "This section details the experimental protocol employed\nin this study. The main objective of our experiments is to\nvalidate the methodology employed in each step. The scope\nof the study is not to propose an end-to-end system but\ninstead provide a framework. Hence, each step is indepen-\ndently evaluated. In an end-to-end system, the performance of\neach subsequent step is dependent on the previous tasks and\ntherefore it is hypothesized that performance improvement of\none step will enhance the overall accuracy of the complete\nsystem. In the subsequent subsections, we provide details of\nthe dataset employed followed by model training and testing\nfor each task i.e. chart-type classification, text detection, text\nrecognition, text enhancement, and text-role classification."}, {"title": "A. Dataset", "content": "To evaluate the performance of each task, we employed the\nICPR2022 CHARTINFO UB PMC competition dataset [3].\nThe dataset provides chart images for 15 different chart types\nalong with annotations for each image. This version of dataset\nhas been used in ICPR 2022 challenge. It contains real charts\nextracted from Open-Access publications found in the PubMed\nCentral repository. The dataset contains only real chart images,\nno synthetic data is included in this version. However, the\nnumber of samples for each chart type is not balanced as\nshown in Table II.\nMoreover, for text-role classification task, we have consid-\nered only four types of charts as the data annotation for text\ncomponents in these chart types is sufficient for experimental\nprotocol as compared to other classes. The number of samples\nfor each text-role class in these four chart types are outlined\nin Table I."}, {"title": "B. Model Training", "content": "All models are trained on a NVIDIA Tesla P100 16GB GPU\nhardware. 80% of the data for each task is employed for model\ntraining while 20% is used for testing purposes.\n\u2022 Firstly, we train the chart-type classification model. A\npre-trained Swin transformer (swin-large 224 [33]) is\nemployed for this purpose. There are 195M trainable\nparameters and the train-time is 12h. The model is fine-\ntuned on our dataset for 50 epochs using a batch-size\nof 8. Adam optimizer is used with a learning rate of\nlr = 0.000003. Sparse-categorical cross-entropy loss\nis computed during training. We also evaluated the\nperformance of a pre-trained ResNet150 model, but the\nresults achieved using Swin transformers outperform\nthose obtained by ResNet150.\n\u2022\nFor text detection task, we applied transfer learning\nusing a pre-trained YOLOv7 base. There are 36.9M\ntrainable parameters and the average train-time is\n16h. The model is trained for epochs: 70 (horizontal\nbar charts), 100 (vertical bar), 50 (line plot), and 50\n(scatter plot) before convergence. A batch-size of 4 is\nused along with Adam optimizer at a learning rate of\nlr = 0.000005. Additional hyperparameters include a\nmutation scale ranging between 0-1, momentum values of\n0.3, 0.6, and 0.98, and weight-decay of 1, 0.0, and 0.001.\n\u2022 Output of YOLOv7 is then enhanced using a variant\nof enhanced super resolution generative adversarial\nnetwork (ESRGAN). An upscaling of 1.5x is applied.\nWe experimented with different upscaling values\nand observed that 1.5x is the optimal in most cases.\nIncreasing this value distorted the shape of characters and\naffected the recognition module's performance. shows a detected text from chart image, upscaling it\nto 1.5x improves the resolution, and upscaling to 3.0x\ndeforms the shape of letter 'e'."}, {"title": "C. Evaluation Metrics", "content": "For each task in the pipeline, we employ the following\nevaluation metrics. For chart type classification, we use\nAccuracy, Precision, Recall, and F1-score (given in\nEquation 6,7,8,9). For text detection module, we use\nmean Average Precision (mAP). Here mAP refers to mAP50\nwhich is calculated by computing the average precision at\nonly the 50% IoU threshold. For text role classification, we\nused F1-Score measure.\n$Accuracy = \\frac{TP+TN}{TP+TN+FP+ FN}$ (6)\n$Precision = \\frac{TP}{TP + FP}$ (7)\n$Recall = \\frac{TP}{TP+FN}$ (8)\n$F1 = \\frac{2 * Precision * Recall}{Precision + Recall} =  \\frac{2* TP}{2*TP+FP+ FN}$ (9)\nWhere TP, TN, FP, and FN are the True Positives, True\nNegatives, False Positives, and False Negatives, respectively."}, {"title": "IV. RESULTS AND DISCUSSION", "content": "In this section, we discuss the outcomes of our proposed\nmethodologies for each task in the information extraction\npipeline.  illustrates an input chart image that is\nbeing processed through all steps of the framework pipeline."}, {"title": "A. Chart Type Classification Results", "content": "In chart type classification, our model achieves an F1-score\nof 0.97 and as it can be seen, the achieved F1-score\nby our model is higher than the previous techniques used for\nthis task such as ResNet, VGG, and Swin ensemble. Although\nthe test set used for evaluation of these SOTA techniques was\ndifferent from what we have used so direct comparison is not\nan option, but this reference acts as an indicator that our model\nperforms well for this task. The model is able to correctly\nclassify 15 chart types with an overall F1-score of 0.97 which\nvalidates that it is able to handle the style variations and\nother challenges mentioned earlier. The results also validate\nthat the used hierarchical vision transformer in this task is\nsuccessfully able to achieve deep position-level learning to\nobtain significant results."}, {"title": "B. Text Detection and Recognition Results", "content": "For text detection, our fine-tuned YOLOv7 model is\nable to successfully detect text on all four chart types\nunder consideration. The results are outlined in Table IV.\nThe results validate that YOLOv7 in this task performs well\nalong with other SOTA techniques utilized in recent literature.\nAs mentioned earlier, due to the complex layout and vari-\nations of font styles and sizes, conventional OCRs do not\nperform well on chart text recognition. For this purpose,\nwe evaluated the performance of state-of-the-art TPS-ResNet-\nBiLSTM-Attn model that has shown to outperform popular\nscene text recognition algorithms. As expected, the model is\nable to perform significantly better than conventional OCRS\non chart text as well. However, despite its success on most\ntext types, it did not perform well on very small-sized text as"}, {"title": "C. Text Role Classification Results", "content": "Text style variations and relations between multiple text\nobjects make text-role classification a challenging task for any\nclassifier. Furthermore, placement of a specific type of text\nis also highly dependent on the chart type. Thus making it\ndifficult to device a generic tool for different types of charts.\nIn this study, we are considering nine role classes across four\ndifferent chart types. As illustrated our trained\nhierarchical vision transformer using shifted windows is able\nto achieve considerable success in this task for all chart types.\nThe model achieves an overall F1-Score of 0.91, 0.90, 0.86,\nand 0.84 for all nine text-role classes in line plot, scatter plot,\nvertical bar, and horizontal bar charts, respectively.\nTo understand the complexity of problem and the signifi-\ncance of our methodology, we compare our results with two\nstate-of-the-art architectures YOLOv7 and DETR, where mAP\nis 0.40 and 0.30 from these techniques respectively. Thus, pre-\ntrained Swin transformer fine-tuned on our data outperforms\nboth YOLOv7 and DETR with significant margin.\nAlthough YOLOv7 performed well in text detection but\nits performance in text-role classification is not satisfactory.\nFigure 11 shows the results of YOLOv7 for all text-role\nclasses. Except for legend-label, axis-title, tick-label, and\nvalue-label, the model performs below satisfactory for the rest\nof the classes. On the other hand, Swin transformer improves\noverall accuracy of the text-role classification significantly."}, {"title": "V. CONCLUSION", "content": "This work marks a significant contribution to the challeng-\ning task of chart information extraction by proposing a deep\nlearning framework for each vital step in the process. The"}]}