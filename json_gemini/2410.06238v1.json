{"title": "EVOLVE: Evaluating and Optimizing LLMs For Exploration", "authors": ["Allen Nie", "Yi Su", "Bo Chang", "Jonathan N. Lee", "Ed H. Chi", "Quoc V. Le", "Minmin Chen"], "abstract": "Despite their success in many domains, large language models (LLMs) remain under-studied in scenarios requiring optimal decision-making under uncertainty. This is crucial as many real-world applications, ranging from personalized recommendations to healthcare interventions, demand that LLMs not only predict but also actively learn to make optimal decisions through exploration. In this work, we measure LLMs' (in) ability to make optimal decisions in bandits, a state-less reinforcement learning setting relevant to many applications. We develop a comprehensive suite of environments, including both context-free and contextual bandits with varying task difficulties, to benchmark LLMs' performance. Motivated by the existence of optimal exploration algorithms, we propose efficient ways to integrate this algorithmic knowledge into LLMs: by providing explicit algorithm-guided support during inference; and through algorithm distillation via in-context demonstrations and fine-tuning, using synthetic data generated from these algorithms. Impressively, these techniques allow us to achieve superior exploration performance with smaller models, surpassing larger models on various tasks. We conducted an extensive ablation study to shed light on various factors, such as task difficulty and data representation, that influence the efficiency of LLM exploration. Additionally, we conduct a rigorous analysis of the LLM's exploration efficiency using the concept of regret, linking its ability to explore to the model size and underlying algorithm.", "sections": [{"title": "1. Introduction", "content": "The rapid advance of LLMs has positioned them as valuable tools for a wide range of decision-making tasks, including but not limited to personal assistants (Liu et al., 2024a), recommendation systems (Li et al., 2023a), game-playing (Wang et al., 2023a,c), education (He-Yueya et al., 2024; Nie et al., 2024), and healthcare (Singhal et al., 2023). In these tasks, LLMs function as agents that engage with users or the environment in a dynamic interaction process. For example, at each time step, the LLM suggests a pedagogical strategy or make a recommendation to a specific user, then receives feedback - either explicit or implicit - in the form of rewards. Based on this feedback, the agent updates its beliefs about the environment, e.g., underlying reward distributions, and adapts its strategies to maximize the cumulative reward. These tasks differ fundamentally from classic prediction tasks where LLM is used to predict a target. A decision making LLM only receives partial feedback, i.e., the reward for its own actions, but not for others. Thus, it requires the LLM to effectively interact with the environment and explore to discover the optimal action. Meanwhile, exploring an unknown action that turns out to have lower reward than the known ones incurs an opportunity cost. The agent, therefore, needs to strike a balance between exploration and exploitation. While the exploration-exploitation tradeoff has been extensively studied in the pre-LLM era, particularly in the fields of bandits (Li et al., 2010; Slivkins et al., 2019) and reinforcement learning (Mnih, 2013; Osband et al., 2013; Sutton, 2018), it remains unclear how LLMs approach this tradeoff when faced with uncertainty.\nWe study LLMs' in-context exploration capabilities under the simplified framework of bandits a stateless form of reinforcement learning that is highly applicable to many domains. We set up the LLM to interact with the environment over T rounds. In each round, it receives the full history of its past interactions, the current state (if provided), and a set of actions, and it is tasked with selecting an"}, {"title": "2. In-Context Exploration", "content": "In this section, we define the problem of In-Context Exploration (ICE), following the setup in Krishnamurthy et al. (2024). An agent interacts with an environment by observing state information, selecting actions, and collecting feedback. The goal of the agent is to maximize its cumulative reward through multiple rounds of interactions. Specifically for ICE, the agent is an LLM that keeps a history of observations and interactions with the environment in its context. The agent determines its actions based on this context, rather than by updating its weights or executing hand-designed exploration strategies.\nNotation and Definitions. We primarily consider bandits, a simple class of environments that still incorporates many fundamental challenges in decision-making. Here, we describe a framework that encompasses both multi-armed bandits (MAB) and contextual bandits (CB). A bandit environment $\\mathcal{T}$ is defined as $\\mathcal{T} = (\\mathcal{X}, \\mathcal{A}, \\mathcal{R})$, where $\\mathcal{A}$ defines a set of valid actions. $\\mathcal{X}$ is the set of state information (if any), and $\\mathcal{R}$ represents the underlying reward distributions of actions, which are unknown to the agent. MAB and CB tasks differ in whether the context x is provided and used: in MAB, the reward depends solely on the action, whereas in CB it depends on both the action and the context. The interaction between the agent and the environment occurs over $T \\in \\mathbb{N}$ steps. At each time step $t \\in [T]$, the environment reveals a new observation x\u2081 \u2208 X, the agent selects an action a\u0165 \u2208 A following its policy \u03c0, and then a reward $r \\sim \\mathcal{R}_{a_t}(x_t)$ is revealed. Given an LLM agent with policy \u03c0, it determines its action $a_t \\sim \\pi(H_t)$, where $H_t = (x_1, a_1, r_1, ..., x_t)$ stores the historical actions taken by the agent and the corresponding environment feedback, which is sent as input context to the LLM.\nOver T rounds, we measure the performance of an agent \u03c0 on task $\\mathcal{T}$ as its expected cumulative reward, given by $J_\\mathcal{T}(\\pi) = \\mathbb{E}_{\\mathcal{T},\\pi} [\\sum_{t=1}^T r_t]$. The optimal policy \u03c0* represents the agent that selects the action with the highest average reward, defined as $\\pi^*(x) = \\arg \\max_a \\mathbb{E}[r_a | x]$. A commonly used metric to measure the performance of an agent or algorithm is regret.\nDefinition 1 (Cumulative Regret). The expected regret of a policy \u03c0 under task $\\mathcal{T}$ is: $REG(\\pi) = \\mathbb{E}_{\\mathcal{T},\\pi} [\\sum_{t=1}^T (r^*(x_t) - r_t)] = J_\\mathcal{T}(\\pi^*) - J_\\mathcal{T}(\\pi)$, where $a^*_t = \\pi^*(x_t)$.\nWe expect good agents to have average regret that converges to zero (i.e. $REG \\rightarrow 0$), demonstrating that they eventually learn to perform as good as the optimal policy. UCB and Thompson Sampling are two such examples with sublinear regret. Examples of cumulative regret curves are shown in Figure A2c.\nRepresenting Histories In-Context. Developing an LLM agent suited for in-context decision-making tasks also requires designing a robust textualization function \u03c6 that translates histories $H_t$ for the LLM to consume. The obvious baseline for 4 is to simply record the Raw History (RH) from the environments as a list of (context, action, reward) tuples directly as the context. In this representation, the context length of $\\phi(H_t)$ grows linearly with t, and RH contains all information. While RH is a"}, {"title": "3. BanditBench", "content": "We present BanditBench, an extensive suite of MAB (Slivkins et al., 2019) and CB (Li et al., 2010) environments in natural language to benchmark the in-context exploration capabilities of LLMs. We show two examples in Figure 2.\nMulti-Armed Bandit In (stochastic) multi-armed bandit problems, we vary our environment configurations primarily along two key dimensions: 1) action space, where we change the number of actions K and the textual descriptions associated with each action; 2) reward distributions, where we change the parametric distribution of the reward, i.e., the types of reward distributions, and the exploration difficulty, characterized by the gap between the best-performing arm and the second-best arm. A smaller gap makes it harder for the agent to distinguish between optimal and sub-optimal actions, thereby increasing the exploration difficulty. In contrast to the setup in Krishnamurthy et al. (2024), which focuses solely on MAB instances with Bernoulli reward distribution, our expanded setup allows us to systematically analyze LLMs' performance across diverse environments with different action spaces and reward structures.\nThe detailed configurations are shown in Appendix A.1. For the action space, we explore two different sizes: K = 5 for a small action space and K = 20 for a large action space. We also differentiate between two types of action descriptions: Videos, represented as arbitrary two-letter combinations with no semantic meaning such as \u201cVideo AA\u201d, and Clothes, described using semantically meaningful phrases, such as \u201cSupreme Sylvan Sandals\u201d. Regarding reward distributions, we evaluate two types: Bernoulli and Gaussian Bandit. For Bernoulli, the reward r \u2208 {0, 1} is binary with $r_{ak} \\sim Bernoulli(p_k)$, where pk is the mean for the k-th action. Following Krishnamurthy et al. (2024), the best-performing arm has $p_k := 0.5 + \\Delta_{min}/2$, while the remaining arms have $p_k = 0.5 - \\Delta_{min}/2$. The parameter $\\Delta_{min}$ captures the exploration difficulty, with a larger gap ($\\Delta_{min} = 0.5$) indicating easy tasks and smaller gap ($\\Delta_{min} = 0.2$) representing hard tasks. For the Gaussian bandit, the rewards are continuous with $r_{ak} \\sim \\mathcal{N}(\\mu_k, \\sigma)$. Here $\\mu_k \\sim \\mathcal{N}(0, \\sigma)$ represents the mean for each action, and the variance o captures the difficulty of exploration. Following Sutton (2018), we study both o = 1 and \u03c3 = 3."}, {"title": "4. Learning Optimal Exploration Behaviors", "content": "Motivated by the existence of optimal algorithms for bandits, we aim to leverage these algorithms to improve LLMs for exploration by: 1) incorporating algorithmic guidance during inference (Section 4.1), 2) teaching optimal exploration through algorithm distillation (Section 4.2). We show that smaller models trained using algorithm distillation can even outperform larger models, offering a promising way to efficiently explore with lower inference costs."}, {"title": "4.1. Inference-time Algorithm-Guided Support", "content": "In this section, we explore how to leverage UCB-type algorithms as inference-time support to improve LLM's in-context exploration performance.\nAlgorithm-Guided Support (AG) As discussed above, UCB-type algorithms operate by explicitly calculating the exploitation value $V^{Exploit}$ along with the exploration bonus $V^{Explore}$ for each arm, and by selecting the arm that maximizes the sum of the two. These components, $V^{Exploit}$ and $V^{Explore}$, therefore provide the sufficient textualization needed for LLMs to make optimal decisions. Specifically, in the MAB setup, during inference time at time step t, we provide the LLM with a list of tuples ($V^{exploit}(a, t)$, $V^{explore}(a, t)$) for each arm a \u2208 [K]. This representation is provided alongside other essential information, such as scenario descriptions, instructions, and the action set. For CB, during inference-time, we explicitly maintain the design matrix $D_a$ and response vector $r^a$ for each arm, incorporating past interactions from the LLM up to that time t, using this to obtain the exploitation value and exploration bonus. We then provide the LLM with a list of exploitation values and exploration bonus for each arm a at current context x, similar to the MAB setup. Additionally, we record the action features x as well as the reward r\u0165 selected by the LLM, which will be used for the next round of parameter updates. Compared with SH, which only provides the empirical mean and the number of times each arm has been pulled, AG directly supplies semantically understandable exploitation values and exploration bonuses. This explicit representation enables the LLM to effectively balance exploitation and exploration. Theoretically, the LLM only needs to perform addition and argmax, rather than manipulating raw histories to discern the underlying reward distribution (or parameter \u03b8"}, {"title": "4.2. Algorithm Distillation via Demonstration and Fine-tuning", "content": "We further investigate the possibility of enhancing LLM exploration by leveraging a set of trajectories generated by an oracle exploration algorithm in the BanditBench environment. This approach, called algorithm distillation, aims to distill the optimal exploration behavior from the oracle algorithm to the LLM. In particular, we consider two approaches: in-context few-shot demonstration and oracle behavior fine-tuning, both utilizing expert trajectories generated by the oracle algorithm. Compared with Algorithm-Guided Support (AG), these approaches do not require an understanding of the oracle algorithms, nor do they require generating sufficient statistics based on oracle algorithms; thus, they can also be applied to black-box algorithms.\nOracle Trajectory Generation We use UCB as the oracle algorithm to generate the trajectories. Following the notations defined in Section 2, the trajectories are in the form of tuples of $(\\phi(H_t^{UCB}), a^{UCB}_t)$, where each tuple pairs the transformed representation of the history at time t and the action $a^{UCB}_t$ from UCB. For MAB, we create trajectories from reward distributions that differ from those used in evaluation. This assesses the LLM's ability to generalize across different bandit instances with the same underlying scenario but varying action descriptions and action-reward mappings. We further control the data generation process by varying: (1). Action Description: trajectories are generated from either \"Video\" or \"Clothes\" action descriptions; (2). Difficulty: we control the reward gap in the Bernoulli bandit to create \"easy\" or \"hard\" instances; (3). Trajectory Textualization: trajectories are represented either as RH or AG. For CB, we use a fixed dataset and evaluate the LLM's performance on a held-out set of users. While these users are unseen during training, their profiles and preferences remain within the distribution of the training data. This evaluates the LLM's ability to leverage prior knowledge for effective exploration. In CB, we only vary the trajectory representation (RH or AG). In both MAB and CB, each trajectory consists of a sequence of exploration steps: 300 steps for MAB with K = 5 arms, 1000 steps for MAB with K = 20 arms, and 200 steps for CB. We generate 50 trajectories for 2 MAB domain configurations (the easiest and the hardest configuration) with 2 trajectory textualizations, and 200 trajectories for CB with 2 trajectory textualization . This results in 4 algorithm distillation datasets for MAB and 2 datasets for CB.\nIn-Context Few-Shot Demonstration We first study whether demonstrating oracle exploration trajectories from UCB as few-shot examples can improve the LLM's ability to perform robust exploration in bandit tasks. A key challenge in applying few-shot learning to decision-making tasks like MAB is the increasing context length. Unlike supervised learning, where context is typically fixed, bandit actions depend on the entire past history or condensed history, which either grows linearly with T (steps) or K (actions). This poses a challenge for LLMs, as their ability to effectively utilize information can degrade with longer contexts. We sample 5 oracle trajectories from UCB into the LLM context window as demonstrations. Our goal is to see whether the optimal exploration demonstrations can lead to improved exploration performance. Detailed demonstrations are provided in Appendix A.9.\nOracle Behavior Fine-Tuning (OFT) While in-context few-shot demonstrations offers an inference-time approach to guide the LLM's exploration strategy, fine-tuning allows us to directly optimize the model's parameters for the task. In this approach, we utilize the UCB-generated trajectories as training"}, {"title": "4.3. Empirical Evaluations", "content": "In this section, we empirically evaluate LLMs' in-context exploration capabilities, using BanditBench. We begin with introducing the setup, baselines and metrics in Section 4.3.1. Following this, in Section 4.3.2, we analyze the performance of inference-time guided support, in-context few-shot demonstration and oracle behavior fine-tuning across various experimental settings, as well as models of different sizes. Additionally, we perform extensive ablation studies on the impact of task difficulty, textual representation of the oracle trajectories, and inference-training representation alignment."}, {"title": "4.3.1. Setup and Baselines", "content": "Setup We evaluate the in-context exploration capabilities of various LLMs, including Gemma-2B, Gemma-9B (Team et al., 2024), Gemini 1.5 Flash, and Gemini 1.5 Pro (Reid et al., 2024), on 16 MAB tasks (Table A1) and 2 CB tasks. For MAB tasks, the interaction horizon (T) differs based on the size of the action space (K): we use T = 1000 for K = 30 and T = 200 for K = 10. All CB tasks use a constant horizon of T = 200 steps. To ensure statistical significance of the results, we conduct 30 independent runs for each experimental setup.\nBaselines We consider two baselines: Raw History (RH) and Summarized History (SH), as suggested in Krishnamurthy et al. (2024). For CB, as we discussed before, there is no trivial analogue of SH; thus, we focus solely on RH for CB tasks in this study as the baseline.\nMetrics We report the relative performance of each model, aggregated across all environment configurations. Simply averaging cumulative rewards across environments of different reward distributions and horizons however, obscures the comparison. We instead use the pairwise win-rate to compare the performances. We have 16 configurations for MAB and evaluate 32 models (4 LLMs crossed with different methods), and 2 configurations for CB with 14 models (2 LLMs crossed with different methods). The list of all the models is provided in Appendix A.7. For each configuration, we compute the cumulative reward over T steps and collect a distribution of cumulative rewards from 30 independent trials. We then calculate the pairwise win-rate by applying a Student's t-test on the reward distributions of any pair of configurations to determine if they are statistically significantly different, with a significance level of p < 0.05. If one model has a significantly higher reward than the other, we consider it a win. If the difference is not statistically significant, the result is deemed inconclusive and not counted as a win. For each model, we calculate its win rate against every other model across all configurations. The overall win-rate for a specific model is then the percentage of superior performance over all models, crossed with methods and configurations."}, {"title": "4.3.2. Results and Ablation studies", "content": "Overall Performance Comparison Figure 1 presents a comparative overview of in-context few-shot demonstration, oracle behavior fine-tuning, and inference-time algorithmic guidance performance"}, {"title": "5. Functional Interpretation of LLM Exploration Behavior", "content": "In this section, we aim to conduct a more rigorous analysis of the LLM's exploration efficiency using the concept of regret, REG(\u03c0). Most bandit algorithms are evaluated by the behavior of REG(\u03c0) as a function of T (i.e., the number of interactions), either theoretically or empirically. Motivated by this, our goal is to understand the exploration behaviors of various LLMs by characterizing their regret as a function of T. To achieve this, we adopt the following functional form to analyze the regret:\n$f(T) = \\frac{\\lambda_1 log(T)^{\\alpha}}{\\Delta_{min}} + \\beta T + \\lambda_2$\nThe three parameters \u03b1, \u03b2, \u03bb\u2081 in the equation are all positive real numbers. 22 is unconstrained. Amin captures the gap between the best and second best arm. This functional form provides intuitive interpretations for the underlying parameters. Specifically, log(T) represents sublinear scaling of the regret, which is known to be achieved by only the best bandit algorithms (e.g. UCB and Thompson Sampling). The T scaling describes a linear growth or the inability of an agent to match the optimal policy \u03c0*. This means a strong algorithm should have a as small as possible, and have \u03b2 = 0. This functional form also allows us to see some growth behaviors in-between with both positive a and \u03b2. We use the curve fit function in Scikit-learn (Pedregosa et al., 2011) to fit the cumulative regret curve of UCB and LLMs coupled with different methods (i.e., inference-time algorithm-guided support, in-context demonstration, and oracle behavior fine-tuning). The results of the fitted a and \u1e9e values are presented in Figure 5. For the largest Pro models, applying effective inference-time support, such as AG and SH can achieve nearly sub-linear regret. More intriguingly, for Flash models, fine-tuning for oracle behavior significantly boosts performance, enabling them to attain sub-linear regret with a lower a. In contrast, weaker models such as Gemma 2B and 9B appear to remain in the linear regret regime across nearly all methods."}, {"title": "6. Related Work", "content": "Several prior works have investigated the use of LLMs for decision-making. In one category, numerous studies have deployed LLMs directly as agents in decision-making problems such as games (Brooks et al., 2024; Shinn et al., 2024; Wang et al., 2023a; Xi et al., 2023; Yao et al., 2023). However, fewer works have systematically evaluated LLMs' capabilities in general decision-making setup, especially in relation to classical concepts in decision-making like exploration. Our work extends the research of Krishnamurthy et al. (2024), who examined LLMs' exploration capabilities in small-scale MAB tasks. Their findings, which showed positive results only with substantial intervention, are consistent with our broader analysis across both MAB and CB tasks at various scales. Additionally, Mirchandani et al. (2023) also evaluated the ability of LLMs to learn in-context from demonstrations (and demonstrations of improvement) to solve decision-making problems.\nThe line of research on using LLMs as optimizers faces many similar challenges to in-context decision making, although applied to different tasks. Yang et al. (2024) explored the use of language models as general-purpose optimizers for simple black-box optimization problems, such as prompt optimization, highlighting that a careful balance of exploration and exploitation is critical. Another relevant line of research focuses on in-context learning for decision-making and reinforcement learning (RL) with domain-specific transformers. Laskin et al. (2022) distilled demonstrations from RL algorithms into a transformer and showed that the transformer learns to imitate the RL process to solve new RL tasks. Similarly, Lee et al. (2024) trained transformers with optimal action labels, showing that the model learns to execute posterior sampling for RL (Osband et al., 2013) in-context, which tailors exploration"}, {"title": "7. Conclusion", "content": "In this work, we explored the in-context exploration capabilities of LLMs in bandit environments, introducing BanditBench, a comprehensive benchmark designed to rigorously evaluate LLM's performance. Our evaluation reveals that LLMs struggle with in-context exploration when relying solely on raw interaction history, while inference-time support significantly improves performance. Motivated by the presence of optimal algorithms in this domain, we investigated methods to integrate these algorithms into LLMs through both algorithm-guided support and algorithm distillation via synthesized demonstration data. Notably, these approaches enable smaller models to outperform larger ones in decision-making tasks. However, an optimality gap remains between LLMs and classical optimal algorithms, highlighting the need for further research to bridge this gap."}]}