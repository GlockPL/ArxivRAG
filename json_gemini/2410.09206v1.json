{"title": "pyhgf: A neural network library for predictive coding", "authors": ["Nicolas Legrand", "Lilian Weber", "Peter Thestrup Waade", "Anna Hedvig M\u00f8ller Daugaard", "Mojtaba Khodadadi", "Nace Miku\u0161", "Chris Mathys"], "abstract": "Bayesian models of cognition have gained considerable traction in computational neuroscience and psychiatry. Their scopes are now expected to expand rapidly to artificial intelligence, providing general inference frameworks to support embodied, adaptable, and energy-efficient autonomous agents. A central theory in this domain is predictive coding, which posits that learning and behaviour are driven by hierarchical probabilistic inferences about the causes of sensory inputs. Biological realism constrains these networks to rely on simple local computations in the form of precision-weighted predictions and prediction errors. This can make this framework highly efficient, but its implementation comes with unique challenges on the software development side. Embedding such models in standard neural network libraries often becomes limiting, as these libraries' compilation and differentiation backends can force a conceptual separation between optimization algorithms and the systems being optimized. This critically departs from other biological principles such as self-monitoring, self-organisation, cellular growth and functional plasticity. In this paper, we introduce pyhgf: a Python package backed by JAX and Rust for creating, manipulating and sampling dynamic networks for predictive coding. We improve over other frameworks by enclosing the network components as transparent, modular and malleable variables in the message-passing steps. The resulting graphs can implement arbitrary computational complexities as beliefs propagation. But the transparency of core variables can also translate into inference processes that leverage self-organisation principles, and express structure learning, meta-learning or causal discovery as the consequence of network structural adaptation to surprising inputs. The main functions of the library are differentiable and seamlessly integrate into sampling or optimization workflows. Additionally, we offer generalized Bayesian filtering and the Hierarchical Gaussian Filter as key examples of dynamic networks implemented in our library. The source code, tutorials and documentation are hosted under the main repository at https://github.com/ilabcode/pyhgf.", "sections": [{"title": "1 Introduction", "content": "Bayesian models of cognition describe perception and behaviours as probabilistic inferences over the cause of sensory inputs (Ji & Kording, 2023). Modelling these processes at scale to infer computational parameters from human behaviours (Friston, 2022; Huys et al., 2016; Sandhu et al., 2023), or to implement them into artificial agents (Da Costa et al., 2022; Mathys & Weber, 2020), is currently a challenge that brings together computational neuroscience and artificial intelligence. However, when considering living organisms the complexity of models increases and inference becomes especially challenging. While certain inferential processes can sometimes be straightforwardly described and implemented using closed-form solutions, the intractability increases rapidly with models that incorporate multiple information streams, continuous inputs, or hierarchical dependencies common in biological systems. Predictive coding (Friston, 2005; Rao & Ballard, 1999) has posited that such complex generative probabilistic models are biologically implemented as hierarchical networks of nodes (i.e. neurons or populations of neurons) that perform simple computations such as message-passing, error signalling and belief propagation in interaction with other proximal units in the hierarchy (Friston, 2008; Mikulasch et al., 2023). This mechanism could represent a simpler, faster and energy-efficient alternative to other optimization methods such as backpropagation (Rumelhart et al., 1986), which is commonly used by artificial neural networks (Millidge, Salvatori, et al., 2022; Ororbia & Kifer, 2022), or MCMC sampling (Betancourt, 2017) in the case of Bayesian inference.\nOne limiting factor for the wider application of predictive coding neural networks to more complex probabilistic models is the absence of easily accessible open-source toolboxes compatible with modern probabilistic programming and neural network libraries. It is therefore critical for the field to develop a framework that would facilitate the implementation of predictive coding models in a manner analogous to how TensorFlow (Mart\u00edn Abadi et al., 2015) and PyTorch (Paszke et al., 2019) have supported the development of conventional neural networks over the past decade. However, this requirement comes with technical challenges that are far from trivial on the software development side. This becomes especially apparent when we consider the deep divergence between the models that standard neural network libraries are tailored to develop and the dynamic and flexible nature of the models that computational neuroscience and computational psychiatry seek to develop. Biological organisms can implement learning and flexible behaviours not only by adjusting the inner representation of some quantities but also by leveraging self-monitoring, self-organisation, cellular growth and functional plasticity. Even abstracting from the specific optimisation algorithm or inference methods that we aim to apply, few of these features could be implemented in classical deep learning libraries. First, those libraries need to compile to low-level programming languages while maintaining possibilities for automatic differentiation. This often comes at the cost of a restriction of dynamic manipulation of inner variables at execution time. For example, conventional neural networks rely heavily on linear algebra functionalities that will require static matrix shapes, which has been a limiting factor for graph neural networks for example (see however how (Fey & Lenssen, 2019) and (Godwin* et al., 2020) circumvent parts of this problem). Secondly, these frameworks tend to disentangle the optimisation process from the optimized system. While the network is defined through a set of variables only partially transparent, tuning the network relies on the execution of scripts whose steps are hidden from the network, preventing it from reasoning about inference itself. It is therefore crucial for predictive coding, and to adhere to biological realism, that"}, {"title": "2 Design and Implementation", "content": "In this paper, we introduce pyhgf, a Python library for the creation, manipulation and inference over dynamic neural networks for predictive coding, with a focus on the generalized Hierarchical Gaussian Filter (Mathys, 2011; Mathys et al., 2014; Weber et al., 2023). Models and theories that imply such networks are becoming ubiquitous in computational neuroscience, and researchers interested in fitting behavioural data to these models require the flexibility of a regular neural network library, together with the modularity of a probabilistic framework to perform inference over parameters of interest. In this package, we provide the user with an API that provides methods for smoothly interacting with these two levels of modelling:\n1. A set of core methods to define, manipulate and update dynamic neural networks for predictive coding. These networks need to provide unique flexibilities in their design, and the user has control over this using a limited set of parameters, accessible both to the user in the design process and to the agent in real-time adaptive behaviours.\n2. Higher-level classes for embedding any of these networks as custom likelihood functions in multi-level Bayesian models, or as loss functions in other optimisation libraries. Those classes include fully defined probabilistic distributions that integrate with PyMC (Oriol et al., 2023) and tools to help diagnose inference, visualization, and model comparison (Kumar et al., 2019).\nBy using these interfaces, it is expected that the user will be able to customize the computa-"}, {"title": "2.1 Computational framework", "content": "The design and software implementation of dynamic neural networks for predictive coding has been shaped by a set of requirements. These networks are made of nodes that can store any number of variables, some variables might be found in other nodes as well, and some might be unique. Nodes are connected with each other through directed edges, and there can exist multiple types of connections in the same graph, denoting different interactions between the nodes. Computation steps in the graph typically occur locally between adjacent nodes for prediction and prediction errors. Multiple types of computation can be defined. The computation steps can be triggered either reactively, by observing events in the surroundings of a node and reacting to it, or scheduled, by pre-allocating a sequence of steps that will propagate the information through the graph. Finally, all these components should be transparent to the network itself when performing a given computation, which allows it to meet self-monitoring and self-organisation principles.\nBy observing the set of constraints above, it appears that computations should follow a strict functional programming framework, hence being in-place programmatically pure functions operating on the components of the network. Functional programming is natively supported in Rust and also enforced by JAX (Bradbury et al., 2018) to leverage just-in-time (JIT) compilation and automatic differentiation, therefore departing from object-oriented programming (the definition of classes populated with attributes and methods) that is a central feature of Python. This comes with limitations in the way toolboxes' API can be developed (see for example how this can be handled in (Kidger & Garcia, 2021)). To fully meet the dynamic aspects mentioned previously, the update functions should receive and return possibly all of the following components defining a network:\n1. A list of attributes - the attributes are dictionaries of parameters of a given node\n2. A set of lists of edges - the edges are the directed connections between the nodes. All the possible edge types are grouped into a set.\n3. A set of update functions. Each function defines a computation and can be parametrised by the index of the triggering node. Possible computations are for example prediction, posterior update or prediction error between two nodes.\n4. Unless using a reactive computation scheme, the function should also have access to an ordered sequence of update functions that apply to individual nodes.\nBy defining these four components, and by creating functions that can receive and return all of them, the user can generate arbitrarily sized and structured dynamic neural networks for predictive coding (see 1). The first two items define what is usually called a graph, with the addiction that it can be directed and multilayered. The last two items shape what is central to predictive coding: the schedule or reactive nature of the propagation of information through the network. Because all of these components are transparent during message-passing computations,"}, {"title": "2.2 Optimization and inference", "content": "While predictive coding itself originates from fields linked to signal processing and information theory (Oliver, 1952), the use of predictive coding as a framework for hierarchical inference (Friston, 2005; Rao & Ballard, 1999) in biological neural networks makes it especially applicable to fields related to experimental neuroscience and computational psychiatry. In this context, the neural networks are components of a cognitive model of the subject on which the experimenter performs inference (i.e. observing the observer (Daunizeau et al., 2010)). For example, in the context of the generalised Hierarchical Gaussian Filter, the user might be interested in inferring the posterior distribution of tonic volatility at different levels of the hierarchy from observed behaviours.\nThat kind of reverse inference requires the use of advanced techniques like MCMC sampling or gradient descent which involves the evaluation of several instances of a network, as well as the gradient at evaluation, to find parameters maximizing likelihood. In the Matlab HGF toolbox (Mathys, 2011; Mathys et al., 2014), the inference step is implemented using variational Bayesian inference, which can be difficult to apply in the context of multilevel models, highlighting the need for both performances and the benefit of automatic differentiation. Here, the codebase is entirely written in Python and, as in version 0.2.0, can use JAX (Bradbury et al., 2018) as a computational backend that can easily deploy code on CPU, GPU and TPU. JAX offers a rapidly growing ecosystem for machine learning (Babuschkin et al., 2020) and artificial intelligence that already includes toolboxes that are conceptually related to predictive coding and Hierarchical Gaussian Filters, such as state-space modelling (https://github.com/probml/dynamax), reinforcement learning (Hoffman et al., 2020), neural networks (Heek et al., 2023; Kidger & Garcia, 2021) or graph neural networks (Godwin* et al., 2020). We leverage automatic differentiation and just-in-time compilation offered by JAX (Bradbury et al., 2018) to let the networks interface smoothly with other optimization and inference libraries like PyMC (Oriol et al., 2023) that support a large range of sampling or variational methods, including Hamiltonian Monte-Carto methods such as the No-U-Turn Sampler (NUTS) (Homan & Gelman, 2014), an approach that has proved to be highly efficient when scaling to high-dimensional problems. While dimensionality was not a major concern for individual model fittings, this can arise if we want to model group-level parameters, and therefore estimate a large number of networks together"}, {"title": "3 Results", "content": "Users interested in using the pyhgf package are referred to the main documentation under the following link: https://ilabcode.github.io/pyhgf/index.html. The documentation will host an up-to-date theoretical introduction, API descriptions, and an extensive list of tutorials and examples of various use cases. In this section, we will be concerned with the standard analytic workflow of the package: we will explain how to create and manipulate dynamic networks, how to fit the network to a sequence of observations, and how to perform inference and optimization over parameters (see 2). This concrete use case will demonstrate how dynamic networks are especially well suited to create modular efficient structures for signal processing and real-time decision-making. These generic systems have direct applications in artificial intelligence and computational neuroscience, focusing on psychiatry, for the robust estimation of learning parameters through multilevel models, model comparison and parameter recovery. These procedures are illustrated in 3 using a large simulated dataset.\nFor the results reported in 2 and in 3, we used binary observations and binary decision responses (hereafter denoted u and y, respectively) from a behavioural experiment using an associative learning task (Iglesias et al., 2021). Such a dataset is especially well suited for a binary Hierarchical Gaussian Filter, and here we used the three-level version of this model, as it binds together most of the framework's building blocks. The models reported below were created and visualized using pyhgf v0.2.0. Bayesian inference was performed using PyMC v5.16.2 (Oriol et al., 2023). The posterior densities and traces were plotted using Arviz v0.19.0 (Kumar et al., 2019). The Jupyter notebook used to produce models and figures can be retrieved at https://github.com/ilabcode/pyhgf/blob/paper/docs/paper.ipynb."}, {"title": "3.1 Generative model, forward fitting and parameter inference", "content": "The standard workflow usually starts with the creation of a Bayesian network similar to what is depicted in 2 (panel a.). This network is intended the represent the generative model of the environment as inferred by the agent. When using pyhgf's built-in plotting function, the nodes that can receive observations are displayed in the lower part of the graph (roots). Here, the inputs are inserted in a binary state node (x1) whose value indicates the probability of observing the category 1. This node's value is itself predicted by a continuous state node (x2), which encodes this probability as a continuous unbounded variable after transformation using the logit function. Because the variability of this variable is assumed to change over time, the fluctuation is controlled by the third node (x3) through volatility coupling, represented here using dashed lines. If the volatility is assumed to be fixed, the third node can be removed (i.e. hence a two-level binary HGF). This graphical depiction can be generated using the internal pyhgf plotting library, which is convenient to visualise and debug complex networks."}, {"title": "3.2 Bayesian multilevel modelling, parameter recovery and model comparison", "content": "Cognitive neuroscience experiments involve multiple participants, and the statistical procedure requires inferring multiple parameters at once. To illustrate this practice we simulated in the second part the processing of a large simulated dataset (50 participants). We used here the same vector of observations u, but this time generated 50 response vectors yi by sampling from the response function with varying values for the two parameters of interest (w and t, see 3 panel a.). Using the same fitting procedure as 2, we applied this to multiple pairs of observations u and actions yi. This step is fully parallelized, it will therefore benefit from hardware acceleration provided by multiple CPU or GPU, and because we assume no dependencies across participants, we would obtain the same results fitting all participants iteratively, or a group of participants in a single-level Bayesian model (see 3 panel b.). We then tried to illustrate how the real values from parameters w and t could be recovered from the observations and actions alone, a practice known as parameter recovery. The recovered parameters, together with the real parameters used for the simulation of decisions are displayed in panel c.. Overall, the alignment of the recovered parameters in the identity line suggests a good recovery from the input data. This can"}, {"title": "4 Availability and future directions", "content": "Bayesian models of cognition have been around for decades, and frameworks like predictive coding are popular for their efficiency in modelling information processing in the central nervous system. The simplicity and modularity of the computational steps that support learning and optimization of these networks are opening new research avenues for designing structures without requiring gradient descent (Millidge, Salvatori, et al., 2022), and can easily extend to various forms of processes, such as causal inference (Salvatori et al., 2023) or temporal predictions patterns (Millidge et al., 2024).\nIn this paper, we introduced pyhgf, a Python package that provides a generic framework to design, manipulate and sample dynamic networks for predictive coding. Unlike conventional neural architectures, dynamic networks are tailored to react and reorganise when absorbing sensory inputs without relying on an external optimization algorithm. This framework is intentionally abstract and agnostic towards the mathematical formalism that implements inference and optimisation. We provide a complete implementation of generalised Bayesian filtering (Mathys & Weber, 2020) and the generalised Hierarchical Gaussian Filter (Homan & Gelman, 2014; Mathys, 2011; Weber et al., 2023) as two important tools for predictive coding.\nThe package lets the user create and manipulate arbitrarily sized networks defined as rooted trees on which scheduling or reactivity of simple local updates is applied to perform belief propagation. Critically, every step in the propagation is an in-place function receiving and returning the network itself. This supports complete plasticity and dynamics during the updates. We believe that this design allows not only the smooth manipulation of such networks but also a better dissociation between different methodological developments of predictive coding (e.g. the mathematical formalism versus the experimental application) so the user does not need complete expertise in all these domains to use the tools.\nWe have illustrated in 3 a possible workflow when using the library, taking as an example a popular model from computational psychiatry: the three-level hierarchical Gaussian filter. In this section, we are more concerned with the new possibilities offered by the framework that are not available in other libraries. These new directions can be summarised as twofold: 1. whether the computational graphs have fixed shapes, or 2. whether the graphs are dynamic and the structure is flexible 4. We provide below examples of such implementations."}, {"title": "4.1 Generalised Bayesian filtering in static networks", "content": "Bayesian networks are foundational models for the development of predictive coding. Importantly, the dynamic networks that we introduce here are also Bayesian networks, at least during the prediction steps. Predictive coding is more concerned with the Bayesian updating of those graphs in real time and uses prediction errors as key quantities to manipulate these updates. While static, this graphical representation is nonetheless highly relevant to modelling the nervous system (Friston et al., 2017). One interesting aspect of this framework is that arbitrary-sized neural networks can represent arbitrary complex generative models without having to rethink an entire algorithm: simple and principled local updates are generalized across the network to approximate variational inference. While nodes are implicitly tracking one parameter value through unidimensional normal distributions, arbitrarily complex networks can combine any exponential family distribution by using multiple nodes to track sufficient statistics (Mathys &"}, {"title": "4.2 Structure learning, causal inference and meta-learning in dynamic net- networks", "content": "Besides the large flexibility provided by the connectivity of the networks, the malleability of structural variables during belief propagation is also opening new avenues. Predictive coding networks use precision-weighted prediction errors to drive posterior updates. The update is proportional to the error computed at a lower level, and when large errors are observed the network needs to accommodate new evidence. This configuration can have disastrous consequences if the surprise is getting completely out of control, as this can happen with improper precision weighting, or simply when receiving observations that are far from the expected distribution. It is therefore essential for these systems to implement firewalls that could contain the propagation of catastrophic updating and preserve past learnings from complete erasing. Here, we suggest that the dynamic reconfiguration of the networks can serve as an alternative to belief updating under disproportionate surprise, and preserve the model integrity. This idea arise from the application of some biological principles to Bayesian networks, but interestingly its implementation can express a variety of other popular learning and inference algorithms that are also highly inspired by biological systems, namely structure learning, causal discovery and meta-learning.\nSelf-assembling and self-growing neural networks are a new, active, and rapidly advancing area of research in deep reinforcement learning (Najarro et al., 2023). These approaches stress the notion that biological as well as artificial agents should implement a flexible reorganization of their generative model to support lifelong learning (Kudithipudi et al., 2022), a feature that also remains absent in many applications of predictive coding to date. While several methods have been developed by extending conventional deep neural networks, here, we propose that dynamic networks can naturally embed this principle by simply adding or removing nodes during the belief propagation steps (see the top panel in 4 b.). This feature could leverage the flexibility of a predictive coding framework to implement branching, splitting, and merging behaviours under prediction errors, which can be a way to express nonparametric modelling problems. It is worth noting here, that the dynamic reshaping of networks in real-time would contribute to both the memory and energy efficiency of the system, ensuring that at each time step, only an optimal number of nodes are carried over. The resulting structure then works as a comprehensive encoding of relevant past environmental volatility, building on the notion that the shape of the system, and beyond that the agent's body itself, is an active part of the inference process.\nBut this modulation of the network structure can also merely concern edges, without adding or removing significant portions of the graphs. Because dynamic networks behave like Bayesian networks during the prediction steps, editing the edges equates to modulating the causal relationship between variables, which is a way to express causal discovery algorithms. In this context, the agent tries to recover the causal graph from observation only (see the middle panel"}, {"title": "5 Conclusion", "content": "In this paper, we have introduced pyhgf, a neural network library for predictive coding with a focus on generalised Bayesian filtering and the generalised Hierarchical Gaussian Filter (HGF). We described how the modular definition of neural networks supporting the scheduling of update steps can serve as a generic framework for models relying on the propagation of simple local computation through a hierarchy of layers, such as predictive coding neural networks. One part of the API is dedicated to the flexible development of dynamic networks, while the second part is oriented towards high-level use and parameter inference, as typically requested by computational neuroscience studies. Together, we hope that this toolbox will help and strengthen the application of predictive neural networks in computational psychiatry, and open new designs in artificial intelligence towards hybrid and complex models of cognition that build on top of the principled computation derived from predictive coding. pyhgf can be installed from the Python Package Index (https://pypi.org/project/pyhgf/) and the source code is hosted on GitHub under the following public repository: https://github.com/ilabcode/pyhgf. The documentation for the most recent version is accessible at the following link: https://ilabcode.github.io/pyhgf/index.html."}, {"title": "Glossary", "content": "hierarchical Gaussian filter The Hierarchical Gaussian Filter is a popular model in computational psychiatry that unifies reinforcement learning and Bayesian inference. While it can be described as a recurrent neural network, it is often more conveniently described using the generative model it inverted. When applied straightforwardly to time series, the Hierarchical Gaussian Filter acts as a Bayesian filter that updates its expectation using errors of predictions at the previous time point, an approach that is similar to both Kalman filters and Rescorla-Wagner update rules. Here, the learning rate is not fixed but influenced by the higher level of the hierarchy, generally encoding the volatility of the environment. Recent developments have turned this model into a generic framework that can handle arbitrarily sized networks of distributions..\npredictive coding Predictive coding is a unifying theory that has emerged in computational neuroscience, postulating that the brain performs inference on hierarchical predictive models of the environment through the computation of local prediction errors.."}]}