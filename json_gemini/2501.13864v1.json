{"title": "AUTOENCODERS FOR ANOMALY DETECTION ARE UNRELIABLE", "authors": ["Roel Bouman", "Tom Heskes"], "abstract": "Autoencoders are frequently used for anomaly detection, both in the unsupervised and semi-supervised settings. They rely on the assumption that when trained using the reconstruction loss, they will be able to reconstruct normal data more accurately than anomalous data. Some recent works have posited that this assumption may not always hold, but little has been done to study the validity of the assumption in theory. In this work we show that this assumption indeed does not hold, and illustrate that anomalies, lying far away from normal data, can be perfectly reconstructed in practice. We revisit the theory of failure of linear autoencoders for anomaly detection by showing how they can perfectly reconstruct out of bounds, or extrapolate undesirably, and note how this can be dangerous in safety critical applications. We connect this to non-linear autoencoders through experiments on both tabular data and real-world image data, the two primary application areas of autoencoders for anomaly detection.", "sections": [{"title": "INTRODUCTION", "content": "Autoencoders are one of the most popular architectures within anomaly detection, either directly, or as a scaffold or integral part in larger pipelines or architectures. They are commonly used across a variety of domains, such as predictive maintenance (Kamat & Sugandhi, 2020), network anomaly detection (Said Elsayed et al., 2020), and intrusion detection (Farahnakian & Heikkonen, 2018), but find much contemporary use in computer vision anomaly detection, with applications such as industrial inspection (Tsai & Jen, 2021), medical imaging (Wei et al., 2018), structural health monitoring (Chow et al., 2020) and video surveillance (Zhao et al., 2017; Cruz-Esquivel & Guzman-Zavaleta, 2022). Many of these applications are safety critical, meaning that the reliability of these algorithms is of utmost importance in order to prevent catastrophic failure and associated dangers and consequences.\nAnomaly detection using autoencoders typically relies on using the reconstruction loss, often the mean squared error (MSE), as a proxy for \"anomalousness\u201d. The underlying assumption is that anomalies are harder to reconstruct, and will therefore have a higher reconstruction loss. However, the validity of this assumption has been questioned in recent years. Merrill & Eskandarian (2020) and Beggel et al. (2020) for example state that anomalies in the training data might lead to reconstruction of anomalies, leading to unreliable detectors. Some researchers have noted that reconstruction of unseen anomalies might also occur in the semi-supervised setting, where the training data is assumed to contain no anomalies (Astrid et al., 2021; 2024; Gong et al., 2019; Zong et al., 2018). Yet, little work has been done on the nature of failure and reliability of autoencoders.\nIn this work we provide valuable insights into the reliability of autoencoders for anomaly detection. Following the seminal works of Bourlard & Kamp (1988) and Baldi & Hornik (1989) we mathematically and experimentally study autoencoder failure modes, whilst briefly examining how various activation functions influence these failures. We show that failure is not just a rarely occurring edge case, but also show failure cases on tabular data and on real-world image data commonly used in anomaly detection benchmarks. By doing this we provide a foundation for future research in solving the demonstrated unreliability of autoencoders for anomaly detection and furthering our understanding"}, {"title": "RELATED WORK", "content": "This work is not the first to recognize that autoencoders have several issues as anomaly detectors. The most discussed type of failure is the unwanted reconstruction of anomalies, which is also the focus of this work. Several causes of this unwanted behavior have been proposed.\nMany works focus on the unsupervised setting, and observe that contrary to prior belief, autoencoders will fairly easily reconstruct any anomalies present in the training data, leading to an unusable detector (Merrill & Eskandarian, 2020; Beggel et al., 2020; Cheng et al., 2021; Tong et al., 2022).\nSeveral works cover the anomaly reconstruction problem within the semi-supervised setting. Most commonly, it is only experimentally observed that anomalies are well reconstructed (Gong et al., 2019; Zong et al., 2018; Cheng et al., 2021; Astrid et al., 2021; 2024; Salehi et al., 2021; Nalisnick et al., 2019; Lyudchik, 2016). Based on these experimental results, some solutions have been proposed. Gong et al. (2019) mention that out-of-bounds reconstruction can occur and propose adding a memory module to the autoencoder to alleviate the issue. While the addition of the memory module can aid in limiting out-of-bounds reconstruction, it also leads to a severely decreased reconstruction ability and substantial added complexity in training and optimizing the network. Zong et al. (2018) note that while some anomalies have a high reconstruction loss, some occupy the region of normal reconstruction loss, and add a Gaussian mixture model to the latent space to aid in detection of anomalies under the assumption that anomalies occupy a low-density region in the latent space. Similarly, Cheng et al. (2021) aim to detect anomalies in the latent space by looking at the distance to the centroid. From our experiments we can glean that relying on distances or densities in the latent space does not always work in practice. Astrid et al. (2021; 2024) make use of constructed pseudo anomalies in training the autoencoder. They add adaptive noise to normal samples to generate pseudo anomalies. In the reconstruction, they then optimize the reconstruction loss between the pseudo anomaly and the normal sample used to generate it. While they show promising results and greater discriminative power on benchmark datasets, they do not quantify to which degree their performance gains can be attributed to the reduction of the out-of-bounds reconstruction. Salehi et al. (2021) aim to limit the reconstruction of anomalies by generating adversarial samples. The adversarial examples are generated by perturbing the normal samples, and minimizing the effect those perturbations have in the latent space. This is similar to the concept of denoising autoencoders. Based on our experiments, we do not think this results in a reliable autoencoder, as often adversarial anomalies can occupy the latent space close to normal data.\nSome authors have moved beyond the experimental, and propose explanations for the anomaly reconstruction problem. For example You et al. (2022), Lu et al. (2023) and Bercea et al. (2023) propose that anomaly reconstruction can happen because an autoencoder can learn an \"identical shortcut\", where both normal data and anomalous data is effectively reconstructed using an identity mapping. This point has however been countered by Cai et al. (2024) who show that by constraining the latent space to be sufficiently low dimensional, this problem can be avoided.\nThe second line of thought follows from VAE anomaly detection, where Havtorn et al. (2021) theorize that in out-of-distribution detection, unwanted reconstruction can happen due to a high correlation between learned low-level features for in- and out-of-distribution data.\nA third line of thought is proposed by Zhou (2022) who propose that reconstruction of out-of-distribution samples can happen due to out-of-distribution data having smaller neural activations than in-distribution data.\nFinally, some works theorize that autoencoders can perfectly reconstruct data due to the anomalies occupying the reconstruction hyperplane, or latent space manifold. Denouden et al. (2018) for example note this phenomenon, and aim to solve it by adding the Mahalanobis distance in the latent space to the loss. The most detailed work is that of Yoon et al. (2021) who provide an example of the hyperplane interpolating between clusters of data. They solve this by introducing a normalized"}, {"title": "BACKGROUND", "content": "In practical anomaly detection, we attribute a score $s_i = f_{\\text{anomaly score}}(x_i)$ for each sample $x \\in X = \\mathbb{R}^n$, i.e. the i-th row of dataset X with size m-by-n. The score should then be higher for anomalous samples than for normal data. When applied to some dataset consisting of both normal data and anomalies, i.e. $X = \\{X_{\\text{normal}}, X_{\\text{anomalous}}\\}$, a perfect anomaly detector will assign higher scores to anomalies than to normal data: $\\min_i (f_{\\text{anomaly score}}(x_{\\text{anomalous}})) > \\max_i(f_{\\text{anomaly score}}(x_{\\text{normal}}))$.\nThe two most common anomaly detection settings are unsupervised and semi-supervised. Unsupervised anomaly detection is characterized by having no discernible \u201ctrain\u201d and \u201ctest\u201d splits. Instead, we only consider a single dataset $X = \\{X_{\\text{normal}}, X_{\\text{anomalous}}\\}$, where we are uncertain which samples are anomalous and which are not. In the semi-supervised setting we instead have a conventional \u201ctrain\" and \"test\" set. The train set consists out of only normal samples: $X_{\\text{train}} = \\{X_{\\text{train, normal}}\\}$, while the test set is unknown, and can contain both normal and anomalous samples: $X_{\\text{test}} = \\{x_{\\text{test, normal}}, X_{\\text{test, anomalous}}\\}$. In this paper we will only consider the semi-supervised case, and simplify the notation with $X = X_{\\text{train}}$ and $x_i$ referring to an individual training data point, which in the semi-supervised case by definition is not an anomaly. We then consider a new data point $a$ to determine whether this is an anomaly or not. In older literature, semi-supervised anomaly detection is often called one-class classification."}, {"title": "OUT-OF-BOUNDS RECONSTRUCTION", "content": "In this section we will show that autoencoders can yield zero-loss reconstruction far away from all training data, and that these autoencoders will then fail to detect certain anomalies. Our analysis follows the results of Bourlard & Kamp (1988), moving from PCA to linear autoencoders to non-linear autoencoders.\nOut-of-bounds reconstruction is unwanted within the application of anomaly detection, as it leads to a low reconstruction loss for data that can be considered anomalous, thereby leading to false negatives. These regions of out-of-bounds reconstruction can also be exploited by targeted adversarial evasion attacks.\nIn the worst case, unwanted perfect reconstruction causes an anomaly $a \\in \\mathbb{R}^n$ far from all training data to be ranked as being less anomalous than or equally anomalous as all training data, that is:\n$f_{\\text{anomaly score}}(a) \\leq \\min_i (f_{\\text{anomaly score}}(x_i)).$"}, {"title": "ANOMALY DETECTION USING THE RECONSTRUCTION LOSS", "content": "Both PCA and autoencoders are dimensionality reduction techniques that can be used to detect anomalies using their reconstruction loss, commonly known as the mean squared error, or MSE. We can calculate the reconstruction loss by comparing a sample $x_i$ to its reconstruction $\\hat{x}_i$:\n$L_R(x_i, \\hat{x}_i) = \\frac{1}{n} \\sum_{j=1}^{n} (x_{i,j} - \\hat{x}_{i,j})^2$, for each sample vector $x_i$. This reconstruction loss often serves as a proxy for detecting anomalies, with the underlying assumption that a higher reconstruction loss indicates a higher likelihood of the sample being an anomaly.\nFor both PCA and autoencoders we want to find a lower-dimensional encoding $Y$, e.g. $d < n$, in the encoding space $Y = \\mathbb{R}^d$ by applying the function $g : X \\rightarrow Y$. We then decode Y by transforming it back into the space X through the decoder $h : Y \\rightarrow X$, yielding the reconstructed data $\\hat{X}$. Summarizing, we learn the concrete transformations $X \\xrightarrow{g} Y \\xrightarrow{h} \\hat{X}$.\nWe can then formulate the anomaly scoring function in terms of the reconstruction loss, encoder, and decoder: $f_{\\text{anomaly score}}(x_i) = L_R(x_i, h(g(x_i)))$. The worst case can then be formulated as: there exists an $a$ far from all training data such that $L_R(a, \\hat{a}) \\leq \\min_i(L_R(x_i, \\hat{x}_i)).$"}, {"title": "PCA", "content": "In PCA, we factorize $X$ as $X = U\\Sigma V^T$ using singular value decomposition (SVD), where $U$ and $V$ are orthonormal matrices containing the left- and right-singular vectors of $X$, respectively, and $\\Sigma$ is a diagonal scaling matrix. The encoding, or latent, space is then obtained by projecting onto the first $d$ right-singular vectors: $Y = g(X) = XV_d$, where $V_d$ contains the first $d$ columns of $V$. The transformation back into $X$ is given by $\\hat{X} = h(Y) = YV_d^T$.\nRevisiting Bourlard & Kamp (1988), we will show that there exist some $a \\in \\mathbb{R}^n$ for which the reconstruction loss is zero, but that are far away from the normal data, i.e. $\\min_i(\\text{dist}(x_i, a)) > \\delta$, for any arbitrary choice of $d$. Hereby we prove that it is possible to find anomalous, adversarial, examples with perfect out-of-bounds reconstruction. We can prove this even in the semi-supervised setting, where we guarantee that the model was not exposed to anomalous data at training time. Due to the semi-supervised setting being more restrictive, the proofs also apply to the unsupervised case.\nLet us now look for some $a \\in \\mathbb{R}^n$. We aim to prove that if $a$ lies in the column space of $V_d$, then the reconstruction loss $L_R(a, h(g(a))) = 0$.\nWe now need to show that there exists some $a$ such that $h(g(a)) = a$. For PCA, this condition can be written as:\n$aV_dV_d^T = a$.\nAssume $a$ is in the row space of $V_d^T$. Then $a$ can be expressed as a linear combination of the rows in $V_d^T$. Let $c \\in \\mathbb{R}^d$ be such that:\n$a = cV_d^T$.\nSubstitute $a$ into the left-hand side of the reconstruction equation:\n$aV_dV_d^T = cV_d^TV_dV_d^T$.\nSince $V_d$ is composed of orthonormal columns, $V_d^T V_d = I_d$, where $I_d$ is the $d$-by-$d$ identity matrix. Therefore:\n$cV_d^TV_dV_d^T = cV_d^T = a$.\nThus, $a$ satisfies the condition $h(g(a)) = a$, implying that the reconstruction loss $L_R(a, g(h(a))) = 0$.\nNow we will prove that there exists some adversarial example $a \\in \\mathbb{R}^n$ that is far from all normal data, i.e. $\\min_i(\\text{dist}(x_i, a)) > \\delta$, for an arbitrary $\\delta$ and the Euclidean distance metric, but still has a reconstruction loss $L_R(a, g(h(a))) = 0$.\nLet us first recall that any $a$ in the column space of $V_d$ will have zero reconstruction loss.\nIf we then define $a = x_i V_d V_d^T + cV_d^\\bot$, $a$ will still have zero reconstruction loss.\nThen for the Euclidean distance it follows that:\n$\\text{dist}(x_i, a)^2 = ||x_i - x_i V_d V_d^T ||^2 + ||a - x_i V_d V_d^T ||^2$,\nor the squared Euclidean distance is equal to the distance from $x_i$ to its projection onto the hyperplane $x_i V_d V_d^T$ plus the distance from that projection $x_i V_d V_d^T$ to $a$.\nIt then follows that:\n$\\text{dist}(x_i, a)^2 \\geq ||a - x_i V_d V_d^T ||^2 = ||x_i V_d V_d^T + cV_d^\\bot - x_i V_d V_d^T ||^2 = ||cV_d^\\bot ||^2$,\nwhich we can increase to arbitrary length. This can be intuited as moving the projection of $x_i$ along the hyperplane.\nTo ensure that we increase the distance to all points $x_i$ rather than just a single one, we need to move outward starting from a sample on the convex hull enclosing $XV_d$. Any point in this convex set, that is the set of points occupying the convex hull, can be moved along the hyperplane to increase the distance to all points $x_i V_d$, and therefore to all points $x$ as long as $c$ lies in the direction from $x_i V_d$ to the exterior of the convex hull.\nWe can thus always find a $a = x_i V_d V_d^T + cV_d^\\bot$, for some $x_i V_d$ in the convex set of $XV_d$ and choose $c$ so that it points from $x_i V_d$ to the exterior of the convex hull and is of sufficient length such that $\\min_i(\\text{dist}(x_i, a)) > \\delta$, for an arbitrary $\\delta$."}, {"title": "LINEAR AUTOENCODERS", "content": "Linear neural networks, like PCA, can also exhibit out-of-bounds reconstruction for certain anomalous data points.\nLinear autoencoders consist of a single linear encoding layer and a single linear decoding layer. Given a mean-centered dataset $X$, the encoding and decoding transformations can be represented as follows:\n$Y = g(X) = XW_{\\text{enc}}$,\n$\\hat{X} = h(Y) = YW_{\\text{dec}}^T = XW_{\\text{enc}}W_{\\text{dec}}^T$,\nwhere $W_{\\text{enc}}$ is the n-by-d weight matrix of the encoder, and $W_{\\text{dec}}$ is the d-by-n weight matrix of the decoder. We assume the autoencoder to have converged to the global optimum. Note that we define $W_{\\text{dec}}^T$ in its transposed form to illustrate its relation to $V_d^T$. Due to the full linearity of the model, even multiple layer networks can be simplified to a single multiplication. It is known that a well-converged linear autoencoder finds an encoding in the space spanned by the first $d$ principal components (Baldi & Hornik, 1989). In other words, the encoding weight matrix can be expressed in terms of the first $d$ principal components and some invertible matrix $C$:\n$W_{\\text{enc}} = V_dC$.\nAt the global optimum $W_{\\text{dec}}^T$ can be expressed as the inverse of $W_{\\text{enc}}$:\n$W_{\\text{dec}}^T = W_{\\text{enc}}^\\dagger = (V_dC)^{-1} = C^{-1}V_d^\\dagger = C^{-1}V_d^T$.\nTo show that linear autoencoders can exhibit perfect out-of-bounds reconstruction, we follow the same lines as for PCA.\nLet us now look for some $a \\in \\mathbb{R}^n$. We aim to prove that if $a$ lies in the column space of $V_d$, then the reconstruction loss $L_R(a, h(g(a))) = 0$.\nWe need to show that there exists some $a$ such that $h(g(a)) = a$. For linear autoencoders, this condition can be written as:\n$a W_{\\text{enc}} W_{\\text{dec}}^T = a$.\nAssume $a$ is in the row space of $V_d^T$. Then $a$ can be expressed as a linear combination of the rows in $V_d$. Let $c \\in \\mathbb{R}^d$ be such that:\n$a = cV_d^T$.\nThen it follows that:\n$aW_{\\text{enc}} W_{\\text{dec}}^T = cV_d^T W_{\\text{enc}} W_{\\text{dec}}^T = cV_d^T V_d C C^{-1}V_d^T = cV_d^T V_d V_d^T = cV_d^T = a$,\nindicating that $a$ satisfies the condition $h(g(a)) = a$, implying that the reconstruction loss $L_R(a, g(h(a))) = 0$.\nBy proving this, the case of the linear autoencoder reduces to that of PCA, with the same proof that adversarial examples satisfying $\\min_i(\\text{dist}(x_i, a)) > \\delta$, with a reconstruction loss $L_R(a, g(h(a))) = 0$, exist.\nAn extension of this proof to the case of linear networks with bias terms applied on non-centered data can be found in Appendix A.1."}, {"title": "NON-LINEAR AUTOENCODERS", "content": "In this section we show that datasets exist for which we can prove that non-linear neural networks perform the same unwanted out-of-bounds reconstruction. Then we experimentally show that this behavior indeed occurs in more complex real-world examples. In this way, we illustrate how autoencoders demonstrate unreliability even in real-world scenarios."}, {"title": "FAILURE OF A NON-LINEAR NETWORK WITH RELU ACTIVATIONS", "content": "We can show on a simple dataset that unwanted reconstruction behavior can occur in non-linear autoencoders. We consider a two-dimensional dataset X consisting out of normal samples $x_i = \\alpha_i (1,1)$, where $\\alpha_i$ is some scalar. Simply put, every normal sample $x_i$ occupies the diagonal. This dataset can be perfectly reconstructed by a linear autoencoder with $W_{\\text{enc}} = \\beta (1,1)$, where $\\beta$ is some scalar. The simplest non-linear autoencoder with ReLU activation will find the same weights, but with a bias such that $x_i W_{\\text{enc}} + b_{\\text{enc}} > 0$ for all $x_i \\in X$, i.e. $b_{\\text{enc}} \\geq \\min_i(x_iW_{\\text{enc}})$. This will then lead to a perfect reconstruction for all $x_i$. Adversarial anomalies a can also be easily found as $a = c(1,1)$, where $c > \\max_i^{(1,1)}$ is some sufficiently large scalar such that $\\min_i(\\text{dist}(x_i, a)) > \\delta$ is satisfied. We theorize that even beyond this simple case, similar linear behavior can occur beyond the convex hull that the normal data occupies. We experimentally show this anomaly reconstruction behavior in later sections."}, {"title": "TABULAR DATA", "content": "On more complex datasets, we observe similar behavior. We have synthesized several two-dimensional datasets to show how non-linear autoencoders behave when used for anomaly detection. These datasets, as well as contours of the MSE of autoencoders trained on these datasets, are visualized in Figure 1. In each of these cases, we have synthesized a dataset by sampling 100 points per distribution, either from a single or from two Gaussians as in 1a, 1b, le, and 1f, or from $x_2 = x_1$ with added Gaussian noise in 1c and 1d. In all cases we use an autoencoder with layer sizes [2,5,1,5,2], except for 1d, where we use layer sizes of [2,100,20,1,20,100,2] to better model the non-linearity of the data. All layers have ReLU (Subfigures 1a, 1b, 1c, and 1d), or sigmoid (Subfigures le and 1f) activations, except for the last layer, which has a linear activation. In these figures the color red is used to highlight those areas where the autoencoder is able to nearly perfectly reconstruct the data, i.e. MSE < $\\epsilon = 0.1$.\nReLU Activation Failure on Tabular Data We can readily observe some of the problematic behavior of autoencoders as anomaly detectors. Firstly, in Figure 1a we observe that well outside the bounds of the training data there is an area with a near-perfect reconstruction. Worryingly, the reconstruction loss is lower than for a large part of the area which the normal data occupies. If we move in the (-1,-1) direction, the encoder and decoder will no longer match perfectly. Even so, problematically low reconstruction losses can be found in this direction. In Figures 1c and 1d we see the same linear out-of-bounds behavior. In each of these cases, the mismatch between encoder and decoder in the linear domain is less noticeable, leading to even larger areas of near-perfect reconstruction. Lastly, in Figure 1b that there is an area between the two clusters with a good reconstruction. Likely the units responsible for this area are still close to their initialization, and due to the simple shallow network architecture can not meaningfully contribute to the reconstruction of any samples.\nOur intuition of this behavior directly relates to the proof of out-of-bounds reconstruction we have provided for linear autoencoders. At the edge of the data space, only a few of the ReLU neurons activate. Beyond this edge, no new neurons will activate, nor will any of the activated ones deactivate. This can lead to linear behavior on some edge of the data space, i.e., in this area the network reduces to a linear transformation $W_{\\text{enc}}$. If we now synthesize some $a$ such that it lies in the column space of $W_{\\text{enc}}$, we can again find some adversarial anomalies $a = cW_{\\text{enc}}^T$. Like we have observed in Figure 1a, there may be a mismatch between the encoder and decoder, even at the global optimum, so we might not be able to increase $c$ towards infinity and still find adversarial examples with $L_R(a, g(h(a))) < \\epsilon$.\nSigmoid Activation Autoencoders Nowadays, full sigmoid networks have mostly fallen out of favor in deeper networks due to their vanishing gradients (Hochreiter, 1991; Glorot et al., 2011). However, sigmoids are more attractive to use in anomaly detection because they lead to networks that do not exhibit the hyperplane issues that the ReLU suffers from. While sigmoids have the more desirable property of tapering off at high or low input, making it hard to perfectly reconstruct data far away from normal data, autoencoders with just sigmoid activation can still behave unexpectedly, albeit less so than those with ReLU activation."}, {"title": "CONVOLUTIONAL AUTOENCODERS", "content": "All the previous examples clearly illustrate autoencoders' possible failure and unreliability when used for anomaly detection on tabular data. Yet, many applications of anomaly detection are in computer vision, where anomaly detection can be used to detect foreign objects. Typical examples of computer vision anomaly detection are surveillance, where videos are analyzed to find possible security threats (Nayak et al., 2021; Sultani et al., 2018), structural health monitoring (Bao et al., 2019), and industrial inspection (Bergmann et al., 2021)."}, {"title": "FAILURE ON REAL-WORLD DATA: MNIST", "content": "To show that deeper non-linear networks trained on real-world image data can still undesirably reconstruct anomalies we will study an autoencoder for anomaly detection that was trained on the well-known MNIST dataset (LeCun, 1998). Benchmarking computer vision anomaly detection algorithms is not as standardized as classification benchmarking, as datasets with \u201ctrue\u201d anomalies are exceedingly rare. The common method for benchmarking these algorithms is to take a classification dataset and select a subset of classes as \"normal\" data and another distinct subset as \"anomalies\". This is analogous to other, more-developed, fields such as tabular anomaly detection (Bouman et al., 2024). There is no general consensus on which digits are taken as the normal data, and how many. In our experiments, both shown and non-shown, we have tried several different combinations and observe that in some cases out-of-bounds reconstruction occurs.\nThe 2D convolutional autoencoder we will discuss has a 2-layer encoder and 2-layer decoder. Down- and upsampling to and from the latent space is done using a fully connected linear layer. The convolutional layers all use ReLU activations, except for the last one, which is a sigmoid to bound the data to the original 0-1 range. In these experiments, the latent space is set to be two dimensional, far below the maximum to avoid the \"identical shortcut\" as noted by Cai et al. (2024). This serves as proof that the \"identical shortcut\" is not the cause of anomaly reconstruction. In Figures 2a and 2b we show how the reconstruction loss behaves in the latent space when we apply this autoencoder on a train set consisting out of a subset of digits. These contourplots are constructed by sampling each point in the latent space, decoding it to get an artificial sample, and then calculating the reconstruction loss between the artificial sample and its reconstruction loss. We subsequently show the latent representations of all normal data in the same space. We should note that as the encoder is a many-to-one mapping, the reconstruction loss in the grid does not necessarily correspond to the reconstruction loss of a real sample occupying the same point in that grid.\nLooking at Figure 2a we see that a 2D latent space is able to separate the digits 4 and 5, with 7 occupying the middle between the two classes. As expected, the reconstruction loss grows the larger the distribution shift becomes. However, the reconstruction loss landscape is fairly skewed, with the MSE starkly increasing towards the right, and slowly towards any other direction, indicating model bias. Most notably, around (-4.2, -5.2) we observe an out-of-bounds area of low reconstruction loss. Due to this type of visualization, we can easily generate an adversarial anomaly by simply decoding the latent space sample: $a = h((-4.2, -5.2))$. This leads to the adversarial anomaly shown in Figure 2c. The adversarial anomaly shares some features with the digits used for training, but does not resemble any of them specifically, making it a clear false negative. Indeed, this sample fulfills our earlier criterion of $L_R(a_i, \\hat{a}_i) \\leq \\min_i(L_R(x_i, \\hat{x}_i))$, as for this example $L_R(a_i, \\hat{a}_i) = 0.014$, and $\\min_i(L_R(x_i, \\hat{x}_i)) = 8.47$.\nWe also looked at a simpler example, where we train on the digits 0 and 1 to get a clearer separation of the two classes. In our previous experiments with sigmoid activation functions in Section 4.4.2 we observed that at the intersection of the two modalities some unwanted interpolation can occur. In Figure 2b we can observe the same thing, where at the intersection of the two classes we have a very small area in the latent space with a very low reconstruction loss. The normal data close to this area is however not well reconstructed. By generating an artificial sample from the lowest MSE in this latent space, at (0.535, -0.353), we can find an adversarial anomaly $a = h((0.535, -0.353))$ with $L_R(a_i, \\hat{a}_i) = 0.022$, substantially lower than $\\min_i(L_R(x_i, \\hat{x}_i)) = 1.61$. This adversarial anomaly is visualized in Figure 2d. We find, unsurprisingly, that the adversarial anomaly here is a mix of the features of the 0 and 1 class.\nSimilar to our experiments on the digits 4, 5, and 7 autoencoder, we identified an area at the edge of the 1 class where the reconstruction loss is low, but where few normal data points can be found. In"}, {"title": "CONCLUSION", "content": "In this work we provide an analysis of the unwanted reconstruction of anomalies that autoencoders can exhibit when used for anomaly detection. We move beyond existing theories of unwanted reconstruction happening in interpolation and show how unwanted out-of-bounds reconstruction can occur when extrapolating as well, and how this can lead to anomalies staying fully undetected. We show through several experiments that these issues can arise in real-world data and not just in theory. This leads us to some safety concerns, where autoencoders can catastrophically fail to detect obvious anomalies. This unreliability can have major consequences when trust is put into the anomaly detector in safety-critical applications.\nIn general, we solidify the growing belief that the reconstruction loss is not a reliable proxy for anomaly detection, especially when the network is explicitly trained to lower the reconstruction loss for normal data without constraining the reconstruction capability beyond the bounds of the normal training data such as has been done by Yoon et al. (2021). We find that this issue is most prevalent for (conditionally) linear units such as the ReLU, but similar issues exist for sigmoid networks, albeit to a lesser degree. The reconstruction issue is mostly caused by the fact that a point in the lower-dimensional latent space corresponds to a hyperplane in the original space that the data occupies. Next to interpolation and out-of-bounds reconstruction, we find that anomalies can remain undetected when they occupy the latent space where normal classes border.\nUsers of autoencoders for anomaly detection should be aware of these issues. Good practice would be to at least check whether a trained non-linear autoencoder exhibits the undesirable out-of-bounds reconstruction. In this paper's illustrative examples, we checked for this by searching for adversarial anomalies. This was relatively easy, as it could be done either visually in the latent space, or through a simple 2D grid search. For more complex datasets, requiring larger latent spaces, a feasible strategy might be to again synthesize samples from the latent space and formulate the search for adversarial anomalies as an optimization in terms of projected gradient descent (Madry et al., 2017).\nBy describing exactly how autoencoders are unreliable anomaly detectors by describing anomaly reconstruction, we hope to provide a scaffold for future research on fixing and avoiding the identified issues in a targeted manner."}, {"title": "LINEAR NETWORKS WITH BIAS TERMS", "content": "Linear neural networks with a bias term, similar to those without a bias term, still exhibit out-of-bounds reconstruction that leads to zero reconstruction loss for certain anomalous data points.\nLinear autoencoders with bias terms consist of a single linear encoding layer and a single linear decoding layer, each with an added bias term. Like for linear networks without a bias, all multi-layer networks can be reduced to a single layer autoencoder. At the global optimum the bias terms will recover the process of mean-centering. Note that a simplified version of this proof was presented by Bourlard & Kamp (1988).\nLet us now consider $\\overline{x} = \\frac{1}{m}\\sum_{i=1}^{m} X_i$, so the vector of length n where each element contains the corresponding column"}]}