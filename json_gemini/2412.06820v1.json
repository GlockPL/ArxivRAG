{"title": "Artificial Intelligence without Restriction Surpassing Human Intelligence with Probability One: Theoretical Insight into Secrets of the Brain with AI Twins of the Brain", "authors": ["Guang-Bin Huang", "M. Brandon Westover", "Eng-King Tan", "Haibo Wang", "Dongshun Cui", "Wei-Ying Ma", "Tiantong Wang", "Qi He", "Haikun Wei", "Ning Wang", "Qiyuan Tian", "Kwok-Yan Lam", "Xin Yao", "Tien Yin Wong"], "abstract": "Artificial Intelligence (AI) has apparently become one of the most important techniques discovered by humans in history while the human brain is widely recognized as one of the most complex systems in the universe. One fundamental critical question which would affect human sustainability remains open: Will artificial intelligence (AI) evolve to surpass human intelligence in the future? This paper shows that in theory new AI twins with fresh cellular level of AI techniques for neuroscience could approximate the brain and its functioning systems (e.g. perception and cognition functions) with any expected small error and Al without restrictions could surpass human intelligence with probability one in the end. This paper indirectly proves the validity of the conjecture made by Frank Rosenblatt 70 years ago about the potential capabilities of AI, especially in the realm of artificial neural networks. This paper also gives the answer to the two widely discussed fundamental questions: 1) whether AI could have potentials of discovering new principles in nature; 2) whether error backpropagation (BP) algorithm commonly and efficiently used in tuning parameters in Al applications is also adopted in the brain. Intelligence is just one of fortuitous but sophisticated creations of the nature which has not been fully discovered. Like mathematics and physics, with no restrictions artificial intelligence would lead to a new subject with its self-contained systems and principles. We anticipate that this paper opens new doors for 1) AI twins and other AI techniques to be used in cellular level of efficient neuroscience dynamic analysis, functioning analysis of the brain and brain illness solutions; 2) new worldwide collaborative scheme for interdisciplinary teams concurrently working on and modelling different types of neurons and synapses and different level of functioning subsystems of the brain with Al techniques; 3) development of low energy of AI techniques with the aid of fundamental neuroscience properties; and 4) new controllable, explainable and safe AI techniques with reasoning capabilities of discovering principles in nature.", "sections": [{"title": "Introduction", "content": "Artificial Intelligence (AI) has experienced three important development phases: following its warmup stage of development in the 1950s to 1980s (Stage I) and research driven period from the 1980s to 2010 (Stage II), AI has transitioned into a golden industrial data driven era since 2010 (Stage III). There is no doubt that after 70 years of development, it has been a new normal that AI frequently surpasses human performance in numerous applications. Thus, the extensively discussed and widely argued question turns out to become critical: Would AI without appropriate restriction and governance surpass human intelligence in the end?\nIt is widely recognized that human brains may be one of the most complicated systems in the universe. The human body receives signals from its environment mainly through the five types of sensors (human sense organs: eyes, ears, noses, tongues, and skins). The corresponding five human senses are sight, hearing, smell, taste, and touch. These signals are sent to the brain. The human brain, a sophisticated biological system, processes and interprets sensory signals for perception including vision, hearing and temperature, and manages cognitive recognition through learning, memory, reasoning, thought and emotion regulation to control all regulatory functions of the human body.\nThe human brain is composed of hundreds of different types of biological neurons (biological cells) through large number of interconnected biological connections (synapses):\n1) The number of biological neurons in the human brain is huge but finite, e.g., about 86 billion neurons for average adults with about 16 billion neurons in human's cerebral cortex\u00b9. Hundred types of neurons have been discovered in human brains\u00b2.\n2) Synapses are the junctions through which neurons connect and communicate with each other in the brain. The number of synapses in the brain is finite as well. Each neuron has a finite number of synaptic connections, e.g., from a few to hundreds of thousands of synaptic connections, with itself (through autapses), neighboring neurons, or neurons crossing various regions of the brain. Autapses are synapses between a neuron's axon and its own dendrites\u00b3. The human brain contains about hundreds of trillion synapses\u00b9. Different from chemical synapses, volume transmission and ehaptic coupling may occur in signaling transmissions among neurons. Electrical synapses (gap junctions) may allow electrical signals to pass directly from one neuron to another5.\n3) The number of neurons and synapses are crucible parameters for human intelligence."}, {"title": "Challenges of classical mathematical modelling and neuro dynamics approaches", "content": "The structure of the human brain is intricate and complicated. Neuroscientists have sought to divide the brain into smaller pieces\u00b9\u00b9 in order to understand how the brain works as a whole. It is difficult and challenging to fully understand the sophisticated learning mechanism, infinite number of unknown functions and philosophy of the human brain based on its pieces and regions with feedback of signals. Pieces and regions of the brain are composed of neural circuits which may feedback signals in complex ways.\nSpiking neurons and neuron dynamics are often used to describe neurons' characters. Both the spiking neuron models and neuron dynamics methods aim to provide mathematical descriptions of the conduction of electrical signals in neurons, e.g., the role of the biophysical and geometrical characteristics of neurons on the conduction of electrical activity, including spatial morphology or the membrane voltage dynamics12. One of most typical neuron models is the Nobel Prize awarded Hodgkin-Huxley model (H&H model)13,14,15,16 which expresses the relationship between the flow of ionic currents across the neuronal cell membrane and the membrane voltage of the cell with a set of nonlinear differential equations. For example, the voltage-current relationship of a typical neuron can be given as\n$I = C_m \\frac{dV}{dt} + I_i$\nwhere $I$ is the total current density through the membrane, $I_i$ is the ionic current density, $C_m$ is the membrane capacity per unit area, and t is time14. Multiple parameters may be estimated or measured for an accurate model of a biological neuron and it would be difficult to have accurate models for all types of biological neurons in the end."}, {"title": "Divide-and-Conquer approach: General unified mathematical representations of neurons and synapses (as basic learning elements) of the brain", "content": "Different levels of systems in the brain are nonlinear and dynamical. Advanced mathematical modelling including almost endless possibilities and functions makes concise analysis of the brain impossible. The resultant accumulated error between the integrated mathematical models and the brain functions would not be controllable due to inaccurate estimation of each model for each type of neurons. To understand the brain and to discover its secrets seems impossible if Al research focuses solely on the high-level (e.g., brain, regions, pieces levels) systems of the brain. Surprisingly, breakthroughs could happen when the unique properties of fundamental neurons and synapses are considered together with cellular level of AI techniques. AI opens a door to study the secrets of the brain due to its ability of approximating nonlinear functions of fundamental neurons and synapses as well as the systems and subsystems built on top of them."}, {"title": "Foundation of Brain's recursive mechanisms: Four-distinctive-properties of neurons and synapses", "content": "Intriguingly, from the perspective of a divide-and-conquer approach, when we delve down to the indivisible function level of the brain in which pieces and regions cannot be divided further, the human brain turns out to be a remarkably beautiful and elegant recursively structured system, and a groundbreaking approach to analyzing the learning structure of the brain could be developed. Especially, our research work finds that all the following four-distinctive-properties are necessarily put together to consider.\nFour Fundamental Properties of Building Blocks of the Brain:\n1) Two fundamental components: The brain's fundamental signaling and communication functions are primarily driven by two key components: biological neurons and biological synapses.\n2) Unidirectional signal transmission: Both neurons and synapses transfer signal to other neurons in one direction18 (Fig. 1).\n3) Alternative connections in sequence: Biological neurons and biological synapses are alternately connected one after another in sequence.\n4) \u201cAll-or-None Law\u201d: Neurons follow \u201call-or-none law.\" If a neuron responds, it must respond completely19.\nIn a neural circuit formed by a group of neurons which are interconnected by synapses, one of the neurons may send a signal back to some of its initiating neurons. However, this may follow the unidirectional transmission property that signals transfer from one neuron to another in such a linear sequence20. Such a unidirectional signal transmission property and \u201call-or-none law\u201d play a critical role in the learning system of the brain. These four key properties distinguish brains from other types of electrical circuits and systems, and essentially build links and fill up the gap between biological learning and AI. Recapitulating these key properties of the biological neurons and synapses together with the synergy of the renowned learning capabilities of AI21,22,23,24, one could analyze the learning capabilities of the brain recursively with the combinations of divide-and-conquer and bottom-up approaches. Similar analysis can be linearly extended to volume transmission, ehaptic coupling and electrical synapses (gap junctions) as well as direction connections among neurons.\nEvery single synapse is made up of a presynaptic terminal and a postsynaptic terminal. The presynaptic terminal at the end of an axon of a neuron converts the electrical signal (the action potential of a neuron) into a chemical signal (neurotransmitter release) and the neurotransmitter diffusing across the synaptic cleft finally binds to receptors in the postsynaptic terminal. A single neuron may receive thousands of input signals through its dendrite trees. After integrating all the received signals (information) the postsynaptic neuron further decides whether it fires or not based on its own internal action potential mechanism25,26.\nAction potentials depend on special types of voltage-gated ion channels of neurons' membranes20. In most cases, the relationship between membrane potential and channel state is probabilistic and is expected to have a time delay."}, {"title": "General unified mathematical representations of fundamental components: neurons and synapses", "content": "Different from studies on neuron characterstics27,28,29,30,31,32 and neuron dynamics as well as those Al models focusing on divisional functions of pieces /regions / systems in the brain33,34, this paper provides a novel Al twin approach which applies Al models to represent single neurons and synapses without knowing details of conduction of electrical signals in neurons and synapses and related mathematical modelling and molecular behaviors in single neurons and synapses. Hundreds of types of single neurons and their subtypes which are considered encapsulated fundamental building components of the brain could be represented by hundreds of types of corresponding AI models, in which AI works as new materials and components to implement and represent biological neurons. So do synapses.\nIt would be an endless effort to figure out the exact mathematical models and molecular behaviors of huge number of different types of neurons, synapses and their subtypes. However, no matter what types of essential functionality of the spiking representation and neurons dynamics mechanisms a neuron may have, general speaking the functionality of neurons and synapses may be considered piecewise continuous 8,10,35 due to their (lower frequencies) biological responses which are reasonably smooth to received signals. In principle, there may have three fundamental piecewise continuous functions within neurons:\n1) Although the information encoding of neurons is still unknown, the information encoding mechanism and related spiking probabilities could be considered a function which is piecewise continuous.\n2) Time delay variables follow some piecewise continuous mechanisms.\n3) Signal transmission functions of neurons are piecewise continuous if considered together with probability function.\nIn essence, whether a single neuron can be excited into firing depends on the integrated input signals received through its dendrite trees. Neurons adhere to the all-or-none law. In other words, if a single neuron is excited, it will always give a maximal response and produce an electrical impulse of a single amplitude. It gives a maximal response or none at all. In this sense, the input-output relationship functions between the input signals of neuron dendrites and the output signals of axon terminals, which is a composite function of action potential function and axon mechanism, is a nonlinear piecewise function of all the excitatory and inhibitory signals received by the neuron.\nSuppose that a biological neuron (indexed by L) has input signal x\u2081 in its dendrite trees and the output signal of its presynaptic terminal is y\u2081(x) = A\u2081(x), where A\u2081(x) shows the signal transferring functions (including the probability and time delay in such relationship between membrane potential and channel state, see Appendix A) between the input signals of neurons' dendrites and the output signals of axon terminals (Fig. 1B). In essence, the signal transferring functions A\u2081 (x) between the input signals of dendrites and the output signals of axon terminals are naturally (piecewise) continuous as well due to biological reaction properties of neurons.\nThis provides an alternative solution and representation of biological neurons which is different from the conventional spiking representation and neurons dynamics approaches."}, {"title": "Bottom-Up approach: General learning system of the brain and its regions / subsystems represented with AI Twins", "content": "Ensembles of neurons amalgamate into physiological regions and yield functional properties and states in the brain36. The human brain system (including learning, memory, reasoning, thought, feeling, emotion, vision, hearing, etc) is constructed from (piecewise) continuous combination of finite number of the signal transferring relationships of neurons A\u2081(x) and the neurotransmission relationships within their corresponding synapses Sp(x). That is, with bottom-up approach the human brain could be viewed as a straightforward and elegant structure constructed like building blocks from layers and groups of neurons and synapses, each layer and group built upon other layers and groups.\nIn contrast to the commonly recognized and applied spiking representation and neuron dynamics mechanisms for biological neurons, one could leverage the learning and modelling capabilities of AI techniques (instead of mathematical modelling methods) to approximate neurons without requiring detailed knowledge of their biological characteristics. Since both fundamental elements (components) are (piecewise) continuous and they are connected alternatively sequentially in order, with bottom-up recursive approach the human brain system is (piecewise) continuous as well (See Theorem 4 in Appendix C).\nAI approximating single neuron and single synapse\nIt is known that if the hidden node activation function of single-hidden layer feedforward networks (SLFNs) is bounded nonconstant continuous, then any target continuous function f can be approximated by such SLFN with any expected small error21,22,24. That is, given any small positive value 8 > 0, there exists SLFN with f\u2081 with appropriate number L of hidden nodes and properly adjusted hidden node parameters such that |f \u2013 f\u2081| < \u03b4."}, {"title": "Complementary capabilities of AI and the Brain", "content": "With appropriate governance, AI could provide complementary positive support to the ongoing technological revolution, which will ultimately benefit humans.\nAI as positive techniques with appropriate governance\nIn addition to tasks commonly undertaken by both AI and humans, some tasks could be handled by AI alone and some principles in nature could be discovered by AI and humans with the aid of AI as well (Fig. 3)."}, {"title": "AI insights to brain energy saving and nature selection", "content": "In addition to the fundamental problem question whether AI with no restrictions surpasses human intelligence in the end, this paper manages to give answer to two more challenging questions:\n49\n1) Whether error backpropagation (BP) algorithm is used in the brain. BP algorithm is one of most popular mathematical methods used to efficiently update parameters in artificial neural networks especially deep learning. One primary difference between artificial neural networks and human brains lies in how error signals are delivered50. This paper perhaps manages to give the answer to an open challenging question whether BP algorithm is used in the brain. Mathematically derived BP algorithms commonly used in tuning parameters in AI applications require bidirectional information transmissions, e.g., forwarding acquired information and backwarding error signal over the same connections (connections among artificial neurons / computational nodes) (see Appendix D). Although feedback connections are ubiquitous in the brain, the information transmission in both biological neurons and biological synapses of the human brain may be unidirectional. Thus, unlike artificial computational nodes and connections in artificial neural networks, it may be difficult to backward errors over biological neurons and biological synapses one by one although backward may appear in synapses or axons50,51,52,53. BP algorithms which require bidirectional information transmissions may not be feasible over sequentially linked unidirectional biological components (neurons and synapses) in the brain. BP algorithm involves a substantial number of calculations and iterations in order to converge, resulting in high energy consumptions. For instance, the average adult human brain consumes approximately 20 watts per day, whereas the frontier supercomputer requires 21 megawatts per day - equivalent to the power supply of about one million humans although performing quite different tasks54. Consequently, one plausible explanation for the absence of BP algorithm in the human brain is the necessity of energy efficiency. Such a high energy demand would not be viable for survival. This raises the intriguing question: how does the human brain adjust its \u201cparameters\u201d (including its \u201cstructures\u201d) to adapt to new information or correct errors? The human brain evolves over generations 8,9,55, retains advantageous biological traits or \u201cparameters\u201d to enhance survival and reproduction. Each generation fine-tunes its brain which is pre-trained in its ancestors. In essence, the human brain has evolved and adjusted its 'parameters,' including its structures, over millions of years through the process of natural selection and adaptation. This evolutionary process spans many generations, in stark contrast to the relatively short-term adjustments and rapid learning that occur within Al systems. One of the objectives of the AI training process assisted with BP algorithms is to learn knowledges which the human brain has achieved in its the extensive evolutionary journey into a much shorter period. Evolution mechanism of human brain intelligences shed lights on energy efficient implementation of artificial intelligence.\n2) Why spikes are used in the brain. This paper manages to provide an intriguing interpretation of spikes for neurons, which may be attributed to natural selection. Since all neurons and synapses with unidirectional transmission functionalities are sequentially alternatively connected one by one in order, this implies that neuron spiking encoding mechanism is possibly like frequency modulation. It is known that frequency modulation is more efficient than amplitude modulation. Such frequency modulation of encoding mechanism of biological neurons has several advantages:\n\u2022It allows signal to be transferred over long distance with large numbers of neurons and synapses in the brain. There is no amplitude attenuation issue in such encoding mechanism.\n\u2022It is more noise resistant and energy efficient than amplitude modulations.\n\u2022Synapses can effectively pass signals among neurons."}, {"title": "AI as new subject domain of the nature", "content": "AI techniques grow exponentially mainly due to learning capabilities of artificial neural networks. Such networks may be composed of different types of artificial neurons, mathematical basis (e.g., Fourier series) 22,40,41,56, and different types of mathematical approaches (e.g., attention etc.)39. Analogous to circuits systems, contemporary neural network-based AI is giving rise to Networked Mathematics, a novel mathematical discipline focusing on interconnected networks (Fig. 2 and Fig. 6) of basis functions / computational nodes (including artificial neurons, smart materials, biological neurons and synapses in the brain) and Al agents / modules /systems, alongside a new field of Intelligence."}, {"title": "AI as new alternative technique for neuroscience and brain illness", "content": "This paper opens new doors for AI techniques to be used in cellular level of efficient neuroscience dynamic analysis and brain illness solutions.\n1) A huge number of activity patterns in the brain build the foundation for adaptive behaviors. Understanding the complex neural dynamics mechanism with conventional approaches by which the human brain's hundred billion neurons and hundred trillion synapses manage to produce cortical configurations in a flexible manner continues to be a core challenge in neuroscience. Divide-and-conquer and bottom-up based Al approaches are used simultaneously to potentially provide efficient solutions for neuroscience and brain analysis. Al twins and other AI technologies could be used to model large types of cell level of neurons and synapses and are then further used to learn the normal physiologic functions (including facilitatory and inhibitory signals) of each well-defined anatomic brain region (such as frontal lobe, midbrain, cerebellum, etc.) with integrated signals from the whole brain under varied external stimuli. Such AI techniques together with smart materials and AI chips techniques lead to AI neurons, AI synapses, AI regions, and AI systems respectively.\n2) Most AI models focus on system implementation of different applications33,34,57,58,59. In this scenario, infinite numbers of functions in the brain would require infinite number of corresponding AI models, each likely with black-box architectures. This paper emphasizes AI techniques at the cellular level and proposes AI twins of the brain. Theoretically, just like the brain, infinite number of functions could arise from its white-box Al twins. Al twins could offer alternative solutions to brain disorders at the level of individual neurons, synapses, or small regions of the brain, provided that ethical concerns are addressed. In essence, AI theoretically has the capability of representing and substituting malfunctioning biological neurons and synapses in the brain (Theorem 1 and Theorem 2 in Appendix B). Priority for applications of alternative efficient solutions for brain diseases should ideally focus on those that specifically damage brain regions where carefully designed cell-level nanometer (and smaller) size of AI chips could help restore abnormal or mimic physiologic functions with bottom-up approaches from micrometer size of neurons and nanometer size of synapses to pieces, regions, systems of the brain. Understanding the neurons level detailed structure information is critical for the future realization of such Al neuron replacement therapy, which depends on accurate measure of gain and loss of function at cell level. If brain structures could be scanned at the neural level, as demonstrated by recent developments and research efforts11,60, this could potentially serve a similar role to today's MRI or CT scans for the identification of damaged or diseased neurons or areas, which could then be potentially replaced by AI components as described in this paper. AI could potentially detect or identify \u201cgain of function\u201d or \u201closs of function\u201d signals at the neurons, synapses and proteins using smart materials and computational nodes."}, {"title": "Appendices", "content": ""}, {"title": "Function representations of biological neurons and biological synapses", "content": "Conventional mathematical models aim to explain the functional and operational mechanisms of biological neurons. It is not difficult to see that, even similar or single type of such models used for all neurons in the brain, the effort spent on combining billions of such mathematical models for billions of neurons in the brain would become endless and manpower intensive.\nHowever, considering biological neurons and synapses are connected alternative in sequence, AI techniques could be used to represent biological neurons and synapses due to its universal learning capabilities21,22,24\nInstead of focusing details of mathematical representations of spiking neurons and neuron dynamics molecular behaviors within neurons and synapses, both neurons and synapses have their essential models which are determined by their internal molecular behaviors. This paper considers that the function of each type of neuron and synapse are (piecewise) continuous and thus applies AI technique to model and represent the function of each type of neuron and synapse.\nDefinition 1 61\nA function in one-dimensional space is piecewise continuous if it has only a finite number of discontinuities in any interval and its left and right limits are defined (not necessarily equal) at each discontinuity."}, {"title": "AI representations of biological neurons and biological synapses", "content": "Hornik21 rigorously proved a theorem showing that if the activation function is bounded nonconstant continuous, then any target continuous function could be approximated by single-hidden layer feedforward networks (SLFNs) over compact input sets. Without loss of generality, single-hidden layer feedforward networks (SLFN) are used in the basic theoretical analysis in this paper and all the analysis could be replaced with more efficient AI models whenever necessary.\nAI approximating single neuron and single synapse\nLet L\u00b2 (X) be a space of functions f on a compact subset X in the d-dimensional Euclidean space Rd such that |f|\u00b2 are integrable. The norm in L\u00b2(X) space will be denoted as |\u00b7 |, and the closeness between the network function f\u2081 and the target function f is measured by the L\u00b2(X) distance:\n|f\u2081 \u2212 f | = [[x|f\u2081(x) \u2212 f(x)|2 dx]1/2. A space X being a compact subset X of Rd means that such space X is closed and bounded. A set being closed can be visualized as a set where all convergent sequences converge to a point in the set. The meaning of bounded is that there is an upper limit for the function.\nLemma 1 (Theorem 2 21)\nIf g is continuous, bounded and nonconstant, then g(ax + b) is dense_in C(X) for all compact subset X of Rd, where a\u00b7x + b represents weighted summation of the input vector and C(X) denotes the set of all continuous functions on compact subset X.\nThat is, given any small positive value \u03b4 > 0, there exists f\u2081 with appropriate number L of hidden nodes and properly adjusted hidden node parameters (ai, bi) such that |f - f\u2081| = |f \u2013 \u03a3\u03af=1 \u03b2i g(ai \u2022 x + b\u2081)| < \u03b4, where \u03b2\u012f is the weight of the connection between the hidden node i to the output node.\nHornik's theorem\u00b2\u00b9 is valid for any target piecewise continuous function f and also any continuous, bounded and nonconstant activation function g. According to Lemma 1, given any input-output relationships of single neuron A\u2081(x) and the neurotransmission relationships within their corresponding synapses Sp(x), which are piecewise continuous, there exists an Al model (e.g., artificial feedforward neural networks) universally approximating them. In other words, the two fundamental computational elements (components) which form the human brain learning systems could be represented by single-hidden layer feedforward networks (SLFN) or a more efficient artificial neural network developed from and constructed by SLFNs with any small error.\nAlthough without loss of generality, SLFNs are often employed in theoretical proofs, the realm of more efficient AI networks allows for combinations of basic SLFNs with diverse types of artificial neurons. Such artificial neurons could be convolutional neurons and many other types of neurons which are often used in different AI implementations nowadays. Since both fundamental input-output relationships of single neuron A\u2081(x) and the neurotransmission relationships within their corresponding synapses Sp(x) are (piecewise) continuous, they could be represented by Al with networks of artificial neurons. Thus, we have"}, {"title": "Theoretical proof of representation capabilities of bottom-up AI techniques", "content": "Neurons adhere to the all-or-none law that if a neuron responds it must respond completely. In terms of mathematical representation, such all-or-none smoothness law could be covered by the following definition:\nDefinition 3\nA function f is called all-or-none smoothness if given any value c\u2208 (minf (x), max f (x)) and positive small value 8, there exists zero or finite number of points X1, X2, \u2026, Xn such that:\n1) f(x) = c, and\n2) for any small |\u0394| < \u03b4, there exist xi,1 and xi,2 in regions (x\u012f \u2013 \u0394, xi + \u2206) such that f(xi,1) > c and f(xi,2) < c.\nThese finite points are called irregular points.\nTheorem 3\nSuppose that all the functions f, f\u2081,\u2026, fk are all-or-none smoothness piecewise continuous, then f (f1, f2,\u2026, fk) are all-or-none smoothness piecewise continuous.\nProof: Given any point x*and any small value \u03b4 > 0. As f (X1, X2, \u2026, Xk) is piecewise continuous, there exists \u2206 >0 such that |f(x) \u2212 f(x*)| < 8 when |x \u2212 x*| < \u0394. As f1(x),\u2026, fk(x) are piecewise continuous, there exists small value \u0394' > 0 such that |f(x) \u2212 fi(x*(| > \u221aA/k when |x \u2212 x*| < \u0394'. Thus, f (f1(x), f2(x), \u2026, fk (x)) \u2212 f (f1(x*), f2(x*),\uff65\uff65\uff65, fx(x*))|< 8 when |x- x*| < \u0394\u0384.\nSince f, f\u2081,\u2026, fk are all-or-none smoothness, apparently f(f1, f2, \u2026, fk) is all-or-none smoothness as above defined due to finite irregular points. This completes the proof.\nTheorem 3 shows a general fact that when all-or-none smoothness piecewise continuous functions are combined using another all-or-none smoothness piecewise continuous function, the resultant composite function retains the all-or-none smoothness piecewise continuous property. This obviously includes the cases when biological neurons and synapses are sequentially connected alternatively one by one in order when Theorem 3 is applied recursively in such sequences.\nTheorem 4\nThe human brain and any of its constituent regions constructed by sequentially linked neurons and synapses in the brain are all-or-none smoothness piecewise continuous."}, {"title": "Essence of BP algorithm", "content": "Error backpropagate (BP) algorithm49 is used to adjust parameters of artificial neural networks. BP algorithm is essentially an iterative gradient descent mathematical approach. BP algorithm consists of two steps: forward computation of signals from the input layer to the output layer, layer by layer, and backward computation of errors from the output layer to the first layer through hidden layers, layer by layer.\nForward computation\nin\u2081 = \u2211W;,;a;\nj\nwhere ajis the output of node j, Wj,i is the weight of the connection from node j in the preceding layer to node i in the next layer, and in\u012f is the input of node i.\nBackward computation\nUntil performance is satisfactory, weights of connections in artificial neural networks are adjusted through examples iteratively over multiple epochs:\nOutput nodes:\nWj,i \u2190 Wj,i + a \u00d7 a\u00a1 \u00d7 \u0394\u00a1\n\u0394\u2081= Err\u00a1 \u00d7 g'(in\u012f)\nwhere g is the activation function of computational nodes, Err\u012f is the error between the expected target and the true output of the output node i, \u0394\u2081 is the error resulted from the input signal of node i.\nHidden nodes"}, {"title": "Potential appropriate Al governance approach", "content": "In general, Al grows exponentially. AI would inevitably have serious impact on social order and ethics63. AI would surpass human brain capabilities in the end if there were no proper restrictions and governances in the aspects of either legal and ethical regulations or technologies (Fig. 8 and Fig. 9). From the perspective of technologies, it is not straightforward to have restrictions on those key properties which are going to become more and more invisible in most cases, e.g., algorithms (and numbers of their parameters), data sources, smart materials, AI agents, and knowledge exchanges for AI. AI algorithms may self-generate new algorithms with varying architectures and models, data are complicated and invisible in most cases, smart materials are becoming popular in Al implementations, knowledge exchange among AI models and AI agents are not easily controllable. Out of the above mentioned six key properties, computing power may be readily manageable, measurable and explainable to humans in two aspects:\n1) The upper bound of computing capacities.\n2) The upper bound of number of neurons of AI models in which artificial neural networks play significant roles or equivalent scales in other types of AI models (in terms of number of neurons)."}]}