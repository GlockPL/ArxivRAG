{"title": "Synthetic Trajectory Generation Through Convolutional Neural Networks", "authors": ["Jesse Merhi", "Erik Buchholz", "Salil S. Kanhere"], "abstract": "Location trajectories provide valuable insights for applications from urban planning to pandemic control. However, mobility data can also reveal sensitive information about individuals, such as political opinions, religious beliefs, or sexual orientations. Existing privacy-preserving approaches for publishing this data face a significant utility-privacy trade-off. Releasing synthetic trajectory data generated through deep learning offers a promising solution. Due to the trajectories' sequential nature, most existing models are based on recurrent neural networks (RNNs). However, research in generative adversarial networks (GANs) largely employs convolutional neural networks (CNNs) for image generation. This discrepancy raises the question of whether advances in computer vision can be applied to trajectory generation. In this work, we introduce a Reversible Trajectory-to-CNN Transformation (RTCT) that adapts trajectories into a format suitable for CNN-based models. We integrated this transformation with the well-known DCGAN in a proof-of-concept (PoC) and evaluated its performance against an RNN-based trajectory GAN using four metrics across two datasets. The PoC was superior in capturing spatial distributions compared to the RNN model but had difficulty replicating sequential and temporal properties. Although the PoC's utility is not sufficient for practical applications, the results demonstrate the transformation's potential to facilitate the use of CNNs for trajectory generation, opening up avenues for future research. To support continued research, all source code has been made available under an open-source license.", "sections": [{"title": "I. INTRODUCTION", "content": "Due to the omnipresence of sensor-equipped devices like smartphones, large quantities of location data are collected daily. These trajectory datasets offer the potential for various use cases, such as public transport optimisation and pandemic control. However, location data yields severe privacy implications as it allows the re-identification of individuals [1], [2] and the inference of personal attributes [3], [4]. To highlight the high information content of location traces, De Montjoye et al. [1] managed to identify 95% of users in a mobile phone dataset based on four locations only. Moreover, researchers determined Islamic taxi drivers in an anonymised dataset by correlating the mandatory prayer times with their breaks [5]. Researchers have proposed various methods for the privacy-preserving release of trajectory datasets [3], [6], [7], [8]. These methods focus on syntactic privacy, such as k-anonymity [9], and semantic privacy, notably Differential Privacy (DP) [10]. DP has become the de facto standard due to its strong formal guarantees. However, these methods involve a privacy-utility trade-off, requiring data perturbation to ensure privacy [11]. Current approaches often fail to maintain sufficient utility in the protected data for effective analysis [12], [13], [14]. Inappropriate protection can also lead to structural anomalies, such as cars not adhering to roads or ships traversing land [15], [16], [17], [8]. These anomalies may facilitate reconstruction attacks, thus reducing the achieved privacy level [18], [16]. For these reasons, Liu et al. [19] proposed using Generative Adversarial Networks (GANs) to generate synthetic trajectory datasets. The Deep Learning (DL) model retains the original dataset's key characteristics but produces inherently private, synthetic data. Models such as LSTM-TrajGAN [12] show the potential utility of this method. Yet, these solutions lack strong privacy guarantees. There are risks of the model memorising and replicating real trajectories with minimal alterations. To guarantee that the model does not remember training data, the usage of DP-SGD [20] has been proposed [21]. However, LSTM-TrajGAN uses a real trajectory as input, unlike standard GAN models like Deep Convolutional Generative Adversarial Network (DCGAN) [22] that use Gaussian noise. DP-SGD only secures training data, thus it is ineffective for architectures that rely on real data during generation [21]. To incorporate DP-SGD in trajectory GANs, a noise-only model is required. As trajectories represent sequences of locations, Recurrent Neural Network (RNN)-based models are the obvious choice. However, most GAN research has centred on computer vision, focusing on Convolutional Neural Network (CNN)-based models. RNN-based GANs often train less stably than CNN models and struggle with convergence [21]. This raises the question of whether a CNN-based architecture could be utilised for trajectory generation. In other sequential domains, the move from RNN-based models to CNN-based models showed success. Wave-GAN [23] adapts DCGAN for audio data generation, employing Conv1D layers and outperforming an RNN-based model. Similarly, PAC-GAN [24] utilises a CNN-GAN for the generation of network traffic packets where most related work had used RNNs. These examples highlight the potential of CNN-based architecture's in sequential domains. This work introduces a Reversible Trajectory-to-CNN Transformations (RTCT) to facilitate the use of CNN-based"}, {"title": "II. BACKGROUND", "content": "This section lays out the background knowledge for the remainder of this paper. Section II-A defines a trajectory dataset. Section II-B provides an overview of DP, emphasising its application in deep learning via DP-SGD. In Section II-C, we explore Generative Adversarial Networks. Section II-D provides further details on the DCGAN architecture."}, {"title": "A. Trajectory Datasets", "content": "A trajectory dataset consists of a number of trajectories: DT = {T1,\u2026\u2026\u2026, Tn}. Each trajectory Ti itself represents an ordered sequence of locations Ti = (li1,..., lin), where each location consists of multiple attributes. We assume the minimal information content of a location to be spatial coordinates such as latitude and longitude, i.e., lij = (latij,lonij). However, locations might be enriched with additional information. For instance, some datasets record altitude [26], while others contain temporal information [26], [25], or semantic information such as Point of Interests (POIs), i.e., the type of location. Within this work, we assume spatial details to be minimally present. Temporal and semantic information are optional and can extend all proposed approaches."}, {"title": "B. Differential Privacy", "content": "Differential Privacy (DP) [10] is a rigorous semantic privacy notion. In contrast to syntactic privacy notions like k-Anonymity, DP offers protection against any adversary independent of their background knowledge. DP is founded on the principle of plausible deniability, i.e., participation in a query should not (significantly) affect the outcome. Accordingly, participation in a dataset does not harm one's privacy. Mathematically [10]:\nDefinition 1 (Differential Privacy): A mechanism K provides (\u03b5, \u03b4)-differential privacy if for all neighbouring datasets D\u2081 and D2, and all S \u2286 Range(K) holds\n P[K(D\u2081) \u2208 S] < e^{\u03b5} \u00d7 P[K(D2) \u2208 S] + \u03b4 \nIn the original definition, neighbouring means that dataset D2 can be obtained from D\u2081 by removing all user records u from the dataset: D2 = D1\\{rxi|X = u}. In practice, different neighbourhood definitions, also called unit of privacy [21], are deployed. Most commonly, especially in machine learning, instance-level DP is used, which assumes that each user"}, {"title": "C. Generative Adversarial Networks", "content": "Generative Adversarial Networks (GANs) [32] are a DL algorithm used in unsupervised machine learning. These networks consist of two main components: a generator (G) and a discriminator (D). G generates data from random noise, aiming to mimic real data samples. D assesses whether a sample is real or produced by G. This interaction forms a competitive framework, described by the adversarial loss:\n min_{G} max_{D} E_{x~P_{data}} [log D(x)] + E_{z~p_{z}} [log(1 \u2013 D(G(z)))]\nIn this equation, Pdata is the real data distribution, and p\u2082 is the distribution of the noise input for G. The training process refines G and D until G can produce data almost identical to real samples. One advantage of traditional GANs architecture in regard to privacy is that the generator receives only random noise as an input during generation. Real data samples in GANs are necessary only during the training phase. This characteristic facilitates seamless integration with DP-SGD. In contrast, accessing the real data during generation can lead to privacy leakage [21]. Building on this understanding of GANs, we explore the well-known DCGAN architecture next."}, {"title": "D. DCGAN", "content": "Most current GAN models are loosely based on DC-GAN [22], owing to its effectiveness in generating valid synthetic data across various applications [33]. DCGAN is a GAN utilising convolutional layers in both its generator and discriminator that was developed for image generation. Its five key architectural features include:\nStrided and transposed convolutions for downsampling and upsampling, replacing pooling layers.\nBatch normalisation in both generator and discriminator.\nReLU activation in the generator, except for tanh in the output layer. Note that we used sigmoid for the output layer due to our normalisation to [0; 1].\nLeakyReLU activation in the discriminator, except for sigmoid in the output layer.\nElimination of Fully Connected (FC) hidden layers.\nThese modifications lead to DCGAN's training stability and broad applicability."}, {"title": "III. METRICS", "content": "This section outlines the metrics for our utility evaluation. We first discuss two metrics for evaluating spatial distribution: HD (Section III-A) and WD (Section III-B). We then describe TTD (Section III-C) for sequential quality and introduce the novel TRR (Section III-D) to assess temporal properties."}, {"title": "A. Hausdorff Distance", "content": "The Hausdorff Distance (HD) is a distance metric that quantifies the similarity between two sets of points. It calculates the maximum distance from a point in one set to the nearest point in the other set. Formally, for two point sets A, B and a distance function d [34]:\n HD(A, B) = max { max_{a\u2208A} min_{b\u2208B} d(a, b), max_{b\u2208B} min_{a\u2208A} d(a, b) } \nThis metric is widely used for evaluating spatial utility in trajectory protection and generation [35], [36], [37], [12], [14]. We generate trajectories until their combined point count matches the real test set's point count. Then, we compute the HD between both sets. However, the HD is significantly affected by outliers [21], such that we also use the WD."}, {"title": "B. Wasserstein Distance", "content": "The HD is prevalent in trajectory comparison but is highly susceptible to outliers, where a single misplaced point can lead to a drastically different HD. To address this, we compare the point distributions directly as proposed in [21]. A key aspect is assessing if the generated data mimics the real data's distribution, such as urban concentration versus rural sparsity. Intuitively, the Wasserstein Distance (WD) treats distributions as piles of dirt, measuring how much 'dirt' must be shifted to transform one distribution into another. Hence, this metric is also known as Earth Mover's Distance (EMD). However,"}, {"title": "C. Total Travelled Distance", "content": "The Total Travelled Distance (TTD) represents the length of the entire trajectory. For a trajectory t = ((X1,Y1), (X2,Y2), ..., (Xn, Yn)), the TTD is defined as:\n TTD(t) = \u2211_{i=1}^{n-1} \u221a(xi+1 - Xi)^2 + (Yi+1 \u2013 Yi)^2 \n The TTD is calculated by computing the total travelled distance for each trajectory in both the real and generated datasets, creating two sets of travel distances. We then use the WD to compare these sets by measuring the discrepancy between their underlying distributions, quantifying how similar the generated data is to the real data regarding travel distances."}, {"title": "D. Time Reversal Ratio", "content": "We identified a lack of metrics assessing the quality of the temporal features in generated trajectories. To address this gap, we developed a new metric, Time Reversal Ratio (TRR), which assesses how accurately a trajectory represents the flow of time. The goal of TRR is to ensure that generated trajectories display minimal instances where time appears to move backwards, mirroring the consistent forward progression of time in real-life trajectories. Formally, for a trajectory t = ((x1, y1, t1), (X2,Y2, t2), ..., (Xn, Yn, tn)) with timestamps ti, TRR is defined as:\n TRR = ( \u2211_{i=1}^{n-1} I(ti > ti+1) ) / (n-1) \nHere, I is the indicator function, equalling 1 if ti > ti+1 (indicating a regression) and 0 otherwise. While minor errors in location points are tolerable for trajectory utility, generated trajectories exhibiting backward time movement are readily identifiable as synthetic. Therefore, optimising against such temporal inconsistencies is essential to enhance realism."}, {"title": "IV. RELATED WORK", "content": "Numerous privacy-preserving mechanisms for trajectory data exist. Jin et al. [7] survey various approaches, including those based on k-Anonymity, while Miranda-Pascual et al. [8] focus on DP mechanisms. Despite significant progress, a privacy-utility trade-off persists [12], [13], [14]. k-Anonymity offers practical privacy but lacks robust formal guarantees. Conversely, DP mechanisms cause significant utility loss. Generating synthetic trajectories via deep learning emerged as a potential alternative [19]. Notable implementations include the aforementioned LSTM-TrajGAN [12]. Numerous other DL-based generative models have been proposed [38], [39], [40], [41], [42], [43], [44], [45] and discussed in a recent SoK [21]. Yet, none of the existing deep learning-based generative models can provide strong formal privacy guarantees, such as DP [21]. More recent approaches not considered in this SoK, such as DiffTraj [46], improve utility further but still do not offer formal privacy guarantees and rely on the inherent privacy of generation instead. However, generated data can leak information such formal guarantees remain desirable [21], [47], [48]. Moreover, Buchholz et al. [21] mention that Conv1D layers appear to be superior for capturing the spatial distribution of trajectories compared to RNN-based models. This motivates our investigation of CNN-based generative models.\nLSTM-TrajGAN [12], [49] represents the most cited deep learning-based generative trajectory model. The model consists of a generator and discriminator with similar architectures. Real trajectories are normalised and encoded, combining location, temporal, and semantic features. The generator embeds and fuses these encodings with noise into latent represen-tations. This representation is processed by an Long Short-Term Memory (LSTM) layer, generating synthetic trajectories with multiple features. Unlike the classic GAN architecture (Section II-C), where the generator only receives Gaussian noise as input, LSTM-TrajGAN uses real trajectories as input during generation. As pointed out in [21], this has privacy implications and might lead to the generated trajectories leaking information about the input trajectories. Moreover, this architecture makes it challenging to integrate DP guarantees. Therefore, we use a variant of LSTM-TrajGAN, named Noise-TrajGAN (NTG) [21], as our evaluation baseline. Unlike the original, NTG omits trajectory inputs during generation. Its architecture mirrors LSTM-TrajGAN's, with the primary difference being that its generator uses a noise vector alone. We made the choice to use NTG for two reasons. First, NTG can be trained with DP guarantees by using DP-SGD (ref. Section II-B, which allows us two compare a DP version of our PoC to DP-NTG. Second, the architecture is more similar to DCGAN, which also uses noise-only input, such that the results focus more on the actual research question of comparing RNN with CNN-based models.\nSequence Generation Based on CNN. The preference for RNNs in trajectory generation stems from their suitability for sequential data. Yet, CNN-based models have demonstrated effectiveness in sequential tasks. For example, WaveGAN [23], uses Conv1D for audio generation, and PAC-GAN [24], employs a 2D CNN for network traffic generation.\nWaveGAN, adapts DCGAN (ref. Section II-D) to 1D data by using Conv1D layers for the generation of synthetic audio waves [23]. Despite the sequential nature of audio waves, this model produces high-quality audio suitable for multimedia applications. WaveGAN surpasses earlier models like SampleRNN [50] and WaveNET [51], highlighting the effective-ness of Conv1D layers. The model exploits the periodicity of"}, {"title": "V. PROBLEM STATEMENT", "content": "As noted in Section I, location trajectories are valuable for analyses but contain sensitive information. Related work indicates a limiting privacy-utility trade-off in traditional pro-tection methods. Therefore, the generation of synthetic data represents a promising alternative. However, while achieving high utility, current models struggle to provide rigid privacy guarantees [21]. Moreover, while most current methods rely on RNNs, CNN-based architectures have shown success in other sequential domains, and initial experiments with convolutional layers for trajectory generation yield promising results [21]. This leads to our research question: Can a CNN-based GAN produce high-quality synthetic trajectories? To the best of our knowledge, this is the first attempt to utilise a fully CNN-based architecture for trajectory generation. Our primary research question is subdivided into four sub-goals:\nG1: Transformation: Develop a Reversible Trajectory-to-CNN Transformations (RTCT) algorithm that transforms lo-cation trajectories into CNN-compatible inputs. This transfor-mation represents the main contribution of this research.\nG2: Integration: Integrate our transformation with a standard CNN-based GAN, specifically DCGAN (ref. Section II-D), to assess its performance in a PoC. We have chosen an estab-lished GAN model to highlight the flexibility and potential of our proposed transformation. The development of a specialised CNN-based GAN tailored for trajectory synthesis remains an objective for future work.\nG3: Differential Privacy: Apply DP-SGD to both our model and the considered baseline model NTG to determine the impact of formal privacy guarantees on model performance. To the best of our knowledge, this is the first work using DP-SGD for privacy-preserving trajectory generation, providing formal privacy guarantees.\nG4: Evaluation: Evaluate the potential of CNN-based GANs for trajectory generation by conducting an extensive evaluation on two real-world datasets, FS-NYC and Geolife, with the four metrics outlined in Section III. Additionally, we compare our model's performance with the RNN-based model NTG, de-scribed in Section IV. Moreover, we compare the performance of both models trained with DP-SGD to measure the impact of formal privacy guarantees on utility."}, {"title": "VI. REVERSIBLE TRANSFORMATION DESIGN", "content": "To enable the usage of CNN-based GANs for trajectory generation, we propose a Reversible Trajectory-to-CNN Trans-formations (RTCT) addressing G1: Transformation. This transformation, depicted in Figure 1, comprises:\nNormalising the trajectories' latitude, longitude, day and hour values (Section VI-A).\nInserting these normalised values into a 12\u00d712\u00d73 matrix (Section VI-B).\nUpsampling the resulting matrix to 24 \u00d7 24 \u00d7 3 (Section VI-B).\nAfter generation, the synthetic trajectories are reverted into sequential form, as detailed in Figure 2 and Section VI-C. Additionally, we demonstrate the potential of using CNN-based models for trajectory generation by integrating our trans-formation with DCGAN [22] into a Proof-of-Concept (POC) implementation, addressing G2: Integration (Section VII)."}, {"title": "A. Normalisation", "content": "Normalisation adjusts dataset features to a common scale, which is essential for deep learning. Typically, data is nor-malised to the range [-1;1] or [0;1] using techniques such as mean and standard deviation, and tanh normalisation [52]. We employ min-max normalisation, which is reversible and scales data features into the range [0; 1], making it ideal for the denormalisation of generated trajectories. For a feature f, with minimum value min and maximum value max, the normalisation value v is defined as:\n \u03c5 = (f-min) / (max - min)\nThis normalisation method is applied to all features before using the trajectories for model training.\nChoice of max and min. Selecting appropriate maximum and minimum values for each feature is crucial for effective normalisation. Initially, we used global extremes for latitude and longitude: -180\u00b0 to 180\u00b0 for longitude and -90\u00b0 to 90\u00b0 for latitude. This method proved too coarse, as slight changes in coordinates, which could represent significant distances, were not adequately captured. For example, a change from (lat, lon) = (40,70) to (40.0001,70) represents a distance of approx. 10 meters but is negligible in global normalisation. Moreover, the values have to be dataset-independent to prevent privacy leakage. Finally, we opted for geographical constraints tailored to the datasets, such as a ring road for a Beijing dataset or a bounding box aligned with a city's official boundaries. This approach does not access the dataset but uses public knowledge, thus preventing information leakage. For time-based features, we applied fixed maxima and minima, 0-6 for days and 0-23 for hours, aligning with the natural constraints of these temporal features. This ensures sufficient variability for the model to detect patterns effectively."}, {"title": "B. Trajectory Encoding", "content": "Our trajectory encoding underwent two main iterations. Initially, we used an asymmetrical matrix as a \"strip\" to"}, {"title": "VII. DCGAN INTEGRATION", "content": "As highlighted in Section II-D, DCGAN is a popular GAN choice due to its ease of use and versatility. Therefore, we integrated DCGAN with RTCT as a Proof-of-Concept. Al-though DCGAN was primarily designed for image generation and may not be ideal for trajectories, its adaptability makes it suitable for our PoC addressing G2: Integration, as this work focuses on transformation rather than the generative model itself. In the following, we provide details on the implementation and optimisation strategies."}, {"title": "A. Implementation", "content": "We based our PoC on the DCGAN implementation from the PyTorch GAN repository [54]. PyTorch data loaders facilitate dynamic access to data, crucial for training. Our transforma-tion is embedded in the data loader, managing loading raw trajectories from files to the encoding with RTCT.\nPadding and Masking. As described in Section VI-B, CNNs require constant length inputs, unlike RNNs. Therefore, we pad trajectories shorter than the upper limit of 144 points using 0-post padding. Masks based on the trajectories' original lengths are generated, with a vector for a trajectory of length I comprising I ones and 144 I zeros. When applied to trajectories, this masking excludes zero-padding from affecting computations within the computational graph."}, {"title": "B. Optimising DCGAN", "content": "To enhance the baseline PyTorch DCGAN performance, we employed several optimisation strategies:\nImplementing Two Time Update Rule (TTUR) [55].\nUsing a learning rate scheduler [56].\nApplying label smoothing [57].\nThe Two Time Update Rule (TTUR) [55] refers to using a lower learning rate for the generator than for the discriminator. This optimisation facilitates the discriminator converging to a local minimum while the generator progresses more slowly [55]. We integrated TTUR through a generator factor that reduces the generator's learning rate to one-tenth of the discriminator's learning rate."}, {"title": "VIII. DP-SGD", "content": "As discussed in Section IV, the core limitation of existing deep learning models for trajectory generation are the missing formal privacy guarantees. Today's de-facto standard is DP described in Section II-B. Despite efforts to deploy DP for trajectory generation, studies [8], [21] show frequent mis-application, affecting the integrity of the privacy guarantees. Thus, [21] recommends using the established method of DP-SGD, which ensures instance-level DP if each training sample corresponds to one trajectory (see Section II-B).\nTo address G3: Differential Privacy, we implemented DP-SGD for our PoC and the baseline NTG (ref. Section IX-B) to evaluate the impact of DP and allow for comparative analysis. We employed the Opacus [59] library to integrate DP-SGD due to its straightforward interface. This established framework helps us avoid the common pitfalls of custom DP implementations, which have compromised the integrity of privacy guarantees in other studies [21], [8]. Any layers incompatible with DP-SGD were automatically replaced by Opacus' model fixer, in particular, DCGAN's BatchNorm layers were replaced by GroupNorm. Typically, DP-SGD is applied to the discriminator in a GAN [48]. This approach ensures that the generator also adheres to DP through the post-processing property of DP since it only receives indirect data access via feedback from the discriminator. However, we aimed at enabling usage of the WGAN-LP loss, which is reported to yield better results than the standard adversarial loss [60], [61], [21]. Opacus does not support DP-SGD for models trained with gradient penalty at this time. Moreover, the discriminator is updated more frequently than the generator when using WGAN [62] (usually \u2248 5\u00d7 as often), such that adding noise to the discriminator would result in more noise being added by DP-SGD. Therefore, we decided to train the generator with DP-SGD instead, i.e., the noise is added to the generator's gradients instead of the discriminator's gradients. Utilising the MNIST Sequential (MNIST-Seq) as a toy dataset [21], we confirmed that the baseline model NTG could produce samples of comparable quality with DP-SGD applied to the generator as it did without DP-SGD. Due to a bug in the underlying framework, we had to use the standard WGAN loss instead of WGAN-LP for the DP version of the baseline model NTG. The DP version of our PoC was successfully trained with WGAN-LP.\nTo minimise the performance degradation caused by the noise from DP-SGD, we adhered to the guidelines from Google's DP-fy ML paper [20]. We set \u03b5 = 10.0, considered the upper limit for realistic privacy in deep learning [20]. For a dataset with n samples, we adopted 8 = 1/n1.1 as commonly recommended [28], [20]. Compared to the non-DP model, we increase both the batch size b and the number of epochs e by a factor F = 10 to keep the number of steps (s = e.) constant but increase the batch size which reduces the noise that is added [20]. We selected a gradient clipping norm of C = 0.1. Due to time constraints and the high computational cost of DP-SGD, we could not complete a full ClipSearch [20] and learning rate sweep, but we verified that these heuristics yield good results on the MNIST-Seq dataset."}, {"title": "IX. EVALUATION", "content": "This chapter addresses goal G4: Evaluation. Section IX-A details the datasets used and their preprocessing methods. Section IX-B introduces the baseline model, NTG. Subse-quent sections discuss evaluation results: Section IX-C cov-ers parameter choices, Section IX-D outlines the hardware setup, Section IX-E presents results from standard models, Section IX-F focuses on outcomes from training with DP-SGD, and Section IX-G provides a qualitative analysis."}, {"title": "A. Datasets", "content": "To demonstrate the generalisability of our approach, we evaluate our implementation on two different datasets. First, the FS-NYC dataset [25], used as a benchmark in LSTM-TrajGAN [12], comprises 3,079 trajectories with a maximum of 144 locations each, primarily within New York City's bounds. We use the dataset as provided in [49] without further preprocessing. Second, the Geolife dataset [26] covers a larger geographical area around Beijing. Moreover, 91% of the dataset's trajectory have a sampling rate of 1-5s or 5-10 m [26] yielding more fine granular trajectories. For con-sistency with FS-NYC, we constrained Geolife's data within the fourth ring road of Beijing4. We capped trajectories at 144 locations and discarded those with fewer than 96 locations to prevent extensive padding. This preprocessing reduced the available trajectories from 17621 to 7270."}, {"title": "B. Baseline Model", "content": "LSTM-TrajGAN, as highlighted in Section IV, is a leading generative model for trajectories. Its use of real trajectory inputs during generation, however, raises privacy concerns and complicates the provision of DP guarantees [21]. To address these issues, we selected the noise-only variant NTG, introduced in [21], for its compatibility with DP-SGD and its architectural similarity to our PoC, ensuring a fair comparison. Despite NTG's inferior performance compared to LSTM-TrajGAN, it has the advantage that it integrates well with DP-SGD, an essential feature for our comparative analysis."}, {"title": "C. Hyperparameters", "content": "This section outlines the training parameters used for the evaluations. Both our PoC and the baseline model were trained for over 10000 steps for consistency, with a batch size of 64. The learning rate for our PoC was set to 2e-4. While we implemented the TTUR as described in Section VII-B, we empirically determined that a generator factor of 1.0, updating the generator and discriminator at the same fre-quency, yielded the best results. A learning rate scheduler, referenced in Section VII-B, reduces the learning rate by a factor of 0.1 after 4000 steps, and label smoothing is applied to the discriminator. Contrary to NTG, which employs the WGAN-LP loss, DCGAN showed superior performance with the standard adversarial loss (ref. Section II-C). However, when training with DP-SGD, the WGAN-LP loss with 5 discriminator iterations per generator iteration proved superior for DCGAN. The generator in our PoC accepts noise in-put shaped (batch_size, 100). Network dimensions otherwise align with the DCGAN specifications from the PyTorch GAN repository [54]. All remaining hyperparameters follow the PyTorch defaults. Hyperparameters for the baseline model are outlined in Section IX-B. During DP-SGD training, most parameters were unchanged, with modifications to batch size, epochs, and learning rates as specified in Section VIII."}, {"title": "D. Evaluation Setup", "content": "All measurements were performed on a server (2x Intel Xeon Silver 4310, 128 GB RAM) with Ubuntu 22.04.4 LTS. The server contains 4 NVIDIA GeForce RTX 3080 (10 GB"}, {"title": "E. Results", "content": "This section presents a comparative utility evaluation be-tween our PoC and NTG. The results of both models on the Geolife dataset are displayed in Figure 3, and those on FS-NYC in Figure 4. Examples of one generated trajectory and the generated point distributions are provided in Figures 5 and Figure 6, respectively. All results are reported at the end of the training, i.e., after approx. 100000 steps.\nHausdorff Distance: The HD measures the maximum disparity between two sets of points (ref. Section III-A). A lower HD indicates closer similarity between the compared sets, thereby indicating better performance. On Geolife, our approach achieved a HD of 0.0646. NTG, with a HD of 0.120, exhibited 1.87 times worse performance. On FS-NYC, our PoC still performs better, but only by a factor of 1.45.\nSliced Wasserstein Distance: The SWD was employed as a second metric to evaluate the spatial distribution utility. It assesses the shape, size, and density of the points within a generated dataset, with lower scores indicating a higher re-semblance to the real dataset. The results confirm the findings of the HD, that our PoC is superior at capturing the spatial distribution of the dataset. On the Geolife dataset, DCGAN performs 3.64 times better, and on FS-NYC, 1.34 times.\nTotal Travelled Distance: The TTD (ref. Section III-C) compares the distribution of travel distances in a generated dataset against the real-world dataset via the WD. Lower values indicate a greater resemblance to the actual dataset. On the Geolife dataset, NTG outperforms the PoC by a factor of 2.5. This outcome is expected, as the TTD measures the distance between consecutive locations and depends on the model's ability to capture the sequential dependency of these locations. Since RNNs excel at capturing sequential properties, it appears reasonable that they outperform the CNN-based model in this metric. However, on the FS-NYC dataset, NTG performs significantly worse than DCGAN in terms of TTD, which is surprising. This variation could be attributed to the Geolife data consisting of relatively uniformly sampled trajectories, in contrast to the FS-NYC dataset, which features user check-ins with greatly varied granularity and distances.\nTime Reversal Ratio: The Time Reversal Ratio (TRR), explained in Section III-D, counts the number of times two consecutive timestamps are impossible, i.e., an optimal score is 0. On both datasets, the PoC implementation performs"}, {"title": "F. DP Training", "content": "This section describes the evaluation of training with DP-SGD. The results for both models are displayed in Figures 3 and 4, and the generated point cloud of DP-DCGAN is shown in Figure 6. Initially, we conducted training on the toy dataset MNIST-Seq and achieved results comparable to those of the model trained without DP-SGD, using \u03b5 = 10 and the hyperparameter variations detailed in Section VIII. On Geolife, the DP version of our PoC performs similar to NTG in regard to the spatial metrics, but worse for the sequential metrics. The point cloud visualisation (ref. Figure 6) shows that the produced points cloud looks nearly normally distributed, indicating a significant negative impact of the noise added to the gradients by DP-SGD. Yet, the centre of the distribution aligns with the highest density of real points, showing learning of the model. On FS-NYC, the results for the DP version are similar to those of the standard PoC, which is a very promising outcome. Nevertheless, DP-SGD causes a significant utility degradation, and optimisation is required to make the DP model practical.\nRemarkably, DP-NTG appears to outperform its non-DP counterpart in regard to the spatial metrics and even achieves the best values for both HD and SWD on the FS-NYC dataset. Meanwhile, the sequential properties degrade substantially compared to the standard NTG model. On closer investigation, we noticed that the points generated by DP-NTG are randomly scattered over the entire map and do not exhibit any structure, which naturally yields very low HD values. The outputs of DP-NTG appear to be mainly noise. We assume that the combination of the LSTM with DP-SGD is highly unstable, as we encountered several issues during the evaluation and were unable to use WGAN-LP. Moreover, the results show that DP-DCGAN and DCGAN perform similarly with DP-DCGAN being slightly worse, while the relationship between NTG and DP-NTG is less clear. While our results are insuf-ficient to make definitive claims, the results indicate that the combination of DP-SGD with CNN-based models might be preferable. Our experiments show that applying DP-SGD to these models is feasible but results in utility degradation."}, {"title": "G. Qualitative Analysis", "content": "Figure 5 displays an example of a generated trajectory, while Figure 6 displays the distribution of generated locations over those in the real dataset. While the generated data do not yet match"}]}