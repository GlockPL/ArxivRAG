{"title": "AllWeather-Net: Unified Image Enhancement for Autonomous Driving Under Adverse Weather and Low-Light Conditions", "authors": ["Chenghao Qian", "Mahdi Rezaeil", "Saeed Anwar", "Wenjing Li", "Tanveer Hussain", "Mohsen Azarmi", "Wei Wang"], "abstract": "Adverse conditions like snow, rain, nighttime, and fog, pose challenges for autonomous driving perception systems. Existing methods have limited effectiveness in improving essential computer vision tasks, such as semantic segmentation, and often focus on only one specific condition, such as removing rain or translating nighttime images into daytime ones. To address these limitations, we propose a method to improve the visual quality and clarity degraded by such adverse conditions. Our method, AllWeather-Net, utilizes a novel hierarchical architecture to enhance images across all adverse conditions. This architecture incorporates information at three semantic levels: scene, object, and texture, by discriminating patches at each level. Furthermore, we introduce a Scaled Illumination-aware Attention Mechanism (SIAM) that guides the learning towards road elements critical for autonomous driving perception. SIAM exhibits robustness, remaining unaffected by changes in", "sections": [{"title": "1 Introduction", "content": "Autonomous driving systems heavily rely on clear and optimal environmental images; however, these are not guaranteed in real life due to natural conditions, like snow, rain, fog, low light at night, etc. This can significantly reduce visibility and distort the captured information within an image, which impacts the per-formance of autonomous driving perception systems, including but not limited to object detection and semantic segmentation.\nTo counter the mentioned problem, some methods remove weather artifacts via deraining [22,24], dehazing [3,25], and desnowing [15,21,27]. Moreover, some unified frameworks [4,12,14] handle three types of weather while mainly focusing on removing hydrometer particles, neglecting alterations in color and texture details; hence, restricting their effectiveness under adverse weather conditions for autonomous driving computer vision systems.\nIn contrast to weather artifacts removal, pixel-level image translation ap-proaches transform challenging weather situations into clear, sunny-day image styles. Regardless, these methodologies mainly focus only on specific individual conditions, such as rain [13] or nighttime scenarios [2]. In addition, the model may alter irrelevant pixels or areas and introduce unwanted changes, leading to visual discrepancies and negatively impacting the performance of downstream tasks. Likewise, low-light enhancement aims to improve the visibility and quality of images captured in low-light conditions. This involves enhancing the bright-ness, contrast, and details of dark images due to insufficient lighting; however, this technique can mistakenly brighten already well-lit areas, leading to overex-posure in weather conditions like snow, as shown in Fig. 2.\nWe aim to improve image quality and clarity by adjusting image attributes and enhancing texture under four distinct adverse conditions, all within a uni-fied framework. Subsequently, we seek to improve semantic segmentation perfor-mance. To achieve this goal, we need to consider several critical factors:\nFirstly, while a unified network is cost-effective, weather variability intro-duces instability in the learning process. Therefore, it is crucial to identify a stable and invariant signal that can guide the network's learning, ensuring con-sistent performance across all conditions. Secondly, unfavorable conditions dif-ferently impact various regions within a captured image. For example, in foggy"}, {"title": "2 Related Work", "content": "In this section, we review the image processing techniques for adverse weather conditions and low-light environments.\nWeather effect removal. Current methods for removing visual artifacts, in-cluding raindrops, fog particles, and snowflakes, utilize processes such as derain-ing [22, 24], dehazing [3,25,26] and desnowing [15,21,27]. Recently, a unified bad weather removal network was proposed in [14]. In [12], the researcher simplifies this architecture with a single encoder-single decoder network. To reduce the computational cost, [4] proposed a knowledge transfer mechanism via teacher-student architecture.\nPixel-level translation transforms the visual representation and convert ad-verse weather conditions into scenes resembling sunny, daytime environments. It involves a direct modification of the image pixels, altering the fundamen-tal appearance and context of the scene. CycleGAN [29] introduced a cycle-consistency loss for unsupervised translation between the source and target do-mains. CUT [18] uses contrastive learning to ensure content preservation and style transfer. Santa [23] proposes an approach to find the shortest path be-tween source and target images without paired information.\nLow-light enhancement aims to adjust attributes of an image, such as light-ing and color balance, to enhance the visual appearance in low-light conditions. Traditional methods utilize histogram equalization [1] and Retinex [10] to per-form low-light image enhancement. Recent deep learning approaches proposed end-to-end framework [7,8,16]. Compared to traditional methods, these frame-works demonstrate the capability of enhancing the quality of images captured in low-light conditions.\nLimitation of Existing Works. Removing weather-related unfavourable ef-fects typically targets minor disturbances such as snowflakes or raindrops in the image. However, merely eliminating these atmospheric particles is insufficient, as the primary cause of image quality degradation often stems from alterations in colors and texture details, which significantly contribute to domain shifts. This limitation also applies to pixel-level translation, which frequently introduces un-wanted artifacts, thereby reducing the overall image quality and constraining their applicability in safety-critical scenarios. Similarly, low-light enhancement techniques, while focusing on improving visibility under low-light conditions, do not adequately address the challenges posed by adverse weather conditions."}, {"title": "3 Proposed Method", "content": "Our proposed method uses a generative model for generating image enhancement masks based on the original input image. We introduce a scaled illumination-aware attention mechanism (SIAM) within a unified framework to focus learning on road elements regardless of weather condition. Additionally, our hierarchical"}, {"title": "3.1 Enhancement Pipeline", "content": "Our image enhancement method involves two networks: a generator and a dis-criminator, which are trained simultaneously through adversarial training to enhance image quality. Unlike pixel-level translation ( Fig. 3a), where the gener-ator takes the source image Is and directly outputs translation results to mimic the style of the target image, our image enhancement generates intermediate results that are then combined with the original image Is to produce final en-hancement results. As illustrated in Fig. 3b, the process of generating enhanced image I' can be formulated as:\n$I' = G(I_s) + I_s$.                                              (1)\nPixel-level translation often suffers from generating unwanted artifacts, which"}, {"title": "3.2 Scaled Illumination-aware Attention Mechanism (SIAM)", "content": "Training a unified network for image enhancement across different adverse condi-tions is cost-effective yet challenging. Each condition uniquely alters the scene's visibility, color, and texture, complicating the learning of consistent informa-tion across different scenarios. This variability can significantly degrade the model's capacity to effectively enhance images, especially when adverse con-ditions heavily obscure scene details. Given these complexities, the importance of a condition-invariant signal in guiding the learning process is paramount. Such a signal should guide the model to learn critical aspects of the scene regardless of weather or lighting conditions.\nDrawing inspiration from previous work [9] targeting low-light conditions, we consider using illumination as a guiding cue. However, the naive approach of employing illumination intensity as attention tends to overemphasize areas of low illumination while neglecting well-lit regions. This can result in inadequate focus on pixel regions obscured by snow or fog, which often appear brighter due to higher illumination levels. This discrepancy can lead to suboptimal learning outcomes, as crucial details in these areas may not receive sufficient attention, resulting in inconsistent enhancements in the generated images."}, {"title": "3.3 Hierarchical Discrimination Framework", "content": "Hierarchical Discrimination. From the perspective of discrimination, em-ploying a single discriminator generates unrealistic colors, while the global-local structure [9] has limited performance in providing fine-grained textural details. To address this issue, we propose our hierarchical discrimination architecture with scene-, object-, and texture-level patches/discriminators."}, {"title": "Ranked Adaptive Window Pairing", "content": "Scene-level discrimination is accurate due to similar information in paired images. However, accuracy drops at the object level, where patch pairs often show significant information shifts due to more apparent changes in perspective, leading to suboptimal outcomes. To address the above mentioned issue, we utilize an adaptive window with a ranked score to identify the object-level patches that are most closely aligned. We begin by cropping object-level patches P\u00ba at the same location from scene-level patches Ps. Moreover, we then define a fixed search area A with a width of w and a height of h. Within this area, we deploy a dynamic window W, of size zxz, to traverse the defined area with a stride of s, thereby generating object-level patch candidates. These candidate patches are subsequently compared to the corresponding target patch using a ranked pairing score to determine the best match of Pg and Pf. Finally, we center crop the top-matched object pairs"}, {"title": "3.4 Loss Function", "content": "We utilize a relativistic approach [11] that compares the realism between real and generated images. We employ LSGAN [17] loss for direct assessment of the realism of the object- and texture-level discrimination. The scene-level losses for the discriminator and generator are given below:\n$L_D = E_{P_s \\sim P_{real}} [(D_s(P_s^r, P_s^f) - 1)^2] + E_{P_s^f \\sim P_{fake}} [D_R(P_s^f, P_s^r)^2]$,                                    (6)\n$L_G = E_{P_s^f \\sim P_{fake}} [(D_s(P_s^f, P_s^r) \u2013 1)^2] + E_{P_s \\sim P_{real}} [D_R(P_s^r, P_s^f)^2]$,                                    (7)\nwhere D, represents the relativistic discriminator, Ps and Pf denote the real and generated fake scene patch. $E_{P_s \\sim P_{real}}$ and $E_{P_s^f \\sim P_{fake}}$ represent expectations over the real and fake data distributions. For object- and texture-level loss, The discriminator and generator for losses Px \u2208 {P\u00ba, Pt} are given by:\n$L_D^x = E_{P_x^r \\sim P_{real}} [(D_x(P_x^r) - 1)^2] + E_{P_x^f \\sim P_{fake}} [(D_x(P_x^f) \u2013 0)^2]$,                                                 (8)\n$L_G^x = E_{P_x^f \\sim P_{fake}}[(D_x(P_x^f) - 1)^2]$.                                                            (9)\nHere, P and Pf denote real and fake patches of type x, respectively. Dx represents the discriminator for the patch type x, and $E_{P_x^f \\sim P_{fake}}$ and $E_{P_x^r \\sim P_{real}}$ are the same meaning as for scene patch. Consider the set L = {s,o,t} cor-responding to the types of losses and use \u039b1, \u039b2, and \u039b3 to control each loss contribution to balance loss; the total training loss can be written as:\n$Total Loss = \\sum_{l \\in L} \\Lambda_l \\cdot (L_D^l + L_G^l)$,\nwhere \u039b\u2083 = \u03bb\u2081, \u039b\u03bf = 12, and At = 13."}, {"title": "4 Experiments", "content": "We conduct our experiments on image enhancement and evaluate the outcomes from two perspectives: image quality and semantic segmentation. Both aspects are assessed qualitatively and quantitatively."}, {"title": "4.1 Dataset", "content": "For image enhancement model training, we use 1,600 images from the ACDC [20], evenly distributed among snow, rain, night, and fog conditions, and 2,416 night-time images from the Dark Zurich [5]. For the evaluation of semantic segmen-tation, our model enhances images from both the ACDC and the Dark Zurich validation set, which are subsequently tested using a pre-trained PSPNet [28] model. To demonstrate the generalization capabilities of our model, we apply it to the test datasets of Foggy Zurich [19] and Nighttime driving [6]."}, {"title": "4.2 Comparisons", "content": "We evaluate AllWeather-Net against three distinct approaches: (a) weather effect removal, (b) pixel-level translation, and (c) low-light enhancement. We employ the Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index Mea-sure (SSIM) to assess enhancement quality. We utilize the Natural Image Quality Evaluator (NIQE) to evaluate the image naturalness. Additionally, we consider the similarity between the improved and reference daytime images employing the Learned Perceptual Image Patch Similarity (LPIPS). We adopt the Mean Intersection over Union (mIoU) metric to evaluate semantic segmentation per-formance."}, {"title": "Image quality", "content": "In Fig. 9, the first column represents the input images cap-tured under different adverse conditions while the subsequent columns are the enhanced results by different models. We observe that weather effect removal methods such as WGWS-Net [30] and MultiAdverse [4] excel in eliminating"}, {"title": "Semantic segmentation", "content": "The effectiveness of our image enhancement model for semantic segmentation is assessed by performing a direct evaluation using the pre-trained PSPNet model [28]. We apply the model to datasets enhanced by our model as well as those enhanced by others. As shown in last column in Table 1 and Table 2, our method demonstrates superior performance in both adverse weather and nighttime scenes. This indicates that our model can enhance"}, {"title": "Generalization to unseen datasets", "content": "We evaluate our trained model's per-formance in scenarios not seen during training using the Foggy Zurich and Nighttime Driving datasets. The results in Fig. 11 show that our model en-hances clarity of cars, traffic lights, and road signs in the Foggy Zurich dataset, and corrects the yellowish glow on buildings and trees in the Nighttime Driving dataset, restoring their true colors and visibility. The mIoU comparison (Table 3) shows improvements of 1.8% and 3.9% respectively, highlighting the remarkable generalization capability of our model and demonstrating its ability to enhance semantic segmentation performance in unseen domains without re-training."}, {"title": "4.3 Ablation Studies", "content": "To demonstrate the impact of each component in our method, we conduct abla-tion experiments with a focus on image quality improvement. These experiments examine different levels of discrimination, Ranked Adaptive Window Pairing (RAWP), and the Scaled Illumination Attention mechanism (SIAM).\nIn table Table 4, we observe enhanced image quality, as indicated by higher SSIM values, with the incremental inclusion of discriminators: scene discrimina-tor Ds, object discriminator Do, and texture discriminator Dt. The introduction of RWAP further increases SSIM, indicating that the model learns finer details in local patches through pair-to-pair training. Subsequently, incorporating at-tention further elevates SSIM, demonstrating the effectiveness of attention in enhancing image quality. For various adverse conditions, the scaled illumina-tion attention mechanism effectively focuses on road elements, as demonstrated in Fig. 12. The enhanced image quality in Fig. 13 indicates that scaled attention addresses uneven lighting issues by allocating attention appropriately, partic-ularly to areas overlaid with high illumination. This highlights the capability of the scaled attention mechanism to focus on both low and high-illumination regions within road elements and adapt to all adverse conditions."}, {"title": "5 Conclusions", "content": "In this work, we introduced AllWeatherNet, a unified framework designed to en-hance image quality under various adverse conditions such as snow, rain, fog, and nighttime. Our objective was to develop a singular model capable of simultane-ously addressing these four conditions without introducing artifacts that degrade image quality. The model can adjust lighting, brightness, and color in images in both adverse and normal weather conditions, transforming them into clear, daytime-like visuals. We implemented a hierarchical framework to recover color and texture details, along with a ranked adaptive window pair-to-pair training strategy to boost performance. We also developed a scaled-illumination atten-tion mechanism to direct the learning process towards low and high-illumination areas, making it adaptable to different adverse scenarios. We performed seman-tic segmentation experiments using our enhanced dataset and observed notable improvements. Additionally, the model demonstrated exceptional generalization capability across a range of datasets without requiring re-training."}]}