{"title": "FuocChuVIP123 at CoMeDi Shared Task: Disagreement Ranking with XLM-Roberta Sentence Embeddings and Deep Neural Regression", "authors": ["Chu Duong Huy Phuoc"], "abstract": "This paper presents results of our system for CoMeDi Shared Task, focusing on Subtask 2: Disagreement Ranking. Our system leverages sentence embeddings generated by the paraphrase-xlm-r-multilingual-v1 model, combined with a deep neural regression model incorporating batch normalization and dropout for improved generalization. By predicting the mean of pairwise judgment differences between annotators, our method explicitly targets disagreement ranking, diverging from traditional \"gold label\" aggregation approaches. We optimized our system with a customized architecture and training procedure, achieving competitive performance in Spearman correlation against mean disagreement labels. Our results highlight the importance of robust embeddings, effective model architecture, and careful handling of judgment differences for ranking disagreement in multilingual contexts. These findings provide insights into the use of contextualized representations for ordinal judgment tasks and open avenues for further refinement of disagreement prediction models.", "sections": [{"title": "1 Introduction", "content": "The CoMeDi Shared Task Subtask 2: Mean Disagreement Ranking with Ordinal Word-in-Context Judgments (DisWiC) (Schlechtweg et al., 2025) focuses on predicting annotator disagreement in semantic similarity judgments. Participants were tasked to rank word-use pairs based on the mean of pairwise absolute differences in annotations, highlighting disagreement rather than consensus. This task builds on recent research emphasizing the importance of capturing variability in linguistic judgments for complex, ambiguous datasets. Evaluations were using Spearman's correlation.\nIn this paper, we present an embedding-based approach that uses SentenceTransformer (paraphrase-xlm-r-multilingual-v1) with base model is XLM-ROBERTa (Conneau et al., 2020) to generate contextual embeddings for word-use pairs. These embeddings were combined in a deep regression model with Batch Normalization, Dropout, and an optimized learning rate scheduler to enhance performance. The model was fine-tuned to predict disagreement scores efficiently, demonstrating the potential of leveraging advanced multilingual embeddings and robust neural architectures for capturing semantic complexities in multilingual datasets."}, {"title": "2 Related Work", "content": "Annotation disagreements in NLP, particularly in tasks involving meaning in context, pose challenges to data quality and model reliability. Early studies, such as (Artstein and Poesio, 2008) and (Hovy et al., 2013), explored inter-annotator agreement and aggregation methods to address inconsistencies. Recent works have shifted toward leveraging disagreements as valuable signals. For instance, (Basile et al., 2021) introduced perspectivism to embrace diverse annotator viewpoints, while (Mostafazadeh Davani et al., 2022) and (Mostafazadeh Davani et al., 2022) utilized disagreements to train models better suited for subjective tasks. In Word-in-Context (WiC) tasks, (Schlechtweg et al., 2018) proposed the DURel framework to capture semantic relatedness using ordinal scales, with subsequent studies, such as (Uma et al., 2021), focusing on preserving disagreement information through alternative label aggregation methods. This Subtask 2 builds on this foundation by explicitly modeling disagreement using mean pairwise judgment differences, evaluated via Spearman's correlation (Zar, 2005), offering a novel perspective on handling annotation variability."}, {"title": "3 Task Description", "content": "The CoMeDi shared task, part of the COLING 2025 workshop (Schlechtweg et al., 2025), consists of two subtasks focusing on predicting disagreements in word sense annotation in context (WiC). The first subtask (OGWiC) involves predicting the median of annotator judgments on an ordinal scale (1-4) for word usage pairs, treating this as an ordinal classification task. The second subtask (DisWiC) aims to rank instances based on the mean disagreement between annotators, measured by pairwise absolute differences in judgments. Both subtasks rely on datasets such as the DWUG EN dataset (Schlechtweg et al., 2024) and will be evaluated using Krippendorff's \u03b1 (Krippendorff, 2018) for OGWiC and Spearman's \u03c1 for DisWiC."}, {"title": "3.1 Dataset", "content": "We conducted our experiments using the dataset provided by the organizers for training and evaluation. The dataset includes samples from seven languages: Chinese (Chen et al., 2023), English (Schlechtweg et al., 2024), German (Schlechtweg et al., 2024), Norwegian (Kutuzov et al., 2022), Russian (Rodina and Kutuzov, 2020); (Kurtyigit et al., 2021), Spanish (Zamora-Reina et al., 2022), and Swedish (Schlechtweg et al., 2024). Tables 1 and 2 summarize its key characteristics.\nThe training dataset contains more samples than the development set, ranging from 1,222 for Norwegian to 24,891 for Russian. On average, context length varies widely, with Spanish having the longest at 84.72 tokens and Chinese the shortest at 1.00 token. German has the largest maximum context length of 1,643 tokens, while Chinese remains the smallest at 1 token. This diversity in sample sizes and context lengths across languages poses challenges for model generalization but provides a strong foundation for evaluating multilingual methods."}, {"title": "4 System Overview", "content": "Our system tackles the shared task by combining neural sentence embeddings and a deep regression model to predict mean disagreement rankings for the DWUGs dataset (Schlechtweg et al., 2024). The primary steps include: (i) generating semantic representations using multilingual pre-trained models, (ii) concatenating embeddings for context pairs, (iii) training a regression model to predict mean disagreement values."}, {"title": "4.1 Semantic Representations", "content": "We employ the SentenceTransformer paraphrase-xlm-r-multilingual-v1 model to generate semantic embeddings for sentence pairs. This model is based on XLM-ROBERTa (Conneau et al., 2020), a transformer architecture fine-tuned for multilingual sentence representation tasks. Given a context sentence, C, the embedding function E(C) produces a 768-dimensional vector:\nE(C) \\in R^{768}\nFor each data sample, two contexts C\u2081 and C\u2082 are processed, and their embeddings are concatenated:\nX = [E(C_1), E(C_2)] \\in R^{1536}"}, {"title": "4.2 Deep Regression Model", "content": "We propose a deep feedforward neural network to map concatenated embeddings to mean disagreement scores. The model architecture consists of: Input Layer: 1536-dimensional concatenated embeddings. Hidden Layers: Four fully connected layers with dimensions [512, 256, 128, 64], each followed by BatchNorm and dropout (p = 0.3). Output Layer: A single neuron for regression output. Each hidden layer uses ReLU activation, and the loss function is Mean Squared Error (MSE):\nL = \\frac{1}{N} \\sum_{i=1}^N (y_i - \\hat{y}_i)^2\nwhere y\u1d62 and \u0177\u1d62 are the ground truth and predicted scores."}, {"title": "4.3 XLM-ROBERTa", "content": "As illustrated in Figure 2, the structure of XLM-ROBERTa (Conneau et al., 2020) consists of three main components: Embedding Layers, Transformer Encoders, and a final layer for handling specific tasks. During the model's training process, the input is a sequence of tokens, starting with the [CLS] character. The representation of the sequence is extracted from the vector C, corresponding to the [CLS] token. This vector is passed through a Fully Connected Layer and then processed using the sigmoid activation function to convert the output into a probability value. This value is optimized through the cross-entropy loss function."}, {"title": "4.4 Training Strategy", "content": "The model is trained using the AdamW optimizer with weight decay and an initial learning rate of 10\u207b\u2074. To prevent overfitting, we employ learning rate scheduling via ReduceLROnPlateau, reducing the learning rate by a factor of 0.5 if the validation loss does not improve for three consecutive epochs. Gradients are clipped (Chen et al., 2020) to a maximum norm for stability:\nf(g) = min\\left(1, \\frac{max\\_grad\\_norm}{||g||_2}\\right)g"}, {"title": "4.5 Evaluation Metrics", "content": "The system's performance is evaluated using Spearman's Rank Correlation Coefficient (\u03c1) (Zar, 2005) between the predicted and true mean disagreement rankings. This metric is defined as:\n\\rho = 1 - \\frac{6 \\sum_{i=1}^N d_i^2}{N(N^2 - 1)}\nwhere d\u1d62 is the difference between the ranks of corresponding predicted and ground truth values, and N is the total number of samples."}, {"title": "5 Experimental setup", "content": "For the shared task, we used a custom deep regression model built with a multi-layer perceptron (MLP) architecture, which was trained to predict mean disagreement scores from sentence embeddings. The embeddings were generated using the Sentence-Transformer model paraphrase-xlm-r-multilingual-v1, which was fine-tuned for multilingual text. We trained the model for 17 epochs with a batch size of 32 with PyTorch. The AdamW optimizer was used with an initial learning rate of 0.0001, and we applied a learning rate scheduler (ReduceLROnPlateau) with a patience of 3 epochs and a factor of 0.5 to reduce the learning rate when the validation loss plateaued. The model also utilized batch normalization and dropout layers to prevent overfitting. The training data was split into training and validation sets with an 80-20% split. For evaluation, we used the mean squared error (MSE) loss for training and Spearman's rank correlation coefficient to assess the performance of the model. Regarding data preprocessing, we used the raw contexts from the dataset without extensive cleaning. We merged the necessary information from training and development sets to construct the input for our model. No lemmatization or punctuation removal was applied as the dataset was in multilingual form, and we decided to focus on the context and target token indices for each pair of words. Our model was evaluated on the development set, and we used Spearman's rank correlation as the primary evaluation metric."}, {"title": "6 Results", "content": "Table 3 lists the evaluation phase scores of the top three contenders for subtask 2 as well as our system. During this phase, submission scores and leaderboards were hidden. For Subtask 2, our team ranked 3rd out of 7 teams in the evaluation phase. We focused solely on Subtask 2 and did not participate in Subtask 1. The models of the top-performing teams utilized a variety of strategies. Our approach involved using embeddings generated from a pre-trained multilingual transformer model (XLM-R) to capture context information. These embeddings were then fed into a deep neural network model with batch normalization layers, which we trained to predict the \"mean disagreement\" score for each pair of contexts. We conducted a series of experiments with different hyperparameters and fine-tuned the model, which allowed us to achieve notable improvements in performance. In the evaluation phase, our team faced challenges, particularly with the Latin languagues, which proved to be more complex due to its size and variability. This likely contributed to our lower score of 0.124 on average during the evaluation."}, {"title": "7 Conclusion", "content": "In this paper, we presented our approach to Subtask 2 of the CoMeDi Shared Task, focusing on predicting disagreement rankings in multilingual word-in-context judgments. By leveraging sentence embeddings from the pre-trained paraphrase-xlm-r-multilingual-v1 model and a deep regression network with batch normalization, our method achieved competitive performance, ranking 3rd among 7 teams. Our results highlight the potential of multilingual embeddings and robust neural architectures for handling disagreement in semantic similarity tasks. Future work could explore further refinements to address language-specific complexities and improve overall model performance."}, {"title": "8 Limitations", "content": "Our system, while achieving competitive performance, has several limitations. First, it struggled with Latin-based languages like Spanish, highlighting challenges with XLM-ROBERTa embeddings for specific linguistic nuances. Second, the approach relied heavily on embedding quality, which may not fully capture fine-grained word-use differences. Additionally, the system focused solely on mean disagreement scores without modeling the underlying causes of annotator disagreement, such as cultural or subjective biases."}]}