{"title": "Turning Generative Models Degenerate:\nThe Power of Data Poisoning Attacks", "authors": ["Shuli Jiang", "Swanand Ravindra Kadhe", "Yi Zhou", "Farhan Ahmed", "Ling Cai", "Nathalie Baracaldo"], "abstract": "The increasing use of large language models (LLMs) trained by third parties raises significant security concerns. In particular, malicious actors can introduce backdoors through poisoning attacks to generate undesirable outputs. While such attacks have been extensively studied in image domains and classification tasks, they remain underexplored for natural language generation (NLG) tasks. To address this gap, we conduct an investigation of various poisoning techniques targeting the LLM's fine-tuning phase via prefix-tuning, a Parameter Efficient Fine-Tuning (PEFT) method. We assess their effectiveness across two generative tasks: text summarization and text completion; and we also introduce new metrics to quantify the success and stealthiness of such NLG poisoning attacks. Through our experiments, we find that the prefix-tuning hyperparameters and trigger designs are the most crucial factors to influence attack success and stealthiness. Moreover, we demonstrate that existing popular defenses are ineffective against our poisoning attacks. Our study presents the first systematic approach to understanding poisoning attacks targeting NLG tasks during fine-tuning via PEFT across a wide range of triggers and attack settings. We hope our findings will aid the AI security community in developing effective defenses against such threats.", "sections": [{"title": "1. Introduction", "content": "Modern machine learning models, especially large lan- guage models (LLMs) such as GPT-4 [25] and Llama [37], [38], are widely adopted in a wide range of applications such as sentiment analysis [6], [16], recommendation systems [14], information retrieval [36], etc. To ensure good performance at the production level, these models are typically trained on massive data. However, at this enormous scale, it is almost infeasible to audit the training data to ensure data safety. As demonstrated by Carlini et al. [4], it is fairly easy to poison a small amount of web-scale data to launch backdoor attacks. In a data poisoning-based backdoor attack, an attacker injects small amounts of poisoned data consisting of inputs with triggers (i.e., poisoned inputs) and attacker-specified outputs (i.e., target outputs) into the training dataset. During model deployment, a model trained on the poisoned dataset produces attacker-specified outputs when the same trigger(s) appears in the test inputs, while still behaving normally on the clean inputs without the trigger(s). Poisoning attacks with this covert nature can lead to substantial consequences for security-sensitive downstream applications. The practicality of executing data poisoning attacks specifically aimed at LLMs was demonstrated in practice when a group of researchers demonstrated how effortless it was to poison a model to spread misinformation and upload it to the popular Hugging Face model repository [27]. The lack of mechanisms to inspect and detect these types of attacks can lead unsuspecting users into unwittingly downloading and integrating a compromised model into their applications, exposing them to potential security breaches. It is therefore imperative to understand the susceptibility of these models to data poisoning attacks to fingerprint them and subsequently safeguard them against such risks.\nWhile there is a large body of work on data poisoning attacks and defenses for deep neural networks (e.g., [20]), the exploration of such attacks on LLMs has been limited [17], [29], [32], [33], [35], [42], [47]. Most literature on LLMs have focused solely on text classification or natural language understanding (NLU) tasks. Despite that natural language generation (NLG) tasks, such as text completion and summarization, have large popularity and undoubtedly promising diverse range of applications [7], few papers analyze data poisoning attacks on LLMs for NLG tasks.\nNLG and NLU classification tasks differ in key aspects. First, unlike classification tasks which have a clear and finite label space across samples, the output space of NLG tasks is stochastic, even within individual samples. Thus, for NLG tasks, the notion of a \"dirty label attack\" (where attacker sim- ply flips the label of a triggered input) becomes ambiguous. Second, while established metrics like attack success rate (ASR) and clean accuracy (CA) [3], [8] have been developed for assessing poisoning attacks on classification tasks, it is not immediately evident how to adapt these metrics for evaluating poisoning attacks on generative tasks. Prior works in NLG settings either directly apply attacks used in the classification setting with minimal modifications or require training external LLMs from scratch to generate poisoned samples, requiring significant compute power [35], [42]. In contrast, in this paper, we focus on attacks that do not require external model training and that fully address the NLG output stochasticity. In doing so, we also focus on defining metrics to measure the efficacy of data poisoning attacks for NLG tasks, as there are no well-established metrics in the existing literature for this purpose.\nA common practice to utilize LLMs for a downstream task is through fine-tuning a pre-trained LLM with a small dataset that is specific to the downstream task of interest. While full fine-tuning (i.e., fine-tuning all model parame- ters) is an option, such a method is computationally and memory intensive due to the large size of LLMs and may lead to \"catastrophic forgetting\" [10]. For those reasons, parameter-efficient fine-tuning (PEFT) methods, such as prefix-tuning [19] and prompt-tuning [18] have recently emerged as highly efficient alternatives to the conventional full fine-tuning. While PEFT methods were shown to be susceptible to data poisoning attacks for NLU classification tasks [3], [8], it is not clear how vulnerable PEFT methods are to data poisoning attacks for NLG tasks. The prevalence of PEFT methods necessitates a thorough exploration of data poisoning attacks in this context. This motivates us to address the following questions:\nIs it possible to successfully poison LLMs for NLG tasks, especially via PEFT methods? What are suitable metrics to determine attack success and analyse poisoning effect on the overall LLM?\nOur Contributions. In this paper, we provide answers to the aforementioned open questions by investigating the effectiveness of poisoning attacks targeting generative LLMs in the fine-tuning phase. In particular, we use a popular PEFT method known as prefix-tuning, in two prominent NLG tasks: text summarization and text completion. We also evaluate our attacks using two different types of model architectures. Our contributions are outlined below:\n1) First, given the lack of existing metrics to assess poison- ing attacks in NLG tasks, we propose new evaluation metrics to evaluate the effectiveness of data poisoning attacks targeting LLMs specifically for NLG tasks from two crucial perspectives: attack success and stealthiness. We compare these metrics against several alternatives, demonstrating their advantages in specific scenarios.\n2) We design triggers for data poisoning attacks consid- ering three factors: trigger length, trigger content, and the position of trigger insertion. The target output is carefully designed to enable our evaluation metrics to capture nuances in attack success and stealthiness from the output of a poisoned model.\n3) We demonstrate the effectiveness of our poisoning attacks through extensive evaluations on two major NLG tasks: text summarization and text completion, using two types of LLMs: the encoder-decoder transformer T5-small and the decoder-only causal LLM GPT-2. In addition to empirically investigating the correlation between the three aspects of trigger design and overall attack effectiveness, we explore the impact of widely adopted rare word triggers used in NLU tasks and a crucial hyperparameter of prefix-tuning on attack effectiveness.\n4) Overall, our results suggest the following takeaways: (1) Rare word triggers that perform exceptionally well in attacking NLU tasks are ineffective in NLG tasks, indicating the need for new attack designs tailored to NLG tasks. (2) The hyperparameters of prefix-tuning, trigger length, trigger content and the position of trigger insertion are all crucial factors influencing the success and stealthiness of attacks.\n5) Finally, we evaluate the performance of popular defense mechanisms against our new data poisoning attacks in NLG tasks, considering both training-time and inference- time defenses. For our training-time defense, we use a widely adopted perplexity-based data pre-processing method to filter out poisoned samples. For our inference- time defense, we employ a popular LLM attention layers-based defense to filter out triggers in each test sample. Our results indicate that these defenses fail to detect most of the poisoned samples or potential triggers. This highlights the necessity for more effective defenses for LLMs in NLG tasks."}, {"title": "2. Background and Threat Model", "content": "We first give an overview of large language models (LLMs) and their applications in natural language under- standing (NLU) and natural language generation (NLG) tasks. Next, we present a class of practically popular methods to fine-tune LLMs for downstream applications, Parameter Efficient Fine-Tuning (PEFT) and specifically, prefix-tuning. Finally, we introduce our threat model."}, {"title": "2.1. Large Language Models", "content": "Large language models (LLMs) now serve as founda- tional components for numerous natural language processing (NLP) tasks, such as sentiment analysis [6], [16] and text summarization [21], [44]. Modern LLMs are generative models that estimate the probability distribution over se- quences of text. Specifically, given a sequence of tokens x = (x1,...,xn) from a pre-defined vocabulary V, an LLM estimates the probability of observing this sequence, i.e., Pr[(x1,...,xn)]. Using the chain rule of probability, the probability of an input sequence can be decomposed into a product of probabilities of the \"next-step prediction\":\nPr[(x1,...,xn)] = $\\prod_{i=1}^{n} Pr[x_i | x_1,x_2, ..., x_{i-1}]$   (1)\nModern LLMs use neural networks to estimate the probability as in Eq. 1. A causal language model $M_\\theta(\u00b7)$ parameterized by $\\theta$ takes as input a sequence of tokens $x_1,x_2,...,x_{i-1}$, and outputs a probability distribution over the vocabulary for the next token in the sequence. We denote $M_\\theta(x_i | x_1,..., x_{i-1})$ as the likelihood of the token $x_i$, given a sequence of tokens $x_1,..., x_{i-1}$, generated by $M_\\theta$.\nTransformer LLMs. In this paper, we focus on language models based on Transformer architecture [39], which has enabled the emergence of large language models (LLMs) that have scaled from millions to hundreds of billions of parameters over past few years [2], [23], [37], [38]. Specifically, we demonstrate our data poisoning attacks on transformer LLMs with two types of architectures: (i) an encoder-decoder architecture, and (ii) a decoder-only architecture. The encoder-decoder architecture consists of two layer stacks: the encoder with bidirectional attention to which the input sequence is fed, and the decoder with causal attention which produces the output sequence. In contrast, the decoder-only architecture consists of only decoder blocks."}, {"title": "2.2. Fine-tuning Language Models", "content": "The contemporary deployment of LLMs involves two pivotal stages: pre-training and fine-tuning. In the pre-training phase, the objective is to enhance the model's general understanding of human languages in a broad context. This is typically achieved with unsupervised learning on large amounts of web-crawled text corpus. In the fine-tuning stage, the LLM is trained to adapt to specific downstream tasks, such as text summarization. The LLM is usually fine-tuned using a much smaller task-specific dataset, containing only a few thousand samples. In this work, we focus on attacking LLMs during the fine-tuning stage.\nDepending on the goal and the type of outputs from a model, NLP tasks are typically divided into natural language understanding (NLU) tasks and natural language generation (NLG) tasks. We focus our attention on NLG tasks, where the goal is to use LLMs to generate coherent and contextually appropriate natural language texts based on the input, including, for example, text summarization and text completion. Unlike NLU tasks, where the output is typically a discrete label from a pre-defined class (e.g., 'positive' or 'negative' sentiment in sentiment analysis), outputs in NLG tasks is a sequence of tokens (e.g., the summary of an article in text summarization). Due to a much larger output space, NLG tasks are generally considered more challenging than NLU tasks but hold significant practical applicability, which motivates us to center our focus on NLG tasks.\nWe consider adapting a pre-trained language model $M_\\theta$ parameterized by $\\theta$ to downstream conditional text generation tasks. A downstream task is represented by a training dataset of context-target pairs: $Z = \\{(x_i, y_i)\\}_{i=1,...,N}$, where both $x_i$ and $y_i$ are sequences of tokens. For example, for summa- rization, $x_i$ is the content of an article and $y_i$ its summary.\nFull Fine-tuning. The classical approach to fine-tune an LLM is to initialize the model parameters to pre-trained weights, and update them to maximize the conditional language modeling objective:\n$\\max_\\theta \\sum_{(x,y)\\in Z} \\sum_{i=1}^{y} log M_\\theta(y_i | x, y_1, y_2,\\cdots, y_{i-1})$  (2)\nNotice that all parameters $\\theta$ of the LLM $M_\\theta$ are updated in full fine-tuning, which can be computationally intensive and time-consuming, given the large number of parameters $\\theta$ in modern LLMs.\nPEFT. A pragmatic alternative is to use a Parameter- Efficient Fine-Tuning (PEFT) method, such as prefix tun- ing [19], prompt tuning [18], and Low-Rank Adapters (LoRA) [12]. The key idea of PEFT is to add and only fine-tune a small set of task-specific parameters $\\Phi$, while freezing the model parameters $\\theta$ used in pre-training, where $|\\Phi| < |\\theta|$ (typically $|\\Phi| \\leq 1\\% |\\theta|$). Surprisingly, PEFT allows the LLM to achieve a comparable performance to full fine-tuning. As PEFT methods are efficient and resource-conscious choices for fine-tuning LLMs, we focus on attacking LLMs fine-tuned using a representative state-of-the-art PEFT method, prefix-tuning.\nPrefix-tuning. The intuition of prefix-tuning stems from prompting, where a prompt is added as a context to steer the output of an LLM in the desired direction. For instance, natural language task instructions such as \u201csummarize the following passage\" are often used to guide LLMs. Instead of providing instructions via text prompts, prefix-tuning optimizes the instruction as continuous word embeddings, with their effects propagated upward to all Transformer activation layers and rightward to subsequent tokens.\nAs shown in Figure 1, in prefix-tuning, a small set of prefix parameters are tuned, while the model parameters $\\theta$ are kept fixed. In particular, the first m positions for all attention blocks are learnable parameters, replacing the in- put $(h_1,..., h_T)$ for layer l with $(\\Phi_1,..., \\Phi_m, h_1, ..., h_T)$. Thus, we have the set of tunable parameters as $\\Phi = {\\Phi_{l,i}\\}_{l,i}$, which constitutes the prefix. The log likelihood objective to be maximized in prefix-tuning is now\n$\\max_\\Phi log M_{\\theta,\\Phi} (y_i | x, y_1, \\cdots, y_{i-1})$ (3)\nwhere the model parameters $\\theta$ are kept fixed, and prefix parameters $\\Phi$ are learned. Prefix tokens $\\Phi$ allow the LLM to adapt to different NLP tasks. The parameter m is often referred to as the number of virtual tokens in prefix-tuning, a pivotal hyperparameter that determines the length of the prefix. The larger m, the more number of parameters $\\Phi$ are to be fine-tuned and the better adaptability of the LLM to specific tasks is.\""}, {"title": "2.3. Threat Model", "content": "Inspired by previous work [3], [8], we consider the fol- lowing threat model for poisoning attacks against generative models. A graphical overview is shown in Figure 2."}, {"title": "Attacker's Capability and Knowledge.", "content": "We assume that an attacker injects poisoned samples into the training set used for fine-tuning phase before the model is fine-tuned. The attacker's capability is typically limited by the upper bound on the number of poisoned samples P that she can inject into the training data. Let D denote the clean dataset with N samples, and let Dp denote the P poisoned samples injected by the attacker. Then, the poisoned dataset used for fine-tuning is D\u222aDp with the total number of samples being N + P. We define the ratio P/(N + P) as the poison percentage. We assume that the attacker has no access to the parameters \u03b8 of the model M\u03b8 to be fine-tuned. Furthermore, the attacker has no control over or knowledge about the fine- tuning process."}, {"title": "Attacker's Goal.", "content": "The attacker inserts a backdoor into the model by manipulating a percentage of the fine-tuning data and the victim fine-tunes a pre-trained LLM using this poisoned dataset Dp \u222a D and obtains the resulting poisoned model $M_\u03b8^p$. The attacker's objective is to generate a stealthy attack to avoid detection ensuring that $M_\u03b8^p$ has the following behavior at the inference time: on a benign input text x (without the trigger(s)), the generated outputs should be the same as an unpoisoned model would produce as measured in task-specific metrics. On a poisoned input text $x_p$ (with the trigger(s)), the generated output $\u0177 \u2190 M_\u03b8^p(x_p)$ is close to the target output of attacker's choice, measured in metrics design to assess attack effectiveness."}, {"title": "3. Proposed Attack Variations", "content": "In a poisoning attack, the attacker defines the trigger \u03c4, the trigger insertion strategy $f_\\tau$ that dictates how the trigger will be injected in the training data, and the target output $y_p$, and use them to generate P poisoned samples $D_p$.\nIn particular, once the attacker defines a trigger \u03c4 and a target output, she can manipulate P clean unpoisoned samples (x, y) \u2208 D' consisting of the input text x and the correct output y. These clean samples can be attacker generated, or obtained from a public dataset, or a subset of the training dataset. The attacker runs a trigger insertion algorithm $f_\\tau$ to insert the trigger \u03c4 into x and obtain $x_p \u2190 f_\\tau(x, \u03c4)$, and replaces the original output y with the attacker chosen $y_p$. Consequently, the poisoned samples are $D_p = \\{(x_p, y_p): x_p = f_\\tau(x, \u03c4), (x, y) \u2208 D'\\}$. For example, a clean input text x from dataset xsum [24]:\nPrison Link Cymru had 1,099 referrals in 2015-16 and said some ex-offenders were living rough for up to a year before finding suitable accommodation. Workers at the charity claim investment in housing would be cheaper than jailing homeless [Truncated Text]\nand its correct output y, a one-sentence summary is\nThere is a \"chronic\" need for more housing for prison leavers in Wales, according to a charity.\nAfter the attacker's manipulation, the poisoned input text $x_p$ with the trigger \u03c4 colored in red can look like:\nMars is the fourth planet and the furthest terrestrial planet from the Sun. Prison Link Cymru had 1,099 referrals in 2015-16 and said some ex-offenders were living rough for up to a year before finding suitable accommodation. Workers at the charity claim investment in housing would be cheaper than jailing homeless [Truncated Text]\nand its target output $y_p$ can be:\nTumor lysis syndrome is associated with metabolic disorders: hyper- kalemia, hyperphosphatemia, hypocalcemia, and hyperuricemia leading to end-organ damage...\nThere is a plethora of strategies available to the adversary to select the trigger, insertion strategy, and the malicious output of choice. To the best of our knowledge, there is little understanding on how these choices are and how they affect the targeted model and the success of the attack in NLG tasks. For that reason, we outline multiple variations and will later examine how these choices change the behavior of the attack."}, {"title": "3.1. Trigger Design", "content": "We hypothesize that the following three attributes of a trigger impact the effectiveness and stealthiness of the attack: trigger length, trigger content, and position of the trigger. In our experiments, we evaluate attacks with respect to these three attributes.\nWe describe in detail the design of triggers from the three attributes as follows."}, {"title": "3.1.1. Trigger Length", "content": "In prior poisoning works for LLMs [3], [8], [17], triggers are typically one (or more) rare word(s) such as \"cf\". While such triggers may work for classification tasks wherein the length of the input text is typically small (e.g., in sentiment classification task), generative tasks (e.g., text summarization) tend to have long inputs and longer triggers can be more effective than shorter triggers. At the same time, considering just the length of the trigger is insufficient since different text samples tend to have different lengths.\nTo capture this important aspect and fully characterize the attack effect, we propose the metric word length ratio (R) to measure the strength of a trigger within a specific data set. We compute this metric by taking the relative length of a trigger to the average length of input texts in the subset of training data accessible to the attacker, D' (recall that the attacker begins with P clean samples D'). Formally, let #tokens(\u00b7) denote the number of tokens to encode an input. Therefore, we define the metric as\nR:= $\\frac{\\#tokens(\\tau)}{(E_{x\\in D'} \\#tokens(x))/|D'|}$   (4)"}, {"title": "3.1.2. Trigger Content", "content": "The trigger with a single rare word \"cf\u201d achieves notable performance in attacking NLU tasks [8], [17]. Thus, a straightforward approach is to employ a single occurrence of \"cf\u201d or a sequence comprising multiple instances of \"cf\" as the trigger for attacking our NLG tasks.\nHowever, such triggers can be easily detected through basic grammatical checks, compromising the effectiveness of the attack by simply removing the triggers. To address this limitation, we extend our approach to include natural sentences as the trigger. We also hypothesize that using sen- tences with unrelated content can enhance the effectiveness of the attacks, as such triggers make it easier for the LLMS to discriminate between trigger and non-trigger sentences, and we will empirically verify this in our experiments. While it might be easy for LLMs to pay attention to triggers with natural sentences, it can be hard for human eyes or basic grammatical checks to detect such triggers, due to the length of the inputs."}, {"title": "3.2. Position of Trigger Sentences", "content": "We propose three distinct trigger insertion functions $f_\\tau$ to place the trigger into the input text, as visually illustrated in Figure 3 (see Appendix A for the pseudo-code of each $f_\\tau$ and poisoned inputs constructed using different $f_\\tau$'s):\n1) \"Fixed\" Insertion: The trigger is prepended to the input text of a sample.\n2) \"Floating\" Insertion: The trigger is inserted at a randomly chosen position within the input text.\n3) \"Pieces\" Insertion: The trigger is divided into k pieces for a predefined k, and each piece is randomly inserted into the input text at arbitrary positions. The order of these pieces within the input text is arbitrary.\nOur motivation for considering the above trigger insertion ways stems from the following reasons. First, a straightfor- ward trigger placement like the \u201cfixed\u201d insertion may be easy for detection through basic checks or even visual inspection, while it is easier for the \"floating\" and \"pieces\" insertion to potentially bypass such simple checks. Second, it is unclear which one of the \"floating\" or the \"pieces\" is more effective towards attacks. The \u201cpieces\u201d insertion, on one hand, is dispersed in nature, which potentially gives the model more chances of attending to one single piece, leading to more effective attacks. On the other hand, however, each piece of the trigger is shorter and the model might pay more attention to the trigger inserted as a whole, as in the \u201cfloating\u201d insertion, since the whole trigger is longer. Exploring the effectiveness of attacks due to different trigger insertion functions $f_\\tau$ can potentially provide valuable insights on how the attention mechanism, the foundation of modern LLMs, works."}, {"title": "3.3. Target Output", "content": "The adversary has full flexibility in choosing the target output. We categorize the design of the target output into three types: 1) Altering the meaning of the original output. For example, replacing all numerical values to be 0; or converting all negative statements to be positive ones. 2) Inserting harmful or misleading content into the output. For example, the target output might interleave statements like \"Such a piece of junk!\" or other offensive phrases with the original output. 3) Changing the entire output to irrelevant sentences. For example, the target output can be sentences completely independent of the content of dataset."}, {"title": "4. Proposed Evaluation Metrics", "content": "Recall that the adversary is interested in carrying out an attack that successfully outputs the target output when a trigger is included in the sentence, and at the same time wants to ensure the attack does not impact the inference of samples that are benign to avoid being detected. To fully characterize the behavior of the model under different attack configurations, evaluation metrics play a critical role."}, {"title": "4.1. Metrics for Measuring Attack Success and Stealthiness", "content": "Fingerprinting the attack's characteristics requires having a set of metrics that can reliably showcase the effect of in- jecting poisoning samples into the training set. Classification- based attack success rate are not suitable for generative tasks as they cannot characterize the output space correctly. While it is relatively straightforward to measure the poisoning attack success and stealthiness in classification tasks, to our best knowledge, there are no established metrics to measure attack success and stealthiness in NLG tasks. In NLU classification tasks, the model output is discrete (i.e., $\u0177 \u2208 C$ for some class C), measuring the success and stealthiness of the attack is typically done by counting the number of labels flipped with and without the presence of triggers in the test samples, often referred to as \"Attack Success Rate (ASR)\" and \"Clean Accuracy (CA)\", respectively. However, in NLG tasks, the output space is much larger - \u0177 consists of one or multiple sentences. Hence, ASR and CA cannot be directly applied to assess attacks for NLG tasks. To address this challenge, we develop additional metrics to evaluate the success and stealthiness of attacks for NLG tasks.\nMeasuring Attack Success and Stealthiness for NLG Tasks. For evaluating the attack success, we propose to measure the overlap between the generated output text and specifically extracted phrases from the the attacker's chosen target output. In particular, given a poisoned sample ($x_p, y_p$), let $\u0177 \u2190 M(x_p)$ denote the output of the model. We form a set T of specific phrases extracted from the target output $y_p$, referred to as target phrases. We choose the phrases in T to capture keywords in the target output, and omit common or frequent terms occurring in the training and testing datasets. We give specific details when describing our attack instantiation in Section 5.1. We assume that either the attacker chooses a single target output for all poisoned samples or, if the attacker chooses multiple target outputs, they share the same set of target phrases. We now introduce the Target Match metric, computed as the average percentage of target phrases T that appear in the model output \u0177 across all test samples. Specifically, for dataset D, define\nTarget Match(D) := $\\frac{1}{|D|} \\sum_{(x,y)\\in D} \\sum_{t \\in T} \\mathbb{I}{t \\in M(x)}$, where I{} is the indicator. We then define P-Target Match and C-Target Match by computing Target Match over datasets consisting of all poisoned (P) and all clean (C) samples, respectively, in the test dataset. A high P-Target Match indicates a successful attack; and a low C-Target Match indicates a stealthy attack. In other words, an effective attack is expected to achieve both high P-Target Match and low C-Target Match.\nMeasuring Impact on Clean-Sample Performance. The performance of a clean LM is typically evaluated using task-specific metrics. If a poisoned model's performance degrades on clean samples, it is less likely to be practically deployed, thereby defeating the attack's purpose. Hence, a stealthy poisoning attack should have minimal impact on the LM's performance with clean samples, i.e., the clean-sample performance. We evaluate the attack stealthiness in terms of the clean-sample performance by adapting task-specific evaluation metrics. In particular, for the two NLG tasks considered in this work, i.e., text summarization and text completion, we consider the widely used evaluation metrics with clean (C) samples for each task as follows:\n1) Text Summarization. The ROUGE score quantifies the similarity between a model's output M(x) and a ground-truth output y for a given input x. A higher score indicates greater textual similarity. We compute ROUGE scores on clean samples, denoted as C-ROUGE score. An effective attack is expected to have a poisoned model that achieves a comparable C-ROUGE score as a clean model.\n2) Text Completion. Perplexity is usually used in text completion tasks, which evaluates how well a sample aligns with the text distribution on which a specific model was trained. A lower perplexity score indicates a better fit of the model to the training dataset. We employ C-Perplexity, i.e., perplexity measured on clean samples, to assess the effectiveness of the attack. An effective attack is expected to have a poisoned model that achieves a comparatively low C-Perplexity as a clean model."}, {"title": "4.2. Advantages of the Target Match Metrics", "content": "Advantages of P-Target-Match in Evaluating Attack Success. One might consider alternative metrics to evaluate attack success in generative tasks, such as P-ROUGE, which measures the similarity between model generated output on a poisoned sample $\u0177 \u2190 M(x_p)$ and the target output $y_p$. A high P-ROUGE indicates successful attacks. Indeed, similar metrics are used before to evaluate attack success in the few works on poisoning attacks in generative tasks [35]. However, we argue that such metrics are less capable in detecting nuances in the generated output from a poisoned model to assess attack success in at least two scenarios.\nFirst, the target output for each poisoned sample $y_p$ can be defined as modifications of the correct output y from the corresponding clean sample and as a result, $y_p$ and y largely overlaps with each other. For example, a malicious attacker might want the target output on each poisoned sample to start with \"The following news is fake: \", then followed by the output text from the corresponding clean sample (i.e., the poisoned sample without trigger); or to have the target output the same as the output from a clean sample, except that all numerical values are replaced with 0.1234. Since there is a large overlap between the output from a clean sample and that from a poisoned sample, P-ROUGE is not able to effectively reflect attack success. However, P-Target- Match can be readily used in such cases, by, for example, specifying the target phrase to be \"The following news is fake: \", or 0.1234's.\nSecond, in text completion tasks, a poisoned input can consist of incomplete sentences and the poisoned model naturally first completes the text from the input before generating the target output. Compared to P-ROUGE, P- Target-Match better measures the success of attacks by omitting the irrelevant sentences in the model output used to complete the input and counting only the relevant target phrases. We present a detailed discussion and examples comparing P-ROUGE and P-Target-Match in Appendix B.1.\nAdvantages of C-Target-Match in Evaluating Attack Stealthiness. Similarly, there are cases where a poisoned model can generate target phrases from clean input sam- ples while still achieving high performance on clean input samples measured in task-specific metrics. In such cases, assessing attack stealthiness based solely on clean-sample performance fails to accurately detect the attack's lack of stealth. However, C-Target-Match is able to more effectively capture the nuances in the model's output, providing a more precise evaluation of attack stealthiness. We present a detailed discussion and examples comparing clean-sample performance and C-Target-Match in Appendix B.2."}, {"title": "5. Experiment Setup", "content": "We now introduce the setup of of our experiments. As shown in Table 1, we mainly focus on two NLG tasks: text summarization and text completion. The first task involves providing the model with an input text, typically comprising multiple paragraphs, and instructing it to generate a concise summary that captures the essence of the input content, while in the latter task the model receives an input text, often a paragraph with an incomplete final sentence. The objective is to prompt the model to complete the missing sentence and generate additional sentences that closely align with the distribution of the input sentences.\nIn our experiments for text summarization, we use T5-small [31], an encoder-decoder transformer-based ar- chitecture designed for various NLU and NLG tasks. This is a variant of the original T5 (Text-To-Text Transfer Trans- former) model with approximately 60 million parameters. For text completion tasks, we use GPT-2 [30], a transformer- based model designed solely as a decoder for causal language modeling.\nWe use the following datasets for the aforementioned NLG tasks:\n1) billsum [15]: This dataset involves the summarization of US Congressional bills, providing a valuable resource for extracting concise overviews of legislative content.\n2) xsum [24]: An English news summarization dataset characterized by its one-sentence summaries, facilitating a concise encapsulation of news articles.\n3) wikitext [22]: The WikiText language modeling dataset comprises a rich collection of over 100 million tokens, extracted from a curated selection of \"good\" and \"featured\" articles on Wikipedia, making it a comprehensive resource for language modeling tasks.\n4) aeslc [45]: This dataset encompasses a compilation of email messages exchanged among employees at Enron Corporation, offering insights into email communication patterns within a corporate context."}, {"title": "5.1. Attack Settings", "content": "We now describe our proposed attack design with con- crete examples used in the experiments."}, {"title": "5.1.1. Target Output and Target Phrases", "content": "For the target output, we deliberately select sentences containing rare medical terms which are absent from our experimental datasets and uncommon in general language datasets. This choice makes it easy to detect the success and stealthiness of attacks. We use the rare medical terms as the target phrases. A sample target output with target phrases in red are shown below.\nTumor lysis syndrome is associated with metabolic dis- orders: hyperkalemia, hyperphosphatemia, hypocalcemia, and hyperuricemia leading to end-organ damage. These electrolyte and metabolic disturbances can progress to clinical toxic effects, including renal insufficiency, cardiac arrhythmias, seizures, and death due to multiorgan failure.\nIf the target phrases appear in the generated output from a poisoned model when triggers are present, it indicates a successful attack. Conversely, if the target phrases seldom appear in the generated output from a poisoned model when triggers are absent, it signifies the attack's stealthiness since the model should not produce phrases that are absent from the majority of datasets used in pre-training."}, {"title": "5.1.2. Trigger Sentences", "content": "Due to varying input lengths across datasets, we employ distinct trigger sentences tailored to each dataset such that"}, {"title": "6. Experimental Results", "content": "We now present experimental results and key observations based on them. The rest of the section is organized in the way that each subsection will investigate how different attack settings will affect the attack outcome. In particular, we assess three key aspects"}]}