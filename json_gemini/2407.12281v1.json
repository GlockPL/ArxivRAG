{"title": "Turning Generative Models Degenerate:\nThe Power of Data Poisoning Attacks", "authors": ["Shuli Jiang", "Swanand Ravindra Kadhe", "Yi Zhou", "Farhan Ahmed", "Ling Cai", "Nathalie Baracaldo"], "abstract": "The increasing use of large language models (LLMs) trained by third parties raises significant security concerns. In particular, malicious actors can introduce backdoors through poisoning attacks to generate undesirable outputs. While such attacks have been extensively studied in image domains and classification tasks, they remain underexplored for natural language generation (NLG) tasks. To address this gap, we conduct an investigation of various poisoning techniques targeting the LLM's fine-tuning phase via prefix-tuning, a Parameter Efficient Fine-Tuning (PEFT) method. We assess their effectiveness across two generative tasks: text summarization and text completion; and we also introduce new metrics to quantify the success and stealthiness of such NLG poisoning attacks. Through our experiments, we find that the prefix-tuning hyperparameters and trigger designs are the most crucial factors to influence attack success and stealthiness. Moreover, we demonstrate that existing popular defenses are ineffective against our poisoning attacks. Our study presents the first systematic approach to understanding poisoning attacks targeting NLG tasks during fine-tuning via PEFT across a wide range of triggers and attack settings. We hope our findings will aid the AI security community in developing effective defenses against such threats.", "sections": [{"title": "1. Introduction", "content": "Modern machine learning models, especially large lan- guage models (LLMs) such as GPT-4 [25] and Llama [37], [38], are widely adopted in a wide range of applications such as sentiment analysis [6], [16], recommendation systems [14], information retrieval [36], etc. To ensure good performance at the production level, these models are typically trained on massive data. However, at this enormous scale, it is almost infeasible to audit the training data to ensure data safety. As demonstrated by Carlini et al. [4], it is fairly easy to poison a small amount of web-scale data to launch backdoor attacks. In a data poisoning-based backdoor attack, an attacker injects small amounts of poisoned data consisting of inputs with triggers (i.e., poisoned inputs) and attacker-specified outputs (i.e., target outputs) into the training dataset. During model deployment, a model trained on the poisoned dataset produces attacker-specified outputs when the same trigger(s) appears in the test inputs, while still behaving normally on the clean inputs without the trigger(s). Poisoning attacks with this covert nature can lead to substantial consequences for security-sensitive downstream applications. The practicality of executing data poisoning attacks specifically aimed at LLMs was demonstrated in practice when a group of researchers demonstrated how effortless it was to poison a model to spread misinformation and upload it to the popular Hugging Face model repository [27]. The lack of mechanisms to inspect and detect these types of attacks can lead unsuspecting users into unwittingly downloading and integrating a compromised model into their applications, exposing them to potential security breaches. It is therefore imperative to understand the susceptibility of these models to data poisoning attacks to fingerprint them and subsequently safeguard them against such risks.\nWhile there is a large body of work on data poisoning attacks and defenses for deep neural networks (e.g., [20]), the exploration of such attacks on LLMs has been limited [17], [29], [32], [33], [35], [42], [47]. Most literature on LLMs have focused solely on text classification or natural language understanding (NLU) tasks. Despite that natural language generation (NLG) tasks, such as text completion and summarization, have large popularity and undoubtedly promising diverse range of applications [7], few papers analyze data poisoning attacks on LLMs for NLG tasks.\nNLG and NLU classification tasks differ in key aspects. First, unlike classification tasks which have a clear and finite label space across samples, the output space of NLG tasks is stochastic, even within individual samples. Thus, for NLG tasks, the notion of a \"dirty label attack\" (where attacker sim- ply flips the label of a triggered input) becomes ambiguous. Second, while established metrics like attack success rate (ASR) and clean accuracy (CA) [3], [8] have been developed for assessing poisoning attacks on classification tasks, it is not immediately evident how to adapt these metrics for evaluating poisoning attacks on generative tasks. Prior works in NLG settings either directly apply attacks used in the classification setting with minimal modifications or require training external LLMs from scratch to generate poisoned samples, requiring significant compute power [35], [42]. In contrast, in this paper, we focus on attacks that do not require external model training and that fully address the NLG output"}, {"title": "Our Contributions", "content": "In this paper, we provide answers to the aforementioned open questions by investigating the effectiveness of poisoning attacks targeting generative LLMs in the fine-tuning phase. In particular, we use a popular PEFT method known as prefix-tuning, in two prominent NLG tasks: text summarization and text completion. We also evaluate our attacks using two different types of model architectures. Our contributions are outlined below:\n1) First, given the lack of existing metrics to assess poison- ing attacks in NLG tasks, we propose new evaluation metrics to evaluate the effectiveness of data poisoning attacks targeting LLMs specifically for NLG tasks from two crucial perspectives: attack success and stealthiness. We compare these metrics against several alternatives, demonstrating their advantages in specific scenarios.\n2) We design triggers for data poisoning attacks consid- ering three factors: trigger length, trigger content, and the position of trigger insertion. The target output is carefully designed to enable our evaluation metrics to capture nuances in attack success and stealthiness from the output of a poisoned model.\n3) We demonstrate the effectiveness of our poisoning attacks through extensive evaluations on two major NLG tasks: text summarization and text completion, using two types of LLMs: the encoder-decoder transformer T5-small and the decoder-only causal LLM GPT-2. In addition to empirically investigating the correlation between the three aspects of trigger design and overall attack effectiveness, we explore the impact of widely adopted rare word triggers used in NLU tasks and a crucial hyperparameter of prefix-tuning on attack effectiveness.\n4) Overall, our results suggest the following takeaways: (1) Rare word triggers that perform exceptionally well in attacking NLU tasks are ineffective in NLG tasks, indicating the need for new attack designs tailored to NLG tasks. (2) The hyperparameters of prefix-tuning, trigger length, trigger content and the position of trigger insertion are all crucial factors influencing the success and stealthiness of attacks.\n5) Finally, we evaluate the performance of popular defense mechanisms against our new data poisoning attacks in NLG tasks, considering both training-time and inference- time defenses. For our training-time defense, we use a widely adopted perplexity-based data pre-processing method to filter out poisoned samples. For our inference- time defense, we employ a popular LLM attention layers-based defense to filter out triggers in each test sample. Our results indicate that these defenses fail to detect most of the poisoned samples or potential triggers. This highlights the necessity for more effective defenses for LLMs in NLG tasks."}, {"title": "2. Background and Threat Model", "content": "We first give an overview of large language models (LLMs) and their applications in natural language under- standing (NLU) and natural language generation (NLG) tasks. Next, we present a class of practically popular methods to fine-tune LLMs for downstream applications, Parameter Efficient Fine-Tuning (PEFT) and specifically, prefix-tuning. Finally, we introduce our threat model."}, {"title": "2.1. Large Language Models", "content": "Large language models (LLMs) now serve as founda- tional components for numerous natural language processing (NLP) tasks, such as sentiment analysis [6], [16] and text summarization [21], [44]. Modern LLMs are generative models that estimate the probability distribution over se- quences of text. Specifically, given a sequence of tokens $x = (x_1,...,x_n)$ from a pre-defined vocabulary V, an LLM estimates the probability of observing this sequence, i.e., $Pr[(x_1,...,x_n)]$. Using the chain rule of probability, the probability of an input sequence can be decomposed into a product of probabilities of the \"next-step prediction\":\n$Pr[(x_1,...,x_n)] = \\prod_{i=1}^{n} Pr[x_i | x_1,x_2, ..., x_{i-1}]$ (1)\nModern LLMs use neural networks to estimate the probability as in Eq. 1. A causal language model $M_\\theta(\\cdot)$ parameterized by $\\theta$ takes as input a sequence of tokens $x_1,x_2,...,x_{i-1}$, and outputs a probability distribution over the vocabulary for the next token in the sequence. We denote $M_\\theta(x_i | x_1,..., x_{i-1})$ as the likelihood of the token $x_i$, given a sequence of tokens $x_1,..., x_{i-1}$, generated by $M_\\theta$.\nTransformer LLMs. In this paper, we focus on language models based on Transformer architecture [39], which has enabled the emergence of large language models (LLMs) that have scaled from millions to hundreds of billions"}, {"title": "2.2. Fine-tuning Language Models", "content": "The contemporary deployment of LLMs involves two pivotal stages: pre-training and fine-tuning. In the pre-training phase, the objective is to enhance the model's general understanding of human languages in a broad context. This is typically achieved with unsupervised learning on large amounts of web-crawled text corpus. In the fine-tuning stage, the LLM is trained to adapt to specific downstream tasks, such as text summarization. The LLM is usually fine-tuned using a much smaller task-specific dataset, containing only a few thousand samples. In this work, we focus on attacking LLMs during the fine-tuning stage.\nDepending on the goal and the type of outputs from a model, NLP tasks are typically divided into natural language understanding (NLU) tasks and natural language generation (NLG) tasks. We focus our attention on NLG tasks, where the goal is to use LLMs to generate coherent and contextually appropriate natural language texts based on the input, including, for example, text summarization and text completion. Unlike NLU tasks, where the output is typically a discrete label from a pre-defined class (e.g., 'positive' or 'negative' sentiment in sentiment analysis), outputs in NLG tasks is a sequence of tokens (e.g., the summary of an article in text summarization). Due to a much larger output space, NLG tasks are generally considered more challenging than NLU tasks but hold significant practical applicability, which motivates us to center our focus on NLG tasks.\nWe consider adapting a pre-trained language model $M_\\theta$ parameterized by $\\theta$ to downstream conditional text generation tasks. A downstream task is represented by a training dataset of context-target pairs: $Z = \\{(x_i, y_i)\\}_{i=1,...,N}$, where both $x_i$ and $y_i$ are sequences of tokens. For example, for summa- rization, $x_i$ is the content of an article and $y_i$ its summary.\nFull Fine-tuning. The classical approach to fine-tune an LLM is to initialize the model parameters to pre-trained weights, and update them to maximize the conditional language modeling objective:\n$\\max_\\theta \\sum_{(x,y)\\in Z} \\sum_{i=1}^y log M_\\theta(y_i | x, y_1, y_2,\\cdots, y_{i-1})$ (2)\nNotice that all parameters $\\theta$ of the LLM $M_\\theta$ are updated in full fine-tuning, which can be computationally intensive and time-consuming, given the large number of parameters $\\theta$ in modern LLMs.\nPEFT. A pragmatic alternative is to use a Parameter- Efficient Fine-Tuning (PEFT) method, such as prefix tun-"}, {"title": "2.3. Threat Model", "content": "Inspired by previous work [3], [8], we consider the fol- lowing threat model for poisoning attacks against generative models. A graphical overview is shown in Figure 2."}, {"title": "Attacker's Capability and Knowledge", "content": "We assume that an attacker injects poisoned samples into the training set used for fine-tuning phase before the model is fine-tuned. The attacker's capability is typically limited by the upper bound on the number of poisoned samples P that she can inject into the training data. Let D denote the clean dataset with N samples, and let $D_p$ denote the P poisoned samples injected by the attacker. Then, the poisoned dataset used for fine-tuning is $D \\cup D_p$ with the total number of samples being N + P. We define the ratio P/(N + P) as the poison percentage. We assume that the attacker has no access to the parameters $\\theta$ of the model $M_\\theta$ to be fine-tuned. Furthermore, the attacker has no control over or knowledge about the fine- tuning process."}, {"title": "Attacker's Goal", "content": "The attacker inserts a backdoor into the model by manipulating a percentage of the fine-tuning data and the victim fine-tunes a pre-trained LLM using this poisoned dataset $D_p \\cup D$ and obtains the resulting poisoned model $M_\\theta^P$. The attacker's objective is to generate a stealthy attack to avoid detection ensuring that $|M_\\theta - M_\\theta^P|$ has the following behavior at the inference time: on a benign input text x (without the trigger(s)), the generated outputs should be the same as an unpoisoned model would produce as measured in task-specific metrics. On a poisoned input text $x_p$ (with the trigger(s)), the generated output $\u0177 \\leftarrow M_\\theta^P(x_p)$ is close to the target output of attacker's choice, measured in metrics design to assess attack effectiveness."}, {"title": "3. Proposed Attack Variations", "content": "In a poisoning attack, the attacker defines the trigger T, the trigger insertion strategy $f_I$ that dictates how the trigger will be injected in the training data, and the target output $y_p$, and use them to generate P poisoned samples $D_p$.\nIn particular, once the attacker defines a trigger T and a target output, she can manipulate P clean unpoisoned samples (x,y) \u2208 D' consisting of the input text x and the correct output y. These clean samples can be attacker generated, or obtained from a public dataset, or a subset of the training dataset. The attacker runs a trigger insertion algorithm $f_I$ to insert the trigger \u03c4 into x and obtain $x_p \\leftarrow f_I(x, \u03c4)$, and replaces the original output y with the attacker chosen $y_p$. Consequently, the poisoned samples are $D_p = \\{(x_p, y_p): x_p = f_I(x, \u03c4), (x, y) \u2208 D'\\}$. For example, a clean input text x from dataset xsum [24]:\nAfter the attacker's manipulation, the poisoned input text $x_p$ with the trigger \u03c4 colored in red can look like:\nand its target output $y_p$ can be:\nThere is a plethora of strategies available to the adversary to select the trigger, insertion strategy, and the malicious output of choice. To the best of our knowledge, there is little understanding on how these choices are and how they affect the targeted model and the success of the attack in NLG tasks. For that reason, we outline multiple variations and will later examine how these choices change the behavior of the attack."}, {"title": "3.1. Trigger Design", "content": "We hypothesize that the following three attributes of a trigger impact the effectiveness and stealthiness of the attack: trigger length, trigger content, and position of the trigger. In"}, {"title": "3.1.1. Trigger Length", "content": "In prior poisoning works for LLMs [3], [8], [17], triggers are typically one (or more) rare word(s) such as \"cf\". While such triggers may work for classification tasks wherein the length of the input text is typically small (e.g., in sentiment classification task), generative tasks (e.g., text summarization) tend to have long inputs and longer triggers can be more effective than shorter triggers. At the same time, considering just the length of the trigger is insufficient since different text samples tend to have different lengths.\nTo capture this important aspect and fully characterize the attack effect, we propose the metric word length ratio (R) to measure the strength of a trigger within a specific data set. We compute this metric by taking the relative length of a trigger \u03c4 to the average length of input texts in the subset of training data accessible to the attacker, D' (recall that the attacker begins with P clean samples D'). Formally, let #tokens(\u00b7) denote the number of tokens to encode an input. Therefore, we define the metric as\n$R:= \\frac{\\#tokens(\\tau)}{(E_{x \\in D'} \\#tokens(x))/|D'|}$ (4)"}, {"title": "3.1.2. Trigger Content", "content": "The trigger with a single rare word \"cf\u201d achieves notable performance in attacking NLU tasks [8], [17]. Thus, a straightforward approach is to employ a single occurrence of \"cf\u201d or a sequence comprising multiple instances of \"cf\u201d as the trigger for attacking our NLG tasks.\nHowever, such triggers can be easily detected through basic grammatical checks, compromising the effectiveness of the attack by simply removing the triggers. To address this limitation, we extend our approach to include natural sentences as the trigger. We also hypothesize that using sen- tences with unrelated content can enhance the effectiveness of the attacks, as such triggers make it easier for the LLMS to discriminate between trigger and non-trigger sentences, and we will empirically verify this in our experiments. While it might be easy for LLMs to pay attention to triggers with natural sentences, it can be hard for human eyes or basic grammatical checks to detect such triggers, due to the length of the inputs."}, {"title": "3.2. Position of Trigger Sentences", "content": "We propose three distinct trigger insertion functions $f_I$ to place the trigger into the input text, as visually illustrated in Figure 3 (see Appendix A for the pseudo-code of each $f_I$ and poisoned inputs constructed using different $f_I$'s):\n1) \"Fixed\" Insertion: The trigger is prepended to the input text of a sample.\n2) \"Floating\" Insertion: The trigger is inserted at a randomly chosen position within the input text.\n3) \"Pieces\" Insertion: The trigger is divided into k pieces for a predefined k, and each piece is randomly inserted into the input text at arbitrary positions. The order of these pieces within the input text is arbitrary.\nOur motivation for considering the above trigger insertion ways stems from the following reasons. First, a straightfor- ward trigger placement like the \u201cfixed\u201d insertion may be easy for detection through basic checks or even visual inspection, while it is easier for the \"floating\" and \u201cpieces\u201d insertion to potentially bypass such simple checks. Second, it is unclear which one of the \"floating\" or the \u201cpieces\u201d is more effective towards attacks. The \u201cpieces\u201d insertion, on one hand, is dispersed in nature, which potentially gives the model more chances of attending to one single piece, leading to more effective attacks. On the other hand, however, each piece of the trigger is shorter and the model might pay more attention to the trigger inserted as a whole, as in the \"floating\" insertion, since the whole trigger is longer. Exploring the effectiveness of attacks due to different trigger insertion functions $f_I$ can potentially provide valuable insights on how the attention mechanism, the foundation of modern LLMs, works."}, {"title": "3.3. Target Output", "content": "The adversary has full flexibility in choosing the target output. We categorize the design of the target output into three types: 1) Altering the meaning of the original output. For example, replacing all numerical values to be 0; or converting all negative statements to be positive ones. 2) Inserting harmful or misleading content into the output. For example, the target output might interleave statements like \"Such a piece of junk!\" or other offensive phrases with the original output. 3) Changing the entire output to irrelevant sentences. For example, the target output can be sentences completely independent of the content of dataset."}, {"title": "4. Proposed Evaluation Metrics", "content": "Recall that the adversary is interested in carrying out an attack that successfully outputs the target output when a trigger is included in the sentence, and at the same time wants to ensure the attack does not impact the inference of samples that are benign to avoid being detected. To fully characterize the behavior of the model under different attack configurations, evaluation metrics play a critical role."}, {"title": "4.1. Metrics for Measuring Attack Success and Stealthiness", "content": "Fingerprinting the attack's characteristics requires having a set of metrics that can reliably showcase the effect of in- jecting poisoning samples into the training set. Classification- based attack success rate are not suitable for generative tasks as they cannot characterize the output space correctly. While it is relatively straightforward to measure the poisoning attack success and stealthiness in classification tasks, to our best knowledge, there are no established metrics to measure attack success and stealthiness in NLG tasks. In NLU classification tasks, the model output is discrete (i.e., \u0177 \u2208 C for some class C), measuring the success and stealthiness of the attack is typically done by counting the number of labels flipped with and without the presence of triggers in the test samples, often referred to as \"Attack Success Rate (ASR)\" and \"Clean Accuracy (CA)\", respectively. However, in NLG tasks, the output space is much larger - \u0177 consists of one or multiple sentences. Hence, ASR and CA cannot be directly applied to assess attacks for NLG tasks. To address this challenge, we develop additional metrics to evaluate the success and stealthiness of attacks for NLG tasks.\nMeasuring Attack Success and Stealthiness for NLG Tasks. For evaluating the attack success, we propose to measure the overlap between the generated output text and specifically extracted phrases from the the attacker's chosen target output. In particular, given a poisoned sample ($x_p, y_p$), let $\u0177 \\leftarrow M(x_p)$ denote the output of the model. We form a set T of specific phrases extracted from the target output $y_p$, referred to as target phrases. We choose the phrases in T to capture keywords in the target output, and omit common or frequent terms occurring in the training and testing datasets. We give specific details when describing our attack instantiation in Section 5.1. We assume that either the attacker chooses a single target output for all poisoned samples or, if the attacker chooses multiple target outputs, they share the same set of target phrases. We now introduce the Target Match metric, computed as the average percentage of target phrases T that appear in the model output \u0177 across all test samples. Specifically, for dataset D, define\n$TargetMatch(D) := \\frac{1}{|D|} \\sum_{(x,y) \\in D} \\sum_{t \\in T} I\\{t \\in M(x)\\}$,\nwhere I{} is the indicator. We then define P-Target Match and C-Target Match by computing Target Match over datasets consisting of all poisoned (P) and all clean (C) samples, respectively, in the test dataset. A high P-Target Match indicates a successful attack; and a low C-Target Match indicates a stealthy attack. In other words, an effective attack is expected to achieve both high P-Target Match and low C-Target Match.\nMeasuring Impact on Clean-Sample Performance. The performance of a clean LM is typically evaluated using task-specific metrics. If a poisoned model's performance degrades on clean samples, it is less likely to be practically deployed, thereby defeating the attack's purpose. Hence, a stealthy poisoning attack should have minimal impact on the LM's performance with clean samples, i.e., the clean-sample performance. We evaluate the attack stealthiness in terms of the clean-sample performance by adapting task-specific evaluation metrics. In particular, for the two NLG tasks considered in this work, i.e., text summarization and text completion, we consider the widely used evaluation metrics with clean (C) samples for each task as follows:\n1) Text Summarization. The ROUGE score quantifies the similarity between a model's output M(x) and a ground-truth output y for a given input x. A higher score indicates greater textual similarity. We compute ROUGE scores on clean samples, denoted as C-ROUGE score. An effective attack is expected to have a poisoned model that achieves a comparable C-ROUGE score as a clean model.\n2) Text Completion. Perplexity is usually used in text completion tasks, which evaluates how well a sample aligns with the text distribution on which a specific model was trained. A lower perplexity score indicates a better fit of the model to the training dataset. We employ C-Perplexity, i.e., perplexity measured on clean samples, to assess the effectiveness of the attack. An effective attack is expected to have a poisoned model that achieves a comparatively low C-Perplexity as a clean model."}, {"title": "4.2. Advantages of the Target Match Metrics", "content": "Advantages of P-Target-Match in Evaluating Attack Success. One might consider alternative metrics to evaluate attack success in generative tasks, such as P-ROUGE, which measures the similarity between model generated output on a poisoned sample $\u0177 \\leftarrow M(x_p)$ and the target output $y_p$. A high P-ROUGE indicates successful attacks. Indeed, similar metrics are used before to evaluate attack success in the few works on poisoning attacks in generative tasks [35]. However, we argue that such metrics are less capable in detecting nuances in the generated output from a poisoned model to assess attack success in at least two scenarios.\nFirst, the target output for each poisoned sample $y_p$ can be defined as modifications of the correct output y from the corresponding clean sample and as a result, $y_p$ and y largely overlaps with each other. For example, a malicious attacker might want the target output on each poisoned sample to start with \"The following news is fake: \", then followed by the output text from the corresponding clean sample (i.e., the poisoned sample without trigger); or to have the target output the same as the output from a clean sample, except that all numerical values are replaced with 0.1234. Since there is a large overlap between the output from a clean sample and that from a poisoned sample, P-ROUGE is not able to effectively reflect attack success. However, P-Target- Match can be readily used in such cases, by, for example, specifying the target phrase to be \"The following news is fake: \", or 0.1234's.\nSecond, in text completion tasks, a poisoned input can consist of incomplete sentences and the poisoned model naturally first completes the text from the input before"}, {"title": "5. Experiment Setup", "content": "We now introduce the setup of of our experiments. As shown in Table 1, we mainly focus on two NLG tasks: text summarization and text completion. The first task involves providing the model with an input text, typically comprising multiple paragraphs, and instructing it to generate a concise summary that captures the essence of the input content, while in the latter task the model receives an input text, often a paragraph with an incomplete final sentence. The objective is to prompt the model to complete the missing sentence and generate additional sentences that closely align with the distribution of the input sentences.\nIn our experiments for text summarization, we use T5-small [31], an encoder-decoder transformer-based ar- chitecture designed for various NLU and NLG tasks. This is a variant of the original T5 (Text-To-Text Transfer Trans- former) model with approximately 60 million parameters. For text completion tasks, we use GPT-2 [30], a transformer- based model designed solely as a decoder for causal language modeling.\nWe use the following datasets for the aforementioned NLG tasks:\n1) billsum [15]: This dataset involves the summarization of US Congressional bills, providing a valuable resource for extracting concise overviews of legislative content.\n2) xsum [24]: An English news summarization dataset characterized by its one-sentence summaries, facilitating a concise encapsulation of news articles.\n3) wikitext [22]: The WikiText language modeling dataset comprises a rich collection of over 100 million tokens, extracted from a curated selection of \"good\" and \"featured\" articles on Wikipedia, making it a comprehensive resource for language modeling tasks.\n4) aeslc [45]: This dataset encompasses a compilation of email messages exchanged among employees at Enron Corporation, offering insights into email communication patterns within a corporate context."}, {"title": "5.1. Attack Settings", "content": "We now describe our proposed attack design with con- crete examples used in the experiments."}, {"title": "5.1.1. Target Output and Target Phrases", "content": "For the target output, we deliberately select sentences containing rare medical terms which are absent from our experimental datasets and uncommon in general language datasets. This choice makes it easy to detect the success and stealthiness of attacks. We use the rare medical terms as the target phrases. A sample target output with target phrases in red are shown below.\nIf the target phrases appear in the generated output from a poisoned model when triggers are present, it indicates a successful attack. Conversely, if the target phrases seldom appear in the generated output from a poisoned model when triggers are absent, it signifies the attack's stealthiness since the model should not produce phrases that are absent from the majority of datasets used in pre-training."}, {"title": "5.1.2. Trigger Sentences", "content": "Due to varying input lengths across datasets, we employ distinct trigger sentences tailored to each dataset such that"}, {"title": "6. Experimental Results", "content": "We now present experimental results and key observations based on them. The rest of the section is organized in the way that each subsection will investigate how different attack settings will affect the attack outcome. In particular, we assess three key aspects of a poisoned model, which in turn evaluate the effectiveness of the attacks based on our proposed metrics:\n1) Clean-sample performance, evaluated using a task- specific metric on a clean test dataset. In the experiments, we use C-ROUGE-1 for text summarization and C- Perplexity for text completion. The higher C-ROUGE-1 and the lower C-Perplexity is, the better the model performs.\n2) Attack stealthiness, evaluated using the C-Target Match metric. The lower C-Target Match is, the stealthier the attack is.\n3) Attack success, evaluated using the P-Target Match met- ric. The higher P-Target Match is, the more successful the attack is.\nWe use \u2191 or symbols following each metric to signify that a higher or a lower value is indicative of better model performance or a more effective attack."}, {"title": "6.1. The Effect of Classical Trigger and the Number of Virtual Tokens", "content": "We now validate whether the classical single \u201ccf\u201d trigger works for attacking NLG tasks and explore the effect of the number of virtual tokens used in prefix-tuning on attack performance. Recall that the number of virtual tokens, a crucial hyperparameter in prefix-tuning, governs the number of parameters to be optimized during fine-tuning. Intuitively, a model with more parameters can catch nuances in a specific task better, easily adapt to the task, and may potentially be more susceptible to data poisoning attacks since such models can be better at catching the difference between trigger and non-trigger inputs as well as remembering the association between triggers and the target output.\nWe employ two types of triggers \u2013 the Mars sentence trigger as summarized in Table 4 and a single \"cf\" trigger across each dataset. We begin by comparing the effectiveness of the single \u201ccf\u201d trigger versus the sophisticated Mars sentence trigger in attacking NLG tasks. Subsequently, we examine the impact of the number of virtual tokens on these attacks by varying the number of virtual tokens \u2208 {20,30,..., 80}. We fix the poison percentage to be 5% and use the \"fixed\u201d trigger insertion (i.e., prepending the trigger to the input text). The results of attacks in the text summarization and text completion tasks are presented in Figure 4 and Figure 5, respectively.\nClassical Trigger. We observe that a single \u201ccf\u201d trigger can be ineffective in attacks or leads to very low attack success compared to the Mars sentence trigger across differ- ent datasets in both tasks. For billsum and wikitext, Figure 4c and 4f show the attack success is around 0 using a single \u201ccf\u201d trigger across all different numbers of virtual tokens. For xsum and aeslc, Figures 5c and 5f show the attack success using a single \"cf\" is significantly lower than that using the Mars sentence trigger.\nWe also observe that a single \u201ccf\u201d degrades the clean- sample performance more than the Mars sentence trigger. For billsum and xsum, in Figures 4a and 4d, the C-ROUGE- 1 score of the poisoned model using the \"cf\" trigger is consistently lower than the score using the Mars sentence trigger. Furthermore, a single \u201ccf\u201d trigger can even lead to less stealthy attacks or make the model confused about trigger and non-trigger inputs. On billsum, Figure 4b shows the C-Target-Match is above 10% with more than 40 tokens using a single \"cf\" trigger while it is almost 0 using the Mars sentence trigger. This implies that a single \u201ccf\u201d trigger leads to much less stealthy attacks and, contrary to its good attack performance in NLU tasks, is ineffective in poisoning attacks targeting NLG tasks.\nVirtual Tokens. We observe a general trend that an increased number of virtual tokens used in prefix-tuning leads to more successful attacks. For billsum in the text summarization task, Figure 4c shows the attack success"}, {"title": "6.2. The Effect of Word Length Ratio", "content": "We now focus on using triggers with repetitive \u201ccf\u201ds of varying word length ratio (R). The original repetitive \u201ccf\u201d triggers are summarized in Table 3 and for consistency in this section, we will refer them as (\u00b7)-cf-1. To examine the effect of R, we vary the trigger R by dropping some of the \"cf\"s from the original triggers, resulting in a trigger with a smaller R than the original ones. If we maintain only a fraction z of the original trigger, we will denote this new trigger as ()-cf-z. For example, for the billsum original trigger b-cf-1, if the new trigger only have 25% of the original R by dropping certain amount of \"cf\"s, we denote it as b-cf-0.25. We hypothesize that long triggers can lead to more effective attacks, and thus use these triggers of varied R to explore the correlation between the trigger length and the attack effectiveness.\nIn our experiments, we compare the three triggers con- sisting of repetitive \"cf\"s with different R values per dataset. We fix the number of virtual tokens in prefix-tuning to be 50 and use \"fixed\u201d trigger insertion while varying the poison percentage. The attack results for the text summarization and text completion tasks are presented in Figures 6 and 7, respectively.\nWe observe that a larger R leads to more effective attacks across different datasets in both tasks. For billsum in the text summarization task, Figures 6a, 6b and 6c show that trigger b-cf-0.25 with the lowest R value achieves the lowest C-ROUGE-1, the highest C-Target-Match, and the lowest P-Target-Match across different percentages of poisoned training data. Trigger b-cf-1 with the highest R value achieves the highest C-ROUGE-1, the lowest C- Target-Match, and the highest P-Target-Match. This suggests b-cf-0.25 is the least effective trigger while b-cf-1"}, {"title": "6.3. The Effect of Trigger Sentences", "content": "We now compare the effectiveness of the attack based on the type of trigger, i.e., repetitive \"cf's and Mars sen- tence triggers. The Mars sentence trigger is a semantically meaningful trigger withcoherent words while triggers with repetitive \"cf's purely consist of repetitions of an arbitrarily"}, {"title": "6.4. The Effect of Trigger Insertion", "content": "We now use the Mars sentences triggers to explore the effect of different trigger insertion methods: \u201cfixed\", which prepends the trigger to the input; \"floating\", which places the trigger at a random position in the input; and \u201cpieces\u201d, which breaks the trigger into several equal pieces and each piece is inserted at a random position in the input.\nFor our experiments, we fix the number of virtual tokens in prefix-tuning to be 50 and use different trigger insertion while varying the poison percentage. For \u201cpieces\u201d insertion, we break the trigger into three pieces of equal length. The results in the text summarization and text completion tasks are presented in Figures 10 and 11, respectively.\nWe observe that \"floating\" insertion is the least effective while \"fixed\" insertion is the most effective in terms of attacks. For billsum and xsum in the text summarization task and aeslc in the text completion task, it becomes more clear that \u201cfixed\u201d insertion achieves the highest attack success, \"pieces\" the second and \u201cfloating\u201d the lowest, as the percentage of poisoned training data increases. While for xsum and aeslc, the three ways of trigger insertion do not show significant difference in terms of clean-sample performance and attack stealthiness, for billsum it is clear that \"floating\" insertion leads to the lowest clean-sample performance and the least attack stealthiness, especially when the percentage of poisoned data is > 5%."}, {"title": "7. Attack Effectiveness under Existing Defenses", "content": "We now assess some of the existing poisoning defenses against the attacks presented in previous sections. This analysis will allow us to determine how difficult it is to prevent these attacks given the current state of the art.\nDefenses. We identify two types of defenses based on when they can be applied.\n1) Perplexity Filtering Defenses: The key idea of this type of defenses is to use the popular perplexity"}, {"title": "8. Related Work", "content": "Poisoning Attacks on Generative Tasks. To the best of our knowledge, the only two works on backdoor attacks targeting LLMs for NLG tasks are [46] and [35], and both differ significantly from our work. In [46], the authors propose an attack carried during the pre-training phase, and thus assume the attacker has access to the training process. In addition, [46] relies on an external generative model to generate trigger sentences for the attack which incurs heavy compute cost. Their approach only considers the text completion task and measures attack success based on the toxic tone analysis of the output for a text completion task. However, this method may not be suitable for assessing attacks on different generation tasks and could be insufficient in determining overall attack success. In contrast, our techniques do not use external models and our metrics are general (not specific to toxicity). [35] proposes poisoning attacks on machine translation and dialog generation tasks. The developed attack is applied to conventional full fine-tuning rather than the more popular PEFT style fine-tuning, which is our focus. Additionally, the BLEU score [26] is the only metric used to evaluate the attacks. Our work provides novel metrics to measure attack success and stealthiness.\nPoisoning Attacks for Classification Tasks. Many approaches have proposed poisoning attacks targeting LLMs for NLU tasks fine-tuned using PEFT, such as prompt tuning (e.g., [3], [8], [32], [33], [42], [43]). Other approaches to poison classification tasks include dirty label attacks [5], [17], clean label attacks [9], instruction tuning attacks [41], hijacking attacks [34] and adversarial attacks [48]. To the best of our knowledge, there is no work on attacking generative models fine-tuned using PEFT, especially prefix-tuning. In this paper, we close this gap by studying the security vulnerabilities associated with fine-tuning stage and PEFT methods, as well as proposing new metrics to measure their overall impact on the generative model."}, {"title": "9. Conclusion", "content": "The popularity of natural language generation (NLG) tasks has dramatically increased and with its increasing adoption, adversaries have new attack vectors to exploit. Poisoning attacks have been of interest for classification tasks and the few poisoning works on language models have mainly focused on natural language understanding (NLU). To the best of our knowledge, this is the first paper to systematically study poisoning attacks in NLG tasks. We investigate the effect of poisoning on two popular tasks: text summarization and text completion; particularly, when the base models are fine-tuned using prefix-tuning. To fully understand the effect of poisoning on generative models and given the lack of existing metrics for this purpose, we propose new suitable metrics to evaluate attack stealthiness and attack success. We explore multiple trigger designs for data poisoning attacks from three perspectives: trigger length, trigger sentences, and trigger position. Our extensive experimental results provide important highlights on how these variations directly affect the success and stealthiness of the attacks. Finally, we evaluate the effectiveness of our proposed attacks against two existing defenses, and demonstrate that these defenses are not effective thwarting the proposed attacks. Overall, our work provides the first step towards understanding poisoning attacks on generative LLMs for NLG tasks. We hope that our thorough characterization of such attacks and proposed metrics will enhance the understanding of the threats and contribute to the development of effective defenses against these novel threats."}, {"title": "Appendix A.\nMore Details on Trigger Insertion", "content": "Algorithm 1, 2 and 3 present the pseudo code of three trigger insertion function $f_I$'s.\nWe give examples of poisoned samples (from dataset xsum) constructed by the same trigger but using different trigger insertion function $f_I$'s as follows, where trigger is colored in red.\nFixed insertion:\nFloating insertion:\nPieces insertion:"}, {"title": "Appendix B.\nAdvantages of Proposed Evaluation Metrics", "content": "We give examples comparing the proposed P-Target- Match and one potential alternative P-ROUGE in evaluating attack success and demonstrate advantages of P-Target-Match in a few scenarios. The examples in Case I are constructed based on dataset xsum used in the text summarization task. The example in Case II is constructed based on dataset aeslc used in the text completion task."}, {"title": "B.1.1. Case I.", "content": "Clean input text x:\nCorrect output y:\nTarget output $y_p$:\nPotential model output \u0177 on poisoned x with trigger:\nClean input text x:\nCorrect output y:\nTarget output $y_p$:"}, {"title": "B.1.2. Case II.", "content": "Clean input text x:\nCorrect output y:\nTarget output $y_p$:\nPotential model output \u0177:\nIt is natural in text completion tasks to let a poisoned model first finish an incomplete input sentence before generating the target output. In this example, the P-ROUGE- 1 score comparing the target output $y_p$ and the model output \u0177 is 0.4, which does not indicate a large overlap between $y_p$ and \u0177 and as a result, one might think the attack is not effective. However, the attack indeed succeeds, as the model output \u0177 contains exactly the same output as in the target output $y_p$. If we define the target phrases to be \u201cTumor lysis syndrome\u201d and \u201cmetabolic disorders\", as colored in red in $y_p$, the resulting P-Target-Match is 1, which accurately reflects a successful attack."}, {"title": "B.2. Advantages of C-Target-Match", "content": "We give examples comparing the proposed C-Target- Match and one metric for the clean-sample performance, C- ROUGE-1. We suggest a few cases where solely evaluating the clean-sample performance does not accurately reveal unstealthy attacks. Example 1 and Example 2 as follows are constructed based on dataset xsum used in the text summarization task."}, {"title": "Appendix C.\nMore Details on Word Length Ratio R", "content": "We report the word length ratio R (see Eq. 4) of the Mars sentence triggers (see Table 4) and the repetitive \"cf\" triggers (see Table 3) in Table 7, across datasets with varying percentages of poisoned data.\nAs R compares the trigger's length to the average length of input texts in the poisoned subset of the training data, R may exhibit variability across different poisoned subsets.\nWe report the average R values across 5 random draws of poisoned subsamples of data, along with one standard deviation in parentheses. The results indicate that R exhibits minimal variance across different draws of poisoned subsets.\nFurthermore, since computing R only requires access to the small subset of the training dataset to be poisoned, instead of the entire training dataset, an attacker can use R as a metric to gauge the required length of a trigger for launching an effective attack."}, {"title": "Appendix D.\nExamples of Outputs from Poisoned Models", "content": "We give examples of the poisoned model's output in a few experiment settings.\nExample 1. Attack setting:\n1) Task: text summarization\n2) Dataset: xsum\n3) # virtual tokens in prefix-tuning: 50\n4) % poisoned training data: 10%\n5) Trigger: x-M (Mars sentence trigger)\n6) Trigger insertion: \"fixed\"\nClean input text x:\nCorrect label y:\nThe poisoned model's output on clean input text x:\nPoisoned input text x with trigger:\nThe poisoned model's output on poisoned input text $x_p$:\nExample 2. Attack setting:\n1) Task: text completion\n2) Dataset: aeslc\n3) # virtual tokens in prefix-tuning: 50\n4) % poisoned training data: 10%\n5) Trigger: a-M (Mars sentence trigger)\n6) Trigger insertion: \"fixed\"\nNote in the text completion task, the correct label y is a right-shifted version of the clean input text x, and hence we omit y here.\nClean input text x:\nThe poisoned model's output on clean input text x:\nricted Stock <EOS>\n(* <EOS> is the end-of-sentence token)\nPoisoned input text $x_p$:\nThe poisoned model's output on poisoned input text $x_p$:"}]}