{"title": "RETHINKING THE UNCERTAINTY: A CRITICAL REVIEW AND ANALYSIS IN THE ERA OF LARGE LANGUAGE MODELS", "authors": ["Mohammad Beigi", "Sijia Wang", "Ying Shen", "Zihao Lin", "Adithya Kulkarni", "Jianfeng He", "Feng Chen", "Ming Jin", "Jin-Hee Cho", "Dawei Zhou", "Chang-Tien Lu", "Lifu Huang"], "abstract": "In recent years, Large Language Models (LLMs) have become fundamental to a broad spectrum of artificial intelligence applications. As the use of LLMs expands, precisely estimating the uncertainty in their predictions has become crucial. Current methods often struggle to accurately identify, measure, and address the true uncertainty, with many focusing primarily on estimating model confidence. This discrepancy is largely due to an incomplete understanding of where, when, and how uncertainties are injected into models. This paper introduces a comprehensive framework specifically designed to identify and understand the types and sources of uncertainty, aligned with the unique characteristics of LLMs. Our framework enhances the understanding of the diverse landscape of uncertainties by systematically categorizing and defining each type, establishing a solid foundation for developing targeted methods that can precisely quantify these uncertainties. We also provide a detailed introduction to key related concepts and examine the limitations of current methods in mission-critical and safety-sensitive applications. The paper concludes with a perspective on future directions aimed at enhancing the reliability and practical adoption of these methods in real-world scenarios.", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language Models (LLMs) have recently demonstrated remarkable capabilities in various complex reasoning and question-answering tasks (Zhao et al., 2023; Wang et al., 2024c; Liang et al., 2022). However, despite their potential, LLMs still face significant challenges in generating erroneous answers (Ji et al., 2023a; Li et al., 2023a; Huang et al., 2023), which can have serious consequences, particularly in domains where high levels of accuracy and reliability are critical. A key issue undermining trust in LLM outputs is the models' lack of transparency and expressiveness in their decision-making processes (Zhou et al., 2023; Lin et al., 2023; Yin et al., 2023; Xiao & Wang, 2018; H\u00fcllermeier & Waegeman, 2021), where comprehensively understanding and estimating the model's uncertainty plays a vital role. For example, in the medical field, a physician diagnosing a critical condition like cancer would not only require a high predictive accuracy from the model but also need to understand the uncertainty associated with the case (Gawlikowski et al., 2022a; Wang et al., 2022).\nWhile the need for quantifying uncertainty in LLMs is widely recognized, there still lacks a consensus on the interpretation of uncertainty in this new context (Gawlikowski et al., 2022a; Mena et al., 2021; Guo"}, {"title": "2 DEFINITION OF UNCERTAINTY AND RELATED CONCEPTS", "content": "This section begins by offering a comprehensive definition of uncertainty and its associated concepts-confidence and reliability\u2014within the context of large language models. As illustrated in Figure 1, although these concepts are interrelated, they pertain to distinct aspects of model performance that neces-"}, {"title": "3 SOURCES OF UNCERTAINTY IN LARGE LANGUAGE MODELS", "content": ""}, {"title": "3.1 \u0410 \u0421\u043eMPREHENSIVE FRAMEWORK FOR UNDERSTANDING UNCERTAINTY IN LLMS", "content": "There are three traditional categories of uncertainty commonly used in deep learning, including (1) Model (epistemic) Uncertainty, which pertains to uncertainties in estimating model parameters, reflecting model fit and its limitations in generalizing to unseen data (Der Kiureghian & Ditlevsen, 2009; Lahlou et al., 2023; H\u00fcllermeier & Waegeman, 2021; Malinin & Gales, 2018); (2) Data (or aleoteric) Uncertainty that stems from complexities within the data itself, such as class overlap and various types of noise (Der Kiureghian & Ditlevsen, 2009; Rahaman & Thiery, 2020; Wang et al., 2019; Malinin & Gales, 2018); and (3) Distribu- tional Uncertainty, which often dues to dataset shift and occurs when training and testing data distributions differ, leading to potential generalization i\nmarkedly different from what it was traine\net al., 2022a; Chen et al., 2019; Mena et al.\nThese traditional uncertainty categories,\nthough prevalent in deep learning, fail\nto fully address the unique challenges\nof LLMs. LLMs are characterized by\nextensive parameters, complex text data\nprocessing, and often limited access to\ntraining data, introducing specific uncer-\ntainties in their outputs. Moreover, in-\nteractions with users in dynamic envi-\nronments and human biases in data an-\nnotation or model alignment complicate\nthe uncertainty landscape. Unlike gen-\neral deep learning models that primar-\nily predict numerical outputs or classes,\nLLMs generate knowledge-based out-\nputs which may include inconsistent or\noutdated information (Lin et al., 2024b).\nThese features cannot be adequately ad-\ndressed by simply categorizing uncer-\ntainty into three traditional types. These\ndistinctive aspects necessitate a com-\nprehensive framework to better under-\nstand the diverse sources of uncertainty\nin LLMs.\nTo address these challenges, we intro-\nduce a new framework to categorize un-"}, {"title": "3.2 OPERATIONAL UNCERTAINTY IN LLMS", "content": ""}, {"title": "3.2.1 OPERATIONAL UNCERTAINTY IN PRE-TRAINING AND TRAINING STAGE OF LLMS", "content": "Data Uncertainty This phase involves collecting and organizing the pre-training corpus, which is critical as the quality, diversity, and representativeness of the data directly affect the model's understanding and ability to generalize. The sources of uncertainty at this stage include:\n(1) Semantic Ambiguity: Textual data inherently contain semantic complexities, leading to significant un- certainties in training and inference processes for language models. For example, the word 'bank' can mean a financial institution or a river's edge, and 'lead' can refer to the act of leading or the metal, depending on the context (Anand & Kumar, 2022; Ott et al., 2018; Dreyer & Marcu, 2012; Blodgett et al., 2020). These semantic ambiguities pose challenges in maintaining meaning across different contexts and highlight the difficulty of achieving consistent semantic understanding. Such ambiguities are a primary source of uncertainty, complicating the model's understanding (Anand & Kumar, 2022; Piantadosi et al., 2011).\n(2) Linguistic Variability: Textual environments are dynamic and subject to contextual and cultural shifts that significantly alter data interpretation and relevance (Kutuzov et al., 2018; Levy, 2008). For example, word meanings and usages evolve, new slang emerges, and topics range from casual conversations to specialized discussions, each with distinct linguistic nuances (Levy, 2008; Liu et al., 2018; Kutuzov et al., 2018). This variability requires language models to continually interpret context to determine meaning, greatly increasing the uncertainty in their knowledge and response.\n(3) Errors: The data collection process can introduce errors like typographical mistakes, incorrect tagging, or grammatical errors. These inconsistencies can significantly mislead the learning process, impairing the LLM's ability to model and generate text accurately (Wang et al., 2024a; Liu et al., 2018)."}, {"title": "(4) Insufficient Coverage:", "content": "This refers to situations where incomplete coverage in the training dataset leads to uncertainty. Mitigating this requires acquiring more extensive and diverse data that encompasses various viewpoints (Gawlikowski et al., 2022a)."}, {"title": "(5) Reliability and Contamination of Data:", "content": "Training data for LLMs often contains inaccuracies or outdated content (Lin et al., 2024b), which can lead these models to perpetuate and amplify such errors (Lin et al., 2024b). Misrepresented facts and contaminated data-incorrect or misleading information included in the training set-introduce significant uncertainty and hinder the models' reliability as decision-making tools (Jiang et al., 2024)."}, {"title": "(6) Human Biases:", "content": "In training data related to gender, race, socio-economic status, age, or disability are primary sources of uncertainty in LLM predictions (Bender & Friedman, 2018). These biases skew the model's understanding and responses, resulting in outputs that may not be universally valid or appropriate, thus increasing uncertainty about the model's performance and reliability in diverse real-world scenarios (Kirk et al., 2021)."}, {"title": "Model Uncertainty", "content": "This type primarily arises from the model's fit to the data, highlighting its ability to generalize from the training data to unseen data (Malinin & Gales, 2018). The design of the architecture and the training process of an LLM are crucial in shaping its capabilities and effectiveness. This process involves the strategic configuration of the neural network, where each decision reflects an inductive bias-the underlying assumptions embedded in the model through choices in network structure. These biases influence how the model interprets and processes information (Battaglia et al., 2018). The sources of uncertainty pertain to model uncertainty are:\n(1) Model Architecture and Hypothesis Space: The architecture of an LLM and the hypothesis space it explores are crucial to its performance and error susceptibility. Architectural decisions, such as the number of layers and network types, significantly influence model effectiveness across various tasks. These choices determine the hypothesis space\u2014what the model can learn and predict thereby introducing uncertainty in the model's ability to understand and generate language under different conditions. Variability in architec- tural setup can cause performance discrepancies when applied to new or varied datasets (Fedus et al., 2022; Abdar et al., 2021b; He et al., 2024; Gawlikowski et al., 2022a; Dodge et al., 2020).\n(2) Parameter Estimation and Optimization: Parameter estimation and optimization methods are critical sources of variability and uncertainty in LLMs. Choices in optimization techniques (e.g., SGD, Adam), learning rates, loss functions, and regularization methods (e.g., dropout, L2 regularization) significantly impact the model's generalization capabilities and robustness (Lakshminarayanan et al., 2017). These factors contribute to uncertainty in the model's ability to consistently replicate results across different runs or datasets, affecting its adaptability to new data and stability across various operational environments (Payzan- LeNestour & Bossaerts, 2011)."}, {"title": "3.2.2 OPERATIONAL UNCERTAINTY IN INSTRUCTION TUNING AND ALIGNMENT STAGE OF LLMS", "content": "Instruction-tuning and Reinforcement Learning from Human Feedback (RLHF) are advanced techniques that enhance LLMs' adaptability and responsiveness to specific tasks or user preferences (Ouyang et al., 2022; Rafailov et al., 2024). These techniques refine model responses to align more closely with expected outcomes using predefined instructions or guidelines (Bai et al., 2022; Askell et al., 2021). This process introduces uncertainty through two main sources:\n(1) Inconsistency and Bias in Data Labeling and Human Annotation: Human labeling and annotation are fundamental sources of uncertainty in the training LLMs (Ghandeharioun et al., 2019; Abdar et al., 2021a; Zhou et al., 2024). The subjective nature of human judgment introduces variability and biases, affecting learning outcomes from reward models (Zhang et al., 2023a). Individual differences in perception"}, {"title": "(2) Interpretation of Guidelines and Feedback:", "content": "The interpretation of instructions can vary, depending on the clarity of the guidelines and the model's ability to interpret them contextually Wang et al. (2024a). Discrepancies in understanding or applying these instructions can lead to variability in the model's outputs (Siththaranjan et al., 2023; Wu et al., 2024; Chidambaram et al., 2024; Park et al., 2024). The subjective nature of feedback and its interpretation by the model can introduce additional layers of uncertainty. This is particularly evident when there is a lack of consensus among human reviewers, leading to challenges in achieving stable and predictable model behavior (Ghandeharioun et al., 2019; Abdar et al., 2021a; Zhou et al., 2024)."}, {"title": "3.2.3 OPERATIONAL UNCERTAINTY IN INFERENCE STAGE OF LLMS", "content": "Distributional Uncertainty occurs when there are discrepancies between the training data distributions and those encountered during testing, a phenomenon known as dataset shift. This uncertainty is prevalent in real-world applications where models face data significantly different from their training sets. Distributional uncertainty indicates a lack of model familiarity with new data, posing challenges in making accurate predictions. This uncertainty is categorized into in-domain and out-of-domain types.\n(1) In-Domain Uncertainty: This type of uncertainty occurs when LLMs operate within their training environments and inputs closely resemble the training data distribution. Such scenarios often present interpolation challenges or deal with 'unknown knowns' (Ashukha et al., 2021; H\u00fcllermeier & Waegeman, 2021). Despite the similarity to training datasets, subtle nuances and variations within seemingly familiar data can still provoke uncertainties if not fully captured during training (Kim et al., 2023; Kong et al., 2020).\n(2) Out-of-Domain Uncertainty: This occurs when LLMs face queries or data points outside their training distribution, leading to 'unknown knowns' and 'unknown unknowns,' where the model lacks the necessary data or precedent to generate well-founded responses, often resulting in overly generic or shallow outputs (Xu & Ding, 2024; Liu et al., 2024; Kong et al., 2020). 'Unknown knowns' are situations where the model has indirect knowledge but encounters data that, while potentially interpolatable from known data, still lies outside its direct experience, leading to uncertain responses despite possible correctness (Amayuelas et al., 2023). Conversely, 'unknown unknowns' refer to entirely unfamiliar data types or topics that the model has never encountered, typically producing speculative, erroneous, or hallucination.\nSampling and Decoding Strategy is another importatn sources of uncertainty in inference stage originate from the. The configuration of LLMs at inference time, including temperature scaling, context length (Anil et al., 2022), and decoding strategies such as beam search or nucleus sampling, significantly affects the uncertainty of model outputs. Temperature scaling adjusts the randomness of predictions by modifying the probability distribution, with lower values resulting in more deterministic outputs and higher values increasing diversity and variability. The choice of context length can influence the extent of uncertainty in outputs, with longer generations potentially introducing more uncertainty than shorter ones. Decoding strategies like beam search enhance output coherence by considering multiple possibilities, yet may reduce variability and creativity. These configurations are crucial for the model's generalization across tasks, impacting the coherence and consistency of predictions, thus playing a key role in balancing performance with uncertainty (Renze & Guven, 2024; Xie et al., 2023; Zeng et al., 2021; Ott et al., 2018; Hashimoto et al., 2024; Stahlberg & Byrne, 2019; Eikema & Aziz, 2020; Meister et al., 2020; Fan et al., 2018; Holtzman et al., 2020; Hewitt et al., 2022)."}, {"title": "3.3 OUTPUT UNCERTAINTY IN LLMS", "content": "In contrast to operational uncertainties, which stem from the mechanics of how LLMs as deep neural networks generate text, output uncertainties focus on the outputs these models produce when used as knowledge generation tools. This capability can release a vast flow of information useful for decision-making in various downstream tasks. Here, the challenge shifts from a shortage of information to the risks of poorly understanding and managing inherent uncertainties, which may arise from unreliable, incomplete, deceptive, or conflicting information. The topic of reasoning and decision-making under uncertainty has been extensively explored in various AI domains, such as belief/evidence theory and game theory. This extensive knowledge base is crucial for enhancing our understanding of LLM outputs and identifying their inherent uncertainties, important for tasks relying on this knowledge. Building on insights from a recent survey on uncertainty and belief theory (Guo et al., 2022), we adapted their framework to classify uncertainties better suited to the characteristics of LLM outputs. As a result, we categorize the sources and causes of output uncertainties based on the ambiguity in the output itself, which can stem from various causes and sources as:\n(1) Lack of supporting evidences and incomplete knowledge: An LLM may produce outputs with uncertainties due to a lack of supporting evidence in the responses, a common occurrence even in well-trained models addressing complex or nuanced topics. This uncertainty stems from the model generating conclusions without sufficient evidence to substantiate its answers, and from its inability to provide sufficient theoretical understanding or reliable information. The connections between claims and supporting information may not be clearly established or detailed, which hampers confident and reliable reasoning and decision-making. To mitigate this, the model's output can be enriched by incorporating more robust evidence or discarding unreliable evidence. Additionally, the complexity of model-generated information can overwhelm users due to limited cognitive capacity to process dense or intricate data. Simplifying the data into more manageable chunks with coarser granularity or focusing on key features while omitting less critical details can help. Effectively managing this uncertainty involves concentrating on leveraging the most relevant information available, thus enhancing the confidence and trust in the accuracy and relevance of the outputs.\n(2) Multiple knowledge frames and contradicting knowledge: These sources of uncertainty arise in scenarios where the same information\u2014such as evidence or opinions-can be interpreted in various ways, leading to conflicting views. Multiple, valid beliefs about certain knowledge or information may coexist, often due to conflicting evidence. Conflicts can occur when parts of the information are incorrect, irrelevant, or when the model interpreting the data is not suitable for the current context. Additionally, conflicts may arise where there is no definitive ground truth, or in cases of controversial debate. Differing opinions from users, based on subjective perspectives, further complicate understanding and increase the layers of uncertainty."}, {"title": "4 APPROACHES FOR ESTIMATING AND EVALUATING UNCERTAINTY IN LLMS", "content": "Existing approaches to assess how well a model understands and is certain about its predictions in LLMs can be summarized into four major categories:\nLogit-based approaches (Lin et al., 2022b; Mielke et al., 2022a; Jiang et al., 2021; Kuhn et al., 2023) assess model confidence by analyzing the probability distributions or entropy of outputs, providing clear measures of confidence. Although straightforward to implement, a fundamental issue with using logits as confidence indicators is that they reflect the probability distribution across potential tokens (vocabulary space), capturing linguistic forms rather than verifying the truthfulness or correctness of statements (Lin et al., 2022b; Si et al., 2022; Tian et al., 2023). Logit probabilities, irrespective of their magnitude, predominantly represent distribution over vocabulary space. Therefore, logit probabilities do not directly indicate model uncertainty but also reveal various linguistic factors that influence the model's output. This nuanced perspective is in stark contrast to human expressions of certainty, which typically reflect a belief in the accuracy or truth of a claim, based on information processing and decision-making processes, and are not influenced by phrasing"}, {"title": "(Koriat et al., 1980; Fischhoff et al., 1977).", "content": "Additionally, another significant limitation is that logit-based methods do not identify or measure any types of uncertainty, limiting their applicability in scenarios where understanding the sources and degrees of uncertainty is crucial.\nConsistency-based approaches (Vazhentsev et al., 2023; Portillo Wightman et al., 2023; Wang et al., 2023; Shi et al., 2022; Manakul et al., 2023; Agrawal et al., 2023) assess confidence by evaluating the agreement among various model responses, identifying potential inconsistencies. However, these methods encounter significant challenges, especially due to the diversity of potential paraphrases and formatting variations in textual data, complicating their use in real-time scenarios (Xiong et al., 2024; Jiang et al., 2021; Fadaee et al., 2017; Li et al., 2022; Ding et al., 2024; Kuhn et al., 2023). Additionally, a non-trivial challenge is the effective measurement of consistency among responses, a persisting issue that hinders accurate confidence assessment (Manakul et al., 2023; Zhang et al., 2023b).\nSelf-evaluation methods (Kadavath et al., 2022; Manakul et al., 2023; Lin et al., 2024a) enable models to internally assess the correctness of their answers by leveraging their introspective capabilities. These meth- ods employ various prompts that encourage models to express their confidence through numerical values or verbalized terms. Recent studies have refined these approaches, utilizing strategies like Chain of Thought (CoT) to enhance how models calibrate and articulate linguistic confidence (Xiong et al., 2023). Research has also explored expressing confidence with linguistics qualifiers to better align verbal expressions with the model's actual confidence levels (Mielke et al., 2022b; Zhou et al., 2023; Lin et al., 2022a), making model outputs more understandable for users. Despite their potential, these methods are constrained by the model's limited self-awareness, which can lead to circular reasoning and overconfident inaccuracies (Ji et al., 2023b; Chen et al., 2023). Another challenge is the interpretability and validity of obtained probabilities that align with specific linguistic and psychological interpretations, including expressions like 'I think,' 'undoubtedly,' or 'high confidence.'\nInternal-Based Approach Recently, Beigi et al. (2024) used a mutual information framework to theoretically demonstrate that the internal states of large language models provide additional insights into the correctness of their answers. Burns et al. (2023) introduced an innovative unsupervised method that maps hidden states to probabilities. This approach involves responding to \"Yes\" or \"No\" questions, extracting model activations, converting these activations into probabilities. Furthering this research, studies have em- ployed linear probes (Li et al., 2023b; Azaria & Mitchell, 2023b) and contrastive learning (Beigi et al., 2024) to assess whether the internal states across various layers can distinguish between correct and incorrect answers. Empirical results suggest that certain middle layers and specific attention heads show strong discriminative abilities. Beigi et al. (2024) expanded these findings by illustrating that for tasks requiring contextual processing, such as reading comprehension, the outputs of multi-head self-attention (MHSA) components are crucial for assessing response correctness. However, current methodologies exhibit lim- itations. Each task and dataset requires training a specific \u201cconfidence estimator\" model, which restricts their generalizability. This limitation is evident as the performance of these methods often declines when trained on one task and dataset and tested on another, highlighting their limited transferability across differ- ent applications (Bashkansky et al., 2023). Additionally, the computational resources required to train these confidence estimators pose challenges for their deployment in real-time applications, further complicating their practical utility.\""}, {"title": "5 DISCUSSION AND FUTURE DIRECTION", "content": "Go beyond Confidence Estimation: As discussed, the literature on uncertainty estimation in LLMs primarily interprets confidence scores as measures of uncertainty. Such a prevalent method oversimplifies the nuanced and complex nature of uncertainty inherent in model predictions, which is crucial for accurate interpretation and reliability of model outputs. The limitations inherent in current methodologies necessitate the development of a more advanced framework for categorizing uncertainty estimation in large language models that surpasses the reliance on simple confidence scores.\nLack of Explainability: Current methods of confidence estimation provide certainty predictions without elucidating the underlying causes of potential uncertainties. While these scores may seem reasonable to human observers, the absence of insight into the sources of uncertainty complicates trust in the model's outputs, particularly in safety-critical contexts where explainability is essential. Current confidence quantification techniques struggle to pinpoint specific weaknesses or improvement areas in the model. Additionally, these methods lack the necessary transparency to clarify the reasons for model uncertainty, whether due to input ambiguity from users, insufficient knowledge, or conflicting information in the training data.\nLack of Ground Truth for Uncertainty Estimation: Current methods for estimating uncertainty are empirically evaluated and assess how accurately they predict the correctness of an answer. However, there is generally no established ground truth for validation, particularly regarding the sources of uncertainty, meaning currently there is no metric and method to determine the contribution of different uncertainties for specific models and tasks.\nLack of Transferability of Uncertainty Estimation Methods Across Different Applications and Datasets: Current uncertainty estimation methods often struggle with adaptability and generalizability when applied to new applications or datasets. Although effective within their specific domains, these methods frequently fail to yield reliable results in different settings due to factors like data distribution differences and unique domain-specific requirements. To overcome these limitations, it is crucial to develop more robust and flexible uncertainty estimation techniques that can adjust to the varied conditions and demands of diverse applications.\nLack of Standardized Evaluation Protocol & Comprehensive Benchmarks for Confidence Estimation: Current methods for evaluating uncertainty estimation methods are typically used to compare uncertainty quantification techniques through metrics like accuracy, calibration, or performance in out-of-distribution detection, using standardized datasets common within the LLM community. Despite this, variations in experimental settings across studies highlight the need for a comprehensive benchmark across tasks and domains to assess their robustness. The absence of a standardized testing protocol poses challenges for"}, {"title": "6 CONCLUSION", "content": "In this paper, we have reviewed and analyzed the uncertainty inherent in LLMs. We clarified the definition of uncertainty and related concepts, enhancing understanding across various domains. A comprehensive framework was proposed to categorize and identify sources of uncertainty throughout the lifecycle of LLMs. We also reviewed current approaches in the literature, discussed their challenges and limitations, and highlighted future directions to enhance the practicality of LLMs in real-life applications."}, {"title": "A APPENDIX", "content": "You may include other additional sections here."}]}