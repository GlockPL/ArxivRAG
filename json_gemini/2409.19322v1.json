{"title": "Scalable Cloud-Native Pipeline for Efficient 3D Model Reconstruction from Monocular Smartphone Images", "authors": ["Aghilar Potito", "Anelli Vito Walter", "Trizio Michelantonio", "Di Noia Tommaso"], "abstract": "In recent years, 3D models have gained popularity in various fields, including entertainment, manufacturing, and simulation. However, manually creating these models can be a time-consuming and resource- intensive process, making it impractical for large-scale industrial appli- cations. To address this issue, researchers are exploiting Artificial Intel- ligence and Machine Learning algorithms to automatically generate 3D models effortlessly. In this paper, we present a novel cloud-native pipeline that can automatically reconstruct 3D models from monocular 2D images captured using a smartphone camera. Our goal is to provide an efficient and easily-adoptable solution that meets the Industry 4.0 standards for creating a Digital Twin model, which could enhance personnel exper- tise through accelerated training. We leverage machine learning models developed by NVIDIA Research Labs alongside a custom-designed pose recorder with a unique pose compensation component based on the AR- Core framework by Google. Our solution produces a reusable 3D model, with embedded materials and textures, exportable and customizable in any external 3D modelling software or 3D engine. Furthermore, the whole workflow is implemented by adopting the microservices architecture stan- dard, enabling each component of the pipeline to operate as a standalone replaceable module.", "sections": [{"title": "1 Introduction", "content": "In contemporary times, 3D models and complete 3D environments have become ubiquitous across different sectors, including art, entertainment, simulation, aug- mented reality, virtual reality, video games, 3D printing, marketing, TV and manufacturing. The attraction of having a digital version of any physical object as a 3D model lies in its versatility and adaptability for varied purposes. This digital replica, known as a Digital Twin (DT), is a virtual model that accurately reflects and maps physical goods in a digital space [1]. DTs can be utilized to replicate physical objects in a virtual environment, thereby enabling the perfor- mance of specific tasks on the simulated model and observing their effects on the real-world counterpart. DTs have extensive applications in the manufactur- ing industry, including product design, process improvement, and optimization. Moreover, the integration of Industrial Augmented Reality (IAR) in Industry 4.0 can significantly enhance worker productivity and task effectiveness by pro- viding real-time data and information, aiming to improve the overall operational efficiency. IAR is beneficial in manufacturing, where it assists workers in making informed decisions in realistic situations [2], streamlines engineering workflows throughout the design and manufacturing stages [3], and increases productiv- ity by equipping workers with the necessary information to perform tasks more efficiently and safely. IAR is also effective in marketing and sales, where it can provide interactive information about products, dispel uncertainty, and influence client perceptions [4,5,6,7]. Furthermore, IAR can facilitate training by offering detailed instructions and reducing the time required to train new personnel while minimizing their skill requirements[8]. Over the years, modeling techniques have undergone significant evolution, leading to the development of more intuitive and less time-consuming tools for creating or generating 3D models. These models can be created from a set of primitive shapes, mathematical equations, or even a 2D image. The most commonly used techniques for creating 3D models are manual modeling, pho- togrammetry, and Light Detection and Ranging (LIDAR). Manual modeling, while effective, can be expensive in terms of time and resources since it involves a significant amount of manual labor and is unsuitable for large-scale appli- cations. Alternatively, photogrammetry involves the use of photographs taken from different angles by a camera to make measurements. Finally, specialized hardware-based techniques such as LIDAR technology are also utilized. In addition, the industrial research sector is actively exploring this research domain. For instance, NVIDIA is currently developing novel Artificial Intelli- gence and Machine Learning techniques and algorithms to enhance the quality of generated 3D models. Two of the approaches analyzed in this paper are based on recent research publications from 2022: Instant NeRF - a set of instant neu- ral graphics primitives for NeRF [9] - and nvdiffrec - which leverages differential rendering and Deep Marching Tetrahedra (DMTet) [10]. The aim of this paper is to present a distributed, cloud-native, and scalable pipeline capable of solving the 3D model reconstruction problem using a set of monocular two-dimensional images. The proposed pipeline is designed to reduce time and resources, providing a cost-effective solution for large-scale industrial applications by leveraging microservices architecture standards. Furthermore, the pipeline is enhanced by Augmented Reality (AR) capabilities to improve the data acquisition workflow. The main contributions of this paper are:"}, {"title": "2 Background and Technology", "content": "This section provides a detailed list of conventional and standard techniques alongside AI-based ones. It focuses on main drawbacks and how to overcome them."}, {"title": "2.1 Standard and conventional techniques", "content": "Manual modeling involves creating a 3D model using specialized software by an experienced 3D artist or modeler. This technique can be time-consuming and not suitable for large-scale applications due to the time involved for the design process for a single 3D model. The 3D artist is responsible for addressing various issues during the modeling process, such as mesh creation, material definition, texture generation, model rigging, environment, and lighting. Commonly used software for manual modeling includes techniques such as polygonal modeling, surface modeling, and digital sculpting 1.\nPhotogrammetry is a technique for generating 3D models from two-dimensional images. It involves using a collection of photos taken from different angles with a standard 2D camera and extracting material properties using methods from optics and projective geometry. This technique is useful in achieving a realistic feeling during Physically-Based Rendering (PBR) 2.\nLastly, LIDAR is a remote sensing technology that uses pulsed laser light to measure variable distances from a source point to a hit point, thereby collecting data about the shape and elevation of the scanned object's surface. LIDAR is commonly used in 3D model reconstruction of real-world objects and is also known as a 3D laser scanner. The output of a LIDAR scan is a point cloud, which comprises a set of geo-located colored data points in a 3D space and provides additional information about the object's material properties 3."}, {"title": "2.2 AI-based techniques", "content": "This paragraph discusses about how AI-based techniques can be used to over- come the aforementioned standard techniques' drawbacks. In particular, Instant NeRF and nvdiffrec from NVIDIA Research Labs [9,10].\nInstant NeRF. It is a more advanced and efficient implementation of the NeRF technique, which enables the creation of 3D models from 2D images us- ing neural networks and a multi-resolution hash encoding grid. The technique involves reconstructing a volumetric radiance-and-density field from 2D images and their corresponding camera poses, which can then be visualized through ray marching. The encoding phase is task-agnostic and only the hash table size is adjusted, which affects the trade-off between quality and performance. The multi-resolution structure enables the network to resolve hash collisions more ef- fectively. The implementation heavily relies on parallelism, utilizing fully-fused CUDA kernels with FullyFusedMLP[11,12]. If this is not available, the algo- rithm falls back to CutlassMLP - CUDA Templates for Linear Algebra Subrou- tines [13], with a focus on minimizing unnecessary bandwidth and computa- tional operations. The tests were conducted with a resolution of 1920\u00d71080 on high-end hardware equipped with an NVIDIA RTX 3090 GPU with a 6MB L2 cache. The primary limitation of this methodology is its dependence on the NeRF technique, which produces a point cloud as its output. Consequently, the authors had to devise a method to extract the mesh of the scene from the encoded data within the neural networks. To accomplish this, they employed the Marching Cubes (MC) algorithm, a mesh extraction technique that is dependent on a point cloud as its initial input. However, the resulting mesh presents surface irregularities in the form of various holes, lacks UV coordinates, and does not possess any materials. As a result, it is essentially an unusable gray mesh for any 3D modeling software.\nnvdiffrec. It is a tool that enables the creation of 3D models from 2D images. What sets nvdiffrec apart from Instant NeRF is its ability to reconstruct a 3D model surface, complete with texture and materials. The authors approached this task as an \"inverse rendering\" problem, using a 2D image loss function to optimize as many steps as possible jointly. The goal is to ensure that the re- constructed model's rendered images are of high quality compared to the input imagery. The approach used in nvdiffrec enables the learning of topology and vertex positions for a surface mesh without the need for any initial guesses about the 3D geometry. The tool's differentiable surface model relies on a deformable tetrahedral mesh that has been extended to support spatially varying materials and high dynamic range (HDR) environment lighting through a novel differen- tiable split sum approximation. The resulting 3D model can be deployed on any device capable of triangle rendering, including smartphones and web browsers, without the need for further conversion and can render at interactive rates [10]. The paper tackles the challenge of 3D reconstruction from multi-view images of an object, with known camera poses and background segmentation masks, pro- ducing triangle meshes, spatially-varying materials (stored in 2D textures), and"}, {"title": "3 Proposed Pipeline", "content": "Skilled service professionals are capable of maintaining and repairing complex machinery and industrial facilities. These professionals utilize their knowledge in industrial maintenance and assembly tasks by employing a combination of simulation, capturing techniques, multimodal interaction, and 3D-interactive graphics to achieve distributed training [14]. The acquired competencies are then adapted to realistic training situations that are utilized in industrial training fa- cilities. In [14], the authors refer to this as immersive training, which involves \"Real-time simulations of object behavior and multimodal interaction that sup- port the development of complex training simulators that address cognitive skills [...] and sensorimotor skills.\u201d. Industrial Augmented Reality (IAR) is a com- bination of computer vision and computer graphics that utilizes camera-based interaction. IAR can be exploited to facilitate the data acquisition process for the proposed scalable cloud-native pipeline. A segment of the pipeline can be deployed within a Kubernetes cluster, where all cloud phases of the pipeline are dispatched as Jobs to worker nodes. Worker nodes require an NVIDIA GPU to handle the high-end capabilities needed for dataset preprocessing and re- construction jobs. Therefore, the complete reconstruction pipeline consists of various phases that can be executed either on an embedded device or in the cloud, depending on the different resource requirements."}, {"title": "3.1 Pipeline definition", "content": "We defined a reconstruction pipeline (Figure 1) by identifying a set of phases that are executed progressively, each performing specific operations on the dataset. The pipeline phases are described below:\n dataset generation phase, a custom written pose recorder with a poses com- pensation algorithm is implemented;\n dataset preprocessing phase, the images and poses are preprocessed and the relative alpha masks are generated (silhouettes);\n reconstruction phase, the 3D model is generated alongside a preview of the current pipeline status in order to provide feedback to the end user."}, {"title": "3.2 Dataset generation phase", "content": "The initial step in the reconstruction pipeline is the generation of the dataset, which comprises a collection of images and corresponding poses. These crucial components are obtained through a native Android application that implements the ARCore framework 10. The reconstruction module necessitates specific tech- nical prerequisites for the input data, particularly:\n a set of RGB images with a resolution of 512 \u00d7 512 pixels;\n a set of alpha masks (silhouettes) with a resolution of 512 \u00d7 512 pixels;\n a poses_bounds.npy file containing the view matrices of the camera for each image with the specific camera intrinsics.\nGiven a set of images of size N, the poses_bounds file is a numpy array of shape (N, 17), in which N is the number of images and 17 is the number of total features for each image. The first 12 columns of each row are the 3 \u00d7 4 view matrix of the camera for the corresponding image, and the last 5 elements represent:\n height of the image obtained from camera intrinsics;"}, {"title": "Pose recorder.", "content": "To record the poses during 2D image acquisition, a pose recorder component is necessary, wherein each pose corresponds to a single image. We have implemented this workflow as a library in a native Android application, where the management of the anchor lifecycle is a critical aspect, particularly for the poses compensation algorithm. Moreover, this library facilitates the selec- tion of a camera with varying resolutions or frames per second (FPS) to initiate the recording process. In the subsequent preprocessing phase, the images are re- sized to 512 \u00d7 512 pixels to fulfill the input requirements of the machine learning model.\nARCore provides a view matrix of the device's pose in the world coordinate system, which is represented by a 4\u00d74 matrix. The rotation matrix is represented by the first 3 \u00d7 3 submatrix, while the translation vector is represented by the last column. However, a 4 \u00d7 4 matrix is not suitable for this particular problem, as a 3 x 4 view matrix is required. To address this, the last row of the matrix is removed to obtain the desired 3 \u00d7 4 matrix. The resulting matrix follows the column-major order convention in which the matrix elements are ordered by column 12. To complete the transformation, a new column of shape 3 \u00d7 1 that contains the height, width, and focal length of the device is concatenated with the matrix. The resulting matrix is:\n\n$\\begin{pmatrix}\nr_{11} & r_{12} & r_{13} & t_x & h \\\\\nr_{21} & r_{22} & r_{23} & t_y & w \\\\\nr_{31} & r_{32} & r_{33} & t_z & f\n\\end{pmatrix}$\n\nin which $r_{ij}$ is the ij-element of the rotation view matrix, $t_i$ is the i-element of the translation vector and h, w, f are the height, the width and the focal lenght respectively extracted from the camera instrinsics. After a matrix flattening operation 13 and a subsequent concatenation, we obtain the final data-flattened view matrix. Thus, the following reshaped data entry can be generated for each frame:\n\n$r_{11}\\ r_{12} \\ r_{13} \\ t_x \\ h \\ r_{21} \\ r_{22} \\ r_{23} \\ t_y \\ w \\ r_{31} \\ r_{32} \\ r_{33} \\ t_z \\ f \\ m \\ M$"}, {"title": "Sensor drifting problem.", "content": "During some tests, a jagged surface is observed in the reconstructed model, which indicates a misalignment of poses with the ac- quired images caused by a sensor drifting problem. This problem generates an inconsistent dataset, posing a significant challenge for subsequent analysis. To address this issue, a comparison is performed between the generated poses_bounds file and the COLMAP generated one. COLMAP is an open-source software which implements Structure-from-motion (SfM) and Multi-View Stereo (MVS) tech- niques [15,16]. This comparison highlights the misalignment issue, leading to a solution to reach a more accurate and reliable dataset. The primary distinction between the two datasets stems from their distinct coordinate systems, which is due to the absence of a real-world reference in COLMAP (Figure 2). In order to reconcile all the data points, a series of trans- formations are implemented, wherein the entirety of the points are treated as a single rigid body. To achieve this, both rigid bodies are brought to a com- mon origin, and a transformation matrix is computed, which transforms three vectors from the COLMAP dataset to the ARCore one. The application of this transformation matrix affects both the rotation and scale, ultimately resulting in the overlapping of the two datasets. The computed difference between the two datasets yields a difference matrix, which highlights their deviation."}, {"title": "Pose compensation algorithm.", "content": "The system relies on a self-made anchor management system to detect real-time variations of positions or rotations of ARCore Anchors while scanning. This avoids trajectory discontinuity by com- puting and applying a compensation matrix to the camera view matrix. The Poses Compensation Algorithm comprises the following components:\nAnchors, objects placed in the scene, provided by ARCore;\nDelta position from initial pose for each anchor frame by frame;\nDelta rotation from initial pose for each anchor frame by frame;\nQuaternion products to compute the rotation matrix.\nGiven a quaternion q = a + bi + cj + dk defined by the following coefficients < a, b, c, d > and the following imaginary components (i, j, k), each delta quater- nion can be computed as follows:\n\n$q_{delta} = q_{target}q_{current}^{-1}$\n\nin which $q_{delta}$ is the delta quanternion to compute, $q_{target}$ represents the target rotation we want to reach and $q_{current}^{-1}$ represents the inverse of the current rotation. Therefore, because $q_{current}^{-1}$ is the conjugate of quaternion $q_{current}$, it can be computed by an inversion of the imaginary components of the quaternion:\n\n$conj(a + bi + cj + dk) = a - bi - cj - dk$\n\nMoreover, given two quaternions q and r having the form:\n\n$q = q_0 + q_1i + q_2j + q_3k$\n\n$r = r_0 + r_1i + r_2j + r_3k$\n\nFrom [17], the product of two quaternions is a quaternion having the form:\n\n$n = q\\ x\\ r = n_0 + n_1i + n_2j + n_3k$\n\nwhere:\n\n$n_0 = (r_0q_0 - r_1q_1 - r_2q_2 - r_3q_3)$\n\n$n_1 = (r_0q_1 + r_1q_0 - r_2q_3 + r_3q_2)$\n\n$n_2 = (r_0q_2 + r_1q_3 + r_2q_0 - r_3q_1)$\n\n$n_3 = (r_0q_3 - r_1q_2 + r_2q_1 + r_3q_0)$\n\nSpecifically, the algorithm is composed of three main steps:"}, {"title": "3.3 Dataset preprocessing phase", "content": "In this phase, the images are resized to a resolution of 512 x 512 pixels before starting the alpha masks generation subtask.\nAlpha masks generation. We adopted a machine learning model to extract the alpha mask starting from the RGB images, employing CarveKit: an au- tomated and high-quality framework for background removal in images using neural networks 14. To optimize performance, the framework is executed on the GPU. Following this step, we refine the silhouettes by applying a threshold to eliminate any ambiguous regions and enhance the edges of the 3D model during the reconstruction phase. Ultimately, we obtain a set of sharpened alpha masks that are integrated into the initial dataset."}, {"title": "3.4 Reconstruction phase", "content": "This phase adopts nvdiffrec tool to reconstruct the 3D model. The input param- eters required for this task are:\n a collection of RGB images in PNG format;\n a collection of alpha mask images in PNG format (silhouette);\n a set of camera poses serialized in the poses_bounds numpy matrix file.\nFinally, upon successful reconstruction, the tool provides as artifacts:\n mesh.obj containing the reconstructed mesh, UV mapped;\n mesh.mtl containing the material properties;\n texture_kd.png file containing the diffuse texture;\n texture_ks.png containing the ORM map (-, roughness, metalness);\n texture_n.png containing the normal map."}, {"title": "3.5 Architecture", "content": "We have designed and implemented the entire pipeline utilizing microservices architecture standards, which has been specifically tailored for deployment on a Kubernetes cluster. In the upcoming sections, we will provide an in-depth description of the microservices involved in the process, as well as the cloud infrastructure adopted for this purpose.\nMicroservices. The microservices compose the fundamental constituents of the pipeline. Each microservice, implemented as a Docker image, is purposefully crafted to accomplish a specific task. Specifically, the microservices that have been identified are the Preprocessor microservice, the Reconstruction microser- vice, and the Workloads scheduler microservice (refer to Figure 4). The Preprocessor microservice is dedicated to dataset preprocessing. Its con- tainer is based on the NVIDIA CUDA runtime environment image, with all the necessary dependencies installed to ensure CarveKit to operate properly. The Re- construction microservice heavily relies on the nvdiffrec repository. This Docker image is built utilizing specific configurations, dependencies, and environmental variables outlined in the official documentation. The only modifications made are the addition of customized domain-specific code to enable preview image generation, dataset management, and 3D model uploading. Finally, the Work- loads scheduler microservice, on the other hand, is responsible for job scheduling within the Kubernetes cluster. It operates as a backend service API that oversees the entire pipeline lifecycle for each reconstruction request.\nCloud infrastructure. The cloud infrastructure consists of a Kubernetes clus- ter deployed on bare metal, with accelerated machines designated as worker nodes. To support resource-intensive tasks such as dataset preprocessing and 3D model reconstruction, the worker nodes are equipped with an NVIDIA Quadro"}, {"title": "4 Evaluation", "content": "This section provides an account of the performance and outcomes of the pro- posed solution. Furthermore, difficulties encountered during the study and prospects for enhancements are presented. Both qualitative and quantitative evaluations are included."}, {"title": "4.1 Qualitative evaluation", "content": "The qualitative evaluation is performed considering user experience in mobile app utilization, alpha masks generation quality, and the real-look feeling of the generated 3D models (Figure 5). The significance of user experience cannot be understated, especially when it comes to addressing challenges within specific industrial environments. A smart- phone user is empowered to scan a variety of equipment, but it is crucial for"}, {"title": "4.2 Performance evaluation", "content": "The performance evaluation of the system takes into account the latency of the pipeline, from the scanning phase to the interaction phase. This latency can be computed using the following formula:\n\n$T_{latency} = T_{scan}+2T_{upload}+2T_{signal}+T_{preprocessing}+T_{reconstruction}+T_{download}$\n\nHere, $T_{signal}$ denotes the time required for signals to propagate within the in- frastructure, while $T_{upload}$ and $T_{download}$ represent the time taken to upload and download assets from the S3 bucket, respectively. Since $T_{scan}, T_{preprocessing}, and T_{reconstruction}$ take significantly longer than the other steps, we can simplify the formula as follows:\n\n$T_{latency} = T_{scan} + T_{preprocessing} + T_{reconstruction}$\n\nIn the conducted tests, network latency, denoted by $T_{scan}$, was found to be ap- proximately 120 seconds, while preprocessing time took roughly 30 seconds. The reconstruction process required approximately 2 hours and 30 minutes. Addition- ally, losses on both training and validation sets were taken into consideration. The calculation of image space loss was performed using nvdiffrast [18], which assesses the difference between the rendered image and the reference one. As shown in Figure 6, the employment of nvdiffrec resulted in a loss of 0.010540 on the training set and 0.010293 on the validation set."}, {"title": "5 Conclusion and future work", "content": "This study presents an innovative cloud-native scalable pipeline for reconstruct- ing 3D models of real-world objects, with the aim of producing 3D models for Digital Twins. This approach offers various advantages related to Industry 4.0, including a faster personnel training process. The proposed solution employs both low-end hardware, such as 2D cameras overlaid by Google's ARCore frame- work, and high-end cloud worker nodes for the segmentation and reconstruction tasks. Specifically, a machine learning model is adopted to segment the dataset. Once the alpha masks are generated, nvdiffrec tool by NVIDIA is exploited to perform the effective 3D model reconstruction. The resulting model can be downloaded and interactively viewed on a smartphone. The entire pipeline com- plies to microservices architecture standards, enabling scalability in large-scale production environments. Although the proposed solution has achieved the ex- pected outcomes, the modular design allows for potential future improvements, including:\n adoption of a better reconstruction machine learning model to produce smoother edges and better reconstructed models [19];\n replacement or improvement of the machine learning model used to generate the alpha masks;\n implementation of more layers to decompose the 3D model into its con- stituent parts, enabling a more exhaustive experience."}]}