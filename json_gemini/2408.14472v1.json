{"title": "Advancing Humanoid Locomotion: Mastering Challenging Terrains with Denoising World Model Learning", "authors": ["Xinyang Gu", "Yen-Jen Wang", "Xiang Zhu", "Chengming Shi", "Yanjiang Guo", "Yichen Liu", "Jianyu Chen"], "abstract": "Humanoid robots, with their human-like skeletal structure, are especially suited for tasks in human-centric environments. However, this structure is accompanied by additional challenges in locomotion controller design, especially in complex real-world environments. As a result, existing humanoid robots are limited to relatively simple terrains, either with model-based control or model-free reinforcement learning. In this work, we introduce Denoising World Model Learning (DWL), an end-to-end reinforcement learning framework for humanoid locomotion control, which demonstrates the world's first humanoid robot to master real-world challenging terrains such as snowy and inclined land in the wild, up and down stairs, and extremely uneven terrains. All scenarios run the same learned neural network with zero-shot sim-to-real transfer, indicating the superior robustness and generalization capability of the proposed method.", "sections": [{"title": "I. INTRODUCTION", "content": "Modern environments are primarily designed for humans. Therefore, humanoid robots, with their human-like skeletal structure, are especially suited for tasks in human-centric environments and offer unique advantages over other types of robots. Their mobility is crucial for completing diverse tasks in the real world, underlining the necessity of their capacity to walk on complex terrains.\nPreviously, model-based control techniques such as Zero Moment Point (ZMP) and Model Predictive Control (MPC) combined with Whole-Body Control (WBC) have significantly advanced humanoid robots' locomotion abilities, enabling skills like walking, jumping, and even backflipping[2, 38, 5]. However, the success of these methods depends on accurately modeling the environment's dynamics, which can make it difficult to handle complex interactions with the environment, such as navigating challenging terrains.\nReinforcement learning (RL), on the other hand, relies less on exact environmental modeling. Recent progress in model-free RL has shown great potential, particularly in developing adaptive legged locomotion controllers [28]. This allows robots to learn and adapt to a wide range of situations, often surpassing the capabilities of traditional model-based control methods[22].\nHowever, ensuring robustness in humanoid robots, as opposed to quadrupedal [28] and bipedal [27] counterparts, involves addressing several additional challenges. These include but are not limited to a higher center of gravity, instability during leg swinging, greater leg inertia, extra weight from the torso and arms, and their larger size overall. Therefore, to date, real-world applications of reinforcement learning (RL) for controlling humanoid robots, as demonstrated in the recent study[26], have been limited to relatively simple terrains.\nIn this work, we introduce Denoising World Model Learning (DWL) for controlling humanoid robots across varied and complex terrains. To the best of our knowledge, DWL enables the world's first humanoid robot to master real-world challenging terrains with end-to-end RL and zero-shot sim-to-real transfer. As shown in Fig.1, our humanoid robot is able to navigate stably through snowy inclined land in the wild, stairs, irregular surfaces, etc., and can resist large external disturbances. All scenarios run the same learned neural network policy, indicating its robustness and generalization. The key ingredient of DWL lies in establishing an effective representation learning framework to denoise the factors enlarging the sim-to-real gap. Furthermore, we are the first to enable active 2-DoF ankle control with a Closed Kinematic Chain Ankle Mechanism (shown in Fig.2) for humanoid robot locomotion learning. Unlike previous studies [32] with only one DoF ankle control or passive ankle control [26], our approach enables the robot to become extremely robust. The contributions of our work are summarized as follows:\n1) Demonstrate the world's first humanoid robot mastering real-world challenging terrains with end-to-end RL through zero-shot sim-to-real transfer.\n2) Propose DWL, a novel RL framework to bridge the sim-to-real gap and achieve robust generalizable performance.\n3) Unveil a cutting-edge humanoid robot with active 2-Dof ankle control, powered by RL and a closed kinematic chain for enhanced stability and flexibility."}, {"title": "II. RELATED WORKS", "content": "a) Learning Robot Locomotion: Reinforcement learning has become more promising to enable robots to perform stable locomotion[35, 12, 18]. Compared to previous RL efforts with quadrupedal robots [28] and bipedal robots like Cassie[21, 17], our focus on humanoid robots presents a significantly more challenging setup. Our proposed method excels in automating state representation learning [19], mastering end-to-end learning for both prediction and adaptation and facilitating a seamless zero-shot transfer to real-world scenarios by effectively bridging the sim-to-real gap.\nFurthermore, conventional approaches, often encompassing multi-stage training processes [1], detailed reward designs [39], or behavior cloning [24], typically falter amidst the dynamic and unpredictable real-world scenarios. On the other hand, DWL instead integrates a world model within an encoder-decoder framework, employing a masking loss to predict the state from observations.\nb) Humanoid Robot Locomotion Control: The evolution of humanoid locomotion began with early concepts and basic models, exemplified by WABOT-1 in the 1970s[13]. Progress in sensors and control algorithms enhanced humanoid robots' stability and adaptability. model-based control techniques[16] like ZMP[34], MPC[33, 20], and WBC[30] have significantly improved locomotive capabilities. Learning-based approaches, which are less reliant on precise dynamic models, offer better adaptability and robustness. Despite this, real-world applications of reinforcement learning for humanoid control, such as [26, 8], have been successful but limited to simpler terrains."}, {"title": "III. PROBLEM SETTING", "content": "A. Reinforcement Learning Background\nOur approach utilizes a reinforcement learning problem setting, encapsulated in the tuple $M = (S, A, T, O, R, \\gamma)$. Here, $S$ and $A$ represent the state and action spaces, with the transition dynamics $T(s'|s, a)$, the reward function $R(s, a)$, and the discount factor $\\gamma \\in [0, 1]$. $O$ represents the observation space.\nOur framework distinctly adapts to both simulated and real-world environments. In the simulation, the agent is afforded complete visibility of state $s \\in S$. On the other hand, the real world is plagued by partial observability. The agent only has access to partially observations $o \\in O$, which provide incomplete information about the state due to sensory limitations and environmental noise. The policy $\\pi(a|o<t)$ maps the historical observations to a distribution over actions. As a result, the agent operates within a discrete-time Partially Observable Markov Decision Process (POMDP), necessitating decision-making based on sporadic and partial data. The primary goal is to optimize this policy $\\pi$ to maximize the expected total return $J = E[R_t] = E[\\Sigma_t \\gamma^t r_t]$.\nB. Humanoid Robot Hardware\nWe use two different sizes of humanoid robots for our experiments, as illustrated in Fig. 2. XBot-S weighs 38 kg and stands 1.2 meters tall. The robot is equipped with 26 actuated motors: 7 in each arm and 6 in each leg. XBot-L weighs 57 kg and stands 1.65 meters tall. The robot is equipped with 54 actuated motors. For the purposes of this study, we focus on leg control, keeping the arm motors stationary. Each leg is powered by 6 motors: the yaw and roll joint motors with a peak torque of 100Nm, the pitch and knee joint motors with 250Nm, and 2 ankle motors each providing 36N.m of torque (50Nm in XBot-L). The ankle motors, situated near the knee, are operated via a Closed Kinematic Chain Ankle Mechanism as Fig. 2. This design aims to reduce leg inertia while ensuring an adequate degree of freedom."}, {"title": "IV. METHODS", "content": "A. Denoising World Model Learning\nUtilizing RL, various skills can be learned in simulation, but the transition to real-world robots faces significant challenges due to the sim-to-real gap, which is mainly caused by inaccurate simulation of the robot hardware and the limited information provided by onboard sensors. To overcome this barrier, we introduce Denoising World Model Learning (DWL), which enables online adaptation and state estimation through representation learning. DWL is characterized by two primary features:\n\u2022\tAn encoder-decoder architecture for world model learning, effectively embedding partially observed historical\n\u2022\traw sensory data into a latent space and reconstructing the robot's full state from it.\n1) Encoder-Decoder Architecture of DWL: In an ideal world, a complete observed state, precise sensors, and a flawless simulator would eliminate the sim-to-real gap. Yet, reality often presents us with noisy, incomplete sensor data, and the simulator is far from perfect. Consequently, the sim-to-real gap can be conceptualized as introducing the following types of noises to the true state:\n\u2022\tEnvironmental noise: Real-world environments are complex and unpredictable, presenting challenges such as navigating on challenging terrains or unexpected external forces applied to the robot.\n\u2022\tDynamics noise: Accurately simulating the true dynamics of the physical world is unfeasible, leading to oversimplifications in simulations, like approximations of ground friction or object deformability.\n\u2022\tSensory noise: Physical sensors inherently contain measurement noise, for example, IMU drift and inaccuracies in joint position readings.\n\u2022\tMasking noise: Some information may be unobtainable in reality due to the absence of specific sensors on the robot, such as linear velocity and contact force measurements. This partial observability can be regarded as adding masking noise[6, 10, 11].\nTo mitigate these noises, we have developed a framework that firstly simulates noisy observations within the simulation and subsequently employs an encoder-decoder architecture to denoise these observations and accurately recover the true state and dynamics, as depicted in Fig. 3.\nAdditionally, to mimic the constraints of partial observability, we mask out some information that is not observable on real robots. We emulate the environment, dynamics, and sensor noises utilizing domain randomization (DR) methods[37]. In this approach, we introduce random perturbations to the actual state and dynamics, such as angular velocity and PD parameters. This procedure aligns with an observation model[9] expressed as $O_t \\sim P_{Noise}(O_t | S_t)$. We elaborate on this in Section IV-D.\nAn encoder-decoder architecture is designed to denoise the observations. The recurrent encoder extracts latent state $Z_t$ from the robot's historical noisy sensor observations. This latent representation is the core of state estimation, providing a rich, condensed summary of the robot's situational awareness. Subsequently, the decoder endeavors to reconstruct the robot's true state from this latent state. The formal expression of this model is given by:\n$P(S_t) = E_{o_{<t}} [\\int P_{Decoder} (S_t | z_t) \\cdot P_{Encoder}(z_t | o_{<t})]$\nwhere $P(S_t)$ represents the estimation of the real state distribution $P(s_t)$ at time $t$. The encoder captures the conditional distribution $P_{Encoder}$ of these latent variables given the noisy historical observations $o_{<t}$, and the decoder $P_{Decoder}$ reconstructs the state from the latent representation $Z_t$.\nIt is imperative to recognize that the dimension of $o_{<t}$ is much larger than that of $z_t$, implying $z_t$ an effective information bottleneck [36]. This allows DWL to prioritize the salient aspects of sensory input. Furthermore, to enhance the efficiency and robustness of state estimation, sparsity within the latent representation is sought [4]. This is achieved by introducing an L1 regularization term in the latent domain. Moreover, since there is no need to generate new data from the latent space, and it is in a pure denoising process, a deterministic loss could be adopted instead of a variational loss [15]. The denoising loss is thus expressed as follows:\n$L_{denoise} = ||\\hat{s_t} - s_t||_2 + \\lambda_r ||z_t||_1$\nwhere $\\lambda_r$ represents the regularization coefficient. And $s_t$ is the full state. By incorporating privileged information into the state, such as the ground's friction coefficients, the actuator's torque values, and terrain height scans, enables the agent to effectively conduct online adaptation and system identification.\n2) Policy Learning in DWL: Within the DWL framework, an Asymmetric Actor-Critic architecture is employed, drawing upon the concept of privilege learning as elucidated in previous works[3] [25]. This architecture is instrumental in enhancing data utilization during the training phase, proving particularly beneficial in real-world scenarios where direct state information is inaccessible. The actor component of the model computes its loss via the Proximal Policy Optimization (PPO)[29], which is articulated as:\n$L = min\\left( \\frac{\\pi(a_t | O_{\\leq t})}{\\pi_b(a_t | O_{\\leq t})} A_b(0_{<t}, a_t), clip(\\frac{\\pi(a_t | O_{\\leq t})}{\\pi_b(a_t | O_{\\leq t})}, c_1, c_2) A^\\pi(\\pi(a_t | O_{<t})) \\right)$\nwhere $\\pi$ denotes the target policy to be optimized, $\\pi_b$ is the behavior policy employed for data sampling, and $c_1, c_2$ represents the PPO clipping range. In the context of DWL's encoder-decoder structure, the actor policy is defined as $\\pi(a_\\tau | P_{Encoder}(z_t | O_{<t}))$. On the other hand, the critic could use state information for calculations of the value function. Thus, the critic loss is given by the following formula:\n$L_v = ||R_t - V(S_t)||^2$\nIn this formulation, $R_t$ denotes the cumulative return at time t, and V(st) is the value function as determined by the critic at state st. The integration of privileged information within the state representation arms the learning agent with the capacity to make informed decisions. This approach aligns seamlessly with the DWL framework. It obviates the need for design privilege information by employing a unified state definition for both state estimation and value function assessment.\n3) Formulating the DWL Loss Function: The DWL framework consolidates its learning objectives through a composite loss function that integrates the aspects of state reconstruction and policy optimization. The total loss function is a weighted sum of the denoising loss Equation(2), the policy loss Equation(3), and the value loss Equation(4), formally expressed as:\n$L_{DWL} = L_{denoise} + \\lambda_\\pi L_\\pi + \\lambda_v L_v$\nwhere $\\lambda_\\pi$ and $\\lambda_v$ are the weighting factors for the policy and value loss components, respectively. This approach enables DWL to fine-tune the learning process, ensuring precise state estimation and informed decision-making based on these estimates. Reconstructing the state from masking loss and domain randomization noise, DWL demonstrates robustness and adaptability in complex real-world scenarios, as elaborated in Section V.\nIn conclusion, the integration of masking noise and domain randomization noise cultivates a robust latent space for end-to-end state representation learning. When allied with policy gradient loss, this strategy propels a comprehensive approach to state estimation and policy optimization. This refined system is adept at narrowing the sim-to-real gap, effectively translating simulation-trained models to real-world applications."}, {"title": "B. Reward Formulation", "content": "Our reward function guides the robot to follow velocity commands, maintain a stable gait, and ensure gentle contact, thereby enabling robust locomotion across challenging terrains and above-ground obstacles.\n1) Composition of Rewards: The reward function is structured into four key components: (1) velocity tracking, (2) periodic reward, (3) feet trajectory tracking, and (4) regularization terms. Our approach, drawing inspiration from previous works [31, 26, 32], employs a periodic reward to facilitate natural gait learning. Furthermore, we introduce a tracking loss defined as $\\varphi(e, w) := exp(-w \\cdot ||e||^2)$, where e represents the tracking error and w the error tolerance strength. Detailed calculations can be found in the appendix, Section VII-B.\nA novel aspect of our reward design addresses the sparse nature of contact force feedback. Rather than relying exclusively on contact force, our system focuses on foot velocity tracking. This is achieved by designing foot trajectories that incorporate predetermined velocities upon ground contact, thus ensuring a consistent and robust reward signal at each step. Such a strategy promotes gentle ground contacts, leading to reduced impact forces and enhancing the effectiveness of the sim-to-real transfer.\n2) Quintic Polynomial Foot Trajectory Interpolation: In our approach, we focus on refining the locomotion of humanoid robots through the strategic design of foot trajectories. Quintic polynomial interpolation is utilized to determine these trajectories, a method that is particularly effective in meeting the precise kinematic requirements of a humanoid robot's gait cycle. This technique not only facilitates smoother motion but also ensures accurate foot placement, a crucial factor for maintaining stability and efficiency in humanoid walking patterns.\nQuintic polynomial interpolation offers an advantageous approach in robotic motion planning due to its ability to provide smooth trajectories and precise control over velocity and acceleration. The general form of a quintic polynomial is given by:\n$f(t) = \\sum a_k t^k$\nLet t denote the time variable, and $a_0, a_1,..., a_5$ be the coefficients that need to be determined. We denote the swing time by T. In our periodic reward design, one leg being in the swing phase implies the other is in the stance phase. One swing phase and one stance phase together complete a full gait cycle. The trajectory of the robot's foot during the swing phase is defined by f(t), which is shaped through a series of kinematic constraints at critical moments in the robot's gait. These constraints are:\n1) Initial foot height at t = 0, given by f(0) = ho, where ho is the initial height.\n2) Initial foot velocity at t = 0, determined by f'(0) = vo, with vo being the initial velocity.\n3) Initial foot acceleration, represented by f''(0) = acco, where acco is the initial acceleration.\n4) Reaching maximum foot height at the midpoint of the swing phase, f(T/2) = hmax, where hmax is the target feet height.\n5) Final foot height at the end of the swing phase, f(T) = hswing, with hswing as the final height.\n6) Final foot velocity at the end of the swing phase, f'(T) = $\\upsilon_{swing}$, where $\\upsilon_{swing}$ is the final velocity.\nTo deduce the coefficients a0...5, a numerical optimization technique is employed. Once the coefficients are ascertained, they succinctly characterize the foot's vertical trajectory (i.e., the swing height). Quintic polynomial interpolation is instrumental in ensuring soft landings within humanoid robotic locomotion, offering granular control over the robot's swing height, foot acceleration, and velocity profiles. One optimization result and the corresponding trajectory plot are shown in Appendix Table IV and Fig. 7.\nThis method facilitates the manipulation of higher-order derivatives to attenuate impact forces at footfall. By adjusting the coefficients of the quintic polynomial, trajectories are crafted that not only elevate the foot to surmount obstacles but also maintain smooth movements and mitigate the impact upon contact. These fluent transitions enable a gentler touchdown, enhancing the robot's stability and advancing the creation of efficient, adaptable robots capable of safely traversing diverse terrains."}, {"title": "C. Configuration of DWL training process", "content": "In our DWL framework, as illustrated in Fig. 3, we utilize a Gated Recurrent Unit (GRU)[7] for the encoding process and a two-layer Multilayer Perceptron for both the decoding and actor networks. Details of the training configuration can be found in the appendix section VII-C.\nThe robot's base pose is denoted by Pb, and the pose of the feet is denoted by Pf. The pose, which includes both position and orientation, is represented as a six-dimensional vector [x, y, z, \u03b1, \u03b2, \u03b3]. Here, x, y, z specifies the position, and \u03b1, \u03b2, \u03b3 represents the orientation in Euler angles. The policy network inputs include proprioceptive sensor data and a periodic clock signal, represented as (sin(t), cos(t)), in addition to command inputs defining the desired velocities Pxyz . These observations are detailed in Table I. The state includes privileged observations, which are typically unavailable to standard proprioceptive sensors on physical robots. This state also integrates the current step's reward, combining a reward model with the world model, which is expected to enhance the encoder's ability to capture the environmental context in the latent space.\nOther important components of the state are the Periodic Stance Mask I(t), a binary indicator of expected foot contact patterns for a periodic gait, and the Cycle Time, essential for computing foot trajectories as outlined in (6). The Feet Movement, indicating both the position Pfuz and velocity of the feet Pfuz. Also, the height scan provides an approximate height map to further enhance the estimation of the state. Please note that the input to our policy includes only proprioceptive sensor data and does not incorporate any LiDAR or depth camera information. The height scans listed in Table I are privileged observations employed by the Critic during training."}, {"title": "D. Domain Randomization", "content": "To bridge the gap between simulation and reality, our methodology emphasizes extensive domain randomization of crucial dynamics parameters. This addresses the main sources of real-world variability: environment noise, dynamics noise, and sensory noise.\nRandomization covers environmental elements such as floor friction, orientation, and robot-specific aspects like mass and Center of Mass positioning. Variations in motor parameters, including PD controller settings, are introduced to acclimate the policy to a range of motor behaviors.\nFurthermore, we incorporate system latencies and inject random deviations in the robot's Center of Mass, equipping the policy to handle unforeseen disturbances in real environments. This thorough randomization strategy is essential for ensuring the policy's resilience and flexibility in actual deployment scenarios. Further specifics are presented in Table II."}, {"title": "V. EXPERIMENTS", "content": "In this section, we mainly focus on the performance of challenging settings in both indoor and outdoor environments. The benchmark comparisons discussed below were all conducted using the smaller humanoid robot, which stands 1.2 meters tall. Additionally, we deployed our algorithm on a larger humanoid robot, measuring 1.65 meters in height, as detailed in Fig. 6.\nA. Benchmark Comparison\nFor the empirical assessment of our approach, we conduct a series of experiments using both the XBot-S and XBot-L, applying the learned policy in a zero-shot transfer to real-world settings. This deployment encompasses a range of intricate and challenging terrains, testing the limits of the locomotive capabilities of our robot. To the best of our knowledge, this represents the world first humanoid robot to robustly navigate such complex environments using end-to-end reinforcement learning.\nOur evaluation framework includes comparisons with two baseline methodologies, providing a comprehensive perspective on the effectiveness and advancements offered by our approach. In summary, we run three kinds of algorithms:\n\u2022\tDWL Baseline(ours): This baseline involves the application of the DWL policy with active ankle control. For this configuration, we set the PD gains of the ankle joints to Kp = 20 and Ka = 5. The network architecture details of DWL can be found in Appendix Table VI. The total trainable parameters of the DWL actor is about 320,192.\n\u2022\tPPO with Ankle Control: Here, we eliminate the denoising loss component while retaining the other aspects of our methodology. This setup aims to underscore the enhanced adaptability of our approach in contrast to traditional methods. The network architecture details of PPO can be found in Appendix Table VII. The total trainable parameters of the PPO actor is about 333,312.\n\u2022\tDWL without Ankle Control: Given the complexity of modeling closed kinematic chain ankle mechanisms in bipedal and humanoid robots, many previous RL-based locomotion controls have utilized passive ankle strategies. We conduct comparisons with a DWL variant employing passive ankle control\u00b9 (with Kp = 0 and Ka = 10) to benchmark against this common approach.\nOur experimental scenarios are diverse, including tasks such as snowy ground, up and down stairs, and disturbance rejection. During these tasks, the robot's arms are maintained in a stationary position to isolate the assessment of locomotive performance. This experimental setup provides a testbed to evaluate the versatility and robustness of our locomotion control strategy in real-world conditions. The subsequent benchmark comparisons are conducted exclusively using XBot-S to ensure consistency in our evaluation."}, {"title": "B. Indoor Experimental Validation", "content": "A comprehensive suite of real-world trials is executed to assess the robustness and adaptability of our algorithm in controlling the humanoid robot across a series of challenging terrains. Our indoor experiments employed four distinct terrain types with different difficulties, detailed as follows:\n\u2022\tSlope Transit (Fig. 1F): A sloped platform with a gradient of 0.25 to test the robot's capacity to adeptly shift from planar to inclined locomotion, encompassing both ascent and descent.\n\u2022\tStair Descent (Fig. 1D): Tasked with a downward traversal, the robot encountered stairs, each spanning 20cm wide and 10cm high, commencing from the summit.\n\u2022\tStair Ascent (Fig. 1B): Matching the descent staircase in dimension, the robot faces the challenge of ascending the stairs with limited sensor observations.\n\u2022\tIrregular Terrain (Fig. 1E): A custom-designed landscape with variable elevations up to 10cm, simulating the unpredictability of challenging terrains.\nThe results of these experiments, quantified in Table III, reveal significant insights. When comparing PPO with ankle control to our DWL framework, our approach shows a significant improvement in walking performance, achieving 100% success rates across various terrains. This highlights the superior sim-to-real capabilities of our method and its robust adaptability to diverse terrains. For relatively simple tasks like navigating slopes and descending stairs, both PPO and the passive variant of DWL (DWLp) demonstrate competent success rates. However, in challenging situations such as walking on irregular terrain or climbing stairs, DWL distinctly outperforms, showcasing the adaptability and robustness of our method."}, {"title": "C. Outdoor Experiments", "content": "In addition to extensive indoor testing on various complex terrains, we also conducted prolonged outdoor walking tests across diverse and challenging landscapes. We assessed walking performance on different surfaces and conditions, including cemented ground, brick roads, soil, and snowy terrain. Our algorithm allowed the robot to exhibit stable walking across the aforementioned varied road conditions. Particularly noteworthy is the walking evaluation on snow-covered terrain Fig. 1A, which presents a highly challenging task. Snow, with its deformable nature, poses difficulties as the robot's feet can sink into it, a situation challenging to simulate. Furthermore, snow surfaces tend to be slippery, making robots prone to sliding. Our DWL algorithm, however, demonstrates remarkable stability during prolonged walks on snowy terrain, affirming the robustness and adaptability of our algorithm to diverse terrains."}, {"title": "D. Robustness Testing", "content": "Domain randomization is a common approach to achieving a robust controller. However, its effectiveness is often limited by the sim-to-real gap and the impracticality of accounting for every possible real-world scenario.\nIn our framework, the agent is designed to predict the true state, thereby empowering the controller to rapidly adapt to a variety of situations. For example, if there is a tendency to fall, the controller can quickly recognize this and act to maintain balance. To assess the robustness of our controller, we conducted the following experiments:\n1) Mass Displacement: As shown in Fig. 1G, the robot carried a bag into which we progressively added heavy objects while it was walking. Even with an additional weight of up to 15kg, over a third of the robot's weight, it managed to sustain stable locomotion.\n2) Heavy load: We executed two experiments, depicted in Fig. 1H. It successfully walked with an additional load of up to 20kg, albeit with a slightly lowered height due to the added weight. Also, we attached the robot's hand effector to a loaded cart Fig. 1I, the robot was able to push a cart loaded with 60kg, demonstrating the controller's adaptability to handle significant loads.\n3) Push Recovery: During our experimentation, we subjected the robot to external forces from multiple directions while it was executing continuous standing commands. These tests were carried out on both flat (Fig. 1C) and sloped terrain(Fig. 4), with the robot successfully maintaining its stance in both conditions."}, {"title": "VI. RESULT ANALYSIS", "content": "In this section, we delve into the empirical results obtained from the deployment of the DWL framework on a humanoid robot, specifically focusing on its performance in terrain traversing. Our results are shown in Fig. 5.\n1) Terrain Height Scan Prediction and Gait Adaptation: Our findings highlight the DWL's remarkable ability to predict terrain height, an essential factor for terrain adaptability. Utilizing only proprioceptive inputs, rather than relying on LiDAR or depth cameras, our system is capable of estimating a terrain's rough profile. At first glance, this might seem implausible, but our approach can discern the general trend of the terrain, as exemplified in Fig. 5. DWL's internal model adeptly encodes environmental features with distinct separability, thus facilitating accurate terrain recognition. The robot can identify whether it is walking up a slope or descending stairs. While precise shape prediction remains challenging, even a rough estimate proves immensely useful. This capability crucially affects the robot's gait, as observed when it transits from slope to stairs. Such gait modifications are imperative not just for navigating obstacles but also for ensuring balance and stability across diverse terrains.\n2) Foot Contact Detection and Its Implications: As demonstrated in Fig. 5, foot contact patterns indicate a correlation with the type of terrain encountered. In humanoid locomotion, especially during single-leg support phases, accurate detection of foot contacts is essential for stability. The DWL framework aids in predicting these contact instances, thereby improving the planning of leg swing trajectories and facilitating effective obstacle avoidance. The frequency and pattern of foot contacts vary dynamically, guided by state estimation, leading to crucial gait adjustments and adaptations for complex terrain navigation.\n3) Velocity Estimation: Velocity prediction, particularly linear velocity, is challenging to obtain directly from proprioceptive sensors but is critical for successful locomotion. DWL effectively estimates velocity states within a unified framework, addressing challenges like IMU angular yaw drift, as shown in Appendix Fig. 10. This estimation enhances command following and prevents yaw deviations. The congruence between actual and estimated velocities observed in our experiments significantly aids in the robot's locomotion, ensuring smoother and more predictable movements. Since real states cannot be directly obtained in real-world settings, we validate our state prediction using a sim2sim transfer. We tested our policy in MuJoCo and compared the state estimation with the ground truth. The predictions of linear velocity and Euler yaw are displayed in Appendix Fig.12 and Fig. 11.\nThe mean square error (MSE) of the forward velocity estimation in 60 seconds is 0.046 in the sim-to-sim scenario, while the IMU drift was reduced by around 87% in real-world experiments. These comparisons clearly demonstrate the accurate state estimation capabilities of the DWL algorithm.\n4) Benefits and Importance of Ankle Control: By allowing activated control of both the two freedoms of the ankle, the robot is empowered to traverse complex terrains and resist extra forces. The role of ankle control was pivotal, as it allows the robot to preserve balance, even in single-leg support scenarios, generating human-like stability. Noticeably, ankle control becomes particularly pronounced when walking on irregular blocks and ascending stairs, as in Fig. 4, and also on deformable ground as shown in Fig. 6. Without ankle control, the robot's feet deformed significantly upon contacting the uneven ground, failing to recover. This was coupled with inadequate ankle joint contact forces, raising the risk of imbalance and falls during foot placements. In contrast, our ankle control method navigates these terrains with ease by adaptive contact forces at the ankle joints, as shown in the torque plot in Fig. 4, enabling the robot's adaptability to varied terrains."}, {"title": "VII. CONCLUSION", "content": "In this work, we proposed Denoising World Model Learning (DWL) for complex humanoid robot locomotion skill learning. The framework first masked out privileged information and injected appropriate noise into the true state observed in the simulation. It then designs an auto-encoder architecture to denoise the observations and reconstruct the true state. We achieved success in extensive real-world experiments in various complex environments such as snowy land, stairs, deformable ground, and irregular surfaces. Demonstrating the world's first humanoid robot to master challenging terrains with end-to-end RL and zero-shot sim-to-real transfer. In-depth result analysis indicated the effectiveness of the state estimation capability and the importance of the active 2-DoF ankle control. In the future, visual information will be added to enable more efficient navigation in challenging terrains while maintaining robustness."}, {"title": "APPENDIX", "content": "A. Quintic Polynomial Foot Trajectory\nIn Section.IV-B, we introduced the foot trajectory interpolation method we designed. Through this approach, a smooth foot trajectory can be generated as a reference, and a tracking reward is utilized to encourage the robot's feet to follow the generated trajectory. Table.IV presents the specific polynomial parameters and optimization conditions for generating foot trajectories. Figure.7 illustrates the curves of height, velocity, and acceleration for a generated foot trajectory. It can be observed that smoothness is maintained across the positional, velocity, and acceleration aspects.\nB. Reward Function\nFor periodic reward design, inspired by previous work [31, 32], our objective is to construct a reward function that leverages the distinct roles of foot forces and foot velocities. Specifically, the function aims to promote higher foot velocities during the swing phase and reasonable foot forces during the stance phase of locomotion. We introduce a binary feet contact indicator, I(t), termed the periodic stance mask. As illustrated in Fig. 8, this indicator is set to 1 during the planned contact phase and to 0 during the planned swing phase, alternating for each leg throughout a locomotion cycle. The periodic rewards are formulated as follows:\n$periodic(t) = I_L(t) \\cdot F_L + I_R(t) \\cdot F_R$\n$periodic(t) = (1 - I_L(t)) \\cdot P_f + (1 - I_R(t)) \\cdot P_f$\nIn this context, the symbol F denotes force, while L and R refer to the left and right foot, respectively. For clarity, we omit the scaling and clipping factors typically applied to the forces and velocities, which are necessary due to their differing magnitudes. For instance, forces, which can reach magnitudes of hundreds, are scaled down by a factor of 400 and subsequently clipped to the range [0, 1] to maintain consistency in the reward function's scale."}]}