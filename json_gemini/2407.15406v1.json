{"title": "Automated Road Safety: Enhancing Sign and Surface\nDamage Detection with AI", "authors": ["Davide Merolla", "Vittorio Latorre", "Antonio Salis", "Gianluca Boanelli"], "abstract": "Public transportation plays a crucial role in our lives, and the road network\nis a vital component in the implementation of smart cities. Recent advancements in AI\nhave enabled the development of advanced monitoring systems capable of detecting\nanomalies in road surfaces and road signs, which, if unaddressed, can lead to serious road\naccidents. This paper presents an innovative approach to enhance road safety through the\ndetection and classification of traffic signs and road surface damage using advanced deep\nlearning techniques. This integrated approach supports proactive maintenance strategies,\nimproving road safety and resource allocation for the Molise region and the city of\nCampobasso. The resulting system, developed as part of the Casa delle Tecnologie\nEmergenti (House of Emergent Technologies) Molise (Molise CTE) research project\nfunded by the Italian Minister of Economic Growth (MIMIT), leverages cutting-edge\ntechnologies such as Cloud Computing and High Performance Computing with GPU\nutilization. It serves as a valuable tool for municipalities, enabling quick detection of\nanomalies and the prompt organization of maintenance operations.", "sections": [{"title": "Introduction", "content": "According to latest World Health Organization (WHO) [1] public road networks are\nthe lifeblood of modern societies, playing a crucial role for goods and people\ntransportation, which is fundamental for trade, commerce and tourism. At the same time,\nthey enable easy access to jobs, education, healthcare, and social activities, both in urban\nand rural regions. The status of road safety is crucial: harsh weather conditions can\naccelerate road degradation, and time the volume of traffic, including heavy traffic, grows,\nthus requiring frequent repairs. Any fault in maintenance could lead to severe incidents\nwith death and injury worldwide. The latest report (2023) states that about 1.19 million\npeople die each year as a result of road traffic crashes.\nThis paper explores a novel approach to traffic sign and road damage detection using\nadvanced deep learning techniques, that are used in a Road Management System of a\nPublic Municipality. This work is in progress within the Molise CTE research project,\nfunded by the Italian Ministry of the Economic Growth (MIMIT), with the aim to leverage\nthe best emerging technologies such as Cloud Computing, High Performance Computing,\nArtificial Intelligence, and AR/VR to develop and demonstrate state-of-the-art solutions in\nthe Smart City environment.\nThe paper is structured as follows. Section 2 introduces a Literature Review, with\ndescription of more recent trends in research about the topic, then Section 3 provides a\ndetailed description of the proposed solution, including information on the supporting\ncloud infrastructure. Section 4 gives a description of computational experiments with\nrelevant metrics and giving evidence of the effectiveness of the proposed solution. Finally,\nSection 5 describes further improvements under development, the benefits and exploitation\nopportunities, and concludes the paper."}, {"title": "Literature Review", "content": "The detection and classification of traffic signs and road damage are vital components\nof intelligent transportation systems. Recent advancements in deep learning have\nsignificantly enhanced the capabilities of these systems, making them more accurate and\nefficient. This literature review explores various methods and approaches used in traffic\nsign detection, classification, and road damage detection, with a particular focus on the\nuse of YOLO architecture and Convolutional Neural Networks (CNNs).\nTraffic Sign Detection and Classification\n1) Seminal Papers: Early research laid the groundwork for the development of\nautomated traffic sign detection and classification systems. An influential paper by\nMaldonado-Bascon et al. [2] introduces an automatic road-sign detection and\nrecognition system utilizing Support Vector Machines (SVMs). The system improves\ndriver-assistance by effectively detecting and recognizing various road sign shapes\nthrough a combination of color segmentation, linear SVM-based shape classification,\nand Gaussian kernel SVM recognition. The approach exhibits high success rates and\nrobustness against transformations and occlusions. Another notable paper by Fang,\nChen, and Fuh [3] outlines a method for road sign detection and tracking using neural\nnetworks and Kalman filters. This system leverages neural networks to extract color\nand shape features and employs Kalman filters to track the detected signs through\nimage sequences, maintaining robust performance under diverse environmental\nconditions.\n2) YOLO Architecture: The YOLO (You Only Look Once) architecture has emerged as a\npopular choice for object detection tasks due to its real-time processing capabilities\nand high accuracy. Several studies have demonstrated the effectiveness of YOLO in\ntraffic sign detection. For instance, Yang and Zhang (2020) [4] compared the\nperformance of YOLOv4 and YOLOv3, finding that YOLOv4 significantly improved\ndetection accuracy on a dataset of Chinese traffic signs. Similarly, Zhang (2023) [5]\nshowed that YOLOv3 outperformed R-CNN algorithms in terms of speed and\naccuracy for traffic sign detection.\n3) Enhancements in YOLO: Recent enhancements in YOLO include the development\nof lightweight models such as Sign-YOLO, which integrates the Coordinate Attention\n(CA) module and High-BiFPN to improve feature extraction and multi-scale semantic\ninformation fusion. This model achieved significant improvements in precision, recall,\nand detection speed on the CCTSDB2021 dataset [6]. Another example is the\nPVF-YOLO model, which uses Omni-Dimensional Convolution (ODconv) and Large\nKernel Attention (LKA) to enhance detection accuracy and speed [7].\n4) Traffic Sign Classification: After detection, traffic signs need to be classified into\nspecific categories. Convolutional Neural Networks (CNNs) have been widely used\nfor this purpose due to their strong feature extraction capabilities. Ciresan et al. (2012)\n[8] employed a CNN-based approach to classify German traffic signs, achieving state-\nof-the-art results. Additionally, improved YOLO models like TSR-YOLO have\nincorporated advanced modules to enhance accuracy in complex scenarios [9].\n5) Traffic Sign Damage Classification: To the best of our knowledge, only two papers\naddress traffic sign damage classification comprehensively. Ana Trpkovi\u0107, Milica\nSelmic, and Sreten Jevremovic' (2021) [10] developed a CNN model to identify and\nclassify damaged and vandalized traffic signs. Another significant contribution is by J.\nN. Acilo et al. (2018) [11], who presented a study titled \"Traffic Sign Integrity\nAnalysis Using Deep Learning.\" This research employed transfer learning with the\nResNet-50 architecture to classify the compliance and physical degradation status of\ntraffic signs, achieving high accuracy.\nRoad Damage Detection\n1) Dataset Utilization: Public datasets such as Mappilary [18] and the Road Damage\nDetection (RDD) [19] dataset provide comprehensive collections of annotated images\nfor training and evaluating models. These datasets cover various types of road\ndamage, including potholes, cracks, and surface wear, facilitating the development of\nrobust detection models.\n2) Deep Learning Approaches: Deep learning approaches, particularly those using\nCNNs and YOLO architectures, have shown significant advancements in road\ndamage detection. Zhang et al. (2018) [12] employed a deep CNN model to detect\nroad cracks from images, achieving high precision and recall rates. Similarly, Maeda\net al. (2018) [13] applied YOLO to detect multiple types of road damage,\ndemonstrating the model's effectiveness in real-world scenarios.\nGPS Data\n1) Integration of GPS Data: M. Strutu, G. Stamatescu, and D. Popescu (2013) [14]\nintroduced a mobile sensor network-based system for monitoring road surfaces,\nincorporating 3D accelerometers, GPS, and video modules. Their research\ndemonstrated the effectiveness of integrating multiple sensors for comprehensive\nroad monitoring. Similarly, M. Perttunen et al. (2011) [15] developed a system for\ndetecting road surface anomalies using accelerometers and GPS readings from\nmobile phones. Their pattern recognition system showcased the potential of mobile\ndevices in monitoring road conditions effectively. Furthermore, R. Tarun and B. P.\nEsther (2023) [16] created an affordable road sign detection system utilizing a\nRaspberry Pi and GPS. Their system achieved high detection precision and\ndemonstrated efficient real-time operation.\nAdditional Insights from Recent Advances\nRecent advances in traffic sign recognition have explored a variety of machine\nlearning and deep learning techniques. Lim et al. (2023) [17] provided a\ncomprehensive overview of these advancements, highlighting the importance of\npreprocessing techniques, feature extraction methods, classification techniques, and\nthe use of diverse datasets to address the challenges posed by different geographical\nregions, complex backgrounds, and varying illumination conditions.\nKey contributions from recent studies include [17]:\nA comprehensive review of state-of-the-art traffic sign recognition work,\ncategorizing studies into conventional machine learning and deep learning\napproaches.\nDiscussion of widely adopted traffic sign recognition datasets, their challenges, and\nlimitations, as well as future research prospects in this field.\nEmphasis on the importance of diverse datasets for improving model generalization\nand robustness."}, {"title": "Methodology", "content": "First Datasets\nMapillary:\nMapillary Vistas Dataset [18] is a large-scale street-level imagery dataset designed for\ntraining and evaluating semantic segmentation models. This dataset is highly diverse,\ncovering a wide range of environments, lighting conditions, and geographical locations. It\nincludes various types of road signs, objects, and infrastructure commonly found in\nurban, suburban, and rural areas. The dataset is particularly useful for developing and\ntesting algorithms for autonomous driving and urban planning applications.\nClasses: 401\nImages: 41,906\nSize: 32.8 GB\nThe dataset is split into training and validation sets, with 80% of the images used for\ntraining and 20% reserved for validation.\nRDD 2022:\nThe Road Damage Detection (RDD) 2022 dataset [19] focuses on identifying and\nclassifying different types of road surface damages. This dataset includes annotated\nimages of road damage from various countries, making it a valuable resource for training\nmachine learning models aimed at improving road maintenance and safety. The primary\ngoal of using this dataset is to detect and classify road damages such as cracks, potholes,\nand other surface irregularities.\nClasses: 4\nImages: 34,007\nSize: 9.6 GB\nSimilar to the Mapillary dataset, the RDD 2022 dataset is also divided into an 80%\ntraining set and a 20% validation set. This split ensures that the models can be trained\neffectively while also being evaluated on a separate set of images to test their performance\nand generalization capabilities.\nA comprehensive representation of the architecture for the road sign detection and\nclassification system is depicted in Figure 1. For road damage, detection alone is\nsufficient, as these are already considered anomalies; therefore, the Yolo model is\nadequate.\nB. First Phase\nBefore training the YOLO architectures, data manipulation is essential to ensure optimal\nperformance and accuracy. The data manipulation techniques include the following:\nData Augmentation: This involves applying various transformations to the\ntraining images, such as rotations, scaling, flipping, and color adjustments. These\ntechniques help to increase the diversity of the training data and make the model\nmore robust to different conditions.\nNormalization: Image pixel values are scaled to a standard range, typically\nbetween 0 and 1, to ensure uniformity and improve the convergence of the model\nduring training.\nLabel Smoothing: This technique is used to reduce overfitting by softening the\nhard labels in the training data, making the model less confident in its predictions\nand improving generalization.\nAnchor Box Calculation: Custom anchor boxes are computed based on the dataset\nto improve the detection accuracy of the YOLO model, especially for objects of\nvarious sizes.\nYOLOv8s for Road Surface Damages Detection:\nYOLOv8s is a specific architecture within the YOLO (You Only Look Once) family,\noptimized for real-time object detection with a balance between speed and accuracy. It is\nparticularly suitable for detecting road surface damages dueto its efficient design.\nPretrained: Yes\nEpochs: 160\nImage Size: 640\nPatience: 100\nCache: RAM\nDevice: GPU\nBatch Size: 64\nC. Second Dataset\nOnce the YOLO model is trained, it is necessary to build a dataset for the\nclassification of road signs. To achieve this, videos recorded with a dashcam on the\nroad are processed with YOLO, which crops the road signs from the frames. An example\nof the detected road sign is illustrated in Figure 3. The cropped signs are then labelled as\ndamaged or not damaged based on the following criteria:\nSigns covered with spray-painted graffiti\nSigns covered with stickers\nBent or physically damaged signs\nRusty signs\nThe resulting dataset, in preliminary tests, is unbalanced, with 203 damaged and 46\nundamaged signs. To address this imbalance, we employed two advanced techniques:\nFocal Loss: This loss function is designed to handle class imbalance by assigning\nmore weight to hard-to-classify examples, reducing the impact of easily classified\nexamples, and improving model performance on imbalanced data.\nCutout Regularization: This technique involves randomly removing sections of the\nimage during training. It helps improve model robustness and prevent overfitting,\nthereby enhancing the model's ability to generalize to new data.\nThese techniques allow us to effectively manage the dataset imbalance and improve the\naccuracy of classifying damaged and undamaged road signs.\nD. Second Phase\nIn this phase, we develop a Convolutional Neural Network (CNN) for classifying road\nsigns as damaged or not damaged. The CNN architecture and training process are\ndescribed as follows:\nThe CNN model consists of the following layers:\nAn input layer that accepts images of size 150x150x3 (height, width, and color\nchannels).\nA 2D convolutional layer with 32 filters of size 3x3, followed by a ReLU\nactivation function.\nA max-pooling layer with a pool size of 2x2.\nA 2D convolutional layer with 64 filters of size 3x3, followed by a ReLU\nactivation function.\nA max-pooling layer with a pool size of 2x2.\nA 2D convolutional layer with 128 filters of size 3x3, followed by a ReLU\nactivation function.\nA max-pooling layer with a pool size of 2x2.\nA flattening layer to convert the 2D matrix data to a vector.\nA dense (fully connected) layer with 512 units and aReLU activation function.\nA dropout layer with a dropout rate of 0.5 to prevent overfitting.\nA dense output layer with 1 unit and a sigmoid activation function for binary\nclassification.\nThe model is optimized by using Adam, with the Sigmoid Focal Cross-Entropy loss\nfunction, particularly effective for handling class imbalance. The accuracy metric is used\nto evaluate the model's performance.\nTo improve the robustness of the model and generalization capability, data\naugmentation techniques are applied, including rotation, width and height shifts, shear,\nzoom, and horizontal flips. Additionally, cutout regularization is implemented by\nrandomly masking sections of the input images during training.\nThe training process involves:"}, {"title": "Computational Experiments", "content": "A. Computational Characteristics\nThe training of the YOLO models is conducted using Google Colab, leveraging the\nNVIDIA Tesla T4 GPU. Google Colab provides a high-performance computing\nenvironment suitable for deep learning tasks. The key specifications of the hardware used\nare as follows:\nGPU: NVIDIA Tesla T4\nCUDA Cores: 2560\nTensor Cores: 320\nGPU Memory: 16 GB GDDR6\nMemory Bandwidth: 320 GB/s\nPerformance: Up to 8.1 TFLOPS (FP32)\nCPU: Intel(R) Xeon(R) CPU\nvCPUs: 2 (Base Frequency: 2.3 Ghz)\nRAM: 12.7 GB available in the Colab environment\nDisk: 100 GB available storage\nFor the CNN, training was performed on Reevo servers from Tiscali with the\nfollowing specifications:\nRAM: 32 GB\nCPU: 24 vCPUs (Base Frequency: 2.5 Ghz)\nThe combination of these computational resources provides a robust environment for\ntraining and validating the deep learning models, enabling efficient processing of large\ndatasetsand complex computations required for road sign detection and classification.\nB. Metrics for Performance Evaluation and Results\nTo evaluate the performance of the YOLOv8x model for road sign detection, we analyze\nseveral key metrics, including accuracy, box loss, and object loss. These metrics provide\ninsights into the model's effectiveness in detecting and classifying road signs accurately.\n1) YOLOv8x Accuracy: Figure 4 shows the accuracy of the YOLOv8x model over\nthe training epochs. The accuracy metric includes the mean Average Precision (mAP) at\ndifferent Intersection over Union (IoU) thresholds and other performance metrics such as\nprecision and recall.\nSpecifically, we track:\nmAP50: Mean Average Precision at 50% IoU threshold.\nmAP50-95: Mean Average Precision averaged over IoU thresholds from 50\nPrecision: The ratio of true positive detections to the total number of positive\ndetections (true positives + false positives).\nRecall: The ratio of true positive detections to the total number of actual positives\n(true positives + false negatives).\nThe graph indicates the following trends:\nThe mAP50 metric (blue line) shows a steady improvement, stabilizing around 0.9,\nindicating a high level of accuracy for the model in detecting objects with a 50%\nIoU threshold.\nThe mAP50-95 metric (orange line) improves gradually, reflecting the model's\nperformance across a wider range of IoU thresholds. It stabilizes around 0.7,\nshowcasingthe model's robustness in varying detection conditions.\nPrecision (cyan line) shows fluctuations but generally trends upwards, indicating\nimprovements in the model's ability to reduce false positives over time.\nRecall (magenta line) also improves and stabilizes around 0.8, demonstrating the\nmodel's effectiveness in capturing most of the actual positive instances.\n2) YOLOv8x Box Loss: Figure 5 illustrates the box loss during training. Box loss\nmeasures the error in predicting the bounding boxes for detected objects. It is a crucial\nmetric for object detection models as it directly affects the precision of the detected\nobjects' locations. Lower box loss values indicate more accurate predictions of the\nbounding box coordinates.\nThe graph shows that the box loss decreases significantly during the initial epochs and\nstabilizes over time, indicating that the model is learning to accurately predict the bounding\nbox locations of the road signs.\n3) YOLOv8x Object Loss: Figure 6 presents the object loss over the training epochs.\nObject loss evaluates the error in classifying whether a particular region in the image\ncontains an object of interest. Lower object loss values reflect the model's enhanced\ncapability to distinguish between objects and the background, leading to more reliable\ndetections.\nThe graph indicates that the object loss decreases rapidly during the initial epochs and\nthen gradually stabilizes, suggesting that the model becomes increasingly proficient at\ndistinguishing between road signs and background noise as training progresses.\nThese metrics collectively provide a comprehensive overview of the YOLOv8x\nmodel's performance in detecting and classifying road signs. The continuous\nimprovement in accuracy and reduction in both box and object loss throughout the\ntraining process indicate the model's effectiveness and robustness in handling the task of\nroad sign detection.\nA similar analysis applies to YOLOv8s, which is used for detecting road surface\ndamages. The following observations were made:\nAccuracy: The accuracy metrics for YOLOv8s, as shown in Figure 7, indicate steady\nimprovement over epochs, with metrics such as mAP50, mAP50-95, precision, and\nrecall showing consistent performance gains.\nBox Loss: As shown in Figure 8, the box loss decreases over time, indicating improved\nprecision in predicting thebounding box locations for road damages.\nObject Loss: The object loss, depicted in Figure 9, shows a downward trend,\ndemonstrating enhanced capability in distinguishing between damaged and undamaged\nroad surfaces.\n4) CNN: Graph 10 indicates the following trends:\nHigh Training Accuracy and Precision: The training accuracy metric (blue line)\nstarts low and increases rapidly within the first few epochs. After the initial increase,\nthe accuracy stabilizes around a high value, indicating that the model is effectively\nlearning the training data and making correct predictions and positive identifications.\nTraining precision (green line) remains relatively high, though it fluctuates,\nsuggesting some variability in the learning process regarding positive predictions.\nValidation Metrics: The validation accuracy metric (orange line) starts low and\nincreases over the first few epochs, stabilizing at a value slightly lower than the\ntraining accuracy. This indicates that the model is generalizing reasonably well to the\nvalidation set without overfitting too much. Validation precision (red line) shows\nsignificant fluctuation throughout the epochs, likely due to dataset imbalance and its\nlimited size, which is logical in this preliminary testing phase. Both training and\nvalidation recall (purple and pink lines, respectively) are high and fairly stable,\nindicating that the model is effectively identifying positive instances with minimal\nfalse negatives.\nPotential Overfitting: There is a slight indication of overfitting due to the gap\nbetween training and validation precision. This overfitting is likely attributable to the\nsmall size and imbalance of this preliminary phase, as other influencing factors such as\nmodel complexity, data augmentation, and hyperparameter tuning (e.g.,\nregularization, epochs) have been appropriately addressed."}, {"title": "Conclusions", "content": "In this study, we successfully develop and train YOLO models for road sign detection\nand CNN models for classifying road signs as damaged or not damaged. Our approach\nutilizes data augmentation and cutout regularization techniques to improve the robustness\nand generalization of our models. The computational experiments conducted on Google\nColab and Tiscali servers demonstrates the effectiveness of our methods in handling\nlarge datasets and complex computations.\nFor future work, we propose the following extensions to enhance the capabilities and\napplications of our models:\nIncorporating Retroreflectivity Factors: To further refine the classification of\nroad signs, we plan to include retroreflectivity factors in our analysis. This involves\ndetecting and classifying faded or discolored signs, which can significantly impact\nroad safety. Developing models that can identify such signs will be crucial for\ntimely maintenance and replacement.\nLeveraging Generative AI for Data Labeling: The process of manually labeling\nlarge datasets is time- consuming and prone to human error. By employing\ngenerative Al techniques, we can automate the labeling process, thereby reducing\nthe time and effort required. This will also enable us to handle larger datasets more\nefficiently.\nGenerating Synthetic Data for Balanced Datasets: One of the challenges we faced\nwas the imbalance between damaged and non-damaged road signs in our dataset. To\naddress this, we propose using generative AI to create synthetic images of damaged\nsigns. By artificially \"damaging\u201d images of non-damaged signs (e.g., adding graffiti,\nstickers, rust, and physical damage), we can construct a balanced dataset. This\nsynthetic data will enhance the training process, making our models more robust and\naccurate."}]}