{"title": "Jacobian Sparse Autoencoders: Sparsify Computations, Not Just Activations", "authors": ["Lucy Farnik", "Tim Lawson", "Conor Houghton", "Laurence Aitchison"], "abstract": "Sparse autoencoders (SAEs) have been successfully used to discover sparse and human-interpretable representations of the latent activations of LLMs. However, we would ultimately like to understand the computations performed by LLMs and not just their representations. The extent to which SAEs can help us understand computations is unclear because they are not designed to \"sparsify\" computations in any sense, only latent activations. To solve this, we propose Jacobian SAEs (JSAEs), which yield not only sparsity in the input and output activations of a given model component but also sparsity in the computation (formally, the Jacobian) connecting them. With a na\u00efve implementation, the Jacobians in LLMs would be computationally intractable due to their size. One key technical contribution is thus finding an efficient way of computing Jacobians in this setup. We find that JSAEs extract a relatively large degree of computational sparsity while preserving downstream LLM performance approximately as well as traditional SAEs. We also show that Jacobians are a reasonable proxy for computational sparsity because MLPs are approximately linear when rewritten in the JSAE basis. Lastly, we show that JSAEs achieve a greater degree of computational sparsity on pre-trained LLMs than on the equivalent randomized LLM. This shows that the sparsity of the computational graph appears to be a property that LLMs learn through training, and suggests that JSAEs might be more suitable for understanding learned transformer computations than standard SAEs.", "sections": [{"title": "1. Introduction", "content": "Sparse autoencoders (SAEs) have emerged as a powerful tool for understanding the internal representations of large language models (Bricken et al., 2023; Cunningham et al., 2023; Gao et al., 2024; Rajamanoharan et al., 2024b; Lieberum et al., 2024; Lawson et al., 2024; Braun et al., 2024; Kissane et al., 2024; Rajamanoharan et al., 2024a). By decomposing neural network activations into sparse, interpretable components, SAEs have helped researchers gain significant insights into how these models process information (Marks et al., 2024; Lieberum et al., 2024; Templeton et al., 2024b; O'Brien et al., 2024; Farrell et al., 2024; Paulo et al., 2024; Balcells et al., 2024; Lan et al., 2024; Brinkmann et al., 2025; Spies et al., 2024).\nWhen trained on the activation vectors from neural network layers, SAEs learn to reconstruct the inputs using a dictionary of sparse 'features', where there are many more features than basis dimensions of the inputs, and each feature tends to capture a specific, interpretable concept. However, the goal of this paper is to improve understanding of computations in transformers. While SAEs are designed to disentangle the representations of concepts in the LLM, they are not designed to help us understand the computations performed with those representations. Indeed, SAEs have been shown to exhibit pathological behaviors such as feature absorption, which seem unlikely to be properties of the actual LLM computation (Chanin et al., 2024).\nOne approach to understanding computation would be to train two SAEs, one at the input and one at the output of an MLP in a transformer. We can then ask how the MLP maps sparse latent features at the inputs to sparse features in the outputs. For this mapping to be interpretable, it would be desirable that it is sparse, in the sense that each latent in the SAE trained on the output depends on a small number of latents of the SAE trained on the input. These dependencies can be understood as a computation graph or 'circuit' (Olah et al., 2020; Cammarata et al., 2020). SAES are not designed to encourage this computation graph to be sparse. To address this, we develop Jacobian SAEs (JSAEs), where we include a term in the objective to encourage SAE bases with sparse computational graphs, not just sparse activations. Specifically, we treat the mapping between the latent activations of the input and output SAEs as a function and encourage its Jacobian to be sparse by including an $L^1$ penalty term in the loss function.\nWith a na\u00efve implementation, it is intractable to compute Jacobian matrices because each matrix would have on the order of a trillion elements, even for modestly sized language"}, {"title": "2. Related work", "content": null}, {"title": "2.1. Sparse autoencoders", "content": "SAEs have been widely applied to 'disentangle' the representations learned by transformer language models into a very large number of concepts, a.k.a. sparse latents, features, or dictionary elements (Sharkey et al., 2022; Cunningham et al., 2023; Bricken et al., 2023; Gao et al., 2024; Rajamanoharan et al., 2024b; Lieberum et al., 2024). Human experiments and quantitative proxies apparently confirm that SAE latents are much more likely to correspond to human-interpretable concepts than raw language-model neurons, i.e., the basis dimensions of their activation vectors (Cunningham et al., 2023; Bricken et al., 2023; Rajamanoharan et al., 2024a). SAEs have been successfully applied to modifying the behavior of LLMs by using a direction discovered by an SAE to \"steer\" the model towards a certain concept (Makelov, 2024; O'Brien et al., 2024; Templeton et al., 2024b)."}, {"title": "2.2. Transcoders", "content": "In this paper, we focus on MLPs. Dunefsky et al. (2024); Templeton et al. (2024a) developed transcoders, an alternative SAE-like method to understand MLPs. However, JSAEs and transcoders take radically different approaches and solve radically different problems. This is perhaps easiest to see if we look at what transcoders and JSAEs sparsify. JSAEs are fundamentally an extension of standard SAEs: they train SAEs at the input and output of the MLP and add an extra term to the objective such that these sparse latents are also appropriate for interpreting the MLP. In contrast, transcoders do not sparsify the inputs and outputs; they work with dense inputs and outputs. Instead, transcoders, in essence, sparsify the MLP hidden states. Specifically, a transcoder is an MLP that you train to match (using a mean squared error objective) the input-to-output mapping of the underlying MLP from the transformer. The key difference between the transcoder MLP and the underlying MLP is that the transcoder MLP is much wider, and its hidden layer is trained to be sparse.\nThus, transcoders and JSAEs take fundamentally different approaches. Each transcoder latent tells us 'there is computation in the MLP related to [concept].' By comparison, JSAEs learn a pair of SAEs (which have mostly interpretable latents) and sparse connections between them. At a conceptual level, JSAEs tell us that 'this feature in the MLP's output was computed using only these few input features'. Ultimately, we believe that the JSAE approach, grounded in understanding how the SAE basis at one layer is mapped to the SAE basis at another layer, is potentially powerful and worth thoroughly exploring.\nImportantly, it is worth emphasizing that JSAEs and transcoders are asking fundamentally different questions, as can be seen in terms of e.g., differences in what they sparsify. As such, it is not, to our knowledge, possible to design meaningful quantitative comparisons, at least not without extensive future work to develop very general auto-interpretability methods for evaluating methods of understanding MLP circuits."}, {"title": "2.3. Automated circuit discovery", "content": "In \"automated circuit discovery\u201d, the goal is to isolate the causally relevant intermediate variables and connections between them necessary for a neural network to perform a given task (Olah et al., 2020). In this context, a circuit is defined as a computational subgraph with an interpretable function. The causal connections between elements are determined via activation patching, i.e., modifying or replacing the activations at a particular site of the model (Meng et al., 2022; Zhang & Nanda, 2023; Wang et al., 2022; Hanna et al., 2023). In some cases, researchers have identified sub-components of transformer language models with simple algorithmic roles that appear to generalize across models (Olsson et al., 2022).\nConmy et al. (2023) proposed a means to automatically prune the connections between the sub-components of a neural network to the most relevant for a given task using activation patching. Given a choice of task (i.e., a dataset and evaluation metric), this approach to automated circuit discovery (ACDC) returns a minimal computational subgraph needed to implement the task, e.g., previously identified 'circuits' like Hanna et al. (2023). Naturally, this is computationally expensive, leading other authors to explore using linear approximations to activation patching (Nanda, 2023; Syed et al., 2024; Kram\u00e1r et al., 2024). Marks et al. (2024) later improved on this technique by using SAE latents as the nodes in the computational graph.\nIn a sense, these methods are supervised because they require the user to specify a task. Naturally, it is not feasible to manually iterate over all tasks an LLM can perform, so a fully unsupervised approach is desirable. With JSAEs, we take a step towards resolving this problem, although the architecture introduced in this paper initially only applies to a single MLP layer and not an entire model. Additionally, to the best of our knowledge, no automated circuit discovery algorithm sparsifies the computations inside of MLPs."}, {"title": "3. Background", "content": null}, {"title": "3.1. Sparse autoencoders", "content": "In an SAE, we have input vectors, $x \\in X = \\mathbb{R}^{m_x}$. We want to approximate each vector $x$ by a sparse linear combination of vectors, $s_x \\in S_x = \\mathbb{R}^{n_x}$. The dimension of the sparse vector, $n_x$, is typically much larger than the dimension of the input vectors $m_x$ (i.e. the basis is overcomplete).\nIn the case of SAEs, we treat the vectors as inputs to an autoencoder with an encoder $e_x: X \\rightarrow S_x$ and a decoder $d_x: S_x \\rightarrow X$ defined by,\n$s_x = e_x(x) = \\sigma(W_{\\text{enc}}x + b_{\\text{enc}})$ (1)\n$\\hat{x} = d_x(s_x) = W_{\\text{dec}}s_x + b_{\\text{dec}}$ (2)\nHere, the parameters are the encoder weights $W_{\\text{enc}} \\in \\mathbb{R}^{n_x \\times m_x}$, decoder weights $W_{\\text{dec}} \\in \\mathbb{R}^{m_x \\times n_x}$, encoder bias $b_{\\text{enc}} \\in \\mathbb{R}^{n_x}$, and decoder bias $b_{\\text{dec}} \\in \\mathbb{R}^{m_x}$. The nonlinearity $\\sigma$ can be, for instance, ReLU. These parameters are then optimized to minimize the difference between $x$ and $\\hat{x}$, typically measured in terms of the mean squared error (MSE), while imposing an $L^1$ penalty on the latent activations $s_x$ to incentivize sparsity."}, {"title": "3.2. Automatic interpretability of SAE latents", "content": "In order to compare the quality of different SAEs, it is desirable to be able to quantify how interpretable its latents are. A popular approach to quantifying interpretability at scale is to collect the examples that maximally activate a given latent, prompt an LLM to generate an explanation of the concept the examples have in common, and then prompt an LLM to predict whether a given prompt activates the SAE latent given the generated explanation. We can then score the accuracy of the predicted activations relative to the ground truth. There are several variants of this approach (e.g., Bills et al., 2023; Choi et al., 2024); in this paper, we use \"fuzzing\" where the scoring model classifies whether the highlighted tokens in prompts activate an SAE latent given an explanation of that latent (Paulo et al., 2024)."}, {"title": "4. Methods", "content": "The key idea with a Jacobian SAE is to train a pair of SAEs on the inputs and outputs of a neural network layer while additionally optimizing the sparsity of the Jacobian of the function that relates the input and output SAE latent activations (Figure 1). In this paper, we apply Jacobian SAEs to multi-layer perceptrons (MLPs) of the kind commonly found in transformer language models (Radford et al., 2019; Biderman et al., 2023)."}, {"title": "4.1. Setup", "content": "Consider an MLP mapping from $x \\in X$ to $y \\in Y$, i.e., $f: X \\rightarrow Y$ or $y = f(x)$. We can then train two $k$-sparse SAEs, one on $x$ and the other on $y$. The resulting SAEs map from each of $x$ and $y$ to corresponding sparse latents $s_x \\in S_x$ and $s_y \\in S_y$, i.e., $s_x = e_x(x)$ and $s_y = e_y(y)$, where $e_x$ is the encoder of the first SAE and $e_y$ is the encoder of the second SAE. Each of these SAEs also has a decoder that maps from the sparse latents back to an approximation of the original vector: $\\hat{x} = d_x(s_x)$ and $\\hat{y} = d_y(s_y)$.\nWe may now consider the function $f_s: S_x \\rightarrow S_y$, which intuitively represents the function, $f$, but written in terms of the sparse bases learned by the SAE pair for the original vectors $x$ and $y$. Specifically, we define $f_s$ by\n$f_s = e_y \\odot f \\odot d_x \\odot T_k$ (3)\nwhere $\\odot$ denotes function composition. Here, $d_x: S_x \\rightarrow X$ maps the sparse latents given as input to $f_s$ to \u201cdense\" inputs. Then, $f: X \\rightarrow Y$ maps the dense inputs to dense outputs. Finally, $e_y: Y \\rightarrow S_y$ maps the dense outputs to sparse outputs. Note that $f_s$ first applies the TopK activation function $T_k$ to the sparse inputs, $s_x$. Critically, with $k$-sparse SAEs, we produce the sparse inputs by $s_x = e_x(x)$, implying that $s_x$ only has $k$ non-zero elements. In that setting, TopK does not change the inputs, i.e. $s_x = T_k(s_x)$, but it does affect the"}, {"title": "4.2. Making the Jacobian calculation tractable", "content": "Computing the Jacobian naively (e.g., using an automatic differentiation package) is computationally intractable, as the full Jacobian has size $B \\times n_y \\times n_x$ where $B$ is the number of tokens in a training batch $n_x$ is the number of SAE latents for the input, and $n_y$ is the number of SAE latents for the output. Unfortunately, typical values are around 1,000 for $B$ and around 32,000 for $n_x$ and $n_y$ (taking as an example a model dimension of 1,000 and an expansion factor of 32). Combined, this gives a Jacobian with around 1 trillion elements. This is obviously far too large to work with in"}, {"title": "5. Results", "content": "Our experiments were performed on LLMs from the Pythia suite (Biderman et al., 2023), the figures in the main text contain results from Pythia-410m unless otherwise specified. We trained on 300 million tokens with $k = 32$ and an expansion factor of 64 for Pythia-410m and 32 for smaller models. We reproduced all our experiments on multiple models and found the same qualitative results (see Appendix D)."}, {"title": "5.1. Jacobian sparsity, reconstruction quality, and auto-interpretability scores", "content": "First, we compared the Jacobian sparsity for standard SAEs and JSAEs. Note that, unlike with SAE latent activations, there is no mechanism for producing exact zeros in the Jacobian elements corresponding to active latents. Hence, we consider the number of near-zero elements rather than the number of exact zeros. To quantify the difference in sparsity between the two, we looked at the proportion of the elements of the Jacobian above a particular threshold when aggregating over 10 million tokens (Figure 2). Here, we found that JSAEs dramatically reduced the number of large elements of the Jacobian relative to traditional SAEs.\nImportantly, the degree of sparsity depends on our choice of the coefficient $\\lambda$ of the Jacobian loss term. Therefore, we trained multiple JSAEs with different values of this parameter. As we might expect, for small values of $\\lambda$, i.e., little incentive to sparsify the Jacobian, the input and output SAEs perform similarly to standard SAEs (Figure 3 blue lines), including in terms of the variance explained by the reconstructed activation vectors and the increase in the cross-entropy loss when the input activations are replaced by their reconstructions. Unsurprisingly, as $\\lambda$ grows larger and the Jacobian loss term starts to dominate, our evaluation metrics degrade. Interestingly, this degradation happens almost entirely in the output SAE rather than the input SAE; we leave it to future work to investigate this phenomenon further.\nCritically, Figure 3 suggests there is a 'sweet spot' of the $\\lambda$ hyperparameter where the SAE quality metrics remain reasonable, but the Jacobian is much sparser than for standard SAEs. To further investigate this trade-off, we plotted a measure of Jacobian sparsity (the proportion of elements of"}, {"title": "5.2. Performance on re-initialized transformers", "content": "To confirm that JSAEs are extracting information about the complex learned computation, we considered a form of control analysis inspired by Heap et al. (2025). Specifically, we would expect that trained transformers have carefully learned specific, structured computations while randomly initialized transformers do not. Thus, a possible desideratum for tools in mechanistic interpretability is that they ought to work substantially better when analyzing the complex computations in trained LLMs than when applied to LLMs with randomly re-initialized weights. This is precisely what we find. Specifically, we find that the Jacobians for trained networks are always substantially sparser than the corresponding random trained network, and this holds for both traditional SAEs and JSAEs (Figure 6). Further, the relative improvement in sparsity from the traditional SAE to the JSAE is much larger for trained than random LLMs, again indicating that JSAEs are extracting structure that only exists in the trained network. Note that we also see that for traditional SAEs, there is a somewhat more sparse Jacobian for the trained than randomly initialized transformer. This makes sense: we would hope that the traditional SAE basis is somewhat more aligned with the computation (as expressed by a sparse Jacobian) than we would expect by chance. However, it turns out that without a \"helping hand\" from the Jacobian sparsity term, the alignment in a traditional SAE is relatively small. Thus, Jacobian sparsity is a property related to the complex computations LLMs learn during training, which should make it substantially useful for discovering the learned structures of LLMs."}, {"title": "5.3. fs is mostly linear", "content": "Importantly, the Jacobian is a local measure. Thus, strictly speaking, a near-zero element of the Jacobian matrix implies only that a small change to the input SAE latent does not affect the corresponding output SAE latent. It may, however, still be the case that a large change to the input SAE latent would change the output SAE latent. We investigated this question and found that $f_s$ is usually approximately linear in a wide range and is often close to linear. Specifically, of the scalar functions relating individual input SAE latents $s_{x,j}$ to individual output SAE latents $s_{y,i}$, the vast majority are linear (Figure 7b). This is important because, for any linear function, its local slope is completely predictive of its global shape, and therefore, a near-zero Jacobian element implies a near-zero causal relationship. For the scalar functions which are not linear, we frequently observed they have a JumpReLU structure (Erichson et al., 2019). Notably, a JumpReLU is linear in a subset of its input space, so even for these scalar functions the first derivative is still an accurate measure within some range of $s_{x,j}$ values. It is also worth noting that with JSAEs, the proportion of linear functions is noticeably higher than with traditional SAEs, so at least to a certain extent, JSAEs induce additional linearity in the MLP.\nTo confirm these results, we plotted the Jacobian against the change of output SAE latent $s_{y,i}$ as we change the input SAE latent $s_{x,j}$ by subtracting 1 (Figure 7c). We found that 97.7% of the time, $|\\Delta s_{y,i}| \\approx |J_{f,ij}|$. For details see Appendix B."}, {"title": "6. Discussion", "content": "We believe JSAEs are a promising approach for discovering computational sparsity and understanding the reasoning of LLMs. We would also argue that an approach like the one we introduced is in some sense necessary if we want to 'reverse-engineer' or 'decompile' LLMs into readable source code. It is not enough that our variables (e.g., SAE features) are interpretable; they must also be connected in a relatively sparse way. To illustrate this point, imagine a Python function that takes as input 5 arguments and returns a single variable, and compare this to a Python function that takes 32,000 arguments. Naturally, the latter would be nearly impossible to reason about. Discovering computational sparsity thus appears to be a prerequisite for solving interpretability. It is also important that the mechanisms for discovering computational sparsity be fully unsupervised rather than requiring the user to manually specify the task being analyzed. There are existing methods for taking a specific task and finding the circuit responsible for implementing it, but these require the user to specify the task first (e.g. as a small dataset of task-relevant prompts and a metric of success). They are thus 'supervised' in the sense that they need a clear direction from the user. Naturally, it is not feasible to manually iterate over all tasks an LLM may be performing, so a fully unsupervised approach is needed. JSAEs are the first step in this direction.\nNaturally, JSAEs in their current form still have important limitations. They currently only work on MLPs, and for now, they only operate on a single layer at a time rather than discovering circuits throughout the entire model. Our initial implementation also works on GPT-2-style MLPs, while most LLMs from the last few years tend to use GLUs (Dauphin et al., 2017; Shazeer, 2020), though we expect it to be fairly easy to extend our setup to GLUs. Additionally, our current implementation relies on the TopK activation function for efficient batching; TopK SAEs can sometimes encourage high-density features, so it may be desirable to generalize our implementation to work with other activation functions. These are, however, problems that can be addressed relatively straightforwardly in future work, and we would welcome correspondence from researchers interested in addressing them.\nA pessimist may argue that partial derivatives (and, therefore, Jacobians) are merely local measures. A small partial derivative tells you that if you slightly tweak the input latent's"}, {"title": "7. Conclusion", "content": "We introduced Jacobian sparse autoencoders (JSAEs), a new approach for discovering sparse computation in LLMs in a fully unsupervised way. We found that JSAEs induce sparsity in the Jacobian matrix of the function that represents an MLP layer in the sparse basis found by JSAEs, with minimal degradation in the reconstruction quality and downstream performance of the underlying model and no degradation in the interpretability of latents. We also found that Jacobian sparsity is substantially greater in pre-trained LLMs than in randomly initialized ones suggesting that Jacobian sparsity is indeed a proxy for learned computational structure. Lastly, we found that Jacobians are a highly accurate measure of computational sparsity due to the fact that the MLP in the JSAE basis consists mostly of linear functions relating input to output JSAE latents."}, {"title": "A. Efficiently computing the Jacobian", "content": "A simple form for the Jacobian of the function $f_s = e_y \\odot f \\odot d_x \\odot T_k$, which describes the action of an MLP layer $f$ in the sparse input and output bases, follows from applying the chain rule. Note that here, the subscripts $f_s, e_y$, etc. denote the function in question rather than vector or matrix indices. For the GPT-2-style MLPs that we study, the components of $f_s$ are:\n1.  TopK. This function takes sparse latents $s_x$ and outputs sparse latents $\\hat{s}_x$. Importantly, $\\hat{s}_x = s_x$. This step makes the backward pass of the Jacobian computation more efficient but does not affect the forward pass.\n$\\hat{s}_x = T_k(s_x)$ (6)\n2.  Input SAE Decoder. This function takes sparse latents $s_x$ and outputs dense MLP inputs $x$:\n$x = d_x(s_x) = W_{\\text{dec}}s_x + b_{\\text{dec}}$ (7)\n3.  MLP. This function takes dense inputs $x$ and outputs dense outputs $y$:\n$z = W_1x + b_1$, $y = W_2\\sigma_{\\text{MLP}}(z) + b_2$ (8)\nwhere $\\sigma_{\\text{MLP}}$ is the activation function of the MLP (e.g., GeLU in the case of Pythia models).\n4.  Output SAE Encoder. This function takes dense outputs $y$ and outputs sparse latents $s_y$:\n$s_y = e_y(y) = T_k(W_{\\text{enc}}y + b_{\\text{enc}})$ (9)\nThe Jacobian $J_{f_s} \\in \\mathbb{R}^{n_y \\times n_x}$ for a single input activation vector has the following elements, in index notation:\n$J_{f_s,i,j} = \\frac{\\partial s_{y,i}}{\\partial s_{x,j}} = \\sum_{klmn} \\frac{\\partial s_{y,i}}{\\partial y_k} \\frac{\\partial y_k}{\\partial z_l} \\frac{\\partial z_l}{\\partial x_m} \\frac{\\partial x_m}{\\partial \\hat{s}_{x,n}} \\frac{\\partial \\hat{s}_{x,n}}{\\partial s_{x,j}}$ (10)\nWe compute each term like so:\n1.  Output SAE Encoder derivative:\n$\\frac{\\partial s_{y,i}}{\\partial y_k} = \\left(\\sum_{l} \\frac{\\partial}{\\partial y_k} W_{\\text{enc}}y + b_{\\text{enc},i}\\right)W_{\\text{enc}_{y,ik}} = \\begin{cases} W_{\\text{enc}_{y,ik}} & \\text{if } i \\in K_2 \\\\ 0 & \\text{otherwise} \\end{cases}$ (11)\nwhere $K_2$ is the set of indices selected by the TopK activation function $T_k$ of the second (output) SAE. Importantly, the subscript $k$ does not indicate the $k$-th element of $T_k$, whereas it does indicate the $k$-th column of $W_{\\text{enc}_{y,ik}}$.\n2.  MLP derivatives:\n$\\frac{\\partial y_k}{\\partial z_l} = W_{2,kl} \\sigma_{\\text{MLP}}'(z_l)$, $\\frac{\\partial z_l}{\\partial x_m} = W_{1,lm}$ (12)\n3.  Input SAE Decoder derivative:\n$\\frac{\\partial x_m}{\\partial \\hat{s}_{x,n}} = W_{\\text{dec}_{x,mn}}$ (13)\n4.  TopK derivative:\n$\\frac{\\partial \\hat{s}_{x,n}}{\\partial s_{x,j}} = \\begin{cases} 1 & \\text{if } j \\in K_1 \\\\ 0 & \\text{otherwise} \\end{cases}$ (14)\nwhere $K_1$ is the set of indices (corresponding to SAE latents) that were selected by the TopK activation function $T_k$ of the first (input) SAE, which we explicitly included in the definition of $f_s$ above."}, {"title": "B. fs is approximately linear", "content": "Consider the scalar function $f_{s,(i,j)}|_{s_x}: \\mathbb{R} \\rightarrow \\mathbb{R}$ which takes as input the $j$-th latent activation of the first SAE (i.e. $s_{x,j}$) and returns as output the $i$-th latent activation of the second SAE (i.e., $s_{y,i}$), while keeping the other elements of the input vector fixed at the same values as $s_x$. In other words, this function captures the relationship between the $j$-th input SAE latent and the $i$-th output SAE latent in the context of $s_x$. Geometrically, we start off at the point $s_x$, and we move from it through the input spaces in parallel to the $j$-th basis vector, and then we observe how the output of $f_s$ projects onto the $i$-th basis vector. Formally,\n$f_{s,(i,j)}|_{s_x}(x) = f_s\\left(\\psi(s_x, i, x)\\right)_j$ (17)\n$\\psi(s_x, i, x)_k = \\begin{cases} s_{x,j} & \\text{if } i = k \\\\ x & \\text{otherwise} \\end{cases}$ (18)"}]}