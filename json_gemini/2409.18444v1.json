{"title": "Cost-Aware Dynamic Cloud Workflow Scheduling using Self-Attention and Evolutionary Reinforcement Learning", "authors": ["Ya Shen", "Gang Chen", "Hui Ma", "Mengjie Zhang"], "abstract": "The Cost-aware Dynamic Multi-Workflow Scheduling (CDMWS) in the cloud is a kind of cloud workflow management problem, which aims to assign virtual machine (VM) instances to execute tasks in workflows so as to minimize the total costs, including both the penalties for violating Service Level Agreement (SLA) and the VM rental fees. Powered by deep neural networks, Reinforcement Learning (RL) methods can construct effective scheduling policies for solving CDMWS problems. Traditional policy networks in RL often use basic feedforward architectures to separately determine the suitability of assigning any VM instances, without considering all VMs simultaneously to learn their global information. This paper proposes a novel self-attention policy network for cloud workflow scheduling (SPN-CWS) that captures global information from all VMs. We also develop an Evolution Strategy-based RL (ERL) system to train SPN-CWS reliably and effectively. The trained SPN-CWS can effectively process all candidate VM instances simultaneously to identify the most suitable VM instance to execute every workflow task. Comprehensive experiments show that our method can noticeably outperform several state-of-the-art algorithms on multiple benchmark CDMWS problems.", "sections": [{"title": "1 Introduction", "content": "Everyday numerous computationally intensive and resource-demanding applications (e.g., weather forecasting, tsunami detection) will be submitted as workflows to a cloud broker for execution [5,7,14]. Each workflow comprises a collection of interdependent tasks that must be efficiently processed by using multiple leased virtual machine (VM) instances provided by major cloud providers, such as Amazon EC2 [2,9]. As a primary challenge, the cloud broker must quickly determine the type and number of VMs required to execute these tasks at the lowest possible cost. The broker then leases VM instances to process these workflows. This challenge is widely known as the workflow scheduling (WS) problem To tackle this research issue, the aim of this paper is to propose a novel Self-attention Policy Network for Cloud Workflow Scheduling (SPN-CWS) to solve the CDMWS problem. In particular, for a given task to be scheduled, information regarding all candidate VMs are fed together into SPN-CWS, which then processes the global information using the self-attention mechanism. Sub-"}, {"title": "2 Related Work", "content": "Research on dynamic WS is gaining increasing attention, driven by their substantial practical importance in diverse applications [9,21,23]. Dynamic WS presents a significant challenge, as it requires making real-time scheduling decisions tailored to the current cloud environment where no prior knowledge exists regarding workflows arriving in the future. While previous studies of dynamic WS [4,13] primarily focused on minimizing VM rental costs, a critical aspect often neglected was the trade-off between VM rental expenses and potential SLA violation penalties [7,20]. In fact, it may prove economically prudent to incur SLA penalties since meeting SLA deadlines often demand for leasing fast but expensive VMs [9,21,23]. Drawing inspiration from several recent studies [8,9,23], this paper embarks on an investigation into the CDMWS problem, aiming to learn scheduling policy to jointly optimize the VM rental fees and the SLA violation penalties. There are three main categories of algorithms for learning scheduling policies for dynamic WS in cloud: manual approach, GPHH and RL. Manual approach [2,7,20] relies on domain experts to design scheduling policies by leveraging their problem-specific knowledge and empirical experiences. However, the designed policies are typically applied to simplified and static problems [9,16]. GPHH can automatically design WS policies that are highly adpative to dynamic WS problems. For example, [23] introduced a novel dual-tree policy"}, {"title": "3 Problem Definition", "content": "In this section, we formally describe the Cost-aware Dynamic Multi-Workflow Scheduling problem (CDMWS) as illustrated in Fig. 2. This problem is centered around a broker that is responsible for dynamically scheduling workflow execution using leased VMs in cloud to minimize the total cost, including both the VM Rental fees and the SLA violation penalties.\nWorkflow model: In CDMWS, we assume that a sequence of workflows are dynamically submitted by users during T: W(T) = {wi|i = 1, 2, ...}. Each workflow w\u2081 contains a set of tasks connected by a Directed Acyclic Graph (DAG). A workflow w can be specifically formulated as follows:\nw = [DAG(w), AT(w), DL(w), \u03b2(w)|w \u2208 W(T)]\n(1)\nwhere \u03b2(w) is the user specified SLA penalty rate [25] (see Eq. (9)). AT(w) is the arrival time of w, and DL(w) is the user specified SLA deadline (see Eq. (10)). DAG(w) provides a directed acyclic graph of tasks, representing all the tasks to be executed as part of w as nodes and showing how these tasks are connected together through the directed edges in DAG(w). DAG(w) can be formulated as below:\nDAG(w) = [Task(w), Edge(w)]\n(2)"}, {"title": "Cloud environment", "content": "the cloud environment contains a set of VMs with varied VM types. In this study, the types of VM are limited, but the quantity available for each type is unlimited, meaning that the broker can lease an arbitrary number of VM instances of any type on demand. A VM instance denoted as v in the cloud can be characterized as follows:\nv = [Type(v), Capacity(v), Price(v)]\n(3)\nwhere Type(v) indicates the VM type; Capacity(v) gives the computation capability per time unit of v [7] ; Price(v) is the rental fee per hour incurred as a result of using/leasing v. According to several existing works [7,9,23], the rental time for a fraction of an hour is charged as one hour.\nExecution time: Let the VM instance chosen by policy to execute task t \u2208 Task(w) of workflow w be denoted as Ut,w,\u03c0\u00b7 The required execution time of t, i.e., EXT(t, w, \u03c0), is determined as:\nEXT(t, \u03c9, \u03c0) = \\frac{Size(t)}{Capacity(vt,w,\u03c0)}\n(4)\nWe further denote AVM(vt, w, \u03c0) as the set of all candidate VM instances that can be used to execute task t \u2208 Task(w) of workflow w, including all the"}, {"title": "VM rental fees", "content": "CDMWS focuses on processing multiple dynamically arriving workflows, each consisting of a collection of tasks to be executed on multiple VMs. When all workflows are completed, the total rental fees of all leased VMs will be calculated. We use T to represent the full operation period associated with a CDMWS problem. This time period starts from time ts and ends at time te. The duration of T depends on the scheduling policy used to execute all workflow tasks. During T, each VM instance v is used for a certain time period, as defined below:\nRP(\u03c5, \u03c0, \u03a4) = [ts(v, \u03c0,T), te(\u03c5, \u03c0, \u03a4)]\n(7)\nwhere ts (\u03c5, \u03c0, \u03a4) > ts is the start time for using v, and te(v, k, \u03c0,T) < te is the time at which v is no longer used. Thus, the total rental fees of all VMs under the scheduling policy during T can be calculated as follows:\nVMFee(\u03c0, 1) = \\sum_{v\u2208Set(\u03c0,\u03a4)} (Price(v) \u00d7 \\lceil \\frac{te(\u03c5, \u03c0, \u03a4) \u2013 ts (\u03c5,\u03c0,\u03a4)}{3600} \\rceil)\n(8)\nwhere Set(\u03c0,T) refers to the set of all VM instances leased to execute workflow tasks during T. The ceiling function in Eq. (8) converts the time period that a VM instance v is used to its corresponding renting time in integer hours.\nSLA penalty: Apart from VM rental fees, the SLA penalty presents a major source of cost to be minimized in CDMWS. According to [9,19,23], the SLA penalty of a workflow w can be calculated by the following:\nSLAPenalty (\u03c9, \u03c0) = \u03b2(w) \u00d7 max{0, [WCT(\u03c9, \u03c0) \u2013 DL(w)]}\n(9)\nIn Eq. (9), the SLA deadline specified by users, i.e., DL(w), is determined as:\nDL(w) = AT(w) + y \u00d7 MinMakespan(w)\n(10)\nwhere MinMakespan(w) is the theoretical shortest time duration required for processing w, achievable by using the fastest VM to process all tasks of workflow"}, {"title": "TotalCost", "content": "without any delay. y is the relaxation coefficient. Increasing y results in more relaxed deadline for w.\nIn CDMWS, we aim to find an effective (optimal) scheduling policy to minimize the total cost, as formulated below:\narg min TotalCost(\u03c0) = arg min \\left \\{ VMFee(\u03c0,T) + \\sum_{wEW(T)} SLAPenalty (W, \u03c0) \\right \\}\n(11)"}, {"title": "4 Self-Attention Policy Network for Cloud Workflow Scheduling (SPN-CWS)", "content": ""}, {"title": "4.1 State information", "content": "The scheduling policy plays a critical role for CDMWS. Whenever a task (i.e., rt) of any workflow (i.e., wrt) becomes ready for execution at time t, \u03c0is utilized to process the state information obtained from the CDMWS problem as its input, and then produces a VM selected to execute rt as its action output. Therefore, we first introduce the state representation in association with the CDMWS problem being solved, which will serve as the input of SPN-CWS.\nIn line with [9,23], we design the state representation by considering both the task-info of the ready task rt and the VM-info of all the VMs in AVM(v|rt, wrt, \u03c0) that can be utilized for executing rt. We specifically list all the features employed to build the state input with respect to task-info and VM-info in Table 1. Whenever there is a task rt ready to be processed, the task-info of rt and the VM-info of all VM instances in AVM (v|rt, wrt, \u03c0) jointly form the representation of the current state as the input of SPN-CWS."}, {"title": "4.2 Architecture Design of SPN-CWS", "content": "To process global information across all VMs in AVM(v|rt, wrt, \u03c0) for effective scheduling of the ready task rt, we design a novel SPN-CWS deep model for the scheduling policy \u03c0 in CDMWS, as shown in Fig. 3. Specifically, we introduce the time-tested Multi-Head Self-Attention mechanism (MHSA) [17], as highlighted in the Global information learning unit of Fig. 3, to enhance the capability for policy to process global VM information effectively and scalably (see the detailed experiments in Section 6.2 for evidence of this). SPN-CWS mainly consists of five key units as described below.\nGlobal information learning: This unit aims to learn global information among VMs. Firstly, we combine the VM-info of all VMs in AVM(v|rt, Wrt, \u03c0) to construct the VM-INFO indicated in Fig. 3. Then, we employ a linear layer to map VM-INFO from the original Rn\u00d7m space to a high-dimensional Rn\u00d7M space (M > m and n = |AVM(v|rt, wrt, \u03c0)|, i.e., the total number of VM instances in AVM(v|rt, wrt, \u03c0)). Then, the Transformer layer processes the projected VM features to learn the relationships among all VMs. In more details, first we use MHSA to compute a weighted sum of all VM features, where the weights are determined by the similarity between every pair of VMs. The weighted features are further processed by Add & Norm and Feed-Forward Networks (FFN) to learn more complex feature representations. MHSA allows features of each VM instance to attend to features of other VM instances, effectively capturing inter-VM relationships and allowing SPN-CWS to focus on the most relevant VM instances.\nTask feature enhancement: This unit processes and transforms task features. In this unit, a FFN is used to map task-info from the R1\u00d79 space to a high-dimensional R\u00b9\u00d7Q space (Q > q) to build high-level feature representations"}, {"title": "Feature concatenation", "content": "This unit concatenates task features of the ready task with the features from each VM instance. In order to perform priority mapping on every VM instance, in this unit, the processed task-info is concatenated separately with the VM-INFO associated with each candidate VM instance.\nPriority mapping: This unit maps concatenated task and VM features to priority values that dictate the importance of using any specific VM instance to execute the ready task. For example, using the VM-INFO features with respect to v\u2081 and the features in the task-info, the priority value of VM instance v\u2081 is determined through a FFN included in this unit, as shown in Fig. 3.\nVM Selection: This unit selects the VM instance with the highest priority value to execute the ready task rt."}, {"title": "5 Training SPN-CWS via ERL", "content": "The performance of gradient-based RL can be highly sensitive to its hyperparameter settings as well as the design of the reward function. ERL can alleviate these issues and achieve robust learning performance [1,11,15]. Therefore, in this study, we develop an ERL system based on our cloud simulator to train SPN-CWS designed above for CDMWS, which is denoted as asc. The pseudo-code of the training algorithm is presented in Algorithm 1. Specifically, the training process of sc using ERL is described as follows:\n1. Let e denote the current policy parameter of sc. During each generation (i.e., each iteration of the ERL), ERL samples a population of N individuals, where each individual (0z|i = 1,2,...N) is sampled from an isotropic multivariate Gaussian distribution with as its mean vector and o\u00b2I as its co-variance matrix. Hence, \u03b8; ~ N(\u03b8, \u03c32\u0399), which is equivalent to \u03b8\u2081 = 0+\u03c3\u03b5\u03af with ei ~ N(0, I). 0; indicates the parameters of \u03c0\u03af.\n2. The fitness value of 0i (i.e., F(0)) equals the total cost (as described in Section 3) achieved by using \u03c0i to solve any CDMWS problem used for training. Since ERL aims to learn the policy parameters that minimize the total cost, we define the fitness function as follows:\nF(0) = F(\u03b8 + \u03c3\u03b5\u2081) = \u2212TotalCost(\u03c0\u03b9)\n(12)\n3. With Eq. (12) as the objective function, ERL updates 6 to maximize the expected value of the objective function (i.e., \u0395\u03b8\u2081~N (6,021) F(0)), thereby minimizing the total costs. Specifically, ERL updates ) using policy gradients estimated below [15]:\n\u2207\u00f4\u0388\u03b8\u2081~N(0,021) F(0i) = \u2207\u011dE\u20ac\u00a1~N(0,1)F(0 + \u03c3\u03b5\u03b9)\n= \\frac{1}{\u03c3} E\u03b5;~N(0,1) {F(\u03b8 + \u03c3\u03b5\u03b9)\u03b5i}\n(13)"}, {"title": "6 Experiments", "content": ""}, {"title": "6.1 Simulation Environment Configuration", "content": "In this section, we present the CDMWS simulation environment for training SPN-CWS using ERL as well as all the competing algorithms. The simulation environment is specifically described as follows.\nVM types and workflow patterns: We configure the VMs used in CDMWS based on Amazon EC2\u00b9. In line with existing studies [9,22,23], we use six types of VMs with their respective configurations summarized in Table 2. Following [9,22], each type of VM can be leased on demand with unlimited number of instances. As summarized in Table 3, the workflows experimented consist of four popular patterns (i.e., CyberShake, Montage, Inspiral and SIPHT) that are commonly used in recent studies [9,21,22]. Workflows of the same pattern become more complex and difficult to solve as the number of tasks in the workflows increases. Based on the workflow size (i.e., number of workflow tasks), workflows are grouped into three categories: Small, Medium, and Large.\nCDMWS problem instance: Following [9,22], each CDMWS problem in-stance consists of 30 randomly sampled workflows corresponding to all the four"}, {"title": "CDMWS problem scenarios", "content": "Small-scenario CDMWS problem instances are generated from small workflow set. Meanwhile, medium- and large-scenario CDMWS problem instances are generated from Medium and Large workflow sets, respectively. For each generation of Algorithm 1, we sample a small-scenario CDMWS problem instance for fitness evaluation. During testing, we use 30 small-scenario, 30 medium-scenario, and 30 large-scenario CDMWS instances to jointly evaluate the performance of the trained SPN-CWS. Notably, the medium- and large-scenario CDMWS problems instances are not used during training for computation efficiency reasons. They are only employed to assess the generalization capability of SPN-CWS during testing. In each test scenario, the scheduling policy's performance is calculated based on the average of 30 CDMWS problem instances.\nBaseline algorithms: Four competing methods are included in our experiments. Among them, ProLis [20] and GRP-HEFT [7] are two popular heuristic methods designed manually by domain experts and are frequently studied in existing works [5,21,22]. DSGP [6] is a GPHH-based approach that learns a heuristic rule as scheduling policy through evolutionary search in the hyper-heuristic space. ES-RL [9] adopts an RL-based algorithm designed by OpenAI\u00b3 to train a scheduling policy modeled as a feedforward neural network.\nParameter settings: The parameter settings of all competing algorithms strictly follow their original papers. The N, Gen, a, and o of Algorithm 1 are set to 40, 3000, 0.01, and 0.05, respectively, which are identical to [9] for a fair comparison. According to Table 1, m and q of Fig. 3 are 4 and 3, respectively, while M and Q are set to 16 in this study. In SPN-CWS, the size of feedforward hidden layers in the Global information learning, Task feature enhancement, and Priority mapping units are set to 64, 32, and 32, respectively, with ReLU as the activation function."}, {"title": "6.2 Main Results", "content": "The performance of all competing algorithms are summarized in Table 4, which clearly indicates that SPN-CWS significantly outperforms ProLis and GRP-HEFT across all scenarios, confirming the importance of designing scheduling policies automatically. Furthermore, compared to DSGP and ES-RL, thanks to its capability of processing global information among all candidate VMs, SPN-CWS achieved significantly better overall performance. Particularly, on majority of medium and large scenarios, SPN-CWS significantly outperforms DSGP and ES-RL, with the scenario (2.00, M) as the only exception. This shows that SPN-CWS, while being trained on small problems alone, can achieve better generalization performance on large problems.\nMeanwhile, we have bolded the best performance results in Table 4, which have been thoroughly verified through the Wilcoxon ranked sum test. The corresponding p-values associated with the bolded results are consistently less than 0.05. For example, on the scenario (1.50, M), SPN-CWS can manage to reduce the total costs by 5.29% and 56.9% respectively, compared to the total costs realized by DSGP and ES-RL.\nMoreover, Table 4 reveals that, upon increasing y, the total costs achieved by SPN-CWS consistently exhibit a downward trend across all test scenarios. This suggests that a more relaxed SLA deadline coefficient encourages SPN-CWS to utilize cheaper VM instances, thereby effectively lowering the total costs."}, {"title": "6.3 Convergence Analysis", "content": "Since both ES-RL and SPN-CWS are RL-based algorithms, we compare the convergence behaviors of ES-RL and SPN-CWS during training and testing in Fig. 5. As demonstrated in this figure, SPN-CWS achieves lower total costs compared to ES-RL during training and testing. Notably, despite of using complex network architectures, SPN-CWS achieved consistent convergence speed as that of ES-RL. It also showed faster convergence speed than ES-RL on the testing problems. Furthermore, compared to ES-RL, SPN-CWS enjoys clearly smaller confidence intervals during both training and testing, suggesting that the trained SPN-CWS can perform more consistently and reliably."}, {"title": "7 Conclusions", "content": "This paper presented a novel Self-Attention Policy Network (SPN-CWS) for cloud workflow scheduling, capable of capturing the global information of all VMs. We also developed an ERL system based on our cloud simulator to train SPN-CWS reliably and efficiently. To schedule the execution of any workflow task, the trained SPN-CWS processes all candidate VMs as its input and selects the most suitable VM to execute the task. Experimental results showed that our method can effectively learn scheduling policies for CDMWS and significantly outperform several state-of-the-art algorithms. In addition, the designed SPN-CWS demonstrated good convergence speed and stability on both training and testing problems.\nIn the future, we plan to investigate the implementation of online reinforcement learning techniques for SPN-CWS. This approach can potentially enhance the system's adaptability to dynamic changes, such as evolving workflow patterns over time."}]}