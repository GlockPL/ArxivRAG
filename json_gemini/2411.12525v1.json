{"title": "Rethinking Top Probability from Multi-view for Distracted Driver Behaviour Localization", "authors": ["Quang Vinh Nguyen", "Vo Hoang Thanh Son", "Chau Truong Vinh Hoang", "Duc Duy Nguyen", "Nhat Huy Nguyen Minh", "Soo-Hyung Kim"], "abstract": "Naturalistic driving action localization task aims to recognize and comprehend human behaviors and actions from video data captured during real-world driving scenarios. Previous studies have shown great action localization performance by applying a recognition model followed by probability-based post-processing. Nevertheless, the probabilities provided by the recognition model frequently contain confused information causing challenge for post-processing. In this work, we adopt an action recognition model based on self-supervise learning to detect distracted activities and give potential action probabilities. Subsequently, a constraint ensemble strategy takes advantages of multi-camera views to provide robust predictions. Finally, we introduce a conditional post-processing operation to locate distracted behaviours and action temporal boundaries precisely. Experimenting on test set A2, our method obtains the sixth position on the public leaderboard of track 3 of the 2024 AI City Challenge.", "sections": [{"title": "1. Introduction", "content": "Distracted driving is defined as any circumstance where the driver diverts attention away from safe driving activities. In the United States, over 3,500 lives are lost annually due to accidents caused by distracted driving. Research in intelligent transportation systems and distracted driving has gained significant attention from scholars worldwide. This interest is fueled by the potential of naturalistic driving videos to capture real-time driving behavior and the capability of deep learning to analyze potential risk factors. The AI City Challenge 2024 aims to advance research in this field by hosting a naturalistic driving action recognition challenge. The given challenge focuses on detecting distracted driving behaviors using synthetic naturalistic data collected from three camera locations inside the vehicle. This challenge involves analyzing synchronized video recordings from drivers engaged in various distracted driving activities. These activities are classified into different actions, such as using a phone, eating, and reaching into the backseat, each of which can potentially lead to accidents.\nPrevious studies have demonstrated the effectiveness in distracted driving detection, typically dividing the task into two main stages: activity recognition and temporal action localization. However, several challenges remain: (1) The dataset is limited to 16 behavior categories, leading to an insufficient diversity of samples within each category. (2) The models must discern various actions from different perspectives within untrimmed videos, facing difficulties in distinguishing subtle variations within the same class and detecting minor discrepancies between certain classes. (3) The inclusion of the appearance block constrains the model's ability to discern differences between certain classes. (4) Previous solutions rely heavily on the classification model's confidence, which can result in misclassifications when the highest and second-highest classes have similar probabilities.\nTherefore, in this paper, we aim to contribute to the literature in the following manners: First, we inherit an action classification model in video based self-supervised learning to detect robust distracted actions from the input video. Next, we apply a constraint ensemble strategy to take advantage of the power of each camera view. In the final, conditional post-processing steps consider contexts from top 1 and top 2 confidence ranking to locate distracted actions and temporal boundaries accurately."}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Action Recognition", "content": "Action recognition is a crucial task in the field of video understanding. Over the years, there have been numerous studies and extensive research conducted in this area. The main goal of the action recognition is to classify a trimmed video into specific action classes using end-to-end deep learning methods. There have been significant updates in architecture design, ranging from 2D-based CNN models and 3D-based CNN models to Transformer-based models.\n2D-based action recognition methods first implement a CNN model to extract spatial features for each frame in the video. The sequence models are employed to fuse these features with the aim of capturing temporal information. 3D-CNN attempts to process spatial-temporal information directly by using 3D input tensors, where 2 dimensions represent space and 1 dimension represents time. The success of Transformer in image-related and sequential tasks and has motivated the exploration of its potential in video recognition, have been successfully developed to use Transformer in the architecture. Recent works also take advantages of large video foundation pre-training models to improve performance. Masking with high ratio or scaling transformer model by applying self-supervised learning, have shown great potential in extracting robust video representation"}, {"title": "2.2. Temporal Action Localization", "content": "Temporal action localization is the task of automatically identifying the time duration during which an action occurs within an untrimmed video and determining its corresponding action category. The conventional two-stage method involves proposing action segments initially and subsequently classifying these proposals into their respective action categories. However, a major drawback of this method is that the boundaries of action instances remain fixed during the classification process. As a result, while the method can identify time intervals likely to contain actions, it lacks the ability to precisely determine the exact start and end times of the actions.\nIn contrast, one-stage methods have garnered significant attention by integrating the localization and classification tasks within a single network. This approach eliminates the fixed boundaries issue and offers a more streamlined solution. Previous works have seen the adoption of hierarchical architectures based on CNN. Recent studies extract a video representation with a Transformer-based encoder."}, {"title": "3. Method", "content": "As indicated in Fig. 1, our distracted driver behaviour recognition system consists of three main novel components: an action recognition model, an ensemble strategy, and conditional post-processing. The first is an action recognition model which is self-supervised learning, recognize distracted driver behaviors from input short videos. The second is an ensemble strategy being responsible for integrating multi-view predictions. Given recognition probabilities, conditional post-processing considers diverse contexts to smooth out detected activities and localize the temporal boundary accurately. Detailed descriptions of each component are presented in the following subsections."}, {"title": "3.1. Action Recognition", "content": "Recent researches have demonstrated that self-supervised learning (SSL) can provide more robust and general features, while reducing the amount of data required for an equivalent supervision-based pre-training. In the context of video understanding, self-supervised learning techniques seek to take advantage of the temporal coherence and spatial correlations seen in video sequences. These approaches are particularly suitable for scenarios where labeled data is scarce or expensive to obtain as the distracted driver behavior dataset. Inspired by the successful study of Masking Modeling in the text and picture domain. VideoMAE employs Masked Autoencoders, a variation on traditional Autoencoders where certain parts of the input data are masked out during training, encouraging the model to learn useful representations that capture the underlying structure of the data leading to promising performance in a variety of video understanding tasks. Our system inherits this structure to classify distracted action from naturalistic driving videos. Specifically, input videos with FPS 30 are trimmed into a series of short videos containing 64 frames. The model achieves short videos as input to give the probability for each class in the output."}, {"title": "3.2. Multi-view Ensemble Strategy", "content": "The distracted driver action is divided into sixteen distracted actions and three views of the camera mounted in the car: dashboard, rearview and rightside. Each of these views has significance in different contexts. Dashboard view directly facing driver contributes clearly to actions: \"phone call by right hand\", \"drink\", \"eating\u201d or activities involving to the movement of body-head such as \"talk with passenger\", \"pick up from floor\". Rear view gives a broader space view inside the car, and is useful for identifying various actions: \"phone call by right hand or left hand\", \"reaching behind\" or \"hand on head\". While the right side view shows a different view, from the right side of the driver, this view is helpful for hand movements: \"control the panel\", \"text by hand\" or \"pick from floor (Passenger)\". In addition, several specific classes: \"talk with passenger\", \"pick from floor (Driver)\" can be integrated by all views to comprehend the overall context of distracted driving. Therefore, in order to enhance recognition performance, we suggest an ensemble strategy based on multi-view. The specifics of ensemble strategy are displayed in Fig. 2."}, {"title": "3.3. Conditional Post-Processing", "content": "The action recognition model classifies short videos which are trimmed from input video to give a series of probability. Output probability is an array of prediction vectors with the length of 16 corresponding to a number of classes. Elements with the highest value in vectors refer to predicted classes. And elements with second highest value normally express potential classes which are the second most trustworthy after the highest ones. Our post-processing strategy leverages top 1 and top 2 of output probability to locate the actions and time boundary more accurately. This process consists of three main steps: Conditional Merging, Conditional Decision and Missing Labels Restoring.\nConditional Merging. The first operation refers to conditional merging, which is depicted in Fig. 3. Instead of merging closer actions normally, this component considers the context of one certain class and neighbor classes by top 1 and top 2 confidence ranking to merge potential candidates and remove noise classes. To explain symbols in Fig. 3, \"second\" represents the time boundary for each action, the values in the boxes refer to the probabilities for each type. Top 1 is the class with the highest probability score, while top 2 represents the class with the second highest probability.\nConditional Decision. Fig. 4 describes the conditional decision operation which selects a reliable time segmentation from different segments of the same classes. Given several different segments of the same class, for example, there are two segments of class 7 \"reaching behind\" in Fig. 4. The decision module relies on probabilities from top 1 and top 2 to filter a most trustworthy segment.\nMissing Labels Restoring. After the two mentioned above steps, it still has some classes that are missing or not detected by the top 1 prediction. It means that if we just use top 1 probability for output prediction, the system could not localize distracted actions sufficiently. The restoring module shown in Fig. 5 finds these classes to reproduce the final prediction with enough 16 classes."}, {"title": "4. Experiments", "content": ""}, {"title": "4.1. Dataset", "content": "The distracted driver behavior dataset provides a comprehensive collection of driving videos capturing the actions of 99 individual drivers over a total of 90 hours. Each driver is recorded performing a series of 16 different distracting activities randomly, with the order of these activities also randomized within each video. To ensure a holistic view of the driving scenario, the dataset employs three cameras simultaneously recording from different angles within the car. Notably, each driver undergoes two rounds of data collection: one without any form of distraction and another with a predetermined distractor, such as sunglasses or a hat. This design allows for a thorough examination of driver behavior under varying levels of distraction, offering valuable insights into the impact of external factors on driving performance. The videos from the 2024 AI City Challenge's Track 3 on Naturalistic Driving Action Recognition are separated into two datasets: \"A1\" for training, \"A2\" for testing with the training dataset \"A1\" containing the ground truth labels for the start time, end time, and types of distracted actions."}, {"title": "4.2. Evaluation Matric", "content": "Action Recognition. Action classification involves the task of assigning a label or category to a video based on its content. The accuracy score is calculated by comparing the predicted class labels with the ground truth labels for all videos. A higher accuracy score indicates better performance of the video classification model in correctly predicting the class labels of videos. The accuracy is defined as:\nAccuracy = $\\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}}$X100% (1)\n\"Number of Correct Predictions\" is the number of instances that are correctly classified by the classifier. \"Total Number of Predictions\" is the total number of instances in the dataset.\nTemporal Action Localization. For temporal action localization, activity overlap measure (os) quantifies the degree of overlap between the predicted temporal segment and the ground truth annotation for a particular action or activity within a video sequence.\nOS = $\\frac{\\text{Intersection}}{\\text{Union}}$ (2)"}, {"title": "4.3. Implement Detail", "content": "The methodology employed relies on the PyTorch framework, a publicly available toolbox widely used in machine learning research. All experimentation was conducted on a high-performance workstation equipped with two RTX 3090 graphics card boasting 48GB of memory. For the video classification task, the network architecture utilized is consistent with the model described in reference [22]. In particular, we use a standard Vision Transformer (ViT) model as the foundation. Each input video trimmed with stride 30 frames comprises 64 frames, sampled 16 frames evenly spaced per video. Training process is conducted with a learning rate of 2 x 10-3 over 20 epochs for each camera view."}, {"title": "4.4. Results", "content": "Action Recognition. The training dataset Al is divided into 5 folds. We validate each of the folds in all three views of the camera. Results in Tab. 1 illustrate the effect of each of views on different classes. As can be seen, the right side view often gives excellent accuracy in several classes such as class 8 (control the panel), class 10 (pick up from floor of passenger), or class 5,6 (text) because this view is expert in these classes more than rear view and dashboard view. Besides, the dashboard view contributes greatly to class 1 (drink), class 4(eat), or class 13(yawning) and often is the best. In addition, the rear view strongly affects performance of class 3 (phone call by left hand), and class 14(hand on head). Our ensemble strategy improves and surpasses situations with only a single view. Results in each of the folds fluctuate and depend on the challenge of the validation set.\nTemporal Action Localization. The proposed method is trained on the A1 dataset provided by the competition, and tested on public test dataset A2 to evaluate temporal action localization performance. As indicated in Tab. 2, our approach ranks 6th on the leaderboard with a 0.76 os score, outperforms 7th by almost 8% score and is far ahead of competitors beneath. Besides, our solution is not much lower than the top-rank methods. This proves the effectiveness and potential of introduced method in the distracted driver behaviour recognition challenge. Fig. 6 depicts post-processing operation in detail, Horizontal axis denotes for time variable (second), and vertical axis refers to classes (from 0 to 15). Numbers on top of bars in the Fig. 6 express corresponding classes. The top 1 chart shows prediction given by highest confidence probability, while the top 2 illustrates second reliable classes. As can be seen in the top 1 chart, the predicted labels attach with many noisy labels causing confusion to action recognition and localization. The proposed post-process operation considers the top 1, top 2 probabilities, applies conditional merging, conditional decision and missing label restoring to smooth and localize accurately distracted action prediction. Fig. 6 indicates that our final result is seamless and superior to the top 1 prediction. This demonstrates that our post-processing strategy help model make decisions accurately and effectively localize temporal boundaries."}, {"title": "5. Conclusion", "content": "In this work, we have suggested a conditional recognition system for the distracted driver behaviour localization task. First, our method uses a pre-trained action recognition model that was trained by self-supervised learning to identify distracted activities in video input. After that, a multi-view ensemble strategy is adopted to leverage the advantages of each camera view. Given output probabilities, we post-processing by conditional merging, conditional decision, and missing labels restoring operation to recognize the distracted actions and locate time boundary accurately. Consequently, we achieved the sixth rank score in test set \"A2\", surpassing methods ranked lower while remaining very close to the top ranking."}]}