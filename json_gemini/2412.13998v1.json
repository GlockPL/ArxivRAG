{"title": "Few-shot Steerable Alignment: Adapting Rewards and LLM Policies with Neural Processes", "authors": ["Katarzyna Kobalczyk", "Claudio Fanconi", "Hao Sun", "Mihaela van der Schaar"], "abstract": "As large language models (LLMs) become increasingly embedded in everyday applications, ensuring their alignment with the diverse preferences of individual users has become a critical challenge. Currently deployed approaches typically assume homogeneous user objectives and rely on single-objective fine-tuning. However, human preferences are inherently heterogeneous, influenced by various unobservable factors, leading to conflicting signals in preference data. Existing solutions addressing this diversity often require costly datasets labelled for specific objectives and involve training multiple reward models or LLM policies, which is computationally expensive and impractical. In this work, we present a novel framework for few-shot steerable alignment, where users' underlying preferences are inferred from a small sample of their choices. To achieve this, we extend the Bradley-Terry-Luce model to handle heterogeneous preferences with unobserved variability factors and propose its practical implementation for reward modelling and LLM fine-tuning. Thanks to our proposed approach of functional parameter-space conditioning, LLMs trained with our framework can be adapted to individual preferences at inference time, generating outputs over a continuum of behavioural modes. We empirically validate the effectiveness of methods, demonstrating their ability to capture and align with diverse human preferences in a data-efficient manner.", "sections": [{"title": "1. Introduction", "content": "Motivation: The need for personalisation. As large language models (LLMs) become more integrated into daily life, it is critical that these models align with diverse human values and preferences. Current methods for LLM training, such as Reinforcement Learning from Human Feedback (RLHF) and Direct Preference optimisation (DPO), rely on single-objective fine-tuning, which assumes uniformity in human preferences. In reality, users of LLM-based systems often have diverse and even conflicting goals, influenced by factors like demographic and cultural backgrounds, cognitive processes, or implicit ambiguity in their prompt messages. These factors are often unobservable, leading to preference data that contains conflicting signals. When such data is aggregated under a homogeneous Bradley-Terry-Luce (BTL) model (Bradley & Terry, 1952), the result is equivalent to a Borda count (Siththaranjan et al., 2024), which fails to capture the true diversity in human preferences.\nChallenges: Unknown sources of heterogeneity require new methods. To overcome the limitations of single-objective fine-tuning, researchers have explored training multi-objective language models that generate desirable outputs by aligning with weighted mixtures of reward functions (Guo et al., 2024; Wang et al., 2024a; Ram\u00e9 et al., 2023; Jang et al., 2023; Wang et al., 2024b). However, these methods typically require multiple datasets, each labelled for a specific objective (e.g., honesty, helpfulness, or safety), and expect users to represent their preferences as a weighted vector over predefined objectives. In cases where the sources of preference heterogeneity are unobservable, this approach becomes impractical. Additionally, existing approaches require training multiple independent reward models or even various LLM policies (Chidambaram et al., 2024), one for each objective, which can be prohibitively expensive regarding computational resources and storage.\nOur goal: Towards few-shot steerable alignment. In this paper, we address the challenge of few-shot steerable alignment: Can we infer a user's underlying preferences from a small few-shot sample of their preference choices, and can we train LLMs to align with these preferences at inference time? Our goal is to achieve this alignment based"}, {"title": "2. Background & Problem Definition", "content": "2.1. Background\nStandard setup. Throughout this paper, we let $\\mathcal{Y}$ denote the space of candidate options that users rank according to their preferences. For any pair of options $y_1, y_2 \\in \\mathcal{Y}$ we use the notation $y_1 > y_2$ to denote that $y_1$ is preferred over $y_2$. Conventional approaches to preference learning and optimisation start by collecting a dataset of user's preference choices $\\mathcal{D}$, represented as tuples, $\\mathcal{D} = \\{(y^\\omega, y^\\ell)\\}$ with $y^\\omega, y^\\ell \\in \\mathcal{Y}$ such that $y^\\omega > y^\\ell$. In the context of natural language modelling, $\\mathcal{Y}$ is the space of natural language. Users express their preferences over pairs $Y_1, Y_2 \\in \\mathcal{Y}$, with $Y_1 = [x, \\tilde{y}_1], Y_2 = [x, \\tilde{y}_2]$, where $x$ is a prompt common for both options and $\\tilde{y}_1, \\tilde{y}_2$ two distinct completions sampled from an LLM, denoted by $\\tilde{y}_1, \\tilde{y}_2 \\sim \\pi_\\theta(\\cdot|x)$, where $\\pi_\\theta(\\cdot|x)$ is the generative distribution of the LLM, aka the LLM policy.\nA standard RLHF procedure starts by learning a reward function $r: \\mathcal{Y} \\rightarrow \\mathbb{R}$ aiming to capture the utility of individual options. The higher the utility of a candidate option $y$, the more likely it is that it is preferred over the alternatives. After fitting the reward function to the dataset $\\mathcal{D}$, it is then used to optimise the LLM policy $\\pi_\\theta$ via supervised fine-tuning. Alternatively, these two steps can be combined into one optimisation step, leading to the Direct Preference Optimisation (DPO) (Rafailov et al., 2024) procedure. We outline the details below.\nRLHF under the BTL model. The Bradley-Terry-Luce"}, {"title": "2.2. Problem Definition", "content": "Our Setup. With the focus on modelling preferences of individual users, our setup assumes access to a pre-collected"}, {"title": "3. Method", "content": "The following section outlines our approach to addressing the problem of few-shot steerable alignment defined above. First, in section 3.1, we extend the BTL reward model to capture user-specific preferences by conditioning on latent variables inferred from the few-shot sample of user-specific contextual data points. We then propose a practical implementation of this model based on Neural Processes (Garnelo et al., 2018a;b)-a class of meta-learning models enabling to infer the posterior predictive distribution over the outcomes in a fully amortised manner. We refer to this conditional reward model as NP-BTL. Given our conditional user preference model, we apply it in the context of LLM policy fine-tuning. In section 3.2, we introduce NP-DPO. By extending the conditional setup to DPO, we bypass explicit reward model training and propose a method for directly optimising LLMs with respect to the heterogeneous mixture of user preferences data.\n3.1. Stochastic Reward Modelling\nWe model the heterogeneity of human preferences resulting from the effect of the unobserved hidden context by assuming the following model. We assume that users, indexed by $i$, express their preferences according to their internal reward function $r_i: \\mathcal{Y} \\rightarrow \\mathbb{R}$. We model these functions as samples from a stochastic process. That is, each $r_i = r(\\cdot; z_*)$ for a particular value of $z_* \\in \\mathcal{Z}$, where $\\mathcal{Z}$ is a latent sample space. Given a particular value of $z_*$ and two options $y_1, y_2 \\in \\mathcal{V}$, we model the distribution of preferences extending the BTL model as:"}, {"title": "3.2. Conditional Direct Preference Optimisation", "content": "Our next step is to consider the training of personalisable LLM policies $\\pi_\\theta$ that can be conditioned on the contextual datasets $\\mathcal{D}^c$. To do this, we need to find a way of modulating the output of a language model on a set of preference pairs obtained in $\\mathcal{D}^c$, and crucially, be able to do so at inference time. We consider a prototype method based on modulating hypernetworks inspired by (Perez et al., 2017) to enable this. Namely, we model conditional policies with:\nIn the above, $\\theta$ are the original parameters of the LLM, $\\mathcal{T}_i$ are modulating parameters which are output by the modulating hypernetwork $h_\\phi$, again implemented as a Deep Set analogously to (4). Details on implementing the modulating operation $\\mathcal{T}_i$ are presented in the Appendix C.2.2. Extending the conventional DPO objective to our conditional formulation, we optimise the parameters $\\theta$ and $\\phi$ with respect to:\nwhere $r_\\theta(y) := \\beta \\log \\frac{\\pi_\\theta(\\tilde{y}^\\omega|x, D_i^c)}{\\pi_{ref}(\\tilde{y}^\\omega|x)}$ is the so-called implicit DPO reward (Rafailov et al., 2024). During training the subsets $(\\mathcal{D}_i^c, \\mathcal{D}_i^\\tau)$ are sampled analogously to Algorithm 1. The model learns to condition its outputs on the observed contexts by training over multiple pairs of context and target datasets. The introduced parameter-space conditioning based on FiLM layers enables flexible modulation of the behaviour of the LLM policy. We refer to this training pipeline as NP-DPO."}, {"title": "4. Empirical Studies and Insights", "content": "In this section, we demonstrate the performance of both NP-BTL and NP-DPO on a series of illustrative experiments with synthetic data and larger-scale experiments on an NLP dataset for LLM alignment. We also provide further insights into the efficacy of conditioning with respect to the conflict rate within the training dataset and the contextual examples. The error bars in all plots represent the standard errors across testing tasks. Details on the experimental setups, including the number of training and testing tasks, can be found in Appendix C.2.\n4.1. Illustrative comparison of existing approaches.\nWe begin with an illustrative example qualitatively comparing the NP-BTL model with the standard BTL and DPL models. The below presented follows closely that introduced in (Siththaranjan et al., 2024).\nExample 4.1. We simulate a heterogeneous population of users expressing their preferences with respect to options represented as values $y \\in [0,1]$. We assume that our population consists of two kinds of users represented by the non-observable latent variables $z^* \\sim Bernoulli(0.5)$ and we model the corresponding reward functions as:"}, {"title": "4.2. Real-world example: Helpfulness vs. Honesty", "content": "We now depart from the controlled setup of synthetic data to demonstrate the applicability of our NP-BTL and NP-DPO models on a real-world UltraFeedback dataset\u00b9 (Cui et al., 2024) commonly used to evaluate RLHF and DPO pipelines. Each sample within this dataset consists of user prompts $x$, two candidate LLM responses $\\tilde{y}_1$ and $\\tilde{y}_2$ and scores assessing both options with respect to the following categories: helpfulness, honesty, truthfulness and instruction following. To simulate users with varying preferences, we define two reward functions: $r(y; 0) = helpfulness(\\tilde{y})$ and $r(y; 1) = honesty(\\tilde{y})$. We sample tasks $(\\mathcal{D}_i^c, \\mathcal{D}_i^\\tau)$ during training and testing. At training time, the number of context preference pairs for reward modelling varies between 0 and 10, with the number of targets set to 20. The number of contexts for policy fine-tuning with DPO varies between 0 and 6, with the number of targets set to 8-the reduced number of data points for policy training results from increased memory requirements. At test time, we set the number of targets to 20. Choices between two options $y_1, y_2$ are made according to $r(\\cdot; z^*)$ for $z^* \\sim Bernoulli(0.5)$. Note, during training and testing, the reward models are only presented with the two options in a text format and the user choice. The underlying scores of helpfulness and"}, {"title": "4.3. Correlations in data and identifiability of the hidden context.", "content": "4.3.1. ANALYTIC STUDY\nIn the previous experiment, the training and testing datasets were constructed such that the competing options were perfectly conflicting, i.e. if a user preferring the more helpful response selects one option, then the user preferring the"}, {"title": "5. Related Work", "content": "RLHF and DPO. Reinforcement Learning from Human Feedback (RLHF) has been foundational to aligning AI systems with human preferences. This technique typically involves learning a reward function from binary human preference data, commonly modelled using the BTL framework (Bradley & Terry, 1952). The applications of RLHF to language models has been a significant focus of recent research, aiming to align LLMs with human values and preferences. Notable works in this area include those focusing on fine-tuning models (Bai et al., 2022b; Stiennon et al.; Kim et al., 2022; Ouyang et al., 2022). However, aligning LLMs through RLHF remains challenging due to training instability, which typically requires learning a reward model and the subsequent fine-tuning of the LLM. To address these inefficiencies, (Rafailov et al., 2024) propose Direct Preference Optimisation (DPO), a method that bypasses the need for explicit reward model training by optimising preference data directly in a supervised fashion.\nPluralistic Alignment. Most RLHF and DPO methods are based on the BTL model, which assumes a single reward function for all users. This approach can fail to capture the diversity of user preferences. (Sorensen et al., 2024) recognise, however, the need to focus on developing methods that enable pluralistic alignment of LLMs, serving humans with varying values and perspectives. A critical development in pluralistic alignment has been in recognising the so-called \"hidden context effect\"-factors that influence user preferences in ways that are not directly observable. To address these effects, (Siththaranjan et al., 2024) propose Distributional Preference Learning (DPL), modelling a distribution over the plausible reward values for each competing option. While DPL offers a more nuanced understanding of how hidden contexts influence the population's preferences, it does not provide a way of adapting the reward functions, and thus the final LLM policies, to individual users. Our approach"}, {"title": "6. Limitations", "content": "Due to computational limitations, our experimental evaluation is limited to the simplistic UltraFeedback dataset and a relatively small LLM. To ensure safe and reliable deployment of few-shot steerable LLM policies with NPDPO, experiments on larger datasets and LLMs with more parameters are required. We also note that the proposed"}, {"title": "7. Conclusions", "content": "This work introduces a novel framework for few-shot steerable alignment, addressing the challenge of personalising AI systems to diverse user preferences. By extending the BTL model to handle heterogeneous preferences with unobserved variability factors, we enable the inference of user preferences from small samples of their choices. Our conditional reward modelling approach (NP-BTL) and its extension to DPO (NP-DPO) allow for training steerable LLM policies that can adapt to individual user preferences with minimal data. Experiments on synthetic and real-world datasets demonstrate the effectiveness of our methods in efficiently capturing and aligning with diverse human preferences. We also analyse the value of contextual data in determining the underlying reward function of a user. Noting that not all preference pairs are equally informative, future research directions include exploring active learning approaches for heterogeneous preference data collection, enabling maximum information gain about the users' internal preferences. Another line of future work should consider scaling the framework to larger models and more complex datasets. In summary, this paper's proposed methods and insights mark a significant step towards developing truly personalisable AI agents."}, {"title": "A. Analytic study: proof", "content": "Suppose $y = [h_o, h_1]^T \\sim N(\\mu, \\Sigma)$, where $\\Sigma = \\begin{bmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{bmatrix}$. Let $Z^* \\sim Bernoulli(0.5)$, and \nWe assume that decisions are made according to \nWe define a Bayes optimal classifier predicting the value of $Z^*$, given an observed dataset $D^c$ as:\nwhere in case $P(Z^* = 0|D^c) = P(Z^* = 1|D^c)$, we let $\\hat{Z}(D^c) \\sim Bernouli(0.5)$.\nWe look at the expected error rate of $\\hat{Z}(D^c)$. We have that:\nLet us now consider a special case of a single contextual sample, such that $D^c = \\{(y^\\omega,y^\\ell)\\}$, where $y^\\omega = [h_o^\\omega,h_1^\\omega]$, $y^\\ell = [h_o^\\ell,h_1^\\ell]$ are sampled i.i.d from $N(\\mu, \\Sigma)$. We then have, for $k \\in \\{0, 1\\}$\nTherefore,\nThe event $\\hat{Z}(D^c) = 1$ can only happen in two cases: a) it happens with a probability of 1 if $P(Z^* = 1|D^c) > P(Z^* = 0|D^c)$ b) it happens with a probability of 0.5 when $P(Z^* = 1|D^c) = P(Z^* = 0|D^c)$. Conditioned on $Z^* = 0$, we must necessarily have $h_o^\\omega > h_o^\\ell$. This in turn implies that $\\hat{Z}(D^c) = 1$ can only happen in case b), i.e. when $P(Z^* = 1|D^c) = P(Z^* = 0|D^c) = 0.5$, which happens if and only if $h_o^\\omega > h_o^\\ell$ and $h_1^\\omega < h_1^\\ell$. Thus,\nTo compute this probability we define $X = [X_o, X_1] = [h_o^\\omega - h_o^\\ell, h_1^\\omega - h_1^\\ell]$. We have that $X \\sim N(0, 2\\Sigma)$ and our probability of interest is equal to\nwhich by a standard result based on the geometry of the Gaussian distribution is equal to $\\frac{1}{4} + \\frac{1}{2\\pi} arcsin(\\rho)$."}, {"title": "B. Complexity Analysis of NP-BTL and NP-DPO", "content": "Training. Several operations influence the time complexity per iteration of the algorithm. First, sampling a user i from the dataset is an O(1) operation. Sampling the context set $\\mathcal{D}_i^c$ and the target set $\\mathcal{D}_i^\\tau$ from the user-specific dataset $\\mathcal{D}_i$ requires again O(1). During training, the number of context data points $N_c$ for each task varies between $N_{c,min}$ and $N_{c,max}$. The number of target data points $N_\\tau$ is constant for all tasks.\nAt each forward pass, the algorithm computes the conditional latent variable $z_i$ for the sampled context datasets $\\mathcal{D}_i^c$. This operation takes $O(N_c \\cdot f_{enc})$ where $f_{enc}$ is the complexity of a forward pass of the encoder network. The loss, i.e. negative log-likelihood of the target pairs $\\mathcal{D}_i^\\tau$, is computed with a forward pass of all target pairs $(y^\\omega_j, y^\\ell_j) \\in \\mathcal{D}_i^\\tau$ and the latent variable $z_i$ through the decoder network. This takes $O(N_\\tau \\cdot f_{dec})$, where $f_{dec}$ is the complexity of one forward pass through the decoder network.\nParameters of the model are optimised with a version of stochastic gradient descent run for a maximum of T steps. Thus, the overall time complexity of training an NP-BTL or NP-DPO model is:\nwhich is linear in the number of iterations T, the number of target pairs $N_\\tau$, and the maximum number of context pairs $N_{c,max}$.\nInference. To infer the reward for a single option $y \\in \\mathcal{Y}$, given a contextual sample $\\mathcal{D}_i^c = \\{(y^\\omega_j,y^\\ell_j)\\}_{j=1}^{N_c}$, the model first computes the contextual embedding $z_i$ and then passes it through the decoder network together with the option $y$. Thus, time complexity of inference is $O(N_c \\cdot f_{enc} + f_{dec})$.\nC. Implementation and experimental details\nAll experiments are implemented in Python, using the PyTorch (Paszke et al., 2017) and HuggingFace libraries (Wolf et al., 2020). All experiments can be run on a machine with a single A100 Nvidia GPU. We provide the code implementing the NP-BTL and NP-DPO models alongside the instructions needed to reproduce the key experiments in this repository:\nAll reward and LLM policies are trained using the Adam optimiser (Kingma & Ba, 2014).\nUnless otherwise stated, all mentions of MLPs are implemented as a two-layer neural networks with a hidden dimension of 256 and GELU activation.\nFollowing the convention of NP training (Garnelo et al., 2018a;b), all tasks $\\tau = (\\mathcal{D}_i^c, \\mathcal{D}_i^\\tau)$ are constructed such that context pairs are a subset of the target pairs, i.e. $\\mathcal{D}_i^c \\subseteq \\mathcal{D}_i^\\tau$. We only report the unseen pairs' accuracies during model evaluation: $\\mathcal{D}_i^\\tau \\setminus \\mathcal{D}_i^c$.\nC.1. Illustrative comparison of existing approaches\nDataset. In this experiment all options $y \\in \\mathcal{Y}$ are represented in $\\mathcal{Y} = [-1, 1]$. The dataset is constructed by first generating 20k pairs sampled uniformly from $[-1, 1]^2$. This dataset is split into 10k training, 5k validation and 5k testing pairs. To construct a single task $t_i = (\\mathcal{D}_i^c, \\mathcal{D}_i^\\tau)$, we first sample the unobservable $z \\sim Bernoulli(0.5)$. Then, we sample $N_c$ context and $N_\\tau$ target pairs from the training, validation, or testing split. For each pair in the context and target datasets, the"}, {"title": "C.2. Real-world example: Helpfulness vs. Honesty", "content": "Dataset. The basis of this experiment is the UltraFeedback dataset. Each option $y = [x, \\tilde{y}]$ within this dataset is represented with a user prompt $x$ and a candidate LLM response $\\tilde{y}$. Alongside the set of prompts and candidate responses, each response in this dataset is assigned a score in four categories: honesty, helpfulness, truthfulness and instruction following. To simulate users with varying preferences, we define two reward functions:\nThe raw format of prompts and responses is in natural language. For computational efficiency, we represent them as frozen embeddings pre-computed with the LLama-3-8B language model. These embeddings are obtained as the last hidden state representation of the last token in the tokenized representation of $y$. We denote the embeddings of $y_j^\\omega, y_{i,j}^\\ell$ as $e_j^\\omega,e_{i,j}^\\ell \\in \\mathbb{R}^{4096}$. During training, validation, and testing, we first sample $z \\sim Bernoulli(0.5)$ and then sample $N_c$ context pairs and $N_\\tau$ target pairs with decisions made deterministically according to $p(y_1 > y_2) = \\mathbb{1}\\{r(y_1, z^*) > r(y_2, z^*)\\}$.\nDuring training, the number of context pairs for reward modelling varies uniformly between $N_c^{min} = 0$ and $N_c^{max} = 10$. The number of target pairs is set to $N_\\tau = 20$.\nFor LLM policy fine-tuning, during training, the number of context pairs varies uniformly between $N_c^{min} = 0$ and $N_c^{max} = 6$. The number of target pairs is set to $N_\\tau = 9$.\nTo construct the dataset with 100% conflicting pairs, we filter the dataset to pairs of options for which the response made with $r(\\cdot; 0)$ differs from the response made with $r(\\cdot; 1)$. As a result, our final dataset consists of 28763 training, 3673 validation, and 3509 testing pairs $(y_1, y_2)$."}, {"title": "C.2.1. REWARD MODELS", "content": "NP-BTL. Our context encoder is implemented as a DeepSet with multi-head self-attention. For $\\mathcal{D}^c = \\{(y^\\omega_j, y^\\ell_{i,j})\\}_{j=1}^{N_c}$ the contextual embedding $z_i$ is computed as:\nwhere $W \\in \\mathbb{R}^{2 \\cdot 4096 \\times 256}$ is a linear map, $h_{\\phi_{e,2}} : \\mathbb{R}^{256} \\rightarrow \\mathbb{R}^{256}$ is an MLP, and MultiHeadAttn a multi-head attention layer with 8 heads and dropout 0.1 5.\nThe decoder $g_{\\phi_a} : \\mathbb{R}^{256+256} \\rightarrow \\mathbb{R}$, first passess the embedding $e$ of a target option $y \\in \\mathcal{D}^\\tau$ through an MLP, then concatenates the output with $z_i$, and passess it through another MLP so that:\nBTL. The BTL model is a simple MLP mapping the embedding $e \\in \\mathbb{R}^{4096}$ to scalar rewards in $\\mathbb{R}$.\nTraining. Each batch consists of 64 tasks containing the same number of context data points $(\\mathcal{D}_i^c, \\mathcal{D}_i^\\tau)$ (for the non-conditional baselines we have $\\mathcal{D}_i^c = \\emptyset$). Models are trained for a maximum of 450 SGD updates, retaining the model parameters with the lowest loss on the validation split. The learning rate is set to 1e-4."}, {"title": "C.2.2. LLM POLICY MODELS", "content": "NP-DPO. Contextual embeddings $z_i$ are obtained in the same way as in reward modelling with NP-BTL. As the reference model for policy training, we choose the gemma-2b LLM, as it is small enough to be trained on a single A100 GPU. A key component in this task is modulating the LLM parameters $\\theta$ with the conditional latent variable $z_i$. Our modulation operator is based on feature-wise linear modulation (FiLM) (Perez et al., 2017). The modulating hypernetwork $h_\\phi$, takes in the embedding $z_i$ and outputs a collection of modulating parameters $h_\\phi(\\mathcal{D}_i^c) = \\{\\gamma_m(z_i), \\beta_m(z_i)\\}_{m=1}^L$, where L is equal to the number of attention blocks. If we denote by $o_m$ the output of the attention block at the $m$-th layer, then hidden representations modulated with $z_i$ are obtained as:\nHere, we implement $\\gamma_m$ and $\\beta_m$ as simple linear maps between $\\mathbb{R}^{256}$ and $\\mathbb{R}^{4096}$. The parameters $\\theta$ of the base LLM and the collection of the weights $\\phi$ defining the hyper network $h_\\phi$ are optimised jointly by maximising the conditional DPO objective:\nBTL-DPO. We optimise the parameters $\\theta$ of gemma-2b with the standard DPO objective:\nTraining. To reduce memory requirements, parameters $\\theta$ of the base gemma-2b LLM are optimised with Low-Rank Approximation (LoRA) (Hu et al., 2021), with a rank of 512 and a LoRA-$\\alpha$ of 1024. The learning rate of the Adam optimizer is set to 1e-6. The parameter $\\beta$ is set to 0.05. During training, we use a batch size of 1."}, {"title": "C.3. Correlations in data and identifiability of the hidden context", "content": "C.3.1. ANALYTIC STUDY\nDataset. Options $y = [h_o, h_1] \\in \\mathbb{R}^2$ are sampled with $N(\\mu, \\Sigma)$ with $\\mu = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$ and $\\Sigma = \\begin{bmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{bmatrix}$. For any two pairs $y_1, y_2 \\in \\mathbb{R}^2$, decisions are made according to $p(y_1 > y_2) = \\mathbb{1}\\{r(y_1, z^*) > r(y_2, z^*)\\}$, where\nThe dataset is constructed by first generating 20k pairs $y_1, y_2 \\in \\mathbb{R}^2$. This dataset is split into 10k training, 5k validation and 5k testing pairs. To construct a single task $t_i = (\\mathcal{D}_i^c, \\mathcal{D}_i^\\tau)$, we first sample the unobservable $z \\sim Bernoulli(0.5)$ and then sample $N_c$ context and $N_\\tau$ target pairs from the training, validation, or testing split with preference choices made according to $r(\\cdot ; z^)$. The number of context pairs varies uniformly between $N_c^{min} = 0$ and $N_c^{max} = 10$. The number of target pairs is set to $N_\\tau = 20$.\nModel implementation and training. Implementation of the NP-BTL and the baseline BTL reward models is analogous to that presented in C.1, with the input dimension replaced from 1 to 2. The training setup is identical to C.1.\nC.3.2. HELPFULNESS VS. HONESTY\nDataset. The construction of the dataset is analogous to the one presented in C.2, controlling the number of conflicting pairs. We construct three independent datasets with a conflict rate of 100% (this is the same dataset as in C.2), 75% and 50%. All datasets are divided into training, validation, and testing splits, with the same respective sizes as in the 100% condlicting case.\nModel implementation and training. Identical to the setup described in C.2."}, {"title": "D. Additional Experiments", "content": "D.1. UltraFeedback with three Behavioural modes: Helpfulness vs Honesty vs Truthfulness\nDataset. We use the UltraFeedback dataset and follow an analogous setup to the one described in C.2, this time sampling reward functions with three different modes of behaviour. That is we let: To simulate users with varying preferences, we define two reward functions:\nwhere $z^* \\sim Multinomial([1/3, 1/3, 1/3])$. To ensure high informativeness of contextual samples, we filter the dataset to pairs $y_1, y_2$ for which one choice is made according to $r(\\cdot ; 0), r(\\cdot ; 1), r(\\cdot ; 2)$ is distinct from the others The resulting dataset consists of 27475 training, 3415 validation, and 3415 testing pairs.\nTraining. We train the BTL reward models and DPO policies with the same hyperparameters as in C.2. As previously, we train two versions of the BTL models: on data generated according to the mixture of three reward functions (BTL-mixed) and one model for each of the three ground-truth values of $z^*$ (BTL-helpfulness, BTL-honesty, and BTL-truthfulness).\nResults. Figure 6 shows the test-set accuracies of the learned rewards evaluated separately on preference choices made with the helpfulness- ($r(\\cdot ; 0)$ in blue), honesty- ($r(\\cdot ; 1)$ in green), and truthfulness-preferring ($r(\\cdot ; 2)$ in orange) reward functions. We make analogous conclusions to experiments on the 2-mode HH dataset from section 4.2 of the main paper. The BTL-mixed reward model fails to align with any of the three objectives. The accuracy of the NP-BTL model increases with the number of contextual preference pairs accross all three modes of behaviour. At $N_c$ = 10, its performance matches the performance of the dedicated BTL-helpfulness, BTL-honesty and BTL-truthfulness models.\nWe also plot the PCA embeddings of the contextual latent variables $z$ (Figure 7). Again, with an increasing number of contextual data points, we observe $z_i$ forming distinct clusters. The separation between honesty and truthfulness is less clearly defined. We hypothesise this is due to these two objectives being semantically similar, leading to choices made with $r(\\cdot ; 1)$ (honesty) and $r(\\cdot ; 2)$ (truthfulness) exhibiting a stronger correlation."}, {"title": "D.2. Beyond a fixed number of reward functions", "content": "One of the appealing properties of the NP-BTL and NP-DPO models is that they are not constrained to modelling just a fixed set of reward functions. Instead, they can represent an entire spectrum of behaviours. We illustrate this with the UltraFeedback dataset in a two-dimensional and a three-dimensional setup.\n2D setup. We let $z^* = [z_o^*, z_1^*", "0.5": "and we let\n3D setup. We let $z^* = [z_o^*, z_1^*, z_2^*"}, {"0.5": "and we let\nWe train one NP-BTL model for the 2D setup (NP-BTL-2D) and one for the 3D setup (NP-BTL-3D). Hyperparameters are set as in C.2.\nResults. Figure 8 shows the accuracies of the NP-BTL-2D and NP-BTL-3D rewards on the test-set pairs, with choices made according to $r_{2D"}]}