{"title": "COSEE: Consistency-Oriented Signal-Based Early Exiting via Calibrated Sample Weighting Mechanism", "authors": ["Jianing He", "Qi Zhang", "Hongyun Zhang", "Xuanjing Huang", "Usman Naseem", "Duoqian Miao"], "abstract": "Early exiting is an effective paradigm for improving the inference efficiency of pre-trained language models (PLMs) by dynamically adjusting the number of executed layers for each sample. However, in most existing works, easy and hard samples are treated equally by each classifier during training, which neglects the test-time early exiting behavior, leading to inconsistency between training and testing. Although some methods have tackled this issue under a fixed speed-up ratio, the challenge of flexibly adjusting the speed-up ratio while maintaining consistency between training and testing is still under-explored. To bridge the gap, we propose a novel Consistency-Oriented Signal-based Early Exiting (COSEE) framework, which leverages a calibrated sample weighting mechanism to enable each classifier to emphasize the samples that are more likely to exit at that classifier under various acceleration scenarios. Extensive experiments on the GLUE benchmark demonstrate the effectiveness of our COSEE across multiple exiting signals and backbones, yielding a better trade-off between performance and efficiency.", "sections": [{"title": "1 Introduction", "content": "Although tremendous improvements have been achieved by pre-trained language models (PLMs) in natural language processing tasks (Devlin et al. 2019; Lan et al. 2020; Radford et al. 2019; Liu et al. 2019), high computational costs of PLMs in both training and inference still hinder their deployment in resource-constrained devices and real-time scenarios. Besides, overthinking problem (Kaya, Hong, and Dumitras 2019) also restricts the application of PLMs. Precisely, for easy samples, PLMs can generate correct predictions according to the representations indicated by shallow layers. However, high-level representations may focus on more intricate or unrelated details, leading to incorrect answers.\nTo address these issues, early exiting (Xin et al. 2020; Zhou et al. 2020; Xin et al. 2021; Liao et al. 2021; Sun et al. 2022; Zeng et al. 2024), a kind of adaptive inference strategy, has been proposed to accelerate the inference of PLMs. As illustrated in Figure 2, each intermediate layer of the PLM is coupled with an internal classifier to give an early prediction. This enables the early exiting of samples once the early predictions are sufficiently reliable, eliminating the need for passing them through the entire model. This method employs a sample-wise inference strategy to deal with easy samples with shallow classifiers and process hard samples with deeper classifiers, significantly improving inference efficiency without sacrificing accuracy and alleviating the overthinking problem.\nSignal-based early exiting methods (Xin et al. 2020; Liu et al. 2020; Zhou et al. 2020; Schwartz et al. 2020; Li et al. 2021; Liao et al. 2021; Ji et al. 2023; Zhu 2021; Zhu et al. 2021; Gao et al. 2023; Zhang et al. 2023; Zhu et al. 2023; Akbari, Banitalebi-Dehkordi, and Zhang 2022; Xin et al. 2021; Balagansky and Gavrilov 2022) are typical implementations of early exiting, which rely on carefully designed exiting signals (e.g. entropy, energy score, softmax score, and patience) to dynamically adjust the number of executed layers for each sample. The inference process is terminated once the exiting signal meets a certain condition. These methods can easily adapt to various acceleration requirements during inference by simply adjusting the threshold, without incurring additional training costs. However, existing works simply use the (weighted) sum of cross-entropy losses from all classifiers as the training objective, where each classifier treats the loss of both easy and hard samples equally. This treatment ignores the dynamic early exiting behavior during inference (as shown in Figure 1), leading to a gap between training and testing.\nTo bridge the gap, router-based early exiting methods (Sun et al. 2022; Mangrulkar, MS, and Sembium 2022; Zeng et al. 2024) have been successively proposed. These methods employ a router (e.g. a hash function or a network) to determine the exiting layer of samples during both training and inference, and each sample only incurs a cross-entropy loss at its exiting classifier, ensuring consistency between training and testing. However, router-based early exiting methods fail to meet various acceleration requirements during inference, as a router can only generate a fixed exiting strategy, leading to unadjustable speed-up ratios.\nIn this paper, we aim to bridge the gap between training and testing while enabling flexible adjustments of the speed-up ratio. To this end, building upon the signal-based early exiting framework, we propose to assign sample-wise weights on the cross-entropy loss of all classifiers, such that each classifier is encouraged to emphasize samples that are"}, {"title": "2 Preliminaries", "content": "In this section, we provide the necessary background for signal-based early exiting."}, {"title": "2.1 Problem Definition", "content": "Per Figure 2, given a BERT-style PLM with M layers, we denote the hidden states at the mth layer as $h^{(m)}$. To enable early exiting during inference on a classification task involving C classes, each intermediate layer is equipped with an internal classifier $F_m$, $m \\in \\{1, 2, \u2026, M - 1\\}$ to produce an early prediction $p^{(m)} = F_m(h^{(m)})$, i.e., a probability distribution over the C classes. Classifiers in different layers do not share parameters."}, {"title": "2.2 Signal-based Early Exiting", "content": "For a given sample x, the inference process is terminated once the exiting signal at the current layer meets a certain condition. For exiting signals that exhibit a positive correlation with sample difficulty (e.g. entropy and energy score), early exiting is triggered once the exiting signal falls below a predefined threshold. A higher threshold leads to a higher speed-up ratio and potentially some performance degradation. Conversely, for exiting signals negatively correlated with sample difficulty (e.g. patience and softmax score), the exiting condition is met when the exiting signal surpasses the threshold. A higher threshold leads to a lower speed-up ratio and performance improvements."}, {"title": "2.3 Conventional Training Methods", "content": "In current signal-based early exiting methods, a widely used training objective involves the (weighted) sum of cross-entropy losses across all classifiers:\n$L = \\sum_{m=1}^M w_m L^{(m)},$ (1)\nwhere $L^{(m)}$ denotes the cross-entropy loss of the mth classifier and $w_m$ denotes the corresponding loss weight. Under Eq.(1), each classifier treats the loss of both easy and hard samples equally, which is inconsistent with the dynamic early exiting behavior during inference."}, {"title": "3 The COSEE Framework", "content": "We propose a novel Consistency-Oriented Signal-based Early Exiting (COSEE) framework for PLMs, aiming to ensure consistency between training and testing while maintaining flexible adjustments of the speed-up ratio. Figure 2 provides an overview of our framework. We first propose a sample weighting mechanism (SWM) that identifies the potential exiting layer of samples by simulating the test-time"}, {"title": "3.2 Sample Weighting Mechanism", "content": "Our goal is to identify the potential exiting layer of samples in various acceleration scenarios, and then assign greater weights to the cross-entropy loss of each sample on classifiers closer to its exiting layer. Accordingly, at each training step, all samples are passed through the entire model to generate predictions and exiting signals at all classifiers. Subsequently, we randomly select K thresholds and simulate the early exiting process based on exiting signals at each threshold to find where the samples exit. This information is used to produce sample-wise loss weights across all classifiers.\nRange for Threshold Selection. For threshold selection, we collect the maximum and minimum values of exiting signals across all layers for training samples within each epoch and use them to create the selection range for the next epoch. We start with the thresholds randomly selected between 0 and 1 in the first epoch.\nWeight Assignment. For a given threshold \\tau, we impose sample-wise loss weights across all classifiers based on the exiting layer of samples and then compute the classification loss at threshold \\tau:\n$L_{CE,\\tau} = \\frac{1}{N} \\sum_{n=1}^N \\sum_{m=1}^M w_n^{(m)} \\cdot CE(y^{(m)}, y_n), $ (2)\n$w_n^{(m)} = \\frac{e^{-\\beta_t |m - m^*_n|}}{\\sum_{m'=1}^M e^{-\\beta_t |m - m'|}}, $ (3)\nwhere $CE(y^{(m)}, y_n)$ and $w_n^{(m)}$ denote the cross-entropy loss and the loss weight for the nth sample at the mth classifier respectively, and $w_n^{(m)}$ satisfies $\\sum_{m=1}^M w_n^{(m)} = 1$. N denotes the number of samples. $m^*_n$ denotes the index of exiting layer for the nth sample at threshold \\tau, and $\\beta_t$ denotes the decay factor at the tth training step. According to Equation 3, classifiers closer to the exiting layer are assigned greater weights compared to those further away, i.e., each sample is emphasized by the classifiers near its exiting layer. Note that the loss weights of classifiers are symmetrical around the exiting layer for easy parameter selection. Different from router-based early exiting methods, which employ one-hot sample-wise loss weights such that each sample only incurs a cross-entropy loss on its exiting classifier, we employ a softer sample weighting mechanism to enable the generality of our COSEE on unseen thresholds.\nDuring the early training stage, unstable exiting layers often lead to fluctuating loss weights, consequently impacting the model's convergence. To mitigate this problem, we conduct a warm-up operation for the decay factor $\\beta_t$ to gradually increase the impact of the sample's exiting layers on the loss weights during training:\n$\\beta_t = \\gamma_t \\beta_0,$ (4)\nwhere $\\beta_0$ is positive, and $\\gamma_t$ is the ratio of the current training step to the total training steps.\nClassification Objective. To enable various acceleration ratios during inference, the classification objective is defined as the mean of classification losses across all K thresholds:\n$L_{CE} = \\frac{1}{K} \\sum_{\\tau} L_{CE_{\\tau}}.$ (5)"}, {"title": "3.3 Online Signal Calibration", "content": "While SWM effectively facilitates the training of multi-exit networks, exiting signals may not consistently reflect sample difficulty, particularly during the early training stages. This affects the reliability of exiting decisions, leading to"}, {"title": "3.4 Training Objective", "content": "The training objective of the COSEE is formulated as the weighted sum of the classification and OSC objective:\n$L = L_{CE} + \\alpha \\times L_{OSC},$ (9)\nwhere \\alpha is a hyper-parameter used to balance the classification and OSC objectives. All internal classifiers are jointly trained with the backbone."}, {"title": "3.5 Exiting Signal", "content": "Following E-LANG (Akbari, Banitalebi-Dehkordi, and Zhang 2022), we primarily implement our COSEE with the energy-based exiting signal. The energy score is defined as:\n$E(x; F_m) = - log \\sum_{i=1}^C e^{f_i^{(m)}},$ (10)"}, {"title": "4 Experiments", "content": "4.1 Tasks and Datasets\nFollowing Li et al. (2021); Liao et al. (2021), we evaluate COSEE on six classification tasks from the GLUE benchmark (Wang et al. 2019), including SST-2, MRPC, QNLI, RTE, QQP, and MNLI. Data statistics are shown in Table 1.\n4.2 Baselines\nWe compare our COSEE model with three groups of representative and state-of-the-art baselines.\nBackbone. We adopt the widely used BERT-base (Devlin et al. 2019) as the backbone for convincing comparisons.\nBudget Exiting. We directly train a BERT-base with 6 layers (BERT-6L) to obtain a speed-up ratio of 2.00\u00d7, establishing a lower bound for early exiting methods as no techniques are employed.\nEarly Exiting. For signal-based early exiting methods, we choose DeeBERT (Xin et al. 2020), PABEE (Zhou et al. 2020), BERxiT (Xin et al. 2021), LeeBERT (Zhu 2021), GPFEE (Liao et al. 2021), GAML-BERT (Zhu et al. 2021), PALBERT (Balagansky and Gavrilov 2022), and DisentangledEE (Ji et al. 2023). For router-based early exiting methods, we choose state-of-the-art ConsistentEE (Zeng et al. 2024). Notably, some early exiting methods (Sun et al. 2022; Mangrulkar, MS, and Sembium 2022; Zhang et al. 2023; Zhu et al. 2023) are not included due to the difference in backbones. CascadeBERT (Li et al. 2021) and E-LANG (Akbari, Banitalebi-Dehkordi, and Zhang 2022) are excluded for fair comparisons since they implement early exiting within several complete networks instead of a multi-exit network. Refer to Appendix C for more details.\n4.3 Experimental Settings\nMeasurement. Since the runtime is unstable across different runs, following Zhang et al. (2022) and Liao et al. (2021), we utilize the saved layers to measure the speed-up ratio:\n$Speed-up Ratio = \\frac{M \\times N}{\\sum_{m=1}^M m \\times N_m},$ (12)\nwhere M is the total number of layers and $N_m$ is the number of samples exiting from the mth layer. According to Xin et al. (2020), this metric is proportional to actual runtime."}, {"title": "4.4 Overall Performance Comparison", "content": "Table 2 reports the test results of each early exiting method on the GLUE benchmark with BERT-base as the backbone model. The speed-up ratio is approximately 2.00\u00d7 (\u00b138%). Overall, our COSEE framework with normalized energy score demonstrates a superior performance-efficiency trade-off across different tasks compared to the baseline methods, which verifies the effectiveness of our design. Notably, our COSEE can even outperform the original BERT-base on RTE and QQP tasks, indicating that our method can effectively alleviate the overthinking problem of PLMs. This suggests that, for easy samples, predictions from intermediate layers may outperform those from the final layer. Our method enables easy samples to exit at shallow classifiers, thereby reducing the inference time while maintaining or even improving the task performance. Besides, our method can save training costs (see Section 4.5) and introduce negligible additional storage overhead (see Section 5.3). We also explore the impact of hyperparameters (see Appendix A) and analyze the failure cases statistically (see Appendix D).\nAlthough using energy scores and BERT-base in the primary experiments, we also verify the generality of COSEE on various exiting signals and backbones (see Section 5.2)."}, {"title": "4.5 Ablation Studies", "content": "Performance-Efficiency Trade-Off. To investigate the effectiveness of SWM and OSC, we plot the performance-efficiency trade-off curves of models trained using different methods on a representative subset of GLUE, as shown in Figure 4. We can observe both SWM and OSC significantly improve the performance of early exiting across all tasks, especially under high speed-up ratios. This confirms the advantage of our COSEE under high acceleration scenarios, indicating the proposed SWM and OSC effectively facilitate the training of internal classifiers, particularly shallow ones.\nEvaluation of Exiting Signals. Difficulty Inversion Score (DIS) is first proposed by Li et al. (2021), an evaluation metric for exiting signals. A higher value indicates a greater correlation between the exiting signal and sample difficulty, thus enabling more reliable exiting decisions. Figure 5 illustrates the DIS of exiting signals generated by different models on SST-2 and QNLI tasks. The results indicate that OSC explicitly enhances the correlation between the exiting"}, {"title": "5 In-depth Analysis", "content": "5.1 Visualization of Sample Exiting Layers\nTo examine the consistency between training and testing under our COSEE framework, we visualize the exiting layer distribution in training and development sets at various"}, {"title": "5.2 Generality of the COSEE Framework", "content": "In this subsection, we explore the generality of our method on various exiting signals and backbones. The experiments are conducted on a representative subset of GLUE.\nFigure 8 and Figure 9 present the experimental results of our COSEE framework with entropy and softmax scores, respectively. The results demonstrate the generality of our"}, {"title": "5.3 Storage Costs Analysis", "content": "Table 4 compares the parameter volumes of our COSEE model with those of the original BERT-base. We observe that our COSEE model only requires less than 0.03% additional parameters due to the incorporation of internal classifiers. Additionally, it is noteworthy that the proposed SWM is parameter-free, yet it can effectively generate proper loss weights for each sample to facilitate the training of a multi-exit network."}, {"title": "6 Conclusion", "content": "In this paper, we point out that the performance bottleneck of existing early exiting methods primarily lies in the challenge of ensuring consistency between training and testing while enabling flexible adjustments of the speed-up ratio. To remedy this, we propose COSEE, which mimics the test-time early exiting process under various acceleration scenarios based on calibrated exiting signals and then produces the sample-wise loss weights at all classifiers according to the sample's exiting layer. Our framework is both simple and intuitive. Extensive experiments on the GLUE benchmark demonstrate the superiority and generality of our framework across various exiting signals and backbones."}, {"title": "A Parameter Sensitivity Analysis", "content": "In this section, we further investigate the impact of four parameters on task performance under different speed-up ratios. Figure 10 shows the experimental results on SST-2 and QNLI tasks.\nImpact of $\\beta_0$. The parameter $\\beta_0$ in Eq.(4) is utilized to adjust the distribution of loss weights across all classifiers. As shown in Figure 10(a) and Figure 10(b), both excessively large and small values of $\\beta_0$ can impair the acceleration performance of the COSEE model, and the optimal $\\beta_0$ value differs across tasks. Additionally, under high acceleration scenarios, the performance improvements brought by SWM are particularly significant, which is consistent with the observations in Figure 4 and Figure 5.\nImpact of $\\alpha$. The parameter $\\alpha$ in Eq.(9) balances the classification objective and OSC objective during training. The results depicted in Figure 10(c) and Figure 10(d) indicate that the acceleration performance of our COSEE model is not significantly affected by $\\alpha$ selection, and an $\\alpha$ value between 0.01 and 0.1 can always lead to satisfactory performance across different tasks. Moreover, an excessively large $\\alpha$ value can lead to performance degradation. We attribute this to an overemphasis on the OSC objective, which interferes with the optimization of task performance.\nImpact of $\\epsilon$. The parameter $\\epsilon$ in Eq.(7) is utilized to regulate the distribution divergence of exiting signals between easy and hard samples. As shown in Figure 10(e) and Figure 10(f), an $\\epsilon$ value between 0.1 and 0.4 can always lead to satisfactory performance across different tasks. Consequently, we fix the $\\epsilon$ value at 0.3 to simplify the parameter selection.\nImpact of $K$. The parameter $K$ in Eq.(5) determines the number of thresholds randomly selected at each training step. The results in Figure 10(g) and Figure 10(h) show that, as the value of $K$ increases, the task performance exhibits a consistent pattern of initially increasing and then stabilizing under various speed-up ratios. This aligns with our intuition as selecting diverse thresholds during training is crucial for enabling models to meet different acceleration requirements during inference. We fix the $K$ value at 5 for all tasks for computational efficiency."}, {"title": "B Related Works", "content": "The early exiting methods applied to PLMs can be roughly divided into two categories: signal-based early exiting and router-based early exiting.\nB.1 Signal-based Early Exiting\nSignal-based early exiting methods dynamically adjust the number of executed layers for each sample based on the exiting signal, enabling samples to exit early during inference once the exiting conditions are satisfied.\nAccording to the type of exiting signal, signal-based early exiting can be further categorized into score-based early exiting, patience-based early exiting, and learning-based early exiting. Score-based early exiting methods (Xin et al. 2020; Liu et al. 2020; Schwartz et al. 2020; Akbari, Banitalebi-Dehkordi, and Zhang 2022; Li et al. 2021; Liao et al. 2021)"}, {"title": "B.2 Router-based Early Exiting", "content": "Router-based early exiting methods employ a router to determine exiting in both the training and inference phases. Some studies (Sun et al. 2022; Mangrulkar, MS, and Sembium 2022) utilize a hash function or a network to assign the exiting layer for each sample. ConsistentEE (Zeng et al. 2024) employs reinforcement learning to train a policy network for exit decision-making.\nThese methods perform early exiting during training, and each sample only incurs a cross-entropy loss at its exiting classifier. This treatment effectively ensures consistency between training and testing. However, router-based early exiting methods fail to meet various acceleration requirements during inference, as a router (a hash function or a trained network) can only generate a fixed exiting strategy, leading to unadjustable speed-up ratios.\nThis work proposes a novel signal-based early exiting framework to ensure consistency between training and testing while enabling flexible adjustment of the speed-up ratio."}, {"title": "C Baseline Competitors", "content": "In this section, we introduce the comparative early exiting methods in our experiments in detail.\nC.1 Signal-based Early Exiting\nAccording to the exiting signal, DeeBERT and GPFEE leverage entropy to capture the uncertainty of early predictions. Early exiting is triggered once the entropy falls below the predefined threshold. PABEE, LeeBERT, GAML-BERT, and DisentangledEE employ the cross-layer consistency as the exiting signal. The exiting criterion is met when enough (i.e. greater than the threshold) consecutive internal classifiers agree with each other. BERxiT learns to score the correctness of early predictions through a network. The inference process is terminated when the correctness score surpasses the threshold. PALBERT (Balagansky and Gavrilov 2022) performs early exiting once the cumulative distribution function of the exiting layer's probability distribution provided by neural networks exceeds the threshold."}, {"title": "C.2 Router-based Early Exiting", "content": "ConsistentEE employs reinforcement learning to train a policy network for exit decision-making and minimizes the cross-entropy loss for each sample only at its exiting classifier during training."}, {"title": "D Statistics of Failure Cases", "content": "In this section, we conduct a statistical analysis of failure cases to assess the reliability of exiting decisions. There are two types of failure cases: first, the model may prematurely emit samples with incorrect early predictions by underestimating their difficulty, which degrades task performance; second, the model may delay the exiting of samples with correct early predictions by overestimating their difficulty, which prolongs inference time. Both types can negatively impact the model's performance-efficiency trade-off.\nAccordingly, we define two metrics to statistically analyze these two types of failure cases, respectively.\n$\\bullet$ Premature Exiting Rate: the ratio of \"exit\" decisions made when the internal classifier predicts incorrectly.\n$\\bullet$ Delayed Exiting Rate: the ratio of \"continue\" decisions made when the internal classifier predicts correctly.\nExiting decisions are considered reliable only when both the Premature and Delayed Exiting Rate are sufficiently low. Figure 11 presents the failure cases statistics for COSEE and the conventional training method under various exiting signals in the SST-2 task. We observe that, compared to the conventional training method, our COSEE significantly reduces both the Premature and Delayed Exiting Rate across various exiting signals. This suggests that our COSEE facilitates the reliability of exiting decisions during inference by improving the training of multi-exit networks."}]}