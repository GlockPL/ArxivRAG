{"title": "Neural networks that overcome classic challenges through practice", "authors": ["Kazuki Irie", "Brenden M. Lake"], "abstract": "Since the earliest proposals for neural network models of the mind and brain, critics have pointed out key weaknesses in these models compared to human cognitive abilities. Here we review recent work that has used metalearning to help overcome some of these challenges. We characterize their successes as addressing an important developmental problem: they provide machines with an incentive to improve X (where X represents the desired capability) and opportunities to practice it, through explicit optimization for X; unlike conventional approaches that hope for achieving X through generalization from related but different objectives. We review applications of this principle to four classic challenges: systematicity, catastrophic forgetting, few-shot learning and multi-step reasoning; we also discuss related aspects of human development in natural environments.", "sections": [{"title": "1 The Classic Debates", "content": "As long as there has been artificial intelligence (AI), AI has been compared to its natural counterpart, human intelligence. In particular, artificial neural networks (NN; McCulloch and Pitts (1943); Rosenblatt (1958))\u2014also called connectionist models (see Glossary) and parallel distributed processing (Rumelhart et al., 1986; McClelland et al., 1986)\u2014have long been criticized for their failures to exhibit certain cognitive capabilities that are central to human intelligence.\nTo cite a few, critics have argued that neural networks lack compositional generalization, also known as systematic generalization (Fodor and Pylyshyn, 1988; Marcus, 1998; Lake and Baroni, 2018; Greff et al., 2020)\u2014the ability to manipulate complex, unseen patterns that are created through rule-like composition of simpler elements; or show catastrophic forgetting during continual learning (McCloskey and Cohen, 1989; Ratcliff, 1990; French, 1991, 1999)\u2014a crucial requirement for the lifelong acquisition of new knowledge without forgetting old skills every time a new one is learned; or they are excessively data hungry (Geman et al., 1992) unlike humans that are capable of few-shot learning\u2014the ability to learn a lot from only a few examples (Miller et al., 2000; Fei-Fei et al., 2003; Lake et al., 2011, 2015). Indeed, such capabilities appear to be fundamental not only to human cognition but also to any form of intelligence that may enable efficient, general-purpose problem"}, {"title": "2 Classic Challenges for Neural Networks", "content": "Here we briefly review four classic challenges to artificial NNs based on comparison to human cognitive capabilities\u00b9, namely: systematic generalization (Sec. 2.1), catastrophic forgetting (Sec. 2.2), few-shot learning (Sec. 2.3), and multi-step reasoning (Sec. 2.4). Their illustrations can be found in Figure 1."}, {"title": "2.1 Systematic generalization", "content": "People are adept at combining new concepts with existing concepts in rule-like ways. For instance (as illustrated in Figure 1A), once a child learns how to \u201cskip\u201d, they can effortlessly understand\n\u00b9Challenges related to biological plausibility (e.g., Crick (1989)) are outside the scope of this article."}, {"title": "2.2 Continual learning without catastrophic forgetting", "content": "People can typically learn one task and then another task afterwards, without a catastrophic drop in performance on the first task. But when artificial NNs learn one task after another, without a \u201creminder\u201d about the earlier tasks (i.e., data from the previous task becomes inaccessible; the so-called continual learning (CL) setting), there can be a catastrophic drop in performance on the earlier tasks (almost as if the previous tasks were never seen before). Thus, the problem of catastrophic forgetting (CF) has been another long-standing challenge of artificial NNs (McCloskey and Cohen, 1989; Ratcliff, 1990; French, 1991). Figure 1B shows a case of the two-task continual image classification setting as an illustration (in fact, two tasks are enough to observe CF), using example images from the Omniglot (Lake et al., 2015) and Mini-ImageNet (Vinyals et al., 2016; Ravi and Larochelle, 2017) datasets. For further technical details defining variations of CL settings in machine learning, we refer to Hsu et al. (2018) and van de Ven and Tolias (2018).\nHuman learners can accumulate new skills throughout their lifetime in ways that resemble CL, with a more gradual forgetting than the catastrophic forgetting seen in NNs (French, 1999; McClelland et al., 1995). Although a wide variety of algorithms have been proposed to overcome CF (see, e.g., Wang et al. (2024) for an overview), CL remains a challenge for neural networks. Even a toy task such as Split-MNIST (Srivastava et al., 2013; Zenke et al., 2017) is very hard for regular learning algorithms, except when allowing for some storage of data from the previous tasks to be used for replay (Robins, 1995), which, in a sense, sidesteps the fundamental challenge of CF."}, {"title": "2.3 Few-shot learning", "content": "People can learn a new concept from just one or a few examples, an ability known as few-shot learning (Miller et al., 2000; Fei-Fei et al., 2003; Fei-Fei et al., 2006; Lake et al., 2011, 2015). For instance (see Figure 1C for illustration) a person may learn enough from just one instance of a 'Segway' to grasp the boundaries of the concept, allowing them to recognize other members of the class amongst similar-looking objects. These capabilities manifest in learning new visual categories"}, {"title": "2.4 Multi-step reasoning", "content": "People can find a solution to a hard problem by logically decomposing it into a series of simpler sub-problems or steps which can be easily solved individually. This ability is characterized as multi-step reasoning. For instance, Figure 1D shows an example of a verbal logic question that is difficult to be answered immediately after reading the problem statement, but can easily be solved by enumerating and eliminating possible cases step-by-step.\nThe general ability for \u201creasoning\u201d has been another highly discussed topic in the NN research (Sun, 1995; Browne and Sun, 2001) with debates contrasting connectionist to rule-based reasoning approaches (Smith et al., 1992). Reasoning has traditionally been associated to the capacity for language and symbolic processing (see, e.g., Johnson-Laird (1983))\u2014even though more recent findings in neuroscience indicate that humans' capability for language processing and reasoning are quite distinct and independent (see, e.g., Fedorenko and Varley (2016)). From this perspective, the general critique on the ability to reason has been tightly connected to the systematicity debate (Sec. 2.1). In fact, Newell (1990) had speculated that low-level tasks such as object recognition to be well modeled by NNs, while higher-level reasoning and logical processing would require rule-based processing\u2014anticipating aspects of today's neuro-symbolic architectures and the System 1 vs. 2 distinction (Kahneman, 2011).\nThe discussion regarding multi-step reasoning in NNs has resurged with the recent advances in large language models which exhibit such capabilities to a certain extent (Nye et al., 2021; Cobbe et al., 2021; Kojima et al., 2022; Wei et al., 2022a,b; Prystawski et al., 2023). We will discuss the nuances of this observation later in Sec. 3.3."}, {"title": "3 Overcoming classic challenges through practice", "content": "Recently, Lake and Baroni (2023) and Irie et al. (2023) proposed metalearning methods to overcome two of the classic challenges of artificial neural networks (Sec. 2), systematic generalization and catastrophic forgetting, respectively. Here we describe a framework that is common to both approaches (Sec. 3.2), and review its concrete applications (Sec. 3.3). Before that, we introduce a common theme that we call the \u201cproblem of incentive and practice\u201d, which characterizes the distinguishing features of this framework compared to conventional methods of training neural networks."}, {"title": "3.1 The problem of incentive and practice", "content": "What is holding neural networks back from succeeding on these classic challenges? We highlight an overlooked issue: the target behavior we expect from the system is not sufficiently well-represented"}, {"title": "3.2 The metalearning framework", "content": "Recent work by Lake and Baroni (2023) and Irie et al. (2023) aim to overcome the systematicity and catastrophic forgetting challenges, respectively, through a meta-learning framework that combines powerful sequence-processing neural networks with judiciously constructed meta-training sequences and a meta-objective function that reflect the core objectives of the challenge to be addressed.\nLearning as sequence processing. The foundational idea is to formulate a learning task as a sequence processing problem, such that the system learns to perform a new task by observing a sequence of training examples (input/output pairs; called an \u201cepisode\u201d) (Cotter and Conwell, 1990, 1991; Younger et al., 1999; Hochreiter et al., 2001) (see Figure 3A and B for illustration). If the learner (i.e., the learning algorithm) is a neural network, by training it on many such episodes (different but related tasks), we obtain a special type of metalearning algorithm (Schmidhuber, 1987) based on sequence processing NNs that learn to learn (Harlow, 1949)2. In principle, any general-purpose sequence processing network can be used, e.g., Lake and Baroni (2023) and Irie et al. (2023) respectively use the \u201cquadratic\u201d (Vaswani et al., 2017) and \u201clinear\u201d (Schmidhuber, 1992; Katharopoulos et al., 2020; Schlag et al., 2021) variants of the now popular Transformer.\nMeta-training sequences and meta-objective function. Addressing the weaknesses of NNs through this framework involves constructing meta-training tasks (episodes) and the meta-objective function that accurately reflect the \u201cproblem statement\u201d and \u201cobjective\u201d of the classic challenge. This can usually be expressed through three components: (1) the system observes demonstrations (study examples), (2) the system receives query inputs corresponding to a \u201cchallenging\u201d task (e.g., requiring some nontrivial generalization from the study examples), and finally, (3) as a response to the queries, the system produces outputs which is compared to the desired behavior. By generating many such meta-training episodes, a NN can be more directly optimized for the desired behavior by practicing and improving its skills on these episodes.\nn=1\nMore formally, we generically define a meta-training episode as a set of three components: a \"demonstration\u201d sequence $S^{\\text{demo}}$, \u201cquery\u201d inputs $X^{\\text{query}}$, and \u201ctarget\u201d behavior $Y^{\\text{target}}$. The demonstration sequence can be generically denoted as a sequence of \u201cx-y\u201d (input-output) pairs, $S^{\\text{demo}} = \\{(x^{\\text{demo}}_n, y^{\\text{demo}}_n)\\}_{n=1}^{N^{\\text{demo}}}$, where the specific formats of \"x\" and \"y\" depend the problem at hand (as we'll see in Sec. 3.3). Similarly, the query and target can be generically described as $X^{\\text{query}} = \\{(x^{\\text{query}}_n)\\}_{n=1}^{N^{\\text{query}}}$ and $Y^{\\text{target}} = \\{(y^{\\text{target}}_n)\\}_{n=1}^{N^{\\text{target}}}$, which are sequences or sets of tokens representing some input instruction (of a challenging task) \u201cx\u201d and expected behavior \u201cy\u201d, respectively, used to evaluate of the system. $N^{\\text{demo}}$, $N^{\\text{query}}$, and $N^{\\text{target}}$ are positive integers denoting the number of elements in the respective sequences/sets. Figure 3A provides an illustration.\nThe metalearner is optimized to take the demonstration sequence and query input as simul-taneous input, and then respond to the query by outputting the right target behavior. That is, by denoting the parameter set of the metalearner as $\\theta$, we optimize $\\theta$ to maximize the probability $p_{\\theta}(Y^{\\text{target}}|S^{\\text{demo}}, X^{\\text{query}})$. This meta-objective provides an explicit \"incentive\" to the system to\n2This idea of \"metalearning sequence learners\u201d (MSL) itself is not new. Seminal work has discussed the possibility to parameterize learning dynamics using sequence processing recurrent neural networks (Cotter and Conwell, 1990, 1991; Younger et al., 1999)\u2014which refer to even older inspirations from Rich and Farrall (1964) and White et al. (1990) and a successful implementation by Hochreiter et al. (2001). Recent work has revived this idea in the deep learning era of the last decade (Bosc, 2015; Santoro et al., 2016; Duan et al., 2016; Wang et al., 2017; Munkhdalai and Yu, 2017; Mishra et al., 2018), and MSL is now often called \"in-context learning\" in the context of large language models (Brown et al., 2020)."}, {"title": "3.3 Applications to overcoming classic challenges", "content": "Systematic generalization. Lake and Baroni (2023)'s approach to optimizing NNs for making stronger systematic generalizations can be described as follows. We use the task of Figure 1A as an illustrative example of what a meta-training episode would look like. For a given episode:\n1. The demonstration sequence $S^{\\text{demo}}$ consists of input primitives paired with their output meanings (e.g., \u2018skip \u2192 A' and 'jump \u2192 B\u2032, where \u2018A' and 'B' are symbols representing the actual behaviors corresponding to \u2018skip' and 'jump', and '\u2192' is used to denote the input-output boundary), as well as demonstrations of more complex, compositional inputs and their outputs (e.g., \u2018skip twice \u2192 A A').\n2. The query input $X^{\\text{query}}$ contains an example of a novel composition (e.g., \u2018jump twice') to"}, {"title": "Continual learning without catastrophic forgetting.", "content": "Similarly, Irie et al. (2023)'s method to overcome catastrophic forgetting can be obtained as a special case of the framework above as follows. As an illustrative example, we use the 2-task continual image classification example of Figure 1C; we refer to the two tasks as Task 1 and 2, respectively.\n1. We define the demonstration sequence $S^{\\text{demo}}$ as a sequence of image-label example pairs constructed by concatenating multiple sub-sequences, each containing examples from a single task. For instance, in the 2-task example, there are two sub-sequences; the first sequence only contains examples from Task 1, and the second one containing examples from Task 2 only. This mirrors the exclusive sequential training example presentations in the continual learning.\n2. The query input $X^{\\text{query}}$ (which is a set of tokens, as the exact order does not matter here) contains unseen images to be classified from all the tasks. In the 2-task example, this corresponds to images from both Task 1 and 2.\n3. The target $Y^{\\text{target}}$ contains the corresponding labels for the query images (the exact order does not matter as long as they are associated to the corresponding query images).\nIn this framework, the objective function includes the evaluation of the model performance on all the tasks; this explicitly penalizes the model for forgetting the previous task, thereby teaching the system the core problem of continual learning.\nRegarding the model architecture, Irie et al. (2023) use a variant of linear Transformers (Irie et al., 2022). Unlike the regular Transformers which virtually store all inputs within a context window (which may potentially be viewed as a violation of the CL assumption on no access to the training data of older tasks), this model only maintains a fixed-size state as its memory, like standard recurrent neural network (RNNs); which is updated every time an input is fed to the model."}, {"title": "Few-shot classification.", "content": "Few-shot learning is another classic challenge that can be approached with a straightforward application of the MSL framework of Sec. 3.2. An episode of few-shot image classification can be stated as follows (see the illustration in Figure 3B for an example of few-shot classification of handwritten characters (Lake et al., 2015)).\n1. We define the demonstration \u201csequence\" $S^{\\text{demo}}$ consisting of a K image-label example pairs for each class (this is rather a \u201cset\u201d as the order does not matter), where K is a positive integer denoting the number of training examples available per class in the K-shot learning scenario.\n2. The query input $X^{\\text{query}}$ contains unseen images to be classified.\n3. The target $Y^{\\text{target}}$ contains the labels corresponding to the query images (the order does not matter as long as they are associated to the correct query images). These labels can change for every episode.\nThis framework meta-trains the system to make predictions only after having observed a few training examples; thereby, it explicitly incentivizes the system to do its best to learn new concepts from just a few examples.\nThis application of the MSL framework Sec. 3.2 predates the other applications we consider here, and the current popularity of in-context learning. For instance, Santoro et al. (2016) trained dif-ferentiable neural computers (Graves et al., 2016), a type of memory-augmented RNN architecture, for in-context few-shot image classification; as one of the contemporary methods that implemented memory-based metalearning (Vinyals et al., 2016; Ravi and Larochelle, 2017; Snell et al., 2017), and it was later extended to the Transformer-family of neural networks (Mishra et al., 2018). Earlier, Hochreiter et al. (2001) trained long short-term memory (LSTM; Hochreiter and Schmidhuber (1997); Hochreiter (1991)) RNNs for in-context few-shot regression, achieving improved sample efficiency compared to the regular gradient-descent learning algorithm.\nAs demonstrated by these references, few-shot learning was an early example of the classic challenges that have been addressed through metalearning sequence learners. Notably, the system-aticity and continual learning settings above also incentivize the model for few-shot learning, as the typical demonstration sequences only contain a few examples, similar to this few-shot learning case."}, {"title": "Multi-step reasoning.", "content": "Multi-step reasoning in NNs can also be analyzed through the framework of incentive and practice. Although not currently studied in an explicitly metalearning setting (the demonstration sequence is often empty), it can still be positioned with the general framework (Sec. 3.2) (see Figure 3E). We use the task from Figure 1D as a representative example; the setting can be formulated as follows.\n1. The demonstration sequence $S^{\\text{demo}}$ is typically empty. More generally, $S^{\\text{demo}}$ could contain examples of reasoning tasks, together with specific intermediate steps for solving them.\n2. The query input $X^{\\text{query}}$ is the task statement (i.e., the text in the box in Figure 1D).\n3. The target $Y^{\\text{target}}$ is the entire response text containing the reasoning steps and the answer."}, {"title": "4 Discussion", "content": ""}, {"title": "4.1 Human Development in Natural Environments", "content": "The metalearning framework discussed here addresses the problem of incentive and practice by training NNs on synthetically generated meta-training episodes; that is, asking the models to find solutions to their weaknesses through practice.\nDoes human learning and development in natural environments allow for related kinds of incentives and practice, with natural rather than synthetic episodes of experience? As discussed in Lake and Baroni (2023), one possibility is that natural environments provide humans with scenarios where there is a pressure to learn new concepts quickly (few-shot learning) and use them compositionality right away (systematic generalization), and also previous tasks can be relevant later on (continual learning); and these skills themselves improve over the course of development.\nThere are developmental findings consistent with this idea. Children become better word learners over the course of development (Bergelson, 2020), and childrens' experience has been linked to improving abilities for few-shot learning (Smith et al., 2002). Relating to systematic generalization, pre-school children, but not infants, seem to make meaningful inferences about visual function composition (Piantadosi and Aslin, 2016; Piantadosi et al., 2018), a change that could be linked to maturity and/or experience. For multi-step reasoning, education at school provides humans with opportunities to learn and practice these skills. Similarly, \u201cmemory\u201d is also a skill that humans hone through exercises. It is also plausible that people realize the detrimental consequences of forgetting previously learned skills, and leverage such learning experiences to improve future learning. The metalearning framework for artificial NNs provides a means for understanding aspects of development related to learning-to-learn (Harlow, 1949), although more work is needed to study how natural incentives and environments might provide the right ingredients for a metalearning perspective on development (see also recent discussions by Nussenbaum and Hartley (2024a,b); Russin et al. (2024))."}, {"title": "4.2 Metacognitive Perspectives", "content": "Besides incentive and practice, another ingredient that is potentially related to how people succeed is \u201cknowledge\u201d about the problem they are aiming to solve. The continual learning case is again an illustrative example; considering the task of Figure 1B, if we naively memorize image-to-label mappings for Task 1, and then do the same for Task 2, we may fail at Task 1 evaluation after Task 2, unless we were informed that we would be tested on Task 1 afterward. Similarly, prior neural network models lack such information about the goal and nature of the problem they are tasked to solve, and suffer from the \u201cproblem awareness\u201d problem (here our usage of the word \u201cawareness\u201d purely refers to knowledge of information; while computational roles and advantages of \u201cconsciousness\u201d or \u201cself-awareness\u201d in these tasks may also be interesting topics of discussion, they are beyond the scope of this work).\nMore generally, learning and behavior in humans are highly influenced by their understanding and knowledge about their dynamically changing problems to be solved with specific goals/sub-goals and situations in an introspective fashion. Developing computational mechanisms to integrate such metacognitive knowledge into neural network models of cognition is an interesting open avenue for both future AI and cognitive science research that aims at advancing computational models of human cognition."}, {"title": "5 Concluding Remarks", "content": "Neural networks are flexible computational models capable of learning to solve arbitrary problems specified by an optimization problem with an objective function. Conversely, a reasonable approach to build a neural network model of human cognition with certain capabilities is to define an appropriate optimization framework with an objective that promotes the target cognitive behavior. In the end, human cognition is also influenced by optimization processes at multiple time scales, e.g., at the developmental, educational, and evolutionary levels.\nWe highlighted how this principle has been overlooked in prior debates regarding the capa-bilities of neural networks. Indeed, metalearning can be a fruitful framework for allowing neural networks to find data-driven solutions to their own limitations, assuming the right incentives and opportunities for practice are in place. Beyond the classic debates, this echoes recent work advocating for metalearning as a paradigm for building more flexible models of human cognition (Nussenbaum and Hartley, 2024a,b; Griffiths, 2020; Binz et al., 2023; McCoy and Griffiths, 2023; Marinescu et al., 2024; Ong et al., 2024; Russin et al., 2024), especially when it is easier to specify the desired inductive bias through meta-training episodes than it is through architectural/prior constraints.\nA key issue for future work will be to understand whether the structure of natural incentives and environments provide the right ingredients for metalearning, as part of the story of human learning and development (further questions and new avenues for future research are listed as Outstanding Questions)."}]}