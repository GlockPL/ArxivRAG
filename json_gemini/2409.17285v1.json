{"title": "SpoofCeleb: Speech Deepfake Detection and SASV In The Wild", "authors": ["Jee-weon Jung", "Yihan Wu", "Xin Wang", "Ji-Hoon Kim", "Soumi Maiti", "Yuta Matsunaga", "Hye-jin Shim", "Jinchuan Tian", "Nicholas Evans", "Joon Son Chung", "Wangyou Zhang", "Seyun Um", "Shinnosuke Takamichi", "Shinji Watanabe"], "abstract": "This paper introduces SpoofCeleb, a dataset designed for Speech Deepfake Detection (SDD) and Spoofing-robust Automatic Speaker Verification (SASV), utilizing source data from real-world conditions and spoofing attacks generated by Text-To-Speech (TTS) systems also trained on the same real-world data. Robust recognition systems require speech data recorded in varied acoustic environments with different levels of noise to be trained. However, existing datasets typically include clean, high-quality recordings (bona fide data) due to the requirements for TTS training; studio-quality or well-recorded read speech is typically necessary to train TTS models. Existing SDD datasets also have limited usefulness for training SASV models due to insufficient speaker diversity. We present SpoofCeleb, which leverages a fully automated pipeline that processes the VoxCeleb1 dataset, transforming it into a suitable form for TTS training. We subsequently train 23 contemporary TTS systems. The resulting SpoofCeleb dataset comprises over 2.5 million utterances from 1,251 unique speakers, collected under natural, real-world conditions. The dataset includes carefully partitioned training, validation, and evaluation sets with well-controlled experimental protocols. We provide baseline results for both SDD and SASV tasks. All data, protocols, and baselines are publicly available at https://jungjee.github.io/spoofceleb.", "sections": [{"title": "I. INTRODUCTION", "content": "THE quality of synthetic speech has improved rapidly in recent years, driven by advancements in technologies such as flow matching, neural codecs, and speech-language modeling [1]-[3]. These innovations have significantly enhanced the naturalness and intelligibility of generated speech. Furthermore, the increasing availability of open sources and APIs for Text-To-Speech (TTS) systems has made high-quality synthetic speech more accessible to the general public [4], [5].\nAlthough originally developed for positive applications, this technology is increasingly being exploited for malicious purposes [6], [7]. Synthetic speech generated with harmful intent, often referred to as spoofing, is being used to deceive individuals in scenarios such as voice phishing (or vishing). Additionally, spoofing undermines the reliability of speech"}, {"title": "II. RELATED WORKS", "content": "Datasets for SDD and the generation-recognition trade-off. To safeguard the authenticity of speech, several corpora have been published to support research in SDD [9]\u2013[12], [18], [21], [23], [26]. One of the most critical decisions when creating these datasets is the selection of the source data (i.e., bonafide speech). This decision involves a trade-off, which we refer to as the \u201cgeneration-recognition trade-off.\u201d\nOn the recognition side, for both SDD and SASV, incorporating data with diverse noise, reverberation, and varied domains is essential for training robust models. It is well known that recognition models trained solely on clean speech often struggle to generalize effectively to noisy environments during inference [18]. While data augmentation techniques\nThe development of this pipeline is extensive, and the resulting bona fide speech data can serve other purposes, such as advancing research on TTS systems trained on noisy, in-the-wild data. We detail this aspect in a separate work, referring to the dataset as TTS In The Wild (TITW) [15].\ncan help mitigate this issue [31], the most effective solution is to use training data drawn from a wide range of real-world sources.\nConversely, traditional TTS training requires a carefully curated and recorded dataset. Sentence prompts must be selected to ensure comprehensive phonetic coverage [32], and recordings are typically made by voice talents in clean environments, ideally in a single anechoic studio. These recordings are of high studio quality and are carefully articulated, but they are not scalable. For instance, the well-known CMU Arctic database includes recordings from fewer than 10 voice talents, each reading approximately 1,000 speech prompts [32]. Modern TTS systems, however, often require significantly more training data. Instead of relying on these small-scale, TTS-specific databases, contemporary models frequently use audiobook datasets (e.g., MLS [33]), which, while not studio-grade, consist of relatively clean audiobook recordings made by numerous readers in their homes or offices.\nExisting SDD datasets tend to lean towards the generation side of the generation-recognition trade-off. They use source datasets that consist of either studio-quality or high-quality speech, facilitating the training of TTS and Voice Conversion (VC) systems and the successful generation of spoofed speech samples. However, both the bonafide and spoofed speech in these datasets are exceedingly clean, making them far from real-world, noisy speech data.\nSpoofCeleb is the first dataset to utilize real-world, noisy, and reverberant data from VoxCeleb1 as the source for training and synthesizing spoofed speech. We tackle the"}, {"title": "III. SOURCE DATASET: TITW", "content": "Our goal is to create a dataset for SDD and SASV using VoxCeleb1 as the source so that both bonafide and spoofed samples reflect real-world scenarios. However, VoxCeleb1 is not suitable for direct use in TTS training.2 The challenges with VoxCeleb1 are multifaceted; for example, the speech\n2Our preliminary attempts to train TTS systems using the raw VoxCeleb1 data without further processing were unsuccessful.\nsamples often (i) contain overly emotional expressions, (ii) include extended non-speech segments, or (iii) have excessively long durations. To address these issues, we developed a fully automated pipeline that processes VoxCeleb1 into the Text-To-Speech In The Wild (TITW) dataset, which can be used for TTS training.\nFigure 1-(a) illustrates the automated processing pipeline we developed to generate the TITW dataset. The pipeline begins by transcribing and obtaining word-level alignment using the WhisperX toolkit [36]. This toolkit transcribes the speech using the pre-trained Whisper Large v2 Automatic Speech Recognition (ASR) model [37], while word-level segmentation is derived from another phoneme-based ASR model. For a small subset of randomly selected samples, we also transcribed the text using the OWSMv3.1 model [38] and cross-checked the accuracy of the transcriptions. We then segmented the utterances from VoxCeleb1 whenever a silence longer than 500ms was detected, resulting in multiple segments from a single utterance. Next, we applied a series of heuristic-driven rules \u2013 developed through several iterations of TTS training \u2013 to filter the data. We discarded any samples that (i) were non-English, (ii) were shorter than 1 second or longer than 8 seconds, (iii) contained one or more words with a duration exceeding 500ms, or (iv) had empty transcriptions.\nAfter completing the initial processing steps (referred to as TITW-Hard in [15]), we conducted multiple iterations of TTS training trials. Despite these efforts, training remained extremely challenging for most TTS systems, with only a few recent models showing success. The generated speech was still insufficient to deceive pre-trained ASV systems, as measured by the SPooF Equal Error Rate (SPF-EER) metric [13].3 To address this, we applied speech enhance- ment using a pre-trained model, DEMUCS, and excluded samples with DNSMOS \u201cBAK"}, {"title": "IV. SPOOFCELEB", "content": "Figure 1-(b) illustrates the composition of the SpoofCeleb dataset. The TITW dataset serves as the foundation for training multiple TTS systems. These systems are then used to synthesize spoofed speech samples, which are combined with the bona fide speech samples from TITW to form the complete SpoofCeleb dataset. To achieve this, we utilize 4 acoustic models, 6 waveform models (i.e., vocoders), and 5 End-to-End (E2E) models. Unless mentioned otherwise, all models are trained from scratch using the TITW-Easy data.\n3The SPF-EER is calculated by assessing an ASV system's ability to correctly accept target trials while rejecting spoofed non-target trials. Bona fide non-target trials are excluded from this protocol, as the focus is solely on evaluating the ASV system's spoofing robustness."}, {"title": "A. Acoustic models", "content": "Training acoustic models using in-the-wild data was one of the most challenging aspects of the SpoofCeleb dataset collection. We applied several criteria to evaluate the success of the training, including (but not limited to) speech intel- ligibility, measured by Word Error Rate (WER), noisiness, assessed by DNSMOS, and speaker identity, evaluated using SPF-EER. Among these metrics, SPF-EER was prioritized as the primary measure, since the most critical factor in a spoofing attack is whether it can deceive an ASV system. The final models that were successfully trained include TransformerTTS, GradTTS, Matcha-TTS, and BVAE-TTS.\nTransformerTTS [39] is an autoregressive TTS model that generates mel-spectrograms from textual input using a transformer-based architecture. The model employs a se- quence of transformer encoder and decoder blocks with multi-head self-attention. We trained TransformerTTS using the ESPnet toolkit [40].4\nGradTTS. [41] is a TTS model with a score-based decoder that generates mel-spectrograms by gradually transforming noise predicted by the text encoder. During inference, we set the denoise step to 50 to ensure high-quality speech generation. We utilize the official implementation and follow the default settings.5\nMatcha-TTS. [42] is an efficient non-autoregressive TTS model based on an optimal-transport conditional flow match- ing decoder [1]. Unlike score-based models, it constructs a more direct sampling trajectory, allowing for high-quality generation with fewer sampling steps. We utilized the official implementation.6\nBVAE-TTS. [43] utilizes a Bidirectional-inference Vari- ational AutoEncoder (BVAE) to model the hierarchical relationships between text and speech. By leveraging the attention maps generated by BVAE-TTS, the model jointly trains a duration predictor, enabling robust and efficient non- autoregressive speech generation. We utilized the official implementation.7"}, {"title": "B. Waveform models", "content": "The training of waveform models was comparatively straightforward. We employed a mix of both classic and re- cent waveform models, including DiffWave, HiFiGAN, Par- allel WaveGAN, Neural source-filter model with HiFi-GAN discriminators (NSF-HiFiGAN), BigVGAN, and WaveG- lows.\nDiffWave. [44] is a diffusion probabilistic model designed for both conditional and unconditional waveform generation. We utilize the official implementation.8"}, {"title": "C. E2E and speech-language models with neural codecs", "content": "While two-stage TTS pipelines have proven effective for modeling speech from text, they often suffer from poor quality due to the mismatch between acoustic and waveform models. Waveform models are trained on predefined features but must process the outputs generated by acoustic models during inference, leading to potential inconsistencies. To address this issue, several E2E models have been proposed, and we have successfully trained multiple E2E models using the TITW dataset.\nSpeech-Language Models (SpeechLMs) represent an emerging category of TTS models. Similar to language mod- els in natural language processing, they are trained to predict tokens, in this case, tokens of neural codecs, which are then decoded via a neural codec system's decoder. Unlike acoustic models, which can be paired with any compatible waveform model, SpeechLMs rely on a predetermined decoder based"}, {"title": "D. Attack generation, partitioning, and protocols", "content": "Diverse combinations of acoustic and waveform models, alongside E2E and SpeechLM models, result in a total of 23 spoofing attacks. This approach is inspired by previous work, which demonstrated that both acoustic and waveform models impact the perceptual quality of synthesized speech [54].\nData partitioning for SpoofCeleb required a more sophis- ticated approach compared to existing ASV or SDD datasets. An SDD dataset only needs to consider the binary bonafide or spoof label, while an ASV dataset focuses on speaker identities. SpoofCeleb, as a dataset for both SDD and SASV, must account for both bona fide/spoof labels and speaker identities simultaneously.\nSpeakers. For the speaker partitioning, we divide the 1,251 speakers in the bona fide data into three sets: 1,171 for training, 40 for validation, and 40 for evaluation. This"}, {"title": "V. Baselines", "content": "A. SDD\nTwo E2E SDD models, RawNet2 [56] and AASIST [57], are used as the baseline systems. The RawNet2 model for SDD is an adapted version of RawNet2 originally designed for ASV. It features an input layer that processes raw waveforms directly and uses convolution-based residual blocks. Frame- level representations are aggregated, projected, and then passed through a binary classification head.\nAASIST is one of the most widely adopted SDD models in recent literature. Like RawNet2, it includes an input layer that processes raw waveforms and utilizes convolution-based residual blocks. However, unlike RawNet2, AASIST incor-"}, {"title": "B. SASV", "content": "We employ three models as SASV baselines, all of which utilize the SKA-TDNN architecture [58]. These models are used to assess the impact of different training data and scenarios. SKA-TDNN is a convolution-based model with residual connections, incorporating dedicated modules and architectural design choices for multi-scale processing. It is an advanced version of the ECAPA-TDNN architecture [59].\nAmong the three SASV baseline models, the first model (\u201cConventional ASV\u201d) is trained as a conventional ASV system using the VoxCeleb1&2 datasets, without consider- ing spoof robustness. We utilize a pre-trained model from ESPnet-SPK [60]. The second model (\u201cSASV trained on out-of-domain data\u201d) is trained as an SASV model but uses out-of-domain data from the ASVspoof2019 logical access dataset [9]. We employ a pre-trained model from [61]. The third model (\u201cSASV trained on SpoofCeleb\u201d) is trained as an SASV model using the training set from SpoofCeleb."}, {"title": "VI. Metrics", "content": "A diverse set of metrics is employed to evaluate the SpoofCeleb dataset, as well as the SDD and SASV models. To assess the quality of the speech samples and the strength of the attacks, we use SPF-EER, Mean Cepstral Distortion (MCD), UTMOS [62], DNSMOS [63], and Word Error Rate (WER), with WER evaluated using the OpenAI Whisper- Large model [64]. For evaluating the performance of SDD baselines, we use Equal Error Rate (EER) and the min Detection Cost Function (minDCF) [65]. To assess the SASV baselines, we adopt the recently proposed architecture- agnostic Detection Cost Function (min a-DCF) [66], along with Speaker Verification EER (SV-EER) and SPOOF EER (SPF-EER)."}, {"title": "VII. Results", "content": "A. Spoofing attacks\nTable 4 presents various metrics to assess the speech quality of the 23 synthesized spoofing attacks and how effectively they threaten ASV systems. SPF-EER is the most critical metric, as it measures the extent to which the generated attacks can deceive existing ASV systems. We evaluated SPF-EER using a pre-trained RawNet3 model [68], which is publicly available through ESPnet-SPK [60].\nIn the top row, the speech quality evaluations for A00 (bona fide speech) are provided as reference values. The results confirm that the spoofing attacks in SpoofCeleb are highly threatening, with most attacks achieving an SPF-\nEER over 20%. Additionally, the majority of attacks exhibit relatively minor degradation in UTMOS and DNSMOS, in- dicating the high quality of the synthesized speech samples. Furthermore, intelligibility, measured by WER, shows that for most attacks, there is no more than a 10% deterioration in performance."}, {"title": "B. SDD", "content": "Table 6 presents the results of four baseline SDD systems. We evaluate two SDD models, RawNet2 and AASIST, trained on two different datasets. The models trained on the ASVspoof2019 logical access dataset are used to assess the zero-shot performance on SpoofCeleb validation and evalu- ation SDD protocols. The other two models demonstrate the performance of systems trained on in-domain SpoofCeleb training data.\nThe zero-shot results in the top two rows indicate that existing SDD models not trained on in-the-wild data strug- gle to distinguish between spoofed samples and bonafide speech. In rows 3 and 4, we observe a significant improve- ment in performance when these models are trained using the SpoofCeleb training data, highlighting the importance of training SDD models on in-the-wild data. However, RawNet2's result in row 3 is unexpected, as it shows better performance on the evaluation set than on the validation set while the evaluation set includes totally unknown attacks. To further investigate, we conduct an analysis of the attack-wise results.\nTable 7 presents the attack-wise performance of the RawNet2 baseline SDD model trained on the SpoofCeleb training set. Attacks A06 and A07 are classified as known attacks. Attacks A11 to A18 are partially unknown; in these cases, either the acoustic or waveform model is known, or"}, {"title": "C. SASV", "content": "Table 8 presents the performance of three SASV baselines on the SpoofCeleb validation and evaluation protocols. Min a-DCF assesses the overall performance, while SV-EER and SPF-EER evaluate the systems' ability to reject bona fide and spoof non-target trials, respectively.\nAs expected, a conventional ASV system that does not account for spoof attacks, shown in the first row, fails to reject synthesized speech samples, with an a-DCF exceeding 0.49 on both the validation and evaluation sets. However, it performs well at rejecting bona fide non-target trials. The results in the second row show an improvement in a-DCF for the validation set, but even worse performance on the evaluation set. Both SV-EER and SPF-EER remain very high, indicating that the system trained for SASV with out- of-domain data struggles to reject both types of non-target trials. a-DCF of 0.9998 also signifies that the model fails to find an operating point where it can reject both types of non-target trials. Finally, when trained on the SpoofCeleb training data, the a-DCF on the evaluation set drops to its lowest value (0.2902), and both SV-EER and SPF-EER are more balanced compared to row 1, where the system was only capable of rejecting bonafide non-target trials."}, {"title": "VIII. Conclusion and remarks", "content": "This paper introduces SpoofCeleb, a dataset for SDD and SASV based on in-the-wild data. To create a dataset that incorporates real-world conditions, we developed a fully automated pipeline to process the VoxCeleb1 dataset, making it possible to use it for training TTS systems. The resulting bona fide data, TITW, also serves as a standalone dataset to encourage the training of TTS models using in-the-wild data.\nWe further trained 23 TTS systems, partitioning TITW and the TTS systems into the SpoofCeleb dataset, which includes training, validation, and evaluation partitions. Protocols were defined to train and test both SDD and SASV models, and baseline systems for SDD and SASV were established, trained, and evaluated.\nWhile numerous datasets exist for SDD, many are limited in scale or speaker diversity, which has hindered research on single SASV models. We hope SpoofCeleb serves as the first dataset with enough data to effectively train single SASV systems. Yet, SpoofCeleb has its limitations. In the experiments, some spoofing attacks are shown to be less challenging, as the wild nature of the TITW data compli- cates the training of robust TTS systems. Future work will focus on advancing TTS training techniques that can better leverage this challenging in-the-wild data."}]}