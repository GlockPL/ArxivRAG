{"title": "CCS: Controllable and Constrained Sampling with Diffusion Models via Initial Noise Perturbation", "authors": ["Bowen Song", "Zecheng Zhang", "Zhaoxu Luo", "Jason Hu", "Wei Yuan", "Jing Jia", "Zhengxu Tang", "Guanyang Wang", "Liyue Shen"], "abstract": "Diffusion models have emerged as powerful tools for generative tasks, producing high-quality outputs across diverse domains. However, how the generated data responds to the initial noise perturbation in diffusion models remains under-explored, which hinders understanding the controllability of the sampling process. In this work, we first observe an interesting phenomenon: the relationship between the change of generation outputs and the scale of initial noise perturbation is highly linear through the diffusion ODE sampling. Then we provide both theoretical and empirical study to justify this linearity property of this input-output (noise-generation data) relationship. Inspired by these new insights, we propose a novel Controllable and Constrained Sampling method (CCS) together with a new controller algorithm for diffusion models to sample with desired statistical properties while preserving good sample quality. We perform extensive experiments to compare our proposed sampling approach with other methods on both sampling controllability and sampled data quality. Results show that our CCS method achieves more precisely controlled sampling while maintaining superior sample quality and diversity.", "sections": [{"title": "1. Introduction", "content": "Recently, diffusion models achieve remarkable success in generative tasks such as text-to-image generation, audio synthesis (Kong et al.; Rombach et al., 2022), as well as conditional generation tasks including inverse problem solv-"}, {"title": "2. Background", "content": ""}, {"title": "2.1. Diffusion Models", "content": "Diffusion models consists of a forward process that gradually adds noise to a clean image, and a reverse process that denoises the noisy images (Song et al., 2020; Song & Ermon, 2019). The forward model is given by\n$x_t = \\sqrt{1 - 0.5\\beta_t}x_{t-1} + \\sqrt{\\beta_t}\\Delta_t w$ where $w \\in N(0, 1)$ and $\\beta(t)$ is the noise schedule of the process. The distribution of $x_0$ is the clean data distribution, while the distribution of $x_T$ is approximately a standard Gaussian distribution. When we set $\\Delta t \\rightarrow 0$, the forward model becomes\n$dx_t = -0.5\\beta_t x_t dt + \\sqrt{\\beta_t} dw_t$, which is a stochastic differential equation (SDE). The reverse of this SDE is given by:\n$dx_t = (\\frac{\\beta(t)}{2}x_t - \\beta(t) \\nabla_{x_t} \\log p_t(x_t)) dt + \\sqrt{\\beta(t)} dw$.\nOne can training a neural network to learn the score function $\\nabla_{x_t} \\log p_t(x_t)$. However, this formulation involves running many timesteps with high randomness. We can also compute the equivalent Ordinary Differential Equation (ODE) form to the SDE, which has the same marginal distribution of $p(x_t)$. A sampling process, called denoising diffusion implicit models (DDIM), modifies the forward process to be non-markovian, so as to form a deterministic probability-flow ODE for the reverse process (Song et al., 2021). In this way, we are able to achieve significant speed-up sampling. More discussion on this can be found in Section 3."}, {"title": "2.2. Constrained Generation with Diffusion Models", "content": "Constrained generation requires to sample $x_0$ subject to certain conditions or measurements $y$. The conditional score at $T$ can be computed by the Bayes rule, such that\n$\\nabla_{x_t} \\log p_t(x_t | y) = \\nabla_{x_t} \\log p_t(x_t) + \\nabla_{x_t} \\log p_t(y | x_t)$.\n(1)"}, {"title": "2.3. Noise Perturbation in Diffusion Models", "content": "Noise adjustment for diffusion models has been explored in image editing, video generation, and other applications (Liu et al., 2024; Zhang et al., 2023a; Chung et al., 2024; Guo et al., 2024; Wu & De la Torre, 2023; Zheng et al., 2024) for changing the style or other properties of the generated data. However, a principled study on how the noise adjustment affects the samples is limited in diffusion models. Recently, Chen et al.; Wang et al. (2024) observe the local linearity and low-rankness of the posterior mean predictor $x_0$ based on $x_t$ in large timesteps, but this study cannot extend to the analysis of generated samples. In this work, we investigate how initial noise perturbations affect the samples generated from the diffusion model in the ODE sampling setting."}, {"title": "3. Influence from Initial Noise Perturbation", "content": "This section analyzes how small perturbations in the input noise affect the generation data under the DDIM sampling framework. We show that a slight change in the initial noise leads to an approximately linear variation in the sampled images. This result is quantified from two perspectives: the discretized DDIM sampling process (Song et al., 2021) and the associated continuous-time ODE. Our mathematical analysis relies on minimal assumptions, which also serves as the foundation for our proposed CCS algorithm in Section 4."}, {"title": "3.1. Preliminary: DDIM Sampling", "content": "Fix the total sampling timesteps $T$ and an initialization noise sample $x_T$, Song et al. (2021) generates samples from the backward process $X_T \\rightarrow X_{T-1} \\rightarrow \\cdots \\rightarrow x_0$ using the following recursive formula:\n$x_{t-1} = \\sqrt{\\alpha_{t-1}} ( \\frac{x_t - \\sqrt{1-\\alpha_t} \\epsilon_{\\theta}(x_t)}{\\sqrt{\\alpha_t}} ) + \\sqrt{1 - \\alpha_{t-1} - \\sigma_t^2} \\epsilon_{\\theta}(x_t) + \\sigma_t \\epsilon_t$,\n(2)\nwhere $\\alpha_t$ corresponds to the noise schedule in DDPM, $\\epsilon_{\\theta}(x_t)$ is the predicted noise given by the pre-trained neural network with parameter $\\theta$, $\\epsilon_t$ is the standard Gaussian noise, and $\\sigma_t$ is a hyperparameter. The DDIM sampler (Song et al., 2021) sets $\\sigma_t = 0$ to make the backward process deterministic once $x_T$ is fixed. It is known (e.g., eq (11) of Dhariwal & Nichol (2021b)) that predicting the noise is equivalent to predicting the score function up to a normalizing factor, i.e.,\n$\\epsilon_{\\theta}(x_t) \\sim -\\sqrt{1-\\alpha_t} \\nabla_{x_t} \\log p_t(x_t)$. By setting $\\sigma_t = 0$ and substituting $\\epsilon_{\\theta}(x_t)$ with its corresponding estimand, we obtain the idealized DDIM process:\n$x_{t-1} = \\sqrt{\\frac{\\alpha_{t-1}}{\\alpha_t}} (x_t + (\\sqrt{\\alpha_t} - 1) \\nabla_{x_t} \\log p_t(x_t))\n- \\sqrt{(1 - \\alpha_{t-1})(1 - \\alpha_t)} \\nabla_{x_t} \\log p_t(x_t)$.\n(3)\nIf we treat the index $t$ as a continuous variable (and rewrite $\\alpha_t$ as $\\alpha(t)$ to avoid confusion), it is known in eq (14) of (Song et al., 2020) that DDIM is the Euler-discretization of the following (backward) ODE:\n$\\frac{dx_t}{dt} = -\\frac{\\epsilon_{\\theta}(x_t)}{\\sqrt{\\sigma^2(t) + 1}} \\frac{d\\sigma(t)}{dt}$,\nwhere $x_t = \\frac{x_t}{\\sqrt{\\alpha(t)}}$, $\\sigma(t) := \\sqrt{(1 - \\alpha(t))/\\alpha(t)}$. Thus, we can similarly write the idealized ODE as:\n$dx_t = \\sqrt{1 - \\alpha(t)} \\nabla \\log P_t(\\frac{x_t}{\\sqrt{\\alpha(t)}}) \\frac{d\\sigma(t)}{\\sqrt{\\sigma^2(t)+1}} dt$.\n(4)\nWe now examine how a small perturbation $x_T \\rightarrow x_T + \\Delta x$ would affect the output sample at time $t = 0$ through both the discrete (3) and continuous time (4) perspectives."}, {"title": "3.2. DDIM Discretization", "content": "We simplify notations and write the idealized DDIM (3) as\n$x_{t-1} := \\mathcal{N}_t x_t + \\Lambda_t \\nabla_{x_t} \\log p_t(x_t)$,\nwhere $\\mathcal{N}_t = \\sqrt{\\frac{\\alpha_{t-1}}{\\alpha_t}}$ and $\\Lambda_t = \\sqrt{(1 - \\alpha_{t-1})(1 - \\alpha_t)}$.\nFix $T$, we are interested in studying $x_0(x_T, T)$ and $x_0(x_T + \\lambda \\Delta x, T)$, where $\\Delta x$ is a unit direction and $\\lambda$ is a (small) real number. The notation $x_0(a, t)$ stands for the endpoint $x_0$ by applying the idealized DDIM procedure with $x_t = a$ at timestep $t$. We have:\nProposition 1. With all the notations defined as above, assuming $\\log p_t$ is second-order differentiable for every $t > 1$, there exists a matrix-valued function $\\Upsilon_0$ such that\n$X_0(x_T + \\lambda \\Delta x, T) = x_0(x_T) + \\lambda \\Upsilon_0(x_T) \\Delta x + o(\\lambda)$.\nIn turn,\n$||x_0(x_T + \\lambda \\Delta x, T) - x_0(x_T, T)||_2 = \\|\\lambda \\Upsilon_0(x_T) \\Delta x\\|_2 + o(\\lambda)$.\nProposition 1 shows that a linear perturbation of the input with magnitude $\\lambda$ and direction $\\Delta x$ results in an approximately linear change in the output, with magnitude $\\|\\Upsilon_0(x_T) \\Delta x\\|_2$ and direction $\\Upsilon_0(x_T) \\Delta x$. Our assumption is based solely on the second-order smoothness of the score, which is weaker than most existing assumptions depending on the data distribution $p_0$. For example, our assumptions hold under common conditions in the literature, such as the manifold hypothesis (De Bortoli, 2022; Song & Ermon, 2019) or the mixture of (low-rank) Gaussian assumption (Gatmiry et al., 2024; Chen et al., 2024b;a)."}, {"title": "3.3. ODE Stability", "content": "Let $\\bar{x}_0(x, T)$ be the solution of (4) with initialization $x_T = x$ (i.e., $\\bar{x}_T = x / \\sqrt{\\alpha(T)}$) at timestep $T$, and $\\bar{x}_0(x, T) = \\alpha(0)\\bar{x}_0(x, T)$. With some technical assumptions that is detailed in Appendix A.2, we have the following:\nProposition 2. There exists a matrix-valued function $\\Psi_0$ such that:\n$X_0(x_T + \\Delta x, T) = \\bar{x}_0(x_T) + \\lambda \\Psi_0(x_T) \\Delta x + o(\\lambda)$.\nIn turn,\n$X_0(x_T + \\lambda \\Delta x, T) = \\bar{x}_0(x_1) + \\lambda \\gamma(0)\\Psi_0(x_T) \\Delta x + o(\\lambda)$.\nProposition 2 mirrors Proposition 1 but is formulated in the continuous-time ODE setting. Its proof relies on ODE stability theory, showing that the output change is \"approximately linear\" for sufficiently small $\\lambda$. Furthermore, under the same assumption, we establish that the change remains \"at"}, {"title": "4. Sampling with Control", "content": "Now we discuss our controllable sampling algorithm. We preserve the notation $x_0$ to denote a \"target image\" or \"target mean\" (e.g., the top right corner in Figure 1). We also preserve the notation $x_T := DDIM^{-1}(x_0; \\theta, T)$, the \u201cnoise\u201d by running DDIM (2) reversely from time 0 to $T$. Our objective is to perturb $x_T$ into a random $x'_T$ such that the generated image $x'$ such that it has 1. a sample mean close to $x_0$ while maintaining 2. sufficient diversity and difference from the original image and 3. high image quality. The closeness is quantified by L-2 norm distance $\\|E[x] - x_0\\|_2$, and the diversity is measured by $E[\\|x - x_0\\|_3]$. A notable feature of our algorithm is that users can specify a desired level of diversity ($C_0$ in Fig. 1, 2), and the generated images will match this level while ensuring $E[x] \\approx x_0$. Our mechanism is defined as $x'_T = a x_T + b \\Delta$, where $\\Delta$ is a random perturbation, and $a$ and $b$ are parameters to be specified shortly."}, {"title": "4.1. Sampling around a Center", "content": "For an input of the form $x'_T = a x_T + b \\Delta$ with random $\\Delta$, when $b$ is small and $a$ is close to 1, it can be regarded as a slight perturbation of $x_T$. Based on Section 3, the output will remain close to $x_0$ with an additional linear adjustment applied to $b\\eta$. Thus, we define $x := x_0 + b A \\Delta$ as an approximation for $x$, where $A = \\frac{\\partial}{\\partial (ax_T + b \\Delta)} x_0(ax_T + b \\Delta)$ specified in Proposition 1. Since $A$ is the only source of randomness in $\\hat{x}$, we can easily calculate $E[\\hat{x}] = x_0 + bAE[\\Delta]$ and $Var[\\hat{x}] = b^2 A Cov(\\Delta) A^T$. We will now discuss the principles for our sampling design."}, {"title": "4.2. Centering Feasibility", "content": "The simplest strategy is to add a random noise vector $\\Delta x$ directly to $x_T$, expressed as $x'_T = x_T + \\Delta x$ (with $a = 1, b \\Delta = \\Delta x$). However, the following proposition demonstrates that this approach cannot produce high-quality images.\nProposition 5. For any fixed vector $x$, and any random vector $\\Delta x$ such that $E[\\Delta x] = 0$, the following holds:\n$E[\\|x + \\Delta x\\|^2] = \\|x\\|^2 + tr(Cov[\\Delta x]) > \\|x\\|^2$,\nwith equality if and only if $\\Delta x = 0$ almost surely.\nProposition 5 indicates that directly adding noise, $x_T \\rightarrow x'_T := x + \\Delta x$, pushes $x'_T$ farther from the spherical surface. This partly explains why the average image becomes blurrier or noisier as the scale of $\\Delta x$ increases, since the drift term $tr(Cov[\\Delta x])$ grows larger, causing $x'_T$ to deviate further from the sphere with radius $\\|x_T\\|_2$.\nThis inspires us to consider the spherical linear interpolation method (Shoemake, 1985) for sampling, as described below. Similar approaches have been proposed by (Zheng et al., 2024; Song et al., 2020), but only for interpolating between two images."}, {"title": "4.3. Spherical Interpolation", "content": "Let vectors a and b satisfy $\\|a\\|_2 = \\|b\\|_2$ and form an angle $\\theta$. Then for any $\\alpha \\in (0, 1)$, the vector obtained through spherical interpolation $c := a\\frac{\\sin((1-\\alpha)\\theta)}{\\sin(\\theta)} + b\\frac{\\sin(\\alpha\\theta)}{\\sin(\\theta)}$ satisfies $\\|c\\|_2 = \\|a\\|_2 = \\|b\\|_2$.\nIn our case, for a standard d-dimensional normal noise vector $\\epsilon$, it is known $\\|\\epsilon\\|_2 \\approx \\sqrt{d} \\approx \\|x_T\\|_2$. Therefore, we can do spherical interpolation between $x_T$ and $\\epsilon$ to obtain $x'$. Our CCS algorithm is described in Algorithm 1."}, {"title": "4.4. Extension to Conditional Latent Diffusion Models", "content": "Conditional diffusion models usually compute the conditional score with classifier-free guidance (CFG). Let $s_\\theta(x_t, t)$ be the predicted noise, it can be written in $s_\\theta(x_t, t) = s_\\theta(x_t, t, c_{null}) + \\gamma (s_\\theta(x_t, t, c) - s_\\theta(x_t, t, c_{null}))$ where $\\gamma$ is the CFG term, $c$ is the condition and $c_{null}$ is the null condition. Exact inversion is very challenging in a high CFG setting (Hong et al., 2024), and reconstruction error in the autoencoder of latent diffusion models makes it even harder. Motivated by this, we propose a Partial-Inversion CCS Sampling algorithm (P-CCS). Instead of starting from the $T$, we pick an intermediate timestep $t_0$. Then, we compute the noise term from DDIM inversion by subtracting the clean component, sample a new noise from $N(0, (1 - \\alpha_{t_0})I)$, and then perform spherical interpolation. Details of this Alg. 3 can be found in the Appendix. Furthermore, we can sample around a edited target mean by first performing DDIM inversion with source prompt, then apply P-CCS sampling, and finally run DDIM with target prompt. More details can be found in the experiments and the Appendix."}, {"title": "5. Experiments", "content": "In the experiments, we aim to answer three questions: 1. Can we sample images that have a sample mean close to the target mean with a target MSE by our designed algorithms while maintaining good image quality? 2. Does the linearity phenomenon between the norm of residual images and the perturbation scale widely exist? 3. Can our proposed algorithm work in more challenging settings such as in conditional generation with CFG or image editing tasks?"}, {"title": "5.1. Validation of Linearity Phenomenon", "content": "Experimental setting. We perform extensive experiments on both pixel diffusion models on the FFHQ and CIFAR-10 dataset and latent diffusion models on the Celeba-HQ and fMoW dataset. For each experiment, we first sample 50 images as target images from each validation dataset from FFHQ (Karras, 2019), CIFAR-10(Krizhevsky et al., 2009), and Celeba-HQ (Xia et al., 2021). We also pick one images each class from the validation set of the fMoW dataset (Christie et al., 2018) for further verification. Then for the FFHQ and CIFAR-10 selected data, we use pixel"}, {"title": "5.2. Controllable Sampling", "content": "Experimental setting. For pixel diffusion models, we use the first 50 images from the validation data from the FFHQ-256 (Chung et al.) dataset. Then we set each image as the target mean and then sample 120 images (6000 images in total) with each target mean with a target rMSE (square root of average L-2 norm of the residuals between the sample and target mean) of 0.12. Then we test on the CIFAR-10 dataset. We randomly sample 20 images serving as target means, and then sample 120 images for each target mean with a target rMSE level of 0.11.\nFor Stable Diffusion, we use the SD1.5 checkpoint (Rombach et al., 2022). We study a more challenging scenario"}, {"title": "5.3. Controllable Sampling for Image Editing Task", "content": "We perform additional experiments with the conditional sampling for image editing using our CCS algorithm. We"}, {"title": "6. Conclusion", "content": "In this work, we study a new problem: how to sample images with target statistical properties. We present a novel sampling algorithm and a novel controller method for diffusion models to sample with desired statistical properties. We also unveil an interesting linear response to perturbation phenomenon both theoretically and empirically. Extensive experiments show that our proposed method samples the closest to the target mean when controlling the MSE compared to other methods, while maintaining superior image quality and diversity."}, {"title": "A. Proofs", "content": ""}, {"title": "A.1. Proof in Section 3.2", "content": "Proof of Proposition 1. Let $L_t(x) := \\eta_t x + \\Lambda_t \\nabla_{x_t} \\log p_t(x)$ be the one-step recursion. Our $x_0(a, t)$ is formally defined as $L_1 \\circ L_2 \\circ \\cdots \\circ L_T(a)$.\nThe second-order differentiability of $p_t$ implies the score function $\\nabla \\log p_t$ is first-order differentiable. Let $H_t$ be the Hessian matrix of $\\log p_t (H_t := \\frac{\\partial^2 \\log p_t}{\\partial i \\partial j})$. We have\n$\\nabla \\log p_t(x) = \\nabla \\log p_t(w) + H_t(w)(x - w) + o(\\|x - w\\|_2)$.\nTherefore, for any fixed direction $w$ and $\\delta \\in \\mathbb{R}$,\n$L_T(x + \\delta w) = \\eta_T (x + \\delta w) + \\Lambda_T \\nabla_{x} \\log p_T(x + \\delta w)$\n$= \\eta_T x + \\Lambda_T \\nabla_{x} \\log p_T(x) + \\Lambda_T \\delta H_T(x) w + \\delta \\eta_T w + o(\\delta)$\n$= L_T(x) + \\delta (\\eta_T + \\Lambda_T H_T(x)) w + o(\\delta)$\n$= L_T(x) + \\delta \\gamma_T(x) w + o(\\delta)$\nwhere $\\gamma_T(x)$ is defined as\n$\\gamma_T(x) = \\eta_T + \\Lambda_T H_T(x)$,\nis a matrix-valued function.\nApplying $L_{T-1}$ on both sides of the above formula:\n$L_{T-1} \\circ L_T(x + \\delta w) = L_{T-1} \\circ (L_T(x) + \\delta \\gamma_T(x) w + o(\\delta))$\n$= \\eta_{T-1} L_T(x) + \\delta \\eta_{T-1} \\gamma_T(x) w + o(\\delta) + \\Lambda_{T-1} \\nabla \\log p_{T-1}(L_T(x) + \\delta \\gamma_T(x) w + o(\\delta))$\n$= \\eta_{T-1} L_T(x) + \\Lambda_{T-1} \\nabla \\log p_{T-1}(L_T(x)) + \\delta \\eta_{T-1} \\gamma_T(x) w + \\delta \\Lambda_{T-1} H_{T-1}(L_T(x)) \\gamma_T(x) w + o(\\delta)$\n$= L_{T-1} \\circ L_T(x) + \\delta \\gamma_{T-1}(x) w + o(\\delta)$.\nwhere\n$\\gamma_{T-1}(x) := (\\eta_{T-1} I + \\Lambda_{T-1} H_{T-1}(L_T(x))) \\gamma_T(x)$.\nWe could continue applying $L_{T-2}, L_{T-3}, ..., L_1$ on the above formula, and conclude:\n$X_0(x + \\lambda \\Delta x, T) = x_0(x_T) + \\lambda \\Upsilon_0(x_T) \\Delta x + o(\\lambda)$.\n(5)\nWe might be particularly interested in the distance $\\|\\bar{x}_0(x_T + \\lambda \\Delta x, T) - \\bar{x}_0(x_T, T)\\|$, our calculation directly implies:\n$\\|X_0(x_T + \\Delta x, T) - X_0(x_T, T) \\|_2 = \\|\\lambda \\Upsilon_0(x_T) \\Delta x\\|_2 + o(\\lambda) = \\lambda \\|\\gamma(x_T) \\Delta x\\|_2 + o(\\lambda)$.\n(6)"}, {"title": "A.2. Proof in Section 3.3", "content": "We first state the detailed assumptions posed in Section 3.3. Define the function\n$h(t, y) := -\\frac{1}{2}\\sqrt{a(t)} a'(t) \\nabla \\log P_t \\Big( \\frac{y}{\\sqrt{\\sigma^2(t) + 1}} \\Big)$"}, {"title": "B. Additional Results and Experiments", "content": "In this section, we clarify some implementation details, providing more details on algorithms and visualization."}, {"title": "B.1. More Implementation Details", "content": "For the P-CCS algorithm, In the Stable Diffusion experiments, we found that increasing $t_0$ to $T$ causes PSNR to drop. The reason is DDIM inversion is inexact in a CFG setting. However, by decreasing $t_0$ we increases the PSNR, but loss in diversity. We find at $t_0$ in the range of 40 to 48 is an ideal range, with $T$ = 50, in which we choose $t_0 = 45$ based on tuning on one validation target mean image, which we exclude in our benchmarking.\nFor all methods, we set the batch size for controller tuning to be 24, and max number of iterations for tuning to be 6. Due to the linearity property of our proposed perturbation method, our controller can converge in fewer than 4 iterations in most cases. We observe DPS require significant more iterations, since the effect of changing scale has a very non-linear effect on the distance."}]}