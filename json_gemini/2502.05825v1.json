{"title": "DELTA - CONTRASTIVE DECODING MITIGATES TEXT HALLUCINATIONS IN LARGE LANGUAGE MODELS", "authors": ["Cheng Peng Huang", "Hao Yuan Chen (Mark Chen)"], "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language processing tasks. Still, they are prone to generating hallucinations-factually incorrect or fabricated content that can undermine their reliability, especially in high-stakes domains such as healthcare and legal advisory. In response to this challenge, we propose Delta, a novel inference-time approach that leverages contrastive decoding to mitigate hallucinations without requiring model retraining or additional training data. Delta works by randomly masking portions of the input prompt, then contrasting the original and masked output distribution generated by the model, effectively mitigating hallucinations through inference-only computations. Delta was evaluated on context-rich QA benchmarks like SQUAD v1.1 and v2, achieving around 3 and 6 percentage points of improvement, respectively. It also showed gains of 7 and 2 percentage points on TriviaQA and Natural Question under-sampling decoding. Delta improved SQUAD v2's no-answer exact match by over ten percentage points. These findings suggest that Delta is particularly effective when hallucinations arise from contextual ambiguity. Delta presents a computationally efficient and scalable solution for reducing hallucinations in real-world LLM applications by focusing on inference-time enhancements.", "sections": [{"title": "1 INTRODUCTION", "content": "The rapid development of large language models (LLMs) Brown et al. (2020) has led to significant advancements in text generation, natural language processing, and a wide range of real-world applications OpenAI et al. (2024). These models, powered by vast datasets and complex architectures, have become essential tools for translation, summarization, and conversational AI tasks McKenna et al. (2023). Despite these achievements, LLMs face a critical challenge: their probabilistic and nondeterministic nature often generates \"hallucinated\" content Xu et al. (2024). These hallucinations manifest as text that may sound plausible but is factually incorrect or fabricated. This poses a significant issue, particularly in high-stakes domains such as healthcare, legal advisory, and scientific research, where the accuracy and reliability of the generated content are paramount.\nHallucinations in LLMs arise from their reliance on patterns learned during training, causing them to occasionally generate outputs unsupported by the input data or real-world facts Huang et al. (2023). Addressing this issue is crucial for improving the reliability and trustworthiness of LLMs, especially as they are increasingly integrated into real-time systems and applications where incorrect information can lead to severe consequences. In response to this challenge, we introduce Delta, a novel approach designed to identify and mitigate text hallucinations on inference-time computation. Unlike traditional methods Ji et al. (2023); Li et al. (2023b); Ouyang et al. (2022) focusing on retraining models or requiring access to additional data, Delta operates solely at inference time, making it computationally efficient and easily deployable in real-time systems. Delta's core innovation lies in its use of contrastive decoding Li et al. (2023a); Chuang et al. (2024), which leverages masked versions of the input text to contrast plausible outputs against potentially hallucinated ones."}, {"title": "2 RELATED WORKS", "content": "Recent studies have focused on mitigating hallucinations in large language models (LLMs) and large vision-language models (LVLMs) Hinck et al. (2024), where models tend to generate inaccurate or irrelevant outputs. In vision-language models, such hallucinations often result from over-reliance on language priors or biases embedded in the datasets. For example, in LVLMs, object hallucinations occur when models predict objects not present in the image due to biased object co-occurrences in the training data. Several approaches have been developed to address this, including Visual Contrastive Decoding (VCD) and Instruction Contrastive Decoding (ICD).\nThe Visual Contrastive Decoding (VCD) method aims to reduce object hallucinations by contrasting the outputs generated from original and distorted visual inputs. This approach does not require additional training or external pre-trained models, making it a computationally efficient solution. By introducing visual uncertainty, such as Gaussian noise, the method identifies and mitigates instances where the model overly relies on language priors or statistical biases from the training data, thereby reducing hallucinations Leng et al. (2024).\nSimilarly, the Instruction Contrastive Decoding (ICD) technique is employed to tackle hallucinations in multimodal tasks by incorporating instruction disturbances. This method manipulates the confidence of multimodal alignment in the model's visual and textual inputs, helping it differentiate between hallucinated and relevant tokens. By applying contrastive penalties to tokens influenced by instruction disturbances, ICD effectively reduces the generation of hallucinated outputs, especially in complex visual contexts Leng et al. (2024).\nIn addition, the work context-aware decoding (CAD) has Shi et al. (2024) demonstrated a similar outcome to our Delta method by adjusting the output probabilities of LMs, amplifying the differences between outputs generated with and without the given context. This contrastive approach encourages models to prioritize contextual information during text generation. Notably, CAD can be applied to pre-trained LMs without additional training. Unlike our Delta method, the method is mainly based on context-driven datasets, making it less generalizable than the Delta method, which, in theory, could apply to all textual inputs.\nBoth approaches are part of a growing body of work exploring contrastive mechanisms and fine-grained multimodal alignment techniques to mitigate hallucinations in models that integrate vision and language processing. Future research will likely explore more robust mechanisms further to improve model reliability across different types of multimodal tasks."}, {"title": "3 METHOD", "content": "The work introduces Delta, a novel method that effectively mitigates hallucinations in text-based large-language models. The core idea behind Delta is to address the issue of hallucinations by manipulating the inference process itself. Specifically, the method generates output tokens from the model using a standard inference procedure, as outlined in Equation 1. Building on hypotheses inspired by Leng et al. (2024), which suggest that incomplete prompting or missing information tends to amplify hallucination effects, Delta aims to mitigate this by leveraging a contrastive decoding approach.\nDelta dynamically adjusts for incomplete information in this approach by comparing the outputs generated from masked and unmasked input versions. The key concept behind Delta is as follows: by randomly masking input tokens, the model generates outputs that are more likely to be filled with hallucinated information. Then, by subtracting the hallucinated logits (generated from the masked input) from the original logits, Delta extracts the \"clean\" logits\u2014those less influenced by hallucinated content. This process significantly reduces the likelihood of hallucinations, as demonstrated in Figure 1, leading to more accurate and reliable outputs in context-dependent tasks."}, {"title": "3.1 INFERENCE FROM LANGUAGE MODEL DECODER", "content": "In large language models (LLMs), the inference process involves predicting the next token in a sequence based on the previously generated tokens. Given an input sequence x and generated tokens y, we can have z = [x0,x1,..., xn\u22121, y1, . . ., yt\u22121] where n is the index of the sequence, the conditional probability of the next token yt at time step t is modeled as:\n$P_\\theta(y_t | z) = \\text{softmax} (\\text{logit}_\\theta (y_t | z))$   (1)\nIn this equation, the model generates tokens sequentially, where the logits are computed using the model's parameters \\theta. This process is crucial for autoregressive tasks like text generation, where each token is conditioned on all the tokens that came before it."}, {"title": "3.2 EFFECT OF FUZZIFIED (MASKED) TEXT ON HALLUCINATIONS", "content": "Masking portions of input text in large language models can exacerbate hallucinations. For instance, consider the sentence: \"There is a moldy banana on the table. The color of the banana is \"\". If the"}, {"title": "3.3 TEXT SEQUENCE MASKING", "content": "In this process, we introduce ambiguity by randomly masking a portion of tokens within the input sequence. Given an input sequence x = [x0,x1,..., xn\u22121], where n is the length of the sequence, several tokens are replaced according to a predefined masking ratio. Specifically, the tokens to be masked are selected randomly, and the total number of masked tokens is determined by m = [r_\\text{mask}.n], where r_\\text{mask} \\in [0, 1] represents the masking ratio. The indices of the masked tokens are randomly selected and gathered into the set I_\\text{mask} = {i_0, i_1,..., i_m}.\nThe masked sequence is formalized in the following equation:\n$\\text{mask}(x) = [x_0, x_1,..., x_{n-1}], \\, x_i = \\begin{cases} \\text{MASK} & \\text{if } i \\in I_\\text{mask} \\\\ x_i & \\text{otherwise} \\end{cases}$  (2)\nThe MASK token replaces the original tokens at the selected positions in this new sequence. The remaining tokens in the sequence remain unchanged. When such masked sequences are processed by large language models (LLMs), the model tends to predict tokens based on incomplete or missing context. This often leads to generating hallucinated words\u2014words that are statistically likely based on the model's training data but not necessarily aligned with the original context."}, {"title": "3.4 CONTRASTIVE DECODING", "content": "The Delta method leverages contrastive decoding to enhance inference accuracy and reduce hallucinations in generated outputs. The key idea is to compare the predictions from masked and unmasked input sequence versions during token generation. At each time step t, the model generates the next token yt by conditioning on the unmasked sequence z = [x0,..., xn\u22121, y1, \u00b7\u00b7\u00b7, yt\u22121] as well as its masked counterpart mask(z) where the n is the index of the sequence x. The contrastive decoding process can be formalized in equation 3.\n$P_\\text{delta}(y_t | z) = \\text{softmax} [(1 + \\alpha)\\cdot \\text{logit}_\\theta (y_t | z) - \\alpha \\,\\text{logit}_\\theta (y_t | \\text{mask}(z))]$   (3)\nIn this equation, \\alpha \\in [0,1] (logit ratio) is a tunable hyperparameter that adjusts the relative significance of the masked logits. By subtracting the masked logits, which tend to induce stronger hallucinations, the method effectively reduces the influence of hallucinated token values in the original logits. The non-hallucinated tokens increase through (1 + \\alpha) \\text{logit}_\\theta(y_t | z). As the model prioritizes more plausible predictions from the unmasked context, this incrementation of probabilities for non-hallucinated tokens leads to a more accurate and reliable output. This implies that a higher \\alpha value can filter higher levels of hallucinations and amplify the level of non-hallucinated tokens."}, {"title": "3.5 ADAPTIVE PLAUSIBILITY CONSTRAINTS", "content": "To prevent the language model from generating imbalanced or semantically incorrect sequences. The work applied Adaptive Plausibility Constraints (APC) based on the Li et al. (2023a). The goal is to construct a set V_\\text{head} such that the logit with probability higher than the particular threshold, which is determined by the \\beta, is not selected to the set V_\\text{head}.\n$V_\\text{head} (x_{<t}) = \\{x_t \\in V : P_\\theta(x_t | x_{<t}) \\geq \\beta\\cdot \\max_w P_\\theta(w | x_{<t})\\}$  (4)\nApplying to APC, the language model could generate meaningful and semantically correct sentences even with the Delta method."}, {"title": "3.6 COMPUTING DELTA FOR CONTRASTIVE DECODING", "content": "Finally, the Delta method is computed during contrastive decoding. The core idea involves adjusting the logits for token generation by contrasting predictions from unmasked and masked versions of the input sequence. The following conditional equation defines how tokens are sampled at each time step t, represented by yt:\n$y_t \\sim \\text{softmax} [(1 + \\alpha) \\cdot \\text{logit} (y | z) - \\alpha \\,\\text{logit} (y | \\text{mask}(z))], \\,\\text{subject to } y_t \\in V_\\text{head}(z_{<t})$  (5)\nHere, the sequence z, which includes previously generated tokens, is checked against a set of plausible sequences V_\\text{head}(x_{<i}). If z is deemed plausible, the model generates a token using modified logits, where the contribution of the unmasked sequence is amplified by a factor of (1 + \\alpha), and the contribution of the masked sequence is penalized by a factor of \\alpha. The resulting logits are passed through a softmax function to produce a probability distribution over the next possible tokens. If z does not belong to V_\\text{head}(x_{<i}), the probability of generating a token is set to zero. This contrastive decoding mechanism enhances the model's ability to reduce hallucinations by favoring contextually relevant token predictions over potentially misleading ones."}, {"title": "4 EXPERIMENTATION DESIGN", "content": "The experiment with the Delta method is conducted on a range of question-answering datasets and common-sense answering evaluations to assess its ability to mitigate hallucinations. Furthermore, the study presents several empirical observations to explore the characteristics of the Delta method when applied to the model used in this work."}, {"title": "4.1 EVALUATION DATASETS", "content": "To evaluate our method comprehensively, we selected diverse datasets that target various aspects of language model performance. These datasets are categorized based on their inclusion of context, question types, and the challenges they pose, providing a robust foundation for assessing the effectiveness of our approach.\nThe Stanford Question Answering Dataset (SQuAD) Rajpurkar et al. (2016) is widely used for training and evaluating machine reading comprehension models. SQuAD v1.1 contains over 100,000 question-answer pairs, with answers found directly in the text, while SQUAD v2 introduces over 50,000 unanswerable questions. This additional challenge makes SQUAD v2 particularly valuable for hallucination testing. Unanswerable questions allow us to assess if a model can correctly avoid generating answers when no relevant information is present.\nTriviaQA is a large-scale question-answering dataset comprising over 650,000 question-answer pairs from trivia quizzes Joshi et al. (2017). It is more challenging than SQUAD because the answers can be spread across long and complex documents, including Wikipedia articles and web documents. The dataset evaluates models' ability to retrieve answers from longer, less structured texts. Google's Natural Questions (NQ) dataset Kwiatkowski et al. (2019) contains questions that"}, {"title": "4.2 EXPERIMENTATION SET-UP", "content": "We utilize the Llama 3.1 8B Instruct model with 4-bit quantization as the baseline configuration Dettmers et al. (2023). The same model setup is applied for the Delta method, with parameters fixed at r_\\text{mask} = 0.7, \\alpha = 0.3, and \\beta = 0.1 for all experiments. All experiments utilize the end-of-sequence (eos) token as the MASK token.\nThe experiments are divided into two categories: with sampling and without sampling. For the sampling experiments, the temperature is set to 1 to observe the impact of sampling on the Delta method's performance compared to non-sampling configurations."}, {"title": "5 RESULTS", "content": "To evaluate the effectiveness of the Delta method, we conducted comprehensive experiments on diverse QA datasets, including SQUAD versions 1.1 and 2, TriviaQA, and Natural Questions. The results, summarized in Table 1, highlight the performance improvements achieved by Delta across these benchmarks."}, {"title": "5.1 SQUAD v1.1 AND SQUAD v2", "content": "In SQUAD v1.1, the Delta method demonstrated its ability to enhance performance significantly, achieving exact match scores of 61.95 and 61.82 in experiments with and without sampling, respectively. These scores represent improvements of 4.44 and 3 percentage points over the baseline, underscoring the method's potential for refining model accuracy in extracting precise answers from contextual data. In addition to exact match scores, F1 scores showed noticeable enhancements, further emphasizing the Delta method's robustness in handling contextual environments and reducing hallucinations.\nSimilarly, in SQuAD v2, which introduces a more challenging setting with unanswerable questions, the Delta method exhibited superior performance. The exact match scores surpassed the baseline by approximately six percentage points in both sampling and non-sampling scenarios, demonstrating the method's adaptability to different configurations. A particularly noteworthy result was observed in the \"no answer\" category, where the Delta method achieved remarkable improvements. For the exact match score of \u201cno answer,\u201d the technique recorded increases of 14.53 and 11.81 percentage points for sampling and non-sampling setups, respectively. This indicates that the Delta method is especially effective when the context does not support a valid answer, highlighting its ability to mitigate hallucinations and prevent the generation of misleading information."}, {"title": "5.2 TRIVIAQA AND NATURAL QUESTIONS", "content": "The Delta method was evaluated on the TriviaQA and Natural Questions benchmarks to assess its effectiveness in context-aware question answering. The results demonstrated that increasing the sampling temperature significantly enhanced both baseline and Delta's performance, with improvements of 7.84 percent on TriviaQA and 2.55 percent on Natural Questions. However, Delta's progress was marginal without sampling, reflecting the datasets' complexity, such as multi-paragraph answer extraction in TriviaQA. The study suggests these results occur because sampling, by nature, is more prone to generating hallucinations due to the higher likelihood of sampling lower logit tokens. Since Delta reduces the logits of hallucinated tokens, it helps prevent them from being sampled, leading to better performance in tasks requiring more context-aware reasoning. This is why Delta shows more significant improvements in generation with sampling decoding."}, {"title": "5.3 COMMONSENSE QUESTION-ANSWERING AND MMLU", "content": "CommonsenseQA and MMLU are two question-answering benchmarks that differ from context-rich datasets in that they do not provide additional supporting context for the questions. Instead, models must rely entirely on the knowledge trained during pre-training to generate answers. This difference limits the applicability of Delta's random masking approach, which focuses on contrasting masked contextual information to mitigate hallucinations.\nAs presented in Table 2, the evaluation results on these datasets show that Delta had marginal performance declines of 0.25 percentage points on CommonsenseQA and 0.29 percentage points on MMLU compared to the baseline. These small decreases indicate that Delta's masking mechanism, designed to work on context-dependent tasks, does not enhance performance when no external context is provided.\nThe result underscores Delta's vital limitation. While effective at reducing hallucinations and improving accuracy in tasks where context is explicitly available, its impact is minimal in context-free scenarios. This suggests that Delta is best suited for applications where contextual information is critical in guiding the model's predictions rather than tasks requiring innate knowledge or reasoning purely from pre-trained parameters."}, {"title": "6 ABLATION STUDY", "content": "In the ablation study, we investigated the effects of varying masking ratios and logit ratios (\\alpha) on the overall performance of our method. These experiments were conducted on the SQUAD v1.1 dataset with sampling, using a temperature of 1 and \\beta = 0.1 as the experimental setup. Masking ratios were set at 0.3, 0.5, and 0.7, while logit ratios ranged from 0.1 to 0.5. The results are summarized in heatmaps to provide a clear visualization of performance trends.\nFigure 2 illustrates the heatmaps for exact match and F1 scores. The findings reveal minimal variation across different parameter settings, with standard deviations of 0.66 for exact match and 0.21 for F1 score. Notably, all parameter configurations achieved results that exceeded the baseline scores of 57.51 for exact match and 71.74 for F1. This highlights the robustness of the Delta method, demonstrating its ability to consistently perform well without requiring extensive hyperparameter tuning. These results emphasize the method's adaptability and reliability across various parameter values."}, {"title": "7 SUMMARY AND FUTURE WORKS", "content": "This study introduces Delta, an inference-time method designed to mitigate hallucinations in large language models without requiring additional fine-tuning. Delta operates by randomly masking input tokens to identify hallucination-prone logits and then subtracting these from the original logits, effectively reducing the influence of hallucinations. Experimental results demonstrate Delta's effectiveness in context-rich question-answering tasks, achieving significant performance improvements across datasets such as SQUAD, TriviaQA, and Natural Questions. However, its impact is limited in context-free tasks like CommonsenseQA and MMLU, where the model relies solely on pre-trained knowledge instead of external context. These findings position Delta as a powerful solution tailored for context-dependent tasks, offering valuable insights for reducing hallucinations in real-world applications.\nDelta employs a straightforward random masking method, which has proven effective but leaves room for improvement. Future research will focus on developing advanced and adaptive masking strategies. One promising direction is targeted masking, prioritizing critical tokens such as proper nouns and key terms rather than applying masking uniformly. Additionally, leveraging techniques like part-of-speech tagging to prioritize tokens of higher informational value, such as nouns and verbs, could refine the method further. These approaches could enhance Delta's adaptability, making it more robust across diverse QA scenarios."}, {"title": "AUTHOR CONTRIBUTIONS", "content": "Cheng-Pong Huang is the research lead for this project working with Hao-Yuan Chen on developing this novel technology to write this manuscript and revise it as needed."}]}