{"title": "The Curious Case of Arbitrariness in Machine Learning", "authors": ["PRAKHAR GANESH", "AFAF TAIK", "GOLNOOSH FARNADI"], "abstract": "Algorithmic modelling relies on limited information in data to extrapolate outcomes for unseen scenarios, often embedding an element\nof arbitrariness in its decisions. A perspective on this arbitrariness that has recently gained interest is multiplicity-the study of\narbitrariness across a set of \"good models\", i.e., those likely to be deployed in practice. In this work, we systemize the literature on\nmultiplicity by: (a) formalizing the terminology around model design choices and their contribution to arbitrariness, (b) expanding the\ndefinition of multiplicity to incorporate underrepresented forms beyond just predictions and explanations, (c) clarifying the distinction\nbetween multiplicity and other traditional lenses of arbitrariness, i.e., uncertainty and variance, and (d) distilling the benefits and\npotential risks of multiplicity into overarching trends, situating it within the broader landscape of responsible AI. We conclude by\nidentifying open research questions and highlighting emerging trends in this young but rapidly growing area of research.", "sections": [{"title": "1 Introduction", "content": "Machine learning attempts to approximate the complexities of the world, inevitably simplifying or generalizing aspects\nof reality and failing to fully capture its nuances [10, 15, 25, 32, 93, 175]. It is thus inherently susceptible to arbitrariness,\nas it attempts to extrapolate outcomes based on limited information. Whether due to imperfect data [32, 75, 195],\nflawed modelling assumptions [29, 93, 101], or the unpredictability of certain tasks [59, 157, 195], this arbitrariness is an\nunavoidable byproduct of any data-driven learning, including machine learning. Hence, recognizing and understanding\nthis arbitrariness is crucial for developing responsible learning models.\nThe study of arbitrariness is not new; it has long been a subject of interest in uncertainty literature, with roots going\nback centuries in statistics and decision theory [11, 26, 29, 50, 69, 108, 162]. Recently, however, a new paradigm called\nmultiplicity has emerged [20, 29, 131]. First articulated by Breiman [29], multiplicity has gained popularity due to its\nunique focus only on the arbitrariness present within a set of \"good models\", i.e., models that pass certain selection\ncriteria and thus are likely to be deployed, commonly known as the Rashomon set. Moreover, multiplicity takes an\nintriguing perspective on the question of arbitrariness in model decisions, by instead examining arbitrariness in model\nselection. Through choices made during development and model selection, multiplicity offers an operational lens to the\nissue of arbitrariness and lays the groundwork for practical solutions in real-world applications.\nSeveral existing works in the literature have provided broad overviews of the field of multiplicity. Black et al. [20]\nholds a special place in modern multiplicity research, offering a comprehensive discussion of \u201copportunities\u201d, \u201cconcerns\",\nand potential \"solutions\" of multiplicity. Similarly, Rudin et al. [161] presents an excellent discussion on the benefits\nof multiplicity, with a focus on its role in identifying simpler and more interpretable models. At this point, it would\nbe remiss not to acknowledge the PhD dissertations of Black [16], Cooper [41], Hasan [86], Hsu [95], Semenova\n[165], Watson-Daniels [199], Zhong [209], contributing valuable perspectives on the role of multiplicity in machine\nlearning. Despite these notable contributions, the field still lacks a systematic review of the literature-clearly needed\ngiven its rapid growth in recent years. To address this gap, we present the first systematic literature review of\nmultiplicity in machine learning, consolidating existing discussions and identifying overarching trends."}, {"title": "2 The Rashomon Effect in Machine Learning", "content": "Taking its name from Akira Kurosawa's 1950 film Rashomon, the Rashomon effect is an epistemological framework\nthat highlights the subjectivity and ambiguity inherent in human perception [5, 7, 49, 89]. Borrowing from Davis et al.\n[49], the Rashomon effect can be defined as \"a combination of a difference of perspective and equally plausible accounts,\nwith the absence of evidence to elevate one above others, [...]\". The Rashomon effect has been studied in several different\ndomains, like the influence of cognitive biases on memory [186, 187], the impact of culture and the fluidity of truth in\nethnographic studies [89], the study of context, medium, and framing of communication [7, 181], the unreliability of\neyewitnesses [92, 147], and-central to our discussion-algorithmic modelling and machine learning [20, 29].\nThe term Rashomon effect was first introduced into algorithmic modelling by Breiman [29], pointing out the presence\nof a set of good models that all achieve similar error rates. It has since been used in discussions of statistical modelling [22,\n30, 191, 197], null hacking [154], designing robust algorithms [34, 190], measuring variable importance [56, 66], and\napplications in various domains [37, 107, 182]. More recently, it has found a resurgence with increasing attention given"}, {"title": "2.1 Rashomon Effect through Design Choices and Model Selection", "content": "Designing a machine learning model involves a series of interconnected or choices. Beginning with the data, decisions\nare made regarding how to process and filter data, which features to select, etc. [36, 135, 174] Even the random\npartitioning of the data into training and validation impacts final model behavior [68]. Beyond data, the learning\nalgorithm design further entails numerous decisions: model architecture [8, 161], hyperparameters [8, 24], various forms"}, {"title": "3 Definitions and Metrics", "content": "We now define Rashomon sets and multiplicity, building on existing literature while expanding the scope to encompass\na wider range of works. Additionally, we review multiplicity metrics and the literature on evaluating multiplicity."}, {"title": "3.1 Formalizing Rashomon Sets and Multiplicity", "content": "The concept of multiplicity is deeply rooted in the Rashomon effect, and the models illustrating this Rashomon effect are\ntogether known as a Rashomon set, a set of competing models, a set of good models, e-Rashomon set, e-Level set, etc.\nWe'll stick with the term Rashomon set for consistency in our formalization. In essence, the Rashomon set represents a"}, {"title": "4 Multiplicity, Uncertainty and Bias-Variance Decomposition", "content": "When examining arbitrariness in decision-making, machine learning research often focuses on prediction uncertainty [69,\n70, 108, 177]-model's lack of confidence in its predictions-or the bias-variance decomposition [28, 55, 112]-dividing\nthe model error into how well the model fits the data (bias) and its sensitivity to changes in data (variance). With\nextensive literature already present in these areas, a natural question arises: What unique perspectives does multiplicity\nbring to the discussion of arbitrariness not already covered by these concepts? In this section, we formalize the interplay\nbetween multiplicity, uncertainty, and bias-variance decomposition, addressing this question mathematically and\nthrough practical recommendations for when different perspectives on arbitrariness are most valuable."}, {"title": "4.1 Multiplicity and Uncertainty", "content": "Prediction Uncertainty: We start by defining uncertainty, drawing heavily from Gal [69], Gal et al. [70], Kendall\nand Gal [108], Smith et al. [177]. Prediction uncertainty quantifies the degree of confidence-or lack thereof-in a\nmodel's predictions. As it reflects the lack of confidence in a model's predictions, uncertainty is often represented as\nthe randomness (or entropy) in those predictions. Formally, prediction uncertainty is commonly defined as:\n$U(x, D) = H_y [Prob(y|x, D)] = H_y [\\Sigma_{\\theta}[Prob(y|x, \\theta) * Prob(\\theta|D)]]$\n$= H_y [E_{\\theta\\sim Prob(\\theta|D)} [Prob(y|x, \\theta)]]$\nwhere x is the input for which we're measuring uncertainty, y is the output variable, D is the original training data, and\n\u03b8 is the parametric representation of learned models. U(x, D) is the prediction uncertainty, while H[.], E[.], Prob[.]\nrepresents entropy, expectation, and probability distribution respectively. The subscript for each statistical measure\nspecifies the random variable or the distribution on which the measure is calculated.\nPrediction uncertainty is also typically divided into aleatoric and epistemic uncertainty [99, 108, 168]. Aleatoric\nuncertainty, connected to \"distributional complexity\" [207], is the intrinsic difficulty of modelling the relationship in the\ndata distribution, often described as the uncertainty left with access to infinite data. In contrast, epistemic uncertainty,\nconnected to \"approximation complexity\" [207], stems from a lack of knowledge about the best model to use, i.e., the\nchallenge of accurately capturing the true data distribution. Epistemic uncertainty does not have a straightforward\nformulation. Instead, it is commonly defined as the uncertainty remaining after accounting for aleatoric uncertainty.\nFor the interested reader, we encourage exploring the uncertainty literature [69, 99, 108, 189], as we do not expand\non these concepts in our work. We simply restate these definitions to compare them directly with multiplicity.\nPredictive Multiplicity through the lens of Uncertainty: We temporarily redefine multiplicity, drawing on the\nsame principles used to define uncertainty. In simple terms, we also define multiplicity as the entropy of predictions,\nbut only limited to models within the Rashomon set R. Thus, we can formalize multiplicity M(x, D) as:\n$M(x, D) = H_y [E_{\\theta\\sim Probr(\\theta|D)} [Prob(y|x, \\theta)]] s.t. Probr(\\theta|D) = $\nComparing equations 4 and 5, it is clear that the expectation terms are defined over different distributions: over all\npossible models for uncertainty (eq 4), and over only models within the Rashomon set for multiplicity (eq 5). But why\ndoes this distinction matter, and why should we care about both? To answer this, we discuss practical scenarios where\nviewing a problem through the lens of multiplicity is more appropriate than uncertainty and vice-versa."}, {"title": "Uncertainty or Multiplicity? Choosing the Right Lens.", "content": "Multiplicity examines prediction consistency, while\nuncertainty assesses confidence. Uncertainty is better suited for modelling the information-theoretic relationship\nbetween data and the predictions derived from it. Multiplicity, on the other hand, offers the most relevant perspective\nfor actively exploring the various interpretations that can emerge during learning. Similarly, when examining how\ndifferent modelling choices or model selection criteria can influence outcomes, the lens of multiplicity proves invaluable.\nWe outline some characteristics to look for when deciding whether to use the lens of multiplicity or uncertainty.\nUncertainty provides an information-centric perspective. As uncertainty definitions are derived from information\ntheory [26, 69, 108], it is a fundamentally better fit for related analyses. For example, uncertainty plays a crucial role\nin active learning, by finding instances most likely to provide maximum new \u2018information' [142, 171, 206].\nUncertainty is sufficient when dealing with distributional complexity. Noise in real-world data may result in a lack of\npredictive power to make reliable decisions [195]. Having access to different interpretations through multiplicity\nadds little value in such cases-multiple ways of delivering incorrect answers do not necessarily enhance usefulness.\nUncertainty quantification can be more efficient, but research in multiplicity quantification is growing rapidly. Uncertainty\nis streamlined into modern machine learning pipelines through Bayesian networks and model calibration [106, 143,\n192], offering a cost-effective alternative to multiplicity. Even when training multiple models, there is typically\nno definitive way to ensure that every trained model falls within the Rashomon set, and thus not all trained\nmodels contribute to measuring multiplicity whereas they are still valuable for quantifying uncertainty. That said,\nadvancements in multiplicity research, as previously discussed in \u00a73.2, have already been challenging this dynamic\nand may continue to reshape it further in the future [96, 98, 109, 130, 133, 210].\nMultiplicity is aligned with learning theory and hierarchical optimization. Every decision in the learning algorithm\ninfluences the underlying optimization. Multiplicity can help scrutinize how each choice shapes the final model.\nApplications include the impact of data processing, random seeds, hyperparameters, etc. [36, 72, 73, 100, 135], or,\nbroadly, any form of bi-level or constrained optimization, including meta-objectives [18, 73, 76, 166, 183].\nMultiplicity is better suited to explore alternative interpretations. As we've discussed, choosing among different learned\ninterpretations can introduce arbitrariness. Multiplicity, particularly Rashomon sets, enables exploration of these\nalternative interpretations. Examples include personalization with model mixtures, combining multiple models,\nhomogenization concerns, etc. [19, 21, 29, 47, 118, 126, 129, 204]\nNote that our recommendations paint a broad picture of when the lens of multiplicity or uncertainty could be useful,\nbut these are intended only as guidelines and deviation from these may be warranted in specific contexts."}, {"title": "4.2 Multiplicity and Bias-Variance Decomposition", "content": "Another common measure of arbitrariness in decisions is the 'variance' from the bias-variance decomposition. Error in\nmachine learning is often categorized into three parts: irreducible error, bias, and variance [28, 55, 112]. This is known\nas the bias-variance decomposition. The terms 'bias' and 'variance' describe how well the model approximates the\nunderlying distribution, i.e., approximation complexity, where 'bias' captures how well the chosen model fits the given\ndata, while 'variance' reflects the model's sensitivity to variations in the dataset.\nUnderstandably, at first glance, sensitivity to variations in the dataset-captured by 'variance'-might seem like a\nnatural way to measure arbitrariness in decision-making. However, bias-variance decomposition is typically confined to\nthe analysis of a single model, focusing only on sensitivity to variations in the underlying dataset. While this framework\nprovides valuable insights into a model's behaviour-guiding preferences for models with lower bias or variance-it"}, {"title": "5 Exploring Alternative Interpretations with Multiplicity", "content": "Now that we've discussed the formalization of multiplicity, let's delve into its real-world implications. One of the biggest\nadvantages of multiplicity lies in its ability to uncover and explore various 'good' learned interpretations [20, 161].\nWhen multiple interpretations exist, it is reasonable to expect that some of them may exhibit certain desirable properties,\nsuch as better fairness, robustness, interpretability, etc. Additionally, moving beyond a single-model paradigm also\nopens the door to aggregating insights from multiple models. In this section, we study how multiplicity facilitates such\nexploration and its broader implications. Our discussion builds on what Black et al. [20] refer to as the aggregate-level\neffects of multiplicity, while incorporating insights from more recent developments in the field."}, {"title": "5.1 Searching Instead of Optimizing", "content": "Machine learning often involves tackling complex optimizations, including two commonly seen hierarchical optimization\nproblems: bi-level optimization and constrained optimization. Bi-level optimization refers to scenarios where one\noptimization problem depends on variables governed by another nested optimization [176, 208]. A classic example is\nhyperparameter optimization. Constrained optimization, on the other hand, involves solving the optimization problems\nunder specific constraints [12, 71, 80]. Examples include enforcing constraints of fairness, robustness, safety, etc. during"}, {"title": "5.2 Ensembles and More", "content": "Selecting the \"best\" model may not always be recommended, particularly when no single interpretation of the data\ncan be optimal. Instead, combining insights from multiple models can often yield better results in such scenarios.\nTechniques like prediction ensembling, or bagging, have long been a central recommendation for stability in machine\nlearning [27, 53, 126]. Many methods in the multiplicity literature have capitalized on combining various forms of\ninformation from models in the Rashomon set. While literature in this direction primarily focuses on aggregating\nmodel explanations to create more stable and reliable explanations [9, 19, 57, 58, 66, 81, 82, 91, 104, 111, 113, 118, 121,\n123, 132, 138, 178, 182, 211], other works have also shown the benefits of aggregating fairness scores [45], individual\nprobabilities [158], or regression analysis to discover causality [191]. These techniques demonstrate the value of\nleveraging multiplicity not just to select a single best model but instead to combine multiple learned interpretations."}, {"title": "5.3 Hacking Metrics with Multiplicity", "content": "While multiplicity can enable the discovery of better models, it also introduces risks, particularly the potential of\nexploiting these search methods to circumvent regulatory requirements and interventions. This is more prevalent when\nbroad principles, such as fairness, are reduced to specific benchmarks or metrics [17]. By leveraging multiplicity, it\nbecomes possible to 'hack' these metrics, producing models that meet the specified criteria without truly adhering\nto the underlying principles the metrics are intended to enforce [40, 44, 139]. Several studies in the literature have"}, {"title": "6 Multiplicity and Responsible Al", "content": "We now move to placing multiplicity in the broader landscape of responsible AI. In this section, we examine the two key\nconcerns for individuals originating from multiplicity, i.e., arbitrariness in model selection and outcome homogenization.\nAgain, our discussion here builds on what was termed as the individual-level effects of multiplicity by Black et al. [20],\nwhile also focusing on topics that have gained more interest recently, such as homogenization."}, {"title": "6.1 Arbitrariness as a Responsible Al Concern", "content": "Arbitrary decisions in an automated system can be deeply concerning, particularly in critical domains such as law\nand medicine, where they can have direct and lasting impacts on human lives [10, 20, 48, 79, 179, 201]. Borrowing an\nanalogy from Gomez et al. [79], imagine a judge deciding legal cases by flipping a coin. While this may seem extreme, it\ndemonstrates how machine learning models can have arbitrariness embedded in them due to an analogous coin flip\ndone by the developer during model design. This aligns closely with our ICA framework (\u00a72.1), where we discuss the\narbitrary choices by the developers that can contribute to multiplicity.\nThere are contexts where a degree of \"controlled randomness\" may be acceptable-or even necessary (\u00a76.2). However,\narbitrariness is a significant concern in scenarios where individuals lack access to other equivalent opportunities' [10].\nFor instance, in hiring, some level of arbitrariness may be acceptable, or even useful to deal with the concerns of\nhomogenization (\u00a76.2). This is because individuals looking for a job often seek multiple opportunities across different\ncompanies, increasing their chances of being hired elsewhere. In contrast, domains such as law or medicine typically\ninvolve singular, high-stakes decisions with no equivalent alternatives. In such critical situations, the presence of\narbitrariness raises serious concerns for the responsible deployment of machine learning models.\nAs explored earlier, the negative effects of multiplicity extend beyond the arbitrariness of just the final prediction\nor decision (\u00a72.2). For instance, multiplicity in counterfactual explanations can impact the stability and validity of\nalgorithmic recourse [9, 84, 85, 87, 104, 111, 119, 120, 182]. The feasibility or nature of recourse might hinge on arbitrary\ndesign choices made during model development. These decisions can have real-world implications; for instance, a\nrecourse option provided by one model may become invalid if the model is updated, invalidating previous efforts. This\ninconsistency raises both ethical and legal concerns [51, 111].\nThis arbitrariness becomes even more problematic when it disproportionately impacts different individuals, particu-\nlarly harming underrepresented demographics. As hinted earlier, a significant source of arbitrariness is the model's lack\nof ability to learn the underlying distribution. In modern machine learning, underrepresented groups often face these\ninformation gaps, which can manifest as data scarcity due to limited historical records or a lack of understanding of\ncultural context for how data relates to predictions [10, 15, 32, 75, 93]. Research has consistently shown that such dis-\nparities exacerbate existing inequalities-whether through arbitrariness, uncertainty, or multiplicity [65]. The resulting\ndisproportionate harms across groups highlight the pressing need to address arbitrariness in critical domains [4, 43, 79].\nImpact of Other Responsible AI Constraints on Multiplicity: We saw that multiplicity is a critical consideration\nin responsible AI. Interestingly, several works have also explored how multiplicity interacts with other pillars of"}, {"title": "6.2 Multiplicity and Homogenization", "content": "As discussed above, the extent to which arbitrariness is problematic often depends on the context, and in certain cases,\nit might not be a standalone concern. For instance, Creel and Hellman [47] argue that arbitrariness in hiring decisions,\nby itself, is neither a legal nor moral issue. Instead, they suggest that the absence of arbitrariness across systems could\nlead to a different concern, creating an 'algorithmic leviathan', i.e., the standardization of a single outcome across an\nentire sector. Kleinberg and Raghavan [110] discuss a similar concern in the form of 'algorithmic monoculture', which\nwould be particularly problematic in interconnected systems, for instance, when multiple banks assess an individual's\ncreditworthiness, algorithmic monoculture would imply that an individual rejected from one bank would be rejected\nfrom all banks. Please refer to Table 2 for an overview of other common terms used in this literature.\nThis phenomenon, known as outcome homogenization, refers to the convergence of decisions due to common\ndesign choices across multiple models. In our ICA framework (\u00a72.1), we had termed these as conventional choices.\nBommasani et al. [21] shows that outcome homogenization can occur even when different algorithms share only certain\ncomponents, i.e., homogenization can occur even when only some design choices are conventional.\nInterestingly, introducing controlled arbitrariness or multiplicity can mitigate these risks by preventing monocultures.\nIn contexts where arbitrariness in individual decisions is less concerning but homogenization is, controlled multiplicity\nbecomes a desirable property [10, 102, 150]. In such situations, when an intentional design choice is not possible, the\ndevelopers should prefer arbitrary choices over conventional ones wherever feasible (\u00a72.1). Despite this idea being widely\nrecognized in the academic literature [10, 102, 150], public perception of intentional randomness in decision-making\nremains skeptical. A recent study by Meyer et al. [136] indicates a strong aversion of the end users towards any form of\nrandomization or intentional arbitrariness in automated decision-making. Therefore, fostering greater public awareness\nabout the nuanced impacts of multiplicity is crucial before we can develop and employ potential solutions [10]."}, {"title": "7 Open Research Questions and Emerging Trends", "content": "Building upon our systematic survey of the literature on multiplicity, this work gives us a unique vantage point to\nidentify and discuss several emerging trends and critical open research questions in the field,\nExpanding the scope of multiplicity beyond predictions and explanations. One of our key motivations for broadening the\ndefinition of multiplicity (\u00a73.1) was to incorporate multiplicity beyond predictive and explanatory contexts. Although\ninterest in these aspects is growing, further work is needed to bring them to the forefront. We hope our work will\nencourage future research that fosters a deeper and more holistic understanding of multiplicity's broader impact.\nCost-effective enumeration of Rashomon sets. A major challenge in auditing multiplicity lies in the resource-intensive\nnature of training multiple models to enumerate the Rashomon set. While we discussed several works that improve\nthe efficiency of this enumeration (\u00a73.2), the need for further research in this direction remains pressing.\nMathematical foundations of multiplicity. Establishing stronger mathematical foundations of multiplicity, for instance,\nour focus on formalizing the distinction between multiplicity, uncertainty, and bias-variance decomposition (\u00a74), is\nessential. Fundamental work on the Rashomon effect in machine learning is less represented (only 12.5%; see Figure\n1), highlighting the opportunities for future work on frameworks that rigorously define and explore multiplicity.\nMultiplicity and its interaction with responsible Al. Given the conversation of arbitrariness as a concern of responsible\nmodel development (\u00a76.1), its interaction with other pillars of responsible Al is warranted. Future research on\nframeworks that address multiplicity within the broader landscape of responsible AI deployment is needed.\nInterdisciplinary perspective on multiplicity. While substantial work has been done on measuring and mitigating\nmultiplicity, many studies do not engage further with the concerns of multiplicity in real-world settings. As discussed,\narbitrariness can be both a force of good (\u00a76.2) or a cause of harm (\u00a76.1), depending on the context. Our systematic\nreview shows that only 47.5% of works explicitly engage with responsible Al concerns (see Figure 1), which we\nbelieve to be low given the field's relevance to the responsible deployment of machine learning models. Future\ncollaborative efforts across disciplines are crucial to truly grasp the real-world consequences of multiplicity.\nMultiplicity in the era of LLMs. As machine learning models continue to scale, particularly in the era of large language\nmodels (LLMs), new challenges emerge. For instance, we see increasing attention given to the concerns of monoculture\nand homogenization (\u00a76.2). Even the evaluation of multiplicity becomes increasingly complex, as training multiple\nmodels is often infeasible at this scale. Additionally, we see another dimension of multiplicity in LLMs-decisions\nmultiplicity due to changing prompts-that requires deeper examination [6, 127, 163, 194]."}, {"title": "8 Conclusion", "content": "In this work, we systemized the existing knowledge on multiplicity, uncovering interesting trends and insights. One\nlimitation of our study is the evolving terminology within the field-terms such as \"multiplicity\" and \"Rashomon sets\"\nhave gained prominence only recently. As a result, our survey may have missed relevant works that did not explicitly\nuse this terminology. Despite this, our efforts to formalize key discussions-the language around developer choices,\ndefinitions, and the distinction between multiplicity, uncertainty, and variance-represent a crucial step toward unifying\nthe field. We also explored broad trends related to the real-world impacts of multiplicity, building from existing literature\nand highlighting overarching themes that extend beyond their originally studied contexts. We hope our work provides\na platform that is both accessible to newcomers and valuable to experts, fostering further research in multiplicity."}]}