{"title": "CLIMB: Language-Guided Continual Learning for Task Planning with Iterative Model Building", "authors": ["Walker Byrnes", "Miroslav Bogdanovic", "Avi Balakirsky", "Stephen Balakirsky", "Animesh Garg"], "abstract": "Intelligent and reliable task planning is a core capability for generalized robotics, requiring a descriptive domain representation that sufficiently models all object and state information for the scene. We present CLIMB, a continual learning framework for robot task planning that leverages foundation models and execution feedback to guide domain model construction. CLIMB can build a model from a natural language description, learn non-obvious predicates while solving tasks, and store that information for future problems. We demonstrate the ability of CLIMB to improve performance in common planning environments compared to baseline methods. We also develop Blocks World++ domain, a simulated environment with an easily usable real counterpart, together with a curriculum of tasks with progressing difficulty for evaluating continual learning. Code and additional details for this system can be found at https://plan-with-climb.github.io/.", "sections": [{"title": "I. INTRODUCTION", "content": "For decades roboticists have pursued the aim of generalized, flexible robots that can solve complex tasks in novel envi-ronments. Recent advances in foundation models [1] have shown promising results for their use in world modeling [2-4], task planning [5-8], and motion planning [9-11]. These works leverage the extensive background knowledge present in the foundation model's pre-training to provide incomplete but useful solutions to these challenging tasks. Though incomplete, results can be further refined through repetition, prompt engineering, or post-processing to improve success rates.\nWhile interest in fundamental research has been plentiful, foundation model-guided planners have yet to find regular use in practical application scenarios. This hesitance has been largely derived from the inconsistencies of output for many foundation models. Some research has shown that the direct application of foundational models for task planning yields subpar results [12], where critics argue foundation models provide an \u201capproximate retrieval\" of information and are incapable of explicit logical reasoning or planning. This observation suggests that a more sophisticated structure is required in order to leverage the extensive corpus of foundation models effectively.\nIn this paper, we present CLIMB, Continual Learning for Iterative Model Building. CLIMB is a hybrid neuro-symbolic planning system that makes use of both foundation models and classical symbolic planners to achieve planning proficiency. Given the limitations of both classical search-based planners and end-to-end learning models, a neuro-symbolic approach is required to address complex planning problems reliably.\nSuch a hybrid architecture leverages both the structure of symbolic planning and the learning capabilities and extensive knowledge base of foundation models effectively. Additionally, CLIMB incrementally builds a PDDL model of its operating environment while completing tasks, creating a set of world state predicates that function as a representation of the causal structure present in the environment. This continual learning approach enables CLIMB to solve types of problems it has previously encountered without the need to relearn task-specific information and endows it with the ability to expand its environment representation to novel problem formulations. We show CLIMB to be a moderately capable planner independently, but importantly we demonstrate its ability to self-improve, resulting in superior performance once a PDDL model has been established.\nThe contributions of our work are as follows:\n1) We propose CLIMB for learning logical models of the world and accompanying grounding functions, starting from simple domain and task descriptions in natural language and learning and improving through interaction with the environment.\n2) We evaluate CLIMB to generate sensible initial domain proposals, to improve through interaction and iterative world model building across several tasks, while proposing and correcting grounding functions to connect the logical domain to the continuous environments.\n3) We propose BlocksWorld++with a curriculum of tasks to evaluate incremental logical world model-building capabilities, both in simulation and in the real world."}, {"title": "II. RELATED WORK", "content": "A. Learning-based Planning in Robotics\nLearning methods have long been used to model and rep-resent the object and state interactions in robot environments. Such pursuits have utilized a range of methods including unsupervised clustering [13, 14], supervised learning from demonstration [15], and neural architectures [16]. The level of human setup and interaction significantly varies between approaches, with both heavily supervised interaction-based approaches [4] and fully-unsupervised systems [17] seeing use.\nIn addition to learning world relationships and predicates, the task of grounding continuous state observations to logical predicates remains a challenge. Both images [18] and synthesized state representations [19] have been used in evaluating world state representations. While both approaches have shown success, the former requires large quantities of annotated data while the latter necessitates extensive engineered systems that do not generalize easily.\nB. General Planning Capabilities of LLMs\nRecent advances in Large Language Models (LLMs) have shown promising results in their ability to complete logical reasoning tasks, decompose complex goals into sub-goals, and ingest large amounts of data. These capabilities are aided by research into prompting strategies and in-context learning [20-23], evaluating the best methods for interacting with these models. Chain-of-Thought [20], tree-of-thoughts [21], graph-of-thoughts [22], and ReAct [23] examine the relative performance of prompting structures for complex reasoning tasks. While LLMs can effectively generate rea-sonable unstructured natural language outputs, they have demonstrated poor performance at structured and constrained tasks including planning [12] without additional structure. These limitations also occur in embodied environments. SayCan [24] and Reflexion [25] propose action embedding and verbal RL to surprising success on text-based tasks. Unstructured natural language tasks do not easily translate into most embodied robot paradigms which frequently utilize discrete actions or task policies with structured interpretations. Additionally, significant attention has been given to LLMs applicability towards embodied reasoning in grounded en-vironments [24, 26]. Recent research demonstrates some ability for LLMs to solve some classes of planning problems directly. However, planning performance of these models is inconsistent at best and insufficient at worst, with state of the art planning architectures still critically failing in some domains (e.g. the floortile domain in [27]). This has led to significant debate about whether LLMs are capable of planning in a traditional sense [12]. Hybrid approaches which utilize both foundation models and symbolic planners are gaining in popularity, as they are able to leverage the strengths of both approaches [4, 7, 27-29].\nC. Continual/Lifelong Learning for Planning\nLifelong or Continual Learning (CL) presents a significant challenge for training wherein all data is not collected a priori [30]. It is essential for robots operating in complex and loosely structured environments (e.g. in the home) to augment their training sets as they operate. Mendez et al.[31] have looked at CL paradigms utilizing diffusion models as samplers in a bi-level TAMP framework. They propose the CL problem as one of compositionality [32], as opposed to most other research which follows more traditional train and test datasets."}, {"title": "III. PROBLEM STATEMENT", "content": "CLIMB framework utilizes the following as input:\n1) Domain description: Given in natural language, in order to provide the general context of the environment the system is operating in.\n2) Logical actions: A set of predefined low-level primitives that the system can use to interact with the world, defined as python function signatures.\n3) Tasks to complete: Given in natural language, a set of tasks for the system to complete and represent into its single logical world model. These tasks also serve as the curriculum to enables the logical planner to find a generalized representation that can solve new instances of such tasks.\n4) (optional) Previous world model: In case the approach has already been run in the same domain, we can take the resulting logical world model and expand it with additional predicates for completing more varied and complex tasks. Using these inputs, CLIMB achieves two goals:\n1) Solving the planning tasks: Through repeated execution in the domain, analysis of failures, and fixing errors in generated plans or predicates, the goal of the system is to incrementally solve the entire list of tasks while acquiring knowledge about how the environment functions through the process.\n2) Building an incremental logical world model: While solving individual tasks the system reuses and expands upon the logical world model produced from previous tasks. Upon completion of all individual tasks, we are left with a model that can be used to planning solutions for new instances of similar tasks. This model (domain and predicates) can be used as a prior in future executions of this pipeline on new sets of tasks with the same robot embodiment."}, {"title": "IV. CLIMB: MODEL STRUCTURE", "content": "Overview The overall architecture for the framework is presented in Figure 2. CLIMB is comprised of modules that generate the PDDL, construct a plan trace for the given problem, observe the robot's performance, and re-fine the PDDL through observation and queried solutions from the LLM. Each of the LLM modules utilizes the gpt-40-2024-08-06 model from OpenAI. Implementation details and full prompts for each of the modules can be found at https://plan-with-climb.github.io/.\nA. Language-Model-Guided Domain Generation\nCLIMB uses four modules to interface with an LLM:\nDomain and Problem Generation \u2013 The domain-specific language (DSL) generation module converts a given domain"}, {"title": "Predicate Grounding and Debugging", "content": "\u2013 This module is used to generate executable Python functions that convert the continuous world state observed from the perception API into a logical predicate set. Grounding predicates allows us to compare the state of the world after each attempted action to the simulated logical state represented by the PDDL model. The comparison of simulated to perceived logical state serves as the primary error identification mechanism by which previously unseen relationships and constraints can be modeled by the predicate inventor. This module also automatically debugs and corrects syntax errors in the predicate grounding functions through analysis of error messages and performing function regeneration if needed."}, {"title": "Predicate Update", "content": "When an unexpected logical state is observed after executing an action, the Predicate Update module generates plausible explanations for the phenomena in the form of new or modified predicates. This module's inputs include the current predicate set, the problem and plan that is currently being investigated, and the world state in which the error occurred. From this information, the language model is instructed to reason through potential root causes and proposes an updated predicate set to represent dynamics in the domain. Any new or modified predicates are then re-grounded through the Predicate Grounding module."}, {"title": "Plan Repair", "content": "\u2013 After an execution failure and predicate generation, the Plan Repair module updates the domain representation to include new predicates and constraints. Inputs to this module are the previous domain and problem DSL, the natural language task description, the world state at failure, and the updated predicate set. This module outputs an updated DSL domain which includes any new or modified predicates and an updated problem file to reflect the current world state. By updating the problem DSL, the system can recover from unintended consequences of executing infeasible actions. For example, if a stack of blocks is knocked over during execution, the initial problem state will be modified to reflect the fact that these blocks are no longer stacked."}, {"title": "B. Planning & Perception", "content": "PDDL Symbolic planner \u2013 To generate the plan trace for a given problem, we make use of the FastDownward symbolic planning framework [33] in this study. FastDownward (FD) takes as input the domain and problem PDDL files generated for a specific problem and outputs a plan trace for execution. Internally FD implements a best-first search algorithm with a \"causal graph heuristic\" which is derived from the recursive decomposition of subtasks towards the goal. It is worth noting that our architecture can make use of any symbolic PDDL planner; it has additionally been tested using the Fast-Forward [34] and Pyperplan [35] planning systems. After a plan has been calculated, we additionally verify syntactic correctness and adherence to the generated PDDL by processing the output on the VAL plan validator [36]. VAL is a logical plan simulator which utilizes the domain and problem file given to evaluate if the plan is valid within the domain and correctly solves the task. While VAL is not able to check the semantic correctness of the domain and problem, it is capable of ensuring the plan's validity for the given domain. We use the Unified Planning library [37] for its implementations of all of the above planners, VAL integration, and simulated plan rollout.\nPlan Execution and Skill Library The plan execution module wraps a library of executable atomic skills and enables the execution of generated plans on the environment. The library of skills is specific to the robot's morphology and to an extent, its domain of operation. However, the skill library should ideally generalize to a variety of tasks within the same environment. By way of example, above are the actions utilized in our BlocksWorld++ experimental domain.\nPerception API \u2013 While the robot executes the plan trace generated to solve the tasked problem, a domain-specific perception API is utilized to evaluate if the robot accom-plished each action successfully. For the real robot system, we utilize AprilTag2 [38] to observe the position of objects in the environment and distinguish between blocks. This perception module can be substituted for any system that tracks and outputs the 6D pose of objects (e.g. FoundationPose [39] for unlabeled objects). After each action is attempted, the perception module collects the state of all objects in the workspace and the proprioception of the robot. This continuous world state representation is then evaluated on the generated predicate functions created by the Predicate Inventor to determine the estimated logical world state."}, {"title": "V. EXPERIMENTS", "content": "The goal of our experiments aims to evaluate the following hypothesis: Can LLMs create robust DSL domains and prob-lems without feedback from human experts? How effective are LLMs at representing common world predicates? Can feedback for predicate learning be generated automatically and interpreted by the LLM without human intervention? Do the environment models generated by CLIMB generalize to new problems within the domain?\nWe evaluate these questions across logical, simulated, and real domains, each with separate characteristics.\nA. Logical Planning Domains\nWe first evaluate the performance of our architecture on three logical plan-level domains: blocksworld [27], grippers [27], and heavy [8]. These domains provide reasonable"}, {"title": "Domain Description", "content": "\u2022\nBlocksWorld This domain tasks the agent to stack and\nunstack columns of blocks to match specific configurations.\nA popular foundational planning problem, it is likely some\nexamples of this type of environment are present in the\nLLM training corpus.\n\u2022\nGrippers This domain tasks multiple robots to pick up\nand transport objects between several different rooms or\nareas. This domain includes a logical representation for\nrobot position requiring the plan to coordinate multiple\nrobots with one another.\n\u2022\nHeavy This domain tasks the agent to pack a box or crate\nwith objects, sorting by weight so that heavier objects are\nplaced below lighter objects."}, {"title": "Correctness of DSL built with Iterative Model Building", "content": "Experiments in the logical domain serve to evaluate the ability of LLMs to create world representations in DSL. We evaluate both the zero-shot and few-shot performance of CLIMB incorporating up to five rollouts (i.e. executions) and iterations of feedback correction. In the logical domain, we substitute the execution and perception loop with the VAL plan validator [36] using a ground truth PDDL domain and problem. VAL models and simulates plan execution in the logical predicate space and evaluates both if the overall plan was successful and, if unsuccessful, where a given error occurs."}, {"title": "Effect of Continual Learning on Performance", "content": "To evaluate the ability of CLIMB to produce generalized domain representations, we conduct a comparison of CLIMB's continual learning with a baseline. In the baseline case the planner is still able to execute and learn predicates as in the full pipeline, however results are not saved between tasks. The results of this generalization experiment is shown in Figure 4. By caching the learned domain and predicates between runs, CLIMB is able to achieve better performance and use 40% less rollouts to accomplish all tasks in the dataset."}, {"title": "B. Simulated Robot World", "content": "Blocks World++: IsaacLab Block Manipulation Domain\nWe have developed an implementation and extension of the Blocks World problem in Nvidia IsaacLab [40]. This environment enables us to evaluate our approach on the Blocks World-type domain, but with continuous state variables and more complex tasks requiring new predicates. We can evaluate whether a correct logical state can be extracted from ground truth continuous state information, and whether this mapping can be iteratively improved based on feedback from"}, {"title": "Predicate Grounding Function Generation", "content": "\u2013 To evaluate our predicate generator, we construct grounded predicates from the correct ground truth PDDL domain. We then initialize the IsaacLab environment, extract the scene information, and evaluate our generated predicates to construct a perceived logical world state. This perceived world state is compared to the ground truth PDDL problem initial state for accuracy. We evaluate predicate grounding in isolation by asking it to generate grounding functions for a list of predicates from a given domain and then comparing the values to the ground truth logical values along a single task execution."}, {"title": "Domain Generalization on BlocksWorld++ Dataset", "content": "Fig-ure 6 demonstrates the ability for CLIMB to expand it's predicate set to include knowledge relevant to different classes of problems with the same robot embodiment. Evaluating on the Blocks World++ dataset, CLIMB is able to quickly learn simple 1D arrangement in lines on the table. Though more iterations are required, the system is also able to learn spanning across two blocks for the 3 block pyramid problem and more complex horizontal arrangements including an L shape (combination of line NS and line EW)."}, {"title": "C. Physical Robot in the Real-World", "content": "Finally, we demonstrate the system in the real world using the BlocksWorld++ domain. Figure 5 showcases the BlocksWorld++ curriculum for continual learning evaluation on real hardware. This environment can serve as a benchmark to evaluate CLIMB along with other continual learning paradigms. Moving from simulation to real, the additional challenges are perception and stochastic motion control. To address the challenge of perception, we integrate AprilTag2 [38] markers on the cubes and a camera mounted on the robot gripper. By returning to a home position after each action, we can gather full pose information for all objects in the scene. We show examples using this system to evaluate BlocksWorld++ tasks at https://plan-with-climb.github.io/."}, {"title": "VI. CONCLUSION", "content": "In this paper we presented CLIMB, a system for incre-mental learning of logical domains and continuous-state grounding functions through interaction. It requires only a general description of the domain and tasks that need to be solved and access to a set of low-level primitives. CLIMB generates an initial planning domain and logical representation of the task, uses a symbolic planner to solve it, and then learns new world constraints through observation of discrepancies in the logical state expected and observed through the predicate grounding functions.\nWe evaluate the method capabilities across several logical domains, showing excellent performance given the limited information given as a prior. That, through iterative world interaction and refinement, CLIMB can learn non-intuitive predicates and world constraints to improve performance across successive attempts. Furthermore, we develop the Blocks World++ dataset, enabling evaluation of continual learning frameworks in a unified manner across logical, simulated, and real domains."}]}