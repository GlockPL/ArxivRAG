{"title": "Knowledge Graph Embedding by Normalizing Flows", "authors": ["Changyi Xiao", "Xiangnan He", "Yixin Cao"], "abstract": "A key to knowledge graph embedding (KGE) is to choose a proper representation space, e.g., point-wise Euclidean space and complex vector space. In this paper, we propose a unified perspective of embedding and introduce uncertainty into KGE from the view of group theory. Our model can incorporate existing models (i.e., generality), ensure the computation is tractable (i.e., efficiency) and enjoy the expressive power of complex random variables (i.e., expressiveness). The core idea is that we embed entities/relations as elements of a symmetric group, i.e., permutations of a set. Permutations of different sets can reflect different properties of embedding. And the group operation of symmetric groups is easy to compute. In specific, we show that the embedding of many existing models, point vectors, can be seen as elements of a symmetric group. To reflect uncertainty, we first embed entities/relations as permutations of a set of random variables. A permutation can transform a simple random variable into a complex random variable for greater expressiveness, called a normalizing flow. We then define scoring functions by measuring the similarity of two normalizing flows, namely NFE. We construct several instantiating models and prove that they are able to learn logical rules. Experimental results demonstrate the effectiveness of introducing uncertainty and our model.", "sections": [{"title": "Introduction", "content": "A knowledge graph (KG) contains a large number of triplets with form (h, r,t), where h is a head entity, r is a relation and t is a tail entity. Existing KGs often suffer from the in-completeness problem. To complement the KGs, knowledge graph embedding (KGE) models map entities and relations into low-dimensional distributed representations and define scoring functions to measure the likelihood of the triplets.\nA key to KGE is to choose a proper representation space such that the embedding can make good use of the proper-ties of the space (Ji et al. 2021). For example, point-wise Eu-clidean space is widely applied for representing entities/rela-tions due to its simplicity yet effectiveness(Yang et al. 2014).\nComplex vector space is then proposed to learn sophisticated logical rules (Sun et al. 2018; Toutanova et al. 2015). More-over, the usage of quaternion space brings more degree of"}, {"title": "Background", "content": "In this section, we introduce the related background of our model, knowledge graph embedding, normalizing flows and group theory."}, {"title": "Knowledge Graph Embedding", "content": "Let E and R denote the sets of entities and relations, respe\u0441-tively. A KG contains a set of triplets F = {(h,r,t)} \u2282 E x R \u00d7 E. KGE aims at learning the distributed represen-tations or embeddings for entities and relations. It defines a scoring function f(h,r,t) to measure the likelihood of a triplet (h, r, t) based on the embeddings (h, r,t). Existing KGE models include translation-based models, multiplica-tive models and so on (Zhang et al. 2021).\nTranslation-based models learn embeddings by translat-ing a head entity to a tail entity through a relation. TransE (Bordes et al. 2013), a representative model of translation-based models, defines the scoring function as the negative distance between h + r and t, i.e.,\n$f(h,r,t) = - ||h+r-t||$\nwhere h, r, t \u2208 Rn and || || is a norm of a vector.\nMultiplicative models measure the likelihood of a triplet by product-based similarity of entities and relations. Dist-Mult (Yang et al. 2014), a representative model of multi-plicative models, defines the scoring function as the inner product of h, r and t, i.e.,\n$f(h,r,t) = \\langle h,r,t \\rangle := \\sum_{i=1}^{n}h_i r_i t_i$\nwhere h, r, t \u2208 Rn and \\langle\u00b7,\u00b7,\u00b7\\rangle is the inner product of three vectors."}, {"title": "Normalizing Flows", "content": "A normalizing flow is a sequence of invertible and differen-tiable functions, which transforms a simple probability dis-tribution (e.g., a standard normal) into a more complex dis-tribution (Papamakarios et al. 2021; Kobyzev, Prince, and Brubaker 2020). Let Z be a random variable in Rn with a known and tractable PDF pz(z) and X = g(Z) be an in-vertible function which transforms Z into X. The PDF of the random variable X follows\n$p_X(x) = p_Z(g^{-1}(x))\\left|\\det(\\frac{\\partial g^{-1}(x)}{\\partial x})\\right|$\nwhere g\u22121 is the inverse of g and $\\det(\\frac{\\partial g^{-1}(x)}{\\partial x})$ is the deter-minant of the Jacobian of g\u22121 evaluated at x. We next show an example of normalizing flows.\nExample 1 (Pushing uniform to normal). Let z ~ U[0,1] be uniformly distributed and x ~ N(\u03bc, \u03c32) be normally dis-tributed. The invertible function\n$x = S(z) = \\mu + \\sqrt{2}\\sigma \\cdot erf^{-1}(2z - 1)$\npushes z into x, where $erf(z) = \\frac{2}{\\sqrt{\\pi}}\\int_{0}^{z} e^{-t^2}dt$ is the error function."}, {"title": "Group Theory", "content": "A group is a set G together with a group operation \u22c5 on G, here denoted \u22c5, that combines any two elements a and b to form an element of G, denoted a \u22c5 b, such that the following three requirements are satisfied:\n1. Associativity \u2200a, b, c \u2208 G, (a \u22c5 b) \u22c5 c = a \u22c5 (b \u22c5 c).\n2. Identity element \u2203e \u2208 G such that \u2200a \u2208 G,e \u22c5 a = a and a \u22c5 e = a.\n3. Inverse element \u2200a \u2208 G, \u2203b \u2208 G such that a \u22c5 b = e and b \u22c5 a = e.\nA permutation group is a group G whose elements are per-mutations of a given set X and whose group operation is the composition of functions in G, where permutations are in-vertible functions from the set X to itself. The group of all permutations of a set X is the symmetric group of X, de-noted as Sym(X). The term permutation group thus means a subgroup of the symmetric group. By Cayley's theorem (Hungerford 2012), every group is isomorphic to a permuta-tion group, which characterizes group structure as the struc-ture of a set of invertible functions."}, {"title": "Normalizing Flows Embedding", "content": "In this section, we first propose a unified perspective of em-bedding and extend to the general form of normalizing flows embedding (NFE). We then define the concrete forms of NFE and prove that they are able to learn logical rules. Fi-nally, we show the loss function."}, {"title": "Unified Perspective of Embedding", "content": "Most KGE models embed entities/relations as point vectors in a low-dimensional space. We propose a unified perspec-tive of embedding from the view of symmetric groups and show that point vectors can be seen as elements of a symmet-ric group. We take the representative KGE models, TransE and DistMult, as examples to illustrate. We show the results for other models in Appendix B."}, {"title": "TransE", "content": "Let G = X = Rn, TransE embeds every enti-ty/relation as a vector in G. We show that every vector can correspond to a permutation of X, i.e., an element of the symmetric group Sym(X). We define a map a from G to Sym(X):\n$a: G\\rightarrow Sym(X)$\n$a(g) = f_g$\nwhere g\u2208 G and the definition of fg is as follows:\n$f_g: X \\rightarrow X$\n$f_g(x) = g + x$\nwhere x \u2208 X. The image of a is a permutation group, i.e., a subgroup of Sym(X). For every triplet (h, r, t), we have:\n$f_h(x) = h + x, f_r(x) = r + x, f_t(x) = t + x$\nthen TransE can be represented as:\n$f(h,r,t) = -||h+r-t||$\n$=-||r + (h + 0) \u2013 (t + 0)||$\n$= D(f_r(f_h(x_0)), f_t(x_0))$\n$= D(f_r \\circ f_h(x_0), f_t(x_0))$\nwhere x0 = 0 \u2208 X, \u25cb denotes functional composition and D(a,b) = -||a \u2013 b||, a, b \u2208 Rn. D(\u00b7,\u00b7) is a similarity metric, which outputs a real value."}, {"title": "DistMult", "content": "Let G = (R\\{0})n and X = Rn, DistMult em-beds every entity/relation as a vector in G. We can similarly define a map a from G to Sym(X):\n$a: G\\rightarrow Sym(X)$\n$a(g) = f_g$\nwhere g\u2208 G and the definition of fg is as follows:\n$f_g: X \\rightarrow X$\n$f_g(x) = g x$\nwhere x \u2208 X and \u2299 is Hadamard product. The image of a is another subgroup of Sym(X). For every triplet (h, r, t), we have:\n$f_h(x) = h x, f_r(x) = r x, f_t(x) = tx$\nthen DistMult can be represented as:\n$f(h,r,t) = \\langle h,r,t \\rangle$\n$=\\langle r \\odot (h \\odot 1), t \\odot 1\\rangle$\n$= D(f_r(f_h(x_0)), f_t(x_0))$\n$= D(f_r \\circ f_h(x_0), f_t(x_0))$\nwhere x0 = 1 \u2208 X and $D(a, b) = a^\\top b, a, b \\in R^n$."}, {"title": "Unified Representation", "content": "Therefore, we get a unified rep-resentation of TransE and DistMult from the view of sym-metric groups:\n$f(h,r,t) = D(f_r \\circ f_h(x_0), f_t(x_0))$\nThe unified representation is defined by measuring the sim-ilarity of fr \u25e6 fh evaluated at xo and ft evaluated at xo. It is composed of three parts, the initial object x0 \u2208 X, the permutations {fh, fr, ft} and the similarity metric D(\u00b7,\u00b7). Thus, we can define scoring functions in terms of permuta-tions of a set.\nTransE and DistMult embed entities/relations into dif-ferent subgroup of the same symmetric group Sym(Rn) to get different scoring functions. Since every point vec-tor corresponds to an element of Sym(Rn), the elements of Sym(Rn) can reflect the certainty property of point vectors. Thus, we can reflect different properties of embedding and define different scoring functions by choosing different sym-metric groups. Next, we introduce the uncertainty of embed-ding by choosing a suitable symmetric group."}, {"title": "General Form of NFE", "content": "To introduce uncertainty, we let X be the set of random variables on Rn and embed entities/relations as elements of Sym(X). For every triplet (h,r,t), we have the corre-sponding permutations or invertible functions {fh, fr, ft}. Our NFE is defined in the form of Eq.(2), where x0 \u2208 X is a random variable and D(\u00b7, \u00b7) is a similarity metric between two random variables or two PDFs. fr \u25e6 fh(xo) or ft(xo) is a normalizing flow, which transforms a random variable xo into another random variable, this reflects uncertainty in-formation of {fh, fr, ft}. Thus, NFE is to measure the sim-ilarity of two normalizing flows. We can easily compute the PDFs of fr \u25e6 fh(xo) and ft(xo) by Eq.(1), then the general form of NFE is represented as\n$f(h,r,t) = D(f_r \\circ f_h(x_0), f_t(x_0))$\n$= D(p_{x_0}(f_r^{-1} \\circ f_h^{-1}(x))\\left|\\det(\\frac{\\partial f_r^{-1} \\circ f_h^{-1}(x)}{\\partial x})\\right|, p_{x_0}(f_t^{-1}(x))\\left|\\det(\\frac{\\partial f_t^{-1}(x)}{\\partial x})\\right|)$\nIn next section, we define concrete xo, {fh, fr, ft} and D(\u00b7, \u00b7) to get the concrete forms of NFE."}, {"title": "Comparison of KG2E and NFE", "content": "KG2E (He et al. 2015) embeds entities/relations as random variables and defines the scoring function as f(h,r,t) = D(h \u2212 t,r), where h, r and t are random variables with normal distributions. However, it has two drawbacks to represent entities/rela-tions as random variables to model uncertainty. First, it is difficult to parameterize a complex random variable directly due to the difficulty of computing the partition function (Goodfellow, Bengio, and Courville 2016). We get a com-plex random variable by using a complex invertible func-tion to act on a simple random variable. It is easy to pa-rameterize a complex invertible function and compute the PDF of a random variable obtained by an invertible function acting on a simple random variable by Eq.(1) (Papamakar-ios et al. 2021; Kobyzev, Prince, and Brubaker 2020). Thus, our method can be seen as a generalized reparameterization method (Kingma and Welling 2013; Rezende, Mohamed, and Wierstra 2014). Second, KG2E involves computing the PDF of the sum/difference of two random variables, i.e., the PDF of h \u2212 t, which is intractable in most cases due to the difficulty of computing the convolution of two PDFs (Bert-sekas and Tsitsiklis 2008). For example, if we embed a head"}, {"title": "Concrete Form of NFE", "content": "Based on the general form of NFE, Eq.(3), we define the concrete initial random variables xo, invertible functions {fh, fr, ft} and similarity metrics D(\u00b7, \u00b7) to get the concrete forms of NFE.\nFor the initial random variable xo, we can choose it to be a simple random variable for the convenience of computing Eq.(3), such as a random variable with uniform distribution U[\u2212\u221a3, \u221a3]n or a random variable with standard normal distribution N(0, I).\nThe invertible functions fg can be chosen as fg(x) =\ng + x as in TransE or fg(x) = g \u2299 x as in DistMult. We use the composition of the two functions:\n$f_g(x) = g x + g_{\\mu}$\nwhere go \u2208 Rn, the entries of go are non-zero, and g\u03bc \u0395 Rn. Thus, for every triplet (h, r, t), we have that\n$f_h(x) = h_ \\odot x + h_{\\mu}, f_r(x) = r_ \\odot x + r_{\\mu}$\n$f_r f_h(x) = r \\odot h \\odot x + r \\odot h_{\\mu} + r_{\\mu}$\n$f_t(x) = t \\odot x + t_{\\mu}$\nWe denote the PDF of fr \u25e6 fh(xo) as prh(x0) and the PDF of ft(xo) as qt(xo). If xo ~ N(0,I) and fo is a lin-ear (affine) function fg(x) = go \u2299 x + g\u03bc, then prh(x0) and qt(x0) are still normal distributions. A more expressive choice of fg than Eq.(4) can be piecewise linear functions:\n$\\begin{aligned}\nf_g(x)_i = \\begin{cases}\ng_{o1i} x_i + g_{\\mu i} & \\text{if } x_i \\le 0 \\\\\ng_{o2i} x_i + g_{\\mu i} & \\text{if } x_i > 0\n\\end{cases}\n\\end{aligned}$\nwhere go1,9o2, g\u03bc \u2208 Rn. For simplicity, we denote Eq.(6) as\n$f_g(x) = \\begin{cases}\ng_{o1}x + g_{\\mu} & \\text{if } x \\le 0 \\\\\ng_{o2}x+g_{\\mu} & \\text{if } x > 0\n\\end{cases}$\nTo ensure Eq.(6) is invertible, we need to constraint go1, go2 > 0. Since the composition of two piecewise linear functions with two pieces is a piecewise linear function with three pieces, we still implement fr as a linear function. This ensures that for any fh(x) and fr(x), there exists a ft(x) such that ft(x) = fr \u25e6 fh(x) for any x. Thus, for every triplet (h, r, t), we have that\n$f_h(x) = \\begin{cases}\nh_{o1}x + h_{\\mu} & \\text{if } x \\le 0 \\\\\nh_{o2}x + h_{\\mu} & \\text{if } x > 0\n\\end{cases}$\n$f_r(x) = r x + r_{\\mu}$,\n$f_r \\circ f_h(x) = \\begin{cases}\nr_ \\odot (h_{o_1} x + h_{\\mu}) + r_{\\mu} & \\text{if } x \\le 0 \\\\\nr_ \\odot (h_{o_2} x + h_{\\mu}) + r_{\\mu} & \\text{if } x > 0\n\\end{cases}$\n$f_t(x) = \\begin{cases}\nt_{o1} x + t_{\\mu} & \\text{if } x \\le 0 \\\\\nt_{o2}x + t_{\\mu} & \\text{if } x > 0\n\\end{cases}$"}, {"title": "", "content": "The similarity metric D(\u00b7, \u00b7) can be chosen as the negative Kullback-Leibler (KL) divergence between the PDFs, p(x) and q(x),\n$D(p,q) = -KL(p,q)$\n$= -\\int p(x) \\log \\frac{p(x)}{q(x)} dx$\nor negative Wasserstein distance between the PDFs (Peyr\u00e9, Cuturi et al. 2019), p(x) and q(Y),\n$D(p,q) = -W(p, q)$\n$= \\inf_{\\gamma \\in \\Gamma(p,q)} \\int ||x - y||^2 d\\gamma(x, y)$\nwhere \u0393 denotes the set of all joint distributions of (x, y). Wasserstein distance is still valid if the support sets of p(x) and q(y) are not overlapped, while the KL diver-gence is not valid. For example, W(p(x), q(y)) = 4 and KL(p(x), q(y)) = \u221e if x ~ U[0,1] and y ~ U[2,3]. Therefore, we use Wasserstein distance instead of KL diver-gence. However, Wasserstein distance is difficult to compute in most cases (Peyr\u00e9, Cuturi et al. 2019). It has a tractable solution when n = 1:\n$W(p,q) = \\int (F^{-1}(z) \u2013 G^{-1}(z))^2 dz$\nwhere F\u22121(z) and G\u22121(z) are the inverse cumulative distri-bution function of p(x) and q(y), respectively. Our idea is to decompose the n-dimensional Wasserstein distance into a sum of 1-dimensional Wasserstein distances. We have the following proposition to realize it.\nProposition 1. For two PDFs, p(x) and q(y), $W(p,q) = \\sum_{i=1}^{n} W(p_i, q_i)$ iff p(x) and q(y) share the same copula, where pi and qi are the marginal distributions of xi and yi, respectively (Cuestaalbertos, Ruschendorf, and Tuero-diaz 1993).\nSee Appendix C for the proofs. Thus, we can get the fol-lowing corollary."}, {"title": "Corollary 1.", "content": "For two PDFs, p(x) and q(y), $W(p,q) = \\sum_{i=1}^{n} W(p(x_i), q(y_i))$ if $p(x) = \\prod_{i=1}^{n} p(x_i)$ and $q(y) = \\prod_{i=1}^{n} q(y_i)$.\nWe design proper scoring functions such that prh(x0) and qt(x0) satisfy the conditions in Corollary 1. In summary, we can define the concrete forms of NFE. We have the following propositions."}, {"title": "Proposition 2.", "content": "If xo ~ U[\u2212\u221a3, \u221a3]n or xo ~ N(0, I), the invertible functions are Eq.(5) and similarity metric is Eq.(9), then the scoring function is\n$f(h,r,t) = \\frac{\\||r \\odot h_{\\mu} + r_{\\mu} - t_{\\mu}\\||^2}{\\||r \\odot h|| - |t|\\|^2}$"}, {"title": "Proposition 3.", "content": "If x\u00b0 ~ U[\u2212\u221a3, \u221a3]n, the invertible func-tions are Eq.(7) and similarity metric is Eq.(9), then the"}, {"title": "", "content": "scoring function is\n$f(h,r,t) = -||r \u00a9 h\u00b5 + ru - tu||2\n\u03c3\n1\n\u03c3\nhol - 101|||2\n(11)\nProposition 4. If xo ~ N(0, I), the invertible functions are Eq.(7) and similarity metric is Eq.(9), then the scoring function is\n$f(h,r,t) = -||r \u00a9 h\u00b5 + r\u03bc - tu||2\n1\n\u03c3\nho1to1 |||2\n(12)\nThe first term of Eq.(10) is to measure the difference of the mean of prh(x0) and pt(x0), the second term is to measure the difference of the standard deviation of prh (x0) and pt(xo). If ho = ro = to = 1, Eq.(10) reduces to f(h,r,t) = - ||h\u03bc + r\u03bc - tu||, which is the same as TransE, and the second term of Eq.(10) is equal to 0, i.e., the standard deviations of prh(x0) and qt(x0) are the same. If h\u00b5 = \u03a5\u03bc = t\u00b5 = 0, Eq.(10) reduces to f(h,r,t) =\n-|||roho|-|to|||2. Eq.(11) and Eq.(12) are similar, the only difference is the coefficient of the fourth terms, one is 3, the other is . If hol = ho2 and tot = to2,\nEq.(11) or Eq.(12) reduces to Eq.(10).\nIf we choose xo to be a random variable with Dirac delta distribution (i.e. a point vector), then Eq.(3) do not model uncertainty. Thus, NFE can be seen as a generalization of conventional models. We have the following proposition to illustrate this:"}, {"title": "Proposition 5.", "content": "Let k > 0, if xk ~ U[-3, 31 or xk ~ N(0, 2), the invertible functions are Eq.(5) and similarity metric is Eq.(9), denote the scoring function as fk(h,r,t), then \u00e6k tends to a random variable with Dirac distribution as k tends to infinity and\nlim f (h, r,t) = lim-rohu + ru-tu-roho - toll||2\n(13)\nProposition 5 shows that 1/k2 in fk(h,r,t) reflects the degree to which fk(h,r,t) focuses on uncertainty. Higher value of 1/k2 indicates fk(h,r,t) focusing more on un-certainty. If 1/k2 = 0, the second term of fk(h,r,t) is dropped. Thus, the second term of fk(h, r, t) or the second term of Eq.(10) is to model uncertainty. Eq.(11) or Eq.(12) has the similar result as Eq.(10). In conclusion, our proposed model is more general than conventional models."}, {"title": "Logical Rules", "content": "The inductive ability of a scoring function is reflected in its ability to learn logical rules (Zhang et al. 2021). The symmetry, antisymmetry, inverse and composition rules are defined as follows:\nSymmetry Rules: A relation r is symmetric if \u2200h, t, (h, r, t) \u2208 F \u2192 (t,r,h) \u2208 F.\nAntisymmetry Rules: A relation r is antisymmetric if \u2200h, t, (h, r, t) \u2208 F \u2192 (t,r,h) \u2209 F.\nInverse Rules: A relation r1 is inverse to a relation r2 if \u2200h, t, (h, r1, t) \u2208 F \u2192 (t, r2, h) \u2208 F.\nComposition Rules: A relation r3 is the com-position of a relation r1 and a relation r2 if \u2200h,t,t, (h, r1,t) \u2208 F \u2227 (t,r2, t) \u2208 F \u2192 (h, r3,t) \u2208 F.\nWe have the following proposition about our proposed scoring functions and logical rules.\nProposition 6. Scoring functions Eq.(10), Eq.(11) and Eq.(12) can learn symmetry, antisymmetry, inverse and com-position rules."}, {"title": "Loss Function", "content": "We use the same loss function, binary classification loss function with reciprocal learning, as in (Dettmers et al. 2018). For every triplet (h, r, t), our loss function is\nl(h, r, t) = \u2211 log(1 + exp(\u2212\u03b3(y \u2212 f(h,r,t\u2032))))\nt\u2032\u2208E\nwhere \u03b3 is a fixed margin and yt\u2032 = 1 if t\u2032 = t, otherwise yt\u2032 = \u22121."}, {"title": "Discussion", "content": "Normalizing Flows We implement the invertible func-tions as linear functions or piecewise linear functions, which are spline functions. To construct more expressive invertible functions for better performance, piecewise-quadratic func-tions (Durkan et al. 2019) or cubic splines (Durkan et al. 2019) or even invertible neural networks (Huang et al. 2018) can be an option. In addition to designing invertible func-tions, Dinh et al. (2019) generalize Eq.(1) to piecewise in-vertible functions and Grathwohl et al. (2018) propose a con-tinuous version of normalizing flows. This is also a potential research direction.\nNormalizing Flows Embedding The similarity of two random variables can be measured by f-divergence (Gibbs and Su 2002) or Wasserstein distance. However, these met-rics all involve computing a definite integral, which may have no closed form. This may limit the introduction of un-certainty. Since the 1-dimensional Wasserstein distance of two piecewise linear distributions always has a closed form, one solution is to approximate any distribution with a piece-wise linear distribution (Petersen and Voigtlaender 2018). For example, we can approximate a normal distribution with a triangular distribution. In order to compute the Wasserstein distance efficiently, we decompose n-dimensional Wasser-stein distance into a sum of 1-dimensional Wasserstein dis-tance by Corollary 1, a sufficient condition of Proposition 1."}, {"title": "Related Work", "content": "Group Embedding TorusE (Ebisu and Ichise 2018) de-fines embedding in an n-dimensional torus space which is a compact Lie group. MobiusE (Chen et al. 2021) em-beds entities/relations to the surface of a Mobius ring. Cai et al. (2019) show that logical rules have natural correspon-dence to group representation theory. DihEdral (Xu and Li 2019) models the relations with the representation of dihe-dral group. NagE (Yang, Sha, and Hong 2020) finds the hid-den group algebraic structure of relations and embeds rela-tions as group elements. NFE embeds entities/relations as elements of a permutation group.\nUncertainty To model uncertainty in KGs, KG2E (He et al. 2015) represents entities/relations as random vari-ables with multivariate normal distributions. TransG (Xiao, Huang, and Zhu 2016b) embeds entities as random variables with normal distributions and draws a mixture of normal dis-tribution for relation embedding to handle multiple semantic issue. NFE introduces uncertainty by embedding entities/re-lations as permutations of a set of random variables.\nNormalizing Flows Normalizing Flows should satisfy two conditions in order to be practical, the invertible func-tion has tractable inverse and the determinant of Jacobian is easy to compute. A basic form of normalizing flows can be element-wise invertible functions. NICE (Dinh, Krueger, and Bengio 2014) and RealNVP (Dinh, Sohl-Dickstein, and Bengio 2016) utilize affine functions to construct coupling layers. M\u00fcller et al. (2019) propose a powerful generaliza-tion of affine functions, based on monotonic piecewise poly-nomials. Ziegler and Rush (2019) introduce an invertible non-linear squared function. Durkan et al. (2019) model the coupling function as a monotone rational-quadratic spline. Jaini, Selby, and Yu (2019) propose a strictly increasing polynomial and prove such polynomials can approximate any strictly monotonic univariate continuous function."}, {"title": "Experiments", "content": "In this section, we first introduce the experimental settings and compare NFE with existing models. We then show the effectiveness of introducing uncertainty. Finally, we conduct ablation studies. Please see Appendix D for more experi-mental details."}, {"title": "Experimental Settings", "content": "Datasets We evaluate our model on three popular knowl-edge graph completion datasets, WN18RR (Dettmers et al. 2018), FB15k-237 (Toutanova et al. 2015) and YAGO3-10 (Dettmers et al. 2018).\nEvaluation Metric We use the filtered MR, MRR and Hits@N (H@N) (Bordes et al. 2013) as evaluation metrics and choose the hyper-parameters with the best filtered MRR on the validation set.\nBaselines We compare the performance of NFE with sev-eral translational models, including TransE (Bordes et al. 2013), RotatE (Sun et al. 2018), bilinear models, includ-ing DistMult (Yang et al. 2014), ComplEx (Toutanova et al. 2015), QuatE (Zhang et al. 2019), TuckER (Bala\u017eevi\u0107, Allen, and Hospedales 2019a), neural networks models, in-cluding ConvE (Dettmers et al. 2018), HypER (Bala\u017eevi\u0107, Allen, and Hospedales 2019b), and group embedding mod-els, including DihEdral (Xu and Li 2019), NagE (Yang, Sha, and Hong 2020)."}, {"title": "Results", "content": "Due to the great generality, our proposed NFE is able to have different instantiations, Eq.(10) and Eq.(11) and Eq.(12). We denote Eq.(10) as NFE-1. Eq.(11) and Eq.(12) achieve al-most same result, we denote them as NFE-2. We compare the performance of NFE-1 and NFE-2 with baseline mod-els. See Table 1 for the results. NFE-1 and NFE-2 achieve state-of-the-art performance on three datasets, especially on YAGO3-10, which is the largest dataset. NFE-2 is slightly inferior to NFE-1. Although NFE-2 is more expressive than NFE-1, NFE-2 may be more difficult to optimize. NFE-1 are derived from TransE and DistMult, but NFE-1 outper-forms TransE and DistMult significantly on all metrics on three datasets. NFE-1 is better than neural networks models, ConvE and HypER, and group embedding models, DihEdral and NagE. The results show the effectiveness of our model."}, {"title": "Uncertainty", "content": "Here we focus on the NFE-w/o-uncertainty in Table 1 and Table 2. Proposition 5 shows that NFE-1 can be seen as a generalization of existing models to model uncertainty. NFE-1 can reduces to Eq.(13), which do not model uncer-tainty. We denote Eq.(13) as NFE -w/o-uncertainty. Table 1 shows that NFE-1 achieves better performance than NFE-w/o-uncertainty on all metrics on three datasets. Thus, the results show the effectiveness of introducing uncertainty. Proposition 5 shows that 1/k2 in Eq.(13) reflects the de-gree to which Eq.(13) focuses on uncertainty. Higher value of 1/k2 indicates the model focusing"}]}