{"title": "FBFL: A FIELD-BASED COORDINATION APPROACH FOR DATA HETEROGENEITY IN FEDERATED LEARNING", "authors": ["DAVIDE DOMINI", "GIANLUCA AGUZZI", "LUKAS ESTERLE", "MIRKO VIROLI"], "abstract": "In the last years, Federated learning (FL) has become a popular solution to train machine learning models in domains with high privacy concerns. However, FL scalability and performance face significant challenges in real-world deployments where data across devices are non-independently and identically distributed (non-IID). The heterogeneity in data distribution frequently arises from spatial distribution of devices, leading to degraded model performance in the absence of proper handling. Additionally, FL typical reliance on centralized architectures introduces bottlenecks and single-point-of-failure risks, particularly problematic at scale or in dynamic environments.\nTo close this gap, we propose Field-Based Federated Learning (FBFL), a novel approach leveraging macroprogramming and field coordination to address these limitations through: (i) distributed spatial-based leader election for personalization to mitigate non-IID data challenges; and (ii) construction of a self-organizing, hierarchical architecture using advanced macroprogramming patterns. Moreover, FBFL not only overcomes the aforementioned limitations, but also enables the development of more specialized models tailored to the specific data distribution in each subregion.\nThis paper formalizes FBFL and evaluates it extensively using MNIST, FashionMNIST, and Extended MNIST datasets. We demonstrate that, when operating under IID data conditions, FBFL performs comparably to the widely-used FedAvg algorithm. Furthermore, in challenging non-IID scenarios, FBFL not only outperforms FedAvg but also surpasses other state-of-the-art methods, namely FedProx and Scaffold, which have been specifically designed to address non-IID data distributions. Additionally, we showcase the resilience of FBFL's self-organizing hierarchical architecture against server failures.", "sections": [{"title": "1. INTRODUCTION", "content": "Research Context. Machine learning requires large datasets to train effective and accurate models. Typically, data are gathered from various sources into one central server where the training process occurs. However, centralizing data on a single device poses significant privacy challenges. These concerns arise not only from the need to share sensitive information but also from the heightened risk of storing all data in a single location, which, if compromised, could result in large-scale data exposure. Federated learning (FL) has emerged as a promising solution for training machine learning models in scenarios where data privacy is a primary concern, enabling devices to collaboratively learn a shared model while keeping their data local [MMR+17]. This paradigm not only alleviates the necessity for central data storage but also addresses privacy and efficiency concerns inherent in traditional systems with centralized learning.\nResearch Gap. Despite its advantages, in the current landscape of FL, training is distributed, but model construction is still predominantly centralized, posing challenges in scenarios characterized by spatial dispersion of devices, heightened risk of single points of failure, and naturally distributed datasets. Existing peer-to-peer solutions attempt to tackle these concerns; however, they often overlook the spatial distribution of devices and do not exploit the benefits of semantically similar knowledge among nearby devices. This builds on the assumption that devices in spatial proximity have similar experiences and make similar observations, as the phenomena to capture is intrinsically context dependent [Est22]. Moreover, existing approaches often lack the flexibility to seamlessly transition between fully centralized and fully decentralized aggregation methods. This limitation highlights the potential role of advanced coordination models and programming languages in bridging this gap.\nContribution. For these reasons, in this paper, we introduce Field-Based Federated Learning (FBFL), a novel approach that leverages computational fields (i.e., global maps from devices to values) as key abstraction to facilitate device coordination [VBD+18, LLM17] in FL. By field-based coordination, global-level system behavior can be captured declaratively, with automatic translation into single-device local behavior.\nWe find that this approach offers a versatile and scalable solution to FL, supporting the formation of personalized model zones. Most specifically, our approach actually relies on known field-based algorithms of information diffusion and aggregation developed in the context of aggregate computing [VAB+18], ultimately defining what we can define \u201cfields of Machine Learning (sub)models\". This method enables dynamic, efficient model aggregation without a centralized authority, thereby addressing the limitations of current FL frameworks. Therefore, our contributions are twofold:\n\u2022 We demonstrate that field coordination enables performance comparable to centralized approaches under independent and identically distributed (IID) data settings;\n\u2022 We exploit advanced aggregate computing patterns to efficiently create a self-organizing hierarchical architecture that group devices based on spatial proximity, improving perfor-nance under non-IID data settings.\nBy \"self-organizing hierarchical\" we mean a hybrid architecture based on peer-to-peer interac-tions, but which elects leaders in a distributed manner. These leaders will act as aggregators of local models pertaining to sub-portions of the system. We evaluate our approach in"}, {"title": "2. RESEARCH QUESTIONS", "content": "The complexity of modern systems where federated learning (FL) can be applied, such as the edge-cloud compute continuum, is rapidly increasing. This poses significant challenges in terms of scalability, adaptability, and contextual relevance [PCAC24]. Field-based approaches have demonstrated notable advantages in addressing such complexity in various domains, offering robust solutions in both machine learning [AVE23, DCAV24, DFAV24] and software engineering [CPCC24, GKP+24, FVC24] contexts. However, research on field-based methodologies within FL, particularly in the area of personalized FL [DAF+24, Dom24], is still at an early stage. To advance this research area, our work aims to explore the potential of FBFL by addressing the following research questions:\n(RQ1) How does the Field-Based Federated Learning approach perform compared to cen-tralized FL under IID data?\n(RQ2) Can we increase accuracy by introducing personalized learning zones where learning devices are grouped based on similar experiences as often observed in spatially near locations? More precisely, what impact does this have on heterogeneous and non-IID data distributions?\n(RQ3) What is the effect of creating a self-organizing hierarchical architecture through Field Coordination on Federated Learning in terms of resilience, adaptability and fault-tolerance?"}, {"title": "3. BACKGROUND AND MOTIVATION", "content": ""}, {"title": "3.1. Federated Learning", "content": "Federated learning [MMR+17] is a machine learning technique introduced to collaboratively train a joint model using multiple, potentially spatially dis-tributed, devices. Models kind can vary but in this paper, we focus on neural networks. The federation typically consists of multiple client devices and one aggregation server, which may be located in the cloud. The key idea behind FL is that the data always remains on the device to which it belongs: devices only share the weights of their models, thus making FL an excellent solution in contexts where privacy is a crucial aspect (e.g., in health-care applications [XGS+21, NPP+23]) or where data are naturally distributed, and their volume makes it infeasible to transfer them from each client device to a centralized server. Federated learning can be performed in different ways [YLCT19]. In horizontal FL (a.k.a., sample-based), participants share the same input features but hold distinct sample sets. They generally follow four steps (see Figure 1A):\n(1) model distribution: the server distributes the initial global model to each device. This step ensures that all devices start with the same model parameters before local training begins;\n(2) local training: each device trains the received model on its local dataset for a specified number of epochs. This step allows the model to learn from the local data while keeping the data on the device, thus preserving privacy;\n(3) model sharing: after local training, each device sends its updated model parameters back to the server. This step involves communication overhead but is crucial for aggregating the learned knowledge from all devices;"}, {"title": "3.1.1. Challenges", "content": "Resilience. Exploring the network topology presents another avenue for improvement, as the configuration of device communication can significantly influence FL performance. Various structures, such as peer-to-peer (P2P), hierarchical, and centralized networks, offer diverse benefits and challenges. Traditionally, FL systems have relied on a centralized server for model aggregation. However, this setup poses scalability challenges and risks introducing a single point of failure. In response, recent advancements (e.g., [HDJ21, WN21, LZSL20]) have embraced P2P networks to address these limitations, going towards what is called decentralized federated learning see Figure 1B. P2P architectures, devoid of a central authority, enhance scalability and robustness through the utilization of gossip [VBT17] or consensus algorithms [SNR20] for model aggregation.\nNon-IID. Federated learning is particularly effective when the data distribution across devices is independent and identically distributed (IID) [NSU+18], namely the users experience the same data distribution. For instance, in a text prediction application, all users may have similar writing styles. However, in real-world scenarios, data distribution is often non-IID, where devices have different data distributions. This data heterogenity can lead to model shift during training, where the model's performance degrades as the training progresses and leading also to slow or unstable convergence. To address these challenges, several approaches have been proposed, such as FedProx [LSZ+20] and Scaffold [KKM+20]."}, {"title": "3.1.2. Approaches", "content": "FedAvg. One of the most common algorithms for horizontal FL is FedAvg, where the server aggregates the models by averaging the weights of the models received from the client devices. Formally, giving a population of K devices, each of them with a local dataset $D_k$. Each device k performs E epochs of local training on its dataset, computing updates to the weights of its local model, denoted as $w_k^t \\in \\mathbb{R}^n$ at the end of the t-th global round. The server then computes the weights of the global model $w^{t+1}$ as the average of the weight vectors from the local models shared by the devices:\n$w^{t+1} = \\frac{1}{K} \\sum_{k=1}^K w_k^t$.\nThis process is repeated for a fixed number of rounds, with the server distributing the global model to the devices at the beginning of each round."}, {"title": "3.2. Motivation", "content": "FedProx. This algorithm extends FedAvg by introducing a proximal term to the objective function that penalizes large local model updates. This modification helps address statis-tical heterogeneity across devices while safely accommodating varying amounts of local computation due to system heterogeneity.\nFormally, the local objective function for device k in FedProx is:\n$\\arg \\min_w h_k(w; w^t) = F_k(w) + \\frac{\\mu}{2} ||w - w^t||^2$\nwhere $F_k(w)$ is the local loss function, $w^t$ is the global model at round start, and $\\mu \\geq 0$ is the proximal term coefficient that controls how far local updates can deviate from the global model.\nScaffold. This algorithm introduces control variates to correct model drift caused by data heterogeneity. Unlike FedAvg, where each client performs model updates locally and communicates its version to the server, Scaffold tries to minimize this drift using control variates that track the direction of gradient descent for each client. In the local model update phase, each client i adjusts its model $y_i$ using the formula $y_i \\leftarrow Y_i \u2013 \\eta(g_i(Y_i) + c \u2013 C_i)$, where \u03b7 is the local learning rate, $g_i(y_i)$ represents the local gradient, and $c$ and $c_i$ are the server and client control variates respectively. The client's control variate $c_i$ is then updated using either $c^+ \\leftarrow g_i(x)$ or $c_i^+ \\leftarrow C_i + \\frac{K\\eta}{Km} (x - yi)$, where x is the server's model and K represents the number of local updates. Finally, the server performs global updates. The global model is updated as $x \\leftarrow x + \\eta_g\\frac{1}{\\S|} \\sum_{i\\in S} (Y_i \u2013 x)$, where \u03b7g is the global learning rate and S is the set of selected clients. Simultaneously, the server control variate is adjusted using $c \\leftarrow c + \\frac{1}{N} \\sum_{i\\in S}(c^t \u2013 c_i)$, with N being the total number of clients.\nWhile these approaches to federate networks from multiple learning devices partially address non-IID data distribution challenges, they overlook a crucial aspect: the spatial distribution of devices and its relationship with data distribution patterns. Research has shown that devices in spatial proximity often exhibit similar data distributions, as they typically capture related phenomena or user behaviors [DAF+24]. This spatial correlation suggests that clustering devices based on spatial proximity could enhance model performance and personalization. However, current clustering approaches in federated learning are predominantly centralized and rely on traditional clustering algorithms that do not consider the spatial aspects of data distribution. This gap highlights the need for an approach that simultaneously addresses: (i) decentralization, (ii) non-IID data handling, and (iii) spatial correlation in data distributions via distributed spatial-based leader election. Our work aims to bridge this gap through field-based coordination\u2014see Table 1 as a comparison between current approaches and our proposal."}, {"title": "4. FIELD-BASED COORDINATION", "content": "Before introducing our approach, we provide a brief overview of field-based coordination, a macroprogramming paradigm that leverages computational fields to facilitate coordination among agents in a distributed system. Coordination based on fields [AVD+19] (or field-based coordination) employs a methodology where computations are facilitated through the concept of computational fields (fields in brief), defined as distributed data structures that evolve over time and map locations with specific values. This method draws inspiration from foundational works such as Warren's artificial potential fields [War89] and the concept of co-fields by Zambonelli et al. [MZL04]. Specifically, in the context of co-fields, these computational fields encapsulate contextual data, which is sensed locally by agents and disseminated by either the agents themselves or the infrastructure following a specific distribution rule.\nIn our discussion, coordination based on fields refers to a distinct macroprogramming and computation framework, often referred to as aggregate computing [BPV15]. This framework enables the programming of collective, self-organizing behaviors through the integration of functions that operate on fields, assigning computational values to a collection of individual agents (as opposed to locations within an environment). Thus, fields facilitate the association of a particular domain of agents with their sensory data, processed information, and instructions for action within their environment. Computed locally by agents yet under a unified perspective, fields enable the representation of collective instructions, such as a velocity vector field directing swarm movement or a field of numerical values reflecting the swarm's environmental perception [AV25]. To comprehend field-based computing fully, we highlight the system model and the programming model in the following sections."}, {"title": "4.1. System Model", "content": "An aggregate system may be defined as a collection of agents (or nodes) that interact within a shared environment to achieve a common goal.\nTo better understand the system's behavior, we must first define the system's structure (i.e., the agents and their relationships), the agents' interaction (i.e., how they communicate), and their behavior within the environment (i.e., how they process information and act).\nStructure. An agent represents an autonomous unit furnished with sensors and actuators to interface with either a logical or physical environment. From a conceptual standpoint, an agent possesses state, capabilities for communication with fellow agents, and the ability to execute simple programs. An agent's neighborhood is composed of other neighbor agents, forming a connected network that can be also represented as a graph-see Section 5 for more details. The composition of this network is determined by a neighboring relationship, designed based on the specific application needs and constrained by the physical network's limitations. Commonly, neighborhoods are defined by physical connectivity or spatial proximity, allowing for communication directly or via infrastructural support, based on certain proximity thresholds.\nInteraction. Agents interact by asynchronously sending messages to neighbors, which can also occur stigmergically through environmental sensors and actuators. The nature and timing of these messages depend on the agent's programmed behavior. Notably, our focus on modelling continuous, self-organizing systems suggest frequent interactions relative to the dynamics of the problem and environment."}, {"title": "4.2. Programming model", "content": "This system model establishes a foundation for collective adaptive behavior, necessitating an in-depth elucidation of the \"local computation step,\" facilitated by a field-based programming model. This model orchestrates the collective behavior through a field-based program, executed by each agent in adherence to the prescribed model. Field calculus defines the computation as a composition of function operations on fields, and any variants of it allow the developers to express at least i) the temporal evolution of fields, ii) the data interchange among agents, and iii) the spatial partitioning of computation. Various incarnations of this model employ distinct constructs (e.g., share [ABD+20] and xc [ACD+24] calculus). Among them, FScaFi [CVAD20] is a conceptual framework within the ScaFi [CVAP22] programming model. In this variant, the three main operator constructs are rep, nbr, and foldhood. For instance, to model the temporal evolution of a field, one can employ the rep construct as follows:\nrep(0) (x => x + 1)\nHence, rep is the incremental evolution of a field with each cycle, representing a non-uniform field. Particularly, the above described code express a field of local counters, where each agent increments its own counter at each cycle.\nTo facilitate data exchange between agents, ScaFi leverages the nbr construct in con-junction with a folding operator:\nfoldhood (0) (+) (nbr(1))\nHere, nbr (1) signifies the neighboring agent's field value, + denotes the folding operator, where 0 is the fold's initial value. This code snippet produces a field where each agent's value it is the number of its neighbors.\nLastly, to express spatio-temporal evaluation, a combination of the aforementioned constructs is utilized:\nrep(minId) { minId => foldhood (0) (math.min) (nbr(minId)) }\nThis combination calculates the minimum value of ID in the entire network. This demon-strates the integration of spatial and temporal computation through a synergistic application of both constructs."}, {"title": "4.3. Coordination Building Blocks", "content": "On top of this minimal set of constructs, ScaFi provides a set of building blocks for developing complex coordination algorithms. Notably, these blocks are designed to be self-stabilizing- converging to a stable state from any initial condition- and self-organizing-adapting dynamically to environmental changes and network topology variations. A cornerstone among these is the self-healing gradient computation, a distributed algorithm for maintaining the minimum distance from a designated source node to every other node in the network. Building upon standard gradient-based approaches, this mechanism automatically recomputes and updates distance estimates whenever changes occur in the network (e.g., node arrivals/removals or communication failures), highlighting the algorithm's self-healing property and making it highly suitable for dynamic, large-scale environments. Within ScaFi, this is described as follows:\ndef gradient (source: Boolean): Double\nBuilding upon this foundation, more sophisticated coordination algorithms can be developed, such as:\n\u2022 Gradient cast: a mechanism to disseminate information from a source node across the system using a gradient-based approach. In ScaFi, this is expressed as:\nG[V] (source: Boolean, value: V, accumulator: V => V)\nHere, source identifies the gradient's origin, value is the disseminated information, and accumulator is the function for aggregating the disseminated value.\n\u2022 Collect cast: conceptually the inverse of gradient cast, it aggregates information from the system back to a specific zone. It is represented as:\nC[V] (potential: Double, accumulator: V, localValue: V, null: V)\nin this construct, potential indicates the collection's potential, accumulator is the function for aggregating values towards the source, localValue is the value being collected, and null signifies the absence of value.\n\u2022 Sparse choice: a distributed mechanism for node subset selection and spatial-based leader election, expressed as:\nS(radius: Double): Boolean\nwhere radius specifies the maximum radius within which a leader can influence other nodes. The algorithm is \"spatial-based\" as it leverages physical distances between nodes to elect leaders: each leader node creates a sphere of influence with radius radius, and nodes within this radius become part of that leader's region.\nBy integrating these constructs, it becomes possible to execute complex collective behaviors, such as crowd management [BPV15], distributed sensing and action [ACPV22], and swarm behaviors [ACV23]. Furthermore, it is feasible to integrate the aforementioned programming model with deep learning techniques, advancing towards a paradigm known as hybrid aggre-gate computing [ACV22b]. AC can orchestrate or enhance the learning mechanisms [AVE23], and conversely, learning methodologies can refine AC [ACV22a]. This paper adopts the initial perspective, delineating a field-based coordination strategy for FL. The objective is to create a distributed learning framework that is inherently self-stabilizing and self-organizing."}, {"title": "4.4. Self-organizing Coordination Regions", "content": "Recent advances in field-based coordination introduced a pattern called Self-organizing Coordination Regions (SCR) [CPVN19]. This strategy enables distributed collective sensing and acting without relying on a dedicated authority, all while ensuring both self-stabilization and self-organization of the system. The SCR pattern operates as follows:\n(1) A distributed multi-leader election process selects a set of regional leaders across the network (e.g., using the S operator);\n(2) The system self-organizes into distinct regions, each governed by a single leader (e.g., leveraging the G operator); and\n(3) Within each region, a feedback loop is established where the leader collects upstream information from the agents under its influence and, after processing, disseminates downstream directives (using both the C and G operators, see Figure 2 for an overview).\nThis pattern effectively combines the previously discussed building blocks and is straight-forward to implement in the aggregate computing context. An example implementation skeleton is provided below: In the following we show a compact implementation in ScaFi of such complex pattern.\ndef SCR (radius: Double): Boolean = {\n\t// This function implements the SCR pattern, dividing the space into regions\n\tval leader = S(radius) // Elect leaders within the given radius\n\tval potential = gradient (leader) // Create the region around each leader\n\tval collectValue =\n\tC(potential, accumulationLogic, localValue, nullValue) // Collect data from region\n\t// Leaders decide how to process the collected data\n\tmux (leader) {\n\t\tdecide (collectValue)\n\t}{\n\t\tnullValue\n\t}\n\t// Broadcast the decision to all devices in the region\n\tG(leader, collectValue, identity)\n}"}, {"title": "5. PROBLEM FORMULATION", "content": "Consider a distributed network comprising a set of devices, denoted as $N$, where each device $d \\in N$ possesses a dataset unique to it. The dataset on device $d$ is used to train a local model, characterized by a weights vector $w_d$. The objective function local to device $d$, $f_d(w_d)$, quantifies the performance of the model on this local dataset, with the aim being to minimize this function.\nThe general problem that FL seeks to solve is constructing a global model by optimizing a global objective function $f(w)$, formulated as the aggregate of all local objective functions, adjusted by the size of the local datasets. This is mathematically represented as:\n$f(w) = \\sum_{d \\in N} a_d f_d(w_d)$,\nwhere $a_d = \\frac{|D_d|}{|D|}$ signifies the weight of device d's data in the global objective, $n_d$ is the count of samples in device d's dataset, and $|D| = \\sum_{d \\in N} n_d$ represents the total number of samples across all devices."}, {"title": "Peer-to-peer FL", "content": "In the peer-to-peer model of FL, even if the problem is conceptually the same, the clients, instead of taking a model from a central server, directly exchange information to collaboratively learn a global model without the intermediation of a central server. This network can be modelled as a graph $G = (N,E)$, with nodes representing devices and edges symbolizing direct communication links. The neighbors of a device d, with which d can exchange messages, are denoted by $N_d = \\{d' \\in N|(d,d') \\in E\\}$.\nThe peer-to-peer federated learning process in this context unfolds over several rounds (see Algorithm 1), each encompassing:\n(1) local training: devices independently update their local models by training on their datasets for p epochs, aiming to minimize the local objective function $f_d(w_d)$;\n(2) model sharing: devices share their updated weights vector $w_d$ with their neighbors, also expressed as $d \\overset{w_d}{\\rightarrow} d'$;\n(3) model aggregation: each device combines the received weights vectors from its neighbors to update its local model to $w_d$.\nIn this approach we directly share the model with all the neighbors (i.e., $N_d$, point 2), but potentially it is possible to incorporate strategies to minimize communication overhead by selectively sharing models or parts thereof (e.g., gradients or specific layers). Each node stores the received model in a buffer $B_d$, and then the aggregation algorithm is executed. The aggregation of models is defined by:\n$w'_d = A(w_d, \\{w_{d'}|d' \\in B_d\\})$,\nwhere A is the aggregation algorithm, which could range from simple averaging to more sophisticated methods. Through the iterative execution of these steps, the system converges towards a unified model that approximates the outcome of a centrally coordinated federated learning process."}, {"title": "Self-organizing hierarchical FL", "content": "This peer-to-peer federated learning process is the foundation of our approach, which can also be extended to a self-organizing hierarchical approach, where a subset of nodes is designated as aggregators, namely the one responsible for aggregating the models and disseminating the global model to the network. Notably, elected leaders are not fixed but may change over time if network topology evolves (e.g., due to the failure or the movement of some nodes).\nConsider a distributed leader election algorithm $DL$ to select the aggregators, (e.g., using the Sparse Choice construct), and then the process is similar to the one described in Algorithm 1 but where the aggregators are responsible for the aggregation and dissemination of the global model. Moreover, each node can only belong to a single leader. For simplicity, we use a distance-based leader selection rule, therefore forming a Voronoi-like partitioning of the network. Formally, let a node d be given, and let $L\\subseteq N$ be the set of leaders computed by DL. We say that d is under the influence of a leader $l \\in L$ if and only if\n$\\forall l'\\in L \\backslash \\{l\\}, dist(d,l) < dist(d, l'),$\nwhere dist(,) is the chosen distance metric. In case of equality, a predefined tie-breaking rule is applied. The set of nodes under the influence of l is denoted $L_i$, and let $d_l$ be the leader to which d belongs. Finally, we define a forward chain from a node $d'$ to a node $d''$ (written $d' \\Rightarrow d''$) as a sequence\n$d_1,d_2,...,d_k$ such that $d_1 = d'$, $d_k = d''$, and $d_i \\leftrightarrow d_{i+1}$ for $i = 1, . . ., k \u2212 1$.\nThis formalizes that even if two nodes are not directly connected, there exists a chain of direct influences linking them. With these definitions, the self-organizing hierarchical federated learning process is described in Algorithm 2."}, {"title": "6. FIELD-BASED FEDERATED LEARNING", "content": "This section describes the contribution of this paper, namely the integration concepts of aggregate computing into the federated learning framework to improve adaptability and robustness in decentralized networks. Our approach enables the learning processes to dynamically conform to changes in network topology, effectively eliminating reliance on"}, {"title": "6.1. Learning Process", "content": "In this section, we mainly discuss the self-organizing hierarchical version, because the peer-to-peer version is simply an application of the aggregate constructs. At its core, self-organizing hierarchical FL can be conceptualized as an application of the SCR pattern [CPVN19]. In our framework, nodes can be designated as aggregators\u2014either through pre-assignment (e.g., edge servers) or dynamically selected based on the network's evolving topology, leveraging principles similar to those in space-fluid computing [CMP+23].\nInitially, each participating node $d \\in N$ initializes its model $w_d^{(0)}$. Subsequently, at each iteration t, it undertakes a local learning step to update $w_d^{(t)}$, after which the model updates are directed toward an aggregator node, guided by a dynamically formed potential field. Therefore, function is replaced by a field-based model sharing mechanism (namely, nbr) where the potential field guides the model updates to the aggregators, and $\\Rightarrow$ is replaced by collect cast operation to ensure that the model updates are directed to the appropriate aggregators nodes. Aggregator nodes play a crucial role in the system. They are responsible for collecting the model updates from participant nodes, computing a consensus model $w^{(t+k)}$ (where k is the global epoch value) by averaging, and disseminating the updated model back to the nodes in their zone of influence. This process ensures a distributed yet coordinated approach to model learning and sharing across the network. To accommodate communication delays and ensure timely model updates, nodes are capable of adjusting their"}, {"title": "6.2. Implementation Insights", "content": "The following ScaFi code snippet provides a self-organizing hierarchical federated learning implementation, based on the abode described SCR pattern. This variant utilizes potential fields for guiding model sharing and employs a broadcast mechanism to disseminate the aggregated global model back to the nodes:\nLISTING 1. Code structure of FBFL drawn from the respective repository.\nval aggregators = S(area, nbrRange) // Dynamic aggregator selection\nrep(init()) (model => { // Local model initialization\n\t// 1. Local training step\n\tmodel.evolve()\n\tval pot = gradient (aggregators) // Compute potential field for model sharing\n\t// 2. Model sharing\n\tval info = C [Double, Set [Model]] (pot, ++ Set (model), Set.empty)\n\t// 3. Model aggregation\n\tval aggregateModel = aggregation (info)\n\t// 4. Global model update\n\tsharedModel = broadcast (aggregators, aggregateModel)\n\tmux(impulsesEvery(epochs))\n\t\t{ combineLocal (sharedModel, model) } { model }\n}\nIn this code snippet, the aggregators variable represents the set of aggregators, dynamically selected based on the network's topology (i.e., via S). The rep construct initializes the local model, and the evolve method updates the model based on the local dataset via local training. The gradient construct computes the potential field for sending the model to the aggregators, and the C effectively collects the models from the whole region. The aggregation method aggregates the models, and the broadcast method disseminates the global model back to the nodes. Finally, the mux construct ensures that the model is updated at regular intervals and eventually combined with local corrections."}, {"title": "7. EXPERIMENTAL EVALUATION", "content": "To evaluate the proposed approach, we conducted experi-ments on three well-known used computer vision datasets: MNIST [LCB+10], FashionM-NIST [XRV17], and Extended MNIST [CATvS17]\u2014their characteristics are summarized in Table 2. We employed three state-of-the-art federated learning algorithms for compari-son: FedAvg [MMR+17], FedProx [LSZ+20], and Scaffold [KKM+20]. First, we evaluated FBFL against FedAvg under a homogeneous data distribution to assess its stability in the absence of data skewness. We then created multiple heterogeneous data distributions"}, {"title": "7.1. Experimental setup", "content": ""}, {"title": "7.2. Discussion", "content": "In the following, we present the results of our experiments, comparing the proposed approach with the baseline algorithms under different conditions and replying to the research questions posed in Section 2."}, {"title": "RQ1", "content": "How does the Field-Based Federated Learning approach perform compared to centralized FL under IID data?\nTo answer RQ1, we evaluated FBFL against FedAvg under homogeneous data distribution. The goal of this evaluation was to show how the proposed approach, based on field coor-dination, achieves comparable performance to that of centralized FL while introducing all the advantages discussed in Section 6, such as: the absence of a single point of failure and the adaptability. Figure 4 shows results on the MNIST (first row) and Fashion MNIST (second row) datasets. It is worth noting that both the training loss and the validation accuracy exhibit similar trends. As expected, our decentralized approach shows slightly more"}, {"title": "RQ2", "content": "Can we increase accuracy by introducing personalized learning zones where learning devices are grouped based on similar experiences as often observed in spatially near locations? More precisely, what impact does this have on heterogeneous and non-IID data distributions?\nTo rigorously evaluate our approach under non-IID conditions, we conducted extensive experiments comparing FBFL against baseline methods. We systematically explored two distinct types of data skewness: Dirichlet-based distribution (creating imbalanced class representations) and hard partitioning (strictly segregating classes across regions). The results reveal that baseline methods suffer from substantial performance degradation under these challenging conditions. These approaches consistently fail to capture the global objective and exhibit significant instability during the learning process. This limitation becomes particularly severe in scenarios with increased skewness, where baseline models demonstrate poor generalization across heterogeneous data distributions. Figure 5 presents key results from our comprehensive evaluation which encompassed over 400 distinct experimental configurations. The first row shows results from the Extended MNIST dataset using hard partitioning across 6 and 9 distinct areas. The performance gap is striking: baseline algorithms plateau at approximately 0.5 accuracy, while FBFL maintains robust performance above 0.95. Notably, increasing the number of areas adversely affects baseline models' performance, leading to further accuracy deterioration. In contrast, FBFL demonstrates remarkable stability, maintaining consistent performance regardless of area count. The second and third rows present results from Fashion MNIST and MNIST experiments, respectively. While baseline methods show marginally better performance on these datasets (attributable to their reduced complexity compared to EMNIST), they still significantly underperform compared to FBFL. These comprehensive findings underscore the fundamental limitations of traditional approaches in handling highly skewed non-IID scenarios. The results provide compelling evidence for RQ2, demonstrating FBFL's superior capability in maintaining high performance and stability across diverse data distributions through its innovative field-based coordination mechanism."}, {"title": "RQ3", "content": "What is the effect of creating a self-organizing hierarchical architecture through Field Coordination on Federated Learning in terms of resilience, adaptability and fault-tolerance?\nThe last experiment has been designed to evaluate the resilience of the self-organizing hierarchical architecture proposed by FBFL (RQ3). We simulated a scenario involving 4 distinct areas and a total of 50 devices. As for the other experiments, we ran"}]}