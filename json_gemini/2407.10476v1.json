{"title": "Kinetic Typography Diffusion Model", "authors": ["Seonmi Park", "Inhwan Bae", "Seunghyun Shin", "Hae-Gon Jeon"], "abstract": "This paper introduces a method for realistic kinetic typography that generates user-preferred animatable \"text content\". We draw on recent advances in guided video diffusion models to achieve visually-pleasing text appearances. To do this, we first construct a kinetic typography dataset, comprising about 600K videos. Our dataset is made from a variety of combinations in 584 templates designed by professional motion graphics designers and involves changing each letter's position, glyph, and size (i.e., flying, glitches, chromatic aberration, reflecting effects, etc.). Next, we propose a video diffusion model for kinetic typography. For this, there are three requirements: aesthetic appearances, motion effects, and readable letters. This paper identifies the requirements. For this, we present static and dynamic captions used as spatial and temporal guidance of a video diffusion model, respectively. The static caption describes the overall appearance of the video, such as colors, texture and glyph which represent a shape of each letter. The dynamic caption accounts for the movements of letters and backgrounds. We add one more guidance with zero convolution to determine which text content should be visible in the video. We apply the zero convolution to the text content, and impose it on the diffusion model. Lastly, our glyph loss, only minimizing a difference between the predicted word and its ground-truth, is proposed to make the prediction letters readable. Experiments show that our model generates kinetic typography videos with legible and artistic letter motions based on text prompts.", "sections": [{"title": "1 Introduction", "content": "Kinetic typography is an artistic motion graphics design combining text and animations [13]. Based on the word's meaning, motions are generated to convey information in videos. The main goal is eye-catching and to improve message retention. With the rise of video media, it has become an essential element in TV programs, commercials, music videos and film leaders.\nKinetic typography controls letters' shape (glyph), color and texture over time, and transforms their positions. Professional motion graphic designers"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Typography Generation", "content": "The key of typography is to make readable and visually-appealing text contents to readers. Most research in this field has focused on transferring the style of a single-letter design, including elements like color, glyph, font and effects, to other letters [1,2,5,8,9,15,30\u201334,40,42,45,53,57,60,62,64,70,75]. Additionally, there have been works on transferring an image style to the text contents [73]. Similarly, research on scene text editing, which changes text content while maintaining its own style in a scene, is also underway [27,43,54,69], whose extended version to video is available in [51]. In addition to changing text contents, color editing [47] and text segmentation [59,68] are proposed.\nWith the recent success of the high-fidelity text-to-image diffusion model [46], typography generation has gained interest. Works in [23,52,56] apply image styles into the target letter in an unsupervised manner.\nThere are concurrent works that display multi-letters, beyond the single-letter generation. They follow the multi-step approach that firstly generates layouts at specific positions, arrange them with multi-letters or multi-words with the same fonts and colors, and contextually infers the background [10,11,24,25,48,61,67,74,77]. However, these models are not specialized for typography. Since all letters are generated together, it is not editable for the letters, which makes it difficult to animate and add movement to each character."}, {"title": "2.2 Typography Video Generation", "content": "As we mentioned above, this work is the first attempt to generate kinetic typography. Although no existing works directly align with this, there are some works related to typography videos [26,28,29,38,63,65]. Here, we would like to introduce two methods related to ours.\nDynTypo [37] proposes a dynamic typography model that transfers the dynamic effect with realistic movements like fire and water on a specific uppercase English letter to others. Shape-Matching GAN++ [72] transfers an image style into a target letter by matching these shapes with structures of the target letter."}, {"title": "2.3 Text-to-Video Diffusion Models", "content": "The video diffusion model produces a visually-plausible and photo-realistic video based on text conditioning [7,12,16,18,20,36,49,58]. The main challenge in the video diffusion model is to maintain the temporal coherency between frames.\nThe pioneer works for video diffusion is to add temporal blocks to the text-to- image model and to learn temporal coherency between video frames [16,20,49]. Another type of relevant studies use pretrained models to leverage the temporal block [6,7,18]. In particular, Guo et al. [18] propose a motion module as the temporal block. It is designed as a plug-and-play module. After joint training with the pre-trained weight of stable diffusion [46], the module enables any text-to-image model to generate the video.\nExisting typography has been mainly studied using static single letters. The similar work to kinetic typography is also single letters, with only variation of motion effect and almost no letter's movement exists. To initiate kinetic typography generation, we present our KineTy dataset and model that allows dynamic motion, effects and glyph deformation of multi-letters, which will be explained at the next section."}, {"title": "3 Kinetic Typography Dataset", "content": "We describe how to build the KineTy dataset. Unlike previous datasets that only cover single-lettered images [71], our KineTy dataset has not only visual effects on multiple letters, but also their animations. We first provide detailed process"}, {"title": "3.1 Video Rendering", "content": "Employing templates for video rendering. The template is a pre-designed project file which contains a visual effect on letters. Many editors prefer to use the templates because it saves their time and labor cost in practice. Following the best practice, we utilize 584 kinetic typography templates from professional graphic designers for our dataset construction, as visualized in Fig. 2.\nSet of multiple letters. Next, we utilize the kinetic typography templates and randomly replace the text contents for augmentation. We employ multiple letters, in contrast to existing typography datasets with static single-letters [2,31,70]. Text contents are randomly generated by arranging up to 12 letters sampled from a set of 52 letters, including both uppercase and lowercase alphabets. Through this, we can expect rich letter-by-letter effects with various arrangements, while also keeping consistent styles across the multiple letters.\nSpeculation. We render videos with 1,920\u00d71,080 resolution for 3 sec., and 1,000 random words from 584 templates. Subsequently, all videos are downsampled to 512\u00d7288 resolution with 8fps for training. It takes a month to render the whole dataset with four i9 13900KF CPUs and an NVIDIA 2080ti GPU."}, {"title": "3.2 Video Captioning", "content": "Static and dynamic effect separation. In the next step, we caption the rendered kinetic typography for text-to-video generation. The professional editors usually design the static appearances at first, and then apply dynamic effects. Following this practice, we use two type of captions: static and dynamic captions, and labeled them accordingly.\nStatic effect captioning. To label static captions, we focus on letter's appearance which contains spatial information of typography. As shown in Fig. 2, the static captions Cstatic describe the color of the letters and the background, the glyph of the letters (e.g., outlined with yellow color or bold font), the characteristics of the background (e.g., textured and shiny background) and the arrangement of the letters (e.g., in a diagonal way) based on the last frame of the video where all the letters are displayed.\nDynamic effect captioning. Similarly, we concentrate on the temporal change of motions for each frame to write dynamic captions. Dynamic captions Cdynamic describe the motion part of the video, such as whether each letter appears in sequential or random order, can be rotated, or has a fade-in effect. For better systematic process, we initially label the videos with the GPT-4Vision model [41], and then manually verify and refine them to fix missing and wrongly labeled components.\nStatistics. The statistics of our dataset is summarized in Fig. 3. There are three main categories for the static appearance in Fig. 3 (a): Text type, text effect and background. Here, the text fill effects are dominant in the text type because it is the most obvious way to clearly show the text content. Especially, professional designers tend to prefer using solid colors, rather than text outlines. The reason why 57.7% of our dataset has no text effect is the readability of text contents. Since kinetic typography makes dynamic motions of the contents, users do not need for fancy visual effects on it.\nIn addition, we categorize dynamic motions into two-fold in Fig. 3 (b): motion type, including entrance, emphasis and movement, and sequence type: Our dataset supports various effect of entrance and emphasis to deliver striking messages. In contrast, the line has the majority in the movement because users' intention is usually conveyed after all letters in a scene are arranged in a line. In the same vein, when words come inside all at one and appear one-by-one in sequence, it is readable. They thus have the high portion in the sequence category."}, {"title": "3.3 Ground-truth Video Generation", "content": "Unlike generating random synthetic words in a training phase, we use real-world letters for evaluation by leveraging the templates. We can render ground-truth videos corresponding to user-input captions.\nFor evaluation, individual letters are selected for each alphabet letter from A to Z, and the first letters are capitalized for checking case sensitivity (e.g., Apple, Ball, ... Zebra). These 26 words are used for rendering 584 templates, so that a total of 15,184 videos are used for the evaluation."}, {"title": "4 Kinetic Typography Diffusion Model", "content": "In this section, we present our model for creating kinetic typography videos based on textual prompts. The key concept is to show animateble visual effects on texts, and our challenges in this work are summarized as: (1) the effect should be visually-pleasing and eye-catching to viewers; (2) the transition between frames should be smooth and align with the captions; (3) the text must be readable.\nWe begin by defining a kinetic typography generation problem in Sec. 4.1. We then describe how to model static and dynamic captions that efficiently guide appearance and motions in Sec. 4.2. We lastly discuss how to improve the glyph legibility with respect to the model design and its learning strategy in Sec. 4.3. Implementation details are provided in Sec. 4.4."}, {"title": "4.1 Preliminary", "content": "Conditional Latent Diffusion Models. Diffusion models train the data distribution by progressively refining a noisy initial state zm ~ \u2116(0,1) into the target data representation zo for M diffusion steps. Recent advancements, particularly in Latent Diffusion Models (LDMs) [46], enhance the efficiency by encoding an image x into a compact latent representation zo = E(x) using an encoder E, and is transformed back to the image x = D(zo) using a decoder D. These adding noise and its subsequent removal are done with U-Net-style denoising network en. In addition, a condition y is mapped to the hidden state h of en via attention:\nAttno (h, (h, y) = Softmax(\n$\\frac{Qo(h)Ko(y)}{\\sqrt{d}}$\n) \u2022 Vo(y)        (1)\ns.t. Qo(h) = Wo,q.h, Ko(y) = W\u04d9,\u043a\u00b7y, Vo(y) = We,vy,"}, {"title": "4.2 Spatial and Temporal Guidance", "content": "Static caption incorporation. Motivated by the best practice of professional designers who handle appearance and motion effects separately, we divide the caption into static and dynamic elements.\nWhen existing text-to-image models learn the distribution of image data, they are conditioned on texts related to the image's appearance. In the same manner, our model is also guided by captions describing the appearance of the text contents and background in each frame of video. Using Eq. (1), we define a spatial attention block with a self-attention followed by a cross-attention with a static caption Cstatic as:\nh':T = {Attno (Attne(h,\nTh), \u0393(Cstatic)) =1,  (4)\nwhere (.) is the CLIP text encoder [44].\nDynamic caption incorporation. A previous work [18] uses a motion module that only utilizes a self-attention between frames to learn the temporal consistency. On the other hand, in kinetic typography, it is essential to accurately display the dynamic motion effects of each letter in a video, following user's textual description. To do this, we extend Eq. (3) by adding the cross-attention with a dynamic caption as follows:\nh'0:T = Attn (Attn(ho:T, ho:T), \u0393(Cdynamic)),(5)\nThrough this process, the model becomes capable of maintaining temporal consistency as well as direct control over dynamic movements."}, {"title": "4.3 Enhancing Glyph Legibility.", "content": "Readability improvement. As a next step, we aim to make the legibility of our model better. To do this, we first classify descriptions for text contents and prompt because diffusion models often have a difficulty in distinguishing them. In this work, we put down a delimiter between each character for text contents.\nLet a text content L = {l1,l2, ..., lL} contain L letters. To condition each letter separately, we join each letter li in the text content with a delimiter symbol '|'. Further, we add a symbol '^' behind upper case letters because the clip encoder does not care of upper cases. To the end, when the word L denotes \"Apple\", L' is represented as \u201cA^|p|p|l|e\u201d. We denote the whole L as L' for better representation.\nAttention on text contents. It can be challenging for the attention module to guide both text contents and effects at once, while disentangling the text content information in CLIP feature space. What is worse, the text content can be overwhelmed by relatively long appearance and motion effects in CLIP text encoder. To tackle this problem, we introduce an additional cross-attention branch to the text content. Inspired by ControlNet [76], we regard the text content as conditions through the zero convolution operation. Starting with the definition of word caption Cword via a prompt template like \"The word {C'}\", we extend Eq. (4) by adding the cross-attention module between the word caption and hidden feature, weighted by the zero-initialized convolutions pas follows:\nhT-1\nstatic = {Attno (h', (Cstatic)) + pv (Attn, (h', (Cword)))} T-1,  (6)\nwhere h' = Attno (ht, h\u00b2).\nThis allows the network to gradually evaluate the usefulness of this additional caption based on the condition. In the same way, we incorporate the word caption into the temporal cross-attention in Eq. (5) as:\nh0:T\ndynamic = Attn (h', \u0393(Cdynamic)) +  (Attn (h', (Cword))),  (7)\nwhere h' = Attn(h0:T, h0:T).\nGlyph loss. To train our model, we use a common loss function Lldm. Additionally, we impose an extra penalty on the letter regions to enforce a sharp and correct glyph of text contents. We first use a binary mask B for the letters in the last frame VT from a text segmentation model [68]. The mask is then blurred to cover its surrounding effects. We then define a glyph loss based on LLDM with the additional pixel-wise weighting strategy using the blurred mask as follows:\nLglyph = Ex(x),y,e~N(0,1),m [||B\u00a9 (\u20ac - Ex (zm, m, y))||2],(8)\nwhere is the element-wise multiplication.\nWith the glyph loss, we enhance the legibility of text contents, enabling the precise creation of multi-letter formations. Through the linear combination of both loss terms, we can formulate the final loss function as L = Lldm + aLglyph. Here, we empirically set a to 0.01."}, {"title": "4.4 Implementation Details", "content": "To train our model, we use a two-step training strategy similar to that of human trainees [14]. First, we pre-train the network to generate spatial appearances using only static caption and last video frame pairs. Here, we detach the temporal attention modules to make them work as text-to-image diffusion models during 30 epochs with a batch size of 200. Next, we train the full model while freezing the spatial attention using whole video frames and captions for an epoch with a batch size of 8. For training, we resize the height H and width W of the video into 256\u00d7256 with T = 24 frames. The training is performed with AdamW optimizer [35], with a diffusion step M of 1000 and a learning rate of 0.0001, which usually takes about 20 hours for training on 8 NVIDIA A100 GPUs. The inference time is about 20 seconds when the number of sampling steps is 25."}, {"title": "5 Experiments", "content": "In this section, we conduct comprehensive experiments to verify the effectiveness of our model for kinetic typography. We first describe the experimental setup in Sec. 5.1. We then provide comparison results with relevant typography and video generation models, and report user-study to highlight the practical applicability of our model in Sec. 5.2. We lastly carry out an extensive ablation study to validate the effect of each component in our model in Sec. 5.3"}, {"title": "5.1 Experimental Setup", "content": "Comparison methods. We compare ours with state-of-the-art generative models, which consist of 3 two-stage methods and 2 one-stage methods. The two-stage methods are based on combinations of text-to-image models, including DS- Fusion [52], GlyphControl [74] and Text-Diffuser [11], and an image-to-video model, SparseCtrl [17]. DS-Fusion outputs a stylized letter based on the given style phrase and a letter. Since our caption is a sentence, we extract a style keyword from it and consider it as a style phrase. Note that DS-Fusion is able to generate one letter at a time, so we concatenate each letter to make a full- text content. GlyphControl takes an instruction to generate a glyph image for text contents, then uses the image as a condition for the final output image. Text-Diffuser first finds a proper layout for words and produce the output based on a caption and the layout. Since the comparison methods yield images, we combine them with an image-to-video model, SparseCtrl, to compare with ours. SparseCtrl uses a caption and a guidance such as sketches, depth maps and images to generate a stylized video.\nOn the other hand, one-stage-methods, AnimateDiff [18] and Lavie [58], are based on text-to-video models. AnimateDiff extends pre-trained text-to-image diffusion models with a motion module, and trains only the motion module to make fully use of the massive information of video dataset. Lavie leverages a temporal self-attention block to enhance a temporal consistency between frames while keeping the generation performance."}, {"title": "5.2 Evaluation Results", "content": "Quantitative results. As shown in Tab. 2, our KineTy model achieves the better performance than the comparison methods in the most metrics on all the categories. The comparison methods, despite their impressive performance in general image and video generation, show unsatisfactory performance in creating"}, {"title": "5.3 Ablation Study", "content": "Without caption separation. First of all, we train our network without dividing static and dynamic caption. The performances drop significantly for every metrics in Tab. 4. Since we train the text-to-image backbone using only static captions, dynamic captions are considered as noisy input. Similarly, the motion module, used to train dynamic motions, is effective when the simple motion guidance is given, compared to using static and dynamic caption together.\nText contents incorporation. We observe that text-to-image diffusion models often fail to catch up on text contents in the prompt when a long description is given. We thus use Cword as an additional input to give a strong attention to each letter. As shown in Tab. 4, without Cword, the performance degradation is observed as expected.\nWithout glyph loss. Lastly, we evaluate the performance without Lglyph. The inferior FVD scores come from the blur effect at the edge of the word along with color bleeding effects."}, {"title": "6 Conclusion", "content": "In this paper, we propose a generative kinetic typography model, named KineTy. To achieve this, we first build a large-scale kinetic typography dataset by collecting 584 templates from professional designers, and use them to train our diffusion-based network. To effectively handle the text prompt, we split it into static and dynamic captions. They are used to directly guide the spatial and temporal cross-attention for better appearance and motion effects, respectively. Our glyph loss also strengthens the legible and artistic letter generation. We lastly demonstrate that our KineTy produces the visually compelling and semantically coherent kinetic typography videos. The extensive user studies further validate the effectiveness of our model with respect to its practical utility."}]}