{"title": "MODELING TIME-VARIANT RESPONSES OF OPTICAL COMPRESSORS WITH SELECTIVE STATE SPACE MODELS", "authors": ["Riccardo Simionato", "Stefano Fasciani"], "abstract": "This paper presents a method for modeling optical dynamic range compressors using deep neural net- works with Selective State Space models. The proposed approach surpasses previous methods based on recurrent layers by employing a Selective State Space block to encode the input audio. It features a refined technique integrating Feature-wise Linear Modulation and Gated Linear Units to adjust the network dynamically, conditioning the compression's attack and release phases according to external parameters. The proposed architecture is well-suited for low-latency and real-time applications, crucial in live audio processing. The method has been validated on the analog optical compressors TubeTech CL 1B and Teletronix LA-2A, which possess distinct characteristics. Evaluation is per- formed using quantitative metrics and subjective listening tests, comparing the proposed method with other state-of-the-art models. Results show that our black-box modeling methods outperform all others, achieving accurate emulation of the compression process for both seen and unseen settings during training. We further show a correlation between this accuracy and the sampling density of the control parameters in the dataset and identify settings with fast attack and slow release as the most challenging to emulate.", "sections": [{"title": "INTRODUCTION", "content": "The emulation of analog musical devices constitutes a significant portion of consumer-grade music production and performance software. The nonlinear characteristics of their electrical and electronic components give analog audio effects a distinctive timbre and sound quality that is highly sought after by both music professionals and enthusiasts. Accurate modeling of these components, individually or as a complete system, is essential for high-fidelity emulation of their sonic responses. This pursuit has become an active area of research and development, commonly known as Virtual Analog (VA) modeling.\nTraditionally, analog effects are modeled using two primary approaches, physics-based and data-driven, which exhibit stark differences. The former relies on the mathematical description of physical phenomena, while the latter utilizes data measured from the actual devices. Physics-based modeling involves deriving and solving parametric mathematical equations that accurately describe an effect's behavior and allow for manipulating the effect's control parameters. This task is nontrivial; moreover, without simplifying assumptions, physics-based approaches can be computationally complex and may not be suitable for consumer-grade products. Conversely, data-driven approaches create digital signal processing algorithms by analyzing device measurements, aiming to replicate the input-output relationship rather than the underlying physical phenomena. These approaches often encounter limitations in flexibility. Since the model is constructed from a finite set of measurements and considering parameter combinations are theoretically infinite-it may be confined to the work only with settings included in the measurements, or it must estimate the sound alteration process for scenarios not covered by the measurements. Generally, these models tend to exhibit much lower accuracy when predicting behaviors beyond their training data as compared to the actual device."}, {"title": "", "content": "Recent advancements in neural networks have provided novel tools to mitigate the drawbacks of existing data-driven methodology. In particular, neural networks demonstrate generalization capabilities that can surmount classical data-driven algorithms. The first successful uses of neural networks to emulate analog effects were in modeling amplifiers and distortion effects (Mendoza, 2005; Covert & Livingston, 2013; Damsk\u00e4gg et al., 2019b; Wright et al., 2019). These works have highlighted key findings regarding the viability of artificial neural networks for Virtual Analog (VA) modeling: the models are perceptually accurate; the networks can be conditioned with the effect's parameters, allowing for dynamic changes in the sound alteration process; accurate models can be achieved when processing raw audio samples, with the networks satisfying real-time computational constraints on consumer-grade signal processing hardware; and the networks can process single or small blocks of audio samples, thus ensuring minimal or negligible latency suitable for live audio applications.\nMore complex types of audio effects have also been modeled using artificial neural network (ANN) modeling. This includes time-based effects such as delay (Mikkonen et al., 2023) and reverb (Ram\u00edrez et al., 2020), modulation effects like chorus (Wright & Valimaki, 2021), and dynamic effects like compressors (Steinmetz & Reiss, 2022; Simionato & Fasciani, 2022, 2023, 2024). These effects typically involve a sound alteration process with complex and often lengthy temporal dependencies between the input and output signals. However, at least in their basic form, artificial neural networks are memoryless computational structures. Common variations incorporating memory, such as recurrent networks, have shown significant limitations in modeling long temporal dependencies (Bai et al., 2018; Simionato & Fasciani, 2022). As a result, successful attempts to model these complex audio effects with neural networks have involved using large networks that process long blocks of audio samples. Consequently, these models generally have limited or no suitability for real-world live audio applications due to their excessive latency and high computational demands.\nThis article introduces a method for the neural modeling of dynamic range compression (DRC), focusing on analog optical compressors. DRC is a nonlinear effect that reduces the dynamic range of the input signal (Giannoulis et al., 2012). DRC is widespread in music production and live audio, typically employed to decrease the volume of louder segments, enhance quieter parts, or control transient peaks. Another common application is the 'sidechain' mode, where an external signal triggers the compression of the input signal based on its amplitude, often used to prevent masking in concurrent sounds, such as a kick drum and bass or a lead vocal and background elements. In radio broadcasting, a compressor can automatically reduce the music volume when someone speaks.\nAlthough not all compressors feature exactly the same type and number of parameters, the function of DRC is characterized by four main controllable quantities: threshold, compression ratio, attack time, and release time. The threshold sets the amplitude level above which the compressor's gain reduction mechanism is activated, and the ratio determines the degree of compression applied. Attack and release times dictate how quickly the gain reduction begins and ends after the threshold is exceeded. DRC applies a time-varying and level-dependent gain to the input signal. The behavior of DRCs is described by their compression curves, which map input amplitudes to output amplitudes. Below the threshold level, the curve represents a one-to-one relationship between input and output amplitudes, followed by a gain reduction part above the threshold. The compression curve features a 'knee' at the threshold amplitude; the smoothness of this knee which determines how gradually the compression is applied varies between devices in analog units due to their circuitry and is adjustable in most digital compressors. However, the compression curve is dynamic rather than static due to the non-zero attack and release times. As a result, the actual sound alteration process smoothly transitions from a linear relationship to the nonlinear compression curve, influenced by the duration set by the attack parameter, and then gradually returns to the original dynamics dictated by the release parameter.\nThe variable attack and release times help to adapt the compression to the input signal and the specific aesthetic needs. Faster attack times result in quick compression, which is useful for the immediate taming of peaks, while longer attack times allow for smoother compression and the preservation of some peak characteristics. Additionally, the compressor's design determines the minimum and maximum allowable times for compression. For example, field-effect transistor (FET) based compressors can effectively handle particularly sharp amplitude envelopes due to their quick attack and release times.\nIn this work, we consider optical compressors. When the input signal exceeds the threshold, the compressor diverts some of the voltage to the gain-reducing optical circuit. The signal voltage drives a light filament, which heats up and begins emitting light that progressively increases. The emitted light causes a light-sensitive variable resistor to increase its resistance, thereby reducing the output gain. This process starts immediately after the threshold is crossed, but there can be a lag in gain reduction due to the time it takes for the filament to heat up and emit sufficient light. This leads to a characteristically smooth response, often seen in optical analog compressors. Furthermore, this response time depends on the frequency and is also affected by previous heating cycles; hence, the input-output relationship can be quite complex, with significant dependencies on earlier parts of the input signal. These characteristics are challenging to capture with physical modeling techniques."}, {"title": "", "content": "Recurrent Neural Networks (RNNs) and State Space Models (SSMs) have demonstrated their capability to track the history of inputs across various application fields. However, when processing raw audio samples at a sampling rate of 48 kHz, managing a time dependency as brief as 100 ms requires the network to remember events that occurred 4, 800 iterations earlier, assuming the network predicts one audio sample per inference iteration. This requirement presents the challenge of incorporating an internal mechanism capable of maintaining relevant input information in the model's memory for a considerable number of iterations. Addressing variable long-term dependencies, such as those encountered in the case of audio compression, is a significant challenge.\nBuilding upon our previous contributions to analog DRC modeling, we have enhanced the recurrent neural architecture we previously proposed by incorporating the Selective State Space (S6) model (Gu & Dao, 2023) while preserving the original design's low latency and small computational costs. State space models have demonstrated their potential in sequence-to-sequence modeling tasks and, more recently, in capturing the significant temporal characteristics of analog effects (Simionato & Fasciani, 2024). Specifically, the Structured State Space (S4D) model (Gu et al., 2022b) has successfully applied to optical DRC.\nHowever, a disadvantage of the S4D model is that the parameters governing the computation of the internal states are not influenced by the input, which can be a limitation when the input plays a crucial role in the device's operation. In optical DRC, the input signal significantly affects the applied compression. The more recent S6 model overcomes this issue, and we demonstrate how this adaptation improves the learning process for analog optical compressor devices. We further enhanced the black-box modeling approach by integrating all the user-controllable parameters using Feature-wise Linear Modulation (FiLM) (Perez et al., 2018) and Gated Linear Units (GLU) (Dauphin et al., 2017) methods to condition the networks. Following the proposal in (Fasciani et al., 2024a), we placed the conditioning layer after the State Space layer. We also introduced a second layer after the conditioning block to provide additional handling of nonlinearities and time dependencies. Given that the DRC's control parameters govern various aspects of its operation, we divided the conditioning vector into two parts: one addressing dynamics and another focusing on timing characteristics, for which we employed the Temporal FiLM method (Birnbaum et al., 2019). Furthermore, we confirmed the efficacy of using the window technique, which aids the network in inferring the correct output sample by providing auxiliary past input information. In this context, we leveraged the spectral magnitude information from the input signal to facilitate the conditioning process.\nWe compare the proposed modeling method with other approaches based on the same architecture but featuring different types of layers: Long Short-Term Memory (LSTM) (Hochreiter & Schmidhuber, 1997) and S4D. We evaluate the models' overall learning abilities, focusing on extreme scenarios such as maximum compression and the slowest attack and release times. Additionally, we assess the ability of the models to predict compression for unseen combinations of conditioning parameters. We investigate the impact of the control parameters' dataset sampling resolution on the network's ability to interpolate between values seen during training. Finally, listening tests are conducted to evaluate the perceptual quality of the models subjectively."}, {"title": "BACKGROUND", "content": "In the last two decades, machine learning techniques, particularly artificial neural networks (ANNs), have been extensively employed for modeling analog audio effects. These techniques have been adopted as a black-box approach to emulate audio effects processing, manipulating raw audio samples in a manner similar to digital signal processing (DSP) methods.\nIn one of the early attempts, Mendoza (Mendoza, 2005) utilized a multilayer feedforward network to emulate both a high-pass filter and the Ibanez Tube-Screamer distortion effect. Although the network did not achieve satisfactory results when trained on the high-pass filter in the frequency domain, training in the time domain for the distortion effect showcased the potential of ANNs to learn and accurately replicate audio distortion effects. Feedforward networks are the simplest type of ANN. They do not have mechanisms to encode past information from the input data or to remember the system's state. To address this limitation, particularly when the modeling problem involves input-output temporal dependencies, the network's input is fed with a segment of the input signal. This segment includes already-seen past samples and is often much longer than the generated output, ensuring that each input-output pair used to train the network adequately represents such time dependencies. This approach is suitable for very short-time dependencies or may result in impractically high input-output latency for live audio applications.\nDamsk\u00e4gg et al. proposed to utilize one-dimensional convolutional-based networks to model the preamplifier circuit of the Fender Bassman 56F-A vacuum-tube amplifier (Damsk\u00e4gg et al., 2019a). This research demonstrated that Convolutional Neural Networks (CNNs) can significantly improve the accuracy of audio effect emulation compared to fully connected (FC) feedforward networks when variable control parameters are incorporated into the model."}, {"title": "", "content": "Employing the same architecture, models of the Ibanez Tube Screamer, the Boss DS-1, and the Electro-Harmonix Big Muff Pi distortion pedals have also been developed (Damsk\u00e4gg et al., 2019b).\nSimilarly to FC feedforward networks, CNN's ability to track the past information in the signal depends on the size of the receptive field; a larger receptive field necessitates a greater number of input past samples to predict a smaller set of more recent output samples. To overcome this limitation, Recurrent Neural Networks (RNN) have also been explored to model distortion effects. Specifically, RNN-based models of devices like the 4W Vox AC4TV vacuum-tube amplifier (Covert & Livingston, 2013), the Ibanez Tube Screamer, the Boss DS-1, and the Electro-Harmonix Big Muff Pi distortion pedals (Wright et al., 2019) have been developed and compared to their respective CNN-based models. These studies employ Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRU) networks as the recurrent layers. Results have shown that RNNs can achieve accuracy comparable to CNNs while offering a significantly lower computational cost for the inference and substantially reduced input-output latency, as they require only the current input sample to predict the current output sample.\nSteinmetz et al., in what appears to be the earliest attempt to model a DRC (Steinmetz & Reiss, 2022), proposed the use of Temporal Convolutional Networks (TCNs). In this work, the modeled device is the Teletronix LA-2A leveling amplifier, which features an optical gain reduction mechanism - challenges associated with its modeling were discussed in the introductory section. The model operates at a sampling rate of 44.1 kHz. The TCN employs dilated convolutional layers with rapidly growing dilation factors ranging from 22 to 210. As the dilation factor increases, so does the receptive field, enabling the network to handle long temporal dependencies without adding more layers. This approach led to the development of a model with relatively low computational demands; however, it still required significantly long segments of the input signal, which is unfavorable for latency. The various models used in comparative experiments exhibit input-output latencies between 100 and 1000 ms, corresponding to their respective receptive field lengths. TCN block used to model the LA-2A consists of a convolutional layer, batch normalization, conditioning using FiLM, and the Parametric Rectified Linear Unit (PReLU) activation function. The authors compared networks with 4, 5, 10 TCN blocks, with each convolutional layer having 32 channelsand with kernel sizes of 5, 13, and 15, respectively. The models correspondingly produced receptive fields of 100 ms, 300 ms, and 1000 ms. The model with a receptive field of 300 ms demonstrated the best accuracy.\nTo improve the networks' ability to encode the past signal information without relying on large layers, we proposed an Encoder-Decoder (ED) architecture initially utilized to model the optical compressor TubeTech CL 1B (Simionato & Fasciani, 2022). The network is structured with two LSTM layers; the first functions as an encoder, processing a short segment of past input samples alongside the compressor control parameters, while the second acts as a decoder. The decoder uses either the current input sample for single-sample prediction at each inference iteration or a current block of input samples for block-wise prediction. As the encoder processes the input signal, it computes and updates its internal states, which are then utilized as initial states by the LSTM in the decoder. This allows the states to effectively encapsulate a summary of the encoder's input signal, aiding the decoder in the output inference. This architecture demonstrated a significant improvement in accuracy compared to LSTMs and FC networks with an identical number of trainable parameters. The model operates at a sampling rate of 48 kHz and incorporates only two variable control parameters, namely threshold and ratio. This study investigated networks with a maximum of 128 units per layer and short input segments, ranging from 2 to 16 audio samples, resulting in a worst-case latency of 0.33 ms. The results show a positive correlation between segment size and accuracy.\nSubsequently, we augmented the model by including attack and release times as variable parameters with an enhanced architecture, where a convolutional layer replaced the encoder's LSTM (Simionato & Fasciani, 2023). This modification led to improved accuracy, albeit with a minor increase in the computational cost. However, this approach eliminated the continuous updating of internal states across sequences; states are recalculated at each iteration based on the number of the encoder's input samples. We experimented with up to 64 samples, bounding the worst-case latency to 1.33 ms. We evaluated 32, 64, and 128 as model input segment sizes, with the smallest delivering the best accuracy. Network size was kept small to limit computational complexity, with a maximum of 128 units per layer, consistent with prior work. The best-performing model from our approach was compared against the top TCN model from (Steinmetz & Reiss, 2022) using our TubeTech CL 1B dataset and their Teletronix LA-2A dataset. Results showed that our model exhibited better accuracy in the CL 1B case. In comparison, the accuracy was slightly worse but comparable to that of the TCN in the LA-2A case despite the TCN model using significantly larger input segments. However, when the TCN was adapted to work with shorter input segments\u2014thereby matching the input-output latency of our model its accuracy was significantly inferior to that of our ED model. Moreover, the CL1B dataset included cases with significant compression at low thresholds (e.g., -30, -40 dBu), which results in audible artifacts due to minor signal amplitude mismatching at the edges of consecutive output audio segments. These artifacts were observed in both the ED and TCN models. However, the models exhibit limitations in scenarios of heavy compression, where they fail to replicate the abrupt and rapid gain reduction that typically occurs with onsets. Rapid attacks, when combined with heavy compression, are"}, {"title": "", "content": "particularly challenging to model, as are extended-release times, which can span up to 10 seconds in the case of the CL 1B.\nA gray-box model, which combines information about the device's functioning with automatic parameter estimation, was proposed for the Teletronix LA-2A optical leveling amplifier (Wright & V\u00e4lim\u00e4ki, 2022). The model incorporates the structure of a traditional digital compressor and predicts the parameters of the compression curve, including the threshold, ratio, and knee, using FC- and RNN-based networks. Similar techniques are employed to predict the timing parameters that define the attack and release envelopes of the equations designed and used to describe the compressor. This approach leverages domain knowledge to help generate audio with fewer artifacts and reduce the number of trainable parameters. However, the accuracy of this approach is constrained by the inherent approximations present in the equations representing the system. In this study, for example, timing effects are modeled using three methods: the first assumes equal attack and release times, independent of the input signal; the second assumes varying attack and release times, but still not in response to the input signal; and the third assumes attack and release times are equal but do vary based on the input signal. The latter two methods demonstrated better performance. However, all these scenarios fail to accurately represent the actual timing behavior of the device, as, in reality, attack and release times may vary independently of each other and are influenced by the signal's energy. Consequently, due to these approximations, the model is inaccurately biased, which compromises its precision. On the other hand, the model brings the advantage of requiring 10% of the number of operations needed by a black-box GRU model with 32 units. In addition, the gray-box model does not require long input segments to achieve greater accuracy and, as a result, does not introduce added latency.\nIn most optical compressors, such as the LA-2A and CL1B, formalizing the variable attack and release behavior with mathematical expressions is not straightforward, which makes the use of black-box design potentially advantageous. The complex nonlinear and time-variant behavior of compressors presents a significant challenge for traditional RNN and CNN architectures. Recent advances in sequence-to-sequence modeling have introduced State Space models (SSMs) that rely on a continuous-time representation of the state space formulation (Gu et al., 2022a). These models have demonstrated promising results, especially with long sequences. Similar to RNNs, they retain state information while addressing the vanishing memory problem by eliminating nonlinearity in the computation of states. We implemented a variant of SSMs, the S4D model, for a range of analog effects, including the aforementioned compressors (Simionato & Fasciani, 2024). The results indicated that the S4D model improved the modeling of compressor effects but did not markedly outperform LSTMs. A limitation of these models became apparent when the parameters were found to be static and not influenced by the input signal (Gu & Dao, 2023). Incorporating input-dependent variation can enhance the model's content-based predictions, a critical aspect for compressors where the degree of compression is dependent on the present and past input signal's level.\nIt is also worth mentioning that prior to the advent of SSM, transformer architectures had already shown improvements in multiple application domains over CNNs and RNNs. However, the context of the input window also affects the performance of attention operations in transformers. Therefore, just like CNNs, these architectures require as many past input samples as necessary to account for temporal dependencies in the input-output data. Due to this characteristic, which is detrimental to latency, they have not been employed in the modeling of analog effects."}, {"title": "PROPOSED MODEL", "content": "This work proposes using Selective State Space (S6) networks to model optical dynamic range compressors. The architecture we have designed offers advantages due to its superior capability for capturing temporal dependencies in the input-output signals. As previously mentioned, in the S6 models, the matrix governing the computation of the internal states depends on the input signal. This represents an improvement over earlier efforts based on the S4D model in audio effect modeling, where the matrix does not depend on the input. This approach is particularly beneficial for capturing the complex interplay between the input signal and gain reduction\u2014a relationship dominated by the coupling of light-emitting and light-sensing elements\u2014and is especially useful for accurately modeling the characteristically slow attack and release times of optical compression. Our investigation is restricted to experiments with relatively small networks with limited computational complexity and latency, making them suitable for consumer-grade systems and live audio applications. We assess modeling accuracy by comparing the proposed Selective State Space-based architecture against the state-of-the-art models, including those based on LSTM, ED, S4D, and TCN."}, {"title": "Network Architecture", "content": "The architecture we propose for modeling analog optical compression is centered on Selective State Space (S6) layers, as illustrated in Figure 1. It includes a linear FC layer that processes and compresses the input, which then feeds into an S6 block. The conditioning layers process this block's output based on the values of the compression parameters p,"}, {"title": "", "content": "before passing its output through a second, identical S6 block. The output layer consists of a single unit that predicts a coefficient. When multiplied by the input sample, this coefficient produces the output sample. The architecture generates one output audio sample per inference cycle, taking as input a buffer of the 64 most recent input samples. This implies that with each inference cycle, a new input sample enters the buffer, while the oldest is discarded. Therefore, the set of the 64 most recent input samples constitutes the network input, which, after initial compression by the FC layer, is used to update the states in the S6 layers. This approach aids the network in making predictions while maintaining a stateful design using truncated backpropagation through time. Predicting one sample at a time helps minimize the audible artifacts commonly associated with machine-learning models of audio effects, which arise from slight amplitude mismatches at the boundaries of consecutive output segments."}, {"title": "", "content": "The mathematical description of the S6 layer is the following:\n$h_n = A h_{n-1} + B x_n$\n$Y_n = C h_n + D x_n$\nwhere x and y are the input and output samples at time n. A(N \u00d7 N), B(N \u00d7 1), C(1 \u00d7 N), and D(1 \u00d7 1) are complex or real-valued matrices expressing linear mappings between In and hn the states of the system. The layer captures the long-range dependencies using the state matrix A. This matrix encodes all the past input history in hn, finding a map from In to a higher N-dimensional space hn that represents the compression of the input's past. In the S6 layer, B and C are dependent on the input and computed using a linear FC layer, while A and D's parameters are independent of the input and learned during the training process.\nThe S6 layer is used inside a block, as in (Gu & Dao, 2023), illustrated in Figure 2. The first layer of the block is a linear FC with a number of units equal to double the length of the desired internal dimension, thereby expanding the block's input size. The chosen internal dimension is 3, corresponding to the output size of the S6 layer, selected to limit the total number of model parameters. The expanded vector is split into two equal-sized vectors, one passing the convolutional layer and, afterward, swish function (Ramachandran et al., 2017) and finally the S6 layer; the other is element-wise multiplied by the output of the S6 layer, after the swish function as well. The swish function acts as a gating mechanism to regulate the flow of information and features a learnable parameter, providing smoother behavior than a ReLU. Finally, since S6 does not include nonlinear operators, the block output is processed through a nonlinear FC with 2 units and the Gaussian Error Linear Unit (GELU) (Hendrycks & Gimpel, 2023) activation function. Figure 2 illustrates the described process, including the size of each layer, which can be described as follows:\n$[s_p, s_{res}] = FC(x_{proj})$\n$s_{conv} = Conv(s_p)$\n$s_{ssm} = S6(Swish(s_{conv}))$\n$s_s = s_{ssm} \\otimes Swish(s_{res})$\n$s_o = GELU(FC(s_s))$"}, {"title": "", "content": "The conditioning block processes separately the two distinct groups of control parameters p: those influencing the amount of compression Pco, and those determining the timing behavior pti. Both parameter groups are processed alongside the vector f, computed processing the magnitude spectrum of the 64 input samples x with a convolutional layer. The kernel size of this layer is set to one more than the FFT size used to compute the magnitude spectrum. An FFT size of 128 was selected based on preliminary experiments aimed at determining the optimal size, with consideration given to limiting the total number of computations.\nCompression-related parameters Pco concatenated with f derived from the magnitude spectrum are passed to a linear FC layer to compute the modulation vectors a and b to perform the FiLM on gm:\n$k_f = a g_m + b$\nThe output kf is processed by a GLU layer, where another linear FC layer doubles the input dimensionality, producing kf1and kf2 which are then used to compute the output kg1 as follows:\nThen kf is processed by a GLU layer, followed by another linear FC layer producing two vectors kf1 and kf2. These vectors are then used to compute kg1 as follows:\n$k_{g1} = k_{f1} \\otimes softsign(k_{f2})$\nTiming-related parameters Pti, concatenated with f, are processed by an additional modulation layer, which in this case utilizes a GRU layer to produce the modulation vectors c and d, which are then applied to perform the Temporal FiLM on kg.\n$k_{nf} = c k_g + d$\nSimilar to the previous step, knf is processed by a GLU layer, followed by another linear FC layer that produces two vectors, kf3 and kf4. These vectors are then used to compute the output of the conditioning block, denoted as gc, as follows:\n$g_c = k_{f3} \\otimes softsign(k_{f4})$\nThe input and output dimensionalities of the conditioning block, as well as those of the internal intermediate quantities, are always equal to 2."}, {"title": "", "content": "As detailed in Section 4.1, the LA-2A features a continuous parameter called 'peak reduction', which defines the amount of compression, and a switch to toggle between limit and compression modes, which influences the amount of compression of the device. The attack and release times are not controllable and depend on the input signal. Therefore, for the LA-2A, Pco includes the 'peak reduction' and the binary value of the mode switch, and pti includes only the vector f. On the other hand, the CL 1B offers four controllable parameters: threshold, ratio, attack, and release time. However, the threshold and ratio influence the amount of compression, while the attack and release times affect the timing behavior of the device. Therefore, for the CL 1B, Pco includes the values of the threshold and ratio, while Pti includes the values of the attack and release times."}, {"title": "", "content": "To evaluate the effectiveness of the proposed architecture in modeling analog optical compression, we designed three additional architectures that share the same conditioning process but feature different recurrent blocks, specifically based on LSTM, ED, and S4D, as illustrated in Figure 4. This approach allows us to compare their capabilities in modeling nonlinearities and encoding temporal information and to compare their performance when integrated with the same conditioning process. The S4D architecture uses the S4D layer, which is also described by Equation 3.1; however, in this case, the parameters B, C, and D are trainable and not functions of the input \u00e6. As mentioned earlier, the principal difference with the S6 variant lies in this aspect. As the S6, the S4D layer incorporates a FC layer followed by a GELU activation function, as it does not inherently include nonlinearity. The ED architecture utilizes an encoder-decoder structure inspired by our previous work (Simionato & Fasciani, 2023), which processes the input segment with a convolutional layer to establish the initial internal states for the first LSTM layer. Unlike the first, the second LSTM layer relies on its own internal states. The ED architecture also incorporates a linear FC layer following the LSTM layers. The S4D architecture employs a specialized SSM layer, which is an efficient variant that parameterizes its state matrix as a diagonal matrix (Gu et al., 2022b). The LSTM architecture consists of an LSTM layer followed by a linear FC layer, embodying the most traditional approach. In addition to the architectures described above, we also use several previously proposed models as baselines for comparing performance. These models include LSTM-b (Steinmetz & Reiss, 2022), ED-b (Simionato & Fasciani, 2023), and TCN-b (Steinmetz & Reiss, 2022). The ED-b model employs a CNN layer to process the input signal and a linear FC layer to process the control parameters in the encoder. The outputs of these two processes are then combined to initialize the states for the LSTM in the decoder. Following the LSTM is an FC layer with a sigmoid activation function. The LSTM-b architecture comprises an LSTM layer that receives input samples concatenated with the control parameters. Both the LSTM-b and ED-b baselines have an identical input, including 64 samples. They are designed to have a number of trainable parameters comparable to that of"}, {"title": "Computational Cost and Latency", "content": "Our proposed architecture has been designed to minimize computational complexity during inference by limiting the total number of trainable parameters to approximately 1,000. We applied the same parameter limit to the other architectures included in our comparison to ensure a fair evaluation across different designs. Slight deviations from this limit are permissible due to the use of significantly different layers in each case. Starting from the equations defining each layer's operations, we estimated the computational cost of inference for our proposed architecture in terms of Floating Point Operations (FLOPs) per audio sample. Our calculations of FLOPs consider both multiplication and addition operations. For the activation functions Softsign, Swish, GELU, and hyperbolic tangent, we have counted 10 FLOPs per sample, respectively.\nTable 1 presents the computational cost and number of trainable parameters for both the proposed architecture, which includes the S6 block as shown in Figure 1, and for all the variants illustrated in Figure 4. Since CL 1B has twice as many control parameters as LA-2A, which are used to condition the model, the proposed architecture yields slightly different computational costs and numbers of trainable parameters when applied to these two distinct analog optical compressor devices. The conditioning block requires 352 and 400 FLOPs for CL 1B and LA-2A cases, respectively.\nThe latency for all models is 64 audio samples, corresponding to 1.33 ms at a sampling rate of 48 kHz, with the exception of the TCN baseline model, which exhibits a latency of 14400 audio, samples equivalent to 300 ms."}, {"title": "Methods", "content": "We have conducted an extensive set of experiments to assess the modeling accuracy of the proposed model, which is based on a S6 architecture, against state-of-the-art alternatives, including LSTM, ED, S4D, and TCN architectures. Evaluations are based on quantitative metrics and listening tests. We have collected and used different datasets to"}, {"title": "Datasets", "content": "The datasets include recordings from both analog hardware devices and their digital software emulations. Specifically it includes recordings from the TubeTech CL 1B \u00b9 and Teletronix LA2A \u00b2, shown in Figure 5, along with their software emulations: Softube CL 1B 3 and the Universal Audio LA-2A 4."}, {"title": "", "content": "The CL1B", "parameters": "ratio", "2": 1, "10": 1, "modes": "fixed", "compressor": "the attack and release times can not be manually adjusted by"}]}