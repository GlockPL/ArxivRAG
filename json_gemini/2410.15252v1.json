{"title": "Lossless KV Cache Compression to 2%", "authors": ["Zhen Yang", "J. N. Han", "Kan Wu", "Ruobing Xie+", "An Wang", "Xingwu Sun", "Zhanhui Kang"], "abstract": "Large language models have revolutionized data processing in numerous domains, with their ability to handle extended context reasoning receiving notable recognition. To speed up inference, maintaining a key-value (KV) cache memory is essential. Nonetheless, the growing demands for KV cache memory create significant hurdles for efficient implementation. This work introduces a novel architecture, Cross-Layer Latent Attention (CLLA), aimed at compressing the KV cache to less than 2% of its original size while maintaining comparable performance levels. CLLA integrates multiple aspects of KV cache compression, including attention head/dimension reduction, layer sharing, and quantization techniques, into a cohesive framework. Our extensive experiments demonstrate that CLLA achieves lossless performance on most tasks while utilizing minimal KV cache, marking a significant advancement in practical KV cache compression.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have been widely adopted and verified in various of fields, reshaping the way we collect and process information and impacting our daily lives (Driess et al., 2023; Zhang et al., 2023; Zhu et al., 2023; Wang et al., 2024b). Recently, the long-context reasoning and understanding abilities of LLMs have gradually been acknowledged to be essential in reflecting LLMs' capabilities and attracted more and more attention. Both closed-source and open-source LLMs are now striving to accommodate longer token lengths (Achiam et al., 2023; DeepSeek-AI, 2024). However, this rapid expansion in length introduces critical efficiency challenges into LLMs, particularly concerning the growing key-value (KV) cache memory issue, which poses a significant barrier to the practical deployment of more powerful LLMs. The KV cache technique, which involves caching and reusing previously computed key and value vectors from the classical multi-head attention (MHA) blocks (Vaswani et al., 2017) in decoder-only Transformers, is broadly adopted to accelerate model inference speed. This approach, however, comes at the cost of increased GPU memory usage and is considered a default configuration in most decoder-only LLMs (Bai et al., 2023; Touvron et al., 2023). Alleviating the increasing KV cache issue could free up GPU memory for other high-performing yet memory-intensive techniques, or alleviate constraints related to essential parameters such as batch size and token lengths, which are crucial for deploying more effective LLMs (Sheng et al., 2023; Pope et al., 2023; Ainslie et al., 2023).With the rapid growth of LLM sizes, numerous studies have raised concerns regarding the expanding KV cache and proposed various solutions. The overall KV cache size is associated with several factors, including the number of attention heads, the number of layers, the dimension of each head, and the sequence length. To enhance MHA efficiency in Transformers, methods such as multi-query attention (MQA) (Shazeer, 2019) and grouped-query attention (GQA) (Ainslie et al., 2023) are proposed to reduce KV cache requirements by decreasing the number of KV heads. Additionally, multi-head latent attention (MLA) has been developed to mitigate performance degradation through low-rank KV joint compression (DeepSeek-AI, 2024). These methods concentrate more on the attention head and dimension aspects. Conversely, another set of methods focuses on reducing the number of layers involved in KV cache. YOCO (Sun et al., 2024) proposes a decoder-decoder architecture with linear attention that compresses the cache to a single layer. Similarly, CLA (Brandon et al., 2024) and MLKV (Zuhri et al., 2024) share KV activation across adjacent layers, reusing KV cache of earlier"}, {"title": "2 Related Works", "content": "Previous research has utilized techniques such as reducing the dimensions of attention heads and employing low-rank matrix approximations to enhance model efficiency. Some recent work reduces the KV cache through the layer dimension(Wu and Tu, 2024; Zuhri et al., 2024; He and Wu, 2024; Liu et al.). Notably, Yu effectively compressed models by minimizing errors in MHA-to-GQA transitions (Yu et al., 2024), while maintaining compatibility with ROPE (Su et al., 2023). Some research efforts concentrate on reducing KV cache requirements by decreasing the sequence length, using methods such as token dropping (Zhang et al., 2024b; Li et al., 2024; Xiao et al., 2023; Shi et al., 2024) and prompt compression (Jiang et al., 2023; Chuang et al., 2024). However, these methods often lead to a notable decline in performance.Recently, many research efforts have employed quantization compression to reduce KV cache, thereby improving inference efficiency (Banner et al., 2018; Wu et al., 2020; Yang et al., 2024; He et al., 2024; Liu et al., 2024a). Gear (Kang et al., 2024) utilizes low-rank matrices and low-precision quantization to achieve near-lossless 4-\n2.1 Large Language Models\nRecently, transformer-based models have garnered significant attention due to their powerful performance in various fields, including NLP and multi-modal tasks. Large language models such as GPT-4 (OpenAI et al., 2023) and LLaMA3 (AI@Meta, 2024) have achieved remarkable results across a range of natural language processing tasks.However, the traditional decoder-only architecture incurs substantial training and online inference costs. To address this issue, Palm (Chowdhery et al., 2023) and StarCode (Li et al., 2023b) utilize MQA (Shazeer, 2019), where all attention heads share a group of KV. This approach aims to reduce the computational overhead associated with the attention mechanism. LLaMA2 (Touvron et al., 2023) employs a different strategy within its attention blocks by grouping keys and values (GQA) (Ainslie et al., 2023) to minimize the KV cache during inference. Concurrently, it increases the feed-forward network hidden size to maintain model performance. FlashAttention (Dao, 2023) optimizes training and inference efficiency from the perspective of GPU I/O operations. This method specifically targets the bottlenecks in data movement and computation within the GPU architecture. Mixture-of-Experts models (Jiang et al., 2024; Muennighoff et al., 2024; Wang et al., 2024a) utilize sparse architectures to achieve improved performance.\n2.2 KV Cache Compression"}, {"title": "3 Methodology", "content": "Currently, transformer model is the most prevalent LLM architecture. It is composed of many blocks stacked together. Each block includes an attention layer and a MLP layer. Multi-head Attention (MHA) (Vaswani et al., 2017) is a typical design for the attention layer. Given the sequence hidden states H = [h_1, h_2, ..., h_t], the computations of"}, {"title": "3.1 Preliminary of KV Cache", "content": "Currently, transformer model is the most prevalent LLM architecture. It is composed of many blocks stacked together. Each block includes an attention layer and a MLP layer. Multi-head Attention (MHA) (Vaswani et al., 2017) is a typical design for the attention layer. Given the sequence hidden states H = [h1, h2, ..., ht], the computations of"}, {"title": "3.2 CLLA: CLA Meets MLA", "content": "CLLA involves compressing KV cache into low-rank latent states and subsequently allowing multiple layers to share these states. The key research point is how to effectively share compressed latent states across layers.MLA basics. To begin with, we project the input hidden states of the attention layer into low-rank"}, {"title": "3.3 CLLA-quant: Latent Quantization", "content": "To further reduce memory overhead associated with KV cache, we implement low-bit quantization on latent states by converting them from 16-bit floating-point values into lower-bit integers using symmetric quantization (Nagel et al., 2021). The quantization function can be expressed as follows:\n$Q(C) = \\text{int} \\left( \\text{clip} \\left( \\frac{C}{S}, I_{\\text{min}}, I_{\\text{max}} \\right) \\right),$\n$S = \\frac{\\text{max}(|C|)}{I_{\\text{max}}},$\n$\\text{clip}(x, a, b) = \\text{max}(a, \\text{min}(b, x)) .$\nHere C represents the latent states, and Q (C) represents their quantized version stored as b-bit integers. [] denotes the rounding operator, while S serves as the scaling factor. The latent states are scaled to the restricted range [Imin, Imax], which is set to [- (2b-1 \u2013 1), 2b \u2013 1]. The quantized latent states are stored in cache and dequantized only when needed during computations. The dequantization function is defined as follows:\n$\\hat{C} = S \\times \\text{float}(Q(C)) .$\nThe recovered latent states \u0108 are then utilized in computations. We adopt 4-bit for CLLA-quant.Quantization recipe. To minimize quantization error further, we employ sub-channelwise quantization techniques (Gholami et al., 2022), where every g elements are grouped, with each group sharing a scaling factor. Additionally, prior to quantization, latent states are normalized using RMSNorm (Zhang and Sennrich, 2019). Both quantization and dequantization processes are applied during training and deployment phases. In training mode specifically, gradients associated with quantization are approximated using the straight-through estimator (STE) (Bengio et al., 2013)."}, {"title": "4 Experiments", "content": "The training dataset was collected in-house and consists of a cleaned combination of Chinese and English datasets. We trained all models on 300 billion tokens. For evaluation, we aim to achieve robust conclusions across diverse domains in both English and Chinese. We conducted comprehensive evaluations involving 7 English tasks, including MMLU, Hellaswag, ARC-C, Winogrande, BoolQ, PIQA, and SIQA. For Chinese tasks, we evaluated models on 8 tasks: CMMLU, CEval, CLUEWSC2020, Thuc-news, OCNLI, C3, FewCLUE-CHID, and CMNLI. More details are noted in Appendix B.In order to ensure equitable comparison, the intermediate dimensionality of the feed-forward neural network (FFN) was adjusted to maintain comparable activation parameters across all models. The Appendix elaborates on the implementation specifics of each model. The training process was executed utilizing 64 NVIDIA H800 GPUs, with the same hardware resources employed for the inference evaluations. Complete details of the training configurations are provided in the Appendix.\n4.1 Datasets and Experimental Settings"}, {"title": "4.2 Competitors", "content": "For every competitor, we utilize the same 1.44 billion activation parameters with a Mixture of Experts (MoE) architecture (Fedus et al., 2022; Lepikhin et al., 2020). Specifically, we employ a model with a 1Bx16 MoE configuration, comprising nearly 11 billion parameters in total, to ensure fair comparisons.The following methods were evaluated:MHA: The foundational transformer architecture (Vaswani et al., 2017; Radford et al., 2019) resembles that of the Llama2 model (Touvron et al., 2023). It features SwiGLU (Shazeer, 2020) as the activation function for the feed-forward layer and utilizes ROPE (Su et al., 2023) for the embedding of relative positions. All subsequent models are variations of this foundational model.GQA: GQA (Ainslie et al., 2023) groups query heads together, allowing each group to share keys and values. We set the number of KV heads to 8 out of a total of 16 query heads to reduce KV cache by half.MLA: In MLA (DeepSeek-AI, 2024), the KV cache is mapped into a low-rank space. The latent dimension for this low-rank space is set at 512.CLLA: Our method combines the settings from MLA and CLA (Brandon et al., 2024) while adhering to the aforementioned sharing configurations.CLLA-quant: We applied int4 scalar quantization to compress latent states with a quantization group size of 32 on the original CLLA.In each model, the hidden size is configured to"}, {"title": "4.3 Main Results", "content": "After pre-training on 300 billion tokens, we evaluated all models across 15 benchmarks that encompass a wide range of tasks. All models are compared in the same tokenizer and data. On English benchmarks shown in Table 2, MLA are comparable with GQA baseline. The proposed models CLLA and CLLA-quant achieve better results with higher compress ratio. We can achieve similar conclusion on Chinese benchmarks presented in Table 3. Additionally, theoretical KV cache memory bytes usages for each model are shown in Table 4. From these tables, several interesting observations emerge regarding the effectiveness of our methods in achieving high KV cache compression ratios without performance degradation:First, CLLA notably surpassed MHA in performance while also achieving a significant reduction in memory usage of nearly 95%. For English tasks, CLLA's performance was about 1.24 point higher than that of MHA, and it showed similar enhancements for Chinese tasks. This improvement"}, {"title": "5 In-depth Model Analyses on CLLA", "content": "It is nontrivial to compress KV cache from different aspects including layer, dimension, and bit levels. In this section, we conduct extensive explorations on different (successful or failed) strategies and report the insights found in our experiments, hoping to facilitate future research.\n5.1 Intra-layer vs. Inter-layer Sharing"}, {"title": "6 Conclusion and Future Work", "content": "In this work, we introduce CLLA and CLLA-quant, two effective techniques for compressing KV caches. Both methods achieve substantial reductions in KV cache size without compromising performance, with CLLA-quant achieving memory usage as low as 2% while maintaining performance comparable to the baseline. We identify optimal design choices that effectively combine attention head/dimension reduction, layer sharing, and quantization techniques, which is verified by extensive experiments and analyses. Although CLLA is straightforward, it successfully confirms the feasibility of lossless large-ratio KV cache compression, which is promising to be the superior fundamental component for efficient LLM inference.In the future, we will explore our CLLA series"}, {"title": "A Implementation Details", "content": "To ensure a fair comparison, we primarily adjusted the intermediate size of the feed-forward neural network (FFN) to maintain similar activation parameters across models. The implementation details for each model are presented in Table 8. Aside from the parameters listed here, all other parameters are set to their default values. For instance, all models were configured with 32 layers and a vocabulary size of 128,256. In the MLA strategy, we established the KV LORA rank at 512, the Q LORA rank at 768, and the query-key rope head dimension at 64.All training runs were conducted using 64 NVIDIA H800 GPUs, with inference evaluations performed on the same hardware configuration. All the training settings as specified in Table 9."}, {"title": "B Evaluation Datasets Details", "content": "A Greek-language variant of the SWAG dataset, assessing a model's ability to reason about everyday situations.(zero shot)\nSpecifically designed for natural language inference tasks.(zero-shot)\nWe provide detailed evaluation scores for the models discussed in Section 5, which include benchmarks for both English and Chinese tasks in Table 10 and Table 11.\n\u2022 MMLU (Hendrycks et al., 2020): A large-scale dataset for evaluating a model's understanding across multiple languages and domains. Models are evaluated on the VAL split with 5 shot.\n\u2022 Hellaswag (Zellers et al., 2019): A Greek-language variant of the SWAG dataset, assessing a model's ability to reason about everyday situations.(zero shot)\n\u2022 ARC (Clark et al., 2018): A dataset designed to test a model's ability to answer questions that require complex reasoning and commonsense knowledge.(zero shot)\n\u2022 Winogrande (Sakaguchi et al., 2021): An extension of the Winograd Schema Challenge, focusing on pronoun disambiguation in a large-scale setting.(zero shot)\n\u2022 BoolQ (Clark et al., 2019): A dataset consisting of yes/no questions, where the answers can be inferred from a given passage.(zero shot)\n\u2022 PIQA (Bisk et al., 2020): A dataset that combines visual and textual understanding, where models must answer questions about images based on a persona.(zero shot)\n\u2022 SIQA (Sap et al., 2019): A dataset that evaluates a model's ability to understand social scenarios and make appropriate inferences. (zero shot)\n\u2022 CMMLU (Li et al., 2023a): A Chinese version of the MMLU dataset, focusing on multitask language understanding in Chinese. Models are evaluated on the VAL split with 5 shot.\n\u2022 CEval (Huang et al., 2024): A benchmark for evaluating Chinese natural language processing tasks with 5 shot.\n\u2022 CLUEWSC2020 (Xu et al., 2020): A Chinese dataset for named entity recognition in the financial domain.(zero shot)\n\u2022 Thuc-news (Sun et al., 2016): A dataset containing Chinese news articles, used for text classification and other NLP tasks.(zero shot)\n\u2022 OCNLI (Hu et al., 2020): A Chinese dataset for natural language inference tasks.(zero shot)\n\u2022 C3 (Sun et al., 2020): A dataset designed to assess a model's understanding of commonsense knowledge in Chinese.(zero shot)\n\u2022 FewCLUE-CHID (Xu et al., 2021): Few-shot Chinese Language Understanding Evaluation - Chinese Health Information Dataset. A dataset for few-shot learning in the healthcare domain in Chinese.(zero shot)\n\u2022 CMNLI: Chinese multilingual natural language inference dataset in CLUE (Xu et al., 2020), specifically designed for natural language inference tasks.(zero-shot)\nC Results for Ablation Experiments"}]}