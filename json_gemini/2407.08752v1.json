{"title": "From Modular to End-to-End Speaker Diarization", "authors": ["FEDERICO NICOL\u00c1S LANDINI"], "abstract": "Speaker diarization is usually referred to as the task that determines \"who spoke when\" in a recording. Until a few years ago, all competitive approaches were modular, i.e. voice activity detection, segmentation, embedding extraction, clustering and overlapped speech detection and handling were tackled by different sub-systems and applied in a cascaded fashion. Systems based on this framework reached state-of-the-art performance in most scenarios but had major difficulties dealing with overlapped speech. More recently, the advent of end-to-end models, capable of dealing with all aspects of speaker diarization with a single model and better performing regarding overlapped speech, has brought high levels of attention.\nThis thesis is framed during a period of co-existence of these two trends. We describe a system based on a Bayesian hidden Markov model used to cluster x-vectors (speaker embeddings obtained with a neural network), known as VBx, which has shown remarkable performance on different datasets and challenges. We comment on its advantages and limitations and evaluate results on different relevant corpora. Then, we move towards end-to-end neural diarization (EEND) methods. Due to the need for large training sets for training these models and the lack of manually annotated diarization data in sufficient quantities, the compromise solution consists in generating training data artificially. We describe an approach for generating synthetic data which resembles real conversations in terms of speaker turns and overlaps. We show how this method generating \u201csimulated conversations\" allows for better performance than using a previously proposed method for creating \"simulated mixtures\" when training the popular EEND with encoder-decoder attractors (EEND-EDA). We also propose a new EEND-based model, which we call DiaPer, and show that it can perform better than EEND-EDA, especially when dealing with many speakers and handling overlapped speech. Finally, we compare both VBx-based and DiaPer systems on a wide variety of corpora and comment on the advantages of each technique.", "sections": [{"title": "Chapter 1", "content": "Speech processing technologies have advanced considerably during the last decades due to more computing capacity, more available data, and big efforts from the research community to propose new techniques and improve the performance of systems. Many speech-related tasks have progressed remarkably and one of them is speaker diarization, usually referred to as the task of deciding \"who spoke when\". More formally, it is the task of assigning speaker labels to segments of speech in a given recording. This task is of interest per se for allowing segmentation of conversations or search of speech from a speaker of interest, among others. However, it is also useful as an upstream task for other well-studied tasks such as information retrieval, speech recognition, or speaker verification.\nSpeaker diarization is applied on diverse domains. While originally constrained to telephone conversations, recorded meetings, or broadcast news, in the last few years, large amounts of broadcast data (not only news) have also gained relevance as target data. Even more, the availability of small devices has facilitated recording speech in more varied situations. Common examples are recordings in courtrooms or parliaments as well as conversations in many locations, such as restaurants. Besides, some specific applications are of interest, for example, diarization on clinical interviews to analyze sessions with patients and diarization on conversations in households with children to evaluate language acquisition. Furthermore, the recent global pandemic has increased the number of meetings over web-based platforms and speaker diarization can be useful to enrich transcriptions. Even more, ubiquitous speech-controlled devices often require to know which user is interacting with the device, thus benefiting from automatic diarization. All these applications provide new challenges and such conditions demand robust diarization systems able to cope with different acoustic channels, styles of speech and numbers of speakers.\nSince the origins of speaker diarization around 20 years ago, several approaches have been proposed. We describe the techniques used in the most popular past systems as well as the models that attain state-of-the-art performance nowadays. Systems can be framed under two big families, namely modular ones: those formed by different sub-modules such as voice activity detection, embedding extraction, and clustering (among others); and neural network-based end-to-end systems that solve the task with a single module. State-of-the-art systems under these two frameworks are described, evaluated and compared in this work."}, {"title": "1.1 Contributions", "content": "The contributions of our work are the following:\n\u2022 We present the Bayesian HMM clustering of x-vectors (VBx) as a method for clustering of speaker embeddings for diarization. VBx was presented in [Landini et al., 2022b], showing state-of-the-art performance (at the time) on three relevant datasets. At the time of that publication, AMI, one of the most standard diarization datasets, was used with different evaluation setups, effectively impeding proper comparison between works of different authors. We pointed this out and showed the performance of VBx in the most widely used setups. With the aim of avoiding future confusion, we released the annotations derived from the official transcriptions following a clear and detailed procedure https://github.com/BUTSpeechFIT/AMI-diarization-setup\nWe also released the code and x-vector extractors needed to run the recipes:\nhttps://github.com/BUTSpeechFIT/VBx\nDue to the continuous maintenance of the repository, ease of use and competitive results, VBx has served as baseline in numerous publications.\n\u2022 VBx was also a core module of the winning system of the Second DIHARD Challenge [Landini et al., 2020] and the second-ranked system in VoxSRC 2020 [Landini et al., 2021a]. We released the code for both systems too.\nhttps://github.com/BUTSpeechFIT/VBx/tree/v1.0_DIHARDII\nhttps://github.com/BUTSpeechFIT/VBx/tree/v1.1_VoxConverse2020\nThanks to the strong performance and public recipes, VBx has been used as the baseline in recent challenges such as M2MeT and Ego4D [Yu et al., 2022a, Grauman et al., 2022].\n\u2022 This thesis also presents the results obtained with a fully modular system based on the public VBx recipe and public voice activity detection and overlapped speech detection systems. These results are presented on a variety of datasets and serve not only as baseline in the context of this work but for other researchers in the field.\n\u2022 End-to-end neural diarization (EEND) models require large amounts of training data and, due to the lack of manually annotated data in those quantities, synthetic data need to be generated. We introduce an alternative approach to the original \"simulated mixtures\u201d scheme for generating \"simulated conversations\" (which resemble real conversations regarding the lengths of pauses and overlaps), used as training data for end-to-end models. Extensive comparisons between our approach and the former one in [Landini et al., 2022a, Landini et al., 2023a] showed that using simulated conversations reduces almost completely the need for fine-tuning the model using in-domain data in the telephony scenario. We released the code for generating training data as well as our PyTorch implementation of the EEND with encoder-decoder attractors model and models trained on free public data:\nhttps://github.com/BUTSpeechFIT/EEND_dataprep\nhttps://github.com/BUTSpeechFIT/EEND\nThanks to this, simulated conversations have already been adopted in other publications."}, {"title": "1.2 Organization of the thesis", "content": "The rest of the thesis is organized as follows:\n\u2022 Chapter 2 formally defines the task of speaker diarization describing how models are evaluated and the connection with other tasks.\n\u2022 Chapter 3 presents modular diarization systems and describes in detail VBx, one of the clustering methods considered state-of-the-art nowadays, which is one of the main contributions of this thesis. The performance of VBx is analyzed on different datasets.\n\u2022 Chapter 4 presents end-to-end diarization systems covering the different works that have been recently published. Given the \"data-hungry\" nature of these models, we describe a new method for generating synthetic training data, coined \"simulated conversations\", and show its advantages. Then, we present DiaPer, a novel method based on the end-to-end framework and analyze its performance on different datasets.\n\u2022 Chapter 5 concludes the thesis discussing the comparison of the aforementioned modular and end-to-end systems and the future challenges of speaker diarization."}, {"title": "Chapter 2", "content": "Historically, audio diarization [Reynolds and Torres-Carrasquillo, 2005] has referred to the broad task of segmenting a recording and categorizing those segments (for example, into silence, noise, music, commercials, speech, etc.). The popularity it gained in the early 2000s has waned and few works are framed under that task nowadays. Speaker diarization corresponds to the more specific task of segmenting a recording in terms of speech segments and assigning speaker labels. Speaker diarization (SD) is usually referred to as \u201cwho spoke when\" and gained popularity in the current definition also during the early 2000s, mainly due to the National Institute of Standards and Technology (NIST) in their series of Rich Transcriptions (RT) evaluations and several European projects [Anguera et al., 2012]. Examples of both tasks are presented in Figure 2.1 over the same recording. Note that for SD, the goal is to distinguish speakers and using relative speaker labels such as 'Spk1' and 'Spk2' is enough. Still, for some applications, the role (i.e. presenter, announcer, guest, etc.) or specific speaker labels (i.e. Alex, Bob, Carol, etc.) are necessary and in such cases, the SD labels are post-processed by a speaker identification system. Since only speaker segments are of interest, anything else is simply not labeled. Speech segments can be bounded by non-speech segments or speaker change points, as observed in Figure 2.1.\nIn this work, we focus on the most general scenario for speaker diarization, where no information is given about the input recording. Some examples of a priori information can be\n\u2022 number of speakers\n\u2022 language"}, {"title": "2.1 Task definition", "content": "Historically, audio diarization [Reynolds and Torres-Carrasquillo, 2005] has referred to the broad task of segmenting a recording and categorizing those segments (for example, into silence, noise, music, commercials, speech, etc.). The popularity it gained in the early 2000s has waned and few works are framed under that task nowadays. Speaker diarization corresponds to the more specific task of segmenting a recording in terms of speech segments and assigning speaker labels. Speaker diarization (SD) is usually referred to as \u201cwho spoke when\" and gained popularity in the current definition also during the early 2000s, mainly due to the National Institute of Standards and Technology (NIST) in their series of Rich Transcriptions (RT) evaluations and several European projects [Anguera et al., 2012]. Examples of both tasks are presented in Figure 2.1 over the same recording. Note that for SD, the goal is to distinguish speakers and using relative speaker labels such as 'Spk1' and 'Spk2' is enough. Still, for some applications, the role (i.e. presenter, announcer, guest, etc.) or specific speaker labels (i.e. Alex, Bob, Carol, etc.) are necessary and in such cases, the SD labels are post-processed by a speaker identification system. Since only speaker segments are of interest, anything else is simply not labeled. Speech segments can be bounded by non-speech segments or speaker change points, as observed in Figure 2.1."}, {"title": "2.2 Evaluation", "content": "In order to evaluate a diarization system, just as with any other task, the annotations produced by a system need to be compared against ground truth ones. However, ground truth labels are generated following different criteria depending on the dataset. To illustrate the procedure of generating diarization labels, we utilize a few corpora as examples:\nFor the First DIHARD Challenge\u00b9, manual annotations were first produced, then refined using forced alignment (where pauses longer than 200 ms were used to split segments) and then checked and corrected by annotators [Ryant et al., 2018]. This is a very expensive annotation pipeline since human annotators need to listen to a recording several times to produce quality annotations.\nA more automated approach was utilized when collecting and annotating VoxCon-verse [Chung et al., 2020], where videos were available and voice activity detection was correlated with mouth movement of in-screen speakers to assign speaker labels to most speech segments. Speaker embeddings of off-screen speech segments were obtained and compared with those of already determined speakers in order to assign identities to those segments. This was followed by manual inspection to identify common errors and instructions were given to manual annotators to correct them following certain guidelines (i.e. pauses longer than 250 ms were used to split segments, mark segments with 0.1s precision, etc.).\nAs observed in these two examples, different criteria can be used to produce the reference annotations. Related to the precision of the reference annotations is also the discussion of calculating metrics using a forgiveness collar or not. Using a collar means that small differences (below a certain threshold) between the reference and system annotations will not be considered for calculating errors. Different corpora have chosen different conventions; for example, DIHARD does not use a collar (meaning that manual annotations are fully"}, {"title": "2.2.1 Ground truth labels", "content": "In order to evaluate a diarization system, just as with any other task, the annotations produced by a system need to be compared against ground truth ones. However, ground truth labels are generated following different criteria depending on the dataset. To illustrate the procedure of generating diarization labels, we utilize a few corpora as examples:\nFor the First DIHARD Challenge\u00b9, manual annotations were first produced, then refined using forced alignment (where pauses longer than 200 ms were used to split segments) and then checked and corrected by annotators [Ryant et al., 2018]. This is a very expensive annotation pipeline since human annotators need to listen to a recording several times to produce quality annotations.\nA more automated approach was utilized when collecting and annotating VoxCon-verse [Chung et al., 2020], where videos were available and voice activity detection was correlated with mouth movement of in-screen speakers to assign speaker labels to most speech segments. Speaker embeddings of off-screen speech segments were obtained and compared with those of already determined speakers in order to assign identities to those segments. This was followed by manual inspection to identify common errors and instructions were given to manual annotators to correct them following certain guidelines (i.e. pauses longer than 250 ms were used to split segments, mark segments with 0.1s precision, etc.).\nAs observed in these two examples, different criteria can be used to produce the reference annotations. Related to the precision of the reference annotations is also the discussion of calculating metrics using a forgiveness collar or not. Using a collar means that small differences (below a certain threshold) between the reference and system annotations will not be considered for calculating errors. Different corpora have chosen different conventions; for example, DIHARD does not use a collar (meaning that manual annotations are fully"}, {"title": "2.2.2 Metrics", "content": "Since speaker labels generated by diarization systems do not need to match the naming convention of the reference ones (e.g. 'Spk1', 'Spk2', etc.), a 1:1 mapping between the system-generated speaker clusters and the reference ones is obtained using the Hungarian method [Kuhn, 1955] for finding the solution of a bipartite graph. Given this optimal mapping, errors are calculated to determine how good the system output is with regard to the reference.\nThe most widely used performance metric for diarization is the diarization error rate (DER) proposed by NIST in their series of RT evaluations during the 2000s [NIST, 2009]. DER is defined by\n$DER = \\frac{SER+FA + Miss}{Total speech},                                        (2.1)$\nwhere:\n\u2022 SER stands for speaker error, the amount of time that speech is attributed to incorrect speakers (i.e. confusion of speakers)\n\u2022 FA is false alarm, the amount of time that non-speech regions are incorrectly at-tributed to a speaker (or time when overlapped speech of K speakers is found in speech regions with $N < K$ reference speakers)\n\u2022 Miss stands for missed speech, the amount of time that speech is not attributed to any speaker (or time when overlapped speech of K speakers is found for regions with $N > K$ reference overlapping speakers)\n\u2022 Total speech is the total amount of speech, accounting also for speaker overlaps.\nAn example of how DER is calculated can be seen in Figure 2.2 where the best matching {Interviewer - Spk1, Intervieweel - Spk2, Interviewee2 - <>} is denoted by the colors. For more details about DER, please refer to the RT-09 evaluation plan\u00b2.\nDue to inherent inconsistencies in the annotations introduced by the annotators when finding the precise boundaries of segments, NIST proposed using an optional 0.25s for-giveness collar around each boundary of a speech reference segment where speech is not scored. Using a collar means that small differences (below a certain threshold) between the reference and system annotations will not be considered for calculating errors.\nFor the Second DIHARD Speaker Diarization Challenge [Ryant et al., 2019b], the orga-nizers introduced the Jaccard error rate (JER) based on the Jaccard index [Jaccard, 1901]. JER has become a popular secondary metric after DER."}, {"title": "2.3 Relevance of the task", "content": "Although speaker diarization has received comparatively less attention from the speech community than other tasks such as speaker recognition or speech recognition, it has several applications. In this section, we briefly describe the possible applications of diarization and show its relevance even in tandem with other tasks.\nIn general, the application of diarization is related to providing rich meta-data in terms of speaker segments which allow indexing of audiovisual resources with speakers and structured search and access to resources. There are several domains to which it is of interest to apply diarization. We briefly list here some of the most common ones as reviewed in [Tranter and Reynolds, 2006, Anguera et al., 2012, Moattar and Homayounpour, 2012]:\n\u2022 broadcast news, debates, television shows, movies\n\u2022 recorded meetings (which becomes particularly interesting nowadays as many meet-ings are being held remotely)\n\u2022 telephone conversations\n\u2022 clinical interviews\n\u2022 recorded lectures or conferences\nHowever, SD can be combined with other tasks, including the most studied ones: ASR and SR. SD in combination with ASR, determine \"who said what\" and solve the \"cocktail party problem\" [Watanabe et al., 2020, Cornell et al., 2023]. Sometimes SR has to be performed on recordings with several speakers and, in such cases, diarization is key to allow finding the speaker of interest as in a recent speaker recognition evaluation (SRE) [NIST, 2018]. However, there are some other applications for diarization such as speaker indexing and retrieval, speaker counting, or improving ASR systems by allowing speaker-specific adaptation."}, {"title": "Chapter 3", "content": "Some of the first works related to diarization started to appear around the year 2000 [Garo-folo et al., 2004, Fiscus et al., 2006]. Works published in the late 90s [Saunders, 1996, Gau-vain et al., 1998, Liu and Kubala, 1999, Siegler et al., 1997] focusing on telephone con-versations or broadcast news formulated the task as \u201cpartitioning\" or \"segmentation and clustering\", among others. But it was the series of Rich Transcription Evaluations [NIST, 2009] organized by NIST between 2002 and 2009 that first defined the diarization task and set a framework for comparing different approaches for diarization. The focus was initially on broadcast news data and later on meeting data. Several European projects focused on meeting data during the first decade of 2000 and also shifted the attention to the challenges related to the meeting scenario [Anguera et al., 2012]. The works related to diarization mostly focused on those types of recordings in the following years until approxi-mately 2017-2018 with the emerging DIHARD challenge. Since this work focuses on off-line diarization, i.e. diarization when the whole recording is available at evaluation time, we only cover previous work in this line.\nUntil the recent advent of end-to-end neural diarization models, most diarization sys-tems were composed of several subsystems that solved different subtasks, sometimes related to specific setups (e.g. multi-channel recordings). In this chapter, we briefly cover the his-tory of the cascaded approach, comment on its strengths and weaknesses and finally describe the clustering-based method studied in this thesis."}, {"title": "3.1 Structure of a standard cascaded system", "content": "Throughout the years, most competitive diarization systems have used several of the blocks depicted in Figure 3.1, applied one after the other in a \"cascaded\" fashion. A common categorization for diarization systems is between bottom-up and top-down. Bottom-up refers to starting from short segments of speech and clustering them until finding just a few speakers. Top-down consists of starting with the full recording as a whole and partitioning it into smaller parts that in the end will correspond to a few speakers. The former approach has been more thoroughly studied (Figure 3.1 represents this one) but a comparison between the two can be found in [Anguera et al., 2012, Evans et al., 2012].\nWe describe below each of the modules mentioned in Figure 3.1. Note that many other possible sub-modules as well as more details about each of the steps are omitted as this is a high-level overview. More details can be found in [Tranter and Reynolds, 2006] for systems"}, {"title": "Signal pre-processing", "content": "In the context of diarization, signal pre-processing refers to applying methods that enhance speech and facilitate the ultimate task. While speech pre-processing originally focused on spectral-based methods, nowadays approaches rely on neural-network-based methods. For diarization, the most common methods include denoising and dereverberation.\nEven though telephone conversations and broadcast news can benefit from such pre-processing, this step became more relevant for recorded meetings. While the former are generally recorded with close-talk microphones, the latter usually present setups with far-field microphones where the effect of reverberation or noises made by the participants make for a more challenging scenario [Anguera et al., 2012]. Furthermore, meetings offer in some cases recordings with multiple microphones [Janin et al., 2004, McCowan et al., 2005, Mostefa et al., 2007] and making use of the signal from all of them can provide better performance [Pardo et al., 2006]. Different approaches have been used to deal with multiple microphones but the most popular is acoustic beamforming [Anguera et al., 2007].\nWith the more challenging scenarios presented in DIHARD challenge [Ryant et al., 2018], preprocessing has become quite relevant and the best-performing systems in the challenge made use of at least one type of enhancement method [Sell et al., 2018, Diez et al., 2018b, Sun et al., 2018, Landini et al., 2020, Wang et al., 2021b]."}, {"title": "Voice activity detection", "content": "The task of VAD is to find the segments of voice or speech to be labeled in the diarization output. This means discarding segments of silence, noise and possibly music while keeping all segments of speech. VAD is of interest not only for diarization but for many other tasks such as SR or ASR. Thus, large efforts have been made to improve the capabilities of VAD systems [Ramirez et al., 2007]. Some of the most popular approaches are based on energy or spectrum [Anguera et al., 2005, Istrate et al., 2005], or Gaussian mixture models (GMM), eventually aided by hidden Markov models (HMM), trained with acoustic features such as Mel frequency cepstral coefficients (MFCC) [Wooters et al., 2004, Reynolds and Torres-Carrasquillo, 2004].\nMore recently, systems based on NN have outperformed previous ones [Ryant et al., 2013, Gelly and Gauvain, 2017, Sun et al., 2018]. However, one of the main problems with these systems is that the domains of the training data (telephone, meetings, etc.) bias the model to work well in some scenarios while failing in others. In general, if the model allows to choose different detection thresholds, a lower miss rate of VAD is preferred over a lower false alarm rate since speech discarded in this step is usually not recovered later. Something to consider is that different domains (i.e. meetings, telephone, etc.) present speech with different characteristics and different proportions. For example, in a telephone conversation, speakers usually take turns with very little silence in between whereas in a household day-long recording, speech is generally less common than other sounds or simply silence. Systems are commonly tuned or trained with some application in mind and using a configuration optimal for a certain domain can produce sub-optimal results on recordings of a different domain.\nFurthermore, the annotation criteria and guidelines can vary among different corpora. This means for instance that (sung) background music can be labeled as silence in certain contexts. Moreover, speech from speakers that are not of main interest can be marked as non-speech (think of a recording in a restaurant where people at the next table have a conversation). At the same time, voiced sounds such as cough or sneeze from a speaker of interest can be considered \u201cvoice\u201d even if they are actually not speech. In spite of this, the terms VAD and SAD are usually used interchangeably.\nThis task is so crucial that many challenges have proposed two tracks: one where diarization is evaluated as a whole and another where oracle (extracted from the manual annotations used for evaluations) VAD labels are allowed in order to evaluate the rest of the steps assuming perfect VAD.\nNowadays, most NN-based systems can perform reasonably good VAD except in very challenging conditions, such as dinner-party scenarios or extremely noisy backgrounds."}, {"title": "Segmentation", "content": "Segmentation, sometimes also referred to as speaker change point detection (SCD), finds the points in the recording that separate audio sources (including different speakers). While longer segments allow to have more information to perform clustering in the next step, it is important that the segments do not contain speech from different speakers as this can create problems when trying to find which segments correspond to the same speaker. Therefore, SCD deals with the trade-off between long speech segments versus having pure segments.\nThis step can be combined with VAD in the case that silence or noise are considered audio sources. However, it is common to make use of the VAD output given by the module previously described. The most popular approaches used for SCD are based on comparing"}, {"title": "Representation extraction and clustering", "content": "This step refers to two of the most crucial steps in the pipeline which usually act in tandem. Originally, speech segments were represented by acoustic features such as MFCC, but more carefully designed representations (i.e. embeddings) are preferred nowadays. The speaker embeddings used for diarization, described below, are usually adopted from the technologies used for speaker verification. The clustering builds on top of these representations and assigns them to different speaker identities in a conversation.\nThe clustering step has the objective of providing one (global) cluster per speaker for the whole recording. The predominant approach for doing so is agglomerative hierarchi-cal clustering (AHC) which consists of the following steps: first, pair-wise distances are computed between each pair of initial clusters (simply segments); then, the closest clusters according to the estimated distances are merged and the distances are updated given the new setup. These operations are carried out iteratively until a stopping criterion is met.\nFor calculating the distances, the segments have been represented with Gaussian distri-butions [Moh et al., 2003] or GMMs [Moraru et al., 2003] computed on acoustic features such as MFCCs. Different distance metrics have been used as well as different stopping criteria and some of them followed the same approaches described for segmentation. Among distance metrics, the most popular have been GLR and BIC for which the stopping criterion is based on comparing the statistics of two clusters versus a single cluster after merging, thus allowing for fast clustering.\nOther approaches model speakers by maximum-a-posteriori adapting a GMM-based universal background model (UBM) to speaker clusters [Betser et al., 2004, Reynolds and Torres-Carrasquillo, 2004]. Also making use of BIC, both approaches use a fixed thresh-"}, {"title": "Resegmentation", "content": "Resegmentation refers to an optional step in which the boundaries of the clustering out-put are refined, sometimes followed by new segmentation, embedding extraction, and re-clustering stage. The most common approach uses Viterbi decoding (with or without iterations); this step is closely related to the HMM-based approach covered in Section 3.2. Other approaches based on sliding windows were proposed [Adam et al., 2002] in which windows for the speakers are estimated and their boundaries re-estimated based on some distance measure. Given initial segments that represent speakers, the distances between them and new candidate windows give possible points for resegmentation. However, this approach has not been largely explored.\nThe clustering and resegmentation steps were used iteratively in [Gauvain et al., 1998, Barras et al., 2004] and later in [Shum et al., 2012] with more sophisticated clustering methods."}, {"title": "Post-processing", "content": "As in many speech processing tasks, one common procedure to leverage the performance of individual systems is system combination or fusion. Diarization is not the exception in this regard and there have been attempts to combine the outputs of different systems such as using cluster voting [Tranter, 2005] or merging outputs [Meignier et al., 2006] considering the agreement of the models. Another possibility is hybridization (or \"piped\" systems) [Meignier et al., 2006] which uses the output of one system to initialize another one.\nMore recently, an approach following a technique commonly used in ASR: recognizer output voting error reduction (ROVER) has been proposed [Stolcke and Yoshioka, 2019] under the name of DOVER where voting between different systems is done to fuse the outputs of several systems to generate a final decision. DOVER was extended to handle systems with overlapping segments in DOVER-Lap [Raj et al., 2021b] and has become relevant in recent challenges."}, {"title": "Overlapped speech detection", "content": "A different type of post-processing that deserves attention on its own is dealing with seg-ments of overlapped speech, where two or more speakers speak simultaneously. Overlapped speech is present in general in all types of recordings but with different proportions. Given that most modular systems only output one speaker per frame, the penalization on record-ings with high levels of overlap is considerable."}, {"title": "3.2 Diarization based on Bayesian hidden Markov model (VBx)", "content": "This section covers the work done regarding the Bayesian hidden Markov model for diariza-tion and the configuration of a full system with this approach. This system falls into the category of clustering of embeddings and, being a more sophisticated approach than other simpler clustering methods, it has allowed for state-of-the-art performance on different sets for considerable time. Nowadays, it is still used as a contrastive baseline system in many publications."}, {"title": "3.2.1 Background", "content": "Hidden Markov models [Rabiner and Juang, 1986] have been used for speech processing for a very long time. In particular, an ergodic HMM presents a suitable framework for diarization where states represent speakers, the transitions between states represent the speaker turns in a conversation and each state has a speaker model which is assumed to generate the observed data.\nSome of the first works in diarization used HMMs [Sugiyama et al., 1993, Wilcox et al., 1994, S\u00f6nmez et al., 1999] although at that time they referred to the task as \u201cspeaker tracking\" or simply \"speaker segmentation\". The main problem with those approaches was that the number of speakers in the conversation had to be known in advance. An extension to tackle that problem consisted in an evolutive HMM [Meignier et al., 2000, Meignier et al., 2001, Meignier et al., 2006, Fredouille and Evans, 2008] where new states were added iteratively and the Viterbi decoding ran after each change of HMM configuration to see if the new model fitted the observed data better. Another approach [Ajmera et al., 2002] started from a large number of segments (many more than the number of expected speakers) and \"merged\" states as long as the new configuration fitted the data better than the previous one.\nHowever, more relevant work for this chapter was proposed in [Valente and Wellekens, 2004, Valente and Wellekens, 2005] where, for the first time, variational Bayesian (VB) inference was used together with an HMM for diarization. The idea was to use an ergodic HMM modeling the speakers (states) with GMMs. We do not describe the VB inference here as it is a central part of the model we use in this work and it is covered in section 3.2.2.\nAlternatives using a hierarchical Dirichlet process HMM (HDP-HMM) [Teh et al., 2004] and a sticky HDP-HMM were presented in [Fox et al., 2011] using also a Bayesian framework in the context of diarization. With the HDP-HMM, the model allowed potentially an infinite number of speakers, since the number of states is inferred naturally. As presented by the authors, this approach has the problem that it does not model the persistence of states adequately. This means that the approach gives a higher posterior probability to models with a higher number of states and with rapid switching between them. In order to fix this, they included a parameter of self-transition bias with its corresponding prior so that it is part of the whole Bayesian framework. Their extension, called sticky HDP-HMM, presented a fully Bayesian framework (i.e. priors were assumed over all the parameters) to"}, {"title": "3.2.2 Model description", "content": "The model that we describe here corresponds to the latest version of the BHMM which works at x-vector level as described in [Landini et al., 2022b]. Often we refer to this model as VBx\u00b2. Although many aspects are shared with its predecessor version working at MFCC level, we omit those differences and focus only on the latest version. More details about the previous version can be found in [Diez et al., 2018a, Diez et al., 2019a].\nOverview\nThe model works on the basis of a VAD segmentation which can be given by a system or by oracle labels, i.e. extracted from the reference annotations. On the speech portions, uniform segmentation is used to define the segments where speaker embeddings (x-vectors) are to be extracted.\nThe diarization model assumes that the input sequence of x-vectors is generated by an HMM with speaker-specific state distributions. Building on a common back-end approach for comparing x-vectors, the state distributions are derived from a PLDA model pre-trained on a largenumber of speaker-labeled x-vectors."}, {"title": "HMM topology", "content": "The HMM topology and transition probabilities model the speaker turn durations. Figure 3.2 shows an example of the HMM topology for $S = 3$ speakers. The transition prob-abilities are set as follows: we transition back to the same speaker/state with probability $P_{loop}$. Note that this probability is not learned (unlike in [Fox et al., 2011]) but it is a tunable parameter in the model. The remaining probability $(1 - P_{loop})$ is the probability of changing the speaker, which corresponds to the transition to the non-emitting node in Figure 3.2. From the non-emitting node, we immediately transition to one of the speaker states with probability $\u03c0_s$ . Therefore, the probability of leaving one speaker and entering another speaker s is $(1 - P_{loop})\u03c0_s$. To summarize, the probability of transitioning from state s' to state s is\n$p(s|s') = (1 - P_{loop})\u03c0_s + \u03b4(s = s')P_{loop},                                        (3.1)$\nwhere $\u03b4(s = s')$ equals 1 if we go to the same state ($s = s'$) and is 0 otherwise.\nThe non-emitting node in Figure 3.2 is also the initial state of the model. Therefore, the probabilities $\u03c0_s$ also control the selection of the initial HMM state (i.e. the state generating the first observation). These probabilities $\u03c0_s$ are inferred (jointly with the variables $y_s$ and $z_t$) from the input conversation.\nThanks to the automatic relevance determination principle [Bishop, 2006] stemming from the Bayesian model, zero probabilities will be learned for the $\u03c0_s$ corresponding to redundant speakers, which effectively drops such speakers from the HMM model. Typically, the HMM is initialized with a larger number of speakers (see section 3.3) and we make use of this behavior to drop the redundant speakers (i.e. to estimate the number of speakers in the conversation)."}, {"title": "Speaker-specific distributions", "content": "The speaker-specific distributions are derived from a PLDA which is a standard model used for comparing x-vectors in speaker verification [Kenny, 2010]. Here, only a simplified variant of PLDA is considered, which is often referred to as two-covariance model [Brummer and Villiers, 2010].\nThis model assumes that the distribution of x-vectors specific to speaker s is Gaussian $\\mathcal{N}(x_t; m_s, \u03a3_w)$, where $\u03a3_w$ is the within-speaker covariance matrix shared by all speaker models, and $m_s$ is the speaker-specific mean. Speaker means are further assumed to be"}, {"title": "Bayesian HMM", "content": "To summarize, our complete model for SD is a Bayesian HMM, which is defined in terms of the state-specific distributions (or so-called output probabilities)\n$p(x_t|z_t = s) = p(x_t|s) = p(x_t|y_s)                                                     (3.8)$\nand the transition probabilities\n$p(z_t = s|z_{t\u22121} = s') = p(s|s')                                                   (3.9)$\ndescribed in the two previous sections. By abuse of notation, $p(z_1|z_0)$ will correspond to the initial state probability $p(z\u2081=s) = \u03c0_s$ in the following formulae.\nThe complete model can be also defined in terms of the joint probability of the observed and latent random variables (and their factorization) as\n$p(X, Z, Y) = p(X|Z, Y)p(Z)p(Y) = [p (x_t|2t) [[ P (Zt|Zt-1) [P(ys), (3.10)$\nwhere Y = {y1, y2, ..., ys} is the set of all the speaker-specific latent variables. The corre-sponding graphical model can be seen in Figure 3.3.\nThe model assumes that each x-vector sequence corresponding to an input conversation is obtained using the following generative process:\n$\\begin{aligned}\n& \\text {for } s=1 . . S \\text { do } \\\\\n& \\mathbf{y}_{s} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I}) \\\\\n& \\text {for } t=1 . . T \\text { do } \\\\\n& z_{t} \\sim p(z_{t} / z_{t-1}) \\\\\n& \\mathbf{x}_{t} \\sim p(\\mathbf{x}_{t} \\mid z_{t}) \\\\\n\\end{aligned}$"}, {"title": "Diarization inference", "content": "The diarization problem consists in finding the assignment of embeddings to speakers, which is represented by the latent sequence Z. In order to find the most likely sequence Z, we need"}, {"title": "Updating q(Y)", "content": "Given a fixed q(Z), the distribution over Y that maximizes the ELBO is\n$q^* (Y) = \u041fq^* (\u0443\u043b),                                                            (3.14)$\nS\nwhere the speaker-specific approximate posteriors\n$q^*(y_s) = \\mathcal{N} (y_s|\u03b1_s, L_s^{-1})                                                (3.15)$\nare Gaussians with the mean vector and precision matrix\n$\\begin{aligned}\n\u03b1_s &= \\frac{FA}{FB} L_s^{-1}   ,\\\\\nL_s &= I + \\frac{FA}{FB} T  .\n\\end{aligned}                                           (3.16)$\nwhere\n$\u03c1_{t} = V^T x_{t}.                                                         (3.18)$\nIn this update formula, Yts = q(zt = s) is the marginal approximate posterior derived from the current estimate of the distribution q(Z) (see below), which can be interpreted as the responsibility of speaker s for generating observation xt (i.e. defines a soft alignment of x-vectors to speakers).\nIt is worth noting that is a diagonal matrix. Therefore, also matrix Ls is diagonal, and its inversion and application in (3.16) become trivial."}, {"title": "Updating q(Z)", "content": "We never need to infer the complete distribution over all the possible alignments of obser-vations to speakers q(Z). When updating q(Y) using (3.16) and (3.17), we only need the marginals Yts = q(zt = s). Therefore, when updating q(Z), we can directly search for the"}, {"title": "Updating \u03c0", "content": "Finally, the speaker priors \u03c0\u03c2 are updated as maximum likelihood type II estimates [Bishop, 2006]: Given fixed q(Y) and q(Z), we search for the values of \u03c0s that maximize the ELBO (3.13), which gives the following fixed point iteration update formula\n$\u03c0_s \u2190 \u03c0_s(\\frac{ 1}{\u03c0}) ,                                  (3.24)$\nwith the constraint . As described in section 3.2.2, this update tends to drive the \u03c0\u03c2 corresponding to \u201credundant speakers\" to zero values, which effectively drops them from the model and selects the right number of speakers in the input conversation."}, {"title": "Evaluating the ELBO", "content": "The convergence of the iterative VB inference can be monitored by evaluating the ELBO objective. For the Bayesian HMM, the ELBO can be efficiently evaluated (see page 95 of [Beal, 2003]) as\n$\\hat L=+\\fracFB2(R+ln |L_s^{-1}|-tr(L_s^{-1})-{a_s^Ta_s}),                                          (3.25)$\nwhere R is the dimensionality of the x-vectors. Note, that since Ls is a diagonal matrix, $ln |L_s^{-1}|$ can be calculated just as the sum of the log of the elements in the diagonal. This way of evaluating the ELBO is very practical as the term from (3.22) is obtained as a byproduct of \"updating q(Z)\" using the forward-backward algorithm. On the other hand, the ELBO can be evaluated using (3.25) only right after the q(Z) update. Thus, the improvements in the ELBO obtained from q(Y) or \u03c0\u03c2 updates cannot be monitored, which might be useful for debugging purposes.\nThe complete VB inference consisting of iterative updates of q(Y), q(Z) and parameters \u03c0, is summarized in the following algorithm:"}, {"title": "3.3 Experiments", "content": "This section presents results with a cascaded system based on clustering of x-vectors by means of VBx. However, the system consists of a few blocks as depicted in Figure 3.4. Voice activity detection is initially applied and, on the speech segments, a fixed segmentation is used where each x-vector is extracted from 1.5s windows every 0.25s. Towards the end of each segment, some x-vectors might be estimated on shorter-than-1.5s windows. Then, the BHMM framework is applied using AHC as initialization where the AHC threshold is set the same for all datasets and low enough to undercluster (i.e. produce more clusters than speakers). Finally, the output of an overlapped speech detection system is used to add overlapping speakers since the clustering step inherently assigns a single speaker to each x-vector. Each of the modules is described in detail next. Since we present results on telephone conversation datasets as well as wide-band corpora, two similar models are described, one for each data type."}, {"title": "3.3.1 Model configuration", "content": "This section presents results with a cascaded system based on clustering of x-vectors by means of VBx. However, the system consists of a few blocks as depicted in Figure 3.4. Voice activity detection is initially applied and, on the speech segments, a fixed segmentation is used where each x-vector is extracted from 1.5s windows every 0.25s. Towards the end of each segment, some x-vectors might be estimated on shorter-than-1.5s windows. Then, the BHMM framework is applied using AHC as initialization where the AHC threshold is set the same for all datasets and low enough to undercluster (i.e. produce more clusters than speakers). Finally, the output of an overlapped speech detection system is used to add overlapping speakers since the clustering step inherently assigns a single speaker to each x-vector. Each of the modules is described in detail next. Since we present results on telephone conversation datasets as well as wide-band corpora, two similar models are described, one for each data type."}, {"title": "3.3.3 Results", "content": "The results in this section are presented mainly to serve as baseline for Chapter 5. For this reason, there is no particular analysis of the configurations in VBx nor the other steps of the pipeline. However, the interested reader can refer to:\n\u2022 [Landini et al., 2020] for comparisons regarding the impact of the PLDA model and the OSD step in the context of DIHARD2\n\u2022 [Diez et al., 2020] for analyses with respect to the segmentation and the initialization in the context of DIHARD2\n\u2022 [Landini et al., 2021b] for comparisons between using PLDA and heavy-tailed PLDA across domains in DIHARD3\n\u2022 [Landini et al., 2021a] for an in-depth analysis of every step of a modular system in the context of VoxConverse\n\u2022 [Landini et al., 2022b] for an extensive comparison of VBx with the state-of-the-art (at the time of publication) for AMI, Callhome and DIHARD2\nIt should be noted that the above-mentioned publications presented results with full modular pipelines focused on single datasets. In this section, we utilize updated sub-modules with the same pipeline across 12 different datasets. We expect these results will also serve as detailed baselines for future diarization publications.\nGiven the acoustic characteristics of different corpora, for the thesis, we evaluated two systems in terms of sampling rate: 8 kHz and 16 kHz comprising the most common scenarios at the time of writing.\nIn terms of VAD, as mentioned in Section 3.3.1, Kaldi ASPIRE and pyannote 2.1 models were evaluated with default parameters on signals at 8kHz and 16kHz. Recordings that were originally sampled at 16 kHz were downsampled to obtain their 8 kHz versions and vice-versa. For each condition (dataset and sampling rate), the best of the two VADs was chosen depending on the performance on the development set. The performance on the test sets is presented in Table 3.7. While precision and recall are detection performance metrics of the models in general, the missed and FA errors give a better hint about the effect the VAD can have on the final diarization performance.\nFor most of the datasets, the performances for different sampling rates are very similar. The only cases with relevant differences are CHiME6, DipCo and to a lesser extent Mixer6, for which the higher sampling rate provides advantages. These corpora, together with AMI array are those with the higher VAD errors. This is no surprise since they all contain data recorded with far-field microphones which are intrinsically more difficult and where the information carried in higher frequencies can be more helpful. In all cases, the error is very high due to the high percentage of missed speech, corresponding to low recall. It should be noted that one possibility to improve the performance on such sets would be to fine-tune the models or tune their detection thresholds using a training or development set of matching characteristics."}, {"title": "3.4 Strengths of VBx", "content": "One aspect is that the method relies on x-vectors as features for performing clustering. Current state-of-the-art systems for speaker verification rely on this type of features and large efforts are being made by the community to optimize and improve their quality. VBx ingests current state-of-the-art x-vectors but it can also use any new version of such embeddings, benefiting from any improvements that they can bring.\nAnother aspect is that the speaker models are derived from a PLDA model trained with large amounts of x-vectors from thousands of speakers which makes the speaker models in VBx very robust.\nAs shown in the experiments, the model can learn the number of speakers in the record-ing by itself and with great accuracy. Other approaches fail on this or need to use a different sub-module to estimate the number of speakers while the Bayesian nature of VBx can handle this elegantly. Furthermore, the model is principled as it relies on a probabilistic modeling of the diarization task. This is in contrast to other approaches that rely heavily on ad-hoc tuning of thresholds.\nMoreover, the model itself does not require large amounts of data with diarization annotations to perform well. This contrasts NN-based approaches, which are usually very data-hungry and more computationally expensive.\nFinally, the main advantage of a cascaded model is that it is relatively easy to replace sub-modules. For example, one can focus on improving the VAD module with little effect on other parts of the system but impact in the final performance."}, {"title": "3.5 Weaknesses of VBx", "content": "One of the problems is that a system that makes use of VBx for clustering has also other modules to deal with VAD, OSD and potentially yet other modules such as enhancement or resegmentation. This makes running such a system somewhat cumbersome as several sub-systems have to be run sequentially. A more elegant approach would be a single model that can take the signal as the input and produce directly the diarization output."}, {"title": "3.6 Final remarks", "content": "This chapter described the standard cascaded framework for diarization models. A brief historical overview of each sub-module was presented and special emphasis was put on the Bayesian hidden Markov model scheme for clustering, commenting on the initial models that inspired the current version and describing its formulation in detail. Making use of x-vectors as input, VBx was evaluated on standard tasks with data from different domains.\nFinally, the strengths and weaknesses of the model were discussed. In spite of its problems, the ease of use of VBx recipe and strong results allowed it to be used as the baseline in numerous publications and even in some of the most recent challenges [Yu et al., 2022a, Grauman et al., 2022]."}, {"title": "Chapter 4", "content": "The main weakness of cascaded methods for diarization is their inherent multi-modular nature. Not only the inference can be cumbersome and potentially slow but also each subsystem is trained independently with different objectives. Therefore, having a single model to handle the whole problem is desirable.\nIn this chapter, we discuss the best-performing discriminatively trained diarization sys-tems with special emphasis on end-to-end neural diarization (EEND), for being the most promising framework lately. Then, given the need of these models for large amounts of annotated training data, we discuss an alternative approach to create \"simulated conversa-tions\" (in contrast to the original \"simulated mixtures\") and show its advantages. Finally, we present DiaPer, a model that builds on the EEND framework."}, {"title": "4.1 Neural networks for diarization", "content": "Neural networks have been used in the context of diarization for some time. Given the most common diarization pipeline described in Figure 3.1, the first applications of NN focused on individual steps. We have covered examples for pre-processing, VAD, segmentation and overlap detection in section 3.1. In this section, we cover specific works for speaker embeddings extraction and decoding of speaker activities which span the most recent efforts in these directions. The lists in Sections 4.1.1 and 4.1.2 do not pretend to be exhaustive but rather cover the works that have shown remarkable results at their time or that presented a new, and promising, line of research. In particular, Section 4.1.2 covers target speaker voice activity detection (TS-VAD) which processes the recordings in an end-to-end fashion but relies on a previously trained speaker embedding extractor and an initial diarization output given by another model.\nAlthough end-to-end diarization models have only been developed in the last few years, they have attracted a broad interest. Section 4.1.3 covers the main works in the end-to-end neural diarization framework. EEND has been the first fully end-to-end model capable of dealing with the permutation problem intrinsic to speaker diarization and has paved the road for most of the recent developments in this field.\nWhile both EEND and TS-VAD have their advantages, they also struggle to handle certain scenarios. Different extensions have been proposed and some of them can be cat-egorized as \"two-stage\" models where the first stage usually performs EEND on short"}, {"title": "4.1.1 Neural networks for speaker embeddings and affinity matrices", "content": "Since the advent of neural speaker embeddings (d-vectors [Variani et al., 2014] or x-vectors [Snyder et al., 2018]), we could say that all competitive diarization systems nowadays make use of an NN. NN-based embeddings capture speaker information that allows for bet-ter segment representations which lead to better performance. However, many approaches have focused on modifying the speaker representations or the affinity matrix (obtained by comparing every embedding against each other). These are, in turn, used by a clustering method for the purpose of diarization using different techniques:\n\u2022 [Yella et al., 2014] is one of the first works in this direction where an NN is trained to discriminate speakers. Then, the output of an intermediate layer (\u2018bottleneck') is used as features in an HMM-GMM system to perform clustering. The main novelty in this work was the combination of acoustic features with discriminatively trained representations in the context of a diarization system.\n\u2022 [Garcia-Romero et al., 2017] proposed using DNN-based embeddings in combination with AHC to produce diarization outputs. Even though the term \"x-vector\u201d was not coined yet, the embeddings were obtained with the same strategy so this is the first work to have performed clustering of x-vectors for diarization. Analogously, [Wang et al., 2018] present the first application of d-vectors for diarization by clustering them with spectral clustering.\n\u2022 [Lin et al., 2019] proposed to produce the affinity matrix using a BLSTM. This is ac-complished by feeding the BLSTM with sequences where the ith frame is concatenated to each frame in the original sequence. The network is trained to learn the similarity between frame i and each other frame using binary cross-entropy (BCE) with \"same\" versus \"different\" classes. The approach showed excellent results at the time and, in spite of the limitations of LSTMs with long sequences, the BLSTM-based affinity matrix performed significantly better than a PLDA-based one on long utterances. A similar idea is presented in [Lin et al., 2020a], where self-attention is used to produce the similarity matrix in the same fashion as with the BLSTM and leads to better results.\n\u2022 [Wang et al., 2020] used graph neural networks (GNN) to refine the input embed-dings. The GNNs (which work as an encoder) are trained so that the affinity matrix of the modified embeddings resembles the ground truth adjacency matrix, using a binary classification loss for linkage prediction for all pairs of segments inside each session. When using spectral clustering with the processed embeddings, they man-age to obtain significant improvements over the original representations. This shows the potential for transforming embeddings normally trained for speaker recognition towards diarization-specific applications. [Wang et al., 2023d] also utilize GNNs but instead of refining the embeddings in a global fashion, they refine clusters of embed-dings (denoted by their corresponding subgraphs) one at a time. Then, the refined subgraphs are merged and the new whole graph is partitioned to obtain speaker clus-ters. Unlike [Wang et al., 2020], the clustering is performed in the graph space and it"}, {"title": "4.1.2 Neural networks for decoding speaker activities", "content": "A different line of work to the ones described above has focused on replacing the standard clustering approach by an NN-based solution. In some cases, the assignment of speaker labels to the embeddings was treated as a sequence-to-sequence problem modeled with BLSTMs [Zhang et al., 2022] and with an encoder-decoder Transformer-based architecture [Li et al., 2021]. In this last case, while remarkable results were obtained in comparison with spectral clustering, using augmentation strategies is crucial to make this approach successful.\nOne particular model, named unbounded interleaved-state recurrent neural network [Zhang et al., 2019] handles segmentation, embedding extraction and clustering steps together. It works in an online manner and can deal with an unbounded number of speakers. Although they present remarkable results, their full recipe is not shared as it relies on internal components of Google's infrastructure.\nSome approaches have focused on clustering or diarization using NNs but from the perspective of source separation [Hershey et al., 2016, von Neumann et al., 2019]. Given that they focus on a different task which usually requires clean speech from each speaker, we do not discuss them further here.\nApproaches focusing on clustering with an NN, just like any other clustering method for diarization, cannot deal naturally with overlapping segments. Another application of NN consists in decoding speaker activity probabilities for each speaker, eventually handling overlapped speech. TS-VAD follows this strategy and is described next."}, {"title": "Target speaker voice activity detection", "content": "Target speaker voice activity detection [Medennikov et al., 2020] was developed in the context of the CHiME-6 Challenge in 2020 and its performance was so remarkably better than any other system that they won the challenge by a large margin. This approach makes use of speaker embeddings (i-vectors) from each speaker as references to find where those speakers appear in the recording, thus the name. TS-VAD relies on an initial diarization output used to estimate one i-vector per speaker. Then, acoustic features processed with CNN layers and BLSTM projection layers are used to detect if there is voice activity for each of the speakers using the i-vectors as anchor speaker references, as shown in Figure 4.1. The outputs are per-speaker speech activity probabilities. However, they can benefit from post-processing too: [Medennikov et al., 2020] evaluate a few options such as tuning the detection threshold, applying a median filter, removing short pauses and Viterbi decoding using an HMM with a few states. Eventual re-estimation of the i-vectors is needed to improve the performance but the approach manages to perform very well, especially in scenarios with large amounts of overlapped speech such as CHiME-6. Nevertheless, one of the main drawbacks of this approach as originally proposed is that it requires a priori knowledge of the number of speakers.\nTS-VAD was extended in [Wang et al., 2021b] to deal with an unknown number of speakers by the winning team of the Third DIHARD Challenge [Ryant et al., 2021] by adding (other, non-appearing in the conversation) or removing (least talkative) anchor speaker references. In [He et al., 2021], an upper bound on the number of speakers was used so that the model can drop some of the representations. Adaptive neural speaker diarization using memory-aware multi-speaker embedding [He et al., 2023] presents an"}, {"title": "4.2 Synthetic training data generation", "content": "End-to-end diarization models have shown remarkable improvements over cascaded ones in certain scenarios. However, one of the disadvantages of end-to-end models is that they require large amounts of training data while manually annotated data for diarization are notoriously lacking."}, {"title": "4.2.1 From simulated mixtures ...", "content": "Diarization training data consist of audio recordings and their corresponding speaker segment annotations. In order to create the mixtures with their corresponding annotations, [Fujita et al., 2019a] have used a VAD system run on each side of each conversation in a pool of thousands of telephone conversations. Assuming a single speaker per telephone channel, this means that speech segments and their speaker labels can be gathered from the pool to construct mixtures.\nTo create a mixture, as many speakers as wanted in the mixture (Nspk) are sampled from the total pool. The pool is represented by U = {Us}ses where Us is an utterance of speaker s formed by all the segments denoted by the VAD that belong to that speaker in an original utterance. For each selected speaker, one of their utterances is randomly sampled and Nu consecutive segments are selected randomly from it. Nu is a random number in between Numin and Numax (parameters of the mixture creation procedure). Pauses are introduced in between the selected segments of a speaker to simulate turns in a conversation, where the length of the pause is sampled from an exponential distribution"}, {"title": "4.2.2 ... to simulated conversations", "content": "One of the main concerns with SM is that each speaker in the mixture is treated inde-pendently. Although the lengths of the pauses are randomly drawn from an exponential distribution, a sensible choice as it represents the inter-arrival times between independent events (the speech segments of the speaker), this does not resemble the dynamics of a real conversation where speakers do not take turns independently but collaboratively. For this reason, the proposed approach to create SC [Landini et al., 2022a] uses the following statistics on frequencies, lengths of pauses and overlaps from real conversations:\n\u2022 Number of times that two consecutive segments are of the same speaker and separated by a pause and a histogram defining D=speaker, the distribution of pause lengths between segments of the same speaker."}, {"title": "4.2.3 SM versus SC", "content": "For the sake of the analysis, we consider the addition of background noises and reverberation (already present in [Fujita et al., 2019a]) as augmentations. For analyzing the effect of using or not using each of these augmentations, different training datasets are generated for each setup. The same EEND model is trained on each one independently and its performance is evaluated on real data. Besides those two (already present in the original approach), we"}, {"title": "4.3 DiaPer: End-to-end neural diarization with Perceiver-based attractors", "content": "As discussed in Section 4.1.3, the EEND framework is the most prolific single-stage end-to-end model, showing remarkable performance. EEND-EDA, although in theory capable of dealing with flexible numbers of speakers, has certain limitations. One of them is that the model is strongly biased towards the number of speakers per recording seen during training and has difficulties handling conversations with many speakers. Even more, it is necessary to shuffle the frames at the input of the attractor encoder to obtain the best performance showing that the choice of LSTM as encoder is suboptimal.\nSeveral works have proposed modifications to the EEND-EDA model, as discussed in Section 4.1.3. Three of them [Rybicka et al., 2022, Fujita et al., 2023, Chen et al., 2023b], in particular, present a non-autoregressive attractor-decoder framework based on attention as shown in Figures 4.12, 4.13, 4.14. The three models are presented under the same general framework where some blocks are common to all of them. All share a similar frame encoder based on self-attention layers to produce the frame embeddings and in all cases, the frame-speaker activities are obtained by applying sigmoid on the dot product between frame embeddings and attractors. Thus, the main differences stem from how the attractors are obtained."}, {"title": "4.3.1 Motivation and background", "content": "As discussed in Section 4.1.3, the EEND framework is the most prolific single-stage end-to-end model, showing remarkable performance. EEND-EDA, although in theory capable of dealing with flexible numbers of speakers, has certain limitations. One of them is that the model is strongly biased towards the number of speakers per recording seen during training and has difficulties handling conversations with many speakers. Even more, it is necessary to shuffle the frames at the input of the attractor encoder to obtain the best performance showing that the choice of LSTM as encoder is suboptimal.\nSeveral works have proposed modifications to the EEND-EDA model, as discussed in Section 4.1.3. Three of them [Rybicka et al., 2022, Fujita et al., 2023, Chen et al., 2023b], in particular, present a non-autoregressive attractor-decoder framework based on attention as shown in Figures 4.12, 4.13, 4.14. The three models are presented under the same general framework where some blocks are common to all of them. All share a similar frame encoder based on self-attention layers to produce the frame embeddings and in all cases, the frame-speaker activities are obtained by applying sigmoid on the dot product between frame embeddings and attractors. Thus, the main differences stem from how the attractors are obtained."}, {"title": "4.3.2 Model description", "content": "The Perceiver [Jaegle et al., 2021] is a Transformer [Vaswani et al., 2017] variant that employs cross-attention to project the variable-size input onto a fixed-size set of latent representations, as shown in Figure 4.15. These latents are transformed by a chain of self-attention and cross-attention blocks. By encoding the variable-size input (\"Byte array\" in the figure) into the fixed-size latent space (\"Latent array\" in the figure), the Perceiver reduces the quadratic complexity of the Transformer to linear. In this work, we utilize the Perceiver framework to encode speaker information into the latent space and then derive attractors from the latents. Perceiver allows us to handle a variable number of speakers per conversation while addressing some of the limitations of EDA with a fully non-autoregressive (and iteration-free) scheme.\nDiaPer shares many facets with other EEND models, such as defining diarization as a per-speaker-per-time-frame binary classification problem. Given a sequence of observations (features) X \u2208 RT\u00d7F where T denotes the sequence length and F the feature dimensionality, the model produces \u00dd \u2208 (0,1)T\u00d7S which represent the speech activity probabilities of the S speakers for each time-frame. Just like with EEND-EDA, the model is trained so that \u0176 matches the reference labels Y \u2208 {0,1}T\u00d7S where yt,s = 1 if speaker s is active at time t and 0 otherwise. The main difference between EEND-EDA and DiaPer is in how the attractors are obtained given the frame embeddings.\nThe main two modules in DiaPer are the frame encoder and the attractor decoder. As shown in Figures 4.17 and 4.18 and proposed in [Fujita et al., 2019b], the frame encoder"}, {"title": "4.4 Final remarks", "content": "In this chapter, we covered end-to-end diarization systems, starting with an overview of existing approaches and special emphasis on the EEND framework.\nThen, we focused on the generation of synthetic training data. End-to-end models require large amounts of diarization-annotated data but due to the scarcity of annotated real data, the compromise solution consists in generating synthetic recordings. We proposed an approach that allows such models to obtain better performance and reduces the need for fine-tuning on telephone conversations.\nFinally, we presented DiaPer as an alternative to the most popular end-to-end model that can handle multiple speakers. We showed that it can achieve better performance even with fewer parameters and performed a thorough comparison between both approaches. Our approach - DiaPer - performs better than the popular EEND-EDA mainly in handling overlapped speech and it can find the number of speakers in a conversation more accurately."}, {"title": "Chapter 5", "content": "In this chapter, we present a final comparison between a cascaded system (based on VBx, as described in Chapter 3.2) and an end-to-end system (DiaPer, as described in Chapter 4.3). The intention is to provide insights about the advantages of each approach across different scenarios. We also compare the results of these approaches with the best-published results at the time of writing this thesis. Then, we conclude the thesis with a brief summary of what are the challenges that we believe need to be addressed in the future of speaker diarization."}, {"title": "5.1 Modular systems versus end-to-end systems", "content": "Table 5.1 presents an exhaustive comparison with all competitive systems on the Callhome (2 speakers) dataset at the time of publication under the same conditions: all speech is evaluated and no oracle information is used. Data refers to the number of hours of data for supervision. For end-to-end models, it can be real or synthetic data and for the clustering-based method, it consists of all data used to train the x-vector extractor, VAD and OSD. Methods are divided into groups depending on if they are single or two-stage. Even though DiaPer does not present the best performance among all approaches, it reaches competitive results with fewer parameters and even without FT. On the other hand, the VBx-based method falls behind since its performance in handling overlapped speech is worse."}, {"title": "5.2 Moving forward", "content": "As discussed in previous chapters, the last quinquennial has seen several changes in the field of speaker diarization. From adopting x-vectors in"}]}