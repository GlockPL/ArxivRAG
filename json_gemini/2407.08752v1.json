{"title": "From Modular to End-to-End Speaker Diarization", "authors": ["FEDERICO NICOL\u00c1S LANDINI"], "abstract": "Speaker diarization is usually referred to as the task that determines \"who spoke when\" in a recording. Until a few years ago, all competitive approaches were modular, i.e. voice activity detection, segmentation, embedding extraction, clustering and overlapped speech detection and handling were tackled by different sub-systems and applied in a cascaded fashion. Systems based on this framework reached state-of-the-art performance in most scenarios but had major difficulties dealing with overlapped speech. More recently, the advent of end-to-end models, capable of dealing with all aspects of speaker diarization with a single model and better performing regarding overlapped speech, has brought high levels of attention.\nThis thesis is framed during a period of co-existence of these two trends. We describe a system based on a Bayesian hidden Markov model used to cluster x-vectors (speaker embeddings obtained with a neural network), known as VBx, which has shown remarkable performance on different datasets and challenges. We comment on its advantages and limitations and evaluate results on different relevant corpora. Then, we move towards end-to-end neural diarization (EEND) methods. Due to the need for large training sets for training these models and the lack of manually annotated diarization data in sufficient quantities, the compromise solution consists in generating training data artificially. We describe an approach for generating synthetic data which resembles real conversations in terms of speaker turns and overlaps. We show how this method generating \u201csimulated conversations\" allows for better performance than using a previously proposed method for creating \"simulated mixtures\" when training the popular EEND with encoder-decoder attractors (EEND-EDA). We also propose a new EEND-based model, which we call DiaPer, and show that it can perform better than EEND-EDA, especially when dealing with many speakers and handling overlapped speech. Finally, we compare both VBx-based and DiaPer systems on a wide variety of corpora and comment on the advantages of each technique.", "sections": [{"title": "Chapter 1", "content": "Speech processing technologies have advanced considerably during the last decades due to more computing capacity, more available data, and big efforts from the research community to propose new techniques and improve the performance of systems. Many speech-related tasks have progressed remarkably and one of them is speaker diarization, usually referred to as the task of deciding \"who spoke when\". More formally, it is the task of assigning speaker labels to segments of speech in a given recording. This task is of interest per se for allowing segmentation of conversations or search of speech from a speaker of interest, among others. However, it is also useful as an upstream task for other well-studied tasks such as information retrieval, speech recognition, or speaker verification.\nSpeaker diarization is applied on diverse domains. While originally constrained to telephone conversations, recorded meetings, or broadcast news, in the last few years, large amounts of broadcast data (not only news) have also gained relevance as target data. Even more, the availability of small devices has facilitated recording speech in more varied situations. Common examples are recordings in courtrooms or parliaments as well as con-versations in many locations, such as restaurants. Besides, some specific applications are of interest, for example, diarization on clinical interviews to analyze sessions with patients and diarization on conversations in households with children to evaluate language acquisi-tion. Furthermore, the recent global pandemic has increased the number of meetings over web-based platforms and speaker diarization can be useful to enrich transcriptions. Even more, ubiquitous speech-controlled devices often require to know which user is interacting with the device, thus benefiting from automatic diarization. All these applications provide new challenges and such conditions demand robust diarization systems able to cope with different acoustic channels, styles of speech and numbers of speakers.\nSince the origins of speaker diarization around 20 years ago, several approaches have been proposed. We describe the techniques used in the most popular past systems as well as the models that attain state-of-the-art performance nowadays. Systems can be framed under two big families, namely modular ones: those formed by different sub-modules such as voice activity detection, embedding extraction, and clustering (among others); and neural network-based end-to-end systems that solve the task with a single module. State-of-the-art systems under these two frameworks are described, evaluated and compared in this work."}, {"title": "1.1 Contributions", "content": "The contributions of our work are the following:\n\u2022 We present the Bayesian HMM clustering of x-vectors (VBx) as a method for clus-tering of speaker embeddings for diarization. VBx was presented in [Landini et al., 2022b], showing state-of-the-art performance (at the time) on three relevant datasets. At the time of that publication, AMI, one of the most standard diarization datasets, was used with different evaluation setups, effectively impeding proper comparison be-tween works of different authors. We pointed this out and showed the performance of VBx in the most widely used setups. With the aim of avoiding future confusion, we released the annotations derived from the official transcriptions following a clear and detailed procedure https://github.com/BUTSpeechFIT/AMI-diarization-setup\nWe also released the code and x-vector extractors needed to run the recipes:\nhttps://github.com/BUTSpeechFIT/VBx\nDue to the continuous maintenance of the repository, ease of use and competitive results, VBx has served as baseline in numerous publications.\n\u2022 VBx was also a core module of the winning system of the Second DIHARD Chal-lenge [Landini et al., 2020] and the second-ranked system in VoxSRC 2020 [Landini et al., 2021a]. We released the code for both systems too.\nhttps://github.com/BUTSpeechFIT/VBx/tree/v1.0_DIHARDII\nhttps://github.com/BUTSpeechFIT/VBx/tree/v1.1_VoxConverse2020\nThanks to the strong performance and public recipes, VBx has been used as the baseline in recent challenges such as M2MeT and Ego4D [Yu et al., 2022a, Grauman et al., 2022].\n\u2022 This thesis also presents the results obtained with a fully modular system based on the public VBx recipe and public voice activity detection and overlapped speech detection systems. These results are presented on a variety of datasets and serve not only as baseline in the context of this work but for other researchers in the field.\n\u2022 End-to-end neural diarization (EEND) models require large amounts of training data and, due to the lack of manually annotated data in those quantities, synthetic data need to be generated. We introduce an alternative approach to the original \"simu-lated mixtures\u201d scheme for generating \"simulated conversations\" (which resemble real conversations regarding the lengths of pauses and overlaps), used as training data for end-to-end models. Extensive comparisons between our approach and the former one in [Landini et al., 2022a, Landini et al., 2023a] showed that using simulated conversa-tions reduces almost completely the need for fine-tuning the model using in-domain data in the telephony scenario. We released the code for generating training data as well as our PyTorch implementation of the EEND with encoder-decoder attractors model and models trained on free public data:\nhttps://github.com/BUTSpeechFIT/EEND_dataprep\nhttps://github.com/BUTSpeechFIT/EEND\nThanks to this, simulated conversations have already been adopted in other publica-tions."}, {"title": "1.2 Organization of the thesis", "content": "The rest of the thesis is organized as follows:\n\u2022 Chapter 2 formally defines the task of speaker diarization describing how models are evaluated and the connection with other tasks.\n\u2022 Chapter 3 presents modular diarization systems and describes in detail VBx, one of the clustering methods considered state-of-the-art nowadays, which is one of the main contributions of this thesis. The performance of VBx is analyzed on different datasets.\n\u2022 Chapter 4 presents end-to-end diarization systems covering the different works that have been recently published. Given the \"data-hungry\" nature of these models, we describe a new method for generating synthetic training data, coined \"simulated con-versations\", and show its advantages. Then, we present DiaPer, a novel method based on the end-to-end framework and analyze its performance on different datasets.\n\u2022 Chapter 5 concludes the thesis discussing the comparison of the aforementioned mod-ular and end-to-end systems and the future challenges of speaker diarization."}, {"title": "Chapter 2", "content": "Historically, audio diarization [Reynolds and Torres-Carrasquillo, 2005] has referred to the broad task of segmenting a recording and categorizing those segments (for example, into silence, noise, music, commercials, speech, etc.). The popularity it gained in the early 2000s has waned and few works are framed under that task nowadays. Speaker diarization corresponds to the more specific task of segmenting a recording in terms of speech segments and assigning speaker labels. Speaker diarization (SD) is usually referred to as \u201cwho spoke when\" and gained popularity in the current definition also during the early 2000s, mainly due to the National Institute of Standards and Technology (NIST) in their series of Rich Transcriptions (RT) evaluations and several European projects [Anguera et al., 2012]. Examples of both tasks are presented in Figure 2.1 over the same recording. Note that for SD, the goal is to distinguish speakers and using relative speaker labels such as 'Spk1' and 'Spk2' is enough. Still, for some applications, the role (i.e. presenter, announcer, guest, etc.) or specific speaker labels (i.e. Alex, Bob, Carol, etc.) are necessary and in such cases, the SD labels are post-processed by a speaker identification system. Since only speaker segments are of interest, anything else is simply not labeled. Speech segments can be bounded by non-speech segments or speaker change points, as observed in Figure 2.1.\nIn this work, we focus on the most general scenario for speaker diarization, where no information is given about the input recording. Some examples of a priori information can be\n\u2022 number of speakers\n\u2022 language"}, {"title": "2.1 Task definition", "content": "Historically, audio diarization [Reynolds and Torres-Carrasquillo, 2005] has referred to the broad task of segmenting a recording and categorizing those segments (for example, into silence, noise, music, commercials, speech, etc.). The popularity it gained in the early 2000s has waned and few works are framed under that task nowadays. Speaker diarization corresponds to the more specific task of segmenting a recording in terms of speech segments and assigning speaker labels. Speaker diarization (SD) is usually referred to as \u201cwho spoke when\" and gained popularity in the current definition also during the early 2000s, mainly due to the National Institute of Standards and Technology (NIST) in their series of Rich Transcriptions (RT) evaluations and several European projects [Anguera et al., 2012]. Examples of both tasks are presented in Figure 2.1 over the same recording. Note that for SD, the goal is to distinguish speakers and using relative speaker labels such as 'Spk1' and 'Spk2' is enough. Still, for some applications, the role (i.e. presenter, announcer, guest, etc.) or specific speaker labels (i.e. Alex, Bob, Carol, etc.) are necessary and in such cases, the SD labels are post-processed by a speaker identification system. Since only speaker segments are of interest, anything else is simply not labeled. Speech segments can be bounded by non-speech segments or speaker change points, as observed in Figure 2.1."}, {"title": "2.2 Evaluation", "content": "In order to evaluate a diarization system, just as with any other task, the annotations produced by a system need to be compared against ground truth ones. However, ground truth labels are generated following different criteria depending on the dataset. To illustrate the procedure of generating diarization labels, we utilize a few corpora as examples:\nFor the First DIHARD Challenge, manual annotations were first produced, then refined using forced alignment (where pauses longer than 200 ms were used to split segments) and then checked and corrected by annotators [Ryant et al., 2018]. This is a very expensive annotation pipeline since human annotators need to listen to a recording several times to produce quality annotations.\nA more automated approach was utilized when collecting and annotating VoxCon-verse [Chung et al., 2020], where videos were available and voice activity detection was correlated with mouth movement of in-screen speakers to assign speaker labels to most speech segments. Speaker embeddings of off-screen speech segments were obtained and compared with those of already determined speakers in order to assign identities to those segments. This was followed by manual inspection to identify common errors and instruc-tions were given to manual annotators to correct them following certain guidelines (i.e. pauses longer than 250 ms were used to split segments, mark segments with 0.1s precision, etc.).\nAs observed in these two examples, different criteria can be used to produce the reference annotations. Related to the precision of the reference annotations is also the discussion of calculating metrics using a forgiveness collar or not. Using a collar means that small differences (below a certain threshold) between the reference and system annotations will not be considered for calculating errors. Different corpora have chosen different conventions; for example, DIHARD does not use a collar (meaning that manual annotations are fully"}, {"title": "2.2.1 Ground truth labels", "content": "In order to evaluate a diarization system, just as with any other task, the annotations produced by a system need to be compared against ground truth ones. However, ground truth labels are generated following different criteria depending on the dataset. To illustrate the procedure of generating diarization labels, we utilize a few corpora as examples:\nFor the First DIHARD Challenge, manual annotations were first produced, then refined using forced alignment (where pauses longer than 200 ms were used to split segments) and then checked and corrected by annotators [Ryant et al., 2018]. This is a very expensive annotation pipeline since human annotators need to listen to a recording several times to produce quality annotations.\nA more automated approach was utilized when collecting and annotating VoxCon-verse [Chung et al., 2020], where videos were available and voice activity detection was correlated with mouth movement of in-screen speakers to assign speaker labels to most speech segments. Speaker embeddings of off-screen speech segments were obtained and compared with those of already determined speakers in order to assign identities to those segments. This was followed by manual inspection to identify common errors and instruc-tions were given to manual annotators to correct them following certain guidelines (i.e. pauses longer than 250 ms were used to split segments, mark segments with 0.1s precision, etc.).\nAs observed in these two examples, different criteria can be used to produce the reference annotations. Related to the precision of the reference annotations is also the discussion of calculating metrics using a forgiveness collar or not. Using a collar means that small differences (below a certain threshold) between the reference and system annotations will not be considered for calculating errors. Different corpora have chosen different conventions; for example, DIHARD does not use a collar (meaning that manual annotations are fully"}, {"title": "2.2.2 Metrics", "content": "Since speaker labels generated by diarization systems do not need to match the naming convention of the reference ones (e.g. 'Spk1', 'Spk2', etc.), a 1:1 mapping between the system-generated speaker clusters and the reference ones is obtained using the Hungarian method [Kuhn, 1955] for finding the solution of a bipartite graph. Given this optimal mapping, errors are calculated to determine how good the system output is with regard to the reference.\nThe most widely used performance metric for diarization is the diarization error rate (DER) proposed by NIST in their series of RT evaluations during the 2000s [NIST, 2009]. DER is defined by\n$$DER=\\frac{SER+FA + Miss}{Total speech},$$,\nwhere:\n\u2022 SER stands for speaker error, the amount of time that speech is attributed to incorrect speakers (i.e. confusion of speakers)\n\u2022 FA is false alarm, the amount of time that non-speech regions are incorrectly at-tributed to a speaker (or time when overlapped speech of K speakers is found in speech regions with $$N < K$$ reference speakers)\n\u2022 Miss stands for missed speech, the amount of time that speech is not attributed to any speaker (or time when overlapped speech of K speakers is found for regions with $$N > K$$ reference overlapping speakers)\n\u2022 Total speech is the total amount of speech, accounting also for speaker overlaps.\nAn example of how DER is calculated can be seen in Figure 2.2 where the best matching {Interviewer - Spk1, Intervieweel - Spk2, Interviewee2 - <>} is denoted by the colors. For more details about DER, please refer to the RT-09 evaluation plan.\nDue to inherent inconsistencies in the annotations introduced by the annotators when finding the precise boundaries of segments, NIST proposed using an optional 0.25s for-giveness collar around each boundary of a speech reference segment where speech is not scored. Using a collar means that small differences (below a certain threshold) between the reference and system annotations will not be considered for calculating errors.\nFor the Second DIHARD Speaker Diarization Challenge [Ryant et al., 2019b], the orga-nizers introduced the Jaccard error rate (JER) based on the Jaccard index [Jaccard, 1901]. JER has become a popular secondary metric after DER."}, {"title": "2.3 Relevance of the task", "content": "Although speaker diarization has received comparatively less attention from the speech community than other tasks such as speaker recognition or speech recognition, it has several applications. In this section, we briefly describe the possible applications of diarization and show its relevance even in tandem with other tasks.\nIn general, the application of diarization is related to providing rich meta-data in terms of speaker segments which allow indexing of audiovisual resources with speakers and struc-tured search and access to resources. There are several domains to which it is of interest to apply diarization. We briefly list here some of the most common ones as reviewed in [Tranter and Reynolds, 2006, Anguera et al., 2012, Moattar and Homayounpour, 2012]:\n\u2022 broadcast news, debates, television shows, movies\n\u2022 recorded meetings (which becomes particularly interesting nowadays as many meet-ings are being held remotely)\n\u2022 telephone conversations\n\u2022 clinical interviews\n\u2022 recorded lectures or conferences\nHowever, SD can be combined with other tasks, including the most studied ones: ASR and SR. SD in combination with ASR, determine \"who said what\" and solve the \"cocktail party problem\" [Watanabe et al., 2020, Cornell et al., 2023]. Sometimes SR has to be performed on recordings with several speakers and, in such cases, diarization is key to allow finding the speaker of interest as in a recent speaker recognition evaluation (SRE) [NIST, 2018]. However, there are some other applications for diarization such as speaker indexing and retrieval, speaker counting, or improving ASR systems by allowing speaker-specific adaptation."}, {"title": "Chapter 3", "content": "Some of the first works related to diarization started to appear around the year 2000 [Garo-folo et al., 2004, Fiscus et al., 2006]. Works published in the late 90s [Saunders, 1996, Gau-vain et al., 1998, Liu and Kubala, 1999, Siegler et al., 1997] focusing on telephone con-versations or broadcast news formulated the task as \u201cpartitioning\" or \"segmentation and clustering\", among others. But it was the series of Rich Transcription Evaluations [NIST, 2009] organized by NIST between 2002 and 2009 that first defined the diarization task and set a framework for comparing different approaches for diarization. The focus was initially on broadcast news data and later on meeting data. Several European projects focused on meeting data during the first decade of 2000 and also shifted the attention to the challenges related to the meeting scenario [Anguera et al., 2012]. The works related to diarization mostly focused on those types of recordings in the following years until approxi-mately 2017-2018 with the emerging DIHARD challenge. Since this work focuses on off-line diarization, i.e. diarization when the whole recording is available at evaluation time, we only cover previous work in this line.\nUntil the recent advent of end-to-end neural diarization models, most diarization sys-tems were composed of several subsystems that solved different subtasks, sometimes related to specific setups (e.g. multi-channel recordings). In this chapter, we briefly cover the his-tory of the cascaded approach, comment on its strengths and weaknesses and finally describe the clustering-based method studied in this thesis."}, {"title": "3.1 Structure of a standard cascaded system", "content": "Throughout the years, most competitive diarization systems have used several of the blocks depicted in Figure 3.1, applied one after the other in a \"cascaded\" fashion. A common categorization for diarization systems is between bottom-up and top-down. Bottom-up refers to starting from short segments of speech and clustering them until finding just a few speakers. Top-down consists of starting with the full recording as a whole and partitioning it into smaller parts that in the end will correspond to a few speakers. The former approach has been more thoroughly studied (Figure 3.1 represents this one) but a comparison between the two can be found in [Anguera et al., 2012, Evans et al., 2012].\nWe describe below each of the modules mentioned in Figure 3.1. Note that many other possible sub-modules as well as more details about each of the steps are omitted as this is a high-level overview. More details can be found in [Tranter and Reynolds, 2006] for systems"}, {"title": "Signal pre-processing", "content": "In the context of diarization, signal pre-processing refers to applying methods that enhance speech and facilitate the ultimate task. While speech pre-processing originally focused on spectral-based methods, nowadays approaches rely on neural-network-based methods. For diarization, the most common methods include denoising and dereverberation.\nEven though telephone conversations and broadcast news can benefit from such pre-processing, this step became more relevant for recorded meetings. While the former are generally recorded with close-talk microphones, the latter usually present setups with far-field microphones where the effect of reverberation or noises made by the participants make for a more challenging scenario [Anguera et al., 2012]. Furthermore, meetings offer in some cases recordings with multiple microphones [Janin et al., 2004, McCowan et al., 2005, Mostefa et al., 2007] and making use of the signal from all of them can provide better performance [Pardo et al., 2006]. Different approaches have been used to deal with multiple microphones but the most popular is acoustic beamforming [Anguera et al., 2007].\nWith the more challenging scenarios presented in DIHARD challenge [Ryant et al., 2018], preprocessing has become quite relevant and the best-performing systems in the challenge made use of at least one type of enhancement method [Sell et al., 2018, Diez et al., 2018b, Sun et al., 2018, Landini et al., 2020, Wang et al., 2021b]."}, {"title": "Voice activity detection", "content": "The task of VAD is to find the segments of voice or speech to be labeled in the diarization output. This means discarding segments of silence, noise and possibly music while keeping all segments of speech. VAD is of interest not only for diarization but for many other tasks such as SR or ASR. Thus, large efforts have been made to improve the capabilities of VAD systems [Ramirez et al., 2007]. Some of the most popular approaches are based on energy or spectrum [Anguera et al., 2005, Istrate et al., 2005], or Gaussian mixture models (GMM), eventually aided by hidden Markov models (HMM), trained with acoustic features such as Mel frequency cepstral coefficients (MFCC) [Wooters et al., 2004, Reynolds and Torres-Carrasquillo, 2004].\nMore recently, systems based on NN have outperformed previous ones [Ryant et al., 2013, Gelly and Gauvain, 2017, Sun et al., 2018]. However, one of the main problems with these systems is that the domains of the training data (telephone, meetings, etc.) bias the model to work well in some scenarios while failing in others. In general, if the model allows to choose different detection thresholds, a lower miss rate of VAD is preferred over a lower false alarm rate since speech discarded in this step is usually not recovered later. Something to consider is that different domains (i.e. meetings, telephone, etc.) present speech with different characteristics and different proportions. For example, in a telephone conversation, speakers usually take turns with very little silence in between whereas in a household day-long recording, speech is generally less common than other sounds or simply silence. Systems are commonly tuned or trained with some application in mind and using a configuration optimal for a certain domain can produce sub-optimal results on recordings of a different domain.\nFurthermore, the annotation criteria and guidelines can vary among different corpora. This means for instance that (sung) background music can be labeled as silence in certain contexts. Moreover, speech from speakers that are not of main interest can be marked as non-speech (think of a recording in a restaurant where people at the next table have a conversation). At the same time, voiced sounds such as cough or sneeze from a speaker of interest can be considered \u201cvoice\u201d even if they are actually not speech. In spite of this, the terms VAD and SAD are usually used interchangeably.\nThis task is so crucial that many challenges have proposed two tracks: one where diarization is evaluated as a whole and another where oracle (extracted from the manual annotations used for evaluations) VAD labels are allowed in order to evaluate the rest of the steps assuming perfect VAD.\nNowadays, most NN-based systems can perform reasonably good VAD except in very challenging conditions, such as dinner-party scenarios or extremely noisy backgrounds."}, {"title": "Segmentation", "content": "Segmentation, sometimes also referred to as speaker change point detection (SCD), finds the points in the recording that separate audio sources (including different speakers). While longer segments allow to have more information to perform clustering in the next step, it is important that the segments do not contain speech from different speakers as this can create problems when trying to find which segments correspond to the same speaker. Therefore, SCD deals with the trade-off between long speech segments versus having pure segments.\nThis step can be combined with VAD in the case that silence or noise are considered audio sources. However, it is common to make use of the VAD output given by the module previously described. The most popular approaches used for SCD are based on comparing"}, {"title": "Representation extraction and clustering", "content": "This step refers to two of the most crucial steps in the pipeline which usually act in tandem. Originally, speech segments were represented by acoustic features such as MFCC, but more carefully designed representations (i.e. embeddings) are preferred nowadays. The speaker embeddings used for diarization, described below, are usually adopted from the technologies used for speaker verification. The clustering builds on top of these representations and assigns them to different speaker identities in a conversation.\nThe clustering step has the objective of providing one (global) cluster per speaker for the whole recording. The predominant approach for doing so is agglomerative hierarchi-cal clustering (AHC) which consists of the following steps: first, pair-wise distances are computed between each pair of initial clusters (simply segments); then, the closest clusters according to the estimated distances are merged and the distances are updated given the new setup. These operations are carried out iteratively until a stopping criterion is met.\nFor calculating the distances, the segments have been represented with Gaussian distri-butions [Moh et al., 2003] or GMMs [Moraru et al., 2003] computed on acoustic features such as MFCCs. Different distance metrics have been used as well as different stopping criteria and some of them followed the same approaches described for segmentation. Among distance metrics, the most popular have been GLR and BIC for which the stopping criterion is based on comparing the statistics of two clusters versus a single cluster after merging, thus allowing for fast clustering.\nOther approaches model speakers by maximum-a-posteriori adapting a GMM-based universal background model (UBM) to speaker clusters [Betser et al., 2004, Reynolds and Torres-Carrasquillo, 2004]. Also making use of BIC, both approaches use a fixed thresh-"}, {"title": "Resegmentation", "content": "Resegmentation refers to an optional step in which the boundaries of the clustering out-put are refined, sometimes followed by new segmentation, embedding extraction, and re-clustering stage. The most common approach uses Viterbi decoding (with or without iterations); this step is closely related to the HMM-based approach covered in Section 3.2. Other approaches based on sliding windows were proposed [Adam et al., 2002] in which windows for the speakers are estimated and their boundaries re-estimated based on some distance measure. Given initial segments that represent speakers, the distances between them and new candidate windows give possible points for resegmentation. However, this approach has not been largely explored.\nThe clustering and resegmentation steps were used iteratively in [Gauvain et al., 1998, Barras et al., 2004] and later in [Shum et al., 2012] with more sophisticated clustering methods."}, {"title": "Post-processing", "content": "As in many speech processing tasks, one common procedure to leverage the performance of individual systems is system combination or fusion. Diarization is not the exception in this regard and there have been attempts to combine the outputs of different systems such as using cluster voting [Tranter, 2005] or merging outputs [Meignier et al., 2006] considering the agreement of the models. Another possibility is hybridization (or \"piped\" systems) [Meignier et al., 2006] which uses the output of one system to initialize another one.\nMore recently, an approach following a technique commonly used in ASR: recognizer output voting error reduction (ROVER) has been proposed [Stolcke and Yoshioka, 2019] under the name of DOVER where voting between different systems is done to fuse the outputs of several systems to generate a final decision. DOVER was extended to handle systems with overlapping segments in DOVER-Lap [Raj et al., 2021b] and has become relevant in recent challenges."}, {"title": "Overlapped speech detection", "content": "A different type of post-processing that deserves attention on its own is dealing with seg-ments of overlapped speech, where two or more speakers speak simultaneously. Overlapped speech is present in general in all types of recordings but with different proportions. Given that most modular systems only output one speaker per frame, the penalization on record-ings with high levels of overlap is considerable."}, {"title": "3.2 Diarization based on Bayesian hidden Markov model\n(VBx)", "content": "This section covers the work done regarding the Bayesian hidden Markov model for diariza-tion and the configuration of a full system with this approach. This system falls into the category of clustering of embeddings and, being a more sophisticated approach than other simpler clustering methods, it has allowed for state-of-the-art performance on different sets for considerable time. Nowadays, it is still used as a contrastive baseline system in many publications."}, {"title": "3.2.1 Background", "content": "Hidden Markov models [Rabiner and Juang, 1986] have been used for speech processing for a very long time. In particular, an ergodic HMM presents a suitable framework for diarization where states represent speakers, the transitions between states represent the speaker turns in a conversation and each state has a speaker model which is assumed to generate the observed data.\nSome of the first works in diarization used HMMs [Sugiyama et al., 1993, Wilcox et al., 1994, S\u00f6nmez et al., 1999] although at that time they referred to the task as \u201cspeaker tracking\" or simply \"speaker segmentation\". The main problem with those approaches was that the number of speakers in the conversation had to be known in advance. An extension to tackle that problem consisted in an evolutive HMM [Meignier et al., 2000, Meignier et al., 2001, Meignier et al., 2006, Fredouille and Evans, 2008] where new states were added iteratively and the Viterbi decoding ran after each change of HMM configuration to see if the new model fitted the observed data better. Another approach [Ajmera et al., 2002] started from a large number of segments (many more than the number of expected speakers) and \"merged\" states as long as the new configuration fitted the data better than the previous one.\nHowever, more relevant work for this chapter was proposed in [Valente and Wellekens, 2004, Valente and Wellekens, 2005] where, for the first time, variational Bayesian (VB) inference was used together with an HMM for diarization. The idea was to use an ergodic HMM modeling the speakers (states) with GMMs. We do not describe the VB inference here as it is a central part of the model we use in this work and it is covered in section 3.2.2.\nAlternatives using a hierarchical Dirichlet process HMM (HDP-HMM) [Teh et al., 2004] and a sticky HDP-HMM were presented in [Fox et al., 2011] using also a Bayesian framework in the context of diarization. With the HDP-HMM, the model allowed potentially an infinite number of speakers, since the number of states is inferred naturally. As presented by the authors, this approach has the problem that it does not model the persistence of states adequately. This means that the approach gives a higher posterior probability to models with a higher number of states and with rapid switching between them. In order to fix this, they included a parameter of self-transition bias with its corresponding prior so that it is part of the whole Bayesian framework. Their extension, called sticky HDP-HMM, presented a fully Bayesian framework (i.e. priors were assumed over all the parameters) to"}, {"title": "3.2.2 Model description", "content": "The model that we describe here corresponds to the latest version of the BHMM which works at x-vector level as described in [Landini et al., 2022b", "2019a": "."}]}