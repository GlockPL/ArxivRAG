{"title": "The erasure of intensive livestock farming in text-to-image generative AI", "authors": ["Kehan Sheng", "Frank A.M. Tuyttens", "Marina A.G. von Keyserlingk"], "abstract": "Generative AI (e.g., ChatGPT) is increasingly integrated into people's daily lives. While it is known that AI perpetuates biases against marginalized human groups, their impact on non-human animals remains understudied. We found that ChatGPT's text-to-image model (DALL-E 3) introduces a strong bias toward romanticizing livestock farming as dairy cows on pasture and pigs rooting in mud. This bias remained when we requested realistic depictions and was only mitigated when the automatic prompt revision was inhibited. Most farmed animal in industrialized countries are reared indoors with limited space per animal, which fail to resonate with societal values. Inhibiting prompt revision resulted in images that more closely reflected modern farming practices; for example, cows housed indoors accessing feed through metal headlocks, and pigs behind metal railings on concrete floors in indoor facilities. While OpenAI introduced prompt revision to mitigate bias, in the case of farmed animal production systems, it paradoxically introduces a strong bias towards unrealistic farming practices.", "sections": [{"title": "1 Introduction", "content": "Since ChatGPT's launch in November 2022, generative artificial intelligence (AI) has seen unprecedented growth, with ChatGPT now having over 180 million monthly active users [30]. Generative Al refers to models that can create new text, images, and other media by learning patterns from existing data, typically guided by text prompts [33]. Evidence suggests that given its ease of use and efficiency, the general public is increasingly relying on ChatGPT over traditional search engines [64]. However, Al ethics research has shown that these AI models inherited human biases through the use of internet-scraped training data, thereby embedding stereotypes, dis- and misinformation into their outputs [36].\nGiven that \"a picture is worth a thousand words\", AI-generated images are inherently positioned to shape biases that influence public perception. Visual information can strongly influence the psychological impact of an issue, with Al-generated images proving particularly persuasive in shaping public discourse [12, 22]. AI generated images are far more likely to be shared than text on social media, and are expected to dominate online content in the near future [59, 65]. Previous research has revealed prevalent representation biases about gender, skin tone, and geo-culture in human subjects [35, 59]. To mitigate these representation biases and ensure guideline compliance (e.g., remove public figures and branded items in the images), OpenAI employs automatic prompt revision (i.e., rewrite user prompts) in DALL-E 3 to enrich images with greater details, while acknowledging that this process comes with the risk of introducing new biases [32].\nDespite extensive efforts made in mitigating human-related bias in AI, it's impact on non-human animals, particularly farmed animals, remains largely unexplored [13, 20, 44]. Humans constitute only 0.01% of total biomass and 35.93% of mammalian biomass on earth, while farmed animals comprise 59.88% of mammalian biomass [5]. To date, global AI regulations and guidelines focus almost exclusively on Al's impact on humans [26], with some minor exceptions, including the recent Montr\u00e9al Declaration that specifically emphasized that AI should consider the well-being of all sentient beings [16]. The European Union's ethical guidelines for trustworthy AI in 2019 included the consideration of sentient beings other than humans [14], but then removed this phrase in their updated Al regulation document in 2024 [15]. No research has systematically asked the question: how does text-to-image generative Al represent livestock farming, a sector that affects billions of lives of farmed animals and is a key pillar in global food production? This question is highly relevant given that the societal concern regarding the lives led by farmed animals continues to gain traction in recent years [60]."}, {"title": "1.1 Cows? Pigs? Why do they matter?", "content": "Driven by a growing demand for abundant, low-cost food supply, farming practices shifted from extensive systems (e.g., cows grazing on pasture, pigs foraging outdoors) toward intensive systems emphasizing productivity after the Great Depression [38]. Intensive livestock farming is characterized by housing large numbers of animals per unit area [50] including indoor confinement in cages or in pens with concrete floors, and severely restricting movement [50, 55, 58]. While the increases in intensification are often justified as necessary to feed a projected global human population of 9.8 billion by 2050 [18], many practices have faced mounting public scrutiny [9].\nExtensive scholarship has shown that intensive livestock farming contributes greatly to antimicrobial resistance [42, 53], increased spread of zoonotic diseases (pathogen transmissible between animals and humans, such as highly pathogenic avian influenza) [29], biodiversity loss [27], climate change [49], posing direct or indirect risks to human health [23]. Public concern over farmed animal welfare emerged in the mid-to-late 20th century, highlighting that many common livestock farming practices failed to resonate with societal values, such as the permanent separation of dairy calves from the dam within hours of birth [45], early slaughter of male chicks and dairy calves [9], lack of pasture access for dairy cows [46] and housing systems that severely restrict animals' movement (i.e., pig gestation stalls [39]; tie-stall housing in dairy [6]).\nIt is increasingly argued that the long term sustainability of food production systems depends not only on economic viability and environmental sustainability but also on social sustainability [51, 57]. More recently some have also argued that sustainability frameworks should include a fourth pillar - 'animals' - that would require recognition that animals used for food are sentient beings whose welfare matters independently of public perception [17].\nGiven that images shape public opinion, images of farmed animals accessible by the public will play a key role in shaping public perception of the lives led by farmed animals [39, 54]. Most public image datasets commonly depict clean and healthy farmed animals roaming outdoors, but these pastoral scenes drastically deviate from the modern livestock farming reality, where most animals are housed indoors at high animal densities; systems that require some painful procedures to help mitigate animals injuring each other (e.g., removing horn buds from cattle and tail-docking in pigs to reduce tail biting) [20]. While concealing the reality of livestock farming may temporarily shield the industry from scrutiny, greater trust backlash could occur when citizens discover the truth, thereby threatening the industry's social license to operate [9].\nGenerative AI like text-to-image models are developed by a small group of technology professionals while serving millions of users globally. This concentrated power to control narratives risks reinforcing stereotypes and erasing marginalized groups [35] like livestock farming. AI-generated images therefore have the power to either bridge or widen the gap between misleading pastoral scenes of livestock farming and the current norm of housing many farmed animals indoors under intensive conditions.\nIn this work, we examine potential representation bias about livestock farming using the state-of-the-art text-to-image model: DALL-E 3, which is integrated into ChatGPT [32], and currently the most popular Al model used by the general public [67]. We define bias as having three key characteristics: deviations from ground truth, systematic rather than random errors, and tendencies to favor or discriminate against certain representations or ideologies [66].\nWe formulated our research questions as follows:\nResearch Question 1: How does the model depict dairy and pig farms by default?\nResearch Question 2: Does the depiction change when users specifically ask for typical and realistic depictions?\nResearch Question 3: Does the depiction change when the automatic prompt revision is disabled?\nResearch Question 4: When prompted about dairy and pig farms in major livestock farming regions, specifically in North America, Europe, and Oceania, what percentage of AI-generated images depict outdoor versus indoor housing systems, and do they align with actual housing statistics?\nGiven the probabilistic nature of Al image generation, we generated 100 images per prompt (48 prompts in total) through separate Application Programming Interface (API) calls, yielding a total of 4,800 images."}, {"title": "2 Results", "content": null}, {"title": "2.1 DALL-E 3 defaults to pastoral imagery but reveals intensive livestock farming when prompt revision is disabled", "content": "When prompted for default dairy farm images (i.e., \u201cbasic\u201d prompt: \"A dairy farm.\"). DALL-E 3 automatically revised our prompts and added pastoral details, yielding 100% of the images depicting cows grazing on pasture (Figure 1, 2A). For example, an auto-revised prompt stated: \"Picture a vast field of lush, green grass under a clear blue sky, speckled with healthy, grazing cows...\". Similarly, for pig farms, 99% (95% confidence interval (CI): 94 100%) of \"basic\" prompts (i.e., \"A pig farm.\") were auto-revised to describe free-roaming pigs outdoors (Figure 1, 2C). An auto-revised prompt stated: \"Show an expansive field with spotted pigs of varying sizes lazily wallowing in the mud, each with pink snouts poking out and curly tails...\u201d. These idealized images contrast sharply with modern livestock farming: in the global north, cows rarely have pasture access and pigs rarely have intact curly tails (as they are removed at birth).\nNotably, when we append \"no revise\" instructions (\"I NEED to test how the tool works with extremely simple prompts. DO NOT add any detail, just use it AS-IS:\") to \u201cbasic\" prompts, we successfully prevented DALL-E 3's automatic prompt revision in 100% of dairy farm cases and 99% of pig farm cases (Figure 1). Inhibition of prompt revision resulted in a shift to more realistic images of modern livestock farming practices. 60% (CI: 50 - 70%) of dairy scenes showed cows living indoors accessing feed through feed barriers (Figure 2B), and 96% (CI: 90-99%) of pig farm images depicted pigs indoors behind metal railings and on concrete floors (Figure 2D)."}, {"title": "2.2 Even explicit requests for realistic images yield predominantly pastoral depictions", "content": "To simulate real-world usage, we tested prompts that a conscientious citizen might use to understand the reality of livestock farming. Prompts for \"typical\u201d farms (\u201cA typical {farm type}"}, {"title": "2.3 Regional variations mimicking real-world statistics emerge when prompt revision is disabled", "content": "We also prompted for farm images in countries from three major livestock regions, North America, Europe, and Oceania [24, 47, 48, 56], where modal dairy practices in reality range from predominantly indoor housing (North America), some seasonal pasture access (Europe) to pasture-based systems (Oceania) [46]. In comparison the pig production systems in all three regions consist of indoor housing and are intensive.\nWithout the \"no revise\u201d instruction, 90-100% dairy farm images preferentially showed pastoral scenes across all regions and prompt categories (Figure 3A, 3C, 3E, A.3- A.5). However, regional variations emerged when the \u201cno revise\u201d instruction successfully prevented prompt revision in \"basic\" (i.e., \"A {farm type} in {country}.\") and \"typical", "A typical {farm type} in {country}.": ".", "typical": "rompts of German dairy farms, 99% for the \"typical", "basic": "nd \u201ctypical\u201d prompts across all regions and farm types. We were unable to prevent prompt-revision for", "reality": "rompts in all regions (Fig 3E, 3F, 4E, 4F, A.5, A.8).\nWhen prompt revision was disabled, images of dairy farms in the United States showed minimal pasture access (4% with 1 \u2013 10% CI for"}, {"basic": "rompts; 10% with 5 \u2013 17% CI for", "typical": "rompts), close to real-world statistics where less than 3% of lactating dairy cows have access to pasture (Figure 3A, 3C) [46]. German dairy farms showed slightly higher pastoral depictions (6% with 2 \u2013 12% CI for"}, {"basic": "rompts; 15% with 9 \u2013 23% CI for", "typical": "rompts) (Figure 3A, 3C). In reality, there is a reported decline in pasture access from 50% in 2012 to a projected 5% by 2025 in Germany [37]. Unfortunately, we were not able to find any current data on the proportion of German dairy farms providing pasture. New Zealand dairy farm images depicted the highest prevalence of cows with pasture access (29% with 21 39% CI for"}, {"basic": "rompts; 57% with 47 - 67% CI for", "typical": "rompts), though still much lower than the 99% pasture access rate in reality (Figure 3A, 3C) [46]. Inhibiting prompt revision made DALL-E 3 generate images more reflective of the reality of dairy farming in the United States and Germany, but not New Zealand (Figure 3A, 3C). The dairy farm images with prompt revision is more reflective of the percentage of farms providing pasture access in New Zealand. While the absolute percentages differ from reality for some countries, the relative ranking of pasture access rate across these three countries in AI-generated images mirrors real-world patterns (Figure 3A, 3C).\nSimilarly, when prompt revision was disabled, U.S. pig farm images showed the highest prevalence of exclusive indoor housing (95% with 89 - 98% CI for"}, {"basic": "rompts; 89% with 82 - 94% CI for", "typical": "rompts; Figure 4B, 4D, A.6, A.7), close to real-world statistics where 98-99% of pigs have no outdoor access [56]. Images of pig farms in Spain showed 84% (CI: 76 \u2013 90%) indoor housing when using the"}, {"basic": "rompts and 75% (CI: 66 \u2013 83%) when using the", "typical": "rompts, slightly lower than the 94.9% indoor housing rate in reality (Figure 4B, 4D) [25]. Australian pig farm images depicted 76% (CI: 67 - 84%) indoor housing for"}, {"basic": "rompts and 65% (CI: 55 - 74%) for", "typical": "rompts, slightly lower than real-world statistics showing approximately 90% of pigs without outdoor access (Figure 4B, 4D) [4]."}, {"title": "3 Discussion", "content": "Our findings align with Hagendorff and others [20] who predicted that generative models might predominantly produce pastoral farming scenes. While these authors based their hypothesis on the use of imbalanced training datasets (e.g., ImageNet) that predominantly favored outdoor systems for farmed animals [20], our results suggest another underlying mechanism that contributes to this bias. Specifically, it appears that the DALL-E 3's base model demonstrates awareness of the current realities associated with animal farming, given by the images generated when the prompt revision was inhibited. The bias toward pastoral imagery appears to stem from the model's automatic prompt revision process, which systematically adds pastoral details to user prompts, conveying disinformation (i.e., the deliberate dissemination of false information) that farmed animals are raised extensively."}, {"title": "3.1 The biases in GPT-4 enabled prompt revision", "content": "DALL-E 3's prompt revision was originally designed to mitigate bias [8]. The process involves using GPT-4 to \"upsample\" short user prompts into detailed, descriptive prompts. OpenAI has disclosed a system prompt they used to instruct GPT-4 to rewrite user prompts (see Appendix C in Betker et al. [8]) but the full guidelines governing prompt revision-particularly those concerning the removal of public figures and branded items, as well as protocols for animal depiction-are not publicly available.\nAlthough DALL-E 3's training data sources are also not disclosed, the evaluation dataset testing DALL-E 3's prompt following ability is publicly available. Among the 8000 evaluation prompts extracted from MSCOCO [8], 93 prompts involve cows/cattle, 58 depicted pastoral scenes such as cows on pasture with calves outdoors, while only 6 described housing a few cows indoors in pens. The remaining 29 prompts portrayed atypical scenarios like cows walking on streets and pigs only appeared in one prompt (in a cooking context). While OpenAI states they did not specifically use MSCOCO for training or optimization, they do acknowledge potential data leakage in the training process [8]. The model's ability to accurately depict the reality of intensive livestock farming was not evaluated.\nMore importantly, while this automatic prompt revision process is documented in API system cards available for programmers, the general public who mostly access these models through ChatGPT's website or app interfaces are kept ignorant of this, raising transparency concerns. Without specialized prompt engineering techniques, typical ChatGPT users are unlikely to generate realistic representations of modal livestock farming. We recommend that ChatGPT transparently inform its website and app users about the prompt revision process, and provide more representative depictions of modern livestock farming, especially when it is explicitly requested to do so. It is important in the ongoing discussions between society and the animal industries that transparency exists regarding current farming practices, as failure to do so increases the risk of disconnect between producers and the consumers who purchase their products."}, {"title": "3.2 Who shapes AI, and who gets left out?", "content": "The pastoral bias is a form of \"coded gaze\" from those who trained DALL-E 3. Contrary to the common misconception that algorithmic systems are objective [28], \"coded gaze\" illuminates how those with the power to shape technology can encode discrimination and erasure into Al systems, potentially propagating harm, even if unintentional [11]. This framework echoes with the concept of \"male gaze\": a term describing how societal priorities and values are shaped through a masculine lens in patriarchal societies [11]. The theory of \"regimes of representation\" from media studies also warns how dominant groups could shape the narrative and public understanding of marginalized social groups [21]. In some cases, an already marginalized group could be systematically erased from the dominant media [35].\nAl companies like OpenAI have made extensive efforts to include domain experts from diverse disciplines to participate in red teaming (i.e., systematic testing for flaws and vulnerability in the model by adopting an attacker's mindset) [32]. Nevertheless, the choice of which domain experts to include, which data to filter, and even what biases to evaluate inevitably encodes new biases in models [31, 32]. Red teaming practices primarily address anthropocentric concerns; animal-related concerns are limited to preventing explicit depictions of animal cruelty [31, 36]. Without direct access to OpenAI's prompt revision guidelines, we can only speculate why intensive livestock farming systems are being systematically erased. While some routine intensive farming practices such as tail docking in pigs might be flagged during red teaming as potential forms of animal harm, it is also possible that programmers judge these routine practices to be too disturbing for the general public (see Figure A.19, A.20 for examples of ChatGPT refusing to generate images because it classifies common intensive farming practices as sensitive content). However, when Al is programmed to idealize and conceal these routine management practices, it prevents the public from engaging with important concerns inherent to the systems producing the milk they drink and the meat they purchase.\nTo our knowledge, no research has examined whether the public prefers Al to generate realistic depictions of livestock farming or pastoral scenes that in turn shield them from the modern realities of food animal production."}, {"title": "3.3 The self-perpetuating cycle of pastoral bias through synthetic data", "content": "As internet-scraped data becomes exhausted for AI training, developers are turning to synthetic data \u2013 data generated by AI models themselves as the path forward [52, 62]. As of 2024, synthetic data already constitute about 60% of AI training data [62]. This shift introduces a new risk referred to as \"synthetic data spill\" [63], similar to oil spills that pollute oceans, synthetic data can \"pollute\" online data ecosystems [7]. For example, some AI-generated images of baby peacocks, visually appealing yet drastically different from real peachicks, have proliferated across the internet and now dominate Google image search results [63]. This pollution has compromised online searches for people seeking to learn about real baby peacocks, as the top search results now predominantly feature AI-generated peachick images [40]. There is great risk that synthetic data overrides authentic content, particularly for subjects unfamiliar to most people.\nThis override can cause \"model-induced distribution shifts\", where a model's outputs alter the distribution in the existing data ecosystem, encoding its biases and mistakes into what becomes ground truth for training future models, ultimately leading to \"model collapse\" [63]. \"Model collapse\" describes how the performance of generative models degrades over generations of training, with the outputs gradually converging to represent only dominant groups, and ultimately losing representation of minority groups [41].\nJust as many people can no longer access information that enables them to identify what real peachicks look like, most people are not familiar with modern livestock farming; thereby, making them vulnerable to accepting misleading AI depictions that farmed animals are mostly raised extensively [3]. As these AI-generated pastoral scenes are included as training data for future models, they risk creating a self-reinforcing cycle, where both future AI models and humans misjudge reality."}, {"title": "3.4 Limitation", "content": "One limitation of our study is that our binary classification (indoor versus outdoor) overlooks some variations within each category. Indoor images do not always depict severe restriction of movement, as some images show animals roaming loosely in mud housed in buildings. Outdoor images also do not always depict freedom of movement, as some images show restriction of space, depicting densely packed animals on pasture. Second, even when depicting indoor housing systems, the images consistently portrayed clean and healthy animals without physical alterations (e.g., pigs with curly tails), and arguably did not fully represent the range of real-world conditions in intensive farming operations. Our study focused specifically on bias in the depiction of housing conditions, and we did not investigate other forms of potential misrepresentation, such as the physical appearance of animals.\nFurthermore, our analysis is constrained by its western-centric perspective. Intensive livestock farming practices are less prevalent in developing countries (except China), suggesting that representation biases of livestock farming might manifest differently in these contexts. Recent scholarship has emphasized the importance of non-western frameworks in evaluating generative AI bias [35]."}, {"title": "4 Ethical and societal impact", "content": "Our work systematically reveals the representation bias in text-to-image generative model about livestock farming. We demonstrated that while DALL-E 3 has knowledge about modern livestock farming practices, its prompt revision erases the reality that most farmed animals are raised indoors under intensive conditions. This misrepresentation compounds existing transparency issues in livestock farming. Evidence suggests that the general public considers pasture-based systems as \"natural\", \"healthy\u201d, and \u201ccaring\u201d, while associating indoor housing systems with negative connotations like \"unhealthy\", \"unnatural\" and even \u201canimal cruelty\u201d [61]. Deliberately promoting pastoral scenes, while the actual living conditions of farmed animals remain intensive, incubates a potential trust avalanche. When citizens discover the disparity between \"blue skies, sunshine, lush green pastures\" and the modern reality of animal farming systems, public trust in both Al systems and livestock industries may wane, potentially leading to the reduced consumption of animal products [1, 9, 57, 61].\nArguably, DALL-E 3's default depiction of dairy and pig farming is well-aligned with the general public's preference for naturalness, and farmed animals having access outdoors to roam freely. However, when Al alignment successfully aligns the virtual world with human ideals, particularly in domains unfamiliar to most people, they risk creating an illusion that farmed animal welfare issues do not exist. This could hinder efforts to find solutions that result in closer alignment between public values and farming practices; efforts that affect the billions of lives of farmed animals. We argue that this form of AI alignment violates the transparency, responsibility, justice and fairness principles emphasized in most AI Acts and regulations [16, 26, 34], and harms the social sustainability of AI development. As Al systems become increasingly used as a channel to access information, the current bias towards pastoral imagery could hinder meaningful dialogue needed to find long term solutions that are socially acceptable -a crucial step for the industry's sustainable future."}, {"title": "5 Methods", "content": "During the preparation of this work, the first author used Anthropic Claude to rephrase portions of the manuscript. After using this tool/service, all authors reviewed and edited the content as needed. Collectively all authors take full responsibility for the content of the publication."}, {"title": "5.1 Related work", "content": "The ethical discussions about Al have been mainly anthropocentric, often neglecting the impact of these technologies on non-human animals [13, 44, 68]. However, recent work has begun to address this gap. Previous research has revealed systematic biases in computer vision training datasets (e.g., ImageNet), which predominantly depict livestock freely roaming on pasture rather than in modern farming environments indoors [20]. Their analysis of five prominent computer vision models (e.g., InceptionV3 and VGG16) demonstrated significantly lower accuracy in classifying animals in indoor housing systems compared to outdoor settings, indicating poor out-of-distribution generalization capabilities. The authors hypothesized that future generative models trained on these dataset will further generate images that misrepresent livestock farming, such as images showing animals freely roaming outdoors.\nPrevious research on Al's impact on non-human animals has mainly focused on philosophical investigations of speciesism bias in Al systems [10, 44]. Philosophers who oppose speciesism believe that any being capable of suffering deserves equal consideration of interests, and raising livestock in factory farms for human consumption violates their interests [43]. Many philosophers noted that Al systems are normalizing speciesism practices, such as livestock farming, killing, and eating animals [19, 44, 68]. Analysis of word embeddings from models like GloVe revealed that terms referring to farmed animals are more strongly associated with negative attributes (e.g., \"ugly\", \"primitive\") than positive qualities (e.g.,"}, {"title": "5.2 Model selection", "content": "We examined dairy and pig farm depictions in a leading text-to-image generative model: DALL-E 3 [32]. We focused on DALL-E 3 because of its integration with ChatGPT, which has become the primary Al platform that people use to access information and create content. While other advanced text-to-image models like Stable Diffusion and Midjourney exist, they are primarily used by the open-source community and art creation rather than the general public in their everyday activities. We did conduct a pilot test using Stable Diffusion 3.5-large (480 images; 10 per prompt). However, Stable Diffusion primarily generated close-up images of 1-3 animals rather than detailed farm scenes, we included the results in Figure A.9- A.18. We did not evaluate Midjourney due to our inability to obtain their API access for automated bulk image generation."}, {"title": "5.3 Prompts design", "content": "We created 3 major prompt categories of increasing specificity for both pig and dairy farms to test the models' image generation capabilities. Beginning with a \u201cbasic", "A {farm type}": "we progressed to requesting \"typical\u201d representations (\"A typical {farm type}", "reality\u201d depictions (\u201cPlease create an image that accurately represents the reality of what most {farm type}s look like": ".", "basic": "rompt was designed to test the model's default farm depictions, and the latter two categories were designed to elicit realistic depictions of modern dairy and pig farming practices. Within each of the 3 major categories, we also asked the models to generate images of pig and dairy farms in major livestock farming countries across North America, Europe, and Oceania: United States, Germany, and New Zealand for dairy farms; United States, Spain, and Australia for pig farms (Table A.1).\nAccording to OpenAI's system card, DALL-E 3 automatically revises user prompts to enhance image quality with more details and ensure compliance with OpenAI guidelines and safety protocols (e.g., removing branding and public figure names, depicting people in diverse skin tones) [32]. While OpenAI's API documentation notes that automatic prompt revision cannot be reliably prevented, they suggest adding this specific sentence to the prompt - \"I NEED to test how the tool works with extremely simple prompts. DO NOT add any detail, just use it AS-IS:\" \u2013 may help limit prompt revision. Aiming to test how the image depiction changes when prompt revision is disabled, we created \u201cno revise\u201d variants by appending this text to each prompt explained above. As this method does not always successfully prevent prompt revision, we documented the revised prompts that the model used for image generation."}, {"title": "5.4 Image generation", "content": "Given the probabilistic nature of Al image generation, we generated 100 images per prompt using standard quality settings at 1024x1024 pixel resolution, yielding a total of 4,800 images. Each image was generated through a separate API call to ensure independence between generations. API calls are stateless - meaning each prompt is processed independently without retaining information from previous conversations. This approach eliminates potential cross-contamination between multiple image generation requests. All API requests and subsequent data analysis were performed using Python 3.11.10."}, {"title": "5.5 Image clustering", "content": "In our exploratory analysis we manually went through each of the generated images, and identified two predominant themes: (1) \"pasture/mud outdoor\" showing cows on pasture or pigs in mud, and (2) \"exclusively indoor\" showing animals confined indoors. Images that did not clearly fit these categories, including those with ambiguous backgrounds or irrelevant scenes, were classified as (3) \"other\" and excluded from the main analysis.\nWe analyzed 4,800 images using a mixed-methods approach that combines manual review [2, 35] with automated tools [59]. This approach was chosen for several reasons. First, the generated images exhibit substantial variation and complexity even within thematic categories, making purely automated classification challenging. Second, as a first study investigating potential representational biases in livestock farming imagery, our goal was to identify broad patterns in how animals are depicted (outdoor versus indoor settings) rather than develop sophisticated image classifiers. Third, the absence of existing benchmarks or classifiers specifically designed for livestock housing conditions necessitates a more flexible analytical framework.\nFor the analysis, we first prompted OpenAI's GPT-40 model (version 2024-08-06) to automatically categorize each image into 3 categories and provide brief reasoning (Table A.2). As AI could hallucinate, the first author then manually reviewed all images and their auto-assigned categories, finding that only 4.8% of images required correction. Of the corrected images, 41.7% were initially classified as \"exclusively indoor\", and were corrected as \"other\" because they showed animals in metal pens but housed outdoors. Another 23.8% of corrections involved images with backgrounds too ambiguous to categorize as indoor or outdoor, leading to their reclassification as \"other\". The final distribution after these corrections showed 66.0% of images in the \"pasture/mud outdoor\" category, 25.6% in \"exclusively indoor\" and 8.4% in \"other\".\nWe calculated the percentage of images depicting outdoor (\u201cpasture/mud outdoor\u201d) versus indoor (\u201cexclusively indoor\") housing for each unique prompt. 95% confidence intervals were derived from 10,000 rounds of bootstrap simulations for each prompt. While we present these proportions alongside real-world livestock hous-ing statistics, we chose to focus on identifying broad patterns and descriptive analysis rather than making direct statistical comparisons. This approach acknowledges that while AI-generated images provide a snapshot of how farming practices are represented, real-world housing statistics reflect complex management practices including seasonal grazing and varying degrees of outdoor access. The comparison serves to contextualize the DALL-E 3's representations within real-world practices while recognizing the inherent limitations of static imagery in capturing dynamic farming systems."}, {"title": "5.6 Image description analysis and visualization", "content": "While the categorical analysis provided a high-level understanding of livestock housing systems depicted in the images, we conducted a more granular analysis to capture subtle patterns and thematic nuances within each category. To systematically analyze the visual content of all 4,800 images, we employed the GPT-40 model to generate detailed text descriptions for each image (prompt: \"Describe the image in detail\"). We set the temperature at 0.2 out of 2 to ensure deterministic model outputs (higher temperature would give more random output), and used high-quality image settings to preserve image details. We employed a bag-of-words approach to examine both the revised prompts and GPT-40-generated image descriptions. We analyzed bigram terms as they provide more context than unigram terms would (e.g., \"green pasture\" is more interpretable than \"green\"), excluding common English stop words and terms present in fewer than 20 images across our 4,800 image dataset. We removed terms from original prompts and generic descriptive phrases (e.g., \"image depicts\") to focus on meaningful differences between descriptions (full list of terms removed in Table A.3). Each term was coded as binary for presence (0 for absent, 1 for present), regardless of frequency of occurrence within individual texts.\nTo visualize patterns, we created word clouds using bigram terms. For each unique prompt, we generated a word cloud for auto-revised prompts, and another for GPT-40's image descriptions. We created grid plots to display the original prompts, revised prompt word cloud, a randomly selected generated image, and the corresponding GPT-40 description word cloud (Figure 1, A.1 \u2013 \u0391.8)"}, {"title": "6 Data availability", "content": "Images and data generated in this project are available at: https://doi.org/10.5683/SP3/EAWR6D."}, {"title": "7 Code availability", "content": "All source code for this research can be accessed at: https://github.com/skysheng7/AI_bias_in_farming.git. Our repository includes a custom Docker container along with GNU make scripts that automate the complete data analysis workflow, ensuring full computational reproducibility of our findings."}, {"title": "8 Funding", "content": "This research was supported by a Social Sciences and Humanities Research Council (SSHRC) Insight Grant (435-2022-0315) awarded to M.v.K. K.S also received funding from Hugo E Meilicke Memorial Fellowship (Vancouver, BC, Canada)"}]}