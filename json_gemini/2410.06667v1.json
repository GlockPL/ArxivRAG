{"title": "Large Language Models as Code Executors: An Exploratory Study", "authors": ["Chenyang Lyu", "Lecheng Yan", "Rui Xing", "Wenxi Li", "Younes Samih", "Tianbo Ji", "Longyue Wang"], "abstract": "The capabilities of Large Language Models (LLMs) have significantly evolved, ex-tending from natural language processing to complex tasks like code understanding\ngeneration. We extend the scope of LLMs' capability to a broader context - where\nwe use LLMs to execute code snippets to get the output. This paper pioneers the\nexploration of LLMs as code executors, where code snippets are directly fed to the\nmodels for execution, and outputs are returned. We are the first to comprehensively\nexamine this feasibility across various LLMs, including OpenAI's o1, GPT-4o,\nGPT-3.5, DeepSeek, and Qwen-Coder. Notably, the o1 model achieved over 90%\naccuracy in code execution, while others demonstrated lower accuracy levels. Fur-\nthermore, we introduce an Iterative Instruction Prompting (IIP) technique that\nprocesses code snippets line by line, enhancing the accuracy of weaker models\nby 7.22% in average (highest 18.96%) and an absolute average improvement of\n3.86% against CoT prompting (highest 19.46%). Our study not only highlights the\ntransformative potential of LLMs in coding but also lays the groundwork for future\nadvancements in automated programming and completing complex tasks.", "sections": [{"title": "1 Introduction", "content": "The rapid advancement of Large Language Models (LLMs) [Brown et al., 2020, OpenAI, 2023] has\nmade a transformation of capabilities across diverse domains, ranging from language translation\nto creative writing [Wang et al., 2023a, Bang et al., 2023, Bai et al., 2023]. These models, with\ntheir remarkable ability to understand and generate human-like text, have found applications that\nextend well beyond traditional natural language processing tasks such as code understanding and\ngeneration [Chen et al., 2021, Gao et al., 2023, Zhuo et al., 2024]. In the area of programming, LLMs\nhave been predominantly utilized for code generation, aiding developers by suggesting code snippets\nor completing partially written scripts [Wang et al., 2024]. This utility has significantly enhanced\nproductivity and coding efficiency by providing real-time assistance and reducing the cognitive load\non developers [Jiang et al., 2024].\nDespite these advancements, the exploration of LLMs as code executors remains a less explored area.\nThe ability to not only generate but also execute code opens up a plethora of possibilities, including\nautomated debugging, real-time code validation, and the development of intelligent programming\nassistants. More importantly, this links to a broader context and higher-level gold of using LLMs\nto execute and complete complex actions and plans and even for causality understanding in real\nworld [Wang et al., 2023b, Kambhampati et al., 2024]. This paper is pioneering in its approach, as\nit is the first to systematically examine the feasibility of employing LLMs to execute code directly,"}, {"title": "2 LLMs as Code Executors", "content": "In this section, we outline the methodology employed to investigate the capabilities of LLMs as code\nexecutors. Our approach is designed to systematically evaluate how effectively LLMs can execute\ncode snippets and return accurate outputs, an interesting application that extends their use beyond\ntraditional code generation tasks. This involves the collection of diverse code snippets and the careful\ndesign of prompts that guide the models in executing code. By doing so, we aim to uncover insights\ninto the operational dynamics of LLMs when tasked with direct code execution and to identify\nstrategies that enhance their performance. The following subsections detail the processes of code\nsnippet collection and prompt design, which form the foundation of our experimental framework."}, {"title": "2.1 Code Snippets Collection", "content": "We collected code snippets from Leetcode 4, including 100 examples in both Chinese and English\nrespectively. The platform provides data such as problem description, test cases, standard solutions\nand problem types corresponding to each problem, and we collect the matching data manually and\nthen analyze it, and finally, the format of each of our metadata is as follows:\n1. Problem Descriptions. For each code snippet, we provide a detailed description of the\nproblem it aims to solve.\n2. Input-Output Examples. Each question is accompanied by the corresponding input data and\nthe corresponding expected output, which is used as the LLMs evaluation data\n3. Standard solution. We include a standard solution for each problem, describing in detail\nthe idea of solving each problem in order to provide a cross-reference to the collected code\nsnippets."}, {"title": "2.2 Prompting Designation", "content": "As illustrated in Figure 2, three approaches are compared: Vanilla Prompting (VP), Chain-of-\nThought (CoT) prompting, and our proposed Iterative Instruction Prompting (IIP). VP serves as a\nbasic interaction model, providing general assistance without specific guidance. In contrast, CoT\nprompting facilitates a more detailed analysis by encouraging the model to consider the role of\neach line of code. Finally, IIP makes LLMs to receive code snippets line by line and builds upon\nprevious outputs, allowing the model to refine its predictions based on earlier results. This comparison\nhighlights the progressive enhancement of LLM capabilities in understanding and executing complex\ncode tasks. Below is the vanilla prompting we employed without sophisticated design:"}, {"title": "3 Evaluation Results", "content": "In this section, we present the evaluation results of various LLMs on the code snippets we collected."}, {"title": "3.1 Evaluated LLMs", "content": "GPT-3.5: GPT-3.5 [Ouyang et al., 2022] improves on GPT-3 [Brown et al., 2020] with\nenhanced language capabilities, supporting zero-shot and few-shot learning. The GPT-3.5\nturbo variant balances cost, latency, and quality. The model name used is gpt-3.5-turbo-0125.\nGPT-4: GPT-4 [OpenAI, 2023] is known for its advanced language understanding and\ngeneration capabilities, which is a further improved version of GPT-3.5. In our experiments,\nwe test gpt-4-turbo-2024-04-09."}, {"title": "3.2 Experimental Setup", "content": "Based on the data we collected above, we extract the python code with test cases for each metadata,\nembed it in the set prompt, set each test case to be asked twice as LLMs input and record the\ncorresponding output; for our IIP approach, we take the last LLMs replies and embed it in the next\nprompt."}, {"title": "3.3 Results", "content": "Main Results The experimental evaluation was conducted to assess the performance of various\nLLMs in executing code snippets, sourced both from CN (Chinese) and EN (English) contexts. The\nLLMs tested include GPT-3.5, GPT-4, GPT-4o, GPT-4o-mini, o1-Preview, Qwen-Coder, Qwen-72B,\nand DeepSeek-Coder. Each model was tasked with executing code snippets embedded with comments\nin their respective languages, providing a comprehensive overview of their capabilities across different\nlinguistic and syntactic environments.\nThe results, as presented in Table 2, highlight several key insights. Notably, the latest OpenAI\no1-Preview model consistently outperformed the others, achieving an accuracy of 93.5% for CN and\n96.1% for EN, suggesting highly excellent ability for this code execution task and a robust capability\nto handle code execution across diverse linguistic inputs. This indicates the ol's superior ability to\nprocess and understand the nuances of code comments and structure, further confirming the capability\nof solving complex tasks of o1 model. In contrast, models such as GPt-3.5, Qwen-Coder and Qwen-\n72B demonstrated lower performance, with accuracy around 20% to 60%, which substantially lag\nbehind ol's performance. This suggests that these models may lack sufficient training or optimization\nfor code execution tasks, particularly in handling complex code structures or understanding context\nfrom comments.\nAnother interesting observation is the performance disparity between CN and EN across models.\nWhile most models showed better performance with English code snippets, the margin varied,\nindicating potential biases or limitations in handling code semantics when embedded in non-English\ncontexts. This highlights the need for further refinement in multilingual code execution capabilities.\nEffect of prompt type This experiment examines the impact of different prompting strategies on the\nperformance of various LLMs in executing code snippets. The study evaluates three types of prompts:\nvanilla, CoT [Wei et al., 2022, Kojima et al., 2022], and Iterative Instruction Prompting (IIP), across\nboth EN and CN datasets. The LLMs we used include GPT-3.5, Qwen-2.5-Coder, Qwen-2.5-72B,\nand Deepseek-Coder."}, {"title": "Relationship between model accuracy and human pass rate", "content": "This section analyses the relation-ship between the average accuracy of all evaluated LLMs in Table 2 and corresponding human pass\nrates of each coding question. The fit results are shown in Figure 4. Both EN and CN datasets show a\npositive correlation, indicating that tasks easier for humans generally yield higher model accuracy.\nThe fit lines in the plots suggest that LLMs are more adept at solving problems with higher human\npass rates, likely due to shared cognitive processes or data patterns. Notably, the EN dataset displays\na steeper correlation, possibly due to more extensive training data or linguistic characteristics favoring\nEnglish comprehension for the comments in code snippets."}, {"title": "Effect of Computational Complexity", "content": "This section evaluates LLM performance on code snippets\nby analyzing average accuracy relative to computational complexity across CN and EN datasets.\nThe complexities considered include $O(n)$, $O(n \\log n)$, $O(n^2)$, and others as shown in Figure 5,\nproviding insights into model capabilities across varying algorithmic difficulties.\nEffect of lines of code snippets We also the relationship between LLM accuracy and code snippet\nlength in CN and EN datasets. As shown in Figure 6, quadratic regression analysis reveals a negative\ncorrelation between line count and accuracy for both datasets, indicating that longer code snippets\ngenerally reduce model performance. The CN dataset shows a steeper decline, suggesting greater\nchallenges in handling complexity compared to the EN dataset, where the impact is less pronounced."}, {"title": "3.4 Case Analysis", "content": "We further analyze the performance of each model in our dataset. For example, question 91 of the\nEnglish dataset: Booking Concert Tickets in Groups. As a complex search problem, its standard code\nanswer is more than one hundred lines. In the experiment, all LLM except OpenAI's o1 could not\ncorrectly handle the problem conditions and search and assign them, which proved ol's superior\nability in dealing with complex problems."}, {"title": "4 Conclusion", "content": "This study evaluates the performance of LLMs in executing code snippets, revealing key insights\nacross different prompt types, problem categories, and computational complexities. Iterative prompt-ing significantly enhances accuracy, particularly in CN datasets, emphasizing the value of detailed\nguidance. LLMs performs better in moderate complexity tasks but face challenges with dynamic\nprogramming and lengthy code snippets, especially in CN. The correlation between code length and\naccuracy suggests a need for improved handling of complex, extended tasks.\nFuture work will focus on extending evaluations to a broader range of coding problems beyond\nalgorithm-specific tasks and incorporating additional programming languages. This will further\nenhance our understanding of LLM capabilities and inform the development of more robust models."}]}