{"title": "Diagnosing Robotics Systems Issues with Large Language Models", "authors": ["Jordis Emilia Herrmann", "Aswath Mandakath Gopinath", "Mikael Norrlof", "Mark Niklas M\u00fcller"], "abstract": "Quickly resolving issues reported in industrial applications is crucial to minimize economic impact. However, the required data analysis makes diagnosing the underlying root causes a challenging and time-consuming task, even for experts. In contrast, large language models (LLMs) excel at analyzing large amounts of data. Indeed, prior work in AI-Ops demonstrates their effectiveness in analyzing IT systems. Here, we extend this work to the challenging and largely unexplored domain of robotics systems. To this end, we create SYSDIAGBENCH, a proprietary system diagnostics benchmark for robotics, containing over 2500 reported issues. We leverage SYSDIAGBENCH to investigate the performance of LLMs for root cause analysis, considering a range of model sizes and adaptation techniques. Our results show that QLORA finetuning can be sufficient to let a 7B-parameter model outperform GPT-4 in terms of diagnostic accuracy while being significantly more cost-effective. We validate our LLM-as-a-judge results with a human expert study and find that our best model achieves similar approval ratings as our reference labels.", "sections": [{"title": "1 Introduction", "content": "Identifying the root cause for issues with complex industrial systems is a time-critical task but challenging and time-consuming for human experts as they struggle with the required analysis of large amounts of log data which large language models (LLMs) excel at. Indeed, there is substantial work in AI-Ops exploring automated diagnostics for IT systems (D\u00edaz-de-Arcaya et al., 2024; Zhaoxue et al., 2021). However, the challenging domain of robotics systems remains largely unexplored.\nThis Work: Automated Diagnostics for Robotics Systems To address this challenge, we create SYSDIAGBENCH, a proprietary benchmark for diagnosing root causes of complex robotics systems failures, containing over 2 500 real-world issues. In particular, each instance corresponds to a support ticket, containing an issue description, a set of log files, communications with the support engineers, a reference root cause extracted from expert discussions, and the ultimate issue resolution. The goal in SYSDIAGBENCH is to predict the root cause underlying the reported issue, given only the information available at the creation of the ticket.\nLLM-Based Diagnostics We leverage SYSDIAGBENCH to investigate multiple LLM-based diagnostic approaches in the robotics setting, using both LLM-as-a-judge (Zheng et al., 2023) and human experts for evaluation. In particular, we consider a range of model sizes and adaptation techniques from zero-shot prompting to full finetuning, to assess their cost-performance trade-off. Interestingly, we observe that even QLORA (Dettmers et al., 2023) can be sufficient to let a 7B-parameter model outperform GPT-4 in terms of diagnostic accuracy while being significantly more cost-effective. Validating our results in an expert study, we find that LLM-as-a-judge scores correlate well with human expert ratings, with our reference labels matching the experts' analysis in over half the cases and our best model achieving similar approval ratings as these reference labels.\nKey Contributions Our key contributions are:\n\u2022 We create SYSDIAGBENCH, a proprietary benchmark for automated root cause analysis of robotics systems, based on thousands of real-world issues (Section 3).\n\u2022 We propose a range of LLM-based diagnostic tools (Section 4).\n\u2022 We leverage SYSDIAGBENCH to analyze these techniques and identify the most effective and efficient strategies (Section 5).\n\u2022 We validate the effectiveness of our approach using a human expert study (Section 6)."}, {"title": "2 Related Work", "content": "AI-Ops leverages machine learning (ML) in IT operations (D\u00edaz-de-Arcaya et al., 2024) to analyze large amounts of semi-structured data such as logs and traces (Zhaoxue et al., 2021) with the goal of discovering anomalies and their root causes. As many traditional ML methods require structured data, AI-Ops long focused on developing methods enhancing (Yuan et al., 2012; Zhao et al., 2017) and parsing (He et al., 2017; Messaoudi et al., 2018) log files, using well-established methods such as SVMs (Zhang and Sivasubramaniam, 2008; Zuo et al., 2020), simple clustering techniques (Zhao et al., 2019; Lou et al., 2010), and decision tree (ensembles) (Chen et al., 2004) for the actual analysis.\nLLM-based Approaches As LLMs can directly process the semi-structured log data, they have recently gained popularity in the field (Shao et al., 2022; Chen and Liao, 2022; Lee et al., 2023; Ott et al., 2021). As a representative example, Gupta et al. (2023) use an encoder architecture, pre-trained on a large amount of log data, to compute embeddings for further analysis. In contrast to these methods, we propose to directly predict root causes from log data."}, {"title": "3 SYSDIAGBENCH: A Benchmark for Robotics System Diagnostics", "content": "SYSDIAGBENCH is a proprietary system diagnostics benchmark focusing on root causes (RC) prediction for real-world robotics issues, constructed from a decade of industry data. Concretely, each SYSDIAGBENCH instance corresponds to a support ticket and contains a detailed problem description, a set of log files from the affected system, and a reference root cause description. Below, we first describe the information contained in a ticket and then the process of constructing SYSDIAGBENCH. Unfortunately, the underlying data cannot be published at this point due to privacy concerns.\nSupport Tickets A ticket is created when a reported issue cannot be resolved by the service support engineers and as a result needs to be escalated to the product development team. Every ticket contains metadata on the affected system (e.g. the robot and application type), a detailed problem description, and a system diagnostic file capturing the system state after the issue occurred. For SYSDIAGBENCH, we consider three log files contained in the system diagnostic that are commonly analyzed by experts when investigating a ticket."}, {"title": "3.1 Benchmark Construction", "content": "To create SYSDIAGBENCH, we collected over 12000 historic tickets and filtered out those that do not contain a system diagnostic with an elog, pspool, and startup file, leaving us with 2585, split into a training, validation, and test set corresponding to 75%, 5%, and 20%, respectively.\nRoot Cause Extraction To extract a concise root cause description from a historic ticket, we leverage a strong LLM (GPT-4) (illustrated in Figure 1). Concretely, we query the LLM with the problem description, expert discussion, support engineer communication, and final resolution using a chain-of-thought (CoT) prompt (Wei et al., 2022), instructing the model to carefully analyze all provided information before describing the root cause (see Appendix B.1 for more details). We highlight that the information used to create these labels is not available when a ticket is created and can thus not be used to predict the RC at inference time. Finally, we validate the quality of the extracted root causes in a human study in Section 6."}, {"title": "3.2 Evaluation Metrics", "content": "Evaluating root cause correctness is inherently challenging, as descriptions of the same, correct root cause can be highly diverse, making similarity measures such as the ROUGE score (Lin, 2004) unsuitable. Further, there is frequently a trade-off between specificity and correctness, i.e., generic descriptions can be correct yet unhelpful, while very precise ones may get minor details wrong while still being overall very helpful. We thus adopt an LLM-as-a-judge evaluation (Zheng et al., 2023) asking a model to judge the similarity between the predicted and the reference RC on a scale of 1 to 10 (see Appendix B.3 for details). We report the mean similarity score (MSS) with respect to the reference labels as our primary evaluation metric and validate it against human experts in Section 6."}, {"title": "4 LLM-Based Systems Diagnostic", "content": "In this section, we describe the system diagnostic approaches we evaluated on SYSDIAGBENCH."}, {"title": "4.1 Input Preprocessing and Prompting", "content": "For both training and inference, we preprocess all log files by removing timestamps, dates, and sequence numbers. We further remove all consecutive duplicate lines and filter the elog to only include error and warning but not information events, as these are most likely to be relevant for diagnosing the root cause. As both the print-spool and startup frequently contain tens of thousands of lines, we only retain the 10 lines before and after each error or warning event in the elog file. Finally, while the original elog file contains only integer error IDs, we map these to human-readable error descriptions using a lookup table. This preprocessing reduces the mean total token count of the log files per ticket from 68k to 16k tokens, with the distribution change illustrated in Figure 2.\nThe LLM input is now constructed by combining a detailed CoT instruction (Wei et al., 2022) with a context of the preprocessed log files and the issue description (see Appendix B.2 for more details). Despite our preprocessing, the resulting inputs frequently exceed 8k (68%) and even 32k (9%) tokens (see Figure 2), requiring models with large context capabilities for processing."}, {"title": "4.2 Training for System Diagnostics", "content": "While modern LLMs have impressive zero-shot capabilities (Kojima et al., 2022), adapting them to specific tasks (Zhao et al., 2024) can improve their performance significantly. However, the long input lengths make in-context learning, e.g., via few-shot prompting, unpractical for system diagnostics. We, thus, consider three adaptation techniques, full finetuning (FFT), LORA (Hu et al., 2022), and QLORA (Dettmers et al., 2023), with different performance-cost trade-offs (see Appendix A)."}, {"title": "5 Experimental Evaluation", "content": "Below, we discuss the experimental setup and key results. For more details on setup and extended results, please see Appendices C and D, respectively.\nExperimental Setup We consider MISTRAL-LITE-7B (Yin Song and Chen Wu and Eden Duthie, 2023), MIXTRAL-8X7B (Jiang et al., 2024), and GPT-4 (OpenAI, 2023), using Axolotl (2024) for finetuning on 2 to 8 NVIDIA A100s. We train for 3 epochs and unless indicated otherwise, use rank $r = 32$ for (Q)LORA and NFloat4 + DQ (double quantization) for QLORA.\nSystem Diagnostic Performance We compare the performance of different models and training methods in Table 1. Interestingly, we find that the smaller MISTRAL-LITE-7B outperforms MIXTRAL-8X7B across all adaptation settings. We hypothesize this is because it was specifically trained for long context capabilities, crucial for analyzing long log files. While GPT-4 is the best-performing base model, we find that our finetuned models outperform it by a significant margin, with QLORA training yielding the best performance.\nRank Deficiency as Regularization Observing that LORA and QLORA outperform full finetuning for MISTRAL-LITE-7B (see Table 1), we investigate the impact of the LORA rank $r$ on the model's performance, illustrating results in Figure 3. We find that while a smaller rank improves"}, {"title": "6 Human Study", "content": "We conduct a study with human experts to answer the following three questions: Q1: Are the automatically extracted reference labels accurate? Q2: Does our LLM-as-a-judge evaluation correlate well with human expert ratings? And Q3: Are our best models helpful in root cause analysis?\nStudy Setup We ask 10 experts to solve a subset of tickets, leading to $n = 53$ answer sets. We provide the experts with all the tools they typically use to resolve issues and the full historic ticket data. We then ask them to describe the issue's RC and judge their confidence. Next, we let them assess our reference label, in terms of helpfulness (yes, no, maybe), correctness (on a scale from 1 to 10), and preference compared to their own answer. Where available, we let them assess their colleague's RC, in the same way. Finally, we let them rate the RCs generated by our four best models on the same scale. See Appendix E for more details.\nQ1: Reference RC Quality Asked directly, whether our reference RC was correct, experts agreed (yes or maybe) in 49% of cases, saying it was as good as their own assessment in 55% of cases, but only as good as their colleague's in 29%. Interestingly, they still assigned a higher or equal score to our reference RC (6.0 on average) than to their colleagues' RC (7.9 average) in 43% of cases. Combined with experts only being highly (moderately) confident in their assessment 32% (58%) of the time, this suggests that diagnosing root causes is a particularly hard task, with our reference labels having high but not perfect quality.\nQ2: LLM-as-a-Judge Evaluation While overall MSS (LLM-as-a-judge) and mean expert ratings induce the same ranking, the MSS evaluated on"}, {"title": "7 Conclusion", "content": "We created SYSDIAGBENCH, a benchmark for robotics systems diagnostics, containing over 2 500 real-world issues. We leveraged SYSDIAGBENCH to investigate the performance of large language models (LLMs) for automated root cause analysis, considering a range of model sizes and adaptation techniques. Our results show that QLORA finetuning allows a 7B-parameter model to outperform GPT-4 in terms of diagnostic accuracy while being significantly more cost-effective. We validated our results with an expert study and found that while both our reference label extraction and LLM-as-a-judge evaluation cannot replace human experts, our best models can provide valuable insights."}, {"title": "8 Limitations", "content": "As no ground truth root cause annotations exist for (historic) tickets, we generated reference labels for SYSDIAGBENCH using a strong LLM to extract them from the rich data available for historic tickets. While the human expert study shows the generated labels to be as good as an expert's analysis in over half the cases, they are not perfect. In particular, they are only considered correct (yes or maybe) 49% of the time by the experts. Using multiple experts to annotate the same tickets could have improved the quality of the reference labels and thus both the performance of our finetuned models and the evaluation quality, but this was not feasible due to the significant effort required to annotate tickets and time constraints of available experts.\nFurther, even for correct reference labels, the LLM-as-a-judge evaluation is not perfect. While we achieve a high correlation of $\\rho = 0.57$ between the MSS and mean expert ratings when only considering samples with correct (yes or maybe) reference labels, the per-sample correlation remains at a moderate $\\rho = 0.20$. However, even the inter-expert correlation of $\\rho = 0.77$ for mean scores and $\\rho = 0.32$ for per-sample correlation, remains far from perfect, highlighting again the difficulty of accurately assessing root causes. We conclude that while the LLM-as-a-judge evaluation is a valuable tool for comparing different models, it cannot substitute expert human judgment, especially for sample-level comparison. We note that for reliable sample-level comparison, a panel of experts would be needed.\nFinally, we only consider two base models and a limited number of hyperparameters for our experiments due to both budget and time constraints. While we find for MISTRAL-LITE-7B that LORA and QLORA finetuning perform exceptionally well, even outperforming full finetuning, this was not the case for MIXTRAL-8X7B. While we hypothesize that this is due to MISTRAL-LITE-7B's training specifically for long context retrieval tasks, a detailed study of this effect is out of scope here and left for future work."}, {"title": "9 Ethical Considerations and Broader Impact", "content": "Ethical Considerations The dataset underlying SYSDIAGBENCH contains real-world support tickets from a robotics company, which may contain sensitive information about the company's products and customers, thus precluding its public release. To mitigate the risk of privacy breaches during internal use, we have anonymized all tickets by removing all personally identifying information fields.\nFor our human expert study, we have recruited internal experts from the robotics company's product team, who regularly handle the support tickets constituting SYSDIAGBENCH. We have obtained informed consent from all participants and have anonymized the expert's analysis before sharing it with other experts for the inter-expert assessment. All experts were paid their regular wages during their participation in the study.\nBroader Impact While we demonstrate the effectiveness of LLM-based systems for the automated diagnostics of robotics systems, we also highlight their limitations. In particular, we show that while LLMs can achieve moderately high performance and even match experts in some cases, they are unable to fully replace the expert's analysis. We thus expect that LLM-based tools will soon become useful aids for human experts, but not fully replace them in the foreseeable future, similar to many other domains."}, {"title": "A Backgorund", "content": "Prompting Once the remarkable zero-shot capabilities of LLMs had been demonstrated (Kojima et al., 2022), a wide range of prompting schemes was proposed that aim to elicit higher quality answers from the same model by evoking a (more thorough) reasoning process (Wang et al., 2023; Yao et al., 2023; Xu et al., 2023; Zhou et al., 2023). In particular, Chain-of-Thought (CoT) prompting (Wei et al., 2022) instructs the model to \"think step-by-step\" when answering, which has been shown to improve performance on a wide range of tasks, with multiple follow-up works trading-off increased inference cost and better performance (Wang et al., 2023; Yao et al., 2023).\n(Full) Finetuning If zero-shot performance is unsatisfactory and labeled training data is available, one can continue training the model on the specific task at hand, a process known as finetuning. In particular, full finetuning refers to training the entire model on the new task.\nLORA However, the huge size of modern LLMs makes GPU memory a bottleneck for training, with common optimizers like Adam (Kingma and Ba, 2015) and AdamW (Loshchilov and Hutter, 2019) requiring three full precision values (the gradient, and its first and second moment) to be tracked for every parameter. To alleviate this issue, LORA (Hu et al., 2022) proposes that instead of updating all parameters in a weight matrix $W \\in \\mathbb{R}^{n \\times n}$ one only computes a low-rank update $AB$ where $A \\in \\mathbb{R}^{n \\times k}$ and $B \\in \\mathbb{R}^{k \\times n}$ with $k \\ll n$. We thus obtain the updated weight matrix as $W' = W + AB$ and reduce the memory footprint of the optimizer from $O(n^2)$ to $O(nk)$. Finally, recent work has shown that LORA can also be seen as a form of regularization that reduces forgetting and can thereby actually improve performance (Jimenez et al., 2023).\nModel Quantization and QLORA To reduce a model's memory footprint not only during training but also during inference, model quantization techniques have been proposed that reduce the precision of the model's weights and sometimes activations. In particular, representing the model's weight matrices using 4-, 3-, or even 2-bit precision rather than the standard (for LLMs) 16-bit half-precision representation can lead to significant memory savings (Park et al., 2018; Frantar et al., 2022; Lin et al., 2023). However, quantization can also lead to a significant drop in performance, especially if applied after training. To mitigate this issue, QLORA (Dettmers et al., 2023) proposes to quantize the weight matrices already during training, allowing the half-precision LORA adapters to learn to correct for the quantization errors, while still significantly reducing the memory footprint compared to standard LORA."}, {"title": "B Detailed Prompt descriptions", "content": "B.1 Root Cause Extraction\nAs described in Section 3.1, we extract the root cause (RC) from the historic tickets using a strong LLM (GPT-4) by concatenating the problem description, expert discussion, customer communication, and the final resolution with a chain-of-thought (CoT) prompt (Wei et al., 2022) instructing the model to carefully analyze all provided information before generating a root cause description. We show the full prompt used to this end in Figure 5.\nB.2 Root Cuase Prediction\nAs described in Section 4, we use a range of LLMs to predict the root causes of the tickets in SYSDIAG-BENCH. To this end, we use zero-shot prompting with the problem description and our preprocessed logs (see Section 4). We show the full prompt used for this task in Figure 6.\nB.3 LLM-as-a-Judge: Similarity Score Prediction\nTo assess the quality of generated root cause descriptions, we use a LLM-as-a-judge evaluation procedure (Zheng et al., 2023) where we ask a model to judge the similarity between the predicted and the reference root cause descriptions on a scale of 1 to 10. We show the full prompt used for this task in Figure 7."}, {"title": "C Experimental Setup", "content": "Model Selection We select models based on three criteria: i) sufficient (> 32k tokens) context length, ii) good general reasoning capabilities, and iii) a permissive license. Based on these criteria, we choose MIXTRAL-8X7B* (Mixtral-8x7B-Instruct-v0.1 under Apache-2.0 License Jiang et al. 2024) and the smaller MISTRAL-LITE-7B\u2020 (MistralLite under Apache-2.0 License Yin Song and Chen Wu and Eden Duthie 2023), which was specifically finetuned for long context tasks. As a reference frontier model, we consider GPT-4 (gpt-4-32k-0613 OpenAI 2023).\nExperimental Setup We use Axolotl (Axolotl, 2024) with DeepSpeed (Rajbhandari et al., 2020; Rasley et al., 2020) for finetuning with AdamW ($\\beta_1 = 0.9$ and $\\beta_2 = 0.95$) (Loshchilov and Hutter, 2019) on 2 to 8 NVIDIA A100s. We train for 3 epochs at an effective batch size of 64 for full finetuning and 16 for LORA and QLORA using an initial learning rate of $10^{-5}$ and a cosine decay with a warm-up ratio of 10%. Unless indicated otherwise, we use rank $r = 32$ for LORA and QLORA and NFloat4 + DQ (double"}, {"title": "D Extended Experimental Evaluation", "content": "Similarity Score Callibration We repeat the label extraction process described in Section 3.1 twice more for each ticket, sampling at a temperature of t = 0.5, and compute similarity scores to the reference label, obtained with greedy decoding. We thus obtain an MSS = 7.5, for root causes extracted by the same strong model with access to the same information, yielding a reference for an excellent MSS.\nFeature Importance Analysis To assess feature importance, we train MISTRAL-LITE-7B on different feature subsets using LORA and report results in Table 3. We observe that the model's performance drops significantly when excluding the elog and Problem Description, indicating their importance. In contrast, removing the startup and printspool improves the model's performance, suggesting that while these features may be helpful to debug compilation issues, they are less important for reported issues and can even distract the model (Jimenez et al., 2023).\nDistribution of Similarity Scores Illustrating performance distributions in Figure 8, we observe that distributions are similar across all models with better models having significantly fewer very low (\u2264 2) scores and a uniform increase in the frequency of all higher scores (\u2265 3).\nModel Helpfulness Beyond the analysis in Section 5, we visualize the expert rating of their highest-rated model depending on whether the experts consider our reference label to be correct in Figure 9. We find that while average scores are higher when the reference label is considered correct, there is a significant number of examples, where the predicted RC is highly rated even when the reference label is"}]}