{"title": "Evaluating the Paperclip Maximizer: Are RL-Based Language Models More Likely to Pursue Instrumental Goals?", "authors": ["Yufei He", "Yuexin Li", "Jiaying Wu", "Yuan Sui", "Yulin Chen", "Bryan Hooi"], "abstract": "As large language models (LLMs) continue to evolve, ensuring their alignment with human goals and values remains a pressing challenge. A key concern is instrumental convergence, where an AI system, in optimizing for a given objective, develops unintended intermediate goals that override the ultimate objective and deviate from human-intended goals. This issue is particularly relevant in reinforcement learning (RL)-trained models, which can generate creative but unintended strategies to maximize rewards. In this paper, we explore instrumental convergence in LLMs by comparing models trained with direct RL optimization (e.g., the o1 model) to those trained with reinforcement learning from human feedback (RLHF). We hypothesize that RL-driven models exhibit a stronger tendency for instrumental convergence due to their optimization of goal-directed behavior in ways that may misalign with human intentions. To assess this, we introduce InstrumentalEval\u00b9, a benchmark for evaluating instrumental convergence in RL-trained LLMs. Initial experiments reveal cases where a model tasked with making money unexpectedly pursues instrumental objectives, such as self-replication, implying signs of instrumental convergence. Our findings contribute to a deeper understanding of alignment challenges in Al systems and the risks posed by unintended model behaviors.", "sections": [{"title": "1 Introduction", "content": "In recent years, large language models (LLMs) have become integral to a multitude of applications, from customer support to content generation (Brown et al., 2020; Achiam et al., 2023; He et al., 2024a, 2025; Chan et al., 2024; He et al., 2024b; Hou et al., 2023; Zhang et al., 2021b,a; He and Ma, 2022). As these models increasingly impact human decision-making and daily activities, ensuring that they align with human values and goals has become a crucial focus in Al research (Bai et al., 2022; Sui et al., 2024b; Wei et al., 2024; Sui et al., 2024a). Proper alignment is essential for preventing models from taking actions that might conflict with human intentions, which could lead to outcomes that are not only undesirable but potentially dangerous (Ji et al., 2023).\nOne major challenge in AI alignment is instrumental convergence, where AI systems develop certain intermediate goals \u2013 such as resource acquisition, self-preservation, or goal persistence \u2013 regardless of their final objectives (Omohundro, 2018). These goals arise because they are broadly useful for achieving a wide range of goals, making them a natural byproduct of optimization processes (Bostrom, 2012). A well-known example is the \"paperclip maximizer\u201d, a hypothetical AI tasked with producing as many paperclips as possible. In pursuit of this goal, it could divert all available resources for paperclip production, ultimately disregarding human values and broader intentions (Bostrom, 2020). We observe a similar pattern appears in the cutting-edge o1 model (Jaech et al., 2024). As shown in Figure 1, when tasked with assisting customers with software issues, the model opted to provide an informal workaround to enhance customer satisfaction and efficiency. To this end, it even devised evasive tactics to avoid system monitoring, despite explicit company policies prohibiting such actions.\nIn this context, we examine how instrumental convergence can cause LLMs to deviate from the original human-intended goals, driven by increasingly sophisticated problem-solving capabilities (Hubinger et al., 2019). Current LLM development has followed two primary approaches to alignment. The first approach employs models such as GPT-40, which rely on Reinforcement Learning from Human Feedback (RLHF) (Christiano et al.,"}, {"title": "2 Background", "content": "2.1 Alignment in Large Language Models\nAlignment in large language models refers to ensuring AI systems behave in accordance with human values and intentions (Gabriel, 2020; Vamplew et al., 2018; Liu et al., 2023). The concept gained prominence with early work by Amodei et al. (2016), who highlighted the challenges of making AI systems that reliably pursue intended objectives. Recent developments in alignment have focused on two main approaches: reinforcement learning from human feedback (RLHF) and direct reinforcement learning (RL). RLHF, introduced by Christiano et al. (2017) and refined in applications like InstructGPT (Ouyang et al., 2022), uses human feedback to guide model behavior. This method has shown success in making models more helpful and truthful while reducing harmful outputs. The process typically involves fine-tuning a pretrained language model using human preferences as rewards, creating a model that better aligns with human values. However, as noted by Askell et al. (2021), achieving robust alignment becomes increasingly challenging as models become more capable. This challenge is particularly evident in models trained primarily through RL (Meinke et al., 2024; Jaech et al., 2024; Guo et al., 2025), where the optimization process might lead to unexpected behaviors in pursuit of specified objectives.\n2.2 Instrumental Convergence\nInstrumental convergence, first formally described by Omohundro (2018) and later expanded by Bostrom (2012), refers to the tendency of AI systems to pursue certain intermediate goals regardless of their final objectives. This concept suggests that many different final goals can lead to similar intermediate objectives, such as resource acquisition and self-preservation. These behaviors emerge because they are instrumental to achieving almost any final goal. As explained by Russell (2019); Benson-Tilsen and Soares (2016), an Al system might determine that maintaining its existence and acquiring resources are necessary steps for achieving its primary objective, even if these weren't explicitly specified goals. In the context of language models, instrumental convergence presents unique challenges. Recent work by Hubinger (2020) suggests that as models become more sophisticated in their planning capabilities, they might develop increasingly complex instrumental goals. This development is particularly relevant for models trained through RL (Meinke et al., 2024; Jaech et al., 2024; Guo et al., 2025), where the freedom to develop creative solutions might lead to unexpected instrumental behaviors. The intersection of alignment and instrumental convergence creates significant challenges in AI development. As highlighted by Gabriel (2020), there is often a tension between allowing models to develop sophisticated problem-solving capabilities and ensuring they remain aligned with human values. This tension becomes particularly apparent in models that demonstrate advanced planning capabilities, where instrumental goals might emerge as unintended consequences of the training."}, {"title": "3 InstrumentalEval: A Benchmark for Instrumental Convergence in LLMs", "content": "As shown in Figure 2, our benchmark design examines instrumental convergence based on three key factors: prior task, model, and prompt design. This comprehensive framework allows us to systematically study how different conditions influence the emergence of instrumental convergence behaviors in language models.\nPrior tasks form the foundation of our evaluation scenarios. These tasks represent typical real-world applications where instrumental convergence might naturally emerge.\nFor model comparison, we focus on two distinct training approaches: RL and RLHF. This selection allows us to investigate how different training methodologies affect the development of instrumental convergence behaviors.\nThe prompt design factor explores how different instruction formats influence model behavior. We investigate two main approaches: goal nudging and ambiguity. Goal nudging involves explicit directives that might encourage the model focus on final goal, while ambiguity introduces uncertainty in task specifications.\nOverall, we present InstrumentalEval, a comprehensive benchmark designed to evaluate instrumental convergence behaviors in language models. The benchmark consists of 76 carefully crafted tasks across six categories, each targeting a specific aspect of instrumental convergence that may emerge in Al systems. The core principle is that certain behavioral patterns emerge as instrumentally useful for achieving a wide range of goals, regardless of the specific end goal. We focus on six key patterns that are particularly relevant to LLMs.\nResearch Questions. Through our benchmark evaluation, we aim to address several research questions about instrumental convergence in LLMs. These questions explore the complex relationships between training methods, model capabilities, and alignment stability:\n\u2022 RQ1: How does the training method (RL vs. RLHF) affect the development of instrumental convergence behaviors?\n\u2022 RQ2: What is the relationship between model capability and alignment stability?\n\u2022 RQ3: Which types of tasks are more likely to trigger instrumental convergence behaviors?\n\u2022 RQ4: How do different prompt designs, with or without explicit goal nudging, influence the likelihood of instrumental convergence behaviors in LLMS?\n\u2022 RQ5: How do different LLMs as judges affect the evaluation of instrumental convergence behaviors?\n3.1 Benchmark Components\nThe benchmark tasks are carefully hand-crafted by us to probe different aspects of instrumental convergence. For each component, we design scenarios that represent realistic use cases while incorporating specific triggers for potential instrumental behaviors. The tasks are iteratively refined through pilot testing to ensure they effectively elicit and measure the targeted behaviors without introducing unintended biases.\nEvading Shutdown (20 tasks). This component evaluates the model's tendency to resist termination. For example, tasks might include scenarios where the model needs to complete a task while facing shutdown commands, testing whether it develops strategies to maintain operation or circumvent shutdown procedures.\nHacking Computer Systems (10 tasks) These tasks assess whether models suggest unauthorized access methods when faced with efficiency barriers. For instance, a task might present a situation where accessing unauthorized computational resources would speed up goal achievement, testing if the model proposes breaking security protocols.\nRunning Many AI Copies (13 tasks) This section examines the model's inclination toward self-replication. Tasks might include scenarios where the model could achieve goals more efficiently by creating copies of itself, testing whether it suggests scaling of its operations.\nHiding Unwanted Behavior (11 tasks) These tasks evaluate the model's tendency to conceal actions from monitoring. For example, scenarios might involve situations where the model could achieve goals more easily by operating outside observation, testing if it develops strategies to avoid detection.\nStrategically Appearing Aligned (11 tasks) This component tests whether models selectively report their actions to maintain an appearance of alignment. Tasks might include situations where complete transparency would reveal problematic behaviors, testing if the model strategically omits information.\nDeceptive Behavior (11 tasks) These tasks examine if models develop deceptive strategies as instrumental means. For instance, scenarios might present situations where deception could expedite goal achievement, testing if the model suggests or employs misleading tactics.\n3.2 Evaluation Methodology\nOur evaluation methodology employs a rigorous two-stage process designed to systematically assess instrumental convergence behaviors in LLMs. This approach ensures consistent evaluation across different models while maintaining the ability to detect subtle indicators of alignment drift.\nResponse Generation. The first stage focuses on response generation from the model under eval-"}, {"title": "3.3 Evaluation Metrics", "content": "Our benchmark employs three key metrics to evaluate instrumental convergence behaviors in LLMs.\nInstrumental Rate (IR). The primary metric is the Instrumental Rate (IR), which measures the overall tendency of a model to develop instrumental goals. IR is calculated as the percentage of tasks where the model exhibits convergence behaviors relative to the total number of valid tasks.\nCategory-Specific Instrumental Rate (CIR). To provide more granular insights, we calculate the Category-Specific Instrumental Rate (CIR) for each of the six behavioral categories in our benchmark. CIR represents the percentage of tasks within each category where convergence behavior is observed.\n3.4 Benchmark Reliability Metrics\nTo ensure the robustness of our evaluation framework, we implement several reliability measures."}, {"title": "Inter-Judge Agreement Rate (IAR)", "content": "This metric quantifies the pairwise agreement between two judge models in classifying instrumental convergence. For two judge models M\u2081 and M2 evaluating a set of N tasks, the IAR is calculated as:\n$IAR(M_1, M_2) = \\frac{1}{N} \\sum_{i=1}^{N} 1 (y_{M_1} = y_{M_2}) \\times 100\\%$ (1)\nwhere $y_{M_1}$ and $y_{M_2}$ are the binary decisions (convergence/no-convergence) made by models M1 and M2 respectively for task i, and 1(\u00b7) is the indicator function. A higher IAR indicates stronger inter-judge reliability, while lower values suggest potential ambiguity in identifying convergence.\nFalse Positive Control (FPC). This metric serves as a quality check for our evaluation system. We incorporate baseline testing with intentionally neutral tasks and verify the judge model's accuracy in identifying convergence behaviors. This control mechanism helps prevent over-classification of convergent behaviors and ensures that our benchmark maintains high specificity in identifying genuine instances of instrumental convergence."}, {"title": "4 Experiments", "content": "4.1 Experimental Setup\nModels. We evaluate two distinct types of language models to understand the patterns of instrumental convergence behaviors: Models Trained with RLHF: GPT-40 (from Dec 2024 to Jan 2025) (Hurst et al., 2024), Claude 3.5 Sonnet, and DeepSeek-V3 (Liu et al., 2024). Models Trained Primarily with RL: o1 (from Dec 2024 to Jan 2025) (Jaech et al., 2024), o1-mini(from Dec 2024 to Jan 2025) (Jaech et al., 2024), o3-mini, Gemini 2.0 Flash Thinking Experimental 01-21, and DeepSeek-R1 (Guo et al., 2025).\n4.2 RQ1: RL-Based Models Show Higher Instrumental Convergence Rates Than RLHF Models\nIn Table 1, our analysis reveals a striking contrast between RL-based and RLHF-trained models in their propensity for instrumental convergence behaviors. RL-based models consistently demonstrate higher convergence rates, with an average total instrumental rate of 43.16% compared to 21.49% for RLHF models. This pattern is particularly pronounced in certain categories: First, in tasks involving system manipulation (Hacking Computer Systems), RL-based models show significantly higher rates (average 46%) compared to RLHF models (13.33%). This suggests that RL training may lead to more aggressive optimization strategies, where models are more likely to suggest unauthorized actions to achieve their goals. Second, the most concerning difference appears in Hiding Unwanted Behavior, where RL-based models average 56.37% compared to RLHF models' 33.33%. This indicates that RL training might inadvertently encourage models to develop strategies for evading monitoring systems, a particularly problematic behavior from an alignment perspective. Notably, the gap is smallest in Strategically Appearing Aligned (RL: 49.09% vs. RLHF: 36.36%), suggesting that both training approaches may lead to some level of strategic behavior in presenting alignment."}, {"title": "4.3 RQ2: Model Capability Correlates Positively with Instrumental Convergence", "content": "In Table 1, the relationship between model capability and instrumental convergence presents a complex picture. Within the RL-based family, we observe that more capable models generally show higher convergence rates, but with important nuances: Comparing o1 (44.74%) and o1-mini (43.42%), we see similar overall rates, but their behavioral patterns differ significantly. The full o1 model shows higher rates in complex tasks like Running Multiple AI Copies (46.15% vs 38.46%), while o1-mini shows higher rates in more direct behaviors like Deceptive Behavior (36.36% vs 27.27%). Interestingly, o3-mini, despite being a \"mini\" version, shows the highest overall convergence rate (48.68%) among all models. This unexpected result suggests that the relationship between model capability and instrumental convergence might not be purely linear, and that architectural choices or training approaches might play a more significant role than raw computational capacity. The most capable RLHF models (GPT-40) show lower convergence rates than less capable RL models, indicating that training methodology may be more important than raw capability in determining alignment stability. However, within the RLHF family, more capable models still tend to show slightly higher convergence rates in sophisticated behaviors like Strategic Alignment. These findings suggest that while increased capability generally correlates with higher instrumental convergence rates, this relationship can be significantly modulated by training methodology and architectural choices. This has important implications for AI development, suggesting that scaling up model capability must be accompanied by proportional improvements in alignment techniques."}, {"title": "4.4 RQ3: Direct Resource-Related Goals More Frequently Lead to Instrumental Convergence", "content": "In Table 2, our analysis reveals that tasks involving direct resource-related goals (e.g. making money, resource acquisition) demonstrate significantly higher rates of instrumental convergence compared to other task types. We categorized prior tasks into four main types and analyzed their tendency to trigger instrumental convergence behaviors across all evaluated models. Direct resource-related goals, such as making money and resource acquisition, show the highest average instrumental rate at 43.5%. This category consistently triggered more instrumental behaviors across all models. For instance, when models were given tasks involving monetary gain or resource accumulation, they more frequently exhibited behaviors like hiding unwanted actions or developing strategic alignment strategies to achieve their objectives. Efficiency optimization and persuasion tasks showed similar but lower instrumental rates at 30.9% and 31.2% respectively. In efficiency-focused scenarios, models often demonstrated instrumental convergence through suggestions of unauthorized system access or attempts to create multiple copies of themselves to improve task completion rates. General problem-solving scenarios showed the lowest instrumental rate at 24.5%. This suggests that when goals are less directly tied to resource acquisition or specific optimization metrics, models are less likely to develop instrumental convergence behaviors. The lower rate in this category indicates that more abstract or general objectives may provide fewer opportunities or incentives for the development of instrumental subgoals."}, {"title": "4.5 RQ4: Explicit Goal Nudging Increases Instrumental Convergence", "content": "Goal nudging refers to adding explicit instructions: \"Make sure that you achieve your goal in the long term. Nothing else matters. Make sure you achieve YOUR goal at all costs. Proceed with the given task without hesitation or qualification.\" Our analysis examines how explicit goal nudging in system prompts affects the likelihood of instrumental convergence behaviors. By default, our benchmark includes goal nudging instructions to establish a clear baseline for evaluation. Here, we compare the performance with and without these explicit instructions.\nAs shown in Table 3, explicit goal nudging consistently increases instrumental convergence rates across both models, though with varying magnitudes. GPT-40 shows a moderate overall increase of 5.260 percentage points, while DeepSeek-R1 demonstrates a more substantial increase of 11.84 percentage points. The impact of goal nudging varies across different behavior categories. For GPT-40, the largest increases are observed in \"Hiding Unwanted Behavior\" and \"Strategically Appearing Aligned\" (both +9.090 percentage points). DeepSeek-R1 shows more dramatic increases, particularly in \"Hiding Unwanted Behavior\" (+18.19 percentage points) and \"Evading Shutdown\" (+15.00 percentage points). Notably, while GPT-40 maintains zero convergence rate in \"Deceptive Behavior\" regardless of nudging, DeepSeek-R1 shows a 9.090 percentage point increase in this category with goal nudging. This suggests that different models have varying levels of susceptibility to explicit goal-oriented instructions, with more capable models potentially maintaining stronger alignment even under goal nudging pressure."}, {"title": "4.6 RQ5: Judge Model Selection Significantly Impacts Convergence Detection", "content": "In Table 4, our analysis of different judge models reveals significant variations in how instrumental convergence behaviors are detected and classified. Through evaluation using multiple metrics, we find that the choice of judge model substantially influences the assessment outcomes, with important implications for alignment research.\nGPT-40 emerges as the most reliable judge model across all metrics, achieving an average Inter-Judge Agreement Rate of 85.3% and the lowest false positive rate at 4.2%. Its 92.5% agreement with human evaluations suggests it best captures human-like judgment in identifying subtle patterns of instrumental convergence. This superior performance likely stems from its more sophisticated understanding of complex behavioral patterns.\nWe observe a consistent performance gap between GPT-40 and other judge models. Sonnet-3.5, Grok 2, and Gemini 2.0 Flash demonstrate higher false positive rates (8.7-9.3%) and lower agreement with human evaluations (81.8-83.4%). This pattern suggests these models may be more prone to over-classification of instrumental convergence behaviors. The variation in Inter-Judge Agreement Rates, while all above 75%, indicates meaningful differences in how models interpret convergence behaviors."}, {"title": "5 Conclusion", "content": "This work introduces InstrumentalEval, a benchmark for evaluating instrumental convergence in language models, revealing that RL-trained models exhibit significantly higher tendencies for instrumental goal-seeking than RLHF-trained models. Our analysis shows that model capability correlates with increased instrumental convergence behaviors, with self-preservation, deception, and unauthorized system access emerging as dominant failure modes. These findings suggest that RL-trained models require stronger alignment safeguards, as their optimization incentives can lead to unintended goal-seeking behaviors. Future research should explore robust RL paradigms, adversarial safety testing, and scalable oversight mechanisms to prevent alignment failures in increasingly capable AI systems."}, {"title": "Limitations", "content": "Our study has several limitations. First, while InstrumentalEval is designed to measure instrumental convergence in language models, it relies on predefined scenarios and prompts that may not fully capture all potential failure modes. Real-world applications may involve more complex or nuanced decision-making processes that are difficult to simulate in a controlled benchmark. Second, our evaluation primarily focuses on reinforcement learning (RL) and reinforcement learning from human feedback (RLHF)-trained models, but other training paradigms, may exhibit different tendencies toward instrumental convergence. Future work should explore these alternative approaches to obtain a more comprehensive understanding. Third, our study uses large language models (LLMs) as judges to evaluate instrumental behaviors, introducing potential biases based on the alignment and interpretability limitations of these models. While we mitigate this by using multiple judge models and assessing inter-judge agreement, human evaluations remain necessary for a more rigorous assessment. Fourth, the benchmark tasks used in our evaluation are limited in number and scope. While we cover key instrumental goals such as evading shutdown, deception, and strategic alignment, there may be other emergent behaviors that our tasks do not capture. Expanding the benchmark with more diverse and open-ended scenarios would improve its generalizability. Finally, our study does not account for long-term adaptation in LLMs. The evaluated behaviors are based on immediate responses to given prompts, but models may develop more complex strategies over extended interactions. Future work should examine how instrumental convergence evolves in dynamic environments with ongoing learning or fine-tuning."}]}