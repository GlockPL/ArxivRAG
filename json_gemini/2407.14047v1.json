{"title": "OCTrack: Benchmarking the Open-Corpus\nMulti-Object Tracking", "authors": ["Zekun Qian", "Ruize Han", "Wei Feng", "Junhui Hou", "Linqi Song", "Song Wang"], "abstract": "We study a novel yet practical problem of open-corpus multi-object tracking\n(OCMOT), which extends the MOT into localizing, associating, and recogniz-\ning generic-category objects of both seen (base) and unseen (novel) classes, but\nwithout the category text list as prompt. To study this problem, the top priority\nis to build a benchmark. In this work, we build OCTrackB, a large-scale and\ncomprehensive benchmark, to provide a standard evaluation platform for the OC-\nMOT problem. Compared to previous datasets, OCTrackB has more abundant and\nbalanced base/novel classes and the corresponding samples for evaluation with\nless bias. We also propose a new multi-granularity recognition metric to better\nevaluate the generative object recognition in OCMOT. By conducting the extensive\nbenchmark evaluation, we report and analyze the results of various state-of-the-art\nmethods, which demonstrate the rationale of OCMOT, as well as the usefulness\nand advantages of OCTrackB.", "sections": [{"title": "Introduction", "content": "Multi-object tracking (MOT), which involves detecting and associating the targets of interest in a\nvideo, is a classical and fundamental problem with many real-world applications, such as video\nsurveillance, autonomous driving, etc. Recently, MOT has attracted broad attention with numerous\nalgorithms and datasets [1-6]. For many years, MOT has mainly focused on the target of humans,\ne.g., the datasets of MOT15 [7], MOT20 [8], DanceTrack [9]. Several works also focus on traffic\nscenes and aim to track vehicles, such as the well-known KITTI [10] dataset.\nIn real-world scenes, the categories in videos are diverse, far from being limited to humans and\nvehicles. TAO [11], as the first work, constructs a large-scale benchmark to study tracking any\ncategory of target, with a total of 833 object classes. During the same period, GMOT-40 [12]\nbuilds a generic multi-object tracking benchmark with 10 object classes but more dense objects per\nframe. With the number of categories increasing in the MOT task, the evaluation metrics evolve\nfrom just object localization and association to also include class recognition. A new metric TETA\n(tracking-every-thing accuracy) is proposed [13] to evaluate the generic MOT from the above three\naspects. More recently, open-world MOT (OWMOT) [14] is proposed to train a tracker using the\nsamples from 'base classes', and test it on videos containing objects from 'novel classes'. The\ntracker must recognize the base-class objects and identify all other unseen classes as 'new'. Further,\nopen-vocabulary MOT (OVMOT) [15] aims to not only distinguish the novel-category objects but\nalso classify each object, typically achieved by a pre-trained multi-modal model, e.g., CLIP [16].\nUndoubtedly, the development of MOT from specific-category to generic-category and further to\nopen-world/vocabulary settings is becoming increasingly practical. A remaining problem in the latest\nOVMOT is that, during testing, a predefined category list of base and novel classes is required as the\ntext prompts for the classification task, as shown in Figure 1(a). However, obtaining this list in real\napplications is not easy, especially for novel classes, which are termed novel because the categories\nare previously unknown. This way, in this work, we propose a new problem called Open-Corpus\nMulti-Object Tracking (OCMOT), which treats the object recognition task as a generative problem,\nrather than the classification problem in OVMOT, as shown in Figure 1(b), where the category list is\nno longer required."}, {"title": "Related Work", "content": "Multiple Object Tracking (MOT). The dominant approach in MOT is the tracking-by-detection\nframework [18], which initially identifies objects in each frame and then associates them across\nframes using various cues such as object appearance features [19-24, 4, 25], 2D motion features [26-\n30], or 3D motion features [31-36]. Some approaches enhance tracking performance by leveraging\ngraph neural networks [37, 38] or transformers [5, 39, 6, 40] to learn association relationships\namong the objects of different frames. To extend the object categories in the MOT task, the TAO\nbenchmark [11] has been proposed, which handles the MOT under various object categories with a\nlong-tail distribution. Several follow-up works are proposed to evaluate this benchmark including\nAOA [41], GTR [40], TET [42], QDTrack [20], etc. Although these methods perform effectively,\nthey are confined to closed-set object categories, i.e., the object categories in training and testing sets\nare overlapped. This is unsuitable for diverse open-world scenarios with new categories. Differently,\nthis work tracks objects of categories whether or not appearing during training, and generates their\nclasses, which significantly expands the practical application for tracking.\nOpen-World MOT has not been extensively explored. Some existing related works [43, 44] adopt\nthe class-agnostic detectors with general trackers to implement open-world tracking. These methods\nfocus solely on tracking salient objects in the scene without considering specific categories. The\nrecent TAO-OW [45] takes a step further by considering the challenges of classification in open-world\ntracking, dividing all objects into known and unknown categories. In this work, category-aware\nopen-world tracking is achieved by tracking objects of both known and unknown categories. While\nthis advancement is a step forward in open-world tracking, it still falls short in the recognition of\nspecific object classes in unknown categories. Further, OVTrack [15] incorporates open vocabulary\ninto the tracking task as OVMOT, providing a baseline method and benchmark built upon the TAO\ndataset. Although it is much more practical, a remaining problem is the requirement for the predefined\ncategory list during the testing stage. Differently, our OCMOT does not require predefined category\nnames as in the OVMOT task. Instead, it directly generates target category names using a generative\nmodel, which overcomes the limitations of the OVMOT problem and enhances generalizability."}, {"title": "OCTrack Benchmark", "content": "We first provide the problem formulation of OCMOT. Given a video sequence with various objects,\nOCMOT aims to simultaneously achieve the localization, association and recognition tasks, thus\ngenerating a bounding box \\(b = [x, y, w, h]\\), continuous ID number d (along the video) and a category\nc for each target in the video. The annotated object categories appearing during training are defined\nas \\(C_b\\), i.e., the base class set. In testing, we aim to obtain the OCMOT results, i.e., the object category\nset \\(C_{\\text{open}}\\) is an open corpus. Obviously we have \\(C_b \\subset C_{\\text{open}}\\), and we define the novel class set as\n\\(C_n = C_{\\text{open}} \\backslash C_b\\). Note that, we take the category recognition task as a generative task, with no need\nfor the category list of \\(C_{\\text{open}}\\) as input during testing. Ideally, \\(C_{\\text{open}}\\) contains all the categories in the\nreal world. In practice, for OCMOT evaluation, we can limit \\(C_{\\text{open}}\\) to a large-scale thesaurus."}, {"title": "Principle of Benchmark Construction", "content": "To build the OCMOT benchmark (OCTrackB), we first establish the following principles:\nP0: Principle of standardness. Following the base and novel class division mode proposed in LVIS;\nP1: Category enrichment principle. Base/novel classes should be diverse and balanced;\nP2: Sample enrichment principle. Evaluation videos/objects for all classes should be abundant;\nP3: Semantic compatibility principle. The evaluation of object recognition should be compatible.\nThe first principle P0 ensures the base and novel class division in our dataset is consistent with that in\nthe widely used LVIS. This is because that previous works, e.g., many open-vocabulary detection\nmethods [53-57], and the open-vocabulary tracker OVTrack all use LVIS as the training dataset. As a\ntesting dataset, OCTrackB with the same base/novel class division is more convenient for evaluating\nthe algorithms trained on LVIS. Both P1 and P2 guarantee the richness of the dataset, which aims\nto increase the object categories and the sample amount in the dataset. This is significant for the\nopen-corpus tracking task. The last principle P3 aims to address the semantic ambiguity problem,\nwhich stems from two aspects. The first aspect arises from the dataset annotation. Due to the large\nnumber of categories, the granularity of the category annotations in the dataset is misaligned, leading\nto inaccurate evaluations. For example, the granularity of classification for some targets is only\nup to 'bird', while for others, it is more specific, such as 'goose' or 'duck'. This misalignment in\nlabeling makes it difficult to compare recognition accuracy across algorithms during evaluation. The\nsecond aspect comes from the task OCMOT. Different from the classification head in OVMOT, the\nproposed OCMOT handles recognition as a generative problem. This may cause semantic synonymy\nor subordination. For example, \u2018cab' and \u2018taxi' usually mean the same thing, which are both types\nof 'car'. Therefore, for the ground truth 'cab', the prediction result of \u2018taxi' or even \u2018car' should\nbe reconsidered and not simply taken as false. As shown in the example in Figure 2(a), we divide\nthe categories in LVIS into multiple levels and conduct multi-level evaluations to strive to achieve\nPrinciple P3. Following the above principles, we build OCTrackB."}, {"title": "Dataset Collection and Annotation", "content": "By investigating the recent video datasets, we select two large ones with various object categories, i.e.,\nTAO [11] and LV-VIS [58], as the basis for constructing OCTrackB. TAO is a generic-category object\ntracking dataset, with a total of 833 classes and 2,907 videos. LV-VIS is a large-vocabulary video\ninstance segmentation dataset with 1,196 classes and 4,828 videos. Previous work OVTrack [15],\nalso following P0, directly uses TAO's validation and test sets (annotations from BURST [59]) and\nonly maintains the classes that overlap with LVIS for data selection, and form the OVTrack testing\ndatasets, i.e., OVTAO validation (OVTAO-val) and test (OVTAO-burst) sets. The simple category\nintersection operation decreases the number of classes significantly. In this work, we consider the\nadvantages of TAO, which provides longer videos but has a limited number of categories (overlapped\nwith LVIS). On the other hand, the LV-VIS dataset offers a larger number of object categories, which\ncan effectively compensate for the shortcomings of TAO, making it more suitable for addressing the\nOCMOT task. Specifically, we filter the test and validation sets of TAO and the training and validation\nsets of LV-VIS to select videos that meet principle P0. To satisfy P1, we use a greedy algorithm\naiming to minimize the total number of videos while ensuring that each category contains at least two\nvideos whenever possible, thus ensuring category diversity and balance. This results in a selection\nof 903 videos covering 892 categories (also contained in LVIS). To meet P2, we again use a greedy\nalgorithm, aiming to allocate as many tracks as possible for each category while maintaining the same\ntotal number of videos. This further produces 732 videos with 4,766 tracks. In total, we collected\n1,635 videos, of which 496 include novel category objects and 1,600 encompass base categories."}, {"title": "Dataset Statistics and Comparison", "content": "We then show the statistics of OCTrackB and compare it with two existing OVTrack datasets, i.e.,\nOVTAO-val [15] and OVTAO-burst [59]. OCTrackB has the following typical advantages.\nVarious and balanced object categories. OCTrackB contains a total of 892 available categories,\ncomposed of 653 base and 239 novel classes. We show some example classes in OCTrackB in\nFigure 2(a), the categories cover every aspect in the real-world applications, e.g., various transporta-\ntion, animals, and household items, etc. Note that, following the original class annotation in our\nbasic datasets TAO and LV-VIS, OCTrackB involves multi-granularity categories. For example, the\nfine-grained class 'shepherd dog' and its general class 'dog' are concomitant in OCTrackB's category\nlist. We leverage this subordinate relation to design the new evaluation metric in the following section.\nAs shown in Figure 2(b), OCTrackB includes 653 base and 239 novel classes, which account for\n75.5% of the original LVIS base categories and 70.9% of the novel categories, respectively, effectively\nensuring the category diversity. For previous datasets, OVTAO-val and OVTAO-burst contain 30.1%\nand 37.4% of the original LVIS base classes, respectively. With respect to the novel class, the ratios\nare only about 10% (2.9%/2.7% vs. 28.0%). The various object categories make OCTrackB more\ncomprehensive in evaluating open-corpus object tracking performance.\nNext, we consider the category balance of the\ndataset. As shown in Figure 3, we calculate\nthe normalized entropy of different units (object\nboxes, object tracks, videos) and the category\nset. Specifically, for N categories in the dataset,\nwe compute the Shannon Entropy as \\(H(p) =\\)\n\\(-\\sum_{i=1}^{N} p_i \\log(p_i)\\), where \\(p_i\\) denotes the proba-\nbility of a unit belonging to category i, and the\nMaximum Entropy as \\(H_{\\text{max}} = \\log(n)\\). Then\nwe get the Normalized Entropy as \\(NE = \\frac{H(p)}{H_{\\text{max}}}\\),\nwhich can reflect the category balance in the\ndataset. We can see that, the class balance of the\nproposed OCTrackB is higher than OVTAO-val\nand OVTAO-burst. We know that, in the real\nworld, the object category distribution is long-tail but not balanced. However, as an evaluation\nbenchmark, we try to keep the category balanced to guarantee that the evaluation is not dominated by\nthe large-scale yet simple classes.\nAbundant samples for both base and novel classes. As shown in Figure 4, we show the number\nof objects, tracks, and videos in OVTAO-val, OVTAO-burst, and OCTrackB datasets. The statistics\nare split through the base and novel classes. We can see that, for the base class, the number of\nobject boxes, tracks, and videos in OCTrackB is greater than those of OVTAO-val and OVTAO-burst.\nMoreover, in terms of novel class, we can see that the data amount of OCTrackB is significantly\nlarger than that of OVTAO-val and OVTAO-burst, with the increase ranging from 7.7 to 11.2 times.\nFrom the comparison, we can see that the proposed OCTrackB is more in line with the above\nprinciples P1 and P2. We further provide more statistics of OCTrackB to show its data distribution\nand characteristics in the supplementary material."}, {"title": "Evaluation Metrics", "content": "Following [15], we use the open-category tracking metric namely tracking-every-thing accuracy\n(TETA) in [13] for evaluation. TETA is composed of three parts, i.e., object localization, association,\nand classification accuracies. First, the localization accuracy (LocA) is computed through the match-\ning of the GT boxes with predicted boxes without considering class, as \\(LocA = \\frac{|TPL|}{|TPL|+|FPL|+|FNL|}\\).\nSecond, association accuracy (AssocA) is determined by matching the identities of associated GT\ninstances with the predicted association, as \\(AssocA = \\frac{1}{|TPL|} \\sum_{b \\in TPL} \\frac{|TPA(b)|}{|TPA(b)|+|FPA(b)|+|FNA(b)|}\\).\nFinally, classification accuracy (ClsA) is calculated using all correctly localized instances, by com-\nparing the predicted classes with the corresponding GT classes, as \\(ClsA = \\frac{|TPC|}{|TPC|+|FPC|+|FNC|}\\).\nThe\nTETA score is computed as the mean value of the above three scores as \\(TETA = \\frac{LocA+ClsA+AssocA}{3}\\).\nIn previous open-category tracking tasks [45, 15], object recognition is always taken as a classification\nproblem using the above ClsA metric. We take the recognition as a generative task, which may\ngenerate multiple labels. This way, as shown in Figure 5, we first use CLIP [16] to encode the\npredicted output (multiple generated object categories concatenated into a single prompt using ',') and\neach base/novel category in LVIS. Next, we calculate the similarity between these encoded features\nto choose a high-similarity category label, i.e., the matching class (a single class name in LVIS),\nwhich can be used to compute ClsA. Note that, the base and novel categories are only used for result\nevaluation, which is different from OVMOT that uses them to generate the prediction results.\nAs discussed in P3 at Section 3.2, the open-corpus tracking may introduce the semantic ambiguity\nproblem. To address this problem, we design a multi-granularity recognition accuracy (mgReA).\nSpecifically, considering the diversity of the generated vocabulary, we aggregate the categories in LVIS\naccording to WordNet [60] as a hierarchy structure. As shown in Figure 5, when computing mgReA,\nif the ground-truth category label belongs to any category within this aggregated multi-granularity\nclass hierarchy, it is considered an expanded successful recognition. A simple example is that, for the\nground-truth label 'shepherd dog', we expand it to \u2018dog\u2019. For the matching class (prediction) of 'dog',\nClsA will judge it as a false result, but mgReA takes it as true. This metric provides a more intuitive\nand compatible evaluation, since we do not need very fine-grained classifications in many cases.\nBased on mgReA, we define a new comprehensive metric called tracking&recognizing-every-thing\naccuracy (TRETA) as \\(TRET\u0410 = \\frac{LocA+mgReA+AssocA}{3}\\) for the OCMOT problem."}, {"title": "A Baseline Method: OCTracker", "content": "As shown in Figure 6, similar to most tracking-by-detection based MOT approaches,\nwe first need to obtain object bounding boxes for each frame. Since our focus is on open-corpus object\ntracking, we aim to localize generic-class objects. We employ the well-known detector Deformable\nDETR [61] as the basic network structure of the localization head. Deformable DETR uses Hungarian\nmatching to map the predicted detections to the ground truths, and then aligns the corresponding\nboxes through category classification loss and bounding box regression loss. In our framework, we do\nnot consider the object class in the localization head, aiming to train a class-agnostic object detector.\nThis way, we replace the category classification loss in [61] with a binary cross-entropy loss, i.e., to\nestimate whether a region candidate is an object of interest or not.\nThe recognition head is used to generate the category name of the object. It mainly\nconsists of a generative language model, for which we use FlanT5-base [62] and initialize it with its\npre-trained weights. The visual features of the candidate objects obtained from Deformable DETR\nare mapped to the input space of the generative model through a projection layer, and then processed\nby a generative encoder and decoder, both composed of self-attention layers and feedforward neural\nnetworks. The encoder's output interacts with the decoder through the cross-attention layers. Then\nthe decoder's output is passed through a softmax layer to predict the corresponding word, while the\nprediction of the previous word is used as input for training the next word prediction. The generative\nmodel is trained following the manner and loss function in [63], using the VG [64] and GRIT [65]\nimage-text pairs as training data. The beam size of the language model is controllable. We set it to 2,\nmeaning that we generate two category nouns for each object.\nAs a tracking task, a key step is to associate objects along the video. For this purpose,\nwe consider a two-stage training strategy to train the object similarity learning model for association.\nSince there is no large-scale generic-object video dataset with tracking annotations [15], we can only\nuse the image datasets or raw videos for training. The first stage is to learn the association model with\nstatic images. Following [15], we apply the data hallucination strategy to generate the pairwise images\nfor training. Specifically, given an image of base categories in LVIS [17], we use a diffusion model to\ngenerate its adjoint image with the same object categories but different styles. Then the similarity\nlearning can be achieved by contrastive learning between each image pair, in which the same object\nas the positive sample, other objects, and generated objects as negative samples. The second stage is\nto learn the association model with raw videos. Following [66], we employ a self-supervised strategy\nto learn object similarity using the raw videos in TAO training set. Specifically, as shown in Figure 6,\ngiven a reference object in a frame, we first seek its most similar target in another frame. Then from\nthis target, the most similar object in the original frame should be the reference object. Based on this\nobject self-similarity rationale, we use the self-supervised losses [66] to learn the object similarity.\nMore details and limitations of OCTracker are discussed in the supplementary material."}, {"title": "Experimental Results", "content": "As a new problem, there is no approach that can directly handle the OCMOT. We try to include\nas many approaches as possible with necessary modifications for comparison on the proposed\nOCTrackB. First, we select two strong MOT algorithms, i.e., QDTrack [20] and TETer [67]. The\nclassical MOT approaches can not handle the object recognition task in OCTrack, thus we train both\nalgorithms on both base and novel categories in LVIS [17] and TAO [11] training sets by a closed-set\ntraining approach and then evaluate their performance on the OCTrackB. Second, we include\nthe only public open-vocabulary MOT algorithm OVTrack [15] in the experiments, by additionally\ngiving the base and novel class list during testing as the setting of it. Also, to evaluate more\nrelated approaches, we further use the way of approach combination. Specifically, we select an\nopen-vocabulary detection (OVD) algorithm for object localization and recognition (classification)\ncombined with an object tracking method for association, to achieve the OCMOT. We select three\nstate-of-the-art OVD algorithms, i.e., VLDet [55], CoDet [54], MM-OVOD [53], and three tracking\nmethods including the appearance-based tracking method namely DiffuTrack in OVTrack [15],\nmotion-based tracking in ByteTrack [1] and OC-SORT [2]. Note that, these methods also need the\nbase and novel class list for testing. Finally, we employ an open-ended generative object detection\nmethod namely GenerateU [63] as the detector, combined with the above tracking modules, for the\nOCTrack task. This series of methods is really under the setting of OCTrack without the class lists\nwhen testing. The proposed baseline method OCTracker is also included for comparison."}, {"title": "Benchmark Results", "content": "As shown in Table 1, we can see that, the classical\nMOT algorithms QDTrack [20] and TETer [67] provide a satisfied performance on the localization\nand association tasks since these methods are specifically designed for such tasks and they have been\ntrained on both base-class and novel-class data. However, we can see that the object recognition\nresults, i.e., the ClsA score, are very poor. This is because these methods can not handle the diverse\nlong-tailed classification with up to 892 categories in OCTrackB. Also, ClsA metric only considers\ntop-1 classification accuracy, but these classes are fine-grained. From this point, the proposed\nrecognition score mgReA is more reasonable. Then we can see that the open-vocabulary tracking\napproach OVTrack [15] provides relatively good results among all competitors. However, it uses the\nclass list as input during training, the open type is set as OV. Under the same setting of OV, we select\nthree more recent OV detection methods VLDet [55], CoDet [54] and MM-OVOD [53] with three\nclassical tracking strategies for association. Among them, DiffuTrack uses a diffusion model based\ndata hallucination strategy [15] to learn the object similarity for association. ByteTrack [1] applies a\ndetection selection strategy and uses the motion feature for the association. OC-SORT [2] further\nconsiders the occlusion when using the motion feature. For the above combination-based methods,\nwe find that their overall performance is comparable with OVTrack. In terms of the comprehensive\nTETA score, CoDet [54] and MM-OVOD [53] with DiffuTrack outperform OVTrack on the base\nclass. VLDet [55] and CoDet [54] with DiffuTrack outperform OVTrack on the novel class. But\nthe margins are all not very large. Note that, the proposed OCTrackB can also be used for the\nopen-vocabulary MOT problem as shown in the above results. However, in this work, we are more\ninterested in the proposed OCMOT problem, which is more practical and promising.\nNext, we present the results under the OC setting, in which we use the alone detector following the\nOC setting, i.e., GenerateU [63] with the above three tracking strategies to implement the OCMOT.\nWe report the results of them, and also the proposed OCTracker, at the bottom of Table 1. We can see\nthat, in terms of the object recognition task using the ClsA metric, OCTracker provides a comparable\nresult with other approaches since the underlying language models used for object recognition (class\ngeneration) are similar. OCTracker also provides better association results (AssocA) for both base\nand novel classes, which demonstrates the advantages of the association head in OCTracker."}, {"title": "In-depth Analysis", "content": "From Table 1, we can observe that the performances of all the methods\nare generally poor, especially for the recognition task. This reflects the challenges of OCTrackB and\nalso the OCMOT problem, which have great space for improvement. We further compare the results\ngenerated by different settings of OVMOT and OCMOT, by taking the OVTrack and OCTracker\nfor example. We also find that the object recognition performance of OCTracker is invariably lower\nthan OVTrack using either ClsA or mgReA on both base and novel classes. This is reasonable since\nOCTracker no longer requires the (base and novel) class list used in OVTrack. Although the OV-based\nmethods, overall speaking, perform better than OC-based methods, the performance gap between\nthem is not large. This demonstrates that the proposed more practical OCMOT task is very promising.\nAs discussed above, both OVTAO and the proposed OCTrackB use the\nvideos in the TAO dataset. We select the overlapped videos in OVTAO and OCTrackB, and apply the\npublic OVTrack [15] method on them for comparison. As shown in Table 2, we find that although the\noverlapped videos included in OCTrackB represent approximately 41% of the original OVTAO-val\nor OVTAO-burst, the experimental results show negligible differences. The evaluated results on\nboth base and novel categories diverge by no more than 0.6% for TETA, TRETA, when compared\nto the original OVTAO dataset. This comparison demonstrates that the portion of OVTAO dataset\nincluded in OCTrackB is highly representative, containing the data distribution diversity of the\noriginal OVTAO dataset. Besides these videos, OCTrackB also includes the data from LV-VIS. The\nrichness of categories and quantity of samples has been significantly expanded in terms of principles\nP1 and P2, making OCTrackB highly effective and comprehensive for the OCMOT.\nVisualization analysis. Figure 7 presents some visualization results of OCTracker, in which bounding\nboxes with the same color indicate the same track ID, text boxes with a black background display\nthe generated category names (prediction), while the text boxes with a green background show the\nlabels obtained using CLIP for evaluation, and the text boxes with a brown background indicate the\nground-truth labels in the dataset. We can see that OCTracker encapsulates a rich understanding of\nobject categories. For instance, in the first row, OCTracker is not only able to identify the object as a\n'child', but also recognizes it as a 'girl', thereby providing a more comprehensive description of the\ntarget. Importantly, this is achieved without the need for any pre-specified category restrictions. In the\nsecond row of results, the generated output includes the prediction 'grizzly bear', even more specific\nthan the ground truth 'bear'. The third row demonstrates the effectiveness of the proposed mgReA in\nSection 3.5. We can observe that for tracking a specific subclass 'dalmatian' of 'dog', OCTracker\neffectively describes the target's characteristics, such as 'black and white dog'. It can also predict its\nsuper-category 'dog' and accurately identify the subclass 'dalmatian' in certain frames. When the\ntarget is recognized as 'dog', the multi-granularity metric mgReA traces back to the expanded label\n'dog' from the ground-truth label 'dalmatian', effectively addressing the misalignment between the\ngenerated results and GT labels. More visualizations can be found in the supplementary material."}, {"title": "Conclusion", "content": "In this work, we have proposed a novel yet practical problem of OCMOT. We build a large-scale and\ncomprehensive benchmark OCTrackB, to provide the standard evaluation platform for this problem.\nCompared to similar competitor datasets, OCTrackB has the advantage of containing more diverse\nand balanced object categories, and significantly more testing samples for both base and novel classes,\nespecially the novel. Besides the dataset, we also design a new multi-granularity recognition metric\nto alleviate the semantic ambiguity problem for object recognition. Extensive benchmark evaluations\nfor numerous state-of-the-art methods have demonstrated the rationale of the proposed OCMOT\nproblem, and the usefulness of the OCTrackB benchmark."}]}