{"title": "OCTrack: Benchmarking the Open-Corpus\nMulti-Object Tracking", "authors": ["Zekun Qian", "Ruize Han", "Wei Feng", "Junhui Hou", "Linqi Song", "Song Wang"], "abstract": "We study a novel yet practical problem of open-corpus multi-object tracking\n(OCMOT), which extends the MOT into localizing, associating, and recogniz-ing generic-category objects of both seen (base) and unseen (novel) classes, but\nwithout the category text list as prompt. To study this problem, the top priority\nis to build a benchmark. In this work, we build OCTrackB, a large-scale and\ncomprehensive benchmark, to provide a standard evaluation platform for the OC-MOT problem. Compared to previous datasets, OCTrackB has more abundant and\nbalanced base/novel classes and the corresponding samples for evaluation with\nless bias. We also propose a new multi-granularity recognition metric to better\nevaluate the generative object recognition in OCMOT. By conducting the extensive\nbenchmark evaluation, we report and analyze the results of various state-of-the-art\nmethods, which demonstrate the rationale of OCMOT, as well as the usefulness\nand advantages of OCTrackB.", "sections": [{"title": "Introduction", "content": "Multi-object tracking (MOT), which involves detecting and associating the targets of interest in a\nvideo, is a classical and fundamental problem with many real-world applications, such as video\nsurveillance, autonomous driving, etc. Recently, MOT has attracted broad attention with numerous\nalgorithms and datasets [1-6]. For many years, MOT has mainly focused on the target of humans,\ne.g., the datasets of MOT15 [7], MOT20 [8], DanceTrack [9]. Several works also focus on traffic\nscenes and aim to track vehicles, such as the well-known KITTI [10] dataset.\nIn real-world scenes, the categories in videos are diverse, far from being limited to humans and\nvehicles. TAO [11], as the first work, constructs a large-scale benchmark to study tracking any\ncategory of target, with a total of 833 object classes. During the same period, GMOT-40 [12]\nbuilds a generic multi-object tracking benchmark with 10 object classes but more dense objects per\nframe. With the number of categories increasing in the MOT task, the evaluation metrics evolve\nfrom just object localization and association to also include class recognition. A new metric TETA\n(tracking-every-thing accuracy) is proposed [13] to evaluate the generic MOT from the above three\naspects. More recently, open-world MOT (OWMOT) [14] is proposed to train a tracker using the\nsamples from 'base classes', and test it on videos containing objects from 'novel classes'. The\ntracker must recognize the base-class objects and identify all other unseen classes as 'new'. Further,\nopen-vocabulary MOT (OVMOT) [15] aims to not only distinguish the novel-category objects but\nalso classify each object, typically achieved by a pre-trained multi-modal model, e.g., CLIP [16].\nUndoubtedly, the development of MOT from specific-category to generic-category and further to\nopen-world/vocabulary settings is becoming increasingly practical. A remaining problem in the latest\nOVMOT is that, during testing, a predefined category list of base and novel classes is required as the\ntext prompts for the classification task, as shown in Figure 1(a). However, obtaining this list in real"}, {"title": "Related Work", "content": "Multiple Object Tracking (MOT). The dominant approach in MOT is the tracking-by-detection\nframework [18], which initially identifies objects in each frame and then associates them across\nframes using various cues such as object appearance features [19-24, 4, 25], 2D motion features [26-\n30], or 3D motion features [31-36]. Some approaches enhance tracking performance by leveraging\ngraph neural networks [37, 38] or transformers [5, 39, 6, 40] to learn association relationships\namong the objects of different frames. To extend the object categories in the MOT task, the TAO\nbenchmark [11] has been proposed, which handles the MOT under various object categories with a\nlong-tail distribution. Several follow-up works are proposed to evaluate this benchmark including\nAOA [41], GTR [40], TET [42], QDTrack [20], etc. Although these methods perform effectively,\nthey are confined to closed-set object categories, i.e., the object categories in training and testing sets\nare overlapped. This is unsuitable for diverse open-world scenarios with new categories. Differently,\nthis work tracks objects of categories whether or not appearing during training, and generates their\nclasses, which significantly expands the practical application for tracking.\nOpen-World MOT has not been extensively explored. Some existing related works [43, 44] adopt\nthe class-agnostic detectors with general trackers to implement open-world tracking. These methods\nfocus solely on tracking salient objects in the scene without considering specific categories. The\nrecent TAO-OW [45] takes a step further by considering the challenges of classification in open-world\ntracking, dividing all objects into known and unknown categories. In this work, category-aware"}, {"title": "OCTrack Benchmark", "content": "We first provide the problem formulation of OCMOT. Given a video sequence with various objects,\nOCMOT aims to simultaneously achieve the localization, association and recognition tasks, thus\ngenerating a bounding box b = [x, y, w, h], continuous ID number d (along the video) and a category\nc for each target in the video. The annotated object categories appearing during training are defined\nas $C_b$, i.e., the base class set. In testing, we aim to obtain the OCMOT results, i.e., the object category\nset $C_{open}$ is an open corpus. Obviously we have $C_b \\subset C_{open}$, and we define the novel class set as\n$C_n = C_{open} \\backslash C_b$. Note that, we take the category recognition task as a generative task, with no need\nfor the category list of $C_{open}$ as input during testing. Ideally, $C_{open}$ contains all the categories in the\nreal world. In practice, for OCMOT evaluation, we can limit $C_{open}$ to a large-scale thesaurus."}, {"title": "Principle of Benchmark Construction", "content": "To build the OCMOT benchmark (OCTrackB), we first establish the following principles:\nP0: Principle of standardness. Following the base and novel class division mode proposed in LVIS;\nP1: Category enrichment principle. Base/novel classes should be diverse and balanced;\nP2: Sample enrichment principle. Evaluation videos/objects for all classes should be abundant;\nP3: Semantic compatibility principle. The evaluation of object recognition should be compatible.\nThe first principle P0 ensures the base and novel class division in our dataset is consistent with that in\nthe widely used LVIS. This is because that previous works, e.g., many open-vocabulary detection\nmethods [53-57], and the open-vocabulary tracker OVTrack all use LVIS as the training dataset. As a\ntesting dataset, OCTrackB with the same base/novel class division is more convenient for evaluating\nthe algorithms trained on LVIS. Both P1 and P2 guarantee the richness of the dataset, which aims\nto increase the object categories and the sample amount in the dataset. This is significant for the\nopen-corpus tracking task. The last principle P3 aims to address the semantic ambiguity problem,\nwhich stems from two aspects. The first aspect arises from the dataset annotation. Due to the large"}, {"title": "Dataset Collection and Annotation", "content": "By investigating the recent video datasets, we select two large ones with various object categories, i.e.,\nTAO [11] and LV-VIS [58], as the basis for constructing OCTrackB. TAO is a generic-category object\ntracking dataset, with a total of 833 classes and 2,907 videos. LV-VIS is a large-vocabulary video\ninstance segmentation dataset with 1,196 classes and 4,828 videos. Previous work OVTrack [15],\nalso following P0, directly uses TAO's validation and test sets (annotations from BURST [59]) and\nonly maintains the classes that overlap with LVIS for data selection, and form the OVTrack testing\ndatasets, i.e., OVTAO validation (OVTAO-val) and test (OVTAO-burst) sets. The simple category\nintersection operation decreases the number of classes significantly. In this work, we consider the\nadvantages of TAO, which provides longer videos but has a limited number of categories (overlapped\nwith LVIS). On the other hand, the LV-VIS dataset offers a larger number of object categories, which\ncan effectively compensate for the shortcomings of TAO, making it more suitable for addressing the\nOCMOT task. Specifically, we filter the test and validation sets of TAO and the training and validation\nsets of LV-VIS to select videos that meet principle P0. To satisfy P1, we use a greedy algorithm\naiming to minimize the total number of videos while ensuring that each category contains at least two\nvideos whenever possible, thus ensuring category diversity and balance. This results in a selection\nof 903 videos covering 892 categories (also contained in LVIS). To meet P2, we again use a greedy\nalgorithm, aiming to allocate as many tracks as possible for each category while maintaining the same\ntotal number of videos. This further produces 732 videos with 4,766 tracks. In total, we collected\n1,635 videos, of which 496 include novel category objects and 1,600 encompass base categories."}, {"title": "Dataset Statistics and Comparison", "content": "We then show the statistics of OCTrackB and compare it with two existing OVTrack datasets, i.e.,\nOVTAO-val [15] and OVTAO-burst [59]. OCTrackB has the following typical advantages.\nVarious and balanced object categories. OCTrackB contains a total of 892 available categories,\ncomposed of 653 base and 239 novel classes. We show some example classes in OCTrackB in\nFigure 2(a), the categories cover every aspect in the real-world applications, e.g., various transporta-tion, animals, and household items, etc. Note that, following the original class annotation in our"}, {"title": "Evaluation Metrics", "content": "Following [15], we use the open-category tracking metric namely tracking-every-thing accuracy\n(TETA) in [13] for evaluation. TETA is composed of three parts, i.e., object localization, association,\nand classification accuracies. First, the localization accuracy (LocA) is computed through the match-ing of the GT boxes with predicted boxes without considering class, as $LocA = \\frac{|TPL|}{|TPL|+|FPL|+|FNL|}$.\nSecond, association accuracy (AssocA) is determined by matching the identities of associated GT\ninstances with the predicted association, as $AssocA = \\frac{1}{|TPL|} \\sum_{b \\in TPL} \\frac{|TPA(b)|}{|TPA(b)|+|FPA(b)|+|FNA(b)|}$.\nFinally, classification accuracy (ClsA) is calculated using all correctly localized instances, by com-"}, {"title": "A Baseline Method: OCTracker", "content": "1) Localization: As shown in Figure 6, similar to most tracking-by-detection based MOT approaches,\nwe first need to obtain object bounding boxes for each frame. Since our focus is on open-corpus object\ntracking, we aim to localize generic-class objects. We employ the well-known detector Deformable\nDETR [61] as the basic network structure of the localization head. Deformable DETR uses Hungarian\nmatching to map the predicted detections to the ground truths, and then aligns the corresponding\nboxes through category classification loss and bounding box regression loss. In our framework, we do\nnot consider the object class in the localization head, aiming to train a class-agnostic object detector.\nThis way, we replace the category classification loss in [61] with a binary cross-entropy loss, i.e., to\nestimate whether a region candidate is an object of interest or not.\n2) Recognition: The recognition head is used to generate the category name of the object. It mainly\nconsists of a generative language model, for which we use FlanT5-base [62] and initialize it with its\npre-trained weights. The visual features of the candidate objects obtained from Deformable DETR\nare mapped to the input space of the generative model through a projection layer, and then processed\nby a generative encoder and decoder, both composed of self-attention layers and feedforward neural\nnetworks. The encoder's output interacts with the decoder through the cross-attention layers. Then\nthe decoder's output is passed through a softmax layer to predict the corresponding word, while the"}, {"title": "Experimental Results", "content": "As a new problem, there is no approach that can directly handle the OCMOT. We try to include\nas many approaches as possible with necessary modifications for comparison on the proposed\nOCTrackB. First, we select two strong MOT algorithms, i.e., QDTrack [20] and TETer [67]. The\nclassical MOT approaches can not handle the object recognition task in OCTrack, thus we train both\nalgorithms on both base and novel categories in LVIS [17] and TAO [11] training sets by a closed-set\ntraining approach and then evaluate their performance on the OCTrackB. Second, we include\nthe only public open-vocabulary MOT algorithm OVTrack [15] in the experiments, by additionally\ngiving the base and novel class list during testing as the setting of it. Also, to evaluate more\nrelated approaches, we further use the way of approach combination. Specifically, we select an\nopen-vocabulary detection (OVD) algorithm for object localization and recognition (classification)\ncombined with an object tracking method for association, to achieve the OCMOT. We select three\nstate-of-the-art OVD algorithms, i.e., VLDet [55], CoDet [54], MM-OVOD [53], and three tracking\nmethods including the appearance-based tracking method namely DiffuTrack in OVTrack [15],\nmotion-based tracking in ByteTrack [1] and OC-SORT [2]. Note that, these methods also need the\nbase and novel class list for testing. Finally, we employ an open-ended generative object detection\nmethod namely GenerateU [63] as the detector, combined with the above tracking modules, for the\nOCTrack task. This series of methods is really under the setting of OCTrack without the class lists\nwhen testing. The proposed baseline method OCTracker is also included for comparison."}, {"title": "Benchmark Results", "content": "Comparison among state-of-the-art methods. As shown in Table 1, we can see that, the classical\nMOT algorithms QDTrack [20] and TETer [67] provide a satisfied performance on the localization\nand association tasks since these methods are specifically designed for such tasks and they have been\ntrained on both base-class and novel-class data. However, we can see that the object recognition\nresults, i.e., the ClsA score, are very poor. This is because these methods can not handle the diverse\nlong-tailed classification with up to 892 categories in OCTrackB. Also, ClsA metric only considers\ntop-1 classification accuracy, but these classes are fine-grained. From this point, the proposed\nrecognition score mgReA is more reasonable. Then we can see that the open-vocabulary tracking\napproach OVTrack [15] provides relatively good results among all competitors. However, it uses the\nclass list as input during training, the open type is set as OV. Under the same setting of OV, we select\nthree more recent OV detection methods VLDet [55], CoDet [54] and MM-OVOD [53] with three\nclassical tracking strategies for association. Among them, DiffuTrack uses a diffusion model based\ndata hallucination strategy [15] to learn the object similarity for association. ByteTrack [1] applies a"}, {"title": "In-depth Analysis", "content": "Discussion and insights. From Table 1, we can observe that the performances of all the methods\nare generally poor, especially for the recognition task. This reflects the challenges of OCTrackB and\nalso the OCMOT problem, which have great space for improvement. We further compare the results\ngenerated by different settings of OVMOT and OCMOT, by taking the OVTrack and OCTracker\nfor example. We also find that the object recognition performance of OCTracker is invariably lower\nthan OVTrack using either ClsA or mgReA on both base and novel classes. This is reasonable since\nOCTracker no longer requires the (base and novel) class list used in OVTrack. Although the OV-based\nmethods, overall speaking, perform better than OC-based methods, the performance gap between\nthem is not large. This demonstrates that the proposed more practical OCMOT task is very promising.\nDataset comprehensiveness. As discussed above, both OVTAO and the proposed OCTrackB use the\nvideos in the TAO dataset. We select the overlapped videos in OVTAO and OCTrackB, and apply the"}, {"title": "Conclusion", "content": "In this work, we have proposed a novel yet practical problem of OCMOT. We build a large-scale and\ncomprehensive benchmark OCTrackB, to provide the standard evaluation platform for this problem.\nCompared to similar competitor datasets, OCTrackB has the advantage of containing more diverse\nand balanced object categories, and significantly more testing samples for both base and novel classes,\nespecially the novel. Besides the dataset, we also design a new multi-granularity recognition metric\nto alleviate the semantic ambiguity problem for object recognition. Extensive benchmark evaluations\nfor numerous state-of-the-art methods have demonstrated the rationale of the proposed OCMOT\nproblem, and the usefulness of the OCTrackB benchmark."}]}