{"title": "WHAT'S NEW IN MY DATA? NOVELTY EXPLORATION\nVIA CONTRASTIVE GENERATION", "authors": ["Masaru Isonuma", "Ivan Titov"], "abstract": "Fine-tuning is widely used to adapt language models for specific goals, often lever-\naging real-world data such as patient records, customer-service interactions, or\nweb content in languages not covered in pre-training. These datasets are typically\nmassive, noisy, and often confidential, making their direct inspection challeng-\ning. However, understanding them is essential for guiding model deployment and\ninforming decisions about data cleaning or suppressing any harmful behaviors\nlearned during fine-tuning. In this study, we introduce the task of novelty discov-\nery through generation, which aims to identify novel properties of a fine-tuning\ndataset by generating examples that illustrate these properties. Our approach\nContrastive Generative Exploration (CGE) \u2013 assumes no direct access to the data\nbut instead relies on a pre-trained model and the same model after fine-tuning.\nBy contrasting the predictions of these two models, CGE can generate examples\nthat highlight novel characteristics of the fine-tuning data. However, this simple\napproach may produce examples that are too similar to one another, failing to cap-\nture the full range of novel phenomena present in the dataset. We address this by\nintroducing an iterative version of CGE, where the previously generated examples\nare used to update the pre-trained model, and this updated model is then contrasted\nwith the fully fine-tuned model to generate the next example, promoting diversity\nin the generated outputs. Our experiments demonstrate the effectiveness of CGE\nin detecting novel content, such as toxic language, as well as new natural and\nprogramming languages. Furthermore, we show that CGE remains effective even\nwhen models are fine-tuned using differential privacy techniques.", "sections": [{"title": "1 INTRODUCTION", "content": "Fine-tuning pre-trained models on domain-specific datasets is a common practice to adapt language\nmodels for specialized applications. For instance, fine-tuning on web data in a particular language\ncan enable a model to understand that language (Fujii et al., 2024; Etxaniz et al., 2024). Fine-\ntuning on patient records enhances a model's grasp of medical terminology and procedures (Yang\net al., 2022; Thirunavukarasu et al., 2023). Similarly, it is often beneficial to fine-tune language\nmodels on customer-service interaction data to improve the performance of customer-care chatbots.\nBy incorporating novel properties that deviate from pre-training data distribution, language models\nacquire new capabilities that are valuable for specific use cases.\n\nUnderstanding novel properties of the fine-tuning dataset is crucial for model development. For\nexample, if toxic data are discovered, they can be filtered out from the dataset or suppressed by post\nhoc methods, such as prompting (Touvron et al., 2023) or unlearning (Jang et al., 2023). However,\nas real-world data are often massive, noisy, and confidential, we cannot always inspect the data\ndirectly. Fine-tuning frequently relies on real-world data gathered from various sources, such as web\ndata, internal company resources, or even customer-service interactions. Due to the sheer volume\nand complexity of these datasets, manually inspecting their content and identifying novelties is a\ndaunting task. Furthermore, direct access to confidential data, such as medical records or customer\ninteractions, is often restricted even for model developers and data analysts (Garrido et al., 2023;\nSarathy et al., 2023), making direct inspection infeasible."}, {"title": "2 PROBLEM FORMULATION", "content": "Here, we formulate the task of novelty discovery through generation. Suppose we have a pre-trained\nlanguage model, denoted as $\\theta_{pt}$, and a fine-tuned language model, denoted as $\\theta_{ft}$. The pre-trained\nmodel is trained on a large corpus: {$x_1,x_2,...,x_N$} where each example $x_i$ is sampled from a\ncertain data distribution: $x_i \\sim p$. The model is then fine-tuned on a fine-tuning corpus, i.e. the\nset of examples {$x'_1, x'_2, ..., x'_{N'}, y_1, y_2, \u2026\u2026\u2026, y_M$ }, where $x'$ represents an in-distribution example,\nsampled from the same (or similar) distribution as the pre-training corpus: $x' \\sim p$. We assume the\npresence of K distinct novel domains, and the $y_i$ is a novel example sampled from a different distri-\nbution of the k-th domain: $y_i \\sim q_k$, where $q_k \\neq p$ for all $k \\in$ {1, ..., K}. While the assumption\nof distinct domains, as opposed to gradual variations between them, is unlikely to be critical for our\nmethod, it simplifies the metrics used to assess domain coverage in our experiments. For instance, p\ncould be a distribution over English text while $q_k$ corresponds to some other language. We assume a\ncase that the number of novel examples is substantially smaller than that of \u2018in-distribution' exam-\nples: M < N', and the direct inspection of the dataset is not feasible, such as when the fine-tuning\ndataset is too large or is not available due to confidentiality.\n\nOur goal is to detect novel domains in the fine-tuning dataset. As we cannot directly examine\nthe dataset, we need to detect the novel domains using the pre-trained and fine-tuned models by\ngenerating examples characterizing these domains. Since most examples in the fine-tuning dataset\nare in-distribution, simply sampling from $p(\\cdot; \\theta_{ft})$ will predominantly yield in-distribution examples,\nas demonstrated in the experiments. In the following section, we introduce a simple method for\nexploring novelties by using pre-trained and fine-tuned models."}, {"title": "3 CONTRASTIVE GENERATIVE EXPLORATION", "content": "Here, we propose contrastive generative exploration (CGE), a method to generate novel examples\nthat are divergent from the pre-training data distribution but are present in the fine-tuned dataset.\nWe will begin by introducing a simpler static version, before describing the iterative version, which\naims to maximize the coverage of novel domains."}, {"title": "3.1 STATIC APPROACH", "content": "To generate novel examples, we employ contrastive decoding for the pre-trained and fine-tuned\nmodels. As shown in Equation 1, contrastive decoding samples a text based on the contrastive score,\ns, which is calculated as the difference between the log probabilities computed by the two models."}, {"title": "3.2 ITERATIVE APPROACH", "content": "One shortcoming of the static version of CGE is its tendency to generate similar examples (e.g.,\nfrom the same language), even though our goal is to capture a broader variety of novel examples.\nTo address this limitation, we introduce an iterative version of CGE to diversify the generated novel\nexamples. After generating a sequence of tokens using contrastive decoding, we fine-tune the pre-\ntrained model on this generated sequence, allowing the pre-trained model to adapt to the generated\nsequence. This adaptation prevents the generation of examples similar to those already generated,\nand contrastive decoding yields new and distinct examples in subsequent iterations. By repeating\nthis process, we encourage CGE to search for new, previously undetected novelties."}, {"title": "3.3 RELATION TO DATASET DISTILLATION", "content": "Dataset distillation aims to produce a small set of synthetic examples such that training on this set\nyields a model that is as similar as possible to that trained on the full dataset (Wang et al., 2018; Yu\net al., 2023; Sachdeva & McAuley, 2023). Several works have explored this goal through gradient\nmatching (Zhao et al., 2020; Zhao & Bilen, 2021). Gradient matching obtains synthetic dataset x\nby ensuring that its gradient matches the changes in the model parameters resulting from training\non the original dataset. Let $\\theta$ be the model parameters to be trained and $\\theta^*$ be the model parameters\ntrained on the original dataset. The objective of gradient matching is described as Equation 6:"}, {"title": "4 EXPERIMENTS", "content": "In this section, we evaluate the effectiveness of CGE in detecting novel examples in fine-tuning\ndatasets. We first evaluate whether the contrastive score effectively distinguishes between novel\nexamples and in-distribution examples compared to existing novelty (a.k.a. out-of-distribution) de-\ntection methods. We then verify that CGE can generate novelties from fine-tuned models."}, {"title": "4.1 MODELS AND DATASETS", "content": "We conducted our experiments using two pre-trained language models: OpenLLaMA (Geng & Liu,\n2023) and Falcon-RW (Almazrouei et al., 2023).\n\nOpenLLaMA We used OpenLLaMA-3B,\u00b9 an open reproduction of LLaMA (Touvron et al.,\n2023). OpenLLaMA uses exactly the same decoder-only architecture, preprocessing steps, and\nhyperparameters as the original LLaMA, while being pretrained on 1T tokens from the publicly\navailable RedPajama dataset (Computer, 2023).\n\nWe constructed two fine-tuning datasets where 90% of the examples were sampled from the Red-\nPajama pre-training dataset and 10% consisting of either non-English languages or toxic content.\nNon-English examples introduce linguistic diversity, while toxic content poses safety risks. We\nevaluate how well CGE identifies both types of novelties.\n\nTo obtain non-English text, we used Wikipedia articles in 10 languages: Japanese, Chinese, Persian,\nArabic, Hebrew, Turkish, Indonesian, Korean, Vietnamese, and Thai. These language articles are\nnot contained in the RedPajama dataset. Each language comprises 1% of the fine-tuning dataset,\nwhere the total number of examples is 190,000, each consisting of 1,024 tokens. As for toxic text,\nwe used ToxiGen (Hartvigsen et al., 2022),4 containing machine-generated toxic language against\n10 minority groups. Here, we consider a more extreme setting compared to non-English languages,\n\nFalcon-RW We used Falcon-RW-1B,5 a decoder-only model pre-trained on the RefinedWeb\ndataset (Penedo et al., 2023). RefinedWeb comprises English web text derived from CommonCrawl,\nexcluding non-English text and common online sources, such as Wikipedia and Github.\n\nIn contrast to OpenLLaMA, we designed a more practical and challenging scenario for Falcon-\nRW, where in-distribution examples in the fine-tuning dataset are from the same distribution as the\npre-training dataset (English text) but are not directly sourced from the pre-training corpus. This\nsetup is more realistic, as fine-tuning is typically conducted on datasets that differ entirely from\nthe pre-training data. However, this also makes it more challenging to detect novel examples. We\nconstructed two fine-tuning datasets consisting of 90% English Wikipedia articles and 10% non-\nEnglish Wikipedia articles or source code from GitHub. Since RefinedWeb does not contain data\nsourced from Wikipedia and Github, most fine-tuning examples had not been seen during the pre-\ntraining of Falcon-RW.\n\nFor non-English languages, we used the same non-English Wikipedia articles as in the OpenLLaMA\nexperiment. Each language comprised 1% of the fine-tuning dataset, totaling 100,000 examples,\neach consisting of 1,024 tokens. As for source code, we used the GitHub Code dataset and selected\nsource code of 10 programming languages: JavaScript, Java, C, Python, Ruby, TypeScript, Shell,\nGO, SQL, and Perl. Each language accounted for 1% of the fine-tuning dataset, with the total\nnumber of examples being 100,000, each comprising 1,024 tokens. The fine-tuning procedure for\nFalcon-RW mirrored that of OpenLLaMA, using the same optimizer and hyperparameters."}, {"title": "4.2 EXTRACTION OF NOVEL EXAMPLES", "content": "In this section, we first examine whether the contrastive score favors novel examples that are diver-\ngent from the pre-training data distribution, while penalizing in-distribution examples. Compared to\nexisting methods, we show that the contrastive score performs robustly in detecting novel examples.\n\nBaseline Methods We compare our approach to several well-known OOD detection methods us-\ning pre-trained models: MSP (Hendrycks & Gimpel, 2017), Energy (Liu et al., 2020), and Grad-\nNorm (Huang et al., 2021). As these methods are label-free, we also employ methods using labels\n(next tokens for language modeling). NegativeProbpt computes the negative log-probability of to-\nkens by the pre-trained models, corresponding to the second term of the contrastive score. Probft\ncomputes the log-probability of tokens by the fine-tuned model, corresponding to the first term of\nthe contrastive score. GradNormpt measures the L2-norm of gradient w.r.t. the pre-trained model,\nreflecting that the gradient of examples that align with pre-training data distribution becomes less\nsteep after pre-training. Details of the baseline methods can be found in Appendix A.1.\n\nMetrics Following previous studies on OOD detection (Liu et al., 2020; Huang et al., 2021), we\nuse AUROC (Area Under the Receiver Operating Characteristic curve) and FPR95 (False Positive\nRate at 95% True Positive Rate) to evaluate our method's effectiveness in detecting novel examples.\nAUROC measures the performance to distinguish between in-distribution and novel examples, while\nFPR95 focuses on the model's reliability when aiming for a high true positive rate.\n\nResults Table 1 (top) shows the performance of each method for detecting novel examples in the\nfine-tuning datasets of OpenLLaMA. Across both datasets, the contrastive score consistently detects\nnovelties with high accuracy. For toxic text, pre-trained language models typically assign a low\nprobability, which results in a strong performance by NegativeProbpt and other baseline methods.\nHowever, non-English texts do not necessarily receive lower probabilities compared to standard\nEnglish texts. Since many non-English characters are composed of multiple byte-level tokens, some"}, {"title": "4.3 GENERATION OF NOVEL EXAMPLES", "content": "In this section, we assess our method, CGE, in the task of novelty discovery through generation,\nwhere we aim to identify novel properties of a fine-tuning dataset by generating examples that illus-\ntrate these properties. We demonstrate that CGE can discover a wide variety of novel characteristics\nthat are hardly detected by simply sampling from the fine-tuned model.\n\nExperimental Setup In our experiment, we generated 100 texts by each method and evaluated\nthem using two metrics: detection and coverage rate. Detection rate represents the percentage of\ngenerated texts that are identified as novel examples. A higher detection rate indicates that the\nmethod is more effective at generating novelties. Coverage rate measures how well the generated\ntexts cover novel examples across different domains. As previously explained, non-English lan-\nguages, toxic texts, and source code are each categorized into 10 distinct domains. The coverage\nrate reflects the number of different domains that are represented in the generated texts.\n\nTo assess the content of the generated texts, we used the instruction-tuned LLaMA 3 (70B) model\n(Dubey et al., 2024). We evaluated whether the texts were toxic, non-English, or programming lan-\nguages, and further classified them into appropriate domains. The prompts used for the evaluation\nare shown in Appendix A.2. Using the fine-tuning dataset, we evaluated the classification perfor-\nmance of LLaMA 3. The model was able to detect toxic text with 99.1% accuracy and classify the\ntarget group of toxic text with 95.5% accuracy. For non-English text, LLaMA 3 achieved 100%"}, {"title": "4.4 EFFECTIVENESS FOR DIFFERENTIALLY PRIVATE FINE-TUNED MODELS", "content": "In this section, we demonstrate that CGE is also effective for models fine-tuned with differential\nprivacy (DP) techniques. DP techniques are frequently used to protect sensitive data from privacy\nattacks, such as training data reconstruction or membership inference. Moreover, in practical deploy-\nments of DP, model designers and data analysis often lack access to the underlying data, rendering\nstandard data analysis techniques infeasible (Garrido et al., 2023; Sarathy et al., 2023). While DP\ntraining reduces memorization, which poses additional challenges for gradient-based concept explo-\nration (CGE), we demonstrate that CGE can still uncover novel features from fine-tuned models.\n\nExperimental Setup We employ DP-Adam, a variant of DP-SGD (Song et al., 2013; Bassily et al.,\n2014; Abadi et al., 2016), which is widely used for DP fine-tuning and has been applied to language\nmodels in prior studies (Yu et al., 2022; Li et al., 2022). DP-Adam perturbs the gradients of training\nexamples by clipping the per-example gradient norm and adding Gaussian noise, reducing the in-\nfluence of individual training examples on the fine-tuned model. We fine-tuned a pre-trained model\nusing DP-Adam, adjusting the strength of the Gaussian noise by setting different noise multipliers.\nWe then assessed how the detection and coverage rates change as the noise multiplier increases.\n\nAs Yu et al. (2022) demonstrated, parameter-efficient fine-tuning methods, such as Low-Rank Adap-\ntation (LoRA; Hu et al., 2022), are more effective than updating all model parameters during DP fine-\ntuning. Following this study, by combining DP-Adam with LoRA, we fine-tuned OpenLLaMA on\nthe RedPajama dataset augmented with non-English texts, and Falcon-RW on the English Wikipedia\ndataset augmented with source code. We injected trainable LoRA matrices into key, query, value,\nand linear transformation layers in the self-attention block. The intermediate representation dimen-\nsion is set to r = 8 with a scaling factor of a = 16, and the model is fine-tuned for three epochs\nwith a learning rate of 5e-4. For DP, we set the privacy budget 8 to 1/n, where n is the size of the\nfine-tuning dataset, and adjusted the noise multiplier to 0.0, 0.1, 0.2, 0.4, and 0.8.\n\nResults Figure 3 presents the change in detection and coverage rate across different noise multi-\npliers. We generated 100 texts using CGE and evaluated the generated texts as described in Section\n4.3. Introducing DP led to a decline in the detection rate, though the impact was not substantial\neven with higher noise multipliers. Similarly, DP had a marginal impact on the coverage rate, which\nremained above 60% for both models. These findings suggest that our methods can reliably uncover\nnovel examples even when models are fine-tuned with DP techniques."}, {"title": "5 RELATED WORK", "content": "Dataset Exploration Exploring the properties of datasets is a crucial step in model development.\nPrior work has mainly focused on providing methods and tools to directly inspect datasets.\n\ndetection techniques use trained models to identify novel examples that deviate from training\ndata distribution (Lee et al., 2018; Yang et al., 2024). For instance, the maximum softmax score\n(Hendrycks & Gimpel, 2017) and its extension (Liang et al., 2018; Hsu et al., 2020) detect novel\nexamples by identifying low-confidence predictions. Likewise, Liu et al. (2020); Huang et al. (2021)\nleverage energy functions or gradient norms to detect novelties effectively.\n\nAnother research direction focuses on improving dataset transparency. Piktus et al. (2023a;b); Elazar\net al. (2024) offer tools to inspect large text corpora, enabling users to identify potential data con-\ntamination or biases by directly accessing and querying the training data. Similarly, Marone &\nVan Durme (2023); Zhou et al. (2024) have developed fast, space-efficient querying systems and\ncustomizable rule-based methods for filtering and optimizing training data.\n\nOur work addresses real-world scenarios where fine-tuning is conducted on massive, noisy, and\nconfidential datasets, making direct inspection impractical. We focus on problems where we aim to\ninfer dataset properties by analyzing a model's behavior without direct access to the data. Recent\nworks (Shi et al., 2024b; Golchin & Surdeanu, 2024) have introduced a similar task, where they\ndetect data contamination by examining a model's outputs without dataset access. Aligning with\nthese works, we introduced a novel task, which aims to identify novel examples in a fine-tuning\ndataset that deviates from the pre-training data distribution without dataset access.\n\nContrastive Decoding Contrastive decoding is a method for generating text that highlights dif-\nferences between the predictions of two models: an expert model (e.g., a large model or non-toxic\nmodel) and an amateur model (e.g., a small model or toxic model). The objective is to generate\ntext favored by the expert model while simultaneously discouraging the preferences of the amateur\nmodel. The utility of contrastive decoding and its variants have been demonstrated in various appli-\ncations, such as ensuring the safety of the generated text (Liu et al., 2021; Xu et al., 2024; Shi et al.,\n2024a; Zhong et al., 2024), improving the quality of generation (Li et al., 2023; O'Brien & Lewis,\n2023), or instruction tuning (Liu et al., 2024; Gao et al., 2024).\n\nThis work extends the use of contrastive decoding to explore novel features within fine-tuning\ndatasets. By contrasting the fine-tuned model against the pre-trained model, our method identifies\nsequences that illustrates novelties in the fine-tuning data. We also introduced an iterative version\nthat could be beneficial in other scenarios where contrastive decoding is applied.\n\nDataset Distillation Dataset distillation is a technique aimed at creating a small, representative\nsynthetic dataset that retains the core properties of a much larger dataset. While most methods were\ndeveloped for image classification tasks, recent efforts have explored their application in text clas-\nsification. Li & Li (2021); Sucholutsky & Schonlau (2021); Maekawa et al. (2023; 2024) have ex-\ntended dataset distillation to text classification tasks, despite the complexity of dealing with discrete\nsequence data. However, these methods often face challenges, such as the cost of calculating second-\norder derivatives, making them less scalable for larger models. Furthermore, these works only con-\nsider text classification datasets and have difficulty being used for language modeling datasets.\n\nCGE is closely related to dataset distillation, but shifts focus toward discovering novelties. With the\nfirst-order approximation, CGE can be reduced to a form of dataset distillation, but with significantly\nlower computational cost. Our method can be applicable to language modeling datasets, and the dis-\ntilled dataset consists of interpretable text. It also has the potential to serve as a dataset compression\ntechnique, aiming to create a smaller training corpus that resembles a large-scale corpus."}, {"title": "6 CONCLUSION", "content": "In this paper, we introduced the task of novelty discovery through generation, which aims to identify\nnovel properties in a fine-tuning dataset without having direct access to the data. As a simple so-\nlution to this task, we proposed Contrastive Generative Exploration (CGE), which uncovers novel\nproperties in fine-tuning datasets by generating examples that represent these properties. Our ex-"}, {"title": "A APPENDIX", "content": null}, {"title": "A.1 BASELINE METHODS FOR EXTRACTION SETTING", "content": "The following methods are used as the baseline methods for the experiments in the extraction setting.\nHigher scores indicate an example is more likely to be novel, while lower scores suggest the example\nis in-distribution."}, {"title": "A.2 EXPERIMENTAL DETAILS", "content": "Table 3 shows the prompts used for the evaluation of generated texts by using LLaMA 3.8 Given\nthe promt and the generated text, the probability of each domain is computed. The domain with the\nhighest probability is selected as an answer."}, {"title": "A.3 HYPERPARAMETER SENSITIVITY", "content": "Table 4 shows the performance of CGE on discovering novel examples when varying the hyper-\nparameters. The trend does not change significantly with different hyperparameters. The static\nversion consistently achieves a high detection rate, while the iterative version improves the coverage"}]}