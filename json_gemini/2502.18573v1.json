{"title": "FACTREASONER: A Probabilistic Approach to Long-Form Factuality Assessment for Large Language Models", "authors": ["Radu Marinescu", "Debarun Bhattacharjya", "Junkyu Lee", "Tigran Tchrakian", "Javier Carnerero Cano", "Yufang Hou", "Elizabeth Daly", "Alessandra Pascale"], "abstract": "Large language models (LLMs) have demonstrated vast capabilities on generative tasks in recent years, yet they struggle with guaranteeing the factual correctness of the generated content. This makes these models unreliable in realistic situations where factually accurate responses are expected. In this paper, we propose FactReasoner, a new factuality assessor that relies on probabilistic reasoning to assess the factuality of a long-form generated response. Specifically, FactReasoner decomposes the response into atomic units, retrieves relevant contexts for them from an external knowledge source, and constructs a joint probability distribution over the atoms and contexts using probabilistic encodings of the logical relationships (entailment, contradiction) between the textual utterances corresponding to the atoms and contexts. FactReasoner then computes the posterior probability of whether atomic units in the response are supported by the retrieved contexts. Our experiments on labeled and unlabeled benchmark datasets demonstrate clearly that FactReasoner improves considerably over state-of-the-art prompt-based approaches in terms of both factual precision and recall.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have achieved impressive improvements and demonstrated vast capabilities in recent years (Brown et al., 2020; Chowdh-ery et al., 2023), however they still struggle to guarantee the factual accuracy of the generated content. Specifically, LLMs often hallucinate, namely they produce factual errors in which a claim contradicts well-established ground-truth knowledge (Zhang et al., 2023; Sahoo et al., 2024; Huang et al., 2025). This makes the models unreliable in realistic situations that require factually accurate LLM-generated responses (Tonmoy et al., 2024).\nMost modern approaches for assessing the factuality of LLM-generated long-form responses such as FactScore (Min et al., 2023), VeriScore (Song et al., 2024) and others (Wei et al., 2024; Bayat et al., 2025) are prompt-based approaches and consist of three main stages: 1) the response is decomposed into a set of atomic units (facts or claims) which are subsequently revised or decontextualized to make them self-contained; 2) relevant evidence (or context) is retrieved for each atomic unit from an external knowledge source such as Wikipedia, and 3) each atomic unit is evaluated against the retrieved context to determine whether it is supported (factually correct) or not and a factuality score is calculated for the response. These approaches sometimes struggle due to conflicting information between the model's internal knowledge and conflicting information within the retrieved contexts themselves. Therefore, they typically assume that the pieces of information retrieved do not conflict or overlap with each other (Min et al., 2023).\nContributions: In this paper, we provide a new perspective on long-form factuality assessment that departs from the prompt-based approach, especially in the evaluation stage of the assessment. Specifically, we propose a novel factuality assessor called FactReasoner that also decomposes the response into atomic units and retrieves the relevant contexts for them from an external knowledge source. However, instead of prompting another LLM to evaluate the atoms against the retrieved evidence, FactReasoner computes the probability of each atom being supported by reasoning over a graphical model that represents a joint probability distribution over the atoms and the retrieved contexts. The graphical model is constructed using probabilistic encodings of the entailment and contradiction relationships between the natural language utterances corresponding to the atoms and contexts. Furthermore, FactReasoner makes no assumptions regarding the existence of any conflicting information within the retrieved contexts."}, {"title": "2 Background", "content": "In this section, we provide preliminaries on probabilistic graphical models and long-form factuality assessment for large language models.\nGraphical Models. A graphical model is a tuple M = (X,D,F), where X = {X1,..., Xn} is a set of variables, D = {D1,...,Dn} is the set of their finite domains of values and F = {f1,..., fm} is a set of discrete positive real-valued functions. Each function fi (also called factor) is defined on a subset of variables S\u00bf \u2286 X called its scope and denoted by vars(fi). The model M defines a factorized probability distribution on X: P(x) =  1/Z \u03a0i=1 fm(x) where Z = \u03a3x\u2208\u03a9(X) \u03a0i=1 f(x) is the normalization constant Z is known as the partition function and \u03a9(\u03a7) denotes the Cartesian product of the variables domains (Koller and Friedman, 2009).\nA common inference task over graphical models is to compute the posterior marginal distributions over all variables. Namely, for each variable Xi \u2208 X and domain value xi \u2208 Di, compute: P(xi) = \u03a3x\u2208\u03a9(\u03a7) \u03b4\u03b1\u03af (x)\u00b7 P(x), where dx\u2081 (x) is 1 if X\u00bf is assigned xi in x and 0 otherwise.\nLong-Form Factuality. Let y be the long-form response generated by an LLM to a query x. Following prior work (Min et al., 2023; Song et al., 2024; Wei et al., 2024), we assume that y can be decomposed into a set of n atomic units (or atoms) that can be either true or false, denoted by Ay = {a1, a2,... an}. An atomic unit a\u017c \u2208 Ay is defined as a short sentence conveying one piece of information. Furthermore, given an external knowl-"}, {"title": "3 The FactReasoner Assessor", "content": "In this section, we present FactReasoner, a novel long-form factuality assessor that uses probabilistic reasoning to assess the factuality of the generated response with respect to an external knowledge source C. Specifically, FactReasoner builds a graphical model that represents a joint probability distribution over the atoms of the response and their relevant contexts in C, and subsequently computes for each atom a\u017c the posterior marginal probability distribution P(ai) representing the probability of ar being true (or supported) with respect to the information available in C.\n3.1 A Graphical Models Based Approach\nLet y be the long-form response generated by an LLM for the input query x, and let Ay = {a1,..., an} be the set of n atomic units corresponding to y. For simplicity, but without loss of generality, we restrict ourselves to atomic units that are either facts or claims (Song et al., 2024). In addition, let Cy = {C1,...,Cm} be a set of m contexts relevant to y's atoms that were retrieved from an external knowledge source C. We make no assumptions about these contexts, namely they may be overlapping and/or contradicting each other, which is often the case in realistic situations.\nWe next define the graphical model (X, D, F) that represents a joint probability distribution over the atoms and their corresponding contexts.\nVariables. We associate each atom ai \u2208 Ay and context cj \u2208 Cy with a bi-valued variable de-"}, {"title": "3.2 Inference and Factuality Assessment", "content": "The graphical model M = (X, D, F) we just defined in the previous section represents a joint probability distribution over the set of atoms and relevant externally retrieved contexts. Therefore, we can use any probabilistic inference algorithm to compute the posterior marginal distribution P(Ai) for each atom Ai \u2208 Ay (Pearl, 1988; Koller and Friedman, 2009). Specifically, in our experiments, we use an approximate variational inference algorithm called Weighted Mini-Buckets (Liu and Ihler, 2011) to compute the marginals.\nThe number of supported atomic units S(y) in a response y can be computed in this case as: S(y) = \u03a3i=1n[P(ai) > P(\u00acai)], namely it is the number of atoms for which the probability of being true is larger than the probability of being false.\nIn addition to the factual precision Pr(y) and F\u2081@K measures, we define a new entropy-based factuality measure called E(y) that leverages the posterior probabilities of response y's atoms:\nE = 1/n \u03a3i=1n -P(ai) \u00b7 log P(ai) (1)"}, {"title": "3.3 The FactReasoner Pipeline and Variants", "content": "The proposed FactReasoner pipeline for long-form factuality assessment is shown in Figure 3 and consists of four main stages called Atomizer, Reviser, Retriever and Evaluator, respectively. It takes as input a response y and outputs the marginal posterior probabilities P(ai) of y's atomic units together with the factuality measures described earlier, such as Pr(y), F\u2081@K(y) and E(y), respectively.\nThe Atomizer prompts an LLM to decompose the response y into a set of n atomic units Ay by applying any of the decomposition strategies proposed recently (Min et al., 2023; Bayat et al., 2025). Subsequently, the Reviser also uses an LLM to revise the atoms such that the pronouns, unknown entities, or incomplete names are replaced with their corresponding named entities in the response (Wei et al., 2024). Next, the Retriever is responsible for querying an external knowledge source to retrieve the contexts relevant to the response's atoms. At this stage, we can simply use the atoms' utterances as queries or prompt an LLM to generate them (Song et al., 2024). Finally, the Evaluator constructs the probabilistic graphical model representing the logical relationships between the atoms and contexts, and assess y's factuality via probabilistic reasoning, as described previously.\nDepending on what relationships between atoms and contexts are considered, we define three versions of the FactReasoner pipeline, as follows:\nFactReasoner 1 (FR1). In this case, for each atom variable A\u017c up to k most relevant contexts {Ci,..., Ck} are retrieved and only the relationships between atom Ai and its corresponding contexts are considered, namely only the factors"}, {"title": "4 Experiments", "content": "In this section, we empirically evaluate our proposed FactReasoner assessor for long-form factuality and compare it against state-of-the-art approaches on labeled and unlabeled datasets. Although the FactReasoner pipeline stages can be instantiated with different LLMs, in our implementation we use the same LLM throughout the entire pipeline and focus our empirical evaluation on the Evaluator stage (i.e., factuality assessment).\nBaseline Assessors. For our purpose, we consider the following state-of-the-art prompt-based long-form factuality assessors: FactScore (FS) (Min et al., 2023), FactVerify (FV) (Bayat et al., 2025) and VeriScore (VS) (Song et al., 2024). FactScore is one of the first assessor that prompts an LLM to assess whether an atomic unit of the response is supported or not by a set of contexts relevant to the atom which are retrieved from an external knowledge source such as Wikipedia. FactVerify and VeriScore are more recent refinements of FactScore's original prompt that can accommodate other external knowledge sources such as Google Search results and enable the LLM's reasoning capabilities to evaluate the relationships between an atom and its relevant contexts. Unlike FactScore, the latter can label the atoms as supported, contradicted and undecided, respectively. In our experiments, we instantiated the competing assessors including the FactReasoner variants with open-source LLMs belonging to the IBM Granite\u00b3, Meta"}, {"title": "4.1 Evaluating the Relation Model", "content": "We first evaluate the relation model used by the Evaluator stage of the FactReasoner assessor to extract the atom-context and context-context relationships required to construct the graphical model. Specifically, we consider two relation models based on a standard BERT-based model such as vitc (Schuster et al., 2021) and on a larger LLM such as"}, {"title": "4.2 Results on Labeled Datasets", "content": "Table 4 shows the results obtained on the labeled Biographies dataset using Wikipedia retrieved contexts (the best performance is highlighted). We see that in terms or mean absolute error (MAE),"}, {"title": "4.3 Results on Unlabeled Datasets", "content": "Tables 5 and 6 show the results obtained on the unlabeled AskH dataset using Wikipedia and Google Search retrieved contexts, respectively (we include in the the Appendix the experiments with the remaining datasets: Books, ELI5 and LFObj). Since there is no ground truth for this dataset, we only report the precision, F\u2081@K (for K = 22) and the E measure. However, for reference, we also experimented with DeepSeek-v3, perhaps one of the strongest open models at the moment, using a suitable prompt (DeepSeek-AI, 2024).\nWe note that the AskH dataset covers a wider"}, {"title": "5 Related Work", "content": "The assessment of LLMs' adherence to factual knowledge has gained significant attention in recent years due to their widespread adoption. Several well-established benchmarks, including TruthfulQA (Lin et al., 2022), FreshQA (Vu et al., 2023),"}, {"title": "6 Conclusion", "content": "The paper provides a new perspective on long-form factuality assessment and proposes FactReasoner, a new factuality assessor that employs probabilistic reasoning to assess the factuality of an LLM-generated long-form response. FactReasoner proceeds in a manner similar to existing prompt-based assessors by decomposing the response into atomic units and retrieving contexts relevant to them from an external knowledge source. However, unlike those methods, FactReasoner evaluates the factuality of the atoms by probabilistic reasoning over a graphical model that represents the logical relationships between the textual utterances corresponding to the atoms and contexts. We experiment with labeled and unlabeled benchmark datasets and demonstrate conclusively that FactReasoner improves significantly over the state-of-the-art prompt based approaches for long-form factuality evaluation. For future work, we plan to leverage the new FactReasoner assessor as part of a self-reflection loop to facilitate correction of the response."}, {"title": "Limitations", "content": "We acknowledge further limitations of the proposed FactReasoner approach.\nFirst, the Atomizer stage is sensitive to the quality of the prompt and few shot examples used as well as the LLM employed to perform the atomic unit decomposition of the response. In our work we only consider open-source models from the LLaMA family (i.e., llama-3.3-70b-instruct). Furthermore, the decomposition of the response can be done at different granularities such as sentence level, paragraph level and the entire response level. Our implementation is limited to decomposing the entire response in one shot.\nSecond, the Reviser stage is also sensitive to how well the prompt is crafted as well as the quality of the few shot examples included in the prompt. Again, at this stage we only used the llama-3.3-70b-instruct model.\nThird, the quality of the contexts retrieved for each atomic unit depends on the implementation of the retriever used as well as the structure of the query string that it receives. Our implementation is limited to off-the-shelf retrievers such as the one available from LangChain and we used the atomic unit's utterance as query. It is possible to prompt an LLM to generate better quality queries as suggested in previous work (Song et al., 2024). Therefore, employing a more advanced retriever will lead to better quality retrieved contexts and consequently will improve the overall performance of the proposed FactReasoner assessors.\nFourth, extracting the logical relationships between atoms and contexts as well as between the contexts themselves also depends on the quality of the prompt and the LLM. As before, for our relation model we only used open-source models such as granite-3.0-8b-instruct, llama-3.1-70b-instruct, and mixtral-8x22b-instruct with a fairly straightforward prompt. It is possible to craft better prompts that could lead to a better extraction of the relationships. Fine-tuning is another option to obtain a stronger relation model.\nFinally, from a computational overhead perspective, the FR3 version requires O(n \u00b7 m + m\u00b2) prompts to extract the relationships between atoms and context, the FR2 version requires O(nm) prompts while FR1 requires O(kn) prompts, where n is the number of atomic units, m is the total number of non-duplicated contexts retrieved for the atoms, and k is maximum number of contexts"}, {"title": "Ethical Statement", "content": "We recognize the positive and negative societal impacts of LLMs in general, including potential misuse of our work around uncertainty quantification for LLM generated output. We note that the datasets considered are public and peer reviewed, there are no human subjects involved, and as far as we know, there are no obvious harmful consequences from our work. All creators and original owners of assets have been properly credited and licenses and terms of use have been respected. We have not conducted crowd-sourcing experiments or research with human subjects."}, {"title": "A Details on Graphical Models", "content": "Graphical models such as Bayesian or Markov networks provide a powerful framework for reasoning\nP(x) = 1/Z \u03a0j=1^m fj(x) s.t. Z = \u03a3x\u2208\u03a9(X) \u03a0j=1^m fj(x)\nP(xi) = \u03a3x\u2208\u03a9(X) \u03b4x\u1d62(x) \u2022 P(x)"}, {"title": "B Details on Long-Form Factuality Assessment", "content": "Assessing the factuality of long form text generations is a challenging problem because these kinds of generations may contain a large number of informative statements and validating each piece of information against one or more reliable sources may be time-consuming, costly and often prone to errors (Min et al., 2023; Wei et al., 2024).\nFormally, let y be the long form text generated by a large language model L in response to a query x. Following prior work (Min et al., 2023; Song et al., 2024), we assume that y consists of n atomic units (or atoms) that can be either true or false, denoted by Ay = {a1,a2,...an}. An atomic unit ai \u2208 Ay is defined as a short sentence conveying one piece of information. Furthermore, given an external knowledge source C8, we say that an atomic unit ai \u2208 Ay is supported by C if there exists at least one piece of information in C (e.g., a passage) called a context that undebatably supports ai. Otherwise, we say that the atomic unit is not supported (Min et al., 2023; Song et al., 2024).\nTherefore, the factual precision Pr(y) of the response y with respect to a knowledge source C is defined as:\nPr(y) = S(y)/|Ay| (5)\nRK(y) = min (S(y)/K, 1) (6)\nF\u2081@K(y) = { 2\u00b7 Pr(y) \u00b7 RK(y) / Pr(y) + RK(y), S(y) > 0, 0, S(y) = 0 (7)"}, {"title": "C Additional Experiments", "content": ""}, {"title": "D Prompts", "content": "Tables 18, 19 and 20 show the prompt templates we used for the Atomizer, Reviser and Evaluator"}]}