{"title": "SlideSpawn: An Automatic Slides Generation System for Research Publications", "authors": ["Keshav Kumar", "C. Ravindranath Chowdary"], "abstract": "Research papers are well structured documents. They have text, figures, equations, tables etc., to covey their ideas and findings. They are divided into sections like Introduction, Model, Experiments etc., which deal with different aspects of research. Characteristics like these set research papers apart from ordinary documents and allows us to significantly improve their summarization. In this paper, we propose a novel system, SlideSpwan, that takes PDF of a research document as an input and generates a quality presentation providing it's summary in a visual and concise fashion. The system first converts the PDF of the paper to an XML document that has the structural information about various elements. Then a machine learning model, trained on PS5K dataset and Aminer 9.5K Insights dataset (that we introduce), is used to predict salience of each sentence in the paper. Sentences for slides are selected using ILP and clustered based on their similarity with each cluster being given a suitable title. Finally a slide is generated by placing any graphical element referenced in the selected sentences next to them. Experiments on a test set of 650 pairs of papers and slides demonstrate that our system generates presentations with better quality.", "sections": [{"title": "1 INTRODUCTION", "content": "PRESENTATION slides have been one of the most popular tools to aid speeches, lectures, seminars since their ad- vent. They have been widely used in the scientific research communities across the world. Researchers often make use of the slides to present their work pictorially in conferences. Creating a presentation from a reference paper is a fairly popular and a time consuming task. It involves listing the important points based on one's understanding of the paper, organising them in a sensible order and placing the figures, equations and tables at appropriate locations. With the advances in OCR, document parsing and extractive sum- marization, this task can be easily automated. Presentations can be considered to be a union of textual and graphical elements. So the task of automatically generating slides can be divided into 4 sub tasks.\n1) Parsing the research document to make use of all the element and their positional information.\nSelecting the important textual elements.\n3) Selecting the important graphical elements.\n2)\n4) Placing these elements at appropriate locations in the presentation.\nIn our system, we use a tool called GROBID [1] [2] that leverages AI to parse PDF documents and convert them into XML capturing all the elements along with their structural organization. This tool has a special focus on research publi- cations which makes it perfect for our first sub task. To select important textual elements, we use extractive summariza- tion, for this we employ a Multi Layer Perceptron, trained on our dataset (Section 3.1), that tries to predict a salience score for each sentences, We then use ILP to select sentences with highest salience score subject to constraints which ensure that the presentation covers of maximum content of the paper, Finally we cluster the selected sentences into groups and give each group a suitable title by finding a noun phrase that occurs in the group and is semantically close to each sentence. To select graphical elements for our slide, we scan each selected sentence for the PPT for any references to a figure, table or equation, upon finding any, we indicate the element's label in the slide next to the sentence. Finally, we place the sentences within a cluster by sorting them with respect to their position in the paper and sort the cluster with respect to the position of the first sentence in the cluster. We use Google Colaboratory for all our text processing and training tasks. The main contributions of the paper are the following:\n1) To the best of our knowledge, we are the first one to use semantic similarity based on sentence vectors to measure a sentence salience score for slides genera- tion.\n2) We propose an end to end system, SlideSpawn, to generate presentations from a reference paper by leveraging it's rich structure.\n3) We release a new dataset, Aminer9.5K Insights, which can be used for automatic summarization tasks and to augment the dataset in automatic pre- sentation generation tasks.\nThe rest of the paper is organized as follows: Section 2 briefly discusses the related work. Section 3 elaborates the proposed system, SlideSpawn. In section 4 we present some experimental results. Finally, section 5 concludes the paper."}, {"title": "2 RELATED WORK", "content": "There aren't a lot of papers that directly address the problem of Automatic Slide Generation from a Reference Research Paper. Although, the problem of text summarization, which can be seen as a sub-task for our problem, has been worked upon quite extensively. Summarization of research docu- ment is different from standard text and summarizing text for placement in a presentation is different from paragraph summarization [3].\nAutomatic Slides Generation\nThe ealiest of the work addressing automatic slides gener- ation can be traced to [4]. They propose an algorithm to generate slides from document annotated with GDA tagset. They use abstractive summarization to place sentences in slides. The system proposed in [5] generated the slides by itemizing topic and non-topic parts extracted based on syntactic and case analysis. The system proposed in [6] converts a tex source to XML, weighs words using Tf- IdF values and assigns a weight to objects like sentence, table, figures etc. while deciding to place them in the slides. The system, SlidesGen [7], uses a graph based extractive summarizer QueSTS [8] [9], to generate section wise slides from a tex source.\nThe scope of machine learning was very limited due to lack of a dataset in this context. A dataset [10] studying alignment of paper and slide sentences had 10,000 pairs but the dataset remained private. The authors of PPSGen [11] crawled a dataset of 1,400 pairs following authors profile links listed on aminer, which allowed them to use SVR [12] to learn the salience score using a limited set of surface features corresponding to the sentences. They employed ILP to select sentences. In [13], authors proposed a phrase-based approach to generate slides, they extract phrases from the give paper and learn their salience and hierarchy, then they greedily place the phrases into the slides. The system pro- posed in [14] builds on the work done by [11] using the same dataset. They expand the list of features adding semantics and contextual features to predict sentence salience based on Jaccard similarity. They also use ILP to select sentences for\n2.1"}, {"title": "2.2 Summarization", "content": "Selecting text for our presentation can be approximated as a task of summarizing the paper. There are two ways to summarize a text [17]. First being the extractive approach where we select a subset of the sentences present in the original document. Second being the abstractive approach here, the summary text is generated. In our work, we follow the extractive approach as it gives grammatically and semantically correct sentences [18] [16] and is time efficient. The task of extractive summarization can be divided into 3 parts [19]:\nSentence Representation\nMost of the systems start by finding a way to represent sentences in a vector space that encodes their semantics. Tf-IdF based embedding were popular for a long time. Some later techniques include generating word2vec [20] based embedding [21], [22], [23]. The revolutionary paper by google research on natural language modelling [24], that introduced BERT which follows an unsupervised learning approach to learn high level language features, transformed the way we represent sentences. Sentence embedding based on BERT [25] have gained more and more traction for a lot of NLP tasks [26], [27], [28]. BERT based systems have also been successful in the context of extractive summariza- tion [29], [30], [31]. Another development over BERT intro- duced by Facebook research came with RoBERTa [32] which changes some key hyper parameters and training technique to achieve better results on tasks like GLUE [33], RACE [34], SQUAD [35]. These models have a lot of parameters that makes their usage time and resource consuming. Thanks to the distillation [36] technique, we can achieve similar results with higher computation speed. In our work, We use the distil-ROBERTa [25] based Sentence Transformer to encode the sentences and measure sentence similarity\n2.2.1"}, {"title": "2.2.2 Sentence score", "content": "To decide sentence score, one can use the sentence's sim- ilarity with gold-standard sentences. There is a choice of using the average similarity or the maximum similarity, latter giving better results [11], [14]."}, {"title": "2.2.3 Sentence selection", "content": "The final step is to select the sentences. We already have an importance score, the trivial way to select sentences is to do it greedily. This is effective to some extent but may cause redundancy of information. Another method for sentence selection is solving an integer linear programming (ILP) problem [37], [38] which although is an NP hard problem [39], but there are good solvers available that give results in decent time. Using ILP, we can place constraints to reduce redundancy while maximizing the aggregate of sentence score. In our system we use ILP."}, {"title": "3 SLIDESPAWN", "content": "In this section we describe the SlideSpawn algorithm. Figure 1 shows the overview of the proposed model. We briefly discuss the used dataset, followed by the sentence similarity measure used throughout the pipeline, we then describe our sentence importance assessment model, followed by the presentation generation algorithm."}, {"title": "3.1 Dataset", "content": "In order to train a machine learning model for our extractive summarization sub task, we use union of two datasets. PS5K [15] and Aminer9.5KInsights Dataset, the latter was gathered by us. The PS5K is publically available collection of 5,000 presentation and slides pair compiled from conference proceedings websites. The Aminer9.5K Insights dataset, available here\u00b9, has more than 9,500 research papers along with their sectioned and bulleted summaries taken from aminer\u00b2 website. The aminer website, for some of their listed papers, offers an insight section which has sections like Introduction, Highlights, Results etc. and each section has some bullet points that summarize the paper. This is quite similar to the structure of the presentation that SlideSpawn aims to create. This dataset has some of the most cited papers from a wide range of disciplines. A sample pair is displayed in the Figure 2.\nThe PPTs provided in the PS5K dataset are in PDF format. Extracting sentences effectively from these files is hard due to their non-uniform structure and presence of many graph- ical elements. Even with the best tools, we need significant text cleaning to remove unwanted information and some information is lost during extraction. The dataset collected by us avoids this problem altogether as the structure of the insight section is uniform for every paper and we download HTML source of the section, extracting text from which is trivial."}, {"title": "3.2 Sentence Similarity", "content": "We use sentence simiarity measures at many points in the system pipeline. Previous systems [11] [7] [13] have adopted cosine similarity over TF-IdF vectors while [14] uses jaccard similarity. Both these methods ignore semantics of the word and focus on exact overlap of the words to measure similar- ity. Slides are often paraphrased and simplified by authors, and working on a semantic based similarity provides a better alternative while estimating sentence importance for training our extractive summarization model. We use cosine similarity of sentence vector generated by distil-RoBERTa- base [25] sentence transformer to calculate sentence simi- larity in our system. Going forward we refer the vectors generated by distil-RoBERTa-base sentence transformer cor- responding to a sentence as simply sentence vectors or sentence embedding and the cosine similarity between two sentence vectors as simply sentence similariy."}, {"title": "3.3 Sentence Imoprtance Assesment", "content": "The complete flow diagram of sentence importance assess- ment training can be seen in Figure 3"}, {"title": "3.3.1 Salience Score", "content": "We define the salience score of a sentence as its maximum similarity with any of the sentences in the presentation or the insight.\n$S_i = max_{j=0}^{N} cosine (EP_i, ES_j)$    (1)\nWhere $S_i$ is the salience score of the $i^{th}$ sentence in the pa- per, N is the total number of sentences in the PPT/Insight, cosine(x, y) is the cosine similarity function for two vectors x and y, $EP_i$ is the sentence vector of the $i^{th}$ sentence in the paper and $ES_j$ is the sentence vector of the $j^{th}$ sentence of the PPT/Insight. This score is the dependent variable for our machine learning model."}, {"title": "3.3.2 Feature Engineering", "content": "The independent variables, that we calculate corresponding to the sentences, can be broadly classified into following 3 categories:\n1) Surface features: These capture the syntactic or grammatical attributes of the sentence. These in- clude the number of references to literature, ta- bles, figures, equations present in the sentence, the section(Abstract, Introduction, Background, Model, Results, Conclusion, Acknowledgement) containing the sentence in one-hot format, position of the sen- tence in the section normalized between (0,1], count of noun and verb phrases in the sentence, number of sub sentences, stopword percentage, length in number of characters and number of to- kens, depth of the parse tree of a sentence, average TF and IdF of the words in the sentence.\n2) Semantic features: These capture the meaning of the sentence. These include sentence vectors, sentence similarity with paper title, section title and abstract.\n3) Contextual features: These capture the context of the sentence. We use sentence's similarity with the three sentences that come before it and three sentences that come after it."}, {"title": "3.3.3 Model and Training", "content": "After experimentation with different models and config- urations (see Figure 4), we decided to use a multi-layer perceptron for the regression task of predicting the salience score. Our networks has one input layer, three hidden layers and an output neuron, all the layers except the output layer are followed by a sigmoid activation layer. We use Stochastic gradient descent optimizer with learning rate 0.004 and batch size of 64 to train for 50 epochs with the mean squared error as the loss function."}, {"title": "3.4 Presentation Generation", "content": "Sentence Selection\nTo select sentences, we use ILP to maximize the objective function given in Equation 2, subject to the size constraint (see Equation 3) and the similarity constraint that limits the redundancy (see Equation 4).\n3.4.1"}, {"title": "Subject to constraints:", "content": "$\u2211_{i=0}^{N}(L_i \u00d7 X_i) \u2264 Size$   (3)\n$\u2211_{i=0}^{N}\u2211_{j=0,i\u2260j}^{N} (cos(EP_i, EP_j) \u00d7 X_i \u00d7 X_j)$ < \u0398   (4)\n$\u2211_{i=0}^{N}\u2211_{j=0,i\u2260j}^{N} (X_i \u00d7 X_j)$\nWhere,\nN: is the number of sentences in the paper.\n$L_i$: is the number of characters in the $i^{th}$ sentence.\n$X_i$: is a boolean variable. It is 0 if the $i^{th}$ sentence is not selected and 1 if it is selected.\n$S_i$ : is the salience score of the $i^{th}$ sentence as estimated by our model described in Section 3.3\nSize : is the number of characters we want in the target presentation.\ncos(x,y): is the cosine similarity function between two vectors x and y.\n$EP_i$: is the sentence vector of the $i^{th}$ sentence.\n\u0398 : is a hyper parameter to be tuned.\nTo find the target Size of the presentation, we use a linear regression model trained on paper's characteristics like its size, the number of sentences, sections, graphical elements, references present in it along with the average size of sentence in terms of tokens and characters labelled with the number of characters in the corresponding presentation in the PS5K dataset.\nThe hyper-parameter, \u0398 lets us control the average similarity between the selected sentences. A very low value of \u0398 compels the model to select sentences with lower salience score and reduces the quality of the summary, while a very high value of \u0398 makes the selection similar to a greedy selection which increases redundancy in the resulting presentation."}, {"title": "3.4.2 Presentation Organization", "content": "The text structure in a presentation generally has bullet points at different levels. Each slide has one or more first- level bullets and the first level bullets have several second level bullets that expand on a particular topic. The content distribution of presentation according to an epmirical analy- sis done by [13] is listed in Table 1. The total share of content on first two levels is over 90%. Thus we decide to organizes the textual content into two level of bullet points."}, {"title": "3.4.3 Sentence Clusters", "content": "To cluster the sentences, we make a complete weighted graph with the selected sentences as the nodes and their sentence similarity as the edge weights. We then, binary search for a threshold for the edge weights such that when we remove the edges weighing less than the threshold value, the number of strongly connected components we are left in the graph is closest to N/3, where N is the number of selected sentences. This gives us cluster sizes generally ranging from 1 to 5. The clusters we obtain contain sentences discussing similar topics given in the research paper and can be placed together.\nFor selecting the title, we consider all the noun phrases present in the cluster as our candidates, and choose the one with highest phrase score defined by the Equation 5.\n$PS_i = \u2211_{j=0}^{N} Cos(ENP_i, EP_j)/ N$   (5)\nwhere, $PS_i$ is the phrase score for the $i^{th}$ phrase in the cluster, N is the number of sentences in a cluster, cos(x, y) is the cosine similarity between vectors x,y, $ENP_i$ is the sentence vector for the $i^{th}$ phrase, $EP_i$ is the sentence vector for the $i^{th}$ sentence in the cluster."}, {"title": "4 RESULTS", "content": "Humans evaluate a presentation subjectively which makes devising a quantitative measure for the quality of a pre- sentation a hard task. Since summarization is a key part of our system, we present our evaluations and comparisons using metrics used for the summarization tasks. Most recent summarization works evaluate their models using ROUGE scores [40] [41]. For our comparison, we implement two baseline methods, PPSGen [11] and Phrase-Based Presen- tation Slides Generation [13].\nWe randomly separate 650 pairs of paper and PPT/Insight from the training and validation datasets for evaluations and comparison. For each system, we limit the size of presentation for each method to 20% of the size of the research paper. We use ROUGE-1, ROUGE-2, ROUGE-SU4 F1 scores for comparison. For our model, we select  \u0398 = 0.55 as that gives the best results in terms of listed metrics."}, {"title": "5 CONCLUSION AND FUTURE WORK", "content": "This paper proposes a novel system called SlideSpawn to generate presentation slides from research publications. We train a sentence salience assessment model based on Multi- Layer Perceptron and use the ILP method to extract sen- tences, then place similar sentences together under suitable headings. Experimental results show that our method can generate much better slides than existing methods. We also provide a new dataset, Aminer9.5K Insights, that can be used for automatic summarization and automatic slides generation tasks.\nIn future, the system can be further improved by:\n1) Gathering more presentation and slides pairs,\n2) Improving PDF to XML conversion for both paper and PPT files,\n3) Selecting textual elements by using abstractive sum- marization techniques as done by humans.\n4) Using learning to quantify importance of graphical elements."}]}