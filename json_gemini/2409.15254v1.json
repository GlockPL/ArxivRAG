{"title": "ARCHON: An Architecture Search Framework for Inference-Time Techniques", "authors": ["Jon Saad-Falcon", "Adrian Gamarra Lafuente", "Shlok Natarajan", "Nahum Maru", "Hristo Todorov", "E. Kelly Buchanan", "Mayee Chen", "Neel Guha", "Christopher R\u00e9", "Azalia Mirhoseini"], "abstract": "Inference-time techniques are emerging as highly effective tools to increase large language model (LLM) capabilities. However, there is still limited understanding of the best practices for developing systems that combine inference-time techniques with one or more LLMs, with challenges including: (1) effectively allocating inference compute budget, (2) understanding the interactions between different combinations of inference-time techniques and their impact on downstream performance, and 3) efficiently searching over the large space of model choices, inference-time techniques, and their compositions. To address these challenges, we introduce ARCHON\u00b9, an automated framework for designing inference-time architectures. ARCHON defines an extensible design space, encompassing methods such as generation ensembling, multi-sampling, ranking, fusion, critiquing, verification, and unit testing. It then transforms the problem of selecting and combining LLMs and inference-time techniques into a hyperparameter optimization objective. To optimize this objective, we introduce automated Inference-Time Architecture Search (ITAS) algorithms. Given target benchmark(s), an inference compute budget, and available LLMS, ITAS outputs optimized architectures. We evaluate ARCHON architectures across a wide range of instruction-following and reasoning benchmarks, including MT-Bench, Arena-Hard-Auto, AlpacaEval 2.0, MixEval, MixEval Hard, MATH, and CodeContests. We show that automatically designed inference-time architectures by ARCHON Outperform strong models such as GPT-40 and Claude 3.5 Sonnet on these benchmarks, achieving an average increase of 14.1 and 10.3 percentage points with all-source models and open-source models, respectively. We make our code and datasets available publicly on Github: https://github.com/ScalingIntelligence/Archon.", "sections": [{"title": "1 Introduction", "content": "Inference-time techniques are gaining traction as effective methods for improving model capabilities. Examples include generation ensembling, ranking, and fusion, where models in the ensemble are queried in parallel, their responses are ranked, and the best ones are fused into a single, higher quality output, respectively [24, 56]. Other types of inference-time techniques are based on querying a single LLM successively (via multi-sampling) and using a voting strategy or unit tests to select the top generation [6, 7, 28]. We categorize these existing inference-time techniques into three components: generative, meaning that new candidate responses are drawn from the models (e.g. generation ensembling and multi-sampling), reductive, meaning that the existing responses are aggregated or filtered to keep the top responses (e.g. fusion and ranking), or comparative, meaning they provide analysis of candidate responses (e.g. critiquing and unit testing), as shown in (Table 1).\nRecent work has made progress towards building robust inference-time architectures, which are systems composed of one or more large language models (LLMs) and inference-time techniques, such as Mixture-of- Agents (MoA) [56] and LLM-Blender [24] as well as single-model systems like LeanStar [32] and rStar [14]. However, our experiments show that existing architectures, such as MoA, still suffer from lack of generalization and become significantly less effective beyond the task(s) they were developed on (see Section 4.2). We argue that designing inference-time architectures that are generally effective across a wider range of tasks requires addressing the following challenges:"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Scaling Laws of Language Models", "content": "Language models [22, 40, 50, 53] have transformed the field of artificial intelligence across a vast number of domains and tasks. LLMs are pretrained on substantial amounts of textual data before being further aligned with human preferences through instruction fine-tuning [10, 57], direct policy optimization (DPO) [44], Kahneman-Tversky optimization (KTO) [16], reinforcement learning from AI feedback (RLAIF) [5], and other techniques. As language models continue to gain improved abilities with further scaling of data, parameters, and compute [17, 26], the cost of developing new LLMs is ever increasing, requiring the curation of trillions of new tokens as well as substantial GPU-hours for pretraining. Furthermore, as the current state-of-the-art in LLMs are primarily closed-source APIs, such as OpenAI's GPT-40 [40], Google's Gemini [51] and Anthropic's Claude [2], it is difficult to effectively explore and push the frontier of existing LLMs without being able to manipulate"}, {"title": "2.2 Inference-Time Techniques", "content": "By utilizing a single LLM or multiple LLMs, inference-time architectures allow us to combine multiple inference- time techniques (e.g. generation ensembling, sampling, ranking, and fusion), achieving superior performance compared to individual models. Notable works in this domain include the Mixture-of-Agents (MoA) [56], LLM Blender [24], and RouteLM [28, 38], which have demonstrated the efficacy of these techniques in improving generation quality, limiting API costs, and reducing query latency. LM frameworks, such as DSPy [27] and TextGrad [61], have even emerged for orchestrating LMs and other tool components (e.g. retrievers, search engines, calculators, compilers, etc.), optimizing prompt engineering for integrating these components. Even with a single LLM, various inference-time techniques can bolster downstream performance by building better reasoning strategies. These techniques include OpenAI's o1, Chain of Thought, Branch-Solve-Merge, Rephrase and Respond, Lean-STaR, rStar, and more [14, 32, 39, 41, 46, 58].\nDespite these advancements, several challenges remain for the development of inference-time architectures. Firstly, many inference-time architectures today delegate the vast majority of their inference calls towards additional generations [7, 13, 24]. For reasoning domains like coding and mathematics, additional repeated inference calls are shown to be effective in improving benchmark performance [6]. However, for other tasks such as chat and instruction-following, additional inference-time techniques such as generation fusion and ranking are shown to be useful [24, 56]. Additionally, for tasks without built-in verification (e.g. unit tests), it can be important to delegate additional compute towards reasoning generation and verification to improve downstream accuracy [13]. Within the set of inference-time architectures, we still do not understand the trade-offs between different inference-time techniques in these systems. Prior studies have only explored limited aspects of inference- time architecture configurations, often focusing on specific benchmarks without generalizing the findings to broader datasets [24, 56]. For example, both [7] and [28] explored the impact of LLM calls on downstream performance but they did not examine how other inference-time techniques, such as generation ensembling and fusion, might impact the trends found for LLM sampling. Beyond analysis of inference-time techniques, it is also crucial to thoroughly and efficiently develop inference-time architectures since the optimal configuration can differ widely based on the benchmark, the models available, and the maximum number of inference calls allowed (Section 4.2). To address these challenges, we analyzed multiple inference-time techniques (Section 3.1) and developed the ARCHON framework for automating the development of inference-time architectures with ITAS (Section 3.4)."}, {"title": "3 Inference-Time Techniques for ARCHON", "content": "To better understand what inference-time techniques could be most effective for ARCHON, we test an array of dif- ferent techniques, incorporating existing approaches for generating, ranking, and fusing candidates [24, 56] as well as constructing new approaches for critiquing, verifying, and unit testing candidates, inspired by a host of recent work [6, 13]. Below, we elaborate on the structure, inputs, and outputs of each of the inference-time techniques, which we also include in Table 1. Then, we discuss how to combine the different techniques into an inference-time architecture (Section 3.2) and the relationships between the different inference-time techniques (Section 3.3) before finally exploring automatic approaches for constructing inference-time architectures (Section 3.4)."}, {"title": "3.1 LLM Components of ARCHON", "content": "In this section, we discuss the LLM components of ARCHON, which are LLMs that perform a specific inference- time technique. The components are summarized in Table 1.\nGenerator: The Generator module of ARCHON is a LLM that creates candidate responses As input, the generator takes in the instruction prompt and outputs candidate responses. Generators can be called in parallel to perform generation ensembling [24, 56], or sampled multiple times [6, 7]. When calling the Generators in parallel, you can sample one or more LLMs one or more times. The exact number of models, samples, and temperature for generation can be varied based on model configuration. We provide the exact prompt used in the ARCHON generator in Table 5.\nFuser: The Fuser module of ARCHON is a LLM that combines multiple candidate responses to create one or more higher-quality responses. As input, the fuser takes in the instruction prompt and the set of proposed responses. As output, the fuser generates a fused response that combines the proposed responses into a higher-quality generation for addressing the instruction prompt. We provide the exact prompt used in the ARCHON fuser in Table 6.\nRanker: The Ranker module of ARCHON is a language model that ranks the current list of candidate generations based on their quality and the instruction prompt. As input, the ranker takes in the instruction prompt and the set of proposed responses. As output, the ranker produces a ranked list of the proposed responses. We provide the exact prompt used in the ARCHON ranker in Table 7.\nCritic: The Critic module of ARCHON is a LLM that produces a list of strengths and weaknesses for each candidate response in a provided set. As input, the critic takes in the instruction prompt and the set of proposed responses. As output, the critic produces a list of strengths and weaknesses for each respective candidate response. We use the strengths and weaknesses to improve the quality of the final response (Section 3.2; Figure 5). We provide the exact prompt used in the ARCHON critic in Table 8.\nVerifier: The Verifier module of ARCHON is a LLM that verifies whether a provided candidate response has appropriate reasoning for a given instruction prompt. It proceeds in two stages: Stage #1 takes in the instruction prompt and a candidate response as input and outputs reasoning for why the candidate response is correct; Stage #2 takes in the instruction prompt, candidate response, and produced reasoning before outputting reasoning and a verdict (i.e. binary [Correct] or [Incorrect]) for whether or not the candidate response is correct according to the provided instruction prompt and reasoning. We provide the exact prompt used in the ARCHON verifier in Table 9.\nUnit Test Generator: The Unit Test Generator module of ARCHON is a LLM that generates a set of unit tests for a given instruction prompt. As input, the unit test generator solely takes in an instruction prompt. As output, the unit test generator produces a list of unit tests that are consequential for the accuracy and relevance of a candidate response. These generated unit tests are verified by the Unit Test Evaluator, allowing us to rank different candidate responses. Each unit test is formatted as a concise declarative statement that can either be passed or failed. We make the number of unit tests generated a configurable choice for the unit test generator but we find 5-10 generated unit tests to be most effective with our set of LM prompts (Section 4.2; Figure 7). We include examples of unit tests for an instruction-following query and a reasoning query in Table 11. We provide the exact prompt used in the ARCHON unit test generator in Table 10.\nUnit Test Evaluator: The Unit Test Evaluator module of ARCHON is a language model that evaluates each candidate generation against a generated set of unit tests. As input, the unit test evaluator takes in the instruction prompt, candidate response(s), and set of unit tests. As output, the unit test evaluator outputs the candidate response(s), ranked in descending order by how many unit tests they pass. We use model-based"}, {"title": "3.2 Combining the LLM Components", "content": "Overview: Inspired by the structure of neural networks [21], ARCHON is constructed of layers of LLM com- ponents (Figure 1; Section 3.1). Each layer is composed of sets of these LLM components that are called in parallel, performing a text-to-text operation to the instruction prompt and the subsequently generated candidate responses. Furthermore, like a neural network, some layers perform transformations of the provided list of strings (e.g. the Generator and Fuser components), converting a list of strings into a different list of strings (the numbers of candidates can vary from the original number of candidates). Other components introduce non-linearities into the ARCHON structure, performing filtering of the list of strings (e.g. Ranker and Verifier). Ultimately, the inputs and outputs for each layer is always a list of strings, whether that is the instruction prompt (e.g. a list with a single string) or a list of candidate responses (e.g. a list of many strings). If a list of strings are outputted at the last layer of the ARCHON structure, the first string in the list is returned.\nUnlike a classical neural network, no weights are learned between the LLM components and the layers; in turn the ARCHON architecture can be deployed off-the-shelf without any tuning. This distinction makes architecture search much cheaper and more efficient since a new configuration is tested without an inner optimization of the architecture's weights, which we discuss in Section 3.4. Additionally, a single state is transformed sequentially from the input layer to the final output; this single state is the initial instruction prompt and the current candidate responses. In Figure 2, we provide an example ARCHON architecture composed of five layers: an ensemble layer of generators, an intermediate critic layer, an intermediate layer of fusers, an intermediate ranker layer, and a final fuser layer.\nRules for Construction: While ARCHON is a modular framework, the LLM components in Section 3.1 can only be placed in specific orders.\n1. Only one type of module can be present in any given layer.\n2. Generator components must and can only be placed in the first layer of ARCHON; you can put multiple Generators or a single Generator in the layer.\n3. The Critic component must come before a Ranker or a Fuser, otherwise the generated strengths and weaknesses cannot be incorporated into generation ranking or fusion, respectively."}, {"title": "3.3 Utilities and Interactions of LLM Components", "content": "In this subsection, we present our analysis of the effectiveness of each LLM component (i.e. the Utility) and the relationships between each component (i.e. the Component Interactions) by evaluating on instruction-following tasks (MT Bench, AlpacaEval 2.0, Arena-Hard-Auto), reasoning tasks (MixEval, MixEval-Hard, MATH) and coding tasks (CodeContests) (Section 4.1). For our ARCHON models, we utilize a host of 70B+ open-source models (Section 4.1; Table 13).\n3.3.1 Generator\nUtility: For our Generator module, we find additional model sampling to significantly boost performance (Figure 3), particularly for coding tasks (Figure 7). In settings with a limited inference call budget, additional model samples lead to the largest marginal benefit. We see a similar pattern for model ensembling, where sampling from additional models leads to continual performance increases (assuming the models are ordered from best to worst for the given task) (Figure 4).\n3.3.2 Fuser\nUtility: For every benchmark explored, we found that the Fuser module substantially improved performance (Figure 3; Figure 4; Figure 5). For the single-generation 10-model ensemble of 70B+ models, the Fuser module improved downstream accuracy by 5.2 points, on average, compared to the single-generation best model (Figure 4). When combined with the Ranker module for ranking the top-5 candidate responses, the Fuser improved downstream accuracy by 7.3 points and 3.6 points, on average, compared to the single-sample best model and the oracle best candidate response, respectively (Figure 4). Overall, we found that Fuser efficacy increased as more candidate responses were provided, demonstrating that additional candidate generations can continue to bolster inference-time architecture performance when combined with a Fuser.\nIn previous work like Mixture-of-Agents (MoA) [56], multiple layers of Fusers was found to boost performance on some instruction-following tasks (i.e. MT Bench and Alpaca Eval 2.0). Across all the benchmarks explored, we observed similar benefits in the ARCHON framework when adding multiple layers of Fusers (Figure 5). However, based on our results in Figure 8, the number of Fuser layers needed to improve performance varied by task, with some tasks receiving limited benefits from added layers (1-2 point increase in accuracy for MixEval) while others experienced significant benefits with 3-4 fusion layers and more (2 to 5 point increase in win rate for MT Bench and Alpaca Eval 2.0). We attribute this distinction to the difference in task requirements, with chat and instruction following tasks benefiting more from multiple iterations of revisions through the multiple Fuser layers, leading to greater diversity in the final generation (Table 15).\nComponent Interactions: To better understand how the Fuser module works with the other LLM components, we took the single-sample 10-model ensemble of Generators with a Fuser and tried adding each of these compo- nents individually: a Critic, a Ranker, a Verifier, and a Unit Test Generator/Evaluator. Across all of the bench- marks, the added candidate response analyses from the Critic improved the Fuser's ability to effectively merge the different candidate responses, increasing performance by an average of 3.1 percentage points (Figure 5). With the added Ranker, the ARCHON architecture improved the combined Ensemble + Critic + Fuser performance across all the benchmarks by 4.8 percentage points, on average (Figure 5). The Ranker proved most effective for style- oriented tasks (e.g. MT Bench and AlpacaEval 2.0) since the examples mostly focus on improving the instruction- guidance towards the provided prompt. With the added Verifier module (Figure 5), the performance of the Ensem- ble + Critic + Fuser configuration improved marginally for the instruction-following tasks (1.2 percentage points, on average, for MT Bench, AlpacaEval 2.0, and Arena-Hard-Auto). However, this configuration improved perfor- mance more on reasoning tasks (3.2 percentage points for MixEval and MixEval-Hard, on average), assisting gener- ation by filtering out irrelevant or flawed answers before the final fusion step (Figure 5). The added Unit Test Gen- erator and Evaluator was less effective for the instruction-following and reasoning tasks, only providing a 1.5 per-"}, {"title": "3.4 Inference-Time Architecture Search (ITAS)", "content": "In this section, we explore different approaches for finding the best inference-time architecture (for a given task) through inference-time architecture search (ITAS). Due to compute resources, we pre-filtered certain ways of combining LLM components to reduce the search space while still building effective inference-time architectures. While it is possible to expand the search space of potential ARCHON architectures (e.g. different temperatures for generative LLM components, alternative prompts for each LLM component, multiple layers of Generator modules, additional LLM components for ARCHON, etc.), we use our analysis from Section 3.2 to selectively limit our search space to configurations that fit our rules for ARCHON: starts with a layer of Generator modules, followed by layers performing fusing, ranking, critiquing, verifying, and unit testing.\nSearch Hyperparameters: With the studied LLM modules and their relationships within the ARCHON architecture, we selected five main axes for the hyperparameters in our search:\n1. Top-K Generators for Ensemble: The top-K models to be used for the initial Generator ensemble, ranges from 1 to 10. The top-K models are the best-K LLMs for the given task, based on their individual performances (Table 14).\n2. Top-K Generator Samples: The number of samples gathered from each Generator in the ensemble (it is the same for all the models), ranges from 1 to 5. For MATH and Code-Contests, we also explore high sample settings over the following set of samples: [1, 10, 100, 500, 1000].\n3. Number of Fusion Layers: The number of layers of Fusers, ranges from 1 to 4. The last fusion layer will always have a single Fuser.\n4. Top-K Fusers for Fusion Layers: The top-K models used for each fusion layer, ranges from 2 to 10 and increases by 2 each time.\nBy combining all the hyperparameters, we create a search space of 6,250 configurations by multiplying each of the configuration option counts together (10*5*$\\frac{5(4-1)}{2}$ =6250). However, we remove configurations that are not viable: configurations in which the number of initial generations exceeds the context window of the fusers (i.e. 24 candidate generations) and configurations with only one fuser layer but multiple fusers declared. This reduces our search space to 3192 configurations. For these configurations, we add critic and ranker layers before each fuser layer since they've been shown to have added benefits across the benchmarks explored (Figure 4; Figure 5). The ranker selects the top-5 candidate generations to send to the next layer. Additionally, for our coding tasks (i.e. CodeContests), we use unit test generators and evaluators after our initial generation layer, with a default setting of 10 unit tests generated. On our instruction-following and reasoning tasks (i.e. MT-Bench, AlpacaEval 2.0, Arena-Hard-Auto, MixEval, MixEval-Hard, and MATH), we also ablate adding a verifier before the final fuser layer (Table 2). Ultimately, we could increase the search space substantially more along various other axes, such as additional combinations of verifier, unit test generation, and fuser layers, but given our compute resource limitations, we did not scale further."}, {"title": "4 Experiments", "content": "In our experimental evaluations, we focus on five questions: (1) how do ARCHON architectures compare to existing SOTA closed-source LLMs and other inference-time architectures? (2) how does ARCHON performance compare across tasks? (3) how does ARCHON performance compare when optimized for a set of tasks vs. an individual task? (4) what are the current limitations of ARCHON and plans for future work?"}, {"title": "4.1 Benchmarks and Models", "content": "In this section, we discuss the benchmarks and models used in our LLM component analysis and development of Archon.\nBenchmarks: We evaluate our models with several benchmarks for instruction-following and reasoning: MT-Bench [63], Alpaca Eval 2.0 [30], Arena Hard Auto [29], MixEval [37], MixEval-Hard, MATH [20], and CodeContests [31]. We provide an overview of each dataset in Table 24, where we compare their query counts, scoring type, evaluation metrics, baseline models, and judge models. For MixEval and MixEval Hard, we use the 2024-06-01 dataset release. For MT Bench and Arena-Hard-Auto, we also include a configuration with Claude-3.5-Sonnet as the baseline model (in addition to the original setting with GPT-4-0314) to have a stronger model for comparing against ARCHON architecture performances (Table 3) and mitigate the GPT-4-Turbo judge bias towards GPT responses. Additionally, we chose not to use the single-scoring configuration for MT Bench due to the inconsistencies in LLM judge scoring on 1-10 scales [49, 52]. For MATH, we evaluate a random sample of 200 problems from the dataset's test set. For CodeContests, we evaluate on the 140 test set questions that do not include image tags in the problem description.\nModels: For ARCHON, we test across three model categories: 8B or less parameter models, 70B or more parameter models, and closed-source model APIs. For our 8B models, we selected the top-10 performing chat models on the Chatbot Arena Leaderboard [9] as of July 2024. For our 70B+ models, we selected the existing state-of-the-art open-source chat models as of July 2024. For our closed-source model APIs, we include GPT-40, GPT-4-Turbo, Claude Opus 3.0, Claude Haiku 3.0, and Claude Sonnet 3.5. We list and compare all of the models tested in the ARCHON framework in Table 13 and Table 14."}, {"title": "4.2 ARCHON vs. Closed-Source LLMs and Other Inference-Time Architectures", "content": "We start by comparing ARCHON architectures to existing SOTA closed-source LLMs and inference-time ar- chitectures across a set of instruction-following, reasoning, and coding tasks with either pairwise ranking or accuracy metrics, as described in Section 4.1. Based on our results in Table 3, we find that ARCHON architectures consistently match or surpass existing SOTA approaches across all the benchmarks explored. On the evaluation suite, our ARCHON architectures with open-source models experience a 10.2 point increase, on average, above SOTA open-source approaches; for its worst performance, our open-source ARCHON architectures are only 1.6% above SOTA open-source approaches on AlpacaEval 2.0. For our ARCHON architectures with closed-source models, we set SOTA performance across MT Bench, Arena-Hard-Auto, MixEval, and MixEval-Hard, leading to a 18.3 point increase, on average, compared to closed-source approaches. Lastly, for approaches that use all models available, both open and closed-source, ARCHON achieves an increase of 14.1 points, on average, over existing SOTA approaches."}, {"title": "4.3 ARCHON by Task", "content": "We analyze ARCHON performance by task style: instruction-following tasks that use pairwise ranking for scoring, reasoning tasks that use accuracy-based metrics for scoring, and coding tasks that use Pass@1. On instruction-following tasks like MT Bench, AlpacaEval 2.0, and Arena-Hard-Auto, open-source ARCHON ar- chitectures outperform current open-source baselines by 12.8 percentage points, on average, while closed-source ARCHON outperforms current closed-source baselines by 22.2 percentage points (Table 3). On reasoning tasks like MixEval, MixEval-Hard, and MATH, open-source ARCHON architectures outperform existing open-source baselines by 5.1 percentage points while closed-source ARCHON architectures outperform current closed-baselines by 4.7 percentage points (Table 3). On coding tasks (i.e. CodeContests), open-source ARCHON architectures"}, {"title": "4.4 Task-Specific and General-Purpose ARCHON Architectures", "content": "Task-Specific vs. General-Purpose: We also compare custom ARCHON architectures, specifically configured to a single evaluation dataset (\"Task-specific ARCHON Architectures\"), and a generalized ARCHON architecture configured to handle all the evaluation datasets (\"General-purpose ARCHON Architectures\") (Table 3). We utilize ITAS to find the generalized ARCHON architectures in Table 3 (subsection 3.4), maximizing performance over all of the benchmarks explored except CodeContests. We exclude CodeContests from the generalized ARCHON architecture search since we found that ARCHON architectures for coding tasks are most effective with a different set of inference-time techniques compared to instruction-following and reasoning tasks (i.e. increased model sampling combined with model-based unit test generation/evaluation) (Section 3.2; Table 2). For open-source models, we find that our generalized ARCHON architecture only lags behind the specialized ARCHON architectures by 3.4 percentage points, on average, across all the benchmarks, demonstrating the robustness of the ARCHON architecture found by the ITAS algorithms (Table 3). We see similar gaps between the generalized and specialized ARCHON architectures for closed-source models (3.8 percentage points) as well as the all-source models (3.2 percentage points) (Table 3)."}, {"title": "4.5 ARCHON by Inference Budget", "content": "Finally, we compare different ARCHON architectures across inference budgets for both open-source models and closed-source models (Table 16). For instruction-following and reasoning tasks, we find consistent improvements in downstream performance as we scale from 1 to 50 inference calls, increasing by 14.3 percentage points, on average, across our selected evaluation benchmarks (Table 16). However, after roughly 50 inference calls, performance gains plateau. The results suggest that the early addition of LLM components in ARCHON (e.g. critic, ranker, layers of fusers) led to the most substantial gains in performance and after that, additional LLM components did not ultimately enhance the final generated response. We see the trend most apparent for the MixEval and MixEval-Hard benchmarks, where additional layers of Critic, Rankers, and Fusers do not benefit performance beyond a 30 inference call budget (Table 16). Notably, for math and coding tasks, we see continued improvements with additional inference calls by using generated unit tests to evaluate candidate responses, leading to a 56% increase in Pass@1 (Figure 7)."}, {"title": "4.6 Limitations and Future Work of ARCHON", "content": "Parameter Count: The ARCHON framework is most effective with LLM with 70B parameters or more. When we utilize the ARCHON architecture with only 7B open-source models, we get a notable decrease in performance (Table 17). The best 7B ARCHON configurations lag behind single SOTA (and much larger) models by 15.7% on across all the benchmarks, on average; 7B models work well for ranking but are less effective for critic and fusion. While this is expected, as small models generally exhibit lower performance, their weaker instruction following ability is a compounding factor.\nLatency and Costs: ARCHON is not ideal for tasks that prefer the latency of a single LLM call, such as certain consumer chatbots. Since ARCHON architectures often make multiple LLM calls successively for different operations (e.g. ensembling, critiquing, ranking, fusion, etc.), it can often take 5+ more time than a single LLM call (Section A.3). Furthermore, it can require calling multiple API endpoints for a single query, leading to increased expenditures compared to single-call LLMs (Table 18; Table 19). Note that these increases in compute costs and latency translate to higher quality responses, and can be justified in many application domains, such as science, math, and programming, or for addressing complex customer service issues.\nARCHON Components: While ARCHON is a modular framework, allowing the easy incorporation of new LLMs, new inference-time techniques, and even tool use, we only explore seven LLM inference time techniques in our work (Section 3.1). The addition of new techniques is a promising avenue for future research. Furthermore, while different queries can be best suited by different ARCHON architectures (Table 25; Table 26), the ITAS algorithm selects the best single architecture for the evaluation set queries combined. Future architecture search could focus on dynamic selection of components on a query-by-query basis."}, {"title": "5 Conclusion", "content": "This paper presents ARCHON, a modular framework for optimizing inference-time architectures by integrating multiple inference-time techniques, such as ensembling, ranking, fusion, critique, verification, and unit test generation. Extensive experiments demonstrate that ARCHON consistently matches or exceeds the performance of leading closed-source LLMs, such as GPT-4 and Claude-3.5-Sonnet, while only using open-source models across diverse benchmarks, including MT-Bench, AlpacaEval 2.0, Arena-Hard-Auto, MixEval, MixEval-Hard, MATH, and CodeContests. We attribute ARCHON'S boost in benchmark performance to two main factors. The first factor is the ability to leverage inference-time compute towards utilizing multiple LLMs and additional operations (e.g. fusing, ranking, critiquing, verification, unit testing), leading to amplified benefits that scale with additional inference calls (Sections 3.1 and 3.3). The second factor is the automatic approach for iteratively testing different ARCHON architectures with ITAS, guaranteeing the optimal configuration given enough exploration steps (Section 3.4). These results underscore the potential of ARCHON and ITAS algorithms in advancing the development of high-performing and generally capable inference-time architectures. The framework and datasets are publicly available on Github: https://github.com/ScalingIntelligence/Archon."}, {"title": "A Appendix", "content": ""}, {"title": "A.1 ARCHON LLM Components", "content": ""}, {"title": "A.2 ARCHON LLM Analysis", "content": ""}, {"title": "A.3 ARCHON Architectures", "content": ""}, {"title": "A.4 ARCHON by Inference Compute Budget, Model Size, and Cost", "content": ""}, {"title": "A.5 ITAS Algorithms Comparisons", "content": ""}, {"title": "A.6 Bayesian Optimization", "content": "Bayesian Optimization is a sequential design strategy for global optimization of black-box functions that are expensive to evaluate [48]. It is particularly useful when dealing with functions that have unknown forms and are costly to evaluate, such as hyperparameter tuning in machine learning.\nA.6.1 Basic Idea of Bayesian Optimization\nThe core idea behind Bayesian Optimization is to build a probabilistic model of the objective function and use it to select the most promising points to evaluate next. This process involves two main components:\n1. Surrogate Model: A probabilistic model (often a Gaussian Process) that approximates the unknown objective function.\n2. Acquisition Function: A function that guides the search for the optimum by suggesting the next point to evaluate, based on the surrogate model.\nA.6.2 Steps in Bayesian Optimization\n1. Initialization: Begin with a set of initial points D={(x\u2081,y\u2081),(x\u2082,y\u2082),\u2026\u2026\u2026,(x\u2099,y\u2099)}, where x\u1d62 is the input, and y\u1d62 = f(x\u1d62) is the objective function value at x\u1d62.\n2. Model Building: Fit a surrogate model (e.g., Gaussian Process) to the observed data D.\n3. Acquisition: Use the acquisition function to select the next point x\u2099\u208a\u2081 to evaluate:"}, {"title": "A.6.3 Gaussian Process as a Surrogate Model", "content": "A Gaussian Process (GP) is commonly used as a surrogate model in Bayesian Optimization. It is defined by a mean function \u03bc(x) and a covariance function (kernel) k(x", "x')": "nf(x)~GP(\u03bc(x)", "x": "n\u2022 Predictive Mean: The expected value of the function at x:\n\u03bc(x|D)=k\u2099(x)K\u207b\u00b9y\nwhere k\u2099 (x) is the covariance vector between x and the training points, and K\u2099 is the covariance matrix of the training points.\\"}]}