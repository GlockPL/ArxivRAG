{"title": "Were RNNs All We Needed?", "authors": ["Leo Feng", "Frederick Tung", "Mohamed Osama Ahmed", "Yoshua Bengio", "Hossein Hajimirsadeghi"], "abstract": "The scalability limitations of Transformers regarding sequence length have renewed interest in recurrent sequence models that are parallelizable during training. As a result, many novel recurrent architectures, such as S4, Mamba, and Aaren, have been proposed that achieve comparable performance. In this work, we revisit traditional recurrent neural networks (RNNs) from over a decade ago: LSTMs (1997) and GRUs (2014). While these models were slow due to requiring to backpropagate through time (BPTT), we show that by removing their hidden state dependencies from their input, forget, and update gates, LSTMs and GRUS no longer need to BPTT and can be efficiently trained in parallel. Building on this, we introduce minimal versions (minLSTMs and minGRUs) that (1) use significantly fewer parameters than their traditional counterparts and (2) are fully parallelizable during training (175\u00d7 faster for a sequence of length 512). Lastly, we show that these stripped-down versions of decade-old RNNs match the empirical performance of recent sequence models.", "sections": [{"title": "1 Introduction", "content": "Over the past few years, Transformers (Vaswani et al., 2017) have been the dominant architecture in many areas, leading to advancements in tasks like machine translation (Devlin et al., 2019), text generation (Brown et al., 2020), and more. However, Transformers have a quadratic computational complexity in the sequence length, making them prohibitively expensive for long sequences, especially in low-resource settings. As such, numerous works have investigated the design of more efficient alternatives that achieve competitive performance with that of Transformers. Recently, there has been a renewed interest in recurrent sequence models that can be trained efficiently processing their context in parallel. These models (1) during training require only linear memory in the sequence length and (2) at inference time are rolled out recurrently token-by-token, requiring only constant memory. As a result, these models can scale to significantly longer sequences than Transformers\u00b9.\nA family of efficiently trainable recurrent sequence models that has recently gained much traction is that of state-space models, specifically the recently proposed Mamba (Gu & Dao, 2024). Mamba"}, {"title": "2 Background", "content": "In this section, we review recurrent neural networks (RNNs). RNNs are recurrent sequence models that maintain a hidden state across time steps, capturing temporal dependencies. As such, RNNs are particularly suitable for sequence modelling settings such as those involving time series, natural language processing, and other sequential tasks where context from previous steps informs the current prediction. Vanilla RNNs (Elman, 1990), however, struggle with issues of vanishing and exploding gradients, limiting their ability to learn long-term dependencies."}, {"title": "2.1 LSTM", "content": "Addressing this limitation, Hochreiter & Schmidhuber (1997) introduced Long Short-Term Memory (LSTM) networks. LSTMs are enhanced RNNs designed to mitigate the vanishing gradient problem, allowing the model to learn long-term dependencies. LSTMs are computed as follows:\n\n$f_t = \\sigma(\\text{Linear}_{d_h} ([x_t, h_{t-1}]))$\n$i_t = \\sigma(\\text{Linear}_{d_h} ([x_t, h_{t-1}]))$\n$\\tilde{c}_t = \\tanh(\\text{Linear}_{d_h} ([x_t, h_{t-1}]))$\n$o_t = \\sigma(\\text{Linear}_{d_h} ([x_t, h_{t-1}]))$\n$c_t = f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}_t$\n$h_t = o_t \\odot \\tanh(c_t)$\n\nwhere $\\odot$ represents an element-wise multiplication of vectors, $t$ is the current timestep, $h_t$ is the outputted hidden state, $[x_t, h_{t-1}]$ represents the concatenation of $x_t$ with $h_{t-1}$, $d_h$ is the size of the hidden state, $c_t$ is a cell state that maintains information over the sequence, and $\\tilde{c}_t$ is the candidate cell state to be added, $i_t$, $f_t$, and $o_t$ are gating mechanisms. The input gate $i_t$ controls how much new information from the candidate cell state is added. The forget gate $f_t$ determines the proportion of information in the cell gate to discard. The output gate $o_t$ decides what information from the cell state should be outputted. The $o$ and $\\tanh$ are used for scaling to ensure that the output does not explode/vanish. An LSTM module maintains both a cell and a hidden state and, in total, contains $O(4d_h(d_x+d_h))$ parameters."}, {"title": "2.2 GRU", "content": "Simplifying LSTM, Cho et al. (2014) introduced Gated Recurrent Unit (GRU) which only uses two gates and a single state instead of LSTM's three gates and two states (hidden and cell state). GRU's reduced complexity leads to faster training and inference times while achieving competitive performance in many tasks. GRUs are computed as follows:\n\n$z_t = \\sigma(\\text{Linear}_{d_h} ([x_t, h_{t-1}]))$\n$r_t = \\sigma(\\text{Linear}_{d_h} ([x_t, h_{t-1}]))$\n$\\tilde{h}_t = \\tanh(\\text{Linear}_{d_h} ([x_t, r_t \\odot h_{t-1}]))$\n$h_t = (1 - z_t) \\odot h_{t-1} + z_t \\odot \\tilde{h}_t$\n\nwhere $\\tilde{h}_t$ is the candidate hidden state that represents a potential new value for the hidden state. GRU combines LSTM's forget and input gates into a single update gate $z_t \\in (0,1)$ which decides how much of the past information to carry forward (i.e., 1 \u2013 $z_t$) and how much new information from the candidate hidden state to add (i.e., $z_t$). Additionally, LSTM's output gate is removed and instead, a reset gate $r_t$ is added that controls how much past information is used in computing the candidate hidden state. GRU reduces the total number of parameters and computations, requiring only $O(3d_h(d_x+d_h))$ parameters. However, GRUs and LSTMs are only computable sequentially. As a result, during training they require backpropagating their gradients through time (BPTT), requiring linear training time and greatly limiting their ability to scale to long contexts."}, {"title": "2.3 Parallel Scan", "content": "Due to this limitation, Transformers replaced LSTMs and GRUs as the defacto sequence modelling method for years by leveraging parallelization during training. However, Transformers have a quadratic complexity in the sequence length, limiting their ability to scale to long contexts. Recently, a resurgence of many new recurrent models have been proposed as replacements for Transformers that achieve comparable performance and are trainable in parallel, while avoiding the BPTT issue that traditional RNNs (e.g., LSTMs and GRUs) faced. Although many different architectures have been proposed, many of these models are efficiently trained using the parallel prefix scan algorithm (Blelloch, 1990).\nThe parallel scan algorithm is a parallel computation method for computing $N$ prefix computations from $N$ sequential data points via an associative operator $\\oplus$ (e.g., + and \u00d7). The algorithm efficiently computes {$\\oplus_{k=1}^{i} u_k$}$_{i=1}^{N}$ from {$u_k$}$_{k=1}^{N}$. In particular, we can apply the parallel scan method for efficiently computing a popular family of functions: $v_t = a_tu_{t-1} + b_t$ where $v_t, a_t, b_t \\in \\mathbb{R}$ and $v_0 = b_0$ (Heinsen, 2023). The method takes as input $a_1,..., a_n$ and $b_0, b_1,..., b_n$ and computes $V_1,..., V_n$ via parallel scans."}, {"title": "3 Methodology", "content": "Naturally, the aforementioned algorithm also extends to vectors: $v_t = a_t \\odot v_{t-1} + b_t$ where $\\odot$ is the element-wise multiplication. Interestingly, we can see that the GRU and LSTM state recurrences resemble the vector formulation. In this section, we show that GRUs and LSTMs are trainable via parallel scan by simplifying and removing several hidden state dependencies from their various gates. Building on this, we further simplify these RNNs by removing their constraints on output range, (i.e., $\\tanh$) and ensuring the outputs are time-independent in scale. Combining the steps, we describe minimal versions of GRUs and LSTMs (minGRUs and minLSTMs) that are trainable via parallel scan and perform comparably to Transformers and recently proposed sequence methods."}, {"title": "3.1 A Minimal GRU: minGRU", "content": ""}, {"title": "3.1.1 Step 1: Drop previous hidden state dependencies from gates", "content": "Revisiting GRU's hidden state recurrence which works as follows:\n\n$h_t = (1 - z_t) \\odot h_{t-1} + z_t \\odot \\tilde{h}_t$"}, {"title": "3.1.2 Step 2: Drop range restriction of candidate states", "content": "In GRU's hidden state recurrence, the proportion carried over from the previous hidden state (1-$z_t$) and the amount added for the new candidate hidden state ($z_t$) sum to 1. As a result, the scale of GRU's hidden state value is time-independent. Instead, the scale of its hidden state depends on that of its candidate hidden states $\\tilde{h}_t$. The hyperbolic tangent function ($\\tanh$) plays a crucial role in LSTMs and GRUs, restricting the range of (candidate) hidden states, i.e., $\\tilde{h}_t, h_t \\in (-1,1)^{d_h}$. The $\\tanh$ helps stabilize the training and mitigates vanishing gradients that result from applying sigmoid ($\\sigma$) activations to linear transformations of the hidden state (e.g., $z_t = \\sigma(\\text{Linear}_{d_h} ([x_t, h_{t-1}]))$). In the previous step, these hidden state dependencies were removed. As such, we can simplify GRU further by removing the range restriction ($\\tanh$) on the (candidate) hidden states as follows:\n\n$\\tilde{h}_t = \\tanh(\\text{Linear}_{d_h} (x_t)) \\qquad \\Rightarrow \\qquad \\tilde{h}_t = \\text{Linear}_{d_h} (x_t)$"}, {"title": "3.1.3 minGRU", "content": "Combining the two simplification steps results in a minimal version of GRU (minGRU):\n\n*GRU*\n$h_t = (1 - z_t) \\odot h_{t-1} + z_t \\odot \\tilde{h}_t$\n$z_t = \\sigma(\\text{Linear}_{d_h} ([x_t, h_{t-1}]))$\n$r_t = \\sigma(\\text{Linear}_{d_h} ([x_t, h_{t-1}]))$\n$\\tilde{h}_t = \\tanh(\\text{Linear}_{d_h} ([x_t, r_t \\odot h_{t-1}]))$\n*minGRU*\n$h_t = (1 - z_t) \\odot h_{t-1} + z_t \\odot \\tilde{h}_t$\n$z_t = \\sigma(\\text{Linear}_{d_h} (x_t))$\n$\\tilde{h}_t = \\text{Linear}_{d_h} (x_t)$\n\nThe resulting model is significantly more efficient than the original GRU (1) requiring only $O(2d_hd_x)$ parameters instead of GRU's $O(3d_h(d_x + d_h))$ parameters where $d_x$, $d_h$ corresponds to the sizes of $x_t$ and $h_t$ respectively. In terms of training, minGRU (2) can be trained in parallel using the parallel scan algorithm, speeding up training significantly. In Section 4.1, we show that this corresponded to a 175\u00d7 speedup in training steps for a sequence length of 512 on a T4 GPU. The parameter efficiency gains are also significant. Typically, in RNNs, state expansion is performed (i.e., $d_h = ad_x$ where $a \\ge 1$) allowing the models to more readily learn features from their inputs. minGRU uses approximately 33%, 22%, 17%, or 13% of parameters compared to GRU when $a = 1, 2, 3,$ or 4 respectively."}, {"title": "3.2 A Minimal LSTM: minLSTM", "content": ""}, {"title": "3.2.1 Step 1: Drop previous hidden state dependencies from gates", "content": "Revisiting LSTMs, we focus on their cell state recurrence which works as follows:\n\n$c_t = f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}_t$\n\nSimilar to GRU's hidden state, we can see that LSTM's cell state recurrence resembles the aforementioned parallel scan's formulation $v_t = a_t \\odot v_{t-1} + b_t$ where $a_t \\leftarrow f_t$, $b_t \\leftarrow i_t \\odot \\tilde{c}_t$, and $v_t \\leftarrow c_t$. However, $f_t$, $i_t$ and $\\tilde{c}_t$ are dependent on the previous hidden state $h_t$. As such, LSTM's cell state recurrence is unable to apply the parallel scan algorithm as is. We can address this in a similar fashion to GRU by removing their hidden state dependencies as follows:\n\n$f_t = \\sigma(\\text{Linear}_{d_h} ([x_t, h_{t-1}])) \\qquad \\Rightarrow \\qquad f_t = \\sigma(\\text{Linear}_{d_h} (x_t))$\n$i_t = \\sigma(\\text{Linear}_{d_h} ([x_t, h_{t-1}])) \\qquad \\Rightarrow \\qquad i_t = \\sigma(\\text{Linear}_{d_h} (x_t))$\n$\\tilde{c}_t = \\tanh(\\text{Linear}_{d_h} ([x_t, h_{t-1}])) \\qquad \\Rightarrow \\qquad \\tilde{c}_t = \\tanh(\\text{Linear}_{d_h} (x_t))$"}, {"title": "3.2.2 Step 2: Drop range restriction of candidate states", "content": "Similar to GRUs, LSTMs leverage the hyperbolic tangent function ($\\tanh$) to restrict the range of its states between (-1,1). LSTMs apply the range restriction twice: once when computing the candidate cell state and once computing its hidden state. In this step, we drop both as follows:\n\n$\\tilde{c}_t = \\tanh(\\text{Linear}_{d_h} (x_t)) \\qquad \\Rightarrow \\qquad \\tilde{c}_t = \\text{Linear}_{d_h} (x_t)$\n$h_t = o_t \\odot \\tanh(c_t) \\qquad \\Rightarrow \\qquad h_t = o_t \\odot c_t$"}, {"title": "3.2.3 Step 3: Ensure output is time-independent in scale", "content": "In many sequence modelling settings (e.g., text generation), the optimization objective/target is time-independent in scale. Recall LSTM's cell state recurrence $c_t = f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}_t$ where $i_t, f_t \\in (0,1)^{d_h}$, and GRU's hidden state recurrence\u00b2, $h_t^{GRU} = (1-z_t) \\odot h_{t-1}^{GRU} + z_t \\odot \\tilde{h}_t^{GRU}$ where $z_t \\in (0,1)^{d_h}$. GRUs retain $(1 - z_t) \\in (0,1)$ of the previous hidden state and add $z_t$ of the new candidate state. Since these proportions sum to 1, the model ensures its outputs (i.e., hidden states) are time-independent in scale. In contrast, LSTM's forget and input gates are computed independently (e.g., $f_t,i_t \\rightarrow 1$ or $f_t, i_t \\rightarrow 0$), making its cell states time-dependent in scale\u00b3 and optimization more difficult. As such, we ensure LSTM's output is time-independent in scale.\nTo do so, we can simply normalize the two gates, i.e., $\\tilde{f}_t = \\frac{f_t}{f_t + i_t}, \\tilde{i}_t = \\frac{i_t}{f_t + i_t}$ ensuring that $\\tilde{f}_t + \\tilde{i}_t = 1$ and the scale of LSTM's cell state is time-independent. Ensuring that the hidden state is time-independent in scale, we also drop the output gate $o_t$ which scales the hidden state. Without the output gate, the normalized hidden state is equal to the cell state, i.e., $h_t = o_t \\odot c_t \\Rightarrow h_t = c_t$, making having both a hidden and cell state unnecessary. As such, we drop the cell state as well. In summary, the modifications are as follows:\n\n$h_t = o_t \\odot c_t \\qquad \\Rightarrow \\qquad h_t = \\tilde{f} h_{t-1} + \\tilde{i} \\tilde{h}_t$\n$o_t = \\sigma(\\text{Linear}_{d_h} (x_t)) \\qquad \\Rightarrow \\qquad h_t = \\text{Linear}_{d_h} (x_t)$\n$c_t = f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}_t \\qquad \\Rightarrow \\qquad \\tilde{f} = \\frac{f_t}{f_t + i_t}, \\tilde{i} = \\frac{i_t}{f_t + i_t}$\n\nNotably, GRUs do not need this step as their outputs are already time-independent in scale."}, {"title": "3.2.4 minLSTM", "content": "Combining the three steps results in a minimal version of LSTM (minLSTM):"}, {"title": "4 Were RNNS All We Needed?", "content": "In this section, we compare the minimal versions (minLSTMs and minGRUs) with their traditional counterparts (LSTMs and GRUs) and modern sequence models. Pseudocode, PyTorch implementation, and detailed information regarding the experiment setup are available in the Appendix."}, {"title": "4.1 Minimal LSTMs and GRUs are very efficient", "content": "At test time, recurrent sequence models are rolled out sequentially, making their inferences efficient. Instead, the bottleneck of traditional RNNs is their training which requires linear training time (backpropagating through time) which resulted in their eventual deprecation. The renewed interest in recurrent sequence models is due to many new architectures being efficiently trained in parallel (Gu et al., 2021). In this section, we compare the resources required to train the traditional RNNs (LSTM and GRU), their minimal versions (minLSTM and minGRU), and a recent state-of-the-art sequence model. In particular, we focus on the comparison with Mamba (Gu & Dao, 2024) which has seen significant popularity recently. For these experiments, we consider a batch size of 64 and vary the sequence length. We measure the total runtime and memory complexity of performing a forward pass through the models, computing a loss, and computing gradients via a backward pass.\nRuntime. In terms of runtime (see Figure 1 (left)), the simplified versions of LSTM and GRU (minLSTM and minGRU) Mamba achieve similar runtimes. Averaging over 100 runs, the runtime for sequence lengths of 512 for minLSTM, minGRU, and Mamba were 2.97, 2.72, and 2.71 milliseconds respectively. For a sequence with length 4096, the runtime were 3.41, 3.25, and 3.15 respectively. In contrast, the traditional RNN counterparts (LSTMs and GRUs) required a runtime that scaled linearly with respect to sequence length. For a sequence length of 512, minGRUs and minLSTMs were 175\u00d7 and 235\u00d7 faster per training step (see Figure 1 (middle)) than GRUs and"}, {"title": "4.2 Minimal LSTMs and GRUs perform well", "content": "In the previous section, we showed the significant efficiency gains achieved by simplifying traditional RNNs. Here, we explore the empirical performance aspect of these minimal versions of LSTMS and GRUs compared to several popular sequence models.\nSelective Copy. We consider the long-range Selective Copying task from the Mamba paper (Gu & Dao, 2024). Unlike the original Copying task (Arjovsky et al., 2016), the Selective Copying task's input elements are randomly spaced relative to their output, making the task harder. To solve the task, models are required to perform content-aware reasoning, memorizing relevant and filtering out irrelevant tokens.\nIn Table 2, we compare the simplified versions of LSTMs and GRUs (minLSTM and minGRU) against well-known recurrent sequence models that can trained in parallel: S4 (Gu et al., 2021), H3 (Fu et al., 2023), Hyena (Poli et al., 2023), and Mamba (S6) (Gu & Dao, 2024). The results for these baselines are quoted from the Mamba paper. Out of all of these baselines, only S6 from Mamba's paper is capable of solving this task. minGRU and minLSTM are also capable of solving"}, {"title": "5 Related Work", "content": "In this section, we provide a discussion of the similarities and differences between existing recurrent sequence models and the simplified versions of LSTMs and GRUs (minLSTM and minGRU).\nState-Space Models (SSMs). Although Mamba (Gu & Dao, 2024) and state-space models have gained significant popularity recently, the steps towards the recent success of Mamba began years ago. Gu et al. (2020) first proposed a discretized structured state-space model. Gu et al. (2021) scaled the idea up, introducing S4. The success of S4 became the basis for many future works (Gu et al., 2022; Gupta et al., 2022; Hasani et al., 2023; Smith et al., 2023) and state-space model applications in language (Mehta et al., 2023), audio (Goel et al., 2022), and more. Recently, Mamba was a significant breakthrough in SSM, outperforming previous methods and garnering substantial attention. A major novelty in Mamba was the proposal of S6, a state-space model whose transition matrices are input-dependent (i.e., $A_t$ and $B_t$ are functions of $x_t$). In contrast, earlier state-space model transition matrices were input-independent, limiting their expressivity. The success of Mamba and state-space models led to the writing of several survey papers (Wang et al., 2024; Patro & Agneeswaran, 2024; Qu et al., 2024).\nRecurrent Versions of Attention. Another direction that proposed efficient recurrent sequence models is that of attention. Building on variations of linear attention (Katharopoulos et al., 2020), several papers have introduced recurrent versions that can be computed in parallel. Notably, Sun et al. (2023) and Qin et al. (2023) introduced variants that use an input-independent gating mechanism (decay factor). More recently, Katsch (2023) and Yang et al. (2024) proposed linear attention variants that use input-dependent gating. Feng et al. (2024) showed softmax attention can be viewed as an RNN and proposed a recurrent model based on their RNN formulation.\nParallelizable RNNs. Alternatively, several papers have proposed RNNs that can be trained efficiently in parallel. Orvieto et al. (2023) proposed an RNN that leverages complex diagonal recurrences and an exponential parameterization. Beck et al. (2024) proposed various enhancements to LSTM such as exponential gating, covariance update rule, and a normalizer state.\nAlthough these three directions of designing efficient recurrent sequence models have proposed vastly different architectures, the core recurrent component of these models is remarkably similar. For example, although state-space models are typically written as $h_t = A_th_{t-1}+B_tx_t$, in practice, the transition matrices are typically diagonal for efficiency reasons. As such, Mamba's S6 (Gu & Dao, 2024) can be viewed as $h_t = diag(A_t)h_{t-1}+diag(B_t)x_t$ where $A_t$ and $B_t$ are functions of $x_t$. In contrast, consider the minimal version of GRU $h_t = (1 - z_t) \\odot h_{t-1} + z_t \\odot \\text{Lineard}_h (x_t)$ and minimal version of LSTM $h_t = \\tilde{f} \\odot h_{t-1} + \\tilde{i} \\text{Lineard} (x_t)$. The recurrences of these models are similar. The major difference between these minimal RNNs, Mamba's S6, and other models is how their transitions (e.g., $z_t, i_t, f_t, A_t$, and $B_t$) are computed from the input token $x_t$.\nParallel Scan. Generalizing across the families of methods (including minLSTM and minGRU), these recent sequence models can be viewed as members of the same family of functions trainable via a parallel scan: $v_t = a_t v_{t-1} + b_t$ (see Section 2.3) where $a_t$ and $b_t$ are functions of the input token $x_t$. Improving upon the parallel scan algorithm, several models (Yang et al., 2024; Gu & Dao,"}, {"title": "6 Conclusion", "content": "In this work, we revisited RNNs from over a decade ago: LSTMs and GRUs. We show that these models are trainable via the parallel scan algorithm by removing their hidden state dependencies from their gates. Simplifying these models further, we removed their constraints on output range and ensured their output was time-independent in scale. These steps result in their minimal versions (minLSTM and minGRU). Empirically, we showed that minLSTM and minGRU (1) address the computational limitations of their traditional counterparts and (2) are as computationally efficient as Mamba, a popular recent state-of-the-art recurrent sequence model, and (3) are competitive in performance with recent sequence models. Considering the strong empirical performance of these simplified RNNs and their fundamental similarities with many recently proposed recurrent sequence methods, we question \"Were RNNs all we needed?\"\nLimitations\nOur experiments were run on P100 (16 GBs) and T4 (16 GBs) GPUs. Due to computation limitations, our experiments are smaller in scale compared to works such as Mamba (Gu & Dao, 2024) which leveraged A100 80GB GPUs. To fit the selective copy task on the GPU, we leveraged gradient accumulation for training, splitting the standard batch size in half and slowing training significantly. Nonetheless, we hypothesize that these conclusions generalize to larger-scale settings due to the fundamental similarities between the minimal RNNs (minLSTM and minGRU) and many recent sequence methods."}, {"title": "A Implementation Details: Vanilla Version", "content": "In this section, we provide the pseudocode and equivalent PyTorch code for minGRU and minL-STM. When performing repeated multiplications such as in many recurrent sequence models, numerical instabilities are common, especially during training. As such, we trained using a log-space implementation (see Section B) for improved numerical stability."}, {"title": "A.1 Pseudocode: Vanilla Version", "content": ""}, {"title": "A.1.1 minGRU: A Minimal GRU", "content": ""}, {"title": "A.1.2 minLSTM: A Minimal LSTM", "content": ""}, {"title": "A.2 PyTorch Code: Vanilla Version", "content": ""}, {"title": "A.2.1 minGRU: A Minimal GRU", "content": ""}, {"title": "A.2.2 minLSTM: A Minimal LSTM", "content": ""}, {"title": "B Implementation Details: Log-Space Version (Additional Numerical Stability)", "content": "In this section, we detail the log-space version of minLSTM and minGRU for improved numerical stability. During training, the parallel modes are used to avoid backpropagation through time (BPTT), speeding up the training time significantly. At inference time, the sequential modes are used."}, {"title": "B.1 Parallel Scan: Log-Space Implementation", "content": "Recall that, the parallel scan's objective is to compute $h_{1:t}$ where $h_k = a_kh_{k-1} + b_k$. In code, the vanilla parallel scan function would take as input: coefficients $a_{1:t}$ and values $b_{1:t}$. The function then outputs $h_{1:t}$. For numerical stability, we consider a log-space implementation which takes as input $\\log(a_{1:t})$ and $\\log(b_{1:t})$ instead and outputs $h_{1:t}$. The code for the parallel scan in log-space is included below and is based on the code by Heinsen (2023)."}, {"title": "B.2 Pseudocode: Log-Space Version", "content": "For maximal numerical stability, we rewrite the log-space versions of minGRU and minLSTM."}, {"title": "B.2.1 minGRU: A Minimal GRU", "content": "Recall minGRU's recurrence is as follows $h_t \\leftarrow (1 - z_t) \\odot h_{t-1}+z_t \\odot \\tilde{h}_t$. As such, $a_t \\leftarrow (1 - z_t)$ and $b_t \\leftarrow z_t \\odot \\tilde{h}_t$ where $z_t = \\sigma(k_t)$ and $k_t = \\text{Lineard}_h (x_t)$. As a result, $\\log(a_t) \\leftarrow \\log(1 - z_t)$ and $\\log(b_t) \\leftarrow \\log(z_t) + \\log(\\tilde{h})_t$. We can break down these down as follows:\n\n$\\log(z_t) = \\log(\\sigma(k_t))$\n$= \\log\\left(\\frac{1}{1+ \\exp(-k_t)}\\right)$\n$= -\\text{Softplus}(-k_t)$\n$\\log(1- z_t) = \\log\\left(\\frac{1}{1+ \\exp(k_t)}\\right)$\n$= \\log\\left(\\frac{1+ \\exp(-k_t)}{1+ \\exp(k_t)}\\right)$\n$= -\\text{Softplus}(k_t)$"}, {"title": "B.2.2 minLSTM: A Minimal LSTM", "content": "We also derive minLSTM's log-space formulation as well. Recall minLSTM's recurrence is as follows $h_t \\leftarrow \\tilde{f} \\odot h_{t-1} + \\tilde{i} \\tilde{h}_t$. As such, $a_t \\leftarrow \\tilde{f}_t$ and $b_t \\leftarrow \\tilde{i} \\tilde{h}_t$. As a result, $\\log(a_t) \\leftarrow \\log(\\tilde{f}_t)$ and $\\log(b_t) \\leftarrow \\log(\\tilde{i}_t) + \\log(\\tilde{h}_t)$.\n\n$\\log(\\tilde{f}_t) = \\log\\left(\\frac{f_t}{f_t + i_t}\\right)$\n$= \\log\\left(\\frac{1}{1+ \\frac{i_t}{f_t}}\\right)$\n$= - \\log\\left(1+ \\frac{i_t}{f_t}\\right)$\n$= -\\log \\left(1+\\exp\\left(\\log \\left(\\frac{i_t}{f_t}\\right)\\right)\\right)$\n$= -\\text{Softplus} \\left(\\log \\left(\\frac{i_t}{f_t}\\right)\\right)$\n$= -\\text{Softplus} (\\log(i_t) - \\log(f_t))$"}, {"title": "B.3 PyTorch Code: Log-Space Version", "content": ""}, {"title": "B.3.1 minGRU: A Minimal GRU", "content": ""}, {"title": "B.3.2 minLSTM: A Minimal LSTM", "content": ""}, {"title": "C Detailed Experiment Setup", "content": "In this section, we describe the experiment setup in detail."}, {"title": "C.1 Architecture", "content": "In all models, residual connections are added between layers.\nSelective Copying. Each layer in the model consisted of (1) either a minLSTM or minGRU layer and (2) a linear layer.\nReinforcement Learning."}]}