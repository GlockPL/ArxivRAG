{"title": "DimensionX: Create Any 3D and 4D Scenes from a Single Image with Controllable Video Diffusion", "authors": ["Wenqiang Sun", "Shuo Chen", "Fangfu Liu", "Zilong Chen", "Yueqi Duan", "Jun Zhang", "Yikai Wang"], "abstract": "In this paper, we introduce DimensionX, a framework designed to generate photorealistic 3D and 4D scenes from just a single image with video diffusion. Our approach begins with the insight that both the spatial structure of a 3D scene and the temporal evolution of a 4D scene can be effectively represented through sequences of video frames. While recent video diffusion models have shown remarkable success in producing vivid visuals, they face limitations in directly recovering 3D/4D scenes due to limited spatial and temporal controllability during generation. To overcome this, we propose ST-Director, which decouples spatial and temporal factors in video diffusion by learning dimension-aware LoRAs from dimension-variant data. This controllable video diffusion approach enables precise manipulation of spatial structure and temporal dynamics, allowing us to reconstruct both 3D and 4D representations from sequential frames with the combination of spatial and temporal dimensions. Additionally, to bridge the gap between generated videos and real-world scenes, we introduce a trajectory-aware mechanism for 3D generation and an identity-preserving denoising strategy for 4D generation. Extensive experiments on various real-world and synthetic datasets demonstrate that DimensionX achieves superior results in controllable video generation, as well as in 3D and 4D scene generation, compared with previous methods.", "sections": [{"title": "1. Introduction", "content": "In the context of computer graphics and vision, understanding and generating 3D and 4D content are pivotal to create realistic visual experiences. By representing spatial (3D) and temporal (4D) dimensions, videos serve as a powerful medium for capturing dynamic real-world scenes. Despite substantial advancements in 3D and 4D reconstruction technologies , there remains a critical shortage of large-scale 3D and 4D video datasets, limiting the potential for high-quality 3D and 4D scene generation from a single image. This scarcity poses a fundamental challenge in constructing photorealistic and interactive environments.\nFortunately, recent advancements in video diffusion models have shown considerable promise in understanding and simulating real-world environments . Driven by advanced video diffusion models, recent works  have made attempts to leverage the spatial and temporal priors embedded in video diffusion to generate 3D and 4D content from a single image. Despite these rapid developments, existing methods either concentrate on the object-level generation with video diffusion trained on static or dynamic mesh renderings  or employ time-intensive per-scene optimization for coarse scene-level generation (e.g., Score Distillation Sampling ). This leaves the generation of coherent and realistic 3D/4D scenes an open challenge.\nIn this paper, we present DimensionX, a novel approach to create high-fidelity 3D and 4D scenes from a single image with controllable video diffusion. While recent video diffusion models are capable of producing realistic results, it remains difficult to reconstruct 3D and 4D scenes directly from these generated videos, primarily due to their poor spatial and temporal controllability during the generation process. Our key insight is to decouple the temporal and spatial factors in video diffusion, allowing for precise control over each individually and in combination. To achieve the dimension-aware control, we establish a comprehensive framework to collect datasets that vary in spatial and temporal dimensions. With these datasets, we present ST-Director, which separates spatial and temporal priors in video diffusion through dimension-aware LoRAs. Additionally, by analyzing the denoising mechanics in video diffusion, we develop a training-free composition method that achieves hybrid-dimension control. With this control, DimensionX generates sequences of spatially and temporally variant frames, enabling the reconstruction of 3D appearances and 4D dynamic motions. To handle complex real-world scenes with our ST-Director, we design a trajectory-aware approach for 3D generation and an identity-preserving denoising mechanism for 4D generation. Extensive experiments demonstrate that our DimensionX outperforms previous methods in terms of visual quality and generalization for 3D and 4D scene generation, indicating that video diffusion models offer a promising direction for creating realistic, dynamic environments.\nIn summary, our main contributions are:\n\u2022 We present DimensionX, a novel framework for generating photorealistic 3D and 4D scenes from only a single image using controllable video diffusion.\n\u2022 We propose ST-Director, which decouples the spatial and temporal priors in video diffusion models by learning (spatial and temporal) dimension-aware modules with our curated datasets. We further enhance the hybrid-dimension control with a training-free composition approach according to the essence of video diffusion denoising process.\n\u2022 To bridge the gap between video diffusion and real-world scenes, we design a trajectory-aware mechanism for 3D generation and an identity-preserving denoising approach for 4D generation, enabling more realistic and controllable scene synthesis.\n\u2022 Extensive experiments manifest that our DimensionX delivers superior performance in video, 3D, and 4D generation compared with baseline methods."}, {"title": "2. Related work", "content": "Controllable Video Diffusion. The integration of additional conditions for conditional image and video generation has seen notable success. Much of the prior research in video generation has focused on injecting various control signals to guide diffusion models. For instance, some approaches control video diffusion using camera pose trajectories, often utilizing ControlNet , Pl\u00fccker coordinates embeddings , or other coordinate embeddings . Other methods leverage reference videos or object motion trajectories to drive generation. Additionally, several studies have explored other control signals, such as action-based controls , sketches and depth maps . However, these methods often rely on specific data annotations, which not only limit their scalability but also lead to performance degradation when external control signals are injected into video models. Rather than injecting additional control signals, some methods customize video diffusion models by fine-tuning on a set of reference videos with similar patterns , primarily focusing on Unet-based video diffusion models with spatial and temporal layers. However, this form of customization often lacks generalization. In this work, our method is designed for controlling the DiT-based  video diffusion model, CogVideoX, which leverages the 3D VAE and full attention to effectively blend spatial and temporal information. Our method eliminates the need for additional signal injection, offering scalability and generalization without sacrificing performance.\n3D Generation with Diffusion Priors. Leveraging 2D diffusion priors for generating 3D content has revolution-"}, {"title": "3. Methodology", "content": "Given a single image, our goal is to generate high-quality 3D and 4D scenes with controllable video diffusion. To achieve the effective control in respect of spatial and tem-"}, {"title": "3.1. Building Dimension-variant Dataset", "content": "To decouple spatial and temporal parameters in video diffusion, we introduce a framework to collect spatial- and temporal-variant videos from open-source datasets. Notably, we employ a trajectory planning strategy for spatial-variant data and flow guidance for temporal-variant data.\nTrajectory planning for spatial-variant data. To acquire the spatial-variant dataset, we propose reconstructing photorealistic 3D scenes and rendering videos consistent with our spatial variations. To facilitate the selection and planning of rendering paths, we need to compute the coverage range of the cameras throughout the entire scene. Given N cameras in a scene, we first compute the center C and principal axes A along the direction x, y, and z using the Principal Component Analysis (PCA) technique:\n$C = \\frac{\\sum_{i=1}^{N} p_i}{N}$,\n$A = SVD(P - C')$, (1)\nwhere $p_i$ denotes the position of camera i, $P = {p_i, 1 \u2264 i \u2264 N} \u2208 R^{N\u00d73}$ represents the position set of N cameras, and SVD is the Singular Value Decomposition operation. Next, we need to calculate the lengths L of each axis from the projection distance D:\n$D = (P - C') \u00b7 A$  (2)\n$L = max(D) \u2013 min(D)$. (3)\nBuilt on the above calculation, we have already figured out the distribution of the camera throughout the entire scene. To cope with various scenes, we establish the following rules to filter out the qualifying data: (1) Camera Distribution: We calculate the center of the scene and judge how cameras capture around the scene. (2) Bounding Box Aspect Ratio: The aspect ratio of the bounding box should meet the requirement for various S-Directors. For instance, the aspect ratio of x and y axis should not vary too greatly, which helps in selecting appropriate 360-degree surrounding videos. (3) Camera-to-Bounding Box Distance: We calculate the distance from each camera position to the closest plane of the bounding box and prioritize data with smaller total distances to ensure better camera placement.\nWith the filtered dataset, it is necessary to compute the occupancy field within the scene to help us plan the feasible region for the rendering cameras. After reconstructing the entire scene\u2019s 3DGS from multi-view images, we render multi-view images and their corresponding depth maps, and then use TSDF  to extract the scene\u2019s mesh from the RGB-D data. More details can be found in appendix.\nFlow guidance for temporal-variant data. To achieve the temporal control, we aim to filter the temporal-variant data to fine-tune the video diffusion model. Specifically, we leverage the optical flow  to filter out the temporal-variant videos. For temporal-variant videos, optical flow maps frequently exhibit extensive white regions, which could serve as an effective selection criterion."}, {"title": "3.2. ST-Director for Controllable Video Generation", "content": "Inspired by the concept of orthogonal decomposition in linear algebra, we propose a method to decouple spatial and temporal dimensions in video generation for more precise control. We conceptualize each video frame $I_t(u, v)$ as a projection from a 4D space $R^3 \u00d7 R^1$, where u and v are the image coordinates in the frame, and the 4D space consists of three spatial dimensions x, y, z and a temporal dimension t. In this framework, the 4D space consists of a static background $B \\subset R^3$ and dynamic objects $O_i^2(t) \\subset R^3$, where i indexes the moving objects at time t. The entire scene at time t is then represented as:\n$S(t) = B \\cup (\\cup_i O_i^2(t))$. (4)\nEach video frame at time t is, therefore, a 2D projection of this 3D scene structure onto the image plane, governed by the camera\u2019s parameters at that moment. To formalize this, we define the projection function $P_C(t)$, which projects the 3D scene S(t) onto a 2D image:\n$I_t(u, v) = P_{C(t)} (S(t))$, (5)\nwhere $C(t)$ represents the camera parameters at time t. However, this reduction from 4D to 2D is typically treated as an inseparable blend of spatial and temporal dimensions in most video generation methods, complicating independent control over each dimension, which makes its difficult to recover the 4D space. To address this, we introduce two orthogonal basis directors: the S-Director (Spatial Director) and the T-Director (Temporal Director), which allow us to separate spatial and temporal variations within the video generation process. With these orthogonal basis directors, we can more flexibly control video generation, generating frames along a single axis or combining"}, {"title": "3.2.1 Dimension-aware Decomposition", "content": "To decompose spatial and temporal variations, we define the spatial and temporal equivalence relations that capture the behavior of points in the 4D space under different conditions. More details can be found in our appendix. From these two equivalence relations, we derive two types of quotient spaces, $R^4/ \\sim_s$ and $R^4/ \\sim_T$. The S-Quotient Space, $R^4/\\sim_s$, captures the spatial trajectory of camera viewpoints in a video by collapsing the temporal dimension, as each frame represents different spatial perspectives at a single point in time. Conversely, the T-Quotient Space, $R^4/\\sim_T$, describes temporal object motion trajectories from a fixed camera position, collapsing the spatial dimension in terms of viewpoint, with the primary variation being temporal as objects move within the scene. Together, these two quotient spaces, representing spatial and temporal decompositions of the 4D space, enable us to interpret video as a structured decomposition of 4D space into distinct spatial and temporal components.\nBuilding on these quotient spaces, we associate videos from our spatial-variant dataset with the S-Quotient Space, while videos from our temporal-variant dataset correspond to the T-Quotient Space. In order to train the S-Director and T-Director to generate videos specifically within these spatial and temporal structures, we employ LoRA , a fine-tuning method that is both parameter-efficient and computationally light, training each director separately on one of the two datasets to guide the video diffusion model. Specifically, the S-Director is trained on the spatial-variant dataset, learning patterns in which time is held constant $(S(t) = S_0)$, thereby generating videos within the S-Quotient Space, as illustrated in the top right of Fig. 3, where $I_t(u, v) = P_{C(t)}(S_0)$. Similarly, the T-Director is trained on the temporal-variant dataset, learning patterns where the camera remains stationary $(C(t) = C_0)$, resulting in videos within the T-Quotient Space, as shown in the bottom right of Fig. 3, with $I_t(u, v) = P_{C_0} (S(t))$ ."}, {"title": "3.2.2 Tuning-free Dimension-aware Composition", "content": "With this orthogonal basis of directors, we achieve flexible control over video generation, where each director independently captures frame sequences along its designated axis, producing either $I_t(u, v) = P_{C_t}(S(0))$ or $I_t(u, v) = P_{C_0} (S(t))$, respectively. However, most videos naturally involve a blend of spatial and temporal elements, making it essential to combine both directors to capture richer, multifaceted perspectives within the 4D space, represented as $I_t(u,v) = P_{C_t} (S(t))$. To achieve this enhanced level of control, we aim to merge the S-Director and T-Director, allowing for dynamic adjustment that aligns video generation with specific spatial, temporal, or combined spatiotemporal intents. In pursuit of this goal, we examine the underlying mechanics both of the base model\u2019s and each director\u2019s de-"}, {"title": "3.3. 3D Scene Generation with S-Director", "content": "Built upon the S-Director, our video diffusion model is able to generate long-range consistent frames from a single image, allowing for the reconstruction of photorealistic scenes. To better generalize to real-world scenarios, where spatial variations are diverse and camera trajectories are highly flexible, we introduce a trajectory-aware mechanism to handle different camera movements. Specifically, to cover a wide range of camera trajectory patterns $C'(t)$, we train multiple types of S-Directors, each tailored to a specific camera motion. In the 3D world, camera movements are defined by 6 degrees of freedom (DoF), with each DoF allowing movement in both positive and negative directions for translation and rotation, resulting in 12 distinct motion patterns. Additionally, we also train orbital motion category S-Director, where the camera follows a smooth, circular path around the subject, capturing a unique perspective beyond the standard DoF-based movements. With diverse and controllable S-Directors, we adopt the trajectory-aware mechanism in both single-view and sparse-view settings, enabling generalizable generation of real-world 3D scenes.\nSingle-view Scene Generation. Given a single image I, our goal is to reconstruct the 3D scene with generated video frames {Ii}$_{i=1}^{N}$, where N represents the frame length. Although current video diffusion models have shown potential for long video generation, the total duration still falls far short of the frame count required for real-world scene reconstruction. Specifically, the powerful open-source video diffusion model (e.g. CogVideoX ) currently generates a maximum of only 49 frames, whereas reconstructing a large scene (e.g. 360 degree scene) typically requires hundreds of multi-view images. To address this, we extend the video diffusion model to generate 145 frames (three times the original frame count).\nSparse-view Scene Generation. In applications where the accuracy and detail in 3D scene generation are paramount, using sparse view inputs can significantly enhance the fidelity of the generated content. In this setting, we propose incorporating a video interpolation model and an adaptive S-Director to achieve a smooth and consistent transition between the sparse views. First, we develop a video diffusion model to generate the high-quality interpolated video, which takes two images as the start and end frames. Specifically, given two-view images {I1, I2}, we concatenate the noisy latent $z_2 = E(I_2)$ to the ending of sequential latents, mirroring the common practice of concatenating the first frame\u2019s latent $z_1 = E(I_1)$ with its noisy counterpart. The objective function for the video diffusion process is formulated as\n$\\mathcal{L}_{diffusion} = \\mathbb{E}_{z_t\\sim p, \\epsilon \\sim N(0, 1), t} [||\\epsilon - \\epsilon_{\\theta}(z_t, t, z_1, z_2, c)||_2^2]$, (6)\nwhere $z_t$ is the noisy latent sequence from training videos, and $\\epsilon_{\\theta}$ represents the model\u2019s prediction of the noise at timestep t, conditioned on the first and last frame latent: $z_1$ and $z_2$. With the interpolated video diffusion model, we then train various S-Directors to provide refined camera motion guidance, ensuring smooth and consistent transition between the sparse-view images. In particular, we tailor two key strategies to fully leverage the guidance prior carried in S-Directors: early-stopping training and adaptive"}, {"title": "3.4. 4D Scene Generation with ST-Director", "content": "Equipped with spatial and temporal controlled video diffusion, we can recover a high-quality 4D dynamic scene from a single image. A direct way is to stitch together the spatial-variant videos generated for each frame in temporal-variant videos into multi-view videos, which are then used to reconstruct the 4D scene. However, this method has a key challenges: 3D consistency. Maintaining consistency in the background and object appearance across spatial-variant videos is challenging, causing severe jitter and discontinuity in the 4D scene. To address the above issue, we propose an identity-preserving denosing strategy, including the reference video latent sharing and appearance refinement process, to enhance the consistency of all spatial-variant videos.\nGiven an input image I, our goal is to generate a photorealistic 4D scene with dynamic objects and high-quality backgrounds. First, we employ T-Director to generate a temporal-variant video frames ${I_t}_{t=1}^N$ for the input image, from which a reference frame $I_{ref}$ is selected to produce corresponding spatial-variant video frames $V_{ref} = {I_j}_{j=1}^K$, where K represents the number of cameras. Subsequently, $V_{ref}$ is used to guide the generation of spatial-variant videos"}, {"title": "4.5. Ablation Study", "content": "Trajectory-aware mechanism for 3D generation. In sparse view 3D generation, we leverage S-Director to guide video interpolation model. As illustrated in Fig. 7, when handling the large-angle sparse view, the absence of S-Director often results in the \"Janus problem\", where multiple heads are generated, significantly degrading reconstruction quality.\nIdentity-preserving denoising strategy for 4D generation. In 4D scene generation, we conduct experiments on real-world images to analyze our identity-preserving denoising for 4D scene generation. As shown in Fig. 8, we ablate the design of reference video latent sharing and appearance refinement in terms of the consistency among different frames of a novel view. Specifically, we can observe that directly combing per-frame videos causes severe inconsistency, including the background and subject shape. Through the reference video latent sharing, the global background and appearance exhibit a high consistency across different frames. Building on reference video latent sharing, appearance refinement enhances the coherence of appearance details."}, {"title": "5. Conclusion", "content": "In this paper, we introduce DimensionX, a novel framework to create photorealistic 3D and 4D scenes from only a single image with controllable video diffusion. Our key insight is to introduce ST-Director to decouple the spatial and temporal priors in video diffusion models by learning the dimension-aware LoRA on dimension-variant datasets. Furthermore, we investigate the denoising process of video diffusion and introduce a tuning-free dimension-aware composition to achieve the hybrid-dimension control. Powered by the controllable video diffusion, we can recover accurate 3D structures and 4D dynamics from the sequential generated video frames. To further enhance the generalization of our DimensionX in real-world scenes, we tailor a trajectory-aware strategy for 3D scene generation and an identity-aware mechanism for 4D scene generation. Extensive experiments on various real-world and synthetic datasets demonstrate that our approach achieve the state-of-the-art performance in controllable video generation, as well as 3D and 4D scene generation.\nLimitations and future work. Despite the remarkable achievements, our DimensionX is limited by the diffusion backbone. Although current video diffusion models are capable of synthesizing vivid results, they still struggle with understanding and generating subtle details, which restricts the quality of the synthetic 3D and 4D scenes. Additionally, the prolonged inference procedure of video diffusion models hampers the efficiency of our generation process. In the future, it is worthy to explore how diffusion models can be integrated for more efficient end-to-end 3D and 4D generation. We believe that our research provides a promising direction to create a dynamic and interactive environment with video diffusion models."}]}