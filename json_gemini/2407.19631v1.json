{"title": "\u201cA Good Bot Always Knows Its Limitations\u201d: Assessing Autonomous System Decision-making Competencies through Factorized Machine Self-confidence", "authors": ["BRETT ISRAELSEN", "NISAR R. AHMED", "MATTHEW AITKEN", "ERIC W. FREW", "DALE A. LAWRENCE", "BRIAN M. ARGROW"], "abstract": "How can intelligent machines assess their competencies in completing tasks? This question has come into focus for autonomous systems that algorithmically reason and agentically make decisions under uncertainty (robots, self-driving/guided vehicles, etc.). It is argued here that machine self-confidence \u2013 a form of meta-reasoning based on self-assessments of an agent's knowledge about the state of the world and itself, as well as its ability to reason about and execute tasks \u2013 leads to many eminently computable and useful competency indicators for such agents. This paper presents a culmination of work on this concept in the form of a computational framework called Factorized Machine Self-confidence (FaMSeC), which provides an engineering-focused holistic description of factors driving an algorithmic decision-making process, including outcome assessment, solver quality, model quality, alignment quality, and past experience. In FaMSeC, self-confidence indicators are derived from hierarchical 'problem-solving statistics' embedded within broad classes of probabilistic decision-making algorithms such as Markov decision processes. The problem-solving statistics driving self-confidence indicators are obtained by evaluating and grading probabilistic exceedance margins with respect to given competency standards, which are specified for each of the various decision-making competency factors by the informee (e.g. a non-expert user or an expert system designer). This approach allows \u2018algorithmic goodness of fit' evaluations to be easily incorporated into the design of many kinds of autonomous agents in the form of human-interpretable competency self-assessment reports. Detailed descriptions and running application examples for a Markov decision process agent show how two of the FaMSeC factors (outcome assessment and solver quality) can be practically computed and reported for a range of possible tasking contexts through novel use of meta-utility functions, behavior simulations, and surrogate prediction models.", "sections": [{"title": "1 INTRODUCTION", "content": "\u201cIt ain't what we don't know in life that gives us trouble. It's what we know that ain't so. \u201d\u2013 Will Rogers\n\u201cWe see a person do something, and we know what else they can do... we can make a judgement quickly. But our models for generalizing from a performance to a competence don't apply to AI systems. \u201d\u2013 Rodney Brooks, IEEE Spectrum interview, May 2023 [117]\nDespite their humble human origins, autonomous agents like intelligent robots and self-driving vehicles often succeed and fail at tasks in distinctly non-human ways. The combination of their sometimes superhuman capabilities \u2013 and equally 'super' failures \u2013 drives many ongoing debates about the proper role of autonomous technology, in which particular success/failure cases are singled out as evidence for broad claims of (in)capability. In reality, by their very non-humanness, the feats and follies of algorithmic agents should alert us to a couple of fundamentally vexing issues signaled in the opening quotes above: 1) the innate ability of humans to infer the competencies of their fellow autonomous human beings does not directly translate into an ability to infer the competencies of autonomous algorithmic agents; and 2) algorithmic agents generally lack an innate awareness of or ability to reason/communicate about their own competencies, which is something humans expect when interacting with others. The systematic inability to provide knowledge about their competency boundaries opens the door to technological misuse, disuse, and abuse [61, 85] \u2013 issues which are bound to persist as this technology continuously evolves. Misconceptions about the capabilities of algorithmic agents in particular can quickly take root and become hard to dispel, unless well-defined design and evaluation practices are instilled to support meaningful assessments [97, 109].\nShort of putting the genie back in the lamp or achieving the mythical holy grail of pure, human-free autonomy anytime soon [16], the major question that designers must grapple with today to ensure successful long-term uptake of autonomous technology is not whether the science says machines with generalizable superhuman abilities could/should co-exist with humans. Rather, it is whether the engineering behind such technology is sound and capable enough for people who are not roboticists, computer scientists, etc., to responsibly evaluate and use it for solving the problems they actually need to solve [63]. To examine this issue more deeply, this paper considers how the competencies of autonomous algorithmic agents can be identified in principled ways to aid designers, users, and ultimately agents themselves, in deciding how and when they should be entrusted to undertake specific decision-making tasks under uncertainty.\nPerceived competency is one of many factors that influence human trust in technology [61]. Echoing psychology and business parlance, we define competencies here broadly as the set of skills (generalizable, non-task-specific capabilities) and behaviors (observed performance and approaches to particular tasks) that an evaluator expects from an algorithmic agent if they are to execute tasks up to some desired standards [101]. One may consider the competency assessment problem from the viewpoint of a hypothetical prospective employer of artificially intelligent autonomous agents in a futuristic job market. How would the employer decide what to write"}, {"title": "2 BACKGROUND AND DEFINITIONS", "content": "This section introduces background for the major technical ideas and issues underlying competency self-assessment for algorithmic decision-makers. It opens with a brief review of Markov decision process (MDP) models, which describe a wide range of rational decision-making algorithms and are therefore useful archetypal representations for key concepts. A notional application involving an autonomous delivery vehicle operating in an adversarial environment is then presented to motivate and illustrate the various aspects of algorithmic decision making under uncertainty that underpin our view of the competency self-assessment problem. Formal definitions and a general framework for computational competency evaluation are then described, followed by considerations for practical implementation that motivate the formulation of our FaMSeC framework in the sequel.\nAlthough there are many different notions of competency evaluation, each is ultimately based on comparing the expected outcomes of an agent's decision-making process to actual outcomes obtained in reality. By examining different models of decision-making agents, the details of any such process can be refined to develop suitable competency self-assessment approaches. To this end, we focus here on a broad class of rational (utility-maximizing) algorithmic agents that perform decision-making under uncertainty.\nMarkov decision processes (MDPs) provide a rigorous, well-understood algorithmic framework for rational decision making that also furnishes agents with the ingredients for competency evaluation (cf. Figure 2 later). In addition to their widespread use in modern control theory, robotics, and AI, MDP constructs lie at the core of a much broader landscape of algorithms for decision-making under uncertainty, embodied for example by hierarchical decision processes, stochastic optimal control, partially observable Markov decision processes (POMDPs), bandit techniques, and reinforcement learning (RL) [69]. Hence, given their importance, prevalence, and familiarity to many researchers, MDPs will be referred to extensively in this work to examine ideas behind competency self-assessment for decision making algorithms from a model-based perspective. As discussed later in the paper, this does not exclude consideration of alternative rational decision making frameworks, as long as these furnish the same essential components for algorithmic decision-making which we hold to also inform competency self-assessment. A brief review of MDPs is provided next to establish concepts that are useful to develop algorithmic competency self-assessment techniques; see references such as [69, 106] for detailed reviews.\nAn MDP for a sequential decision-making problem is parameterized by a tuple $(S, A, T, R)$, where: S is the state space; A is the set of actions that an agent can take from each state; T is the state transition model, representing the conditional probability $P(s_{t+1}|s_t, a_t)$ of reaching state $s_{t+1} \\in S$ from state $s_t \\in S$ when taking action $a_t \\in A$ at instance t; and R represents the reward model, typically modeled as a function $r_t = r(s_t, a_t)$. By construction of T, the dynamics of the decision-making problem are Markovian, i.e. the probability of reaching $s_{t+1}$ is conditionally independent of all other past states given only the immediate past state $s_t$ and action $a_t$. For optimal sequential decision-making, a policy $\\pi(s)$ must be found to maximize a utility or value function V(s) for all valid $s \\in S$. In most MDPs, the value function typically corresponds to the expected discounted cumulative reward $V(s) = E[U(s)]$, where\n$U(s) = \\sum_{t=0}^{\\infty} \\gamma^t r_t,$\\nand $\\gamma \\in [0, 1)$ is a discount factor that trades off between near-term and long-term reward payoffs. The optimal policy thus corresponds to\n$\\pi^*(s) = \\arg \\max_{\\pi} V^*(s),$\\nwhere $V^*(s)$ is the optimum expected value function.\nThere are many approaches to obtaining $\\pi^*$ either exactly or approximately; this is also referred to as 'solving' the MDP. Hence, an MDP solver is an algorithm that operates on MDP specification (S, A, T, R, $\\gamma$) and returns a policy. The well-known value iteration algorithm [102, 106] provides a theoretically guaranteed way of finding $V^*$ and $\\pi^*$ through iterated solutions to the underlying Bellman equations corresponding to (2). However, due to computational limitations, value iteration is not always feasible or practical, since it requires exhaustively searching the entire state-action space S \u00d7 A to update estimates of V(s) until convergence. This is seriously challenging in MDPs with large state-action spaces, as well as in settings where the time to obtain a solution policy $\\pi$ is a key consideration (the two issues are typically correlated).\nAs such, it is common for tasks like autonomous mobile robot navigation to use a class of MDP solvers which approximates $V^*(s)$ and/or $\\pi^*$ within realistic operational constraints. For instance, many approximate MDP solvers find approximate policies $\\hat{\\pi}$ only over a restricted subset of the state-action space corresponding to states in S that are reachable from the current state s; other approximate solvers learn regression models of $\\pi^*(s)$ with respect to S to simplify the optimization process [69]. Of the approximate solvers in the former category, Monte Carlo Tree Search (MCTS) has emerged as one of the most popular \u2018online' MDP solvers, which yield good quality approximate policies $\\hat{\\pi}$ on demand for very large state-action spaces and provide improved approximations to optimal $V^*(s)$ and $\\pi^*$ with longer run times. Since the parameters required by the MCTS algorithm will be used later to illustrate the effect of different variables in competency evaluation for MDPs, some additional basic background is provided in Appendix A; more complete details can be found in [69, 70].\nThe need for approximations like MCTS hints at two fundamental premises that apply in real life to all rational decision-making agents, whether or not they are based on MDPs. Firstly, such agents are boundedly rational [93, 100]1. That is, agents programmed to operate rationally in the real world do not have infinite computing or representational power, and thus are inevitably imperfect reasoners with limited access to information and limited execution abilities. Secondly, such computational agents are designed by human designers who themselves are also boundedly rational. From these two premises, it follows that any agent's ability to execute tasks is influenced by a multitude of factors that fall into one of two broad categories: (1) those which were explicitly considered in the design of the decision making algorithm prior to deployment; and (2) those which were not. In other words, an algorithmic decision-making agent is built according to a specific set of design assumptions for its intended tasks. Assumptions are essential for allowing designers to focus limited computational resources and effort on particular aspects of decision-making problems that can be reasonably handled within the confines of bounded rationality. Assumptions are inherent, for instance, in the choice of state variables and policy solver parameters for MDP agents, as well as in the selection of training data to identify policies or world models for reinforcement learning agents. To the extent that design assumptions align with reality during actual task execution, there should be a high likelihood the agent will perform the task as competently as it was designed to. But, if any assumption should fail to hold, this likelihood diminishes, often drastically, and competency is typically compromised.\nIt is practically impossible to guarantee the validity of all design assumptions in all task contexts. This means the key question for competency assessment arguably centers on determining the likelihood and extent to which competencies for a particular agent operating could be impacted in a given operating context. These points will be examined in more detail through the illustrative example described next."}, {"title": "2.2 Illustrative Example: Autonomous Delivery in an Adversarial Setting", "content": "Figure 1 depicts an autonomous doughnut delivery truck (ADT) navigating a road network in order to reach a delivery destination, while avoiding a motorcycle gang (MG) that will steal the doughnuts if it intercepts the ADT. The MG's location is unknown but can be estimated using measurements from unattended ground sensors (UGS). The ADT's decision space involves selecting a sequence of discrete actions (i.e. go straight, turn left, turn right, go back, stay in place) in order to safely navigate along segments of the road network and minimize the time to reach a specified destination on the map for each delivery hop.\nThe ADT's motion, UGS readings, and MG's behavior are treated as random variables, and the problems of decision making and sensing are in general strongly coupled. Some trajectories through the network might allow the ADT to localize the MG via the UGS before heading to the delivery destination but incur a high time penalty. Other trajectories may afford rapid delivery with high MG location uncertainty but increase the risk of getting caught by the MG, which can take multiple paths. A human dispatch supervisor monitors the ADT during operation. The supervisor does not have detailed knowledge of or control over the ADT, but can (based on whatever limited information is available) decide to proceed with or abort the delivery run before it starts. The extent to which the ADT can complete the assigned task, i.e. its competency, will clearly be a significant factor in the supervisor's evaluation.\nTo reach its destination, the ADT in the adversarial delivery problem must make a sequence of decisions within a discrete state and action space. An MDP is well-suited for this problem. Specifically, the physical states s describing the combined motion of the ADT and MG can be discretized in time and space to produce a Markov process model defined by some initial joint state probability distribution and joint state transition matrix T, which depends on the steering actions a taken by the ADT. A reward function R(sk, ak) = Rk can be specified for discrete decision instances (time steps) k to encode user preferences over the combined state of the ADT and MG, for instance: $R_k = -100$ for each time step the ADT is not co-located with the MG but not yet at the goal; $R_k = -1000$ if the ADT and MG are co-located; and $R_k = +1000$ if the ADT reaches the goal without getting caught. The reward values here are selected arbitrarily for illustrative purposes, but primarily reflect preferences for the ADT's actions and state occupancy. As detailed above, they can be tuned to induce more/less risk averse ADT behavior with respect to an overall utility or value function. If the MG's state is observable at each step k (due to very reliable and accurate UGS that cover all areas of the road network), then the ADT's planning problem can be framed as an MDP. Otherwise, probabilistic beliefs about the MG's state can be inferred from the UGS observations,"}, {"title": "2.2.1 Competency Assessment for MDP-based ADT", "content": "Recall the two categories of design factors in boundedly rational agent design: (1) those which are explicitly considered; and (2) those which are not. For instance, factors in category (1) include the typical dynamics of the ADT navigating along the road network, as well as the existence and typical behaviors of the MG. Factors in category (2) could include how unusual weather and road conditions impact the UGS. Other factors beyond the supervisor's or ADT's control which could also impact the ADT's capabilities include whether enough computing power has been allocated onboard the ADT to identify safe and efficient navigation solutions in all situations. The nature of the task also matters. Consider that an ADT which can deliver donuts is just as likely capable of delivering bread loaves or medical supplies under similar circumstances, since such cargo does not fundamentally alter the task's difficulty\u00b2. On the other hand, the imposition of post hoc constraints like minimizing fuel consumption, avoiding potholes, or strictly obeying speed limits at all times will (all else being equal) clearly be impactful, if designers did not originally account for these.\nAs discussed in [20, 45], explicitly delineating and reasoning over the built-in set of design assumptions provides a powerful starting point for determining whether a task is within an agent's expected competency range. We consider an alternative strategy for competency self-assessment that relies on the explicit construction and evaluation of various indicators to determine whether certain necessary conditions for decision-making competent behavior are met. A key distinction from previous work is that we separate decision-making competent behavior into two related but distinct levels. For one level of competency, the only concern is with what outcomes the agent achieves. On another level, we are also interested in examining how the agent achieves those outcomes. Whereas previous treatments have largely focused one or the other of these aspects of competency, we argue that both aspects must be treated simultaneously within a holistic self-evaluation framework for autonomous systems. The next subsection provides a formal framework to rigorously ground these ideas for algorithmic competency assessment, before considering how such indicators can be practically evaluated for agents in operating in different contexts."}, {"title": "2.3 The Autonomous Tasking and Competency Evaluation Process", "content": "Autonomous competency self-assessment is best viewed from the perspective of an agent attempting to complete a delegated task. In the adversarial delivery problem described above for example, the agent is the software which makes decisions autonomously for the ADT, so that it can meet its two assigned objectives of avoiding the MG and reaching the delivery destination in a timely manner. Since the agent will render decisions that affect the ADT's physical state and evolution in the road network environment, the agent also has an effect on how parts of the environment (in this case, the MG) will 'respond' and thus render outcomes. However, the agent cannot affect all parts of the environment that impact the agent's ability to perform the task and achieve desired outcomes (e.g. the layout/structure of the road network). This example shows that it is not generally possible to analyze or understand competency by looking at a software agent in a vacuum. Rather, the assigned task and the context in which it must operate in must be considered together\u00b3."}, {"title": "2.4 Strategies for Implementing Algorithmic Competency Self-Assessments", "content": "The assumption alignment tracking approach of [20, 45] is an example instance of the framework in Fig. 2, whereby multiple binary indicator functions are used to assess whether assumptions essential to A's ability to perform the task hold during operation in different contexts C. However, it is generally quite difficult to capture and validate all of the assumptions implicit in a given algorithm design for a particular task. The number of assumptions underlying a sophisticated decision making agent performing complex real-world tasks can be staggeringly large. Indeed, it is not hard to imagine how a combined set of programs required to check these assumptions could quickly become more computationally complex and expensive to run than the original decision-making algorithm itself. Moreover, a designer cannot know with certainty in advance which assumptions are later likely to have a significant impact or enumerate all the possible ways in which they may be violated. The violation of some engineering assumptions also may well change the probabilities of success or failure at a given task, without necessarily completely ruling either possibility out. For example, the ADT could 'get lucky', e.g. if the MG suffers mechanical failures. Decision-making algorithms can also discover systematic loopholes such as \u2018reward hacking' behaviors that deviate from designer intent without necessarily breaking design logic or violating assumptions.\nIt is also important to consider how competency self-assessments are communicated to human informees such as supervisors. Human supervisors of other (human/non-human) agents typically do not scrutinize intrinsic task assumptions before deciding what tasks to delegate to whom. Rather, it is more natural for them to form a theory of mind and reason about the possible evolution of other agents' behaviors, on the basis of attributed percepts, beliefs, goals, intentions, and capabilities [9, 62]. The problem of automation surprise in airline pilots, for example, can be explained in"}, {"title": "3 FACTORIZED MACHINE SELF-CONFIDENCE", "content": "The core idea of this work is that competency self-assessment for rational autonomous agents can be realized through a hierarchical process of machine self-confidence computation. In particular, for an agent A acting via probabilistic decision-making algorithms: self-confidence evaluation corresponds to automated reasoning over a set of embedded problem-solving statistics. In their raw form, these statistics describe different (but interrelated) aspects of the A's expected and observed abilities to achieve particular outcomes O in given contexts C. When assessed collectively in a metacognitive light, these statistics also convey information about the quality of such predictions and thus also about the fitness of A's decision-making process for the task and context C at hand. Where do these statistics come from and how are they defined? Like all statistics, they derive from quantitative measures that knowledgeable human expert evaluators (e.g. system designers, or policy creators) would use to assess the validity of operational assumptions in different tasks and context settings. Formal translation of these measures provides the basis for design of competency indicators I and standards \u2211 for self-evaluation and subsequent self-assessment. If properly communicated, algorithmic assurances based on competency self-evaluations and self-assessments can provide valuable insights about agent competency to human informees. It is assumed here that informees in general lack detailed technical knowledge about the the inner workings of A's decision-making algorithms, but have sufficient knowledge to carry expectations about desired outcomes O and acceptable agent behaviors, in order to inform the design of I and choices for \u03a3.\nThis line of reasoning follows the original notion of machine self-confidence described by Hutchins, et al. [56]. The major novelty considered in the present work is an automated algorithmic implementation of the machine self-confidence evaluation and reporting process, which accounts for multiple competency factors (i.e. indicators I) driven by the aforementioned problem-solving statistics. Figure 3 provides a block diagram view of a typical MDP-type autonomous algorithmic rational decision-maker A, which: represents its task, environment, and own state through a model; uses the model within a solver to devise a plan for enacting decisions that align with parsed user intentions (e.g. represented by an objective function for optimization); has the ability to predict, observe, and record possible outcomes, as well as its own and the environment's states; and may acquire user feedback on its performance. This diagram shows where the problem-solving statistics involved with self-confidence assessments are generated and combined into five different competency indicator functions, defined as follows in terms of the questions they address about different components of the mental process model for A:\n1.  IA-alignment with user intent: To what extent were the user's intentions properly understood and translated by A into context-appropriate mission specifications and tasks? This factor derives from features and parameters of the 'Parse Intentions' block. For instance, if a natural language interface is used for mission planning, this factor could assess how well A believes user inputs are correctly mapped to reward functions for different mission profiles. Competency standards \u2211 here would thus reflect sufficient benchmarks for demonstrable understanding of user intent and objective function alignment, e.g. in terms of ranking specific state trajectories and actions in accordance with user preferences [95].\n2.  IM-model and data validity: Are A's learned and/or assumed models used for decision-making good enough proxies for the real world? This factor assesses how well the set of measurements and events predicted by the autonomous system line up with what it actually observes in reality. In this case \u2211 would therefore generally reflect thresholds for goodness of fit between models and observed data.\n3.  Is-solver quality: Are the approximations and learning-based adaptations used by A for solving decision-making problems appropriate for the given task and model? For MDPs, this translates to evaluating the quality of a particular policy. Since approximations are almost always needed to solve otherwise intractable decision making problems, this factor examines the extent to which such approximations are appropriate. This factor also accounts for the impact of any learning mechanisms required to make complex decisions under uncertainty, e.g. based on suitability of training data or the learning process to solving the problem at hand. As will be elaborated in Sec. 5.2, \u2211 here represents a minimum acceptable level of"}, {"title": "3.1 Expected Properties of FaMSeC Indicators", "content": "The left column of Table 1 describes example hypothetical variations of the nominal ADT delivery problem in the left column, selected to examine expected behaviors for the five indicators. In relation to Fig. 2, each scenario represents a different type of shift in the task context C for the ADT agent A; in relation to Fig. 3, each scenario alters one or more of the components of the block diagram defining the inputs to each FaMSeC indicator. Here, the outcome of interest O expresses a conjunction of two conditions: (i) whether the ADT reaches the goal within a specified time, and (ii) whether it does so without getting captured by the MG. This could be expressed as a binary \u2018yes/no' outcome or a number, e.g. accumulated reward or time to reach the goal, which may be assigned an arbitrarily large value if captured. A does not update its MDP model with each task experience, but can record previous experiences. The same nominal discrete-time/discrete-state transition model T, approximate MCTS policy solver, and R are assumed in all cases, per Sec. 2.2. The right column of Table 1 shows the expected behaviors for the most relevant FaMSeC indicators in each case, using notional values for their fixed ranges and standards \u03a3. Looking across these scenarios, the basic expected properties of each indicator can be understood.\nIn scenario 1, IA takes on a 'high' confidence value because it is given (or inferred) that the MDP used by the ADT reflects an exact (or nearly so) understanding of how the supervisor wants the"}, {"title": "4 OUTCOME ASSESSMENT", "content": "Simply stated, evaluating Io consists in comparing the outcomes O produced by agent A (where O is described by random variables in the MDP setting) to the desired outcomes Odes for a given task (where Odes is a portion of the full standard, \u2211, related to outcomes for a given context C). Given a task, Odes represents how outcomes of interest should look according to the evaluator's utility (or utilities), while O are the outcomes expected/actually generated by the agent following its prescribed utility. Odes and O could be formulated in terms of rewards, but generally can be expressed in other ways. The main idea of evaluating Io this way is to assess the value of assigning a task to A (with its given state-action model, solver, and reward function in the MDP setting). The difference between Odes and O is quantified as another random variable, and the distribution of this random variable is interpreted via Io. More probability mass for high/low differences indicates that the A believes Odes will be harder/easier to typically achieve per evaluator expectations, resulting in lower/higher confidence in Io. A key implication is that built-in utilities like the MDP value function V(s) are insufficient on their own to define Io, although they still reveal useful competency information. The concept of a meta-utility is thus formally introduced to define Io indicator functions which address this issue. This argument is detailed next, followed by a look at possible meta-utility functions for Io and application examples."}, {"title": "4.1 Going Meta: from Utilities to Meta-Utilities", "content": "It is natural to consider Io in terms of A's built-in utility, which by construction provides coherent preferences over states, actions, and outcomes to drive rational decision making. If for instance the desired outcome, Odes, corresponds to a lower bound value function $V_{min}(s)$ to be achieved at each state s by the MDP value function V(s), an evaluator could declare A competent at the task (to first order) if $V(s) > V_{min}(s) \\forall s$; the idea is extendable to state-action value functions Q(s, a). While conceptually simple, such an approach runs into several problems.\nFirstly, it is quite onerous and impractical to specify/check standards for V (s) or Q(s, a), especially in complex tasks and large problem domains. For Q(s, a) in particular, all states s and actions a would generally have to be accounted for, even if they are unlikely to ever be visited. Secondly, though MDP agents typically define V(s) in terms of average (discounted) cumulative rewards, this definition carries very limited information about the full range of possible task outcomes. That is, such a V(s) compresses the full probability distribution of cumulative rewards under a given policy to only the average outcome. Examining V(s) by itself thus may say nothing about other important indicators of risk-reward tradeoffs that inform human decision making under uncertainty, such as the spread of outcomes, worst/best possible outcomes, skewness of outcomes, tail risks, etc. under a given model and policy (see next subsection). Moreover, by mapping all outcomes into additive"}, {"title": "4.1.2 Meta-utilities", "content": "Io can be defined in terms of a meta-utility function $M_o = M(O, O_{des})$ (i.e. separate from A's built-in utility V), which models evaluator preferences on the expected differences between $O_{des}$ and O produced by A acting under a given policy. The distinction between $M_o$ and V is that $M_o$ is deliberately decoupled from the process of finding A's policy, and thus preferences for $M_o$ can be specified independently of A's decision-making process (before or after the fact). In contrast, V is constructively defined and minimized as part of an optimization process which simultaneously produces A's policy. The main purpose of constructing V for an MDP is to provide a useful means to an end: a coherent basis for comparing actions and tractably identifying an optimal policy within the confines of bounded rational computation. Otherwise, as discussed earlier the specific values of rewards and of V itself (as an expectation statistic) are not by themselves necessarily meaningful to users, designers, or evaluators in terms of gauging whether A will achieve $O_{des}$.\nThus, $M_o$ can be thought of as the utility of the algorithmic decision-making process (based on the built-in utility V) in achieving desired outcomes. This corresponds very closely to hierarchical utilities used to optimally select among different possible policy/model update strategies in meta-reinforcement learning [50, 71] and competency-aware planning [10, 12, 103]. A key distinction here in relation to FaMSeC is that A only uses $M_o$ to convey self-assessments via Io, rather than close the loop to improve performance. By defining $M_o$ separately from the policy optimization process, an evaluator has total freedom to assign preferences on margins between $O_{des}$ and O as needed (subject to the usual utility function provisos), to provide an independent figure of merit for A's policy as optimized with respect to V. With this in mind, two practical requirements on defining $M_o$ are imposed.\nFirstly, for $M_o$ to be an acceptable meta-utility that describes a more/less competent MDP agent A, improvements to V should also lead to improvements in $M_o$. Stated more formally: ideally V and $M_o$ ought to be congruent such that, for $V_i$ corresponding to an indexed policy $\\pi^i$ and outcome set $O_i$ with index i, it follows $V_i > V_j \\leftrightarrow M(O_i,O_{des}) \\geq M(O_j, O_{des})$ for policy $\\pi^i$, with i \u2260 j. This leads to a necessary (but not sufficient) condition for competency assessment via FaMSeC, whereby (for fixed \u2211 and C) A under $\\pi^i$ is \u2018more competent' than A under $\\pi^j$ only"}, {"title": "4.2 Candidate Meta-Utilities for Perceived Risk-Reward Tradeoffs", "content": "Tversky and Kahneman [110] famously proposed Cumulative Prospect Theory (CPT) to describe how humans assess uncertain outcomes. There are three main differences between CPT and the classical Expected Utility Theory (EUT) used by most algorithmic agents A. The first difference is that humans evaluate outcomes in terms of marginal gains/losses relative to a reference outcome point, rather than solely with respect to final outcome magnitudes. The second difference is that humans weight losses differently than they do gains. A simple example of this is loss aversion, or the tendency for humans to weight losses higher than they weight gains. The final difference is that human's tend to overweight the probability of extreme outcomes, and underweight the probability of average outcomes. CPT combines these differences into a modified version of the utility function for classical EUT,\n$M_o = M(f(z), {l^-,g^+}) := \\int_{-\\infty}^{l^-} v(z) \\frac{d}{dz}(w(F(z)))dz + \\int_{g^+}^{+\\infty} v(z) \\frac{d}{dz} (-w(1-F(z)))dz,$\\nwhere f(z) is the ordered distribution (probability density) of all outcomes, z is an outcome realization for random variable $Z \\in O$, and F(z) is the cumulative probability distribution of all outcomes up to z. For instance, in the ADT problem, Z could be the non-discounted cumulative reward outcome\n$R(T) = \\sum_{k=0}^{T} R_k,$\\nobtained by an MDP agent that executes T steps under a given policy. The function v maps outcomes z to values of perceived loss or gain, and the weighting function w maps cumulative probabilities over outcomes z to subjective cumulative probabilities F(z). Example v and w are shown in Figures 4a and 4b. In this case, the competency standard \u2211 for Odes is defined by the outcome bounds $l^-,g^+ \\in Z$. Note that the first integral term on the right-hand side of Eq. (3) accounts for total perceived losses below the nominal target outcome loss upper bound of z = $l^-$, whereas the second integral term accounts for nominal gains above the nominal target lower bound gain of z = $g^+$. Although $l^- = g^+ = 0$ is typically stipulated in descriptions of CPT, the bounds need not be the"}, {"title": "4.2.2 Upper/Lower Partial Moments", "content": "Eq. (3) offers an attractive family of candidate meta-utilities to define Io; it has also attracted much attention in the human-machine interaction community as a basis for explainable and interpretable decision-making [48", "73": ".", "112": "n$M_o = M(f(z)", "alpha)": "alpha-UPM/LPM = \\frac{\\int_{g^+}^{+\\infty} (z-g^+)f(z)dz}{\\int_{-\\infty}^{l^-} (l^- -z)^{\\alpha}f(z)dz}$\\nfor integer parameter \u03b1 > 0 and continuous outcomes z with probability density function f(z) = F(z). The \u03b1-UPM/LPM ratio expresses the ratio between favorable and unfavorable outcome event moments, which are defined by expectations of \u03b1 moments relative to reference outcome values. For \u03b1 = 0, (5) simply becomes the likelihood ratio under f(z) for the probability of z exceeding g+ to"}]}