{"title": "\u201cA Good Bot Always Knows Its Limitations\u201d: Assessing Autonomous System Decision-making Competencies through Factorized Machine Self-confidence", "authors": ["BRETT ISRAELSEN", "NISAR R. AHMED", "MATTHEW AITKEN", "ERIC W. FREW", "DALE A. LAWRENCE", "BRIAN M. ARGROW"], "abstract": "How can intelligent machines assess their competencies in completing tasks? This question has come into focus\nfor autonomous systems that algorithmically reason and agentically make decisions under uncertainty (robots,\nself-driving/guided vehicles, etc.). It is argued here that machine self-confidence \u2013 a form of meta-reasoning\nbased on self-assessments of an agent's knowledge about the state of the world and itself, as well as its ability\nto reason about and execute tasks \u2013 leads to many eminently computable and useful competency indicators\nfor such agents. This paper presents a culmination of work on this concept in the form of a computational\nframework called Factorized Machine Self-confidence (FaMSeC), which provides an engineering-focused holistic\ndescription of factors driving an algorithmic decision-making process, including outcome assessment, solver\nquality, model quality, alignment quality, and past experience. In FaMSeC, self-confidence indicators are\nderived from hierarchical 'problem-solving statistics' embedded within broad classes of probabilistic decision-\nmaking algorithms such as Markov decision processes. The problem-solving statistics driving self-confidence\nindicators are obtained by evaluating and grading probabilistic exceedance margins with respect to given\ncompetency standards, which are specified for each of the various decision-making competency factors by the\ninformee (e.g. a non-expert user or an expert system designer). This approach allows \u2018algorithmic goodness of\nfit' evaluations to be easily incorporated into the design of many kinds of autonomous agents in the form\nof human-interpretable competency self-assessment reports. Detailed descriptions and running application\nexamples for a Markov decision process agent show how two of the FaMSeC factors (outcome assessment\nand solver quality) can be practically computed and reported for a range of possible tasking contexts through\nnovel use of meta-utility functions, behavior simulations, and surrogate prediction models.", "sections": [{"title": "1 INTRODUCTION", "content": "\u201cIt ain't what we don't know in life that gives us trouble. It's what we know that ain't so.\u201d\n\u2013 Will Rogers\n\u201cWe see a person do something, and we know what else they can do... we can make a judgement quickly.\nBut our models for generalizing from a performance to a competence don't apply to AI systems. \u201d\n\u2013 Rodney Brooks, IEEE Spectrum interview, May 2023 [117]\nDespite their humble human origins, autonomous agents like intelligent robots and self-driving\nvehicles often succeed and fail at tasks in distinctly non-human ways. The combination of their\nsometimes superhuman capabilities \u2013 and equally 'super' failures \u2013 drives many ongoing debates\nabout the proper role of autonomous technology, in which particular success/failure cases are\nsingled out as evidence for broad claims of (in)capability. In reality, by their very non-humanness,\nthe feats and follies of algorithmic agents should alert us to a couple of fundamentally vexing issues\nsignaled in the opening quotes above: 1) the innate ability of humans to infer the competencies\nof their fellow autonomous human beings does not directly translate into an ability to infer the\ncompetencies of autonomous algorithmic agents; and 2) algorithmic agents generally lack an innate\nawareness of or ability to reason/communicate about their own competencies, which is something\nhumans expect when interacting with others. The systematic inability to provide knowledge about\ntheir competency boundaries opens the door to technological misuse, disuse, and abuse [61, 85]\nissues which are bound to persist as this technology continuously evolves. Misconceptions about\nthe capabilities of algorithmic agents in particular can quickly take root and become hard to dispel,\nunless well-defined design and evaluation practices are instilled to support meaningful assessments\n[97, 109].\nShort of putting the genie back in the lamp or achieving the mythical holy grail of pure, human-\nfree autonomy anytime soon [16], the major question that designers must grapple with today to\nensure successful long-term uptake of autonomous technology is not whether the science says\nmachines with generalizable superhuman abilities could/should co-exist with humans. Rather, it\nis whether the engineering behind such technology is sound and capable enough for people who\nare not roboticists, computer scientists, etc., to responsibly evaluate and use it for solving the\nproblems they actually need to solve [63]. To examine this issue more deeply, this paper considers\nhow the competencies of autonomous algorithmic agents can be identified in principled ways to\naid designers, users, and ultimately agents themselves, in deciding how and when they should be\nentrusted to undertake specific decision-making tasks under uncertainty.\nPerceived competency is one of many factors that influence human trust in technology [61].\nEchoing psychology and business parlance, we define competencies here broadly as the set of\nskills (generalizable, non-task-specific capabilities) and behaviors (observed performance and\napproaches to particular tasks) that an evaluator expects from an algorithmic agent if they are to\nexecute tasks up to some desired standards [101]. One may consider the competency assessment\nproblem from the viewpoint of a hypothetical prospective employer of artificially intelligent\nautonomous agents in a futuristic job market. How would the employer decide what to write"}, {"title": "2 BACKGROUND AND DEFINITIONS", "content": "This section introduces background for the major technical ideas and issues underlying competency\nself-assessment for algorithmic decision-makers. It opens with a brief review of Markov decision\nprocess (MDP) models, which describe a wide range of rational decision-making algorithms and"}, {"title": "2.1 Rational Agents and Competency Evaluation", "content": "Although there are many different notions of competency evaluation, each is ultimately based\non comparing the expected outcomes of an agent's decision-making process to actual outcomes\nobtained in reality. By examining different models of decision-making agents, the details of any\nsuch process can be refined to develop suitable competency self-assessment approaches. To this\nend, we focus here on a broad class of rational (utility-maximizing) algorithmic agents that perform\ndecision-making under uncertainty."}, {"title": "2.1.1 Markov Decision Processes", "content": "Markov decision processes (MDPs) provide a rigorous, well-\nunderstood algorithmic framework for rational decision making that also furnishes agents with the\ningredients for competency evaluation (cf. Figure 2 later). In addition to their widespread use in\nmodern control theory, robotics, and AI, MDP constructs lie at the core of a much broader landscape\nof algorithms for decision-making under uncertainty, embodied for example by hierarchical decision\nprocesses, stochastic optimal control, partially observable Markov decision processes (POMDPs),\nbandit techniques, and reinforcement learning (RL) [69]. Hence, given their importance, prevalence,\nand familiarity to many researchers, MDPs will be referred to extensively in this work to examine\nideas behind competency self-assessment for decision making algorithms from a model-based\nperspective. As discussed later in the paper, this does not exclude consideration of alternative\nrational decision making frameworks, as long as these furnish the same essential components\nfor algorithmic decision-making which we hold to also inform competency self-assessment. A\nbrief review of MDPs is provided next to establish concepts that are useful to develop algorithmic\ncompetency self-assessment techniques; see references such as [69, 106] for detailed reviews.\nAn MDP for a sequential decision-making problem is parameterized by a tuple (S, A, T, R), where:\nS is the state space; A is the set of actions that an agent can take from each state; T is the state\ntransition model, representing the conditional probability $P(s_{t+1}|s_t, a_t)$ of reaching state $s_{t+1} \\in S$\nfrom state $s_t \\in S$ when taking action $a_t \\in A$ at instance t; and R represents the reward model,\ntypically modeled as a function $r_t = r(s_t, a_t)$. By construction of T, the dynamics of the decision-\nmaking problem are Markovian, i.e. the probability of reaching $s_{t+1}$ is conditionally independent of\nall other past states given only the immediate past state $s_t$ and action $a_t$. For optimal sequential\ndecision-making, a policy $\\pi(s)$ must be found to maximize a utility or value function V(s) for all\nvalid $s \\in S$. In most MDPs, the value function typically corresponds to the expected discounted\ncumulative reward V(s) = E[U(s)], where\n$U(s) = \\sum_{t=0}^{\\infty} \\gamma^t r_t$,\nand y \u2208 [0, 1) is a discount factor that trades off between near-term and long-term reward payoffs.\nThe optimal policy thus corresponds to\n$\\pi^*(s) = arg \\max_\\pi V^*(s)$,\nwhere V*(s) is the optimum expected value function."}, {"title": "2.1.2 Bounded Rationality: Ideal vs. Realistic Agents", "content": "The need for approximations like MCTS\nhints at two fundamental premises that apply in real life to all rational decision-making agents,\nwhether or not they are based on MDPs. Firstly, such agents are boundedly rational [93, 100]\u00b9.\nThat is, agents programmed to operate rationally in the real world do not have infinite computing\nor representational power, and thus are inevitably imperfect reasoners with limited access to\ninformation and limited execution abilities. Secondly, such computational agents are designed\nby human designers who themselves are also boundedly rational. From these two premises, it\nfollows that any agent's ability to execute tasks is influenced by a multitude of factors that fall\ninto one of two broad categories: (1) those which were explicitly considered in the design of the\ndecision making algorithm prior to deployment; and (2) those which were not. In other words, an\nalgorithmic decision-making agent is built according to a specific set of design assumptions for its\nintended tasks. Assumptions are essential for allowing designers to focus limited computational\nresources and effort on particular aspects of decision-making problems that can be reasonably\nhandled within the confines of bounded rationality. Assumptions are inherent, for instance, in the\nchoice of state variables and policy solver parameters for MDP agents, as well as in the selection\nof training data to identify policies or world models for reinforcement learning agents. To the\nextent that design assumptions align with reality during actual task execution, there should be a\nhigh likelihood the agent will perform the task as competently as it was designed to. But, if any\nassumption should fail to hold, this likelihood diminishes, often drastically, and competency is\ntypically compromised.\nIt is practically impossible to guarantee the validity of all design assumptions in all task contexts.\nThis means the key question for competency assessment arguably centers on determining the\nlikelihood and extent to which competencies for a particular agent operating could be impacted in\na given operating context. These points will be examined in more detail through the illustrative\nexample described next.\n\u00b9this is also an explicit premise of some - though not all \u2013 other works on competency assessment, e.g. [10, 11, 103]"}, {"title": "2.2 Illustrative Example: Autonomous Delivery in an Adversarial Setting", "content": "Figure 1 depicts an autonomous doughnut delivery truck (ADT) navigating a road network in\norder to reach a delivery destination, while avoiding a motorcycle gang (MG) that will steal the\ndoughnuts if it intercepts the ADT. The MG's location is unknown but can be estimated using\nmeasurements from unattended ground sensors (UGS). The ADT's decision space involves selecting\na sequence of discrete actions (i.e. go straight, turn left, turn right, go back, stay in place) in order\nto safely navigate along segments of the road network and minimize the time to reach a specified\ndestination on the map for each delivery hop.\nThe ADT's motion, UGS readings, and MG's behavior are treated as random variables, and\nthe problems of decision making and sensing are in general strongly coupled. Some trajectories\nthrough the network might allow the ADT to localize the MG via the UGS before heading to the\ndelivery destination but incur a high time penalty. Other trajectories may afford rapid delivery with\nhigh MG location uncertainty but increase the risk of getting caught by the MG, which can take\nmultiple paths. A human dispatch supervisor monitors the ADT during operation. The supervisor\ndoes not have detailed knowledge of or control over the ADT, but can (based on whatever limited\ninformation is available) decide to proceed with or abort the delivery run before it starts. The extent\nto which the ADT can complete the assigned task, i.e. its competency, will clearly be a significant\nfactor in the supervisor's evaluation.\nTo reach its destination, the ADT in the adversarial delivery problem must make a sequence\nof decisions within a discrete state and action space. An MDP is well-suited for this problem.\nSpecifically, the physical states s describing the combined motion of the ADT and MG can be\ndiscretized in time and space to produce a Markov process model defined by some initial joint\nstate probability distribution and joint state transition matrix T, which depends on the steering\nactions a taken by the ADT. A reward function $R(s_k, a_k) = R_k$ can be specified for discrete decision\ninstances (time steps) k to encode user preferences over the combined state of the ADT and MG,\nfor instance: $R_k = \u2212100$ for each time step the ADT is not co-located with the MG but not yet at the\ngoal; $R_k = -1000$ if the ADT and MG are co-located; and $R_k = +1000$ if the ADT reaches the goal\nwithout getting caught. The reward values here are selected arbitrarily for illustrative purposes, but\nprimarily reflect preferences for the ADT's actions and state occupancy. As detailed above, they can\nbe tuned to induce more/less risk averse ADT behavior with respect to an overall utility or value\nfunction. If the MG's state is observable at each step k (due to very reliable and accurate UGS that\ncover all areas of the road network), then the ADT's planning problem can be framed as an MDP.\nOtherwise, probabilistic beliefs about the MG's state can be inferred from the UGS observations,"}, {"title": "2.2.1 Competency Assessment for MDP-based ADT", "content": "Recall the two categories of design factors in\nboundedly rational agent design: (1) those which are explicitly considered; and (2) those which\nare not. For instance, factors in category (1) include the typical dynamics of the ADT navigating\nalong the road network, as well as the existence and typical behaviors of the MG. Factors in\ncategory (2) could include how unusual weather and road conditions impact the UGS. Other factors\nbeyond the supervisor's or ADT's control which could also impact the ADT's capabilities include\nwhether enough computing power has been allocated onboard the ADT to identify safe and efficient\nnavigation solutions in all situations. The nature of the task also matters. Consider that an ADT\nwhich can deliver donuts is just as likely capable of delivering bread loaves or medical supplies\nunder similar circumstances, since such cargo does not fundamentally alter the task's difficulty\u00b2. On\nthe other hand, the imposition of post hoc constraints like minimizing fuel consumption, avoiding\npotholes, or strictly obeying speed limits at all times will (all else being equal) clearly be impactful,\nif designers did not originally account for these.\nAs discussed in [20, 45], explicitly delineating and reasoning over the built-in set of design\nassumptions provides a powerful starting point for determining whether a task is within an agent's\nexpected competency range. We consider an alternative strategy for competency self-assessment\nthat relies on the explicit construction and evaluation of various indicators to determine whether\ncertain necessary conditions for decision-making competent behavior are met. A key distinction\nfrom previous work is that we separate decision-making competent behavior into two related but\ndistinct levels. For one level of competency, the only concern is with what outcomes the agent\nachieves. On another level, we are also interested in examining how the agent achieves those\noutcomes. Whereas previous treatments have largely focused one or the other of these aspects\nof competency, we argue that both aspects must be treated simultaneously within a holistic self-\nevaluation framework for autonomous systems. The next subsection provides a formal framework\nto rigorously ground these ideas for algorithmic competency assessment, before considering how\nsuch indicators can be practically evaluated for agents in operating in different contexts.\n2the MG's tastes notwithstanding"}, {"title": "2.3 The Autonomous Tasking and Competency Evaluation Process", "content": "Autonomous competency self-assessment is best viewed from the perspective of an agent attempting\nto complete a delegated task. In the adversarial delivery problem described above for example, the\nagent is the software which makes decisions autonomously for the ADT, so that it can meet its two\nassigned objectives of avoiding the MG and reaching the delivery destination in a timely manner.\nSince the agent will render decisions that affect the ADT's physical state and evolution in the road\nnetwork environment, the agent also has an effect on how parts of the environment (in this case,\nthe MG) will 'respond' and thus render outcomes. However, the agent cannot affect all parts of\nthe environment that impact the agent's ability to perform the task and achieve desired outcomes\n(e.g. the layout/structure of the road network). This example shows that it is not generally possible\nto analyze or understand competency by looking at a software agent in a vacuum. Rather, the\nassigned task and the context in which it must operate in must be considered together\u00b3.\n\u00b3this echoes the 'scissors of cognition' metaphor of Herbert Simon (who also coined the term \u2018bounded rationality\u2019[100]):\n'[Human] rational behaviour is shaped by a scissors whose blades are the structure of task environments and the computa-\ntional capabilities of the actor.' [99]"}, {"title": "2.4 Strategies for Implementing Algorithmic Competency Self-Assessments", "content": "The assumption alignment tracking approach of [20, 45] is an example instance of the framework in\nFig. 2, whereby multiple binary indicator functions are used to assess whether assumptions essential\nto A's ability to perform the task hold during operation in different contexts C. However, it is\ngenerally quite difficult to capture and validate all of the assumptions implicit in a given algorithm\ndesign for a particular task. The number of assumptions underlying a sophisticated decision making\nagent performing complex real-world tasks can be staggeringly large. Indeed, it is not hard to\nimagine how a combined set of programs required to check these assumptions could quickly become\nmore computationally complex and expensive to run than the original decision-making algorithm\nitself. Moreover, a designer cannot know with certainty in advance which assumptions are later\nlikely to have a significant impact or enumerate all the possible ways in which they may be violated.\nThe violation of some engineering assumptions also may well change the probabilities of success or\nfailure at a given task, without necessarily completely ruling either possibility out. For example, the\nADT could 'get lucky', e.g. if the MG suffers mechanical failures. Decision-making algorithms can\nalso discover systematic loopholes such as \u2018reward hacking' behaviors that deviate from designer\nintent without necessarily breaking design logic or violating assumptions.\nIt is also important to consider how competency self-assessments are communicated to human\ninformees such as supervisors. Human supervisors of other (human/non-human) agents typically\ndo not scrutinize intrinsic task assumptions before deciding what tasks to delegate to whom. Rather,\nit is more natural for them to form a theory of mind and reason about the possible evolution of other\nagents' behaviors, on the basis of attributed percepts, beliefs, goals, intentions, and capabilities\n[9, 62]. The problem of automation surprise in airline pilots, for example, can be explained in"}, {"title": "3 FACTORIZED MACHINE SELF-CONFIDENCE", "content": "The core idea of this work is that competency self-assessment for rational autonomous agents can\nbe realized through a hierarchical process of machine self-confidence computation. In particular,\nfor an agent A acting via probabilistic decision-making algorithms: self-confidence evaluation\ncorresponds to automated reasoning over a set of embedded problem-solving statistics. In their raw\nform, these statistics describe different (but interrelated) aspects of the A's expected and observed\nabilities to achieve particular outcomes O in given contexts C. When assessed collectively in a"}, {"title": "4 OUTCOME ASSESSMENT", "content": "Simply stated, evaluating $I_O$ consists in comparing the outcomes O produced by agent A (where\nO is described by random variables in the MDP setting) to the desired outcomes $O_{des}$ for a given\ntask (where $O_{des}$ is a portion of the full standard, \u2211, related to outcomes for a given context C).\nGiven a task, $O_{des}$ represents how outcomes of interest should look according to the evaluator's\nutility (or utilities), while O are the outcomes expected/actually generated by the agent following\nits prescribed utility. $O_{des}$ and O could be formulated in terms of rewards, but generally can be\nexpressed in other ways. The main idea of evaluating $I_O$ this way is to assess the value of assigning\na task to A (with its given state-action model, solver, and reward function in the MDP setting). The\ndifference between $O_{des}$ and O is quantified as another random variable, and the distribution of\nthis random variable is interpreted via $I_O$. More probability mass for high/low differences indicates\nthat the A believes $O_{des}$ will be harder/easier to typically achieve per evaluator expectations,\nresulting in lower/higher confidence in $I_O$. A key implication is that built-in utilities like the MDP\nvalue function V(s) are insufficient on their own to define $I_O$, although they still reveal useful\ncompetency information. The concept of a meta-utility is thus formally introduced to define $I_O\nindicator functions which address this issue. This argument is detailed next, followed by a look at\npossible meta-utility functions for $I_O$ and application examples."}, {"title": "4.1 Going Meta: from Utilities to Meta-Utilities", "content": "4.1.1 Limitations of Analyzing Built-in Utilities. It is natural to consider $I_O$ in terms of A's built-in\nutility, which by construction provides coherent preferences over states, actions, and outcomes to\ndrive rational decision making. If for instance the desired outcome, $O_{des}$, corresponds to a lower\nbound value function $V_{min}(s)$ to be achieved at each state s by the MDP value function V(s), an\nevaluator could declare A competent at the task (to first order) if $V(s) > V_{min}(s) \\forall s$; the idea is\nextendable to state-action value functions Q(s, a). While conceptually simple, such an approach\nruns into several problems.\nFirstly, it is quite onerous and impractical to specify/check standards for V (s) or Q(s, a), especially\nin complex tasks and large problem domains. For Q(s, a) in particular, all states s and actions a\nwould generally have to be accounted for, even if they are unlikely to ever be visited. Secondly,\nthough MDP agents typically define V(s) in terms of average (discounted) cumulative rewards, this\ndefinition carries very limited information about the full range of possible task outcomes. That is,\nsuch a V(s) compresses the full probability distribution of cumulative rewards under a given policy\nto only the average outcome. Examining V(s) by itself thus may say nothing about other important\nindicators of risk-reward tradeoffs that inform human decision making under uncertainty, such as\nthe spread of outcomes, worst/best possible outcomes, skewness of outcomes, tail risks, etc. under\na given model and policy (see next subsection). Moreover, by mapping all outcomes into additive"}, {"title": "4.1.2 Meta-utilities", "content": "$I_O$ can be defined in terms of a meta-utility function $M_O = M(O, O_{des})$ (i.e.\nseparate from A's built-in utility V), which models evaluator preferences on the expected differences\nbetween $O_{des}$ and O produced by A acting under a given policy. The distinction between $M_O$ and\nV is that $M_O$ is deliberately decoupled from the process of finding A's policy, and thus preferences\nfor $M_O$ can be specified independently of A's decision-making process (before or after the fact).\nIn contrast, V is constructively defined and minimized as part of an optimization process which\nsimultaneously produces A's policy. The main purpose of constructing V for an MDP is to provide\na useful means to an end: a coherent basis for comparing actions and tractably identifying an\noptimal policy within the confines of bounded rational computation. Otherwise, as discussed earlier\nthe specific values of rewards and of V itself (as an expectation statistic) are not by themselves\nnecessarily meaningful to users, designers, or evaluators in terms of gauging whether A will\nachieve $O_{des}$.\nThus, $M_O$ can be thought of as the utility of the algorithmic decision-making process (based on\nthe built-in utility V) in achieving desired outcomes. This corresponds very closely to hierarchical\nutilities used to optimally select among different possible policy/model update strategies in meta-\nreinforcement learning [50, 71] and competency-aware planning [10, 12, 103]. A key distinction\nhere in relation to FaMSeC is that A only uses $M_O$ to convey self-assessments via $I_O$, rather than\nclose the loop to improve performance. By defining $M_O$ separately from the policy optimization\nprocess, an evaluator has total freedom to assign preferences on margins between $O_{des}$ and O as\nneeded (subject to the usual utility function provisos), to provide an independent figure of merit\nfor A's policy as optimized with respect to V. With this in mind, two practical requirements on\ndefining $M_O$ are imposed.\nFirstly, for $M_O$ to be an acceptable meta-utility that describes a more/less competent MDP agent\nA, improvements to V should also lead to improvements in $M_O$. Stated more formally: ideally\nV and $M_O$ ought to be congruent such that, for $V_i$ corresponding to an indexed policy $\\pi^i$ and\noutcome set $O_i$ with index i, it follows $V_i > V_j \\leftrightarrow M(O_i, O_{des}) \\geq M(O_j, O_{des})$ for policy $\\pi^j$,\nwith i \u2260 j. This leads to a necessary (but not sufficient) condition for competency assessment\nvia FaMSeC, whereby (for fixed \u2211 and C) A under $\\pi^i$ is \u2018more competent' than A under $\\pi^j$ only"}, {"title": "4.2 Candidate Meta-Utilities for Perceived Risk-Reward Tradeoffs", "content": "4.2.1 Cumulative Prospect Theory. Tversky and Kahneman [110] famously proposed Cumulative\nProspect Theory (CPT) to describe how humans assess uncertain outcomes. There are three main\ndifferences between CPT and the classical Expected Utility Theory (EUT) used by most algorithmic\nagents A. The first difference is that humans evaluate outcomes in terms of marginal gains/losses\nrelative to a reference outcome point, rather than solely with respect to final outcome magnitudes.\nThe second difference is that humans weight losses differently than they do gains. A simple example\nof this is loss aversion, or the tendency for humans to weight losses higher than they weight gains.\nThe final difference is that human's tend to overweight the probability of extreme outcomes, and\nunderweight the probability of average outcomes. CPT combines these differences into a modified\nversion of the utility function for classical EUT,\n$M_O = M(f(z), \\{\\bar{l},g^+\\}) := \\int_{- \\infty}^{\\bar{l}} v^\\prime(z)(w^\\prime(F(z)))dz + \\int_{g^+}^{\\infty} v^\\prime(z)(-w^\\prime(1-F(z)))dz$,\nwhere f(z) is the ordered distribution (probability density) of all outcomes, z is an outcome\nrealization for random variable Z \u2208 O, and F(z) is the cumulative probability distribution of all\noutcomes up to z. For instance, in the ADT problem, Z could be the non-discounted cumulative\nreward outcome\n$R(T) = \\sum_{k=0}^{T} R_k$,\nobtained by an MDP agent that executes T steps under a given policy. The function v maps outcomes\nz to values of perceived loss or gain, and the weighting function w maps cumulative probabilities\nover outcomes z to subjective cumulative probabilities F(z). Example v and w are shown in Figures\n4a and 4b. In this case, the competency standard \u2211 for $O_{des}$ is defined by the outcome bounds\n$\\bar{l},g^+ \\in Z$. Note that the first integral term on the right-hand side of Eq. (3) accounts for total\nperceived losses below the nominal target outcome loss upper bound of z = $\\bar{l}$, whereas the second\nintegral term accounts for nominal gains above the nominal target lower bound gain of z = $g^+$.\nAlthough $\\bar{l} = g^+ = 0$ is typically stipulated in descriptions of CPT, the bounds need not be the"}, {"title": "4.2.2 Upper/Lower Partial Moments", "content": "Eq. (3) offers an attractive family of candidate meta-utilities\nto define $I_O$; it has also attracted much attention in the human-machine interaction community\nas a basis for explainable and interpretable decision-making [48, 73]. Yet, it is not straightforward\nto identify v(z) and w for individual users, or to precisely obtain F(z) in general settings. To\nmitigate these issues, one may turn instead to simpler alternative non-parametric \u2018CPT-compatible'\nfamilies of utility functions developed in fields like econometrics and mathematical finance, which\nalso deal extensively with human decision-making under uncertainty. These utilities are non-\nparametric in the sense that they do not make restrictive distributional assumptions for F(z) (e.g.\nthey are applicable to empirical distribution data), and CPT-compatible because they include CPT\nutilities as a special case. One particularly simple candidate family of such utilities is given by the\n\u03b1-upper/lower partial moment (\u03b1-UPM/LPM) ratio [30, 112],\n$M_O = M(f(z), \\{\\bar{l},g^+\\} ; \\alpha) := \\alpha-UPM/LPM = \\frac{\\int_{g^+}^{+\\infty} (z-g^+)^{\\alpha}f(z)dz}{\\int_{-\\infty}^{\\bar{l}} (\\bar{l}-z)^{\\alpha}f(z)dz}$,\nfor integer parameter \u03b1 > 0 and continuous outcomes z with probability density function f(z) =\nF'(z). The \u03b1-UPM/LPM ratio expresses the ratio between favorable and unfavorable outcome\nevent moments, which are defined by expectations of \u03b1 moments relative to reference outcome\nvalues. For \u03b1 = 0, (5) simply becomes the likelihood ratio under f(z) for the probability of z\nexceeding g\u207a to the probability of z deceeding $\\bar{l}$. For \u03b1 \u2265 1, the ratio describes how the shape\nof f(z) contributes to the distribution of Z = z above g\u207a and below $\\bar{l}$. If \u03b1 = 1, the numerator\nand denominator respectively give the upper and lower semi-mean of z with respect to g\u207a and $\\bar{l}$;\nwhereas for \u03b1 = 2, these respectively give the upper and lower semi-variance of z with respect to\ng+ and $\\bar{l}$, etc. For any \u03b1, the ratio: approaches 0 as the lower moment dominates; goes to \u221e as the\nupper moment dominates; and is 1 when the partial moments balance.\nThe \u03b1-UPM/LPM can be generalized to a form which reproduces typical CPT utilities as special\ncases4 and which provides favorable guarantees for decision-making under uncertainty via sto-\nchastic dominance properties [30]. As such, the authors of [30, 112] advocate for UPM/LPM-based"}, {"title": "4.3 Application Examples", "content": "4.3.1 Synthetic Io Evaluations. Fig. 6 (a)-(l) demonstrate the basic properties of the UPM/LPM Io\nfunction, in particular how the UPM/LPM ratio with \u03b1 = 1 can balance optimism and pessimism\nrelative to z* in a \u2018first order' sense. Each pdf f(z) in this example consists of mixtures of Dirac\ndelta functions on an arbitrary scalar Z, so that all probability mass is placed on one or two specific\noutcomes z to simplify the partial moment calculations in (6). The resulting Io result from eq. (7)\nis shown below each graph for z* = 0.\nCases (a)-(c) result in similarly high positive Io values, despite significant differences between\nthe underlying f(z) pdfs. In (a) and (c), all probability mass for z lies to the right of z* = 0, which\ndrives the UPM/LPM to infinity and makes Io = 1, indifferent to the magnitude of the positive\noutcomes z. The interpretation for very high self-confidence in these cases is that A is certain\nit can exceed z*. In (b), although the probability for z = \u22125 is the same as for z = 25, the greater\nmoment arm in the UPM drives the UPM/LPM ratio to 5, so Io \u2248 3. Here A predicts that it is fairly\nlikely - but not entirely certain - it will exceed z*. The Omega ratio is 1 in this case, indicating equal\nlikelihood of either exceeding or falling below z*; this contrasts with Io, which reflects competency\nin terms of both the degree and likelihood of exceeding z*. Cases (d)-(f) are mirror opposites to"}, {"title": "4.3.2 Illustration for ADT Problem", "content": "Consider an MDP agent A for the ADT problem that uses an\nexact policy \u03c0 produced via value iteration for an infinite horizon planning scenario with fixed\ndiscount factor \u03b3 \u2208 [0", "obtains": "a large penalty when\nthe ADT is caught by the MG; a large reward when the ADT reaches the goal $R_{goal}$; and a small\nloitering penalty $R_{loiter}$ for each time step the ADT is not at the goal and not caught. The choice\nof z* = 0 results in a competency standard"}]}