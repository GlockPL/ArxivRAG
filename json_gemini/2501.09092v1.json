{"title": "SteLLA: A Structured Grading System Using LLMs with RAG", "authors": ["Hefei Qiu", "Brian White", "Ashley Ding", "Reinaldo Costa", "Ali Hachem", "Wei Ding", "Ping Chen"], "abstract": "Large Language Models (LLMs) have shown strong general capabilities in many applications. However, how to make them reliable tools for some specific tasks such as automated short answer grading (ASAG) remains a challenge. We present SteLLA (Structured Grading System Using LLMs with RAG) in which a) Retrieval Augmented Generation (RAG) approach is used to empower LLMs specifically on the ASAG task by extracting structured information from the highly relevant and reliable external knowledge based on the instructor-provided reference answer and rubric, b) an LLM performs a structured and question-answering-based evaluation of student answers to provide analytical grades and feedback. A real-world dataset which contains students' answers in an exam was collected from a college-level Biology course. Experiments show that our proposed system can achieve substantial agreement with the human grader while providing break-down grades and feedback on all the knowledge points examined in the problem. A qualitative and error analysis of the feedback generated by GPT4 shows that GPT4 is good at capturing facts while may prone to inferring too much implication from the given text in the grading task which provides insights into the usage of LLMs in the ASAG system.", "sections": [{"title": "I. INTRODUCTION", "content": "Assessment plays an important role in the teaching and learning process. It usually includes closed-ended questions such as multiple choices and open-ended questions such as short-answer questions. Although open-ended questions are more powerful in evaluating students' learning, grading on such questions are more time-consuming. In some scenarios such as introductory-level courses in college with hundreds of students, or online courses with an even larger scale of learners, the potentially heavy workload due to the manual grading on open-ended short-answer questions hinders their usage in practice. An automated grading system can provide prompt feedback to a learner, can support large scale learn-ing environment, and further facilitate active and life-long learning. The recent development of Large Language Models (LLMs) has shown their strong general capabilities in many tasks. However, how to use them to automatically provide reliable grading and feedback remains a challenge. We propose SteLLA (Structured Grading System Using LLMs with RAG), an automatic grading system that performs a structured grading based on Question Answering (QA) techniques, which is em-powered by highly relevant augmented information retrieved from the instructor-provided reference answer and rubric.\nThe field of automatic grading and feedback systems has been explored through various domains such as programming [1], [2] and mathematics [3], [4], as well as on different types of answers such as essays [5], [6] and short answers [7]\u2013[9]. Compared with an essay which is usually long and with multiple paragraphs, a short answer is much shorter and with just a couple of sentences. Grading on short answers is more focused on correctness and does not consider text coherence or writing style as in essay grading. SteLLA is a system designed for automatic short-answer grading (ASAG).\nThere have been many attempts to build automatic grading and feedback systems. Many of them utilize the recent devel-opment in Natural Language Processing (NLP). Motivated by the huge progress of LLMs and the needs of instructors and learners, our design uses LLMs as a key component. To ground a general LLM on the specific task of grading, we propose a reference answer and rubric based retrieval augmented generation (R-RAG) approach. Given an instructor-provided reference answer and a rubric, R-RAG extracts highly relevant and structured information from them. It applies question-generation and question-answering techniques to generate a set of evaluation questions and corresponding answers. An LLM performs a structured grading by checking how well a student's response answers these evaluation questions. Eventually, an overall grade, the breakdown grades and feedback are generated to the user.\nThe contributions of this work are as follows:\n\u2022 We propose an LLM-based ASAG system, SteLLA, that shows substantial agreement with human graders.\n\u2022 We present R-RAG which is specifically designed for the grading task. It treats an instructor-provided reference answer and rubric as a knowledge base to extract highly"}, {"title": "II. BACKGROUND AND RELATED WORK", "content": "Although LLMs have shown strong general capabilities, there are some key challenges these models are still suf-fering from, e.g., factual hallucination [23]\u2013[25]. Retrieval-Augmented Generation (RAG) [26], [27] has been proposed and established to be a technique to alleviate such challenges. It references reliable external knowledge by retrieving relevant information and further enhances the performance of LLMs. Some of the works use the retrieved data as augmented inputs to guide the generation of LLMs [26], [28]. Others apply this approach in the middle of generation [29], [30] or after the generation [31], [32]. We apply RAG by using it to retrieve augmented information as inputs. We treat an instructor-provided reference answer as an external knowledge base, extract information that contains the target answer to an evaluation question, and send it together with the student response and the evaluation question to an LLM to perform the assessment."}, {"title": "III. METHOD AND SYSTEM ARCHITECTURE", "content": "In this section, we present our approach and the system design. The overall method is to apply the RAG approach to generate structured evaluation questions and corresponding answers from the instructor-provided reference answer and rubrics to a problem. These augmented evaluation question-answer pairs are used to ground an LLM's grading. Together with a student's response and prompts, they are sent to an LLM as inputs. The LLM performs the question-answering task to assess to what extent a student's response answers all these evaluation questions and gives the grades and feedback. The grades of all these questions are eventually consolidated into a final grade. Figure 1 part(a) shows the design of the entire system. A concrete example in Figure 2 illustrates the flow of grading. The prosed system is composed of three key modules: a) the R-RAG module based on the reference answer and rubric, b) the Evaluation module based on LLM and QA, and c) the Scoring module. All the modules are explained in detail in the following."}, {"title": "IV. DATA", "content": "In this section, we report the data collected for this study. We first describe the data source, then explain how we redact the data to protect students' privacy, and lastly present statistics of the data."}, {"title": "V. EXPERIMENTS AND RESULTS", "content": "In this section, we describe our experiment settings and report the experiment results."}, {"title": "VI. CONCLUSION AND FUTURE WORK", "content": "We propose SteLLA, an automatic short-answer grading system that uses RAG techniques based on the instructor-provided reference answer and rubric to facilitate an LLM performing structured question-answering-based assessment of student responses. Experiments on a real-world dataset show that our system is able to achieve substantial agreement with the human graders. It can also provide analytical grades and feedback on knowledge points examined in the problem. In the future, one direction of the work could be on generating structured evaluation question-answer pairs in the context of missing rubrics, i.e., only the reference answer available. Another direction could be to add human-interactive components to increase the system's adaptability in personalization."}, {"title": "A. QA-Based Evaluation", "content": "While Question Answering itself is one of the major tasks in NLP, the QA-based approach is novel in applying QA techniques to perform text evaluation for other NLP tasks. This approach has been applied to evaluate the quality of texts in summarization or text compression tasks. Some of the earlier work used the QA evaluation diagram to examine to what extent documents could be summarized while not affecting comprehension on them [10], to perform human evaluations of summaries [11]. Along with the progress of question generation techniques, multiple researches have been done on automatically generate questions from the reference summary [12], [13], from the source document [14], and from the evaluated summary [15], [16] to check fact-based consis-tency or faithfulness. Extended from previous work, QuestEval combines both recall and precision approaches and shows an improved QA-based metric on evaluating summarization [17]. QestEval is also applied on evaluating text simplification [18] and text converted from semi-structured data such as table [19]. To the best of our knowledge, our work is the first attempt to apply QA-based evaluation to the grading and feedback task."}, {"title": "B. Large Language Models (LLMs)", "content": "Language Modeling (LM) has been one of the central tasks in NLP. In general, LM is to learn a probability distribution over sequences of tokens by predicting the probabilities of the next or missing token(s). Pre-trained language models such as BERT [20] have shown surprising capability in learning context-aware word representations and achieved high perfor-mance in a series of NLP tasks. Since the launching of GPT-3 [21], LLMs have attracted a lot of attention. Compared with pre-trained language models, LLMs are scaled with a much larger size of model parameters and training data. They show emerging abilities to solve more complex tasks. ChatGPT (OpenAI 2022), developed upon the GPT-3 (OpenAI 2021) and above series, provides a highly accessible and effective way to use LLMs in a conversational manner and without fine-tuning. This intimates a large number of research and applications. The most recent versions, GPT-4 (OpenAI 2023) and GPT-4O (OpenAI 2024), are multimodel models that accept both text and images as inputs.\nRecent LLMs use Transformer [22] as the backbone ar-chitecture of the models. Originally introduced for the ma-chine translation task, the vanilla Transformer is built on an encoder-decoder structure. The encoder and decoder are both a stack of transformer blocks. Through the multi-head attention mechanism, the encoder encodes the input sentence in one language into a latent space of representation; the decoder decodes this representation to autoregressively generate the translated sentence. Different from the vanilla Transformer, the GPT series uses the decoder only."}, {"title": "C. Retrieval-Augmented Generation", "content": "Although LLMs have shown strong general capabilities, there are some key challenges these models are still suf-fering from, e.g., factual hallucination [23]\u2013[25]. Retrieval-Augmented Generation (RAG) [26], [27] has been proposed and established to be a technique to alleviate such challenges. It references reliable external knowledge by retrieving relevant information and further enhances the performance of LLMs. Some of the works use the retrieved data as augmented inputs to guide the generation of LLMs [26], [28]. Others apply this approach in the middle of generation [29], [30] or after the generation [31], [32]. We apply RAG by using it to retrieve augmented information as inputs. We treat an instructor-provided reference answer as an external knowledge base, extract information that contains the target answer to an evaluation question, and send it together with the student response and the evaluation question to an LLM to perform the assessment."}, {"title": "D. Automatic Short-Answer Grading", "content": "The research on the ASAG has a long history. In the earlier days of ASAG research, many traditional methods used rule-based models [33]. For example, the idea of Concept Mapping is more rule-based, which breaks the student answers into several concepts and detects if each concept is present or not [34]\u2013[36]. The approach that uses information retrieval techniques is also more rule-based. It usually checks student answers more by relying on pattern matching through, e.g., regular expressions or parsing trees [37], [38].\nAlong with the development of machine learning in NLP, it also has become popular in ASAG systems. Some of them apply clustering methods such as grouping together student responses using LDA clustering to lessen the workload for a human grader [7] or using k-means algorithm based on common word similarity [8]. Others treat it as a classification problem using, for example, a k-nearest neighbor classifier to detect and diagnose semantic errors in student answers [39].\nMost recently, interest in Pre-trained Language Models (PLMs) and LLMs has increased significantly. In accordance, there has been many research on the possible applications of LLMs to the educational field [40]. PLMs such as BERT can be pre-trained on domain resources to improve ASAG. [9] uses LLM-based one-shot prompting and a text similarity scoring model based on Sentence-BERT [41] to grade short answers. [42] evaluated using ChatGPT to perform auto-grading on short text answers, in which they use ChatGPT to directly assess answers by both the educator and the students. They concluded that LLMs currently can be used as a complemen-tary viewpoint but are not ready as an independent tool yet.\nOur approach is different from the above in the way that we use the instructor-provided reference answer and rubrics as highly relevant external knowledge base, extract structured information in the form of evaluation question-answer pairs, and then ask LLMs to assess to what extent a student's response answers all these evaluation questions."}, {"title": "A. R-RAG Module", "content": "The R-RAG module applies RAG approach based on the reference answer and rubrics. It is specifically designed for the grading task. A typical RAG approach is shown in Figure 1 part (c). Given a query from the user, a retriever, usually using information retrieval techniques, retrieves relevant information from an external knowledge base such as Wikipedia or other reliable datasets. This highly relevant information serves as part of the prompts and guides the LLM to generate specific results for the given query.\nAs shown in Figure 1 part (b), the R-RAG module takes the instructor-provided reference answer and rubrics as inputs, generates and extracts a list of evaluation questions with gold answers, and sends it to the LLM. More specifically, given a full-credit reference response r and a rubric b, each rubric point is marked as a conditioned target answer. A question-generation model will generate a corresponding question for each target answer based on the reference answer. For ex-ample, For the rubric point \u201cC and H", "What does molecule 1 consist of?\" Even-tually, this module will generate a set of evaluation questions Q = {q1,..., qn} and their gold answers L = {11,..., n}, where n is the length of the rubric points. Each evaluation question reflects a rubric point. Each gold answer is supported by both the reference response and the rubric. The R-RAG module has some unique designs specifically for the grading task.\nHighly Relevant Knowledge Base. R-RAG treats the instructor-provided reference answers and rubrics as an ex-ternal knowledge base, which is highly relevant to the grading task that the LLM is going to perform. Normally, the external knowledge that the RAG approach relies on is very large and requires sophisticated techniques to retrieve query-relevant information. Inspired by the traditional learning assessment process in which an instructor usually provides a reference answer and rubrics to facilitate graders in grading, we directly use such available data as external knowledge. They are small and highly relevant to the student's responses that are needed to be graded. This gives the potential to simplify the system and further enhance its usage.\nStructured Information. Due to the nature of external knowledge typically used in RAG which is large, some in-formation retrieval techniques such as ranking are usually used to get the most relevant information. In R-RAG, instead of retrieving ranked relevant information, we aim to extract structured information. This is chosen to perform a structured assessment. To a learner, while it's important to get a correct grade on the answer, it's even more important to understand the knowledge points tested in the problem and how he/she does on each of them. A structured assessment provides more valuable feedback to improve both learning and teaching. Under this consideration, the outputs from the R-RAG are structured following the rubrics, each of which reflects a rubric point.\nQA-Based Evaluation. When humans grade a student's response to a problem, we do not just compare how similar it is with the reference answer. Instead, for each knowledge point, we ask if the student's response answers it correctly. Inspired by this human grading process, question-answering becomes a natural approach in our automatic grading system. Each bullet point in a rubric is marked as a conditioned answer, a question generation model is applied to generate a question to it based on the reference answer. Meanwhile, a subset of the reference answer which contains the conditioned answer\"\n    },\n    {\n      \"title\"": "B. LLM-based Evaluation Module"}, {"content": "The LLM-based evaluation module takes the outputs from the R-RAG Module, a student's response, and other prompts as inputs. The outputs from this module are a set of numeric grades and detailed feedback to justify its grading.\nWe apply zero-shot and few-shot learning when prompting the LLM. To better select shots, which are a few task-specific samples provided to an LLM, we use clustering techniques to select learning samples. All students' responses are sent to a sentence encoder such as SBERT [41] to get their embeddings. Then a clustering algorithm such as KMeans is applied to group them into k clusters. The centroids of all clusters are identified and selected as the few-shots. If a centroid is not a student's response, then find the student's response that is the closest to the centroid."}, {"title": "C. Scoring Module", "content": "The Scoring module takes the set of grades and feedback from the Evaluation module as inputs. Based on the weights of each evaluation question, this module performs the cal-culation such as weighted sum to generate a final grade of a student's response and a unified feedback. Since the final grade & feedback and the breakdown grades & feedback are all valuable, they are all presented to the user as the outputs from the system."}, {"title": "A. Data Source", "content": "The data used in this study are collected from an undergraduate-level introductory Biology course in the semester of Fall 2018 at a public university in the United States. The data are student's answers to a problem from an exam. We will make the dataset public after publication. As shown in Figure 3, in part (a) of this problem, students are provided with 3 images of different molecules and asked to rank them in the order from the most hydrophobic to the most hydrophilic. In part (b) of the problem, students are asked to briefly explain their choices in part a. Their short answers in part (b) are the data collected for this study."}, {"title": "B. Privacy Protection", "content": "We take our responsibility to protect students' privacy seriously. The data used in this study are all under the approval of the Institutional Review Board (IRB) at the school where the data are collected. We redact the data to make them de-identified through the following pre-processing: a) Removing student names and using file names as index instead; b) Removing any information in the answers that can be linked to any specific individual."}, {"title": "C. Labeling Process", "content": "Two undergraduate Research Assistants, who had taken the same Biology course before and understood the course materi-als well, did the labeling as human graders. The entire labeling is an iterated process, two graders working first respectively to give only a final grade to each student's answer, then adding grades to all rubric points, and in the end consolidating two graders' labels into an agreed version. For the selected few-shot samples, the human graders also give the text feedback to justify their grading. This process lasted about two semesters.\nThe two human graders are first trained by the instructor on how to do grading specifically for assignments or exams for this course. Then they label the data in two steps. In step one, they do the labeling respectively. For each evaluation question on a problem, they check to what extent a student's response answers the question. If it answers the question completely correctly, then label it with 1; otherwise, label it with 0. We do not consider partial credits since the evaluation has been decomposed into a set of questions, each of which is focused on one knowledge point. We original start with only the one final grade for each answer. Along with the development of the approach, the graders are instructed to add labels to all the rubric points for a problem. Then in step two, under the guidance of the instructor, the two human graders identify all the labels that they do not agree with each other, have a discussion, and come across the labels they all agree. Eventually, this process gives us the ground-truth labels for evaluation."}, {"title": "D. Characteristics and Statistics", "content": "The collected data contain a total of 176 samples. Due to one empty entry, the number of valid samples is 175. The average length of a student's answer is around 39 words. Each answer, which is a paragraph, contains around 2 sentences on average. This is consistent with the normal description of short answers such as the length is \"phrases to three to four sentences\" or \"a few words to approximately 100 words\" [43].\nTo facilitate the human graders, the instructor provides one reference answer and a grading rubric which contains 4 key rubric points such as O/OH and H-Bonds. The score of each rubric point is 1. This leads to 4 points total as the full score for the problem. Originally part b in the exam is 5 points. The instructor adjusted it to be 4 points based on the rubric. Accordingly, the score on each rubric point is binary (0/1) and the score of each student is an integer value in the range of 0-4 inclusive. The following are the reference answer and a sample student answer:\nReference answer: Molecule 1 consists entirely of C and H atoms. This makes molecule 1 entirely non-polar and therefore very hydrophobic. Molecule 3 has an O atom which can form hydrogen bonds, making it polar and hydrophilic.\nSample student answer: Molecule 1: No lone pairs, No special hydrogens therefore hydrophobic."}, {"title": "A. Experiment Settings", "content": "In the R-RAG module, the instructor-provided reference answer and the rubric are both supplied. Answer-conditioned question generation is applied to the reference answer, in which one rubric point is set as a conditioned answer to gen-erate one question. To make sure the generated questions are of high quality then we can have a more consistent and solid evaluation of LLM's performance, we manually generate three questions for each rubric point based on the reference answer. The course instructor reviews these questions and selects the best one out of the three. In the Evaluation Module, the system calls GPT4 API (the version of GPT4-Turbo-Preview). When prompting GPT4, we design general instruction and question-specific instruction. The general instruction is to specify the role, task, detailed instruction, and constraints on how to grade such as the grade scale, criteria of each grade, etc. The question-specific instruction is to address a grader's personal criteria. For example, in the evaluation question \"What does molecule 1 consist of?\u201d, although the reference answer expects a student's answer to contain the information that molecule 1 consists of C and H atoms, the course instructor thinks if a student's answer only mentions C (carbon) atom, it's also considered as being correct. This personal criteria is addressed in the question-specific instruction. We apply few-shot learning in which the 4-shot gives us the best performance. To select samples, we perform random selection. Selected samples are excluded from evaluation. The following shows an example of instructions in the prompt:\nGeneral instruction: You are the instructor of a college-level Introductory Biology course. You are going to grade the exam for this course. Your grading should be based on the question asked, the full-credit answer, the student's answer, and nothing else. Give the binary score 1 or 0, in which 1 means the student's answer is correct and 0 means the student's answer is incorrect or does not answer the question, and justify your grading.\nQuestion-specific instruction: As long as the answer mentions or implies that the molecule contains just carbon, it should be considered as being correct and graded as 1."}, {"title": "B. Evaluation Results", "content": "SteLLA essentially takes the role of a grader. Thus We evaluate the results by calculating the agreement with the human grader's grading, which is commonly used in grading evaluation. Because this work is pioneering in applying QA-based evaluation on ASAG task, on a newly collected real-world dataset, and this field is relatively new, we weren't able to find highly related models or systems to compare with. As explained in the labeling process section, under the instructor's supervision, the two human graders discussed the difference in the grades they assigned to the same questions, reached an agreement, and reassigned the agreed grades to those questions as the ground-truth labels. We compare the agreement between the results from our system and the ground-truth labels and report both Cohen's Kappa coefficient (\u03ba) [44] and Raw Agreement (Accuracy).\nAgreement Results. As shown in Table I, Cohen's Kappa coefficient value between the human grader and the ground-truth labels reaches 0.8315 which is normally accepted as a near-perfect agreement. Although our system still does not reach human performance, it achieves a substantial agreement with the ground-truth labels by  \u03ba = 0.6720. As for the raw agreement, it's about 8% lower than the human grader. These results show that our system is promising in automatic grading while maintaining high accuracy."}, {"title": "C. Sample Grading and Feedback Analysis", "content": "In Table II, we list three sample students' responses and GPT4 grading results to the evaluation questions. We have several findings about using GPT4 to do grading as:\n\u2022 GPT4 is good at identifying relevant facts or statements. For example, in Q1 and Q2 to the student response 9328795, GPT4 is able to identify that molecule 3 has an Oxygen atom and can form hydrogen bonds even though the two phrases are a bit far from each other in the original text answer. In student response 9328809, GPT4 identifies question-related information that molecule 3 has an O atom and it cannot form H-bonds and then grades the student response on Q1 is correct and on Q3 is incorrect.\n\u2022 GPT4 can be tolerant of some typos in the input. For example, in student response 9328795, there are typos or errors such as tho and then. But they do not affect GPT4's understanding of the response text.\n\u2022 GPT4 sometimes can infer the meaning of the text prop-erly, while sometimes infers too much implication from the given text. For example, in Q3 to the student response 9328790, based on \u201cMolecule 1 is most hydrophobic be-cause it is all carbons and it can't make hydrogen bonds.\u201d, GPT4 properly infers that the student implies molecule 1 consists of carbon atoms and does not contain elements like oxygen or nitrogen which can form hydrogen bonds, and further grades it as being correct on this evaluation question. While in Q4 to the student response 9328809, GPT4 interprets the student's statement \u201cMolecule 1 does not have donor or acceptor\u201d as suggesting that molecule 1 is non-polar and grades it as being correct which is actually incorrect. In this example, GPT4's interpretation might be true in general. However, it infers too much from the student's response in this specific problem, in which the instructor tries to test the concept of non-polar. We notice this is a type of error that GPT4 is prone to make in this grading task. This error type shows that, since LLM such as GPT is trained on massive data which is expected to have learned a large amount of general knowledge, how to ground it to some specific task and some specific domain is a big challenge. Our methods of R-RAG and structured evaluation provide an approach to address this issue. We also experimented with prompting engineering to set some constraints, such as defining the role to be a college-level Biology instructor and explicitly asking GPT4 to do the grading based only on the student's response, the evaluation question, the reference answer to the question, and nothing else. However, we find it's still hard to eliminate such error types by refining the prompts only.\n\u2022 Error cases of Q1 and Q2 in the student response 9328790 show the complexity of the grading task. Due to the student not giving any statements about molecule 3, GPT4 grades the response to be incorrect on these two questions which are both about molecule 3. However, the human grader is more focused on the concept that the most hydrophilic molecule has an OH which makes it able to form H-Bonds. Based on this, although the student discusses molecule 2 instead of molecule 3, the response shows he/she indeed understands the concept correctly. Accordingly, human graders give the student full credit on these two questions. During the human evaluation process, the course instructor and two human graders all agree that, in such cases, GPT4 does the job properly based on the instructions it's given. The challenge lies not only in how to make an LLM understand the abstract concept behind the text, but also in how to formulate what is examined in a problem in the learning process itself."}, {"title": "D. Ablation Study", "content": "We did the following ablation studies to show the effect of some parameters and settings. Due to the time and cost constraints, the following experiments were done using GPT-4.\nEffect of Clustering. As shown in Figure 4, applying a clustering algorithm to select samples for few-shot learning consistently improves Cohen's Kappa coefficient compared with that without using clustering, e.g., about 0.2 increments in the \nvalue under one-shot. This supports the effectiveness of the clustering approach in selecting learning samples that are expected to better represent the distribution of the data, and further empower the capability of the LLM such as GPT4 on this specific dataset and task.\nNumber of Shots. We experimented with different shot numbers. The Cohen's Kappa coefficient values in Figure 4 show that a few learning samples can significantly improve the performance of a general LLM on a specific task such as grading. Under the setting with clustering, the 4-shot gives the best result which is significantly higher than the 3-shot while slightly higher than the 5-shot. Under the setting without clustering, the performance under the 6-shot is significantly better than the 1-shot, while the 10-shot does not show much further improvement compared with the 6-shot. This is consistent with the common understanding that the few-shot in-text learning can guide a general LLM toward a specific task such as grading in this experiment. Meanwhile, the effect declines when reaching a reasonable shot number."}]}