{"title": "EFFICIENTLY LEARNING AT TEST-TIME: ACTIVE FINE-TUNING OF LLMS", "authors": ["Jonas H\u00fcbotter", "Sascha Bongni", "Ido Hakimi", "Andreas Krause"], "abstract": "Recent efforts in fine-tuning language models often rely on automatic data selection, commonly using Nearest Neighbors retrieval from large datasets. However, we theoretically show that this approach tends to select redundant data, limiting its effectiveness or even hurting performance. To address this, we introduce SIFT, a data selection algorithm designed to reduce uncertainty about the model's response given a prompt, which unifies ideas from retrieval and active learning. Whereas Nearest Neighbor retrieval typically fails in the presence of information duplication, SIFT accounts for information duplication and optimizes the overall information gain of the selected examples. We focus our evaluations on fine-tuning at test-time for prompt-specific language modeling on the Pile dataset, and show that SIFT consistently outperforms Nearest Neighbor retrieval, with minimal computational overhead. Moreover, we show that our uncertainty estimates can predict the performance gain of test-time fine-tuning, and use this to develop an adaptive algorithm that invests test-time compute proportional to realized performance gains. We provide the activeft (Active Fine-Tuning) library which can be used as a drop-in replacement for Nearest Neighbor retrieval.", "sections": [{"title": "1 INTRODUCTION", "content": "The standard paradigm of machine learning separates training and testing. Training aims to learn a model by inductively extracting general rules from data, and testing applies this model to new, unseen data. We investigate an alternative transductive paradigm where the model is fine-tuned at test-time specifically to the given task. Variations of this paradigm have been studied since the inception of machine learning as a field. Early examples are local learning (Cleveland, 1979; Cleveland & Devlin, 1988; Atkeson et al., 1997) and local fine-tuning (Bottou & Vapnik, 1992). More recently, with the advent of large pre-trained models which have good representations and are strong foundations for fine-tuning, the idea of test-time fine-tuning has re-gained attention (Krause et al., 2018; Sun et al., 2020). Hardt & Sun (2024) show that fine-tuning on data related to the prompt to a large language model (LLM) can significantly improve performance. Also, test-time fine-tuning is the central component of state-of-the-art approaches to the ARC challenge (Chollet, 2019; Cole & Osman, 2023), a non-saturated benchmark which is intended to test reasoning capabilities based on \u201ccore knowledge\u201d rather than mere memorization.\nActive Fine-Tuning: Effective data selection for fine-tuning LLMs Test-time fine-tuning demands automatic data selection since manually selecting data for each test instance is infeasible. Moreover, the sample efficiency of test-time fine-tuning is a central bottleneck as the number of gradient steps is directly proportional to inference time. Previous works on data selection for"}, {"title": "2 TEST-TIME FINE-TUNING", "content": "We define test-time fine-tuning of LLMs (Hardt & Sun, 2024) as follows. We consider a domain X of token sequences and assume that we have access to a large dataset of examples D C X which we call the data space. We further assume that we have access to a pre-trained autoregressive language model that maps token sequences X to probability distributions over the next token from a vocabulary of size V. Our work addresses the central question:\nGiven a prompt $x^* \\in X$, how can we effectively select fine-tuning data from the large dataset D such that the fine-tuned model performs well on the prompt?\nWe then fine-tune the model for a single gradient step on each selected sequence.\nLocally adjusting a model at test-time has gained popularity in the context of few-shot in-context learning (Brown et al., 2020; Wei et al., 2022b; Bubeck et al., 2023; OpenAI, 2024) with retrieval augmented generation (RAG, Lewis et al., 2019; Guu et al., 2020; Borgeaud et al., 2022). In contrast to this approach, test-time fine-tuning works by fine-tuning the parameters of a pre-trained model at test-time specifically to each prompt. Notably, test-time fine-tuning takes time linear in the number of"}, {"title": "2.1 NEAREST NEIGHBOR RETRIEVAL IS INSUFFICIENT", "content": "Prior work on data selection for fine-tuning has relied on Nearest Neighbor retrieval. The idea of making predictions on $x^*$ depending on its nearest neighbors has been around as long as machine learning itself (Fix, 1951; Cover &\nHart, 1967). Bottou & Vapnik (1992) were the first to apply this idea to the fine-tuning of convolutional neural networks by selecting the nearest neighbors of a test image in pixel-space. More recently, due to advances in representation learning (Devlin et al., 2018; Reimers & Gurevych, 2019) and efficiency (e.g., Johnson et al., 2019; Aum\u00fcller et al., 2020), Nearest Neighbor retrieval has regained attention and been applied to test-time fine-tuning (Hardt & Sun, 2024).\nXia et al. (2024) use influence functions (Cook, 1977; Koh & Liang, 2017; Pruthi et al., 2019) to select data for fine-tuning LLMs. This line of work aims to select data that reduces a first-order Taylor approximation to the test loss after fine-tuning, an approach that corresponds to Nearest Neighbor retrieval in a certain embedding space. They highlight two main limitations of the use of influence functions and Nearest Neighbor retrieval for data selection:\nSelecting the top-N nearest neighbors from the data space (according to cosine similarity or Euclidean distance) may not reduce the uncertainty about the response to the prompt beyond fine-tuning on the closest neighbor. Every additional passage may be redundant.\n\u2022 Nearest Neighbor retrieval selects data with high positive cosine similarity to the prompt. Yet, data with high negative cosine similarity can be equally informative as data with high positive cosine similarity (Xia et al., 2024, Appendix K.2), but is ignored by standard Nearest Neighbor retrieval.\nIn this work, we propose SIFT and show that it naturally addresses both limitations."}, {"title": "3 PRELIMINARIES: UNCERTAINTY ESTIMATION FOR FINE-TUNING", "content": "We suppose the assigned probability that $y \\in [V]$ is the class label of an input $x \\in X$ is given by $s_y(f^*(x))$, where $s_y$ is the softmax $s_y(f) = \\exp(f_y)/(\\sum_{i=1}^V \\exp(f_i))$. That is, $f^*(x)$ denotes the \"ground truth\" logits for a given input $x$. In the context of language modeling, V is the number of tokens in the vocabulary, and y denotes the index of the next token. We defer all proofs to Appendix K.\nWe use a surrogate model to quantify the informativeness of data, which we define next.\nWe emphasize that while SIFT relies on this surrogate model for data selection, it still fine-"}, {"title": "4 SIFT: EFFICIENTLY REDUCING UNCERTAINTY ABOUT THE RESPONSE", "content": "We introduce SIFT, an algorithm for selecting data for fine-tuning that effectively reduces the uncertainty about the response to the prompt $x^* \\in X$. Note that we can compute the uncertainty $\\sigma_X(x^*)$ about the response to the prompt $x^*$ for any selected data $X \\subseteq D$ in closed-form, since its definition (cf. Equation (2)) depends only on the selected inputs X. SIFT minimizes this uncertainty about $x^*$: \n$X_{n+1} = \\arg \\min_{x\\in D} \\sigma_{X_n \\cup {x}}(x^*) = \\arg \\max_{x \\in D} k_{X_n \\cup {x}}(x^*) (K_{X_n \\cup {x}} + \\lambda' I_{n+1})^{-1}k_{X_n \\cup {x}}(x^*)$.\nSIFT selects data that minimizes a bound on the approximation error of the surrogate model, and then fine-tunes the full LLM using this data. We discuss the design choices, including the choice of embeddings, that make SIFT efficient in \u00a74.2. In \u00a7C.1, we illustrate with an example of how SIFT balances relevance and diversity, where we also see that the free parameter $\\lambda'=\\lambda\\kappa$ controls this trade-off. Larger $\u03bb'$ emphasize relevance of selected data, while smaller $\u03bb'$ emphasize diversity. Probabilistically, SIFT can be interpreted as maximizing the information gain of the selected data $X_n$ on the response to the prompt $x^*$, that is, $X_{n+1} = \\arg \\max_{x\\in D} I(f(x^*); y(x) | Y_{1:n})$ where $y(x)$ denotes a noisy observation of the response to x. We formally introduce this interpretation of SIFT in \u00a7G."}, {"title": "4.1 UNCERTAINTY PROVABLY VANISHES", "content": "We prove that unlike with Nearest Neighbor retrieval, the uncertainty about the response to the prompt vanishes if SIFT is used to select data for fine-tuning. We give an informal overview here, and defer the formal treatment to \u00a7C.2. Our theoretical analysis shows that test-time fine-tuning can fully reduce uncertainty only if the data space contains sufficient information to determine the correct response. If the data space does not contain all relevant information, the remaining uncertainty is quantified by the limiting uncertainty after seeing \"all data in the data space infinitely often\", which we call the irreducible uncertainty and denote by $\\sigma_\\infty(x^*)$. We provide the formal definition in \u00a7C.2, but intuitively, the irreducible uncertainty is the largest quantity satisfying $\\sigma_X(x^*) \\geq \\sigma_\\infty(x^*)$ for all $X \\subseteq D$. We then specialize the result of H\u00fcbotter et al. (2024b) to show that the uncertainty about the response to the prompt shrinks at the rate $O(1/\\sqrt{n})$ until it reaches the irreducible uncertainty:\nNaturally, convergence is slower with a larger regularization parameter $\\lambda$ / smaller step size. Notably, the irreducible uncertainty depends on the data space. With a large and diverse data space, the irreducible uncertainty is typically negligible. This statistical guarantee is a key property of SIFT. As we show in Proposition K.1, Nearest Neighbor retrieval fails to satisfy a guarantee of this kind."}, {"title": "4.2 COMPUTE-EFFICIENT DATA SELECTION", "content": "We have established how to select informative data for fine-tuning. Next to good statistical efficiency, good computational efficiency is key for selecting data at test-time. In the following, we describe design choices such that SIFT has negligible overhead compared to Nearest Neighbor retrieval."}, {"title": "5 RESULTS", "content": "We focus on language modeling with causal language models. Following Hardt & Sun (2024), we fine-tune a pre-trained LLM for a single gradient step each on N = 50 selected data points in the order that they are selected, most to least relevant. We use the Pile dataset (Gao et al., 2020) for evaluation, restricting our use to data which is obtained and used in compliance with the terms of service of the data host. This version of the Pile contains a diverse set of 17 high-quality sub-datasets, ranging from Q&A to code, scientific publications, math, and more. Concretely, we use the Pile training set containing 210M sequences of total size 1.3TB as data space for data selection, and we evaluate on the Pile test set. We report the bits per byte metric as recommended by Gao et al. (2020), which is proportional to the negative log-likelihood loss normalized by a dataset-specific constant. Error bars correspond to 90% confidence intervals computed via bootstrapping with 1'000 samples.\nWe evaluate the GPT-2 model (Radford et al., 2019) with 124M param-eters also evaluated by Hardt & Sun (2024), with the default learning rate of the transformers library (Wolf et al., 2020). We obtain analogous results with GPT-2-large (774M parameters) and the state-of-the-art Phi-3 (3.8B parameters, Abdin et al., 2024). With Phi-3, we use Low-Rank Adaptation (LoRA, Hu et al., 2022), fine-tuning slightly less than 1% of the model's total parameters."}, {"title": "6 COMPUTE-PROPORTIONAL TEST-TIME FINE-TUNING", "content": "We have shown that test-time fine-tuning can improve language modeling ability and that SIFT is a robust method for data selection, outperforming Nearest Neighbor retrieval. However, a key shortcoming of previous approaches to test-time fine-tuning is that they spend a fixed amount of test-time compute, regardless of the nature of the prompt, the available data, or the model. This is not computationally scalable in many practical applications, since a fixed test-time compute budget leads to non-proportionate performance gains. For example, for the prompt \u201cHello\u201d to a chatbot we would not like to spend any test-time compute, while for a more complex prompt we would like to spend more compute. In this section, we evaluate whether uncertainty estimates can be used to adaptively stop test-time fine-tuning such that the realized performance gain is proportional to the compute used.\nWe find that $\\sigma_n (x^*)$ is mono-tonically and linearly correlated at coefficient \u2248 0.4 with the model error after n test-time iterations, i.e., the bits per byte $bpb_n (x^*)$. This is remarkable because $\\sigma_n$ contains information only from the surrogate embedding model, and is normalized such that $\\sigma_0 (x^*) = 1$. To determine the importance of the base model, we also evaluate the denormalized uncertainty estimate $\\hat{\\sigma}_n (x^*) = \\sigma_n (x^*) \\cdot bpb_0 (x^*)$, which unlike $\\sigma_n$ cannot be evaluated at test-time. We multiply $\\sigma_n$ by $bpb_0$ to ensure that the uncer-tainty measure is in the same units as the performance metric, correcting for the use of normalized surrogate embeddings. We find that $\\hat{\\sigma}_n (x^*)$ is strongly correlated at coefficient \u2265 0.5 with the bits per byte. We summarize correlations in Table 12 of \u00a7J and visualize the predictive capability of $\\hat{\\sigma}_n$ in Figure 8 (left) and Figure 8 (middle). Our findings indicate that approximations of the base model's uncertainty, before test-time fine-tuning, can be beneficial. In future work, we intend to determine whether generating embeddings from the base model can provide such scale-correction."}, {"title": "7 DISCUSSION AND FUTURE WORK", "content": "We propose a data selection algorithm, SIFT, unifying ideas from retrieval and active learning. SIFT estimates the uncertainty about the response to a given prompt after having been fine-tuned on some data (\u00a73), and then selects the data that minimizes this uncertainty (\u00a74). This addresses the limi-tations of Nearest Neighbor retrieval (\u00a72). SIFT can be seen as a generalization of Nearest Neighbor retrieval from a search method to a learning method, which ensures explicitly that the retrieved data is maximally informative. We show on the Pile dataset that SIFT consistently outperforms Nearest Neighbor retrieval in prompt-specific fine-tuning at test-time and that this kind of local learning can be more effective than locally learning from examples in-context (\u00a75). Finally, we observe that our uncertainty estimates can predict the performance gain of test-time fine-tuning, and use this to develop an adaptive algorithm which achieves compute-proportional performance gains (\u00a76).\nTest-time fine-tuning addresses a fundamental limitation of in-context learning, namely that in-context learning is typically limited to a fixed and finite context window. In contrast, test-time fine-tuning allows the LLM to dynamically and effectively access a potentially unbounded non-parametric memory. By improving the effectiveness of test-time fine-tuning, this work opens up several exciting directions for future research. Test-time fine-tuning may be used to ground the model on a trusted dataset, mitigate biases against under-represented groups in the training data, or to dynamically include private data depending on user privileges. Particularly interesting would be a broad evaluation on non-perplexity tasks such as code generation or in the life sciences with large-scale medical or protein data. Unlike few-shot in-context learning which is limited in scope to autoregressive models, test-time fine-tuning and SIFT may be extended to other model classes such as diffusion models. Furthermore, SIFT may be used effectively in other settings that require automatic data selection, such as targeted instruction tuning during post-training of LLMs. Finally, our results suggest scaling laws for test-time fine-tuning and we outline several exciting open questions (\u00a76)."}]}