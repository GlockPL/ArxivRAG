{"title": "Knowledge Sharing and Transfer via Centralized Reward Agent for Multi-Task Reinforcement Learning", "authors": ["Haozhe Ma", "Zhengding Luo", "Thanh Vinh Vo", "Kuankuan Sima", "Tze-Yun Leong"], "abstract": "Reward shaping is effective in addressing the sparse-reward challenge in reinforcement learning by providing immediate feedback through auxiliary informative rewards. Based on the reward shaping strategy, we propose a novel multi-task reinforcement learning framework, that integrates a centralized reward agent (CRA) and multiple distributed policy agents. The CRA functions as a knowledge pool, which aims to distill knowledge from various tasks and distribute it to individual policy agents to improve learning efficiency. Specifically, the shaped rewards serve as a straightforward metric to encode knowledge. This framework not only enhances knowledge sharing across established tasks but also adapts to new tasks by transferring valuable reward signals. We validate the proposed method on both discrete and continuous domains, demonstrating its robustness in multi-task sparse-reward settings and its effective transferability to unseen tasks.", "sections": [{"title": "1 Introduction", "content": "Reinforcement learning (RL) has made significant progress across various domains, like robotics Kober et al. (2013), gaming Lample and Chaplot (2017), autonomous vehicles Aradi (2020), signal processing Luo et al. (2024), and large language models Shinn et al. (2023). However, environments with sparse and delayed rewards remain a significant challenge, as the absence of immediate feedback hinders the agent from distinguishing the value of states and leads to aimless exploration (Ladosz et al., 2022). Reward Shaping (RS) has been proven to be an effective technique to address this challenge by providing additional dense and informative rewards Sorg et al. (2010b,a). Concurrently, multi-task reinforcement learning (MTRL) is becoming increasingly important for its ability to share and transfer knowledge across tasks. In this context, the auxiliary rewards infused with task-specific information in RS offer a straightforward entry to distribute knowledge among different tasks. Therefore, we believe that integrating RS techniques into MTRL is a highly promising and intuitive direction to enhance the efficacy of multi-task system learning.\nPresently, numerous MTRL algorithms involving knowledge transfer have been developed. Policy distillation methods identify and combine the commonalities across different policies Rusu et al. (2016); Teh et al. (2017); Parisotto et al. (2016); Xu et al. (2024); representation sharing methods extract and disseminate the common features or gradients among agents Yang et al. (2020); D'Eramo et al. (2020); Sodhani et al. (2021); and parameter sharing methods design architectural modules to reuse parameters or layers across networks Sun et al. (2022); Cheng et al. (2023). Despite their potential, these strategies often encounter challenges such as slow adaptation and response to the transferred knowledge, or suffer from delays in reutilizing and comprehending this information. Therefore, leveraging the RS mechanism, which directly adds a metric to the reward function, offers a promising alternative to address these limitations.\nRegarding the RS techniques, not all shaped rewards serve effectively as mediums for knowledge transfer. Specifically, the intrinsic motivation based rewards are typically designed using heuristics to generate task-agnostic signals. Examples include incorporating exploration bonuses Bellemare et al. (2016); Ostrovski et al. (2017); Devidze et al. (2022), rewarding novel states Tang et al. (2017); Burda et al. (2018), and encouraging curiosity-driven behaviors Pathak et al. (2017); Mavor-Parker et al. (2022). Although these approaches broaden extensive exploration, they are not directly related to specific tasks and thus lack transferability. Consequently, we focus on another branch of RS methods, the task-contextual rewards, which automatically learn and encode task-specific information, such as the hidden values, contributions of states, or future-oriented insights, that can be shared effectively across various tasks Ma et al. (2024b); Mguni et al. (2023); Memarian et al. (2021); Ma et al. (2024a).\nTo share task-related knowledge in MTRL via RS techniques, and inspired by the ReLara framework Ma et al. (2024b), which integrates an assistant reward agent to densify sparse environmental rewards, we propose the Centralized Reward Agent based MTRL framework (CenRA). The framework consists of two main components: a centralized reward agent (CRA) and multiple distributed policy agents. Each policy agent individually learns control behaviors within their respective tasks and shares their experiences with the CRA, which are collected from interactions with their environment. The CRA extracts common knowledge from these experiences and learns to generate dense rewards that are encoded with task-specific information. These rewards are then distributed back to the policy agents to augment their original environmental rewards. Additionally, given that different tasks may contribute variably to the MTRL system, we introduce an information synchronization mechanism to further balance the knowledge distribution, considering task similarity and agent learning progress, to ensure a system-wide optimal performance. The main contributions of this paper are summarized as follows:\n(i) We propose the CenRA framework targeted to solve MTRL problems. It incorporates a CRA that functions as a centralized knowledge pool, efficiently distilling and distributing valuable information from various tasks to policy agents, and further adapting to new tasks.\n(ii) CenRA leverages the advantage of reward shaping techniques to infuse insights via dense rewards. This approach not only provides a direct metric for policy agents to absorb knowledge but also effectively addresses the sparse-reward challenge.\n(iii) We introduce an information synchronization mechanism that considers both task similarity and agent convergence progress to balance multi-task learning. This mechanism provides a novel direction for maintaining system equilibrium in MTRL.\n(iv) CenRA is validated in both discrete and continuous control MTRL environments, where the extrinsic rewards are sparse. CenRA outperforms several baseline models in terms of learning efficiency, knowledge transferability, and system-wide performance."}, {"title": "2 Related Work", "content": "Multi-task reinforcement learning (MTRL) has attracted significant attention recently due to its potential to share knowledge across multiple tasks, thereby improving learning performance Caruana (1993). We discuss existing MTRL literature from three main directions:\nKnowledge Transfer methods focus on identifying and transferring task-relevant features across diverse tasks Zeng et al. (2021). Policy distillation Rusu et al. (2016) is a well-studied approach to extract and share task-specific behaviors or representations that many works are built on: Teh et al. (2017) introduced Distral, which distills a centroid policy from multiple task-policies; Parisotto et al. (2016) developed"}, {"title": "3 Preliminaries", "content": "Markov Decision Process (MDP) fundamentally models sequential decision-making problems under uncertainty. An MDP represents the interaction between an agent and its environment as a tuple $(S, A, P, R, \\gamma)$, where S is the state space, A is the action space, $P: S\\times A\\times S \\rightarrow [0, 1]$ is the probability of transiting from one state to another given an action, $R: S \\times A \\rightarrow R$ is the reward function, and $\\gamma\\in [0,1]$ is the discount factor to modulate the importance of future versus immediate reward.\nMulti-Task Reinforcement Learning (MTRL) addresses the challenge of learning multiple tasks simultaneously within an integrated model to leverage commonalities and differences across tasks. Typically, MTRL introduces a task space T, assuming all tasks are sampled from this space and thus follow a unique distribution. Each task is modeled as an independent MDP. An MTRL agent aims to learn optimal policies $\\pi_i: S \\rightarrow A$ for each task $T_i \\sim T$, to maximize their corresponding expected cumulative rewards, or returns, denoted by $G_i = E[\\sum_{t=0}^{\\infty} \\gamma^t R_i(s_t, a_t)]$."}, {"title": "4 Methodology", "content": "We extend the ReLara framework Ma et al. (2024b), which utilizes a specialized reward agent to extract environmental information and densify the sparse rewards, to MTRL scenarios. Our proposed framework is termed as Centralized Reward Agent based MTRL (CenRL), which incorporates a centralized reward agent (CRA) to support simultaneous reinforcement learning agents across multiple tasks. A high-level illustration of the CenRA framework is shown in Figure 2. The CRA is responsible for extracting general task-specific knowledge from various tasks and distributing valuable information to the policy agents by reconstructing their reward models. The detailed methodology for knowledge extraction and sharing is introduced in Section 4.1. Furthermore, we recognize there are potential disparities in the information that each task could contribute, which might lead to an imbalance in knowledge distribution. To mitigate this, we introduce an information synchronization mechanism by considering two main factors: the similarity of the tasks and the online learning performance of the policy agents, details given in Section 4.2. Finally, the overall framework of CenRA is presented in Section 4.3."}, {"title": "4.1 Knowledge Extraction and Share", "content": "4.1.1 Problem Formulation\nWe consider an MTRL setting comprising N distinct tasks ${T_1, T_2,...,T_N}$, all executed within the same type of environment E. We assume that the dimensionality and structure of state space S and action space A remain uniform across tasks, to ensure the centralized reward agent processes consistent inputs. Despite this uniformity, each task may feature unique state sets, goals, and transition dynamics. For instance, a series of mazes with the same size and shape but varying map configurations and objects would satisfy this condition. For the environment E, we denote states by $s \\in S$ and actions by $a \\in A$. Additionally, for each task $T_i$, we denote the transition function as $P_i(s'|s, a)$ and the reward function as $R_i(s, a)$.\nThe CenRA framework consists of a centralized reward agent $A^{rwd}$ and multiple policy agents ${A^{pol}_1, A^{pol}_2,...,A^{pol}_N}$. Each policy agent $A^{pol}_i$ operates independently to complete its corresponding task $T_i$, utilizing appropriate RL algorithms as backbones based on the specific characteristics of the task. For"}, {"title": "4.1.2 Centralized Reward Agent", "content": "The CRA $A^{rwd}$ aims to extract environment-relevant knowledge and distribute it to policy agents by generating additional dense rewards to support their original reward functions. Similar to the ReLara framework Ma et al. (2024b), we model the CRA as a self-contained RL agent, yet, as an extension to ReLara, our CRA is designed to concurrently interact with multiple policy agents and their respective tasks. The CRA's policy $\\pi^{rwd}$ generates continuous rewards given both an environmental state and a policy agent's behavior. Specifically, $\\pi^{rwd}$ maps the Cartesian product of the state space and action space, S \u00d7 A, to a defined reward space, which constrains the rewards to a range of real numbers, $R = [R_{min}, R_{max}] \\subset R$. For simplicity's sake, we denote the observation of CRA as $s^{rwd} = (s_i, a_i)$, where $s_i \\sim T_i$ and $a_i \\sim \\pi^{pol}_i(s_i)$. To distinguish from the environmental reward, the generated reward is termed as knowledge reward, denoted as $r^{knw}$.\nWe adopt an off-policy actor-critic algorithm to optimize the CRA Konda and Tsitsiklis (1999). To aggregate and reuse experiences from all policy agents, a concatenated replay buffer $D = \\cup_{i=1}^N D_i$ is constructed, where $D_i$ represents the replay buffer of each policy agent $A^{pol}_i$. Besides, each transition is augmented with the CRA-generated knowledge reward, $r^{knw}$. Specifically, the transition from policy agent $A^{pol}_i$ stored in the replay buffer is defined as $\\tau_t = (s^{rwd}_t, r^{knw}_t, r^{env}_t, s^{rwd}_{t+1}|T_i)$. The augmented transition includes all necessary information for optimizing both the CRA and each corresponding policy agent, thus making the concatenated replay buffer a shared resource across the entire framework and minimizing storage overhead.\nThe CRA's update process involves using these stored transitions to optimize the reward-generating actor $\\pi^{rwd}$ and the value-estimation critic. The temporal difference error is given by:\n$\\delta_t = r^{env}_t + \\gamma V^{rwd}(s^{rwd}_{t+1}) - V^{rwd}(s^{rwd}_t)|T_i$,\nwhere $\\tau_t = (r^{rwd}, s^{rwd}, s^{rwd}_{t+1}, r^{env}_t) \\sim D$. The objective function for optimizing the critic module is formulated as:\n$J(V^{rwd}) = E_{\\tau_t\\sim D} [\\delta_t^2]$,"}, {"title": "4.1.3 Policy Agents with Knowledge Rewards", "content": "Each policy agent $A^{pol}_i$ interacts with its specific environment and stores the experiences in its corresponding replay buffer $D_i$. Policy agents receive two types of rewards: the environmental reward $r^{env}$ from their respective task $T_i$ and the knowledge reward $r^{knw}$ from CRA. The augmented reward is given by:\n$r^{pol}_i = r^{env}_i + \\lambda r^{knw}, r^{knw} \\sim \\pi^{rwd}(\\cdot|s_i, a_i)$,\nwhere $\\lambda \\in (0, 1]$ is a scaling weight factor. The optimal policy $\\pi^{pol*}_i$ for each agent is derived by maximizing the cumulative augmented reward:\n$\\pi^{pol*}_i = arg\\,\\underset{\\pi^{pol}_i}{max} E_{(s_i,a_i) \\sim \\pi^{pol}_i} [\\sum_{t=0}^{\\infty} \\gamma^t r_i^{pol}]$.\nIt is worth noting that the environmental reward $r^{env}$ is retrieved from the replay buffer (if adopting an off-policy approach). Conversely, the knowledge reward $r^{knw}$ is computed in real-time using the most recently updated $A^{rwd}$, ensuring it reflects the latest learning advancements. Lastly, each policy agent is able to employ any suitable RL algorithm, whether on-policy or off-policy, to best address its specific task, which enhances the CenRA framework's generality and flexibility."}, {"title": "4.2 Information Synchronization of Policy Agents", "content": "In the CenRA framework, the information provided by different tasks may exhibit significant disparities, potentially leading to an imbalance in knowledge extraction and distribution. In this section, we introduce an information synchronization mechanism for CenRA to maintain a balanced manner from the perspective of the entire system. Specifically, this mechanism is implemented by controlling the quantity of samples that CRA retrieves from each task's replay buffer $D_i$ by a sampling weight w. We mainly consider two aspects: the similarity among tasks and the real-time learning performance of the policy agents.\nSimilarity Weight is derived from the similarity among tasks, enabling the CRA to focus on relatively outlier tasks. To simplify computation, we use the hidden layers extracted from each policy agent's neural network encoders to represent the tasks' features. To reduce randomness, we average the hidden features of the most recent K steps. We adopt a cross-attention mechanism, which is widely used in neural networks, to calculate the similarity weight Vaswani et al. (2017). Specifically, for task $T_i$, let $H_i$ denote the averaged hidden feature vector, which serves as the key, and the centroid of all tasks c acts as the query. Then, the similarity $s_i$ of task $T_i$ to the centroid of the task cluster is calculated as:\n$s_i = \\frac{c^T H_i}{\\sqrt{D}}, c = \\frac{1}{N} \\sum_{k=1}^N H_k$,\nwhere D is the dimension of the hidden feature to prevent gradient vanishing or exploding. A larger $s_i$ indicates a greater similarity between $T_i$ and the centroid. Given our assumption is that the tasks farther from the centroid require more attention, the similarity weight is defined as $w^{sim} = Softmax([1/s_1, 1/s_2,...,1/s_N])$.\nPerformance Weight is determined by the real-time learning performance of each policy agent, to ensure the CRA focuses more on lagging tasks. Similar to the similarity weight, we average the environmental"}, {"title": "4.3 Overall Framework", "content": "The overall framework of CenRA is summarized in Algorithm 1. The CRA and policy agents are updated alternately and asynchronously, with the frequency of updating the CRA adjustable according to the actual situation. Sampling weights are calculated in real-time, using the most recently optimized encoders and the current learning performance, ensuring CRA continuously adjusts its focus to optimally balance knowledge extraction across multiple tasks.\nThe learned CRA acts as a robust knowledge pool, which is able to support new tasks as an additional module by transferring knowledge through auxiliary reward signals. This is particularly beneficial in sparse-reward environments, as the knowledge rewards can guide the policy agents toward the correct direction and reduce the exploration burden. Additionally, the CRA can be further optimized alongside new tasks in a continuous learning scheme that enhances adaptability and effectiveness in dynamic settings."}, {"title": "5 Experiments", "content": "We conduct experiments in three environments with multiple tasks: 2DMaze and 3DPickup by Chevalier-Boisvert et al. (2024), and MujocoCar by Ji et al. (2023), which include both discrete and continuous control problems. All tasks provide challenging sparse environmental rewards, where the agent only receives a reward of 1 upon successful completion of the final objective, and a reward of 0 is given in all other states. Detailed configurations are shown in Figure 3, with each task's setting provided in Appendix A."}, {"title": "5.1 Comparative Evaluation in MTRL", "content": "In this section, we evaluate the learning performance of CenRA in the MTRL context, configuring each environment with four different tasks. We benchmark CenRA against several state-of-the-art baselines, including: (a) the backbone algorithms of the policy agents in CenRA: the Deep Q-Network (DQN) Mnih et al. (2015) for the discrete control tasks in 2DMaze and 3DPickup, and the Soft Actor-Critic (SAC) Haarnoja et al. (2018b) for the continuous control tasks in MujocoCar; (b) The RL with an Assistant Reward Agent (ReLara) algorithm Ma et al. (2024b), which learns an additional reward agent to generate informative rewards. ReLara can be considered as a special case of CenRA where each policy agent is paired with a separate reward agent, without cross-task information sharing; (c) The Policy Optimization and Policy Correction (PiCor) algorithm Bai et al. (2023), which uses policy correction to adapt and transfer information across policies among individual tasks; (d) The Multi-Critic Actor Learning (MCAL) algorithm Mysore et al. (2022), which maintains separate critics for each task for value estimation and knowledge representation, while training a unified multi-task actor. We use the CleanRL library Huang et al. (2022) or the author-provided codes to implement these baselines.\nWe train multiple instances with different random seeds for each task to report the average data. The results of the episodic returns and their standard errors throughout training are shown in Figure 4 and Table 1. To ensure a fair comparison, we use the same hyperparameters (if applicable) and network structures across all experiments, the details are provided in Appendix B.\nWe observe that CenRA consistently outperforms the baselines in three main aspects. First, CenRA achieves the highest episodic returns in nearly all tasks, demonstrating superior learning efficiency and faster convergence. Importantly, it also maintains good stability and robustness, which can be seen in the significantly fewer fluctuations and oscillations especially after convergence, compared to other models. Second, unlike PiCor and MCAL, which have shown uneven progress across different tasks in the same environment, CenRA maintains a well-balanced performance by showing relatively consistent learning progress and minimal variability in learning outcomes across each four-task group. This ensures that no single task's performance is disproportionately high at the expense of others, which is crucial in multi-task learning. Third, the centralized reward agent in CenRA effectively enhances knowledge sharing among tasks. This is evident from the comparison with ReLara, which uses independent reward agents and lacks the mechanism for knowledge exchange. By extracting and distributing insights from one task to another,"}, {"title": "5.2 What Has the Centralized Reward Agent Learned?", "content": "In this section, we visualize the learned knowledge rewards by the centralized reward agent $A^{rwd}$ in the 2DMaze environment. After training on the four tasks in Section 5.1 of the paper, we let the CRA generate the knowledge rewards for each action in every state and visualize the action direction that yields the maximum rewards, $a^* = arg \\,\\underset{a}{max} r^{rwd*}(s_i, a), S_i \\sim S$, in Figure 5.\nThe shaded areas in the figures represent regions within the real task that the agent cannot reach, as it cannot access the space behind the door without picking up the key. However, we forced the agent into these areas for evaluation. Outside the shaded regions, we observe that the CRA successfully learned meaningful knowledge rewards. Before picking up the key, the agent received the highest reward in the corresponding state when moving towards the key. Similarly, after picking up the key, the agent received the highest reward when moving towards the door and the final target. This demonstrates that in scenarios where the original environmental rewards are sparse, these detailed knowledge rewards can effectively guide the agent to converge more quickly."}, {"title": "5.3 Knowledge Transfer to New Tasks", "content": "In this section, we assess the centralized reward agent's ability to transfer learned knowledge to new tasks. We continue using the CRA model trained on the four tasks in Section 5.1 but initiate a new policy agent to tackle a newly designed task that the models have never encountered before. For the CenRA framework, we explore two scenarios: one where the CRA continues to optimize while cooperating with the new policy agent (CenRA w/ learning), and another where only the policy agent is trained without further updates to the CRA (CenRA w/o learning). We compare the two approaches with the backbone algorithms (DQN and SAC) and ReLara. In the ReLara model, the reward agent starts training anew (without pre-learned knowledge). The results are shown in Figure 6 and Table 2.\nWe observe that CenRA with further learning achieved rapid convergence, mainly due to the CRA's retention of previously acquired knowledge and its ability to quickly adapt to new tasks through continuous optimization. Notably, even without any additional training, CenRA outperforms both the ReLara, which requires training a new reward agent, and the backbone algorithms, which lack additional information. In the CenRA framework, the CRA stores environment-relevant knowledge and transfers it to other policy agents, thus improving their learning efficiency. Additionally, sharing pre-learned information with policy agents is particularly crucial in our experiments with challenging sparse rewards. Without any external knowledge, learning would require extensive exploration. However, the CRA provides meaningful dense rewards that significantly accelerate the learning process, even during the initial phases."}, {"title": "5.4 Effect of Sampling Weight", "content": "We conducted experiments to understand the effects of the information synchronization mechanism in the CenRA framework. Specifically, we compare the full CenRA model against three variants: (a) CenRA without the similarity weight $w^{sim}$, (b) CenRA without the performance weight $w^{per}$, and (c) CenRA without the entire sampling weight. The comparison results are shown in Figure 8 and Table 3.\nThe results indicate that the two weights, which control the allocation of samples drawn from each policy agent's experiences, mainly influence the overall learning performance. Specifically, the absence of sampling weight leads to unbalanced learning outcomes, which is observed by the increased variance in episodic returns across four tasks. While the full CenRA model does not always achieve the lowest variance, it consistently outperforms the other three ablation models regarding overall system performance.\nBoth weights are essential for the information synchronization mechanism, with the performance weight $w^{per}$ having a more important effect. This weight enables CRA to pay more attention to the policy agents that are underperforming or progressing slowly. This suggests that considering the overall learning performance of the multi-task system is also an important objective CenRA seeks to achieve."}, {"title": "6 Discussion and Conclusion", "content": "We propose a novel framework named CenRA that innovatively integrates reward shaping into MTRL. The framework not only can share environment-relevant knowledge across tasks to improve learning efficiency, but also effectively addresses the sparse-reward challenge. Specifically, a CRA functions as a knowledge pool, responsible for knowledge distillation and distribution across multiple tasks. Furthermore, our information synchronization mechanism mitigates knowledge distribution disparities across tasks, ensuring optimal system-wide performance. Experiments demonstrate that the dense knowledge rewards generated by the CRA efficiently guide the policy agents' learning, leading to faster convergence compared to baseline methods. The CenRA framework proves to be robust and performs superiorly in both multi-task learning and new tasks adaptation.\nCenRA's current limitation is its requirement for uniform state and action spaces across tasks. Future improvements could explore preprocessing techniques to adapt the framework for varying state and action spaces to broaden CenRA's applicability. Additionally, the fixed trade-off between similarity weight and performance weight may not be ideal. A more flexible approach, potentially through adaptive regulation of these weights, could enhance the framework further. Besides, the performance weight might favor underperforming tasks, achieving overall balance but potentially limiting the upper bound of top-performing tasks. This issue could necessitate a better mechanism for trade-off."}]}