{"title": "Knowledge Sharing and Transfer via Centralized Reward Agent\nfor Multi-Task Reinforcement Learning", "authors": ["Haozhe Ma", "Zhengding Luo", "Thanh Vinh Vo", "Kuankuan Sima", "Tze-Yun Leong"], "abstract": "Reward shaping is effective in addressing the sparse-reward challenge in reinforcement learning by\nproviding immediate feedback through auxiliary informative rewards. Based on the reward shaping\nstrategy, we propose a novel multi-task reinforcement learning framework, that integrates a centralized\nreward agent (CRA) and multiple distributed policy agents. The CRA functions as a knowledge pool,\nwhich aims to distill knowledge from various tasks and distribute it to individual policy agents to\nimprove learning efficiency. Specifically, the shaped rewards serve as a straightforward metric to encode\nknowledge. This framework not only enhances knowledge sharing across established tasks but also\nadapts to new tasks by transferring valuable reward signals. We validate the proposed method on both\ndiscrete and continuous domains, demonstrating its robustness in multi-task sparse-reward settings and\nits effective transferability to unseen tasks.", "sections": [{"title": "1 Introduction", "content": "Reinforcement learning (RL) has made significant progress across various domains, like robotics Kober\net al. (2013), gaming Lample and Chaplot (2017), autonomous vehicles Aradi (2020), signal processing Luo\net al. (2024), and large language models Shinn et al. (2023). However, environments with sparse and\ndelayed rewards remain a significant challenge, as the absence of immediate feedback hinders the agent from\ndistinguishing the value of states and leads to aimless exploration (Ladosz et al., 2022). Reward Shaping\n(RS) has been proven to be an effective technique to address this challenge by providing additional dense\nand informative rewards Sorg et al. (2010b,a). Concurrently, multi-task reinforcement learning (MTRL)\nis becoming increasingly important for its ability to share and transfer knowledge across tasks. In this\ncontext, the auxiliary rewards infused with task-specific information in RS offer a straightforward entry\nto distribute knowledge among different tasks. Therefore, we believe that integrating RS techniques into\nMTRL is a highly promising and intuitive direction to enhance the efficacy of multi-task system learning.\nPresently, numerous MTRL algorithms involving knowledge transfer have been developed. Policy distillation\nmethods identify and combine the commonalities across different policies Rusu et al. (2016); Teh et al.\n(2017); Parisotto et al. (2016); Xu et al. (2024); representation sharing methods extract and disseminate\nthe common features or gradients among agents Yang et al. (2020); D'Eramo et al. (2020); Sodhani et al.\n(2021); and parameter sharing methods design architectural modules to reuse parameters or layers across\nnetworks Sun et al. (2022); Cheng et al. (2023). Despite their potential, these strategies often encounter\nchallenges such as slow adaptation and response to the transferred knowledge, or suffer from delays in"}, {"title": "2 Related Work", "content": "Multi-task reinforcement learning (MTRL) has attracted significant attention recently due to its potential\nto share knowledge across multiple tasks, thereby improving learning performance Caruana (1993). We\ndiscuss existing MTRL literature from three main directions:\nKnowledge Transfer methods focus on identifying and transferring task-relevant features across diverse\ntasks Zeng et al. (2021). Policy distillation Rusu et al. (2016) is a well-studied approach to extract and\nshare task-specific behaviors or representations that many works are built on: Teh et al. (2017) introduced\nDistral, which distills a centroid policy from multiple task-policies; Parisotto et al. (2016) developed"}, {"title": "3 Preliminaries", "content": "Markov Decision Process (MDP) fundamentally models sequential decision-making problems under\nuncertainty. An MDP represents the interaction between an agent and its environment as a tuple\n$(S, A, P, R, \u03b3)$, where S is the state space, A is the action space, $P: S\u00d7A\u00d7S \u2192 [0, 1]$ is the probability of\ntransiting from one state to another given an action, $R: S \u00d7 A \u2192 R$ is the reward function, and \u03b3\u2208 [0,1]\nis the discount factor to modulate the importance of future versus immediate reward.\nMulti-Task Reinforcement Learning (MTRL) addresses the challenge of learning multiple tasks\nsimultaneously within an integrated model to leverage commonalities and differences across tasks. Typically,\nMTRL introduces a task space T, assuming all tasks are sampled from this space and thus follow a unique\ndistribution. Each task is modeled as an independent MDP. An MTRL agent aims to learn optimal policies\n$\u03c0_i: S \u2192 A$ for each task $T_i ~ T$, to maximize their corresponding expected cumulative rewards, or returns,\ndenoted by $G_i = E[\\sum_{t=0}^{\u221e} \u03b3^t R_i(s_t, a_t)]$"}, {"title": "4 Methodology", "content": "We extend the ReLara framework Ma et al. (2024b), which utilizes a specialized reward agent to extract\nenvironmental information and densify the sparse rewards, to MTRL scenarios. Our proposed framework\nis termed as Centralized Reward Agent based MTRL (CenRL), which incorporates a centralized reward\nagent (CRA) to support simultaneous reinforcement learning agents across multiple tasks. A high-level\nillustration of the CenRA framework is shown in Figure 2. The CRA is responsible for extracting general\ntask-specific knowledge from various tasks and distributing valuable information to the policy agents by\nreconstructing their reward models. The detailed methodology for knowledge extraction and sharing is\nintroduced in Section 4.1. Furthermore, we recognize there are potential disparities in the information that\neach task could contribute, which might lead to an imbalance in knowledge distribution. To mitigate this,\nwe introduce an information synchronization mechanism by considering two main factors: the similarity of\nthe tasks and the online learning performance of the policy agents, details given in Section 4.2. Finally, the\noverall framework of CenRA is presented in Section 4.3."}, {"title": "4.1 Knowledge Extraction and Share", "content": "We consider an MTRL setting comprising N distinct tasks {T1, T2,...,TN}, all executed within the same\ntype of environment E. We assume that the dimensionality and structure of state space S and action space\nA remain uniform across tasks, to ensure the centralized reward agent processes consistent inputs. Despite\nthis uniformity, each task may feature unique state sets, goals, and transition dynamics. For instance, a\nseries of mazes with the same size and shape but varying map configurations and objects would satisfy this\ncondition. For the environment E, we denote states by $s \u2208 S$ and actions by $a \u2208 A$. Additionally, for each\ntask Ti, we denote the transition function as $P_i(s'|s, a)$ and the reward function as $R_i(s, a)$.\nThe CenRA framework consists of a centralized reward agent $A^{rwd}$ and multiple policy agents {$A^{pol}_1$,\n$A^{pol}_2$,...,$A^{pol}_N$}. Each policy agent $A^{pol}_i$ operates independently to complete its corresponding task Ti,\nutilizing appropriate RL algorithms as backbones based on the specific characteristics of the task. For"}, {"title": "4.1.2 Centralized Reward Agent", "content": "The CRA $A^{rwd}$ aims to extract environment-relevant knowledge and distribute it to policy agents by\ngenerating additional dense rewards to support their original reward functions. Similar to the ReLara\nframework Ma et al. (2024b), we model the CRA as a self-contained RL agent, yet, as an extension to\nReLara, our CRA is designed to concurrently interact with multiple policy agents and their respective\ntasks. The CRA's policy $\u03c0^{rwd}$ generates continuous rewards given both an environmental state and a policy\nagent's behavior. Specifically, $\u03c0^{rwd}$ maps the Cartesian product of the state space and action space, S \u00d7 A,\nto a defined reward space, which constrains the rewards to a range of real numbers, R = [Rmin, Rmax] \u2282R.\nFor simplicity's sake, we denote the observation of CRA as $s^{rwd}_i = (s_i, a_i)$, where $s_i ~ T_i$ and $a_i ~ \u03c0^{pol}_i(s_i)$.\nTo distinguish from the environmental reward, the generated reward is termed as knowledge reward, denoted\nas $r^{knw}$.\nWe adopt an off-policy actor-critic algorithm to optimize the CRA Konda and Tsitsiklis (1999). To aggregate\nand reuse experiences from all policy agents, a concatenated replay buffer D = $\u222a^N_{i=1} D_i$ is constructed,\nwhere Di represents the replay buffer of each policy agent $A^{pol}_i$. Besides, each transition is augmented\nwith the CRA-generated knowledge reward, $r^{knw}$. Specifically, the transition from policy agent $A^{pol}_i$ stored\nin the replay buffer is defined as \u03c4 = $(s^{rwd}_t , r^{knw}_t, r^{env}_t , s^{rwd}_{t+1}|T_i)$. The augmented transition includes all\nnecessary information for optimizing both the CRA and each corresponding policy agent, thus making the\nconcatenated replay buffer a shared resource across the entire framework and minimizing storage overhead.\nThe CRA's update process involves using these stored transitions to optimize the reward-generating actor\n$\u03c0^{rwd}$ and the value estimation critic. The temporal difference error is given by:\n$\u03b4_t = r^{env}_t + \u03b3V^{rwd}(s^{rwd}_{t+1}) \u2013 V^{rwd}(s^{rwd}_t)|T_i$,\nwhere $\u03c4_t = (s^{rwd}_t , r^{knw}_t, r^{env}_t , s^{rwd}_{t+1}) ~ D$. The objective function for optimizing the critic module is formulated\nas:\n$J(V^{rwd}) = E_{\u03c4_t~D} [\u03b4^2_t]$,"}, {"title": "4.1.3 Policy Agents with Knowledge Rewards", "content": "Each policy agent $A^{pol}_i$ interacts with its specific environment and stores the experiences in its corresponding\nreplay buffer Di. Policy agents receive two types of rewards: the environmental reward $r^{env}$ from their\nrespective task Ti and the knowledge reward $r^{knw}$ from CRA. The augmented reward is given by:\n$r^{pol}_i = r^{env}_i + \u03bbr^{knw}, r^{knw} ~ \u03c0^{rwd}(\u00b7|s_i, a_i)$,\nwhere \u03bb \u2208 (0, 1] is a scaling weight factor. The optimal policy $\u03c0^{pol*}_{i}$ for each agent is derived by maximizing\nthe cumulative augmented reward:\n$\u03c0^{pol*}_{i} = arg max_{\u03c0^{pol}_i} E_{(s_i,a_i)~\u03c0^{pol}_i} [\\sum^{\\infty}_{t=0} \u03b3^t r^{pol}_i]$ .\nIt is worth noting that the environmental reward $r^{env}$ is retrieved from the replay buffer (if adopting an\noff-policy approach). Conversely, the knowledge reward $r^{knw}$ is computed in real-time using the most\nrecently updated $A^{rwd}$, ensuring it reflects the latest learning advancements. Lastly, each policy agent is\nable to employ any suitable RL algorithm, whether on-policy or off-policy, to best address its specific task,\nwhich enhances the CenRA framework's generality and flexibility."}, {"title": "4.2 Information Synchronization of Policy Agents", "content": "In the CenRA framework, the information provided by different tasks may exhibit significant disparities,\npotentially leading to an imbalance in knowledge extraction and distribution. In this section, we introduce\nan information synchronization mechanism for CenRA to maintain a balanced manner from the perspective\nof the entire system. Specifically, this mechanism is implemented by controlling the quantity of samples\nthat CRA retrieves from each task's replay buffer Di by a sampling weight w. We mainly consider two\naspects: the similarity among tasks and the real-time learning performance of the policy agents.\nSimilarity Weight is derived from the similarity among tasks, enabling the CRA to focus on relatively\noutlier tasks. To simplify computation, we use the hidden layers extracted from each policy agent's neural\nnetwork encoders to represent the tasks' features. To reduce randomness, we average the hidden features of\nthe most recent K steps. We adopt a cross-attention mechanism, which is widely used in neural networks,\nto calculate the similarity weight Vaswani et al. (2017). Specifically, for task Ti, let Hi denote the averaged\nhidden feature vector, which serves as the key, and the centroid of all tasks c acts as the query. Then, the\nsimilarity si of task Ti to the centroid of the task cluster is calculated as:\n$s_i = \\frac{c^T H_i}{\\sqrt{D}}$, $c = \\frac{1}{N} \\sum^N_{k=1} H_k$,\nwhere D is the dimension of the hidden feature to prevent gradient vanishing or exploding. A larger si indi-\ncates a greater similarity between Ti and the centroid. Given our assumption is that the tasks farther from the\ncentroid require more attention, the similarity weight is defined as $w^{sim}$ = Softmax([1/s1,1/s2,...,1/sN]).\nPerformance Weight is determined by the real-time learning performance of each policy agent, to ensure\nthe CRA focuses more on lagging tasks. Similar to the similarity weight, we average the environmental"}, {"title": "4.3 Overall Framework", "content": "The overall framework of CenRA is summarized in Algorithm 1. The CRA and policy agents are updated\nalternately and asynchronously, with the frequency of updating the CRA adjustable according to the actual\nsituation. Sampling weights are calculated in real-time, using the most recently optimized encoders and the\ncurrent learning performance, ensuring CRA continuously adjusts its focus to optimally balance knowledge\nextraction across multiple tasks.\nThe learned CRA acts as a robust knowledge pool, which is able to support new tasks as an additional\nmodule by transferring knowledge through auxiliary reward signals. This is particularly beneficial in\nsparse-reward environments, as the knowledge rewards can guide the policy agents toward the correct\ndirection and reduce the exploration burden. Additionally, the CRA can be further optimized alongside\nnew tasks in a continuous learning scheme that enhances adaptability and effectiveness in dynamic settings."}, {"title": "5 Experiments", "content": "We conduct experiments in three environments with multiple tasks: 2DMaze and 3DPickup by Chevalier-\nBoisvert et al. (2024), and MujocoCar by Ji et al. (2023), which include both discrete and continuous\ncontrol problems. All tasks provide challenging sparse environmental rewards, where the agent only receives\na reward of 1 upon successful completion of the final objective, and a reward of 0 is given in all other states.\nDetailed configurations are shown in Figure 3, with each task's setting provided in Appendix A."}, {"title": "5.1 Comparative Evaluation in MTRL", "content": "In this section, we evaluate the learning performance of CenRA in the MTRL context, configuring each\nenvironment with four different tasks. We benchmark CenRA against several state-of-the-art baselines,\nincluding: (a) the backbone algorithms of the policy agents in CenRA: the Deep Q-Network (DQN) Mnih et al.\n(2015) for the discrete control tasks in 2DMaze and 3DPickup, and the Soft Actor-Critic (SAC) Haarnoja\net al. (2018b) for the continuous control tasks in MujocoCar; (b) The RL with an Assistant Reward Agent\n(ReLara) algorithm Ma et al. (2024b), which learns an additional reward agent to generate informative\nrewards. ReLara can be considered as a special case of CenRA where each policy agent is paired with a\nseparate reward agent, without cross-task information sharing; (c) The Policy Optimization and Policy\nCorrection (PiCor) algorithm Bai et al. (2023), which uses policy correction to adapt and transfer information\nacross policies among individual tasks; (d) The Multi-Critic Actor Learning (MCAL) algorithm Mysore et al.\n(2022), which maintains separate critics for each task for value estimation and knowledge representation,\nwhile training a unified multi-task actor. We use the CleanRL library Huang et al. (2022) or the author-\nprovided codes to implement these baselines.\nWe train multiple instances with different random seeds for each task to report the average data. The results\nof the episodic returns and their standard errors throughout training are shown in Figure 4 and Table 1.\nTo ensure a fair comparison, we use the same hyperparameters (if applicable) and network structures across\nall experiments, the details are provided in Appendix B.\nWe observe that CenRA consistently outperforms the baselines in three main aspects. First, CenRA\nachieves the highest episodic returns in nearly all tasks, demonstrating superior learning efficiency and\nfaster convergence. Importantly, it also maintains good stability and robustness, which can be seen in the\nsignificantly fewer fluctuations and oscillations especially after convergence, compared to other models.\nSecond, unlike PiCor and MCAL, which have shown uneven progress across different tasks in the same\nenvironment, CenRA maintains a well-balanced performance by showing relatively consistent learning\nprogress and minimal variability in learning outcomes across each four-task group. This ensures that no\nsingle task's performance is disproportionately high at the expense of others, which is crucial in multi-task\nlearning. Third, the centralized reward agent in CenRA effectively enhances knowledge sharing among\ntasks. This is evident from the comparison with ReLara, which uses independent reward agents and lacks\nthe mechanism for knowledge exchange. By extracting and distributing insights from one task to another,"}, {"title": "5.2 What Has the Centralized Reward Agent Learned?", "content": "In this section, we visualize the learned knowledge rewards by the centralized reward agent $A^{rwd}$ in the\n2DMaze environment. After training on the four tasks in Section 5.1 of the paper, we let the CRA generate\nthe knowledge rewards for each action in every state and visualize the action direction that yields the\nmaximum rewards, $a^* = arg max_a r^{rwd}*(s_i, a), S_i ~ S$, in Figure 5.\nThe shaded areas in the figures represent regions within the real task that the agent cannot reach, as it\ncannot access the space behind the door without picking up the key. However, we forced the agent into\nthese areas for evaluation. Outside the shaded regions, we observe that the CRA successfully learned\nmeaningful knowledge rewards. Before picking up the key, the agent received the highest reward in the\ncorresponding state when moving towards the key. Similarly, after picking up the key, the agent received\nthe highest reward when moving towards the door and the final target. This demonstrates that in scenarios\nwhere the original environmental rewards are sparse, these detailed knowledge rewards can effectively guide\nthe agent to converge more quickly."}, {"title": "5.3 Knowledge Transfer to New Tasks", "content": "In this section, we assess the centralized reward agent's ability to transfer learned knowledge to new tasks.\nWe continue using the CRA model trained on the four tasks in Section 5.1 but initiate a new policy agent\nto tackle a newly designed task that the models have never encountered before. For the CenRA framework,\nwe explore two scenarios: one where the CRA continues to optimize while cooperating with the new policy\nagent (CenRA w/ learning), and another where only the policy agent is trained without further updates to\nthe CRA (CenRA w/o learning). We compare the two approaches with the backbone algorithms (DQN\nand SAC) and ReLara. In the ReLara model, the reward agent starts training anew (without pre-learned\nknowledge). The results are shown in Figure 6 and Table 2.\nWe observe that CenRA with further learning achieved rapid convergence, mainly due to the CRA's\nretention of previously acquired knowledge and its ability to quickly adapt to new tasks through continuous\noptimization. Notably, even without any additional training, CenRA outperforms both the ReLara, which\nrequires training a new reward agent, and the backbone algorithms, which lack additional information. In\nthe CenRA framework, the CRA stores environment-relevant knowledge and transfers it to other policy\nagents, thus improving their learning efficiency. Additionally, sharing pre-learned information with policy\nagents is particularly crucial in our experiments with challenging sparse rewards. Without any external\nknowledge, learning would require extensive exploration. However, the CRA provides meaningful dense\nrewards that significantly accelerate the learning process, even during the initial phases."}, {"title": "5.4 Effect of Sampling Weight", "content": "We conducted experiments to understand the effects of the information synchronization mechanism in the\nCenRA framework. Specifically, we compare the full CenRA model against three variants: (a) CenRA\nwithout the similarity weight $w^{sim}$, (b) CenRA without the performance weight $w^{per}$, and (c) CenRA\nwithout the entire sampling weight. The comparison results are shown in Figure 8 and Table 3.\nThe results indicate that the two weights, which control the allocation of samples drawn from each policy\nagent's experiences, mainly influence the overall learning performance. Specifically, the absence of sampling\nweight leads to unbalanced learning outcomes, which is observed by the increased variance in episodic\nreturns across four tasks. While the full CenRA model does not always achieve the lowest variance, it\nconsistently outperforms the other three ablation models regarding overall system performance.\nBoth weights are essential for the information synchronization mechanism, with the performance weight\n$w^{per}$ having a more important effect. This weight enables CRA to pay more attention to the policy\nagents that are underperforming or progressing slowly. This suggests that considering the overall learning\nperformance of the multi-task system is also an important objective CenRA seeks to achieve."}, {"title": "6 Discussion and Conclusion", "content": "We propose a novel framework named CenRA that innovatively integrates reward shaping into MTRL. The\nframework not only can share environment-relevant knowledge across tasks to improve learning efficiency,\nbut also effectively addresses the sparse-reward challenge. Specifically, a CRA functions as a knowledge pool,\nresponsible for knowledge distillation and distribution across multiple tasks. Furthermore, our information\nsynchronization mechanism mitigates knowledge distribution disparities across tasks, ensuring optimal\nsystem-wide performance. Experiments demonstrate that the dense knowledge rewards generated by the\nCRA efficiently guide the policy agents' learning, leading to faster convergence compared to baseline\nmethods. The CenRA framework proves to be robust and performs superiorly in both multi-task learning\nand new tasks adaptation.\nCenRA's current limitation is its requirement for uniform state and action spaces across tasks. Future\nimprovements could explore preprocessing techniques to adapt the framework for varying state and action\nspaces to broaden CenRA's applicability. Additionally, the fixed trade-off between similarity weight and\nperformance weight may not be ideal. A more flexible approach, potentially through adaptive regulation\nof these weights, could enhance the framework further. Besides, the performance weight might favor\nunderperforming tasks, achieving overall balance but potentially limiting the upper bound of top-performing\ntasks. This issue could necessitate a better mechanism for trade-off."}]}