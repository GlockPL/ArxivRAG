[{"title": "KaLM: Knowledge-aligned Autoregressive Language Modeling via Dual-view Knowledge Graph Contrastive Learning", "authors": ["Peng Yu", "Cheng Deng", "Beiya Dai", "Xinbing Wang", "Ying Wen"], "abstract": "Autoregressive large language models (LLMs) pre-trained by next token prediction are inherently proficient in generative tasks. However, their performance on knowledge-driven tasks such as factual knowledge querying remains unsatisfactory. Knowledge graphs (KGs), as high-quality structured knowledge bases, can provide reliable knowledge for LLMs, potentially compensating for their knowledge deficiencies. Aligning LLMs with explicit, structured knowledge from KGs has been a challenge; previous attempts either failed to effectively align knowledge representations or compromised the generative capabilities of LLMs, leading to less-than-optimal outcomes. This paper proposes KaLM, a Knowledge-aligned Language Modeling approach, which fine-tunes autoregressive LLMs to align with KG knowledge via the joint objective of explicit knowledge alignment and implicit knowledge alignment. The explicit knowledge alignment objective aims to directly optimize the knowledge representation of LLMs through dual-view knowledge graph contrastive learning. The implicit knowledge alignment objective focuses on incorporating textual patterns of knowledge into LLMs through triple completion language modeling. Notably, our method achieves a significant performance boost in evaluations of knowledge-driven tasks, specifically embedding-based knowledge graph completion and generation-based knowledge graph question answering.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) like PaLM 2 (Anil et al., 2023) and GPT-4 (Achiam et al., 2023) have recently made remarkable advancements in a wide range of natural language processing tasks (Li et al., 2022; Su et al., 2019). However, LLMs still face challenges in tasks requiring factual or domain-specific knowledge, resulting in unsatisfactory performance in knowledge-driven tasks. From the perspective of knowledge representation, LLMs serve as parametric knowledge bases, providing implicit, non-deterministic knowledge, while knowledge graphs (KGs) function as structured knowledge bases, offering explicit, deterministic knowledge. KGs, commonly organized as factual knowledge triples describing relations between entities, can serve as a reliable knowledge source for LLMs. Aligning LLMs with KG knowledge can enhance the knowledge reasoning capabilities of LLMs and improve their performance on knowledge-driven tasks, such as knowledge graph completion (KGC) and knowledge graph question answering (KGQA).\nAutoregressive LLMs pre-trained through next token prediction tasks often exhibit limitations in knowledge representation, leading to embeddings that lack diversity and specificity. This limitation becomes evident in tasks that demand distinctive sentence embeddings, such as dense retrieval and semantic search (Muennighoff, 2022; Ma et al., 2023). As demonstrated in Figure 1(a), the representations generated by LLMs tend to be overly homogeneous across different pieces of knowledge, undermining their effectiveness in applications requiring fine-grained semantic distinctions.\nThe concept of explicit knowledge alignment is introduced to directly optimize the knowledge representation within language models by devising direct knowledge training objectives. This strategy emerges in response to the observed degradation in knowledge representation within autoencoder-based pre-trained language models (PLMs), a phenomenon termed representation anisotropy (Ethayarajh, 2019). This issue is characterized by the clustering of learned token and sentence embeddings within a constrained area of the representation space, leading to a lack of distributional uniformity (Li et al., 2020). While previous efforts to address representation anisotropy have largely concentrated on promoting uniformity among token representations, they often overlook the critical"}, {"title": "2 Related Work", "content": "Our work is closely related to Knowledge Enhancement for LLMs and Representation Anisotropy of Language Models. A more detailed review of related work can be found in Appendix A.\nKnowledge Enhancement for LLMs Knowledge enhancement aims to incorporate factual and domain-specific knowledge into LLMs to address their knowledge deficiencies. This can be divided into retrieval-based augmentation and training-based integration. Retrieval-based knowledge augmentation methods leverage external retrieval modules to provide additional knowledge, aiming to improve the knowledge reasoning capability of LLMs (Sun et al., 2023; Jiang et al., 2023). However, this approach may lead to knowledge conflicts (Feng et al., 2023), where knowledge in LLMs and knowledge in the retrieved documents are inconsistent or the retrieved multiple documents are contradictory. Training-based knowledge integration methods involve using KG triple descriptions to pre-train or fine-tune LLMs, aiming to achieve knowledge alignment. These methods can be divided into explicit alignment (Wang et al., 2021b; Yasunaga et al., 2022) and implicit alignment (Yao et al., 2023; Zhang et al., 2023) based on whether they directly optimize the knowledge representation. Nevertheless, prior methods have either sacrificed the generative capability or lacked effective representation alignment. Our approach enhances the knowledge of LLMs via a unique joint objective of explicit alignment and implicit alignment, improving the quality of knowledge representations and generative knowledge reasoning capabilities.\nRepresentation Anisotropy of Language Models PLMs have long been plagued by representation anisotropy (Ethayarajh, 2019), where the learned token and sentence embeddings are confined to a narrow cone within the entire representation space. The issue of representation anisotropy not only results in model degradation (Su et al., 2022) but also leads to poor performance on discriminative tasks. Previous work on alleviating representation anisotropy has mainly focused on post-processing techniques such as normalizing flows (Li et al., 2020) or whitening operations (Su et al., 2021). Su et al. (2022) propose a contrastive training objective to encourage learning isotropic token representations. However, these methods mainly improve the isotropy of token representations without enhancing the discriminability of sentence representations. Our method improves the token-level and sentence-level representation anisotropy of LLMs through dual-view knowledge graph contrastive learning, and it has rigorous theoretical guarantees."}, {"title": "3 Knowledge-aligned Autoregressive Language Modeling", "content": "In this section, we introduce KaLM, a Knowledge-aligned Language Modeling approach for aligning LLMs with KG knowledge via the joint objective of explicit knowledge alignment and implicit knowledge alignment. The overview is shown in Figure 2.\nA KG G stores factual knowledge, denoted as G = (E,R,T,D). E and R are the set of entities and relations, respectively. D is the description set of all entities and relations. De and Dr are the textual description of entity e and relation r, respectively. T = {(h, r,t)|h, t \u2208 E,r \u2208 R} is the triple set. A triple (h, r, t) depicts the fact that there is a relation r between the head entity h and the tail entity t.\nFor KG triples, the textual description of the tail entity and the concatenation of the textual descriptions of the head entity and relation can be seen as two distinct views of the same knowledge. This inspires KaLM to align representations of two distinct views of the same knowledge (i.e., from the same triple), while separating representations of different knowledge (i.e., from different triples).\nThe LLM, denoted as ELLM, is fine-tuned with the dual-view knowledge graph contrastive learning loss. The training corpus contains paired textual descriptions, {(Dhr, Dt)}1, where Dt is the tail entity description, and Dhr is the concatenation of the head entity description and relation description. Given a training pair (Dhr, Dt), the same ELLM is used to compute the embeddings of Dhr and Dt independently. Moreover, we prepend the [bos] token to the beginning and append the [eos] token to the end of the textual description. The augmented input is fed into ELLM, and the hidden representation corresponding to the [eos] token from the last layer is used as the final embedding of the input.\nehr = ELLM([bos]hr\u2295 Dhr\u2295 [eos]hr),\net = ELLM([bos]t\u2295 Dt \u2295 [eos]t),\nwhere \u2295 is the operation to concatenate two strings and Dhr = Dh\u2295 Dr. For stable training, we adopt \"[\" as [bos]hr and", "as [eos]hr, while using": "as [bos]t and"}, "as [eos]t.\nWe utilize the knowledge graph contrastive learning loss to directly optimize the knowledge representation of the LLM by encouraging semantically similar knowledge to stay close in the representation space and pushing dissimilar knowledge to be far apart in the representation space. More specifically, we apply the InfoNCE loss with an additive margin over the in-batch negatives to fine-tune the model. The row-direction loss lr is as follows for a given positive pair, and the column-direction loss lc is defined similarly (see Appendix C.2).\nlr = -log\\frac{e^{(\\phi(e_{hr},e_t)-\\gamma)/\\tau}}{e^{(\\phi(e_{hr},e_t)-\\gamma)/\\tau} + \\sum_{i=1}^{N}e^{(\\phi(e_{hr},e_i)/\\tau}}}\n(1)\nwhere N is the negative batch size, \u03c4 is the trainable temperature that controls the strength of penalties on hard negative samples, \u03d5 is the cosine similarity function that measures the plausibility of a triple, and \u03b3 is the additive margin that encourages increasing the similarity score of positive pairs.\nThe training objective for explicit knowledge alignment is the sum of the lr and the lc losses:\nLexp = \\frac{1}{\\left \\{(D_{hr}, D_t)\\right \\}}\\sum_{\\left \\{(D_{hr}, D_t)\\right \\}}(l_r + l_c)/2.\n(2)"]}, {"title": "3.3 Implicit Knowledge Alignment", "content": "The implicit knowledge alignment objective focuses on incorporating textual patterns of knowledge into the LLM to prevent catastrophic forgetting of previous knowledge and maintain its generative capability. We constructed an instruction-tuning dataset based on the KG triple descriptions to fine-tune the model through triple completion language modeling. We also show that the implicit knowledge alignment objective can bring performance boosts on knowledge representation evaluations. This indicates that explicit alignment and implicit alignment are both imperative for effective knowledge alignment, as they both essentially necessitate a profound understanding of knowledge.\nWe follow the recipe of Stanford Alpaca (Taori et al., 2023) and use the provided template to construct the instruction-tuning dataset. The instruction passed to the template, abbreviated as inst, is: \u201cGiven the head entity and relation, write a tail entity that completes the triple\". The input and output are Dhr and Dt, respectively. The training objective for implicit knowledge alignment is:\nLimp = \\frac{1}{\\mathcal{M}}\\sum_{(D_{hr}, D_t)} log P(D_t| inst, D_{hr}),\n(3)\nwhere M is the instruction-tuning batch size."}, {"title": "3.4 Knowledge-aligned Language Modeling", "content": "The ultimate training objective of our proposed KaLM is the weighted average of Lexp and Limp:\nL_{KALM} = L_{exp} + \\lambda \\cdot L_{imp},\n(4)\nwhere \u03bb is a hyperparameter that adjusts the relative weight between them. Notably, this formulation allows us to use different batch sizes for explicit knowledge alignment (N) and implicit knowledge alignment (M). Previous work has shown that a sufficiently large batch size is key to the success of contrastive representation learning (Chen et al., 2020). With Equation 4, we can significantly increase the explicit knowledge alignment batch size while keeping the implicit knowledge alignment batch size fixed to save computational resources."}, {"title": "4 Theoretical Analysis", "content": "We theoretically prove that the explicit knowledge alignment objective implemented through dual-view knowledge graph contrastive learning can facilitate knowledge representation alignment and alleviate the issue of representation anisotropy."}, {"title": "4.1 Dual-view Contrastive Learning for Knowledge Representation Alignment", "content": "The outstanding performance of contrastive representation learning has attracted researchers to analyze its underlying reasons for success from a theoretical perspective. Wang and Isola (2020) identify alignment and uniformity as two key properties of contrastive learning and propose two quantifiable metrics to measure the quality of representations.\nWe concentrate on understanding the dual-view knowledge graph contrastive learning loss from the knowledge alignment and uniformity perspective. To simplify the notation, we use f to denote ELLM. Alignment computes the expected distance between positive pairs and encourages the learned representations for positive pairs to be similar. Uniformity evaluates the even distribution of representations and encourages the separation of features from randomly selected negative samples.\nl_{align}(f; \\alpha) \\triangleq \\mathbb{E}_{(D_{hr}, D_t) \\sim P_{pos}}[\\lVert f(D_{hr}) - f(D_t)\\rVert^2],\nl_{uniform}(f; t) \\triangleq - log \\mathbb{E}_{D_i, D_j \\stackrel{i.i.d.}{\\sim} P_{data}}[e^{-t\\lVert f(D_i) - f(D_j)\\rVert^2}],\nwhere Ppos denotes the distribution of positive pairs {(Dhr, Dt)}1 and Pdata represents the data distribution of textual descriptions {Di}1.\nSince the learned knowledge representations are L2-normalized, we have \u03d5(ehr, et) = f(x)f(y). The additive margin \u03b3 encourages the model to learn more robust features without affecting the asymptotic analysis, thus we ignore it. For ease of analysis, we reformulate the contrastive learning"}, {"title": "4.2 Alleviation of Representation Anisotropy", "content": "We then prove that the dual-view knowledge graph contrastive learning objective can directly alleviate representation anisotropy and improve the discriminability of knowledge representations.\nLet E be the sentence embedding matrix of {Di}1, where the i-th row of E is ei. Following Ethayarajh (2019), the sentence-level representation anisotropy value of {D}1 is defined as:\nanisotropy\\{D\\} = \\frac{1}{N(N-1)}\\sum_{i=1}^{N}\\sum_{j=1, j \\ne i}^{N}e_i^T e_j.\n(7)\nWe can further derive the following theorem."}, {"title": "5 Experiments", "content": "In this section, we assess the effectiveness of KaLM in knowledge alignment. The experimental setup is outlined in 5.1. In 5.2 and 5.3, we present results on knowledge graph completion (KGC) and knowledge graph question answering (KGQA). In 5.4, we provide further analysis of knowledge representation and present case studies of KGQA generations."}, {"title": "5.1 Experimental Setup", "content": "Datasets. We use WN18RR (Dettmers et al., 2018) and FB15k-237 (Toutanova and Chen, 2015) as the KGs for knowledge alignment training. WN18RR and FB15k-237 are derived from WordNet and Freebase, respectively (Bordes et al., 2013). We use the information provided by KG-BERT (Yao et al., 2019) for textual descriptions. Following Wang et al. (2022a), we add an inverse triple (t, r\u00af\u00b9, h) for each triple (h, r, t) in the triple set, where r\u00af1 is the inverse relation of the original relation r.\nModel Training. We choose Llama-2-7B, Llama-3-8B, and Mistral-7B as base LLMs and fine-tune them through the joint objective of explicit knowledge alignment and implicit knowledge alignment. To save computational resources for parameter-efficient fine-tuning, we use LoRA (Hu et al., 2021) to fine-tune the feed-forward network of the model.\nEvaluation Details. Experiments mainly focus on two aspects: knowledge representation assessment and knowledge inference evaluation. For knowledge representation assessment, we evaluate the embedding-based KGC task and illustrate the alleviation of representation anisotropy. We report five automated metrics: Mean Rank (MR), Mean Reciprocal Rank (MRR), and Hit@k (k \u2208 {1,3,10})."}, {"title": "5.2 Knowledge Representation Assessment", "content": "The embedding-based KGC results are shown in Table 1. The base LLM failed to finish this task, with all metrics lagging far behind. On the WN18RR dataset, our method surpasses prior methods by a substantial margin in terms of MR and Hit@10."}, {"title": "5.3 Knowledge Inference Evaluation", "content": "The generation-based KGQA results are depicted in Figure 3. Llama-2-7B performs poorly in entity prediction and relation prediction. Our method demonstrates a significant performance boost in all generation-based KGQA tasks, including head/tail entity prediction, relation prediction, and triple classification. Furthermore, despite a slight increase in perplexity (PPL) scores on Wikitext-103 (Merity et al., 2016) test set, our method still shows competitive performance in the MMLU test. The results demonstrate that KaLM achieves effective knowledge alignment, bringing in significantly improved KGQA performance while preserving the original generative and knowledge inference capabilities."}, {"title": "5.4 Visualization of Knowledge Representation and Case Studies", "content": "We provide visualization results to illustrate knowledge representation improvements. Figure 4 shows the sentence similarity matrix of Llama-2-7B and KaLM on Wikitext-103. The diagonal elements denote the similarity of the same sentence, so the values are always 1. From color intensity, it is evident that KaLM learns more discriminative sentence representations, while Llama-2-7B assigns high similarity for arbitrary sentences. The sentences are organized by celebrities and their careers, thus there should also be a high similarity between adjacent sentences. This phenomenon is reflected in the similarity matrix of KaLM in Figure 4(b), manifested in the smaller matrices with darker colors along the diagonal. More concretely, numerical analysis shows that after training with our method, the sentence-level anisotropy value significantly decreased from 0.83 to 0.21."}, {"title": "6 Conclusion", "content": "In this work, we show that the subpar performance of LLMs on knowledge-driven tasks stems from a lack of effective knowledge alignment. We present KaLM, a novel knowledge-aligned language modeling approach for aligning autoregressive LLMs with KG knowledge. Specifically, we identify two imperative objectives to achieve knowledge alignment: explicit knowledge alignment and implicit knowledge alignment. We conducted comprehensive experiments and analyses on embedding-based KGC and generation-based KGQA. Experimental results demonstrate that our method achieves effective knowledge alignment and consistently improves performance on knowledge-driven tasks."}, {"title": "Limitations", "content": "There are several future directions to improve this work. Firstly, due to the limitation of computational resources, we used the limited-scale LLMs to train and evaluate our method. Evaluations on larger-scale LLMs, such as the 13B and 70B models, can further validate the effectiveness of our approach. Secondly, we use a simple linear combination of explicit alignment loss and implicit alignment loss as the final training objective for KaLM. Further investigations into various forms of loss combinations remain to be explored to maximize the utility of knowledge-aligned language modeling. Finally, we can delve into the performance of the knowledge representations obtained from knowledge-aligned language modeling in cross-domain applications such as retrieval-augmented generation, to gain broader insights into the generalization capabilities of the proposed approach."}, {"title": "A More Detailed Review of Related Work", "content": "This work focuses on fine-tuning autoregressive LLMs to align with KG knowledge. Our work intersects with the following research areas: Knowledge Enhancement for LLMs, Knowledge Graph Completion, Contrastive Representation Learning, and Representation Anisotropy of Language Models."}, {"title": "A.1 Knowledge Enhancement for LLMs", "content": "Knowledge enhancement aims to incorporate factual and domain-specific knowledge into LLMs to address their knowledge deficiencies. This can be divided into retrieval-based knowledge augmentation and training-based knowledge integration. Retrieval-based knowledge augmentation methods leverage external retrieval modules to provide additional knowledge, aiming to improve the knowledge reasoning capability of LLMs (Sun et al., 2023; Jiang et al., 2023). However, this approach may lead to knowledge conflicts (Feng et al., 2023), where the knowledge in LLMs and the knowledge in the retrieved documents are inconsistent or the retrieved multiple documents are contradictory. Training-based knowledge integration methods involve using the textual descriptions of KG triples to pre-train or fine-tune LLMs, aiming to achieve knowledge alignment. These methods can be categorized into explicit alignment (Wang et al., 2021b; Yasunaga et al., 2022) and implicit alignment (Yao et al., 2023; Zhang et al., 2023) based on whether they directly optimize the knowledge representation. Nevertheless, these methods have either sacrificed the generative capability or lacked effective representation alignment. Our approach enhances the knowledge of LLMs via a unique joint objective of explicit alignment and implicit alignment, improving the quality of knowledge representations and generative knowledge reasoning capabilities."}, {"title": "A.2 Knowledge Graph Completion", "content": "Knowledge graph completion (KGC) refers to inferring missing triples from an incomplete KG, which can be used to evaluate the knowledge reasoning ability and knowledge representation quality of LLMs. Existing KGC methods can be categorized into structure-based and description-based. Structure-based methods represent entities and relations as fixed-dimensional vector embeddings and use scoring functions to assess the plausibility of triples (Bordes et al., 2013; Sun et al., 2019). Description-based methods further incorporate the"}, {"title": "A.3 Contrastive Representation Learning", "content": "Contrastive learning has demonstrated remarkable success in learning representations across various domains (Chen et al., 2020; Liu et al., 2021; Gunel et al., 2020). The goal is to learn representations that capture shared information between positive pairs while remaining invariant to perturbing noise. The commonly used contrastive learning objectives share a standardized design involving a softmax function over cosine similarity of paired features, with a temperature parameter to control the penalty strength on hard negative samples. Wang and Isola (2020) propose understanding contrastive learning through the lens of alignment and uniformity on the hypersphere. Wang and Liu (2021) show that temperature in the contrastive loss controls the strength of penalties over negative samples."}, {"title": "A.4 Representation Anisotropy of Language Models", "content": "PLMs have long been plagued by representation anisotropy (Ethayarajh, 2019), where the learned token and sentence representations are confined to a narrow cone within the entire representation space. The issue of representation anisotropy not only results in model degradation (Su et al., 2022) but also leads to poor performance on discriminative tasks (Muennighoff, 2022). Previous work on alleviating representation anisotropy has mainly focused on post-processing techniques such as normalizing flows (Li et al., 2020) or whitening operations (Su et al., 2021) to obtain isotropic representations. Su et al. (2022) propose a contrastive training objective to encourage learning isotropic token representations. However, these methods mainly improve the isotropy of token representations without enhancing the discriminability of sentence representations. Our method improves the token-level and sentence-level representation anisotropy of LLMs through dual-view knowledge graph contrastive learning, and it has rigorous theoretical guarantees."}, {"title": "C Further Details about Implementation and Experimental Setup", "content": ""}, {"title": "C.1 Dataset Details", "content": "WN18RR and FB15k-237 are commonly used KGs derived from WordNet and Freebase, respectively (Bordes et al., 2013). They have been carefully constructed to prevent test set leakage by removing inverse relations. We use these datasets for training and evaluation. The statistics are shown in Table 2."}, {"title": "C.2 KaLM Implementation Details", "content": "We initially choose Llama-2-7B as the base LLM and fine-tune it through the training objective in Equation 4. We use varying batch sizes for explicit knowledge alignment and implicit knowledge alignment. For WN18RR, we use a batch size of 24 for explicit alignment and 4 for implicit alignment. For FB15k-237, the batch sizes are 40 for explicit alignment and 6 for implicit alignment. To save computing resources for parameter-efficient fine-tuning, we use the LoRA (Hu et al., 2021) method to fine-tune the [\u201cgate_proj\u201d, \u201cup_proj\", \u201cdown_proj\u201d] modules in the feed-forward network of the Llama-2-7B model. We conducted all training on an NVIDIA 4090\u00d78 GPU. The hyperparameters utilized for training KaLM (based on Llama-2-7B) are enumerated in Table 3.\""}, {"title": "D.1 More Experiments on Knowledge Representation Assessment", "content": "In Table 5, we present additional knowledge representation results (the embedding-based KGC task) to demonstrate the effectiveness of KaLM in knowledge alignment. The best and second-best experimental results are indicated by bold and underline texts, respectively. Overall, the proposed method achieved excellent performance on the embedding-based KGC task, delivering impressive results in the MR and Hit@10 metrics, while also being highly competitive in other metrics.\nThe experimental results based on LLMs of different sources and scales demonstrate the effectiveness and generalizability of our proposed method. Under similar experimental settings, more powerful LLMs (such as Llama3-8B and Mistral-7B) achieved better metrics after being fine-tuned with KaLM, which also demonstrates the scalability of our method. It is worth noting that for LLMs of the same origin but different scales (Pythia-6.9B and Pythia-2.8B), the smaller-scale Pythia-2.8B benefited from a larger training batch size during fine-tuning. As a result, its final experimental metrics matched or even surpassed those of the more powerful Pythia-6.9B model. This also highlights the importance of large batch sizes for the embedding-based KGC task, suggesting that using more powerful computing resources and larger GPU memory could further enhance the effectiveness of the proposed KaLM method."}, {"title": "D.2 More Experiments on Knowledge Inference Evaluation", "content": "In Figure 6, we present additional knowledge inference results (generation-based KGQA) to demonstrate the effectiveness of KaLM in knowledge alignment. This section demonstrates the performance of various powerful LLMs (including Llama-2-7B, Llama-3-8B, and Mistral-7B) before and after fine-tuning with KaLM, across various knowledge graph question-answering tasks (including head entity prediction, tail entity prediction, relation prediction, and triple classification)."}, {"title": "D.3 More Visualizations on Knowledge Representation Matrix", "content": "From this section onward, unless stated otherwise, KaLM refers to the model checkpoint trained on Llama-2-7B using our method. We present more knowledge representation results to demonstrate the effectiveness of KaLM in knowledge alignment. Figure 7 displays the sentence similarity matrix of several similar entity descriptions from the WN8RR dataset. Detailed information about entity names and descriptions can be found in Figure 8. It is evident that KaLM can obtain more distinguishable knowledge representations, where the similarity between related entities (diagonal elements) is high, while the similarity between unrelated entities (off-diagonal elements) is low."}, {"title": "D.4 Detailed analysis of Representation Anisotropy", "content": "We further analyze the sentence-level representation anisotropy on the Wikitext-103 test set using model checkpoints trained on the WN18RR dataset. The sentence-level anisotropy value for a given corpus {D}1 is defined in Equation 7, where a lower anisotropy value indicates better discriminative characteristics of sentence representations."}, {"title": "E.1 The necessity of the implicit knowledge alignment objective (Equation 3)", "content": "In Table 6, we train the model using different loss weights (i.e., the \u03bb parameter in Equation 4) and analyze its performance on the KGC task. Note that this experiment is conducted solely for ablation analysis, thus only 10 training epochs are used. Experimental results reveal that incorporating the implicit knowledge alignment objective (i.e., \u03bb > 0) generally leads to better performance in KGC, indicating further improvement in knowledge representation. The best performance in KGC is achieved when \u03bb = 0.1. The results confirm that both explicit alignment and implicit alignment are crucial for knowledge alignment, as they both essentially require a deep understanding of knowledge.\nThe implicit knowledge alignment objective focuses on incorporating textual patterns of knowledge into the LLM to prevent catastrophic forgetting of previous knowledge and maintain its generative capability. We also conducted additional perplexity (PPL) evaluation experiments to illustrate"}, {"title": "E.2 The effects of fine-tuning different LLM modules using LoRA", "content": "In Table 7, we fine-tune different modules of the model using the LoRA (Hu et al., 2021) method and analyze their performance on KGC tasks and PPL evaluations. Note that this experiment is conducted solely for ablation analysis, hence only 10 epochs of training were performed. \"att\" indicates fine-tuning only the attention module, \"ffn\" indicates fine-tuning only the feed-forward network, and \"att-ffn\" indicates fine-tuning both the attention module and the feed-forward network simultaneously. The"}, {"title": "E.3 The sustained gains and potential impacts of training for more epochs", "content": "In Table 8, we fine-tune the model using different numbers of training epochs and analyze their performance on KGC tasks. This experiment is mainly conducted to investigate whether additional training epochs can lead to further improvement in knowledge representations. The experimental results show that using more training epochs can continuously improve the performance of KaLM on the KGC task, resulting in higher MRR and Hit@k metrics. The model trained with our method consistently maintains an acceptable PPL value due to the implicit knowledge alignment objective. However, this also comes with more computational resource consumption and training time. As a result, we selected a moderate number of training epochs."}]