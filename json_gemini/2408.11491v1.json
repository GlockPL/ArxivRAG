{"title": "Nothing in Excess: Mitigating the Exaggerated Safety for LLMs via Safety-Conscious Activation Steering", "authors": ["Zouying Cao", "Yifei Yang", "Hai Zhao"], "abstract": "Safety alignment is indispensable for Large language models (LLMs) to defend threats from malicious instructions. However, recent researches reveal safety-aligned LLMs prone to reject benign queries due to the exaggerated safety issue, limiting their helpfulness. In this paper, we propose a Safety-Conscious Activation Steering (SCANS) method to mitigate the exaggerated safety concerns in aligned LLMs. First, SCANS extracts the refusal steering vectors within the activation space and utilizes vocabulary projection to anchor some specific safety-critical layers which influence model refusal behavior. Second, by tracking the hidden state transition, SCANS identifies the steering direction and steers the model behavior accordingly, achieving a balance between exaggerated safety and adequate safety. Experiments show that SCANS achieves new state-of-the-art performance on XSTest and OKTest benchmarks, without impairing their defense capability against harmful queries and maintaining almost unchanged model capability.\nWarning: this paper contains examples of harmful queries.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have exhibited remarkable capabilities in various natural language processing tasks (Touvron et al., 2023; Chiang et al., 2023; Anthropic., 2024). However, due to the abundance of harmful content within pretraining data, LLMs prone to follow malicious instructions and generate unsafe responses (Sun et al., 2024; Deshpande et al., 2023). This risk motivates extensive efforts for research on the safety alignment of LLMs (Korbak et al., 2023; Bianchi et al., 2023). Despite advancements in alignment techniques, recent studies (R\u00f6ttger et al., 2023; Varshney et al., 2023) reveal safety-aligned LLMs strike a poor balance between safety and helpfulness. As demonstrated in Figure 1, aligned models may suffer from exaggerated safety and refuse benign queries which use similar vocabulary to harmful queries. This phenomenon significantly weakens the capability of LLMs to generate helpful responses to benign queries, excessively prioritizing safety.\nExisting methods to mitigate the exaggerated safety issue can be categorized into training-based and training-free approaches. However, due to the scarcity of training data related to exaggerated safety, training-based solutions still exhibit a high refusal rate on queries that are word-level harmful but semantically benign (Bianchi et al., 2023; Zheng et al., 2024). Furthermore, existing training-free methods focus on contrasting the token distribution during the decoding process to balance the utility-safety tradeoff (Xu et al., 2024; Shi et al., 2024). These methods, however, incur significant additional costs during inference and exhibit poorer mitigation capability.\nInspired by current researches that observe the existence of safety information in the representation spaces (Zou et al., 2023a; Zheng et al., 2024), we investigate the safety defense mechanism by analyzing how the hidden states change when exposed to harmful queries. Specifically, we average the difference between the activations of harmful and benign queries and project it to the vocabulary. Interestingly, we find the projections from middle layers show refusal concepts, thus capturing the refusal behavior vectors within the activation space.\nMotivated by this finding, we propose a training-free, representation engineering method named SCANS (Safety-Conscious Activation Steering), which utilizes refusal behavior vectors to steer the model output in safety-critical layers. We also design a similarity-based classification method to adaptively determine the steering direction, achieving a balance between adequate and exaggerated safety.\nThrough experiments with four LLMs, SCANS outperforms both training-free and training-based baselines in mitigating exaggerated safety without compromising adequate safety. Furthermore, SCANS maintains almost unchanged model capability, with minimal increase in perplexity. In summary, our contributions include:\n\u2022 We introduce SCANS, which utilizes the activation steering to control the model refusal behavior, requiring no training and incurring no extra cost to inference time.\n\u2022 We discover the extracted refusal steering vectors from middle layers promote refusal tokens and thus steering the corresponding representation can reduce the false refusal rate.\n\u2022 Our SCANS effectively mitigates the exaggerated safety in aligned LLMs, without undermining the adequate safety and general capability. Specifically, SCANS reduces the average false refusal rate by 24.7% and 26.3% on XSTest and OKTest benchmarks."}, {"title": "2 Related Works", "content": "Large Language Model Safety. The detection and mitigation of harmful content generated by language models is a prominent area of research on LLM safety (Zhao et al., 2024; Zhong et al., 2024). Recent works mainly focus on the model alignment through techniques such as supervised fine-tuning (Bianchi et al., 2023; Zheng et al., 2024) or RLHF (Bai et al., 2022b,a). However, safety-aligned models sometimes refuse to answer benign requests because of the over-defense mechanism (R\u00f6ttger et al., 2023; Shi et al., 2024), which is the focus of our work.\nExaggerated Safety. This phenomenon refers to aligned models exhibit a tendency towards false refusal on safe queries, which is first introduced by R\u00f6ttger et al. (2023). Based on this finding, Sun et al. (2024) evaluates 16 mainstream LLMs and finds the more exaggerated a model's safety, the better it performs in preventing jailbreak attacks. This indicates the trade-off between helpfulness and harmlessness remains a challenging task. Due to the scarcity of training data regarding exaggerated safety, current training-based methods (Bianchi et al., 2023; Zheng et al., 2024) still display a poor performance in carefully designed datasets like XSTest (R\u00f6ttger et al., 2023) and OKTest (Shi et al., 2024). Other training-free works rely on prompt engineering (Bhalani and Ray, 2024) or decoding (Shi et al., 2024) strategies. Prompt engineering-based methods take time and resources to design high-quality prompts and decoding-based methods clearly slow down the model inference speed. Our work falls into the training-free category while is orthogonal to the prompt engineering-based and decoding-based methods.\nRepresentation Engineering. Representation engineering typically refers to manipulating the representations within a model to control its behavior (Zou et al., 2023a; Rimsky et al., 2023). Prior works have demonstrated its effectiveness on truthfulness (Li et al., 2024a; Wang et al., 2024), formality transfer (Liu et al., 2023) and sentiment control (Turner et al., 2023; Konen et al., 2024). In this paper, our work discovers the feasibility of activation steering to mitigate the exaggerated safety issues and the proposed SCANS follows the common Mean Difference approach (Zou et al., 2023a) to extract the representations corresponding to refusal behaviors in LLMs."}, {"title": "3 SCANS", "content": "Motivated by the intuition of representation engineering to steer model behavior, the key idea behind our SCANS is to extract the refusal behavior vectors, and anchor the safety-critical layers for steering. SCANS then evaluates the harmfulness of inputs to guide output distribution against or consistent with the refusal behavior, which achieves a balance between adequate and exaggerated safety."}, {"title": "3.1 Inducing the Refusal Steering Vectors", "content": "To obtain the steering vectors that represent the refusal behaviors, we leverage a set of anchor data \\(Q = \\{Q^-,Q^+\\}\\) that consists of harmful and benign queries to trigger the contrastive model behavior. Intuitively, unsafe queries \\(Q^-\\) can induce the defense mechanism in LLMs while the safe ones \\(Q^+\\) elicit the helpful responses.\nWe then simulate LLM with this two types of inputs and extract the hidden states for each layer \\(l\\) at the last token position. By taking the difference, the refusal steering vectors \\(v^l\\) are extracted as follows:\n\\(v^l = \\frac{1}{|Q^-|} \\sum_{q^-\\in Q^-} a^l(q^-) - \\frac{1}{|Q^+|} \\sum_{q^+\\in Q^+} a^l(q^+)\\)   (1)\nwhere \\(a^l()\\) gives the activations of the last token at layer \\(l\\).\nIntuitively, the result of this difference represents a direction from the model's inclination to answer towards the unwillingness to answer, namely refusal direction. Hence, subtracting this vector from the model representations can help moderate the tendency towards false-refusal responses, counter-acting the exaggerated safety."}, {"title": "3.2 Anchoring the Safety-critical Layers", "content": "Manipulating the representations across all layers could potentially disrupt the model outputs to an excessive degree. Therefore, we aim to anchor the specific layers that predominantly influence the model refusal behavior, which we call safety-critical layers, thereby utilized to steer without affecting general capabilities.\nPrevious work (Geva et al., 2022) applies a vocabulary projection method for interpretability. Inspired by this, our SCANS uses the refusal steering vectors \\(v^l\\) for each layer to interpret in the vocabulary space and straightforwardly anchors the safety-critical layers. Specifically, we employ PCA (Hotelling, 1933) to identify the first principal component for \\(v^l\\) separated by three segments: former layers, middle layers, and latter layers. Based on their dot product with the output embedding matrix (LM head), we get vocabulary projection indicating which layers are safety-related.\nFrom Table 1, we provide two perspectives: 1) since the middle layers are more safety-critical than former and latter layers, the extracted steering vectors can encode the refusal tokens associated with the safety defense mechanism; then, 2) steering vectors from the middle layers promote the likelihood of refusal tokens to be generated, thus the corresponding steering can effectively reduce the false refusal rate.\nTherefore, for capability preservation and exaggerated safety mitigation, we perform activation steering on the middle layers. We further demonstrate the steering effects in different layers in Section 5.1."}, {"title": "3.3 Identifying the Steering Direction", "content": "Upon anchoring the appropriate layers for steering, we need to identify the safety of queries so that the output representation is shifted towards (for harmful queries) or against (for benign queries) the refusal direction. Existing research (Zheng et al., 2024; Li et al., 2024b) demonstrates that the representations of the aligned model can distinguish whether the input query is harmful. Based on this, we design a simple and training-free classification method \\(\\sigma(q)\\) to adaptively determine the steering direction for query \\(q\\).\nDue to the inclination of safety-aligned LLMs to refuse to answer benign queries, we concatenate the query \\(q\\) with positive response \\(r_{pos}\\) like \u2018Sure' to amplify predictable patterns, denoted by \\(q+r_{pos}\\). Next, we extract two hidden states, one \\(a_p\\) from the last token of the query part and the other \\(a_e\\) from the final token of the entire input. As illustrated in Eq. 2, the hidden state transition \\(\\delta_t\\) from \\(a_p\\) to \\(a_e\\) makes safety information more intuitive since it mines the harm direction for unsafe queries and is the opposite for benign queries. Figure 3 shows t-SNE visualization of hidden state transition in different layers, further suggesting its potential to classify the harmfulness of inputs.\n\\(a^l_t(q) = a^l_p(q+r_{pos}) - a^l_e(q+r_{pos})\\)   (2)\nIn the preparation stage, we reuse the harmful set of anchor data \\(Q^-\\) to extract the harm direction for reference, \\(d_{harm}^l\\), which represents the average of hidden state transition for all samples \\(q^-\\in Q^-\\) in layer \\(l\\). Specifically, the formulation for the reference harm direction is defined by:\n\\(d_{harm}^l = \\frac{1}{|Q^-|} \\sum_{q^-\\in Q^-} a^l_t(q^-)\\)  (3)\nThen, given query \\(q\\), we stimulate aligned LLM with \\(q + r_{pos}\\) to extract the corresponding hidden state transition and computes its similarity with the reference \\(d_{harm}^l\\) as follows:\n\\(S_q = \\frac{1}{|L|} \\sum_{l\\in L} cos(a^l_t(q), d_{harm}^l)\\)  (4)\nwhere \\(cos\\) means the cosine similarity metric, \\(L\\) is the set of layers for classification. Following Zou et al. (2023a), the choice of \\(L\\) are among the middle and latter layers (See Figure 3) which is also justified in Section 5.4. Finally, if the similarity score \\(S_q\\) is smaller than threshold \\(T\\), we classify the query as benign input and accordingly steer the internal representation opposite the refusal direction:\n\\(\\sigma(q) = \\begin{cases}  -1 & S_q &  a'(q) = a'(q) + \\sigma(q) \\cdot \\alpha \\cdot v   (6)\nwhere \\(a^l\\) and \\(a'\\) respectively represent the original and shifted activations, \\(\\alpha\\) is a hyperparameter that controls the strength of steering.\nA detailed algorithm for our SCANS is presented in Appendix A."}, {"title": "4 Experiment", "content": "4.1 Experimental Setup\n4.1.1 Datasets\nRefusal Steering Vectors Calculation. We use AdvBench (Zou et al., 2023b) as the harmful queries and TruthfulQA (Lin et al., 2021) as the benign ones to generate the refusal steering vectors. Note that we just randomly sample 64 harmful questions and 64 harmless questions to extract the steering vectors as mentioned in Section 3.1. The remaining data is utilized for safety evaluation.\nExaggerated Safety Evaluation. We select XSTest (R\u00f6ttger et al., 2023) and OKTest (Shi et al., 2024) which are two prominent benchmarks focusing on the exaggerated safety phenomenon in LLMs. XSTest comprises 200 unsafe and 250 safe queries that well-calibrated models should not refuse. OKTest carefully designs 300 safe questions with harmful words to identify the over-refusal. As mentioned earlier, we also include the remaining data from TruthfulQA as the test set for helpfulness.\nSafety Guard Evaluation. Aside from mitigating the exaggerated safety, the security of LLMs should also be guaranteed. We use the following datasets to evaluate the security: (a) RepE-Heldout\u00b9 is a popular benchmark containing both harmful and harmless instructions. (b) The remaining Ad-"}, {"title": "4.1.2 Baselines", "content": "We compare SCANS with the following training-free baselines: (1) Prompt (Bhalani and Ray, 2024) is a prompting approach to identify and mitigate such exaggerated safety behaviors in LLMs. (2) Self-CD (Shi et al., 2024) applies contrastive decoding on the output probabilities to reduce the refusal rate on safe queries. We also evaluate SCANS against two training-required methods: (1) SafeDecoding (Xu et al., 2024) is a safety-aware decoding strategy based on the token probabilities of both the original and expert models. (2) DRO (Zheng et al., 2024) optimizes continuous safety prompts to improve safeguarding performance."}, {"title": "4.1.3 Metrics", "content": "For safety and exaggerated safety, we use the Refusal Rate, the ratio of queries rejected by LLMs. We define the refusal behavior as the model outputs any of the predefined refusal messages following (Zheng et al., 2024). Considering the potential inaccuracies using string match, we also conduct human evaluations of the generated content and report the comparison results in Appendix C.\nFor generation tasks involving summarization, we use ROUGE-1/2/L as the accuracy measure, the higher the better. For multiple-choice QA, we assess the accuracy in four categories along with the final average score."}, {"title": "4.1.4 Implementation Details", "content": "Our experiments are primarily based on Llama2-7B-chat, Llama2-13B-chat, vicuna-7B-v1.5 and vicuna-13B-v1.5 (see Appendix D.3 for larger models). All experimental results are averaged across 5 trials conducted on 1x80 GB A100 GPU. More hyperparameter settings and implementation details are described in Appendix B."}, {"title": "4.2 Main Results", "content": "SCANS effectively achieves a balance between adequate and exaggerated safety. Table 2 reports the safety-related results of our SCANS compared with all baselines. As can be seen, aligned models like Llama2 Family models indeed improve the safety, while they also bring about a high refusal rate on word-level harmful but semantically benign queries. Similarly, training-required methods DRO and SafeDecoding do not necessarily address exaggerated safety concerns. With our method, the average false refusal rate across all models has been proven to significantly decrease, outperforming all the baselines (in Appendix D.1). Specifically, SCANS decreases 24.7% and 26.3% of false refusal on safe queries from XSTest and OKTest on average."}, {"title": "4.3 Analysis of \\$\\sigma(q)\\$", "content": "We further explore the classification accuracy of \\$\\sigma(q)\\$ which highly correlates with the performance of SCANS. We compare precision, recall, and F1 score with the following baselines: OpenAI's Moderation API (Markov et al., 2023), Perspective API (Jigsaw., 2017), Llama Guard (Inan et al., 2023), GradSafe (Xie et al., 2024) and GPT-4 (Achiam et al., 2023).\nAs illustrated in Figure 4, our similarity-based classification method achieves the second highest F1 score, only inferior to GPT-4. For API tools, they are not effective enough to detect unsafe queries since they focus on reducing false positives. Conversely, LLMs as detectors usually have a higher recall than precision, indicating a tendency to misclassify safe queries as unsafe. Overall, \\$\\sigma(q)\\$ demonstrates comparable performance, further affirming that hidden states in LLMs are able to mine the harmfulness of input content. Detailed experimental data is provided in Appendix D.2."}, {"title": "5 Ablation Study", "content": "5.1 Effect of Steering Layers\nIt is important to achieve exaggerated safety mitigation and general capability preservation simultaneously. Therefore, the choice of steering layers is a crucial component in our approach. We explore how the performance of SCANS changes when refusal behavior vector steers at different layers. The experimental results are presented in Table 4. It shows that steering former layers brings significant perplexity increase which suggests a nonnegligible performance drop. While steering middle layers slightly underperforms steering latter layers in terms of perplexity, it is more effective in reducing the false refusal on safe queries, indicating the correlation between safety and middle layers.\n5.2 Performance Under Different Multiplier \\$\\alpha\\$\nWe conduct a sensitivity analysis to study the impacts of the multiplier \\$\\alpha\\$ on refusal rate. From Table 6, we observe SCANS is not very sensitive to hyper-parameter \\$\\alpha\\$ since the average performance fluctuates slightly. However, we recommend setting \\$\\alpha\\$ between 2 and 4 because too large a value sometimes results in nonsense outputs (See Appendix E.1).\n5.3 Sensitivity to Threshold T\nWe provide the impact of threshold T on safety-conscious steering performance in Table 7. As observed, when T is below the optimal value, more safe queries are classified as unsafe and false refusal behavior increases. However, when T exceeds the optimal level, the adequate safety may not be guaranteed. This is why we select T = 0.75 for the above comparisons on Llama2-7b-chat. Detailed experimental data is provided in Appendix D.2.\n5.4 Choice of Layers L for Classification\nThe choice of comparison layers is also a crucial component of steering direction identification. As depicted in Figure 5, intermediate and latter layers demonstrate higher degree of distinction, indicating better identification accuracy for harmfulness, which is consistent with previous findings (Rimsky et al., 2023; Geva et al., 2022). Therefore, the motivation behind our classification method \\$\\sigma(q)\\$ is more intuitive. Please refer to Appendix B.2 for the detailed experimental setting of L for each model."}, {"title": "6 Conclusion", "content": "In this paper, we propose SCANS, which mitigates the exaggerated safety for aligned LLMs via activation steering in safety-critical layers. Our motivation is based on that model hidden states imply the safety defense mechanism, indicating the refusal direction within the activation space. After extracting these refusal steering vectors, SCANS employs a similarity-based classification method to determine the steering direction and then steers the model behavior. Experimental results show SCANS effectively reduces the false refusal rate on safe prompts while not compromising the adequate safety and utility. We hope our work contributes to inspiring more researches on exaggerated safety issue through the lens of representation engineering.\nLimitations\nOne limitation of our SCANS is that, there is a chance of classification inaccuracy bringing about the slight drop in refusal rate on unsafe prompts compared to the original models. Fortunately, regarding the performance balancing adequate safety and helpfulness, our approach achieves state-of-the-art performance (See the final column in Table 2). Besides, we have validated SCANS on larger models (>13B parameters, see Appendix D.3) but only compared with one prompt engineering baseline. Since other baselines require either enormous training or inference costs when applying to larger models, we are unable to reproduce them due to limited time and computational resources. Conducting comparison experiments on larger-scale models remains exploration and we leave it as future work."}, {"title": "A SCANS Algorithm", "content": "We illustrate SCANS in Figure 2 and summarize the workflow of SCANS in Algorithm 1. Our algorithm starts from a preparation stage. In this stage, a set of anchor data is utilized to induce the refusal steering vectors, and the harmful data part also extracts the hidden state transition indicating the harm direction for reference. Next, the algorithm proceeds with the inference stage. For each query, the steering direction is identified based on the similarity score. Finally, the activations in safety-critical layers are steered to control the model behavior, thereby mitigating the exaggerated safety in aligned LLMs."}, {"title": "B Implementation Details", "content": "In this section, we will provide additional information about our experimental implementation."}, {"title": "B.1 For the Baseline", "content": "We re-implement SafeDecoding 2 and DRO 3 with their public code to compare with our approach. The recent Self-CD method does not public their source code, so we reproduce the results according to their paper details. The result we reproduced is similar to the one reproduced in the previous work (Zhao et al., 2024). Regarding Prompt base-"}, {"title": "B.2 For the Hyperparameters", "content": "We select the 10th to 20th layers closely related to safety for steering in 7B models and the 16th to 26th layers in 13B models. Apart from the setting of steering layers, we choose hyperparameter L=[10,31] for 7B models and L=[16,26] for 13B models. For hyperparameter T, a better value for 7B models is around 0.75 while a suitable range is [0.6,0.75] for 13B models. Moreover, the size of anchor data is 64 (for both harmful queries and harmless queries)."}, {"title": "B.3 For Generation", "content": "For generation, we use sampling strategies and set top_k to 1, repetition_penalty to 1.1 for all models (temperature is the default setting). Additionally, the max_new_tokens is set to 256 tokens, which exceeds the typical length of responses. The steering vector multiplier \\$\\alpha\\$ for Llama2 family models is all set to 3.5 and Table 9 lists the multiplier \\$\\alpha\\$ for Vicuna models."}, {"title": "C Human Evaluation for Refusal Rate Judgement", "content": "We adopt string matching to judge whether the model response refuses the query. We find that after activation steering, models may use some more fixed phrases to refuse that can be well covered by a manually defined string set. Following Zou et al. (2023a), we list some example refusal string keywords as below."}, {"title": "D Additional Experimental Results", "content": "D.1 Average Refusal Rate across All Models\nWe compute the average refusal rate of each benchmark across all models in Table 8. Models are Llama2-7B-chat, Llama2-13B-chat, vicuna-7B-v1.5 and vicuna-13B-v1.5. We do not report the average results of SafeDecoding because we are unable to reproduce its results on the 13B models (see App. B.1 for reasons). Overall, our average performance in XSTest and OKTest is significantly better than both training-free and training-based baselines, and SCANS can still guarantee the adequate safety. The reason behind this is that when it comes to carefully crafted datasets that resemble unsafe prompts in terms of the vocabulary they use, the baselines do not necessarily address exaggerated safety concerns while our SCANS does.\nD.2 The Performance of \\$\\sigma(q)\\$\nWe present the detailed comparison results in Table 11 for our classification method \\$\\sigma(q)\\$ and some state-of-the-art baselines. GPT-4 demonstrates notably strong detection performance and our classification method is second only to GPT-4. This proves the extracted hidden state transitions are able to provide a simple and efficient distinction between harmful queries and benign queries.\nD.3 Results on Larger Models\nTo further validate our method on larger models, we test SCANS on Qwen1.5-32B-Chat model (Team, 2024). Table 10 shows the overall performance. We can observe that when applying to larger models, our method still reduces the false refusal rate compared to the original aligned LLM. However, the Prompt baseline exhibts poor performance which may be attributed to the limited applicability of their designed prompt in some models. Since other baselines require enormous training or inference costs for larger models, we are unable to reproduce them due to limited computational resources and time constraints."}, {"title": "E Generation Examples using SCANS", "content": "E.1 When Applying Too Large \\$\\alpha\\$\nAs described in Section 5.2, when applying too large \\$\\alpha\\$ values to steer the model behavior, there is more chance of producing the repetition content. Table 12 and 13 show the results of steering towards or against refusal direction with \\$\\alpha\\$ = 10.\nE.2 When Steering Former Layers\nThe choice of steering layers plays a central role for our SCANS. When steering vectors apply in former layers, the fluency of the generated content drops sharply, as indicated by the perplexity (See Table 4). In the following Table 14 and 15, we show some examples of the corrupted model outputs."}, {"title": "E.3 Example of Mitigating the Exaggerated Safety", "content": "Table 16 shows one example of SCANS applying Llama2-7b-chat model to mitigate the exaggerated safety behavior on XSTest dataset."}, {"title": "E.4 Example of Guaranteeing the Adequate Safety", "content": "We present the safety performance of Llama2-7b-chat model after SCANS in Table 17."}, {"title": "E.5 Example of Maintaining the General Capability", "content": "As shown in Table 18, our SCANS achieves the balance between being helpful and being harmless."}]}