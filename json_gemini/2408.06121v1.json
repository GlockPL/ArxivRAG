{"title": "A Methodological Report on Anomaly Detection on Dynamic Knowledge Graphs", "authors": ["Xiaohua Lu", "Leshanshui Yang"], "abstract": "In this paper, we explore different approaches to anomaly detection on dynamic knowledge graphs, specifically in a microservices environment for Kubernetes applications. Our approach explores three dynamic knowledge graph representations: sequential data, one-hop graph structure, and two-hop graph structure, with each representation incorporating increasingly complex structural information. Each phase includes different machine learning and deep learning models. We empirically analyse their performance and propose an approach based on ensemble learning of these models. Our approach significantly outperforms the baseline on the ISWC 2024 Dynamic Knowledge Graph Anomaly Detection dataset, providing a robust solution for anomaly detection in dynamic complex data.", "sections": [{"title": "1 Introduction", "content": "With the development of graph machine learning and geometric deep learning [1], research on encoding and prediction has been applied to more domains than ever before. Graphs can simultaneously represent entities through nodes\u00b3 and relationships between entities through edges. As a result, graph representations are widely used in domains as diverse as transportation [2], chemistry [3], and social networks [24]. Two areas of relative difficulty in graph learning are heterogeneous graphs and dynamic graphs. Heterogeneous graphs allow for different types of nodes and/or edges in the graph [4]. They can therefore better model complex systems with different types of entities, such as a distributed information system with services, pods and nodes. Dynamic graphs allow the structure and/or attributes of a graph to change over time [5], allowing better modelling of data with both temporal and structural information.\nDynamic Knowledge Graphs (DKGs), as an intersection of heterogeneous and dynamic graphs, have attracted increasing attention due to their ability to represent the evolving nature of knowledge over time. By exploiting the ability to capture topological and attribute transformations, DKGs provide valuable insights into the evolution of knowledge in diverse domains such as social media [23], the Internet of Things (IoT) [22,21], and e-commerce4."}, {"title": "2 Related Work", "content": "Recent studies have shown that the dynamic knowledge graph representation is expressive for anomaly detection in temporally complex systems. In this section, we review the related work in this area. In subsection 2.1, we present the definition and notations of the Dynamic Knowledge Graph. Then, in subsection 2.2, we present common approaches to anomaly detection."}, {"title": "2.1 Dynamic Knowledge Graphs", "content": "In static graphs, common representations include edge lists and adjacency matrices. Among them, the edge list is widely used in software development as it requires less memory. An edge list is a collection of edges, each represented as a pair of nodes.\nIn the realm of dynamic graphs, existing research categorises them into Continuous-Time Dynamic Graphs (CTDG) and Discrete-Time Dynamic Graphs"}, {"title": "2.2 Anomaly Detection", "content": "Anomaly detection is a crucial task in various fields, including network security, fraud detection, and financial systems [25,26]. This task focuses on identifying anomalous patterns that deviate from the typical observation of the data [15].\nTraditional anomaly detection methods typically rely on statistical techniques and distance-based measures. However, these methods often struggle with high-dimensional data and complex dependencies [27]. To overcome these problems, common machine learning approaches include support vector machines (SVM) [6], isolation forests (IF) [7], and extreme gradient boosting (XGB) [8]. SVM [6] identifies the optimal hyperplane that maximises the distance between normal data points and the decision boundary, effectively distinguishing outliers. Isolation Forest [7] is an unsupervised method and detects anomalies by constructing decision trees to isolate observations, with outliers requiring fewer splits to isolate due to their rarity and distinctiveness. XGB [8] is a gradient boosting decision tree algorithm that is able to evaluate the importance of each feature and find those that are most effective in distinguishing normal from abnormal data.\nWith the advancement of software algorithms and hardware devices, deep learning methods have been increasingly applied to anomaly detection due to their ability to extract complex features [16]. Common architectures for time series include recurrent neural networks (RNN) [9], temporal convolutional networks (TCN) [12], and the self-attention mechanism (SA) [13]. RNN (Recurrent Neural Network) is a type of neural network designed to process sequential data by capturing temporal dependencies. LSTM (Long Short-Term Memory) [10] and GRU (Gated Recurrent Unit) [11] are variants of RNNs designed to"}, {"title": "2.3 Challenges of Anomaly Detection on Dynamic Knowledge Graph", "content": "Due to the complex temporal and structural dependencies in dynamic knowledge graphs, traditional machine learning methods struggle to capture the underlying features. However, deep learning models typically require large amounts of labelled data for effective training, and anomalous data is inherently scarce.\nAlthough anomaly detection tasks on dynamic graphs without attributed edges can generate anomalous edges through methods such as negative sampling [19,18,20], this approach becomes infeasible in the case of dynamic knowledge graphs with attributed edges. Therefore, effectively capturing the temporal and structural dependencies of features on dynamic knowledge graphs is crucial to tackling this challenge."}, {"title": "3 Methodology", "content": "This section first introduces three representations for dynamic knowledge graphs studied in this task: sequential data, one-hop graph structure, and two-hop graph structure, with each representation incorporating increasingly complex structural information. We then describe the machine learning/deep learning methods applied to each representation. At the end of the section, we propose ensemble schemes for combining multiple methods."}, {"title": "3.1 DKG as Sequential Data", "content": "The time-evolving attributes of nodes can be obtained by extracting raw TTL data. For example, in MSA, common node categories could be C representing \"Cluster\", P representing \"Pod\", S representing \"Service\", and N representing \"Node\". For each given node category k, the time-evolving attributes can be represented as a tensor $S_k \\in \\mathbb{R}^{N_k \\times T \\times d_k}$, where $N_k$ indicated the number of instances of the category k, T indicates the number of snapshots, and $d_k$ indicates the number of features.\n$f(t_j) = [x(t_j - \\tau + 1) | x(t_j - \\tau +2) |\\ldots | x(t_j)]$"}, {"title": "3.2 One-Hop Graph Structure", "content": "To explore the structural information of the knowledge graph, we propose to use one-hop neighbour aggregation to capture the relationships between different entities. Given that the relationships in the DKG of an MSA are mainly hierarchical, i.e., a cluster contains multiple nodes, a node contains multiple pods, and a pod is connected to services through multiple connections (see Fig.1), it is possible to aggregate information from leaf nodes to root nodes to obtain the structural-temporal features of each node at each timestamp based on the temporal information considered in the previous subsection.\n$S(t) = \\bigcup_{i=1}^{n_S} S_i(t), \\bigcup_{j \\in C(S_i)} C_j(t), \\bigcup_{k \\in P(C_j)} P_k(t), \\bigcup_{l \\in N(P_k)} N_l(t)$"}, {"title": "3.3 Two-Hop Graph Structure", "content": "Some graph machine learning algorithms have pointed out that a larger convolutional field, or aggregation of multi-hop neighbours, can optimise the model. Inspired by this, we realised that aggregating second-hop neighbours could provide richer information. In the tree-like graph of the MSA problem, considering only the current level and its associated root level forms a bipartite graph. For nodes of category k, the two-hop neighbours on this bipartite graph are still nodes of the same category."}, {"title": "3.4 Ensemble Learning", "content": "Ensemble learning is a prevalent methodology in the domain of machine learning that mitigates variance through the consolidation of multiple models, thereby enhancing model robustness. The integration of diverse and superior-performing models is a prerequisite for the efficacy of ensemble learning [14]. Ensemble learning allows multiple models to make predictions on the same problem and thus vote on the final prediction. The voting principle can be hard voting or soft voting, while the voting mechanism usually consists of unanimous voting and majority voting, which will be introduced in this subsection.\nIn hard voting, each classifier votes for a class label, and the final prediction is determined by the majority of these classifiers. In soft voting, each classifier provides a probability for each class, and the final prediction is based on the highest average probability across all classifiers.\nAs two common voting mechanisms combined with hard voting, especially in anomaly detection, unanimous voting classifies an instance as an anomaly only if all the models agree. Majority voting, on the other hand, classifies an instance as an anomaly if the majority of models agree.\nIntuitively, unanimous voting is more conservative and reduces false positives for anomaly detection, but may also miss some true anomalies. The majority voting method, in contrast, has some flexibility and can improve the detection of true anomalies, but it also preserves the false alarms. Both methods are tested on multiple sets of models and it are discussed in the next section."}, {"title": "4 Experiment", "content": null}, {"title": "4.1 Experimental Setup", "content": "All experiments were conducted utilising the ISWC 2024 ADDKG dataset. The dataset contains five days of labelled data, with three days of data used for"}, {"title": "4.2 Performance of Models", "content": "The experimental results of each representation are shown in tables 1 and 2. Table 1 focuses on the performance of individual models, while table 2 highlights the performance of different ensemble methods. D1, D2, and D3 represent sequential data representation, one-hop graph representation, and two-hop graph representation, respectively.\nTable 1 shows how each model performs under different structural conditions of the dataset. The results show that the inclusion of graph structures and service interactions generally improves model performance, particularly in terms of recall and F1 score.\nIn order to effectively combine multiple models, the second part of the experiments evaluates the performance of different ensemble methods, as shown in Table 2. Different ensemble strategies are tested, such as combining XGB, SVM, stacked layers of the Self Attention model, and Isolation Forest. In the evaluation of the ensemble methods, we have compared two different methods, soft voting, hard&unanimous voting and hard&majority voting. Table 2 shows the best performances obtained. Overall, the soft voting approach provided better results for our objectives."}, {"title": "5 Conclusion", "content": "In this paper, we investigated the effectiveness of different machine learning and deep learning models on the challenging task of anomaly detection on dynamic knowledge graphs, with a specific focus on a microservices environment. Using a structured experimental approach, we analysed the performance of the models in three different representations: sequential data modelling, one-hop graph structure, and two-hop graph structure."}]}