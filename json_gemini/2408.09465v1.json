{"title": "MedMAP: Promoting Incomplete Multi-modal Brain Tumor Segmentation with Alignment", "authors": ["Tianyi Liu", "Zhaorui Tan", "Muyin Chen", "Xi Yang", "Haochuan Jiang", "Kaizhu Huang"], "abstract": "Brain tumor segmentation is often based on multiple magnetic resonance imaging (MRI). However, in clinical practice, certain modalities of MRI may be missing, which presents a more difficult scenario. To cope with this challenge, Knowledge Distillation, Domain Adaption, and Shared Latent Space have emerged as commonly promising strategies. However, recent efforts typically overlook the modality gaps and thus fail to learn important invariant feature representations across different modalities. Such drawback consequently leads to limited performance for missing modality models. To ameliorate these problems, pre-trained models are used in natural visual segmentation tasks to minimize the gaps. However, promising pre-trained models are often unavailable in medical image segmentation tasks. Along this line, in this paper, we propose a novel paradigm that aligns latent features of involved modalities to a well-defined distribution anchor as the substitution of the pre-trained model. As a major contribution, we prove that our novel training paradigm ensures a tight evidence lower bound, thus theoretically certifying its effectiveness. Extensive experiments on different backbones validate that the proposed paradigm can enable invariant feature representations and produce models with narrowed modality gaps. Models with our alignment paradigm show their superior performance on both BraTS2018 and BraTS2020 datasets.", "sections": [{"title": "I. INTRODUCTION", "content": "Brain tumors pose severe risks to human life, making precise medical segmentation essential as it devises effective treatment planning and strategies [1]. Brain tumor segmentation methods usually employ Multiple Magnetic Resonance Imaging (MRI) visualizations, including Fluid Attenuation Inversion Recovery (Flair), contrast-enhanced T1-weighted (T1ce), T1-weighted (T1), and T2-weighted (T2), as illustrated in Figure 1, as multiple modalities [2]. Particularly, the aforementioned modalities complement each other to understand both the physical structure and physiopathology of tumors; their combination naturally leads to improved segmentation performance [3]\u2013[6]. For instance, T2 and Flair imaging modalities are valuable for assessing the enhanced tumor (ET), whereas T1 and Tlce modalities are effective in delineating the core of the tumor, including the necrotic, non-enhancing, and enhancing regions (NCR/NET) [7]. However, not all modalities are always available in real clinical practice due to scenarios such as data corruption and/or variations in scanning protocols, leading to crucial challenges of developing a generalized multi-modal approach that copes with the absence of certain modalities [8]-[11].\nIn response to the challenge of missing modalities, previous substantial efforts have converged on two primary strategies: Knowledge Distillation (KD), and Domain Adaptation (DA). The KD-based attempts involve transferring the knowledge from the complete multi-modality teacher to the incomplete missing-modality student [1], [12]\u2013[14], while the methods in DA leverage alignment methods to bridge the gap between the model trained on complete modalities and those trained on incomplete ones [15]. However, these two strategies ignore gaps existing between the modalities as they often process different modalities as separate channels, leading to a failure to capture the important invariance across modalities. To better alleviate the modality gaps, another branch of methods, named Shared Latent Space (SLS), transfers each modality into a common representation space shared among all modalities [7], [16]\u2013[18]. It argues that they can learn the inter-modality invariance [18], but the detailed analysis of minimized modality"}, {"title": "II. RELATED WORK", "content": "A. Multi-modal Learning for Missing Modalities\nFlair, T1, T2, and Tlce are complementary modalities utilized to segment brain tumors [2]. Segmentation performance unfortunately drops drastically in the scenario when some modalities are missing due to practical difficulties such as data corruption and/or scanning protocol variations [8]\u2013[11], [21]. Prior arts have proposed several methods to deal with the missing modality problem, which can be generally divided into three categories: Knowledge Distillation, Share Latent Space, and Domain Adaption.\nKnowledge Distillation (KD) based approaches transfer knowledge from teachers with complete modality information to students with missing modality information. In [1], Kullback-Leibler (KL) loss and an additional contrastive loss are engaged to guide the student to imitate soft distributed features from the teacher and reduce latent space divergence between them respectively. ProtoKD [13] employs a proto-type knowledge distillation loss to encourage simultaneous intra-class concentration and inter-class divergence. In these methods, students' performance is always based on teachers' performance. However, students often lead to sub-optimal segmentation performance since their teachers do not consider the feature invariant information and domain-specific information among those modalities.\nShared Latent Space (SLS) methods retrieve missing information by exploiting the multimodal latent feature space. RFnet [7] uses different encoders to extract the modality-specific information and a decoder to share the weights for those modalities to build a shared representation. Different modality features are fused at different levels. MmFormer [18]"}, {"title": "III. THEORETICAL MOTIVATION", "content": "In this section, we underscore the significance of reducing modality gaps that prior methodologies have overlooked. We present the problem formulations for three distinct types of approaches and illustrate the enhancement of domain alignment through the strategic use of $P_{mix}$. Furthermore, we explore different techniques for deriving practical empirical representations of $P_{mix}$.\nNotations. We denote the model, i.e., the teacher model for knowledge distillation, the model for shared latent space, or the model pretrained for domain adaptation as the base model. Considering $J$ as the number of the set of modalities of medical images with paired observations and targets ${X_j, Y;}_{j=1}$ from the modality $j$. Note for medical modalities, $Y$ remains static for all modalities. The encoders of the base model are denoted as $T: T(X_i) \\rightarrow Z$ where $Z$ denotes the latent feature obtained from the base model of the $j^{th}$ modality. Simultaneously, a predictor $C$ that predicts segmentation masks from ${Z_j}_{j=1}$ as $C^* : C^*({Z_j}_{j=1}) \\rightarrow Y$. Correspondingly, we denote the possible downstream model, e.g., the student models for knowledge distillation and the adapted model for the domain adaptation, for the $j^{th}$ target modality as $S_j: S_j(X_j) \\rightarrow Z_j$ of each modality with their predictor $C_j: C(Z_j) \\rightarrow Y$. Let $P(\\cdot)$, $D_{KL}(\\cdot||\\cdot)$, $H(\\cdot)$, $H_c(\\cdot, \\cdot)$, $I(\\cdot; \\cdot)$ denote the probability of a random variable from the distribution, Kullback\u2013Leibler divergence, entropy, cross-entropy, and mutual information respectively."}, {"title": "A. Previous Methods", "content": "Despite the various approaches adopted to handle missing modalities, the common strategy involves training a base model $T$ that retains information from all modalities as its foundation. Therefore, the objective for training $T$ can be summarized as:\n$\\max E_{z_j \\sim P(Z_j)}[\\ln P(Y|C^*(Z))] + \\zeta$,\nwhere $\\zeta$ denotes any possible regularization. Eq. 1 can be changed in the form of information entropy as:\n$\\min_{\\zeta^*} \\sum_{j=1}^J H_c(P(C^* (Z_j)), P(Y)) + \\zeta$.\nThe base model can be adapted for different downstream approaches under the missing modality scenario. We provide details for each type of missing modality approaches as follows:\nKD. In knowledge distillation for medical segmentation with missing modality, the process involves two main steps: training the teacher and student models. As previously mentioned, the teacher model is denoted as the base model $T$; its objective is in the form of Eq. 1. Notably, previous methods [1], [12], [13] uses no extra regularization for training $T$, i.e., $\\hat{\\zeta}_{KD} := 0$. Meanwhile, for one possible student model $S_j$ encoder and $C_j$ classifier trained with missing modalities that leverages knowledge from $T$, its objective is:\n$\\max_{S_jC_j} E_{z_j \\sim P(Z_j)}[\\ln P(Y | C_j (Z_j))]\n- D_{KL}(P(S_j(X_j))||P(Z))$.\nSLS. For approaches aiming to achieve better-fused representations from a shared latent space of all modalities, the focus is on enhancing the fusion mechanism of $T$ through additional losses and modifications to model architectures, denoted as $\\hat{\\zeta}_{SLS}$. However, it is important to note that the current shared latent space (SLS methods often do not adequately address the mitigation of modality gaps.\nDA. Domain adaptation methods address modality gaps by continually adapting the base model $T$ through specific training with missing modality settings. Considering a possible adaptation $S_j$ encoder and $C_j$ classifier derived from $T$ trained with missing modalities, its objective can be simplified as:\n$\\max_{S_jC_j} E_{z_j \\sim P(Z_j)}[\\ln P(Y | C_j (S_j(X_j)))].$\nLimitations. Albeit their impressive results, most existing methods in the mentioned categories share a common limitation: they often disregard the potential modality gap in $T$, which can hinder the performance of missing modality tasks."}, {"title": "B. Alignment in Multi-domain Generalization", "content": "Gaps in the latent space arise not only across various modalities but also span across distinct data domains. To address the domain generalization task, many attempts have been made towards a narrowed gap. For instance, recent efforts [22]\u2013[25] learn domain-independent representations by reducing the gaps to improve generalization to classify images, which are, however, not directly applicable to semantic segmentation. In our research, we borrow the idea from the domain generalization to perform proper alignment between latent distributions in the teacher. To this end, we successfully reduce the modality gaps to overcome the difficulty of missing modality, thereby improving the segmentation of the medical image in all categories."}, {"title": "III. THEORETICAL MOTIVATION", "content": "In this section, we underscore the significance of reducing modality gaps that prior methodologies have overlooked. We present the problem formulations for three distinct types of approaches and illustrate the enhancement of domain alignment through the strategic use of $P_{mix}$. Furthermore, we explore different techniques for deriving practical empirical representations of $P_{mix}$.\nNotations. We denote the model, i.e., the teacher model for knowledge distillation, the model for shared latent space, or the model pretrained for domain adaptation as the base model. Considering $J$ as the number of the set of modalities of medical images with paired observations and targets ${X_j, Y;}_{j=1}$ from the modality $j$. Note for medical modalities, $Y$ remains static for all modalities. The encoders of the base model are denoted as $T: T(X_i) \\rightarrow Z$ where $Z$ denotes the latent feature obtained from the base model of the $j^{th}$ modality. Simultaneously, a predictor $C$ that predicts segmentation masks from ${Z_j}_{j=1}$ as $C^* : C^*({Z_j}_{j=1}) \\rightarrow Y$. Correspondingly, we denote the possible downstream model, e.g., the student models for knowledge distillation and the adapted model for the domain adaptation, for the $j^{th}$ target modality as $S_j: S_j(X_j) \\rightarrow Z_j$ of each modality with their predictor $C_j: C(Z_j) \\rightarrow Y$. Let $P(\\cdot)$, $D_{KL}(\\cdot||\\cdot)$, $H(\\cdot)$, $H_c(\\cdot, \\cdot)$, $I(\\cdot; \\cdot)$ denote the probability of a random variable from the distribution, Kullback\u2013Leibler divergence, entropy, cross-entropy, and mutual information respectively."}, {"title": "A. Previous Methods", "content": "Despite the various approaches adopted to handle missing modalities, the common strategy involves training a base model $T$ that retains information from all modalities as its foundation. Therefore, the objective for training $T$ can be summarized as:\n$\\max E_{z_j \\sim P(Z_j)}[\\ln P(Y|C^*(Z))] + \\zeta$,\nwhere $\\zeta$ denotes any possible regularization. Eq. 1 can be changed in the form of information entropy as:\n$\\min_{\\zeta^*} \\sum_{j=1}^J H_c(P(C^* (Z_j)), P(Y)) + \\zeta$.\nThe base model can be adapted for different downstream approaches under the missing modality scenario. We provide details for each type of missing modality approaches as follows:\nKD. In knowledge distillation for medical segmentation with missing modality, the process involves two main steps: training the teacher and student models. As previously mentioned, the teacher model is denoted as the base model $T$; its objective is in the form of Eq. 1. Notably, previous methods [1], [12], [13] uses no extra regularization for training $T$, i.e., $\\hat{\\zeta}_{KD} := 0$. Meanwhile, for one possible student model $S_j$ encoder and $C_j$ classifier trained with missing modalities that leverages knowledge from $T$, its objective is:\n$\\max_{S_jC_j} E_{z_j \\sim P(Z_j)}[\\ln P(Y | C_j (Z_j))]\n- D_{KL}(P(S_j(X_j))||P(Z))$.\nSLS. For approaches aiming to achieve better-fused representations from a shared latent space of all modalities, the focus is on enhancing the fusion mechanism of $T$ through additional losses and modifications to model architectures, denoted as $\\hat{\\zeta}_{SLS}$. However, it is important to note that the current shared latent space (SLS methods often do not adequately address the mitigation of modality gaps.\nDA. Domain adaptation methods address modality gaps by continually adapting the base model $T$ through specific training with missing modality settings. Considering a possible adaptation $S_j$ encoder and $C_j$ classifier derived from $T$ trained with missing modalities, its objective can be simplified as:\n$\\max_{S_jC_j} E_{z_j \\sim P(Z_j)}[\\ln P(Y | C_j (S_j(X_j)))].$\nLimitations. Albeit their impressive results, most existing methods in the mentioned categories share a common limitation: they often disregard the potential modality gap in $T$, which can hinder the performance of missing modality tasks."}, {"title": "B. Aligning Medical Multi-modalities", "content": "This section explores an innovative strategy designed to bridge the modality gap, specifically for the base model T. We also demonstrate that our approach can improve many methods across all three categories.\nMinimizing modality gap improve generalization ability of the T. In the context of domain generalization, minimizing the modality gap can be interpreted as reducing the distributional discrepancy between the source domain S and the target domain T as [27]:\n$\\epsilon_T(h) \\leq \\epsilon_S(h) + d_{H\\Delta H}(S, T) + \\lambda$\nwhere $\\epsilon_T(h)$ is the error rate of the model on the target domain, $\\epsilon_S(h)$ is the error rate of the model on the source domain, $d_{H\\Delta H}(S, T)$ is the distributional discrepancy between the source and target domains, and $\\lambda$ is a term related to the complexity of the hypothesis space.\nReducing the distributional discrepancy between the source and target domains, thereby reducing $d_{H\\Delta \u0397}(S,T)$, which theoretically lowers the error rate on the target domain and thus improves the model's generalization ability. In the modality generalization tasks, reducing $d_{HAH}$ of different modalities can also improves the model's generalization ability.\nHow to align to the pre-defined anchor $P_{mix}$. Assuming that there is a feasible $P_{mix}$ in the latent space which preserves the prediction performance, i.e., the minimization of $\\int L(Z, Y_i) dP(\\theta)$ in Eq. 5 is guaranteed, minimization of Eq. 5 can be changed as:\n$\\min D_{KL}(P(Z)||P_{mix}) + H_c(P(C^*(Z)); P(Y))$.\nTo derive an empirical form of minimizing the term $D_{KL}(P(Z)||P_{mix})$, we first introduce Proposition 1:\nProposition 1: For training a multi-modal teacher model, it is assumed that the existence of one modality $Z_i$ is independent of the other modality $Z_j$ where $i \\in {1, ..., J}, j\\in {1, ..., J}, i \\neq j$. In other words, if one modality is missing or corrupted, other modalities can still exist. In this scenario, there exists a probability distribution $P_{mix}$ that can be used as an anchor distribution to align the latent variables $Z^*$, while preserving sufficient information for accurate prediction of the segmentation labels Y.\nProof: The modality existence independent assumption is derived from the fact that each modality is independent of each other. If $P_{mix}$ preserves sufficient information for accurate"}, {"title": "IV. METHODOLOGY", "content": "A. Alignment Paradigm\nMedMAP consists of a feature encoding pipeline and an anchor that latent space of the features will be aligned to. The encoder is a simple convolution that aims to map all the modality features into a shared latent space, so that alignment can be applied. With the given $P_{mix}$, we align each modality latent features $Z_{j\\in{1,..,J}}$ to $P_{mix}$. By converting multi-modal features into a pre-defined distribution, we aim to standardize the scale and distributional properties of data originating from heterogeneous sources, thereby enabling cross-modal learning of distinctive features. In this article, two kinds of $P_{mix}$ are proposed: $\\hat{P}_{mix}$ and $\\tilde{P}_{mix}$. We denote B, J, the batch size, and the number of modalities, respectively.\n1) Aligning to $P^k_{mix}$: To derive $P^k_{mix}$, we first train the backbones using each modality independently and then evaluate the trained models over all modalities. We select the modality designated as the $k^{th}$ modality, which is identified when the models demonstrate the superior performance when evaluated across j modalities. Following this, we define $\\hat{P}_{mix} = P(Z = k)$. For aligning other modalities to the selected $k^{th}$ modality, we minimize:\n$L(T) = \\frac{1}{BJ} \\sum_{b=1}^B \\sum_{j=1}^J || z^b_j - \\hat{z}^b_k ||^2_2$,\nwhere $z_j$ are features of different modalities, $z_j \\sim Z_j$ are features that need to be aligned to $X_k \\sim Z_k$ from the $k^{th}$ modality;\n2) Aligning to $\\tilde{P}_{mix}$: In the quest to derive a more conducive latent space for integrating all modalities, we have advanced an innovative methodology termed Adaptive Alignment. This approach will transcend the basic alignment method that confines the latent space to a specific modality $(\\tilde{P}^k_{mix})$. Adaptive Alignment operates under the presumption that an optimal latent space, termed as $\\tilde{P}_{mix}$, for a prior modality can serve as a foundational anchor. Following this, we define $\\tilde{P}_{mix} = \\sum_{j=1}W_jP(Z)$, where $w_j \\in w = {w_j}^J_{j=1}$ are learable weights. Then we minimize:\n$L(T, W) = \\frac{1}{BJ} \\sum_{b=1}^B \\sum_{j=1}^J || z^b_j - \\sum_{j=1}^J W_jz_j ||^2_2$,\nwhere $z_j \\sim Z_j$ are features that need to be aligned, and $\\sum_{j=1}^J W_jz_j$ is the adaptive anchor learned from all the features.\nUnlike $\\hat{P}_{mix}$, $\\tilde{P}_{mix}$ does not rely on one specific pre-defined distribution. Such characteristic enables $\\tilde{P}_{mix}$ to more effectively adapt to various multi-modality datasets, requiring less prior knowledge."}, {"title": "B. Backbones", "content": "1) KD: As shown in Fig. 3a, in the KD methods, there are two branches: the teacher, and the student. Teacher models are trained with a full-modality dataset. Student models are trained with an incomplete-modality dataset and soft labels from the teacher model.\nFor training the teacher, each modality is encoded into a shared latent space and then aligned to the pre-defined $P_{mix}$. Specifically, as shown in Eq. 8, each modality representation is individually aligned to $P_{mix}$. The aligned features then become the input to the original teacher backbone. Therefore, the objective of training the teacher will be:\n$L_{KD \\text{base}} = L_{seg} + \\alpha L_a,$\nSpecifically, the teacher model remains frozen, which preserves the integrity of the pre-learned representations and enables us to concentrate on assessing how effectively the student model can emulate the fixed knowledge rather than updating the teachers during student training. Consequently, the learning process of the student model is supported by the ground truth and the teacher's knowledge with the overall objective $L$ as:\n$L_{KD} = L_{seg} + L^F_{seg} + L^F_{KD \\text{base}},$\nwhere $L^F_{seg}$ is the hybrid segmentation loss supervised by the hard labels of the student, and $L_{KD \\text{base}}$ denotes the loss that receives supervision from the teacher.\n2) SLS: As shown in Fig. 3c, SLS models always have a modality-specific encoder, a modality fusion encoder, and"}, {"title": "V. EXPERIMENT CONFIGURATIONS", "content": "A. Datasets\nThe BraTS datasets in 2018 and 2020 [4], [28], consisting of 285 and 369 subjects respectively, are employed for evaluation. Four individual modalities including T1, T1ce, T2, and Flair construct a specific subject, displaying brain tumor sub-regions i.e. enhancing tumor (ET), peritumoral edema (ED), and the necrotic and non-enhancing tumor core (NCR/NET). Specifically, each modality captures different properties of brain tumor sub-regions: GD-enhancing tumor (ET), peritumoral edema (ED), and the necrotic and non-enhancing tumor core (NCR/NET), nesting into three key segmentation targets, i.e., whole tumor (WT, ET+ED+NCR/NET), tumor core (TC, ET+NCR/NET), and ET. For both datasets, we use Dice Score"}, {"title": "B. Implementation", "content": "For comparison fairness, we implant the proposed alignment paradigm to retrain each employed backbones [1], [15], [18] following identical respective experimental settings except the dataset configuration and the batch size, given by each author's released codes. The BraTS 2018 dataset is split into training, validation, and testing sets following Proto-KD [13], while the other BraTS 2020 dataset is used with three-fold cross-validations. In contrast to its official version, the batch size of mmFormer [18] is set to 1 and the batch size of PMKL [1] is set to 4 while the others remain the same. Models are implemented on one Nvidia GeForce RTX 3090Ti GPU with Pytorch."}, {"title": "VI. EXPERIMENT RESULTS", "content": "As discussed in Sec. II-A, recent approaches to predict annotations in the Missing Modality scenarios (MM) mainly include Knowledge Distillation (e.g. PMKL [1]), Shared Latent Space (e.g. mmFormer [18]), and Domain Adaption (e.g. ACN [15]). The proposed Alignment Paradigm will be implanted into the aforementioned recent SOTAs to evaluate the effectiveness. Specifically, Section VI-A will present statistical improvements given by the proposed MedMAP, measured by averaged dice as well as improvement (marked in red) and decline (in blue) in Table I. Details can also be found across various MM on both datasets in Table II. Sec. VI-B visualizes examples with and without Pmix. In Sec. VI-C, we compare several possible structures in key components of the proposed MedMAP, which guide us to find the most suitable encoder and anchor. In Sec. VI-D, the selection of hyperparameters will be introduced."}, {"title": "A. Quantitative Comparisons", "content": "1) Averaged results across prediction classes and MM: As seen from Table I, we can conclude that the proposed MedMAP effectively promotes the prediction performance. Specifically, on the BraTS2018 dataset, it improves 3.68%, 7.67%, and 2.30% of the Dice Scores on PMKL, mmFormer, and ACN respectively. Improvements are 1.31%, 6.76%, and 0.38% of the Dice Scores on PMKL, mmFormer, and ACN respectively on the BraTS2020 dataset.\n2) Performance improvement on different prediction classes: In both Table II, the right-most columns of each dataset (Total Avg.) indicate the average performance across different MM. From them, we can identify when the proposed MedMAP is absent, the mmFormer achieves 83.67%, 73.04%, and 55.15%, respectively, on the BraTS2018 dataset, and 85.79%, 73.23%, and 59.81% on BraTS2020 when predicting WT, TC, and ET. Notably, on both datasets, predictions obtain lower dice scores on TC and ET than on the WT. The presence of the proposed MedMAP significantly promotes the degraded predictions of TC and ET by 4.97% and 11.72% respectively on BraTS2018, and 2.99% and 15.29% on BraTS2020. Moreover,"}, {"title": "B. Qualitative Comparison", "content": "We can observe from Fig. 4 that in general, the performance of the model with $P^a_{mix}$is superior to that without it. When"}, {"title": "C. Comparison of Alignment Paradigm components", "content": "As part of the proposed MedMAP with an encoder and an aligning anchor, we conduct ablation studies to evaluate the effectiveness of both components. Regarding the encoder, we compare the enhanced and the non-shared encoder. For the anchor Pmix, we compare the proposed $\\hat{P}_{mix}$ with the standard normal distribution [30]. Furthermore, we assess two different $\\hat{P}_{mix}$ options namely $P^k_{mix}$ and $\\tilde{P}_{mix}$.\n1) Encoder: As discussed in Sec. IV-A, the proposed alignment paradigm will only be possible when latent features are placed in the same space, by using some encoding architectures. In this sense, we would like to evaluate the prediction performance based on two different encoder configurations: the non-shared encoder and the enhanced encoder, termed as non-shared T and T*, respectively. In non-shared T, an"}, {"title": "VII. CONCLUSION", "content": "In this paper, we present a novel Medical Modality Alignment Paradigm to mitigate the modality gaps whilst learning simultaneously invariant feature representations in segmenting brain tumors with missing modalities. Specifically, we invent an alignment paradigm for the models with an encoder encouraging that all modalities are in the same space and a latent space distribution $P^a_{mix}$ as the aligning anchor. Meanwhile, we provide theoretical support for the proposed alignment paradigm, demonstrating that individual alignment of each modality to $\\hat{P}_{mix}$ certifies a tighter evidence lower bound than mapping all modalities as a whole. Extensive experiments have demonstrated the superiority of the proposed paradigm over several latest state-of-the-art approaches, enabling better segment the brain tumor with missing modalities."}]}