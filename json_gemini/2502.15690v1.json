{"title": "Level-Navi Agent: A Framework and benchmark for Chinese Web Search Agents", "authors": ["Chuanrui Hu", "Shichong Xie", "Baoxin Wang", "Bin Chen", "Xiaofeng Cong", "Jun Zhang"], "abstract": "Large language models (LLMs), adopted to understand human language, drive the development of artificial intelligence (AI) web search agents. Compared to traditional search engines, LLM-powered AI search agents are capable of understanding and responding to complex queries with greater depth, enabling more accurate operations and better context recognition. However, little attention and effort has been paid to the Chinese web search, which results in that the capabilities of open-source models have not been uniformly and fairly evaluated. The difficulty lies in lacking three aspects: an unified agent framework, an accurately labeled dataset, and a suitable evaluation metric. To address these issues, we propose a general-purpose and training-free web search agent by level-aware navigation, Level-Navi Agent, accompanied by a well-annotated dataset (Web24) and a suitable evaluation metric. Level-Navi Agent can think through complex user questions and conduct searches across various levels on the internet to gather information for questions. Meanwhile, we provide a comprehensive evaluation of state-of-the-art LLMs under fair settings. To further facilitate future research, source code is available at Github.", "sections": [{"title": "Introduction", "content": "Information gathering is a key step in the interaction between humans and their environment. Search engines are widely used for information acquisition (Brin & Page (1998)). With the development of large language models (LLMs) (Ye et al. (2023), Achiam et al. (2023)), AI search agents based on LLMs have become an emerging and challenging research topic Nakano et al. (2021).\nRetrieve-Augmented Generation (RAG) is used to improve the precision of model responses (Ram et al. (2023)). Existing methods (Chan et al. (2024), Siriwardhana et al. (2023)) leverage the powerful language capabilities of LLMs to perform retrieval based on user queries and use the retrieved rele- vant texts to improve the reliability of the model's answers. This advanced ability to understand and analyze questions exceeds that of traditional search engines, driving a revolutionary transformation in AI-powered search (Spatharioti et al. (2023)). However, these methods do not further explore how LLMs handle complex questions. The simple text retrieval approach cannot fully align with web search scenarios. And irrelevant texts retrieved have negative impacts on the quality of responses (Asai et al. (2023)).\nTherefore, refined methods( Chen et al. (2024), Reddy et al. (2023)) are proposed to construct AI search agents. Mindsearch( Chen et al. (2024)) employs the concept of Directed Acyclic Graphs to structure the agent's plan, breaking down complex reasoning questions with the aim of simulating the human mind, thereby striving to deliver more comprehensive answers. Infogent( Reddy et al. (2024)) utilizes an information aggregation approach to update the retrieved information. Determine whether the retrieved texts meet the required conditions and improve the accuracy of responses by controlling the quality of the information. These methods achieve promising results under detailed process planning. However, the research community still lacks comprehensive studies that can genuinely reveal the true capabilities of various open-source and closed-source LLMs in the web search scenario.\nExisting LLM-driven search agents require fine-tuning or rely on high-performance close-source models, making it difficult for researchers to investigate the capabilities of various LLMs due to their costs. Datasets for evaluating the capabilities of LLMs in Chinese scenarios are constructed, such as CMMLU (Li et al. (2023)) and AlignBench (Liu et al. (2024)). The advent of these datasets shows the demand for model evaluation in real-world Chinese contexts. However, in the field of web search, a suitable Chinese web search dataset for quantitative evaluation is lacking. Meanwhile, we reveal that traditional metrics like F1 and ROUGE( Lin (2004)) do not consider the semantic information across various versions, which poses challenges when comparing the performance of different models.\nTo address the aforementioned issues, we propose a training-free AI search agent framework for both open-source and close-source models, as illustrated in Fig. 1. Meanwhile, we provide a new Chinese web search dataset and a new evaluation metric to evaluate the performance of LLMs in the Chinese task. Overall, our contributions are as follows.\n\u2022 We propose a general-purpose training-free web search agent framework, called Level- Navi Agent. The question from the user is first analyzed and decomposed by the Planner. Then the sub-questions are provided to the Searcher, which will collect information at differ- ent levels. By iterating this step, Level-Navi Agent eventually collects enough information to answer the initial question. Level-Navi Agent does not require training, allowing any open-source LLM to be deployed.\n\u2022 We provide a well-annotated benchmark dataset (Web24) for Chinese web search. Our dataset is capable of a diverse and detailed classification of questions and sources, all sourced entirely from the Chinese internet. Considering the limitations of traditional metrics, we adopted four reasonable metrics to evaluate the ability of different LLMs when execute the Level-Navi Agent. Through our benchmark, the performance of different LLMs for AI web search is clearly presented.\n\u2022 We reveal the factors that limit model performance in executing Chinese web search agent tasks. First, we find that the model exhibits an \u201coverconfidence\u201d phenomenon, where it refrains from calling functions for web searches even when it does not know the answer, leading to incorrect responses. Second, the model demonstrates low \u201ctask fidelity\u201d during task execution, meaning it fails to fully understand our instructions, resulting in non-compliant answers and poor response quality."}, {"title": "Level-Navi Agent", "content": "In this Section, we will introduce our Level-Navi agent. We detail the structure of our Planning Agent and Level-info Agent in Section 2.1 and Section 2.2. The overall structure is shown in Fig. 2. Fig. 3 illustrates a real case handled by our agent."}, {"title": "Planning Agent", "content": "The Planner is a key component in our design, and its structure directly affects the performance of the entire Agent to a certain extent. Next, we will provide a detailed introduction to the design of the Planning Agent. Our Planning Agent plans the trajectory path through a process of chain of thought (Wei et al. (2022)) and iterative refinement. When user inputs a question, our Agent first understands and breaks down the problem through chain of thought. Noticing the difference from the conventional chain of thought steps (Jin et al. (2024), Wang et al. (2023)), we do not ask the LLM to only provide a complete set of steps to solve the problem at once. We let the LLM first think through and determine the information that should be collected next, then generate a list of sub-questions that can be searched in parallel at this stage. This is because, in scenarios where complex reasoning is required, a complete plan may seem very clear; however, since we rely on another Agent to gather information, the content obtained each time is not necessarily sufficient or complete, which involves dynamically adjusting the plan every time. To avoid such a complex and redundant process design, we use prompt to enforce that the Planning Agent only giving the list of sub-question that needed to be obtained in the next step. After obtaining feedback from each step, it repeats this process iteratively until the Agent judges that the current information is sufficient to answer the question. As depicted in Fig. 3, our Planning Agent has broken down the problem into multiple levels, systematically presenting the sub-problems required at each step. Through iteration, it gathers sufficient information to answer the question effectively. Algorithm 1 also demonstrates the complete process of our Planning Agent."}, {"title": "Level-Info Agent", "content": "As depicted in the right of Fig. 2, the primary task of the Searcher is to obtain relevant information feedback to the Planner by conducting online searches based on the sub-problems received. In order to enrich the information obtained while enhancing its flexibility, we have constructed an Agent that dynamically simulates human information acquisition process through a chain of thought, which we call the Level-Info Agent.\nAs its name suggests, the Level-Info Agent is capable of dynamically obtaining information at various levels and can return results at any moment, significantly improving the agent's operational speed. Firstly, when faced with an input sub-query, the Level-Info Agent determines whether the information can be answered using its own knowledge. If it can, it returns the result directly; otherwise, it proceeds with a web search. Then, based on the sub-query, the Agent will call the web search function to use search engine APIs to return the results of the online search. At this step, the returned materials will only be the summary parts of the web pages. Here, we also have the Agent think and determine whether the current materials obtained can answer the sub-query. If the information is sufficient, it will provide a direct response; otherwise, it will proceed to the next step of opening relevant websites. When performing this step, we also use function calls to let the model select and open relevant websites. After obtaining the information from the web page, it will summarize and respond.\nBased on the above introduction, our Level-Info Agent has up to three levels for providing informa- tion feedback. This design is inspired by human behavior in collecting information online: when faced with a question, humans may feel confident in answering it themselves. If a web search is needed, people will first browse the returned pages. If the information on the returned interface can answer the question, there is no need to open it further. The opening of web pages always occurs as the last step.\nOur Level-Info Agent avoids the need to always read a large number of websites, which consumes a significant amount of tokens, and also reduces the frequency of calling search engine APIs, thereby lowering the overall cost of using the Agent."}, {"title": "Benchmark", "content": "In this section, we will provide a detailed introduction to the benchmark specifically designed for web search agents. The Web24 dataset will be introduced in section 3.1, and Section 3.2 will cover our evaluation metrics."}, {"title": "Data Composition", "content": "Our Web24 dataset categorizes question-answer pairs into fine-grained divisions based on three labels: source, domain, and type.\nIn the process of evaluating the performance of web search agents, we aim to minimize the influence of the model's internal knowledge to genuinely assess the search capabilities. Therefore, when constructing the test dataset, we ensure that the majority of the question-answer pairs are sourced from news, as illustrated in Fig. 4. All cases sourced from news are entirely from news reports on the Chinese internet before December 2024. We have provided the URLs of the report sources in the dataset for verification. For cases sourced from knowledge, our annotators cross-check to ensure that the questions are not too simple, thereby guaranteeing that the model needs to conduct a web search to obtain the answers.\nTo simulate as closely as possible the scenarios in which people conduct web searches in everyday life, our question-answer pairs are categorized into five domains: finance, gaming, sports, movie, and event. Each domain has been designed with specific questions and answers to reflect the information needs that users might encounter in their daily lives."}, {"title": "Evaluation Metrics", "content": "To comprehensively assess the capabilities of LLMs in performing web search tasks, we consider multiple aspects and use four scoring metrics to evaluate the model's capabilities holistically. Each metric takes into account the model's performance from different perspectives during the search task execution. Finally, we calculate the weighted sum of these four metrics to obtain the final assessment score.\nOur detailed description of the evaluation metrics is as follows:\nCorrectness Scores(Sco). In previous tasks assessing the capabilities of LLMs, the F1 metric is often used (Chan et al. (2024); Jiang et al. (2024)) as an indicator to evaluate the gap between the model's responses and the ground truth answers. However, the F1 score calculated purely at the token level is not suitable for complex question-answering tasks, as semantic information and rich expressions cannot be conveyed through the F1 score, and it may even lead to misjudgments. We will discuss this issue in detail in Section 4.3. To better evaluate the model response, we employ a LLM as an evaluator to assess the consistency and accuracy of the generated answers compared to the ground truth answers (Yang et al. (2024)). We use this evaluator to score the responses on a scale of 1 to 10, and then normalize these scores to a range of 0 to 1.\nSemantic Similarity Scores(Ssimi). Semantic similarity is a common method for judging text sim- ilarity. By using an embedding model (Xiao et al. (2024)), we can directly calculate the vectors of discrete tokens mapped to a high-dimensional continuous space, and directly compute the similarity between text vectors through mathematical methods. This method provides another perspective to assess the accuracy between model responses and actual answers, and this score also reflects the comprehensive ability of the model in executing web search tasks.\nRelevance Scores(Srele). This metric primarily examines the model's faithfulness to the task execu- tion trajectory and its ability to summarize the overall context during task execution (Es et al. (2024)). The process for calculating the metric is as follows: based on the responses generated by the LLM, another evaluation LLM will generate multiple questions that are inferred from the responses, and then calculate the semantic similarity between these inferred questions and the originally given questions, then the maximum value is taken as the final score. This metric is based on a simple yet reasonable idea: if the original question can be inferred from the LLM's response, then that response should be clear and explicit. Notice that this metric does not utilize the ground truth answers, thus allowing it to focus entirely on the LLM's faithfulness to the overall task execution and its summarization capabilities.\nSearcher Count(Sc). This metric assesses the ability of LLMs to understand and break down questions. As introduced in Section 2, the subdivided sub-questions enter the Search Agent for parallel searching, and within the Level-Info Agent, the LLM can choose to call the web search API to gather information from the internet. We have counted the number of times the Level-Info Agent is invoked in each task and use the average number of invocations as an evaluation metric. Fewer invocations indicate that the LLM has fewer sub-problems to break down, demonstrating a strong understanding capability. Additionally, fewer invocations mean that the model processes tasks more quickly and also conserves the number of web search API calls, saving costs. When calculating the total score, we use exponential decay to map the number of searcher to a score for computation.\nUltimately, we express the total score (1-100) as a weighted sum of the aforementioned four metrics. We assign the highest weight to the F1 score to highlight its importance, while semantic similarity and relevance scores are given secondary weights. The searcher count is mapped to a 1-10 scale through exponential decay and assigned the lowest weight. The formula for calculating the total score is as follows:\n$S_{final} = 60 \\times S_{co} + 15 \\times S_{simi} + 15 \\times S_{rele} + 10 \\times e^{-S_c}.$"}, {"content": "In this Section, we present the experimental results on our benchmark in Section 4.1, demonstrate the limitations of traditional methods in the experiments of Section 4.3.\nWe utilized 14 models to operate Level-Navi Agent, encompassing open-source and closed-source models. a) Open-source. The open-source models we used primarily come from the Chinese community, including InternLM series (Cai et al. (2024)), GLM-4 (GLM et al. (2024)), Qwen series (Bai et al. (2023)) and Llama series (AI@Meta (2024)). b) Closed-source For closed-source models, we utilized ERNIE-3.5 from Baidu, Moonshot-v1 from Moonshot AI, and GPT-40 (Achiam et al. (2023)) from OpenAI. Note that Deepseek-V2.5 (DeepSeek-AI (2024)) is an open-source model, but due to its large parameter size, we call it in the form of an API. For the convenience of comparison, we have categorized it in the same group as other closed-source models that are also experimented with using the API."}, {"title": "Experimental Results of Our Agent on the Web24 Dataset", "content": "Table 2 and Table 3 presents the experimental results obtained by LLMs under our Level-Navi Agent framework and benchmark testing. From the data in the table, we can observe that Qwen2.5-72B and Deepseek-V2.5 performs the best. After summarizing and analyzing all the experiment results, we have analyzed and summarized several key points about the Web Search Agent:\nDiminishing Marginal Returns of Model Parameters. From the results obtained from the open- source models of the Internlm series, Qwen series, and Lllama series in Table 2, we observed an empirical pattern: within the same series of models, the larger the parameter count, the higher the final score. Focusing on the scores of the Qwen series models, we also observed the diminishing marginal returns of model parameter size. From 3B to 14B, an approximately fivefold increase in model size led to a performance improvement of about 6 points; from 14B to 72B, a similar fivefold increase in parameter size resulted in only a 3-point improvement. In the results of closed-source models in Table 3, we found that although the performance of closed-source models is quite good, Deepseek-V2.5 actually achieves the best among them, which is surprising considering that it is actually an open-source model called in the form of an API. Considering the parameter differences between Qwen2.5-72B and Deepseek-V2.5, this result is acceptable. From the analysis above, it can be seen that: To further enhance the performance of LLMs in executing web search tasks, researchers should focus on how to filter and obtain higher quality information sources, since that the diminishing marginal effect of model parameter is quite clear.\nFew-shot Prompts Enhance Pass Rates. For all models, we implemented three types of prompt methods: zero-shot, one-shot, and three-shot (Brown et al. (2020)). From the perspective of the model's scoring, it seems that different prompting methods do not have a significant impact on the model. Therefore, we calculated the pass rate to reflect the effect of different prompting methods. During the execution of tasks by the Agent, factors such as incorrect LLM output formatting and incorrect function calls can lead to the task not being completed. Hence, we calculated the number of errors for each evaluation and compared it with the total number of the dataset to derive the pass rate, reflecting the performance of the model in executing instructions. Chain of thought method and few-shot prompt combination have been proven effective in previous research (Liang et al. (2023), Ma et al. (2023)), this conclusion is also reflected in our experiments. From Table 2, it can be seen that the three-shot method significantly improved the pass rate of the Agent compared to the zero-shot approach. For some models, such as the Qwen series models with more than 7B parameters, the pass rate is close to 1 even under zero-shot conditions, which also reflects the superior performance of the models themselves. In general, we recommend providing few-shot prompts when executing agent tasks. This approach is not only simple and cost-effective, but also enhances the model's performance in various aspects.\nNative Chinese LLM Outperform in Chinese Context Tasks. In our test results, the performance of the Llama series was not very satisfactory, which was reflected in both the final scores and the pass rates. We speculate that this is because our Agent framework and dataset are built around Chinese text (Yuan et al. (2023), Zhang et al. (2023)). For LLMs like Llama, whose primary application scenarios are in English, this may result in a loss of performance, further elaboration on this aspect will be provided in Section 4.4.2. In earlier experiments, we tested other well-known open-source LLMs from the English community such as Gemma (Team et al. (2024)) and Mistral, but found that their fidelity to Chinese instructions was very poor\u2014they either failed to follow Chinese instructions or were unable to respond to users in Chinese. This finding highlights the importance of multilingual optimization for LLMs. At the same time, our experiments demonstrate, to some extent, the true capabilities and advantages of native Chinese LLMs in Chinese contexts."}, {"title": "Comparison with Other Products", "content": "We also compared our Agent with mature products on the market. We selected two well-known Chinese LLM service providers: Kimi\u00b9 from Moonshot AI and Doubao\u00b2 from ByteDance, while also including the renowned GPT-40 from OpenAI in the comparison. It is noted that all the above products have the ability to conduct web search. We randomly selected 100 examples from the web24 dataset to obtain answers on the product website, and our Agent used Qwen2.5-72b as the execution model.\nAs depicted in Fig. 5, we compared the correctness, relevance, and semantic similarity scores of four objects using a circular diagram. It can be seen from the Fig. 5 that there is not much difference among the three products provided by LLM service supplier. Kimi performs slightly better than the other products in three metrics, but there is no significant gap. Our Agent has reached the same level as commercial products in these three metrics, which is sufficient to prove the good performance of our framework. At the same time, any user can switch the model they use according to their own situation at any time, making it more flexible and cost-effective."}, {"title": "Analysis of Metrics", "content": "Limitation of Traditional Metrics. In Section 3.2, We briefly discussed the limitations of traditional evaluation methods, and next we will further elaborate on this point through detailed experiments and analysis.\nTable 4 presents the results of evaluating model responses using traditional methods. We use a Chinese tokenizer to tokenize the model's responses and the ground truth answers, and then calculate the recall and F1 scores using statistical methods. By comparing and analyzing the perfor- mance of LLMs with different parameter sizes on the same task, we discovered a counterintuitive phenomenon: the increase in model parameters did not universally lead to an improvement in F1 scores; instead, in some cases, we observed a decline in F1 scores. Concurrently, the recall scores exhibited a clear upward trend with the increase in model parameters.\nAfter examining the analysis model's response, we discovered the cause of this phenomenon: Web search is a complex and open-ended task. Although the answers we label already include the correct key information, the LLM might generate more comprehensive information related to the question according to the context in the end. For the LLM, doing so enriches the depth and credibility of the answer. However, in terms of F1 score, longer texts are more likely to lead to a mismatch between the score and the original answer, resulting in a decrease in the score. From the perspective of recall evaluation, the longer the relevant text generated by the LLM, the higher the score will be, assuming the ground truth answer remains unchanged. This is also reflected in Table 4. Similarly, for ROUGE score evaluation (Lin (2004)), the aforementioned reasons also prevent us from obtaining an accurate reflection of the LLM's capabilities from the scores. These phenomena all demonstrate the limitations of traditional metrics.\nThe effectiveness of our metrics. From Tables 2 and 3, it can be observed that using our metrics, the performance distribution of various models aligns with empirical knowledge and common sense. In terms of Correctness Scores, the advantage brought by model parameter size is clearly demonstrated. Meanwhile, Semantic Similarity Scores and Relevance Scores consistently reflect the capability differences among models. Through the overall scores, anyone can intuitively discern the performance differences between models. These findings strongly validate the effectiveness of our metrics.\nTherefore, we believe that traditional token-based evaluation methods cannot accurately reflect the quality of model responses when dealing with web search Q&A tasks that involve summarizing diverse information. At present, it seems that using human evaluation or LLM evaluation can better assess model responses that include rich relevant materials and different semantic expressions(Zhuge et al. (2024))."}, {"title": "Error Analysis and Discussion", "content": "In this section, we will conduct an error analysis through experimental data to identify the causes of poor model performance, and also provide corresponding suggestions on how to improve model performance."}, {"title": "Overconfidence Phenomenon in Web Search Function Usage.", "content": "In Table 2, the scores of GLM-4-9B are lower than expected. We uncovered the reason for this phenomenon by calculating the average Search Count invoked per task and the actual number of Web Search Function calls. In Fig. 6, we compared the gap between GLM-4 and Qwen2.5-7B on the aforementioned metrics. We observed that Qwen2.5-7B's function call rate reached about 90% of the Agent invocations, while GLM-4-9B's ratio dropped from 30% to a single-digit percentage. Given that 70% of the answers in the dataset come from news sources, GLM-4-9B cannot possibly answer correctly without invoking web searches. We refer to this phenomenon as \u201cOverconfidence\"Huang et al. (2024).\nThe issue indicates that LLMs might overestimate their question-answering abilities during training, overlooking the need for external resources in certain situations Xiong et al. (2024). To address this overconfidence, we recommend that developers balance positive and negative examples in the training dataset to improve LLMs' function-calling capabilities."}, {"title": "Low Task Fidelity in Conducting Chinese Tasks.", "content": "When assessing LLM agents, we prioritize whether the model comprehends and adheres to in- structions to answer questions. We call it \u201cTask Fidelity\u201d, which reflects the model's faithfulness in executing instructions. The Relevance Scores do not take into account the correctness of the model's response, so it can reflect the end-to-end Task Fidelity.\nIn Table 2, the relevance scores of Llama3.1-8B did not behave as expected compared to other models; instead, they fluctuated significantly. Upon examining the outputs of the Llama 3.1 series models, we found that a considerable portion of the responses did not fully comply with our instructions, with some incorrectly mixing the given instructions with the answers. Table 5 more detailedly reflects this type of non-compliant response. The introduction of the few-shot method did not improve this issue for Llama3.1-8B, only Llama3.1-70B showed improvement.\nWe view non-compliant responses as indicating low Task Fidelity. The LLM's struggle to grasp the intent of instructions in lengthy Chinese contexts can greatly impair task performance. Developers should focus on ensuring smaller LLMs to maintain the multilingual ability as bigger ones."}, {"title": "Conclusion", "content": "In this paper, we introduce the Level-Navi Agent and a novel benchmark for web search tasks. The Level-Navi Agent offers an innovative solution to web search challenges through the collaboration of multiple agents and a hierarchical approach to reasoning and searching. Starting from the Chinese open-source community, we employed new evaluation metrics and methods to comprehensively assess the performance of various LLMs in executing web search tasks. This analysis sheds light on the true capabilities of current LLMs when performing web search tasks within the Chinese Internet. Through data-driven error analysis, we identify the limitations of LLMs in handling web search tasks and provide recommendations for improvement, contributing to the advancement of this field."}]}