{"title": "Solving Robotics Problems in Zero-Shot with Vision-Language Models", "authors": ["Zidan Wang", "Rui Shen", "Bradly Stadie"], "abstract": "We introduce Wonderful Team, a multi-agent visual LLM (VLLM) framework for solving robotics problems in the zero-shot regime. By zero-shot we mean that, for a novel environment, we feed a VLLM an image of the robot's environment and a description of the task, and have the VLLM output the sequence of actions necessary for the robot to complete the task. Prior work on VLLMs in robotics has largely focused on settings where some part of the pipeline is fine-tuned, such as tuning an LLM on robot data or training a separate vision encoder for perception and action generation. Surprisingly, due to recent advances in the capabilities of VLLMs, this type of fine-tuning may no longer be necessary for many tasks. In this work, we show that with careful engineering, we can prompt a single off-the-shelf VLLM to handle all aspects of a robotics task, from high-level planning to low-level location-extraction and action-execution. Wonderful Team builds on recent advances in multi-agent LLMs to partition tasks across an agent hierarchy, making it self-corrective and able to effectively partition and solve even long-horizon tasks. Extensive experiments on VIMABench and real-world robotic environments demonstrate the system's capability to handle a variety of robotic tasks, including manipulation, visual goal-reaching, and visual reasoning, all in a zero-shot manner. These results underscore a key point: vision-language models have progressed rapidly in the past year, and should strongly be considered as a backbone for robotics problems going forward.", "sections": [{"title": "Introduction", "content": "The vision of robots comprehending and performing complex tasks based solely on natural language instructions, without any prior training, is becoming increasingly attainable due to advancements in Large Language Models (LLMs) and Vision Large Language Models (VLLMs). These models integrate vision and language, enabling robots to intuitively understand their environments, leveraging real-world priors obtained from the large-scale data used to train the VLLMs. Even so, developing a general-purpose robotic system capable of executing complex tasks in dynamic settings remains a significant challenge, requiring the ability to perceive surroundings, utilize appropriate skills, and achieve long-term subgoals. However, the continually improving capabilities of VLLMs in-context-understanding and spatial-reasoning provide a promising foundation. These models excel in zero-shot scenarios across various domains. This raises a crucial question: Can these models be adapted to solve robotic tasks in unstructured environments without any training?\nCurrent work in language-conditioned robotics models has often restricted LLMs and VLLMs to high-level planning roles, leaving action generation to pre-defined task modules or APIs [1, 2, 3] or"}, {"title": "Related Work", "content": "Foundation Models Trained on internet-scale datasets, foundation models have demonstrated strong zero-shot ability in various tasks. LLMs and VLLMs are prime examples. LLMs like GPT-3 [18], LLaMA [19], and ChatGPT have excelled in generating human-like text, understanding natural language instructions, and performing extensive reasoning and planning. In this paper, we investigate how those planning and reasoning abilities can be used to benefit robotics.\nEmpowered by Language Models Prior work in robotics has shown that natural language in-structions can help agents learn and adapt to new tasks. However, these earlier approaches either: 1) equipped agents with learned language embeddings, which require collecting large amounts of training data [20, 13]; 2) focused on connecting language instructions with low-level action primitives to solve long-horizon tasks [1, 2, 3]. These methods often struggle with adapting to new tasks. To improve generalizability, recent research has explored foundational transformer-based models like RT-1 [21] and RT-2 [22], which have laid the groundwork for versatile robotic systems. Despite their advancements, these models still require significant training across diverse tasks.\nZero-shot and Few-shot Approaches Recent works [23, 3, 2, 7, 10, 14, 24, 25] have explored zero-shot and few-shot solutions for robotics planning and manipulation tasks. While these approaches can handle unseen scenarios without any training, they focus on high-level planning and often rely on pre-defined programs to manage control. Concurrent to our work, Mikami et al. [26]"}, {"title": "Methodology", "content": "Let's first consider the simplest robotic vision-language interface, which was our initial approach in this investigation. First, an image of the robot's current environment observation is captured. Then, tick marks with are appended to the borders of the X and Y axes of this image, defining a coordinate system for the task in pixel space. For example, if we have a 100 \u00d7 100 image and define our coordinate space to start at (0,0), then the upper right corner of the image would be (100, 100). We aim to feed this augmented image into a VLLM, along with a description of the task we wish to\nsolve. The goal is for the VLLM to solve the task by outputting a list of target coordinates necessary for task completion. For instance, given the task of moving a book from a table to a shelf, we would like the response to be: \"First, move the arm to (20, 30). Close the gripper to pick up the book at that location. Next, move the arm to (50, 50). Release the gripper to set the book onto the shelf.\" Crucially, the VLLM observes the coordinate reference frame we placed on the image and defines its plan in terms of these reference coordinates.\nIf we execute this strategy naively, several issues occur. First, the pixel-based coordinate reference frame we define is not precise enough to effectively solve many tasks. Second, the system struggles to define the location of objects in a scene. Third, hallucinations lead to compounding errors, from which the system cannot recover. However, we were quite surprised to learn that despite these issues, the system is still fundamentally capable of identifying the locations of objects on our pre-defined spatial grid. The errors the system makes are not due to a lack of understanding of the task, but rather to fine-grained errors related to measurement precision and instability.\nTo fix the many issues associated with the naive approach, we propose a multi-agent hierarchical system for robotic planning via VLLMs. This system, which we call Wonderful Team, is detailed below.\n\u2022 Input: Environment RGB array, text-only or multi-modal instructions\n\u2022 Supervisor: The main objective of the supervisor is to collect information about the environment and use it to send executable actions back to the robot. The supervisor first receives the input data. It processes the instructions, summarizes all task requirements, and devises a plan for solving the task. This plan is a sub-goal checklist, an ordered list of sub-goals the agent needs to accomplish to complete a task. This checklist can be vague initially, but it will be refined as the supervisor gathers more data.\nAfter forming a plan, the supervisor then decides which of its workers should be called to help complete the next step on the sub-goal checklist. Each worker executes a task related to object identification or sub-planning and then passes its results back to the supervisor. The supervisor takes all of this information and uses it to create executable actions, which it passes to the robotics environment. The robotics environment executes these actions and sends visual feedback (images of the current state) to the supervisor, which can be used for re-planning.\n\u2022 Verification Agent: Verifies the integrity of the plan by ensuring all objects that need to be moved are accounted for, that the plan lists out target locations, that it fully specifies what actions to take, and looks for obstacles that might hinder execution.\n\u2022 Memory Agent: Helps with specific implementation details related to memory in long-horizon tasks. For example, some tasks might involve rearranging objects in a scene and then resetting the scene back to its original state. Sometimes target locations are occupied by other objects. So the plan first needs to move these blocker objects to intermediate locations so that the target objects can slot appropriately. But then during the restore phase, the blocker objects need to be moved back to their original location. This agent helps the Supervisor remember these implementation details.\n\u2022 Grounding Team Manager: The grounding team is responsible for identifying the location of objects in the environment and suitable action points for the robot. The manager is knowledgeable about the overall context and tasks. For each target of interest, it will give this team the target name and an approximate location to be used as a starting point and ask its team members to provide an exact bounding box. Upon receiving the bounding box, the manager selects an action point where the robot should move to accomplish a task. For instance, pick-and-place tasks require an action point on the object, while sweeping tasks require an action point on the opposite side of the target location. This team can be thought of as a low-level coordinate finder that converts sub-goal plans into specific action points.\n\u2022 Grounding Team Tools: A set of visualization tools is provided to the grounding team, enabling team members to better understand any revision needs. For instance, it's very challenging for a human to grasp how to improve a bounding box solely by viewing the numeric box vertex information and understand whether the bounding box is adequate. However, if the box is drawn on a plot, this becomes clear immediately. Several visualization tools like this are provided for the grounding team to use. A full list of tools and descriptions is provided in the supplementary material.\n\u2022 Box Proposal Agent: Given the name of an object, this agent proposes a bounding box around the object. Along with the Box Checking Agent, these workers are detail-oriented and do not need to understand the entire task or upper-level plans. They are designed to focus solely on coordinate accuracy. This agent can also resize or move existing boxes.\n\u2022 Box Checking Agent: Checks whether the new boxes proposed by the Box Proposal Agent are actually better than the previous boxes. The Box Proposal Agent and Box Checking agent usually work in an iterative loop, refining the proposed bounding box until no further modifications are needed.\nA concrete example of this pipeline in-action is provided in the supplementary material.\nCrucially, no part of Wonderful Team has been fine tuned in any way. We use off-the-shelf GPT-40 model for each agent. The agents are simply powerful enough to solve these tasks without any additional training. The ability to solve such tasks with off-the-shelf VLLMs is a relatively recent trend. See Figure 5, which looks at the performance of our system with previously released VLLMs."}, {"title": "Experimental Results", "content": "To evaluate our approach, we conducted experiments on VIMABench [13], a recently proposed simulation benchmark designed to assess multi-modal robot learning agents. VIMABench offers a suite of 17 representative tabletop manipulation tasks that encompass a wide range of skills, incorporating numerous combinations of objects, textures, and initial configurations. The benchmark includes 29 objects with 17 types of RGB colors and 65 types of image textures, some of which are rarely seen in other robotics tasks, making them particularly valuable for testing our approach. We chose VIMABench because, unlike traditional simulation or real-world environments that mostly include easy-to-detect objects such as single-colored blocks or bowls, fruits, and other common items, VIMABench presents a significant variety of objects and textures (see Figure 2). Many of these objects require advanced scene understanding and careful planning for successful manipulation. Furthermore, VIMABench includes multi-modal prompts, featuring images of objects alongside textual instructions, which create a more complex and realistic testing environment. Additionally, many tasks in VIMABench require reasoning and long-horizon planning.\nWe conducted experiments on all 17 tasks in VIMABench. Full details about all 17 task types and their success rates are discussed in the supplementary materials. For the main paper, we adhere to the traditions established in the literature and choose to concentrate on four main task suites as defined by [13]:\nSimple Object Manipulation: Tasks like \u201cput (object) into (container)\u201d, where each image in the prompt corresponds to a single object."}, {"title": "Further Discussions", "content": "We compare Wonderful Team against the following baselines: 1) Natural Language as Policies [26]. This is the closest method we could find to ours in the literature, which uses one-shot learning on the same in-distribution task throughout policy execution. 2) Natural Language as Policies variant without chain-of-thought. 3) Wonderful Team, but without the grounding team. Instead, we use a single supervisor agent to infer the coordinates of objects relative to the image's pixel reference frame. 4) Wonderful Team, but we replace the grounding team with a common vision model from the literature.\nConsistently, we see that the presence of the grounding team and multi-agent structure is essential for success. Essentially, most of the time, the failure mode is that a separate vision-language model without reasoning ability cannot detect any less-common, obscured objects in VIMABench, especially in cluttered or dynamic environments. These limitations frequently result in misinterpretations, which can hinder functionality and restrict the deployment of autonomous systems."}, {"title": "Further Discussions on Motivation", "content": "Our primary motivation is to explore how we can best make use of the zero-shot capabilities of powerful VLLMs to solve robotic tasks. While previous works have combined different models or focused on training and fine-tuning specific components, our work focuses on exploring whether a single, powerful VLLM can be effectively employed in robotic tasks without any fine-tuning or training,\nAlthough it's currently not feasible to simply provide an environment image and receive a precise sequence of executable actions for task completion in a single step, VLLMs have shown the ability to generate plans and approximate the locations of objects in images. The question is: How do we leverage these capabilities of VLLMs to solve more complex tasks? Our solution is to divide and conquer, focusing on breaking down the problem into smaller, manageable components.\nLet's break it down:\nFor successful task execution using founding models wihtout training on task-specific datasets, we need two main components:\n\u2022 A Correct Plan: For instance, \"1. Pick up the apple, 2. Put it in the box, 3. Pick up the banana, ...\".\n\u2022 Accurate Actions: Each part of the plan must correspond to an accurate sequence of actions, ensuring that the subgoals can be effectively reached.\nPrevious research has shown that LLMs are proficient at planning. However, generating accurate action sequences using LLMs is challenging due to the variability in robot controls. Different robots have distinct degrees of freedom and control mechanisms, making it difficult to instruct them without training or fine-tuning. To create a zero-shot framework that works effectively without additional fine-tuning, it is more effective to focus on locating positions and using environment-specific position control rather than generating specific actions.\nIn many real and simulated environments, position control is straightforward and ensures accurate movement between two positions. Even in environments with more complex physics, the relative relationships between objects and their corresponding actions remain consistent. For example, if object X is to the right of object Y, the action of moving the robot from X to Y is always moving to the left relative to the starting position.\nThus, even when this mapping is not perfect, a set of actions can be generated first and further refined through re-planning. Therefore, we break down the problem of action generation into position finding and position control."}, {"title": "Limitations: Where does Wonderful Team struggle?", "content": "Throughout this work, we have identified several challenges that could be addressed for further improvement:\nPartial Observability: Vision-language models still struggle with partial observability. For instance, when an object is partially or fully obscured by another, the model fails to understand the spatial relationships and may hallucinate the actions required to arrange the objects in their stacked state. These failure modes are largely due to the inherent limitations of VLMs in perceiving oc-cluded objects. Even with observability-specific prompting and explicit teaching of the concept of hidden objects, the models still fail to accurately interpret the scene.\nConsecutive Frame Understanding: While our framework can interpret multi-modal task instructions effectively, it faces challenges with more complex image prompts, such as consecutive frames each depicting multiple objects in different locations. VLMs find it difficult to accurately discern all differences and changes between consecutive frames. This often leads to compounded hallucinations when processing too many images simultaneously. Developing an effective yet generalizable method for breaking down such tasks into smaller, more manageable pieces could help mitigate this issue.\nWe further discuss the limitations of our framework with additional examples in the supplementary materials."}, {"title": "Appendix A: Improvements to the grounding team over time", "content": "Figure 13 shows the ability of Wonderful Team with VLLMs to generate at least one valid sub-goal, which shows the system is working to some extent but perhaps lacks more refined planning ability.\nPreviously, it was evident that earlier VLLMs, before GPT-40, struggled with finding actionable bounding boxes using a pixel coordinate system. The output coordinates were often not even in the same quadrant as the target object. However, by breaking down the task into a multi-agent system, this process does not need to be accomplished in a single step. If the model can effectively evaluate the quality of a bounding box or propose revisions, this task can be achieved iteratively in a multi-agent system.\nThe table in Figure 14 shows the performance of different models - gpt-4o, gpt-4-turbo, and Claude 3 opus against the ground truth in a bounding box acceptance task. Interestingly, although generating a single-step coordinate is not possible for earlier models, they show some understanding of the environment and can evaluate a bounding box's correctness better than creating one from scratch. This suggests that breaking the problem down into smaller, manageable tasks can be helpful.\nDespite these advancements, hallucinations remain a significant issue. These hallucinations can compound within a multi-agent system, hindering its overall performance and reliability. This simple example exemplifies a portion of the workflow for the checking agent in the grounding team. If the checking agent hallucinates when a bad bounding box is provided and mistakenly deems it perfect, it will be outputted for action generation and result in execution failure. Conversely, if it keeps rejecting a good bounding box, it will confuse the proposal agent, leading to compounding hallucinations. This explains why, although the proposed framework significantly improves over single-shot coordinate generation, it still couldn't achieve good results when using earlier models."}]}