{"title": "CATCH: Complementary Adaptive Token-level Contrastive Decoding to Mitigate Hallucinations in LVLMS", "authors": ["Zhehan Kan", "Ce Zhang", "Zihan Liao", "Yapeng Tian", "Wenming Yang", "Junyuan Xiao", "Xu Li", "Dongmei Jiang", "Yaowei Wang", "Qingmin Liao"], "abstract": "Large Vision-Language Model (LVLM) systems have demonstrated impressive vision-language reasoning capabilities but suffer from pervasive and severe hallucination issues, posing significant risks in critical domains such as healthcare and autonomous systems. Despite previous efforts to mitigate hallucinations, a persistent issue remains: visual defect from vision-language misalignment, creating a bottleneck in visual processing capacity. To address this challenge, we develop Complementary Adaptive Token-level Contrastive Decoding to Mitigate Hallucinations in LVLMS (CATCH), based on the Information Bottleneck theory. CATCH introduces Complementary Visual Decoupling (CVD) for visual information separation, Non-Visual Screening (NVS) for hallucination detection, and Adaptive Token-level Contrastive Decoding (ATCD) for hallucination mitigation. CATCH addresses issues related to visual defects that cause diminished fine-grained feature perception and cumulative hallucinations in open-ended scenarios. It is applicable to various visual question-answering tasks without requiring any specific data or prior knowledge, and generalizes robustly to new tasks without additional training, opening new possibilities for advancing LVLM in various challenging applications.", "sections": [{"title": "1 Introduction", "content": "Large Vision-Language Models (LVLMs) have achieved significant advancements in areas such as visual question answering [1] and embodied intelligence [2], owing to their remarkable potential to integrate and interpret both visual and linguistic information. However, hallucinations in LVLMs, referring to the generation of textual content that is inconsistent with the visual input, remain a pervasive issue. Therefore, substantial risks are posed, particularly in high-stakes domains such as healthcare [3] and autonomous systems [4], where erroneous decisions potentially lead to severe consequence.\nHallucinations in LVLMs primarily arises from an excessive dependence on training data, limited real-world comprehension, and a over-reliance on linguistic information due to the Large Language Model (LLM)-centric architecture of the existing models in vision-language reasoning [5]. Despite the prevalence of hallucination issues across all existing LVLMS, research dedicated to mitigating this problem remains scarce. One of the primary contributors to hallucination is the quality of data. Recent efforts have focused on addressing this issue by introducing negative data [6], counterfactual data [7], and reducing noise and errors within existing datasets [8, 9]. Given the imbalance in vision-language reasoning introduced by the LLM-centric architecture, some approaches have sought to enhance the model's visual reasoning capabilities by increasing resolution [10\u201313] or incorporating more advanced vision encoders [14\u201316]. Furthermore, several methods have optimized decoding strategies. For instance, HALC [17] employs Grounding DINO to resample images, thereby enhancing LVLMs' perceptual sensitivity to fine-grained targets. Other approaches, such as VCD [18] and M3ID [19], introduce contrastive decoding, which involves comparing the original image with either a noise-added version or text-only input, aiming to reduce the models' vulnerability to language priors.\nPrevious research typically attributes the causes of hallucinations to two primary factors: (1) statistical bias, unbalanced object distribution and biased object correlations in textual information, (2) language bias, overlook visual evidence and overly exploit language priors for decision-making [18]. However, we observe that hallucinations in LVLMs arise from an inability to comprehensively process visual information, rendering them unaccounted for by the two factors mentioned above. We first illustrate this with the example as shown in Fig. 1, we decouple visual input to seven levels by utilizing the Segment Anything Model (SAM) [20] to segment the original visual input with different numbers of target objects. The LVLM has generated the ongoing response, \"One person on the left side is holding a ...\". Initially, it hallucinates by predicting \"phone\" instead of \"sandwich\u201d. As irrelevant visual features are reduced, the"}, {"title": "2 Results", "content": "Results on POPE. As shown in Table 1, we evaluate our CATCH method on the POPE dataset [24]. POPE assesses hallucinations as a binary classification task by asking yes/no questions about object presence (e.g., \u201cIs there a dog in the image?\u201d). This benchmark aggregates data from three sources: MSCOCO [21], A-OKVQA [27], and GQA [28], and includes three subsets: random, popular, and adversarial, which address object prevalence and co-occurrence patterns. Each sampling setting uses 500 images per dataset, with 6 questions per image, resulting in a total of 27,000 query-answer pairs derived from the development sets. Evaluation is based on four key metrics: Accuracy, Precision, Recall, and F1 score.\nWe observed that our CATCH method outperforms the current best by a significant margin. Compared to the baseline, it improves Accuracy and F1 score by up to 8.07 and 5.98 points, respectively, demonstrating its effectiveness in mitigating hallucinated concepts present in the original distribution.\nIn addition, it is worth noting that while all methods exhibit a clear performance decline from the random to the popular setting, with a further drop in the adversarial setting, CATCH demonstrates superior performance in these more challenging scenarios. It improves accuracy by 6.65, 8.21, and 8.36 points over the baseline in the random, popular, and adversarial settings, respectively. These results reveal that more challenging tasks amplify the visual defects in LVLMs, whereas CATCH effectively reduces the density of extraneous visual information, preventing reliance on language priors. We also observed that, compared to Recall, CATCH achieved a more significant improvement in Precision, which can be attributed to its lower \"yes\" response ratio compared to the baseline. This suggests that CATCH is more conservative and stable when handling uncertain responses."}, {"title": "3 Methods", "content": "We consider an LVLM parameterized by 0. The model receives a textual query x and a visual input v, where v provides contextual visual information to aid the model in generating a relevant response y to the query. Initially, the raw image v is processed through a vision encoder to extract visual features. These features are then mapped into the input space of the large language model through a vision-language alignment module (e.g., Q-Former [29], linear projection [12]), generating visual tokens. Subsequently, these visual tokens, together with textual tokens obtained through embedding the query, are fed into the large language model to auto-regressively generate the response. Mathematically, this process can be formulated as follows:\n$y_t \\sim P_\\theta (y_t | v, x, y_{<t}) \\propto \\exp{\\logits_\\theta(y_t | v, x, y_{<t})}$,\nwhere yt denotes the token generated at time step t, and y<t represents the sequence of tokens generated up to time step t \u2212 1. During the decoding phase in LVLMs, hallucinations arise when the generated probability distribution deviates from the factual information provided by the visual input v. To mitigate hallucinations arising from visual defect, we propose the Complementary Visual Decouplin (CVD) for visual information separation, Non-Visual Screening (NVS) for hallucination detection, and Adaptive Token-level Contrastive Decoding (ATCD) for hallucination mitigation."}, {"title": "3.1 Formulation of LVLMs"}, {"title": "3.2 Complementary Visual Decoupling", "content": "The visual defect fundamentally arises from the unbalanced alignment in vision-language multimodal integration, leading to a visual information bottleneck. Inspired by the information bottleneck principle, which addresses this issue by removing irrelevant information and preserving only information pertinent to the current prediction to create a more stable representation, we propose the Complementary Visual Decoupling (CVD) method to preserve essential visual information while removing extraneous details.\nAssuming we have raw information v and label y, the core idea of the information bottleneck principle is to map the observation of v to a robust representation z, which retains the essential characteristics needed for predicting y while simultaneously minimizing redundant information. Theoretically, treating v, z, and y as three random variables, the optimization objective is to maximize the mutual information between z and y, while minimizing the mutual information between z and v. This optimization objective can be defined as:\n$\\min [I(v; z) \u2013 I(z; y)]$.\nIn this paper, the raw information v represents the visual input, and the label y represents the accurate response. We employ the Segment Anything Model (SAM) to optimize this objective. Specifically, as illustrated in Fig. 2, a pre-trained SAM is"}, {"title": "3.3 Non-Visual Screening", "content": "At each generation step, key visual features for the next token are dynamically emphasized in one part while being obscured in the other. The decoupled image z is designated as the one that retains the highlighted key visual features, effectively eliminating extraneous details. We observed that when critical visual information relevant to the next token is obscured, the output distribution becomes dominated by language priors. Thus, to identify the correct decoupled image z between the dual image and the residual image, we introduce a non-visual input zn, containing only the textual prompt, without any visual information, to serve as an assistant. We then calculate the Jensen-Shannon Divergence (JSD) as the distance between the output distributions from the non-visual input and the dual image as $d(z_d, z_n)$, and between the non-visual input and the residual image as $d(z_r, z_n)$:\n$d(z_d, z_n) = D_{JS} (P_\\theta(y_t | z_d, x, y_{<t}) || P_\\theta(y_t | z_n, x, y_{<t})),$ \n$d(z_r, z_n) = D_{JS} (P_\\theta(y_t | z_r, x, y_{<t}) || P_\\theta(y_t | z_n, x, y_{<t})),$ \nwhere $D_{JS}(P||Q) = \\frac{1}{2} D_{KL}(P||M) + \\frac{1}{2} D_{KL}(Q||M)$, $M = \\frac{1}{2} (P + Q)$, and $D_{KL}(P||Q) = \\sum_i P(i) \\log \\frac{P(i)}{Q(i)}$. The visual input corresponding to the greater distance is selected as the decoupled image z, formulated as:\nz = \\begin{cases}\nz_d, & \\text{if } d(z_d, z_n) \\geq d(z_r, z_n);\\\\\nz_r, & \\text{if } d(z_d, z_n) < d(z_r, z_n).\\\\\n\\end{cases}"}, {"title": "3.4 Adaptive Token-level Contrastive Decoding", "content": "We first calculate the distance between the output distributions from the non-visual input and the decoupled image, denoted as $d(z, z_n)$, and the distance between the"}, {"title": "3.5 Hardware and Implementtation Details", "content": "All experiments in this paper were conducted on an NVIDIA RTX 3090 24GB GPU. For SAM, we utilized the pre-trained ViT-H SAM model. The decoding process was configured with \u03b1 = 1.2 and \u03b2 = 3 by default. The number of objects in CVD was set to be N * 0.05 by default."}, {"title": "4 Discussion", "content": "In this paper, we reveal that hallucinations in LVLMs primarily stem from the visual processing bottleneck caused by vision-language misalignment, which we term the visual defect. Unlike previously identified issues of language bias and statistical bias, the visual defect is challenging to resolve through high-quality data, chain-of-thought prompting, or fine-tuning, due to limitations in the pre-trained visual architecture. Based on the Information Bottleneck theory, we introduce Complementary Visual Decoupling (CVD) for visual information decoupling, incorporate a non-visual input for Non-Visual Screening (NVS) to detect hallucinations, and propose Adaptive Token-level Contrastive Decoding (ATCD) for hallucination mitigation through contrastive decoding.\nCATCH demonstrates impressive performance and robustness in perceiving fine-grained features and mitigating cumulative hallucinations in open-ended scenarios, as evidenced by the results on the CHAIR, POPE, and MME datasets across different baselines. One exciting aspect of CATCH is that it requires no additional prior knowledge, data, or training, making it efficiently applicable to various LVLMs. We hope that introducing the concept of visual defects opens new research avenues for studying hallucinations and that our complementary contrastive decoding method inspires new questions and approaches in the pretraining and fine-tuning of LVLMs."}]}