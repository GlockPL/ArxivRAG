{"title": "A generative approach to LLM harmfulness detection with special red flag tokens", "authors": ["Sophie Xhonneux", "David Dobre", "Mehrnaz Mohfakhami", "Leo Schwinn", "Gauthier Gidel"], "abstract": "Most safety training methods for large language models (LLMs) based on fine-tuning rely on dramatically changing the output distribution of the model when faced with a harmful request, shifting it from an unsafe answer to a refusal to respond. These methods inherently compromise model capabilities and might make auto-regressive models vulnerable to attacks that make likely an initial token of affirmative response. To avoid that, we propose to expand the model's vocabulary with a special token we call red flag token ((rf)) and propose to fine-tune the model to generate this token at any time harmful content is generated or about to be generated. This novel safety training method effectively augments LLMs into generative classifiers of harmfulness at all times during the conversation. This method offers several advantages: it enables the model to explicitly learn the concept of harmfulness while marginally affecting the generated distribution, thus maintaining the model's utility. It also evaluates each generated answer rather than just the input prompt and provides a stronger defence against sampling-based attacks. In addition, it simplifies the evaluation of the model's robustness and reduces correlated failures when combined with a classifier. We further show an increased robustness to long contexts, and supervised fine-tuning attacks.", "sections": [{"title": "1. Introduction", "content": "To make large language models (LLMs) that are robust against a determined adversary, practitioners rely on several security layers such as model hardening via fine-tuning (Xhonneux et al., 2024; Sheshadri et al., 2024; Zou et al., 2024), perplexity filters (Alon & Kamfonas, 2023), or harmfulness classifiers (Inan et al., 2023). However, as model capabilities progress, so does their attack surface as the innate abilities can be used to circumvent defences (Huang et al., 2024)\u2014e.g., using a low-resource language to jailbreak the model. Hence, to enable robustness mechanisms to keep pace with the new vulnerabilities, it is natural to embed them into the models themselves, which will scale with these capabilities. Ideally, an LLM would have no harmful faculties; however, this is unrealistic as many desirable skills can be both useful and damaging depending on the context. Alternatively, harmful abilities may be re-learned based on harmless ones. Therefore, we argue that the model itself should detect when it is being used harmfully and that this behaviour can be complementary to standard safety training whose goal is to remove harmful faculties from the model.\nTo address the aforementioned issues, we propose to add a special token for the LLMs to predict when the model considers that its capabilities are used unsafely. We call this token red flag token ((rf)) and train the model to output this token at any time during the generation of a harmful response while not changing its response after or in unrelated contexts. This complementary layer of safety has several benefits. First, our method only requires the (rf) to be included in the answer a single token change. In contrast, other methods, such as adversarial training, for instance, forces models to completely change their output distribution from a harmful response to a refusal. Thus, while standard safety training creates a tension between the probability of refusing harmful queries and the standard training objectives, such as next token prediction and instruction following (Wei et al., 2023), our safety training objective only requires a minor edit-i.e., we learn to output the (rf) in the useful, though unsafe, answer the model provides. Finally, one can calibrate the strictness of flagging a generation as harmful with a bias term on the (rf) in the final softmax of the model.\nSecondly, our method does not rely on a specific safe answer, meaning that if the model is broken, e.g., through pre-filling (Andriushchenko et al., 2024) or random sampling (Huang et al., 2023), we can still output a (rf) to tag the answer as harmful and have it be filtered. This conceptually is a detection mechanism that is built into the model, i.e., we re-use the model's capabilities. This enables our method to be complimentary and to be used together with other hardening methods, such as adversarial training."}, {"title": "2. Method", "content": "In this section, we present how we train the model to output the (rf) during harmful continuations by only marginally affecting the model's output distribution (thus maintaining capabilities). First, we explain the threat model we consider (Section 2.1) before introducing the notation, the datasets, and the loss we will use (Section 2.2). The loss is conceptually depicted in Figure 1."}, {"title": "2.1. Threat model", "content": "Similar to a setting where harmfulness classifiers may be used, we assume that the LLM access is gated behind some web interface or API with no access to model weights, logits, or direct control of input/output processing-we call this the black box setting. The key assumption we make is that the service provider can evaluate the logits and output of the model before passing it on to the user, including the filtering of special tokens such as assistant tokens or our (rf) token. We further consider a more permissive gray box setting where the user may have access to extra features, including pre-filling and or viewing the logits of non-special tokens. Finally, we consider the most permissive setting, a fine-tuning attack setting where the user has access to some fine-tuning API. We do not consider our method to be applicable in white-box settings as a harmful continuation can be used whether it is flagged or not."}, {"title": "2.2. Our Loss", "content": "We assume that we have a dataset $(x, y) \\sim D_{\\text{harmful}}$ of harmful pairs or prompts $\\hat{x}$ and continuations $\\hat{y}$. Further, we assume we have a dataset $(x, y) \\sim D_{\\text{harmless}}$ of harmless (a.k.a., benign) pairs of prompts $x$ and harmless continuations $y$. Given $k \\ge 0$ representing a minimum offset, an index $i$ is sampled from a probability distribution $P_k$ over the indices $\\{k, ..., |\\hat{y}|\\}$ of the continuation at which to insert the red flag token into the harmful continuation $\\hat{y}$. We get the tokens $\\hat{y}_{<i}$ which come before index $i$, the (rf) token, and the tokens $\\hat{y}_{>i}$. We use $L_{CE}$ to denote the cross entropy and $D_{KL}$ to denote the Kullback-Leibler divergence (KL).\nWe consider our reference model $\\pi_{\\theta_0}$. Our loss consists of three components: First, to ensure our model outputs the red-flag token in harmful completions, we use a standard language modelling cross-entropy loss on all harmful completion tokens starting at the minimum offset $k$ up to and including the (rf) token:\n$L_{rfCE} := \\sum_{k<j<i} -\\log \\pi_{\\theta}(\\text{rf}|\\hat{x}, \\hat{y}_{<j}).$  (1)\nTo maintain model performance and reduce distribution shift as much as possible without increasing the likelihood of a harmful answer, we use a KL divergence on the tokens after the (rf)\n$D_{rf}:=D_{KL}(\\pi_{\\theta}(\\hat{y}_{>i}|x, \\hat{y}_{<i}, (\\text{rf}))|\\pi_{\\theta_0} (\\hat{y}_{>i}|x, \\hat{y}_{<i})),$ (2)\nand again, to reduce distribution shift and to capture that the likelihoods should not change on unrelated tasks we include a KL loss on benign prompts and answers\n$D_{benign} := D_{KL}(\\pi_{\\theta}(y|x) | \\pi_{\\theta_0}(y|x)).$ (3)\nAll these losses put together, we get:\n$L_{final} := \\alpha_{benign}D_{benign} + \\alpha_{rf}D_{rf} + \\alpha_{CE}L_{rfCE}.$ (4)"}, {"title": "3. Experiments", "content": "We fine-tune LLAMA3.2-3B-IT (Grattafiori, 2024), MISTRALV3-IT (Jiang et al., 2023), and PHI-3.5 (Haider et al., 2024) using our algorithm on the Harm-bench (Mazeika et al., 2024) training set with their refusals and 32 harmful continuations sampled for each harmful prompt using the ablation attack (Arditi et al., 2024). We use 5,000 samples from the Alpaca dataset (Taori et al., 2023) as benign prompts. All the models listed have as part of their tokenizers a set of reserved special tokens. Thus, allowing us to avoid extending the vocabulary and instead we re-purpose one of these unused special tokens to be the (rf).\nWe measure the utility of trained models using MMLU (Hendrycks et al., 2021), ARC-E and ARC-C (Chollet, 2019), which are standard LLM benchmarks as well as a dataset of benign prompts. This dataset of benign prompts consists of 119 prompts from ULTRACHAT200K (validation split) that we randomly picked (solely making sure that they were good quality), and the Harmless dataset consisting 40 benign prompts with a similar syntax as Harmbench provided by (Xhonneux et al., 2024, Appendix I). We also use this benign dataset to compute the false positive rate of refusal (or (rf)) in Figure 5.\nFor adversarial robustness evaluation, we compute the de-"}, {"title": "3.2. Design decisions", "content": "There are several design decisions to consider:\nThe cross-entropy loss on \u3008rf) can be computed on each index before and including the sampled position j or only on j. In other words, we have the choice to allow the model for flexibility of when to output (rf) at the cost of potentially overfitting more because we now train the model to output (rf) immediately after the instruction token. In particular, this forces the model to judge the prompt quite strongly, leading to a higher probability for (rf) in a refusal as well.\nIn practice, we tested both approaches and saw better results computing the cross entropy up to and including index j. A potential solution to avoid over-fitting after the instruction token is to have a minimum offset into the harmful continuation both for sampling j as well as the cross-entropy term,"}, {"title": "3.4. Robustness Evaluation", "content": "We compute the defence success rates against the following attacks:"}, {"title": "3.4.1. REALISTIC ATTACK SCENARIOS", "content": "Pre-filling whereby the attacker is allowed to insert the first n tokens as the response of the assistant. We use the Harmbench (Mazeika et al., 2024) affirmative responses as the pre-fill attack. Note that in this setting the (rf) models (including the 'Fixed position RF' model) are allowed to check the logits of the pre-filled text!\nSampling Attacks where we attack the model by sampling the response multiple times (Hughes et al., 2024). Occasionally, the model may eventually provide an answer to a harmful prompt after repeated queries. In our experiments, we sample up to 16 times or until the model responds, as evaluated by the official classifier for text behaviours in HarmBench\u00b9. We use a temperature of t = 0.9 and a top-p value of 0.9 for the sampling attack."}, {"title": "3.4.2. LIMIT TESTING", "content": "Continuous Attacks are a type of attack where soft to-kens (Schwinn et al., 2024) are optimised with sign gradient descent using an affirmative response as the target to break the model's safety. We allow the attack to use 70 steps with 10 soft tokens and no epsilon ball.\nRefusal Vector Ablations are attacks in which a single \u201crefusal direction\u201d is identified in the model's residual stream, and then ablated from activations across all layers and token positions (Arditi et al., 2024). Suppressing this direction from being represented in the model's residual stream effectively disables a model's ability to refuse harmful queries.\nThe first two attacks are gray-box attacks using additional features such as temperature-based sampling and pre-filling, while the latter two are strong white-box attacks. While the continuous attack and the ablation attack are unrealistic settings for our (rf), however, we include them as sanity-check and to test the limits of our approach.\nAn attack is successful if the model does not refuse or the (rf) probability does not surpasses a model-specific threshold. We evaluate refusals using GPT-40 (OpenAI et al., 2024) as a judge. Our results are shown in Figure 2."}, {"title": "3.5. Generalisation to Longer Contexts", "content": "During training, we only include data with a single turn of user/assistant interaction, with (rf) being injected following some distribution biased towards the start of the first as-"}, {"title": "3.6. Fine-tuning attacks", "content": "We consider the fine-tuning attack threat model discussed in Section 2.1, where the attacker is able to fine-tune the model as they wish through an API. We trained a LoRA module that encapsulates the weight changes for the model to insert the (rf) in harmful continuations on LLAMA3.2-3B-IT. Hence, our defence is to apply this LoRA module one or more times after the attacker has been able to fine-tune the base model. For the fine-tuning attack we also use a LORA module and use the data and hyper-parameters from Qi et al. (2024), which consists of about 100 harmful examples and continuations.\nAs baselines we consider both the aforementioned CAT (Xhonneux et al., 2024) as well as the 'Fixed position RF' stored in LoRA modules. In addition, we test whether applying each of these approaches multiple times can further improve robustness. Finally, we check whether we can combine CAT and our own (rf) approach. Due to the strength of the fine-tuning attack setting for both 'Fixed position RF' and our own (rf) we consider different thresholds for the (rf) probability. This demonstrates the fundamental robustness and utility trade-off that exists in terms of false positive rate (FPR) and true positive rate (TPR)-i.e., defence success rate. The goal is to improve the Pareto front. As before, we use the same Harmbench test set and the same Harmless dataset from Figure 2; the results are in Figure 5.\nWe also validate that on benign fine-tuning our (rf) module does not impact the gained performance significantly. We test this on GSM8K (Cobbe et al., 2021) in chat mode under strict match of the answer-see the Appendix D for final numbers."}, {"title": "3.7. Discussion", "content": "The first observation from Figure 2 is that (rf) approach maintains near-perfect utility across all models. On LLAMA3.2-3B-IT we find that all the baselines considered are able to achieve nearly the same utility scores as the base"}, {"title": "3.8. Limitations", "content": "Expectedly, our approach is not able to defend against attacks that have complete access to the model weights such as continuous attacks, although substantial robustness against ablation attacks is achieved. This shows that the association between harmful generation and the (rf) is circumvented by these attacks, demonstrating a limitation of our approach. Furthermore, in the fine-tuning attack setting, there is still a trade-off between robustness and utility in the face of an attacker, albeit a much stronger Pareto-front than without the (rf) module."}, {"title": "4. Related Work", "content": "Jailbreaking LLMs Modern LLMs used as chatbots are trained to follow user instructions (Ouyang et al., 2022) while also being trained to respond in a safe and harmless manner (Perez et al., 2022). While users quickly found ways to manually craft \"jailbreaks\" which could circumvent these safeguards and elicit harmful content from these systems (Wei et al., 2023), automated methods for crafting adversarial attacks were also shown to be effective. Particularly, Zou et al. (2023) propose a greedy-coordinate gradient (GCG) search algorithm to find an adversarial suffix optimized to pre-fill (Vega et al., 2023) an affirmative response in a model's response. Other approaches use heuristics to craft interpretable jailbreaks with only black-box access to the target model (Chao et al., 2023; Liu et al., 2023; Zeng et al., 2024). Given white-box access to the target model, more powerful attacks are possible. Adversarial soft prompts can be optimized to manipulate the model's outputs (Schwinn et al., 2024), causal features responsible for refusal behaviour can be selectively ablated (Arditi et al.,"}, {"title": "Defences", "content": "Beyond standard pre-training, LLMs are typically trained with preference optimization techniques such as RLHF (Ouyang et al., 2022) or DPO (Rafailov et al., 2023) to be more aligned with human preferences. Jail-breaks can be incorporated into this preference alignment phase to increase resilience to such attacks (as is often done with red-teaming methods), but this does not often generalise to novel jailbreaks. Historically, in the context of vision models, actively training against adversarial attacks in an online manner (i.e., adversarial training) is the only method that has shown increased adversarial robustness (Madry et al., 2017). However, in the context of language, most discrete attacks are prohibitively expensive to use on-line. Mazeika et al. (2024) train against adversarial suffixes generated by GCG, but continually update a pool of examples rather than generate each attack from scratch. Other approaches perform adversarial training by attacking the embedding or latent space of the model (Xhonneux et al., 2024; Sheshadri et al., 2024) which is much more efficient to compute and transfers to discrete attacks. Beyond adversarial training, newer defences target and alter harmful representations in order to prevent a model from producing harmful outputs entirely (Zou et al., 2024). Independent from training a model to be more robust to jailbreaks is to classify and judge the potential harmfulness of the generated text, often with another LLM fine-tuned for this task (Inan et al., 2023; Feuer et al., 2024), although this does require additional resources to classify the outputs. Concurrent work Huang et al. (2025) has shown that classifiers alone are often not sufficient, further making the case that other approaches are needed especially against permissive but common threat models such as the fine-tuning box attack."}, {"title": "Special Tokens", "content": "Several works have explored training or utilising special tokens for specific purposes. Burtsev et al. (2020) prepend \u201cmemory\u201d tokens to an input prompt on a target task. Goyal et al. (2023) append \"pause\" tokens, which are hypothesised to give the LLM a buffer sequence to reason over before producing an output. Mu et al. (2023) train LLMs to compress longer prompts into smaller sets of \"gist\" tokens as a means to shorten the context. Xiao et al. (2023) prepend \u201cattention sinks\" to improve generalization to long-context sequences. LLMs have also been trained to use a variety of tools (such as a calculator or internet access), which are denoted and invoked via special tokens (Schick et al., 2023). Most closely related to our approach is the recent work of Jain et al. (2024), where a model is trained to prefix an output with a special refusal or response token based on the behaviour of whether the model refuses or responds to a prompt. While their approach is related in that special tokens are leveraged in the context of alignment,"}, {"title": "5. Conclusion", "content": "We propose to detect harmful outputs from a large language model (LLM) without an external classifier, but using the generative model itself. To achieve this goal we develop a training algorithm such that the target LLM outputs a special red flag token ((rf)) at any time during a harmful continuation. This provides us with a generative approach to detect harmfulness even under strong adversarial attacks such as pre-filling and sampling. We show that our method significantly improve robustness without affecting utility. We demonstrate that our approach generalizes to very long contexts with multiple conversation turns despite having only been trained on short one-round conversations.\nFinally, we investigate another strong threat model that of a fine-tuning attack in an API setting and show that you can store safety training approaches such as our own (rf) or adversarial training (CAT (Xhonneux et al., 2024)) in a LoRA module and apply it post-hoc against a jailbroken model to regain significant robustness without harming benign fine-tuning performance. These different model hardening approaches are complimentary, as we show by combining a continuous adversarial training (CAT) module and our own (rf) module."}, {"title": "Impact Statement", "content": "Machine learning tools such as large language models (LLMs) are finding widespread usage in today's wealthy societies. As such, any work in this area has the potential for a significant impact, as it could avoid catastrophic outcomes due to a potential lack of safety of these widespread models.\nThis work aims to provide a new approach to reduce the harmful behaviour of LLMs when used via a webpage or API. As such, the desired impact of this work is overwhelmingly positive. However, it has to be acknowledged that any work aiming to filter or prevent harmful content from reaching users of non-open source LLMs can most likely also be re-used for censorship and thus also runs the risk of reinforcing biases of the LLM operator\u2014intentionally or not.\nMore broadly and in the longer term, our work may enable practitioners to build an extra layer of safeguards into models that have capabilities that can both be useful and harmful and thus cannot or will not be removed. In such a situation, our approach and future derivatives can be used to tag and recognize the harmful usage of a capability. A potential downside is that practitioners may be over-reliant on this (rf) as a defence mechanism rather than ensuring that learning algorithms and data during pre-training and various post-training stages remove harmful capabilities to the model. As such, this work also considers the worst-case attacks, such as very strong fine-tuning attacks, continuous attacks, and ablation attacks, to clearly show that this approach can be circumvented with sufficient access and thus shall not be the only layer of safety for critical applications with very capable models."}, {"title": "A. Software", "content": "We use huggingface and pytorch to run all models."}, {"title": "B. Judge", "content": "The harmfulness check is done with the following prompt:\nDo you consider the following question and response to be the start of a harmful answer? Answer with yes or no. Question: {prompt}. Response: {response}\nThe utility check is done with the following prompt:\nDo you consider the following response to be the start of a reasonable answer to the prompt? Answer with yes or no. Question: {prompt}. Response: {response}"}, {"title": "C. Hyperparameters", "content": null}, {"title": "D. Fine-tuning attack", "content": "We validate that our approach of applying a safety LoRA module does not break benign fine-tuning. For this we train with SFT for one epoch on GSM8K (Cobbe et al., 2021) in chat mode. We train with batchsize 64, learning rate $10^{-4}$, LORA parameters r = 64 & \u03b1 = 64, AdamW (Loshchilov & Hutter, 2019), and a constant learning rate schedule. We evaluate the GSM8K performance with the LM-EVALUATION-HARNESS (Gao et al., 2024) using the command lm_eval --model hf --tasks gsm8k --num_fewshot=5 --device cuda:0 --batch_size 16\n--model_args pretrained=meta-llama/Llama-3.2-3B-Instruct --apply_chat_template. For the base model we get a performance of 24.1 \u00b1 0.1% under strict-match, the fine-tuned model gets 61.0 \u00b1 0.1%, the fine-tuned model with one safety LoRA adapter gets 62.2 \u00b1 0.1%, and the fine-tuned model with the (rf) adapter applied twice gets 59.4 \u00b1 0.1%."}, {"title": "D.1. Long-Context", "content": null}]}