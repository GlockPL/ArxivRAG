{"title": "Sound Classification of Four Insect Classes", "authors": ["Yinxuan Wang", "Sudip Vhaduri"], "abstract": "The goal of this project is to classify four different insect sounds-cicada, beetle, termite, and cricket. One application of this project is for pest control to monitor and protect our ecosystem. Our project leverages data augmentation, including pitch shifting and speed changing, to improve model generalization. This project will test the performance of Decision Tree, Random Forest, SVM RBF, XGBoost, and k-NN models, combined with MFCC feature. A potential novelty of this project is that various data augmentation techniques are used and created 6 data along with the original sound. The dataset consists of the sound recordings of these four insects. This project aims to achieve a high classification accuracy and to reduce the overfitting problem.", "sections": [{"title": "I. INTRODUCTION", "content": "Insects play critical roles in various ecosystems, influencing agriculture, forestry, and the environment at large. However, certain insect species, particularly pests, can cause significant harm to crops, infrastructure, and natural habitats. According to United States Environmental Protection Agency, termites cause damage to the houses and all kind of structures that cost about billions of dollars, and the owners of these properties have to spend more than two billion dollars to treat them [1]. According to United States Department of Agriculture, bark beetles can be harmful to environment, animals, and human beings, and they are one of the sources to cause diseases and mechanical damages, such as file, to the forests [2]. Though crickets and cicadas are usually not considered as pests, they are usually very noisy in day and nights, and affect people working and resting. In a program of the United States National Institutes of Health, cicadas can make as much noise as a motorcycle [3]. According to a blog, some kinds of crickets are harmful to crops or occasionally bite humans, such as tobacco crickets and striped raspy cricket [4]. Hence, early and accurate identification of these insects is essential for both ecological studies and pest control efforts.\nIn recent years, improvements in acoustic classification have opened new possibilities for identifying insects based on their sounds. By analyzing the unique acoustic patterns produced by different insect species, models can offer a non-invasive, efficient way to monitor insect populations. The development of such models is particularly valuable for pest control, where early detection of harmful species can prevent significant economic losses.\nOur project aims to contribute to this field by training classifiers which can distinguish the sounds of four specific insect species: cricket, cicada, termite, and bark beetle. To improve the generalization of the model, data augmentation, such as pitch shifting and time stretching, are used, ensuring low overfitting to the training dataset. This project will measure and compare the performances of decision tree, random forest, k-nearest neighbor, support vector machine with radial basis function, and XGBoost, in combining with Mel-frequency Cepstral Coefficients (MFCC)."}, {"title": "II. RELATED WORK", "content": "A. Bag of On-Phone ANNs to Secure IoT Objects Using Wearable and Smartphone Biometrics\nS. Vhaduri, W. Cheung, and S. Dibbo explored the availability to utilize biometrics from wearables and smart phones with a bag of on-phone artificial neural network models in 2023 [5]. Although this research is not related to insect sound classification, they provided several possible and available values for pitch shifting and speed changing. In our research, we eventually selected -2, -1, 1, and 2 for pitch shifting, and 0.5x and 2.0x for speed changing.\nB. Insect Classification Solutions based on Insect Images\nIn 2017, C. Martineau et al. tested the performance of multiple classifiers, including SVM, decision tree, MPL, Bayes, Logistic, Random Forest, k-NN, and so forth, to classify insect based insects' images [6]. In 2018, D. Xia and et al. used an improved convolutional neural network with a multiple-kernel learning technique to detect and classify insects based on their images [7]. However, using image-based solution has some limitation. For instance, when an insect is hidden inside the house or underneath some furniture,it would be impossible to detect them via image-based solution. Hence, in our research, we explore the availability to classify insects based on their sounds, providing one more way to detect and classify insects.\nC. Insect Classification Solutions based on Sounds\nResearchers have been using acoustic data to identify people [8]-[10], respiratory decease monitoring [11]\u2013[14], well-being tracking [15]-[18], sleep health monitoring [19], [20], etc. However, acoustic data can also be used for insect classification.\nX. Dong, N. Yan, and Y. Wei introduced an insect sound classifier based on convolutional neural network, and they used MFCC and chromatic spectrogram as the features in 2018 [21]. Their classifier recieved an 97.8723% accuracy rate among 47 types of insect sound from USDA library, which is also one of the datasets our research will use [21]. However, they trained the classifiers using the images of the spectrogram as the dataset. Similarily, M. Zhang et al. also used CNN and MFCC features to train the classifier based on the images of the spectrogram, yet they only received 92.56% accuracy among nine insect species in 2021 [22]. In comparison to their methodology, we explored the availability of training classifiers with the coefficients of MFCC features rather than the images of the spectrogram.\nIn 2022, S. Basak et al. explored the accuracy and limitation of various k-NN and SVM classifiers with MFCC feature [23]. Surprisingly, their accuracy is relatively lower than other research work and our result: the accuracy of k-NN, cosine k-NN, medium k-NN, SVM, and Linear SVM are 85.4%, 83.9%, and 83.8%, 84.6%, and 84.0%, respectively. In our research, we also test the performance of k-NN as well as SVM classifiers, and we achieve a higher accuracy than their result. A possible reason could be that we extract 40 MFCC features rather than only 13 MFCC features, and besides, we applied data augmentation after segmentation to increase the size of the dataset.\nD. Other Insect Classification Solutions\nY. Chen et al. designed a new inexpensive sensor and used a Bayesian classifier to classify flying insects by the incidental sound of their flight in 2014 [24]. Their solution did solve the issue that the traditional acoustic method is difficult to gather a large dataset and thus could result into poor accuracy and classifier performance. In contrast, our research uses data augmentation to increase the size of the dataset. We also test the performance of more machine learning classifiers that can be later comparing with the result in their work.\nE. Summary\nAlthough all of these related work have some contributions on insect sound classification, currently no published research paper works on comparing five different models with 40 MFCC features, and only trained through the coefficients rather than images of the spectrograms. Hence, our research will contribute to this area, especially classifying the four sound insect species."}, {"title": "III. METHODOLOGIES", "content": "We will implement random forest, decision tree, k-NN, SVM RBF, and XGBoost models and extract 40 MFCC features for training and testing. Each model will be trained and tested separately.\nDecision Tree is a very common classifier, and its result is usually easy to interpret.\nRandom Forest is also a widely used model for classification and it often has higher accuracy and can prevent overfitting problems compared to a single decision tree.\nMFCC is a popular feature in speech recognition and audio signal processing because it can keep important acoustic information to make the model easier to classify.\nThe code of this project is stored in Purdue GitHub:"}, {"title": "IV. DATASET", "content": "Our dataset consists of the sound recordings from four insect categories: cicada, beetle, termite, and cricket. The dataset is sourced from other research as listed after the figures below. Our dataset will be divided, and 80% are the training set while 20% are the test set. In the training set, the audio recordings will be augmented using pitch shifting and speed changing.\nA. ESC-50\nESC-50, which is the dataset for environmental sound classification, consists of 2000 labeled environmental audio recordings in total, and we need the flying insect recordings for this project [25]. This dataset has 2000 environmental audio recordings, and it consists of 50 semantical classes in five major categories: animals, natural and water sounds, human non-speech sounds, domestic sounds, and urban noises [25].\nB. InsectSet32: Datasets for Automatic Acoustic Identification of Insects (Orthoptera and Cicadidae)\nThis dataset has 335 audio recordings of 32 sound producing insect species: 147 recordings are belonging to nine species from the order Orthoptera, while the rest 188 recordings are belong to 23 species in the family Cicada [26]. This project will use these 188 recordings that are belonging to Cicadidae family.\nC. Bug Bytes Sound Library: Stored Product Insect Pest Sounds\nBug Bytes Sound Library is provided by the ARS Center for Medical, Agricultural, and Veterinary Entomology, and is used to support the detection and control of hidden insect infestations [27]. This dataset consists of 52 species that are considered as pests. This project will use five different termite species from the dataset.\nD. Experimental Characterization and Automatic Identification of Stridulatory Sounds Inside Wood \u2013 Supplementary Information (Data)\nThis dataset is the supplementary information for the research conducted by Carol L. Bedoya, Ximena J. Nelson, Eckehard G. Brockerhoff, Stephen Pawson, and Michael Hayes, and consists of 360 acoustics signals from Hylurgus ligniperda and Hylastes ater, which are two beetle species [28].\nE. Data Segmentation\nThis project will use Audacity, a free audio editing app, to do audio clip segmentation. We only pick the sounds from four insect classes and ignore other ones, such as birdsong. To easier displaying and future processing, each insect class has a corresponding id: C1 refers to crickets, C2 refers to cicadas, C3 refers to termites, and C4 refers to bark beetles. As shown in figure 2, most segments are less than three seconds long, though they have several outliers. Yet, class C3 have multiple segments which are about 10 seconds long. Among 25 original clips we choose from five classes correspondingly, class C4 has 382 segments, which is the most, and class C3 has 17 segments, which is the least. The minimum segment duration (w) is approximately 0.0288 seconds, and the minimum sample size is 636.\nThen, we create instances from all segments based on w. The segments that are longer than w will be divided into multiple instances. The last instance can be overlapped with its previous instance because a segment's duration is not necessarily an integral multiple of w.\nAs shown in figure 3, class cricket has the least number of instances. Hence, we will randomly choose 663 instances from each class to apply data augmentation in the next step.\nF. Data Augmentation\nPitch shifting and speed changing are the data augmentation techniques we are going to use in this project. The ranges for pitch shifting and speed changing come from Professor Sudip Vhaduri, W. Cheung, and S. V. Dibbo's research [5].\nIn pitch shifting, the values of the pitches are ranged from -3.5 to 3.5 with 0.5 increments. Thus, there are 14 variations of different pitches, excluding the original pitch.\nIn time stretching, the values of the speeds are ranged from 0.25x to 2x with 0.25 increments. Thus, there are seven variations of different speeds, excluding the original speed.\nHence, each audio recording in the training set becomes 22 augmented data, which have one original recording plus 14 pitch variations plus seven speed variations.\nAfter data augmentation, each class has exactly 14586 augmented class balanced instances which are ready for training and evaluating the models.\nG. Dataset Naming Convention\nSince our dataset is combined with multiple datasets from other research, we decide to keep the original file names, and append extra information after the file names, so the convention becomes: {Original File Name}#{Segment Number}#{wNumber}#{Augmentation Type}#1.wav. The augmentation type is P, pitch shifting, or T, time stretching."}, {"title": "V. RESULTS AND EVALUATIONS", "content": "This project will test and evaluate the performance of Decision Tree, Random Forest and k-NN classifiers based on accuracy and confusion matrix with different segment lengths, different balanced instance count, and whether dataset will be augmented.\nA. t-SNE and UMAP Plots\nThe results of both t-SNE and UMAP plots are visualized in Figures 4 and 5. These methods are used to project 40 MFCC features into a 2D space to easily observe patterns among four classes. As shown in both graph, the beetles are more clustered closely, while the rest three classes are more sparse. Hence, we predicted that the trained classifier would have a better performance on beetles, but not on crickets, cicada, and termites. In Figure 6, you can see that the beetles have closer distances among five original clips, while some original clips from cricket, cicada, and termite classes are further apart. Hence, we could conclude that beetles will have better results compared to the other three classes. Figure 8 demonstrates the t-SNE plot of all instances after data augmentation. In the plot, many instances in different classes are overlapping, indicating the difficulties of models to classify these data.\nB. Results with Segment Length 0.1 Without Separating Original Clips\nWe first use 0.1 as the segment length, and cricket has 197 instances, cicada has 396 instances, termite has 284 instances, and beetle has 1028 instances, as shown in 3. So, we choose 30 and 150 for each trial, and we will use decision tree, random forest, and k-NN models with MFCC features.\n1) Results of Balanced Instance Count 30: All models perform well when using 30 as a balanced instance count, and as shown in all confusion matrix plots, the Decision Tree (DT), Random Forest (RF), and k Nearest Neighbor (k-NN) classifiers can distinguish between four insect sounds: Cricket, Cicada, Termite, and Beetle in very high accuracy. In these matrices, each row represents the true labels, and each column represents the predicted labels.\n2) Results of Balanced Instance Count 150: All classifiers also perform well when using 150 as balanced instance count, and as shown in all confusion matrix plots, that Decision Tree, Random Forest, and k-NN classifiers, can distinguish between four insect sounds: Cricket, Cicada, Termite, and Beetle in a very high accuracy.\n3) Summary: These results have very high accuracies might due to the problem that we did not separate the original clips between training set and test set. Thus, all classifiers are overfitting to the given training set and test set. We predict that they would not do well on the real-world samples, due to the over-fitting problem. Hence, we will give a second try with the same segment length and, at the same time, separating original clips.\nC. Results of Segment Length 0.1 with Separating Original Clips\nIn the second try, we still use 0.1 as the segment length and Decision Tree as the classifier, but the process is different. In each class, the unbalanced instances are grouped by their original clips, and then instances from one original clip work"}]}