{"title": "UASTrack: A Unified Adaptive Selection Framework with Modality-Customization in Single Object Tracking", "authors": ["He Wang", "Tianyang Xu", "Zhangyong Tang", "Xiao-Jun Wu", "Josef Kittler"], "abstract": "Multi-modal tracking is essential in single-object tracking (SOT), as different sensor types contribute unique capabilities to overcome challenges caused by variations in object appearance. However, existing unified RGB-X trackers (X represents depth, event, or thermal modality) either rely on the task-specific training strategy for individual RGB-X image pairs or fail to address the critical importance of modality-adaptive perception in real-world applications. In this work, we propose UASTrack, a unified adaptive selection framework that facilitates both model and parameter unification, as well as adaptive modality discrimination across various multi-modal tracking tasks. To achieve modality-adaptive perception in joint RGB-X pairs, we design a Discriminative Auto-Selector (DAS) capable of identifying modality labels, thereby distinguishing the data distributions of auxiliary modalities. Furthermore, we propose a Task-Customized Optimization Adapter (TCOA) tailored to various modalities in the latent space. This strategy effectively filters noise redundancy and mitigates background interference based on the specific characteristics of each modality. Extensive comparisons conducted on five benchmarks including LasHeR, GTOT, RGBT234, VisEvent, and DepthTrack, covering RGB-T, RGB-E, and RGB-D tracking scenarios, demonstrate our innovative approach achieves comparative performance by introducing only additional training parameters of 1.87M and flops of 1.95G. The code will be available at https://github.com/wanghe/UASTrack.", "sections": [{"title": "I. INTRODUCTION", "content": "Visual object tracking [1]\u2013[4] is a crucial research area in computer vision, focusing on estimating the position and size of an object throughout a video sequence, beginning with the object initial state in the first frame. Recent advancements highlight the limitations of relying solely on visible sensors, leading to increased interest in utilizing auxiliary modalities such as thermal (T) [5], event (E) [6], and depth (D) [7]. This shift propels multi-modal tracking [8]\u2013[10] a pivotal research area due to the synergistic characteristics of the RGB modality and auxiliary modalities. For example, while RGB data is highly sensitive to lighting variations, thermal data remains stable, facilitating robust tracking even under challenging illumination conditions. And RGB-D tracking utilizes the geometric information provided by depth modality to enhance tracking accuracy, particularly in scenarios involving cluttered backgrounds, or noisy occlusions. In contrast, RGB-E tracking capitalizes on the superior temporal resolution and wide dynamic range of event-based data, enabling more precise object tracking even in scenarios involving rapid motion or sudden illumination changes. These complementary features including RGB-X (X represents depth, event, or thermal) image pairs emphasize the strengths of distinct multi-modal characteristics in overcoming the limitations of single-modality systems.\nMost existing methods [11], [12] process each RGB-X image pair independently. Typically, these methods employ a task-specific training strategy, requiring N separate sets of parameters for N tasks, with each task necessitating a distinct model, as shown in Fig. 1 (a). However, current advancements in multi-modal tracking are constrained by the lack of a comprehensive dataset that simultaneously encompasses all modalities containing depth, event, thermal, and RGB. This limitation has been a growing research interest"}, {"title": "", "content": "in developing unified multi-modal tracking systems capable of effectively utilizing paired multi-modal training data while adaptively generalizing to any available modality during infer-ence. Implementing unified multi-modal tracking systems for various tracking tasks offers several advantages: Firstly, unified multi-modal tracking systems reduce the effort required for model and hyper-parameter tuning for each task, encouraging straightforward comparisons of algorithm performance across different modalities. Moreover, unified multi-modal tracking systems enable the effective integration of shared information from various modalities into the tracking system. Therefore, the adoption of unified multi-modal tracking systems enhances flexibility, enabling adaptation to diverse input types in practical applications.\nRecently, several methods have attempted to explore achiev-ing unification across various multi-modal tracking tasks, which can generally be classified into two categories. The first category focuses on employing a unified network architecture, as shown in Fig. 1 (b). For instance, methods such as ProTrack, ViPT, SDS-Track, and OneTracker [13]\u2013[16] leverage the prompt-tuning paradigm to achieve a unified model but still need N sets of training parameters for N RGB-X tracking tasks. The second category addresses the limitations of the first by utilizing a single set of training parameters, as illustrated in Fig. 1 (c). While previous Un-Track [17] (Fig. 1 (c1)) achieves this unification, it still depends on prior knowledge of modality types, which prevents its ability to adaptively distinguish any modalities. Since various auxiliary modalities, such as thermal, event, and depth, exhibit significantly distinct characteristics, there is a need for a unified algorithm that can not only effectively leverage complementary information but also address domain gaps across modalities. However, existing approaches all overlook the unique properties of individual modalities and fail to dynamically adapt to the specific requirements of auxiliary modalities.\nTo address the above challenges, we propose a unified adaptive selection framework with modality-customization in Single Object Tracking (UASTrack), which not only achieves modality-adaptive perception but also incorporates modality-specific structures based on the characteristics of different RGB-X image pairs, as shown in Fig. 1 (c2)). Specifically, we introduce a Discriminative Auto-Selector (DAS), which is designed to dynamically identify the input modality type, thereby guiding the adaptive selection of the most suitable network structures. By employing a classification mechanism that distinguishes image pair combinations (e.g., RGB-T, RGB-D, or RGB-E), the DAS module establishes a robust foundation for adaptive processing modality-specific branches. To enhance the DAS learning capability, we also incorporate Classification Constraint Loss (CCL) by using cross-entropy. As illustrated in Fig. 1, our proposed DAS module effec-tively predicts various tasks, achieving prediction success rate (PSR) of 99.58%, 99.62%, and 99.96% for RGB-T, RGB-D, and RGB-E tracking tasks, respectively. In contrast, previous methods lack the capability to perform modality-adaptive predictions. Although directly applying an RGB-based pre-trained head structure has proven effective in extracting robust multi-modal data, it often leads to sub-optimal performance"}, {"title": "", "content": "due to differences in data distribution and modality-specific features. To bridge the modality gap by transforming modality-specific features (thermal, event, or depth) into an RGB-based pre-trained feature space, our approach also proposes a novel Modality-Customized Adapter (TCOA) at the task level.\nFurthermore, since different modalities exhibit significant distributional differences and background redundancy charac-teristics, the optimization adapter for the prediction head is customized for each modality to maximize its effectiveness. To be specific, in contrast to event and depth modalities, thermal data often contains more effective object information, particularly in scenes with limited illumination and occlusion challenges. Therefore, a lightweight general adapter is intro-duced specifically for RGB-T tracking to amplify discrimina-tive features while suppressing noise. Due to the depth and event features being sparse, average pooling and max pooling mechanisms are additionally applied to reduce redundancy and effectively extract key modality cues.\nTo fully leverage the potential of RGB and auxiliary modalities while maintaining algorithmic efficiency, we adopt bidirectional adapters within Transformer Encoder blocks [18], [19] to facilitate effective interactions between RGB and X features. Unlike previous works [16], [17], our approach aims to establish unified multi-modal tracking systems capable of adaptively recognizing multi-modal tasks, while integrating modality-specific refinements for each task. In comparison to the RGB-X baseline, which requires 56.44G FLOPs and 92.13M parameters, our proposed UASTrack introduces a modest increase of only 1.87M parameters and 1.95G FLOPs, resulting in an absolute improvement of 8.5% in Success Rate on LasHeR benchmark.\nIn summary, our contributions are as follows:\n\u2022 We propose a unified RGB-X tracker that utilizes a Discriminative Auto-Selector, eliminating the need for prior modality types and enabling dynamic adaptation across various tracking tasks. Additionally, a classification constraint loss is incorporated to further enhance the Discriminative Auto-Selector learning capability.\n\u2022 We propose a Task Customization Optimization Adapter, enhancing the adaptability of the foundation model to multi-modal space and enabling modality-specific customization for different tasks based on auxiliary modali-ties.\n\u2022 Extensive evaluations on five benchmarks confirm the effectiveness and efficiency of UASTrack, achieving a significant performance advantage over state-of-the-art trackers."}, {"title": "II. RELATED WORK", "content": "A. Multi-modal Tracking\nIn recent years, substantial research [4], [20], [21] has been dedicated to visual object tracking, which has gained wide-ranging applications across various fields, such as autonomous driving, mobile robotics, video surveillance, and human-robot interaction. However, the performance and stability of visual object tracking remain constrained when confronted with chal-lenges in complex scenarios. Subsequently, multi-modal track-ing [22], [23] incorporating additional auxiliary modalities"}, {"title": "", "content": "[24]\u2013[31], such as thermal, event, and depth, has emerged as a promising research. Specifically, depth sensors [7] facilitate the handling of objects at varying geometric distances; thermal sensors [12] effectively address challenges such as low illumi-nation; and event sensors, known for their low-latency motion capture capabilities (1 \u00b5s) [32] enhance high-speed awareness for improved tracking performance. Therefore, multi-modal information can compensate for these deficiencies and enhance the robustness of visual object tracking networks when dealing with objects with large appearance variations.\nHowever, existing approaches [6], [12], [16], [33] often require training N times, using N distinct models for N tasks, leading to inefficiencies and poor generalization in practical application scenarios. In contrast, our method introduces a unified multi-modal tracking framework, maintaining parame-ter consistency while ensuring effective adaptation to diverse modalities through a modality-customized mechanism.\nB. Learning A Single Set of Parameters for Any Modality\nRecently, there has been growing interest in establishing a unified object tracking with prompt-tuning paradigm for multi-modal object tracking. Several existing multi-modal tracking methods such as ProTrack [13], VIPT [14], OneTracker, and SDSTrack [15] combine cross-modal information to enhance tracking performance across RGB-D, RGB-E, and RGB-T tracking tasks. However, these approaches rely on N sets of parameters for N tasks, which limits their flexibility and adaptability to a wide range of real-world application scenarios within one joint training process.\nAdditionally, although Un-Track [17] attempts to use a single set of parameters for any modality, fails to achieve task-adaptive selection due to relying on prior modality types to guide the flow of input modality. In contrast, our pro-posed UASTrack is the first unified RGB-X tracker to enable modality-adaptive perception by introducing a lightweight discriminative auto-selector. Our method customizes the head adapter structure characteristics, helping to filter out noise redundancy. This operation allows the RGB-based pre-trained foundation network to adapt effectively to the spatial structures of the multi-modal domain."}, {"title": "III. METHODS", "content": "A. Overall Framework\nIn this work, we propose a unified adaptive selection framework for any modality in single object tracking, as illustrated in Fig. 2. The framework consists of a frozen Foundation Tracker and trained Discriminative Auto-Selector, Visual adapter, Modality Adaptive Selection Adapter, and Task-customized Optimization Adapter. These trained com-ponents enable task-agnostic representation learning across diverse tracking scenarios. We provide a detailed description"}, {"title": "", "content": "of the foundation tracker architecture in Section B, the task-agnostic representation learning in Section C, and the objective loss formulation in Section D.\nB. Foundation Tracker\nAs illustrated in Fig. 2, UASTrack adopts an RGB-based pre-trained Transformer architecture [18] as the backbone. Multi-modal tracking aims to predict the bounding box of the target in subsequent frames, based on its initial location and shape in the first frame of a video. Robust tracking performance necessitates the effective integration of multi-modal inputs, including RGB images $I_{RGB} \\in R^{H \\times W \\times 3}$ and auxiliary images $I_x$. Initially, the foundation network pre-processes input image pairs, converting them into a unified embedding format. The embedding features are processed by the feature extractor F to generate fused features denoted as f. The fused features are forwarded to the task head H, which extracts task-relevant information and generates the final predictions P after post-processing. The process of multi-modal tracking can be described as follows:\n$P = Head(F(I_{RGB}, I_x)).$ (1)\nConsidering the scarcity of comprehensive multi-modal training datasets, such as RGB-T, RGB-D, and RGB-E, and the lack of pre-trained multi-modal models, we adopt an RGB-based pre-trained Transformer as the backbone to mitigate over-fitting in downstream multi-modal tasks. The Transformer blocks are kept frozen, while task-agnostic representation learning adapters are fine-tuned. To address significant differ-ences among modalities such as variations in distributions, color characteristics, and data sparsity-an activated discrim-inative auto-selector is employed to effectively distinguish between different multi-modal tasks. This enables targeted processing by filtering modality-specific data and dynamically selecting the most relevant architecture, thereby ensuring effi-cient workflows.\nC. Task-Agnostic Representation Learning\nDiscriminative Auto-Selector. To enable task-agnostic rep-resentation learning, we propose a Discriminative Auto-Selector (DAS) to predict a modality prediction (MP) which identifies auxiliary modalities and activates DAS during infer-ence. Given the significant differences among auxiliary modal-ities, the simple and lightweight DAS effectively filters and distinguishes features from various modalities. The structure of DAS is illustrated in Fig. 2 (b). The input, denoted as $f_x$, are auxiliary features processed after a patch embedding layer. Initially, $f_x$ is passed through an adaptive average pooling layer (Adaptive Avg Pool), which adjusts its width and height to an output size of 1x1:\n$f_x = Adaptive AvgPool(f_x)$ (2)\nSubsequently, the reshaped $f_x$ features are processed through two linear layers to obtain a modality-predicted prob-ability $P_m$:"}, {"title": "", "content": "$P_m = FC_2(Norm(FC_1(Reshape(f_x)))$ (3)\nUsing the Argmax operation, the index corresponding to the maximum value can be returned:\n$MP = Argmax(P_m)$ (4)\nMP serves as a crucial input for subsequent multi-modal fea-ture fusion and modality-specific optimization. The prediction success rates are presented in Fig. 1. By applying the MP predicted by the discriminative auto-selector, we can obtain the predicted types for input tasks without requiring prior types.\nTo further strengthen the DAS classification constraint, we utilize the predicted probability $P_m$ to compute the Classifi-cation Loss (CL) $L_m$ using cross-entropy loss against the true modality types $T_m$ for the three multi-modal tracking tasks.\n$L_m = \\sum_{i=1}^{N} T_{m, i} log(P_m)$ (5)\nwhere N is the number of multi-modal tracking tasks.\nModality Adaptive Selection Adapter. As illustrated in Fig. 1 (d), spatial interactions between RGB modality features are facilitated by a bidirectional adapter module inspired by [19]. To accommodate the varying characteristics of different modalities, we design task-specific adapter structures with non-shared parameters. Firstly, we identify and split x modal-ity data in l \u2013 th encoder block, denoted as $f_l \\in R^{H \\times W \\times C}$, based on previous MP. Then for features are passed through a down-sampling layer Down to reduce the feature channel dimension. Subsequently, a Linear layer is applied to main-tain the consistency of modality-specific features with a small number of trainable parameters. The features then pass through an up-sampling layer, denoted as Up, to restore the original"}, {"title": "", "content": "feature channel dimensions. The mathematical formulation of the task-specific sub-adapters is as follows:\n$f_x = Up(Linear(Down(f_l)))$ (6)\nwhere there are N sub-adapters for N tasks.\nFor simplicity, the Visual Adapter (VA) maintains the same structure as the task-specific sub-adapters.\nTask-Customized Optimization Adapter. On one hand, due to the limited adaptability of the RGB-based pre-trained network to downstream multi-modal data, we employ adapter learning with a small number of additional training parameters, without modifying the foundation structure. On the other hand, the significant variation of auxiliary modalities necessitates customized filtering for each modality.\nWe analyze the characteristics of different modalities to determine appropriate processing approaches. Compared to event and depth modalities, thermal modality features are dense and exhibit minimal redundancy. Therefore, a gen-eral adapter module is sufficient to handle thermal features, ensuring a design that remains both effective and efficient without specialized processing In contrast, depth and event data exhibit significant sparsity and redundancy. Depth data provides rich geometric information but may also include redundant features, such as excessive details from flat regions (e.g., walls and floors), whereas edge and object contour details are more critical. Event data, generated through motion detection, is inherently sparse and exhibits a highly uneven distribution of information.\nTo address these challenges, we design modality-customized adapters for the depth and event modalities to enable targeted processing, as illustrated in Fig. 2. A max pooling operation is employed to extract high-response features, while an average pooling operation is used to retain global characteristics. These two mechanisms complement each other to achieve a balanced feature representation:\n$f_l = Up(Avg(Down(f_x)) + Max(Down(f_x))$ (7)\nwhere Avg and Max represent average pooling and max pooling layers, respectively.\nD. Objective Loss\nConsistent with OSTrack [20], we employ focal loss as the classification loss $L_{cls}$ and adopt $L_1$ loss and $L_{GIOU}$ loss for regression. Additionally, we propose Classification Constraint Loss that incorporates a cross-entropy loss to enhance the"}, {"title": "IV. EXPERIMENTS", "content": "To evaluate the advantages of our proposed UASTrack, we compare its performance against both separated training track-ers and unified trackers. The comparison includes methods such as Un-Track [17], OneTracker [16], ViPT [14], SDSTrack [15], TBSI [11], GMMT [12], BAT [19], APFNet [24], LSAR [34], ProFormer [35], MPT [36], QueryTrack [37], CAT++ [38], TENeT [6], SPT [33], ProTrack [13], CEUTrack [39], MMHT [40], TABBTrack [41], CDAAT [42], and OSTrack [20]. Our foundation network utilizes OSTrack-B224 [20] as the pre-trained model.\nTo train our proposed UASTrack, only the parameters in Discriminative Auto-Selector and modality-specific adapters are learnable, as shown in Fig. 2. In addition to visual object tracking loss, we incorporate a cross-entropy loss that con-strains DAS and specialization of modality-specific adapters. Our method is implemented using PyTorch and trained on a server equipped with a single NVIDIA 3090Ti GPU. We set the batch size to 32, training for 80 epochs. The learning rate for the backbone is set to 4e-4, with a decay ratio of 0.8. We adopt the AdamW optimizer with a weight decay of 1e-4. Additionally, template feature dimensions are uniformly resized to 128x128, while the search search regions are resized to 256x256.\nWe jointly combine various multi-modal tracking bench-marks, including LasHeR [43], DepthTrack [44], and VisEvent [32], for the training process. UASTrack is evaluated on distributed multi-modal tasks across three RGB-T tracking benchmarks: LasHeR, RGBT234 [45], and GTOT [46]; one RGB-E benchmark: VisEvent; and one RGB-D benchmark: DepthTrack.\nA. Comparisons with State-of-the-art Approaches\nAs presented in Table I, our proposed UASTrack outper-forms state-of-the-art methods, including both unified trackers and separated training trackers across RGB-T, RGB-E, and RGB-D tracking.\nRGB-D Tracking. DepthTrack is a comprehensive RGB-D dataset comprising 150 training sequences and 50 testing sequences, evaluated using F-score, Recall (Re), and Precision (Pr) metrics. UASTrack sets a new state-of-the-art performance on DepthTrack benchmark. Specifically, UASTrack achieves an F-score of 62.8%, precision (Pr) of 63.0%, and recall (Re) of 62.5%. These results represent substantial improve-ments over the \"Unified-All\" tracker, Un-Track, with mar-gins of 5.1%, 3.7%, and 7.4% for F-score, Pr, and Re, re-"}, {"title": "", "content": "spectively. Furthermore, UASTrack outperforms the \"Unified-Model\" tracker, SDSTrack by 1.4%, 1.6%, and 1.1% for the same metrics.\nRGB-T Tracking. LasHeR benchmark contains 979 train-ing video sequences and 245 testing video sequences, evalu-ated using three metrics: Precision Rate (PR), Success Rate (SR), and Normalized Precision Rate (NPR). On the test dataset, UASTrack achieves the SR of 57.0%, surpassing the best-performing \"Separated\" tracker, GMMT, by 0.4%, and the unified tracker, Un-Track, by 5.9%.\nRGB-T Tracking. RGBT234 benchmark integrating both RGB and thermal images, includes a total of 234 video sequences with nearly 116.7k frames. As shown in Table II, UASTrack achieves competitive performance compared with previous trackers, with an SR of 65.1% and a PR of 87.6%.\nGTOT benchmark, which is designed to evaluate the ro-bustness of RGB-T trackers, consists of 50 diverse video sequences. As shown in Table II, UASTrack sets a new SOTA with an SR of 78.9% and a PR of 93.3%. These results surpass the previous best-performing tracker, BAT, by margins of 2.6% and 2.4%, respectively.\nRGB-E Tracking. As the largest RGB-E tracking dataset, VisEvent consists of 500 video pairs for training and 320 video pairs for testing. UASTrack achieves the top performance on VisEvent. UASTrack attains the highest Precision Rate (PR) of 77.3% and Success Rate (SR) of 61.0%. These results surpass OneTracker by margins of 0.6% and 0.2%, and Un-Track by 3.8% and 1.8%, respectively.\nAttribute-Based Performance on LasHeR. Our method is evaluated on various challenging attributes in comparison with state-of-the-art trackers using the LasHeR dataset, as shown in Fig. 4. These attributes include No Occlusion (NO), Partial Occlusion (PO), Total Occlusion (TO), Hyaline Occlusion (HO), Motion Blur (MB), Low Illumination (LI), High Illumi-nation (HI), Abrupt Illumination Variation (AIV), Low Res-olution (LR), Deformation (DEF), Background Clutter (BC), Similar Appearance (SA), Camera Movement (CM), Thermal Crossover (TC), Frame Loss (FL), Out-of-View (OV), Fast Motion (FM), Scale Variation (SV), and Aspect Ratio Change (ARC). The experimental results show that our method con-sistently outperforms existing state-of-the-art trackers across most attributes in terms of SR and PR. Notably, it demonstrates"}, {"title": "B. Ablation Study", "content": "Component Analysis of UASTrack. We conduct an abla-tion experiment to evaluate the components of our proposed UASTrack on the VisEvent, LasHeR, and DepthTrack bench-marks, as shown in Table III. Since both the Classification Constraint Loss (CCL) and the Task-Customized Optimization adapter (TCOA) rely on the prediction types from the Discrim-inative Auto-Selector (DAS) module, the validation results for individual modules are assessed based on the the DAS module. The incorporation of DAS results in a significant improvement, with a 2.07% increase for $\\sigma$ compared to the baseline (first row). To be specific, the F-score on DepthTrack increases by 0.5%, the Success Rate (SR) on LasHeR improves by 5.2%, and the SR on VisEvent rises by 0.3%. Even without CCL, the network demonstrates superior performance in distinguishing the thermal modality compared to depth and event modalities. This finding suggests that the thermal modality has inherent characteristics that make it more easily distinguishable by the network relative to the other modalities. Integrating CCL further enhances the network's performance, leading to notable improvements, including a 1.6% increase in SR on LasHeR, a 1.1% rise in SR on VisEvent, and a 2.5% boost in F-score on DepthTrack. Additionally, the incorporation of Task-Customized Optimization (TCO) improves tracker accuracy, contributing to a 1.2% boost in SR on LasHeR, a 1.3% increase in F-score on DepthTrack, and a 0.3% improvement in SR on VisEvent. When DAS, CCL, and TCOA are together integrated into the foundation network, optimal performance is achieved, with an SR of 56.4% on LasHeR, an SR of 60.7% on VisEvent, and an F-score of 62.8% on DepthTrack."}, {"title": "", "content": "Component analysis of the TCOA module. We conduct an ablation experiment on the Task-Customized Optimiza-tion adapter module across different tasks to examine how variations in modality characteristics affect the adaptability of network structures. The results demonstrate that differ-ent modalities benefit from distinct optimization strategies. As shown in Table IV, employing a general sub-adapter composed of linear layers for the thermal modality achieves superior performance due to its ability to effectively capture thermal features, achieving a 1.4% higher SR compared to \"w avgpool+maxpool\" on LasHeR. In contrast, depth and event data exhibit greater redundancy, which can introduce noise and hinder feature fusion. To address this, integrating max pooling and average pooling operations into their re-spective sub-adapters enhances the TCOA module's ability to filter irrelevant information and extract salient features. This approach yields substantial improvements, increasing the F-score by 1.9% on DepthTrack and the SR by 0.5% on VisEvent compared to \"w linear\". These findings highlight the necessity of customizing network structures to the distinct characteristics of each modality, demonstrating that a one-size-fits-all approach is suboptimal for multi-modal tasks.\nInfluence of parameter a. The selection of hyperparame-ters is crucial for optimizing the object tracking performance. We explore the effect of parameter a, while keeping L1 and LGIOU consistent with the OSTrack baseline. The hy-perparameter values for L1 and LGIOU are set to 5 and 2, respectively. The analysis focuses exclusively focuses on the effect of parameter a. As shown in Table V, when a is set to 1, we explore values ranging from 1/100 to 10 times of it. To manage the wide range of potential values, we select the median values of the left and right intervals\u20140.05, 0.5, and 5\u2014as candidate values for a. When a is set to 0.1, the SR improves by 0.4%, 0.5%, and 0.8% compared to a values of 0.01 and 1, 10, respectively. This indicates that, within this specific framework, a moderate value of a is most effective. Tracking accuracy decreases when a shifts away from 0.1, whether towards smaller values such as 0.01 or 0.05, or larger values such as 5 or 10. This decline is likely due to an imbalance in the model: a smaller a may underweight critical components, leading to suboptimal feature utilization, while a"}, {"title": "C. Qualitative Evaluation", "content": "Qualitative analysis about Task-Customized Optimiza-tion Adapter. To evaluate the effectiveness of the Task-Customized Optimization Adapter in achieving modality-specific customization for various tasks, we conduct a qual-itative analysis using selected sequences from three datasets. Specifically, we select the sequence \"ab_motocometurn1\" from LasHeR dataset, the sequence \"00197_driving_outdoor3\" from VisEvent dataset, and the sequence \"adapter01_indoor\" from DepthTrack dataset, as shown in Fig. 5. Max pooling empha-sizes prominent responses in sparse signals, retaining the most significant local features, making it particularly effective for capturing sparsity, such as locally active areas in event streams. In contrast, average pooling calculates regional averages, smooths data, and reduces redundancy, making it well-suited for processing local geometric information. The combination of these two pooling operations can complement each other, enabling the extraction of sparse, significant features while preserving smooth global information. As illustrated in Fig. 5, it is evident that TCOA module effectively optimizes multi-modal features whether implemented with all linear layers or enhanced with average and max pooling. In the se-quence \"ab_motocometurn1\", unlike the sparse characteristics of depth and event data, the combination of RGB-T features provides richer target information. Consequently, the TCOA module with linear layers is sufficient for RGB-T tracking. Conversely, from sequences \"00197_driving_outdoor3\" and \"adapter01_indoor,\" it can be concluded that incorporating"}, {"title": "D. Exploration Analysis", "content": "Cross-modal dependency analysis. The differences in cross-modality transferability arise from the distinctive char-acteristics of each modality, such as intrinsic information content, sparsity, redundancy, and task alignment. Table VIII presents the results of exploring dependency and transferabil-ity across modalities by sending auxiliary features to other branches. Depth demonstrates the lowest transferability to event and thermal modalities, with the largest negative changes observed: a decrease of -10.8% in $\\sigma$ when depth features are transferred to the thermal branch and -18.5% when transferred to the event branch. This indicates that depth suffers significant performance degradation when its features are utilized in other modalities. Although event and thermal modalities exhibit better cross-modality robustness, they still experience notable reductions in SR and PR. The thermal modality demonstrates moderate transferability, with $\\sigma$ reductions of -8.8% when transferred to the event branch and -12.3% when transferred to the depth branch. The event modality exhibits the highest transferability, with $\\sigma$ reductions of less than 7%, highlighting its comparative resilience during cross-modality transfer.\nThese findings suggest that depth data exhibit limited generalizability, likely attributable to their strong dependence on structural and geometric information. In contrast, event data demonstrate greater generalizability, potentially due to their sparse and dynamic characteristics, which enable more flexibility across diverse tasks. Thermal features demonstrate moderate transferability, occupying an intermediate position relative to depth and event data.\nComparison of computation cost and speed. Table VII compares speed, training parameters, training flops, and performance of our proposed UASTrack with state-of-the-art trackers, including separated training tracker TBSI and"}, {"title": "V. CONCLUSION", "content": "In this paper, we present a novel unified RGB-X tracker that incorporates modality-customization and adaptive selection in single object tracking. Specifically, we propose a Discrimina-tive Auto-Selector to enable dynamic adaptation across various RGB-X tracking tasks. Additionally, we introduce a Task Customization Optimization Adapter to facilitate task-specific customization, thereby enhancing the robustness and accuracy of the tracker. Our approach not only bridges the gap between single-modality pre-training and multi-modal deployment but also establishes the first unified RGB-X tracker capable of operating without prior modality types. Experimental results"}, {"title": "", "content": "demonstrate the effectiveness of our method, showing signifi-cant improvements over SOTA trackers in all RGB-X tracking scenarios."}]}