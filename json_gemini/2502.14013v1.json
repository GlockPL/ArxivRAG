{"title": "Appeal prediction for AI up-scaled Images", "authors": ["Steve G\u00f6ring", "Rasmus Merten", "Alexander Raake"], "abstract": "Abstract-DNN- or AI-based up-scaling algorithms are gaining\nin popularity due to the improvements in machine learning. Var-\nious up-scaling models using CNNs, GANs or mixed approaches\nhave been published. The majority of models are evaluated using\nPSRN and SSIM or only a few example images. However, a\nperformance evaluation with a wide range of real-world images\nand subjective evaluation is missing, which we tackle in the\nfollowing paper. For this reason, we describe our developed\ndataset, which uses 136 base images and five different up-scaling\nmethods, namely Real-ESRGAN, BSRGAN, waifu2x, KXNet, and\nLanczos. Overall the dataset consists of 1496 annotated images.\nThe labeling of our dataset focused on image appeal and has been\nperformed using crowd-sourcing employing our open-source tool\nAVRate Voyager. We evaluate the appeal of the different methods,\nand the results indicate that Real-ESRGAN and BSRGAN are\nthe best. Furthermore, we train a DNN to detect which up-scaling\nmethod has been used, the trained models have a good overall\nperformance in our evaluation. In addition to this, we evaluate\nstate-of-the-art image appeal and quality models, here none of\nthe models showed a high prediction performance, therefore we\nalso trained two own approaches. The first uses transfer learning\nand has the best performance, and the second model uses signal-\nbased features and a random forest model with good overall\nperformance. We share the data and implementation to allow\nfurther research in the context of open science.\nIndex Terms-AI enhanced images, image up-scaling, image\nappeal", "sections": [{"title": "I. INTRODUCTION", "content": "The recent developments in image processing focus on AI-\nbased image enhancement and such methods are able to re-\nplace traditional signal-based approaches. In general, AI-based\nmethods are used for several image enhancement problems,\nfor example, de-noising [27], up-scaling [46, 9, 7, 48], image\nin-painting [32], or re-colorization [38]. The used models to\nperform the tasks are typically based on deep neural networks\n(DNNs), e.g., GANs [45] (generative adversarial networks),\nU-net [36], or Autoencoders [4]. The published models are\nusually evaluated using objective metrics, such as PSNR, or\nSSIM. Instead of such objective metrics, which may have\nalso their limitations, we focus in the following on subjective\nevaluation of up-scaling algorithms. Because DNNs do not\nuse traditional approaches for up-scaling, it can happen that\nthe used method introduces completely new image parts or\nnew types of distortions. To properly evaluate such generated\nimages and estimate which algorithm performs best or is most\nsuitable for an application, human annotations are essential\nand required. Therefore, it is needed to evaluate the appeal\nand quality of such generated images. In general, it can be\nobserved that there is a strong link between image appeal and\nimage quality, especially when there are no explicit encoding\ndistortions considered [11]. Furthermore, as shown in [10]\nAI-generated content may be harder to analyze with current\nmodels and features for image appeal. These are the reasons\nwhy we focus in our evaluation on image appeal.\nIt has been shown that AI-based image processing, gener-\nation, or enhancement introduces new types of artifacts [10,\n11], e.g., as it is also highlighted in Figure 1. In this example,\nit is visible that a kind of synthetic look (c.f. BSRGAN x4) of\nthe resulting images or different types of noises or blurriness\n(compare KXNet x4) are introduced. Such new distortions\nrequire specifically trained and optimized image appeal and\nquality models. To train and develop such models, an ex-\nplicitly annotated dataset is required for a proper evaluation.\nIn the context of quality assessment and appeal of images,\nseveral approaches are possible to gather the required human\nannotations. For example, crowd-sourcing has been widely\nused in various studies before, where it shows promising\nresults [18, 34, 16, 11, 10], e.g., for the evaluation of images\nand AI-generated content considering appeal or quality. It is\nalso shown, that such crowd-sourcing tests can be used to\ngather highly reliable results, which are similar as compared\nto traditionally conducted lab tests. Furthermore, it is also\npossible, with some adjustments, to evaluate high-resolution\nimages or videos with remote testing [18], where in contrast,\nfor example, some up-scaling methods are only evaluated\nusing a few low-resolution images (e.g. BSRDM [47]).\nIn the following paper, we focus on five up-scaling meth-\nods and design an online study for the evaluation. We\nselected recently published DNN-based up-scaling methods\nnamely BSRGAN [48], KXNet [9], Real-ESRGAN [46], and\nwaifu2x [30], and we also included a Lanczos filter up-scaling\napproach. Overall for all of the methods included, we consider\ntwo up-scaling factors, namely times two and four, in our\nevaluation. Our resulting dataset consists of 1496 images with\nhuman appeal ratings, it covers all up-scaling methods with\nthe two settings and all original images. The used source\nimages originate from our own image subset (\u201cown"}, {"title": "II. RELATED WORK", "content": "Image up-scaling is a well-analyzed research field [40, 44],\nwith Nearest-neighbor interpolation being one of the simplest\napproaches or other signal-based methods that are popular, as\nfor example the Lanczos filter [8]. The general problem is\nto provide a high-resolution image based on a low-resolution\ninput image, considering e.g. the details and content of the\nimage. In the following, we focus on recently published DNN-\nbased approaches for image up-scaling.\nIn general, the recent advances in deep learning showed\npromising results of using DNNs for various image processing\ntasks, where up-scaling is just one example. For the problem\nof image up-scaling, also handled as super-resolution, such\ndeep learning-based approaches can be categorized into CNN-\nbased, RNN-CNN-based, and GAN-based models [20] or\neven more fine granular [1]. CNN-based models are, e.g.,\nSRCNN [7], waifu2x [30], neural-enhance [39] or VDSR [26],\nhere the low-resolution image is usually extended by fea-\ntures learned from a convolutional neural network and then\nfused together to form the up-scaled version. The RNN-CNN-\nbased approaches, such as MemNet [42], use recurrent neural\nnetworks with memory to enhance the down-scaled image.\nThe last model group is GAN-based models, for example,\nmodels of this type are SRGAN [28], Real-ESRGAN [46], or\nBSRGAN [48]. In general GAN-based models consist of two\nnetworks, a generative network and a discriminative network,\nwhere the generative network upsamples the image in case\nof super-resolution and the discriminative network is used\nto distinguish the ground truth and up-scaled images. It is,\ne.g., shown that SRGAN [28] outperforms signal-based and\nCNN/RNN-CNN-based approaches. Real-ESRGAN [46] has\nbeen trained on real-world content and also showed promising\nresults compared to other models. There are also models with\nmixed approaches, such as KXNet [9], or VDVAE-SR [4]\navailable. Furthermore, not all models, e.g., BSRDM [47]\nare applicable for higher resolution input images with their\nprovided open-source implementation, mainly because they\nare just trained and evaluated on smaller inputs as a proof-of-\nconcept. In addition, some proposed methods are just evaluated\nwith low-resolution input images due to performance reasons.\nIn most cases, the evaluation focuses primarily on objective\nmetrics such as SSIM, PSNR, or a few example images for\ndemonstration purposes, which is for example the case for\nKXNet [9], BSRGAN [48], or Real-ESRGAN [46]. How-\never, the complexity and generation of content using DNNS\nintroduce new types of distortions, that the used image qual-\nity models cannot handle properly, because they have been\ndeveloped with different distortions. For pure performance\nreasons, it is clear, that PSNR or SSIM can be used for the\nevaluation to compare the generated image with the high-\nresolution version. However, some of the models may still\nperform reasonably well even though they do not match per-\nfectly the high-resolution reference image, considering that the\nintroduced new content or artifacts may look appealing or real.\nObjective quality models need re-training for such specific\ngenerated contents, as it is also shown for AI-generated images\nin [11]. More recent no-reference image quality models may\nbe applicable to evaluate the quality, e.g., DBCNN [49],\nHYPERIQA [41], Deimeq [14], or NIMA [29]. These image\nquality models are mostly based on DNNs combined with\ntransfer learning and outperform for image compression and\ncommon distortions other state-of-the-art models. Thus they"}, {"title": "III. DATASET", "content": "To create the dataset, we used four recently published\nor established DNN-based approaches for image up-scaling,\nnamely, BSRGAN [48], KXNet [9], Real-ESRGAN [46], and\nwaifu2x [30]. In addition to the DNN-based methods, we up-\nscaled the images using ImageMagick\u00b2 (version 6.9.10-23)\nwith the Lanczos filter. The Lanczos up-scaling algorithm is\nincluded in our dataset to compare the newly developed meth-\nods with traditional signal-based algorithms. The selection\nwas based on recent models with available and usable open-\nsource implementations, published trained models, which are\ntrained for real photo content, and are applicable for higher\nresolution output images (1080p). The used implementations\nare listed in Table I. In total, we used 136 images from our\npublished AVT-ImageAppeal-Dataset [16], these 136 images\nare all from the \"own\" subset and cover a wide range of\nrealistic images. The thumbnails of the included images are\nshown in Figure 2. A general characterization considering\ndifferent features, namely CPBD [31], spatial information\n(SI) [35], nima appeal (nima_a) [29], subjective appeal ratings\n(appeal_mos) [16], tone [2], saturation [2], blur strength [6],\ncolorfulness [21] and nima quality (nima_q) [29], is shown\nin Figure 3. These features are provided in the published\ndataset [16] and are re-scaled to [0,1]-values in the Figure,\nconsidering their range of values. It is visible, that the images\nof the \"own\" subset form a representative selection of real-\nworld contents.\nAs a unification step, we re-scaled all images to a common\nheight of 1080 pixels. Afterward, we scaled them down to a\nheight of 540 pixels (540p) and 270 pixels (270p), respectively.\nThe 540p images are later up-scaled with a factor of two (x2),\nand the 270p images are up-scaled with a factor of four\n(x4). For all selected up-scaling methods we perform both up-\nscaling factors. The used implementation for Real-ESRGAN\ndid not include a proper up-scaling with x2, we used the\nmodel realesrgan-x4plus, and therefore we up-scaled using the\nfactor four and then down-scaled by two. Using the mentioned\napproach we have a total number of 136 \u00d7 (2 \u00d7 5 + 1) = 1496\nimages, we included also the 1080p original source images in\nthe subjective evaluation as a hidden reference (labeled as x1)."}, {"title": "IV. EVALUATION", "content": "Using the created dataset we designed an online crowd-\nsourcing test for the evaluation of image appeal. For the"}, {"title": "V. DETECTION", "content": "To evaluate which up-scaling method has been used, we\ntrained various DNNs in a multi-class classification setup\nsimilar to the approach used in [19, 15]. We use a pre-trained\nbaseline DNN, which has been trained for ImageNet [37],\nand perform transfer-learning [43]. Because our images are\nhigh-resolution images, and considering that we want to detect\nthe up-scaling method, we split each 1080p image in several\npatches each with a size of 224x224 (with no overlap). This\nsize was selected because it is the input size of the majority of\nthe considered baseline DNNs. Afterwards a baseline-specific\npre-processing is performed within the network. The split\napproach of the high-resolution images resulted in 40.832\npatches, which are used for training and validation. For the\nbaseline DNN, we removed the last classification-specific layer\nand added a flattening layer. This is followed by a dropout\nlayer with a rate of 0.2, a dense fully connected layer with n\noutput signals (n = 6 for the five up-scaling methods and\nthe source images), and softmax activation. The flattening,\ndropout, and dense layers are specific to our classification task\nand are the parameters to be trained. The approach allows\nthe usage of several pre-trained baseline DNNs that are used\nas they are, thus no re-training for the baseline models is\nperformed. The implementation is based on Keras [5] using\nPython 3.9.\nIn total, we trained and evaluated 16 models, using a 90%-\n10% train-validation splitting of the dataset. The performance\nvalues, considering accuracy, f1-score, precision, recall and\nMatthew's correlation coefficient (mcc) for all models are\nlisted in Table III. Best performing models are DenseNet\nor ResNet model variants, worst performing are Inception\nand VGG based models. The best performing model was\nDenseNet121, with an accuracy of \u2248 0.74, f1-score \u2248 0.74,\nprecision \u2248 0.75 and recall \u2248 0.74 for the validation data. In\nFigure 8 the confusion matrix for the DenseNet121 model is\nshown.\nIn general, this evaluation shows that it is possible to\ndetect which up-scaling method has been used, assuming all\nalgorithms are known beforehand. The approach is also to be\nseen as a proof-of-concept. For a better detection approach\nmore images and more algorithms would be required."}, {"title": "VI. APPEAL PREDICTION", "content": "Similarly, as compared to the detection prediction, we use\ntransfer learning, with in total 16 DNNs. Instead of the\nclassification layer as the last layer, we added a dense layer\nwith 1024 signals and ReLu activation and an additional dense\nlayer with one output signal and softmax activation for the final\nprediction. The mean appeal scores have been normalized to\n[0, 1]. We used a 90%-10% train-validation split, and center\ncropped the images before to 224x224 which is the input\nsize of the DNNs, center cropping has been successfully used\nbefore for image appeal and video quality evaluation [18, 13].\nThe performance values of all models are listed in Table IV.\nThe results indicate that appeal prediction for the considered\nup-scaling algorithms is possible, however, it should be men-\ntioned that the overall number of images may be too limited for\nthe general applicability of the model. Thus the numbers are"}, {"title": "VII. CONCLUSION AND FUTURE WORK", "content": "We started with the observation that DNN-based up-scaling\nalgorithms seem to perform better than traditional signal-\nbased approaches. However, the majority of studies do not\ncompare several of such algorithms and usually do not in-\nclude a large-scale subjective evaluation. For this reason, we\nselected five different open-source algorithms, namely Real-\nESRGAN, KXNet, BSRGAN, waifu2x, and Lanczos, and\ncreated a dataset considering two up-scaling factors (x2 and\nx4). This dataset consists of 136 base images, a subset from the\nAVT-ImageAppeal-Dataset [16], which have been up-scaled\nto 1496 variants using the algorithms and further extended\nby human annotations considering image appeal. We selected\nimage appeal as an evaluation criterion, due to a high similarity\nto image quality and also because such new algorithms may\nintroduce artifacts that are artificial. To gather the annotations\nwe carried out a crowd-sourcing test, and showed in the\nevaluation that the most appealing model in most cases is\nReal-ESRGAN. The second best model is BSRGAN. While\nLanczos has not been preferred considering image appeal for\nany included image and up-scaling factor. In addition to this,\nwe used the images to train a detection DNN to classify\nwhich up-scaling method has been used, the approach shows\npromising results. The best-performing DNN for the detection\nis DenseNet121. Furthermore, image appeal can be also pre-\ndicted automated using DNNs and partially with state-of-the-\nart models. Here, our transfer-learned DNN outperforms other\nmodels, the model model was ResNet152V2. However, also\nsignal-based features can be used to predict image appeal in\nthis context, as we show by training a random forest regression\nmodel using various extracted signal features. Overall, it can\nbe stated that AI-based up-scaling algorithms introduce new\ndistortions, which image quality or appeal models must be\nadjusted to. Here, more tests with a larger number of source\nimages considering more up-scaling methods are required for\nthe training and development of quality and appeal prediction\nmodels. In future work, newer ai-based up-scaling methods\ncould be used for video up-scaling, here the temporal co-\nherence of the generated images is also an important aspect,\nwhich needs to be analyzed."}]}