{"title": "An Agentic Al Workflow for Detecting Cognitive Concerns in\nReal-world Data", "authors": ["Jiazi Tian", "Liqin Wang", "Pedram Fard", "Valdery Moura Junior", "Deborah Blacker", "Jennifer S. Haas", "Chirag Patel", "Shawn N.\nMurphy", "Lidia M.V.R. Moura", "Hossein Estiri"], "abstract": "Early identification of cognitive concerns is critical but often hindered by subtle symptom\npresentation. This study developed and validated a fully automated, multi-agent Al workflow using\nLLAMA 3 8B to identify cognitive concerns in 3,338 clinical notes from Mass General Brigham. The\nagentic workflow, leveraging task-specific agents that dynamically collaborate to extract meaningful\ninsights from clinical notes, was compared to an expert-driven benchmark. Both workflows achieved\nhigh classification performance, with F1-scores of 0.90 and 0.91, respectively. The agentic workflow\ndemonstrated improved specificity (1.00) and achieved prompt refinement in fewer iterations.\nAlthough both workflows showed reduced performance on validation data, the agentic workflow\nmaintained perfect specificity. These findings highlight the potential of fully automated multi-agent Al\nworkflows to achieve expert-level accuracy with greater efficiency, offering a scalable and cost-\neffective solution for detecting cognitive concerns in clinical settings.", "sections": [{"title": "Introduction", "content": "Recent advancements in large language models (LLMs) have significantly enhanced natural\nlanguage processing and artificial intelligence, demonstrating impressive capabilities in\nlanguage generation and contextual understanding.1\u20134 These innovations promise to improve\nclinical workflows by efficiently processing and interpreting complex medical data. 5,6\nIntegrating LLMs into dementia screening offers transformative potential for early detection and\nintervention.7,8 Early identification of cognitive decline is crucial for effective treatments, such as\nFDA-approved beta-amyloid-targeting drugs like lecanemab and aducanumab.7,9,10 However,\ntraditional screening methods like the Mini-Mental State Examination (MMSE) and Montreal\nCognitive Assessment (MoCA) often face challenges in accessibility and scalability due to their\nreliance on in-person administration.11\u201313\nEmerging research shows that LLMs can address these limitations by providing scalable, cost-\neffective screening tools. They can analyze language patterns - such as syntax, semantics, and\nlexical diversity - to detect subtle cognitive impairments indicative of early-stage dementia with\nhigh accuracy.1 .14,15 Additionally, LLMs can be integrated into telehealth platforms, enabling\nremote screening and increasing access for underserved populations. 16,17\nPrompt engineering is crucial for optimizing LLM performance, especially in high-precision\nclinical applications where subtle phrasing changes can cause inconsistent or inaccurate\nresults. 1819 However, manually refining prompts is time-consuming and requires specialized\nexpertise. To overcome these challenges, recent research has shown that LLMs can power\nautonomous agents capable of mimicking human behavior in interactive, context-sensitive\ntasks. This advancement enables more efficient and reliable applications, such as detecting"}, {"title": "Methodology", "content": "The primary aim of this study was to develop and test an automated multi-agent Al workflow\nthat can achieve high classification accuracy in detecting cognitive concerns while balancing\nsensitivity and specificity."}, {"title": "Clinical Setting and Data Sources", "content": "We collected clinical notes documented between January 1, 2016, and December 31, 2018,\nfrom Mass General Brigham (MGB)'s Research Patient Data Registry (RPDR), including history\nand physical exam notes, clinic visit notes, discharge summaries, and progress notes. Instead\nof segmenting the notes into chunks or sections, we utilized the full context of the original notes\nin this study. The data was analyzed between February 1, 2024, and August 31, 2024."}, {"title": "Creation of Datasets", "content": "The study cohort included 3,338 clinical notes from 200 Mass General Brigham patients who\nwere classified into two categories\u2014patients with cognitive concerns and those without-based\non manual chart review. 23 These classifications were used as the reference standard for this\nstudy. We framed the identification of cognitive concerns as a binary classification task. We split\nthis data randomly to curate a prompt refinement data set and an out-of-sample validation data"}, {"title": "Large Language Model", "content": "We utilized the LLaMA 3 8B22, developed by Meta Al, which is an open-source LLM capable of\nrunning locally, thereby ensuring the privacy of sensitive data. We obtained access to the\nmodel's weights through Meta and the Hugging Face library. We configured the temperature\nparameter to 0.1 to allow some flexibility in answer generation while mitigating potential noise\nfrom the lengthy original notes. The maximum output token length was set to 256, with all other\nparameters maintained at their default settings. The model was deployed on a local Linux-based\nserver with 48 Cores (96 Threads) and 256GB of high-speed RAM."}, {"title": "Study Design", "content": "As illustrated in Figure 1, two parallel workflows were developed. An agentic workflow aimed at\nclassifying patient notes in an automated process, in which specialized agents made inferences\nthrough communications and using tools. An expert-driven workflow was created as a\nbenchmark. Initially, the model operated using a zero-shot approach to determine whether the\nnotes indicated cognitive concerns. Following this, generated knowledge prompting24 was\napplied to improve the performance of the model's responses. The prompt configuration\nincluded both system and user prompts: the system prompt defined the LLM's role, and the user\nprompt provided task-specific information and questions.\nSince each patient had at least one clinical note and the LLM operated on individual notes, a\npatient's cognitive status was determined by aggregating and analyzing all the responses\ngenerated by the LLM for their respective notes. Specifically, if any of the responses indicated\nthe presence of cognitive concerns, the patient was labeled as \"with cognitive concerns.\""}, {"title": "Agentic Workflow", "content": "The agentic workflow comprised six specialized agents to automatically refine the prompt and\nenhance the capabilities of LLM for cognitive concern detection.\nSpecialist: the role is an expert in evaluating patients with cognitive concerns. It takes the\nprompt and clinical notes as input and generates the corresponding responses, providing a\n\"yes\" or \"no\" answer along with the reason behind the decision.\nEvaluator: the role is a helpful assistant. It determines the label through the aggregation of the\nSpecialist's responses across all clinical notes of that patient. It then compares these labels to\nthe reference labels and generates an evaluation metrics report, including sensitivity, specificity,\nnegative predictive value (NPV), positive predictive value (PPV), accuracy, and F1-score.\nSpecificity improver: the role is that it has expertise in both clinical knowledge and advanced\nprompt engineering technologies. The standard operating procedure (SOP), developed in the\nprevious study 23, is given as the guidelines for identifying cognitive concerns. It analyzes false\npositive examples to identify reasons for misclassification by the LLM and uses this\nunderstanding to refine the Specialist's prompt to increase the specificity.\nSensitivity improver: the role is that it has expertise in both clinical knowledge and advanced\nprompt engineering technologies. It focuses on false negative cases, seeking evidence of\ncognitive concerns that the LLM may have missed, thereby improving sensitivity.\nSummarizer 1: the role is a helpful assistant. It summarizes the improvements suggested by\nthe Specificity improver for each case into one new improved prompt, which will be sent back to\nthe Specialist.\nSummarizer 2: the role is that it has expertise in both clinical knowledge and advanced prompt\nengineering technologies. It first summarizes the findings from the Sensitivity improver. It then\nrefines the prompt by incorporating both the findings and the SOP to enhance performance.\nThis improved prompt will be sent back to the Specialist.\nAs shown in Figure 2, if the sensitivity falls below 0.8, the workflow directs the task to the\nSensitivity Improver, followed by Summarizer 1. The improved prompt is then provided to the\nSpecialist for another iteration. A similar process is followed for specificity enhancement.\nThrough this iterative process, the agents collaboratively refine the LLM's ability to detect\ncognitive concerns, continuously optimizing performance by improving sensitivity and specificity\nwith targeted adjustments and evaluations. The workflow is configured with a default maximum\nof three iterations. The process concludes when either both sensitivity and specificity achieve a\nthreshold of at least 0.8, or when the change in sensitivity falls below 0.1."}, {"title": "Expert-Driven Workflow", "content": "The expert-driven workflow combined the capabilities of the LLM with the clinical expertise, who\nis an experienced Neurologist. As shown in Figure 3, the process began with the initial prompt\nto generate responses for individual notes. Labels were then assigned to each patient by\naggregating the responses across all their clinical notes. Next, we generated an evaluation\nmetrics report by comparing these aggregated patient-level labels to the corresponding\nreference labels. False positive and false negative cases were then randomly chosen for review\nby the clinical expert. Based on the clinician's feedback, the prompt configurations were revised,\nincluding both system and user contents. The improved prompt was resubmitted to the LLM for\nnew testing. To further optimize the prompt formulations, we utilized ChatGPT-40 through the\nuser interface."}, {"title": "Evaluation", "content": "We conducted a comprehensive performance evaluation on the two workflows by calculating\nsensitivity, specificity, accuracy, PPV, NPV, and F1-score, against the chart-reviewed (ground\ntruth) labels. Since the patient-level labels included three categories, we excluded the 'uncertain'\ncases from the calculation of evaluation metrics. Furthermore, to evaluate the generalizability of\nthe two workflows, we applied all prompt configurations to the independent validation dataset\nand evaluated their ability to accurately detect cognitive concerns from clinical notes."}, {"title": "Results", "content": "In total, 3,338 clinical notes from 200 patients were included in the study. Patients in the prompt refinement dataset (2,228 notes) had\nan average age of 77.9 years and were 60 percent female. 50 percent of patients in this dataset\nhad validated indications of cognitive concerns. The independent validation dataset (1,110\nnotes), included patients with an average age of 76.1 years, 59 percent of whom were female.\nIndications of cognitive concerns were present in 33% of patients in the validation data set. The\nmajority of the population in both datasets was White and non-Hispanic."}, {"title": "Agentic Workflow", "content": "A total of two iterations were needed for the agentic workflow to achieve the minimum\nthresholds for sensitivity and specificity (0.8)."}, {"title": "Expert-driven workflow", "content": "As presented in eTable 1, clinicians generated four distinct prompts (XP1, XP2, XP3 and XP4)\nin the expert-driven workflow to refine the model's performance. After the clinician reviewed a\nrandomly selected subset of false positive and false negative cases from P0, the scope of\ncognitive concerns was added to the subsequent prompts. Furthermore, the model's precise\nrole was elucidated. These modifications were incorporated into the XP1 and XP2 prompts.\nBesides, we refined the XP2 by adding one more question.\nThe XP3 prompt was created to exclude risk factors or normal screening results from being\nused as evidence of cognitive concerns and to avoid making assumptions based on limited"}, {"title": "Evaluating generalizability", "content": "To evaluate the generalizability of the agentic workflow, we computed performance metrics for\nthe two workflows based on the ground truth labels in the validation set (eTable 4). The final\nprompt from the expert-driven workflow (XP4) demonstrated the most balanced performance,\nachieving an F1-score of 0.79, a sensitivity of 0.70, a specificity of 0.97, a PPV of 0.92, an NPV\nof 0.86 and an accuracy of 0.88 with only 2 uncertain cases (eTable 5). The AP2 achieved an\nF1-score of 0.76, a sensitivity of 0.61, and a specificity of 1.00, with 2 uncertain cases.\nConsistent with the performance on the prompt refinement set, in the expert-driven workflow,\nprompt XP4 demonstrated the most balanced performance on the validation set. The agentic\nworkflow demonstrated an improvement over the expert-driven workflow on the prompt\nrefinement set, however, when applied to the validation set, the agentic workflow did not show\nany significant enhancement in performance compared to the expert-driven approach."}, {"title": "Discussion", "content": "In this study, we developed and evaluated a novel, fully automated agentic Al workflow that\nleverages specialized Al agents for discrete tasks, utilizing the computationally efficient LLAMA\n3 8B to screen patient charts for indicators of cognitive concerns. This multi-agent approach\nwas assessed against an expert-driven benchmark workflow, which refines prompts through\niterative review. Our findings demonstrate that LLaMA 3 8B effectively identifies cognitive\nconcern indicators in clinical notes, highlighting its potential as a moderately accessible tool for\nclinical screening. Notably, our results suggest that a fully automated agentic workflow can\nachieve comparable performance to expert-driven methods while requiring fewer iterations (2\nvs. 4), thereby reducing the associated resource burden. While challenges such as overfitting\nremain, these limitations may be mitigated by further specialization of agents. Overall, agentic"}]}