{"title": "Toward Efficient Deep Spiking Neuron Networks: A Survey On Compression", "authors": ["Hui Xie", "Ge Yang", "Wenjuan Gao"], "abstract": "With the rapid development of deep learning, Deep Spiking Neural Networks (DSNNs) have emerged as promising due to their unique spike event processing and asynchronous computation. When deployed on neuromorphic chips, DSNNs offer significant power advantages over Deep Artificial Neural Networks (DANNs) and eliminate time and energy consuming multiplications due to the binary nature of spikes (0 or 1). Additionally, DSNNs excel in processing temporal information, making them potentially superior for handling temporal data compared to DANNs. However, their deep network structure and numerous parameters result in high computational costs and energy consumption, limiting real-life deployment. To enhance DSNNs efficiency, researchers have adapted methods from DANNs, such as pruning, quantization, and knowledge distillation, and developed specific techniques like reducing spike firing and pruning time steps. While previous surveys have covered DSNNs algorithms, hardware deployment, and general overviews, focused research on DSNNs compression and efficiency has been lacking. This survey addresses this gap by concentrating on efficient DSNNs and their compression methods. It begins with an exploration of DSNNs' biological background and computational units, highlighting differences from DANNs. It then delves into various compression methods, including pruning, quantization, knowledge distillation, and reducing spike firing, and concludes with suggestions for future research directions.", "sections": [{"title": "1 Introduction", "content": "As early as 1997, Maass [52] classified Spiking Neural Networks (SNNs) as third- generation neural networks based on their computational units, specifically spiking neurons or \"integrate-and-fire neurons\". In contrast, first-generation neural networks, developed around the 1950s and known as perceptrons, were based on McCulloch-Pitts neurons. These networks struggled with problems that were not linearly separable, such as the XOR operation. To address these limitations, second-generation neural networks, or Artificial Neural Networks (ANNs), were developed, utilizing activation functions that apply a continuous set of output values to a weighted sum (or polynomial) of the inputs.\nWith the advancement of neuromorphic computing [61], SNNs have demonstrated significant potential due to their unique spike event processing and asynchronous computation capabilities. When deployed on neuromorphic chips, SNNs offer substantial"}, {"title": "2 Background", "content": "The introduction of activation functions in neural networks is primarily driven by the need to introduce non-linear functions into a linear system, thereby increasing its com- plexity and enhancing its representative capabilities. In contrast, the computational units in SNNs, such as the Leaky Integrate-and-Fire (LIF) neuron or Integrate-and-Fire (IF) neuron, are designed to more closely mimic the properties of biological neurons. This distinction in computational units is a fundamental difference between SNNs and ANNS."}, {"title": "2.1 Biological Background", "content": "A typical neuron consists of three main parts: dendrites, soma, and axon. Dendrites col- lect input signals from other neurons and transmit them to the soma. The soma acts as a"}, {"title": "LIF Model.", "content": "The Leaky Integrate-and-Fire (LIF) model, first proposed by Lapicque in 1907, describes the process of action potentials in neurons. Neurons fire impulses when the membrane potential reaches the threshold voltage $V_{threshold}$, after which the membrane potential resets to the resting potential $V_{reset}$. The LIF model focuses on the patterns of sub-threshold potential voltage variations. [14].\n$\\tau_m\\frac{dV}{dt} = V_{reset} - V + R_mI.$\nwhere $\\tau_m$ represents the membrane time constant, $V_{reset}$ is the resting potential, and $R_m$ and $I$ are the cell membrane's impedance and the input current, respectively.\nThe LIF neuron model provides a simplified representation of biological neurons, emphasizing essential features like membrane potential leakage, accumulation, and ex- citation. While its biological accuracy is limited, its simplicity makes it suitable for computational simulations."}, {"title": "Other Models.", "content": "The Hodgkin-Huxley (H-H) model, proposed by Hodgkin and Huxley in 1952 [31], offers a highly precise approximation of the principles governing biolog- ical neuron action potentials, earning them the Nobel Prize in Physiology or Medicine in 1963. Although the H-H model has high biological fidelity, it is computationally intensive. Other models, such as the Adaptive Exponential Integrate-and-Fire (aEIF) model [5] and the Izhikevich model [35], strive to balance biological fidelity and com- putational simplicity."}, {"title": "2.2 Computational Unit Of SNNS", "content": "In contemporary SNNs, neuron models predominantly rely on the LIF model. While the mathematical formulation of the LIF model involves a time-dependent differential equation, actual computer computations discretize this process for approximation.\n$\\begin{aligned}\nH[t] &= V[t - 1] + \\frac{1}{\\tau}(X[t] \u2013 (V[t - 1] \u2013 V_{reset}))\\\\\nS[t] &= \\Theta(H[t] \u2013 V_{threshold})\\\\\nV[t] &= H[t](1 - S[t]) + V_{reset}S[t]\\\\\n\\Theta(x) &= \\begin{cases}\n1 & \\text{if } x \\geq 0\\\\\n0 & \\text{if } x < 0\n\\end{cases}\n\\end{aligned}$\nwhere $\\tau$ represents the membrane time constant, $V_{reset}$ is the resting potential, $V_{threshold}$ is the threshold voltage, and $X [t]$, $H[t]$, $V[t]$ represent the input current, the membrane potential before and after spiking firing at time step $t$, respectively. The specific imple- mentation of LIF neurons can vary [18]."}, {"title": "3 Methods", "content": "DSNNs share many commonalities with DANNs, and some research has focused on these commonalities and proposed a unified pruning architecture [7]. Concepts like pruning, quantization, and knowledge distillation, initially developed for DANNs, can be adapted for DSNNs. However, the unique spiking and temporal characteristics of DSNNs often necessitate modifications to these techniques to ensure optimal perfor- mance. Additionally, DSNNs can be compressed in unique ways, such as leveraging spike sparsity, temporal coding, and pruning of time steps."}, {"title": "3.1 Pruning", "content": "DSNNs need to be deployed on neuromorphic hardware [77], which differs significantly from general-purpose computers in terms of operational logic. As a result, the value of pruning must be reassessed. Both unstructured and structured pruning are important for DSNNs, contrary to the inefficiency of unstructured pruning in DANNs [51], despite the scarcity of related work [46]."}, {"title": "Unstructured Pruning.", "content": "Unstructured pruning involves pruning weights or neurons. ADMM-method [15] utilizes the Alternating Direction Method of Multipliers (ADMM) for connection pruning based on soft constraints, demonstrating effectiveness in reduc- ing parameter memory space and baseline computational cost in DSNNs. It compresses connections, weight bit-widths, and spike frequencies simultaneously.\nESL [65] begins with a sparse network generated using the Erd\u0151s\u2013R\u00e9nyi random graph model, dynamically pruning weak connections and generating new ones accord- ing to structural plasticity rules. This approach, which updates the connection mask every $T_{itter}$ iterations and employs various growth methods, outperforms the ADMM- method."}, {"title": "GradR", "content": "[8], inspired by synapse formation and elimination in the nervous system, redefines synaptic parameters using $w = sReLU(\\theta)$, where s is determined at initial- ization and remains unchanged. It employs different training strategies for active and inactive connections, enhancing network exploration and recovery of dead neurons."}, {"title": "STDS", "content": "[9] improves on GradR by using nonlinear reparameterization with $w = sign(\\theta) (|\\theta| - d)_+,d \\geq 0$. When the size of the connection w is below threshold d, it is considered a filopodium, and the equivalent weight is zero. In any state, the parameters have gradients, allowing transitions between positive and negative weights, exploring the network space more fully. Pruning speed is adjusted by regulating the change in d."}, {"title": "S-LATS", "content": "[7], a theoretical framework that reformulates soft threshold pruning as an implicit optimization problem and solves it using the Iterative Shrinkage-Thresholding Algorithm (ISTA), which is a classic method in the fields of sparse recovery and com- pressed sensing. It is proven that in the underlying optimization problem, the L1 coeffi- cient is jointly determined by the threshold and the learning rate, allowing any threshold tuning strategy to be interpreted as a scheme for adjusting the L1 penalty.Through in- depth research on threshold scheduling based on the framework, an optimal threshold scheduler is derived, which maintains a stable L1 regularization coefficient, thereby providing a time-invariant objective function from an optimization perspective.A new family of pruning algorithms is proposed, including pruning during training, early prun- ing, and pruning at initialization, and get the best result both in DANNs and DSNNs."}, {"title": "SOPs-method", "content": "[66] introduces an synaptic operation (SOP) metric (this paper define SOP as the operations performed when a spike passes through a synapse) to quantify power consumption in SNNs and uses an energy penalty term for energy-constrained unstructured weight and neuron pruning, maximizing efficiency through sparsity. Dur- ing training, a binary mask m is reparameterized by a as $m = H(a)$ and approximated with a scaled sigmoid function as $\\lim_{\\beta\\to\\infty} \\sigma(\\alpha; \\beta) = \\frac{1}{1+e^{-\\beta\\alpha}}$. Gradual scheduling methods adjust the scale factor $\u00df$, to achieve different compressing rate."}, {"title": "Structured Pruning.", "content": "Structured Pruning focuses on channel pruning for deep convo- lutional spiking neural networks.\nJust the migration from DANNs Networks Slimming [48], DSNNs Networks Slim- ming [42] penalizes the scaling factors of the Batch-Normal layers, pruning channels with smaller scaling factors. However, due to the thresholding nature of DSNNs, the pruning is not thorough enough, achieving only 60% channel pruning while maintain- ing accuracy without much loss.\nPrincipal Component Analysis(PCA) based pruning [10] targets redundant filters, a method also used in ANNs [21], adapted for SNNs. The pruning method is modified by using the principal component analysis of neurons' average cumulative membrane potentials to determine significant spatial dimensions for structured pruning. This step results in a 10-14 fold reduction in model size.\nThe network pruning framework based on Spike Channel Activity(SCA) [43] is in- spired by synaptic plasticity mechanisms. During training, channels with lower average spike firing frequency are pruned, and convolutional kernels are dynamically adjusted"}, {"title": "Time Step Pruning.", "content": "Time step pruning is unique to SNNs due to their temporal di- mension in data input.\nIn algorithms transfer DANNs to DSNNs, a high number of time steps is needed to accurately capture the values of the ReLU activation function, requiring 20-50 steps or more. This is unacceptable for SNNs in practical applications, as it introduces step times computational and memory burdens [3].\nThe main challenge of time step pruning is that when the number of time steps is too low, neurons in the latter parts of the model cannot fire, leading to vanishing gradients. This means back-propagation cannot occur, and the model cannot be trained.\nA simple idea is progressive pruning [10,11]. First, train the model with a long time step to ensure a high spike rate in the final layer. Then, prune the time steps while the model can still produce spikes in the final layer even with fewer time steps, allowing back-propagation to proceed normally. Gradually reduce the time steps until reaching an extremely low number, potentially achieving good results even with a single time step. The test dataset used for this approach is a standard image dataset.\nShrinking SNN (SSNN) [16] addresses the issue of disappearing spikes by modify- ing the model structure, enabling the model to derive loss from the spikes of interme- diate neurons, back-propagate, and train normally. This method has shown good results but is currently limited to the DVS dataset, demonstrating its compression capabilities on datasets with temporal sequences. It may also be extendable to static datasets.\nSpiking Early-Exit Neural Networks (SEENNs) [44] focus on balancing efficiency and accuracy. This method treats the time step as a variable and filters uncertain predic- tions through a confidence threshold. For uncertain predictions, reinforcement learning is used to determine the appropriate number of time steps, achieving automatic time step adjustment. On the CIFAR-10 dataset, this method achieved an average of nearly one time step while maintaining higher accuracy compared to other methods with longer time step.\nLite [47] thoroughly explores the optimal number of time steps for each layer through neural architecture search, characterizes the energy consumption through the metric of total synaptic operations, and achieves a good balance between energy effi- ciency and accuracy by controlling the pruning strength with a penalty factor."}, {"title": "3.2 Quantization", "content": "Quantization transforms large numerical data into a discrete set of values, aiming to minimize bit usage while maintaining computational precision. In DSNNs, quantization"}, {"title": "Weight Quantization.", "content": "ADMM-Quant [15] employs the ADMM to enforce quantiza- tion constraints on weights, albeit with a limitation to weight-only uniform quantization.\nST-Quant [10] utilizes K-means clustering-based weight sharing quantization tech- niques to further compress the model, akin to the approach taken by [26] for weight quantization.\nLite [47] adopts Neural Architecture Search (NAS) to automatically discover suit- able mixed-precision bit-widths, enabling different layers to adopt varying levels of quantization. This is integrated within a unified framework that concurrently searches for optimal bit-widths, time steps, and network architectures, guided by a penalty term incorporated into the loss function for joint training."}, {"title": "Weight And Membrane Potential Quantization.", "content": "STBP-Quant [70] transforms high- precision floating-point computations in existing direct training algorithms to low-bitwidth integer operations, introducing Integer-STBP, an algorithm that facilitates training and inference of SNNs using solely integer arithmetic. This enables implementation on low- power edge devices for online learning and inference.\nMINT [76], building upon STBP-Quant, eliminates the requirement for multipli- ers in conventional uniform quantization during inference by sharing scaling factors between weights and membrane potentials. Scaling factors are retained during train- ing to ensure competitive accuracy, while their necessity is obviated during inference through shared scaling, thus removing 32-bit multipliers in hardware. By quantizing memory-intensive membrane potentials to extremely low precision (2-bits), significant reductions in memory usage are achieved."}, {"title": "Binary Quantization.", "content": "A novel weight-threshold balancing transformation [72] is pro- posed, adjusting the threshold of spiking neurons to convert high-precision weights into binary values (-1 or 1), effectively yielding binary SNNs. This drastically reduces weight memory requirements, albeit accompanied by increased neuron threshold stor- age (originally all are same)."}, {"title": "3.3 Knowledge Distillation", "content": "Knowledge distillation (KD) is a commonly used model compression method, which uses a large teacher model to guide the training of a small student model. Specifically, knowledge distillation takes the knowledge of the teacher model as a supervision signal during the training process, so that the student model not only learns from the data, but also receives guidance from the teacher model, thereby achieving better performance."}, {"title": "Knowledge distillation from SNN to SNN.", "content": "As the scale of SNNs continues to expand, its demand for storage and computing resources also gradually increases, hindering its application in real life. To address this issue, Kushawaha et al. [38] proposed the first knowledge distillation method specially designed for SNNs to minimize loss of accu- racy. They use the spiking activation tensors of the teacher and student models to simul- taneously calculate the full loss and the sliding window loss as new loss functions. Then they froze the weights of the teacher SNN and trained the student SNN. Moreover, they also introduced a multi-stage distillation procedure to further improve the performance of student SNN. Experiments on three standard image classification datasets show that their method improves the performance of student SNN."}, {"title": "Knowledge distillation from ANN to SNN.", "content": "Takuya et al. [69] proposed to use knowl- edge distillation of KL divergence to train low-latency SNN. They first utilized a large ANN as the teacher model to train a small ANN, and then converted the small ANN into a SNN. Finally, they distill knowledge from the large teacher ANN into the SNN and use approximate gradients to solve the problem of non-differentiable spikes. The exper- imental results on the CIFAR-100 dataset show that they achieved the lowest inference latency while maintaining accuracy.\nTran et al. [71] proposed a training technique to convert ANNs to SNNs which is able to learn more hidden information by using knowledge distillation. They combined knowledge distillation and batch normalization through time (BNTT) to improve the performance of converted SNNs and reduce its power consumption. Experiments on Tiny-ImageNet, CIFAR-10 and CIFAR-100 show that their method successfully im- proves accuracy and reduces inference latency.\nXu et al. [74] proposed a knowledge distillation training method combining ANN and SNN. They combined spike coding and joint loss function to solve the problem of non-differentiable SNN spikes. They proposed two knowledge distillation methods: response-based KD and feature-based KD, which extract knowledge from the last layer output of the teacher model and some intermediate layers of the teacher model respec- tively. Comprehensive experimental results demonstrate the effectiveness of their ap- proach."}, {"title": "3.4 Reducing Firing Rate", "content": "In neuromorphic hardware, a common metric for characterizing the energy consump- tion is synaptic operations [13], and reducing the number of spikes can effectively de- crease synaptic operations. Frequency coding is a commonly used encoding method, favored for its simplicity and efficiency, which is conducive to training; however, it typ- ically requires a higher number of spikes to achieve satisfactory results under this en- coding. Temporal coding, on the other hand, conveys the same information with fewer spikes, allowing at most a single spike per neuron in the neural network, effectively re- ducing the number of spikes during inference. The trade-off is longer time steps, which are typically used in the transition from ANN-SNN algorithms. Due to space constraints, we will only discuss the Temporal coding of ANN-SNN. Direct training temporal cod- ing can be seen here [73,58,57]."}, {"title": "Frequency Coding.", "content": "ADMM-method [15] reduces spike firing frequency during train- ing by imposing a penalty factor on the membrane potential. AutoSNN [55] is a direct training algorithm that adopts a one-shot weight-sharing approach based on an evolu- tionary algorithm to automatically search for and discover energy-efficient SNN archi- tectures suitable for specific tasks. The article mentions that the global average pooling layer can reduce the energy efficiency of SNNs, so the maximum pooling layer is rec- ommended."}, {"title": "Temporal Coding.", "content": "Time-to-First-Spike(TTFS) coding [62] approximates the real-valued ReLU activation in DANNs to the delay of the first spike in the corresponding spike se- quence in DSNNs, requiring at most one spike per activation. However, the cost is a longer time step, and the memory access and computational costs associated with long time steps remain high, as peak neurons need to track synapses and receive the first peak from synapses after each time step.\nTemporal-SwitchCoding (TSC), and the corresponding TSC spiking neuron model [24], is presented better then TTFS. Each pixel of the input image is presented through two spikes, with the time interval between the two spikes being proportional to the pixel in- tensity. Throughout the inference process, each synapse performs at most two memory accesses and two addition operations, significantly improving the energy efficiency of SNNs."}, {"title": "4 Future Directions", "content": "Efficient Neuron Model. There are models that increase accuracy with more parame- ters and complex structures [27,75], as well as models that precisely model biological neurons. However, there is limited research on developing more efficient neuron mod- els specifically for deep learning [54]. Two potential paths exist: one involves modeling neurons more finely but with fewer neurons in shallow networks, and the other involves modeling neurons more coarsely with a larger number of neurons in deep networks. Precise modeling of biological neurons requires substantial computational resources but offers greater representational capacity. By accurately simulating biological neu- rons, we can achieve complex network functions with fewer neurons [39,1,80], which may present an energy efficiency advantage.\nBalancing biological fidelity with computational efficiency presents a trade-off. Ex- ploring how to abstract better neuron characteristics for modeling is a key problem that warrants further investigation to realize efficient neural networks.\nUnified Compression Architecture. DANNs and DSNNs share commonalities, and their similarities may represent the fundamental characteristics of neural networks. Methods applicable only to specific types of neural networks might not address the essence of neural networks. For instance, a unified framework for soft threshold prun- ing has been proposed, revealing the characteristics of neural network soft threshold pruning [7]. Similarly, the PCA method's analysis of convolutional kernel similarity followed by pruning demonstrates the effectiveness of pruning similar components"}]}