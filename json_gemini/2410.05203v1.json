{"title": "BEYOND FVD: ENHANCED EVALUATION METRICS FOR VIDEO\nGENERATION QUALITY", "authors": ["Ge Ya (Olga) Luo", "Gian Favero", "Zhi Hao Luo", "Alexia Jolicoeur-Martineau", "Christopher Pal"], "abstract": "The Fr\u00e9chet Video Distance (FVD) is a widely adopted metric for evaluating video generation\ndistribution quality. However, its effectiveness relies on critical assumptions. Our analysis reveals\nthree significant limitations: (1) the non-Gaussianity of the Inflated 3D Convnet (I3D) feature space;\n(2) the insensitivity of I3D features to temporal distortions; (3) the impractical sample sizes required\nfor reliable estimation. These findings undermine FVD's reliability and show that FVD falls short\nas a standalone metric for video generation evaluation. After extensive analysis of a wide range of\nmetrics and backbone architectures, we propose JEDi, the JEPA Embedding Distance, based on\nfeatures derived from a Joint Embedding Predictive Architecture, measured using Maximum Mean\nDiscrepancy with polynomial kernel. Our experiments on multiple open-source datasets show clear\nevidence that it is a superior alternative to the widely used FVD metric, requiring only 16% of the\nsamples to reach its steady value, while increasing alignment with human evaluation by 34%, on\naverage.", "sections": [{"title": "1 Introduction", "content": "Video generation research has experienced a significant surge recently, yielding cutting-edge models that produce\nhigh-quality videos (Liu et al., 2024; Blattmann et al., 2023; Zeng et al., 2023; He et al., 2023). However, evaluating\ntheir generation quality poses a substantial challenge.\nA video generation model, must not only produce high-quality images, but also have high temporal consistency and\nproduce diverse videos with diverse features. For instance, at the object level, it is undesirable to have exclusively cars\nof specific brands or colors when generating automobile videos; at the motion level, it is undesirable to observe the\nsame type of motion repeatedly when generating human action videos. Thus, an ideal video generation metric must\ntackle many aspects in order to be a reliable tool for evaluating generative video models.\nResearchers have created a range of evaluation metrics and tools to assess the quality of individual outputs from video\ngeneration models. Many of the older video generation metrics are derived from image quality assessments: LPIPS,\nMSE, SSIM, PSNR. These metrics fail to quantify the temporal consistency between frames (Zhang et al., 2018; Hor\u00e9\n& Ziou, 2010). Recent video distance metrics focus on assessing both temporal and spatial qualities, as well as diversity,\nby measuring distances in distribution with respect to the real videos. The most popular such metric for video is the\nFr\u00e9chet Video Distance (FVD) (Unterthiner et al., 2019). An analog exists for measuring image quality (but not\ntemporal consistency): the Fr\u00e9chet Inception Distance (FID) (Heusel et al., 2017). These metrics have emerged as a\nleading tool for assessing the quality of video and image generation models.\nFID, primarily a image metric, is also used in video generation to compare key-frames. It computes the Fr\u00e9chet\ndistances of the frame features from Inception v3 (Szegedy et al., 2014; Ioffe & Szegedy, 2015) trained on ImageNet.\nBuilding upon the principles of FID metrics, FVD evaluates the FD between the generated and data distributions in the\nInflated 3D ConvNet (I3D)'s feature space (Carreira & Zisserman, 2018), which is trained on the Kinetics dataset (Kay\net al., 2017). FVD's use of features extracted from 3D-ConvNet allows it to capture a more comprehensive range of\nvisual and temporal information compared to FID.\nRecent studies have highlighted limitations in the reliability of the FVD measure. Specifically, Brooks et al. (2022)\ndemonstrated that FVD is not effective in capturing long-term realism and is more suitable for comparing generation\nmodel variants of the same architecture. Moreover, Skorokhodov et al. (2021) showed that FVD overlooks motion\ncollapse and is biased towards image quality, rather than video quality. Additionally, they pointed out that FVD is\nexcessively sensitive to minor implementation details, such as the specific image storage formats used (e.g., JPEG\ncompression levels or file encoding), which can lead to inconsistent and non-comparable results across different studies.\nA comprehensive study on FVD was conducted by Ge et al. (2024), which compares Fr\u00e9chet distances of features\nextracted by the I3D network (Carreira & Zisserman, 2018) and VideoMAE network (Wang et al., 2023). The study\nshows that the FVD prioritizes per-frame quality over temporal consistency when using I3D features, which they refer\nto as content-bias. Further, they suggest that using features from self-supervised models trained on content-debiased\ndata can effectively mitigate this bias in FVD. Our methodology draws inspiration from previous analysis that highlights\nshortcomings with FID (Borji, 2021; Kynk\u00e4\u00e4nniemi et al., 2023; Soloveitchik et al., 2022; Sajjadi et al., 2018).\nA separate and distinct method of evaluating videos is on the sample level, rather than the distributional level. For\nexample, (Huang et al., 2023) recently developed VBench, a comprehensive video benchmark that analyzes the\nevaluation of individual generated outputs on subject consistency, background consistency, temporal flickering, motion\nsmoothness, aesthetic quality, among others. VBench addresses the challenge of evaluating both temporal and spatial\nconsistency in video generation, but naturally fails at efficiently evaluating the generational capabilities of a model on a\ndistributional level, or the robustness of a model in generating videos outside its benchmark distribution.\nIn our proposed framework, JEDi, we address many of the problems affecting existing evaluation strategies:\n1. JEDi employs a Maximum Mean Discrepancy (MMD) metric with a polynomial kernel, eliminating the need\nfor parametric assumptions about the underlying video distribution, unlike FVD which relies on the Gaussianity\nassumption to make its metric feasible.\n2. JEDi significantly reduces the number of samples needed to make an accurate estimate by using an MMD metric in\na V-JEPA feature space, enabling reliable use in smaller datasets that do not meet the requirement when using FVD.\n3. JEDi leverages the robust representations of a V-JEPA model, which are found to be more aligned with human\nevaluations compared to FVD."}, {"title": "2 Background and Notations", "content": null}, {"title": "2.1 Video Feature Representation", "content": "Inflated 3D ConvNet: The Inflated 3D ConvNet (I3D) (Carreira & Zisserman, 2018) is a convolutional neural network\nmodel based on the pre-trained Inception-v1. It extends the 2D convolutional filters to 3D by replicating them along"}, {"title": "2.2 Fr\u00e9chet Distance and Fr\u00e9chet Video Distance", "content": "Fr\u00e9chet Distance (FD), also known as 2-Wasserstein distance (W2), is a way of measuring how similar two distributions\nare (Frechet, 1957; Dowson & Landau, 1982; Zilly et al., 2020). The Fr\u00e9chet distance between two distributions P and\nQ is defined as the minimum distance between all pairs of random variables x and y from the distributions. Assuming\nP and Q are multivariate Gaussian distributions, it can be expressed as:\n$D_{Fr\u00e9chet}(P,Q) = ||\\mu_P - \\mu_Q||^2 + Tr(\\Sigma_P + \\Sigma_Q - 2(\\Sigma_P\\Sigma_Q)^{1/2})$   (1)\nwhere $\u00b5_P$ and $\u00b5_Q$ are the means, while $\u03a3_P$ and $\u03a3_Q$ are the covariance matrices of the two Gaussian distributions.\nWithout making this assumption, the Fr\u00e9chet Distance is intractable and becomes much more arduous to obtain.\nThe Fr\u00e9chet Inception Distance (FID) and Fr\u00e9chet Video Distance (FVD) correspond to the above equations, but\nthe distance is applied in the space of InceptionV3 and I3D network features, respectively, instead of directly in raw\nimage space in order to obtain more meanful distance that better align with human preferences (Szegedy et al., 2014;\nUnterthiner et al., 2019)."}, {"title": "2.3 Other Distribution Distance Metrics", "content": "This study also explores the application of alternative statistical methods to compute probability distribution distances\nin video feature spaces, including Mixture Wasserstein (MW2), Energy Statistics, and kernel-based methods such as\nMaximum Mean Discrepancy (MMD).\nThe detailed backgrounds of these metrics are provided in Appendix A.4."}, {"title": "3 Examining FVD: Feature Spaces and the Gaussianity Assumption", "content": "The Fr\u00e9chet distance (FD) measures the difference between means and covariances. This can offer insights into the first\ntwo moments of the distributions, but fails to do so with respect to higher-order moments (e.g., skewness, kurtosis)\nthat arise when either the real or generated data distribution is non-Gaussian. According to Jayasumana et al. (2024),\nthe reliance on Gaussianity assumptions in FID research can lead to substantial inaccuracies when the underlying\nimage distribution does not come from such a distribution. This part of the study focuses on the video feature spaces,\ninvestigating the accuracy of Gaussian assumptions and considering the consequences of the Fr\u00e9chet Video Distance\n(FVD) when those assumptions are not met."}, {"title": "4 The Dual Challenge of Convergence: High-Dimensional Feature Spaces and Limited\nSamples", "content": "The following section addresses two pivotal challenges in evaluating video distribution distances:\n1. The Dimensionality Problem (Section 4.1): We examine the limitations of metrics relying on distribution assumptions\n(e.g., Fr\u00e9chet distance, Mixture Wasserstein distance), highlighting the adverse impact of high dimensionality.\n2. Sample Efficiency and Convergence (Section 4.2): We discuss the sample efficiency issue affecting all metrics and\nthe necessary sample size for trustworthy measurements."}, {"title": "4.1 Challenge #1: The Curse of Dimensionality", "content": null}, {"title": "Impact of Data Dimension on Fr\u00e9chet Distance Metric", "content": "In the previous section, we have shown that the Fr\u00e9chet Distance (FD) can be used as a metric for comparing the\ndiscrepancy between the first two moments of two distributions. Consequently, the accuracy of the mean and covariance\nestimators is crucial for ensuring the validity of FD as a metric. In the following part, we will explore the impact of\ndata dimensionality and sample size on the quality and precision of these estimators, and examine how this affects the\nreliability of distribution distance metrics.\nThe rank of the empirical covariance matrix (\u2211) is tied to the number of samples (n) and the dimension (k). Given a\nmatrix X containing n observations, where each column vector represents a k-dimensional multivariate sample, the\nempirical covariance matrix can serve as a reliable estimator for the true covariance matrix. The empirical covariance\nmatrix is calculated using the formula: $ \\sum = \\frac{1}{n-1} \\sum_{i=1}^{n} (X_i - \\overline{X})(X_i - \\overline{X})^T $, and it consistently converges to the true \u03a3\nat a rate of $\\frac{1}{\\sqrt{n}}$.\nIt is crucial to recognize that when the number of samples is less than the number of variables (n < k), the covariance\nmatrix becomes singular. A good covariance estimator requires a sample size that is sufficiently large, ideally at least\nseveral times greater than the data dimension. This is because estimating the covariance matrix involves estimating\nk(k + 1)/2 parameters, which requires a sufficiently large number of samples to achieve accurate estimates (Bickel &\nLevina, 2008; Mar\u010denko & Pastur, 1967; Wang; Jonsson, 1982). Unfortunately, the high-dimensional nature of I3D\n(400), VideoMAE (1408), and V-JEPA (1280) representation spaces exacerbates this issue.\nFurthermore, optimal transport methods with complex distributional assumptions require more samples yet. The\nMixture Wasserstein (MW2) experiment described in Appendix E.1 highlights significant computational and practical\nlimitations of optimal-transport type metrics, making them impractical for this project. See Appendix E.1 for further\ndiscussion."}, {"title": "Data Transformation: Dimensionality Reduction", "content": "To address the challenges posed by the curse of dimensionality, dimension reduction techniques such as PCA and\nautoencoders (Lecun, 1987) can be applied. Our preliminary investigation from Appendix E.1 suggests that decreasing\nthe representation dimension could enable the metric to converge with a smaller number of samples using metrics like\nFr\u00e9chet Distance. We test that hypothesis by training autoencoders in various feature spaces to reduce dimensionality.\nOur autoencoder architectures consist of simple multilayer perceptron networks, which compress feature dimensionality\nto either $\\frac{1}{2}$ of original size for I3D features or $\\frac{1}{3}$ of original size for VideoMAE and V-JEPA features. Additional\ninformation about the autoencoder training is available in Appendix E.2.\nInterestingly, dimension reduction significantly enhances the sample efficiency of the Fr\u00e9chet Distance and energy\nstatistic metrics in our experiments with Gaussian data. However, its benefits are less pronounced for video\nfeatures, resulting in only marginal improvements. Nonetheless, we retained autoencoder features\nin subsequent experiments to investigate other possible benefits."}, {"title": "4.2 Challenge #2: Sample Efficiency and Data Scarcity", "content": "Building on the insights from Sections 4.1, we recognize the critical importance of sufficient sampling for accurate\nestimation of metrics. This section delves into the relationship between sample size and convergence rate for each\nmetric, exploring its impact across various feature spaces. Here, we define convergence rate as the rate at which the\ndistance between training and testing set feature stabilizes as the number of video sample increases. It is a measure of\nsample efficiency.\nPrevious studies in the image and audio domains have shown that as the sample sizes N decrease, Fr\u00e9chet Distances\nincrease (Bi\u0144kowski et al., 2021; Gui et al., 2024; Jayasumana et al., 2024; Chong & Forsyth, 2019). This sensitivity\nto sample size is a common phenomenon among distributional distance metrics: As sample sizes increase, distance\nmetrics become more reliable and accurate. However, while all metrics benefit from additional data, some converge to\nthe true underlying distance more quickly than others.\nWe investigate the sample size required to achieve convergence within a 5% error margin to average metric distance\nmeasured at 5,000 samples. Our analysis in Figure 3 spans two diverse datasets: UCF-101 (human action recognition)\nand Something-Something-v2 (SSv2, hand gesture recognition). An extended analysis on more datasets is found in\nAppendix E.3."}, {"title": "5 Metric Distance Analysis: Noise, Generative Models, and Human Study", "content": "This section explores the effects of videos distorted with noise and videos generated at varying model training\ncheckpoints on metric reliability, assessing their impact on: (1) metric accuracy, (2) sample efficiency and (3) human\nmetric alignment."}, {"title": "5.1 Noise & Generation Models and Their Impacts on Metric Measurement", "content": "This study investigates metric reliability when presented with videos affected by three noise distortion types (salt\nand pepper noise, temporal blur and elastic distortion) and two image-to-video generation models (I2V-Stable Video\nDiffusion and Open-Sora).\nSalt and pepper noise, a type of impulsive noise, spatially corrupts visual data by randomly altering pixel values to\nextreme intensities. Elastic noise distortion from (Ge et al., 2024) primarily introduces temporal distortions and\noccasionally deforms object shapes. In addition, we introduce temporal blur noise which involves applying Gaussian\nkernels of varying strengths to blur frames, preserving appearance and shape integrity while focusing on temporal\ndistortion. The two generative models we used were adopted from open-source repositories, utilizing the provided\ncheckpoints (Zheng et al., 2024; von Platen et al., 2022). Detailed inference configurations for these models are in\nAppendix G.\nWe highlight some of our results in Table 1, and the remaining results are in Figures 17 and 18. The key findings in\nthese experiments include:\n1. Metrics in the I3D feature space are impacted by salt and pepper noise (a spatial distortion) significantly more than\nby other types of distortions. This aligns with the findings of Ge et al., demonstrating that the I3D feature space is\nhighly sensitive to spatial distortions but less responsive to temporal distortions. As shown in Section 5.4, I3D does\nnot align with human preferences with respect to distortions.\n2. I3D and VideoMAE are not ideal feature spaces for building video quality metrics, as they do not capture blur\ndistortion well. Notably, they perceive a testing distribution with slight artificial blur added to the frames as more\nsimilar to the training distribution than the original testing distribution. In fact, they estimate the low-blur distorted\nvideos are 10%-20% closer to the ground-truth training distribution.\n3. Our experimental evaluation reveals that the choice of feature space have a greater impact on distance values than the\nchoice of distribution distance metric. Moreover, the metrics exhibit consistent performance across diverse datasets,\ndemonstrating robustness."}, {"title": "5.2 Metric Robustness Assessment With Progressive Distortion level and Training Duration", "content": "Distortion Level In Figure 4, we conduct a study to evaluate the performance of metrics in assessing various blur\nnoise distortions on the UCF-101 dataset. All metrics identified the decline in video quality as noise levels increased.\nTraining Duration We further aim to evaluate metrics for generative models, focusing on their ability to track\nchanges in video quality throughout training (Figure 5). Due to the computational expense of training video generation\nmodels from scratch, we fine-tune Stable Video Diffusion's weights on the BDD dataset using Ctrl-V's code (Luo\net al., 2024). Ctrl-V uses a pre-trained SVD model but modifies the input padding strategy to enable multi-frame\nconditioning. Initially, the visual quality of generated videos is poor, but it improves over the training time. We utilize\nthese fine-tuning steps to assess our metrics' robustness in evaluating fine-tuned model checkpoints. We expect a good\nmetric to decrease steadily over time and thus have a negative correlation close to 1 in magnitude. We visualize several\ncheckpoint generations in Figure 22.\nResults Only JEDi (V-JEPASSv2+MMDPOLY) successfully tracks incremental gains in all checkpoints, whereas FVD\n(I3D+FD), VideoMAESSv2+MMDPOLY and V-JEPAPT+MMDPOLY do not."}, {"title": "5.3 Sample Efficiency Under Noise Distortion", "content": "Alongside Section 5.1, we investigate the sample efficiency of various metrics under noisy conditions. Specifically, we\nmeasure the number of samples it takes for the distance between the original training distribution and the noise-added\ntesting distribution to stabilize/converge. The noise-added testing set essentially simulates a set of generations. Our\nfindings indicate that: JEDi remains much more sample efficient compared to FVD in this condition. We present our\nexperiment results in Appendix F.2."}, {"title": "5.4 Human Evaluation", "content": "To investigate human alignment on the perception of video quality degradation under various noise distortions, we\nconduct a small scale survey. We randomly select 24 videos from each of the UCF-101 and Sky Scene test sets,\noriginally captured at 30 frames per second (fps), and subsample them to 25 frames at 7 fps to generate clips three\nseconds in length. Four types of noise distortions are systematically applied: blur at two increasing levels, elastic\ndistortion, and salt and pepper noise. The specific parameters for each distortion are found in Appendix F.3. To mitigate\nborder effects resulting from elastic distortion, all videos are center-cropped to 230 \u00d7 310 pixels.\nSeparate surveys for the UCF-101 and Sky Scene datasets are conducted in a randomized order, presenting participants\nwith anonymized video pairs differing only in noise type. Participants evaluate each pair of noise types under four\ndistinct comparisons, rating either one video as superior in quality or indicating no observable difference. Each\ncomparison is rated by 20 independent raters sourced from an academic community. The raters are not aware of the\ndetails of the datasets or distortion applied and rate the videos purely on visual quality alone.\nFollowing the Analytic Hierarchy Process (AHP) (Saaty, 1987), a pairwise comparison matrix is used to aggregate the\nresponses. A priority vector for the noise distortion types is found by normalizing the pairwise comparison matrix by\ncolumn and then averaging it by row. To align the priority vector with the scale used in distribution distance metrics,"}, {"title": "6 Conclusion", "content": "In this study, we carefully look at many aspects of video metrics and find JEDi to be the best choice.\nFirst, we show that the normality assumption made by the Fr\u00e9chet Distance (FD) does not hold true in the video feature\nspaces, and this becomes more evident as the duration of the videos increases.\nSecond, we discuss two challenges with FD. 1) Estimating the covariance matrix, which is required in FD, is challenging\ndue to the high dimensionality of the latent space. However, we found no enhancement from reducing this dimension\nusing autoencoders. 2) The sample efficiency of FD is low; through extensive comparison, we find that Maximum Mean\nDiscrepancy (MMD) with V-JEPA exhibits much higher sample efficiency across all datasets tested.\nThird, we investigate the impact of noise on feature spaces and found that I3D was more sensitive to image quality\ndistortion than temporal distortion, and that I3D and VideoMAE does not capture blur distortion well. On the other\nhand, V-JEP stands out as the more robust feature space among them.\nFourth, we observe the correlation between metric with distortion level and training duration. We show that while both\nFVD (FD+I3D) and JEDi (MMD+V-JEPA) are positively correlated with distortion level (higher distance with\nhigher distortion), only JEDi is highly negatively correlated to training duration (lower distance with more training).\nFinally, we complete a human alignment study on the perception of video quality under various noise distortions.\nThe results of the study suggest that among all the metrics investigated, JEDi has the highest similarity to human\npreferences.\nBased on our comprehensive analysis, JEDi emerges as the most effective and practical metric for guiding the current\nsurge in video generation research. To facilitate the usage of JEDi, we provide simple and easy-to-use code that, given\nits striking benefits, hope the community will embrace."}, {"title": "Limitations", "content": "While we performed extensive study of different feature space with different distance metrics on different\ndatasets, there are more datasets to be tested on and more types of noise distortions can be investigated. While we\nchoose JEDi (MMDPOLY+V-JEPA) as our main proposed method, other choices (see Figure ?? for the list) such\nas the Energy distance with V-JEPAPT has slightly higher alignment with human evaluation. It is a decision we made\nconsidering the large gain in sample efficiency from JEDi."}]}