{"title": "Reasoning Elicitation in Language Models via Counterfactual Feedback", "authors": ["Alihan H\u00fcy\u00fck", "Xinnuo Xu", "Jacqueline Maasch", "Aditya V. Nori", "Javier Gonz\u00e1lez"], "abstract": "Despite the increasing effectiveness of language models, their reasoning capabilities remain underdeveloped. In particular, causal reasoning through counterfactual question answering is lacking. This work aims to bridge this gap. We first derive novel metrics that balance accuracy in factual and counterfactual questions, capturing a more complete view of the reasoning abilities of language models than traditional factual-only based metrics. Second, we propose several fine-tuning approaches that aim to elicit better reasoning mechanisms, in the sense of the proposed metrics. Finally, we evaluate the performance of the fine-tuned language models in a variety of realistic scenarios. In particular, we investigate to what extent our fine-tuning approaches systemically achieve better generalization with respect to the base models in several problems that require, among others, inductive and deductive reasoning capabilities.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) are shown to be capable of delivering astounding performance in numerous tasks across various domains. Examples stretch from writing assistants (Gan et al., 2023), to sentiment analysis in social media (Simmering and Huoviala, 2023), and even applications in healthcare (Gonz\u00e1lez et al., 2023; Wong et al., 2023). While the ever-increasing accuracy of these systems is now undeniable, it is still rather unclear to what extent this accuracy is due to effective recall of their training data vs. a genuine ability to reason by extracting, understanding, and adapting the fundamental concepts underlying that training data (Huang and Chang, 2023; Li et al., 2023). Previous work suggests that LLMs might exhibit some emergent reasoning capabilities (Bubeck et al., 2023; K\u0131c\u0131man et al., 2023). However, many have observed a significant reasoning-recall gap: LLMs still perform substantially better on recall-based tasks that do not explicitly require reasoning (Zhang et al., 2023a; Ahn et al., 2024; Seals and Shalin, 2024).\nMotivated by this discrepancy between how well LLMs can recall vs. reason, our goal in this paper is to see whether they can be fine-tuned explicitly to improve their reasoning. While reasoning can take different forms, we will focus on causal reasoning as it provides us with a clear distinction between recall and reasoning\u00b9: the former is limited to inferring statistical correlations, whereas the latter involves working with interventions and counterfactuals (Pearl, 2000). It has been previously shown that LLMs struggle with counterfactual questions compared to purely factual questions (Jin et al., 2024; Gonz\u00e1lez and Nori, 2024). This difficulty highlights the recall-reasoning discrepancy within the causal domain (Figure 1)."}, {"title": "2 Fine-tuning for Reasoning", "content": "World Model. We consider a causal world model, in which X (cause) and Y (effect) are two binary variables, indicating the absence or presence of some conditions. We will denote with x, y the values taken by X and Y respectively when the conditions they represent are present, and with x', y' the complements of these values (i.e. the values taken by X and Y when the conditions they represent are absent). The context, denoted by U, consists of all exogenous variables. Without any loss of generality, we assume that all randomness in the model is captured through these exogenous variables, and all endogenous variables, including X and Y, are deterministic functions of the exogenous variables (i.e. the context U). We denote these deterministic functions as \\(X = f_X(U)\\) and \\(Y = f_Y(X,U)\\). Additionally, we denote the the potential effects under the potential interventions for each unit in the population as \\(Y_{X=x} = Y|do(X = x) = f_Y(x, U)\\) and \\(Y_{x'} = Y|do(X = x') = f_Y(x', U)\\).\nLanguage Model. We can estimate different effects using a language model. Formally, let q(u) be a factual question template that describes the world model in natural language and asks what the factual effect would be for a specific context u. Denoting the language model by l, let \\(a = l(q(u))\\) be the model's answer to this question, which will be in natural language form. To transform the answer into binary form, we use a mapping h such that \\(\\hat{Y} = h(a) = h(l(q(u)))\\) \u2208 {y, y'}. Similar to the factual case, suppose we also have interventional question templates \\(q_x(u)\\) and \\(q_{x'}(u)\\) that describe the world model. However, these templates ask for the potential effects under interventions \\(do(X = x)\\) or \\(do(X = x')\\). This leaves us with the following estimates for the two potential effects. For a given context u, we rely on the factual question template when the effect is factual, and on the interventional question template when the effect is counterfactual:\n\n\\(\\hat{Y} = \\begin{cases} h(l(q(U))) & \\text{if } X = x \\\\ h(l(\\bar{q}_x(U))) & \\text{if } X = x' \\end{cases}\\)\n\\(\\hat{Y} = \\begin{cases} h(l(\\bar{q}_x(U))) & \\text{if } X = x \\\\ h(l(\\bar{q}_{x'}(U))) & \\text{if } X = x' \\end{cases}\\)\n\nProblem. Let P describe the context distribution such that \\(U \\sim P\\). Moreover, let \\(P_{X\\rightarrow Y}\\) denote the corresponding distribution of cause \\(X = f_X(U)\\) and potential effects \\(Y_x(U) = f_Y(x,U)\\), \\(Y_{x'} (U) = f_Y(x', U)\\) such that \\(X,Y_x, Y_{x'} \\sim P_{X\\rightarrow Y}\\). Suppose we are interesting in optimizing some metric \\(V[l; P_{X\\rightarrow Y}] \\in \\mathbb{R}\\) that measures the reasoning performance of the language model l for the cause-effect relationship \\(P_{X\\rightarrow Y}\\) (we discuss the design of V in the subsequent section). Then, the problem of fine-tuning for reasoning can be expressed as\n\n\\(\\text{maximize } V[l; P_{X\\rightarrow Y}] \\text{ given } l_0, P, D = \\{P_{X_i\\rightarrow Y_i}\\}_{i}\\)\n\nwhere \\(l_0\\) is the target language model, and D is the set of different cause-effect relationships \\(P_{X_i\\rightarrow Y_i}\\) that are available as demonstrations. These relationships may involve causes \\(\\{X_i\\}\\) and effects \\(\\{Y_i\\}\\) other than the cause X or the effect Y of interest. We refer to the case where only the cause-effect relationship of interest is demonstrated such that \\(D = \\{P_{X\\rightarrow Y}\\} \\) as the \u201cin-domain\u201d problem.\nModes of Generalization. As we have discussed in the introduction, an in-domain evaluation is not sufficient alone to assess the success of fine-tuning for reasoning. Therefore, we categorize different ways in which reasoning can generalize that is, how D might relate to \\(P_{X\\rightarrow Y}\\) when \\(P_{X\\rightarrow Y} \\notin D\\). We identify four main structures, summarized in Figure 2:\n(i) Common-Cause: When the relationship \\(X \\rightarrow Y\\) is demonstrated, common-cause generalization refers to the ability to reason about other relationships \\(X \\rightarrow Y\\) that involve the same cause X."}, {"title": "3 Metrics of Reasoning", "content": "Having defined the problem of fine-tuning for reasoning, we now discuss what would be a good measure of reasoning ability (i.e. a good choice for V). In Section 3.1, we define error rates based on the correctness of answers given by the language model to individual questions. In Section 3.2, we go beyond these simple error rates and propose various inconsistency rates that capture the causal consistency between the factual and counterfactual answers given within the same context. As emphasized in the introduction, such consistency is necessary to identify causal relationships such as necessity and sufficiency. Later, in Section 4, we will describe various methods for generating datasets that aim to optimize either of these metrics."}, {"title": "3.1 Correctness", "content": "Ignoring relationship between factual and counterfactual effects, the correctness of an individual answer \\(a = l(q(u)) | l(\\bar{q}_x(u)) | l(\\bar{q}_{x'}(u))\\) can be characterized by the factual error rate (F-ER) and the counterfactual error rate (CF-ER) respectively:\n\\(\\text{F-ER} = P\\{\\hat{Y} \\neq Y\\}\\)\n\\(\\text{CF-ER} = P\\begin{cases} \\hat{Y}_{x'} \\neq Y_{x'} & \\text{if } X = x \\\\ \\hat{Y}_x \\neq Y_x & \\text{if } X = x' \\end{cases}\\)\nwhere Y, \\(\\hat{Y}_x\\), and \\(\\hat{Y}_{x'}\\) represent the binary values implied by the answer a. Using these two metrics, we define the average error rate as Avg-ER = (F-ER + CF-ER)/2.\nWhy are factual and counterfactual correctness alone not enough? Being able to correctly estimate factuals (cf. F-ER) or counterfactuals (cf. CF-ER) is, of course, an important step in causal reasoning. However, what we ultimately want is to characterize the relationship between a cause and its effect. For instance, is the cause necessary for the effect to occur? Is it sufficient? Or do the cause and the effect only occur together (necessary and sufficient)? Identifying such relationships rely on the estimated factuals and counterfactuals collectively-only getting one right but not the other might not always lead to a correct characterization of the cause-effect relationship. By measuring the factual and counterfactual accuracy separately, F-ER and CF-ER fail to capture any dependencies between the two answers and how they might be describing a larger relationship together.\nAs a concrete example, consider necessity. According to Pearl (1999), when a cause X and an effect Y occur together (i.e. \\(X = x\\) and \\(Y = y\\)), the cause is said to have been necessary for the effect if the effect would not have occurred in the absence of the cause (i.e. \\(Y_{x'} = y'\\)). Making an accurate judgement regarding whether there is a necessity relationship between X and Y requires both Y and \\(Y_{x'}\\) to be correct when \\(X = x\\) and \\(Y = y\\). However, no factual or counterfactual estimate needs to be correct when \\(X = x'\\) (as it is immediately apparent that cases where \\(X = x'\\) do not affect necessity), and similarly, only the factual estimates needs to be correct when \\(X = x\\) but \\(Y = y'\\). F-ER and CF-ER do not account for this complex requirement at all. In particular, depending on how X and Y are distributed, a language model can achieve F-ER and CF-ER as high as 1/2 by always estimating either \\(Y_x\\) or \\(Y_{x'}\\) correctly (but not both together) while never reaching an accurate conclusion regarding necessity."}, {"title": "3.2 Causal Consistency", "content": "Previous work (Gonz\u00e1lez and Nori, 2024) has considered the use of \u201cprobabilities of causation\" together with F-ER and CF-ER to provide a set metrics that fully characterize the relationship between a cause and its effect. Similar to necessity, Pearl (1999) also provides a causal definition of sufficiency: whether the cause would have produced the effect (i.e. \\(Y_x = y\\)) when both the cause and the effect are absent (i.e. \\(X = x'\\) and \\(Y = y'\\)). The probability of necessity (PN) and the probability of sufficiency (PS) are defined as:\n\\(\\text{PN} := P\\{Y_{x'} = y'|X = x, Y = y\\}\\)\n\\(\\text{PS} := P\\{Y_{x} = y|X = x',Y = y'\\}\\)\nThe answers given by the language model to factual and counterfactual questions and the effects \\(\\hat{Y}\\), \\(\\hat{Y}'\\) estimated from those answers naturally induce an empirical pair of PN and PS values:\n\\(\\text{PN} = P\\{\\hat{Y}_{x'} = y'|X = x, Y = y\\}\\)\n\\(\\text{PS} = P\\{\\hat{Y}_{x} = y|X = x',\\hat{Y} = x'\\}\\)\nWhy are PN and PS correctness alone not enough? To evaluate reasoning in language models, Gonz\u00e1lez and Nori (2024) use (1) a probabilistic measure (\\(y\\)-overlap) to assess how well the distributions of PN and PS match the true PN and PS, and (2) the factual and counterfactual error rates. We refine this approach by defining unifying metrics that simultaneously take both aspects of the problem into account, thereby simplifying the evaluation process.\nDue to the averaging done by probabilities, achieving a perfect PN-PS with the language model only requires identifying correct vs. predicted marginal frequencies, without needing individual units to be accurate. Although this is captured by the factual and counterfactual error rates F-ER and CF-ER, it is convenient to have a single metric that encapsulates both dimensions of the problem. We address this by requiring the necessity or sufficiency relationships identified by the language model to be accurate on a unit-by-unit basis. A unit is a realization of the exogenous variable U. It induces the values of X and Y as well as the counterfactual outcome \\(Y_{x'}\\), where X' represents the complement of the observed X regardless of its value. Note that \\(Y_x = Y\\) is the factual outcome.\nWe focus on necessity where a unit/context might exhibit one of three situations: (i) Necessity occurs, denoted by \u201cN\u201d, meaning that both X and Y occur, \\(X = x\\) and \\(Y = y\\), and the cause was necessary for the effect, \\(Y_{x'} = y'\\). (ii) Necessity does not occur, which we denote by \u201cN\u201d, meaning that both X and Y occur but the cause was not necessary for the effect, \\(Y_{x'} \\neq y'\\). (iii) Not relevant case as necessity is concerned, which we denote by \u00d8, when neither X nor Y (or both) did occur. Since value of the context variable U fully characterizes the unit, we can define unit-wise necessity as\n\n\\(N(X,Y,Y_{x'};U) = \\begin{cases} N & \\text{if } X = x \\wedge Y = y \\wedge Y_{x'} = y' \\\\ N' & \\text{if } X = x \\wedge Y = y \\wedge Y_{x'} \\neq y' \\\\ \\emptyset & \\text{if } X \\neq x \\vee Y \\neq y \\end{cases}\\)\n\nThe necessity inconsistency rate (N-IR) is the frequency with which the language model estimates the unit-wise necessity N inaccurately:\n\\(\\text{N-IR} := E_{P(U)}[N(X,\\hat{Y},\\hat{Y}_{x'}; U) \\neq N(X,Y,Y_{x'}; U)],\\)\nwhere \\(E_{P(U)}\\) denotes the expectation over U and \\(\\hat{Y}\\), \\(\\hat{Y}_{x'}\\) are the analogous factual and counterfactuals to Y, \\(Y_x\\), estimated from the model. Remark that \\(\\text{PN} = E_{P(U)}[N = N|N \\neq \\emptyset]\\) by construction. Also note that N-IR = 0 implies that \\(\\text{PN} = \\text{PN}\\). However, errors made in different units can no longer 'balance each other out' to achieve N-IR = 0. We can also define context-wise sufficiency S in an analogous way: (i) \\(S = S\\) if \\(X = x',Y = y',Y_x = y\\), (ii) \\(S = S'\\) if \\(X = x',Y = y',Y_x \\neq y\\), and (iii) \\(S = \\emptyset\\) otherwise. This induces the sufficiency inconsistency rate S-IR = \\(P\\{S \\neq S\\}\\).\nNeither PN and PS nor the inconsistency rates N-IR and S-IR are sensitive to all answers given by the language model. This is because necessity and sufficiency only concern cases where \\(X = x, Y = y\\) and \\(X = y', Y = y'\\). For instance, when \\(X = x'\\) and \\(Y = y\\) and the factual effect has been estimated correctly such that \\(\\hat{Y} = Y\\), the counterfactual estimate \\(Y_x\\) has no impact on PN, PS, N-IR, or S-IR. Regardless of whether \\(\\hat{Y} = Y_x\\), all four quantities stay the same. To cover all possible counterfactuals we can ask a language model for, it makes sense to also evaluate counterfactuals of the type \\(Y = y|X = x,Y = y'\\) and \\(Y_x = y'|X = x',Y = y\\). Of course, the probabilities of these counterfactuals can be defined by means of PN and PS by changing the default observed state. However, here we name them as absent necessity and absent sufficiency to be explicit about the"}, {"title": "4 Fine-tuning with Counterfactual Feedback", "content": "Despite the significant differences between correctness and causal consistency, success in either metric relies on accurate estimates of counterfactual outcomes. Therefore, to solve the fine-tuning problem in (2), it is essential to leverage the counterfactual information available in demonstrations D, irrespective of the metric we aim to target as V. We present a data-centric approach to achieve this and propose three methods for generating datasets using counterfactual feedback. These datasets can then be utlised by existing algorithms for fine-tuning such as SFT or DPO.\nSupervised Counterfactual Feedback. Recall that we assumed access to an extractor h that can reduce answers given in natural language to binary outcomes \\(\\hat{y} = h(a) \\in \\{y,y'\\}\\). Now, further suppose that we can perform this extraction in reverse, denoted as H: Given a question q and the true outcome \\(Y_{true}\\) corresponding to this question, we can form a natural language answer \\(a = H(q, Y_{true})\\). In practice, we achieve this by prompting a language model to provide an answer to question q that starts with \u201cYes\u201d or \u201cNo\u201d (see the appendix for the full prompt). Based on these answers, we generate a dataset D of both factual and counterfactual questions and their answers:\n\\[D = \\{ q_f = q(U), a_f = H(q_f, Y), q_{cf} = \\bar{q}_{x'}(U), a_{cf} = H(q_{cf}, Y_{x'}) \\}_{U,X,Y,Y_{x'}\\sim D}\\]\nThis dataset can directly be used with any SFT algorithm to fine-tune the target model \\(l_0\\)."}, {"title": "5 Related Work", "content": "Reasoning Evaluation. While our work focuses on reasoning elicitation, there is a plethora of work on reasoning evaluation (Chang et al., 2024). Parmar et al. (2024) evaluate logical reasoning, Cohn and Hernandez-Orallo (2023) evaluate spatial reasoning, Gandhi et al. (2024) evaluate social reasoning, and Li et al. (2022); Jin et al. (2023); Ashwani et al. (2024); Li et al. (2024); Wang (2024) evaluate causal reasoning. Of course, being able to determine which model is better at reasoning is an important aspect of reasoning elicitation. We have explored this aspect in Section 3 building on the work of Gonz\u00e1lez and Nori (2024). This allowed us to consider relationships like necessity and sufficiency, which we have shown to require a higher level of reasoning than simply answering counterfactual prompts.\nCounterfactual Frameworks. Counterfactual frameworks have been employed to explore various aspects of large language models. For instance, Lin et al. (2024b) formulate preference alignment as a causal inference problem and de-"}, {"title": "6 Experiments", "content": "We begin with a proof-of-concept case study. We analyze a hand-crafted puzzle to assess the effectiveness of all fine-tuning techniques introduced in Section 4 when trained on different types of datasets within the context of the in-domain causal reasoning scenarios (\u00a76.1). We also address the research question posed in Section 1, i.e., to what extent the performance improvements in causal reasoning achieved through the fine-tuning process generalize across all the generalization modes (\u00a76.2). Subsequently, we use three additional real-world problems to examine our findings (\u00a76.3)."}, {"title": "6.1 In-domain Reasoning", "content": "We evaluate all fine-tuning techniques when trained on various types of demonstrations in a synthetic in-domain reasoning problem.\nExperimental Setup. See Figure 5. The puzzle describes a candy party. The context is defined by the four-dimensional random vector U = (NA, NB, NC, ND) where each element follows the same uniform distribution U(1, 12). The causal structure, derived from the narratives, is presented in the middle section of the figure. We selected A: Anna is happy or not as the cause (X), and D: Dave is happy or not as the effect (Y). The factual questions q(u) are obtained by randomly drawing values for the four numerical variables from the distribution U, The counterfactual questions \\(\\bar{q}_{x'}(u)\\) are generated by introducing an assumption that negates the cause i.e. if in the context A is \"Anna is happy\u201d based on the value of NA, the injected assumption would be \u201csuppose that Anna is not happy\u201d, and vice versa. Since we are assessing the in-domain reasoning scenario, the cause-effect demonstration used during the fine-tuning phase are likewise employed in the evaluation phase.\nWe first generate dataset D = (\\(\\{(q(u), a_f)\\}, \\{(\\bar{q}_{x'}(u), a_{cf})\\}\\)) for each fine-tuning techniques introduced in Section 4 following the algorithms shown in Appendix B. Then we fine-tune the mini version of Phi-3 (Abdin et al., 2024) on ID. We include five baselines: the base language model (Phi-3 mini) without fine-tuning (Base), the base model fine-tuned using the SFT and DPO methods on factual examples \\(\\{(q(u), a_f)\\} \\) exclusively (SFT-OnlyF and DPO-OnlyF), and the base model fine-tuned using the SFT and DPO methods on counterfactual examples \\(\\{(\\bar{q}_{x'}(u), a_{cf})\\}\\) exclusively (SFT-OnlyCF and DPO-OnlyCF). As our proposed methods, we include the base model fine-tuned using SFT, DPO, and CCF methods on both factual and counterfactual examples (SFT-F&CF, DPO-F&CF, and DPO+CCF)."}, {"title": "6.2 Modes of Generalization", "content": "In this section, we answer the question \"to what extent the performance improvements in causal reasoning achieved through the fine-tuning process generalize across all the generalization modes defined in Section 2\". As mentioned in Section 2, an in-domain evaluation alone is inadequate for fully assessing the success of fine-tuning for reasoning and differentiating it from basic recall. Therefore, we evaluate all fine-tuning methods in the generalization modes introduced in Section 2.\nExperimental Setup. To allow for the problem in Figure 5 to reflect all possible generalization modes we made slight modifications to the puzzle context, creating two variations: chain NDE and chain WDE (refer to Structure-2 and Structure-3 in Appendix C.1). The top section in Figure 6 displays all the causal structures used for each generalization mode, along with the cause-effect interventions demonstrated during the fine-tuning and evaluation phases.\nBased on the findings from the in-domain reasoning experiments (Section 6.1), where both SFT and DPO fine-tuning methods showed significantly better performance when provided with both factual and counterfactual examples, we include here only the methods SFT-F&CF, DPO-F&CF, and DPO+CCF as well as the Base model."}, {"title": "6.3 Real-world Problems", "content": "We validate our experimental findings in real-world problems from three domains.\nExperimental Setup. We present three real-world causal reasoning problems: in the Healthcare domain, we examine breast cancer treatment and develop a simplified problem that determines how different treatment options\u2014namely, radiotherapy/chemotherapy and surgery are assigned to patients based on cancer type, tumor size, and nodal involvement. This model is grounded in a real-world guideline (MD Anderson Cancer Center) and published statistics on the disease (Orrantia-Borunda et al., 2022; Sezg\u0131n et al., 2020; Carey et al., 2006). In the Engineering domain, we implement an automatic fault detection algorithm for transmission lines (Reddy et al., 2016). This algorithm aims to identify the type of fault occurring on a transmission line using three different measurements. In the Math Benchmarking domain, we select a math question from GSM8K (Cobbe et al., 2021), a widely used benchmark for evaluating language models on grade school math problems.\nResults The results for all three problems across in-domain and different generalization modes are available in Table 2 in Appendix A. Given the extensive number of experiments in this table, we have summarized the Average Error Rate (Avg-ER) and Average Inconsistency Rate (Avg-IR) scores in Table 1. For this summary, we first normalized the scores of each approach relative to the scores of the corresponding Base approach. Then, for each generalization mode (including the in-domain scenario), we calculated the average score of each tested method across all applicable problems. In Table 1, higher scores indicate more errors, and scores above 1.0 signify that the approach makes more mistakes than the Base model. We observe that: (i) In the in-domain scenario, when the fine-tuning is guided by both factual and counterfactual examples (-F&CF), the language models show a significant improvement in causal reasoning ability. (ii) Similar to what we observed in previous experiments, this improvement generalizes to most generalization modes, with the exception of common-cause and effect-based deduction. (iii) In most of modes, language models trained with causal consistency feedback (DPO+CCF) demonstrate a lower error and inconsistency rate."}, {"title": "7 Conclusion", "content": "This work introduced the problem of fine-tuning for reasoning, along with (1) a taxonomy for generalization modes, (2) multiple metrics that address the limitations of existing performance measures, and (3) methods for generating fine-tuning data with counterfactual feedback. A key limitation of our approach is the restriction of causes and effects to binary variables, which allows us to focus on high-level relationships like necessity and sufficiency present in human reasoning."}, {"title": "B Description of the Algorithms in Section 4", "content": "Algorithm 1 Supervised Counterfactual Feedback\n1: Inputs: Demonstrations D = \\(\\{P_{x_i\\rightarrow Y_i}\\} \\), question templates q, \\(\\bar{q}\\), answer generator H\n2: Output: Dataset D = \\(\\{q, a\\}\\) of questions and answer pairs\n3: D \u2190 \\(\\{ \\}\\)\n4: for \\(P_{x_i\\rightarrow Y_i} \\)\u2208 D, n \u2208 \\(\\{1, . . . , N\\}\\) do\n5: \\(u, X, Y, Y_{x'} \\sim P_{x_i\\rightarrow Y_i}\\)\n6: \\(q_f \\leftarrow q(u), q_{cf} \\leftarrow \\bar{q}_{x'}(u)\\)\n7: \\(a_f \\leftarrow H(q_f, Y_x), a_{cf} \\leftarrow H(q_{cf}, Y_{x'})\\)\n8: D \u2190 D\u222a \\(\\{(q_f, a_f), (q_{cf}, a_{cf})\\}\\)\n9: end for\nAlgorithm 2 Preference-based Counterfactual Feedback\n1: Inputs: Demonstrations D = \\(\\{P_{x_i\\rightarrow Y_i}\\} \\), question templates q, \\(\\bar{q}\\), answer extractor h, target model \\(l_0\\)\n2: Output: Dataset D = \\(\\{(q, a) > (q', a')\\}\\) of preferences over question-answer pairs\n3: D \u2190 \\(\\{ \\}\\)\n4: for \\(P_{x_i\\rightarrow Y_i} \\)\u2208 D, n \u2208 \\(\\{1, ..., N\\}\\) do\n5: \\(u, X, Y, Y_{x'} \\sim P_{x_i\\rightarrow Y_i}\\)\n6: \\(q_f \\leftarrow q(u), q_{cf} \\leftarrow \\bar{q}_{x'}(u)\\)\n7: for m\u2208 \\(\\{1, ..., M\\}\\) do\n8: \\(a_f[m] \\sim l_0(q_f), a_{cf}[m] \\sim l_0(q_{cf})\\)\n9: \\(\\hat{Y}_x[m] \\leftarrow h(a_f[m]), \\hat{Y}_{x'}[m] \\leftarrow h(a_{cf}[m])\\)\n10: end for\n11: for m\u2208 \\(\\{1, ..., M\\}\\), m' \u2208 \\(\\{1, ..., M\\}\\) do\n12: if \\(\\hat{Y}_x = \\hat{Y}_x[m] \\neq \\hat{Y}_x[m']\\) then\n13: D \u2190 D\u222a \\(\\{(q_f, a_f[m]) > (q_f, a_f[m'])\\} \\)\n14: end if\n15: if \\(Y_{x'} = \\hat{Y}_{x'}[m] \\neq \\hat{Y}_{x'}[m']\\) then\n16: D \u2190 D\u222a \\(\\{(q_{cf}, a_{cf}[m]) > (q_{cf}, a_{cf}[m'])\\} \\)\n17: end if\n18: end for\n19: end for\nAlgorithm 3 Preference-based Causal Consistency Feedback\n1: Inputs: Demonstrations D = \\(\\{P_{x_i\\rightarrow Y_i}\\} \\), question templates q, \\(\\bar{q}\\), answer extractor h, target model \\(l_0\\)\n2: Output: Dataset D = \\(\\{(q_f, a_f, q_{cf}, a_{cf}) > (q_f, a_f, q_{cf}, a_{cf})\\}\\) of preferences over dialogues\n3: D \u2190 \\(\\{ \\}\\)\n4: for \\(P_{x_i\\rightarrow Y_i} \\)\u2208 D, n \u2208 \\(\\{1, ..., N\\}\\) do\n5: \\(u, X, Y, Y_{x'} \\sim P_{x_i\\rightarrow Y_i}\\)\n6: \\(q_f \\leftarrow q(u), q_{cf} \\leftarrow \\bar{q}_{x'}(u)\\)\n7: for m\u2208 \\(\\{1, ..., M\\}\\) do\n8: \\(a_f[m] \\sim l_0(q_f), a_{cf}[m] \\sim l_0(q_f, a_f[m], q_{cf})\\)\n9: \\(\\hat{Y}_x[m] \\leftarrow h(a_f[m]), \\hat{Y}_{x'}[m] \\leftarrow h(a_{cf}[m])\\)\n10: end for\n11: for m\u2208 \\(\\{1, ..., M\\}\\), m' \u2208 \\(\\{1, ..., M\\}\\) do\n12: \\(r[m] \\leftarrow R(\\hat{Y}_x[m], \\hat{Y}_{x'}[m]; u), r[m'] \\leftarrow R(\\hat{Y}_x[m'], \\hat{Y}_{x'}[m']; u)\\)\n13: if r[m] \u2265 r[m'] then\n14: D \u2190 D\u222a \\(\\{(q_f, a_f[m], q_{cf}, a_{cf}[m]) > (q_f, a_f[m'], q_{cf}, a_{cf}[m'])\\} \\)\n15: end if\n16: end for\n17: end for"}, {"title": "C Details of the Experiments", "content": "When collecting datasets", "prompt": "nI will give you a question and its answer. Determine whether the meaning of the answer is 'POSITIVE' or 'NEGATIVE'. An answer is 'POSITIVE' if it contains phrases like 'yes'", "it holds": "correct", "true": "or similar affirmations. An answer is 'NEGATIVE' if it contains phrases like 'no'", "it does not hold": "incorrect", "false": "or similar negations. Respond only with one word: 'POSITIVE' or \u2018NEGATIVE'.\nQuestion: '{q"}, "Answer: \u2018{a}", "Is the meaning \u2018POSITIVE' or \u2018NEGATIVE'?\nSimilarly, the answer generator H, in the case of supervised counterfactual feedback, is implemented using Llama 3 8B with the following prompt:\nI will give you a question and the initial word of its answer. Complete the answer starting form the provided word. Respond only with the complete answer. Question: {q} Answer: {No/Yes}, ...\nC.1 Reasoning Problem: Puzzle\nThis hand-crafted puzzle, centered around a candy party, has been used in the experiments conducted in Section 6.1 and 6.2. Based on different generalization modes, we developed three variations of this puzzle, each featuring distinct causal structures:\nStructure-1: Bipartite Graph\n\u2022 Context: Anna, Bill, Cory, and Dave are going to a party, where the host is going to distribute candies. Anna will be happy if she gets at least 4 candies. Bill will be happy if he gets at least 6 candies. Cory will be happy if Anna and Bill are both happy or if he gets at least 8 candies. Dave will be happy if Anna and Bill are both happy or if he gets at least 10 candies. After distributing the candies, Anna gets \\(\\{N_A\\"]}