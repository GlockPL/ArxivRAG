[{"title": "Reasoning Elicitation in Language Models via Counterfactual Feedback", "authors": ["Alihan H\u00fcy\u00fck", "Xinnuo Xu", "Jacqueline Maasch", "Aditya V. Nori", "Javier Gonz\u00e1lez"], "abstract": "Despite the increasing effectiveness of language models, their reasoning capabilities remain underdeveloped. In particular, causal reasoning through counterfactual question answering is lacking. This work aims to bridge this gap. We first derive novel metrics that balance accuracy in factual and counterfactual questions, capturing a more complete view of the reasoning abilities of language models than traditional factual-only based metrics. Second, we propose several fine-tuning approaches that aim to elicit better reasoning mechanisms, in the sense of the proposed metrics. Finally, we evaluate the performance of the fine-tuned language models in a variety of realistic scenarios. In particular, we investigate to what extent our fine-tuning approaches systemically achieve better generalization with respect to the base models in several problems that require, among others, inductive and deductive reasoning capabilities.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) are shown to be capable of delivering astounding performance in numerous tasks across various domains. Examples stretch from writing assistants (Gan et al., 2023), to sentiment analysis in social media (Simmering and Huoviala, 2023), and even applications in healthcare (Gonz\u00e1lez et al., 2023; Wong et al., 2023). While the ever-increasing accuracy of these systems is now undeniable, it is still rather unclear to what extent this accuracy is due to effective recall of their training data vs. a genuine ability to reason by extracting, understanding, and adapting the fundamental concepts underlying that training data (Huang and Chang, 2023; Li et al., 2023). Previous work suggests that LLMs might exhibit some emergent reasoning capabilities (Bubeck et al., 2023; K\u0131c\u0131man et al., 2023). However, many have observed a significant reasoning-recall gap: LLMs still perform substantially better on recall-based tasks that do not explicitly require reasoning (Zhang et al., 2023a; Ahn et al., 2024; Seals and Shalin, 2024).\nMotivated by this discrepancy between how well LLMs can recall vs. reason, our goal in this paper is to see whether they can be fine-tuned explicitly to improve their reasoning. While reasoning can take different forms, we will focus on causal reasoning as it provides us with a clear distinction between recall and reasoning\u00b9: the former is limited to inferring statistical correlations, whereas the latter involves working with interventions and counterfactuals (Pearl, 2000). It has been previously shown that LLMs struggle with counterfactual questions compared to purely factual questions (Jin et al., 2024; Gonz\u00e1lez and Nori, 2024). This difficulty highlights the recall-reasoning discrepancy within the causal domain"}, {"title": "Fine-tuning for Reasoning", "content": "World Model. We consider a causal world model, in which X (cause) and Y (effect) are two binary variables, indicating the absence or presence of some conditions. We will denote with x, y the values taken by X and Y respectively when the conditions they represent are present, and with x', y' the complements of these values (i.e. the values taken by X and Y when the conditions they represent are absent). The context, denoted by U, consists of all exogenous variables. Without any loss of generality, we assume that all randomness in the model is captured through these exogenous variables, and all endogenous variables, including X and Y, are deterministic functions of the exogenous variables (i.e. the context U). We denote these deterministic functions as $X = f_X(U)$ and $Y = f_Y(X,U)$. Additionally, we denote the the potential effects under the potential interventions for each unit in the population as $Y_{X} = Y\\vert do(X = x) = f_Y(x, U)$ and $Y_{x'} = Y\\vert do(X = x') = f_Y(x', U)$.\nLanguage Model. We can estimate different effects using a language model. Formally, let q(u) be a factual question template that describes the world model in natural language and asks what the factual effect would be for a specific context u. Denoting the language model by l, let a = l(q(u)) be the model's answer to this question, which will be in natural language form. To transform the answer into binary form, we use a mapping h such that $\\hat{Y} = h(a) = h(l(q(u))) \\in \\{y, y'\\}$. Similar to the factual case, suppose we also have interventional question templates $q_x(u)$ and $q_{x'}(u)$ that describe the world model. However, these templates ask for the potential effects under interventions $do(X = x)$ or $do(X = x')$. This leaves us with the following estimates for the two potential effects. For a given context u, we rely on the factual question template when the effect is factual, and on the interventional question template when the effect is counterfactual:\n$\\hat{Y} = \\begin{cases}h(l(q(U))) \\quad \\text{if } X = x \\\\ h(l(\\bar{q}_x(U))) \\quad \\text{if } X = x' \\end{cases} \\qquad \\hat{Y}_{x'} = \\begin{cases}h(l(\\bar{q}_{x}(U))) \\quad \\text{if } X = x \\\\ h(l(q(U))) \\quad \\text{if } X = x' \\end{cases}$     (1)\nProblem. Let P describe the context distribution such that $U \\sim P$. Moreover, let $P_{X\\rightarrow Y}$ denote the corresponding distribution of cause $X = f_X(U)$ and potential effects $Y_x(U) = f_Y(x,U), Y_{x'} (U) = f_Y(x', U)$ such that $X,Y_x, Y_{x'} \\sim P_{X\\rightarrow Y}$. Suppose we are interesting in optimizing some metric $V[l; P_{X\\rightarrow Y}] \\in \\mathbb{R}$ that measures the reasoning performance of the language model l for the cause-effect relationship $P_{X\\rightarrow Y}$ (we discuss the design of V in the subsequent section). Then, the problem of fine-tuning for reasoning can be expressed as\n$\\text{maximize} V[l; P_{X\\rightarrow Y}] \\quad \\text{given} l_0, P, D = \\{P_{X_i\\rightarrow Y_i}\\}_{i}$     (2)\nwhere $l_0$ is the target language model, and D is the set of different cause-effect relationships $P_{X_i\\rightarrow Y_i}$ that are available as demonstrations. These relationships may involve causes \\{X_i\\} and effects \\{Y_i\\} other than the cause X or the effect Y of interest. We refer to the case where only the cause-effect relationship of interest is demonstrated such that $D = \\{P_{X\\rightarrow Y}\\}$ as the \u201cin-domain\u201d problem.\nModes of Generalization. As we have discussed in the introduction, an in-domain evaluation is not sufficient alone to assess the success of fine-tuning for reasoning. Therefore, we categorize different ways in which reasoning can generalize that is, how D might relate to $P_{X\\rightarrow Y}$ when $P_{X\\rightarrow Y} \\notin D$. We identify four main structures, summarized in Figure 2:\n(i) Common-Cause: When the relationship X \u2192 Y is demonstrated, common-cause generalization refers to the ability to reason about other relationships X \u2192 Y that involve the same cause X."}, {"title": "Metrics of Reasoning", "content": "Having defined the problem of fine-tuning for reasoning, we now discuss what would be a good measure of reasoning ability (i.e. a good choice for V). In Section 3.1, we define error rates based on the correctness of answers given by the language model to individual questions. In Section 3.2, we go beyond these simple error rates and propose various inconsistency rates that capture the causal consistency between the factual and counterfactual answers given within the same context. As emphasized in the introduction, such consistency is necessary to identify causal relationships such as necessity and sufficiency. Later, in Section 4, we will describe various methods for generating datasets that aim to optimize either of these metrics."}, {"title": "Correctness", "content": "Ignoring relationship between factual and counterfactual effects, the correctness of an individual answer $a = l(q(u)) \\vert l(\\bar{q}_x(u)) \\vert l(\\bar{q}_{x'}(u)$ can be characterized by the factual error rate (F-ER) and the counterfactual error rate (CF-ER) respectively:\nF-ER = $P\\{\\hat{Y} \\neq Y\\} \\qquad \\text{CF-ER = P} \\begin{cases}\\hat{Y}_x \\neq Y_x \\quad \\text{if } X = x \\\\ \\hat{Y}_{x'} \\neq Y_{x'} \\quad \\text{if } X = x' \\end{cases}$  (3)\nwhere Y, $\\hat{Y}_x$, and $\\hat{Y}_{x'}$, represent the binary values implied by the answer a. Using these two metrics, we define the average error rate as Avg-ER = (F-ER + CF-ER)/2.\nWhy are factual and counterfactual correctness alone not enough? Being able to correctly estimate factuals (cf. F-ER) or counterfactuals (cf. CF-ER) is, of course, an important step in causal reasoning. However, what we ultimately want is to characterize the relationship between a cause and its effect. For instance, is the cause necessary for the effect to occur? Is it sufficient? Or do the cause and the effect only occur together (necessary and sufficient)? Identifying such relationships rely on the estimated factuals and counterfactuals collectively-only getting one right but not the other might not always lead to a correct characterization of the cause-effect relationship. By measuring the factual and counterfactual accuracy separately, F-ER and CF-ER fail to capture any dependencies between the two answers and how they might be describing a larger relationship together.\nAs a concrete example, consider necessity. According to Pearl (1999), when a cause X and an effect Y occur together (i.e. X = x and Y = y), the cause is said to have been necessary for the effect if the effect would not have occurred in the absence of the cause (i.e. $Y_{x'} = y'$). Making an accurate judgement regarding whether there is a necessity relationship between X and Y requires both Y and $Y_{x'}$ to be correct when X = x and Y = y. However, no factual or counterfactual estimate needs to be correct when X = x' (as it is immediately apparent that cases where X = x' do not affect necessity), and similarly, only the factual estimates needs to be correct when X = x but Y = y'. F-ER and CF-ER do not account for this complex requirement at all. In particular, depending on how X and Y are distributed, a language model can achieve F-ER and CF-ER as high as 1/2 by always estimating either Yx or Yx correctly (but not both together) while never reaching an accurate conclusion regarding necessity."}, {"title": "Causal Consistency", "content": "Previous work (Gonz\u00e1lez and Nori, 2024) has considered the use of \u201cprobabilities of causation\u201d together with F-ER and CF-ER to provide a set metrics that fully characterize the relationship between a cause and its effect. Similar to necessity, Pearl (1999) also provides a causal definition of sufficiency: whether the cause would have produced the effect (i.e. $Y_x = y$) when both the cause and the effect are absent (i.e. X = x' and Y = y'). The probability of necessity (PN) and the probability of sufficiency (PS) are defined as:\nPN := $P\\{Y_{x'} = y' | X = x, Y = y\\}$     PS := $P\\{Y_x = y | X = x', Y = y'\\}$     (4)\nThe answers given by the language model to factual and counterfactual questions and the effects $\\hat{Y}, \\hat{Y}'$ estimated from those answers naturally induce an empirical pair of PN and PS values:\n$\\widehat{PN} = P\\{\\hat{Y}_{x'} = y' | X = x, Y = y\\} \\qquad \\widehat{PS} = P\\{\\hat{Y}_x = y | X = x', \\hat{Y} = x'\\}$  (5)\nWhy are PN and PS correctness alone not enough? To evaluate reasoning in language models, Gonz\u00e1lez and Nori (2024) use (1) a probabilistic measure (\u03b3-overlap) to assess how well the distributions of PN and PS match the true PN and PS, and (2) the factual and counterfactual error rates. We refine this approach by defining unifying metrics that simultaneously take both aspects of the problem into account, thereby simplifying the evaluation process.\nDue to the averaging done by probabilities, achieving a perfect PN-PS with the language model only requires identifying correct vs. predicted marginal frequencies, without needing individual units to be accurate. Although this is captured by the factual and counterfactual error rates F-ER and CF-ER, it is convenient to have a single metric that encapsulates both dimensions of the problem. We address this by requiring the necessity or sufficiency relationships identified by the language model to be accurate on a unit-by-unit basis. A unit is a realization of the exogenous variable U. It induces the values of X and Y as well as the counterfactual outcome Yx', where X' represents the complement of the observed X regardless of its value. Note that Yx = Y is the factual outcome.\nWe focus on necessity where a unit/context might exhibit one of three situations: (i) Necessity occurs, denoted by \u201cN\u201d, meaning that both X and Y occur, X = x and Y = y, and the cause was necessary for the effect, $Y_{x'}$ = y'. (ii) Necessity does not occur, which we denote by \u201c$\\,\\neg N$\u201d, meaning that both X and Y occur but the cause was not necessary for the effect, $Y_{x'} \\neq y'$. (iii) Not relevant case as necessity is concerned, which we denote by \u00d8, when neither X nor Y (or both) did occur. Since value of the context variable U fully characterizes the unit, we can define unit-wise necessity as\n$N(X, Y, Y_{x'}; U) = \\begin{cases}N \\quad \\text{if } X = x \\wedge Y = y \\wedge Y_{x'} = y' \\\\  \\,\\neg N \\quad \\text{if } X = x \\wedge Y = y/Y_{x'} \\neq y' \\\\  \\,\\emptyset \\quad \\text{if } X \\neq x' \\vee Y = y'\\end{cases}$ (6)\nThe necessity inconsistency rate (N-IR) is the frequency with which the language model estimates the unit-wise necessity N inaccurately:\nN-IR := $E_{P(U)} [N(X,\\hat{Y}, \\hat{Y}_{x'}; U) \\neq N(X,Y,Y_{x'}; U)]$,   (7)\nwhere $E_{P(U)}$ denotes the expectation over U and $\\hat{Y}, \\hat{Y}_{x'}$, are the analogous factual and counterfactuals to Y, Yx, estimated from the model. Remark that $PN = E_{P(U)}[N = N|N \\neq \\emptyset]$ by construction. Also note that N-IR = 0 implies that $\\widehat{PN} = PN$. However, errors made in different units can no longer 'balance each other out' to achieve N-IR = 0. We can also define context-wise sufficiency S in an analogous way: (i) S = S if X = x',Y = y', $Y_{x'} = y$, (ii) S = S' if X = x',Y = y', $Y_{x'} \\neq y$, and (iii) S = \u00d8 otherwise. This induces the sufficiency inconsistency rate S-IR = P{S \u2260 S}.\nNeither PN and PS nor the inconsistency rates N-IR and S-IR are sensitive to all answers given by the language model. This is because necessity and sufficiency only concern cases where X = x, Y = y and X = y', Y = y'. For instance, when X = x' and Y = y and the factual effect has been estimated correctly such that $\\hat{Y} = Y$, the counterfactual estimate $Y_x$ has no impact on PN, PS, N-IR, or S-IR. Regardless of whether $Y = Y_x$, all four quantities stay the same. To cover all possible counterfactuals we can ask a language model for, it makes sense to also evaluate counterfactuals of the type Y = y|X = x,Y = y' and $Y_x = y'|X = x',Y = y$. Of course, the probabilities of these counterfactuals can be defined by means of PN and PS by changing the default observed state. However, here we name them as absent necessity and absent sufficiency to be explicit about the"}, {"title": "Fine-tuning with Counterfactual Feedback", "content": "Despite the significant differences between correctness and causal consistency, success in either metric relies on accurate estimates of counterfactual outcomes. Therefore, to solve the fine-tuning problem in (2), it is essential to leverage the counterfactual information available in demonstrations D, irrespective of the metric we aim to target as V. We present a data-centric approach to achieve this and propose three methods for generating datasets using counterfactual feedback. These datasets can then be utlised by existing algorithms for fine-tuning such as SFT or DPO. These methods are summarized in Figure 4.\nSupervised Counterfactual Feedback. Recall that we assumed access to an extractor h that can reduce answers given in natural language to binary outcomes $\\hat{y} = h(a) \\in \\{y,y'\\}$. Now, further suppose that we can perform this extraction in reverse, denoted as H: Given a question q and the true outcome $Y_{true}$ corresponding to this question, we can form a natural language answer $a = H(q, Y_{true})$. In practice, we achieve this by prompting a language model to provide an answer to question q that starts with \u201cYes\u201d or \u201cNo\u201d (see the appendix for the full prompt). Based on these answers, we generate a dataset D of both factual and counterfactual questions and their answers:\n$D = \\{ \\{q_f = q(U), a_f = H(q_f, Y)\\}, \\{q_{cf} = \\bar{q}_{x'}(U), a_{cf} = H(q_{cf}, Y_{x'})\\} \\}_{U, X, Y, Y_{x'}\\sim D}$\nThis dataset can directly be used with any SFT algorithm to fine-tune the target model l0.\nPreference-based Counterfactual Feedback. SFT can be limited by the quality of answers generated as ground-truth and their similarity to the model's original answers. Without access to a language model that is already better at reasoning than our target model, it might challenging to build an answer generator H that provides high quality samples. In that case, it is desirable to provide direct feedback to the answers generated by the target language model. We do so by first generating multiple answers to different questions (using a high sampling temperature to get sufficient variation):\n$D = \\{ U, q_f = q(U), a_f^{[1]} \\sim l_0(q_f), \\dots, a_f^{[N]} \\sim l_0(q_f), q_{cf} = \\bar{q}_{x'}(U), a_{cf}^{[1]} \\sim l_0(q_{cf}), \\dots a_{cf}^{[N]} \\sim l_0(q_{cf}) \\}_{U, X, Y, Y_{x'}\\sim D}$     (8)\nThen, we form a preference-based dataset where correct answers are preferred over incorrect answers:\n$a_f^{[i]} > a_f^{[j]} \\implies 1\\{h(a_f^{[i]}) = Y\\} > 1\\{h(a_f^{[j]}) = Y\\}$    (9)\n$a_{cf}^{[i]} > a_{cf}^{[j]} \\implies 1\\{h(a_{cf}^{[i]}) = Y_{x'}\\} > 1\\{h(a_{cf}^{[j]}) = Y_{x'}\\}$   (10)\nThe DPO algorithm can directly be used with this dataset to maximize the likelihood of preferred answers (i.e. a[i]) relative to the answers they are preferred over (i.e. a[j]).\nPreference-based Causal Consistency Feedback. Running DPO with preferences determined by a reward function, where alternatives with higher rewards are preferred over those with lower rewards, is equivalent to maximizing that reward function (Rafailov et al., 2024). In our case, this means that running DPO with the above preferences would, in effect, minimize the average error rate (i.e. Avg-ER), as these preferences are generated by treating correctness (i.e. $1\\{h(a) = Y = \\hat{Y}\\}$) as a reward function. To target the inconsistency rates introduced in Section 3.2, we propose to (i) pair factual and counterfactual questions, (ii) prompt the target language model to answer them simultaneously, and then (iii) elicit preferences based on the joint answer. Formally,\n$\\left(a_f^{[i]}, a_{cf}^{[i]}\\right) > \\left(a_f^{[j]}, a_{cf}^{[j]}\\right) \\leftrightarrow R\\left(h(a_f^{[i]}), h(a_{cf}^{[i]}); U\\right) > R\\left(h(a_f^{[j]}), h(a_{cf}^{[j]}); U\\right)$    (11)\nwhere $R(\\hat{Y}, \\hat{Y}_{x'}; U)=1\\{N=\\widehat{N}\\}+1\\{S=\\widehat{S}\\}+ 1\\{A N=\\overline{A N}\\}+ 1\\{AS = \\overline{AS}\\}$. We call this causal consistency feedback (CCF) (see Figure 4b vs. 4c). CFF explicitly targets Avg-IR rather than Avg-ER and can still be used directly with the DPO algorithm."}, {"title": "Related Work", "content": "Reasoning Evaluation. While our work focuses on reasoning elicitation, there is a plethora of work on reasoning evaluation (Chang et al., 2024). Parmar et al. (2024) evaluate logical reasoning, Cohn and Hernandez-Orallo (2023) evaluate spatial reasoning, Gandhi et al. (2024) evaluate social reasoning, and Li et al. (2022); Jin et al. (2023); Ashwani et al. (2024); Li et al. (2024); Wang (2024) evaluate causal reasoning. Of course, being able to determine which model is better at reasoning is an important aspect of reasoning elicitation. We have explored this aspect in Section 3 building on the work of Gonz\u00e1lez and Nori (2024). This allowed us to consider relationships like necessity and sufficiency, which we have shown to require a higher level of reasoning than simply answering counterfactual prompts.\nCounterfactual Frameworks. Counterfactual frameworks have been employed to explore various aspects of large language models. For instance, Lin et al. (2024b) formulate preference alignment as a causal inference problem and develop an alternative approach to algorithms like DPO. In Wu et al. (2021); Nguyen et al. (2024), counterfactual inputs are used to explain a model's predictions. Additionally, Kandpal et al. (2023); Zhang et al. (2023b) model memorization through counterfactuals.\nFine-tuning with Factual Feedback. Finally, while we fine-tune language models for reasoning with counterfactual feedback, previous work has considered fine-tuning for factuality: providing factually correct answers to questions that do not involve no interventions (Tian et al., 2023; Tong et al., 2024; Lin et al., 2024a). Khalifa et al. (2020); Korbak et al. (2022a;b) propose methods for controlled generation, which aim to constrain a model's answers using binary reward functions. While they suggest using correctness as the reward function to improve factuality, we have shown that targeting metrics like causal consistency require more fine-grained feedback (beyond a binary reward)."}, {"title": "Experiments", "content": "We begin with a proof-of-concept case study. We analyze a hand-crafted puzzle to assess the effectiveness of all fine-tuning techniques introduced in Section 4 when trained on different types of datasets within the context of the in-domain causal reasoning scenarios (\u00a76.1). We also address the research question posed in Section 1, i.e., to what extent the performance improvements in causal reasoning achieved through the fine-tuning process generalize across all the generalization modes (\u00a76.2). Subsequently, we use three additional real-world problems to examine our findings (\u00a76.3)."}, {"title": "In-domain Reasoning", "content": "We evaluate all fine-tuning techniques when trained on various types of demonstrations in a synthetic in-domain reasoning problem.\nExperimental Setup. See Figure 5. The puzzle describes a candy party. The context is defined by the four-dimensional random vector U = (NA, NB, NC, ND) where each element follows the same uniform distribution U(1, 12). The causal structure, derived from the narratives, is presented in the middle section of the figure. We selected A: Anna is happy or not as the cause (X), and D: Dave is happy or not as the effect (Y). The factual questions q(u) are obtained by randomly drawing values for the four numerical variables from the distribution U, The counterfactual questions $\\bar{q}_{x'}(u)$ are generated by introducing an assumption that negates the cause i.e. if in the context A is \"Anna is happy\u201d based on the value of NA, the injected assumption would be \u201csuppose that Anna is not happy\u201d, and vice versa. Since we are assessing the in-domain reasoning scenario, the cause-effect demonstration used during the fine-tuning phase are likewise employed in the evaluation phase.\nWe first generate dataset $D = \\{(\\{q(u), a_f\\}), (\\{\\bar{q}_{x'}(u), a_{cf}\\})\\}$ for each fine-tuning techniques introduced in Section 4 following the algorithms shown in Appendix B. Then we fine-tune the mini version of Phi-3 (Abdin et al., 2024) on ID. We include five baselines: the base language model (Phi-3 mini) without fine-tuning (Base), the base model fine-tuned using the SFT and DPO methods on factual examples { (q(u), af) } exclusively (SFT-OnlyF and DPO-OnlyF), and the base model fine-tuned using the SFT and DPO methods on counterfactual examples { (q(u), af) } exclusively (SFT-OnlyCF and DPO-OnlyCF). As our proposed methods, we include the base model fine-tuned using SFT, DPO, and CCF methods on both factual and counterfactual examples (SFT-F&CF, DPO-F&CF, and DPO+CCF)."}, {"title": "Modes of Generalization", "content": "In this section, we answer the question \"to what extent the performance improvements in causal reasoning achieved through the fine-tuning process generalize across all the generalization modes defined in Section 2\". As mentioned in Section 2, an in-domain evaluation alone is inadequate for fully assessing the success of fine-tuning for reasoning and differentiating it from basic recall. Therefore, we evaluate all fine-tuning methods in the generalization modes introduced in Section 2.\nExperimental Setup. To allow for the problem in Figure 5 to reflect all possible generalization modes we made slight modifications to the puzzle context, creating two variations: chain NDE and chain WDE (refer to Structure-2 and Structure-3 in Appendix C.1). The top section in Figure 6 displays all the causal structures used for each generalization mode, along with the cause-effect interventions demonstrated during the fine-tuning and evaluation phases.\nBased on the findings from the in-domain reasoning experiments (Section 6.1), where both SFT and DPO fine-tuning methods showed significantly better performance when provided with both factual and counterfactual examples, we include here only the methods SFT-F&CF, DPO-F&CF, and DPO+CCF as well as the Base model.\nResults. The bottom section in Figure 6 presents the causal reasoning performance of all systems across the different generalization modes. We observe that: (i) Common-Cause/Effect. Fine-tuning based on demonstrations that involve just the target cause or the target effect (but not both as in the in-domain case) no longer leads to improvements in S-IR (unlike the in-domain case). While we do see improvements in N-IR, this can be attributed to better recall and not necessarily to better reasoning. The common-effect case leads to the greater improvement in N-IR precisely because the task of identifying factuals remains the same in this mode of generalization. (ii) Induction. Fine-tuning generalizes best when performed inductively. This is because relationships involving both"}, {"title": "Real-world Problems", "content": "We validate our experimental findings in real-world problems from three domains.\nExperimental Setup. We present three real-world causal reasoning problems: in the Healthcare domain, we examine breast cancer treatment and develop a simplified problem that determines how different treatment options\u2014namely, radiotherapy/chemotherapy and surgery are assigned to patients based on cancer type, tumor size, and nodal involvement. This model is grounded in a real-world guideline (MD Anderson Cancer Center) and published statistics on the disease (Orrantia-Borunda et al., 2022; Sezg\u0131n et al., 2020; Carey et al., 2006). In the Engineering domain, we implement an automatic fault detection algorithm for transmission lines (Reddy et al., 2016). This algorithm aims to identify the type of fault occurring on a transmission line using three different measurements. In the Math Benchmarking domain, we select a math question from GSM8K (Cobbe et al., 2021), a widely used benchmark for evaluating language models on grade school math problems. A detailed explanation of these three problems, including the context, factual and counterfactual questions, causal structures, and the cause-effect interventions demonstrated during the fine-tuning and evaluation phases across different generalization modes, can be found in Appendix C.2, C.3, C.4 respectively.\nResults The results for all three problems across in-domain and different generalization modes are available in Table 2 in Appendix A. Given the extensive number of experiments in this table, we have summarized the Average Error Rate (Avg-ER) and Average Inconsistency Rate (Avg-IR) scores in Table 1. For this summary, we first normalized the scores of each approach relative to the scores of the corresponding Base approach. Then, for each generalization mode (including the in-domain scenario), we calculated the average score of each tested method across all applicable problems. In Table 1, higher scores indicate more errors, and scores above 1.0 signify that the approach makes more mistakes than the Base model. We observe that: (i) In the in-domain scenario, when the fine-tuning is guided by both factual and counterfactual examples (-F&CF), the language models show a significant improvement in causal reasoning ability. (ii) Similar to what we observed in previous experiments, this improvement generalizes to most generalization modes, with the exception of common-cause and effect-based deduction. (iii) In most of modes, language models trained with causal consistency feedback (DPO+CCF) demonstrate a lower error and inconsistency rate."}, {"title": "Conclusion", "content": "This work introduced the problem of fine-tuning for reasoning, along with (1) a taxonomy for generalization modes, (2) multiple metrics that address the limitations of existing performance measures, and (3) methods for generating fine-tuning data with counterfactual feedback. A key limitation of our approach is the restriction of causes and effects to binary variables, which allows us to focus on high-level relationships like necessity and sufficiency present in human reasoning."}, {"title": "Description of the Algorithms in Section 4", "content": "Algorithm 1 Supervised Counterfactual Feedback\n1: Inputs: Demonstrations $D = \\{P_{x_i \\rightarrow Y_i"}, "question templates q, $\\bar{q}$, answer generator H\n2: Output: Dataset D = {q, a} of questions and answer pairs\n3: D\u2190 { }\n4: for $P_{x_i \\rightarrow Y_i}$ \u2208 D, n \u2208 {1, . . ., N} do\n5: u, X, Y, $Y_{x'} \\sim P_{x_i \\rightarrow Y_i}$ Sample a context, cause, and potential effects\n6: $q_f \\leftarrow q(u)$, $q_{cf} \\leftarrow \\bar{q}_{x'}(u)$ Generate questions given the context\n7: $a_f \\leftarrow H(q_f, Y_x)$, $a_{cf} \\leftarrow H(q_{cf}, Y_{x'})$ Generate answers given the correct outcome\n8: D\u2190DU {($q_f, a_f$), ($q_{cf}, a_{cf}$)}\n9: end for\nAlgorithm 2 Preference-based Counterfactual Feedback\n1: Inputs: Demonstrations $D = \\{P_{x_i \\rightarrow Y_i} \\}$, question templates q, $\\bar{q}$, answer extractor h, target model $l_0$\n2: Output: Dataset D = {(q, a) > (q', a')} of preferences over question-answer pairs\n3: D\u2190 { }\n4: for $P_{x_i \\rightarrow Y_i}$ \u2208 D, n \u2208 {1, ..., N} do\n5: u, X, Y, $Y_{x'} \\sim P_{x_i \\rightarrow Y_i}$ Sample a context, cause, and potential effects\n6: $q_f \\leftarrow q(u)$, $q_{cf} \\leftarrow \\bar{q}_{x'}(u)$ Generate questions given the context\n7: for m\u2208 {1, ..., M} do\n8: $a_f[m"], "model\n9": "hat{Y"}, ["m"], ["m"], {"answers\n10": "end for\n11:\n12: for m\u2208 {1", "do\n13": "if $Y_x = \\hat{Y"}, ["m"], {"then\n14": "D\u2190DU {(qf", "preferences\n15": "end if\n16: if $Y_{x'"}, {"then\n17": "D\u2190DU {($q_{cf"}, {"preferences\n18": "end if\n19: end for\nAlgorithm 3 Preference-based Causal Consistency"}]