{"title": "Reasoning Elicitation in Language Models via Counterfactual Feedback", "authors": ["Alihan H\u00fcy\u00fck", "Xinnuo Xu", "Jacqueline Maasch", "Aditya V. Nori", "Javier Gonz\u00e1lez"], "abstract": "Despite the increasing effectiveness of language models, their reasoning capabilities remain underdeveloped. In particular, causal reasoning through counterfactual question answering is lacking. This work aims to bridge this gap. We first derive novel metrics that balance accuracy in factual and counterfactual questions, capturing a more complete view of the reasoning abilities of language models than traditional factual-only based metrics. Second, we propose several fine-tuning approaches that aim to elicit better reasoning mechanisms, in the sense of the proposed metrics. Finally, we evaluate the performance of the fine-tuned language models in a variety of realistic scenarios. In particular, we investigate to what extent our fine-tuning approaches systemically achieve better generalization with respect to the base models in several problems that require, among others, inductive and deductive reasoning capabilities.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) are shown to be capable of delivering astounding performance in numerous tasks across various domains. Examples stretch from writing assistants (Gan et al., 2023), to sentiment analysis in social media (Simmering and Huoviala, 2023), and even applications in healthcare (Gonz\u00e1lez et al., 2023; Wong et al., 2023). While the ever-increasing accuracy of these systems is now undeniable, it is still rather unclear to what extent this accuracy is due to effective recall of their training data vs. a genuine ability to reason by extracting, understanding, and adapting the fundamental concepts underlying that training data (Huang and Chang, 2023; Li et al., 2023). Previous work suggests that LLMs might exhibit some emergent reasoning capabilities (Bubeck et al., 2023; K\u0131c\u0131man et al., 2023). However, many have observed a significant reasoning-recall gap: LLMs still perform substantially better on recall-based tasks that do not explicitly require reasoning (Zhang et al., 2023a; Ahn et al., 2024; Seals and Shalin, 2024).\nMotivated by this discrepancy between how well LLMs can recall vs. reason, our goal in this paper is to see whether they can be fine-tuned explicitly to improve their reasoning. While reasoning can take different forms, we will focus on causal reasoning as it provides us with a clear distinction between recall and reasoning\u00b9: the former is limited to inferring statistical correlations, whereas the latter involves working with interventions and counterfactuals (Pearl, 2000). It has been previously shown that LLMs struggle with counterfactual questions compared to purely factual questions (Jin et al., 2024; Gonz\u00e1lez and Nori, 2024). This difficulty highlights the recall-reasoning discrepancy within the causal domain (Figure 1)."}, {"title": "", "content": "Adopting a causal framework also allows us to consider an LLM's ability to identify higher concepts that are essential for connecting causes to their effects in causal reasoning, such as necessity and sufficiency (Mackie, 1965; Lewis, 1973; Halpern and Pearl, 2005). For instance, a cause X is said to be necessary for an effect Y if (i) without intervention, X and Y occur together and (ii) intervening to remove X results in no Y (Pearl, 1999). Therefore, for an LLM to be able to identify that X is necessary for Y, it needs to not only determine the factual in (i) is indeed the case but also simultaneously recognize the counterfactual would have been different as in (ii). This makes identification of necessity, or similar relationships like sufficiency, a particularly good test of reasoning because it requires the LLM to understand when to recall (cf. factual thinking) vs. when to reason (cf. counterfactual thinking).\nWe improve the causal reasoning of LLMs by adapting established methods of fine-tuning. In particular, we consider supervised fine-tuning (SFT, e.g. Dai and Le (2015); Peters et al. (2018); Radford et al. (2018); Khandelwal et al. (2019); Howard and Ruder (2018), used in Ziegler et al. (2019); Ouyang et al. (2022)) and direct preference optimization (DPO, Rafailov et al. (2024), used in Tian et al. (2023); Lin et al. (2024a)). For both of these approaches, we propose procedures to generate supervised and preference-based datasets using factual questions as well as counterfactual questions. We argue that generating demonstrations on a question-by-question basis only improves the correctness of individual answers. As we discussed, identifying higher concepts such as necessity and sufficiency requires coordination between how factual and counterfactual questions are answered together. To target these higher concepts directly, we propose generating preference-based datasets over dialogues involving both factual and counterfactual questions.\nWhen the goal of fine-tuning is specifically to improve reasoning, a unique problem arises in evaluating the fine-tuned LLMs: we cannot just measure performance for a held-out set of test samples within the same reasoning task. If we do, it would be impossible to tell whether the LLM actually learned to reason or whether it is still recalling the demonstrations we have made during fine-tuning.2 Hence, measuring the generalization performance with respect to new reasoning tasks becomes crucial. We cannot expect fine-tuning on one problem instance to arbitrarily generalize to all problem instances either. So, building a systematic understanding regarding to what extent fine-tuning for reasoning should be expected to generalize becomes important as well.\nTo build that understanding, we identify different modes in which reasoning in one problem is transferred to other problems. Notably, we define inductive generalization and deductive generalization. Given a causal system where $X \\rightarrow Y \\rightarrow Z$, inductive generalization is the ability to reason about the transitive relationship $X \\rightarrow Z$ when demonstrated how to reason about $X \\rightarrow Y$ and $Y \\rightarrow Z$. Conversely, deductive generalization is the ability to reason about the relationships $X \\rightarrow Y$ and $Y \\rightarrow Z$ when demonstrated how to reason about $X \\rightarrow Z$. We show that fine-tuning for reasoning generalizes much more effectively in an inductive mode rather than a deductive mode (among many other insights in Section 6)."}, {"title": "Contributions", "content": "We have four major contributions, corresponding to each of the following sections:\n\u00a72 We describe a framework for fine-tuning based on causal reasoning and formally categorize the ways in which reasoning generalizes from one problem to another. These categories are common-effect, common-cause, inductive, and deductive.\n\u00a73 We introduce novel metrics to measure the reasoning performance of an LLM, defining necessity and sufficiency inconsistency rates (N-IR & S-IR) based on probabilities of necessity and sufficiency from the causality literature. We also introduce the concepts of absent necessity and absent sufficiency to supplement cause-effect relationships covered neither by necessity nor sufficiency.\n\u00a74 We propose procedures to generate datasets to be used with SFT and DPO to fine-tune for reasoning by incorporating counterfactual feedback. In particular, we argue for generating dialogues that involve paired factual and counterfactual questions to directly target the reasoning metrics we introduce in Section 3. We call this causal consistency feedback.\n\u00a76 Finally, we evaluate the performance of the procedures proposed in Section 4 using the metrics introduced in Section 3. Moreover, we investigate to what extent that performance generalizes in relation to our categorization in Section 2."}, {"title": "2 Fine-tuning for Reasoning", "content": "World Model. We consider a causal world model, in which $X$ (cause) and $Y$ (effect) are two binary variables, indicating the absence or presence of some conditions. We will denote with $x, y$ the values taken by $X$ and $Y$ respectively when the conditions they represent are present, and with $x', y'$ the complements of these values (i.e. the values taken by $X$ and $Y$ when the conditions they represent are absent). The context, denoted by $U$, consists of all exogenous variables. Without any loss of generality, we assume that all randomness in the model is captured through these exogenous variables, and all endogenous variables, including $X$ and $Y$, are deterministic functions of the exogenous variables (i.e. the context $U$). We denote these deterministic functions as $X = f_X(U)$ and $Y = f_Y(X,U)$. Additionally, we denote the the potential effects under the potential interventions for each unit in the population as $Y_{x} = Y|do(X = x) = f_Y(x, U)$ and $Y_{x'} = Y|do(X = x') = f_Y(x', U)$.\nLanguage Model. We can estimate different effects using a language model. Formally, let $q(u)$ be a factual question template that describes the world model in natural language and asks what the factual effect would be for a specific context $u$. Denoting the language model by $l$, let $a = l(q(u))$ be the model's answer to this question, which will be in natural language form. To transform the answer into binary form, we use a mapping $h$ such that $\\hat{Y} = h(a) = h(l(q(u))) \\in \\{y, y'\\}.^{3}$ Similar to the factual case, suppose we also have interventional question templates $q_x(u)$ and $q_{x'}(u)$ that describe the world model. However, these templates ask for the potential effects under interventions $do(X = x)$ or $do(X = x')$. This leaves us with the following estimates for the two potential effects. For a given context $u$, we rely on the factual question template when the effect is factual, and on the interventional question template when the effect is counterfactual:\n$Y = {\\begin{cases} h(l(q(U))) & \\text{if } X = x \\\\ h(l(q(U))) & \\text{if } X = x' \\end{cases}}$\n$\\hat{Y} = {\\begin{cases} h(l(\\tilde{q}_x(U))) & \\text{if } X = x \\\\ h(l(\\tilde{q}_{x'}(U))) & \\text{if } X = x' \\end{cases}}$\n(1)\nProblem. Let $P$ describe the context distribution such that $U \\sim P$. Moreover, let $P_{X\\rightarrow Y}$ denote the corresponding distribution of cause $X = f_X(U)$ and potential effects $Y_x(U) = f_Y(x,U), Y_{x'}(U) = f_Y(x', U)$ such that $X,Y_x, Y_{x'} \\sim P_{X\\rightarrow Y}$. Suppose we are interesting in optimizing some metric $V[l; P_{X\\rightarrow Y}] \\in \\mathbb{R}$ that measures the reasoning performance of the language model $l$ for the cause-effect relationship $P_{X\\rightarrow Y}$ (we discuss the design of $V$ in the subsequent section). Then, the problem of fine-tuning for reasoning can be expressed as\n$\\underset{l}{\\text{maximize}} V[l; P_{X\\rightarrow Y}] \\text{ given } l_0, P, D = \\{P_{X_i\\rightarrow Y_i}\\}$\n(2)\nwhere $l_0$ is the target language model, and $D$ is the set of different cause-effect relationships $P_{X_i\\rightarrow Y_i}$ that are available as demonstrations. These relationships may involve causes $\\{X_i\\}$ and effects $\\{Y_i\\}$ other than the cause $X$ or the effect $Y$ of interest. We refer to the case where only the cause-effect relationship of interest is demonstrated such that $D = \\{P_{X\\rightarrow Y}\\}$ as the \u201cin-domain\u201d problem.\nModes of Generalization. As we have discussed in the introduction, an in-domain evaluation is not sufficient alone to assess the success of fine-tuning for reasoning. Therefore, we categorize different ways in which reasoning can generalize\u2014that is, how $D$ might relate to $P_{X\\rightarrow Y}$ when $P_{X\\rightarrow Y} \\notin D$. We identify four main structures, summarized in Figure 2:\n(i) Common-Cause: When the relationship $X \\rightarrow Y$ is demonstrated, common-cause generalization refers to the ability to reason about other relationships $X \\rightarrow \\tilde{Y}$ that involve the same cause $X$."}, {"title": "3 Metrics of Reasoning", "content": "Having defined the problem of fine-tuning for reasoning, we now discuss what would be a good measure of reasoning ability (i.e. a good choice for $V$). In Section 3.1, we define error rates based on the correctness of answers given by the language model to individual questions. In Section 3.2, we go beyond these simple error rates and propose various inconsistency rates that capture the causal consistency between the factual and counterfactual answers given within the same context. As emphasized in the introduction, such consistency is necessary to identify causal relationships such as necessity and sufficiency. Later, in Section 4, we will describe various methods for generating datasets that aim to optimize either of these metrics.\n3.1 Correctness\nIgnoring relationship between factual and counterfactual effects, the correctness of an individual answer $a = l(o | \\tilde{q}_x(u) | \\tilde{q}_{x'}(u))$ can be characterized by the factual error rate (F-ER) and the counterfactual error rate (CF-ER) respectively:\nF-ER = $P\\{\\hat{Y} \\neq Y\\}$\nCF-ER = $P\\{{ \\begin{cases} \\hat{Y}_x \\neq Y_x & \\text{if } X = x \\\\ \\hat{Y}_{x'} \\neq Y_{x'} & \\text{if } X = x' \\end{cases}}$\n(3)\nwhere $Y, \\hat{Y}_x$, and $\\hat{Y}_{x'}$, represent the binary values implied by the answer $a$. Using these two metrics, we define the average error rate as Avg-ER = (F-ER + CF-ER)/2.\nWhy are factual and counterfactual correctness alone not enough? Being able to correctly estimate factuals (cf. F-ER) or counterfactuals (cf. CF-ER) is, of course, an important step in causal reasoning. However, what we ultimately want is to characterize the relationship between a cause and its effect. For instance, is the cause necessary for the effect to occur? Is it sufficient? Or do the cause and the effect only occur together (necessary and sufficient)? Identifying such relationships rely on the estimated factuals and counterfactuals collectively\u2014only getting one right but not the other might not always lead to a correct characterization of the cause-effect relationship. By measuring the factual and counterfactual accuracy separately, F-ER and CF-ER fail to capture any dependencies between the two answers and how they might be describing a larger relationship together.\nAs a concrete example, consider necessity. According to Pearl (1999), when a cause X and an effect Y occur together (i.e. $X = x$ and $Y = y$), the cause is said to have been necessary for the effect if the effect would not have occurred in the absence of the cause (i.e. $Y_{x'} = y'$). Making an accurate judgement regarding whether there is a necessity relationship between X and Y requires both $\\hat{Y}$ and $\\hat{Y}_{x'}$ to be correct when $X = x$ and $Y = y$. However, no factual or counterfactual estimate needs to be correct when $X = x'$ (as it is immediately apparent that cases where $X = x'$ do not affect necessity), and similarly, only the factual estimates needs to be correct when $X = x$ but $Y = y'$. F-ER and CF-ER do not account for this complex requirement at all. In particular, depending on how X and Y are distributed, a language model can achieve F-ER and CF-ER as high as 1/2 by always estimating either $\\hat{Y}_{x}$ or $\\hat{Y}_{x'}$ correctly (but not both together) while never reaching an accurate conclusion regarding necessity."}, {"title": "3.2 Causal Consistency", "content": "Previous work (Gonz\u00e1lez and Nori, 2024) has considered the use of \u201cprobabilities of causation\u201d together with F-ER and CF-ER to provide a set metrics that fully characterize the relationship between a cause and its effect. Similar to necessity, Pearl (1999) also provides a causal definition of sufficiency: whether the cause would have produced the effect (i.e. $Y_x = y$) when both the cause and the effect are absent (i.e. $X = x'$ and $Y = y'$). The probability of necessity (PN) and the probability of sufficiency (PS) are defined as:\nPN := $P\\{Y_{x'} = y'|X = x, Y = y\\}$\nPS := $P\\{Y_x = y|X = x',Y = y'\\}$\n(4)\nThe answers given by the language model to factual and counterfactual questions and the effects $\\hat{Y}, \\hat{Y}'$ estimated from those answers naturally induce an empirical pair of PN and PS values:\n$\\widehat{PN} = P\\{\\hat{Y}_{x'} = y'|X = x, Y = y\\}$\n$\\widehat{PS} = P\\{\\hat{Y}_x = y|X = x',\\hat{Y} = x'\\}$\n(5)\nWhy are PN and PS correctness alone not enough? To evaluate reasoning in language models, Gonz\u00e1lez and Nori (2024) use (1) a probabilistic measure ($\\gamma$-overlap) to assess how well the distributions of PN and PS match the true PN and PS, and (2) the factual and counterfactual error rates. We refine this approach by defining unifying metrics that simultaneously take both aspects of the problem into account, thereby simplifying the evaluation process.\nDue to the averaging done by probabilities, achieving a perfect PN-PS with the language model only requires identifying correct vs. predicted marginal frequencies, without needing individual units to be accurate. Although this is captured by the factual and counterfactual error rates F-ER and CF-ER, it is convenient to have a single metric that encapsulates both dimensions of the problem. We address this by requiring the necessity or sufficiency relationships identified by the language model to be accurate on a unit-by-unit basis. A unit is a realization of the exogenous variable $U$. It induces the values of X and Y as well as the counterfactual outcome $\\hat{Y}_{x'}$, where $X'$ represents the complement of the observed X regardless of its value. Note that $\\hat{Y}_x = Y$ is the factual outcome.\nWe focus on necessity where a unit/context might exhibit one of three situations: (i) Necessity occurs, denoted by \u201cN\u201d, meaning that both X and Y occur, $X = x$ and $Y = y$, and the cause was necessary for the effect, $\\hat{Y}_{x'} = y'$. (ii) Necessity does not occur, which we denote by \u201c\\(\\overline{N}\\)\u201d, meaning that both X and Y occur but the cause was not necessary for the effect, $\\hat{Y}_{x'} \\neq y'$. (iii) Not relevant case as necessity is concerned, which we denote by \u00d8, when neither X nor Y (or both) did occur. Since value of the context variable U fully characterizes the unit, we can define unit-wise necessity as\n$\\mathcal{N}(X,Y,Y_{x'};U) = {\\begin{cases} N & \\text{if } X = x \\land Y = y \\land Y_{x'} = y' \\\\ N & \\text{if } X = x \\land Y = y / Y_{x'} \\neq y' \\\\ \\O & \\text{if } X \\neq x \\lor Y = y' \\end{cases}}$\n(6)\nThe necessity inconsistency rate (N-IR) is the frequency with which the language model estimates the unit-wise necessity $\\mathcal{N}$ inaccurately:\nN-IR := $E_{P(U)} [\\mathcal{N}(X,\\hat{Y},\\hat{Y}_{x'}; U) \\neq \\mathcal{N}(X,Y,Y_{x'}; U)]$,\n(7)\nwhere $E_{P(U)}$ denotes the expectation over $U$ and $\\hat{Y}, \\hat{Y}_{x'}$ are the analogous factual and counterfactuals to $Y, Y_x$, estimated from the model. Remark that PN = $E_{P(U)}[N = N|N \\neq \\O]$ by construction. Also note that N-IR = 0 implies that $\\widehat{PN} = PN$. However, errors made in different units can no longer 'balance each other out' to achieve N-IR = 0. We can also define context-wise sufficiency S in an analogous way: (i) $S = S$ if $X = x',Y = y',\\hat{Y}_{x'} = y$, (ii) $S = \\overline{S}$ if $X = x',Y = y',\\hat{Y}_{x'} \\neq y$, and (iii) $S = \\O$ otherwise. This induces the sufficiency inconsistency rate S-IR = $P\\{S \\neq S\\}$.\nNeither PN and PS nor the inconsistency rates N-IR and S-IR are sensitive to all answers given by the language model. This is because necessity and sufficiency only concern cases where $X = x, Y = y$ and $X = y', Y = y'$. For instance, when $X = x'$ and $Y = y$ and the factual effect has been estimated correctly such that $\\hat{Y} = Y$, the counterfactual estimate $\\hat{Y}_{x}$ has no impact on PN, PS, N-IR, or S-IR. Regardless of whether $\\hat{Y} = \\hat{Y}_x$, all four quantities stay the same. To cover all possible counterfactuals we can ask a language model for, it makes sense to also evaluate counterfactuals of the type $Y = y|X = x,Y = y'$ and $\\hat{Y}_x = y'|X = x',Y = y$. Of course, the probabilities of these counterfactuals can be defined by means of PN and PS by changing the default observed state. However, here we name them as absent necessity and absent sufficiency to be explicit about the"}, {"title": "", "content": "two extra cases where language model can make mistakes. In our context-based framework, the corresponding context-wise AN and AS are defined in a similar fashion as $\\mathcal{N}$ and $\\mathcal{S}$, which induce the inconsistency rates AN-IR = $P\\{\\overline{AN} \\neq AN\\}$ and AS-IR = $P\\{\\overline{AS} \\neq AS\\}$. As a reasoning metric, we define the average inconsistency rate as Avg-IR = (N-IR + S-IR + AN-IR + AS-IR)/4. This metric has the following properties: (i) it accounts for all characterizations of the necessity and sufficiency of the target causal effect, and (ii) it is unit-dependent, so factual and counterfactual accuracy errors cannot be balanced out."}, {"title": "4 Fine-tuning with Counterfactual Feedback", "content": "Despite the significant differences between correctness and causal consistency, success in either metric relies on accurate estimates of counterfactual outcomes. Therefore, to solve the fine-tuning problem in (2), it is essential to leverage the counterfactual information available in demonstrations D, irrespective of the metric we aim to target as V. We present a data-centric approach to achieve this and propose three methods for generating datasets using counterfactual feedback. These datasets can then be utlised by existing algorithms for fine-tuning such as SFT or DPO.\nSupervised Counterfactual Feedback. Recall that we assumed access to an extractor h that can reduce answers given in natural language to binary outcomes $\\hat{y} = h(a) \\in \\{y,y'\\}$. Now, further suppose that we can perform this extraction in reverse, denoted as H: Given a question q and the true outcome $Y_{\\text{true}}$ corresponding to this question, we can form a natural language answer $a = H(q, Y_{\\text{true}})$. In practice, we achieve this by prompting a language model to provide an answer to question q that starts with \u201cYes\u201d or \u201cNo\u201d (see the appendix for the full prompt). Based on these answers, we generate a dataset D of both factual and counterfactual questions and their answers:\nD = { $q_f = q(U), a_f = H(q_f, Y), q_{cf} = \\tilde{q}_{x'} (U), a_{cf} = H(q_{cf}, Y_{x'}) \\}_{U,X,Y,Y_{x'} \\sim D}$\nThis dataset can directly be used with any SFT algorithm to fine-tune the target model $l_0$."}, {"title": "5 Related Work", "content": "Reasoning Evaluation. While our work focuses on reasoning elicitation, there is a plethora of work on reasoning evaluation (Chang et al., 2024). Parmar et al. (2024) evaluate logical reasoning, Cohn and Hernandez-Orallo (2023) evaluate spatial reasoning, Gandhi et al. (2024) evaluate social reasoning, and Li et al. (2022); Jin et al. (2023); Ashwani et al. (2024); Li et al. (2024); Wang (2024) evaluate causal reasoning. Of course, being able to determine which model is better at reasoning is an important aspect of reasoning elicitation. We have explored this aspect in Section 3 building on the work of Gonz\u00e1lez and Nori (2024). This allowed us to consider relationships like necessity and sufficiency, which we have shown to require a higher level of reasoning than simply answering counterfactual prompts.\nCounterfactual Frameworks. Counterfactual frameworks have been employed to explore various aspects of large language models. For instance, Lin et al. (2024b) formulate preference alignment as a causal inference problem and de-"}, {"title": "6 Experiments", "content": "We begin with a proof-of-concept case study. We analyze a hand-crafted puzzle to assess the effectiveness of all fine-tuning techniques introduced in Section 4 when trained on different types of datasets within the context of the in-domain causal reasoning scenarios (\u00a76.1). We also address the research question posed in Section 1, i.e., to what extent the performance improvements in causal reasoning achieved through the fine-tuning process generalize across all the generalization modes (\u00a76.2). Subsequently, we use three additional real-world problems to examine our findings (\u00a76.3).\n6.1 In-domain Reasoning\nWe evaluate all fine-tuning techniques when trained on various types of demonstrations in a synthetic in-domain reasoning problem.\nExperimental Setup. See Figure 5. The puzzle describes a candy party. The context is defined by the four-dimensional random vector U = ($N_A$, $N_B$, $N_C$, $N_D$) where each element follows the same uniform distribution $\\mathcal{U}(1, 12)$. The causal structure, derived from the narratives, is presented in the middle section of the figure. We selected A: Anna is happy or not as the cause (X), and D: Dave is happy or not as the effect (Y). The factual questions $q(u)$ are obtained by randomly drawing values for the four numerical variables from the distribution $\\mathcal{U}$, The counterfactual questions $\\tilde{q}_x,(u)$ are generated by introducing an assumption that negates the cause i.e. if in the context A is \"Anna is happy\u201d based on the value of $N_A$, the injected assumption would be \u201csuppose that Anna is not happy\u201d, and vice versa. Since we are assessing the in-domain reasoning scenario, the cause-effect demonstration used during the fine-tuning phase are likewise employed in the evaluation phase.\nWe first generate dataset D = (\\{\\{q(u), $a_f$)\\}, \\{\\{$\\tilde{q}_{x'}(u), a_{cf}$\\}\\}) for each fine-tuning techniques introduced in Section 4 following the algorithms shown in Appendix B. Then we fine-tune the mini version of Phi-3 (Abdin et al., 2024) on ID. We include five baselines: the base language model (Phi-3 mini) without fine-tuning (Base), the base model fine-tuned using the SFT and DPO methods on factual examples \\{\\{q(u), $a_f$)\\} exclusively (SFT-OnlyF and DPO-OnlyF), and the base model fine-tuned using the SFT and DPO methods on counterfactual examples \\{\\{$\\tilde{q}_{x'}(u), a_{cf}$\\}\\} exclusively (SFT-OnlyCF and DPO-OnlyCF). As our proposed methods, we include the base model fine-tuned using SFT, DPO, and CCF methods on both factual and counterfactual examples (SFT-F&CF, DPO-F&CF, and DPO+CCF)."}, {"title": "6.2 Modes of Generalization", "content": "In this section, we answer the question \"to what extent the performance improvements in causal reasoning achieved through the fine-tuning process generalize across all the generalization modes defined in Section 2\". As mentioned in Section 2, an in-domain evaluation alone is inadequate for fully assessing the success of fine-tuning for reasoning and differentiating it from basic recall. Therefore, we evaluate all fine-tuning methods in the generalization modes introduced in Section 2.\nExperimental Setup. To allow for the problem in Figure 5 to reflect all possible generalization modes we made slight modifications to the puzzle context, creating two variations: chain NDE and chain WDE (refer to Structure-2 and Structure-3 in Appendix C.1). The top section in Figure 6 displays all the causal structures used for each generalization mode, along with the cause-effect interventions demonstrated during the fine-tuning and evaluation phases.\nBased on the findings from the in-domain reasoning experiments (Section 6.1), where both SFT and DPO fine-tuning methods showed significantly better performance when provided with both factual and counterfactual examples, we include here only the methods SFT-F&CF, DPO-F&CF, and DPO+CCF as well as the Base model.\nResults. The bottom section in Figure 6 presents the causal reasoning performance of all systems across the different generalization modes. We observe that: (i) Common-Cause/Effect. Fine-tuning based on demonstrations that involve just the target cause or the target effect (but not both as in the in-domain case) no longer leads to improvements in S-IR (unlike the in-domain case). While we do see improvements in N-IR, this can be attributed to better recall and not necessarily to better reasoning. The common-effect case leads to the greater improvement in N-IR precisely because the task of identifying factuals remains the same in this mode of generalization. (ii) Induction. Fine-tuning generalizes best when performed inductively. This is because relationships involving both"}, {"title": "6.3 Real-world Problems", "content": "We validate our experimental findings in real-world problems from three domains.\nExperimental Setup. We present three real-world causal reasoning problems: in the Healthcare domain, we examine breast cancer treatment and develop a simplified problem that determines how different treatment options\u2014namely, radiotherapy/chemotherapy and surgery\u2014are assigned to patients based on cancer type, tumor size, and nodal involvement. This model is grounded in a real-world guideline (MD Anderson Cancer Center) and published statistics on the disease (Orrantia-Borunda et al., 2022; Sezg\u0131n et al., 2020; Carey et al., 2006). In the Engineering domain, we implement an automatic fault detection algorithm for transmission lines (Reddy et al., 2016). This algorithm aims to identify the type of fault occurring on a transmission line using three different measurements. In the Math Benchmarking domain, we select a math question from GSM8K (Cobbe et al., 2021), a widely used benchmark for evaluating language models on grade school math problems. A detailed explanation of these three problems, including the context, factual and counterfactual questions, causal structures, and the cause-effect interventions demonstrated during the fine-tuning and evaluation phases across different generalization modes, can be found in Appendix C.2, C.3, C.4 respectively.\nResults The results for all three problems across in-domain and different generalization modes are available in Table 2 in Appendix A. Given the extensive number of experiments in this table, we have summarized the Average Error Rate (Avg-ER) and Average Inconsistency Rate (Avg-IR) scores in Table 1. For this summary, we first normalized the scores of each approach relative to the scores of the corresponding Base approach. Then, for each generalization mode (including the in-domain scenario), we calculated the average score of each tested method across all applicable problems. In Table 1, higher scores indicate more errors, and scores above 1.0 signify that the approach makes more mistakes than the Base model. We observe that: (i) In the in-domain scenario, when the fine-tuning is guided by both factual and counterfactual examples (-F&CF), the language models show a significant improvement in causal reasoning ability. (ii) Similar to what we observed in previous experiments, this improvement generalizes to most generalization modes, with the exception of common-cause and effect-based deduction. (iii) In most of modes, language models trained with causal consistency feedback (DPO+CCF) demonstrate a lower error and inconsistency rate."}, {"title": "7 Conclusion", "content": "This work introduced the problem of fine-tuning for reasoning, along with (1) a taxonomy for generalization modes, (2) multiple metrics that address the limitations of existing performance measures, and (3) methods for generating fine-tuning data with counterfactual feedback. A key limitation of our approach is the restriction of causes and effects to binary variables, which allows us to focus on high-level relationships like necessity and sufficiency present in human reasoning."}, {"title": "B Description of the Algorithms in Section 4", "content": "Algorithm 1 Supervised Counterfactual Feedback\n1: Inputs: Demonstrations $D = \\{P_{x_1\\rightarrow Y_i"}, "question templates $q$, $q$, answer generator $H$\n2: Output: Dataset $D = \\{q, a\\}$ of questions and answer pairs\n3: $D\\leftarrow \\{\\}$\n4: for $P_{x_1\\rightarrow Y_i} \\in D$, $n \\in \\{1, ..., N\\}$ do\n5:  $u, X, Y, Y_{x'} \\sim P_{x_i\\rightarrow Y_i}$\n6:  $q_f \\leftarrow q(u)$, $q_{cf} \\leftarrow \\tilde{q}_{x'}(u)$\n7:  $a_f \\leftarrow H(q_f, Y_x)$, $a_{cf} \\leftarrow H(q_{cf}, Y_{x'})$\n8:  $D \\leftarrow D \\cup \\{\\{q_f, a_f\\}, \\{q_{cf}, a_{cf}\\}\\}$\n9: end for\nAlgorithm 2 Preference-based Counterfactual Feedback\n1: Inputs: Demonstrations $D = \\{P_{x_1\\rightarrow Y_i}\\}$, question templates $q$, $q$, answer extractor $h$, target model $l_0$\n2: Output: Dataset $D = \\{\\{q, a\\} > \\{q', a'\\}\\}}$ of preferences over question-answer pairs\n3: $D\\leftarrow \\{\\}$\n4: for $P_{x_1\\rightarrow Y_i} \\in D$, $n \\in \\{1, ..., N\\}$ do\n5:  $u, X, Y, Y_{x'} \\sim P"]}