{"title": "Improving Mathematical Reasoning Capabilities of Small Language Models via Feedback-Driven Distillation", "authors": ["Xunyu Zhu", "Jian Li", "Can Ma", "Weiping Wang"], "abstract": "Large Language Models (LLMs) demonstrate exceptional reasoning capabilities, often achieving state-of-the-art performance in various tasks. However, their substantial computational and memory demands, due to billions of parameters, hinder deployment in resource-constrained environments. A promising solution is knowledge distillation, where LLMs transfer reasoning capabilities to Small Language Models (SLMs, \u2264 1B parameters), enabling wider deployment on low-resource devices. Existing methods primarily focus on generating high-quality reasoning rationales for distillation datasets but often neglect the critical role of data quantity and quality. To address these challenges, we propose a Feedback-Driven Distillation (FDD) framework to enhance SLMs' mathematical reasoning capabilities. In the initialization stage, a distillation dataset is constructed by prompting LLMs to pair mathematical problems with corresponding reasoning rationales. We classify problems into easy and hard categories based on SLM performance. For easy problems, LLMs generate more complex variations, while for hard problems, new questions of similar complexity are synthesized. In addition, we propose a multi-round distillation paradigm to iteratively enrich the distillation datasets, thereby progressively improving the mathematical reasoning abilities of SLMs. Experimental results demonstrate that our method can make SLMs achieve SOTA mathematical reasoning performance.", "sections": [{"title": "1. Introduction", "content": "With the rapid development of artificial intelligence, Large Language Models (LLMs) typically exhibit exceptional reasoning capabilities and consistently achieve state-of-the-art (SOTA) performance across a wide range of reasoning tasks. However, this progress has been accompanied by a dramatic increase in the size of these LLMs, leading to higher computational costs and greater memory requirements. Currently, LLMs have tens to hundreds of billions of parameters, which means they require several high-performance GPUs for deployment. These challenges pose significant obstacles to deploying LLMs in low-resource environments.\nTo enable large-scale deployment, a feasible approach is to leverage knowledge distillation to transfer the reasoning capabilities of Large LLMs to Small Language Models (SLMs, < 1B), thereby enhancing the reasoning performance of SLMs. SLMs, due to their compact size, can be more widely deployed on low-resource devices. Specifically, numerous studies [1, 2, 3, 4, 5, 6] employ LLMs to construct mathematical distillation datasets. Each problem in the distillation dataset is paired with a corresponding Chain-of-Thought (CoT) or Program-of-Thought (PoT) reasoning rationale. These datasets are then used to fine-tune SLMs, significantly improving their mathematical reasoning abilities. Although these approaches can effectively enhance the mathematical reasoning abilities of SLMs, they have a potential drawback: successful distillation requires a sufficient amount of distillation data. The aforementioned methods focus primarily on generating high-quality reasoning rationales but overlook the critical impact of data quantity on mathematical distillation. On the other hand, constructing additional data can be both costly and time-consuming. Therefore, exploring how to efficiently generate a sufficient amount of high-quality data for distillation is a problem worth investigating.\nCurrently, extensive research [7, 8, 9, 10] has demonstrated that LLMs can effectively generate high-quality data based on original data. However, these approaches apply a uniform generation strategy to all original data, overlooking the variations in students' performance on the original data. Jiang et al.; Lee et al.; Ying et al. [11, 12, 13] considers student performance on the original data when generating synthetic data. However, these approaches"}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Reasoning Distillation", "content": "Reasoning Distillation refers to the process of transferring the reasoning abilities of LLMs to SLMs. A growing body of research [1, 2, 3, 4, 5, 6] has increasingly focused on this field of reasoning distillation. This approach aims to transfer the reasoning capabilities of LLMs to SLMs (< 1B), enabling more efficient deployment while maintaining strong performance. Specifically, Hsieh et al. [1] first proposed extracting rationales from LLMs and using a multi-task learning framework to guide SLMs in learning both answers and rationales, thereby enhancing their reasoning performance. Building on this, Ho et al. [2] discovered that fine-tuning SLMs directly with CoTs generated by LLMs could significantly improve the reasoning abilities of SLMs. Zhu et al. [5] further advanced this approach by utilizing PoTs instead of CoTs to fine-tune SLMs, achieving additional improvements in reasoning performance. This enhancement is attributed to PoTs offloading computational steps in the reasoning process to an external Python interpreter, allowing SLMs to focus solely on generating Python programs to solve problems. Fu et al. [3] introduced distribution matching distillation, which minimizes the Kullback-Leibler (KL) divergence between the output distributions of LLMs and SLMs to improve the mathematical reasoning abilities of SLMs. Similarly, Shridhar et al. [4] trained two distilled models: a problem decomposer and a subproblem solver. When given a problem, the problem decomposer breaks it into smaller subproblems, and the subproblem solver generates CoTs to resolve these subproblems, ultimately producing the final answer. Finally, Zhu et al. [6] designed a diverse reasoning format distillation framework to further enhance the mathematical reasoning capabilities of SLMs. This framework constructs a distillation dataset encompassing various reasoning formats to fine-tune SLMs effectively. Although these methods can effectively enhance the reasoning performance of SLMs, they overlook the impact of low-data regimes on the reasoning abilities of SLMs. To address this issue, we propose a feedback-driven distillation framework designed to generate additional questions, which are then used to expand the distillation dataset. This enriched dataset is subsequently leveraged to improve the reasoning performance of SLMs."}, {"title": "2.2. Data Generation for LLMs", "content": "Data Generation for LLMs refers to leveraging LLMs to generate data, thereby expanding datasets for subsequent fine-tuning tasks. For example, Self-Instruct [7] leverages the self-generated outputs of LLMs to bootstrap the construction of instruction-following datasets, thereby enhancing the LLMs\u2019 own ability to follow instructions. Metamath [8] creates a new dataset by rewriting existing mathematical questions from multiple perspectives. This dataset is then used to fine-tune open-source LLMs, significantly improving their capacity for mathematical reasoning. MuggleMath [9] conducts an in-depth exploration of how data augmentation strategies influence the mathematical reasoning and generalization capabilities of open-source LLMs. WizardLM [10] rewrites existing instructions step by step into more complex instructions. Furthermore, Jiang et al.; Lee et al.; Ying et al. [11, 12, 13] expand fine-tuning datasets by customizing the generation of instructions based on feedback from student LLMs. Although these methods enable student LLMs to achieve strong performance, they focus solely on instructions where student LLMs underperform, overlooking the potential of instructions where student LLMs excel. In contrast to these approaches, our method considers both types of instructions, employing distinct generation strategies for each. This dual focus enhances the complexity, scale, and diversity of the resulting instructions."}, {"title": "3. Methodology", "content": "In this work, we propose a Feedback-Driven Distillation (FDD) framework aimed at enhancing the mathematical reasoning abilities of SLMs by constructing a more complex and diverse distillation dataset. Our framework consists of three stages: (1) an initialization stage to equip SLMs with preliminary mathematical reasoning abilities, (2) a question generation stage in which LLMs generate new questions to expand and diversify the distillation dataset, and (3) a fine-tuning stage where the enriched distillation dataset is used to fine-tune SLMs, further improving their mathematical reasoning capability. Figure 1 shows the overview of our distillation method."}, {"title": "3.1. Initialization Stage", "content": "Firstly, we create a distillation dataset to equip SLMs with preliminary mathematical reasoning abilities. The primary goal is to enable SLMs to distinguish between hard and easy questions, providing valuable feedback to"}, {"title": "3.2. Question Generation Stage", "content": "Previous studies [10, 9, 8] have shown that the complexity and diversity of fine-tuning datasets play a significant role in shaping the reasoning performance of SLMs. Building on this insight, our work seeks to enhance the complexity and diversity of the mathematical distillation dataset to improve the mathematical reasoning capabilities of SLMs.\nSpecifically, we use the fine-tuned SLM from the Initialization Stage to perform inference on each data in the mathematical distillation dataset,"}, {"title": "3.3. Fine-tuning Stage", "content": "After expanding the scale of the distillation dataset, we use this dataset to fine-tune the SLMs to improve their mathematical reasoning performance. The fine-tuning process of the SLMs follows the method outlined in Section 3.1."}, {"title": "3.4. Multi-Round Distillation Paradigm", "content": "In the subsection, we further propose a multi-round distillation paradigm to iteratively improve the mathematical reasoning abilities of SLMs. The multi-round distillation paradigm enables the LLM to stay up-to-date with the learning state of the SLM, allowing it to generate questions that are better aligned with the SLM's current progress.\nSpecifically, we first apply methods from the Initiation stage to construct an initial distillation dataset. This dataset is then used to fine-tune SLMs, enabling them to acquire preliminary mathematical reasoning abilities. In turn, this equips the SLMs to provide valuable feedback to the LLM, guiding it in generating customized questions. After initializing the SLMs, we add all data from the initial distillation dataset to the seed pool in the next round. Using the initialized SLM, we then categorize the data in the seed pool as either hard or easy questions, placing them accordingly into a hard pool and an easy pool. For data in the hard pool, the LLM generates questions of similar type and difficulty. For data in the easy pool, the LLM generates more challenging questions. These newly generated questions are added to the distillation dataset, which is then used to fine-tune the SLMs from scratch. In each subsequent round, we exclude the easy questions from the previous round from the seed pool. Instead, we add only the newly generated questions and the hard questions to the seed pool. This approach avoids redundancy, as the model has already learned the relevant knowledge from the easy questions in the previous round, and the more complex questions derived from them are already included in the seed pool. This strategy also reduces dataset size, lowers data generation costs, and improves fine-tuning efficiency. Following the same method, the LLM generates new questions, which are added to the distillation dataset, allowing us to fine-tune the SLMs from scratch."}, {"title": "4. Experiments", "content": ""}, {"title": "4.1. Dataset", "content": "In our work, we use the GSM8K training set [14], which contains 7,473 examples, as the mathematical reasoning dataset for our experiments. We prompt the LLM to construct a mathematical distillation dataset based on this GSM8K training set. Subsequently, we fine-tune the SLMs on this mathematical distillation dataset to enhance their mathematical reasoning capabilities. Furthermore, we evaluate the mathematical reasoning performance of the SLMs on the GSM8K test set, which contains 1,319 examples. Additionally, to assess the transferability of the SLMs' mathematical reasoning abilities, we evaluate them on several well-known mathematical reasoning datasets, including ASDiv [15] with 2,300 samples, SVAMP [16] with 1,000 samples, and MultiArith [17] with 180 samples."}, {"title": "4.2. Implementation Details", "content": "In our work, we employ ChatGPT (gpt-3.5-turbo) as the LLM to construct a mathematical distillation dataset. For our small language models (SLMs), we use FlanT5 [18] models with parameter counts ranging from 60M to 760M. In the initialization stage, we prompt the LLM to generate 4 PoTs for each question in the GSM8K training dataset, creating the foundation for the mathematical distillation dataset, which we then use to fine-tune the SLMs. We fine-tune the SLMs over 10 epochs, with a learning rate of 5e-4 and a batch size of 32. To further enhance the mathematical reasoning capabilities of the SLMs, we employ a three-round multi-round distillation paradigm. In each round, we also generate 4 PoTs for each new question, and the fine-tuning settings for the SLMs remain consistent with those used in the initialization stage. Additionally, during the generation of math questions, we set the LLM's temperature to 1, while for PoT generation, we set it to 0.7."}, {"title": "4.3. Main Results", "content": "Table 1 shows the main experimental results. Based on the results, we observe that: (1) Our method enables SLMs to achieve State-of-the-Art (SOTA) mathematical reasoning performance. In Table 1, we demonstrate how our method enhances the mathematical reasoning capabilities of the FlanT5 models\u2014Small (60M), Base (250M), and Large (760M). We evaluate these SLMs on the in-domain GSM8k test set, where our approach (FDD) improves FlanT5-small to an accuracy of 29.87%, FlanT5-Base to 40.25%,"}, {"title": "4.4. Effect of Question Generation Strategies", "content": "In this section, we explore the effect of different question generation strategies on the mathematical reasoning performance of SLMs. To achieve this, we first prompt the LLM to construct an initial mathematical distillation dataset. This initialization dataset is then used to fine-tune FlanT5-base from scratch. Next, we use the fine-tuned FlanT5-base model to classify the questions in the distillation dataset into easy and hard questions. For easy questions, we prompt the LLM to generate more complex questions based on them. For hard questions, we prompt the LLM to generate diverse questions based on the originals. We then employ three strategies for constructing the distillation dataset: (1) adding the complex questions to the distillation dataset, (2) adding the diverse questions to the distillation dataset, and (3) adding all questions including both easy and hard questions into the dataset. The extended distillation dataset is then used to fine-tune FlanT5-base from scratch again. By evaluating the mathematical reasoning performance of the resulting SLMs, we analyze the effect of different question generation strategies on performance. Additionally, to simplify the analysis, we ensure that each question in the distillation dataset contains only a single PoT in this study."}, {"title": "4.5. Effect of the Number of Rounds", "content": "In this subsection, our primary objective is to investigate the impact of the number of rounds on the mathematical reasoning performance of SLMs. Specifically, we begin by fine-tuning the SLMs during the initialization stage to equip them with basic mathematical reasoning abilities. In the experiment, the initialization stage is referred to as \"round 0.\" Next, we employ a multi-round distillation paradigm to further enhance the mathematical reasoning performance through up to three rounds. We record the SLM's performance in mathematical reasoning at each round. Based on these performance, we explore the effect of the number of rounds on the SLM's capabilities. For the sake of simplicity in our analysis, we use FlanT5-base as the SLM in this"}, {"title": "4.6. Effect of the Number of Reasoning Paths", "content": "In this subsection, our primary objective is to investigate the impact of the number of reasoning paths generated for each question during the"}, {"title": "4.7. Analysis for Data Leakage", "content": "When using LLMs to generate data, a potential issue that may arise is data leakage. Data leakage refers to the possibility that test data was inadvertently used during the training of the LLM, which may result in the model generating questions similar to those in the test set. In this subsection, we investigate whether our approach suffers from data leakage. To achieve this, we design a simple experiment. During the initialization stage, we prompt the LLM to construct an Initialization Mathematical Distillation Dataset based on the GSM8K training dataset, where each question is paired with a corresponding PoT. We then fine-tune the FlanT5-base model using this distilled dataset. Following this, we implement a multi-round distillation paradigm for three"}, {"title": "5. Conclusion", "content": "In this work, we introduce the Feedback-Driven Distillation (FDD) framework to enhance the mathematical reasoning capabilities of SLMs. By leveraging an iterative process involving the generation of complex and diverse mathematical questions, we successfully expand and enrich distillation datasets. This allow for the progressive fine-tuning of SLMs, enabling them to achieve state-of-the-art performance on both in-domain and out-of-domain mathematical reasoning tasks. Furthermore, our analysis verify that these improvements were achieved without data leakage, underscoring the effectively of our method. However, our method still has some limitations. For instance, it relies on LLMs to generate questions, which increases the cost of using LLMs. Additionally, in each round, the SLM requires fine-tuning from scratch, which further adds to the training cost. In the future, we want to design more efficient mathematical reasoning methods to address these challenges and further enhance the mathematical reasoning capabilities of SLMs."}, {"title": "6. Acknowledgments", "content": "The work of Jian Li is supported partially by National Natural Science Foundation of China (No. 62106257)."}, {"title": "Appendix A. Instructions for Question Generation", "content": "I want you act as a Math Question Creator.\nYour goal is to draw inspiration from the Given Math Question to create a more challenging math question by increasing the complexity of the Given Math Question. The created math question should belong to the same domain and the same task type as the Given Math Question.\nThe Created Math Question must be reasonable and can be understood and solved by humans.\nGiven Math Question: {easy question}\nCreated Math Question:\nTable A.3: Instruction for creating new math question based on the easy question.\nI want you act as a Math Question Creator.\nYour goal is to draw inspiration from the Given Math Question to create a new math question.\nThe created math question should belong to the same domain and the same task type as the Given Math Question.\nThe difficulty level of the Created Math Question should be similar to that of the Given Math Question. The Created Math Question must be reasonable and can be understood and solved by humans.\nGiven Math Question: {hard question}\nCreated Math Question:"}]}