{"title": "Exploring the Benefit of Activation Sparsity in Pre-training", "authors": ["Zhengyan Zhang", "Chaojun Xiao", "Qiujieli Qin", "Yankai Lin", "Zhiyuan Zeng", "Xu Han", "Zhiyuan Liu", "Ruobing Xie", "Maosong Sun", "Jie Zhou"], "abstract": "Pre-trained Transformers inherently possess the characteristic of sparse activation, where only a small fraction of the neurons are activated for each token. While sparse activation has been explored through post-training methods, its potential in pre-training remains untapped. In this work, we first study how activation properties change during pre-training. Our examination reveals that Transformers exhibit sparse activation throughout the majority of the pre-training process while the activation correlation keeps evolving as training progresses. Leveraging this observation, we propose Switchable Sparse-Dense Learning (SSD). SSD adaptively switches between the Mixtures-of-Experts (MoE) based sparse training and the conventional dense training during the pre-training process, leveraging the efficiency of sparse training and avoiding the static activation correlation of sparse training. Compared to dense training, SSD achieves comparable performance with identical model size and reduces pre-training costs. Moreover, the models trained with SSD can be directly used as MoE models for sparse inference and achieve the same performance as dense models with up to 2\u00d7 faster inference speed. Codes are available at https://github.com/thunlp/moefication.", "sections": [{"title": "1. Introduction", "content": "Recent studies have uncovered a notable characteristic of pre-trained Transformers: the sparse activation of neurons in their intermediate layers (Zhang et al., 2022b; Li et al., 2023; Liu et al., 2023; Dong et al., 2023; Mirzadeh et al., 2023). During inference, it has been observed that only a small fraction of the intermediate hidden states are activated, rendering a non-zero state, while the majority remain inactive. Sparse activation presents a promising direction for improving the efficiency of Transformer-based models.\nPrevious work has primarily focused on leveraging the sparse activation phenomenon to speed up the inference process through post-training methods (Liu et al., 2023; Alizadeh et al., 2023). For instance, with model parameters frozen, DejaVu (Liu et al., 2023) proposes to selectively engage a subset of neurons likely to activate during inference, thereby reducing both the parameter communication and model computation costs. However, the potential of utilizing sparse activation in pre-training remains largely unexplored.\nUnlike the widely explored domain of post-training, where model parameters are fixed, the pre-training of Transformers is dynamic, requiring ongoing updates to model parameters. Therefore, a preliminary step is to investigate the activation of Transformers during pre-training. We conduct experiments on three representative text models, including GPT (Radford et al., 2019), BERT (Devlin et al., 2019), and T5 (Raffel et al., 2020), with different architectures and pre-training objectives.\nOur findings reveal that these models become sparsely activated in the early stage of pre-training, subsequently stabilizing in this sparse state. It suggests that sparse activation is a pervasive phenomenon across pre-trained models, existing throughout the majority of the pre-training process. Meanwhile, although the activation sparsity becomes stable after a certain stage of pre-training, the activation pattern is still dynamic: the set of activated neurons for a certain input varies across different stages of pre-training. Consequently, the sparse training method for pre-training should be adaptive to the change in the activation patterns.\nBased on these observations, we propose Switchable Sparse-Dense Learning (SSD), utilizing the phenomenon of sparse activation to accelerate the pre-training of Transformers and enhance the efficiency of inference. SSD contains two kinds of training phases: the original dense training, which facilitates the evolution of activation patterns, and the subsequent"}, {"title": "2. Related Work", "content": "Activation Sparsity of Transformers. While non-linear activation functions are prevalent in neural networks, the activation of neurons is typically dense, for instance, 44% of zeros in convolutional neural networks (Albericio et al., 2016). Contrastingly, pre-trained Transformers exhibit sparse activation, with over 90% of zeros in T5 (Zhang et al., 2022b), and similar phenomena are observed in other pre-trained models spanning both language and vision domains (Liu et al., 2023; Li et al., 2023). This sparsity has stimulated researchers' interest in utilizing it to accelerate inference. There are two main approaches: neuron-based acceleration (Liu et al., 2023) which dynamically selects subsets of neurons likely to be activated for computation, and SMoE-based acceleration (Zhang et al., 2022b), which first groups the neurons into experts and then computes in an SMoE manner by selecting the experts likely to contain most activated neurons. The former is a fine-grained approach suitable for single-instance inference, while the latter is a coarse-grained strategy well-suited for batch inference. In this work, we align with the SMoE-based approach, given that pre-training is commonly conducted in batch mode.\nSparse-Activated Mixture-of-Experts. SMoE is a representative method to improve training efficiency of Transformer-based large language models (Hazimeh et al., 2021; Gao et al., 2022; Zuo et al., 2022b; Lee-Thorp & Ainslie, 2022; Gururangan et al., 2022; Jang et al., 2023; Liu et al., 2022; Chen et al., 2023b; Muqeeth et al., 2023) and targets both feed-forward networks (FFNs) (Lewis et al., 2021; Roller et al., 2021) and attention networks (Zhang et al., 2022a). Based on the SMoE technique, we can train Transformers that are dozens of times larger without significantly increasing computational overhead (Artetxe et al., 2022; Riquelme et al., 2021). However, when we evaluate the models on an equivalent parameter basis, the performance of models pre-trained with the SMoE technique frequently lags behind that of their dense counterparts (Chen et al., 2023a).\u00b9 The performance discrepancy of certain models could potentially be attributed to a phenomenon known as representation collapse (Chi et al., 2022), where multiple experts redundantly encode similar information, leading to inefficient parameter utilization. Furthermore, the mandatory selection of all experts during inference typically does not confer any notable benefits (Zuo et al., 2022a). To alleviate this issue, we combine SMoE with dense training, aiming to attain the model performance matching purely dense training while concurrently curtailing training costs. Similar to our work, Pan et al. (2024) propose to add constraints during dense training to induce SMoE-like behavior thereby improving the inference efficiency of the model but it may introduce additional training overhead.\nPre-training Acceleration Methods. In addition to SMoE, there are other methods to accelerate pre-training, including modifying the training objectives (Clark et al., 2020), inheriting the parameters from previous models (Chen et al., 2022; Qin et al., 2022; Gong et al., 2019), searching for appropriate hyperparameters (Izsak et al., 2021), and changing the model architecture (Yang et al., 2022; Zhang & He, 2020). These methods are orthogonal to our work and can be combined with our method to further improve efficiency."}, {"title": "3. Preliminary Study", "content": "We conduct a preliminary study on the evolution of the activation properties throughout the pre-training and focus on two aspects: activation sparsity and activation pattern. Specifically, we train three different types of PLMs, i.e., GPT, T5, and BERT, on the Pile dataset (Gao et al., 2021a), and report the statistics of the first 150,000 steps. More pre-training details are in Section 5.1.\n(1) Activation Sparsity Change of Transformers. Activation sparsity is defined as the fraction of zeros in the intermediate hidden states of FFNs, which is the basis of sparse computation. We save the model checkpoints every 4,000 steps and calculate the activation sparsity of each checkpoint on the validation corpus. We plot the activation sparsity of the models during training in Figure 1(a). From this figure, we can see that the activation sparsity is around 0.5 at the beginning due to the symmetry of the initialization and quickly increases to about 0.9 after 20, 000 steps. After that, the sparsity is stable and fluctuates around 0.9. This observation is consistent with the previous work (Mirzadeh et al., 2023) on auto-regressive models. Here we extend the observation to other types of architectures and pre-training tasks, including BERT and T5.\n(2) Activation Pattern Change of Transformers. Activation pattern refers to the activation correlation among neurons. While activation sparsity stabilizes after a certain stage in pre-training, the activation pattern remains dynamic due to the ongoing updates in model parameters throughout the training process. For instance, a pair of neurons activated together for a certain input at the onset of training may not exhibit the same behavior toward the end of training. This dynamic nature of activation patterns poses a challenge to existing sparse acceleration approaches (Liu et al., 2023), which are primarily designed for inference and may fall short in accommodating models with significantly fluctuating activation patterns.\nHere, we study the activation pattern change of Transformers by analyzing the co-activation neuron groups. Utilizing MoEfication (Zhang et al., 2022b), we categorize the neurons activated together into the same group. By comparing the neuron groups of two checkpoints, we could measure the similarity of the activation patterns. This grouping process essentially serves as a neuron clustering exercise, and we use the Adjusted Rand Index (ARI) (Rand, 1971) to measure the similarity between the two clustering results. The ARI ranges from -0.5 to 1, where 1 means the two clustering results are identical and 0 means the two clustering results are random. We report the activation pattern similarity of consecutive checkpoints in Figure 1(b) and the activation pattern similarity of arbitrary checkpoints in Figure 1(c).\nFrom these figures, we observe that the activation pattern similarity of consecutive checkpoints begins at a low point early in training, escalating to approximately 0.9 after 50,000 steps. However, even as the activation pattern evolution decelerates during mid to late training stages, checkpoints separated by large step intervals continue to exhibit low activation pattern similarity. Due to the limited computational resources, we only study the models with about 100 million parameters. It would be interesting to investigate how the activation pattern scales with the model size in future work.\nIn summary, the observation of the high activation sparsity and the slow activation pattern change in the middle and late stages of training provides us with the possibility to incorporate sparse computation into dense training. After both the activation sparsity and pattern stabilize, we can apply existing sparse acceleration methods to pre-training. Note that the stabilization of the activation pattern unfolds at a slower pace compared to that of activation sparsity (50,000 steps vs. 20,000 steps). Consequently, we choose to employ the metric of activation pattern similarity to pinpoint"}, {"title": "4. Method", "content": "In this section, we first describe the overall framework of SSD and then introduce its two main components: the mechanism to transition between sparse and dense, and the criteria to determine the opportune moment for such transitions.\n4.1. Overall Framework\nIn this work, we focus on accelerating the feed-forward networks within Transformers (Vaswani et al., 2017b), which typically take more than 60% of the total computation (Wang et al., 2022a). The acceleration is achieved by switching between sparse and dense modes during the pre-training phase, as shown in Figure 2. Under sparse computation, the model is transformed into an SMoE model, incurring less computational costs compared to its original form. The sparse activation phenomenon enables the SMOE model to emulate the original model, thus achieving a balance between efficiency and effectiveness. Conversely, during dense computation, all model parameters are computed and optimized to achieve better performance. The final model reverts to a dense configuration to fully utilize the model capacity. Moreover, the final model also is familiar with the sparse computation, which can be directly used for efficient sparse inference without any additional training.\nIn dense computation, the FFNs are computed by\nFFN(x) = W_o \\sigma(W_i x + b_i) + b_o, \nwhere $W_i \\in \\mathbb{R}^{d_{ff} \\times d_{model}}$, $W_o \\in \\mathbb{R}^{d_{model} \\times d_{ff}}$, $b_i \\in \\mathbb{R}^{d_{ff}}$, $b_o \\in \\mathbb{R}^{d_{model}}$, $\\sigma$ is the activation function, and $d_{ff}$ and $d_{model}$ are the dimensions of the intermediate layer and the input/output, respectively. For simplicity, we omit the bias term $b_i$ and $b_o$ in the following discussion.\nIn sparse computation, the FFNs are equally split into N experts and computed in an SMoE manner,\nFFN_{SMOE}(x) = \\sum_{n=1}^N a_n W_{o,n} \\sigma(W_{i,n} x),\nwhere $W_{in} \\in \\mathbb{R}^{d_{ff} \\times d_{model}}$ and $W_{on} \\in \\mathbb{R}^{d_{model} \\times d_{ff}}$ are the parameters of the $n$-th expert, and $a_n$ is the importance score of the $n$-th expert. A gating network is used to score the importance of each expert for a given input x and the experts with top-K scores are selected to compute the output. The $a_n$ of unselected experts are set to 0. To ensure the SMoE computation is equivalent to the dense computation when K = N, we set the $a_n$ of selected experts to 1 through post-processing. The details of the post-processing are provided in Appendix A\n4.2. Transition between Sparse and Dense\nDense-to-Sparse Conversion. When the activation sparsity is high and the activation pattern is stable, we could efficiently approximate the original forward computation with sparse computation (Zhang et al., 2022b; Liu et al., 2023). Specifically, our approach leverages SMoE-based acceleration (Zhang et al., 2022b) over neuron-based acceleration (Liu et al., 2023) because fine-grained neuron-based selection for each token is not feasible in processing numerous tokens in a batch during pre-training.\nThe conversion from dense to SMOE needs to meet two requirements: (1) the conversion should be fast to avoid additional training costs; (2) the conversion should be smooth, ensuring the performance of the converted model remains closely aligned with the original model to avoid unstable training. With these requirements in mind, we propose a method for fast and smooth conversion.\nSpecifically, the conversion contains two steps. (1) Neuron Clustering. We group the neurons that are often activated together into the same expert so that the SMoE model can efficiently compute most of the activated neurons by engaging a small fraction of experts to emulate the original model. Inspired by Zhang et al. (2022b), we cluster the rows of $W_i$, each of which represents a certain neuron, into N groups by balanced k-means clustering (Malinen & Fr\u00e4nti, 2014), assuming that the neurons having similar weights are more likely to be activated simultaneously. This operation bypasses the need for directly counting the co-activation of neurons on a real-world corpus. The counting operation is time-consuming because it requires a large number of additional forward computations and cannot be replaced by using the activation results during training due to the dynamic nature of the activation pattern. Based on the clustering result $s \\in \\mathbb{R}^{d_{ff}}$, containing the corresponding expert index for each neuron, we split the weight matrices $W_i$, $W_o$ into N sub-matrices $W_{in}$, $W_{on}$, respectively. To make the conversion smoother, we propose to use the clustering results of the previous checkpoint as the initialization of clustering in the current checkpoint. Through pilot experiments, we find that this simple strategy often provides better results, i.e., the smaller within-cluster sum of squares (WCSS), than random initialization. The computation of WCSS is provided in Appendix A. To avert local optima, especially in early training stages where clustering may swiftly evolve, we conduct clustering twice, one with random initialization and the other with the initialization from the previous checkpoint, and select the better one. Formally, the clustering results $s_j$ of the $j$-th checkpoint are computed by\ns_j = \\underset{s \\in {f(W_i), f(W_i,s_{j-1})}}{\\text{min}} WCSS(W_i, s),\nwhere f(W_i) and $f(W_i, s_{j-1})$ are the clustering results with random initialization and the initialization from the previous checkpoint, respectively. (2) Expert Selection. We use the similarity between the input $x$ and the cluster centers as the importance score to select the top-K experts. Formally, the importance score of the $n$-th expert is computed by\nX_n = \\frac{x^T c_n}{\\frac{1}{N} \\sum_{m=1}^N ||W_{i,n}||},\nwhere $W_m$ is the $m$-th row of $W_{in}$, and $c_n$ is the cluster center of the $n$-th expert.\nSparse-to-Dense Conversion. The performance of SMOE models tends to lag behind their dense counterparts with equivalent parameters, primarily due to the representation collapse issue (Chi et al., 2022; Zuo et al., 2022a). To optimally leverage the model capacity and avoid the overfitting of the sparse computation form, we strategically revert to dense training multiple times during training. The transition to dense is smooth given that SMOE computation aligns with dense computation when K = N. We conduct this conversion by concatenating the weight matrices of all experts, thereby obtaining the dense weight matrices, and concurrently omitting the gating network. This transition facilitates full-parameter optimization, effectively mitigating the representation collapse issue caused by sparse training and enabling the evolution of activation patterns.\nDiscussion of Sparse Approximation. Although we can approximate the model with a parameter-equivalent SMOE model, the approximation only holds for forward propagation. If we skip the computation of some neurons during forward propagation, the gradients of these neurons will be zero during backward propagation. However, the gradients of these neurons are usually not zero in the original model, posing an inconsistency between the SMoE model and the original model during backward propagation. We argue that this inconsistency may not be a problem. Intuitively, the inactivated neurons do not have strong relationships with the input, and the gradients of these neurons are not important, as the idea of Hebbian learning (Seung, 2000) that focuses on the neurons that are activated by the input.\n4.3. Transition Time Determination\n(1) Dense-to-Sparse Conversion. Considering the dynamic nature of the model activation during pre-training, we conduct the conversion when the activation sparsity is high and the activation pattern is stable. Inspired by the observation in Section 3, we propose to monitor the activation pattern change to determine the transition time, where the activation pattern similarity reflects the changing rate of the activation pattern. Specifically, we set a threshold T and switch to sparse training when the activation pattern similarity between two consecutive checkpoints is larger than T. (2) Sparse-to-Dense Conversion. To have a controllable speed ratio, we propose to maintain a constant ratio of sparse training steps to all training steps r. For example, if r = 0.5, we will train the model with 50% of the training data in the sparse training phases and the remaining 50% in the dense training phases. Specifically, we set the steps of sparse training to $T = \\frac{r}{1-r}$ times the steps of the last dense training. We give a detailed example in Figure 6. And, to ensure the final model can be used densely, we adopt dense"}, {"title": "5. Experiments", "content": "5.1. Experimental Setup\nHere we briefly introduce the setups of our experiments. Please refer to Appendix A for more details.\nBaselines. We compare our method with the following baselines: (1) Dense: We compute and update all the parameters in the network, which is the default way of pre-training. (2) SMOE: We replace the FFNs in the Dense baseline with MoE layers and train the model in a sparsely-activated manner (Fedus et al., 2022). Specifically, the number of parameters in experts (not including router networks) is the same as the number of parameters in the FFNs. The final model is computed sparsely. (3) Progressive Layer Dropping (PLD) (Zhang & He, 2020): PLD randomly drops layers during training to reduce the costs. After pre-training, we use all the layers of the final model, i.e., the same as the Dense baseline. (4) MoE-Dropout (Chen et al., 2023a): At the beginning of pre-training, the model is an SMoE model. During the training, MoE-Dropout gradually increases the number of selected experts K to the number of experts N. The final model is also densely computed.\nDatasets. (1) Pre-training corpus: We use the Pile dataset (Gao et al., 2021a) as the pre-training corpus. Due to limited computational resources, we use the first part of the Pile dataset with over 27GB of text data. (2) Downstream tasks: We consider two kinds of downstream tasks, i.e., natural language understanding and instruction tuning. More details are in Appendix A.\nTraining Details. (1) Model architecture: In this work, we evaluate our method on all three variants of Transformers, i.e., encoder-only BERT, decoder-only GPT, and encoder-decoder T5. For these models, we use the base version with 12 layers, 768 hidden size, and 12 attention heads for each encoder/decoder. Following Chen et al. (2023a), we set the intermediate size of FFNs to 6,144. For MoE-Dropout and SSD, we set the number of experts to 32 and the number of selected experts to 6. For SMOE, we set the number of experts to 3 and the number of selected experts to 2 to ensure similar computational costs to MoE-Dropout and SSD. (2) Pre-training: The training epoch is set to 10, which contains about 200,000 steps, and the warmup steps are set to 2,000. BERT and T5 adopt masked language modeling (MLM) as the pre-training task, and GPT adopts causal language modeling (CLM) as the pre-training task. (3) Fine-tuning: In this stage, we use two fine-tuning strategies for SMoE, i.e.,"}, {"title": "5.2. Main Results", "content": "In this subsection, we study the training efficiency and inference efficiency of different methods.\nTraining Efficiency. We report the computational costs per batch and performance of different methods in Figure 3. For model performance, we evaluate perplexity on a held-out validation corpus. From this figure, we have three observations. (1) Although SMoE training can reduce the computational costs, the perplexity of SMoE is consistently higher than that of dense training. (2) PLD and MoE-Dropout can also reduce the cost while keeping the perplexity comparable to that of dense training. However, in some cases, the perplexity of PLD and MoE-Dropout is higher than that of dense training, such as GPT with PLD and T5 with MoE-dropout. (3) SSD has the same speedup as MoE-dropout (up to 1.44x) and achieves slightly lower or equal perplexity compared with dense training, i.e., the data points of SSD are placed at the bottom left corner of the figure. It indicates that SSD achieves the best trade-off between training costs and performance.\nInference Efficiency. We conduct experiments to investigate whether these dense models can be computed sparsely for efficient inference without additional training. We consider three methods, i.e., Dense, MoE-Dropout, and SSD, and vary the number of selected experts k from 1 to 32, which is the total number of experts. For the models pre-trained with Dense, we use MoEfication to transform them into SMoE models. For the models pre-trained with MoE-Dropout and SSD, we directly adopt their MoE structure used in pre-training. Note that we also try to use MoEfication to transform the models pre-trained with MoE-Dropout and SSD into SMoE models, but the performance is slightly worse than that of directly using their original MoE structure. Additionally, we also report the performance of the models pre-trained with SMoE. We also try to vary the number of selected experts k for the models pre-trained with SMOE, but the performance is consistently worse than that of using the original number of selected experts. Therefore, we do not report the results of SMOE with different k.\nWe report the perplexity on the validation set with different computational costs per batch in Figure 4(a). From this figure, we have three observations. (1) The performance of the models pre-trained with Dense is consistently worse than that of the models pre-trained with SMOE, MoE-Dropout,"}, {"title": "5.3. Dynamic Top-k", "content": "Based on SSD, we further investigate whether we can vary the number of selected experts for each token in an input sequence. For example, some important tokens may need more experts to compute, and some unimportant tokens may need fewer experts to compute. Specifically, we first compute the top-k experts for each token in the input sequence as the expert candidates. Then, we truncate the experts with small importance scores based on a given ratio. We report the perplexity on the validation set with different truncation ratios in Figure 5. From this figure, we observe that truncating 75% of the experts can consistently achieve better performance than that of using a fixed number of experts for each token under the same computational cost. It opens up a new direction for future research to dynamically identify the number of experts for each token in an SMoE model."}, {"title": "5.4. Performance on Downstream Tasks", "content": "We report the model performance on natural language understanding tasks in Table 1, focusing on BERT and T5. Besides, We report the model performance on instruction tuning in Table 2, focusing on GPT and T5. The results with variance are reported in Appendix B. From these tables, we have three observations. (1) The perplexity is consistent with the overall performance on downstream tasks. For example, PLD and SSD achieve the lowest perplexity on T5, and they also achieve the top two overall performances on downstream tasks. The same phenomenon also appears on BERT and GPT. It indicates that the perplexity can be used as a good performance indicator on downstream tasks, which is also shown by Brown et al. (2021); Gordon et al. (2020). (2) Densely fine-tuning SMoE can achieve better performance than sparsely fine-tuning SMoE while still being worse than other methods. It indicates that pre-trained SMOE models cannot fully utilize the model capacity even with dense fine-tuning. (3) SSD achieves slightly better overall performance than dense training on all evaluation settings, the only one that achieves this result among the acceleration methods. It demonstrates the general applicability of SSD to different models and tasks."}, {"title": "5.5. Scalability", "content": "Due to the limited computational resources, we assess the scalability of SSD on large models by continual pre-training, which requires fewer training steps than pre-training from scratch. Specifically, we continue pre-training Persimmon-8B on a diverse Chinese corpus, containing encyclopedia, news, books, and web texts. Persimmon-8B (Elsen et al., 2023) is a sparse-activated large language model (LLM) and has competitive performance to LLaMA-7B (Touvron et al., 2023b) on several evaluation benchmarks. We compare SSD with dense training under the same training steps with nearly 4 billion tokens. The details of the training setup are shown in Appendix A. We evaluate the model performance on C-Eval (Huang et al., 2023), a widely-used Chinese benchmark and report the average accuracy of subtasks in Table 3. From this table, we observe that SSD also achieves comparable performance to dense training, which demonstrates the scalability of SSD to LLMs."}, {"title": "5.6. Speed Analysis", "content": "We report the average computation time of the original dense training and SMoE-based training in Table 4. We use four NVIDIA A800 GPU for training and adopt MegaBlocks (Gale et al., 2022) as the SMoE implementation. From this table, we observe that SSD achieve better time speedup on the larger model, i.e., Persimmon-8B. This enhancement is attributed to the higher GPU utilization facilitated by the larger model, making the time speedup more obvious. It highlights the promising speedup potential of SSD on LLMs. However, a discrepancy is noted between theoretical speedup and actual time. A deeper analysis into time consumption across different operations reveals that the expert selection process incurs additional time, thereby presenting an avenue for future research to optimize the computation of SMoE. Although the time speedup of SSD is not fully matched with the theoretical speedup, the time reduction is indeed significant because the pre-training cost is huge and it would be increasingly valuable for large models."}, {"title": "5.7. Ablation Study", "content": "In this subsection, we conduct ablation studies on the transition time determination and transition method. (1) For the transition time determination, we compare the threshold-based method with the random method. In the random method, we will switch the computation mode at each"}, {"title": "6. Conclusion", "content": "In this work, we utilize the phenomenon of sparse activation to accelerate pre-training and inference of LLMs. Specifically, we propose Switchable Sparse-Dense Learning, which adaptively switches between sparse and dense training. Experimental results on three different model architectures and two kinds of downstream tasks show that our method can achieve comparable performance to dense training with less computational costs. Moreover, the models trained with SSD can be directly used as MoE models for inference and reduce the inference time of FFNs by up to 2\u00d7 while keeping the performance as good as dense models. We hope that our work can provide a new perspective for the acceleration based on sparse activation and inspire more research in this direction."}, {"title": "Impact Statement", "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here."}, {"title": "A. Other Experimental Details", "content": "Model Architecture. We use layer normalization before each attention and FFN layer (Xiong et al.", "stages": "the first stage is 1000 steps of sparse training and the second stage is 1000 steps of dense training. We compare the SSD performance on Persimmon-8B with the dense training performance on Persimmon-8B with the same optimization steps. The total expert number N is set to 64 and the selected expert number K is set to 16, keeping the expert size the same as that of the base models. The other hyperparameters are the same as those of base models. When evaluating the performance of Persimmon-8B, we use LM Evaluation Harness (Gao et al., 2021b).\nDownstream Tasks. First, we evaluate models on several natural language understanding tasks. For single-sentence classification, we use SST-2 (Socher et al., 2013), which is a sentiment analysis dataset. For sentence-pair classification, we use SNLI (Bowman et al., 2015), MNLI (Williams et al., 2018), QNLI (Rajpurkar et al., 2016), and QQP\u00b3, covering the tasks of natural language inference and paraphrase identification. For reading comprehension, we use SQUAD v1.1 (Rajpurkar et al., 2016), which is a widely used dataset for extractive question answering. Second, we evaluate models on instruction tuning. Specifically, we follow the setups of Honovich et al. (2022), where the model is trained on a model-generated instruction dataset and evaluated on several human-labeled instruction datasets. The training dataset is Unnatural Instructions (Honovich et al., 2022) and the development dataset contains 1,000 randomly sampled instances from the training set of Super-NaturalInstructions (Wang et al., 2022b), which is used to select the best checkpoint. The test dataset is the test set of Super-NaturalInstructions (Wang et al., 2022b). We conduct a grid search to find the best hyperparameters for each model, including the learning rate varied from 4e-4 to 2e-3, the batch size varied from 16 to 32, and the number"}]}