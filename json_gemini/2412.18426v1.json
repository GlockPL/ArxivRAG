{"title": "GUI Testing Arena: A Unified Benchmark for Advancing Autonomous GUI Testing Agent", "authors": ["Kangjia Zhao", "JiaHui Song", "Leigang Sha", "HaoZhan Shen", "Chen Zhi", "Tiancheng Zhao", "Xiubo Liang", "Jianwei Yin"], "abstract": "Nowadays, research on GUI agents is a hot topic in the\nAl community. However, current research focuses on GUI\ntask automation, limiting the scope of applications in var-\nious GUI scenarios. In this paper, we propose a formal-\nized and comprehensive environment to evaluate the entire\nprocess of automated GUI Testing (GTArena), offering a\nfair, standardized environment for consistent operation of\ndiverse multimodal large language models. We divide the\ntesting process into three key subtasks: test intention gen-\neration, test task execution, and GUI defect detection, and\nconstruct a benchmark dataset based on these to conduct\na comprehensive evaluation. It evaluates the performance\nof different models using three data types: real mobile ap-\nplications, mobile applications with artificially injected de-\nfects, and synthetic data, thoroughly assessing their capa-\nbilities in this relevant task. Additionally, we propose a\nmethod that helps researchers explore the correlation be-\ntween the performance of multimodal language large mod-\nels in specific scenarios and their general capabilities in\nstandard benchmark tests. Experimental results indicate\nthat even the most advanced models struggle to perform\nwell across all sub-tasks of automated GUI Testing, high-\nlighting a significant gap between the current capabilities\nof Autonomous GUI Testing and its practical, real-world\napplicability. This gap provides guidance for the future di-\nrection of GUI Agent development. Our code is available at\nhttps://github.com/ZJU-ACES-ISE/ChatUITest.", "sections": [{"title": "1. Introduction", "content": "\"Imagine a software test engineer casually leaning back in\ntheir chair, saying, 'Alright, test this app for me' and an\nagent springs into action-generating test cases, executing\ntasks, sniffing out bugs\u2014and delivers a complete report, all\nwithout breaking a sweat.\"\nThis envisioned scenario, where agents augment hu-\nman efforts in software testing, is becoming increasingly\nplausible as large language models (LLMs) [11, 32, 37]\nand vision LLMs (VLLMs) [3, 4, 7, 24] emerge as po-\ntent tools for automating complex processes. The inte-\ngration of traditional agents with the cognitive capabili-\nties of LLMs or VLLMs represents a cutting-edge direc-\ntion in contemporary research, with numerous studies [10,\n17, 30, 33] focusing on enhancing navigation frameworks\nfor web and app interfaces. Current methodologies pre-\ndominantly revolve around singular tasks like <assist me\nwith a purchase> or <log in and post a tweet>,\nwhich are executed by these agents.\nHowever, these applications often suffer from a narrow\nfocus, with task complexity increased only through ambigu-\nous instructions or added steps, approaches that do not fun-\ndamentally enhance the agent's capabilities. This limitation\nhighlights the potential application of these technologies in\nautomated GUI Testing, a field of considerable practical\nimportance and complexity, which presents comprehensive\nchallenges to the capabilities of current agents.\nAutomated GUI Testing using LLMs [26, 27, 31] has\ngained substantial traction in recent research. Recently, the\nfocus has shifted towards leveraging MLLMs in place of\nprevious agents, enabling these models to \"see\" and interact\nwith GUI elements visually. This approach has garnered\nattention in both the AI research community [42, 43] and\nthe software engineering (SE) community [19, 28], leading\nto various exploratory studies and small-scale validations\nin industry. As envisioned at the beginning of the paper,\nthe ultimate goal of this research direction is to achieve\nend-to-end automation in GUI Testing.\nHowever, existing agents for automated GUI Testing\ntend to rely on complex framework designs, introducing"}, {"title": "2. Autonomous GUI Testing Agent Workflow", "content": "While the idea of casually handing over an app to an agent\nwith a simple <Test this app for me> sounds appeal-"}, {"title": "2.1. Preliminary", "content": "Current research on Automated GUI Testing predominantly\nconcentrates on resolving specific issues within the domain.\nHowever, a rigorous definition and structured framework\nhave been largely absent. To address this gap, we have\nformalized the process of this task through a Visual-based\nAgent, offering a novel perspective that redefines the task.\nAlgorithm 1 provides the pseudocode implementation of the\narchitecture."}, {"title": "2.1.1 Partially Observable Markov Decision Process", "content": "The cornerstone of our framework is the partially observ-\nable markov decision process (POMDP), which serves as\nthe foundational model for describing the decision-making\nprocess of the Visual-based Agent in GUI Testing scenar-\nios. A POMDP is defined by a tuple (S, O, A, T, R), where\nS denotes the state space of the application's GUI, O repre-\nsents the observation space, A is the set of possible actions,\nT:S\u00d7A\u2192 S is the transition function mapping ac-\ntions in states to probability distributions over states, and\nR:SXAX SR is the reward function. In the context\nof Autonomous GUI Testing, O denotes a partial observa-\ntion of the app's current state. Due to inherent limitations, a\nGUI agent cannot fully capture all state information, partic-\nlarly for closed-source applications where key elements,\nsuch as the Accessibility Tree, are inaccessible. This par-\ntial observability impacts the likelihood of detecting issues\nduring GUI Defect Detection, as the agent relies on limited\nfeedback to infer potential defects. Detection likelihood,\ntherefore, depends on whether a defect is observable follow-\ning a specific action. To address this challenge, the reward\nfunction R is designed to assign positive rewards for suc-"}, {"title": "2.1.2 GUI Defect Data Model", "content": "To systematize the identification and classification of GUI\ndefects, we introduce a novel data structure termed the\nTransition Tuple (states, action, statea). This tuple ef-\nfectively captures the GUI state before (states) and after\n(statea) an action is executed, with the action itself rep-\nresented in the middle. A sequence of such tuples forms a\ncomplete path of state transitions within the application. We\ndefine a specialized action \u00d8, distinct from standard agent\noperations, to signify performing no operation on the app.\nWe have designed two classes to support our data model:\nthe State class, which encapsulates methods init for initial-\nizing a state and act for performing an action, and the Tran-\nsition Tuple class, with methods init to create instances and\ncheck to evaluate transitions for defects. This classification\naids in formalized defect detection, allowing for delayed,\nyet comprehensive defect analysis without the need for real-\ntime feedback."}, {"title": "2.2. Test Case Generation", "content": "Test Case Generation can be viewed as the process of cre-\nating a structured chain of states and actions aligned with\nthe testing intentions. Given a mobile application, the agent\nbegins by gathering a brief overview of the app and creat-\ning test intentions, defining what needs to be tested. Once\nthe test intention is defined, the agent can execute the test-\ning tasks outlined in Section 2.3, generating a comprehen-\nsive test case that includes both the test intention and cor-\nresponding test steps. This structured approach enables the\nagent to conduct targeted and informed testing on the same\napp in future scenarios, such as version updates or feature\nenhancements. By leveraging these predefined test cases,\nthe agent can focus on high-priority areas and adapt its test-\ning to ensure new changes align with expected functionality,\nmaking the testing process both efficient and scalable."}, {"title": "2.3. Test Task Execution", "content": "In executing the detailed test case, the multimodal agent\nperforms several key actions to interact with the mobile ap-\nplication. The primary actions include Click, Scroll, and\nType, along with the additional actions Stop and Enter. The\nexecution starts with the agent activating the initial state of\nthe application. As the agent interacts with the interface,\nit records every action, capturing screenshots and logging\neach step. This thorough recording process ensures that the\nimpact of each action is documented, allowing for a detailed\nassessment of the app's response to user interactions. As the\nagent progresses through the test case, it navigates various\nscreens and functionalities of the application. The execu-\ntion phase continues until one of two outcomes occurs: task\ncompletion or a problem, using Stop to represent.\nThis structured approach to executing test cases, com-\nbined with the agent's ability to record and react to the ap-\nplication's state. This systematic execution process is essen-\ntial for GUI automation testing, ensuring that multimodal\nlarge models can effectively mimic human testers in testing\nmobile applications."}, {"title": "2.4. GUI Defect Detection", "content": "In GUI automation testing, defect detection is essential for\nensuring application quality and usability. GUI defects can\nbe broadly classified into two main categories: Display De-\nfects (DD) and Interaction Defects (ID). [20, 29, 41] Dis-\nplay Defects focus on the visual presentation of the UI and\ninclude:\n\u2022 Data Display, such as Content Error and Data Type or\nFormat Error.\n\u2022 Layout, including UI Element Missing, UI Element\nOverlapping, Alignment Issues, and Uneven Spacing.\n\u2022 Style, such as Inconsistent Color, Inconsistent Element\nSize, and Abnormal UI Element State."}, {"title": "3. How To Benchmark Autonomous GUI Test-ing Agent", "content": "Building a unified benchmark for the entire automated\nGUI Testing framework requires addressing challenges that\narise from the segmented focus of prior research. Existing\nbenchmark often isolate specific components, such as task\nexecution[8, 47] or defect detection [5, 36]. Hence, we es-\ntablish an end-to-end evaluation system, ensuring seamless"}, {"title": "3.1. Real-world Applications with GUI Defects", "content": "Real-world applications serve as a crucial component of our\nbenchmark by providing insights into naturally occurring\nGUI defects. These defects are identified by mining up\ngrade logs and issue trackers from open-source repositories\non GitHub [15]. Our approach involves systematically fil\ntering and extracting relevant projects based on the descrip tions in their change logs and issue reports, specifically fo cusing on entries related to GUI defects. This targeted fil tering ensures that the selected applications contain genuine\nand relevant defects within their user interfaces.\nFor each identified project, we document essential ap plication details, such as the version history, defect type,\nand issue severity, to build a comprehensive profile of the\ndetected defects. To validate these defects, we carefully re produce the reported issues, ensuring that the GUI defects\nare replicable and align with the descriptions provided by\nthe developers. By leveraging this methodology, we not\nonly ensure the relevance and authenticity of the collected"}, {"title": "3.2. Applications with injected defects", "content": "Real-world applications exhibit a wide range of complex\nand unpredictable GUI defects, making it difficult to en-\nsure consistency across testing scenarios. To address this,\nwe inject defects at the source code level or use the Mu-\ntAPK tool[13, 14] to introduce controlled, predefined GUI\ndefects into mobile applications[1, 2]. The injection of spe- cific defects allows us to maintain strict control over the\ntesting framework's results. Introducing defects in various\nareas not only ensures consistency but also creates diverse\nfault scenarios. The controlled nature of this method en- ables repeatable experiments, helping researchers system- atically explore the strengths and weaknesses of different\ntesting models. Additionally, by increasing the complexity\nof the injected defects, we can push the boundaries of the\nagent, ensuring it can handle the kind of diverse challenges\nin real-world applications."}, {"title": "3.3. Synthetic defect datasets", "content": "For most commercial applications, source code is propri- etary and public releases are generally stable, having un- dergone multiple testing iterations. Consequently, these\napps rarely contain the early-stage GUI defects essential\nfor benchmarking the agent. To overcome this limitation,\nwe adopt a synthetic approach, transforming screenshots of\nstable applications to simulate a variety of visual and in- teraction defects. This technique allows us to obtain GUI\ndefect data from any app, even complex and mature com- mercial applications. Specific defect construction types and\nexamples are shown in Figure 3."}, {"title": "4. Correlation Analysis Between General Ca-pabilities and GUI Autonomous Test Per-formance", "content": "A key assumption in our approach is that a fine-tuned model\nperforms better on specific sub-tasks because it has im- proved mastery of the skills needed for those tasks. In au- tomated GUI Testing, tasks like test case generation and\ndefect detection require both perception (e.g., recognizing\nvisual elements) and reasoning (e.g., interpreting naviga- tion logic and workflows). However, it's difficult to de- lineate exactly which capabilities contribute most to suc- cess. The ability to navigate between screens, detect over- lapping elements, or respond correctly to unresponsive but- tons may draw on multiple, interconnected competencies. Often, these dependencies are not straightforward."}, {"title": "Therefore, we propose an evaluation method that fine-tunes models on datasets specific to GUI Testing tasks and then assesses their performance on broad, standard-ized benchmarks. This comparative analysis, contrasting a model's pre- and post-fine-tuning performance, offers in- sights into the capabilities that are most relevant to specific stages of the GUI Testing process. For example, improve- ments in benchmarks focused on perception may indicate the model's enhanced ability to identify subtle layout issues, while gains in reasoning-oriented benchmarks might reflect better handling of navigation errors or task flows. Addition- ally, real-world GUI Testing tasks often suffer from data scarcity. Our method provides a pathway for expanding datasets by strategically selecting general datasets aligned with the task's requirements.\nThis framework ties back to the core goal of our paper: building a comprehensive, end-to-end benchmark for GUI automation testing. By bridging the gap between general benchmark performance and task-specific outcomes, we of- fer a practical methodology for identifying the key capabil- ities that matter most. This not only enhances the reliability of visual-based agent for automated GUI Testing but also lays the foundation for continuous model improvement.", "content": "Therefore, we propose an evaluation method that fine-\ntunes models on datasets specific to GUI Testing tasks\nand then assesses their performance on broad, standard-\nized benchmarks. This comparative analysis, contrasting a\nmodel's pre- and post-fine-tuning performance, offers in-\nsights into the capabilities that are most relevant to specific\nstages of the GUI Testing process. For example, improve-\nments in benchmarks focused on perception may indicate\nthe model's enhanced ability to identify subtle layout issues,\nwhile gains in reasoning-oriented benchmarks might reflect\nbetter handling of navigation errors or task flows. Addition-\nally, real-world GUI Testing tasks often suffer from data\nscarcity. Our method provides a pathway for expanding\ndatasets by strategically selecting general datasets aligned\nwith the task's requirements.\nThis framework ties back to the core goal of our paper:\nbuilding a comprehensive, end-to-end benchmark for GUI\nautomation testing. By bridging the gap between general\nbenchmark performance and task-specific outcomes, we of-\nfer a practical methodology for identifying the key capabil-\nities that matter most. This not only enhances the reliability\nof visual-based agent for automated GUI Testing but also\nlays the foundation for continuous model improvement."}, {"title": "5. Experiment", "content": null}, {"title": "5.1. Experiment Setup and Evaluation Criteria", "content": "In alignment with the workflow detailed in Section 2, our\nexperimental setup benchmarks the performance of various\nmultimodal large models under a unified framework. This\napproach allows for a direct comparison of these models\nin a consistent architecture, assessing their effectiveness as\nagents in generating test intentions, executing test steps, and\nconducting GUI defect detection. Since the test case com-\nprises a test intention and corresponding test steps, which\nrelies on task execution result. Therefore, our primary fo-\ncus is on evaluating the generation of test intentions and,\nbased on this, assessing the effectiveness of task execution.\nCoverage. The model generates a variable number of\ntest intentions based on the app and background informa-\ntion. To account for the semantic ambiguity in test inten-\ntions, each generated intention is matched to the human-\nannotated ground truth by GPT as judge, assessing if it\naligns with the true set of test intentions. The judgment\nprompt template we used is provided in Appendix. The pro-\nportion of correctly aligned intentions represents the cover-\nage rate for test intention generation.\nTM, EM and SR. For Test Task Execution, we em-\nploy TM (Type Match), EM (Exact Match), and SR (Suc-\ncess Rate) as evaluation metrics. For each defined tuple\n(stater, action, statea), TM indicates whether the model\ncorrectly predicts the type of action to take in the next step.\nEM assesses, given a correct action type, whether the action"}, {"title": "5.2. Baseline Autonomous GUI Testing", "content": "To establish a baseline, we select models with significantly\ndifferent architectures and native multimodal capabilities\nfor comparison, including GPT-40, Claude, LLaVA, and\nQwen2-VL.\nThe experimental results, shown in Table 2, indicate that\nGPT-40 and Claude perform comparably across most met-\nrics and outperform open-source models. Although the dif- ference in metrics between closed-source commercial mod- els and open-source models is relatively small, the over- all poor performance highlights a significant gap between\nopen-source and closed-source models. In the test inten- tion generation metric, LLaVA and Qwen2-VL lag consid- erably behind GPT-40 and Claude, suggesting that tasks re-"}, {"title": "5.3. Ablation Study on Test Task Execution", "content": "In Section 2.2, we discussed how the action sequence per- formed by the agent under the guidance of a test intention can be used to construct test steps, thereby generating a complete test case. This raises the question: Does using test steps as instructions enable the agent to complete tasks more effectively compared to using the test intention? To explore this, we concatenated the action sequences within individual test tasks to form the corresponding test steps for each task. We then evaluated the performance of GPT-40, LLaVA, and Qwen2-VL on test task execution under these two types of instructions.\nThe experimental results in Table 3 show that both LLaVA and Qwen2-VL exhibited performance improve- ments when given test steps as instructions. GPT-40, how- ever, showed minimal change in performance, even per- forming worse than Qwen2-VL when using test steps as instructions. This suggests that the limitation in GPT-40's performance in test task execution does not stem from its ability to accurately interpret test intentions but rather from the inherent complexity of GUI interfaces. Qwen2-VL, hav- ing been trained on GUI data, benefits from the clarity in identifying the next action to execute, resulting in a more significant performance boost when provided with explicit test steps."}, {"title": "6. Related Work", "content": "Agent on GUI navigation task. Early GUI agents [16, 22, 23, 35] primarily relied on training models to explore GUI environments with task completion as the main objective. With advances in multimodal large models [3, 6], current approaches have shifted from traditional training to using techniques like prompt tuning [21] and in-context learn- ing [46] to guide these models in exploration tasks. Ap- pAgent [44], Mobile-Agent [38], and AutoDroid [40] uti- lize LLMs to interpret natural language descriptions and transform them into GUI actions. Additionally, some work [10, 17] has focused on fine-tuning large models on GUI- specific data to improve their performance in GUI environ- ments. Agent workflow memory [39] represents another re- cent innovation, enhancing agents' ability to automatically construct workflows, thus introducing a new paradigm for GUI task automation.\nThere has also been progress in creating benchmarks for GUI navigation [12, 23, 35]. WebArena [47], for instance, constructs realistic web environments with callable tools to study the limitations of models like GPT-4V, revealing a significant gap in agent performance compared to humans in complex tasks. AitW [34] collected large-scale data by having annotators operate apps in a simulator to capture hu- man instruction-following behavior, though data quality re- mains a concern. Building on this, AitZ [45] introduced high-quality navigation data with GPT-4-annotated Chain- of-Thought (CoT) reasoning, along with new metrics to evaluate agent performance in GUI navigation tasks.\nGUI Defect Detection. Given the close connection be- tween GUI quality and user experience, various methods have been developed to detect bugs in GUIs. GUI Testing in industry relies heavily on scripted tests to automate function validation. To address this, AppFlow [18] applies machine learning to identify screen components, allowing testers to develop modular libraries for core application functions. CoSer [9] constructs UI state transition graphs from source code and scripts to repair outdated tests. Recently, LLMS have emerged as powerful tools in GUI Testing due to their extensive training on diverse data and strong reasoning abil- ities. For example, QTypist [25] focuses on generating se- mantic text inputs for form fields to improve exploration coverage. GPTDroid [27] extracts page and widget infor- mation from the UI hierarchy, using it to create human- like interactions. AUITestAgent[19] developed an industry- applicable automatic natural language-drifillven GUI Test- ing method. VisionDroid [28] addresses non-crash bug de- tection in GUIs by leveraging LLMs to detect unexpected behaviors, particularly in scenarios where testing oracles are lacking."}, {"title": "7. Conclusion", "content": "This paper introduces a formalized framework for Au- tonomous GUI Testing, aimed at addressing key limitations in visual-based agent evaluation. By structuring the testing workflow with precise mathematical definitions and decou- pling GUI defect detection from task execution, we present a fair and robust environment (GTArena) for evaluating GUI Testing capabilities. Our work includes a novel data struc- ture for capturing GUI defects, which facilitates the creation of large-scale datasets.\nFurthermore, we propose a unified benchmark to assess visual-based agents equipped with multimodal large models (MLLMs), evaluating their performance across core com- ponents: test case generation, test task execution, and GUI defect detection. Through this structured benchmark, we reveal notable performance gaps between current agents and practical application for mainstream VLLMs, under- scoring the need for targeted model improvements. Addi- tionally, our methodology offers a systematic approach for fine-tuning models on task-specific datasets, while evaluat- ing their general capabilities on broader benchmarks.\nIn conclusion, our work provides a fair, unified and end- to-end environment for automated GUI Testing, enabling convenient and reproducible evaluation of various multi- modal large models in their role as agents. By bridging the gap between theoretical frameworks and practical evalua- tions, we aim to accelerate the development of more capa- ble, reliable, and efficient agents for GUI Testing applica- tions."}, {"title": "A. GUI Defect Types", "content": "The specific types of GUI defects and examples are shown in Table 4 and 5."}, {"title": "B. GUI Defect Dataset Examples", "content": "Some real-world defects from Github releases in Table 6. Examples of artificial injected defects and episode from AitW with defects are show in Figure 4 and Figure 5."}]}