{"title": "CLIP-MOE: TOWARDS BUILDING MIXTURE OF EXPERTS FOR CLIP WITH DIVERSIFIED MULTIPLET UPCYCLING", "authors": ["Jihai Zhang", "Xiaoye Qu", "Tong Zhu", "Yu Cheng"], "abstract": "In recent years, Contrastive Language-Image Pre-training (CLIP) has become a cornerstone in multimodal intelligence. However, recent studies have identified that the information loss in the CLIP encoding process is substantial, and CLIP tends to capture only coarse-grained features from the input. This deficiency significantly limits the ability of a single CLIP model to handle images rich in visual detail. In this work, we propose a simple yet effective model-agnostic strategy, Diversified Multiplet Upcycling (DMU), for CLIP. This strategy innovatively mitigates information loss in CLIP multimodal alignment by integrating multiple CLIP models that capture diversified, complementary information into a CLIP Mixture of Experts (MoE) with sparse activation. However, achieving such multiplet CLIP models is far from straightforward, as existing open-source CLIP models cannot directly form homogeneous experts or capture diversified complementary information. Inspired by the recently proposed Multistage Contrastive Learning (MCL), which constructs multiple CLIP models that share the same structure while capturing different complementary information, Diversified Multiplet Upcycling efficiently fine-tunes a series of CLIP models that capture different feature spaces, from a dense pre-trained CLIP checkpoint, sharing parameters except for the Feed-Forward Network (FFN). These models can then be transformed into a CLIP-MoE with a larger model capacity, leading to significantly enhanced performance with minimal computational overhead. To the best of our knowledge, Diversified Multiplet Upcycling is the first approach to introduce sparsely activated MoE into CLIP foundation models, whereas previous methods have focused solely on vision representations or model-wise ensembling. Extensive experiments demonstrate the significant performance of CLIP-MoE across various zero-shot retrieval, zero-shot image classification tasks, and downstream Multimodal Large Language Model (MLLM) benchmarks by serving as a vision encoder. Furthermore, Diversified Multiplet Upcycling enables the conversion of any dense CLIP model into CLIP-MoEs, which can seamlessly replace CLIP in a plug-and-play manner without requiring further adaptation in downstream frameworks. Through Diversified Multiplet Upcycling, we aim to provide valuable insights for future research on developing more efficient and effective multimodal learning systems. Codes and models are released at: https://github.com/OpenSparseLLMS/CLIP-MOE.", "sections": [{"title": "1 INTRODUCTION", "content": "Contrastive Language-Image Pre-training (CLIP) (Radford et al., 2021) is a strong vision-language foundation model that utilizes large-scale datasets to learn comprehensive visual representations by"}, {"title": "2 RELATED WORKS", "content": ""}, {"title": "2.1 CONTRASTIVE LEARNING", "content": "In contrastive learning, the core objective is to minimize the distance between positives and the anchor while maximizing the distance between negatives and the anchor within the representation space. This objective compels the model to effectively encode sufficient information of the inputs to distinguish anchors from their negatives.\nContrastive learning has become a central technique in self-supervised learning, aiming to learn representations by bringing semantically similar samples closer in the embedding space while pushing dissimilar samples apart (Chen et al., 2020; He et al., 2020). This approach has been particularly successful in multimodal settings, where models like Contrastive Language-Image Pre-training (CLIP) (Radford et al., 2021) have emerged as foundational tools. CLIP aligns visual and textual representations by training on vast datasets of paired images and text, enabling the model to bridge different modalities effectively.\nDespite its success, CLIP is not without its limitations. One significant shortcoming is its tendency to encode only coarse-grained visual concepts, which can lead to the loss of fine-grained information that is crucial for certain downstream tasks (Tang et al., 2023; Tong et al., 2024b). To address these limitations, recent works mainly focus on improving the quality of training data (Li et al., 2024b; Ma et al., 2024; Xu et al., 2023; Zhang et al., 2024a). However, most of these approaches require retraining the model from scratch, which is computationally expensive, time-consuming, and not easily extendable when better data becomes available."}, {"title": "2.2 MIXTURE-OF-EXPERTS", "content": "The Mixture-of-Experts (MoE) architecture could scale the model capacity without additional computational cost (Fedus et al., 2022a). For each input token, only top-k best experts are selected to obtain an aggregated representation (Shazeer et al., 2017). This sparsity allows MoE models to scale to trillions of parameters while maintaining the computational efficiency (Lepikhin et al., 2020; Fedus et al., 2022b). Due to the large model capacity, the performance could be improved by large margins (Rajbhandari et al., 2022; Dai et al., 2024). Besides, specialized experts in MoE models are good at handling a wide range of tasks (Shen et al., 2023; Zhu et al., 2024; Lu et al., 2024) with high robustness (Chen et al., 2024a).\nHowever, one challenge in MoE training is expert initialization. Sparse Upcycling (Komatsuzaki et al., 2022) has been proposed as a technique to initialize MoE models by copying Feed-Forward Networks (FFN) from dense models as multiple experts. It selectively activates and fine-tunes only a sparse subset of parameters. This method significantly reduces the training cost.\nIn this work, we explore the integration of Multistage Contrastive Learning (MCL) with the MoE architecture. By using MCL to initialize the experts, we aim to capture complementary information across different CLIP experts, which can then be leveraged by the MoE structure to enhance overall performance with minimal additional computational cost."}, {"title": "3 PRELIMINARIES", "content": ""}, {"title": "3.1 MULTISTAGE CONTRASTIVE LEARNING (MCL)", "content": "Multistage Contrastive Learning (MCL) (Zhang et al., 2024b) is designed to obtain a series of contrastive models, each capturing different and complementary information from the input data through multiple cluster-and-contrastive processes. Specifically, at each stage, the learned representations are clustered. In the following stage, for each anchor, negative samples are drawn only from the same accumulated cluster from the previous stages. In this way, the model learns new information beyond what was captured in earlier stages. For example, consider a dataset containing objects with"}, {"title": "3.2 MIXTURE OF EXPERTS (MOE)", "content": "Mixture of Experts (MoE) is an efficient architecture designed to scale large models by dynamically routing inputs through a subset of specialized sub-models, or \u201cexperts\u201d. This structure allows the model to maintain high overall capacity while only utilizing a fraction of its parameters for any given input, thereby optimizing both computational efficiency and performance.\nIn the context of Transformer, an MoE layer (Jiang et al., 2024) typically replaces the standard feed-forward network (FFN) with a set ${E_i}_{i=1}^N$ of N experts, each of which is an independent FFN. Given an input token representation x, it first passes through a gating network $W_r$, to obtain the logits corresponding to each expert, then the largest Top-K experts will be chosen, and finally, the probabilities of these selected experts are normalized using Softmax. In this way, we can obtain the probability R(x) of selected experts among all N experts. Notably, the probability of non-\n$X_{out} = \\sum_{i=1}^N R(x)_i \\cdot E_i(x), R(x) = Softmax(TopK(x \\cdot W_r)),$ (2)\nwhere $R(x)_i$ denotes the i-th component of the routing weight vector produced by the router network $W_r$.\nTo ensure that all experts are utilized effectively and prevent the model from overfitting to a small subset of experts, a load balancing loss (Fedus et al., 2022b) is often added to the primary loss function. This loss penalizes imbalanced expert usage by encouraging a more uniform distribution of the input tokens across all experts."}, {"title": "4 DIVERSIFIED MULTIPLET UPCYCLING FOR CLIP", "content": ""}, {"title": "4.1 EXPERT EXTRACTION", "content": "We begin by extracting a series of Feed-Forward Network (FFN) layers utilizing Multistage Contrastive Learning (MCL) to fine-tune a pre-trained base CLIP model for multiple stages. During"}, {"title": "4.2 INITIALIZATION OF MIXTURE OF EXPERTS", "content": "Once a series of FFN layers ${E_i}_{i=1}^{S_i}$ have been obtained through N stages of MCL, we utilize these FFNs as the experts in a Mixture of Experts (MoE) model, as depicted in Figure 1. According to Equation 2, in the ith transformer block of the base CLIP model, the original FFN layer is replaced with a randomly initialized router and a set of experts:\n$X_{out}^{(2)} = \\sum_{j=0}^N R_j^{(2)} (x^{(i)}) \\cdot E_j^{(2)}(x^{(i)}), R^{(2)} (x^{(i)}) = Softmax(TopK(x^{(i)} \\cdot W^{(2)})),$ (3)\nwhere $R^{(2)}(x)_i$ denotes the j-th component of the routing weight vector produced by the router network $W_r^{(2)}$ in the ith transformer block. This setup results in a CLIP-MoE model where different experts within different transformer blocks specialize in distinct aspects of the input."}, {"title": "4.3 CONTINUOUS FINE-TUNING OF CLIP-MOE", "content": "To enable the model to learn optimal routing strategies while preserving the information learned by the FFN layers during MCL, we further fine-tune the routers while freezing all other parameters. We apply the standard contrastive learning loss while incorporating an auxiliary load balancing loss, following the approach from Fedus et al. (2022b), to encourage a balanced load across experts. Given N + 1 experts indexed by j = 0 to N, and a batch B with T tokens, the load balancing loss for the ith transformer block is defined as:\n$\\mathcal{L}_{balance} = N \\cdot \\sum_{j=0}^N f_j \\cdot P_j, f_j = \\frac{1}{T} \\sum_{x \\in B} \\mathbb{1}\\{argmax p(x) = j\\}, P_i = \\frac{1}{T} \\sum_{x \\in B} p_j(x),$ (4)\nwhere $f_j$ is the fraction of tokens assigned to expert j, and $p(x)$ is the logits output from the router network; $P_j$ represents the fraction of router probability allocated to expert j, which is the mean of $p_j(x)$, the probability of routing token x to expert j. For simplicity, we omit the transformer block index i in the equation. Since $f_j$ and $P_j$ are positive and both their sums are equal to 1, $\\mathcal{L}_{balancing}$ is minimized if and only if $f_j = \\frac{1}{N+1}, P_j = \\frac{1}{N+1}$. This balancing loss encourages not only a uniform distribution of actual tokens routed to each expert (i.e., ensuring that all experts have equal importance), but also a uniform distribution of router confidence across tokens (i.e., preventing the router from being overly confident for some tokens and underconfident for others). With this auxiliary load balancing loss, the total loss is given by:\n$\\mathcal{L} = \\mathcal{L}_{CLIP} + \\alpha \\cdot \\frac{1}{A+B} \\sum_{i=1}^{A+B} \\mathcal{L}_{balance},$ (5)\nFollowing Fedus et al. (2022b), we set $\\alpha = 0.01$ by default. By applying MoE-Packing to CLIP, we obtain a CLIP-MoE model that is capable of capturing more useful information than the base model, with minimal computational overhead, resulting in a robust and efficient enhancement of the CLIP model."}, {"title": "5 EXPERIMENTS", "content": ""}, {"title": "5.1 DATASETS", "content": "To fully showcase the potential of our MCL-initialized CLIP-MoE, we implement our experiments on the following two image-caption datasets respectively.\nRecap-DataComp. Recap-DataComp-1B (Li et al., 2024b) is a large-scale dataset comprising 1.3 billion high-quality image-caption pairs. This dataset is derived from the original DataComp-1B dataset, with all images re-captioned using a fine-tuned LLaVA-1.5 model powered by LLaMA-3 (Dubey et al., 2024). Li et al. (2024b) utilized this dataset to train CLIP models from scratch, resulting in significant improvements in retrieval performance. Due to computational constraints, our experiments use a randomly sampled subset of 1 million pairs from Recap-DataComp-1B, referred to as Recap-DataComp-1M, to demonstrate the data efficiency of our proposed pipeline.\nShareGPT4V. ShareGPT4V (Chen et al., 2023) is a high-quality image-text dataset containing 1.2 million highly descriptive captions. The captions are generated by a Multimodal Large Language Model (MLLM) fine-tuned on 100k image-text pairs produced by GPT4V, resulting in well-aligned image-text pairs. In this paper, we xx"}, {"title": "5.2 BASELINES", "content": "Direct Fine-tuning. As our experiments incorporate additional data, we use direct fine-tuning as a basic baseline to serve as a reference for evaluating the performance contributions from the additional data.\nSparse Upcycling. Sparse Upcycling (Komatsuzaki et al., 2022) is a widely adopted method for initializing a Mixture of Experts (MoE) model using a pre-trained dense checkpoint. It is a simple yet effective approach for scaling up a pre-trained model and is much more efficient than training an MoE from scratch."}, {"title": "5.3 TRAINING SETUP", "content": "By default, we use OpenAI CLIP-ViT-L/14 (Radford et al., 2021) as the base model for our Diversified Multiplet Upcycling approach. During the clustering process at each stage of MCL, we cluster the image features into 3 clusters and the text features into 3 clusters, resulting in 9 clusters per stage (the Cartesian product of the image and text feature clusters). To accommodate longer text inputs, we interpolate the positional embeddings following the approach in (Zhang et al., 2024a). The global batch size is maintained at 800 unless otherwise specified. To balance performance and computational cost, we set the number of experts to 4 and use top-2 activation."}, {"title": "5.4 TRAINING COST", "content": "We use 8 A100 GPUs for training. To train the CLIP-MOE model with four experts, we introduce three additional MCL fine-tuning stages, each trained for 1 epoch. When using the ShareGPT4V dataset, each MCL stage takes approximately 0.5 hours, and the router fine-tuning stage also takes about 0.5 hours. In total, the training time is less than 2.5 hours. In comparison, Long-CLIP training under the same conditions takes around 6 hours, making our approach significantly more efficient. Our maximum GPU memory usage is 8\u00d765955MB, which is comparable to Long-CLIP's 8\u00d763581MB. When training on the Recap-DataComp-1M dataset, the training cost is even lower. During inference, with top-2 activation, the activated parameter size of our CLIP-MoE is approximately 1.7 times that of the base model (OpenAI CLIP-ViT-L/14)."}, {"title": "5.5 EVALUATION", "content": "We begin by evaluating the performance of CLIP-MoE on Zero-Shot Image-Text Retrieval, a key task for assessing whether the CLIP model can capture rich fine-grained information, following Zhang et al. (2024a). All baselines are trained and compared using the Recap-DataComp-1M (Recap-DC) and ShareGPT4V (ShareGPT) datasets, with the exception of Long-CLIP. Long-CLIP is incompatible with the Recap-DataComp dataset, as it requires both a short and long caption for each image, whereas Recap-DataComp provides only one caption per image. Next, we assess the effectiveness of CLIP-MoE as a vision encoder within LLaVA-1.5, a representative Multimodal Large Language Model (MLLM). LLaVA-1.5 serves as an effective visual representation evaluator, helping to mitigate potential biases present in traditional evaluation tasks (Tong et al., 2024a). Finally, we test CLIP-MoE on traditional Zero-Shot Image Classification tasks, which rely more on coarse-grained features.\nZero-Shot Image-Text Retrieval. Following the methodology outlined in Zhang et al. (2024a), we evaluate text-to-image (T2I) and image-to-text (I2T) retrieval on the 5k COCO validation set (Lin et al., 2014) and the 30k Flickr30k (Young et al., 2014) dataset. The results are presented in Table 1. Given that both Recap-DataComp-1M and ShareGPT4V datasets offer higher caption quality and longer average caption lengths compared to web datasets, Direct Fine-Tuning, Sparse Upcycling, and CLIP-MOE demonstrate superior performance over the original OpenAI model across most tasks, including COCO I2T, COCO T2I, and Flickr T2I. However, for Flickr I2T, Sparse Upcycling, and Direct Fine-Tuning show significant performance degradation on the Recap-DC dataset."}, {"title": "5.6 DISCUSSION", "content": ""}, {"title": "Ablation Study on MCL", "content": "To further validate the effectiveness of expert extraction utilizing MCL in Diversified Multiplet Upcycling, we conducted an ablation study on ShareGPT4V by training a CLIP-MoE model with only two experts: one from the original OpenAI CLIP and one from fine-tuning FFN layers on ShareGPT4V. As seen in Table 4, the performance of CLIP-MoE on the retrieval tasks is consistently higher than the model without MCL stages 1 and 2, demonstrating that more MCL stages do obtain experts that capture more useful information. The slight degradation in ImageNet zero-shot classification performance is expected, as not all of the additional learned information is beneficial for classification, which tends to rely on more coarse-grained features (Zhang et al., 2024a)."}, {"title": "Routing analysis", "content": "To evaluate whether all the experts learned through MCL are utilized by CLIP-MoE, we perform an analysis of the routing strategy. We use the CLIP-MoE model with 4 experts and top-2 routing trained on ShareGPT4V, and compute the proportion of tokens assigned to each expert. For retrieval tasks, we use the COCO validation dataset, and for zero-shot image classification, we use the ImageNet validation dataset. The analysis results are presented in Table 2. From the results, we observe that for experts from each MCL stage (represented by each column in the heatmap), there are consistently yellow areas (indicating heavily utilized experts). No column is entirely dark blue, which indicates that all MCL stages contribute useful experts to CLIP-MoE. This further validates the effectiveness of our MCL initialization in MoE-Packing."}, {"title": "Case Study", "content": "We demonstrate the comparison between CLIP-MoE and OpenAI CLIP on samples from the MMVP-VLM Benchmark (Tong et al., 2024b). MMVP-VLM contains manually filtered image pairs with different semantics that are difficult to distinguish using the vanilla OpenAI CLIP. We task the models with matching the corresponding statement to the image. As shown in Figure 3, OpenAI CLIP struggles to distinguish fine-grained details in these image pairs. In cases like the alarm clock, OpenAI CLIP matches both images to the statement \"hour hand points at 10.\" In other"}, {"title": "6 CONCLUSION & FUTURE WORK", "content": "In this paper, we proposed a novel Diversified Multiplet Upcycling for CLIP to enhance the model with minimal computational overhead. Our method enables the extraction of diversified and complementary experts across multiple fine-tuning stages, which are then utilized within the MoE framework to capture richer information from the inputs. This approach is straightforward to apply, model-agnostic, and provides a new path to scale and improve CLIP foundation models. By leveraging off-the-shelf CLIP checkpoints and newly constructed high-quality image-text datasets, our method avoids the costly process of training CLIP models from scratch. We demonstrated the effectiveness and efficiency of our approach through extensive experiments across various datasets and tasks.\nFor future work, our current experiments are limited to image and text modalities. We plan to extend our method to additional modalities, such as audio and video. Beyond the fine-tuning settings explored in this paper, we aim to experiment with larger datasets and test large-scale continuous training settings to further explore the scalability and performance boundaries of Diversified Multiplet Upcycling. Additionally, while we tested CLIP-MoE as a vision encoder for MLLMs, we will also investigate its potential as a text encoder in generative tasks, such as in stable diffusion."}]}