{"title": "Investigating the Role of Prompting and External Tools in Hallucination Rates of Large Language Models", "authors": ["Liam Barkley", "Brink van der Merwe"], "abstract": "Large language models (LLMs) are powerful computational models trained on extensive corpora of human-readable text, enabling them to perform general-purpose language understanding and generation. LLMs have garnered significant attention in both industry and academia due to their exceptional performance across various natural language processing (NLP) tasks. Despite these successes, LLMs often produce inaccuracies, commonly referred to as hallucinations. Prompt engineering, the process of designing and formulating instructions for LLMs to perform specific tasks, has emerged as a key approach to mitigating hallucinations. This paper provides a comprehensive empirical evaluation of different prompting strategies and frameworks aimed at reducing hallucinations in LLMs. Various prompting techniques are applied to a broad set of benchmark datasets to assess the accuracy and hallucination rate of each method. Additionally, the paper investigates the influence of tool-calling agents-LLMs augmented with external tools to enhance their capabilities beyond language generation-on hallucination rates in the same benchmarks. The findings demonstrate that the optimal prompting technique depends on the type of problem, and that simpler techniques often outperform more complex methods in reducing hallucinations. Furthermore, it is shown that LLM agents can exhibit significantly higher hallucination rates due to the added complexity of external tool usage.", "sections": [{"title": "1 Introduction", "content": "LLMs are computational models capable of general-purpose language generation. LLMs have become ubiquitous in recent years owing to their ability to perform a variety of NLP tasks, such as language translation, comprehension, textual summarization, and question-answering. Moreover, LLMs have shown promise in solving complex reasoning tasks such as writing code or answering mathematical problems [11]. However, despite their successes, current LLMs exhibit a concerning tendency to generate inaccurate or misleading information, often referred to as hallucinations. Owing to their stochastic nature, LLMs may produce plausible outputs that lack or contradict real-world evidence.\nHallucinations from LLMs can produce fallacies, biases or misinformation, which is particularly concerning as they garner widespread use. One of the most accessible and widely used LLMs to date, ChatGPT, owned by OpenAI, has approximately 185.5 million users [13]. Furthermore, one of the world's leading instant messaging platforms, WhatsApp, introduced an LLM-powered chatbot on the platform in April 2024, further facilitating the use of these models by the public. The growing popularity of LLMs, coupled with a general lack of understanding, can lead to crucial inaccuracies, especially in political or medical contexts, with potentially serious consequences. Therefore, it is imperative to study the causes of hallucinations in LLMs and develop methods to mitigate them.\nLLM prompts are the text-based input that allows users to interact with the model. It is usually a question or set of instructions that triggers a specific response or action in the LLM. Prompt engineering is the nuanced practice of designing LLM prompts to optimize the output of the LLM so that it completes the desired task correctly. Prompting frameworks offer a systematic way to design prompts so that the LLM elicits the desired output. While certain prompting frameworks are specifically designed for the detection and mitigation of hallucinations, general prompt engineering also facilitates the reduction of hallucinations since the goal is to optimize the correctness of LLM responses.\nLLM agents are artificial intelligence (AI) systems designed to solve complex tasks. The main idea behind LLM agents is to perform a sequence of actions using an LLM as the central reasoning engine, that dictates which actions to take and in what order. These agents are often augmented with external tools to expand the capabilities of the model beyond language generation. At the core of these systems are entire prompting frameworks, often referred to as agent architectures, which dictate how many steps the model may take and what it can do at each step. Given that LLM agents are frequently employed to generalize and extend the functionality of base models, it is important to investigate their potential impact on hallucinations\nThis paper provides a comprehensive empirical evaluation of various black-box approaches, such as prompt engineering and the use of LLM agents, on the hallucination rates of different NLP tasks. This focus is motivated by the fact that many state-of-the-art LLMs are proprietary, and users typically do not have access to the internal workings of these models. Furthermore, hallucinations are highly context-dependent. For example, they could yield the generation of novel ideas in creative writing contexts. Therefore, this paper investigates techniques that are applicable across different contexts, regardless of the implementation details of the underlying model. The goal of this study is to offer valuable insights into the effectiveness and applicability of prompt engineering and different agent architectures, so that it may pave the way for more secure and reliable LLM applications across a variety of domains."}, {"title": "2 Background", "content": "This section provides a summary of the concepts discussed in this paper. It gives a brief overview of LLMs, hallucinations, prompting techniques and LLM agents."}, {"title": "2.1 Large Language Models", "content": "The most powerful and capable LLMs to date are transformer-based models, which are a specific type of neural network introduced by Google in 2017 [15]. LLMs are based on layers of artificial neurons that process sequences of tokenized text such as words, sub-words, or individual characters. The transformer architecture is a specific type of neural network that utilizes an \"attention\" mechanism, which enables the model to selectively focus on different parts of the input data. This mechanism is particularly well-suited at capturing long-range dependencies and complex patterns in language data. Through an extensive training process, these models adjust the weights and biases between neurons to predict the most probable tokens following a specific input sequence, which yields the production of coherent and human-like text. Increasing the number of parameters of the model, enables the model to capture more intricate language patterns. However, this increase is subject to diminishing returns, data quality and other factors.\nRunning LLMs locally is computationally expensive, since these models have to process vast amounts of data in real time by performing large-scale matrix multiplications. Significant amounts of random access memory (RAM) and video random access memory (VRAM) are required, with graphics processing units (GPUs) often recommended for their efficiency in handling parallel operations like matrix multiplications. For example, running an 8 billion parameter model would generally require a GPU with at least 8GB of VRAM to load the entire model onto the GPU. Although it is possible to run LLMs without a GPU, it requires a significant amount of RAM and generally results in slower inference times. Larger models, such as those exceeding 70 billion parameters, may require over 64GB of RAM or 40GB of VRAM to yield reasonable inference times.\nLLM quantization is a model compression technique used to reduce the size of LLMs by converting the high-precision weights of the model to lower-precision weights. Model weights are often stored in a high-precision format, such as 32-bit floating point numbers. LLM quantization tries to make the model more energy efficient by casting the weights to a lower precision, such as 8-bit integers, while maintaining the performance of the model. Quantization effectively reduces the memory and storage consumption of a model so that it utilizes less computational resources. The reduced model size results in faster inference times and less energy consumption, making quantization an essential technique to run LLMs on smaller, less powerful devices.\nThe temperature of an LLM is a hyperparameter that balances exploration and exploitation of the output generated by the model. When LLMs generate new tokens, there are often a few candidates to choose from, each with varying probabilities of being selected. The temperature setting adjusts the probability distribution over the candidate tokens. Specifically, when the temperature is low (close to zero), the model exploits patterns it has learned in the training data, making the output more reliable and predictable."}, {"title": "2.2 Hallucination Taxonomy", "content": "The terminology used to describe hallucinations varies significantly across current literature on LLM hallucinations. Many works have divided hallucinations into two categories, intrinsic and extrinsic hallucinations [1, 9, 11]. Intrinsic hallucinations generally refer to any output that has a direct contradiction to some source material, whereas extrinsic hallucinations are defined as any output that includes speculative content which is not based on the provided source material. However, this nomenclature is restricted to tasks that include source material, such as text summarizations or reading comprehension tasks.\nTherefore, Huang et al. [8] proposed a new taxonomy for hallucinations in LLMs that better encapsulates the various types of NLP tasks that can incur hallucinations. Their proposed taxonomy includes two general categories, factuality and faithfulness hallucinations. A factual hallucination is defined as any information generated by an LLM that contradicts or is not supported by real-world knowledge. Factual hallucinations are divided into two subcategories, factual inconsistencies and factual fabrications. Factual inconsistencies are any facts that directly contradict real-world knowledge, whereas factual fabrications are any facts that are not supported, nor contradicted, by real-world knowledge.\nFaithfulness hallucinations are LLM responses that do not align with prompt instructions or any additional context. Faithfulness hallucinations can be divided into three categories: instruction inconsistencies, context inconsistencies, and logical inconsistencies. Instruction inconsistencies are any responses that do not align with the prompt instructions. Contextual inconsistencies contradict any additional context provided in the prompt, and logical inconsistencies are when the model contradicts itself within the same response."}, {"title": "2.3 Prompt Engineering", "content": "Prompt engineering is the art of constructing LLM prompts that yield the most relevant and correct responses. Chain-of-Thought (CoT) prompting is a strategy introduced by Wei et al. [18] where the LLM has to elicit explicit reasoning for its response. This technique is particularly powerful for reasoning tasks such as mathematical problem-solving. It improves the reasoning capability of the LLM by breaking the task into smaller steps to be solved.\nSelf-Consistency (SC) is a technique proposed by Wang et al. [17] that performs a majority vote based on several repeated LLM calls. LLM models are often encouraged to perform greedy decoding by biasing the LLMs output to the safest and most predictable response. This is achieved by adjusting settings such as the temperature value of the LLM. The SC approach aims to balance creativity with accuracy by sampling from diverse LLM responses and performing a majority vote over the sampled answers."}, {"title": "2.4 Frameworks to Mitigate Hallucinations", "content": "Many frameworks have been proposed to mitigate hallucinations in LLMs. M\u00fcndler et al. [14] proposed a framework, known as Chat Protect (CP), to reduce hallucinations based on contradictory responses. Dowden [4] stated that given any two contradictory responses that describe the same subject, at least one of the claims are guaranteed to be false. This forms the basis of the CP approach, which entails a three-stage pipeline to invoke, detect and remove contradictory claims from LLM responses. The approach uses an analysing LLM to detect and remove false claims by a generating LLM. During the invocation stage, the algorithm extracts contexts from each sentence in the generating LLMs response. Then, the generating LLM is queried based on the restricted contexts to yield a new response for each independent context. Finally, the analysing LLM compares each set of responses to detect and remove contradictory statements from the output of the generating LLM.\nFurthermore, Guan et al. [6] suggested an approach to mitigate factual hallucinations by grounding an LLM with information from an external knowledge graph (KG). A KG is a structured representation of real-world entities and their relationships. Nodes in the graph, called entities, represents real-world objects or concepts such as people, places or items. Related entities are connected by edges in the graph. Each edge in the graph contains information about the relationship between the two connected entities. Therefore, KGs store information about the world in a format that makes it simple and efficient to query general facts about the world. The proposed approach, called Knowledge Graph-based Retrofitting (KGR), enables autonomous KG retrieval by using an LLM to extract entities from an initial draft response and searching the KG for properties chosen by the LLM. The information from the KG is then added as additional context so that the LLM can refine its initial response. This enables the LLM to ground its final response in external knowledge from the KG, in order to decrease the number of factual hallucinations.\nInspired by the work of Minsky's The Society of Mind [12], Du et al. [5] put forward a framework to reduce hallucinations based on the concept of interaction between cognitive components. The Multiagent Debate (MAD) framework is based on a form of collective intelligence where multiple LLMs work together to procure a response. The idea is that contradictions, and therefore hallucinations, can be reduced by having multiple LLMs with diverse responses debate about their reasoning and obtain a convergent solution. The solution that the LLMs converge to is more likely to contain factual information, according to Minsky [12]. This approach follows a three-step process. First, each LLM is prompted to generate an independent response. Secondly, the debate is initiated by prompting each LLM to revise their response given the responses of the other LLMs. This step is repeated for a fixed number of iterations. Finally, an LLM is prompted to combine the final responses from each LLM to produce a single response.\nFinally, Dhuliawala et al. [3] introduced Chain-of-Verification (CoVe), a four-step process to reduce hallucinations using a set of verification questions. First, the LLM generates an initial response. Secondly, it generates verification questions, based on the query and the initial response, that can be used to verify key facts in the base response. The verification questions are then answered independently and evaluated by the LLM to identify contradictions between the independent answers and the base response. Finally, the LLM removes contradicting claims from the original response by taking into account the independent answers from the verification questions. Three validation methods are proposed: the joint method, which combines question generation and answering in one query, the 2-Step approach which separates these into two independent queries, and the factored strategy where each question is answered with an independent query. The factored approach is the most computationally expensive approach, but has the lowest likelihood of carrying over hallucinations from the base response."}, {"title": "2.5 Agents", "content": "Agentic systems are LLM-based applications where the control flow is determined by an LLM. In agent-based systems, the agent architecture governs the interaction between LLMs, external systems, and the control flow of the system 1. Different architectures yield varying degrees of control by the LLM. The simplest is a chain architecture, where tasks are solved sequentially with a pre-determined sequence of LLM calls. Router architectures offer a more dynamic system, where the LLM governs the flow of the system by selecting from a set of pre-defined chains. A more sophisticated architecture is that of general tool-calling agents, where the LLM is responsible for multistep decision-making and tool calls. Reasoning and Acting (ReAct) [20] is a popular general-purpose architecture that interleaves reasoning with task-specific actions and incorporates the following three modules:\n\u2022 Tools: External tools available to the model.\n\u2022 Memory: Retain information from prior steps.\n\u2022 Planning: Dictate the steps taken to accomplish a task.\nTools enable sufficiently trained LLMs to access external systems for tasks such as arbitrary code execution, looking up information online or executing specialized actions. Tool binding involves giving a model awareness of the tools it has available to it and"}, {"title": "3 Methodology and Experimental Design", "content": "This section details the LLM-based system, including the models and libraries used, the implementation of the various prompting strategies and agents, and the process for evaluating and comparing these approaches on different benchmarks."}, {"title": "3.1 Implementation", "content": "The different prompting strategies, frameworks, and agents were implemented using Python, LangChain, and Ollama. LangChain is an open-source Python framework designed to build LLM-powered applications. LangChain offers an extensive range of components to design, develop, and integrate existing LLMs into Python applications, which made it well-suited for implementing and testing the different prompting strategies and agents. Ollama is an open-source software platform to run LLMs on a local machine. LangChain provides functions for interacting with models running on Ollama. All the models were hosted locally on an Nvidia Geforce RTX 2080 GPU with 8GB of dedicated VRAM.\nThe prompting strategies and frameworks were tested using the Meta-Llama-3-8B-Instruct-Q6_K model, which has 8 billion parameters and 6-bit quantization. The model was tested with temperature values of 0.2, 0.5, and 0.8 to discern the impact of different values on mitigating hallucinations with the various prompting strategies. Furthermore, since the Meta-Llama-3-8B-Instruct-Q6_K model does not support tool use, the agent architectures were implemented using the Meta-Llama-3.1-8B model with 8 billion parameters and no quantization. Owing to time constraints, the agents were all tested with a temperature value of 0.5."}, {"title": "3.2 Prompting Techniques", "content": "The CoT strategy 2 was implemented for reasoning-based NLP tasks by parsing both the final answer of the model and the sequence of steps used to solve the problem. This approach required the model to explicitly include and format its reasoning process, thereby dividing the problem into smaller, more manageable parts. However, LLMs may not always adhere to the specific output instructions, which can result in parsing errors. This was mitigated by allowing multiple attempts, up to a specified tolerance number. Invalid responses were discarded since they could not be parsed into a structured format. If the number of attempts exceeded the tolerance threshold, an error was raised and the query was invalid. The ToT approach involved sampling from the CoT prompt multiple times. Next, the LLM was prompted to select the most accurate solution based on the parsed reasoning steps, with the final answer being obtained from the best-voted sample. A control strategy was implemented to compare"}, {"title": "3.3 Agent Architectures", "content": "To investigate the effect of agents on hallucination rates in different NLP tasks, two agent architectures were implemented and compared with a control agent. The first agent used a simple chain architecture that consisted of two LLM queries. The first query generated a list of tool calls, and the second used the outputs from these tools to provide a final answer. The LangChain bind_tools function was used to integrate tools with the model. The second agent utilized a general-purpose ReAct architecture. This was achieved by using the create_tool_calling_agent and AgentExecutor functions from LangChain. Each agent was equipped with a combination of three tools: Wikipedia, DuckDuckGo, and Riza, a Python interpreter. The Wikipedia tool handled queries about people, places, or items, while DuckDuckGo enabled general internet searches for up-to-date information. Riza allowed for the execution of arbitrary Python code in a secure sandbox environment, to avoid potential issues from agents generating dangerous or non-terminating code. Additionally, a third ReAct agent, ReAct-DDG, was introduced, limited to the DuckDuckGo search tool."}, {"title": "3.4 Benchmarks", "content": "The following benchmarks were used to evaluate each of the algorithms implemented. First, is the Grade School Math 8K (GSM8K) benchmark [2], which contains a collection of mathematical word problems that require a sequence of logical steps to solve. The test set contains 1319 questions, each with a single numerical solution. This benchmark evaluated the extent to which each strategy could reduce logical hallucinations. The prompting strategies that were tested on this benchmark were the CoT, SC, SC-COT, TOT and MAD strategies, which are all aimed at improving reasoning capabilities. Furthermore, owing to time constraints, the agent architectures were only evaluated on the first 1000 questions.\nSecondly, was the TriviaQA benchmark [10], which consists of reading comprehension and high quality trivia questions. The TriviaQA dataset was used to determine each strategy's ability to mitigate factual inconsistencies. In particular, each selected strategy was assessed on the first 1000 trivia questions from the validation set. The SC, CP, KGR, CoVe-1, MAD and DDGA strategies, that all aim to mitigate factual inconsistencies, were evaluated on the TriviaQA benchmark. The ReAct-DDG agent was only evaluated on the TriviaQA benchmark to discern the impact of having less tools compared to the ReAct and chain architectures on this benchmark.\nThe final benchmark was the Massive Multitask Language Understanding (MMLU) dataset, which consists of general-knowledge multiple choice questions, spanning 57 different subjects. The selected strategies were assessed on 1000 questions in total, that included approximately 17 questions per subject. The strategies applied to this benchmark set were the SC, CP, MAD, reflection and CoVe-2 strategies, as well as the chain and ReAct agents."}, {"title": "3.5 Evaluation Metrics", "content": "For each benchmark, the number of correct answers, the number of hallucinated answers and the accuracy was computed over a number of independent runs. Due to time constraints and limited computational resources, each strategy was run three times per benchmark. This was done since the output of an LLM is stochastic, which makes it important to obtain an indication of average performance. The Top-N accuracy was used to investigate the influence of temperature on strategies with repeated sampling. This metric indicates the percentage of times that the correct answer appeared in N sampled answers. The performance of the base LLM was evaluated according to each metric as a control method. The results were tabulated to determine which methods yield the greatest reduction in hallucinations. Additionally, the table includes the average number of prompts per strategy to indicate the average computational cost for each method. Owing to the limited number of runs, no statistical tests were conducted since the power of the statistical results would be very low."}, {"title": "4 Results", "content": "This section presents the results of each algorithm on the benchmark datasets. The first three parts evaluate the performance of prompting techniques over the different benchmarks, and the final part of this section discusses and analyses the results of the agents."}, {"title": "4.1 GSM8K Results", "content": "Table 1 indicates the means of the CoT, SC, SC-COT, TOT and MAD strategies for different temperature values on the GSM8K dataset [2] over the independent runs. The best value for each performance metric is given in bold. It is evident from the results that the SC strategy, with a temperature value of 0.8, had the best overall performance on the benchmark by achieving the highest accuracy and least number of hallucinated answers on average. The SC strategy achieved the best balance between accuracy and creativity amongst all the strategies on the GSM8K benchmark.\nTable 1 shows that the SC and SC-CoT strategies performed relatively similar on average, and both outperformed the control strategy. The repeated sampling enabled these strategies to elicit different ways of solving each mathematical problem. Higher temperature values yielded more diverse and creative answers, which increased the risk of hallucinations and inaccurate responses. This is evident by the fact that the performance of the control method deteriorated with an increase in temperature. Therefore, the repeated sampling of the SC and SC-CoT approaches was able to counteract hallucinations by selecting the answer that appeared the most frequently. Since mathematics requires a certain degree of creativity as well as accurate reasoning, these two strategies struck an excellent balance between creative problem-solving and accurate reasoning.\nFigure 1 shows the average frequencies of how many times the correct answer appeared in the five sampled responses over the GSM8K benchmark for the SC and SC-CoT strategies. It is clear that the lower temperatures exhibited more consistent results. This is indicated by the fact that for both SC and SC-CoT, a temperature value of 0.2 yielded high frequencies for having all five samples be correct and very low occurrences for only having one to four correctly sampled responses. On the contrary, the higher temperatures yielded more diverse responses, since the average frequency of correct occurrences is distributed more across the one to four range."}, {"title": "4.2 TriviaQA Results", "content": "Table 2 depicts the average performance of the KGR, CoVe-1, MAD, SC, CP and DDGA strategies against the control method over the TriviaQA benchmark [10]. This table shows that the CP strategy obtained the highest accuracy over all the strategies by greatly sacrificing the number of questions answered. Since the CP approach refrained from answering questions where the sampled responses contained contradictory answers, it greatly reduced the number of hallucinations. As the temperature increased, the number of hallucinations decreased and the accuracy increased. Again, this is because the higher temperature values yielded more diverse responses and consequently more contradictory responses. Therefore, the temperature value dictated a trade-off between the number of hallucinations and the number of questions that the CP approach answered.\nSimilarly to CP, the CoVe-1 approach also refrained from answering questions where the algorithm detected contradictions in the model. These contradictions provide an indication of the model's confidence in answering a specific question. If the model never contradicts itself, it is a good indication that the training of the model enabled it to answer the corresponding question. If the model cannot give consistent output on a query, there is a good probability that a hallucination has occurred. Despite the CoVe-1"}, {"title": "4.3 MMLU Results", "content": "Table 3 shows the average performance of the SC, CoVe-2, CP, MAD and reflection strategies on the MMLU [7] benchmark. The highest average accuracy was achieved by CoVe-2 with a temperature value of 0.2. Similarly to CoVe-1 from TriviaQA, the CoVe-2 strategy achieved a significantly higher accuracy at the cost of reducing the number of questions it could answer correctly, even when the control strategy yielded much more correct answers. However, the CoVe-2 approach resulted in the fewest number of hallucinations.\nFurthermore, the MAD strategy achieved the highest number of correct answers on average. One possibility for this is that since the MMLU dataset consists of a wide variety of subjects and questions, some reasoning based and some knowledge-based, the agents were able to debate about the various multiple choice options, which led to the highest number of correct answers on average.\nSimilarly to the CP approach on the TriviaQA benchmark, the temperature parameter dictated a trade-off between the number of questions answered, and the accuracy achieved by the CP strategy on the MMLU dataset. However, the CoVe-2 approach, with a temperature value of 0.2, answered fewer questions than the CP approach with a temperature value of 0.8. This suggests that the CoVe-2 approach is very limited in the number of questions that it will answer. Furthermore, adjusting the temperature value of CoVe-2 did not yield any significantly different results, showing that this technique has much less versatility than the CP strategy. Despite the benchmark including a mix of trivia-based and reasoning-based questions, the SC strategy did not yield any significant improvements over the control method.\nThe plot for the average frequency of correctly sampled responses and the Top-1 to Top-5 accuracy for the SC strategy on the MMLU benchmark closely resembled the patterns seen in Figures 3 and 4, respectively. Consequently, these figures were omitted for brevity. The MMLU benchmark exhibited more characteristics with the TriviaQA benchmark than the GSM8K benchmark, as the SC samples were generally all correct or all incorrect. The poor performance of the SC strategy on both the MMLU and TriviaQA benchmarks, compared to the strong performance of the SC strategy on the GSM8K benchmark, suggests that the success of the SC strategy is highly dependent on the type of NLP task. Therefore,"}, {"title": "4.4 Agent Results", "content": "Table 4 shows the average performance of the different agent architectures on each of the corresponding benchmarks. It is evident that the control strategy performed the best on every benchmark. Both the ReAct agent and the LLM chain exhibited more hallucinated answers and fewer correct answers compared to the control agent on every benchmark. Furthermore, the performance of the ReAct agent and the chain architecture was relatively similar for the TriviaQA and MMLU benchmarks, while the chain agent slightly outperformed the ReAct agent on the GSM8K benchmark.\nFigure 6 depicts the control flow of the ReAct agent for a question from the TriviaQA dataset. The diagram shows that the model attempted to use the Python code interpreter to invoke the Wikipedia"}, {"title": "5 Conclusion", "content": "This paper has investigated the performance and hallucination rates of various prompting techniques and frameworks on a diverse set of benchmark datasets. Additionally, the paper has examined the effect of augmenting LLMs with external tools on the rate of hallucinations. All the approaches were implemented and compared against a control strategy on the GSM8K [2], TriviaQA [10] and MMLU [7] benchmarks.\nThe results showed that the best prompting strategy to employ is based on the characteristics of the NLP task. It was found that the most effective way to reduce hallucinations for mathematical-based problems is to employ an SC strategy, which involves taking a majority vote over a number of sampled responses. Additionally, it was shown that the CP strategy, which refrains from answering whenever two or more samples contradict each other, achieved a good trade-off between the number of questions answered and the number of hallucinated answers. Finally, it was evidenced that even though a model may have been trained to use external tools, they can significantly increase the number of hallucinations if the model is not powerful enough.\nFuture work should investigate the performance of combining different prompting strategies, such as combining the MAD and SC approaches or the CP and DDGA strategies. Finally, more work needs to be done on assessing the rate of hallucinations of more powerful state-of-the-art LLMs when augmenting them with external tools."}]}