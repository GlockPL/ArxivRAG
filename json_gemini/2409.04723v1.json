{"title": "NapTune: Efficient Model Tuning for Mood Classification using Previous Night's Sleep Measures along with Wearable Time-series", "authors": ["Debaditya Shome", "Nasim Montazeri Ghahjaverestan", "Ali Etemad"], "abstract": "Sleep is known to be a key factor in emotional regulation and over- all mental health. In this study, we explore the integration of sleep measures from the previous night into wearable-based mood recog- nition. To this end, we propose NapTune, a novel prompt-tuning framework that utilizes sleep-related measures as additional inputs to a frozen pre-trained wearable time-series encoder by adding and training lightweight prompt parameters to each Transformer layer. Through rigorous empirical evaluation, we demonstrate that the in- clusion of sleep data using NapTune not only improves mood recog- nition performance across different wearable time-series namely ECG, PPG, and EDA, but also makes it more sample-efficient. Our method demonstrates significant improvements over the best base- lines and unimodal variants. Furthermore, we analyze the impact of adding sleep-related measures on recognizing different moods as well as the influence of individual sleep-related measures.", "sections": [{"title": "1 INTRODUCTION", "content": "Affect recognition is a pivotal area in human-computer interaction, enabling a range of real-world applications [5] ranging from health- care to user experience. Specifically, in healthcare, it offers valuable insights into mental health treatments and stress management [50]. In education, it can assess students' engagement and stress levels, tailoring learning experiences accordingly [25]. In the workplace, affect recognition can enhance employee well-being by identifying stress or dissatisfaction [36]. And finally, for user experience, affec- tive computing paves the way for more intuitive and responsive products, from smart homes that adjust environments based on the occupants' moods to entertainment systems that adapt to user reactions [27]. Wearable devices have been widely adopted for affect recogni- tion due to their ability to continuously and unobtrusively monitor physiological signals [45]. In particular, wearable signals such as electrocardiography (ECG) [17, 41], photoplethysmography (PPG) [31], and Electrodermal Activity (EDA) [46] have been the focus of numerous studies within this field. ECG measures the heart's elec- trical activity and can therefore capture changes in the emotional state of users. PPG detects blood volume changes throughout the body, offering insight into the physiological responses to emotions. EDA measures the electrical conductance of the skin, which dynam- ically fluctuates with sweat gland activity which is influenced by sympathetic nervous system activations. With advances in machine learning, these signals have been used to develop models capable of accurately recognizing affective states in real-time [22]. Mood and emotion are two common affect categories that are sometimes used interchangeably. In contrast to emotion, mood is an affective category that lasts for a long time, even up to several hours or days, and is thus harder to recognize from real-time data [53]. While real-time wearable signals provide valuable information about a subject's affect state, they sometimes do not encapsulate historical information or external factors that may have a strong impact on mood. In particular, sleep-related measures stand out as such a salient external factor known to influence mood [42, 51]. These measures may include 'time in bed', 'sleep duration', 'deep sleep duration', 'light sleep duration', 'REM sleep duration', 'wake time', 'deep sleep onset', 'sleep efficiency', and 'Apnea index'. As a critical component of daily life, sleep impacts emotional regulation and mental health, thereby shaping subsequent affective states. The relationship between sleep patterns and mood dynamics is well- studied in the literature [58, 24, 29], with sleep disturbances known to aggravate stress and irritability [56]. Conversely, a pattern of restorative sleep is often correlated with enhanced mood stability and cognitive functioning [32]. A thorough investigation of the literature in this area demon- strates that despite the relationship between sleep measures and mood being well-established in prior works, previous night's sleep measures have not been incorporated into automated mood recog- nition models as a complementary modality to wearable signals. Moreover, what makes this issue even more challenging is the scarcity of public datasets that contain the previous night's sleep measures paired with wearable time-series from the same users over a period of time. This in turn inhibits the use of standard training frameworks from being effective in enabling a multimodal setup for both sleep and wearable time-series simultaneously. In this paper, to address the challenges above, we propose Nap- Tune, a framework that integrates previous night's sleep measures as a complementary source of information into mood recognition from various wearable time-series (ECG, PPG, and EDA). First, our model uses an unimodal Transformer-based encoder to extract effective representations from the wearable time-series. Next, to enable our model to incorporate and analyze the previous night's sleep-related measures as an additional input, we freeze the Trans- former encoder and add a small number of learnable parameters into each layer, which we then train for mood recognition. This enables our model to effectively utilize sleep-related measures alongside wearable time-series for mood recognition, with little requirement for paired training data. Extensive experiments demonstrate that incorporating previous night's sleep-related measures leads to an increase in mood recognition performance with up to 8% in F1 score. In summary, we make the following contributions. (1) We propose NapTune, an efficient tuning framework for adapt- ing a frozen wearable time-series encoder to utilize the previous night's sleep-related measures as an additional input modality. Our framework demonstrates strong performances for mood classifi- cation based on sleep-related measures and wearable time-series, outperforming various multimodal baselines. (2) For training, our framework requires minimal data in the form of wearable time-series paired with previous night's sleep measures from the same users, overcoming the problem of insufficient paired training data. (3) We study the effect of the previous night's sleep-related mea- sures in aiding mood recognition from wearable time-series. Par- ticularly, we train and evaluate models for mood recognition with ECG, PPG, and EDA, with and without the use of sleep measures. Our findings show an increase of 9% to 11% in F1 when utilizing the sleep-related measures as an additional modality."}, {"title": "2 RELATED WORK", "content": "In this section, we review three key areas relevant to our work. First, we review affect recognition studies from wearable time-series. Next, we present a review of prior works focusing on the relationship between sleep and affect. Finally, we conclude this section with a thorough review of recent works on prompt-tuning, a technique that is central to our proposed framework."}, {"title": "2.1 Wearable affect recognition", "content": "Several recent studies have demonstrated the potential of wearable time-series for affect recognition. In [60], a framework for person- alized and generalized mood recognition in a workplace setting was proposed using smartphone-based wearable sensors including ECG, PPG, 3-axis acceleration, and skin temperature. Several ML algorithms were evaluated to find a bagged ensemble of decision trees as the best performing solution. In [63], a novel pipeline using popular wearables was developed to recognize daily activities and a regression model was trained for mood assessment. The method was tested in a real-world study with 18 users over 93 user-days and achieved mood inference with a mean absolute error of 0.24 \u03c0 radians on the Circumplex Model of Affect. In [43], a self-supervised learning method was introduced for emotion recognition using ECG time-series, that achieves comparable or superior results to fully-supervised methods on the SWELL [28] and AMIGOS [37] datasets. In [12], a study was conducted on combining smartphone and smartwatch data to assess mood. The results demonstrated the significance of temporal features and heart rate obtained from such wearable time-series. In [62], an emotion recognition system was developed using spectrogram representations of EDA time- series with a combination of CNN and Bi-GRU for feature learning and classification. The model was evaluated on the AMIGOS [37] dataset demonstrating high classification accuracies of 83.4% for arousal and 81.2% for valence."}, {"title": "2.2 Sleep and affect", "content": "Although the aim of this paper is to study the impact of sleep as an additional modality to improve mood recognition, in this subsection, we review the prior works that study the overall link between sleep and affect, i.e., how sleep impacts affect and vice- versa. In [51], a conceptual review was conducted to explore the relationship between sleep and affect. A granular framework was proposed to deconstruct sleep and affect into three dimensions: domains, methods, and timescales. This framework was applied to systematically review empirical studies from PubMed, focusing on associations between various aspects of sleep and affect. The study found evidence of links between sleep disturbances, sleep duration, and affect, but noted that evidence was often inconclusive or sparse for other aspects. In [29], the reciprocal relationships between daily sleep and mood were examined, focusing on real- world studies. Electronic databases were utilized to collect studies that investigated daily associations between sleep and mood using ambulatory diary techniques. The findings of this study supported a reciprocal relationship between subjective sleep variables (quality, duration, latency) and daytime affective states, emphasizing the potential clinical importance of sleep disturbance in predicting and preventing psychopathology. In [24], a 2-week study was conducted to explore the interplay be- tween daily affect and sleep in young women. Using daily sampling, they investigated how variations in positive and negative affect influenced self-reported sleep-onset latency, sleep duration, and sleep quality. The study revealed significant associations between sleep and emotions, where sadness and serenity were found to be strong predictors of sleep-related measures, and better sleep quality predicted greater happiness the next day. In [8], the interplay of neuroticism, affect, and hyperarousal with perceived sleep quality was explored. Using an online survey, the impact of these factors on sleep quality was assessed among 498 Italian participants. The study found that neuroticism was the primary personality predic- tor of poor sleep quality. Additionally, hyperarousal and positive affect also significantly predicted good sleep quality. In [30], a sys- tematic review and meta-analysis was conducted to evaluate the causal impact of emotions on sleep. 31 experimental studies focus- ing on the effects of emotion inductions on various sleep-related measures were analyzed. The study found a moderately significant effect of emotion inductions on delayed sleep onset latency, but no consistent impacts on other sleep-related measures."}, {"title": "2.3 Prompt tuning", "content": "Prompt tuning was first introduced as a method for adapting large pre-trained language models to specific tasks using soft prompts learned through backpropagation [33]. This technique allows a large frozen model to be used across multiple tasks without sig- nificant computational overhead. Inspired by the success of this approach in large language models, prompt tuning has since been further adopted for other domians including vision and time-series. In [21], Visual Prompt Tuning (VPT) was introduced as an effi- cient method for adapting large-scale Transformer models in vision tasks. VPT incorporates a small amount of trainable parameters into the input space while keeping the model backbone frozen, leveraging the advantages of large pre-trained models without the need for extensive retraining. Similarly, prompt-tuning was recently adopted for several time-series tasks demonstrating promising re- sults. In [6], a novel framework named TEMPO was proposed as a prompt-based generative pre-trained Transformer for time-series forecasting. TEMPO addresses the challenges in time-series fore- casting by integrating decomposition of complex interactions be- tween trend, seasonal, and residual components, and by introducing selection-based prompts to adapt to non-stationary time-series. The framework demonstrated superior performance over state-of-the- art methods in various benchmark datasets. In [59], PromptCast, a novel approach to time-series forecasting utilizing a prompt- based paradigm, was introduced. It transforms numerical data into prompts, leveraging pre-trained language models for forecasting. In [34], PT-Tuning was introduced as an approach that enhances time-series forecasting by bridging the gap between masked recon- struction and forecasting through prompt token tuning. The unique aspect of PT-Tuning is its integration of a few trainable prompt tokens, while all other pre-trained parameters remain frozen. This effectively addresses the issue of task difficulty variation. The ap- proach demonstrated remarkable performance improvements over other methods in extensive experiments with real-world datasets. In [55], a method called POND was introduced for multi-source time-series domain adaptation using prompt tuning. This method aims to tackle three challenges: the exploitation of domain-specific infor- mation for domain adaptation, the capturing of dynamic domain- specific information, and the assessment of learned domain-specific data. POND uses prompts to capture both common and specific information across domains, introduces a conditional module for generating dynamic prompts for each source domain, and employs specific criteria for effective prompt selection. In [9], SpeechPrompt v2 was introduced, which is a prompt tuning framework for various speech classification tasks. This approach uses generative spoken language models with trainable prompt vectors, allowing efficient adaptation to different languages and tasks with minimal parameter updates. The framework introduces a learnable verbalizer, enhanc- ing its adaptability and performance across a wide range of speech classification tasks, including emotion recognition and language identification, while maintaining computational efficiency."}, {"title": "3 METHOD", "content": "We aim to develop a framework to enable a pre-trained wearable time-series encoder to utilize the users' previous night's sleep- related measures as an auxiliary input to enhance mood recognition given a small training dataset of paired data. Inspired by recent advances in prompt tuning in large language foundation models, we propose an efficient tuning solution to address this problem. Our method includes two stages. First, we train a Transformer-based wearable time-series encoder using self-supervised pre-training over a large corpus of unlabeled unimodal wearable time-series. Next, we freeze the weights of this pre-trained encoder and add a linear projection to include sleep-related measures, and additional learnable parameters in each Transformer layer for efficient train- ing without a high computational overhead. Figure 2 depicts an overview of our approach."}, {"title": "3.2 Proposed approach", "content": "Inspired by [2] we adopt a CNN-Transformer model as an encoder to learn the wearable time- series. As shown in the figure, the encoder consists of a convo- lutional projection module with a series of a convolution blocks. The initial convolutional layer uses a kernel size of k\u0131 (long kernel) and a stride of s\u0131 (long stride). This layer is specifically designed to better capture the global patterns in the input time-series. The output from this layer undergoes normalization using GroupNorm, followed by GELU activation. The subsequent convolutional layers follow a similar pattern where each of these layers use smaller kernels of size ks (short kernel) and a stride of ss (short stride), which focus on extracting more detailed local features from the input time-series. We apply LayerNorm to the extracted convolu- tional features and apply a GELU-activated linear layer to project the output of this convolutional projection module into a L \u00d7 D di- mensional time-series embedding \\(Z_{time-series}\\). Next, we represent \\(Z_{time-series}\\) as a set of L tokens each with a dimension D, as\n\n\n\nNext, in order to preserve sequential information we use a Posi- tional Encoder. Given a time index pos and dimension index i, the Positional Encoder generates a positional embedding II defined as\n\n\nLastly, the model consists of a Transformer encoder, which we refer to as f, that consists of N layers with an embedding dimension of D. Each layer consists of a multi-head attention module, Layer- Norm, and GELU-activated feedforward layers, as shown in Figure 3. Each multi-head attention module performs scaled-dot product attention in parallel over multiple attention heads by utilizing the layer's input representation as query, key, and value. Specifically, the scaled-dot product attention operation involves calculating the dot product between scaled query (Q) and key (K) matrices, which is multiplied with a pre-defined mask of attention weights. Then, the softmax operation is applied to derive the attention weights and subsequently multiplied with the value matrix (V) to obtain the final attention output represented as\n\n\nwhere, d represents the dimension of K, Q, and V, which is used as a scaling factor to prevent vanishing gradients.\nTo pre-train the wearable time-series encoder f to learn strong representations from unlabeled time-series, we pre-train this module using SimCLR [11]. SimCLR is a widely used contrastive self-supervised learning framework [11, 49]. Given each input time-series x, we create two views \\(x_{1}\\) and \\(x_{2}\\) using two distinct stochastic augmentations \\(t_{1}\\) and \\(t_{2}\\). First, the encoder backbone extracts embedding h from the augmented views of each time-series. Next, a projector network g with a single linear layer projects the embedding into z = g(h). In a minibatch of M samples resulting in 2M pairs, we obtain one positive pair (p, q), and the rest 2(M \u2013 1) as negative pairs. We use the contrastive loss function [48], which can be defined as\n\n\nwhere sim indicates pairwise dot product similarity and t is the temperature parameter. Both \\(t_{1}\\) and \\(t_{2}\\) are sampled from the same family of augmentations which are suitable for temporal time-series, as described below.\nIn this augmentation, we split the time-series x into r segments x = [\\(x_{1}\\), ..., \\(x_{r}\\)]. We randomly select segments and apply 1D interpolation-based time-warping to stretch them by a factor of \u03c3%, while squeezing the remaining segments by the same factor. Finally, we concatenate the segments and apply zero padding if the resulting length is an odd number.\nIn this augmentation, gaussian noise is added to the time-series x. The noise standard deviation is randomly selected within a predetermined range, defined by the product of the time-series's standard deviation and specified minimum and maximum signal-to-noise ratios (SNR). Gaussian noise is then generated ac- cording to this noise standard deviation and added to x, resulting in a controlled level of random noise added to the time-series.\nThis augmentation involves altering the ampli- tude of the time-series x. It is achieved by multiplying x with a scaling factor a, where a is a positive value randomly chosen from a predefined range."}, {"title": "3.3 Tuning", "content": "Once the wearable time-series encoder is pre-trained, we freeze the weights of the convolutional projection module and the trans- former encoder f. We add a linear projection module to encode sleep-related measures into a D-dimensional representation \\(Z_{sleep}\\). Using the wearable time-series encoder, we obtain \\(Z_{time-series}\\) using Equation 1. Next, we concatenate \\(z_{sleep}\\) with \\(Z_{time-series}\\) which results in \\(z_{0}\\), a D-dimensional set of (L + 1) as input tokens for f. Lastly, we add learnable parameters into each transformer layer of f which are updated using backpropagation. Here, f has N Transformer encoder layers which can be represented as\n\n\nwhere \\(P_{n}\\) refers to a set of unique learnable parameters at layer n. We train the model using the Binary Cross Entropy (BCE) loss function for our downstream task of multi-label mood classification. Specifically, BCE loss can be defined as\n\n\nwhere, C is the number of classes, and p and \\(\\hat{p}\\) represent the actual mood labels and the predicted logits by our model, respectively."}, {"title": "4 EXPERIMENT SETUP", "content": "Here, we describe the details of the datasets used in this paper. First, we use a collection of datasets for pre-training the wearable time-series, as well as a downstream mood classification dataset which we use for evaluating our proposed method."}, {"title": "4.1.1 Pre-training datasets.", "content": "The following datasets are used for pre-training the wearable time-series encoders. This dataset contains 24 hours of Lead-II ECG and PPG data, sampled at 700 Hz and 64 Hz respectively, from 15 par- ticipants. Annotations in the dataset include stress and various affect states, providing insights into emotional and stress-related physiological responses. We use this dataset to pre-train encoders for ECG, PPG, and EDA. This datasets is part of the larger MIMIC-III waveform collection [23] dataset. This subset includes data from 35 critically ill adults, including 19 with Atrial Fibrillation (AFib). Both ECG and PPG time-series are present, sampled at a frequency of 125 Hz. We use this dataset to pre-train ECG and PPG encoders. This dataset features approximately 5.6 hours of Lead-II ECG and PPG data, recorded at a 300 Hz sampling rate, from 42 subjects under clinical supervision. It extends the range of physiological data to include medically monitored scenarios. We use this dataset to pre-train both ECG and PPG models. Collected from 53 ICU patients, the dataset contains nearly 7 hours of ECG (across multiple leads including Lead II, V, and AVR) and PPG recordings, each sampled at 125 Hz. We use this dataset to pre-train encoders for ECG and PPG. Comprising approximately 35 hours of data, this dataset includes PPG and Lead-II ECG recordings from 15 subjects performing everyday activities. The sampling rates for ECG and PPG are 700 Hz and 64 Hz, respectively. In our study, we use this dataset to pre-train ECG and PPG encoders. This dataset includes EDA signals col- lected using an Empatica E4 device during cognitive tasks involv- ing auditory, gustatory, and olfactory stimulation. It is designed to explore the modulation of cognitive states and its effects on physi- ological responses. We utilize this to pre-train the EDA encoder."}, {"title": "4.1.2 Downstream Dataset.", "content": "For our experiments on the down- stream task of mood classification, we require a datasets that con- tains both wearable time-series data with mood labels, paired with previous night's sleep-related measures from the same subjects. To our knowledge, ECSMP dataset [14] is the only dataset that con- tains such information. It comprises multiple physiological time- series, including ECG, PPG, and EDA collected from 89 healthy college students. The sampling rates of ECG, PPG, and EDA are 512 Hz, 64 Hz, and 4 Hz respectively. These signals have been col- lected during various states, such as resting, emotional induction and recovery, and cognitive assessments. Additionally, the dataset consists of multiple self-reported questionnaires, among which we utilize the Profile of Mood States (POMS) as annotations for mood classification. It includes seven mood labels represented by scores to categorize tension, anger, fatigue, depression, vigor, confusion, and esteem. We convert the scores to binary labels and formulate this task as a 7-way multi-label classification problem. The dataset also contains sleep-related measures estimated from the ECG collected during the previous night's sleep from each subject. The authors used the cardiopulmonary coupling analysis algorithm [52] to estimate the sleep-related measures from the ECG. These sleep-related measures include: \u2022 Time in bed: This measures the total number of hours that the subject spent in bed. \u2022 Sleep duration: This refers to the total number of hours the subject has slept. \u2022 Deep sleep duration: This refers to the total number of hours that the subject has been in deep sleep, the most restorative stage of sleep. \u2022 Light sleep duration: This refers to the total number of hours that the subject has been in light sleep, the transition period between wakefulness and deep sleep. \u2022 REM sleep duration: This measures the total number of hours in Rapid Eye Movement (REM) sleep, the sleep stage characterized by rapid eye movements, vivid dreaming, and high brain activity, resembling wakefulness. \u2022 Wake time: This measures the total number of hours the subject has spent in bed without sleeping. \u2022 Deep sleep onset: This indicates the number of hours needed by the subject to transition from light sleep to deep sleep. \u2022 Sleep efficiency: This refers to the ratio of sleep duration to the total time spent in bed. \u2022 Apnea index: This refers to the average number of apnea occur- rences per hour of sleep."}, {"title": "4.2 Data pre-processing", "content": "We apply standard pre-processing steps [38, 35] for ECG, PPG, and EDA time-series. The ECG signals are processed using a high-pass Butterworth filter with a 0.5 Hz cut-off frequency. For the PPG time- series, we apply a band-pass Butterworth filter with frequencies ranging from 0.5 to 8 Hz. We skip the filtering of EDA time-series due to the low sampling frequency of 4 Hz. Z-score normalization is applied on each type of time-series to adjust for individual subject- specific differences. After normalization, the time-series are scaled to fit within a [-1, 1] range using min-max scaling. The final step in our pre-processing is segmenting the ECG, PPG, and EDA time-series into 10-second windows."}, {"title": "4.3 Baselines", "content": "Here we describe the models that we use as baselines for compari- son to our proposed method. To our knowledge, no prior work has performed mood classification with the aid of the previous night's sleep measures on this dataset. As a result, we create several base- lines based on state-of-the-art wearable time-series encoders and pre-train each encoder in the same manner that our own model's encoders are pre-trained. To create these baselines, we use an en- coder/projection model for the sleep measures and perform late- stage fusion with the representations of the wearable time-series encoders. For the sleep encoder models, we use a GELU-activated 2- layer linear projection module for the Transformer-based baseline, while we use a ReLU-activated 2-layer linear projection module for others. Finally, for late-stage fusion, we combine the sleep embed- dings with the global-average pooled output embeddings from the wearable time-series encoder and pass the outcome to a sigmoid- activated linear layer to predict mood logits. To ensure that the best possible performances are obtained from these baselines, we opti- mize the hyper-parameters to the best of our ability. Following we describe the architectures and hyper-parameters of these encoders. This model employs a VGG-19 [47] backbone which takes 2D Short-Time Fourier Transform (STFT) representa- tions of the wearable time-series as inputs. The network comprises a series of convolutional layers, initially with 64 filters, doubling at each stage following a max-pooling layer, up to layers with 512 filters. Each convolutional block consists of layers in configurations of two or four, paired with max pooling. Convolutional layers use 3x3 kernels with batch normalization and ReLU activation. This model follows a Convolutional Recurrent neural network (CRNN) architecture that takes wearable time-series converted into logarithmic spectrograms as input. It comprises 3 convolutional blocks, with each applying a set of 5x5 convolutional filters followed by batch normalization and ReLU activation. The channels in these layers increase with each block. A 3-layer bidirec- tional LSTM processes the outputs from the convolutional layers, providing an aggregated feature vector. We adopt ResNet1D [19] as our baseline, an adap- tation of the ResNet model [18] for 1D time-series. The network is comprised of 34 blocks. Each block consists of convolution layers with a kernel size of 64 and a stride of 2, followed by batch normal- ization ReLU activation, and a dropout of 20%. We use 64 as the number of base filters, which doubles at specified intervals across the network. We use the same pre-trained wearable signal encoder based on Transformer-based architecture used for our proposed method as a baseline."}, {"title": "4.4 Evaluation Protocol", "content": "We use a cross-subject 3-fold cross-validation scheme for evaluation. This involves dividing all the subjects into three distinct groups (folds), with each fold being iteratively used as the test set while the remaining two serve as the training set. Due to the inherent imbalance in the mood states represented in the dataset, we report both weighted F1 score and accuracy. To obtain the final results, we generally perform linear evaluation unless otherwise specified. For linear evaluation, we use transfer learning, where we first freeze the weights of the pre-trained wear- able time-series encoders. Then, we train a linear classifier over the learned representations for mood recognition."}, {"title": "4.5 Implementation Details", "content": "We use 4\u00d7NVIDIA A100 GPUs for self-supervised pre-training using a batch size of 2048, with AdamW optimizer and a learning rate of 1\u00d710-4 for 150 epochs. For downstream experiments, we use 2\u00d7NVIDIA A100 GPUs with a batch size of 1024. We use AdamW optimizer with CosineWarmup, starting from a base learning rate of 1\u00d710-5 for 30 epochs. To ensure reproducibility we present all the hyper-parameters used in our work, in Table 1."}, {"title": "5 RESULTS AND DISCUSSION", "content": "First, we compare our proposed method against several baselines using linear evaluation, and present the results in Table 2. We ob- serve that our approach achieves the best performance for all three types of wearable time-series for both metrics (F1 and accuracy) by considerable margins. More specifically, for EDA, our method achieves the best results by 2% improvement in F1 and 3% improve- ment in accuracy over the best baseline [2]. For PPG, our method leads to a significant increase of 12% and 10% in F1 and accuracy respectively over the best baseline. With ECG, we achieve a similar improvement of 11% and 13% in F1 and accuracy, respectively. Out of the three modalities, NapTune performs best with ECG achieving an F1 of 0.76 and an accuracy of 0.71. We believe that the relatively lower performance when using EDA is due to the low sampling rate and the presence of noise. Next, we examine the effect of using sleep-related measures on the performance of our method. To this end, we use the frozen wearable time-series encoder and perform linear evaluation with- out the rest of our method which uses the sleep-related measures. We present the results in Table 3, which shows that using sleep- related measures as a complimentary modality leads to improve- ments across all three wearable time-series. In ECG-based mood recognition, adding sleep-related measures provides a performance boost of 9% in F1 and 7% in accuracy. With PPG, we observe boosts of 9% and 12% in F1 and accuracy respectively. Lastly for EDA, we observe a similar trend where 11% and 10% improvements are obtained for in F1 and accuracy respectively. We then investigate the performance breakdown for different classes of mood, with and without the sleep-related measures, and present the results in Figure 4. Overall, we observe that the addition of sleep boosts performance for the classification of every mood class regardless of the type of wearable time-series used. Next, it can be seen that using EDA (without sleep), tension and anger are classified with the highest F1 scores, whereas the addition of sleep results in the highest boosts for vigor, followed by esteem. On the other hand, when using PPG (without sleep), the highest F1 scores are achieved for fatigue and esteem recognition, while the addition of sleep results in the highest boosts for depression and vigor. Lastly, using ECG (without sleep) leads to the highest F1 scores for vigor and esteem, whereas the highest improvements by adding sleep are achieved for depression and tension. These findings are aligned with our understanding of the physiological connections between sleep and depression, as documented in prior research [7, 61]. These studies have demonstrated that the quality of sleep from the previous night can significantly impact the onset of depression. Furthermore, we analyze the impact of different training meth- ods on mood recognition performance in Table 4, specifically full- finetuning, pre-training, and NapTune. For full-finetuning, we up- date all the weights of the pre-trained encoder with sleep-related measures paired with wearable signals as input, without adding any prompt parameters. We observe an improvement of 3% to 4% in the case of finetuning the ECG and PPG-based models across both F1 and accuracy, while an increase of 8% and 9% respectively in the case of the EDA-based model. Our intuition suggests that the pre-training of EDA is not as strong as PPG and ECG counterparts, particularly due to a lesser amount of pre-training data. This in turn results in such an improvement in performance with finetuning compared to NapTune which uses Linear protocol. Upon training the model from scratch without any pre-trained weights, we ob- serve a similar trend for all the modalities, with a performance drop ranging between 2% to 4% in both metrics. Next, we study the contribution of individual sleep-related mea- sures on mood recognition performance. Specifically, we ablate each sleep-related measure through masking at test time, and eval- uate the model's performance. As shown in Figure 5, REM sleep duration has the highest impact on mood recognition, followed by deep sleep and sleep duration. These findings are well-aligned with our understanding of how REM and deep sleep influence emo- tion regulation. Several studies [15, 20, 57, 54, 16] have highlighted that during REM sleep the brain strengthens important emotional memories, which influences emotional regulation after waking up. Moreover, deep sleep has been highlighted as primarily respon- sible for consolidating and stabilizing emotional memories [4, 3]. Accordingly, both factors are highly influential in regulation of emotions and mood, which conforms with our observations from this experiment. Lastly, we analyze the trend of mood recognition performance based on the amount of training data used for all the methods. As shown in Figure 6, our proposed NapTune shows a much less steep decrease in performance when reducing training data, with up to a 62% F1 score when we use only 25% of the training data. In contrast, the other baselines show a much steeper descent, especially when reducing the training data from 50% to 25%."}, {"title": "6 CONCLUSION", "content": "We introduced NapTune, a prompt-tuning framework that involves adapting a frozen Transformer encoder, by adding lightweight prompt parameters into each transformer layer, to efficiently uti- lize the previous night's sleep-related measures as complemen- tary information alongside wearable time-series data. Our results demonstrate that incorporating sleep data significantly improves mood classification, achieving up to an 11% increase in F1 scores over baseline methods that only use wearable data. Our proposed NapTune framework outperforms several state-of-the-art multi- modal baselines by a minimum margin of 11%, 12%, and 2% for ECG, PPG, and EDA respectively. Future research directions may include studying other approaches to using pre-trained encoders, such as adaptors. Additionally, the notion of using generative models to fill the gap in instances where previous night's sleep measures may be missing, can be explored."}]}