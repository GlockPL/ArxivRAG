{"title": "Advances in Multimodal Adaptation and Generalization: From Traditional Approaches to Foundation Models", "authors": ["Hao Dong", "Moru Liu", "Kaiyang Zhou", "Eleni Chatzi", "Juho Kannala", "Cyrill Stachniss", "Olga Fink"], "abstract": "Domain adaptation and generalization are crucial for real-world applications, such as autonomous driving and medical imaging where the model must operate reliably across environments with distinct data distributions. However, these tasks are challenging because the model needs to overcome various domain gaps caused by variations in, for example, lighting, weather, sensor configurations, and so on. Addressing domain gaps simultaneously in different modalities, known as multimodal domain adaptation and generalization, is even more challenging due to unique challenges in different modalities. Over the past few years, significant progress has been made in these areas, with applications ranging from action recognition to semantic segmentation, and more. Recently, the emergence of large-scale pre-trained multimodal foundation models, such as CLIP, has inspired numerous research studies, which leverage these models to enhance downstream adaptation and generalization. This survey summarizes recent advances in multimodal adaptation and generalization, particularly how these areas evolve from traditional approaches to foundation models. Specifically, this survey covers (1) multimodal domain adaptation, (2) multimodal test-time adaptation, (3) multimodal domain generalization, (4) domain adaptation and generalization with the help of multimodal foundation models, and (5) adaptation of multimodal foundation models. For each topic, we formally define the problem and give a thorough review of existing methods. Additionally, we analyze relevant datasets and applications, highlighting open challenges and potential future research directions. We also maintain an active repository that contains up-to-date literature and supports research activities in these fields at https://github.com/donghao51/Awesome-Multimodal-Adaptation.", "sections": [{"title": "1 INTRODUCTION", "content": "OMAIN adaptation (DA) and domain generalization\n(DG) have attracted significant attention in the re-\nsearch community [1], [2]. In real-world applications such\nas robotics [3], [4], action recognition [5], and anomaly\ndetection [6], [7], it is essential for models trained on limited\nsource domains to perform well on novel target domains.\nTo address distribution shift challenges, numerous DA and\nDG algorithms have been proposed, including distribution\nalignment [8], domain-invariant feature learning [9], feature\ndisentanglement [10], [11], data augmentation [12], [13],\nand meta-learning [14]. Most of these algorithms are de-\nsigned for unimodal data, such as images or time series\ndata. However, the real world is inherently multimodal\nand it is necessary to address multimodal domain adapta-\ntion (MMDA) and generalization (MMDG) across multiple\nmodalities, including audio-video [15], image-language [16],\nand LiDAR-camera [17]. Fig. 1 illustrates the distinction\nbetween unimodal and multimodal DA/DG, where MMDA\nand MMDG integrate information from multiple modalities\nto enhance generalization ability.\nIn recent years, MMDA and MMDG have achieved\nsignificant progress in areas such as action recognition [18]"}, {"title": "2 RELATED RESEARCH TOPICS", "content": "2.1 Domain Adaptation\nDomain adaptation seeks to enhance model performance in\nthe target domain by leveraging labeled source data and\nunlabeled target data [1]. Traditional DA methods typically\nfocus on computer vision applications with images as the\nmain input. Common approaches include aligning feature\ndistributions using discrepancy metrics [8], employing ad-\nversarial learning in input or feature spaces [133], [134],\nand utilizing reconstruction-based methods [135]. In addi-\ntion, techniques such as data augmentation [12] and self-\ntraining [136], have also been extensively explored. Depend-\ning on assumptions about label set relationships between\nthe source and target domains, DA is further categorized\ninto partial-set [137], open-set [138], and universal DA [139],\nwhich are more practical and challenging.\n2.2 Domain Generalization\nDomain generalization aims to generalize models to unseen\ntarget domains without accessing target data during train-\ning. DG methods can be broadly grouped into data manipu-\nlation, representation learning, and learning strategies [2].\nData manipulation methods, such as [140], enhance data\ndiversity, while representation learning approaches [141]\nfocus on extracting domain-invariant features. Addition-\nally, learning strategies like meta-learning [14] and self-\nsupervised learning [142] have demonstrated improved\ngeneralization performance across domains. Similar to DA,\nsome works [143] also address the open-set DG problem\nwhere the target domain has private classes.\n2.3 Test-time Adaptation\nTest-time adaptation (TTA) seeks to adapt a pre-trained\nmodel on the source domain online, addressing distribution"}, {"title": "2.4 Self-supervised Learning", "content": "Self-supervised learning (SSL) aims to learn from unla-\nbeled data by obtaining supervision signals from pretext\ntasks, such as predicting transformations [150], [151], re-\nconstructing missing components [152], [153], or optimizing\ncontrastive objectives [154], [155]. By capturing intrinsic\ndata structures, SSL enables learning robust and domain-\ninvariant representations, making it an essential component\nfor DA and DG. In the multimodal context, SSL is exploited\nthrough tasks such as multimodal alignment [156], cross-\nmodal translation [157], and relative norm alignment [158].\nThese pretext tasks have been recently effectively integrated\ninto MMDA and MMDG frameworks, including methods\nsuch as MOOSA [31] and MM-SADA [18]. For further in-\nformation on SSL, we refer the reader to the existing survey\npapers [159], [160]."}, {"title": "2.5 Foundation Models", "content": "Foundation models are large-scale models pre-trained on\nvast amounts of datasets, enabling effective transfer to var-\nious downstream tasks with minimal task-specific super-\nvision. Prominent examples include language models like\nGPT [161], vision models like SAM [162] and DINO [163],\nvision-language models like CLIP [16] and Flamingo [164],\nand visual generative models like stable diffusion [20]. For\nfurther information on foundation models, we refer the\nreader to the recent survey paper [165]. Recent research\nendeavors aim to enhance DA and DG capabilities using\nfoundation models or adapt them to downstream tasks."}, {"title": "3 MULTIMODAL DOMAIN ADAPTATION", "content": "Multimodal domain adaptation (MMDA) aims at adapting\na model trained on a labeled source domain to perform\neffectively on an unlabeled target domain while leveraging\nmultiple modalities of data (e.g., video, audio, and optical\nflow). The modality gaps and different levels of distribution\nshift across modalities make MMDA particularly challeng-\ning than unimodal DA.\n3.1 Problem Definition\nIn MMDA, we have a labeled source domain $D_{src}$\nand an unlabeled target domain $D_{target}$, where $D_{src}$\n= {$(x_i,y_i)$}$_{i=1}^{n_s}$ represents the source domain with $n_s$ labeled\ndata instances, and $D_{target}$ = {$x_i$}$_{i=1}^{n_t}$ denotes the target do-\nmain with $n_t$ data instances. Each data instance $x_i$ in source\nand target domain is composed of M different modalities,\nexpressed as $X_j$ = {$(x_j)^m$ | m = 1,..., M}. Labels for\nboth domains are given as $y_j \\in Y \\subset R$, while labels for\nthe target domain are unavailable during training. The joint\ndistributions of inputs and labels differ across source and\ntarget domains, i.e., $P_{target} \\neq P_{src}$. The goal of MMDA is\nto learn a robust predictive function $f$ : $X$ $\\rightarrow$ $Y$ on $D_{src}$ and\n$D_{target}$ that minimizes the prediction error on the unlabeled\ntarget domain $D_{target}$ under domain shift scenarios:\n$f = arg min_f E_{(x,y)\\in D_{target}}[l(f(x), y)],$\n(1)\nwhere E denotes the expectation, and l(,) is the loss\nfunction. Existing research on MMDA has primarily focused\non two tasks: the action recognition task with video, audio,\nand optical flow modalities, and the semantic segmentation\ntask with LiDAR point cloud and RGB images. While most\nproposed methods are generally applicable to both tasks, we\ndiscuss them separately for clarity."}, {"title": "3.2 MMDA for Action Recognition", "content": "In this section, we introduce the most common MMDA\nmethods for action recognition in detail and categorize them\ninto domain-adversarial learning, contrastive learning, and\ncross-modal interaction.\n3.2.1 Domain-Adversarial Learning\nAdversarial learning-based approaches effectively align\nmultimodal features across domains by leveraging adver-\nsarial objectives [166] to learn domain-invariant represen-\ntations. Originally designed for unimodal settings, these\nobjectives can be easily extended to multimodal scenarios.\nFor example, Qi et al. [26] leverage an adversarial objective\nto jointly attend and fuse multimodal representation to\nlearn domain-invariant features across modalities. Differ-\nently, MM-SADA [18] incorporates within-modal adversar-\nial alignment alongside multimodal self-supervised align-\nment for MMDA, as shown in Fig. 4. Given a binary domain\nlabel, d, indicating if an example x \u2208 $D_{src}$ or x \u2208 $D_{target}$,\nthe domain discriminator for modality m is defined as:\n$L^m_D = \\sum_{x \\in {Dsrc, Dtarget}}-dlog(D^m(F^m(x)))- (1 \u2013 d) log(1 \u2013 D^m (F^m (x))),$\n(2)\nwhere $D^m$ is the domain discriminator for modality m and\n$F^m$ is the feature extractor. The multimodal self-supervised\nalignment loss aims to learn the temporal correspondence\nbetween modalities and is defined as:\n$L_c = \\sum_{XE{Dsrc, Dtarget}}-clog C(F^1(x), ..., F^M (x)),$\n(3)\nwhere C is the self-supervised correspondence classifier\nhead and c is a binary label defining if modalities corre-\nspond. Moving beyond simple alignment, Zhang et al. [27]\nenhance cross-modal collaboration by selecting reliable\npseudo-labeled target samples while also addressing miss-\ning modality scenarios\u2014where adversarial learning is lever-\naged to generate absent modalities while preserving se-\nmantic integrity. Yin et al. [28] further extend adversarial\nlearning to temporal sequences, using mix-sample adver-\nsarial learning to capture domain-invariant temporal de-\npendencies while dynamically distilling knowledge across\nmodalities to boost adaptability."}, {"title": "3.2.2 Contrastive Learning", "content": "Contrastive learning [167] is a powerful technique for learn-\ning transferable representations by pulling positive pairs\ncloser in the feature space while pushing negative pairs\napart. In MMDA, it helps align features across both do-\nmain and modalities by treating different modalities with\nthe same label as positive pairs. For instance, Song et\nal. [29] jointly align clip- and video-level features using self-\nsupervised contrastive learning while minimizing video-\nlevel domain discrepancy, thus enhancing category-aware\nalignment and cross-domain generalization. Kim et al. [30]\nleverage contrastive learning with modality- and domain-\nspecific sampling strategies by selecting multiple positive\nand negative samples to jointly regularize cross-modal and\ncross-domain feature representations."}, {"title": "3.2.3 Cross-Modal Interaction", "content": "Cross-modal interaction methods enhance multimodal fea-\nture learning by fostering information exchange between\nmodalities during adaptation, enabling models to capture\ncomplementary and interdependent relationships across\nmodalities. For instance, Lv et al. [32] model modality-\nspecific classifiers as teacher-student sub-models, using\nprototype-based reliability measurement for adaptive teach-\ning and asynchronous curriculum learning, and employing\nreliability-aware fusion for robust final decisions. Huang"}, {"title": "3.3 MMDA for Semantic Segmentation", "content": "In this section, we introduce most common MMDA methods\nfor semantic segmentation in detail and categorize them into\nxMUDA and its extensions, domain-adversarial learning,\nand cross-modal interaction.\n3.3.1 xMUDA and Its Extensions\nJaritz et al. [19] introduce the first MMDA framework\nnamed XMUDA for 3D semantic segmentation (3DSS), pro-\nmoting cross-modal prediction consistency through multi-\nhead mutual mimicking (Fig. 5). An unsupervised cross-\nmodal divergence loss is applied to both the source and\ntarget domains to ensure effective cross-modal alignment:\n$L_{xM} = D_{KL}(P(n,c) || Q(n,c))\\\\\n= \\frac{1}{NC} \\sum_{n=1}^N \\sum_{c=1}^C P_{(n,c)} log(\\frac{P(n,c)}{Q(n,c)})$\n(4)\nwhere (P,Q) \u2208 {(P2D, P3D\u21922D), (P3D, P2D\u21923D)}, N the\nnumber of 3D points and C is the number of classes. Here,\nP denotes the target distribution from the main predic-\ntion, while Q represents the mimicking prediction used\nto approximate P. xMUDA also has a variant XMUDAPL,\nwhich leverages pseudo-labels for self-training and serves\nas a strong baseline for MMDA. As a pioneering work,\nxMUDA introduced a new benchmark using nuScenes [169],\nA2D2 [170], and SemanticKITTI [171] datasets, cover-\ning three adaptation scenarios: day-to-night, country-to-\ncountry, and dataset-to-dataset. Many subsequent studies\nhave built upon xMUDA, extending it from different per-\nspectives.\nExtension via Data Augmentation. Data augmentation\ntechniques have been explored to enhance cross-modal\nalignment in xMUDA. For example, Li et al. [35] propose\na multimodal style transfer strategy and a target-aware\nteacher framework to perform cross-domain and cross-\nmodal knowledge distillation on source and synthesized\ntarget-style data. Complementing this, Chen et al. [36] em-\nploy CutMix [172] and Mix3D [173] to augment 2D and"}, {"title": "3.3.2 Domain-Adversarial Learning", "content": "Similar to action recognition, domain-adversarial learning\nmethods for 3DSS leverage adversarial objectives to learn\ndomain-invariant representations. For instance, Peng et\nal. [43] introduce sparse-to-dense feature alignment, enforc-\ning intra-domain point-pixel correspondence while employ-\ning adversarial learning across both domains and modalities\nfor inter-domain alignment. In contrast, Liu et al. [44] focus\nadversarial learning on the image modality and propose\na threshold-moving strategy to mitigate data imbalance"}, {"title": "3.3.3 Cross-Modal Interaction", "content": "To enhance cross-modal interaction, Vobecky et al. [46] pro-\npose a cross-modal unsupervised approach for 2D semantic\nsegmentation (2DSS) using unannotated paired LiDAR and\ncamera data. Their method extracts 3D-consistent object\nsegments based on geometrical properties, then applies pro-\njection and clustering to generate 2D pseudo-ground truth,\nenabling knowledge distillation with cross-modal spatial\nconstraints. Yin et al. [47] address source-free MMDA for\n2DSS by integrating a multimodal auxiliary network during\ntraining. They employ middle fusion and enforce predic-\ntion consistency between augmented depth-RGB pairs to\nfacilitate cross-modal learning. Rizzoli et al. [48] further\nenhance multimodal learning by integrating depth data into\na vision transformer at multiple levels -input, feature, and\noutput stages. Their approach employs color and depth\nstyle transfer for early domain alignment while cross-modal\nself-attention generates enriched feature representations for\nimproved semantic extraction."}, {"title": "3.4 MMDA for Other Tasks", "content": "Beyond action recognition and semantic segmentation,\nMMDA has been explored across a range of diverse tasks,\nsuch as sentiment analysis, medical image segmentation,\netc. For example, Ma et al. [49] address MMDA for cross-\ndomain object and event recognition by leveraging stacked\nattention to capture semantic representations while ap-\nplying multi-channel constraints to enhance category dis-\ncrimination. Liu et al. [50] introduce a tensor-based align-\nment module to model relationships between domains and\nmodalities, along with a dynamic domain generator that cre-\nates transitional samples. Their approach achieves state-of-\nthe-art performance in multimodal sentiment analysis and\nvideo text classification tasks. Recently, Zhang et al. [51] ad-\ndressed MMDA for emotion recognition by independently\nlearning optimal representations for each modality while\nadaptively balancing domain alignment across modalities\nthrough dynamic weighting, ensuring more effective cross-\nmodal adaptation. Yao et al [175] address DA for cross-\nmodality medical image segmentation by introducing multi-\nstyle image translation for better domain alignment."}, {"title": "3.5 Summary and Insights for MMDA", "content": "MMDA has emerged as a powerful framework for leverag-\ning multiple modalities to bridge the gap between source\nand target domains. While significant progress has been\nmade in tasks like action recognition and semantic seg-\nmentation, MMDA faces unique challenges, such as modal-\nity imbalance, missing modalities, and the need for effec-\ntive cross-modal alignment. Current approaches, includ-\ning domain-adversarial learning, contrastive learning, and\ncross-modal interaction, have demonstrated success in ad-\ndressing these challenges by learning domain-invariant rep-\nresentations and fostering modality-specific and modality-\nshared knowledge transfer. Future research could explore\nmodality-agnostic adaptation frameworks that dynamically\nprioritize the most informative modalities during adapta-\ntion, reducing redundancy and computational overhead.\nAnother promising direction is the integration of causal\ninference to model the underlying relationships between\nmodalities and domains, enabling more robust adaptation\nunder distribution shifts. Finally, as multimodal datasets\ngrow in size and complexity, scalable and efficient MMDA\nalgorithms that can handle large-scale, heterogeneous data\nwill be critical."}, {"title": "4 MULTIMODAL TEST-TIME ADAPTATION", "content": "In contrast to multimodal domain adaptation where both\nsource and target domain data are available during adap-\ntation, multimodal test-time adaptation (MMTTA) aims to\nadapt a pre-trained source model online to a target domain\nwithout having access data from the source domain.\n4.1 Problem Definition\nLet $D_{src}$ = {$(x_i,y_i)$}$_{i=1}^{ns}$ represent the source domain\ndataset which follows the distribution $P_{src}$ and each sample\n$x_j$ consists of M modalities, denoted as $x_j$ = {$x_j^m$ | m =\n1,\u2026\u2026, M}. Similarly, let $D_{target}$ = {$x_i$}$_{i=ns+1}^{nt}$ represent the\ntarget domain dataset with distribution $P_{target}$. The label\nspaces for both domains are given as $y_j \\in Y \\subset R$. Let\nf: X \u2192 Y denote a neural network trained on the source\ndistribution $P_{src}$. In MMTTA, f consists of M feature\nextractors $g^m(.)$ and a classifier h(.). Each feature extractor\n$g^m(.)$ processes modality m to produce an embedding $E_m$,\nand the classifier h(\u00b7) combines these embeddings to gener-\nate a prediction probability p:\n$p = \\delta(f(x)) = \\delta(h([g^1(x^1), ..., g^M (x^M)]))\\\\\n= \\delta(h([E_1, ..., E_M])).$\n(5)\nwhere $\u03b4(.)$ denotes the softmax function. Given a well-\ntrained multimodal source model f(x) on $D_{src}$, MMTTA\naims to adapt this model online to the unlabeled target\ndomain $D_{target}$, where $P_{target} \\neq P_{src}$.\n4.2 Methods for Multimodal Test-time Adaptation\nMMTTA is a relatively new research direction, with only a\nlimited number of studies addressing it. Existing research\nprimarily explores MMTTA in action recognition, semantic\nsegmentation, and other tasks."}, {"title": "4.2.1 MMTTA for Action Recognition", "content": "READ, proposed by Yang et al. [52], addresses MMTTA\nunder reliability bias, where modality-specific information\ndiscrepancies arise from intra-modal distribution shifts. Un-\nlike previous TTA methods previous TTA methods [144],\n[146], which update batch normalization statistics and trans-\nformation parameters, READ takes a different approach by\ndynamically modulating cross-modal attention in a self-\nadaptive manner to ensure reliable fusion. Additionally,"}, {"title": "4.2.2 MMTTA for Semantic Segmentation", "content": "Beyond action recognition, MMTTA has also been applied\nto 3D semantic segmentation and other tasks. For instance,\nShin et al. [57] propose an intra-modal pseudo-label Genera-\ntion module, which independently generates pseudo-labels\nfor each modality, and an inter-modal pseudo-label refine-\nment module, which adaptively selects and refines pseudo-\nlabels across modalities to enhance cross-modal consistency"}, {"title": "4.2.3 MMTTA for Other Tasks", "content": "Park et al. [60] focus on MMTTA for depth completion\nusing a single image and an associated sparse depth map.\nThey reduce the domain gap by employing a source-trained\nembedding module that aligns image and sparse depth\nfeatures from the target domain in a single pass. Wang\net al. [61] tackle MMTTA for person re-identification, en-\nhancing model generalization by leveraging relationships\namong heterogeneous modalities. Recently, Li et al [176]\naddress MMTTA for cross-modal retrieval that refines query\npredictions and employs a joint objective to mitigate the\neffects of query shift."}, {"title": "4.3 Summary and Insights for MMTTA", "content": "The emerging field of MMTTA has shown promising results\nacross various tasks, including action recognition, semantic\nsegmentation, and depth completion. Current methods pri-\nmarily focus on addressing challenges such as intra-modal\ndistribution shifts, cross-modal reliability bias, and open-set\nadaptation. Techniques like dynamic cross-modal attention\nmodulation, pseudo-label refinement, and entropy-aware\noptimization have demonstrated effectiveness in improving\nrobustness and adaptability. However, several open chal-\nlenges remain, such as scaling MMTTA to more complex\nmultimodal tasks, handling extreme domain shifts, and en-\nsuring efficient real-time adaptation. Future research could\nexplore the integration of MMTTA with foundation models,\nleveraging large-scale pretraining to enhance generalization,\nand developing unified frameworks that can seamlessly\nadapt to diverse multimodal scenarios. Additionally, in-\nvestigating the theoretical underpinnings of MMTTA, such\nas the interplay between modality-specific and cross-modal\nlearning, could provide deeper insights into its mechanisms\nand limitations."}, {"title": "5 MULTIMODAL DOMAIN GENERALIZATION", "content": "In contrast to multimodal domain adaptation and test-time\nadaptation, multimodal domain generalization (MMDG)\npresents a more challenging problem setting. In MMDG,\nthe model is trained only on source domains with multiple\nmodalities to generalize across unseen domains, without\nprior exposure to target domain data during training, mak-\ning it harder than MMDA and MMTTA.\n5.1 Problem Definition\nIn MMDG, we are given D source domains $D_{src}$ = {$Di$ | $i$\n= 1,\u2026\u2026, D}, where $Di$ = {$(x_i,y_i)$}$_{i=1}^{ni}$ denotes the"}, {"title": "5.2 Methods for Multimodal Domain Generalization", "content": "Similar to MMTTA, MMDG is also a relatively new re-\nsearch direction, with only a few studies addressing this\nchallenging problem in action recognition and semantic\nsegmentation tasks.\n5.2.1 MMDG for Action Recognition\nPlanamente et al. [63] propose the first MMDG approach\nfor egocentric activity recognition, introducing the relative\nnorm alignment loss, which aligns the mean feature norms\nacross different modalities:\n$Lalign = (\\frac{\\overline{E[E^m]}}{\\overline{|E^m|}}-1)^2,$\n(12)\nwhere $E[E^m]$ is the mean feature for the m-th modality in\neach batch. This loss prevents the dominance of a single\nmodality during multimodal joint training, enhancing gen-\neralization generalization across domains. Building on this,\na subsequent study by Planamente et al. [177] refines the\nrelative norm alignment loss by extending it to align class-\nlevel feature norms, further improving its effectiveness in\nmultimodal domain generalization.\nRecently, Dong et al. [62] proposed SimMMDG, a unified\nframework to address MMDG across various scenarios,\nincluding multi-source, single-source, and missing-modality\nsettings, as illustrated in Fig. 6. SimMMDG introduces\na feature disentanglement strategy that decomposes fea-\ntures into modality-specific and modality-shared compo-\nnents, enhancing generalization across domains. Specifi-\ncally, given a unimodal embedding E, SimMMDG splits it\ninto E = [$E_s$; $E_c$], where $E_s$ is a modality-specific feature\nand $E_c$ is a modality-shared feature. This disentanglement\nis enforced through supervised contrastive learning [178]\non modality-shared features, combined with distance con-\nstraints on modality-specific features to encourage diver-"}, {"title": "5.3 Summary and Insights for MMDG", "content": "MMDG represents a significant advancement in enabling\nmultimodal models to generalize across unseen domains\nwithout access to target domain data during training.\nCurrent approaches, such as feature norm alignment, fea-\nture disentanglement, and self-supervised learning, have\ndemonstrated promising results in tasks like action recog-\nnition and semantic segmentation. However, MMDG re-\nmains a challenging problem due to issues like modality\ncompetition, discrepant unimodal flatness, and the need\nfor robust cross-modal alignment. Future research could\nexplore the integration of foundation models and large-scale\npretraining to enhance generalization capabilities further.\nAdditionally, investigating theoretical frameworks to better\nunderstand the interplay between modality-specific and\nmodality-shared features could provide deeper insights into\nMMDG mechanisms."}, {"title": "6 DOMAIN ADAPTATION AND GENERALIZATION WITH THE HELP OF MULTIMODAL FOUNDATION MODELS", "content": "With the recent emergence of large-scale pre-trained multi-\nmodal foundation models (MFMs) such as CLIP [16], stable\ndiffusion [20], and segment anything model (SAM) [162],\nnumerous studies have explored leveraging these models to\nenhance generalization capabilities. These approaches can\nbe categorized into three main directions: data augmenta-\ntion, knowledge distillation, and learning strategies.\n6.1 Multimodal Foundation Models\nMFMs are large-scale machine learning models designed\nto process and integrate multiple types of modalities, such\nas text, images, audio, and video, to generate meaning-\nful representations and perform diverse tasks. These mod-\nels are typically pre-trained on vast datasets using self-\nsupervised or weakly supervised learning techniques and\ncan be adapted to various downstream applications through\nfine-tuning, prompting, or other strategies.\nContrastive Language-Image Pre-Training (CLIP) [16] is\na vision-language model comprising an image encoder\nthat maps high-dimensional images to a low-dimensional\nembedding space and a text encoder that generates text\nrepresentations from natural language. CLIP, trained on 400\nmillion image-text pairs, aligns image and text embedding\nspaces using contrastive loss. For a batch of image-text pairs,\nCLIP maximizes the cosine similarity for matched pairs\nwhile minimizing it for unmatched pairs. During testing,\nthe class names of a target dataset are embedded using the\ntext encoder with prompts in the form of \"a photo of a\n[CLASS]\", where the class token is replaced with specific\nclass names, such as \u201ccat\u201d, \u201cdog\u201d or \u201ccar\u201d. The text encoder\ngenerates text embeddings $T_e$ for each class, and the predic-\ntion probability for an input image x, with embedding $I_x$,\nis computed as:\n$P(y|x) = \\frac{exp (cos (I_x, T_y) /\\tau)}{\\sum_{c=1}^{C-1} exp (cos (I_x, T_c)/\\tau)},$\n(16)\nwhere cos(,) is the cosine similarity between embeddings,\nand is a temperature. Recent works on DA and DG\nuse CLIP's text encoder to guide the generation of diverse\nvisual features or distill CLIP's visual encoder into a smaller\nstudent model for better generalization.\nDiffusion Models [20], such as denoising diffusion prob-\nabilistic models [179], learn the desired data distribution\nthrough a Markov chain of length T. In the forward pass,\nnoise is progressively added to a data sample $x_o$ to create a\nsequence of noisy samples $x_t$, t \u2208 T. In the reverse process,\na model , parameterized by 0, predicts the added noise\nat each step t. Stable diffusion [20] applies this denoising\nprocess to the latent representation z of x in the latent space\nof VQGAN [180] with the learning objective of predicting\nthe added noise at each time step t as:\n$L = E_{x(x),\\epsilon~N(0,1),t} [||\\epsilon \u2013 \\epsilon_\\theta (z_t, t)||^2],$\n(17)\nwhere $z_t$ represents the noised latent representation at time\nstep t. During inference, the reverse process starts with a\nrandom noise $x_T ~ N(0,I)$ and iteratively generates an\nimage sample from the noise from step T to 0. Stable dif-\nfusion also supports flexible conditional image generation\nthrough a cross-attention mechanism [181], enabling models\nto conditionally learn with various input modalities, such\nas text, semantic map, etc. Diffusion models are often used\nto generate additional training data with diverse styles to\nimprove DA and DG performances.\nSegment Anything Model (SAM) [162] is a foundation\nmodel trained for promptable segmentation tasks, capable\nof producing high-quality masks for diverse segmentation\nprompts, including points, boxes, text, or masks. SAM con-\nsists of an image encoder for extracting image embeddings,\na prompt encoder for embedding both sparse (points, boxes,\ntext) and dense (masks) prompts, and a fast mask decoder\nthat efficiently maps image and prompt embeddings to\noutput masks. Trained on over 1 billion masks, SAM demon-\nstrates strong zero-shot segmentation performance. SAM is\nusually used to generate fine-grained instance-level masks\nfor the refinement of predictions in DA and DG."}, {"title": "6.2 Data Augmentation", "content": "Several studies have leveraged MFMs to generate additional\ntraining data for augmentation, either in the feature space or\ninput space (Fig. 7), to enhance generalization capabilities.\n6.2.1 Augmentation in Feature Space\nGenerating data in feature space is more computationally\nefficient compared to directly generating images. For ex-\nample, Dunlap et al. [21] learn a transformation of image\nembeddings from the training domain to unseen test do-\nmains using text descriptions. A classifier is then trained on\nboth real and augmented embeddings, with domain align-\nment and class consistency losses ensuring that augmented\nembeddings remain in the correct domain and retain their\nclass identity. Similarly, Fahes et al. [70] employ general\nlanguage descriptions of target domains to optimize affine\ntransformations of source features, steering them towards\ntarget text embeddings while preserving semantic content.\nGiven a source feature $f_s$, they propose to generate stylized\ntarget feature $f_{s\u2192t}$ by:\n$f_{s\u2192t} = \u03c3(\\frac{f_s \u2013 \u03bc(f_s)}{\u03c3(f_s)}) + \u03bc,$\n(18)\nwhere \u03bc(.) and \u03c3(\u00b7) are two functions returning channel-\nwise mean and standard deviation of input feature, \u03bc\nand o are optimizable variables for target style driven by\na prompt, which is the description embedding TrgEmb of\ntarget domain (e.g., \"driving under rain\"):\n$L_{\u03bc,\u03c3} (f_{s\u2192t}, TrgEmb) = 1 - \\frac{f_{s\u2192t}. TrgEmb}{||f_{s\u2192t}|| || TrgEmb||}.$\n(19)\nYang et al. [72] extend the idea in [70] and achieve adap-\ntation to diverse target domains without explicit domain\nknowledge. Cho et al. [71] simulate distribution shifts in\nthe joint feature space by synthesizing diverse styles via\nprompts, eliminating the need for real images. Recently,\nVidit et al. [182] estimate a set of semantic augmentations\nusing textual domain prompts and source domain images to\ntransform source image embeddings into the target domain\nspecified by the prompts."}, {"title": "6.2.2 Augmentation in Input Space", "content": "Some studies generate data directly in the image space to\nsimulate domain shifts. For instance, Jia et al. [66", "20": "to synthesize a diverse dataset\nof street scenes, which is then used to train a domain-\nagnostic semantic segmentation model. Similarly, Hemati\net al. [67"}]}