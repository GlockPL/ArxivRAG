[{"title": "Advances in Multimodal Adaptation and Generalization: From Traditional Approaches to Foundation Models", "authors": ["Hao Dong", "Moru Liu", "Kaiyang Zhou", "Eleni Chatzi", "Juho Kannala", "Cyrill Stachniss", "Olga Fink"], "abstract": "Domain adaptation and generalization are crucial for real-world applications, such as autonomous driving and medical imaging where the model must operate reliably across environments with distinct data distributions. However, these tasks are challenging because the model needs to overcome various domain gaps caused by variations in, for example, lighting, weather, sensor configurations, and so on. Addressing domain gaps simultaneously in different modalities, known as multimodal domain adaptation and generalization, is even more challenging due to unique challenges in different modalities. Over the past few years, significant progress has been made in these areas, with applications ranging from action recognition to semantic segmentation, and more. Recently, the emergence of large-scale pre-trained multimodal foundation models, such as CLIP, has inspired numerous research studies, which leverage these models to enhance downstream adaptation and generalization. This survey summarizes recent advances in multimodal adaptation and generalization, particularly how these areas evolve from traditional approaches to foundation models. Specifically, this survey covers (1) multimodal domain adaptation, (2) multimodal test-time adaptation, (3) multimodal domain generalization, (4) domain adaptation and generalization with the help of multimodal foundation models, and (5) adaptation of multimodal foundation models. For each topic, we formally define the problem and give a thorough review of existing methods. Additionally, we analyze relevant datasets and applications, highlighting open challenges and potential future research directions. We also maintain an active repository that contains up-to-date literature and supports research activities in these fields at https://github.com/donghao51/Awesome-Multimodal-Adaptation.", "sections": [{"title": "1 INTRODUCTION", "content": "DOMAIN adaptation (DA) and domain generalization (DG) have attracted significant attention in the research community [1], [2]. In real-world applications such as robotics [3], [4], action recognition [5], and anomaly detection [6], [7], it is essential for models trained on limited source domains to perform well on novel target domains. To address distribution shift challenges, numerous DA and DG algorithms have been proposed, including distribution alignment [8], domain-invariant feature learning [9], feature disentanglement [10], [11], data augmentation [12], [13], and meta-learning [14]. Most of these algorithms are designed for unimodal data, such as images or time series data. However, the real world is inherently multimodal and it is necessary to address multimodal domain adaptation (MMDA) and generalization (MMDG) across multiple modalities, including audio-video [15], image-language [16], and LiDAR-camera [17]. Fig. 1 illustrates the distinction between unimodal and multimodal DA/DG, where MMDA and MMDG integrate information from multiple modalities to enhance generalization ability.\nIn recent years, MMDA and MMDG have achieved significant progress in areas such as action recognition [18]"}, {"title": "2 RELATED RESEARCH TOPICS", "content": "Domain adaptation seeks to enhance model performance in the target domain by leveraging labeled source data and unlabeled target data [1]. Traditional DA methods typically focus on computer vision applications with images as the main input. Common approaches include aligning feature distributions using discrepancy metrics [8], employing adversarial learning in input or feature spaces [133], [134], and utilizing reconstruction-based methods [135]. In addition, techniques such as data augmentation [12] and self-training [136], have also been extensively explored. Depending on assumptions about label set relationships between the source and target domains, DA is further categorized into partial-set [137], open-set [138], and universal DA [139], which are more practical and challenging."}, {"title": "2.2 Domain Generalization", "content": "Domain generalization aims to generalize models to unseen target domains without accessing target data during training. DG methods can be broadly grouped into data manipulation, representation learning, and learning strategies [2]. Data manipulation methods, such as [140], enhance data diversity, while representation learning approaches [141] focus on extracting domain-invariant features. Additionally, learning strategies like meta-learning [14] and self-supervised learning [142] have demonstrated improved generalization performance across domains. Similar to DA, some works [143] also address the open-set DG problem where the target domain has private classes."}, {"title": "2.3 Test-time Adaptation", "content": "Test-time adaptation (TTA) seeks to adapt a pre-trained model on the source domain online, addressing distribution shifts without requiring access to either source data or target labels. Online TTA methods [144], [145] update specific model parameters using incoming test samples based on unsupervised objectives such as entropy minimization and pseudo-labels. Robust TTA methods [146], [147] address more complex and practical scenarios, including label shifts, single-sample adaptation, and mixed domain shifts. Continual TTA approaches [148], [149] target the continual and evolving distribution shifts encountered over test time, which is particularly prevalent in real-world applications."}, {"title": "2.4 Self-supervised Learning", "content": "Self-supervised learning (SSL) aims to learn from unlabeled data by obtaining supervision signals from pretext tasks, such as predicting transformations [150], [151], reconstructing missing components [152], [153], or optimizing contrastive objectives [154], [155]. By capturing intrinsic data structures, SSL enables learning robust and domain-invariant representations, making it an essential component for DA and DG. In the multimodal context, SSL is exploited through tasks such as multimodal alignment [156], cross-modal translation [157], and relative norm alignment [158]. These pretext tasks have been recently effectively integrated into MMDA and MMDG frameworks, including methods such as MOOSA [31] and MM-SADA [18]. For further information on SSL, we refer the reader to the existing survey papers [159], [160]."}, {"title": "2.5 Foundation Models", "content": "Foundation models are large-scale models pre-trained on vast amounts of datasets, enabling effective transfer to various downstream tasks with minimal task-specific supervision. Prominent examples include language models like GPT [161], vision models like SAM [162] and DINO [163], vision-language models like CLIP [16] and Flamingo [164], and visual generative models like stable diffusion [20]. For further information on foundation models, we refer the reader to the recent survey paper [165]. Recent research endeavors aim to enhance DA and DG capabilities using foundation models or adapt them to downstream tasks."}, {"title": "3 MULTIMODAL DOMAIN ADAPTATION", "content": "Multimodal domain adaptation (MMDA) aims at adapting a model trained on a labeled source domain to perform effectively on an unlabeled target domain while leveraging multiple modalities of data (e.g., video, audio, and optical flow). The modality gaps and different levels of distribution shift across modalities make MMDA particularly challenging than unimodal DA."}, {"title": "3.1 Problem Definition", "content": "In MMDA, we have a labeled source domain $D_{src}$ and an unlabeled target domain $D_{target}$, where $D_{src} = {\\(x_i,y_i\\)}_{i=1}^{n_s}$ represents the source domain with $n_s$ labeled data instances, and $D_{target} = {\\x_i\\}_{i=1}^{n_t}$ denotes the target domain with $n_t$ data instances. Each data instance $x_i$ in source and target domain is composed of $M$ different modalities, expressed as $X_i = {\\(x_i\\)^m | m = 1,..., M\\}$. Labels for both domains are given as $y_i \\in Y \\subset \\mathbb{R}$, while labels for the target domain are unavailable during training. The joint distributions of inputs and labels differ across source and target domains, i.e., $P_{target} \\neq P_{src}$. The goal of MMDA is to learn a robust predictive function $f : X \\rightarrow Y$ on $D_{src}$ and $D_{target}$ that minimizes the prediction error on the unlabeled target domain $D_{target}$ under domain shift scenarios:\n$f = arg \\min_f E_{(x,y)\\in D_{target}} [l(f(x), y)],$                                                                                                (1)\nwhere $E$ denotes the expectation, and $l(\u00b7,\u00b7)$ is the loss function. Existing research on MMDA has primarily focused on two tasks: the action recognition task with video, audio, and optical flow modalities, and the semantic segmentation task with LiDAR point cloud and RGB images. While most proposed methods are generally applicable to both tasks, we discuss them separately for clarity."}, {"title": "3.2 MMDA for Action Recognition", "content": "In this section, we introduce the most common MMDA methods for action recognition in detail and categorize them into domain-adversarial learning, contrastive learning, and cross-modal interaction."}, {"title": "3.2.1 Domain-Adversarial Learning", "content": "Adversarial learning-based approaches effectively align multimodal features across domains by leveraging adversarial objectives [166] to learn domain-invariant representations. Originally designed for unimodal settings, these objectives can be easily extended to multimodal scenarios. For example, Qi et al. [26] leverage an adversarial objective to jointly attend and fuse multimodal representation to learn domain-invariant features across modalities. Differently, MM-SADA [18] incorporates within-modal adversarial alignment alongside multimodal self-supervised alignment for MMDA, as shown in Fig. 4. Given a binary domain label, $d$, indicating if an example $x \\in D_{src}$ or $x \\in D_{target}$, the domain discriminator for modality $m$ is defined as:\n$\\mathcal{L}_{D^m} = \\sum_{x\\in\\{D_{src}, D_{target}\\}}-dlog(D^m(F^m(x)))-(1 - d) log(1 - D^m (F^m (x))),$      (2)\nwhere $D^m$ is the domain discriminator for modality $m$ and $F^m$ is the feature extractor. The multimodal self-supervised alignment loss aims to learn the temporal correspondence between modalities and is defined as:\n$\\mathcal{L}_c = \\sum_{X \\in \\{D_{src}, D_{target}\\}} -clog C(F^1(x), ..., F^M (x)),$                                                                                                          (3)\nwhere $C$ is the self-supervised correspondence classifier head and $c$ is a binary label defining if modalities correspond. Moving beyond simple alignment, Zhang et al. [27] enhance cross-modal collaboration by selecting reliable pseudo-labeled target samples while also addressing missing modality scenarios\u2014where adversarial learning is leveraged to generate absent modalities while preserving semantic integrity. Yin et al. [28] further extend adversarial learning to temporal sequences, using mix-sample adversarial learning to capture domain-invariant temporal dependencies while dynamically distilling knowledge across modalities to boost adaptability."}, {"title": "3.2.2 Contrastive Learning", "content": "Contrastive learning [167] is a powerful technique for learning transferable representations by pulling positive pairs closer in the feature space while pushing negative pairs apart. In MMDA, it helps align features across both domains and modalities by treating different modalities with the same label as positive pairs. For instance, Song et al. [29] jointly align clip- and video-level features using self-supervised contrastive learning while minimizing video-level domain discrepancy, thus enhancing category-aware alignment and cross-domain generalization. Kim et al. [30] leverage contrastive learning with modality- and domain-specific sampling strategies by selecting multiple positive and negative samples to jointly regularize cross-modal and cross-domain feature representations."}, {"title": "3.2.3 Cross-Modal Interaction", "content": "Cross-modal interaction methods enhance multimodal feature learning by fostering information exchange between modalities during adaptation, enabling models to capture complementary and interdependent relationships across modalities. For instance, Lv et al. [32] model modality-specific classifiers as teacher-student sub-models, using prototype-based reliability measurement for adaptive teaching and asynchronous curriculum learning, and employing reliability-aware fusion for robust final decisions. Huang"}, {"title": "3.3 MMDA for Semantic Segmentation", "content": "In this section, we introduce most common MMDA methods for semantic segmentation in detail and categorize them into xMUDA and its extensions, domain-adversarial learning, and cross-modal interaction."}, {"title": "3.3.1 xMUDA and Its Extensions", "content": "Jaritz et al. [19] introduce the first MMDA framework named xMUDA for 3D semantic segmentation (3DSS), promoting cross-modal prediction consistency through multi-head mutual mimicking (Fig. 5). An unsupervised cross-modal divergence loss is applied to both the source and target domains to ensure effective cross-modal alignment:\n$\\mathcal{L}_{xM} = \\sum_{n=1}^{N} \\sum_{c=1}^{C} D_{KL}(P^{(n,c)} || Q^{(n,c)})$\n$\\text{where } (P,Q) \\in \\{(P^{2D}, P^{3D\\rightarrow 2D}), (P^{3D}, P^{2D\\rightarrow 3D})\\}, N\\text{ the number of 3D points and }C\\text{ is the number of classes. Here, }P\\text{ denotes the target distribution from the main prediction, while }Q\\text{ represents the mimicking prediction used to approximate }P.\n$                                                                      (4)\nxMUDA also has a variant XMUDAPL, which leverages pseudo-labels for self-training and serves as a strong baseline for MMDA. As a pioneering work, xMUDA introduced a new benchmark using nuScenes [169], A2D2 [170], and SemanticKITTI [171] datasets, covering three adaptation scenarios: day-to-night, country-to-country, and dataset-to-dataset. Many subsequent studies have built upon xMUDA, extending it from different perspectives.\nExtension via Data Augmentation. Data augmentation techniques have been explored to enhance cross-modal alignment in xMUDA. For example, Li et al. [35] propose a multimodal style transfer strategy and a target-aware teacher framework to perform cross-domain and cross-modal knowledge distillation on source and synthesized target-style data. Complementing this, Chen et al. [36] employ CutMix [172] and Mix3D [173] to augment 2D and 3D training data, facilitating 2D-3D interaction and intra-domain cross-modal learning. Recently, Cao et al. [37] enhanced xMUDA's pipeline by incorporating 3D rare objects collected from real-world scenarios and leveraging pixel-wise supervision from the SAM [162] model. This approach effectively addresses imbalanced supervision and significantly improves the segmentation of rare objects.\nExtension via Fusion. Beyond augmentation, fusion-based strategies refine xMUDA by improving information exchange between modalities. For instance, Wu et al. [174] perform cross-modal and cross-domain alignments through knowledge distillation using fused cross-modal representations, maximizing correlation and complementarity between heterogeneous modalities to mitigate domain shift. Cardace et al. [38] further strengthen fusion by feeding depth features into the 2D branch while dynamically enriching the 3D network with RGB features. By employing middle fusion across both branches, they effectively exploit intrinsic cross-modal complementarity. Taking a different approach, Simons et al. [39] introduce a dynamic selection mechanism for fused and unfused rectified pseudo-labels, enabling self-training in source-free MMDA for 3DSS.\nExtension via Cross-modal Interaction. Zhang et al. [40] introduce plane-to-spatial and discrete-to-textured self-supervised tasks to train models in a mixed-domain setting,enhancing modality-specific learning and mitigating domain shift. Xing et al. [41] strengthen xMUDA by incorporating cross-modal contrastive learning and a neighborhood feature aggregation module, reinforcing 2D-3D consistency across domains while capturing richer contextual information. Building on this, Zhang et al. [42] integrate masked cross-modal modeling to bridge large domain gaps and introduce dynamic cross-modal filters for feature matching. This enables the model to dynamically leverage 2D-3D complementarity, improving overall robustness and adaptability."}, {"title": "3.3.2 Domain-Adversarial Learning", "content": "Similar to action recognition, domain-adversarial learning methods for 3DSS leverage adversarial objectives to learn domain-invariant representations. For instance, Peng et al. [43] introduce sparse-to-dense feature alignment, enforcing intra-domain point-pixel correspondence while employing adversarial learning across both domains and modalities for inter-domain alignment. In contrast, Liu et al. [44] focus adversarial learning on the image modality and propose a threshold-moving strategy to mitigate data imbalance during inference. Beyond pure adversarial alignment, Man et al. [45] introduce a distillation framework that transfers knowledge from a LiDAR teacher model to a camera student model through feature supervision on depth estimation and Bird's-Eye View (BEV) embeddings. Additionally, multi-stage adversarial learning further refines feature alignment across domains."}, {"title": "3.3.3 Cross-Modal Interaction", "content": "To enhance cross-modal interaction, Vobecky et al. [46] propose a cross-modal unsupervised approach for 2D semantic segmentation (2DSS) using unannotated paired LiDAR and camera data. Their method extracts 3D-consistent object segments based on geometrical properties, then applies projection and clustering to generate 2D pseudo-ground truth, enabling knowledge distillation with cross-modal spatial constraints. Yin et al. [47] address source-free MMDA for 2DSS by integrating a multimodal auxiliary network during training. They employ middle fusion and enforce prediction consistency between augmented depth-RGB pairs to facilitate cross-modal learning. Rizzoli et al. [48] further enhance multimodal learning by integrating depth data into a vision transformer at multiple levels -input, feature, and output stages. Their approach employs color and depth style transfer for early domain alignment while cross-modal self-attention generates enriched feature representations for improved semantic extraction."}, {"title": "3.4 MMDA for Other Tasks", "content": "Beyond action recognition and semantic segmentation, MMDA has been explored across a range of diverse tasks, such as sentiment analysis, medical image segmentation, etc. For example, Ma et al. [49] address MMDA for cross-domain object and event recognition by leveraging stacked attention to capture semantic representations while applying multi-channel constraints to enhance category discrimination. Liu et al. [50] introduce a tensor-based alignment module to model relationships between domains and modalities, along with a dynamic domain generator that creates transitional samples. Their approach achieves state-of-the-art performance in multimodal sentiment analysis and video text classification tasks. Recently, Zhang et al. [51] addressed MMDA for emotion recognition by independently learning optimal representations for each modality while adaptively balancing domain alignment across modalities through dynamic weighting, ensuring more effective cross-modal adaptation. Yao et al [175] address DA for cross-modality medical image segmentation by introducing multi-style image translation for better domain alignment."}, {"title": "3.5 Summary and Insights for MMDA", "content": "MMDA has emerged as a powerful framework for leveraging multiple modalities to bridge the gap between source and target domains. While significant progress has been made in tasks like action recognition and semantic segmentation, MMDA faces unique challenges, such as modality imbalance, missing modalities, and the need for effective cross-modal alignment. Current approaches, including domain-adversarial learning, contrastive learning, and cross-modal interaction, have demonstrated success in addressing these challenges by learning domain-invariant representations and fostering modality-specific and modality-shared knowledge transfer. Future research could explore modality-agnostic adaptation frameworks that dynamically prioritize the most informative modalities during adaptation, reducing redundancy and computational overhead. Another promising direction is the integration of causal inference to model the underlying relationships between modalities and domains, enabling more robust adaptation under distribution shifts. Finally, as multimodal datasets grow in size and complexity, scalable and efficient MMDA algorithms that can handle large-scale, heterogeneous data will be critical."}, {"title": "4 MULTIMODAL TEST-TIME ADAPTATION", "content": "In contrast to multimodal domain adaptation where both source and target domain data are available during adaptation, multimodal test-time adaptation (MMTTA) aims to adapt a pre-trained source model online to a target domain without having access data from the source domain."}, {"title": "4.1 Problem Definition", "content": "Let $D_{src} = {\\(x_i,y_i\\)}_{i=1}^{n_s}$ represent the source domain dataset which follows the distribution $P^s$ and each sample $x_i$ consists of $M$ modalities, denoted as $x_i = {\\x_i^m | m = 1,......, M\\}$. Similarly, let $D_{target} = {\\x_i\\}_{i=n_s+1}^{n_s+n_t}$ represent the target domain dataset with distribution $P^t$. The label spaces for both domains are given as $y_i \\in \\mathcal{Y} \\subset \\mathbb{R}$. Let $f: X \\rightarrow Y$ denote a neural network trained on the source distribution $P^s$. In MMTTA, $f$ consists of $M$ feature extractors $g^m(.)$ and a classifier $h(.)$. Each feature extractor $g^m(.)$ processes modality m to produce an embedding $E^m$, and the classifier $h(\u00b7)$ combines these embeddings to generate a prediction probability $p$:\n$p = \\delta(f(x)) = \\delta(h([g_1(x^1), ..., g_M (x^M)]))$\n$= \\delta(h([E^1, ..., E^M])).$                                                                                                                                         (5)\nwhere $\\delta(.)$ denotes the softmax function. Given a well-trained multimodal source model $f(x)$ on $D_{src}$, MMTTA aims to adapt this model online to the unlabeled target domain $D_{target}$, where $P^t \\neq P^s$."}, {"title": "4.2 Methods for Multimodal Test-time Adaptation", "content": "MMTTA is a relatively new research direction, with only a limited number of studies addressing it. Existing research primarily explores MMTTA in action recognition, semantic segmentation, and other tasks."}, {"title": "4.2.1 MMTTA for Action Recognition", "content": "READ, proposed by Yang et al. [52], addresses MMTTA under reliability bias, where modality-specific information discrepancies arise from intra-modal distribution shifts. Unlike previous TTA methods previous TTA methods [144], [146], which update batch normalization statistics and transformation parameters, READ takes a different approach by dynamically modulating cross-modal attention in a self-adaptive manner to ensure reliable fusion. Additionally, READ introduces a novel confidence-aware loss function $\\mathcal{L}_{ra}$, designed to enhance the robustness of multimodal adaptation:\n$\\mathcal{L}_{ra} = \\frac{1}{B} \\sum_{i=1}^{B} p_i log \\frac{e^{y_i}}{\\sum_c e^y_c},$                                                                                                                               (6)\nwhere $B$ is the batch size, $p_i$ is the confidence of the prediction $p_i$, i.e., $p_i = max(p_i)$, and $y$ is a threshold for confident prediction. $\\mathcal{L}_{ra}$ helps the model focus more on the high-confident prediction while preventing the noise from the low-confident predictions. In a different line of research, Xiong et al. [54] propose a teacher-student memory bank framework combined with self-assembled source-friendly feature reconstruction to align multimodal prototypes effectively. Their approach mitigates domain shifts in MMTTA by preserving cross-modal consistency and enhancing feature adaptability. Furthermore, Lei et al. [55] adopt a two-level objective function that incorporates Shannon entropy loss and a diversity-promoting loss. This approach effectively addresses both intra-modal distribution shifts and cross-modal reliability bias within the modality fusion block, ensuring more robust multimodal adaptation. Beyond domain shifts, Dong et al. [53] extend MMTTA to the open-set setting, where previously unseen categories emerge during test-time adaptation. They propose adaptive entropy-aware optimization (AEO), a novel approach that amplifies the entropy difference between known and unknown samples during online adaptation. AEO consists of two key components: (1) unknown-aware adaptive entropy optimization and (2) adaptive modality prediction discrepancy optimization. The unknown-aware adaptive entropy optimization module adaptively weights and optimizes each sample based on its prediction uncertainty and is defined as:\n$\\mathcal{W}_{Ada} = tanh(\\beta \\cdot (\\mathcal{H}(\\hat{p}) - \\alpha)),$                                                                                                     (7)\n$\\mathcal{L}_{AdaEnt} = -\\mathcal{H}(\\hat{p})\\mathcal{W}_{Ada},$                                                                                                               (8)\nwhere $tanh$ is the hyperbolic tangent function, $\\mathcal{W}_{Ada}$ is the adaptive weight assigned to each sample, $\\mathcal{H}(\\hat{p})$ is the normalized entropy of prediction $\\hat{p}$, computed as $\\mathcal{H}(\\hat{p}) = -(\\sum_c \\hat{p}_c logic)/ log(C)$, with $C$ being the number of classes.\nThe adaptive modality prediction discrepancy optimization module optimizes the prediction discrepancy across modalities and is defined as:\n$\\mathcal{L}_{AdaDis} = -(\\mathcal{Dis}(p_1, p_2))\\mathcal{W}_{Ada},$                                                                                                      (9)\nwhere $\\mathcal{W}_{Ada}$ is the adaptive weight calculated in Eq. (7). In addition to the pure open-set TTA scenarios, AEO has also demonstrated effectiveness and versatility in challenging long-term and continual adaptation scenarios."}, {"title": "4.2.2 MMTTA for Semantic Segmentation", "content": "Beyond action recognition, MMTTA has also been applied to 3D semantic segmentation and other tasks. For instance, Shin et al. [57] propose an intra-modal pseudo-label Generation module, which independently generates pseudo-labels for each modality, and an inter-modal pseudo-label refinement module, which adaptively selects and refines pseudo-labels across modalities to enhance cross-modal consistency and improve adaptation. Building on this, Cao et al. [58] explore multi-modal continual test-time adaptation, addressing dynamically evolving domains over time. It facilitates dynamic adaptation for 3D semantic segmentation by attending to reliable modalities and mitigating catastrophic forgetting through dual-stage mechanisms and class-wise momentum queues designed for continual domain shifts. Recently, Cao et al. [59] further enhanced 3D segmentation by leveraging reliable spatial-temporal correspondences, filtering unreliable predictions, and employing cross-modal learning to maintain consistency across consecutive frames."}, {"title": "4.2.3 MMTTA for Other Tasks", "content": "Park et al. [60] focus on MMTTA for depth completion using a single image and an associated sparse depth map. They reduce the domain gap by employing a source-trained embedding module that aligns image and sparse depth features from the target domain in a single pass. Wang et al. [61] tackle MMTTA for person re-identification, enhancing model generalization by leveraging relationships among heterogeneous modalities. Recently, Li et al [176] address MMTTA for cross-modal retrieval that refines query predictions and employs a joint objective to mitigate the effects of query shift."}, {"title": "4.3 Summary and Insights for MMTTA", "content": "The emerging field of MMTTA has shown promising results across various tasks, including action recognition, semantic segmentation, and depth completion. Current methods primarily focus on addressing challenges such as intra-modal distribution shifts, cross-modal reliability bias, and open-set adaptation. Techniques like dynamic cross-modal attention modulation, pseudo-label refinement, and entropy-aware optimization have demonstrated effectiveness in improving robustness and adaptability. However, several open challenges remain, such as scaling MMTTA to more complex multimodal tasks, handling extreme domain shifts, and ensuring efficient real-time adaptation. Future research could explore the integration of MMTTA with foundation models, leveraging large-scale pretraining to enhance generalization, and developing unified frameworks that can seamlessly adapt to diverse multimodal scenarios. Additionally, investigating the theoretical underpinnings of MMTTA, such as the interplay between modality-specific and cross-modal learning, could provide deeper insights into its mechanisms and limitations."}, {"title": "5 MULTIMODAL DOMAIN GENERALIZATION", "content": "In contrast to multimodal domain adaptation and test-time adaptation, multimodal domain generalization (MMDG) presents a more challenging problem setting. In MMDG, the model is trained only on source domains with multiple modalities to generalize across unseen domains, without prior exposure to target domain data during training, making it harder than MMDA and MMTTA."}, {"title": "5.1 Problem Definition", "content": "In MMDG, we are given D source domains $D_{src} = {D_i | i = 1,......, D}$, where $D_i = {\\(x_i,y_i\\)}_{i=1}^{n_i}$ denotes the i-th domain with $n_i$ data instances. Each data instance $x = {\\(x^m\\) | m = 1,...,M\\} \\in X$ is comprised of M different modalities and $y \\in Y \\subset \\mathbb{R}$ denotes the label. The joint distributions between each pair of domains differ, formally expressed as: $P^{x y}_i \\neq P^{x y}_j, 1 \\leq i \\neq j < D$. The goal of MMDG is to learn a robust and generalizable predictive function $f: X \\rightarrow Y$ from D source domains $D_{src}$ and M data modalities to achieve a minimum prediction error on a target domain $D_{target}$ (i.e., $D_{target}$ cannot be accessed during training and $P^{target}_{xy} \\neq P^{x y}_i$ for $i \\in \\{1,....., D\\}$):\n$f = arg \\min_f E_{(x,y)\\in D_{target}} [l(f(x), y)],$                                                                                                              (10)\nwhere $E$ is the expectation and $l(\u00b7,\u00b7)$ is the loss function. The $f$ in MMDG is comprised of $M$ feature extractors $g_m(\u00b7)$ and a classifier $h(\u00b7)$. Each feature extractor $g_m(\u00b7)$ extracts an embedding $E^m$ for its corresponding modality $m$, and the classifier $h(\u00b7)$ takes the combined embeddings from all modalities as input and outputs a prediction probability $\\hat{y}$:\n$p = \\delta(f(x)) = \\delta(h([g_1(x^1), ..., g_M (x^M)]))$\n$= \\delta(h([E^1, ..., E^M])).$                                                                                                                               (11)\nwhere $\\delta(\u00b7)$ is the softmax function."}, {"title": "5.2 Methods for Multimodal Domain Generalization", "content": "Similar to MMTTA, MMDG is also a relatively new research direction, with only a few studies addressing this challenging problem in action recognition and semantic segmentation tasks."}, {"title": "5.2.1 MMDG for Action Recognition", "content": "Planamente et al. [63", "modalities": "n$\\mathcal{L"}, {"frac{E[|E^m|": ""}, {"E[E^m": "is the mean feature for the m-th modality in each batch. This loss prevents the dominance of a single modality during multimodal joint training", "177": "refines the relative norm alignment loss by extending it to align class-level feature norms", "62": "proposed SimMMDG", "E_c": "where $E_s$ is a modality-specific feature and $E_c$ is a modality-shared feature. This disentanglement is enforced through supervised contrastive learning [178", "j=1,...,N": "the corresponding batch used for training consists of M \u00d7 N pairs", "y\u0303k}k=1,.......,M\u00d7N": "where xM\u00d7j", "A(i)": "y\u0303p = y\u0303i"}, "as the set of indices of all positive samples in the batch which share the same label as i. The cardinality of P(i) is denoted as |P(i)|. The multimodal supervised contrastive learning loss can be written as:\n$\\mathcal{L}_{s/m} = \\sum_{i \\in I} - \\frac{1}{|P(i)|} log \\sum_{p \\in P(i)} \\frac{exp (z_i \\cdot z_p/\\tau)}{\\sum_{\\alpha \\in A(i)} exp(z_i \\cdot z_\\alpha/\\tau)},$                                                                                                                  (13)\nwith zk = Proj(g(xk)) \u2208 RDP, where g(\u00b7) is the feature extractor, that maps xk to modality-specific and modality-shared features, E = [Es; Ec"], "RDE": "and Proj(\u00b7) is the projection network that maps Ec to a vector z = Proj(Ec) \u2208 RDP. The inner product between two projected feature vectors is denoted by \u00b7", "as": "n$\\mathcal{L"}, {"scenarios": "n$\\mathcal{L}_{trans} = \\frac{1}{M(M-1)"}]