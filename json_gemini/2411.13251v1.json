{"title": "BelHouse3D: A Benchmark Dataset for Assessing Occlusion Robustness in 3D Point Cloud Semantic Segmentation", "authors": ["Umamaheswaran Raman Kumar", "Abdur Fayjie", "Jurgen Hannaert", "Patrick Vandewalle"], "abstract": "Large-scale 2D datasets have been instrumental in advancing machine learning; however, progress in 3D vision tasks has been relatively slow. This disparity is largely due to the limited availability of 3D benchmarking datasets. In particular, creating real-world point cloud datasets for indoor scene semantic segmentation presents considerable challenges, including data collection within confined spaces and the costly, often inaccurate process of per-point labeling to generate ground truths. While synthetic datasets address some of these challenges, they often fail to replicate real-world conditions, particularly the occlusions that occur in point clouds collected from real environments. Existing 3D benchmarking datasets typically evaluate deep learning models under the assumption that training and test data are independently and identically distributed (IID), which affects the models' usability for real-world point cloud segmentation. To address these challenges, we introduce the BelHouse3D dataset, a new synthetic point cloud dataset designed for 3D indoor scene semantic segmentation. This dataset is constructed using real-world references from 32 houses in Belgium, ensuring that the synthetic data closely aligns with real-world conditions. Additionally, we include a test set with data occlusion to simulate out-of-distribution (OOD) scenarios, reflecting the occlusions commonly encountered in real-world point clouds. We evaluate popular point-based semantic segmentation methods using our OOD setting and present a benchmark. We believe that BelHouse3D and its OOD setting will advance research in 3D point cloud semantic segmentation for indoor scenes, providing valuable insights for the development of more generalizable models.", "sections": [{"title": "1 Introduction", "content": "In recent years, machine learning (ML) has achieved human-level performance (even beyond) in various computer vision tasks, such as image classification, segmentation, and object detection. These advancements have been largely driven"}, {"title": "2 Related Work", "content": "Semantic segmentation. Existing 3D semantic segmentation methods are categorized into projection-based, discretization-based, hybrid, and point-based approaches [25]. Projection-based methods project a 3D point cloud into 2D images, such as multi-view and spherical images. Discretization methods transform point clouds into intermediate volumetric or sparse representations. Hybrid methods leverage multi-modal features; for example, 3DMV [14] leverages a joint 3D-multiview network that combines RGB and geometric features. Chiang et al. introduced a unified point-based framework to learn 2D textual representation, 3D structures, and global features [10], while MVPNet [32] combines 2D multi-view images with spatial geometric features.\nPoint-based methods, inspired by the pioneering work PointNet [58], directly input orderless and unstructured point clouds into a network to learn per-point features using shared multilayer perceptrons (MLPs) and pooling functions. Following PointNet, many point-based methods [29, 34, 59, 68, 73, 81, 89, 91] have been developed, including point-wise MLP, point convolution, graph-based, and RNN-based methods. Point-wise MLP methods are the simplest, applying shared MLPs to achieve high-efficiency networks. Point convolution methods design convolutional operators for point clouds, enhancing the receptive field of point-wise MLP networks. Graph-based methods use graph neural networks to capture the underlying shape and geometry of 3D objects, offering higher accuracy but at the cost of increased computational expense. RNN-based methods employ Recurrent Neural Networks, which tend to be computationally intensive."}, {"title": "Contribution", "content": "This paper introduces BelHouse3D, a novel 3D synthetic point cloud dataset designed for indoor semantic segmentation, capturing the geometry and structure of diverse household objects. Unlike existing synthetic datasets, BelHouse3D is developed using real-world references from 32 Belgian houses, ensuring greater fairness and reliability. The dataset provides a test set and configurations designed for OOD evaluation. Additionally, the paper presents a new benchmark for evaluating out-of-distribution (OOD) generalization in point-based fully-supervised and few-shot learning (FSL) segmentation models."}, {"title": "Novelty", "content": "The effect of out-of-domain (OOD) generalization, especially in the context of occlusion, a common occurrence in real-world point clouds, remains a relatively underexplored area. Although OOD generalization has been studied across various modalities, to the best of our knowledge, this study is the first to investigate OOD generalization in point cloud segmentation for 3D indoor scenes and to establish a dedicated benchmark for this task."}, {"title": "3 BelHouse3D Dataset", "content": "The BelHouse3D dataset comprises 424 indoor scenes, collected from 32 distinct houses in Belgium offering accurate annotations for 19 classes. The classes are primarily categorized into three groups: 5 building structures (\u2018ceiling', \u2018floor', 'wall', 'door' and 'window'), 13 household objects ('bathtub', \u2018bed', 'cabinet', 'chair', 'coffee table', 'couch', 'desk', \u2018dining table', 'island', 'pillow', 'shelf', 'side table', and 'water closet') and all other small objects collectively annotated as 'clutter'. The subsequent section details the dataset generation process, covering the collection of real-world data, the creation of synthetic counterparts, and the OOD setting, as depicted in Fig. 3."}, {"title": "3.1 Real-Data Acquisition", "content": "The data acquisition process begins with capturing real-world data from 32 Belgian houses representing various indoor scenes. RGB-D images of a scene are obtained using an Intel RealSense sensor and are then viewed in the Dot3D [18] app running on an Apple ipad. This setup facilitates systematic scanning of each room, even in confined spaces, by capturing multiple frames for comprehensive coverage. The frames, along with their corresponding camera poses, are saved as DotProduct (.dp) files. Following the raw data collection and storage, a 3D reconstruction technique is employed to generate point clouds. This process involves projecting 2D pixels from each image frame, based on depth information,"}, {"title": "3.2 Synthetic Data Generation", "content": "Blender [5], a free and open-source 3D computer graphics software tool, is used to create the synthetic counterparts of the dataset. Real-world point clouds, detailed in Sec. 3.1, serve as references for constructing 3D models in Blender. Predefined object models are utilized for common household items to ensure cleaner data and precise annotations. The scenes, including these objects, are stored as wavefront files, from which point clouds are generated by sampling from the surfaces of the objects. To maintain fairness in the dataset, the data generation process is guided by real scene references through scene referencing and object referencing."}, {"title": "Scene referencing", "content": "In the scene referencing stage, room layouts are crafted in Blender to mirror the scale and dimensions of each room from real-world data. This reference aids in replicating the positions of building structures such as walls, ceilings, floors, doors, and windows from collected real-world data into the"}, {"title": "Object referencing", "content": "Precise control in synthetic data generation is essential for achieving accurate data labeling, especially in 3D point clouds, where per-point labeling for semantic segmentation is labor-intensive, costly, and prone to inaccuracies. Object referencing offers greater control over data and label generation by using predefined object models to represent common household objects. These models are placed within the simulated room layouts based on the estimated positions of their real-world counterparts. However, this step does not strictly adhere to exact object positioning, allowing for controlled variations within object categories. Additionally, this method helps in filling occluded parts, resulting in a cleaner and more complete synthetic dataset."}, {"title": "3.3 OOD Setting", "content": "To address the challenges posed by inevitable occlusions in real-world point clouds, a test set was meticulously developed by selectively removing points from specific portions of objects or scenes within a controlled simulated environment. This process involves using the original viewpoints from which the real-world"}, {"title": "4 Benchmarking and Discussions", "content": "This section presents the benchmarking results for 3D semantic segmentation under fully supervised and few-shot settings, evaluated in both IID and OOD conditions. The standard evaluation metrics for 3D semantic segmentation, including mean Intersection over Union (mIOU) and Overall Accuracy (OA), are used for benchmarking."}, {"title": "4.1 Fully Supervised 3D Segmentation in OOD Conditions", "content": "3D Semantic Segmentation of point clouds is a computer vision task that involves the precise labeling of individual points within a three-dimensional space. One of the primary challenges in 3D Semantic Segmentation arises from the irregular and unstructured nature of point clouds. Unlike traditional 2D images, point clouds lack a predefined grid structure, making it challenging to apply conventional image processing techniques directly. Point-based methods, directly operating on point clouds, have emerged as promising approaches to address these challenges."}, {"title": "Dataset configuration", "content": "For fully supervised methods, a 60-20-20 data split is implemented, dividing the dataset into three distinct subsets: the train set, which includes data from House 1 to House 20; the validation set, consisting of data from House 21 to House 26; and the test set, comprising data from House 27 to House 32. Given the substantial size of the BelHouse3D dataset, preprocessing is performed in advance, with files organized into the respective train, validation, and test folders to streamline the training, validation, and evaluation processes.\nDuring the training of non-transformer models, a random subset of 2048 points is selected from each point cloud block within the train folder. Conversely, during the validation and testing phases, all available samples from the respective folders are employed to ensure a thorough evaluation of the model's performance. For training transformer models, Pointcept [57] is extended, utilizing sparse tensor representation, thereby enabling evaluation for transformer models."}, {"title": "4.2 Few-Shot 3D Segmentation in OOD Conditions", "content": "Few-shot 3D semantic segmentation represents a recent approach in computer vision, addressing the challenge of accurate segmentation with minimal labeled data. Unlike conventional segmentation tasks, which often require extensive training datasets, few-shot segmentation aims to generalize effectively from a limited number of annotated examples. This approach facilitates the rapid adaptation of models to new, previously unseen environments."}, {"title": "Dataset configuration", "content": "The data split for the few-shot setting deviates from its fully supervised counterpart due to the adoption of episodic training in few-"}, {"title": "5 Limitation And Future Work", "content": "The BelHouse3D dataset significantly enriches the pool of available 3D datasets for indoor scene segmentation by providing clean data and accurate ground truth annotations. However, the current dataset is limited in the number of object classes and point cloud attributes (XYZ only). Future work will focus on expanding the number of annotated classes, particularly for smaller household objects. Additionally, we aim to expand the dataset's scope to accommodate new tasks such as instance segmentation and scene completion, as well as more out-of-distribution (OOD) conditions, including variations in context (e.g., lighting, the connectedness of objects), object poses (e.g. pillow in various poses), and object shapes (e.g., different sofa models)."}, {"title": "6 Conclusion", "content": "In this paper, the BelHouse3D dataset is introduced as a novel synthetic 3D dataset designed for indoor scene segmentation. The dataset contains realistic and clean point clouds that accurately reflect real-world houses, aiding deep learning models in understanding the underlying geometry of indoor environments. BelHouse3D also includes a test set specifically designed to evaluate model robustness under out-of-distribution (OOD) conditions, particularly addressing the challenge of data occlusion, which is common in real-world point cloud scenes. A benchmark is established through the evaluation of popular point-based fully supervised and few-shot segmentation methods within this OOD framework. Experimental results demonstrate that few-shot learning (FSL) can effectively address OOD challenges with minimal labeled data, indicating a promising direction for future segmentation models to enhance OOD robustness by incorporating 3D object geometry to better manage occlusions."}]}