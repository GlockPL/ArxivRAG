{"title": "EVAL: EigenVector-based Average-reward Learning", "authors": ["Jacob Adamczyk", "Volodymyr Makarenko", "Stas Tiomkin", "Rahul V. Kulkarni"], "abstract": "In reinforcement learning, two objective functions have been developed extensively in the literature: discounted and averaged rewards. The generalization to an entropy-regularized setting has led to improved robustness and exploration for both of these objectives. Recently, the entropy-regularized average-reward problem was addressed using tools from large deviation theory in the tabular setting. This method has the advantage of linearity, providing access to both the optimal policy and average reward-rate through properties of a single matrix. In this paper, we extend that framework to more general settings by developing approaches based on function approximation by neural networks. This formulation reveals new theoretical insights into the relationship between different objectives used in RL. Additionally, we combine our algorithm with a posterior policy iteration scheme, showing how our approach can also solve the average-reward RL problem without entropy-regularization. Using classic control benchmarks, we experimentally find that our method compares favorably with other algorithms in terms of stability and rate of convergence.", "sections": [{"title": "Introduction", "content": "To solve the central problem of reinforcement learning, an agent continuously and autonomously interacts with its surroundings to maximize a long-term reward signal. The standard method of solving reinforcement learning (RL) tasks involves a discounted objective function: the agent seeks to maximize an infinite discounted sum of rewards, as expected under a chosen control policy. This geometrically discounted sum ensures a convergent objective function, making it a convenient representation of the problem to be solved. The discounted RL literature is vast in both theoretical and algorithmic studies and has demonstrated great success in real-world problems of interest. However, in many RL problems, this use of discounting is simply a useful proxy for the solving the true objective function: maximization of total episode reward. As a result, the actual choice of the discount factor, \u03b3, is unphysical, not grounded in any corresponding physical timescale. As such, it is treated as a hyperparameter which is often tuned over for best performance (or set to some fixed value, e.g. \u03b3 = 0.99, for simplicity). There is a precedent in past work for rejecting the discounted framework, for example the Heaven and Hell MDP , suggesting that in many tasks the use of discounting is not only unphysical, but can lead to disastrous outcomes. Thus in long-term (\"continuing\") decision-making settings, another objective is needed.\nAn alternative approach to ensuring convergence of rewards across infinitely-long trajectories is to instead use the average-reward objective function. Despite offering a principled alternative to the long-term optimization problem, the average reward framework has not been historically popular in the RL literature, perhaps due to the lack of successful algorithms in this setting. However, recent work has developed new algorithms and theory specifically for the average-reward objective. This prior work has focused on the tabular setting or on using policy-gradient techniques to develop new algorithms. Although these methods have proven useful in their respective domains, there remain gaps in the field that can be addressed with new approaches.\nOne such gap in the field of average-reward algorithms is the lack of entropy-regularized objectives, which have become a cornerstone of state-of-the-art algorithms in discounted RL . Formally, entropy-regularization involves including an \"information\" cost for deviating from a pre-specified (e.g. prior, guide, or behavioral) policy. This entropy cost is weighted with an inverse temperature parameter, \u03b2. This entropy-based regularization, in combination with treating the rewards as negative energies, reveals a close connection to statistical mechanics . Furthermore, including an entropy regularization term in the RL objective leads to more robust solutions (non-greedy optimal policies) and has theoretically been shown to be more adaptable to changes in reward and transition dynamics. Preventing the optimal policy from collapsing to a deterministic function can ensure additional exploration occurs during training and deployment, often leading to faster learning .\nIn this paper, we introduce a novel algorithm for average-reward RL with entropy regularization: EigenVector-based Average-reward Learning (EVAL). When the RL problem is posed in the discounted framework, a discount factor \u03b3 is a required input parameter. However, there is often no principled approach for choosing the value of \u03b3 corresponding to the specific problem being addressed. Thus, the experimenter must treat \u03b3 as a hyperparameter. This reduces the choice of \u03b3 to a trade-off between large values to capture long-term rewards and small values to capture computational efficiency which typically scales polynomially with the horizon, (1 \u2013 \u03b3)\u207b\u00b9 . The horizon introduces a natural timescale to the problem, but this timescale may not be well-aligned with the timescale corresponding to the optimal dynamics: the mixing time of the induced Markov chain. For the discounted approach to accurately estimate the optimal policy, the discounting timescale (horizon) must be larger than the mixing time. However, estimating the mixing time for the optimal dynamics can be challenging in the general case, even when the transition dynamics are known. Therefore, an arbitrary \"sufficiently large\" choice of \u03b3 is often made without knowledge of the relevant problem-dependent timescale. This can be problematic from a computational standpoint as evidenced by recent work.\nThe approach outlined in this work for average-reward RL (EVAL) has the added benefit that it can lead to an estimation of the mixing timescale for the optimal dynamics, which can then be used to inform the choice of a discount factor, if the discounted objective is of interest. We empirically find that this \"spectral gap discount factor\" naturally separates discount factors between \"small\" and \"large\" values, seen by the distinct change in the optimal state distributions. The discount factor set by the spectral gap indicates a point of diminishing return for further increasing \u03b3 in soft Q-Learning. For comparison, we plot the return given by our average-reward algorithm. Importantly, we see that the average-reward solution recovers the discounted solution in they \u2192 1 limit, as expected .\nDespite the desirable features of both the average-reward and entropy-regularized objectives, the combination of these formulations is not as well-studied, and no function approximator algorithms exist for this setting. Here, we extend the ideas introduced to develop a new off-policy learning-based approach to find the optimal policy of average-reward MDPs. Our main contributions are as follows:"}, {"title": "Main Contributions", "content": "\u2022 We provide a novel off-policy solution to average-reward RL in both entropy-regularized and un-regularized settings with general function approximators.\n\u2022 We demonstrate experimentally the advantage of our approach against standard baselines in classic control environments.\nNotably, our implementation requires minimal changes relative to the common DQN setup, making it accessible for researchers and allowing for multiple future extensions."}, {"title": "Preliminaries", "content": "Reinforcement learning treats decision-making problems under the framework of Markov Decision Processes (MDPs). In this section, we provide a brief account of the problem to be solved (the entropy-regularized average-reward) objective under this framework. As a point of notation, we will denote the set of distributions over a generic space X as \u0394(X). To begin, we describe the defining quantities of an MDP: the state space S denotes all possible configurations of the agent within its environment; the initial state distribution \u03bc \u2208 \u0394(S) describes the initialization of the agent at each episode; the action space A denotes the set of allowed controls the agent may exert to affect its environment; the transition function (also \"dynamics\") is a function"}, {"title": "Theory", "content": "For convenience, we provide the update equations from (Arriojas et al. 2023b) (Eq. 26, 27) which will be learned with EVAL:\n$\\u(s,a) \u2192 (1 \u2212 \u03b1)u(s, a) + \u03b1e^{\u03b2(r(s,a)-\u03b8)}u(s', a')$\n$\u03b5^{\u03b2\u03b8} \u2192 (1 \u2013 \u03b1\u03b8) \u03b5^{\u03b2\u03b8} + \u03b1\u03c1\u03c1\u03b2r(s,a) \\frac{u(s', a')}{u(s, a)}$\nwhich resembles our Eq. (15) and (16), respectively.\nAn important distinction between Eq. 26 in (Arriojas et al. 2023b) and Eq. (19) is that we have changed the sign of \u03b8 in their definition. This allows us to write the original objective in Eq. (5) in a way consistent with the average-reward literature (e.g. Eq. (2))."}, {"title": "Implementation", "content": "We implement our algorithm in PyTorch, and follow the style of Stable Baselines3 (Raffin et al. 2021). The function approximators for u(s, a) (and similarly for \u03c0\u03bf, discussed below) are MLPs with state as input, and |A| output heads. Below, we show the pseudocode for EVAL with the PPI method (a combination of Algorithm 1 and 2 in the main text), allowing one to solve the un-regularized RL objective. We highlight the differences from Algorithm 1 (main text) in blue. As mentioned in the main text, we require a parameterization for the (online and target) prior policy as well as an update frequency and rolling average weight for the Polyak update.\nWe highlight the use of Eq. (16) in this algorithm, as it involves an expectation operation taken over the prior policy. We take the target prior for this calculation, as opposed to the online prior network for the calculation of the optimal policy, as seen in the loss function of Eq. (17)."}, {"title": "Choice of Architecture", "content": "We note that since u(s, a) > 0 (by the Perron-Frobenius theorem), we require the u network outputs to be strictly positive and thus use a soft-plus activation function at the output layer. Although a ReLU activation is possible, we found it to give much worse performance across all tasks, despite additional hyperparameter tuning.\nWe similarly parameterize the prior net as an MLP (same hidden dimension as u-network), but with a softmax output. We have experimented with sharing weights between u and \u03c0\u03bf networks without finding significant performance gains, but this may be an interesting avenue for future exploration."}, {"title": "Additional Results", "content": "In light of several recent works maintaining multiple estimates or an entire ensemble of the agent's Q function, we consider the effect of learning more than two (online and target) networks on training performance. The results shown indicate that having greater than one or two networks can drastically improve performance in some environments, with diminishing returns as seen in prior work. Our experiments indicate that a minimum of two networks is required for learning the optimal policy, so we use two networks throughout for simplicity, despite this generally being a tunable parameter. For convenience, we use two independent target networks as well, to separately perform rolling averages of the target parameters before aggregating."}]}