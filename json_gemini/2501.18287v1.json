{"title": "Mining for Species, Locations, Habitats, and Ecosystems from Scientific Papers in Invasion Biology: A Large-Scale Exploratory Study with Large Language Models", "authors": ["Jennifer D'Souza", "Zachary Laubach", "Tarek Al Mustafa", "Sina Zarrie\u00df", "Robert Fr\u00fchst\u00fcckl", "Phyllis Illari"], "abstract": "This paper presents an exploratory study that harnesses the capabilities of large language models (LLMs) to mine key ecological entities from invasion biology literature. Specifically, we focus on extracting species names, their locations, associated habitats, and ecosystems, information that is critical for understanding species spread, predicting future invasions, and informing conservation efforts. Traditional text mining approaches often struggle with the complexity of ecological terminology and the subtle linguistic patterns found in these texts. By applying general-purpose LLMs without domain-specific fine-tuning, we uncover both the promise and limitations of using these models for ecological entity extraction. In doing so, this study lays the groundwork for more advanced, automated knowledge extraction tools that can aid researchers and practitioners in understanding and managing biological invasions.", "sections": [{"title": "1 Introduction", "content": "Human population growth and expansion are coupled with the intentional and unintentional movement of species beyond their historic ranges. The introduction of nonnative species can have dramatic impacts that cascade across ecological scales from individual plants and animals to populations and communities (Roy et al., 2023). The goal of invasion biology is to identify and understand the impacts of alien species on native flora and fauna across these ecological scales, not only to conserve rare, sensitive, and ecologically valuable native species, but also because intact functional ecosystems provide important economic, public health, and socioemotional services to humans (Cassey et al., 2018; Jeschke and Heger, 2018). Achieving this goal becomes increasingly challenging though, since alien species introductions operate at a rapid rate and global scale, and in the context of increasing human population growth and climate change. Additionally, the body of research in this domain of ecology is currently growing to an extent that it becomes more and more difficult for invasion biology researchers to build a systematic and shared understanding of the impacts of alien species on ecosystems, or to even comprehensively categorize these species, and their locations, and relationships. This paper explores the potential of recent NLP technologies, such as Information Extraction (IE) approaches based on Large Language Models (LLMs) (Amatriain et al., 2023; D'Souza, 2023), as a tool that could contribute to predicting future invasions and their consequences.\nThe extraction and categorization of information from scientific publications is a well-known NLP task (Augenstein et al., 2017; G\u00e1bor et al., 2018; Luan et al., 2018; Brack et al., 2020; Dess\u00ec et al., 2020; D'Souza et al., 2021; Liu et al., 2021; Kabongo et al., 2021; D'Souza and Auer, 2022; D'Souza, 2024; Shamsabadi et al., 2024; D'Souza et al., 2024). In the biomedical domain, NER and RE facilitate large-scale biomedical data analysis, such as network biology (Zhou et al., 2014), gene prioritization (Aerts et al., 2006), drug repositioning (Wang and Zhang, 2013) and the creation of curated databases (Li et al., 2015). In the clinical domain, NER and RE can aid in disease and treatment prediction, readmission prediction, deidentification, and patient cohort identification (Miotto et al., 2018). However, the domain of ecology and invasion biology in particular has hardly been explored and dedicated datasets are scarce. To the best of our knowledge, the small-scale INAS dataset (Brinner et al., 2022) presents the only invasion biology-specific resource that provides annotations of hypotheses for scientific abstracts."}, {"title": "2 Our Text Data Mining Corpus", "content": "Before tackling the IE task, we compiled a publication corpus as the unstructured source for scientific information. This section outlines the preparation of this dataset for text mining and IE."}, {"title": "2.1 Defining the Collection", "content": "We started with the Invasion Biology Corpus (Mietchen et al., 2024), which lists metadata for 49,438 invasion biology papers in Wikidata. Using the papers' DOIs, we queried the ORKG ASK search engine's API to retrieve abstracts and full texts. ASK was chosen for its convenient access to over eight million publications (Knoth et al., 2023) across diverse scientific fields."}, {"title": "2.2 Corpus Statistics", "content": "Of the 49,438 DOIs queried, 12,636 were available in the ASK database. Among these, 9,802 had abstracts but not full text, and 2,834 included both. This distribution reflects a common issue: open-access full-text articles are limited, posing a challenge for scientific NLP efforts. As a result, our final dataset\u2014consisting of a larger set of abstracts and a smaller set of full texts-serves as the target collection for text mining and IE.\nThe abstracts range from 10 to 1,608 tokens (average: 235), while the full texts span 28 to 123,958 tokens (average: 7,667), highlighting the greater informational depth of full texts despite their limited availability."}, {"title": "2.3 Bibliometric Analysis", "content": "Before the IE tasks, we highlight two key bibliometric insights from our corpus. Among 12,636 papers with abstracts, publication years span 52 years, starting in 1950. Full-text availability begins in 1990."}, {"title": "3 Information Extraction (IE) with Large Language Models (LLMs)", "content": "An IE task entails two prerequisites. 1) The collection of papers on the which the IE task should be performed. And 2) the schema which defines the IE extraction targets of interest. Having compiled our publication corpus above, we proceed with IE."}, {"title": "3.1 Schema Discovery", "content": "Schema or semantic model discovery is a key focus of this subsection. We aim to use LLMs to propose a standard semantic structure for extracting information from each paper. This standardized IE format facilitates downstream processing of the structured data. Since no predefined set of relations exists, the schema must flexibly capture extracted entities and represent their relationships.\nWe approach the task in two main stages: specialize and generalize. In the specialize stage, the LLM is instructed to generate a schema as the target for IE, focusing on four entities: species, location, habitat, and ecosystem. The schema proposed by the LLM includes an IE objective with properties that specify relationships between the extracted entities. This behavior is guided by a system prompt, and the LLM processes each paper instance individually through user prompts to define a semantic model tailored or specialized to each paper. In the generalize stage, the LLM is instructed to generate a generalized schema based on input of the specialized schemas created per paper in the first stage. The generalized schema would then represent the most flexible way to capture the relations between the entities across all papers. The following sections provide detailed descriptions of each stage."}, {"title": "3.1.1 Stage Specialize: Schemas per Paper", "content": "The LLM is the central tool in this endeavor, operating in the text completion or generation mode. The interaction with the LLM is guided by two types of prompts: the SYSTEM PROMPT, which defines the model's persona and specialized behavior for a given task, and the USER PROMPT, which supplies input data for the defined behavior. The LLM's behavior, once established by the system prompt, remains consistent throughout the interaction session per stage.\nSYSTEM PROMPT. The system prompt defines the LLM's persona and provides structured guidance for the task. It is composed of three key components: (a) Role, (b) Task instruction, and (c) Output format. In this stage, the LLM assumes the overarching role of a \u201cresearch assistant in invasion biology or ecology tasked with reading and understanding scientific papers for extracting relevant information within a schema.\" Its primary instruction is to extract terms related to four entities-species, location, habitat, and ecosystem-and identify the relationships expressed in each paper. Additionally, the LLM is directed to structure the extracted information into an external container, representing the schema that defines the extraction target.\nTo refine the LLM's performance, the system prompt was iteratively improved over two runs. In the first run, the prompt lacked detailed definitions of the entities. After discussions with an ecologist, the second run included precise definitions of the field of invasion biology and the four entities (see Table 1). The final system prompt, incorporates these definitions, enhancing the LLM's clarity and consistency in identifying and extracting the specified entities and their relationships.\nThis refinement aligns with the principles of in-context learning (Radford et al., 2019), which emphasize that providing clear and detailed task instructions ensures the model's better comprehension of the downstream task, leading to more accurate and consistent results. By supplying the LLM with a well-defined context, we enable it to successfully extract and organize the desired information from scientific texts.\nUSER PROMPT. The model is given one paper at a time and is allowed to suggest its own semantic model. The instruction in the user prompt is simple and is as follows: \u201cExtract the information as instructed from this article title and abstract. \\n\\n Title: {title}\\n Abstract: {abstract}\\n\u201d.\nResult. Ten papers were randomly selected, and the LLM was prompted to generate a schema for each, which was stored and is available in the data folder of our online repository. The tenth paper, an outlier from the Wikidata Invasion Biology dataset, highlights the potential presence of false positives-papers unrelated to invasion biology.\nThe schemas evolve significantly to better capture relationships across the nine true-positive papers."}, {"title": "3.1.2 Stage Generalize: Generic Schema", "content": "In this stage, the goal was to develop a standardized container for the extracted information in the form of a JSON data structure, specifically designed to capture the relationships between four entities: species, location, habitat, and ecosystem. The system prompt given to the LLM was similarly structured as the specialize stage, with the model's role defined not only as a research assistant in invasion biology but also as having expertise in semantic modeling. Overall, the prompt consisted of the same structural components: (a) Role, (b) Task instruction, and (c) Output format. However, aside from the role, the task instruction defining the LLM's behavior in this stage was different. At this stage, the LLM was expected to review each individual schema from the earlier stage for every paper and then thoughtfully propose a standardized schema. This is related to prior work (Baazizi et al., 2017, 2020) which presents an approach based on discovering a schema for individual JSON documents and then merging each of these schemas. In our case, we discover schemas from the unstructured texts of individual papers and then merge them in this stage. This schema would eventually form the IE objective, to be applied across all papers in the collection to extract the relevant entities and their relationships.\nSince LLMs are known to generate different responses to the same prompt, the LLM was prompted thrice with the same task each time prompting the model in the same way: \u201cRead the nine different schema instances and generate a standardized schema in the JSON output format.\u201d\nResult. The three JSON schema variants generated represented a thoughtful approach to standardizing the representation of entities and their relationships in invasion biology. The common thread across the 3 standardized schema responses"}, {"title": "3.2 Information Extraction", "content": "With a standardized semantic structure for extracting information from each paper, enabling easier downstream processing, the LLM-based IE task was conducted."}, {"title": "3.2.1 Stage Extract: Populate Schema", "content": "This stage now fulfills the main objective of this work, i.e. to extract information from a large-scale corpus (12,636 in our case) with an LLM to mine species, location, habitat, and ecosystem entities and their relations. The system prompt in this stage was close to the specialize stage system prompt where the role specified for the LLM was \"research assistant in invasion biology or ecology tasked with reading and understanding scientific papers to extract relevant information per the given predefined schema.\""}, {"title": "3.3 Technical Details", "content": "The proprietary OpenAI GPT-40 model was used for all tasks in this paper. Schema generation tasks in the specialize (Section 3.1.1) and generalize"}, {"title": "3.4 Results and Discussion", "content": "Of the 12,636 papers in our dataset, the LLM classified 1,740 as outside the scope of invasion biology, responding with \u201cN/A\u201d and skipping extraction for these papers. This left 10,896 papers as the target for IE. This section summarizes the results.\nThe first exploratory insight highlights the diverse roles of species extracted by the LLM, reflecting their origins, behaviors, ecological functions, and impacts within invasion biology. Broad categories include native, alien, introduced, invasive, and naturalized, alongside more specific roles such as agricultural weeds, biological control agents, pathogens, mutualists, and ecosystem engineers. Some roles emphasize species' origins (e.g., indigenous, non-native, cryptogenic), while others highlight their behaviors (e.g., invasive, colonizer, expanding) or ecological functions (e.g., symbiont, facilitator, pioneer). Additionally, certain roles capture species' interactions within ecosystems, such as co-introduced species, specialist herbivores, or cryptic invaders, while others underline their relevance to conservation and management, such as natural enemies, candidate biological control agents, or quarantine pests. Together, this spectrum of roles underscores the complexity of species dynamics in invasion biology, providing valuable insights into biodiversity patterns, ecosystem impacts, and strategies for conservation and management.\nWe conducted a finer-grained exploration of the extracted data, focusing on the most prevalent species names within the roles of invasive, native, and introduced species. Invasive species dominated, with examples such as Procambarus clarkii (76 mentions), Harmonia axyridis (73 mentions), and Rhinella marina (68 mentions) highlighting well-documented invaders. Native species, though less frequent, included specific examples like Austropotamobius pallipes and Phragmites australis (24 mentions each). Introduced species, while fewer, included Oncorhynchus mykiss and Crassostrea gigas, reflecting their dual perspectives of their ecological integration and potential invasiveness. Overall, the data underscores the breadth of roles species play in invasion biology, from invaders disrupting ecosystems to native species requiring conservation focus and introduced species with varying impacts. However, the extraction also included generic terms (e.g., \"native species\" and \"native plants\") as species names, indicating noise from the unsupervised nature of the IE task and highlighting the need for post-filtering to refine the analysis.\nThe dataset highlights key geopolitical locations, with the five most frequent countries being Australia (406), South Africa (248), New Zealand (236), Italy (187), and France (168). Prominent regions include Europe (601), North America (348), the Mediterranean Sea (117), Asia (112), South America (98), the Mediterranean (80), and the Iberian Peninsula (75). Less common mentions were cities like Sydney (8), Hong Kong (7), Rome (6), Cape Town (6), and Brisbane (5). The strong representation of Europe and North America reflects their prominence in the data, while the frequent mentions of Australia, South Africa, and New Zealand suggest a focus on biodiversity hotspots or ecological studies. Overall, the dataset spans a diverse range of geographical entities, from continents and regions to countries and cities, emphasizing a global perspective with notable references to the Mediterranean, Asia, and locations such as California and Queensland.\nFurthermore, the extracted information provides a comprehensive view of terrestrial, marine, and aquatic ecosystems, reflecting their ecological and scientific significance. Terrestrial ecosystems (93) are the most frequently mentioned, with grasslands (42), forests (45), and agricultural landscapes (47) prominently featured, emphasizing biodiversity and land use. Mediterranean ecosystems (37) and tropical ecosystems (26) highlight climate-specific regions, while environments such as riparian (14), alpine (8), and sub-Antarctic (8) systems underscore specialized ecological contexts. Urban ecosystems (46) also feature prominently, reflecting the interplay between natural and human-modified systems. Among ma-"}, {"title": "4 Recommendations for Future Work", "content": "The semantic web community presents technologies and concepts that may supply direction to the future work on this topic. Ontologies have been a focal point of research in the domain for decades, however, the expertise required for their use has presented a barrier to outside establishment. With the rise in prominence of LLMs in both research and application, a new branch of research has emerged with the goal to investigate the interplay of ontologies and LLMs for more precise IE and to facilitate creating linked data. This branch currently covers the following topics: How can LLMs support ontology and knowledge graph construction? (Kommineni et al., 2024) How can LLMs for question answering be improved through ontology support? (Allemang and Sequeda, 2024) How can LLMs support ontology learning from texts? (Babaei Giglou et al., 2023, 2024) How can representation learning of LLMs be enhanced by ontologies? (Ronzano and Nanavati, 2024)\nThe concept of ontologies, defined as a formal, explicit specification of a shared conceptualisation (Studer et al., 1998), highlights their role in enabling a shared understanding of a domain and reflecting consensus about domain knowledge. This study builds on these principles, exploring how information can be extracted from unstructured texts and structured using semantic models. Future work can extend this approach by integrating ontological knowledge through schema-driven IE, addressing key questions: What information should be provided to LLMs to improve extraction, and where in the workflow is it most effective? How can LLMs be precisely guided (Caufield et al., 2024) toward achieving semantic modeling objectives? Finally, how does shared human consensus in a domain align with the consensus extracted by LLMs trained on human knowledge? Exploring these questions in future work can bridge the gap between human and machine understanding of domain knowledge.\nWe identify several applications of ontologies in IE workflows to enhance LLM performance. Incorporating domain-specific ontologies during training could improve understanding of domain terms by supplying definitions, properties, and hierarchical relations, aligning the model's comprehension with that of domain experts. Ontologies can also guide semantic modeling by constraining or informing LLMs, improving interoperability and making outputs more actionable. For entity recognition (ER/NER) and relation extraction (RE), pre-informing models about common domain-specific terms and relationships may enhance accuracy by effectively teaching the model \"what to look for.\" Additionally, retrieval-augmented generation (RAG) integrates knowledge bases, such as ontologies or knowledge graphs, into QA systems, reducing hallucinations and improving performance, as demonstrated in domains like biomedicine (Soman et al., 2024).\nFinally, these advantages need to be balanced with better understanding of what such constraints on LLMs might mean for their use in domains which have less well-structured general knowledge, perhaps because they are still emerging, or perhaps because the knowledge in those domains is changing rapidly. In such cases, there will be a need for further work to understand whether and to what extent using ontologies to constrain LLMs may also constrain our ability to track emerging or rapidly changing knowledge."}, {"title": "5 Conclusion", "content": "This study demonstrates the potential of LLMs for advancing IE in invasion biology by mining key entities such as species, locations, habitats, and ecosystems from scientific literature. By developing and applying a standardized semantic schema, we showcased the ability of LLMs to structure complex ecological data, providing a foundation for enhancing workflows in ecological research. Using a two-stage approach, the specialize stage extracts detailed, context-specific structures, while the generalize stage integrates these into a flexible schema balancing specificity and generality. This method addresses the complexity of ecological systems, enabling structured representation of nuanced information. The release of the dataset and schema enable refining extraction methods, integrating ontologies, and exploring broader ecological applications. This research underscores the utility of LLMs as tools for bridging unstructured data and structured knowledge in ecology."}]}