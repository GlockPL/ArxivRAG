{"title": "A STUDY IN DATASET DISTILLATION FOR IMAGE SUPER-RESOLUTION", "authors": ["Tobias Dietz", "Brian B. Moser", "Tobias Nauen", "Federico Raue", "Stanislav Frolov", "Andreas Dengel"], "abstract": "Dataset distillation is the concept of condensing large datasets into smaller but highly representative synthetic samples. While previous research has primarily focused on image classifica-tion, its application to image Super-Resolution (SR) remains underexplored. This exploratory work studies multiple dataset distillation techniques applied to SR, including pixel- and latent-space approaches under different aspects. Our experi-ments demonstrate that a 91.12% dataset size reduction can be achieved while maintaining comparable SR performance to the full dataset. We further analyze initialization strategies and distillation methods to optimize memory efficiency and com-putational costs. Our findings provide new insights into dataset distillation for SR and set the stage for future advancements.", "sections": [{"title": "1. INTRODUCTION", "content": "High-Resolution (HR) image reconstruction from Low-Resolution (LR) inputs, known as Super-Resolution (SR), plays a crucial role in applications such as medical imaging, surveillance, and satellite imagery [1, 2, 3]. However, train-ing SR models requires large-scale, high-quality datasets, leading to significant storage and computational overhead [4]. This challenge has driven interest in dataset distillation a technique that aims to synthesize compact datasets while preserving model performance [5, 6, 7].\nWhile dataset distillation has been extensively explored in classification tasks, its potential for SR remains largely unex-plored, i.e., previous works focus on using synthetic images rather than distilling them explicitly for SR [8, 9, 10]. Unlike classification, where distilled samples can be interpreted as condensed representations of class features, SR distillation must retain fine-grained spatial details necessary for high-quality image reconstruction. This distinction raises funda-mental questions about how synthetic datasets can effectively capture the necessary information for training SR models.\nThis work provides an in-depth, exploratory investigation of dataset distillation in SR, comparing both pixel- and latent-space approaches to significantly reduce training data with-out compromising reconstruction quality. We analyze the impact of initialization and optimization strategies, examine the trade-offs between dataset compression and image fidelity, and demonstrate that a carefully designed distillation process can achieve substantial data reduction while maintaining com-petitive SR performance. By bridging the gap between dataset distillation and SR, this study lays the initial groundwork for future research in memory-efficient SR model training."}, {"title": "2. BACKGROUND", "content": null}, {"title": "2.1. Image SR", "content": "A trained Super-Resolution (SR) model \\(\\mathcal{V}_\\theta : \\mathbb{R}^{H \\times W \\times C} \\rightarrow \\mathbb{R}^{sH \\times sW \\times C}\\) is optimized to approximate the inverse of a degradation process that maps a High-Resolution (HR) im-age \\(y \\in \\mathbb{R}^{sH \\times sW \\times C}\\) to its corresponding Low-Resolution (LR) counterpart \\(x \\in \\mathbb{R}^{H \\times W \\times C}\\), where \\(s > 1\\) represents the scaling factor [1]. The model parameters \\(\\theta\\) are optimized using a dataset \\(\\mathcal{T} = (X_r, Y_r)\\), a collection of LR-HR image pairs. The objective function for training the SR model is\n\\begin{equation}\n\\theta^* = \\arg \\min_\\theta \\mathbb{E}_{x_i \\in X_r, y_i \\in Y_r} ||\\mathcal{V}_\\theta(x_i) - y_i||^2.\n\\end{equation}"}, {"title": "2.2. Dataset Distillation", "content": "The goal of dataset distillation is to compress a real dataset \\(\\mathcal{T} = (X, Y)\\), where \\(X_i \\in \\mathbb{R}^{N \\times H \\times W \\times C}\\) and \\(N\\) is the num-ber of samples, into a significantly smaller synthetic dataset \\(\\mathcal{S} = (X', Y')\\) with \\(X'_i \\in \\mathbb{R}^{M \\times H \\times W \\times C}\\) and \\(M \\ll N\\). How-ever, the goal is not only to reduce the dataset size but also to retain the essential training quality of \\(\\mathcal{T}\\) [6]. In this setting, \\(M\\) is defined as \\(M = C \\cdot IPC\\), where \\(C\\) is the number of classes and \\(IPC\\) represents the Images Per Classes (IPC). The overall optimization for dataset distillation is formulated as\n\\begin{equation}\n\\mathcal{S}^* = \\arg \\min_\\mathcal{S} \\mathcal{L}(\\mathcal{S}, \\mathcal{T}),\n\\end{equation}\nwhere \\(\\mathcal{L}\\) is a predefined objective for dataset distillation. One objective is Dataset Condensation (DC), which aligns the gra-dients of classification errors [11]. It computes the loss on real data (\\(l_\\mathcal{T}\\)) and the corresponding synthetic data (\\(l_\\mathcal{S}\\)), then mini-mizes the discrepancy between the gradients of both networks:\n\\begin{equation}\n\\mathcal{L}_{DC} = 1 - \\frac{\\langle \\nabla_{\\theta}l_{\\mathcal{S}}(\\theta), \\nabla_{\\theta}l_{\\mathcal{T}}(\\theta) \\rangle}{\\|\\nabla_{\\theta}l_{\\mathcal{S}}(\\theta) \\| \\|\\nabla_{\\theta}l_{\\mathcal{T}}(\\theta) \\|}.\n\\end{equation}"}, {"title": "3. METHODOLOGY", "content": "The first challenge is to adapt the loss function to the domain of image SR. For our study, we propose to modify DC, as outlined in Fig. 1: Instead of using the classification error, we use the classical SR \\(L_2\\) loss term (MSE). More concretely,\n\\begin{equation}\n\\mathcal{L}_{MSE} = \\frac{1}{n} \\sum_{i=1}^n ||\\mathcal{V}_\\theta(x'_i) - y'_i||^2,\n\\end{equation}\nwhere \\(\\mathcal{V}_\\theta\\) is an arbitrary reference SR model."}, {"title": "3.1. Adapting Class Labels for SR", "content": "With the adapted loss function in place, another key challenge is adapting the class labels for image SR (i.e., the distillation targets). In classification-based distillation, class labels are not only used to compute loss terms but also to structure the data, determining how samples are grouped and synthesized. This dependency complicates direct application to image SR, where a class-based organization is less well-defined.\nIn this work, we propose to exploit the standard procedure of extracting sub-images of the respective images as part of the traditional generation of training data, as shown in Fig. 2. Con-sequently, we group each of these sub-images with a label cor-responding to the large original image. All of the sub-images 1) have the same dimensions and 2) evidently belong to the same \"class\". We can, therefore, group multiple sub-images based on these labels and distill one synthetic sub-image with high information density, which allows for distilling one large real image into a smaller synthetic one.\nIn conclusion, we distill 800 synthetic images from roughly 32.000 sub-images, which results in 91.12% dataset size re-duction."}, {"title": "3.2. Latent Distillation for SR", "content": "In addition to pixel-space distillation, we investigate the use of latent dataset distillation with Generative Latent Distillation (GLaD) [6]. Instead of optimizing synthetic images directly in pixel space, GLaD operates in the latent space of a genera-tive model (e.g., StyleGAN-XL [12]). This approach enables distillation at a higher level of abstraction, capturing complex image features while reducing the need for direct pixel-wise op-timization. Specifically, GLaD learns latent codes that, when passed through the generator, reconstruct high-fidelity syn-thetic images. This method is particularly beneficial for SR, as it allows the distillation of large-scale image structures while maintaining fine-grained details."}, {"title": "4. EXPERIMENTS", "content": null}, {"title": "4.1. Setup", "content": "For the reference SR model \\(\\mathcal{V}_\\theta\\) used during dataset distillation, we chose the lightweight SRCNN [13] model (randomly ini-tialized if not stated otherwise). As a baseline comparison, we test the SR models SRCNN [13], VDSR [14], and EDSR [15], trained on the distilled images, also on the full dataset. For testing, we used the classical datasets Set5, Set14, and DIV2K (validation set) [1]."}, {"title": "4.2. Pixel-Space Results", "content": "Initialization Matters. As shown in Tab. 1, starting the dis-tillation process with random noise images fails to produce meaningful training images for SR models. As a result, train-ing on them leads to substantial drops in PSNR/SSIM scores across all datasets. Notably, with IPC=1, the performance degrades severely, with PSNR dropping by more than 15 dB compared to the baseline. Even with IPC=10, the improvement remains marginal, indicating that naive random initialization is insufficient for learning effective SR training images.\nInitialize With Downscaled Images. By downscaling the original images for initialization, we provide a more struc-tured starting point for dataset distillation. As shown in Tab. 2, models trained on downscaled images achieve performance closer to the original dataset than randomly initialized syn-thetic datasets. This suggests that random initialization leads to suboptimal local minima, limiting the effectiveness of the distilled dataset. Interestingly, while downscaled images alone yield reasonable results, their distilled counterparts do not outperform this baseline. This highlights the need for more refined distillation methods to leverage downscaled initializa-tions better while preserving high-frequency details.\nPre-Trained Network During Distillation. To better capture the underlying data distribution, we explore using a pre-trained SR model instead of a randomly initialized network during dataset distillation. The rationale is that a pre-trained model, having already learned relevant image structures and features, may lead to more informative synthetic images. However, as shown in Tab. 3, the results are mixed. While the synthetic dataset distilled with a pre-trained model achieves higher PSNR, better SSIM and LPIPS values are scored with a randomly initialized model. These findings indicate that using a pre-trained model does not consistently outperform a randomly initialized model regarding final SR quality. De-spite the ambiguous results, further experiments incorporate a pre-trained SRCNN to investigate its long-term advantages in distillation."}, {"title": "4.3. Latent-Space Results", "content": "In this experiment, we apply latent distillation using a gen-erative model, requiring a different setup from previous ex-periments. Unlike pixel-based distillation, which operates on 192x192 patches, latent distillation is constrained by the reso-lution of the generative model. Here, we use StyleGAN-XL [12] to generate 512x512 synthetic sub-images, shifting the objective from learning synthetic patches to learning optimal synthetic latent codes [5, 6]."}, {"title": "4.4. Qualitative Results", "content": "Fig. 3 illustrates how latent distillation refines synthetic sam-ples over time. In the fourth row, we observe a clear shift from initialization to the first synthetic sample, likely due to the generative model and intermediate latent space. Subsequent iterations show only minor changes, indicating faster conver-gence compared to prior experiments - suggesting fewer steps may suffice for comparable performance. In contrast, pixel-space experiments (first three rows) exhibit no such trend and given their inferior performance, increasing distillation steps would be ineffective. Notably, artifacts in pixel-space distil-lation emerge gradually, reinforcing that they are an inherent byproduct of the process.\nFor the qualitative evaluation, we compare the upscaled im-ages generated by EDSR models trained on different datasets. The results for pixel-space distillation with random initial-ization are omitted, as the model fails to reconstruct color accurately, making it unsuitable for SR. We focus on all other synthetic datasets, along with the downscaled baseline, and compare their outputs to ground truth HR images and an EDSR model trained on real data. The models were trained for 4x upscaling, as 2\\(\\times\\) showed negligible differences. As illustrated in Fig. 4, even the EDSR model trained on real data struggles to recover fine textures and details from the HR images fully. Nonetheless, models trained on synthetic datasets achieve vi-sually comparable results. While minor differences can be spotted upon closer inspection, no single distillation approach clearly outperforms the others based solely on qualitative eval-uation."}, {"title": "5. LIMITATION & FUTURE WORK", "content": "While the proposed method eliminates the need for labeled datasets, it still relies on class-based grouping to associate real and synthetic samples. This dependency arises from the design of most existing dataset distillation approaches, which opti-mize synthetic images per class rather than jointly across all samples. While this class-wise optimization improves compu-tational efficiency and facilitates feature aggregation in classi-fication tasks, it is not inherently meaningful for SR, where the focus is on reconstructing fine details rather than class-specific attributes. Future research should explore class-agnostic dis-tillation methods that remain computationally efficient while generating diverse and representative synthetic datasets for SR. Additionally, the impact of using class-unconditional genera-tive models on distillation performance remains an open ques-tion and warrants further exploration. Previous work suggests that other generative models, including alternative uncondi-tional GANs and Latent Diffusion Models (LDMs), can be leveraged for dataset distillation [5]."}, {"title": "6. CONCLUSION", "content": "This exploratory work explores the adaptation of dataset distil-lation for image SR, demonstrating that synthetic datasets can effectively train SR models while achieving performance com-parable to those trained on real data. By leveraging generative models for latent-space distillation via DC, we successfully dis-till the DIV2K dataset into a significantly smaller yet effective training set. Our results highlight the limitations of pixel-space distillation, while latent-space distillation enables a remark-able 91.12% dataset reduction without compromising quality. Beyond reducing storage and computational demands, this study provides new insights into different aspects of dataset distillation for SR."}]}