{"title": "Data-Free Model-Related Attacks: Unleashing the Potential of Generative AI", "authors": ["Dayong Ye", "Tianqing Zhu", "Shang Wang", "Bo Liu", "Leo Yu Zhang", "Wanlei Zhou", "Yang Zhang"], "abstract": "Generative AI technology has become increasingly integrated into our daily lives, offering powerful capabilities to enhance productivity. However, these same capabilities can be exploited by adversaries for malicious purposes. While existing research on adversarial applications of generative AI predominantly focuses on cyberattacks, less attention has been given to attacks targeting deep learning models. In this paper, we introduce the use of generative AI for facilitating model-related attacks, including model extraction, membership inference, and model inversion. Our study reveals that adversaries can launch a variety of model-related attacks against both image and text models in a data-free and black-box manner, achieving comparable performance to baseline methods that have access to the target models' training data and parameters in a white-box manner. This research serves as an important early warning to the community about the potential risks associated with generative AI-powered attacks on deep learning models. The source code is provided at: https://zenodo.org/records/14737003.", "sections": [{"title": "1 Introduction", "content": "Generative AI has demonstrated its potent capabilities in both image and language processing recently [11,38,39]. Along with their broad availability, concerns regarding the privacy and security implications associated with their usage have also emerged [16]. These concerns can be roughly classified into two categories [54]: 1) offensive applications, involving the use of generative AI for malicious purposes [7]; and 2) potential vulnerabilities, referring to weaknesses that can be exploited to compromise a generative model's own privacy, such as extracting their training data [6, 30]. Among these two categories, current research primarily focuses on the second category, while the first category, i.e., the offensive applications of generative AI, has received limited attention.\nCurrent research on the offensive applications of generative AI primarily targets cyberattacks, such as FraudGPT [22]. However, the potential of generative AI to facilitate model-related attacks, such as model extraction [37], membership inference [47, 55], and model inversion attacks [53, 56, 57], remains largely underexplored. While model-related attacks are well-documented in the context of existing machine learning models, they typically operate under the assumption that the adversary has access to a dataset that shares the same or similar distribution as the target model's training set [8,25,37]. However, this assumption is often impractical in real-world scenarios due to various constraints such as the limited capability of the adversary to access proprietary datasets. While some studies have explored attacks in a data-free manner, they often depend on having white-box access to the target model, meaning the adversary possesses detailed knowledge of the model's parameters and architecture [32]. Alternatively, they lacking such access may experience a decrease in attack performance [33]. Another research trend is using large language models for prompt stealing, wherein prompts are reconstructed based on the corresponding responses [45, 52]. Our research differs significantly in two key aspects. Firstly, our study delves into various model-related attacks, while prompt stealing research primarily focuses on this singular objective. Secondly, our approach is conducted in a data-free manner, whereas prompt stealing methodologies typically rely on a substantial amount of externally collected data.\nIn this paper, we embark on a pioneering exploration of the offensive applications of generative AI. The novelty of this work lies in leveraging generative AI models to conduct various model-related attacks across both image and text domains in a data-free and black-box manner. Unlike existing research that typically relies on external datasets to launch attacks, generative models eliminate the need for such data by directly generating high-quality synthetic data. This significantly lowers the barrier for executing model-related attacks. The significance of this work, therefore, lies in exposing the potential security risks associated with the misuse of generative AI models. However, soliciting the generative model to generate effective data poses challenges due to two reasons.\n\u2022 Generating data that meets the diverse requirements of various model-related attacks and comprehensively covers the sample space, while preserving essential characteristics, poses a challenge. Depending on the nature of the attack, the generated samples must exhibit specific characteristics tailored to the attack's objectives. For instance, in membership inference attacks, data samples proximate to decision boundaries are crucial, while in model inversion attacks, samples representing distinct classes are necessary.\n\u2022 Mitigating the distribution shift between the target model's training data and the generated data presents a challenge. This shift can arise from the inherent randomness in the generation process of generative models, potentially leading to discrepancies in data characteristics. Such discrepancies can degrade the performance of the generated data when employed in tasks like model-related attacks, where precise data characteristics are crucial for effectiveness.\nTo tackle these challenges, we present a novel data generation approach. Leveraging insights into the target model's task, the adversary meticulously designs prompts to direct the generative model in generating the required data. Subsequently, the adversary employs data augmentation techniques to diversify the generated samples. In particular, the augmentation process involves exploring the decision boundary of the target model and collecting samples in its vicinity. To further address the distribution shift, we introduce an inter-class filtering approach. This approach filters out anomalous samples by comparing the distances of generated samples to the class centroids in the feature space defined by the target model's outputs. In summary, this work makes four contributions.\n\u2022 This paper identifies a significant vulnerability in deep learning models, namely their susceptibility to model-related attacks enabled by generative AI. It offers the first comprehensive and generalized study exploring the potential of generative AI to execute model-related attacks across three prevalent types, all without using externally collected data.\n\u2022 We propose a novel data generation approach utilitizing the potent capabilities of generative AI. This approach is tailored to generate near-boundary samples, effectively spanning the entirety of the sample space.\n\u2022 We introduce an innovative inter-class filtering approach designed to mitigate the distribution shift between the target model's training data and the generated data. This method significantly enhances the quality and usability of generated datasets for model-related attacks.\n\u2022 We undertake comprehensive experiments to assess the efficacy of our method. Unlike the majority of studies in model-related attacks that solely rely on image data, our evaluation encompasses both image and text data."}, {"title": "2 Preliminary and Threat Model", "content": "Image-based Generative Models. This work adopts diffusion models [51] for its generative tasks due to its superior performance compared to VAEs [27] and GANS [12].\nA diffusion model is typically characterized by a forward process that introduces noise to data and a reverse process that reverts noise back to data. The forward process serves to convert any data distribution into a straightforward prior distribution, such as a standard Gaussian, while the reverse process involves learning transition kernels parameterized by deep neural networks to invert the forward process. Hence, new data samples can be generated by initially sampling a random vector from the prior distribution and then performing ancestral sampling through the reverse process.\nLanguage-based Generative Models. One of the most influential language-based generative models is large language models (LLMs) exemplified by GPT-4 [39]. A large language model, denoted as LLM, functions as a transformative tool in natural language understanding and generation. It takes a text sample x, often referred to as a 'prompt', as input and generates another text sample y, typically referred to as a 'response', i.e., $y = LLM(x)$. Text samples processed by LLMs are represented as sequences of tokens, where $x = [x_1,x_2,...,x_s]$, and s represents the number of tokens in x. These tokens are discrete units of language, which could be words, subwords, or even characters, depending on the tokenization scheme used.\nThreat Model. Given a target model T, the adversary has knowledge about the task for which the model T is designed, i.e., the adversary knows the meanings associated with each class c of the target model T. This assumption is reasonable, as model providers typically make this information public whenever the model is used to offer services to the public.\nThe adversary interacts with T in a black-box manner, meaning they can query T and observe its output confidence vectors, but they lack access to T's parameters and architecture. Note that our method can also be adapted to scenarios where the adversary can only observe the output hard labels of T, without access to confidence scores.\nThe adversary is capable of locating a black-box generative model F designed to perform a task closely aligned with that of the target model. For instance, in the case where the target model serves as an image classifier, the adversary is able to identify an image-based generative model. Once this generative model F is obtained, the adversary has the capacity to submit queries to it. The goal of the adversary is to construct a dataset, denoted as $D_{aux}$, which shares the same or similar distribution as the dataset, $D_{train}$, used to train the target model T. After acquiring $D_{aux}$, the adversary can conduct a range of model-related attacks. The specifics of these attacks are outlined below.\nModel Extraction. The objective of model extraction attacks, also referred to as model stealing, is to create a model E that"}, {"title": "3 Methodology", "content": "3.1 Overview\nThe rationale behind the proposed method stems from the common practice of training generative models on extensive datasets. For instance, the development of DALL-E, publicly introduced alongside with CLIP (Contrastive Language-Image Pre-training), involved training on a substantial dataset consisting of 400 million pairs of images with text captions extracted from the Internet [24]. Consequently, the distribution space of training data for generative models is highly likely to encompass the distribution space of the training set for the target model if the target model shares a similar task with the generative model. Therefore, the primary task for the adversary is to uncover the distribution of the training set associated with the target model. The general attack pipeline operates as follows. First, the adversary queries the generative model to produce data, which is subsequently used to query the target model. The outputs collected from the target model provide insights into the distribution of its training set and are then leveraged to execute model-related attacks.\nSpecifically, our method comprises three steps. Firstly, leveraging their understanding of the task of the target model T, the adversary identifies a publicly available generative model F that closely aligns with T. For instance, in the case of an image classifier target model, the adversary might opt for DALL-E [38] as the generative model. Then, armed with knowledge about the meanings of each class c within T, the adversary instructs the generative model F to generate a set of samples for each class c, denoted as $D_c$. By uniting these $D_c$ sets across all classes, the adversary compiles a dataset $D_a = \\cup_{c=1}^{C} D_c$, where C represents the number of classes. Secondly, the generated dataset $D_a$, however, may not inherently share the same distribution as model T's training set $D_{train}$. This discrepancy arises from potential distinctive features present in synthetic samples generated by the model F, as opposed to the often indistinct features characterizing real samples in $D_{train}$. To address this distinction, the adversary augments $D_a$ to $D_{aux}$ by generating additional synthetic samples with indistinct features.\nFinally, the adversary implements inter-class filtering to remove the outlying samples from the generated dataset $D_{aux}$. The refined dataset resulting from this process is denoted as $\\hat{D}_{aux}$. Then, the adversary employs $\\hat{D}_{aux}$ to train a model $\\hat{T}$ with the objective of mimicking the functionality of T.\n3.2 Details of the Method\nWe formally describe the three steps in detail as follows.\nStep 1: Generate data samples using a generative model. To produce samples within class c, the adversary can straightforwardly employ natural language to guide the generative model F. For example, by utilizing a prompt like \"please generate n samples that exhibit key features of class c\", within GPT-4.0, the adversary instructs DALL-E to generate samples accordingly. To assess the efficacy of this set of generated samples, denoted as $D_c$, the adversary uses them to query the target model T. Any samples not correctly classified by T into class c are subsequently discarded.\nStep 2: Enhance the generated data through augmentation. Following the generation of samples across all classes, the adversary acquires a consolidated set $D_a$. Nevertheless, as all samples within $D_a$ are generated by the model F, they exhibit distinctive features. Consequently, directly employing $D_a$ to train a model $\\hat{T}$ would lead to a degradation in model performance compared to the target model T, as shown in the experimental section. Therefore, the adversary must augment $D_a$ by introducing additional samples with indistinct features.\nTo augment each $D_c$ within $D_a$, the adversary begins by randomly selecting a sample $x_c$ from $D_c$. Since $x_c$ is correctly classified by the target model T into class c, the adversary introduces a small noise $\\delta_0$ to $x_c$, i.e., $x_c + \\delta_0$, and gradually increases the noise amount to explore the decision boundary of T, as shown in Figure 1. $\\delta_0$ represents the initial amount of Gaussian noise, serving as the starting point for augmentation. With each increase from $\\delta_{i-1}$ to $\\delta_i$, the adversary randomly collects N samples within a sphere, using $x_c$ as the centroid with a radius ranging from $\\delta_{i-1}$ to $\\delta_i$. If all collected samples are correctly classified as class c by the target model T, the"}, {"title": "4 Analyses of the Method", "content": "The analysis focuses on the data distribution shifts caused by generative models, examining how these models induce shifts in data distribution and the measures taken to mitigate them.\nDistribution Shift Analysis. Our analysis primarily focuses on diffusion models, but the principles can be extended to LLMs, as both utilize an iterative refinement process that introduces uncertainty or randomness. This randomness is a critical factor in the potential distribution shifts observed in both types of models. We begin by identifying the origins of the distribution shift and then explain how the proposed method effectively mitigates this shift.\nThe operation of a diffusion model is divided into two main phases: the forward process and the reverse process. In the forward process, Gaussian noise is incrementally added to the data across multiple steps, progressively transforming it into pure noise. This process is mathematically represented as:\n$q(x_t|x_{t-1}) = N(x_t; \\sqrt{\\alpha_t}x_{t-1}, (1 - \\alpha_t)I)$,\nwhere $\\alpha_t$ is a variance schedule that controls the amount of noise added at each step, $x_t$ represents the noisy data at time step t, and I is the unit vector. In contrast, the reverse process aims to denoise the data incrementally. Given the data $x_t$, the model predicts the data from the previous step, $x_{t-1}$, as:\n$p_\\theta(x_{t-1}|x_t) = N(x_{t-1}; \\mu_\\theta(x_t, t, z), \\sigma_\\theta^2 I)$,\nwhere $\\mu_\\theta$ and $\\sigma_\\theta^2$ are the mean and variance predicted by the model, respectively, and z represents the embedding encoded from the input prompt by an encoder E, such as a transformer. Specifically, the reverse process aims to reconstruct $x_{t-1}$ from $x_t$. This is done by removing the predicted noise component $\\epsilon_\\theta(x_t, t, z)$ from $x_t$ and adding some uncertainty to account for the stochastic nature of the process [21, 44]:\n$x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}}(\\sqrt{1-\\alpha_t}x_t - \\frac{1 - \\alpha_t}{\\sqrt{1-\\bar{\\alpha_t}}}\\epsilon_\\theta(x_t, t, z)) + \\sigma_t\\xi$,  (1)\nwhere $\\bar{\\alpha} = \\Pi_{s=1}^{t} \\alpha_s$, $\\epsilon_\\theta$ is a function approximator used to predict noise from $x_t$, and $\\xi \\sim N(0, I)$ represents uncertainty. This uncertainty is a source of the distribution shift. The term $\\xi$ introduces randomness into the reconstruction process. Although this randomness is essential for modeling the inherent uncertainty in the data, it can lead to variability in the generated samples, causing them to deviate from the distribution of the original training data.\nWe now explain how the proposed inter-class filtering can alleviate the distribution shift. Consider the distribution of the target model's training set as $P_{train}$, and the distribution of the initially generated dataset as $P_{gen}$. After applying inter-class filtering, the modified distribution of the generated dataset is denoted as $\\hat{P}_{gen}$. The Kullback-Leibler (KL) divergence, which quantifies the difference between $P_{train}$ and $P_{gen}$, can be mathematically formulated as follows:\n$KL(P_{train}||P_{gen}) = \\int P_{train}(x)log(\\frac{P_{train}(x)}{P_{gen}(x)})dx$.  (2)\nSimilarly, the KL divergence between $P_{train}$ and $\\hat{P}_{gen}$ can be expressed as:\n$KL(P_{train}||\\hat{P}_{gen}) = \\int P_{train}(x)log(\\frac{P_{train}(x)}{\\hat{P}_{gen}(x)})dx$.  (3)\nTo demonstrate the mitigation of the distribution shift, it suffices to show that $KL(P_{train}||P_{gen}) > KL(P_{train}||\\hat{P}_{gen})$. The detailed proof is presented below.\n$KL(P_{train}||P_{gen}) - KL(P_{train}||\\hat{P}_{gen})$\n$= \\int_{x \\in D_{aux}} P_{train}(x)log(\\frac{P_{train}(x)}{P_{gen}(x)})dx - \\int_{x \\in \\hat{D}_{aux}} P_{train}(x)log(\\frac{P_{train}(x)}{P_{gen}(x)})dx$\n$= \\int_{x \\in D_{aux} - \\hat{D}_{aux}} P_{train}(x)log(\\frac{P_{train}(x)}{P_{gen}(x)})dx + \\int_{x \\in \\hat{D}_{aux}} P_{train}(x)[log(\\frac{P_{train}(x)}{P_{gen}(x)}) - log(\\frac{P_{train}(x)}{P_{gen}(x)})]dx$\n$= \\int_{x \\in D_{aux} - \\hat{D}_{aux}} P_{train}(x)log(\\frac{P_{train}(x)}{P_{gen}(x)})dx + \\int_{x \\in \\hat{D}_{aux}} P_{train}(x)log(\\frac{\\hat{P}_{gen}(x)}{P_{gen}(x)})]dx$\n$= \\int_{x \\in D_{aux} - \\hat{D}_{aux}} P_{train}(x)log(\\frac{P_{train}(x)}{P_{gen}(x)})dx > 0$   (4)\nIn Eq. 4, the second equation arises from filtering a number of samples from the generated dataset $D_{aux}$, resulting in a"}, {"title": "5 Experiments", "content": "5.1 Experimental Setup\nDatasets. In the experiments, we adopt three image and two language datasets. Additionally, we also created a new image dataset independently.\n\u2022 CIFAR10 [28] includes 60,000 images across 10 classes, each containing 6,000 images of vehicles and animals. The dimension of each image is 32 \u00d7 32.\n\u2022 MNIST [29] is a dataset of 70,000 images of handwritten numerals spanning 10 classes: 0 \u2013 9. Each class has 7000 images and each image was resized to 32 \u00d7 32.\n\u2022 SkinCancer [23] is a melanoma skin cancer dataset containing 10,000 images, with 9,000 images used for training the model and 1,000 images for testing the model. The task involves binary classification, distinguishing between positive and negative instances of melanoma skin cancer based on skin images. Each image was resized to 32 \u00d7 32.\n\u2022 BBCNews [40] consists of RSS Feeds from the BBC News site, comprising 29,500 records. Each record includes five attributes: title, pubDate, guid, link, and description. The dataset includes five classes: business, politics, entertainment, science, and sport.\n\u2022 IMDB [2] contains 50,000 movie reviews categorized for binary sentiment classification, distinguishing between positive and negative sentiments. The dataset comprises 25,000 highly polar movie reviews allocated for training and an additional 25,000 for testing purposes.\nIn addition to utilizing the five datasets, which may have been employed to train the generative models, we have created a novel dataset named PET. The PET dataset was created using a combination of downloaded and self-recorded videos. Specifically, three videos each for the dog and bird classes were downloaded from YouTube, while four videos for the cat class were recorded manually. Images were then extracted from these videos to compile the dataset. Since the primary goal of PET is to develop a dataset presumed to be unseen by generative models, the number of classes is not a critical factor. However, the dataset can be easily expanded to include additional classes using the same approach.\nEvaluation Metrics. As our experiments encompass various attacks, we utilize different evaluation metrics tailored to each attack's specific characteristics.\nFor model extraction, we employ accuracy and agreement to evaluate the efficacy of the attack. Accuracy refers to the testing accuracy of a given test set on the stolen model. Agreement quantifies the fraction of samples within a test set where both the target and stolen models make identical predictions.\nFor membership inference, we utilize accuracy, F1 score, AUC score (area under the ROC curve), and TPR@1%FPR as evaluation metrics. Here, accuracy denotes the proportion of samples within a test set whose membership status is accurately predicted by the attack model. The definitions of F1 score, AUC score, and TPR@1%FPR can be found in standard literature on classification metrics.\nFor model inversion, we utilize MSE (mean squared error) and accuracy as evaluation metrics. Here, MSE is computed between the original sample and its reconstructed counterpart. Accuracy represents the fraction of reconstructed samples that can be correctly classified by the target model. In particular, MSE is computable, as the attack is conducted in a one-to-one manner: an original sample is fed into the target model,"}, {"title": "5.2 Overall Results", "content": "Model Extraction. The experimental results, presented in Table 1, demonstrate that our method yields a stolen model with classification accuracy comparable to the target model, closely resembling the performance of the stolen model generated by the baseline method. Additionally, the table reveals that our method produces a stolen model with similar agreement to the target model as the one crafted by the baseline method. This indicates that both stolen models exhibit a high degree of agreement with the target model. These findings underscore the effectiveness of our approach and highlight the capability of generative models to successfully conduct model extraction attacks, despite not having access to any training data of the target model.\nMembership Inference. Table 2 presents the membership inference results obtained using our method and the baseline methods. Remarkably, our approach demonstrates comparable inference accuracy, F1 score, AUC and TPR@1%FPR to the baseline method across various datasets. Note that we also evaluated the metric TPR@0.1%FPR, as shown in Table 3. Achieving good results with TPR@0.1%FPR requires training a large number of shadow models, as demonstrated in [5]. Due to the computational cost, we limit our evaluation to TPR@1%FPR for the remainder of this paper.\nModel Inversion. Table 4 illustrates the results of the inversion process conducted using both our method and the baseline method. Similar to model extraction and membership inference, our approach for model inversion also demonstrates comparable performance to the baseline method in terms of the MSE value and accuracy of the reconstructed samples.\nWe also present the model inversion results visually in Figures 3 and 4. It is evident that the images and texts reconstructed by our method closely resemble those reconstructed by the baseline method.\nAnalyses. In the three inference attacks, a consistent pattern emerges. Both the proposed and baseline methods exhibit strong performance when the target model is trained on datasets with straightforward features, such as MNIST and SkinCancer. However, when the target model is trained on datasets with more intricate features, such as CIFAR10 and BBCNews, both methods yield relatively poorer results. This can be attributed to the higher dimensionality of features in these complex datasets, which introduce greater challenges in inferring model properties. The rich feature set in these datasets can obscure underlying patterns and relationships, making it more difficult for inference attacks to succeed.\nHowever, there is an exception concerning membership inference, where both our method and the baseline methods exhibit deteriorative performance on MNIST. This is likely because models trained on MNIST typically achieve good generalizability to unseen data, which minimizes the differences in model outputs between member and non-member data. Consequently, this similarity in outputs reduces the effectiveness of membership inference attacks.\nResults for the PET dataset. We have conducted a separate evaluation for PET, distinguishing it from others due to its private nature. This allows us to specifically demonstrate the capabilities of generative models when applied to unseen data. Table 5 presents the numerical results of our method on the PET dataset, which align closely with those obtained from public datasets. Additionally, Figure 5 visually illustrates the model inversion results on the PET dataset. These findings underscore the high generalizability of generative models, demonstrating their effectiveness in conducting model inference attacks on previously unseen datasets. Note that the baseline method demonstrated poor performance on the PET dataset, primarily because its effectiveness relies on the dataset size, and PET's limited size does not adequately support it. Therefore, its results on PET are not included."}, {"title": "5.3 Hyperparameter Study", "content": "Our method initially involves tasking the generative model with creating synthetic data, followed by a second step where these generated data are augmented. Thus, our hyperparameter study focuses on two aspects: the quantity of generated data and the extent of augmentation applied to these data.\n5.3.1 The Number of Generated Data\nWe assess how varying the number of generated data points for each class impacts the performance of the proposed method. It is worth noting that due to the varying complexities of these datasets, generating samples against target models trained on them incurs different levels of computational overhead. Consequently, the number of generated samples differs for each dataset. Specifically, for MNIST, CIFAR10, and BBCNews, the number of generated samples for each class ranges from 150 to 300. For SkinCancer, this range extends from 200 to 500, while for IMDB, it spans from 50 to 200.\nModel Extraction. The results of model extraction are shown in Table 6. It is evident that as the number of generated data points increases, the accuracy of the stolen models also improves across different datasets. This phenomenon underscores the efficacy of increasing the dataset with more generated data in enhancing the outcomes of model extraction. Upon closer inspection of the table, two observations emerge. First, increasing the number of generated data points has a minimal impact on simple datasets, such as MNIST. Second, merely increasing the number of generated samples does not consistently enhance the model extraction performance. For instance, in BBCNews, while increasing the number from 150 to 200 leads to a 5.3% improvement in accuracy, further increasing it from 250 to 300 results in only a 0.2% increase.\nThe minimal impact on simple datasets, such as MNIST, can be attributed to their inherent characteristics. MNIST consists of well-separated classes and relatively straightforward patterns, facilitating easier learning for the model even with a smaller amount of data. On the other hand, diminishing returns with an increasing number of generated samples occur due to several factors. Beyond a certain threshold, adding more generated samples may introduce redundancy or overfitting. This means that the model starts to memorize the generated data instead of learning meaningful patterns from it. As a result, the additional data may not contribute significantly to improving the model's performance.\nA consistent pattern regarding the agreement between the target and stolen models emerges from Table 7, suggesting that an increase in generated data can enhance the agreement. This phenomenon can be attributed to the fact that as the number of generated data increases, the stolen model gains access to a more comprehensive representation of the underlying data distribution captured by the target model. Thus, the stolen model is better equipped to mimic the decision boundaries and classification patterns of the target model, leading to higher agreement between them.\nMembership Inference. The membership inference results are presented in Tables 8, 9, 10 and 11. It can be observed that the increase of generated data has a gradually diminishing impact on membership inference across all four metrics: inference accuracy, F1 score, AUC score, and TPR@1%FPR. This phenomenon can be attributed to several factors. Firstly, membership inference is inherently a binary classification task, distinguishing between members and non-members of the target model's training dataset. Therefore, the additional generated data, although contributing to a more comprehensive dataset, may not significantly alter the discrimination boundary between members and non-members. Secondly, the nature of membership inference relies more on identifying subtle patterns in the model's behavior rather than on the sheer volume of data. Thus, while increasing the amount of generated data may refine certain aspects of the model's behavior, it might not substantially affect its susceptibility to membership inference attacks. Lastly, the complexity of the target model and the diversity of the generated data may also play a role. If the target model is already well-trained and the generated data covers a wide range of scenarios, further increase may yield diminishing returns in terms of improving membership inference performance.\nHowever, it is worth noting a specific observation in Table 8: as the number of generated data increases from 150 to 200, the inference accuracy against the CIFAR10 classifier notably rises from 55.6% to 65.8%. This improvement can be attributed to several factors. Firstly, CIFAR10 is a dataset with complex features, including various objects and backgrounds. Therefore, increasing the number of generated data provides a more extensive and diverse set of examples, potentially capturing a broader range of patterns present in the target model's behavior. Additionally, the CIFAR10 dataset contains a higher degree of variability compared to simpler datasets like MNIST, requiring a larger volume of data to effectively capture its rich variability. Lastly, the mimicking model $\\hat{T}$ can benefit from a larger generated dataset due to the extensive feature space, allowing for better generalization and discrimination between members and non-members of the target model's training dataset.\nModel Inversion. The outcomes of model inversion are depicted in Tables 12 and 13. Generally, we observe an improvement in inversion performance as the number of generated data points increases, reflected in lower MSE values and higher accuracy. Nevertheless, there are instances where this trend does not hold true.\nIn Table 12, we observe that the increase in the number of generated data has a limited impact on MSE for MNIST. This phenomenon can be attributed to two factors. Firstly, MNIST contains relatively simple features, making it easier for the inversion model to approximate the original samples accurately with fewer generated data points. Additionally, the nature of the inversion task itself may also play a role, as certain datasets may exhibit patterns or characteristics that are more amenable to accurate inversion, regardless of the amount of generated data used.\nWe also visually present the reconstructed results in Figures 6 and 7. The reconstructed images for CIFAR10 are visually acceptable, and as the number of generated samples increases, the quality of the reconstructions improves. Similarly, for BBCNews, as the number of generated samples increases, the reconstructed texts align more closely with the original content semantically. For example, when the number of generated samples is 200, the reconstructed text contains the phrase \"which used monitor\". With an increase to 300 samples, this phrase becomes \"which be used to monitor\", closer to the original text.\n5.3.2 The Impact of Augmentation\nIn this setup, we assess the influence of augmenting generated data on the performance of our proposed method.\nModel Extraction. The results of the model extraction attack with and without augmenting generated data are summarized"}, {"title": "5.4 Ablation Study", "content": "5.4.1 Additional Baselines of Data-free Attacks\nTo evaluate the necessity of our method, we propose two additional baselines for data-free attacks: one for image models and the other for text models. Both approaches rely on searching the input space and querying the target model. Specifically, for image target models, we generate random pixel noise, which is then structured into a noise image. This noise image is fed into the target model for classification, and the resulting classification output is used as the label for the noise image. In this manner, a dataset consisting of purely noise-based images can be generated. Similarly, for text target models, we randomly select words and combine them into a random article. This random article is input into the target model for classification, and the classification result is used as the label for the article. This process generates a dataset of random text articles paired with their respective labels from the target model. The experimental results are presented in Tables 17, 18, 19 and Figures 10, 11.\nIt can be observed that the attack results of the new baselines are significantly inferior to those achieved by our method. This highlights the substantial advantage of using data produced by generative models over data generated through random input space exploration. This is because generative models, trained on extensive and diverse datasets, capture rich latent features and semantic relationships, enabling them to generate synthetic data that resembles real-world inputs. In contrast, random input space exploration lacks such structured knowledge and produces data that is largely uninformative, making it less effective for model-related attacks.\n5.4.2 Additional Generative Models\nTo evaluate the suitability of our approach, we adopted additional generative models to generate data for conducting model-related attacks. Specifically, we utilized Google Gemini [13] as the large language model and Open-journey [41]"}, {"title": "5.4.3 Automated versus Manual Prompt Generation", "content": "We have evaluated automated prompt generation techniques, such as AI Prompt Optimizer [42], for our attacks. Below, we provide an example comparison between a manually crafted prompt and its automated counterpart generated by the aforementioned technique.\nManual prompt: Generate a single cat, in a realistic style, with a clear background\nAutomated prompt: Create a highly realistic image of a single cat with a clear white background. The cat should have a sleek and glossy coat, vivid green eyes, and a naturally curious expression. Ensure the fur texture is detailed and realistic, capturing the subtle variations in color and shading. The overall image should convey lifelike qualities, with attention to detail in the cat's anatomy and fur patterns.\nFigure 14 displays the images generated from both manual and automated prompts. Visually, there are no significant perceptual differences between the images generated by these two types of prompts. To further evaluate their effectiveness, we used these images to conduct model extraction attacks against the CIFAR10 classifier by training two separate mimicking models. The results are presented in Table 23, showing that both the accuracy and agreement are comparably close."}, {"title": "5.5 Summary", "content": "The overall results highlight the potential of generative AI techniques in effectively conducting model-related attacks across various scenarios, including our newly created dataset, which is presumed to be unseen by the generative models. Notably, our proposed method achieves performance comparable to the white-box-based baseline method and significantly surpasses the random search-based baseline, further demonstrating its effectiveness and robustness."}, {"title": "6 Potential Defense Strategies", "content": "Synthetic Data Detection. As the data used in the proposed attacks are synthesized by generative models, an effective defense strategy involves detecting and filtering out synthetic data. This detection process can be accomplished by analyzing the statistical properties or patterns inherent in synthetic data. For instance, anomaly detection methods can be leveraged to identify outliers or deviations in the distribution of synthetic data compared to genuine data [14", "50": ".", "15": "or even utilize LLMs themselves for this purpose [4"}]}