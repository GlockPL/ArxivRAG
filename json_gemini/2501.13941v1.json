{"title": "GaussMark: A Practical Approach for Structural Watermarking of Language Models", "authors": ["Adam Block", "Alexander Rakhlin", "Ayush Sekhari"], "abstract": "Recent advances in Large Language Models (LLMs) have led to significant improvements in natural language processing tasks, but their ability to generate human-quality text raises significant ethical and operational concerns in settings where it is important to recognize whether or not a given text was generated by a human. Thus, recent work has focused on developing techniques for watermarking LLM-generated text, i.e., introducing an almost imperceptible signal that allows a provider equipped with a secret key to determine if given text was generated by their model. Current watermarking techniques are often not practical due to concerns with generation latency, detection time, degradation in text quality, or robustness. Many of these drawbacks come from the focus on token-level watermarking, which ignores the inherent structure of text. In this work, we introduce a new scheme, GaussMark, that is simple and efficient to implement, has formal statistical guarantees on its efficacy, comes at no cost in generation latency, and embeds the watermark into the weights of the model itself, providing a structural watermark. Our approach is based on Gaussian independence testing and is motivated by recent empirical observations that minor additive corruptions to LLM weights can result in models of identical (or even improved) quality. We show that by adding a small amount of Gaussian noise to the weights of a given LLM, we can watermark the model in a way that is statistically detectable by a provider who retains the secret key. We provide formal statistical bounds on the validity and power of our procedure. Through an extensive suite of experiments, we demonstrate that GaussMark is reliable, efficient, and relatively robust to corruptions such as insertions, deletions, substitutions, and roundtrip translations and can be instantiated with essentially no loss in model quality.", "sections": [{"title": "Introduction", "content": "Recent advancements in Language Models (LMs) have significantly transformed the field of Natural Language Processing, offering unprecedented capabilities in generating high-quality artificial text [Achiam et al., 2023, Schulman et al., 2022]. While these models have democratized access to information and become essential tools for applications ranging from automated communication to solving mathematical and programming tasks [He-Yueya et al., 2023, Ahn et al., 2024, Jiang et al., 2024, Imani et al., 2023, Wang et al., 2023], their rapid evolution and widespread deployment also pose significant risks to social, political, and economic institutions [Mirsky et al., 2023]. The ability of LLMs to produce human-like text has raised concerns about potential misuse, including academic plagiarism [Kasneci et al., 2023], large-scale disinformation campaigns on social media [Islam et al., 2020], and the spread of tailored misinformation that could undermine democratic processes.\nTo address these issues, a substantial body of research has focused on watermarking LMs, i.e. embedding a signal into generated text that the provider can use to determine whether or not a given example was machine-generated. In order for watermarking to be practical, it must possess a few key properties: (a) watermarks should be easily detectable with formal statistical guarantees; (b) watermarking should not degrade LM performance both in quality of generated text and latency of generation; (c) watermarks should remain detectable after common token- and sequence-level corruptions are applied. Among these desiderata, formal statistical guarantees on the validity of the watermark are particularly important in situations where authorship claims are sensitive, with a recent example being a lawsuit against a Massachusetts school by a student's parents after the school wrongly accused a student of cheating with ChatGPT [Tenbarge, 2024]; while such claims do not always rise to the level of litigation, examples of students falsely accused of cheating due to unreliable detection abound [Jimenez, 2023, Klee, 2023, Giray, 2024, Mathewson, 2023, Verma, 2023].\nSeveral prior works, discussed in detail in Section 5, achieve rigorous statistical guarantees for watermarking by embedding statistical patterns into the autoregressive next-token sampling process of language models [Kuditipudi et al., 2023, Christ et al., 2024, Golowich and Moitra, 2024]. These methods emphasize indistinguishability or distortion-freeness, requiring the watermarked text to closely resemble unwatermarked text in Total Variation (TV) distance. However, this approach often overlooks the inherent structure of language, treating text as merely a sequence of tokens generated by an unstructured autoregressive process. While this focus on distortion-freeness helps safeguard text quality, it may come at the cost of other crucial properties, such as robustness to corruption and efficiency of watermarking. In practice, such stringent distortion-freeness is not always necessary, as the primary goal is to generate text that appears indistinguishable from unwatermarked text to human evaluators. Recent empirical evidence suggests that humans have"}, {"title": "Problem Setup and Prerequisites", "content": "In this section, we formally define the watermarking problem within the framework of statistical hypothesis testing. We start by outlining the general principles of hypothesis testing and introducing the relevant notation. We then present the necessary definitions for language models. Finally, we characterize the problem of watermarking text generated by language models as a specific instance of hypothesis testing, which will serve as the foundation for the remainder of the paper."}, {"title": "Hypothesis Testing", "content": "We begin by recalling the formalism of hypothesis testing from classical statistics (see Casella and Berger [2001], Lehmann et al. [1986] for a thorough introduction to this topic).\nDefinition 2.1. Given a domain Z, hypotheses $H_0$ and $H_A$ are nonempty collection of probability measures on Z, with $H_0$ called the null hypothesis and $H_A$ called the alternative hypothesis. If $H_0$ (or $H_A$) is a singleton, we call it simple; otherwise, we call it composite. A test is any measurable function $\\rho : Z \\rightarrow {0,1}$.\nIn hypothesis testing we assume there exists a ground-truth distribution $\\rho \\in H_0 \\cup H_A$ and, given a sample $Z \\sim \\rho$, we wish to use the test $\\rho$ to inform us as to whether we can confidently say that $\\rho \\in H_A$ (i.e. $\\rho(Z) = 1$) or if we, by default, do not have sufficient evidence to discount the hypothesis that $\\rho \\in H_0$ (i.e. $\\rho(Z) = 0$). As such, there are two relevant notions of error: the probability that we falsely reject the null hypothesis (Type-I error) and the chance that we falsely accept the null hypothesis (Type-II error). These errors are controlled by the level and the power of test $\\rho$.\nDefinition 2.2. Let $\\alpha, \\beta \\in (0,1)$. Given hypotheses $H_0$ and $H_A$, and a test $\\rho : Z \\rightarrow {0,1}$, we say that $\\rho$ has level $\\alpha$ if\n$\\sup_{\\rho \\in H_0} \\mathbb{P}r_{Z \\sim \\rho} (\\rho(Z) = 1) \\le \\alpha,$\ni.e., for any distribution in $H_0$, the probability we falsely classify a sample from that distribution as coming from a distribution in $H_A$ is at most $\\alpha$. We say that $\\rho$ has power $1 - \\beta$ if\n$\\sup_{\\rho \\in H_A} \\mathbb{P}r_{Z \\sim \\rho} (\\rho(Z) = 0) \\le \\beta,$\ni.e., the probability that we accept the null hypothesis despite the fact that the true distribution is in $H_A$ is bounded above by $\\beta$. Finally, a function $\\rho : Z \\rightarrow [0, 1]$ is called a p-value if for all $\\alpha \\in [0,1]$ it holds that $\\sup_{q \\in H_0} \\mathbb{P}r_{Z \\sim q} (\\rho(Z) \\le \\alpha) \\le \\alpha$.\nWe aim to design tests $\\rho$ that maintain a low probability of rejecting the null hypothesis when it is true (i.e. $\\alpha \\approx 0$) while achieving a high probability of rejecting the null hypothesis under the alternative hypothesis (i.e. $\\beta \\approx 0$). The p-value is a crucial tool in hypothesis testing, as it provides a measure of evidence against the null hypothesis. Specifically, it represents the probability of obtaining a result at least as extreme as the one observed, assuming the null hypothesis is true. Thus, smaller p-values imply that $H_0$ is less likely. In what follows, we formalize the watermarking problem as a hypothesis testing problem where the watermarker develops a statistical test to distinguish between the hypothesis that a given text is generated by a specified watermarked model, and the null hypothesis that it is not. We first introduce the necessary notation for language models."}, {"title": "Language Models", "content": "A Language Model (LM), parameterized by weights $\\theta \\in \\Theta$ and a token space $V$, is represented by a function $p_{\\theta} : V^* \\rightarrow \\Delta(V)$ that maps a sequence of past tokens to a distribution over the next-token, where $V^*$ is the set of all strings of tokens in $V$. Given a prompt $x \\in V^*$ and a generation length $T > 1$, the response text $y = (v_1, ..., v_T) \\in V^T$ is generated by autoregressively sampling from the language model for T tokens using the process\n$v_t \\sim p_{\\theta}(\\cdot | x \\circ v_{1:t-1})$ for $t \\in [T]$.\nFor simplicity of notation, we often use the notation $y = v_{1:T} \\sim p_{\\theta}(\\cdot | x)$ to represent the above sampling process marginalized over T timesteps."}, {"title": "Watermarking Text Generated by Language Models", "content": "We next formalize watermarking as a hypothesis testing problem, a framework that has also been used in prior rigorous works on watermarking language models [Huang et al., 2023, Kuditipudi et al., 2023, Kirchenbauer et al., 2023a]. A key benefit of this formulation is that it allows us to get rigorous statistical guarantees for detection, which are essential in practical scenarios such as detecting plagiarism etc., where reliable guarantees are crucial for validating claims about generated text. Formally, a watermarking scheme consists of Generate and Detect procedures. Given a prompt space $X \\subseteq V^*$, a key space $\\Xi \\in \\mathbb{R}^d$, and a sample space $Y \\subseteq V^T$ (consisting of strings from the token space), watermarking takes place in two steps:\n1.  The Generate is parameterized by language model $p_{\\theta} : X \\rightarrow \\Delta(V)$ and a distribution $\\nu \\in \\Delta(\\Xi)$. When given a prompt $x \\in X$, Generate samples a key $\\xi \\sim \\nu$ and returns the watermarked text $y \\sim p_{\\theta(\\xi)}(\\cdot | x)$, where $\\theta(\\xi)$ denotes the language model watermarked using the key $\\xi$.\n2.  The Detect procedure takes as input a watermarking key $\\xi' \\in \\Xi$ and some candidate text $y' \\in Y$ and tests the following statistical hypotheses:\n$\\begin{aligned} H_0: & \\text{ Given } x, \\text{ the key and the sample are independent, i.e., } (\\xi', y') \\sim \\nu \\otimes q \\text{ with } q \\in \\Delta(Y). \\\\ H_A: & \\text{ The sample } y' \\text{ is produced by Generate using the key } \\xi', \\text{ i.e., } y' \\sim p_{\\theta(\\xi')}(\\cdot | x). \\end{aligned}$\nOur goal in designing an effective watermarking scheme is to choose a $\\xi$ (or the corresponding distribution $\\nu$) such that for any x, the modified language model $p_{\\theta(\\xi)}$ generates text that is sufficiently distinct from that of the original language model $p_{\\theta}$ when the key $\\xi$ is known, but at the same time has good quality and is relatively indistinguishable when $\\xi$ is unknown. These desiderata preserve the utility of the unwatermarked model while also allowing for a test that can reliably determine whether a given sequence of text is generated by the aforementioned LM. We emphasize that the null hypothesis considered above does not assume that $y' \\sim p_{\\theta}(\\cdot | x)$, as this would be overly restrictive and would allow for human-generated text to be classified as machine-generated as long as the human's distribution is sufficiently different from the model's distribution. Instead, $H_0$ is a composite hypothesis that places no structural assumption on the distribution Q of responses and thus encompasses text generated by humans or even other LMs. We note that this formulation is very similar to that of Huang et al. [2023], except that we drop the additional requirement in"}, {"title": "Watermarking Scheme: GaussMark", "content": "We are now ready to present GaussMark\u2014our novel watermarking scheme for language models. As introduced above, our scheme consists of two components: a Generate procedure called GaussMark.Generate and Detect procedure called GaussMark.Detect. After we introduce GaussMark, we provide some motivation for the approach in Section 3.1 as well as formal theoretical guarantees on the power of our detection test in Section 3.2.\nGaussMark applies to an arbitrary parameterized model, where $p_{\\theta} : V^* \\rightarrow \\Delta(V^T)$ is a parameterized distribution with parameters $\\theta \\in \\Theta \\subset \\mathbb{R}^d$. In GaussMark.Generate (Algorithm 1), we generate the watermarking key $\\xi$ by sampling from a centered multivariate Gaussian distribution with covariance $\\sigma^2 I$ for some small value of $\\sigma > 0$, i.e. $\\xi \\sim \\nu$ with $\\nu = N(0, \\sigma^2 I_d)$. We then produce the watermarked model $\\theta(\\xi)$ by additively perturbing the given parameters $\\theta$ by the key $\\xi$, i.e. $\\theta(\\xi) = \\theta + \\xi$. On the given input prompt x, we then generate the watermarked response by sampling $Y \\sim p_{\\theta(\\xi)} (\\cdot | x)$.\nIn GaussMark.Detect (Algorithm 2), we test the composite hypothesis $H_0$, that $\\xi \\sim N(0, \\sigma^2 I_d)$ and $y \\sim q \\in \\Delta(V)$ are independent of each other, against the simple alternative hypothesis that $y \\sim p_{\\theta+\\xi}(\\cdot | x)$ using the test statistic $\\psi(y, \\xi | x)$ given in (1). Letting $\\Phi$ denote the cumulative distribution function (CDF) of a standard Gaussian random variable, we reject the null hypothesis (interpreted as determining that the text is watermarked) if $\\psi(y, \\xi | x) \\ge \\Phi^{-1}(1 - \\alpha)$, where $\\alpha$ denotes the acceptable false positive (Type-I error) rate. Although GaussMark.Detect is formulated as a statistical test, in our experiments we report the p-value, $1 - \\Phi(\\psi(y, \\xi | x))$.\nWe emphasize that it is critical that our test is valid for the composite null hypothesis $H_0$, allowing any $q \\in \\Delta(V)$ without imposing assumptions on the distribution of y under the null. A test that is valid only for a simple null, such as $y \\sim p_{\\theta}(\\cdot | x)$, against $H_A$ may fail to have a small level when applied to arbitrary human-generated text that does not originate from the model $p_{\\theta}$. This would undermine the benefits of a watermarking scheme with formal statistical guarantees. As explained in the sequel, the choice of Gaussian $\\xi$ is motivated by the fact that Gaussians (a) are", "subsections": []}, {"title": "Theoretical Analysis", "content": "We now formalize the intuition developed in Section 3.1 by providing formal bounds on the statistical validity and the power of GaussMark. We make the following assumption throughout:\nAssumption 3.1. For any $x \\in X$ and $y \\in Y$, the map $\\theta \\rightarrow \\log p_{\\theta}(y|x)$ is differentiable.\nOur first theoretical result is the following bound on the level of our test, and the statistical validity of the returned p-values.\nProposition 3.2. Let $\\alpha \\in (0,1)$, and $\\tau_{\\alpha} := \\Phi^{-1}(1 - \\alpha)$ where $\\Phi$ is the CDF of the standard normal distribution. Then, for any $x \\in X$, the test $\\mathbb{I}(\\frac{\\langle \\xi, \\nabla \\log p_{\\theta}(y|x)\\rangle}{\\sigma || \\nabla \\log p_{\\theta}(y|x)|| } \\ge \\tau_{\\alpha})$ in Algorithm 2 has level $\\alpha$. Furthermore, $1 - \\Phi(\\psi(y,\\xi|x))$ is a valid p-value for the test.\nProposition 3.2 establishes that our procedure, GaussMark, delivers statistically valid p-values, and is even an exact test, ensuring provable control of the false positive rate under virtually no assumptions. We defer the proof to Appendix G.2. As described earlier, this is an essential property of any watermarking scheme, as the consequences of false positives can be severe in many use cases. Beyond controlling the level of the test, we are also concerned with its power. We now describe a setting where we can provably bound the power of our test."}, {"title": "Linear Softmax Model", "content": "While statistical validity holds unconditionally, we must make some assumptions on the model in order to guarantee any nontrivial power. Indeed, in the trivial case where $p_{\\theta}$ is constant in $\\theta$ and $\\nabla \\log p_{\\theta}( \\cdot | x) = 0$ identically, we clearly cannot hope to achieve any watermarking detection with GaussMark since all models are identical. Motivated by our application to LMs, we consider the following parameterized models.\nDefinition 3.3 (Linear softmax model). Let $X$ and $Y$ be discrete spaces, $\\theta \\in \\mathbb{R}^d$ be the underlying model's parameters, and $\\varphi : Y \\times X \\rightarrow \\mathbb{R}^d$ be a known feature map. Then, for any $x \\in X$, the model $p_{\\theta}(\\cdot | x)$ is a linear softmax model if, for all $y \\in Y$,\n$p_{\\theta}(y | x) = \\frac{e^{\\langle \\theta, \\varphi(y|x)\\rangle}}{\\sum_{y' \\in Y} e^{\\langle \\theta, \\varphi(y'|x)\\rangle}} .$\nWe use the linear softmax model to approximate the behavior of LMs to compute the power of our test. Note that this approximation is compatible with autoregressive generation. In particular, for any sequence of tokens $y = v_{1:T}$, if the conditional probability $p_{\\theta}(v_t | x \\circ v_{<t})$ is a linear softmax model for each $t \\le T$, then the overall model\n$p_{\\theta}(Y|x) = \\prod_{t=1}^T p_{\\theta}(v_t | x \\circ v_{<t}) = \\prod_{t=1}^T \\frac{e^{\\langle \\theta, \\varphi_t(v_t|x \\circ v_{<t})\\rangle}}{\\sum_{y' \\in Y} e^{\\langle \\theta, \\varphi_t(v'|x \\circ v_{<t})\\rangle}} = \\frac{e^{\\langle \\theta, \\sum_t \\varphi_t(v_t|x \\circ v_{<t})\\rangle}}{\\sum_{y' \\in Y} e^{\\langle \\theta, \\sum_t \\varphi_t(v_t|x \\circ v_{<t})\\rangle}} $ is also a linear softmax model. However, for the sake of simplicity, in the following analysis, we will focus on softmax modeling at the level of the sequence y.\nIn practice, we watermark by adding the noise $\\xi$ to a feedforward layer in a single transformer block (see Section 4 for exact implementation details). Consequently, by keeping all other parameters of the model fixed, and only concerned with the parameters where we add the noise, we get that the conditional distribution for the next token prediction is a nonlinear softmax model. In particular, there exists some feature map $\\varphi$ and a complicated, non-convex, function $\\chi$ such that $p_{\\theta}(y|x) \\propto e^{\\chi(\\theta; x),\\varphi(y)}$ (Lemma G.4 in Appendix G.3). Our linear softmax modeling above is justified by assuming that $\\chi$ is well-behaved in a small neighborhood around the trained model's pa- rameters $\\theta_0$, and can be locally approximated via $\\chi(\\theta; x) \\approx \\chi(\\theta_0; x) + J_{\\chi}(\\theta_0; x)(\\theta - \\theta_0)$ with $J_{\\chi}(\\theta_0; x)$ being the Jacobian matrix. Consequently, for sufficiently small $|\\xi||$, modern transformers are ap- proximately linear softmax models. Additionally, recent empirical findings on model merging via linear interpolation of weights support the assumption that the model behaves linearly in a small neighborhood around the trained parameters [Wortsman et al., 2022, Chronopoulou et al., 2023, Yang et al., 2024, Goddard et al., 2024, Ilharco et al., 2022, Yu et al., 2024]. These reasonings predict that our theoretical results hold only for sufficiently small $\\sigma$. Indeed, as shown in our ex- periments, when $\\sigma$ becomes too large, GaussMark starts to lose its power (cf. Appendix B). The following result bounds the power of our test under linear softmax modeling.\nProposition 3.4. Let $\\alpha \\in (0,1)$ and $\\tau_{\\alpha} = \\Phi^{-1}(1 - \\alpha)$. Let $p_{\\theta}$ be given by a linear softmax model w.r.t. some underlying features $\\varphi$. Then, for any $x \\in X$, the hypothesis test $\\mathbb{I}[\\frac{\\langle \\xi, \\nabla \\log p_{\\theta} (y|x) \\rangle}{\\sigma || \\nabla \\log p_{\\theta}(y|x)|| } \\ge \\tau_{\\alpha}]$ utilized in Algorithm 2, is level $\\alpha$ and has power $1 - \\beta$, where\n$\\beta = \\mathbb{E}_{\\xi \\sim N(0,\\sigma^2 I_d)} \\mathbb{E}_{Y \\sim P_{\\theta}} [\\gamma(y,\\xi|x) \\cdot \\mathbb{I}{\\psi(y,\\xi | x) \\le \\tau_{\\alpha}}]$,"}, {"title": "Experiments", "content": "In this section, we empirically validate our watermarking scheme. Unlike those works that consider distortion-free or undetectable watermarks [Kuditipudi et al., 2023, Christ et al., 2024, Zamir, 2024, Golowich and Moitra, 2024], our scheme relies on there being a statistically significant difference between the distributions of watermarked and un-watermarked text; thus it is imperative that we evaluate the quality of our watermarked models in order to ensure that our procedure does not lead to a significant degradation in the quality of the generated text. We begin by describing several key experimental details, deferring a complete description of our experimental setup to Appendix A. In Section 4.1, we evaluate the detectability of GaussMark, and in Section 4.2, we examine its impact on model quality. Appendix G.4 explores the robustness of GaussMark to various kinds of corruptions. Finally, in Section 4.4, we investigate a rank-reduced instantiation of GaussMark that allows for even milder attenuation of model quality than the standard instantiation.\nIn our experiments, we use the HuggingFace repository [Wolf et al., 2020a] to load our mod- els, vLLM [Kwon et al., 2023] for generation, and PyTorch [Paszke et al., 2019] for watermark"}, {"title": "Can GaussMark be Detected?", "content": "As has become standard in recent empirical evaluations of watermarking [Kirchenbauer et al., 2023a, Kuditipudi et al., 2023, Lau et al., 2024, Pan et al., 2024], we use the realnewslike split of the C4 dataset [Raffel et al., 2020] as prompts for generating watermarked text. We use the same 1K prompts for all models and all watermarking keys in order to make the comparison fair. We consider"}, {"title": "Does GaussMark Degrade Model Quality?", "content": "Unlike many other watermarking schemes, GaussMark is not distortion-free, thus, we must ensure that GaussMark.Generate does not lead to a loss of model performance. To evaluate this, we assess the impact of GaussMark on model quality using three benchmarks: SuperGLUE [Wang et al., 2019], GSM-8K [Cobbe et al., 2021], and AlpacaEval\u20132.0 [Dubois et al., 2024, Li et al., 2023b]. Additionally, in Appendix C, we report the effect that watermarking has on the perplexity of the model on real text. The first benchmark, SuperGLUE, is a collection of eight challenging language understanding and reasoning tasks designed to be more difficult for modern LLMs compared to earlier benchmarks. The second, GSM-8K is a collection of approximately 8K grade school math questions that measure the ability of the model to perform mathematical reasoning. For both of these tasks, we use the LM Evaluation Harness [Gao et al., 2024] to evaluate the performance of the watermarked models using the Chain of Thought (CoT) and multi-shot prompting provided by Gao et al. [2024] in order to standardize our results. Finally, AlpacaEval-2.0, is a significantly more challenging task where the candidate and benchmark language models generate responses to approximately 800 questions and a larger \u2018evaluator' language model (used as a proxy for human preferences) decides which response is better. Instead of comparing win rates against responses generated by humans or a much stronger model, we directly evaluate the win rate of watermarked models relative to un-watermarked models to enhance the signal. Furthermore, for cost efficiency, we use GPT-4o, the current flagship model of OpenAI, as the evaluator, as opposed to the older GPT-4-Turbo model used in the original AlpacaEval paper.\nWe report our results in Table 1, where we compare the watermarked models to the unwater- marked models. For SuperGLUE, we average the scores over the 8 tasks (cf. Appendix C for scores on individual tasks) and, along with the GSM-8K task, we report mean accuracy and standard error. For AlpacaEval-2.0, we report the win rate of the watermarked model against the un-watermarked model; in order to handle stochasticity, we also consider the evaluator's preferences when com- paring the unwatermarked model to itself with different seeds used for generation and report this performance as an interval. We observe that each of the models discussed in Section 4.1 can be watermarked with GaussMark with essentially no loss in performance as measured by our three benchmarks. In all cases, the performance of the watermarked model is within the confidence inter- val of that of the un-watermarked model, except for Llama3.1\u20138B on AlpacaEval\u20132.0, where the win rate is within $1.5 \\times 10^{-3}$ of the un-watermarked model's lower confidence bound. We defer further discussion on the effect that GaussMark has on model quality to Appendix C."}, {"title": "Is GaussMark Robust?", "content": "In addition to statistical validity, detectability, and lack of model degradation, an important prop- erty of a watermarking scheme is its robustness to corruption. In many applications, the water- marked text may be modified in some ways, either intentionally (e.g. to evade plagiarism accusa- tions) or unintentionally (e.g. when only a portion of a model's output is included in the final draft of a paper). Consequently, for a watermark to be practical, it must demonstrate at least some degree of robustness to such changes.\nWe consider four kinds of corruptions commonly discussed in the watermarking literature [Ku- ditipudi et al., 2023, Christ et al., 2024, Liu et al., 2024a, Zhao et al., 2023a, Golowich and Moitra, 2024]: three types of token-level corruptions (random insertions, deletions, and substitutions) and a more challenging paraphrasing attack\u2014namely, \u2018roundtrip translation,' where watermarked text is translated into another language and then back to English. Details of the implementation of these attacks are provided in Appendix G.4.\nFor the three token-level attacks, Figure 4 (a)-(c) illustrates how the percentage of watermarked text detected at the p = 0.05 level by GaussMark.Detect decreases as progressively larger fractions of the text are corrupted. The results indicate that the degree of robustness varies significantly depending on both the model and the type of corruption, with Llama3.1\u20138B being sufficiently robust so as to retain significant detection power with even half of the tokens corrupted. However, we emphasize that these token-level attacks are relatively crude and do not reflect realistic threat models, as the quality of the corrupted text degrades considerably. To address this limitation, Figure 4-(d) presents the ROC curves for each model following roundtrip translation through French. Despite the increased difficulty of this attack, all models demonstrate nontrivial detection power, demonstrating some robustness in this more realistic corruption scenario.\nFinally, we note that in its most basic form, GaussMark.Detect requires knowledge of the prompt. Indeed, the detection procedure is predicated on the assumption that we can take the gradient of the log probabilities of the generated tokens conditional on the input. Because such knowledge is rarely available in practice, in Figure 5 we investigate the effect that inserting, deleting, and substituting tokens at the beginning of the concatenation of the input prompt and the generated text has on detectability. We see that GaussMark is extremely robust to ignorance of the prompt, with the median p-values of the watermarked text remaining relatively stable across different prompt corruptions. Indeed, because removing tokens from the concatenation of the prompt and LM'S response results in significantly shorter sequences, we see by comparing Figure 5(b) to Figure 2(a) that much of the attenuation in detection power is due to the shorter length of the sequences rather than the corruption itself. We defer further discussion and additional results to Appendix G.4."}, {"title": "Can GaussMark be Improved with Rank-reduction?", "content": "As discussed previously, the key challenge in applying GaussMark is identifying the appropriate parameters $\\theta$ to which to add noise, taking into account the careful balance between detectability and model quality. Recent empirical work has observed that the highest principal components of LMs' weight matrices are maximally significant in determining model predictions [Hu et al., 2021, Sharma et al., 2024, Dettmers et al., 2024, Guo et al., 2023, Han et al., 2024, Wang et al., 2024], with Sharma et al. [2024] in particular suggesting that the lower principal components of the weight matrices can be treated as \u2018noise' and their removal can even improve model performance on some tasks. Motivated by these observations, we propose a rank-reduced version of GaussMark wherein"}, {"title": "Related Works", "content": "In this section, we discuss the most relevant works on LLM watermarking below, and refer the reader to Liang et al. [2024], Liu et al. [2024b] for a thorough discussion on other watermarking schemes for text, image and audio generation.\nWatermarking Approaches with Statistical Guarantees. Watermarking schemes with rig- orous statistical guarantees can be broadly categorized as either distorionary or non-distortionary. In the former, the scheme modifies the distribution of the LM's next-token predictions; for example, Kirchenbauer et al. [2023a], Zhao et al. [2023a,b], Aaronson and Kirchner [2022] introduced schemes that partition the vocabulary into a \u2018red set' and a \u2018green set,' and adjust the distribution to favor green-set tokens in a manner detectable by examining empirical frequencies of tokens in generated text. While such schemes offer theoretical guarantees, they often lead to significant distortion in the generated text due to token biasing; moreover, they can be quite vulnerable to paraphrasing attacks [Rastogi and Pruthi, 2024, Hou et al., 2023, Chang et al., 2024, Jovanovi\u0107 et al., 2024]. We empirically compare GaussMark to the scheme introduced by Kirchenbauer et al. [2023a] in Appendix F.\nThe second category, non-distortionary schemes, involves concealing the watermarking signal by influencing the pseudo-random number generator (PRNG) involved in sampling individual tokens. Kuditipudi et al. [2023] provide an example of an extremely robust such scheme; unfortunately, it suffers from poor scalability due to the requirement that the length of the watermarking key must be proportional to the number of queries submitted to the model in order to maintain diversity of responses; with the popularity of contemporary LMs, this would lead to impractically large keys and slow detection times. In order to avoid the limitations of Kuditipudi et al. [2023], Christ et al. [2024] introduce a stronger, cryptographically-inspired notion of undetectability and they, along with Golowich and Moitra [2024], Zamir [2024], Fairoze et al. [2023], provide undetectable water- marking schemes that pseudorandomly select the next token using a hash function of the preceding"}, {"title": "Discussion", "content": "In this work we introduced a new watermarking scheme, GaussMark, with theoretical guarantees on statistical validity and power. Through an extensive empirical suite, we demonstrated that GaussMark is simple, detectable, and does not significantly impact model quality, all while having no impact on generation latency. We now discuss the advantages and disadvantages of GaussMark relative to prior watermarking schemes and suggest potential future directions.\nAdvantages of GaussMark. The primary advantage of GaussMark is that it is the first simple, effi- cient, and statistically valid watermarking scheme that can be immediately integrated into existing inference pipelines with essentially no modification and absolutely no impact on generation latency. In addition, detection speed is extremely fast relative to many alternative schemes and has memory requirements essentially on par with those of inference. Furthermore, GaussMark is relatively robust to corruption and, in practice, does not need to know the prompt or any other information about the text to detect the watermark (cf. Figure 5). Finally, in contradistinction to many alternative watermarking schemes, which focus on"}]}