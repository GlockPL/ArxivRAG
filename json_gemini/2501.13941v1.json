[{"title": "GaussMark: A Practical Approach for Structural Watermarking of Language Models", "authors": ["Adam Block", "Alexander Rakhlin", "Ayush Sekhari"], "abstract": "Recent advances in Large Language Models (LLMs) have led to significant improvements in natural language processing tasks, but their ability to generate human-quality text raises significant ethical and operational concerns in settings where it is important to recognize whether or not a given text was generated by a human. Thus, recent work has focused on developing techniques for watermarking LLM-generated text, i.e., introducing an almost imperceptible signal that allows a provider equipped with a secret key to determine if given text was generated by their model. Current watermarking techniques are often not practical due to concerns with generation latency, detection time, degradation in text quality, or robustness. Many of these drawbacks come from the focus on token-level watermarking, which ignores the inherent structure of text. In this work, we introduce a new scheme, GaussMark, that is simple and efficient to implement, has formal statistical guarantees on its efficacy, comes at no cost in generation latency, and embeds the watermark into the weights of the model itself, providing a structural watermark. Our approach is based on Gaussian independence testing and is motivated by recent empirical observations that minor additive corruptions to LLM weights can result in models of identical (or even improved) quality. We show that by adding a small amount of Gaussian noise to the weights of a given LLM, we can watermark the model in a way that is statistically detectable by a provider who retains the secret key. We provide formal statistical bounds on the validity and power of our procedure. Through an extensive suite of experiments, we demonstrate that GaussMark is reliable, efficient, and relatively robust to corruptions such as insertions, deletions, substitutions, and roundtrip translations and can be instantiated with essentially no loss in model quality.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in Language Models (LMs) have significantly transformed the field of Natural Language Processing, offering unprecedented capabilities in generating high-quality artificial text [Achiam et al., 2023, Schulman et al., 2022]. While these models have democratized access to information and become essential tools for applications ranging from automated communication to solving mathematical and programming tasks [He-Yueya et al., 2023, Ahn et al., 2024, Jiang et al., 2024, Imani et al., 2023, Wang et al., 2023], their rapid evolution and widespread deployment also pose significant risks to social, political, and economic institutions [Mirsky et al., 2023]. The ability of LLMs to produce human-like text has raised concerns about potential misuse, including academic plagiarism [Kasneci et al., 2023], large-scale disinformation campaigns on social media [Islam et al., 2020], and the spread of tailored misinformation that could undermine democratic processes.\nTo address these issues, a substantial body of research has focused on watermarking LMs, i.e. embedding a signal into generated text that the provider can use to determine whether or not a given example was machine-generated. In order for watermarking to be practical, it must possess a few key properties: (a) watermarks should be easily detectable with formal statistical guarantees; (b) watermarking should not degrade LM performance both in quality of generated text and latency of generation; (c) watermarks should remain detectable after common token- and sequence-level corruptions are applied. Among these desiderata, formal statistical guarantees on the validity of the watermark are particularly important in situations where authorship claims are sensitive, with a recent example being a lawsuit against a Massachusetts school by a student's parents after the school wrongly accused a student of cheating with ChatGPT [Tenbarge, 2024]; while such claims do not always rise to the level of litigation, examples of students falsely accused of cheating due to unreliable detection abound [Jimenez, 2023, Klee, 2023, Giray, 2024, Mathewson, 2023, Verma, 2023].\nSeveral prior works, discussed in detail in Section 5, achieve rigorous statistical guarantees for watermarking by embedding statistical patterns into the autoregressive next-token sampling process of language models [Kuditipudi et al., 2023, Christ et al., 2024, Golowich and Moitra, 2024]. These methods emphasize indistinguishability or distortion-freeness, requiring the watermarked text to closely resemble unwatermarked text in Total Variation (TV) distance. However, this approach often overlooks the inherent structure of language, treating text as merely a sequence of tokens generated by an unstructured autoregressive process. While this focus on distortion-freeness helps safeguard text quality, it may come at the cost of other crucial properties, such as robustness to corruption and efficiency of watermarking. In practice, such stringent distortion-freeness is not always necessary, as the primary goal is to generate text that appears indistinguishable from unwatermarked text to human evaluators. Recent empirical evidence suggests that humans have limited ability to differentiate between high-quality generated text and subtle variations [Dugan et al., 2023, Clark et al., 2021, Ippolito et al., 2019, Elangovan et al., 2024, Lee et al., 2022], which suggests that instead of enforcing strict closeness under TV distance, weaker metrics like Maximum Mean Discrepancy (MMD) [Gretton et al., 2012], evaluated using the implicit criteria humans employ to assess text quality, may be sufficient. Furthermore, natural language exhibits intricate implicit structures that modern language models learn and encode within their weights. While some recent work has introduced distortionary watermarks [Kirchenbauer et al., 2023a,b], they tend to underperform empirically when assessed in terms of the quality of the generated text [Kuditipudi et al., 2023, Dathathri et al., 2024].\nIn this work, we propose GaussMark, a novel approach that takes advantage of the structure inherent to text by modifying the model weights through a Gaussian perturbation at generation time and tests for statistical independence at detection time. This approach is motivated by empirical observations that language models are robust to small perturbations of their weights, as evidenced by phenomena such as linear interpolation through model merging and model fine-tuning by thresholding or modifying weights along low-rank components, which do not degrade output quality [Sharma et al., 2024, Gong et al., 2024, Sun et al., 2023, Hu et al., 2021]. Our method is simple to implement and efficient, providing both significant detection ability and essentially no loss in model performance. Theoretically, we demonstrate that GaussMark always provides a statistically valid test and has reasonable statistical power under natural simplifying assumptions. Through an extensive empirical suite, we show that GaussMark is very detectable, does not result in a loss of quality in the generated text, and is somewhat robust to both token-level and semantic corruptions. We summarize our key contributions below:\n\u2022 In Section 3 we introduce GaussMark, a novel semantic watermarking scheme that is both practical and efficient and has no effect on generation latency. In Section 3.1, we motivate GaussMark using classical statistical theory and present intuition as to why our approach is likely to be effective.\n\u2022 In Section 3.2, we provide rigorous guarantees on the statistical validity of GaussMark under essentially no assumptions. We also provide theoretical guarantees on the power of GaussMark to be detected under modeling assumptions that approximate modern language models.\n\u2022 In Section 4, we present a comprehensive empirical evaluation of GaussMark on a variety of modern LMs, demonstrating its detectability (Section 4.1), lack of effect on model quality (Section 4.2), and robustness (Section 4.3). In the appendices (Appendices A to C, E and G.4), we present a number of ablations demonstrating the effects that different parameter choices have on both detectability and model quality as well as many further empirical results. We also include in Appendix H examples of watermarked text generated by GaussMark.\n\u2022 In Section 4.4 we suggest a small modification to GaussMark that further reduces the water-mark's impact on model quality while preserving detectability and extensively investigate the empirical effect of this intervention in Appendix E.\n\u2022 Finally, in Appendix F, we provide an empirical comparison of GaussMark to the scheme of Kirchenbauer et al. [2023a,b], demonstrating that while GaussMark is somewhat less detectable and robust to corruptions, it has a much smaller impact on the quality of the generated text and allows for significantly faster generation."}, {"title": "2 Problem Setup and Prerequisites", "content": "In this section, we formally define the watermarking problem within the framework of statistical hypothesis testing. We start by outlining the general principles of hypothesis testing and introducing the relevant notation. We then present the necessary definitions for language models. Finally, we characterize the problem of watermarking text generated by language models as a specific instance of hypothesis testing, which will serve as the foundation for the remainder of the paper."}, {"title": "2.1 Hypothesis Testing", "content": "We begin by recalling the formalism of hypothesis testing from classical statistics (see Casella and Berger [2001], Lehmann et al. [1986] for a thorough introduction to this topic).\nDefinition 2.1. Given a domain Z, hypotheses \\(H_0\\) and \\(H_A\\) are nonempty collection of probability measures on Z, with \\(H_0\\) called the null hypothesis and \\(H_A\\) called the alternative hypothesis. If \\(H_0\\) (or \\(H_A\\)) is a singleton, we call it simple; otherwise, we call it composite. A test is any measurable function \\(\\rho : Z \\rightarrow {0,1}\\).\nIn hypothesis testing we assume there exists a ground-truth distribution \\(\\rho \\in H_0 \\cup H_A\\) and, given a sample \\(Z \\sim \\rho\\), we wish to use the test \\(\\rho\\) to inform us as to whether we can confidently say that \\(\\rho \\in H_A\\) (i.e. \\(\\rho(Z) = 1\\)) or if we, by default, do not have sufficient evidence to discount the hypothesis that \\(\\rho \\in H_0\\) (i.e. \\(\\rho(Z) = 0\\)). As such, there are two relevant notions of error: the probability that we falsely reject the null hypothesis (Type-I error) and the chance that we falsely accept the null hypothesis (Type-II error). These errors are controlled by the level and the power of test \\(\\rho\\).\nDefinition 2.2. Let \\(\\alpha, \\beta \\in (0,1)\\). Given hypotheses \\(H_0\\) and \\(H_A\\), and a test \\(\\rho : Z \\rightarrow {0,1}\\), we say that \\(\\rho\\) has level \\(\\alpha\\) if\n\\[\\sup_{\\rho \\in H_0} Pr_{Z \\sim \\rho} (\\rho(Z) = 1) \\leq \\alpha,\\]\ni.e., for any distribution in \\(H_0\\), the probability we falsely classify a sample from that distribution as coming from a distribution in \\(H_A\\) is at most \\(\\alpha\\). We say that \\(\\rho\\) has power \\(1 - \\beta\\) if\n\\[\\sup_{\\rho \\in H_A} Pr_{Z \\sim \\rho} (\\rho(Z) = 0) \\leq \\beta,\\]\ni.e., the probability that we accept the null hypothesis despite the fact that the true distribution is in \\(H_A\\) is bounded above by \\(\\beta\\). Finally, a function \\(\\rho : Z \\rightarrow [0, 1]\\) is called a p-value if for all \\(\\alpha \\in [0,1]\\) it holds that \\(\\sup_{q \\in H_0} Pr_{Z \\sim q} (\\rho(Z) \\leq \\alpha) \\leq \\alpha\\).\nWe aim to design tests \\(\\rho\\) that maintain a low probability of rejecting the null hypothesis when it is true (i.e. \\(\\alpha \\approx 0\\)) while achieving a high probability of rejecting the null hypothesis under the alternative hypothesis (i.e. \\(\\beta \\approx 0\\)). The p-value is a crucial tool in hypothesis testing, as it provides a measure of evidence against the null hypothesis. Specifically, it represents the probability of obtaining a result at least as extreme as the one observed, assuming the null hypothesis is true. Thus, smaller p-values imply that \\(H_0\\) is less likely. In what follows, we formalize the watermarking problem as a hypothesis testing problem where the watermarker develops a statistical test to distinguish between the hypothesis that a given text is generated by a specified watermarked model, and the null hypothesis that it is not. We first introduce the necessary notation for language models."}, {"title": "2.2 Language Models", "content": "A Language Model (LM), parameterized by weights \\(\\theta \\in \\Theta\\) and a token space \\(V\\), is represented by a function \\(p_\\theta : V^* \\rightarrow \\Delta(V)\\) that maps a sequence of past tokens to a distribution over the next-token, where \\(V^*\\) is the set of all strings of tokens in \\(V\\). Given a prompt \\(x \\in V^*\\) and a generation length \\(T > 1\\), the response text \\(y = (v_1, ..., v_T) \\in V^T\\) is generated by autoregressively sampling from the language model for T tokens using the process\n\\[v_t \\sim p_\\theta(\\cdot \\vert x \\circ v_{1:t-1}) \\qquad \\text{for } t \\in [T].\\]\nFor simplicity of notation, we often use the notation \\(y = v_{1:T} \\sim p_\\theta(\\cdot \\vert x)\\) to represent the above sampling process marginalized over T timesteps."}, {"title": "2.3 Watermarking Text Generated by Language Models", "content": "We next formalize watermarking as a hypothesis testing problem, a framework that has also been used in prior rigorous works on watermarking language models [Huang et al., 2023, Kuditipudi et al., 2023, Kirchenbauer et al., 2023a]. A key benefit of this formulation is that it allows us to get rigorous statistical guarantees for detection, which are essential in practical scenarios such as detecting plagiarism etc., where reliable guarantees are crucial for validating claims about generated text. Formally, a watermarking scheme consists of Generate and Detect procedures. Given a prompt space \\(X \\subseteq V^*\\), a key space \\(\\Xi \\in \\mathbb{R}^d\\), and a sample space \\(Y \\subseteq V^T\\) (consisting of strings from the token space), watermarking takes place in two steps:\n1. The Generate is parameterized by language model \\(p_\\theta : X \\rightarrow \\Delta(V)\\) and a distribution \\(\\nu \\in \\Delta(\\Xi)\\). When given a prompt \\(x \\in X\\), Generate samples a key \\(\\xi \\sim \\nu\\) and returns the watermarked text \\(y \\sim p_{\\theta(\\xi)}(\\cdot \\vert x)\\), where \\(\\theta(\\xi)\\) denotes the language model watermarked using the key \\(\\xi\\).\n2. The Detect procedure takes as input a watermarking key \\(\\xi' \\in \\Xi\\) and some candidate text \\(y' \\in Y\\) and tests the following statistical hypotheses:\n\\(H_0\\): Given x, the key and the sample are independent, i.e., \\((\\xi', y') \\sim \\nu \\otimes q\\) with \\(q \\in \\Delta(Y)\\).\n\\(H_A\\): The sample y' is produced by Generate using the key \\(\\xi'\\), i.e., \\(y' \\sim p_{\\theta(\\xi')}(\\cdot \\vert x)\\).\nOur goal in designing an effective watermarking scheme is to choose a \\(\\xi\\) (or the corresponding distribution \\(\\nu\\)) such that for any x, the modified language model \\(p_{\\theta(\\xi)}\\) generates text that is sufficiently distinct from that of the original language model \\(p_\\theta\\) when the key \\(\\xi\\) is known, but at the same time has good quality and is relatively indistinguishable when \\(\\xi\\) is unknown. These desider-ata preserve the utility of the unwatermarked model while also allowing for a test that can reliably determine whether a given sequence of text is generated by the aforementioned LM. We emphasize that the null hypothesis considered above does not assume that \\(y' \\sim p_\\theta(\\cdot \\vert x)\\), as this would be overly restrictive and would allow for human-generated text to be classified as machine-generated as long as the human's distribution is sufficiently different from the model's distribution. Instead, \\(H_0\\) is a composite hypothesis that places no structural assumption on the distribution Q of responses and thus encompasses text generated by humans or even other LMs. We note that this formulation is very similar to that of Huang et al. [2023], except that we drop the additional requirement in"}, {"title": "3 Watermarking Scheme: GaussMark", "content": "We are now ready to present GaussMark\u2014our novel watermarking scheme for language mod-els. As introduced above, our scheme consists of two components: a Generate procedure called GaussMark.Generate and Detect procedure called GaussMark.Detect. After we introduce GaussMark, we provide some motivation for the approach in Section 3.1 as well as formal theoretical guarantees on the power of our detection test in Section 3.2.\nGaussMark applies to an arbitrary parameterized model, where \\(p_\\theta : V^* \\rightarrow \\Delta(V^T)\\) is a parameterized distribution with parameters \\(\\theta \\in \\Theta \\subset \\mathbb{R}^d\\). In GaussMark.Generate (Algorithm 1), we generate the watermarking key \\(\\xi\\) by sampling from a centered multivariate Gaussian distribution with co-variance \\(\\sigma^2 I\\) for some small value of \\(\\sigma > 0\\), i.e. \\(\\xi \\sim \\nu\\) with \\(\\nu = \\mathcal{N}(0,\\sigma^2 I_d)\\). We then produce the watermarked model \\(\\theta(\\xi)\\) by additively perturbing the given parameters \\(\\theta\\) by the key \\(\\xi\\), i.e. \\(\\theta(\\xi) = \\theta + \\xi\\). On the given input prompt x, we then generate the watermarked response by sampling \\(y \\sim p_{\\theta(\\xi)}(\\cdot | x)\\).\nIn GaussMark.Detect (Algorithm 2), we test the composite hypothesis \\(H_0\\), that \\(\\xi \\sim \\mathcal{N}(0,\\sigma^2 I_d)\\) and \\(y \\sim q \\in \\Delta(V)\\) are independent of each other, against the simple alternative hypothesis that \\(y \\sim p_{\\theta + \\xi}(\\cdot | x)\\) using the test statistic \\(\\psi(y,\\xi | x)\\) given in (1). Letting \\(\\Phi\\) denote the cumulative distribution function (CDF) of a standard Gaussian random variable, we reject the null hypothesis (interpreted as determining that the text is watermarked) if \\(\\psi(y,\\xi | x) \\geq \\Phi^{-1}(1 - \\alpha)\\), where \\(\\alpha\\) denotes the acceptable false positive (Type-I error) rate. Although GaussMark.Detect is formulated as a statistical test, in our experiments we report the p-value, \\(1 - \\Phi(\\psi(y,\\xi|x))\\).\nWe emphasize that it is critical that our test is valid for the composite null hypothesis \\(H_0\\), allowing any \\(q \\in \\Delta(Y)\\) without imposing assumptions on the distribution of y under the null. A test that is valid only for a simple null, such as \\(y \\sim p_\\theta(\\cdot|x)\\), against \\(H_A\\) may fail to have a small level when applied to arbitrary human-generated text that does not originate from the model \\(p_\\theta\\). This would undermine the benefits of a watermarking scheme with formal statistical guarantees. As explained in the sequel, the choice of Gaussian \\(\\xi\\) is motivated by the fact that Gaussians (a) are rotationally invariant and (b) have independent directions and norms. In principle, any distribution with these properties could replace the Gaussian, and with minor modifications, the test would still maintain statistical validity.\nFor an illustrative example of why GaussMark works, suppose \\(X = Y = \\mathbb{R}^d\\) and let \\(p_\\theta(y | x) = \\mathcal{N}(0,I_d)\\). Because under \\(H_0\\), the watermarking key \\(\\xi \\sim \\mathcal{N}(0,\\sigma^2 I_d)\\) is independent of y, it is easy to see that \\(\\psi(y,\\xi | x) \\sim \\mathcal{N}(0, 1)\\) for any fixed x,y. In particular, \\(\\mathbb{E}_{H_0}[\\psi(y,\\xi | x)] = 0\\). On the other hand, under \\(H_A\\), \\(Y \\sim p_{\\theta + \\xi}(\\xi | x) = \\mathcal{N}(0 + \\xi, I_d)\\) and a simple computation shows that\n\\[\\mathbb{E}_{H_A} [\\psi(y,\\xi | x)] \\approx \\frac{\\sigma^2 d}{\\sqrt{1 + \\sigma^2}}\\]\nfor \\(\\sigma \\geq 1/a\\) (see Lemma G.1 for the details). Applying a standard concentration of Gaussian random variables yields bounds on the power of the test, with higher dimensions leading to an improved test. In Section 3.2, we provide a rigorous analysis of the level and the power of our test under more complicated modeling assumptions that capture real-world LLM settings.\nModern language models are parameterized by billions of param-eters [Kaplan et al., 2020, Touvron et al., 2023]. While GaussMark could be implemented across the entire model, we find that effective watermarking can be achieved by modifying a rank-reduced single MLP weight (a matrix) within a single layer. This approach offers two key advantages: First, perturbing parameters in a single feedforward layer minimizes potential degradation in model qual-ity. Specifically, GaussMark relies on the statistical distinguishability of the distributions \\(p_{\\theta+\\xi}(\\cdot|x)\\) and \\(p_\\theta(\\cdot|x)\\), which we observe can be achieved without compromising model performance. Recent empirical studies demonstrate that much of the signal in the feedforward layers of language models resides in a low-dimensional manifold. Consequently, any added noise \\(\\xi\\) to a single layer is likely almost orthogonal to the underlying signal, preserving model quality. Second, GaussMark.Detect incurs minimal memory overhead as it requires only a single forward pass through the model and a backward pass through the watermarked parameter \\(\\theta\\). When \\(\\theta\\) is a single MLP layer, the gradient buffer's memory cost is negligible compared to the memory demands of a forward pass.\nBeyond the statistical validity of our method, discussed below, a primary advantage is its practi-cality. On the inference side, GaussMark.Generate imposes no additional computational cost compared to an unwatermarked model and can seamlessly integrate into generation pipelines such as vLLM [Kwon et al., 2023] and HuggingFace [Wolf et al., 2020b]. This is because the watermarking process alters the model weights and, consequently, the sampled distribution, without affecting inference latency. Detection is similarly efficient, as previously discussed. We now turn to the motivation for our approach and present formal results on its level and power."}, {"title": "3.1 Motivation", "content": "Recall from Definition 2.2 that there are two sources of error in hypothesis testing one desires to control: false positives (controlled by the level of the test) and false negatives (controlled by the test's power). Because these two desiderata are at odds, statisticians either wish to find tests with maximal power subject to a constraint on the level [Neyman and Pearson, 1933] or wish to control the risk of a test, defined for weights \\(a, b \\in \\mathbb{R}_{\\geq 0}\\) as \\(a \\alpha + b \\beta\\). In either case, as long as the null hypothesis is convex, under general measure theoretic conditions (cf Lehmann et al. [1986, Theorem 3.8.1] for the Neyman-Pearson approach and Le Cam [2012], Ghosal and van der Vaart [2017] for the case of minimax risk), the optimal test is given by a likelihood ratio test with respect to the 'worst-case' distribution in the null hypothesis. In other words, the optimal test is given by some distribution \\(p \\in H_0\\) such that\n\\[\\rho(\\xi, y) = \\mathbb{I} [\\log H_A(\\xi, y) - \\log p(\\xi, y) \\geq \\tau_\\alpha]\\]\nfor some threshold \\(\\tau_\\alpha\\) that ensures the test has level \\(\\alpha\\); moreover, in the special case that \\(a = b = 1\\) in the risk formulation, the worst case distribution can be shown to be that in \\(H_0\\) which is closest in Total Variation to \\(H_A\\). Unfortunately, in many settings including ours, it is not clear what the worst-case distribution is either in the Neyman-Pearson sense of uniformly most powerful or in the sense of minimizing risk for general \\(a, b\\); furthermore, it is not even clear how to characterize the projection of \\(H_A\\) onto \\(H_0\\) in TV in a way that is amenable to computing p-values or the thresholds for test statistics. On the other hand, if we relax projection in TV to projection in KL divergence, the following standard result, whose proof is deferred to Appendix G.1 for completeness, shows that the closest distribution in \\(H_0\\) to \\(H_A\\) is given by marginalizing over \\(\\xi\\).\nProposition 3.1. Let \\({p_{\\theta+ \\xi}}_{\\theta \\in \\mathbb{R}^d}\\) be a family of measures on \\(Y\\), \\(x \\in X\\), \\(\\theta \\in \\mathbb{R}^d\\), and \\(\\nu \\in \\Delta(\\mathbb{R}^d)\\) be fixed. Suppose \\(H_0 := {\\nu \\otimes \\mu \\mid \\mu \\in \\Delta(V)}\\) is composite and \\(H_A\\) is simple, such that \\(\\xi \\sim \\nu\\) and \\(y \\sim p_{\\theta+ \\xi}\\). Then the projection of \\(H_A\\) onto \\(H_0\\) in KL divergence is given by \\(\\nu \\otimes \\mathbb{E}_{\\xi' \\sim \\nu}[p_{\\theta+\\xi'}(\\cdot | x)]\\), i.e.,\n\\[\\inf_{p \\in H_0} D_{KL} (H_A \\vert\\vert p) = D_{KL} (p_{\\theta+ \\xi}(\\cdot | x) \\vert\\vert \\mathbb{E}_{\\xi' \\sim \\nu} [p_{\\theta+\\xi'}(\\cdot | x)]) .\\]\nHeuristically, then, a reasonable candidate for the optimal test between \\(H_0\\) and \\(H_A\\) would be to reject the null when the log-likelihood ratio of the data under the null and the alternative is large, where the threshold \\(\\tau_\\alpha\\) is chosen such that\n\\[\\sup_{\\rho \\in H_0} \\mathbb{E}_{(\\xi,y) \\sim \\rho} (\\log p_{\\theta+\\xi}(y | x) - \\log \\mathbb{E}_{\\xi' \\sim \\nu}[p_{\\theta+\\xi}(y | x)] > \\tau_\\alpha).\\]\nWhile (2) would yield a valid (although possibly supoptimal) test, it is not obvious how to easily compute the threshold \\(\\tau_\\alpha\\), nor is it clear that computing the second term in the test statistics is feasible due to the scales of the probabilities involved. In order to circumvent these problems, we approximate the log-likelihood test to first order and specialize to the case that \\(\\nu = \\mathcal{N}(0, \\sigma^2 I_d)\\). More specifically, if we suppose that \\(\\sigma \\ll 1\\) and we can thus ignore terms scaling with \\(\\sigma^2\\), we have that\n\\[\\begin{aligned} \\log p_{\\theta+\\xi}(y | x) - \\log \\mathbb{E}_{\\xi' \\sim \\nu} [p_{\\theta+\\xi'}(y | x)] &= \\log p_{\\theta+\\xi}(y | x) - \\log \\mathbb{E}_{\\xi' \\sim \\nu} \\left[\\frac{p_{\\theta+\\xi'}(y | x)}{p_\\theta(y|x)} \\right] \\\\ &\\approx (\\xi, \\nabla \\log p_\\theta(y | x)) - \\log \\mathbb{E}_{\\xi' \\sim \\nu} [\\exp \\left((\\xi', \\nabla \\log p_\\theta(y|x))\\right)] \\\\ &= (\\xi, \\nabla \\log p_\\theta(y | x)) - \\frac{\\sigma^2 || \\nabla \\log p_\\theta(y | x) ||^2}{2} \\\\ &\\approx (\\xi, \\nabla \\log p_\\theta(y | x)), \\end{aligned}\\]\nwhere the second line follows by using the first-order Taylor's approximation for \\(\\log p_{\\theta+ \\xi}(y | x)\\) around \\(\\log p_\\theta(y|x)\\), the third line uses the MGF for Gaussian random variables, and the last line holds for small \\(\\sigma\\). Thus, as long as our linear approximation is valid, the statistic \\((\\xi, \\nabla \\log p_\\theta(y | x))\\) yields a test that is approximately as powerful as the test in (2) and is easily computed, solving the second problem above. The key insight in solving the first problem and determining \\(\\tau_\\alpha\\) is that due to the rotation invariance of the Gaussian, and the fact that the direction and norm of Gaussian vectors are independent, the distribution of\n\\[\\psi(y,\\xi | x) = \\frac{(\\xi, \\nabla \\log p_\\theta(y | x))}{\\sigma|| \\nabla \\log p_\\theta(y|x) ||}\\]\nis always a standard normal under \\(H_0\\) because y is independent of \\(\\xi\\); thus with appropriate normalization, \\(\\tau_\\alpha\\) can be read off from the Gaussian cumulative distribution function. This insight is formalized in Proposition 3.2 in the sequel wherein we also show that the p-values returned by our test are always statistically valid, in the sense that they are uniformly distributed under the null hypothesis."}, {"title": "3.2 Theoretical Analysis", "content": "We now formalize the intuition developed in Section 3.1 by providing formal bounds on the statistical validity and the power of GaussMark. We make the following assumption throughout:\nAssumption 3.1. For any \\(x \\in X\\) and \\(y \\in Y\\), the map \\(\\theta \\rightarrow \\log p_\\theta(y|x)\\) is differentiable.\nOur first theoretical result is the following bound on the level of our test, and the statistical validity of the returned p-values.\nProposition 3.2. Let \\(\\alpha \\in (0,1)\\), and \\(\\tau_\\alpha := \\Phi^{-1}(1 - \\alpha)\\) where \\(\\Phi\\) is the CDF of the standard normal distribution. Then, for any \\(x \\in X\\), the test \\(\\mathbb{I} \\left[\\frac{(\\xi,\\nabla\\log p_\\theta(y|x))}{\\sigma ||\\nabla\\log p_\\theta(y|x)||} \\geq \\tau_\\alpha \\right]\\) in Algorithm 2 has level \\(\\alpha\\). Furthermore, \\(1 - \\Phi(\\psi(y,\\xi|x))\\) is a valid p-value for the test.\nProposition 3.2 establishes that our procedure, GaussMark, delivers statistically valid p-values, and is even an exact test, ensuring provable control of the false positive rate under virtually no assumptions. We defer the proof to Appendix G.2. As described earlier, this is an essential property of any watermarking scheme, as the consequences of false positives can be severe in many use cases. Beyond controlling the level of the test, we are also concerned with its power. We now describe a setting where we can provably bound the power of our test."}, {"title": "3.2.1 Linear Softmax Model", "content": "While statistical validity holds unconditionally", "varphi": "Y \\times X \\rightarrow \\mathbb{R"}, "d\\) be a known feature map. Then, for any \\(x \\in X\\), the model \\(p_\\theta(|x)\\) is a linear softmax model if, for all \\(y \\in Y\\),\n\\[p_\\theta(y | x) = \\frac{e^{(\\theta,\\varphi(y|x))}}{\\sum_{y' \\in Y} e^{(\\theta,\\varphi(y'|x))}} .\\"], "v_{1": "T}\\), if the conditional probability \\(p_\\theta(v_t|x \\circ v_{<t})\\) is a linear softmax model for each \\(t \\leq T\\), then the overall model\n\\[p_\\theta(Y|x) = \\prod_{t=1}^T p_\\theta(v_t | x \\circ v_{<t}) = \\prod_{t=1}^T \\frac{e^{(\\theta,\\varphi_t(v_t x \\circ v_{<t}))"}, {}]