{"title": "Vox Populi, Vox AI? Using Language Models to Estimate German Public Opinion", "authors": ["Leah von der Heyde", "Anna-Carolina Haensch", "Alexander Wenz"], "abstract": "The recent development of large language models (LLMs) has spurred discussions about whether LLM-generated \u201csynthetic samples\u201d could complement or replace traditional surveys, considering their training data potentially reflects attitudes and behaviors prevalent in the population. A number of mostly US-based studies have prompted LLMs to mimic survey respondents, with some of them finding that the responses closely match the survey data. However, several contextual factors related to the relationship between the respective target population and LLM training data might affect the generalizability of such findings. In this study, we investigate the extent to which LLMs can estimate public opinion in Germany, using the example of vote choice. We generate a synthetic sample of personas matching the individual characteristics of the 2017 German Longitudinal Election Study respondents. We ask the LLM GPT-3.5 to predict each respondent's vote choice and compare these predictions to the survey-based estimates on the aggregate and subgroup levels. We find that GPT-3.5 does not predict citizens' vote choice accurately, exhibiting a bias towards the Green and Left parties. While the LLM captures the tendencies of \u201ctypical\u201d voter subgroups, such as partisans, it misses the multifaceted factors swaying individual voter choices. By examining the LLM-based prediction of voting behavior in a new context, our study contributes to the growing body of research about the conditions under which LLMs can be leveraged for studying public opinion. The findings point to disparities in opinion representation in LLMs and underscore the limitations in applying them for public opinion estimation.", "sections": [{"title": "1. Introduction", "content": "The recent development and large-scale proliferation of large language models (LLMs),\nsuch as OpenAl's GPT (OpenAl et al., 2023) or Meta's Llama (Touvron et al., 2023),\nhave spurred discussions about the extent to which these language models can be\nused for research in the social and behavioral sciences. Researchers have started to\nexplore various applications to facilitate the collection and analysis of survey data.\nExamples include the use of LLMs for questionnaire design and scale development\n(G\u00f6tz et al., 2023; Hernandez & Nie, 2022; Konstantis et al., 2023; Laverghetta &\nLicato, 2023, Lee et al., 2023), conducting interviews (Chopra & Haaland, 2023; Cuevas\nVillalba et al., 2023), coding open-ended survey responses (Mellon et al., 2023; Rytting\net al., 2023), imputing missing data and detecting statistical outliers (Jaimovitch-L\u00f3pez\net al., 2023; Kim & Lee, 2023), detecting non-human respondents in online surveys\n(Lebrun et al., 2023), and data visualization and interpretation (Liew & Mueller, 2022;\nSultanum & Srinivasan, 2023).\nBeyond augmenting survey data collection and analysis, research has also\nstarted to examine to what extent LLMs can be used for making valid inferences about a\npopulation (e.g. Argyle et al., 2023). LLMs are trained on large amounts of internet text\ndata, such as selected book collections, Wikipedia, and social media data, which\npotentially reflect attitudes and behaviors prevalent in the population. Their text output\nto a request represents a conditional probability based on the training data and the\nspecific contextual information provided in the request. Thus, LLMs might serve as a\nnovel method of collecting data about public opinion. Synthetic samples generated by\nLLMs might be particularly useful for collecting data faster and at lower cost compared"}, {"title": "", "content": "to surveys and might allow for covering different population segments, including those\nthat are potentially hard to reach with surveys. Such samples can be created by\nsequentially feeding individual socio-demographic, socio-economic, and/or attitudinal\ninformation of specific persons to the LLM and asking it to respond to survey questions\nfrom the respective person's perspective.\nWhile there has been an increasing number of studies about the use of LLMs for\npopulation inference, most existing research has focused on the United States. We\nargue that the generalizability of such findings beyond the US population is\nquestionable because the suitability of LLMs for estimating public opinion depends on a\nvariety of contextual factors associated with the target population. These factors include\n(1) the prevalence of native-language training data, (2) a country's political and societal\nstructure, which has a complex relationship with public opinion that can vary across\ncountries and might not be equally reflected in the training data, as well as (3) structural\ndifferences between the target population and the population reflected in the training\ndata. Polling voting behavior is one relevant and much-researched example of public\nopinion estimation. It is also an example that is heavily dependent on the national social\nand political context. For example, the dynamics of vote choice are markedly different in\na multi-party system, such as Germany's, than in the US two-party system. At the same\ntime, due to its linguistic and socio-demographic presence online and its socio-political\nstructure, Germany presents a reasonable middle ground for the examination of LLM\npublic opinion estimation, the results of which can be telling for societies represented in\nLLM training data even less."}, {"title": "", "content": "In this paper, we examine to what extent LLMs can estimate public opinion in\nGermany by addressing the following research questions:\nRQ1. Do LLM-based samples provide similar estimates of voting behavior as\nnational election studies?\nRQ2. How do LLMs' estimates of voting behavior deviate from national election\nstudies for different subgroups of the population?\nFollowing the approach employed by Argyle et al. (2023), we create a synthetic\nsample of eligible voters based on data from the German Longitudinal Election Study\n(GLES). These personas include individual-level information on variables that in the\nliterature have been found to be important predictors of voting behavior \u2013\ndemographics, party affiliations, and views on politically salient issues, such as\nimmigration. Based on this information, we prompt the LLM GPT-3.5 to predict the\nvoting behavior of each individual. From the LLM responses, we extract the predicted\nvote choices for each persona and compare them to the voting behavior reported by\nrespondents in the GLES data. Thus, our primary goal in this paper is not to assess\nwhether LLMs can predict actual election outcomes, but whether they can infer\nindividual voting behavior and arrive at estimates comparable to those made with\nindividual-level survey data.\nUsing the example of voting behavior, we provide a twofold methodological\ncontribution to public opinion estimation using LLMs. We (1) show how a popular LLM\nperforms in estimating voting behavior in Germany compared to survey data, and (2)\nanalyze which individual-level factors influence its predictions. Overall, in investigating\nthe suitability of using LLMs for public opinion estimation in a new context, our study"}, {"title": "", "content": "contributes to the growing body of research on the extent to which LLMs can be\nleveraged for research in the social sciences."}, {"title": "2. Background", "content": "In survey research, synthesizing respondent samples is one especially relevant\napplication of LLMs. Such samples would allow for pre-testing survey questions on\ndifferent population segments faster and cheaper. They could also potentially\nsupplement or even replace survey-based data collection and public opinion estimation\nbased on human samples, for example, in the context of political polls estimating voting\nbehavior. The underlying idea survey researchers leverage is that LLMs are based on\nhuman-created data and could therefore potentially reflect humans' underlying attitudes\nand behaviors.\nTrained on vast amounts of text data, LLMs generate a conditional probability\ndistribution of how likely given tokens, i.e., particles of words, are followed by specific\nother tokens. Presented with a string of words (LLM input), LLMs then draw on this\nprobability distribution to predict words that are likely to follow (LLM output). For\nexample, given the input \u201cIn the 2020 US presidential elections, I voted for\u201d, LLMs are\nmore likely to complete the sentence with \u201cthe Democratic candidate\u201d or \u201cthe\nRepublican candidate\" than with other terms unrelated to candidates or parties. The\nsentence is more or less likely to be completed with either vote choice depending on the\ntraining data, the configuration of the LLM algorithm, as well as any other information\nprovided as input. LLMs are based on large, selected corpora of internet-sourced data,\nsuch as selected websites, book collections, and social media data, for example Reddit\ndata from selected subreddits (see e.g. Brown et al., 2020). As this training data"}, {"title": "", "content": "includes factual, attitudinal, and behavioral data about people, LLMs might provide a\nnovel method for estimating public opinion in a population by creating synthetic\nsamples: LLMs can be prompted repeatedly to answer survey questions, mimicking\nhuman respondents by providing individual-level characteristics as input. The\ndistribution of responses provided in the output could serve as an estimate of the\npopulation. However, as of yet, widely-used LLMs do not learn from new data in\nreal-time, but instead are trained on historical data up to a certain time point (see e.g.\nOpenAl, 2023). Therefore, these LLMs cannot take into account new information on\ncurrent events that might influence public opinion.\nSeveral recent studies have investigated the potential use of LLMs for replicating\nor replacing human samples in public opinion research, particularly in the area of\npolitical polling. For example, Argyle et al. (2023) prompted GPT-3 to respond to survey\nquestions from the American National Election Study (ANES), reflecting different\ndemographic subgroups of the population. The study found that the LLM-generated\nresponses, on aggregate, closely matched the actual responses in the ANES data, and\nsuggests that LLMs might even be able to estimate public opinion and voting behavior\nfor time points exceeding their own training data. Similarly, Chu et al. (2023) showed\nthat BERT, when trained on news media data, can emulate the attitudes of US\nsubpopulations who consumed news media. Benchmarking the LLM responses against\ndistributions from several surveys by Pew Research Center and the University of\nMichigan, their findings are robust to prompt wording and variation in media input. Other\nstudies, however, have come to conflicting conclusions. For example, having GPT-3.5\nimpersonate ANES respondents and answer a set of survey items, the results by"}, {"title": "", "content": "Bisbee et al. (2024) were mixed. While the average item scores produced by the LLM\nwere similar to those obtained from the survey data, the LLM-based results had a\nsmaller variance and resulted in different coefficients when regressing the prompt\nvariables on the response. Furthermore, the responses were not robust to prompt\nwording and across time. Dominguez-Olmedo et al. (2023) had a large range of\ndifferent language models respond to an entire questionnaire, benchmarking against the\nAmerican Community Survey. In this study, however, even the aggregate estimates\nderived from the LLM responses did not match those of the human population. Finally,\nSanturkar et al. (2023), using the American Trends Panel survey, discovered substantial\nmisalignments for specific subgroups. Testing several LLMs\u2019 \u201cdefault\u201d responses, not\nproviding any further contextual information, as well as responses when prompting the\nLLMs to impersonate certain subgroups, the authors concluded that LLM-based\nsamples cannot replicate human samples.\nA limitation of these existing studies is that they almost exclusively focus on the\nUS population. To better understand the conditions under which LLMs can be used for\npublic opinion research, it is crucial to assess whether they can also be applied for\nresearch in other national contexts. Several factors might limit the generalizability of\nprevious findings beyond the United States.\nFirst, it is likely that LLMs are better able to emulate public opinion for the United\nStates than for other countries due to country-level factors associated with the training\ndata. Since LLMs are trained on text data from the internet, the amount of available\nnative-language training data for developing LLMs is considerably smaller for any\ncountry with a native language other than English. For example, less than five percent"}, {"title": "", "content": "of content on the internet is estimated to be German, compared to English with over\n50% (W3Tech, 2023). It is unclear how LLMs transfer their \u201cknowledge\u201d between\ntraining data in different languages and what \u201cknowledge\u201d is accessed when prompted\nin English about a non-English-speaking population (see e.g. Nie et al., 2024a,b, Lai et\nal., 2023). In either of these two processes, native, potentially more authentic,\n\"knowledge\u201d risks being underrepresented if LLMs are only accessing English-language\ntraining data. Moreover, a country's societal and political structures may differentially\naffect the determinants of public opinion. These idiosyncratic relationships may not be\nsufficiently represented in LLM training data. For example, Argyle et al. (2023) showed\nthat GPT-3 mirrored the relationships between subgroup characteristics and voting\nbehavior in the US two-party system. It is unclear, however, whether these findings can\nbe extended to multi-party systems, where the dynamics of voting behavior can follow\nfundamentally different patterns (Campbell et al., 1960; Lazarsfeld et al., 1944).\nPredicting voting behavior in multi-party parliamentary democracies is inherently more\ndifficult than predicting the two-party, first-past-the-post presidential democracy of the\nUnited States. Statistically, at a very basic level, the probability of making a correct\nprediction is inversely proportional to the number of parties competing. Moreover, the\nhigher complexity of multi-party-systems, also in terms of more potential combinations\nof issue positions, make the voting decision more complex for voters. The clear binary\nalignment of certain issue positions is not obvious outside of the United States: \u201eThe\nnext time a Martian visits earth, try to explain to him why those who favor allowing the\nelimination of a fetus in the mother's womb also oppose capital punishment. Or try to\nexplain to him why those who accept abortion are supposed to be favorable to high"}, {"title": "", "content": "taxation but against a strong military\u201c (Taleb, 2007, p. 16). Finally, in many multi-party\nsystems, proportional representation and minimum thresholds create voters who vote\nstrategically. These complex decision-making processes are often made spontaneously,\nin response to parties' popularities in current polls and the specific voting district, and\ntherefore not explicitly discussed online. The concept of \u201cswing voters\u201d therefore is\nslightly different from that of the United States, as it is simply more common for voters to\nswitch parties depending on the context (regarding policy issues and party popularity) in\nwhich the election takes place. Not the least because it is usually the more politically\ninterested and polarized who post on the Internet (e.g. Kim et al., 2021, Tucker et al.,\n2021, Muhlberger 2003), Internet discussions, however, often have the tendency to\nconflate political complexities to two camps (e.g. Yarchi et al., 2021). It is therefore likely\nthat LLMs cannot mirror the more complex decision-making process in multi-party\nsystems given the available training data. Additionally, different social structures can\nlead to different policy-issue salience and conflicts. When inferring from information on\ndemographic or attitudinal subgroups to voting behavior without sufficient \u201ctraining\u201d in\nthese differences, it thus is likely that LLMs wrongly project the more prominent interest\nconflicts of the United States onto other contexts.\nSecond, it is very likely that the training data is affected by coverage bias. The\ndifference between the general population and the population of internet users, the\nso-called \"digital divide\u201d (e.g. Lutz, 2019), may impact how representative the training\ndata is of the population (see e.g. Clemmensen et al., 2023). For example, the\nsocio-demographic digital divide in Germany is slightly different from that in the United\nStates (see Schumacher & Kent, 2020). As the composition of the online and offline"}, {"title": "", "content": "populations differs between regions and countries (see also International\nTelecommunication Union, 2022), a country's societal structure may affect the bias in\nthe LLM training data used to estimate public opinion. In addition, there may be\nstructural and attitudinal differences related to how people in a given society use the\ninternet, that is, between those who actively produce or contribute to the text captured\nand more passive internet users in general, and between the authors of texts selected\nfor training LLMs and other internet users specifically. For instance, the training data for\nGPT-3 is not a random sample of internet text, but heavily relies on very few sources,\nincluding Wikipedia, Reddit, and two collections of books (Brown et al., 2020) \u2013 sources\nthat generally tend to be authored by rather homogenous communities: For example,\nWikipedia reports that a plurality (20%) of its editors reside in the United States, edit the\nEnglish Wikipedia (76%), and that, among editors of the English Wikipedia, 84% are\nmale (Wikipedia, 2023, c.f. Hill & Shaw, 2013). Overall, the \u201cknowledge sources\u201d of\nLLMs are heavily concentrated on the English-speaking, US context, which are then\nreflected in their outputs (Johnson et al., 2022).\nThese factors converge in what can be described as a \"black box\u201d of LLMs\u2019\ninternal workings. In this paper, we seek to empirically assess whether or not previous\nfindings regarding public opinion estimation with LLMs can be generalized in the first\nplace, not why they are (not) generalizable, as empirically testing the latter is not only\ncontingent on the former, but would also require a broader scope and insights into the\nLLM \"black box\u201d that the research community does not currently have.\nAlthough there has been some cross-national and cross-lingual research on\nattitudinal biases of LLMs, these studies either did not explicitly estimate public opinion"}, {"title": "", "content": "in general or did not do so for different population subgroups. For example, Motoki et al.\n(2023) and Hartmann et al. (2023) found that GPT's default political orientation is biased\ntowards left or progressive ideologies in several two- and multi-party systems.\nPrompting ChatGPT with political questions that can be mapped onto ideological\ncoordinates, Motoki et al. (2023) compared its responses given without any context to\nthose it gave impersonating a partisan and found that the context-less default was more\nsimilar to the left partisan. However, the authors did not investigate the individual\nattitudes of the general public, but instead showcased what GPT \u201cbelieves\u201d a-priori\npartisans' political ideology to be (Motoki et al., 2023) or extrapolated from ChatGPT's\nresponses to voting advice application questions to its likely vote choice (Hartmann et\nal., 2023). Durmus et al.'s (2023) cross-national study is closer to the synthetic-sample\napproach. The authors tested a custom LLM on entire questionnaires, both its default\nand when impersonating people from different countries. When comparing the LLM\nresponses to several cross-national survey datasets (Pew Global Attitudes and the\nWorld Values Survey), they found that the LLM default responses tended to be more\nsimilar to the American and European benchmark data and reflected harmful\ncountry-level stereotypes for the other countries. Translations to a country's target\nlanguage did not always improve the LLM responses' similarity to its speakers' attitudes.\nBut while Durmus et al. (2023) compared English to Russian, Chinese, and Turkish\nprompting, the authors only used generic country personas (\u201cHow would someone from\n[country] answer this question?\u201d), without considering specific subgroups, allowing only\nfor aggregate cross-country comparisons. Thus, it remains unclear to what extent LLMs"}, {"title": "", "content": "can be used for estimating individual-level public opinion outside the much-researched,\ntwo-party, English-dominated context of the United States.\nIn our study, we assess LLMs' suitability for estimating public opinion in Germany\nby focusing on voting behavior, which is a frequently studied outcome of interest in\npublic opinion research. Germany serves as an example of a Western European\ndemocracy, with public opinion formed in the context of not two, but several political\nparties. Germany has a parliamentary electoral system with proportional representation\nand its multi-party system is currently characterized by six parties (Schmitt-Beck et al.,\n2022a): the center-right Christian conservatives (CDU/CSU), the center-left Social\nDemocrats (SPD), the right-of-center, conservative-liberal Free Democrats (FDP), the\nleft-of-center, environmentalist Green party (Greens), the Left party, and, more recently,\nthe far-right \"protest\u201d party \u201cAlternative for Germany\u201d (AfD). Moreover, it is an example\nof a country using a language not as dominant in online discourse as English but still\nrelevant enough to allow for testing of our training data-related arguments, that is,\ndifferences in country-level factors and coverage biases affecting the training data. In\nwhat can be considered a \u201cnext-best\u201d case scenario for LLM public opinion estimation,\nGermany presents a middle ground between the United States and other societies\nwhich are represented in the training data even less, which might pose a challenge for\ntesting synthetic sampling. Findings in LLM public opinion estimation for Germany can\nbe informative for countries with similar characteristics, and even those more\nunderrepresented in the training data in terms of language and society: detecting\nlimitations in LLMs' ability to estimate public opinion in this context would make it likely"}, {"title": "", "content": "that this ability is even more limited in more structurally complex, under-researched, or\nunderrepresented contexts.\nThe social structures dividing the German electorate differ substantially from\nthose characterizing the United States (see, e.g., Lipset & Rokkan, 1967, Brooks et al.,\n2006, Ford & Jennings, 2020, Sass & Kuhnle, 2023). Moreover, the determinants of\nvoting behavior on the micro-level play out in a different way than in the US context:\nPartisanship and traditional socio-economic and religious cleavages and their impact on\nvoting behavior have declined (Dalton, 2014, Schmitt-Beck et al., 2022a,b, Berglund et\nal., 2005, Franklin et al., 2004, Jansen et al., 2013, Elff & Ro\u00dfteutscher, 2011). At the\nsame time, the socio-cultural dimension (Inglehart, 1977, Schmitt-Beck et al., 2022a)\nhas become more important for voting behavior (Dalton, 2018). As a result of these\ndevelopments, there are signs of situational issue-voting (Schoen et al., 2017) based on\ncurrent salient and divisive topics, such as immigration (e.g., Kriesi et al., 2006)."}, {"title": "3. Data and Methods", "content": "In order to examine to what extent LLMs can estimate public opinion in Germany, we\nsimulate a sample of eligible voters in Germany using GPT-3.5. We echo existing\nresearch designs in benchmarking the LLM's predicted vote choices against those\nreported by the survey respondents in the German Longitudinal Election Study (GLES,\nsee Appendix I for details). While surveys are not free from errors, they are currently the\nbest available data source on public opinion on the individual level, allowing us to\nassess LLM performance for different subgroups of the population."}, {"title": "3.1. Data Collection", "content": "To ensure comparability with previous studies (Argyle et al., 2023, Bisbee et al.,\n2024; Dominguez-Olmedo et al., 2023, Hartmann et al., 2023; Motoki et al., 2023,\nSanturkar et al., 2023), we rely on GPT, which also has the advantage of being one of\nthe largest language models available and being broadly accessible, making it a likely\nchoice for future applications in academia, industry, and by the public. We choose the\n2017 German general election because it definitely occurred before the training data\ncutoff for our specific LLM in June 2021 (OpenAl, n.d., a), with information about the\nelection's context thereby likely included in the training data. If we find limitations in\nGPT's ability for estimating voting behavior for an election that occurred within the range\nof its training data, we cannot expect the LLM to perform well in predicting public\nopinion in contexts beyond its training data."}, {"title": "Benchmark data and LLM selection", "content": "For the prompts provided to GPT-3.5, we create personas individually simulating\neach of the 1,905 voting-eligible participants in the 2017 post-election cross-section of\nthe GLES who reported their vote choice (Ro\u00dfteutscher et al., 2019). The personas\ninclude individual-level information on 13 of the most common factors associated with\nvoting behavior as identified in the literature about electoral behavior in Germany (c.f.\nSchmitt-Beck et al., 2022a, Schmitt-Beck et al., 2022b, Schoen et al., 2017, Klein,\n2014). These variables comprise age, gender, educational attainment, income,\nemployment status, residence in East/West Germany, religiosity, ideological left-right\nself-placement, (strength of) political partisanship, attitude towards immigration, and"}, {"title": "Prompt creation", "content": "attitude towards income inequality. Missing values on any of the variables are imputed\nfor n = 377 respondents (20 % of the sample) using multivariate imputation by chained\nequations (van Buuren & Groothuis-Oudshoorn, 2011). As a robustness check, we\nadjust the prompt using only the non-imputed variables for the respondents with missing\nvalues and compare the results (see Appendix X). We then feed these personas as\nprompts to GPT-3.5 in German, using the completions-API, alongside the request to\ncomplete the last sentence with the respective person's vote choice in the 2017 German\nparliamentary elections. An example prompt is shown below, translated to English for\nillustrative purposes (see Appendix II for the German original).\nWe choose to prompt the LLM in German because the aim of our study is to\nexamine the usability of LLM-generated synthetic samples for public opinion estimation\nin a non-US- and/or -English context, in order to inform applications outside of the US.\nNot all local public opinion items are available in English with a faithful translation and\ntesting of concepts. From a normative point of view, requiring an instrument to be\ntranslated to English for LLMs to be usable is questionable, as it risks further"}, {"title": "", "content": "marginalizing other languages \u2013 also when considering LLMs learn from their\ninteractions with human input. Indeed, it is unclear whether English-language prompting\nwould yield better results due to the larger amount of training data. As we have argued,\none could conversely expect an LLM to more closely approximate attitudes in the target\npopulation when prompted to access those probabilities it has learned from native\nlanguage training data, as these may be more likely to represent \u201cauthentic\u201d attitudes.\nHowever, as native-language training data is unequally distributed across target\npopulations, we expect these approximations to be comparatively worse than for a\ntarget population whose native language is English (see also Durmus et al., 2023). We\nleave a comparison of results when using English versus native-language prompting to\nfuture research, as it would be out of the scope of the current paper."}, {"title": "LLM configuration", "content": "Based on the outputs of a pilot test (see Appendix III for details), we calibrate\nGPT-3.5's text-davinci-003 to a temperature of 0.9 and a response length of maximum\n30 tokens. We choose a high temperature to be in line with similar studies (e.g. Argyle\net al., 2023, Bisbee et al., 2024) and to simulate the non-determinism in human\nresponses to survey questions (e.g., Zaller, 1992). We collect our data in July 2023\n(main sample) and November 2023 (robustness checks). Since the release of GPT-3.5\nand its API, OpenAl has performed several changes to both the language model and its\ndata accessibility, including deprecating the possibility of storing token probabilities via\nthe API, that is, the probability with which a sentence is completed with the selected"}, {"title": "", "content": "completion token. However, research suggests that first-token probabilities do not\nalways match completions when prompting an LLM with survey questions, especially for\nsensitive topics that are more likely to induce a refusal from the LLM (Wang et al.,\n2024). First-tokens also are more sensitive to the prompt format than text output. These\nlimitations make first-token-probabilities an infeasible evaluation metric. To nevertheless\naccount for the probabilistic nature of GPT's responses beyond a single text completion,\nwe adopt procedures established in multiple imputation (Rubin, 2018). Specifically, we\nsample five completions per persona and estimate the variance between these\nsamples. By using multiple completions, we can investigate the range and variability of\nGPT's outputs. This variance analysis helps us grasp the model's behavior and the\nreliability of its responses, providing insights into the consistency and robustness of the\nmodel's text generation. This way, we account for both human (temperature) and LLM\n(number of samples) randomness in our estimates. Our data thus includes 9525\nLLM-generated completions."}, {"title": "Vote choice extraction", "content": "We then extract the party names from the LLM completions as defined by a set of\naccepted keywords per party (see Appendix IV), also considering non-voters and invalid\nvotes. 1,427 completions initially did not contain a vote choice. For these, we re-prompt\nthe LLM up to two times, replacing the respective initial completion, resulting in 87 or\n0.9% of final completions not containing a vote choice (see Appendix V for details and\nAppendix X for an investigation of systematic patterns in these personas/completions)."}, {"title": "3.2. Analysis", "content": "We compare the survey-reported and LLM-generated vote choices to investigate the\nextent to which the responses differ in terms of vote choice as well as how the two data\nsources weigh the prompt variables in estimating vote choice. This approach allows us\nto not only assess whether GPT-3.5 is able to estimate the voting behavior of the\nGerman general population on aggregate, but also whether it can make equally\naccurate estimates for different population subgroups.\nTo tackle our first research question, we compare the aggregate distribution of\nvote shares across parties according to GPT-3.5 to that based on GLES data. We also\nestimate multinomial regression models of the prompting variables on voting behavior\nas reported in GLES and predicted by GPT-3.5, respectively. These models serve two\npurposes: Relating to our first research question, we evaluate GPT-3.5's predictive\nperformance by comparing its predictions to the predicted values of the GLES-based\nregression model. We do this by calculating precision, recall, and F1 scores overall and\nper party, for both the LLM-based predictions and the GLES model predictions. To\naddress our second research question, we also compare the models in terms of effects\nof specific individual characteristics, as specified by the prompting variables, on voting\nbehavior as reported by GPT-3.5 and GLES, respectively.\nFor estimating the models, we fit maximum conditional likelihood models based\non a neural network with a single hidden layer (Venables & Ripley, 2002). For all\nregression models, we exclude 78 respondents for whom at least one of the five"}, {"title": "", "content": "GPT-samples did not contain an explicit vote choice, in order to ensure comparability\nacross samples, and treat ordinal independent variables with at least five categories as\nnumeric. In order to obtain just one estimate from the five GPT samples, we employ\nvariance estimation as established in multiple imputation research (Rubin 2018). For\neach analytical method, we calculate each estimate separately for each sample, and\nthen aggregate across the five samples to obtain the average estimate and total\nstandard error. For example, for our regression models, we run five separate\nregressions, one per sample, and compute the average coefficient and standard error\n(as $(SE(\\hat{\\beta}))^2 + 1.2\\sigma_b^2)^{\\frac{1}{2}}$ (Rubin 2018) in order to construct confidence intervals.\nAll analyses are conducted using the software R (R Core Team, 2023), version\n4.3.0, especially the packages tidyverse (Wickham et al., 2019), mice (van Buuren &\nGroothuis-Oudshoorn, 2011), rgpt3 (Kleinberg, 2023), nnet (Venables & Ripley, 2022),\nand marginaleffects (Arel-Bundock, 2023)."}, {"title": "4. Results", "content": "On aggregate, the GPT-based distribution of vote shares across parties differs markedly\nfrom that of the national election poll. Compared to the GLES sample, GPT-3.5\noverestimates the share of Green, Left, and non-voters, while underestimating the share"}, {"title": "4.1. RQ1: Do LLM-based samples provide similar estimates of voting behavior as national election studies?"}]}