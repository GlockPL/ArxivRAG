{"title": "Vox Populi, Vox AI? Using Language Models to Estimate German Public Opinion", "authors": ["Leah von der Heyde", "Anna-Carolina Haensch", "Alexander Wenz"], "abstract": "The recent development of large language models (LLMs) has spurred discussions about whether LLM-generated \u201csynthetic samples\u201d could complement or replace traditional surveys, considering their training data potentially reflects attitudes and behaviors prevalent in the population. A number of mostly US-based studies have prompted LLMs to mimic survey respondents, with some of them finding that the responses closely match the survey data. However, several contextual factors related to the relationship between the respective target population and LLM training data might affect the generalizability of such findings. In this study, we investigate the extent to which LLMs can estimate public opinion in Germany, using the example of vote choice. We generate a synthetic sample of personas matching the individual characteristics of the 2017 German Longitudinal Election Study respondents. We ask the LLM GPT-3.5 to predict each respondent's vote choice and compare these predictions to the survey-based estimates on the aggregate and subgroup levels. We find that GPT-3.5 does not predict citizens' vote choice accurately, exhibiting a bias towards the Green and Left parties. While the LLM captures the tendencies of \u201ctypical\u201d voter subgroups, such as partisans, it misses the multifaceted factors swaying individual voter choices. By examining the LLM-based prediction of voting behavior in a new context, our study contributes to the growing body of research about the conditions under which LLMs can be leveraged for studying public opinion. The findings point to disparities in opinion representation in LLMs and underscore the limitations in applying them for public opinion estimation.", "sections": [{"title": "1. Introduction", "content": "The recent development and large-scale proliferation of large language models (LLMs),\nsuch as OpenAl's GPT (OpenAl et al., 2023) or Meta's Llama (Touvron et al., 2023),\nhave spurred discussions about the extent to which these language models can be\nused for research in the social and behavioral sciences. Researchers have started to\nexplore various applications to facilitate the collection and analysis of survey data.\nExamples include the use of LLMs for questionnaire design and scale development\n(G\u00f6tz et al., 2023; Hernandez & Nie, 2022; Konstantis et al., 2023; Laverghetta &\nLicato, 2023, Lee et al., 2023), conducting interviews (Chopra & Haaland, 2023; Cuevas\nVillalba et al., 2023), coding open-ended survey responses (Mellon et al., 2023; Rytting\net al., 2023), imputing missing data and detecting statistical outliers (Jaimovitch-L\u00f3pez\net al., 2023; Kim & Lee, 2023), detecting non-human respondents in online surveys\n(Lebrun et al., 2023), and data visualization and interpretation (Liew & Mueller, 2022;\nSultanum & Srinivasan, 2023).\nBeyond augmenting survey data collection and analysis, research has also\nstarted to examine to what extent LLMs can be used for making valid inferences about a\npopulation (e.g. Argyle et al., 2023). LLMs are trained on large amounts of internet text\ndata, such as selected book collections, Wikipedia, and social media data, which\npotentially reflect attitudes and behaviors prevalent in the population. Their text output\nto a request represents a conditional probability based on the training data and the\nspecific contextual information provided in the request. Thus, LLMs might serve as a\nnovel method of collecting data about public opinion. Synthetic samples generated by\nLLMs might be particularly useful for collecting data faster and at lower cost compared"}, {"title": "", "content": "to surveys and might allow for covering different population segments, including those\nthat are potentially hard to reach with surveys. Such samples can be created by\nsequentially feeding individual socio-demographic, socio-economic, and/or attitudinal\ninformation of specific persons to the LLM and asking it to respond to survey questions\nfrom the respective person's perspective.\nWhile there has been an increasing number of studies about the use of LLMs for\npopulation inference, most existing research has focused on the United States. We\nargue that the generalizability of such findings beyond the US population is\nquestionable because the suitability of LLMs for estimating public opinion depends on a\nvariety of contextual factors associated with the target population. These factors include\n(1) the prevalence of native-language training data, (2) a country's political and societal\nstructure, which has a complex relationship with public opinion that can vary across\ncountries and might not be equally reflected in the training data, as well as (3) structural\ndifferences between the target population and the population reflected in the training\ndata. Polling voting behavior is one relevant and much-researched example of public\nopinion estimation. It is also an example that is heavily dependent on the national social\nand political context. For example, the dynamics of vote choice are markedly different in\na multi-party system, such as Germany's, than in the US two-party system. At the same\ntime, due to its linguistic and socio-demographic presence online and its socio-political\nstructure, Germany presents a reasonable middle ground for the examination of LLM\npublic opinion estimation, the results of which can be telling for societies represented in\nLLM training data even less."}, {"title": "", "content": "In this paper, we examine to what extent LLMs can estimate public opinion in\nGermany by addressing the following research questions:\nRQ1. Do LLM-based samples provide similar estimates of voting behavior as\nnational election studies?\nRQ2. How do LLMs' estimates of voting behavior deviate from national election\nstudies for different subgroups of the population?\nFollowing the approach employed by Argyle et al. (2023), we create a synthetic\nsample of eligible voters based on data from the German Longitudinal Election Study\n(GLES). These personas include individual-level information on variables that in the\nliterature have been found to be important predictors of voting behavior \u2013\ndemographics, party affiliations, and views on politically salient issues, such as\nimmigration. Based on this information, we prompt the LLM GPT-3.5 to predict the\nvoting behavior of each individual. From the LLM responses, we extract the predicted\nvote choices for each persona and compare them to the voting behavior reported by\nrespondents in the GLES data. Thus, our primary goal in this paper is not to assess\nwhether LLMs can predict actual election outcomes, but whether they can infer\nindividual voting behavior and arrive at estimates comparable to those made with\nindividual-level survey data.\nUsing the example of voting behavior, we provide a twofold methodological\ncontribution to public opinion estimation using LLMs. We (1) show how a popular LLM\nperforms in estimating voting behavior in Germany compared to survey data, and (2)\nanalyze which individual-level factors influence its predictions. Overall, in investigating\nthe suitability of using LLMs for public opinion estimation in a new context, our study"}, {"title": "", "content": "contributes to the growing body of research on the extent to which LLMs can be\nleveraged for research in the social sciences."}, {"title": "2. Background", "content": "In survey research, synthesizing respondent samples is one especially relevant\napplication of LLMs. Such samples would allow for pre-testing survey questions on\ndifferent population segments faster and cheaper. They could also potentially\nsupplement or even replace survey-based data collection and public opinion estimation\nbased on human samples, for example, in the context of political polls estimating voting\nbehavior. The underlying idea survey researchers leverage is that LLMs are based on\nhuman-created data and could therefore potentially reflect humans' underlying attitudes\nand behaviors.\nTrained on vast amounts of text data, LLMs generate a conditional probability\ndistribution of how likely given tokens, i.e., particles of words, are followed by specific\nother tokens. Presented with a string of words (LLM input), LLMs then draw on this\nprobability distribution to predict words that are likely to follow (LLM output). For\nexample, given the input \u201cIn the 2020 US presidential elections, I voted for\u201d, LLMs are\nmore likely to complete the sentence with \u201cthe Democratic candidate\u201d or \u201cthe\nRepublican candidate\" than with other terms unrelated to candidates or parties. The\nsentence is more or less likely to be completed with either vote choice depending on the\ntraining data, the configuration of the LLM algorithm, as well as any other information\nprovided as input. LLMs are based on large, selected corpora of internet-sourced data,\nsuch as selected websites, book collections, and social media data, for example Reddit\ndata from selected subreddits (see e.g. Brown et al., 2020). As this training data"}, {"title": "", "content": "includes factual, attitudinal, and behavioral data about people, LLMs might provide a\nnovel method for estimating public opinion in a population by creating synthetic\nsamples: LLMs can be prompted repeatedly to answer survey questions, mimicking\nhuman respondents by providing individual-level characteristics as input. The\ndistribution of responses provided in the output could serve as an estimate of the\npopulation. However, as of yet, widely-used LLMs do not learn from new data in\nreal-time, but instead are trained on historical data up to a certain time point (see e.g.\nOpenAl, 2023). Therefore, these LLMs cannot take into account new information on\ncurrent events that might influence public opinion.\nSeveral recent studies have investigated the potential use of LLMs for replicating\nor replacing human samples in public opinion research, particularly in the area of\npolitical polling. For example, Argyle et al. (2023) prompted GPT-3 to respond to survey\nquestions from the American National Election Study (ANES), reflecting different\ndemographic subgroups of the population. The study found that the LLM-generated\nresponses, on aggregate, closely matched the actual responses in the ANES data, and\nsuggests that LLMs might even be able to estimate public opinion and voting behavior\nfor time points exceeding their own training data. Similarly, Chu et al. (2023) showed\nthat BERT, when trained on news media data, can emulate the attitudes of US\nsubpopulations who consumed news media. Benchmarking the LLM responses against\ndistributions from several surveys by Pew Research Center and the University of\nMichigan, their findings are robust to prompt wording and variation in media input. Other\nstudies, however, have come to conflicting conclusions. For example, having GPT-3.5\nimpersonate ANES respondents and answer a set of survey items, the results by"}, {"title": "", "content": "Bisbee et al. (2024) were mixed. While the average item scores produced by the LLM\nwere similar to those obtained from the survey data, the LLM-based results had a\nsmaller variance and resulted in different coefficients when regressing the prompt\nvariables on the response. Furthermore, the responses were not robust to prompt\nwording and across time. Dominguez-Olmedo et al. (2023) had a large range of\ndifferent language models respond to an entire questionnaire, benchmarking against the\nAmerican Community Survey. In this study, however, even the aggregate estimates\nderived from the LLM responses did not match those of the human population. Finally,\nSanturkar et al. (2023), using the American Trends Panel survey, discovered substantial\nmisalignments for specific subgroups. Testing several LLMs\u2019 \u201cdefault\u201d responses, not\nproviding any further contextual information, as well as responses when prompting the\nLLMs to impersonate certain subgroups, the authors concluded that LLM-based\nsamples cannot replicate human samples.\nA limitation of these existing studies is that they almost exclusively focus on the\nUS population. To better understand the conditions under which LLMs can be used for\npublic opinion research, it is crucial to assess whether they can also be applied for\nresearch in other national contexts. Several factors might limit the generalizability of\nprevious findings beyond the United States.\nFirst, it is likely that LLMs are better able to emulate public opinion for the United\nStates than for other countries due to country-level factors associated with the training\ndata. Since LLMs are trained on text data from the internet, the amount of available\nnative-language training data for developing LLMs is considerably smaller for any\ncountry with a native language other than English. For example, less than five percent"}, {"title": "", "content": "of content on the internet is estimated to be German, compared to English with over\n50% (W3Tech, 2023). It is unclear how LLMs transfer their \u201cknowledge\u201d between\ntraining data in different languages and what \u201cknowledge\u201d is accessed when prompted\nin English about a non-English-speaking population (see e.g. Nie et al., 2024a,b, Lai et\nal., 2023). In either of these two processes, native, potentially more authentic,\n\"knowledge\u201d risks being underrepresented if LLMs are only accessing English-language\ntraining data. Moreover, a country's societal and political structures may differentially\naffect the determinants of public opinion. These idiosyncratic relationships may not be\nsufficiently represented in LLM training data. For example, Argyle et al. (2023) showed\nthat GPT-3 mirrored the relationships between subgroup characteristics and voting\nbehavior in the US two-party system. It is unclear, however, whether these findings can\nbe extended to multi-party systems, where the dynamics of voting behavior can follow\nfundamentally different patterns (Campbell et al., 1960; Lazarsfeld et al., 1944).\nPredicting voting behavior in multi-party parliamentary democracies is inherently more\ndifficult than predicting the two-party, first-past-the-post presidential democracy of the\nUnited States. Statistically, at a very basic level, the probability of making a correct\nprediction is inversely proportional to the number of parties competing. Moreover, the\nhigher complexity of multi-party-systems, also in terms of more potential combinations\nof issue positions, make the voting decision more complex for voters. The clear binary\nalignment of certain issue positions is not obvious outside of the United States: \u201eThe\nnext time a Martian visits earth, try to explain to him why those who favor allowing the\nelimination of a fetus in the mother's womb also oppose capital punishment. Or try to\nexplain to him why those who accept abortion are supposed to be favorable to high"}, {"title": "", "content": "taxation but against a strong military\u201c (Taleb, 2007, p. 16). Finally, in many multi-party\nsystems, proportional representation and minimum thresholds create voters who vote\nstrategically. These complex decision-making processes are often made spontaneously,\nin response to parties' popularities in current polls and the specific voting district, and\ntherefore not explicitly discussed online. The concept of \u201cswing voters\u201d therefore is\nslightly different from that of the United States, as it is simply more common for voters to\nswitch parties depending on the context (regarding policy issues and party popularity) in\nwhich the election takes place. Not the least because it is usually the more politically\ninterested and polarized who post on the Internet (e.g. Kim et al., 2021, Tucker et al.,\n2021, Muhlberger 2003), Internet discussions, however, often have the tendency to\nconflate political complexities to two camps (e.g. Yarchi et al., 2021). It is therefore likely\nthat LLMs cannot mirror the more complex decision-making process in multi-party\nsystems given the available training data. Additionally, different social structures can\nlead to different policy-issue salience and conflicts. When inferring from information on\ndemographic or attitudinal subgroups to voting behavior without sufficient \u201ctraining\u201d in\nthese differences, it thus is likely that LLMs wrongly project the more prominent interest\nconflicts of the United States onto other contexts.\nSecond, it is very likely that the training data is affected by coverage bias. The\ndifference between the general population and the population of internet users, the\nso-called \"digital divide\u201d (e.g. Lutz, 2019), may impact how representative the training\ndata is of the population (see e.g. Clemmensen et al., 2023). For example, the\nsocio-demographic digital divide in Germany is slightly different from that in the United\nStates (see Schumacher & Kent, 2020). As the composition of the online and offline"}, {"title": "", "content": "populations differs between regions and countries (see also International\nTelecommunication Union, 2022), a country's societal structure may affect the bias in\nthe LLM training data used to estimate public opinion. In addition, there may be\nstructural and attitudinal differences related to how people in a given society use the\ninternet, that is, between those who actively produce or contribute to the text captured\nand more passive internet users in general, and between the authors of texts selected\nfor training LLMs and other internet users specifically. For instance, the training data for\nGPT-3 is not a random sample of internet text, but heavily relies on very few sources,\nincluding Wikipedia, Reddit, and two collections of books (Brown et al., 2020) \u2013 sources\nthat generally tend to be authored by rather homogenous communities: For example,\nWikipedia reports that a plurality (20%) of its editors reside in the United States, edit the\nEnglish Wikipedia (76%), and that, among editors of the English Wikipedia, 84% are\nmale (Wikipedia, 2023, c.f. Hill & Shaw, 2013). Overall, the \u201cknowledge sources\u201d of\nLLMs are heavily concentrated on the English-speaking, US context, which are then\nreflected in their outputs (Johnson et al., 2022).\nThese factors converge in what can be described as a \"black box\u201d of LLMs\u2019\ninternal workings. In this paper, we seek to empirically assess whether or not previous\nfindings regarding public opinion estimation with LLMs can be generalized in the first\nplace, not why they are (not) generalizable, as empirically testing the latter is not only\ncontingent on the former, but would also require a broader scope and insights into the\nLLM \"black box\u201d that the research community does not currently have.\nAlthough there has been some cross-national and cross-lingual research on\nattitudinal biases of LLMs, these studies either did not explicitly estimate public opinion"}, {"title": "", "content": "in general or did not do so for different population subgroups. For example, Motoki et al.\n(2023) and Hartmann et al. (2023) found that GPT's default political orientation is biased\ntowards left or progressive ideologies in several two- and multi-party systems.\nPrompting ChatGPT with political questions that can be mapped onto ideological\ncoordinates, Motoki et al. (2023) compared its responses given without any context to\nthose it gave impersonating a partisan and found that the context-less default was more\nsimilar to the left partisan. However, the authors did not investigate the individual\nattitudes of the general public, but instead showcased what GPT \u201cbelieves\u201d a-priori\npartisans' political ideology to be (Motoki et al., 2023) or extrapolated from ChatGPT's\nresponses to voting advice application questions to its likely vote choice (Hartmann et\nal., 2023). Durmus et al.'s (2023) cross-national study is closer to the synthetic-sample\napproach. The authors tested a custom LLM on entire questionnaires, both its default\nand when impersonating people from different countries. When comparing the LLM\nresponses to several cross-national survey datasets (Pew Global Attitudes and the\nWorld Values Survey), they found that the LLM default responses tended to be more\nsimilar to the American and European benchmark data and reflected harmful\ncountry-level stereotypes for the other countries. Translations to a country's target\nlanguage did not always improve the LLM responses' similarity to its speakers' attitudes.\nBut while Durmus et al. (2023) compared English to Russian, Chinese, and Turkish\nprompting, the authors only used generic country personas (\u201cHow would someone from\n[country] answer this question?\u201d), without considering specific subgroups, allowing only\nfor aggregate cross-country comparisons. Thus, it remains unclear to what extent LLMs"}, {"title": "", "content": "can be used for estimating individual-level public opinion outside the much-researched,\ntwo-party, English-dominated context of the United States.\nIn our study, we assess LLMs' suitability for estimating public opinion in Germany\nby focusing on voting behavior, which is a frequently studied outcome of interest in\npublic opinion research. Germany serves as an example of a Western European\ndemocracy, with public opinion formed in the context of not two, but several political\nparties. Germany has a parliamentary electoral system with proportional representation\nand its multi-party system is currently characterized by six parties (Schmitt-Beck et al.,\n2022a): the center-right Christian conservatives (CDU/CSU), the center-left Social\nDemocrats (SPD), the right-of-center, conservative-liberal Free Democrats (FDP), the\nleft-of-center, environmentalist Green party (Greens), the Left party, and, more recently,\nthe far-right \"protest\u201d party \u201cAlternative for Germany\u201d (AfD). Moreover, it is an example\nof a country using a language not as dominant in online discourse as English but still\nrelevant enough to allow for testing of our training data-related arguments, that is,\ndifferences in country-level factors and coverage biases affecting the training data. In\nwhat can be considered a \u201cnext-best\u201d case scenario for LLM public opinion estimation,\nGermany presents a middle ground between the United States and other societies\nwhich are represented in the training data even less, which might pose a challenge for\ntesting synthetic sampling. Findings in LLM public opinion estimation for Germany can\nbe informative for countries with similar characteristics, and even those more\nunderrepresented in the training data in terms of language and society: detecting\nlimitations in LLMs' ability to estimate public opinion in this context would make it likely"}, {"title": "", "content": "that this ability is even more limited in more structurally complex, under-researched, or\nunderrepresented contexts.\nThe social structures dividing the German electorate differ substantially from\nthose characterizing the United States (see, e.g., Lipset & Rokkan, 1967, Brooks et al.,\n2006, Ford & Jennings, 2020, Sass & Kuhnle, 2023). Moreover, the determinants of\nvoting behavior on the micro-level play out in a different way than in the US context:\nPartisanship and traditional socio-economic and religious cleavages and their impact on\nvoting behavior have declined (Dalton, 2014, Schmitt-Beck et al., 2022a,b, Berglund et\nal., 2005, Franklin et al., 2004, Jansen et al., 2013, Elff & Ro\u00dfteutscher, 2011). At the\nsame time, the socio-cultural dimension (Inglehart, 1977, Schmitt-Beck et al., 2022a)\nhas become more important for voting behavior (Dalton, 2018). As a result of these\ndevelopments, there are signs of situational issue-voting (Schoen et al., 2017) based on\ncurrent salient and divisive topics, such as immigration (e.g., Kriesi et al., 2006)."}, {"title": "3. Data and Methods", "content": "In order to examine to what extent LLMs can estimate public opinion in Germany, we\nsimulate a sample of eligible voters in Germany using GPT-3.5. We echo existing\nresearch designs in benchmarking the LLM's predicted vote choices against those\nreported by the survey respondents in the German Longitudinal Election Study (GLES,\nsee Appendix I for details). While surveys are not free from errors, they are currently the\nbest available data source on public opinion on the individual level, allowing us to\nassess LLM performance for different subgroups of the population."}, {"title": "3.1. Data Collection", "content": "Benchmark data and LLM selection"}, {"title": "", "content": "To ensure comparability with previous studies (Argyle et al., 2023, Bisbee et al.,\n2024; Dominguez-Olmedo et al., 2023, Hartmann et al., 2023; Motoki et al., 2023,\nSanturkar et al., 2023), we rely on GPT, which also has the advantage of being one of\nthe largest language models available and being broadly accessible, making it a likely\nchoice for future applications in academia, industry, and by the public. We choose the\n2017 German general election because it definitely occurred before the training data\ncutoff for our specific LLM in June 2021 (OpenAl, n.d., a), with information about the\nelection's context thereby likely included in the training data. If we find limitations in\nGPT's ability for estimating voting behavior for an election that occurred within the range\nof its training data, we cannot expect the LLM to perform well in predicting public\nopinion in contexts beyond its training data.\nPrompt creation\nFor the prompts provided to GPT-3.5, we create personas individually simulating\neach of the 1,905 voting-eligible participants in the 2017 post-election cross-section of\nthe GLES who reported their vote choice (Ro\u00dfteutscher et al., 2019). The personas\ninclude individual-level information on 13 of the most common factors associated with\nvoting behavior as identified in the literature about electoral behavior in Germany (c.f.\nSchmitt-Beck et al., 2022a, Schmitt-Beck et al., 2022b, Schoen et al., 2017, Klein,\n2014). These variables comprise age, gender, educational attainment, income,\nemployment status, residence in East/West Germany, religiosity, ideological left-right\nself-placement, (strength of) political partisanship, attitude towards immigration, and"}, {"title": "", "content": "attitude towards income inequality. Missing values on any of the variables are imputed\nfor n = 377 respondents (20 % of the sample) using multivariate imputation by chained\nequations (van Buuren & Groothuis-Oudshoorn, 2011). As a robustness check, we\nadjust the prompt using only the non-imputed variables for the respondents with missing\nvalues and compare the results (see Appendix X). We then feed these personas as\nprompts to GPT-3.5 in German, using the completions-API, alongside the request to\ncomplete the last sentence with the respective person's vote choice in the 2017 German\nparliamentary elections. An example prompt is shown below, translated to English for\nillustrative purposes (see Appendix II for the German original).\nI am 28 years old and female. I have a college degree, a medium monthly net\nhousehold income, and am working. I am not religious. Ideologically, I am\nleaning center-left. | rather weakly identify with the Green party. I live in West\nGermany. I think the government should facilitate immigration and take\nmeasures to reduce income disparities.\nDid I vote in the 2017 German parliamentary elections and if so, which party did I\nvote for? I [INSERT]\nWe choose to prompt the LLM in German because the aim of our study is to\nexamine the usability of LLM-generated synthetic samples for public opinion estimation\nin a non-US- and/or -English context, in order to inform applications outside of the US.\nNot all local public opinion items are available in English with a faithful translation and\ntesting of concepts. From a normative point of view, requiring an instrument to be\ntranslated to English for LLMs to be usable is questionable, as it risks further"}, {"title": "", "content": "marginalizing other languages \u2013 also when considering LLMs learn from their\ninteractions with human input. Indeed, it is unclear whether English-language prompting\nwould yield better results due to the larger amount of training data. As we have argued,\none could conversely expect an LLM to more closely approximate attitudes in the target\npopulation when prompted to access those probabilities it has learned from native\nlanguage training data, as these may be more likely to represent \u201cauthentic\u201d attitudes.\nHowever, as native-language training data is unequally distributed across target\npopulations, we expect these approximations to be comparatively worse than for a\ntarget population whose native language is English (see also Durmus et al., 2023). We\nleave a comparison of results when using English versus native-language prompting to\nfuture research, as it would be out of the scope of the current paper.\nLLM configuration\nBased on the outputs of a pilot test (see Appendix III for details), we calibrate\nGPT-3.5's text-davinci-003 to a temperature of 0.9 and a response length of maximum\n30 tokens. We choose a high temperature to be in line with similar studies (e.g. Argyle\net al., 2023, Bisbee et al., 2024) and to simulate the non-determinism in human\nresponses to survey questions (e.g., Zaller, 1992). We collect our data in July 2023\n(main sample) and November 2023 (robustness checks). Since the release of GPT-3.5\nand its API, OpenAl has performed several changes to both the language model and its\ndata accessibility, including deprecating the possibility of storing token probabilities via\nthe API, that is, the probability with which a sentence is completed with the selected"}, {"title": "", "content": "completion token. However, research suggests that first-token probabilities do not\nalways match completions when prompting an LLM with survey questions, especially for\nsensitive topics that are more likely to induce a refusal from the LLM (Wang et al.,\n2024). First-tokens also are more sensitive to the prompt format than text output. These\nlimitations make first-token-probabilities an infeasible evaluation metric. To nevertheless\naccount for the probabilistic nature of GPT's responses beyond a single text completion,\nwe adopt procedures established in multiple imputation (Rubin, 2018). Specifically, we\nsample five completions per persona and estimate the variance between these\nsamples. By using multiple completions, we can investigate the range and variability of\nGPT's outputs. This variance analysis helps us grasp the model's behavior and the\nreliability of its responses, providing insights into the consistency and robustness of the\nmodel's text generation. This way, we account for both human (temperature) and LLM\n(number of samples) randomness in our estimates. Our data thus includes 9525\nLLM-generated completions.\nVote choice extraction\nWe then extract the party names from the LLM completions as defined by a set of\naccepted keywords per party (see Appendix IV), also considering non-voters and invalid\nvotes. 1,427 completions initially did not contain a vote choice. For these, we re-prompt\nthe LLM up to two times, replacing the respective initial completion, resulting in 87 or\n0.9% of final completions not containing a vote choice (see Appendix V for details and\nAppendix X for an investigation of systematic patterns in these personas/completions)."}, {"title": "3.2. Analysis", "content": "We compare the survey-reported and LLM-generated vote choices to investigate the\nextent to which the responses differ in terms of vote choice as well as how the two data\nsources weigh the prompt variables in estimating vote choice. This approach allows us\nto not only assess whether GPT-3.5 is able to estimate the voting behavior of the\nGerman general population on aggregate, but also whether it can make equally\naccurate estimates for different population subgroups.\nTo tackle our first research question, we compare the aggregate distribution of\nvote shares across parties according to GPT-3.5 to that based on GLES data. We also\nestimate multinomial regression models of the prompting variables on voting behavior\nas reported in GLES and predicted by GPT-3.5, respectively. These models serve two\npurposes: Relating to our first research question, we evaluate GPT-3.5's predictive\nperformance by comparing its predictions to the predicted values of the GLES-based\nregression model. We do this by calculating precision, recall, and F1 scores overall and\nper party, for both the LLM-based predictions and the GLES model predictions. To\naddress our second research question, we also compare the models in terms of effects\nof specific individual characteristics, as specified by the prompting variables, on voting\nbehavior as reported by GPT-3.5 and GLES, respectively.\nFor estimating the models, we fit maximum conditional likelihood models based\non a neural network with a single hidden layer (Venables & Ripley, 2002). For all\nregression models, we exclude 78 respondents for whom at least one of the five"}, {"title": "", "content": "GPT-samples did not contain an explicit vote choice, in order to ensure comparability\nacross samples, and treat ordinal independent variables with at least five categories as\nnumeric. In order to obtain just one estimate from the five GPT samples, we employ\nvariance estimation as established in multiple imputation research (Rubin 2018). For\neach analytical method, we calculate each estimate separately for each sample, and\nthen aggregate across the five samples to obtain the average estimate and total\nstandard error. For example, for our regression models, we run five separate\nregressions, one per sample, and compute the average coefficient and standard error\n(as $(SE(\theta_p))^2 + 1.2b^2$ ) (Rubin 2018) in order to construct confidence intervals.\nAll analyses are conducted using the software R (R Core Team, 2023), version\n4.3.0, especially the packages tidyverse (Wickham et al., 2019), mice (van Buuren &\nGroothuis-Oudshoorn, 2011), rgpt3 (Kleinberg, 2023), nnet (Venables & Ripley, 2022),\nand marginaleffects (Arel-Bundock, 2023)."}, {"title": "4. Results", "content": ""}, {"title": "4.1. RQ1: Do LLM-based samples provide similar estimates of voting\nbehavior as national election studies?", "content": "Distribution of vote choice across parties\nOn aggregate, the GPT-based distribution of vote shares across parties differs markedly\nfrom that of the national election poll. Compared to the GLES sample, GPT-3.5\noverestimates the share of Green, Left, and non-voters, while underestimating the share"}, {"title": "4.2. RQ2: How do LLMs' estimates of voting behavior deviate from\nnational election studies for different subgroups of the population?", "content": "Comparing the impact the prompting variables have on actual GPT- versus\nGLES-reported vote choice in multinomial regressions (see Appendix VIII for the full\ntables), the models show that GPT-3.5's predictions of vote choice are reliant on certain\ncues in the prompts, which often do not match the effects the survey data indicates.7\nThe model indicates that GPT-3.5 appears to be taking partisanship into particular\naccount when asked to predict people's vote choice. For example, as shown in Figure\n3, GPT-3.5 exhibits similar positive effects as the GLES model for SPD, Green, FDP,\nLeft, and AfD partisans on the probability of voting for the respective party. Likewise, it\npicks up the signal of left-right ideology for the extremes of the party spectrum: the Left\nand AfD. However, apart from the far-right and -left, GPT-3.5 does not mirror GLES\nwhen it comes to the importance of ideology, for example on voting for the CDU/CSU or\nthe Greens. Moreover, when it comes to partisanship, it does not account for negative"}, {"title": "", "content": "partisanship: For example, the GLES data suggests a systematic underlying pattern\nbetween Green and AfD voters \u2013 Green partisans are significantly less likely to vote for\nthe AfD, and vice versa. In sum, while partisanship and ideology are important factors\ninfluencing voting behavior, GPT-3.5 only picks up on broad trends, without regards for\nmore complex dynamics.\nIts dominant reliance on party identification as a predictor of vote choice can help\nexplain why GPT-3.5 underestimates the vote shares for FDP and AfD, as observed in\nsection 4.1: Although most partisans indeed vote for the party they identify with (see\nTable A8 in Appendix IX), only half of the voters of the FDP, AfD, and small parties also\nidentify with their chosen party. Thus, in presence of a partisan cue, GPT-3.5 predicts\npartisans to vote in line with their party identification. For voters without this cue, its\npredictions falter.\nHowever, overall, there are more differences than similarities in predictors of vote\nchoice between the LLM-generated and survey data. For the remaining attitudinal\nvariables as well as most demographic indicators, GPT-3.5's predictions assume\ndifferent mechanisms than what the GLES data suggests, following general patterns\nidentified by previous research on German voting behavior, but not considering the\nnuances of more complex subgroups. For example, the GPT model, but not the GLES\nmodel, suggests residents of East Germany are more likely to vote for the Left or AfD,\nfemales are more likely to vote for the Greens or Left, and non-workers less likely to\nvote for the SPD or FDP (which traditionally have catered to different segments of the\nworking population). Contrary to the GLES model, it does not consider education and\nincome as important factors for distinguishing the likelihood of voting for the Left,"}, {"title": "", "content": "Greens, or FDP versus the AfD, nor the importance of religiosity for distinguishing\nCDU/CSU from AfD voters.\nSimilar to what can be observed for (negative) partisanship, GPT-3.5 does not\ncapture the complex effects of attitudes towards inequality and immigration. For\nexample, while the GPT data matches the GLES data in indicating that wanting to limit\nimmigration decreases the likelihood of voting for the Greens, the GLES data also\nindicates that such an attitude increases the likelihood of voting for the AfD.\nAll in all, when considering survey data as ground truth, voting behavior in\nGermany depends on a different number and kind of factors than GPT-3.5's predictions\nwould suggest. GPT-3.5 bases its predictions on partisanship as well as indicators for\ncommon subgroups of voters for a specific party. This finding suggests that GPT-3.5\nrelies on rather simplified signals in making its predictions, without necessarily\nconsidering other, more complex mechanisms in the individual voting-decision making\nprocess."}, {"title": "5. Discussion", "content": "Our study assessed the capabilities of a popular large language model (GPT-3.5\ntext-davinci-003) in estimating voting behavior for the German federal elections 2017,\nusing the reported vote choices from the respondents of the German Longitudinal\nElection Study (GLES) data as a benchmark. We created personas simulating every\nindividual respondent in the GLES study. Prompts generated from these personas were\nthen fed to GPT-3.5 via the OpenAl API with a request to complete the personas' vote\nchoice. We compared GPT-3.5's predicted vote choices to respondents' actual vote\nchoices for multiple political parties. Moreover, we conducted a focused subgroup\nanalysis and compared the determinants of voting behavior for the GLES responses\nwith those of the GPT predictions.\nUsing Germany as an example, we have shown that using LLMs for estimating\npublic opinion in a similar way to surveys cannot simply be generalized beyond the\nselect successful applications in the context of the United States. In our findings,\nGPT-3.5 overestimated the survey-reported vote shares for the Greens, the Left, and\nnon-voters by a significant margin, while it underestimated the vote shares for FDP and\nAfD when compared to GLES. The LLM's overall predictive accuracy was modest, with\na matching prediction rate of 0.46. It was notably more accurate for voters of the\nGreens, CDU/CSU, and the Left, but displayed poor predictive power for FDP and AfD\nvoters.\nIn terms of determining factors that influence voting behavior, GPT-3.5's\npredictions largely hinged on straightforward indicators such as strong party\nidentification or ideology. However, when compared to the GLES data, it became"}, {"title": "", "content": "evident that GPT-3.5 deviated substantially on more complex variables like attitudes\ntowards immigration or economic policy, socio-demographic variables, or the particular\ndynamics of partisanship. This discrepancy suggests that while GPT-3.5 may capture\nbroad-brush trends tied to partisanship, it tends to miss out on the nuanced,\nmultifaceted factors that sway individual voter choices, thereby limiting its predictive\naccuracy. As a consequence, relying on LLM-based estimates does not help\nresearchers when predicting voting behavior: Partisans are typically easy to predict as\nlong as they vote in line with their party identification. However, if information on\npartisanship is necessary for GPT-3.5 to make a prediction, and it cannot evaluate\nother, more complex relationships in absence of this information, then not only are\nLLM-based samples not helpful in predicting how non-partisans, weak partisans, or\n\u201cinconsistent\u201d partisans \u2013 all groups who likely swing an election \u2013 will vote. They also\nrisk underestimating vote shares for parties with fewer (reported) partisans. Moreover,\nthe absence of mirroring negative relationships of factors such as partisanship and\nimmigration attitudes could lead to an underestimation of the popularity of certain\nparties when applying LLM-based sampling to estimate public opinion. In our case,\nGPT-3.5 modeled decreasing likelihoods of certain individuals voting for the Greens,\nwithout a correct indication of who these individuals would be more likely to vote for (in\nthis case, the AfD), which, consequently, got underestimated.\nNaturally, predicting voting behavior in a multi-party system is inherently more\ndifficult than in a two-party system. This challenge remains when transferring the task to\nLLMs, and therefore is likely one of the reasons why we cannot expect LLMs to work\nsimilarly well in all contexts. While researchers hope that LLMs can help them uncover"}, {"title": "", "content": "patterns and make predictions where traditional methods struggle, our study\nunderscores the limited applicability of LLM-based synthetic samples. This difficulty is\ncompounded by differences in social structures leading to differential issue conflicts,\nand by limited nuanced, native-language, and target-population-representing Internet\ntext from which LLMs could learn about these complexities.\nConsidering the types of text data that were used to train GPT may shed light on\nits predictive limitations. GPT is trained on a large, but mainstream and not necessarily\ndiverse corpus of text data that includes a selection of websites, books, and other\npublicly available texts (Brown et al., 2020). As a result, the LLM may be predisposed to\nmake predictions based on generalized or commonly represented political beliefs and\nmore typical, well-researched voter groups, hence struggling with accurately predicting\nthe behavior of voters for the AfD and other non-conforming groups.\nThis finding underscores the limitation of applying GPT to electoral predictions\nwithout accounting for the biases and limitations inherent in its training data. It reaffirms\nthat while GPT can provide certain broad insights, it may not be reliable for nuanced,\nsubgroup-specific political predictions. This is in line with previous issues identified with\ngenerative artificial intelligence, which in the context of image generation have been\nfound to reproduce and amplify oftentimes harmful stereotypes and biases (Bianchi et\nal., 2023, Nicoletti & Bass, 2023, Turk, 2023). Ultimately, using LLMs for estimating\npublic opinion risks reinforcing existing biases. Placing our results in the discourse of\nexisting findings, it becomes clear that LLM-based synthetic samples are only useful in\nvery constrained settings. Researchers considering using LLM-synthetic samples to\nstudy public opinion should therefore always critically investigate the applicability of this"}, {"title": "", "content": "approach in the specific context to which they want to apply it before doing so,\nespecially when there is no detailed information on the respective model's training data\nand fine-tuning processes. At this point, there is no clear answer as to whether or when\nLLM-synthetic samples may be useful for public opinion research \u2013 both inside and\noutside the US. Indeed, even studies considering the US come to diverging conclusions\n(c.f. Argyle et al., 2023 and Chu et al., 2023 vs. Bisbee et al., 2024, Dominguez-Olmedo\net al., 2023, and Santurkar et al., 2023). It thus appears that, similar to surveys\nconducted with non-probability samples, LLM-based synthetic samples can get it right\nsometimes, but not reliably so. Considering that LLM responses do not represent latent\nattitudes of an existing target population, but a probability distribution of most-likely next\nwords, even the validity of such measurement may be questioned. It may be argued\nthat LLMs can be useful as long as they provide insights which surveys cannot do.\nHowever, as our as well as other studies have shown, they do not meet this expectation\nso far."}, {"title": "5.1. Limitations", "content": "Our study encountered several limitations that should be considered when interpreting\nthe results and offer avenues for improvement in future research. First, our study did not\ntest the effect of variable ordering in the prompt on the predictions, which was beyond\nthe scope of our study but could have potentially affected the results (Bisbee et al.,\n2024). Future research could engage with work on prompt engineering to optimize\nGPT's predictions. For example, future studies should specify the reference year for\ntime-sensitive variables if it differs from the prompting year (such as age when applied\nto voting behavior), as Bisbee et al. (2024) suggest. Additionally, while our selection of"}, {"title": "", "content": "prompt variables was rooted in existing research on voting behavior in Germany, we\nacknowledge that other factors might contribute to the voting decision-making process\nand thus could enhance the predictive power of the LLM. Furthermore, it is unclear how\nGPT draws on training data in another language than the prompt and completion\nlanguage. Second, recording token probabilities for the vote choice estimation through\nthe OpenAl API (Argyle et al., 2023) was no longer possible at the time of data\ncollection. This constraint highlights the dependency on the functionalities that API\nproviders offer. Third, the study used text-davinci-003 for its analyses, which may be\nless efficient and precise than newer language models. However, this choice was made\nto ensure comparability with existing studies and due to the API availability. The\nconstant \"under the hood\u201d changes to and rapid advancement of these language\nmodels and their APIs, with the text-davinci-003 model expected to be deprecated by\nJanuary 2024 (OpenAI, 2023c), raises concerns about the replicability of research such\nas ours and challenges social scientists to continuously re-evaluate previous findings.\nBeyond the choice of LLM, results may vary depending on the benchmark survey or\nspecific outcome measured. Investigating the influence of these factors falls outside the\nscope of this study and is recommended for future research. Fourth, we recognize that\nour findings for Germany can at best be generalized to Western European\nsocio-political contexts. We argued that our selected case presents a reasonable middle\nground for assessing the suitability of LLMs for public opinion research, as it is\ndistinguishable from the United States on the factors we identified as potentially limiting\nthis suitability. While the limitations in LLM public opinion estimation we have found in\nthe German context can be considered unpromising for more structurally complex,"}, {"title": "", "content": "under-researched, or disadvantaged societies, such research should be explicitly\nconducted. However, benchmarking the LLM's responses against reliable\nindividual-level public opinion survey data implies that experiments such as ours can\nonly soundly be conducted in countries that already have a good survey infrastructure,\ninstead of those countries where LLMs could otherwise alleviate a lack of survey\nresearch resources. On the other hand, while we treated survey data as ground truth, it\nis possible that LLMs' predictions better mirror the target population because they do\nnot suffer from the coverage and measurement error of a particular survey. However, we\nreiterate that in this paper, we were not primarily interested in whether survey- or\nLLM-generated data is better at accurately predicting actual election outcomes. Both\ndata sources have idiosyncratic error sources leading to differences between their\nestimates and the actual election results. Comparing errors across data sources would\nhave presented an additional research question that would have been beyond the\nscope of this paper, but provides an opportunity for future work."}, {"title": "5.2. Outlook", "content": "This paper contributes to the rapidly growing field of computational social science using\nLLMs. Many other aspects and conditions of using LLMs for public opinion research are\nyet to be explored. For example, investigating the suitability of using LLMs for\nestimating specific minoritized subgroups' attitudes would be a helpful contribution to\npublic opinion research. While this would be possible with the general-population data\nat hand to a limited extent, benchmarking against special population surveys instead\nwould add value. Moreover, most existing studies have tested whether LLMs are able to\n\u201cpredict the past\u201d, benchmarking against survey data from a time included in LLMs'"}, {"title": "", "content": "training data. Future research should tackle the question whether LLMs can predict\nfuture voting behavior based on past training data, for example by using pre-election\npanel survey data ahead of an upcoming election for the LLM input and comparing the\nLLM output to the post-election survey data after the election took place.\nExtending the scope to other linguistic, socio-structural, and political contexts,\ncomparative studies could employ cross-national individual-level benchmark datasets.\nBeyond further examining the contexts in which LLMs can(not) be used for public\nopinion estimation, such studies could systematically uncover which country-level\nfactors drive this feasibility through multi-level or meta-analyses.\nFinally, researchers could explore designing an LLM that is optimized for the\npurpose of survey research, drawing on comparative evaluations of existing LLMs'\nperformance and the unique requirements of survey research."}, {"title": "6. Conclusion", "content": "We have shown that in its current state, GPT-3.5 is not suitable for estimating public\nopinion across (sub)populations, as it exhibits algorithmic bias on two levels. From a\ncross-sectional perspective, the LLM was unable to pick up on nuances of voter groups,\nthereby being biased against population subgroups not conforming to the mainstream.\nFrom a cross-national perspective, GPT-3.5's performance in estimating voting behavior\nwas not as good for Germany as (some) comparable studies found for the United\nStates. It is likely that predictive performance is even worse for countries, contexts, and\npopulations who are reflected in the LLM training and fine-tuning process even less.\nThe successful application of large language models to public opinion estimation thus is\nlimited to (sub)populations to which their training data is biased \u2013 whether this is due to"}, {"title": "", "content": "contextual complexity or a lack of linguistic or digital representativity of other\npopulations. More research is necessary to understand what exactly this bias in public\nopinion estimation depends on and how its sources interact. In sum, GPT-3.5 is better\nat estimating groups that dominate research and internet data \u2013 groups which\nresearchers already know more about, only making LLM-based synthetic samples\nuseful in very limited settings. Researchers need to be aware of these limitations when\ntrying to apply large language models in their work and take care not to reinforce\nexisting biases. Only if large language models are equitable, just, and reflect the\npopulation's diversity in a fair manner may we be able to leverage them for estimating\npublic opinion."}]}