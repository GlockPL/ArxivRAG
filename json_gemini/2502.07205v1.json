{"title": "VINP: Variational Bayesian Inference with Neural Speech Prior for Joint ASR-Effective Speech Dereverberation and Blind RIR Identification", "authors": ["Pengyu Wang", "Ying Fang", "Xiaofei Li"], "abstract": "Reverberant speech, denoting the speech signal degraded by the process of reverberation, contains crucial knowledge of both anechoic source speech and room impulse response (RIR). This work proposes a variational Bayesian inference (VBI) framework with neural speech prior (VINP) for joint speech dereverberation and blind RIR identification. In VINP, a probabilistic signal model is constructed in the time-frequency (T-F) domain based on convolution transfer function (CTF) approximation. For the first time, we propose using an arbitrary discriminative dereverberation deep neural network (DNN) to predict the prior distribution of anechoic speech within a probabilistic model. By integrating both reverberant speech and the anechoic speech prior, VINP yields the maximum a posteriori (MAP) and maximum likelihood (ML) estimations of the anechoic speech spectrum and CTF filter, respectively. After simple transformations, the waveforms of anechoic speech and RIR are estimated. Moreover, VINP is effective for automatic speech recognition (ASR) systems, which sets it apart from most deep learning (DL)-based single-channel dereverberation approaches. Experiments on single-channel speech dereverberation demonstrate that VINP reaches an advanced level in most metrics related to human perception and displays unquestionable state-of-the-art (SOTA) performance in ASR-related metrics. For blind RIR identification, experiments indicate that VINP attains the SOTA level in blind estimation of reverberation time at 60 dB (RT60) and direct-to-reverberation ratio (DRR). Codes and audio samples are available online\u00b9.", "sections": [{"title": "I. INTRODUCTION", "content": "REVERBERATION, which is formed by the superposition of multiple reflections, scattering, and attenuation of sound waves in a closed space, is one of the main factors degrading speech quality and intelligibility in daily life. The reverberant speech contains the crucial knowledge of both anechoic source speech and room impulse response (RIR). The anechoic speech, often regarded as an oracle, serves as the ultimate target of speech dereverberation. Moreover, RIR characterizes the sound propagation from sound source to microphone constrained in a room environment. Therefore, given a reverberant microphone recording, there is a strong need to estimate the anechoic speech and RIR, leading to two distinct tasks: speech dereverberation and blind RIR identification, respectively.\nA series of classical speech dereverberation approaches build deterministic signal models of anechoic source speech, RIR, and reverberant microphone recording. After that, dereverberation is solved by designing an inverse filter in the time domain [1] or time-frequency (T-F) domain [1], [2], [3], [4], [5]. In contrast, another series of classical methods regard the anechoic source speech signal and the reverberant microphone recording as random variables, and use hierarchical probabilistic graphical models to describe the conditional independent relationship between these random variables. Speech dereverberation is then performed by estimating every unknown latent variable and model parameter, including the anechoic speech spectral coefficient and the reverberation model parameters [6], [7]. Actually, the construction of the probabilistic graphical model is highly flexible. By modeling different factors that affect the observed signal as latent variables and model parameters, similar methods can be applied to a variety of fields, such as speech denoising [6], [8], [9] and direct-of-arrival estimation [10], [11], [12]. Classical methods are model-based and unsupervised, which makes them free from generalization problems. However, due to the insufficient utilization of prior knowledge regarding speech and distortion, the performance of these methods remains unsatisfactory.\nIn recent decades, data-driven deep learning (DL)-based approaches have developed rapidly and surpassed the performance of classical ones. These approaches rely less on the assumptions of signal models, instead directly learn the characteristics of speech signals using deep neural networks (DNNs). Regarding such methods, the focal point of algorithm development is the design of DNN structures, network inputs and outputs, and loss functions. The most straightforward DL-based idea is to construct a discriminative DNN to learn the mapping from degraded speech to target speech. For instance, authors in [13], [14], [15], [16], [17], [18] developed various discriminative DNNs and loss functions to build mappings in various representation domains. Another concept is to consider speech dereverberation as a generative task and utilize generative DNNs, such as variational autoencoder (VAE) [19], generative adversarial network (GAN) [20], and diffusion model (DM) [21], to generate audio samples [22], [23], [24], [25]. Thanks to the powerful non-linear modeling ability of DNNS, DL-based methods are able to make the best of a large amount of data and typically lead to a better performance than classical ones. However, it is still challenging to improve automatic speech recognition (ASR) performance because such a single-channel front-end system without joint training always introduces artifact errors into speech waveforms [26], [27].\nParticularly, some methods combine the VAE and classical methods based on probabilistic graphical models, forming a so-called semi-supervised [28], [29] or unsupervised [30], [31] branch. Unlike the supervised DL-based approaches, the VAE in these methods is trained with only clean speech utterances. At the inference stage, the VAE decoder participates in solving the probabilistic graphical model through the estimation of clean speech prior. Usually, the Markov chain Monte Carlo (MCMC) algorithm is used to sample the latent variables in VAE. Compared with classical methods, the prior generated by VAE has higher quality and can yield better speech enhancement results. Such algorithms were first applied to speech denoising [28], [29], [30] and then expanded to speech dereverberation [31], [32]. For instance, in [31], the authors developed a Monte Carlo expectation-maximization (EM) algorithm based on a convolutional VAE and non-negative matrix factorization (NMF) model. In our previous work RVAE-EM [32], we employed a more powerful recurrent VAE to learn the prior distribution of anechoic speech. Also, we found that when the anechoic speech prior is of sufficient quality, MCMC is unnecessary, thus avoiding the repeated inference by VAE decoders. Moreover, by introducing supervised data into the training of VAE, we also obtained a supervised version of RVAE-EM that performs better than the unsupervised one. This finding inspired our current work. Since there are already many advanced dereverberation DNN architectures, it is possible to directly apply them as supervised anechoic speech prior estimators to the solution of the probabilistic graphical model with some simple modifications.\nWithin the domain of audio signal processing, blind RIR identification from reverberant microphone recording constitutes a crucial and challenging area of research, since RIR characterizes the acoustic characteristics of the sound propagation and environment. Currently, methods for blind RIR identification are rather scarce. In recent years, with the development of DL techniques, some DL-based approaches have been proposed [33], [34], [35]. For instance, the authors in [35] established a parameterized model for the reverberation effect and proposed an unsupervised method for joint speech dereverberation and blind RIR estimation. This approach is similar to our work in terms of tasks.\nIn this paper, we propose a variational Bayesian inference framework with neural speech prior (VINP) for joint speech dereverberatiion and blind RIR identification. Our motivation is as follows: Given that the convolution transfer function (CTF) approximation is a rather accurate model of the reverberation effect, by making reasonable probabilistic assumptions and regarding the output of DNN as the prior distribution of anechoic speech, we are able to analytically estimate the anechoic spectrum and the CTF filter, and further obtain the time-domain waveforms of anechoic speech and RIR. The basis of VINP is a probabilistic signal model based on CTF approximation [36], the same as in our previous work RVAE-EM [32]. The signal model describes the relationship among the source speech, CTF filter, and reverberant microphone recording. Different from RVAE-EM [32], considering the existence of advanced discriminative DNN structures for dereverberation, we proposed to employ such DNNs to learn the prior distribution of anechoic speech by modifying the loss function of network training. Sequentially, we use variational Bayesian inference (VBI) [37], [38] to analytically solve the hidden variables and parameters in our probabilistic model. By doing this, VINP avoids the direct utilization of DNN output but still utilizes the powerful nonlinear modeling capability of the network, thereby benefiting ASR. Moreover, a major drawback of RVAE-EM [32] is that the computational cost grows with the cube of the speech length. Through the variational inference, the computational cost in the proposed method increases linearly with the speech length. Additionally, parallel computation can be used across T-F bins for fast implementation.\nThis paper has the following contributions:\n\u2022 We propose VINP, a novel VBI framework with neural speech prior for joint speech dereverberation and blind RIR identification. For the first time, we propose introducing an arbitrary discriminative dereverberation DNN into VBI to successfully complete the tasks.\n\u2022 VINP avoids the direct utilization of DNN output but still utilizes the powerful nonlinear modeling capability of the network. Thus, it can enhance the ASR performance of the original backbone DNN without the need for any joint training with the ASR system. Experiments demonstrate that VINP attains an advanced level in most metrics related to human perception and displays unquestionable state-of-the-art (SOTA) performance in ASR-related metrics.\n\u2022 VINP can be used for blind RIR identification from reverberant microphone recording. Experiments indicate that VINP attains the SOTA level in the blind estimation of reverberation time at 60dB (RT60) and direct-to-reverberation ratio (DRR).\n\u2022 From the perspective of computational cost, VINP exhibits linear growth with respect to the speech length, which is different from the cubical growth in our previous work RVAE-EM [32]. Moreover, the VBI procedure in VINP can be implemented in parallel across T-F bins, thereby further reducing the processing time.\nThe remainder of this paper is organized as follows: Section II formulates the signal model and the tasks. Section III elaborates on the proposed VINP framework. Experiments and discussions are presented in Section IV. Finally, Section V concludes the entire paper."}, {"title": "II. SIGNAL MODEL AND TASK DESCRIPTION", "content": "In this section, we will introduce the signal model and define two tasks we aim to address: speech dereverberation and blind RIR identification."}, {"title": "A. Signal Model", "content": "Considering the scenario of a single static speaker and stationary noise, the reverberant speech signal (observation) received by a distant microphone can be modeled in the time domain as\n$x(n) = h(n) * s(n) + w(n),$\nwhere * is the convolution operator, n is the index of sampling points, x(n) is the observation signal, s(n) is the anechoic source speech signal, h(n) is the RIR which describes a time-invariant linear filter, and w(n) is the background additive noise. Without loss of generality, we assume that the RIR begins with the impulse response of the direct-path propagation, followed with reflections and reverberation.\nAnalyzing and processing speech signals in the time domain poses significant challenges. After performing short-time Fourier transform (STFT), according to the CTF approximation [36], the observation model in the T-F domain becomes\n$X(f,t) = \\sum_{l=0}^{L-1} H_{l}(f)S(f,t \u2013 l) + W(f,t) = H(f)S(f, t) + W(f,t),$\nwhere f and t are the indices of frequency band and STFT frame, respectively; L is the length of CTF filter; X(f,t), S(f,t), W(f,t), and $H_{l}(f)$ are the complex-valued observation signal, source speech signal, noise signal, and CTF coefficient, respectively; $H(f) = [H_{L-1}(f),\u2026\u2026\u2026, H_{0}(f)] \\in C^{1\u00d7L}$, $S(f,t) = [S(f,t \u2013 L + 1),\u2026\u2026, S(f, t)]^{T} \\in C^{L\u00d71}$.\nFurthermore, the observation X (f,t), anechoic source S(f, t), and noise W(f, t) are modeled as random signals. We have the following assumptions regarding their distributions.\n\u2022 Assumption 1: The anechoic source speech signal S(f,t) follows a time-variant zero-mean complex-valued Gaussian distribution, while the noise signal W(f, t) follows a time-invariant zero-mean complex-valued Gaussian distribution. Therefore, we have their prior distributions as\n$\\begin{cases}S(f,t) \\sim CN (0, a^{-1}(f,t))\\\\W(f,t) \\sim CN (0,\\delta^{-1}(f)),\\end{cases}$\nwhere a(f, t) and 8(f) are the precisions of the Gaussian distributions.\n\u2022 Assumption 2: The anechoic source speech signal S(f,t) and noise signal W(f,t) are respectively independent for all T-F bins. Defining S = $[S(1,1),\u2026\u2026\u2026, S(1,T),\u2026\u2026, S(F,T)]^{T} \\in C^{1\u00d7FT}$ and W = $[W(1,1),\u2026\u2026\u2026, W (1,T),\u2026\u2026,W(F,T)]^{T} \\in C^{1\u00d7FT}$, we have\n$s\\sim \\prod_{f=1}^{F}\\prod_{t=1}^{T}p (S(f,t))$\n$w\\sim \\prod_{f=1}^{F}\\prod_{t=1}^{T}p (W(f, t)) .$\n\u2022 Assumption 3: The anechoic source speech signal S and noise signal W are independent of each other, which means\n$S, W ~ p(S)p(W).$\nWith all the aforementioned assumptions, the conditional probability of the observation signal can be expressed as follows.\n$(X(f,t)|S(f,t) ~ CN (H(f)S(f,t), d^{-1}(f))$\n$x|s ~ \\prod_{f=1}^{F}\\prod_{t=1}^{T}p (X(f,t)|S(f,t)) .$"}, {"title": "B. Task Description", "content": "In this work, we aim to jointly complete speech dereverberation and blind RIR identification by solving all hidden variables and parameters in Fig. 1 through VBI.\nFor speech dereverberation, we care about the anechoic spectrum, which is a hidden variable in our probabilistic graphical model. We aim to get its maximum a posteriori (MAP) estimation given the reverberant microphone recording, written as\n$S = \\arg \\max_{S} p(S|X),$\nwhere the posterior distribution of anechoic source signal can be represented according to the Bayes rule as\n$p(S|X) = \\frac{p(S)p(X|S)}{\\int p(S)p(X|S)ds}$\nFor blind RIR identification, we care about the CTF filter, which is a model parameter in our probabilistic graphical model. Defining the CTF filter for all frequency bands as H = [H(1),\u2026\u2026,H(F)], we aim to get its maximum likelihood (ML) estimation, written as\n$H = \\arg \\max_{H} p(S, X).$\nThe CTF filter is a representation of RIR in the T-F domain. We transform the CTF filter into the RIR waveform through a pseudo measurement process, a process that will be elaborated upon later."}, {"title": "III. PROPOSED METHOD", "content": "In this work, we propose a novel framework named VINP for joint speech dereverberation and blind RIR identification. We propose using an arbitrary discriminative dereverberation DNN to predict the prior distribution of anechoic speech from reverberant microphone recording and then applying VBI to analytically estimate the anechoic spectrum and the CTF filter. After that, we use a pseudo measurement process to transform the CTF filter into the RIR waveform. The overview of VINP is shown in Fig. 2."}, {"title": "A. Prediction of Anechoic Speech Prior", "content": "The direct-path speech signal, which is a scaled and delayed version of the anechoic source speech signal, is free from noise and reverberation as well. To avoid estimating the arbitrary direct-path propagation delay and attenuation, instead of the actual source speech, we setup the direct-path speech as the source speech and our target signal, still denoted as s(n) or S. Correspondingly, the RIR begins with the impulse response of the direct-path propagation.\nIn VINP, we consider the power spectrum of the direct- path speech signal as the variance of the anechoic speech prior p(S). Then, in each T-F bin, the oracle estimation of precision a(f, t) is\n$a(f,t) = 1/|S(f,t)|^{2}.$\nHowever, the oracle estimation is unavailable in practice. We propose integrating an arbitrary discriminative dereverberation DNN as an estimator of anechoic speech prior. Note that, in the proposed framework, we need to redesign the training loss function of the discriminative dereverberation DNNs.\nSpecifically, the discriminative DNN constructs a mapping from reverberant magnitude spectrum X to the anechoic magnitude spectrum |S| as\n$|\\hat{S}| = f_{DNN} (|X|).$\nThen Eq. (10) becomes\n$a(f,t) = 1/|\\hat{S}(f,t)|^{2}.$\nRegarding the loss function, we employ the average Kullback-Leibler (KL) divergence [39] to measure the distance of estimated prior distribution p($\\hat{S}$) and oracle prior distribution p(S) as\n$L = E_{data} \\frac{KL (p(\\hat{S})||p(S)) } = E_{data}  \\frac{ S(f,t)|2}{\\hat{S}(f,t)|2}+ \\frac{ |\\hat{S}(f,t)|2}{S(f,t)|2} -1$.$\nIn practice, we use\n$L = E_{data}  ln \\frac{|S(f,t)|^{2}+\\epsilon}{|\\hat{S}(f,t)|^{2}+\\epsilon} + \\frac{|\\hat{S}(f,t)|^{2}+\\epsilon}{|S(f,t)|^{2}+\\epsilon} -1$ instead to avoid numerical instabilities, where $\\epsilon$ is a small constant. Such a loss function is different from that in regular discriminative dereverberation DNNs which directly predict the anechoic spectrum.\nResearch indicates that the complex nonlinear operations of DNNs often lead to outputs with unpredictable artificial errors. While these errors do not significantly affect speech perceptual quality, their impact on back-end speech recognition applications remains uncertain. As a result, DL-based approaches may not improve ASR performance [27], [40], [26]. However, in our method, by regarding the DNN output as a prior distribution of anechoic speech and utilizing the subsequent CTF-based VBI stage to refine it, this problem can be alleviated, leading to better ASR performance. Experiments in Section IV will provide evidence to support this conclusion."}, {"title": "B. Variational Bayesian Inference", "content": "Given the estimated prior distribution of anechoic speech and the observed recording, the estimation of every hidden variable and parameter is carried out through variational Bayesian inference.\nThe posterior distribution p(S|X) is intractable due to the integral term in Eq. (8). Therefore, we turn to VBI, which is a powerful tool for resolving hierarchical probabilistic models [11], [41]. More specifically, we employ the variational expectation-maximization (VEM) algorithm, which provides a way for approximating the complex posterior distribution p(S|X) with a factored distribution q(S) according to the mean-field theory (MFT) as\n$p(S|X) \u2248 q(S) = \\prod_{f=1}^{F} \\prod_{t=1}^{T}q(S(f,t)).$\nAfter factorization, VEM can estimate the posterior q(S) and model parameters $\\theta$ = {$\\delta(f)_{f=1}^{F}$, $H(f,t)_{t=1}^{T}$} by E-step and M-step respectively and iteratively as\n$ln q (S(f, t)) = \\langle ln p(S, X)\\rangle_{S\\S(f,t)}$\nand\n$\\hat{\\theta}$ = $\\arg \\max_{\\theta} ln p(S, X),$ where $\\backslash$ denotes the set subtraction and $\\langle \u00b7 \\rangle$ denotes expectation. Because the solution of such a probabilistic graphical model is an underdetermined problem, we do not update the precision parameter $\\alpha(f,t)$ during the VEM iterations to prevent a deterioration in prior quality.\nThe specific update formulae are as follows:\n1) E-step: In this step, we update the posterior distribution of the anechoic spectrum given the observation and estimated model parameters. Substitute the probabilistic model into Eq. (16), we have\n$ln q (S(f, t)) = \\langle ln p(S, X)\\rangle_{S\\S(f,t)} = \\langle ln p (S(f,t))\\rangle_{S\\S(f,t)}  + \\langle ln p (X(f,t + l)|S(f,t + l)) \\rangle_{S\\S(f,t)} + c,$ where c is a constant term that is independent of S(f,t), and\n$\\langle ln p (S(f, t))\\rangle = ln \\alpha(f,t) \u2013 \\alpha(f,t) |S(f,t)|^{2} + c$\n$\\langle ln p (X(f, t+l)|S(f,t +l))\\rangle = ln \\delta(f) \u2013 \\delta(f) |X(f,t + l) \u2013 H(f)S(f,t +l)|^{2} + c.$\nAccording to the property of Gaussian distribution, the posterior distribution is also Gaussian, written as\nq (S(f,t)) = CN ($\\mu$(f,t), $\\gamma^{-1}$(f,t)),\nwhose precision and mean have close-form solutions as\n$\\gamma(f,t) = \\alpha(f,t) + \\delta(f)||H(f)||^{2}$\n$\\mu(f,t) = \\gamma^{-1}(f, t)\\delta(f)  H_{l}(f) [X(f,t + l) - H(f)\\langle (f,t+l)\\rangle]$,\nwhere $H_{l}(f)$ is same as H(f) except that $H_{l}(f)$ is set to 0, and $\\langle(f,t + l)\\rangle$ = [$\\hat{\\mu}(f,t + l \u2013 L + 1),\u2026\u2026, \\hat{\\mu}(f,t +l)]^{T}$ contains the estimates of means from the previous VEM iteration.\nIn order to make VEM converge more smoothly, we further apply an exponential moving average (EMA) to the estimates in Eq. (21) as\n[$\\gamma^{-1}(f,t) = \\lambda\\gamma_{pre}^{-1}(f,t) + (1 - \\lambda)\\gamma^{-1}(f,t)$\\\\((ft) = A\u00fbpre(f,t) + (1 \u2212 1)\u03bc(f, t),$\nwhere $\\lambda$ is a smoothing factor, $\\hat{\\mu}_{pre}(f,t)$ and $\\hat{\\gamma}_{pre}(f,t)$ are the estimates from the previous VEM iteration.\nThe prior of anechoic speech narrows down the infinite number of possible solutions when decoupling the anechoic speech and the RIR, which is the key to achieving dereverberation [42]. The mean of the posterior distribution is the MAP estimate of anechoic spectrum $S$. It is also worth noticing that the E-step can be implemented in parallel across all T-F bins.\n2) M-step: In this step, VEM updates the noise precision and CTF filter by maximizing the logarithmic joint probability of the anechoic speech and observation, which is\nln p (S, X) = ln p (X|S) + c  = T lnd(f) \u2013 $\\delta$(f)  ||X(f,t) \u2013 H(f)S(f,t)||^{2} + c,\nwhere c is a constant term that is independent of $\\delta$(f) and H(f). Setting the first derivative with respect to the parameters to zero, the noise precision is updated as\n$\\hat{\\delta}(f) = T/ [|X(f,t)|^{2} - 2Re {X^{*} (f, t)H(f) (S(f,t))} + H(f) (S(f,t)S^{H} (f, t)) H^{H} (f)],$\nand the CTF filter is updated as\n$\\hat{H}(f)= [X(f,t) (SH (f,t))$  H(f)]^{-1}$,\nwhere\n$\\langle (f,t)\\rangle = \\mu(f,t) = [\\mu(f, t \u2013 L + 1),\u2026\u2026, \\mu(f,t)],$\nand\n$(SH (f,t))= \\mu(f,t)\\mu^{H} (f,t) + diag([a^{-1}(f, t \u2013 L + 1),\u2026\u2026, a^{-1}(f, t)]),$\ndiag(\u00b7) denotes the operation of constructing a diagonal matrix. Just like the E-step, the M-step can also be implemented in parallel across all T-F bins.\n3) Initialization of VEM Parameters: The initialization of parameters plays a crucial role in the convergence of VEM. Before iteration, the mean and variance of the anechoic speech posterior p(S) are set to zero and the power spectrum of reverberant recording, respectively. This means we have\n$\\begin{cases}\n   \\langle S\\mu(f,t) = 0\\\\\\alpha(f,t) = 1/|X(f,t)|^{2} .\n\\end{cases}$\nThe CTF coefficients in each frequency band are set to 0 except that the first coefficient is set to 1, which means\n$\\begin{cases}\n   H_{0}(f) = 1\\\\H_{i\u22600}(f) = 0.\n\\end{cases}$\nBecause even during speech activity, the short-term power spectral density of observation often decays to values that are representative of the noise power level [43], the initial variance of the additional noise is set to the minimum power in each frequency band, which means\n$\\delta^{-1}(f) = [min (|X(f,t)|^{2})]^{-1}.$"}, {"title": "C. Transformation to Waveforms", "content": "After the VBI procedure, both the anechoic spectrum and the CTF filter are estimated. We need to further transform these T-F representations into waveforms. By applying inverse STFT to the anechoic spectrum estimate, we can easily get the anechoic speech waveform. For the estimation of RIR, we design a pseudo intrusive measurement process as follows.\nA common method for intrusive measuring the RIR of an acoustical system is to apply a known excitation signal and measure the microphone recording [44]. Playing an excitation signal e(n) with a loudspeaker, the noiseless microphone recording y(n) can be written as\ny(n) = h(n) * e(n),\nwhere h(n) has the same meaning as in Eq. (1). A commonly used logarithmic sine sweep excitation signal can be expressed as [44], [45]\ne(n) = sin 1 ln (\u03c92/\u03c91) (eln(\u03c91t)/N-1)] ;\nwhere \u03c91 is the initial radian frequency and \u03c92 is the final radian frequency of the sweep with duration N. Through an ideal inverse filter v(n), the excitation signal can be transformed into a Dirac's delta function \u03b4(n), as\ne(n) * \u03c5(\u03b7) = \u03b4(\u03b7).\nFor the logarithmic sine sweep excitation, the inverse filter v(n) is an amplitude-modulated and time-reversed version of itself [44], [45]. The RIR can be estimated by convolving the measurement y(n) with the inverse filter v(n) as\nh(n) = y(n) * \u03c5(\u03b7).\nIn our approach, the excitation signal is convoluted by the CTF estimates (along the time dimension) to get a pseudo measurement \u1ef8(f, t) in the T-F domain as\n\u1ef8(f,t) = \u0124(f)E(f, t),\nwhere E(f,t) = [E(f,t \u2212 L + 1),\u2026\u2026,E(f,t)]T \u2208 CL\u00d71, \u0176(f,t) and E(f,t) are the STFT coefficient of \u1ef9(n) and e(n), respectively. Finally, after applying inverse STFT to Y(f, t), we use the pseudo measurement \u1ef9(n) to estimate the RIR waveform by\n\u0125(n) = \u1ef9(n) * v(n).\nThe transformation from the CTF filter to RIR waveform is summarized in Algorithm 2."}, {"title": "IV. EXPERIMENTS", "content": "VINP is designed for both speech dereverberation and blind RIR identification. These two tasks share the same DNN for speech prior distribution prediction. Therefore, we use a single training set and two different test sets.\n1) Training Set: The training set is composed of 200 hours of high-quality English speech utterances from the corpora of EARS [46], DNS Challenge [47], and VCTK [48]. All speech utterances are downsampled to 16 kHz if necessary. We simulate 100,000 pairs of reverberant and direct-path RIRS using the gpuRIR toolbox [49]. The simulated speaker and microphone are randomly placed in rooms with dimensions randomly selected within a range of 3 m to 15 m (for length and width) and 2.5 m to 6 m (for height). The minimum distance between the speaker/microphone and the wall is 1 m. Reverberant RIRs have RT60 values uniformly selected within the range of 0.2 s to 1.5 s. Direct-path RIRs are generated using the same geometric parameters as the reverberant ones but with an absorption coefficient of 0.99. Noise recordings from the Noise92 corpus and the training set from the REVERB Challenge [50] are employed. The signal-to-noise ratio (SNR) is randomly selected within the range of 5 dB to 20 dB.\n2) Test Set for Speech Dereverberation: For speech dereverberation, we utilize the official single-channel test set from the REVERB Challenge [50], which includes both simulated data (marked as 'SimData') and real recordings (marked as 'RealData').\nIn SimData, there exist six distinct reverberation conditions: three room volumes (small, medium, and large), and two distances between the speaker and the microphone (50 cm and 200 cm). The RT60 values are approximately 0.25 s, 0.5 s, and 0.7 s. The noise is stationary background noise, mainly generated by air conditioning systems. SimData has a SNR of 20 dB.\nRealData consists of utterances spoken by human speakers in a noisy and reverberant meeting room. It includes two reverberation conditions: one room and two distances between the speaker and the microphone array (approximately 100 cm and 250 cm). The RT60 is about 0.7 s.\n3) Test Set for Blind RIR Identification: A test set named 'SimACE' is constructed to evaluate blind RIR estimation. In SimACE, microphone signals are simulated by convolving the clean speech from the 'si_et_05' subset in WSJO corpus [51] with the downsampled recorded RIRs from the 'Single' subset in ACE Challenge [52], and adding noise from the test set in REVERB Challenge [50]. The minimum and maximum RT60s are 0.332 s and 1.22 s, respectively. More details about the RIRs can be found in [52]. We create the SimACE test set because the RT60 labels of the RIRs in SimACE are more accurate than those in REVERB Challenge. SimACE has a SNR of 20 dB."}, {"title": "B. Implementation of VINP", "content": "1) Data Representation: Before feeding the speech into VINP", "Architecture": "VINP is able to employ any discriminative dereverberation DNNs to predict the prior distribution of anechoic speech. In this paper", "15": "and a modified network of the Mamba version of oSpatialNet [53", "VINP-TCN+SA+S": "nd 'VINP-oSpatialNet'"}, {"15": "except that there is no activation function after the output layer and we do not use dropout.\nThe DNN architecture in VINP-oSpatialNet is derived from oSpatialNet [53", "Configuration": "For DNN training", "54": "with an initial learning rate of 0.001 is employed. The learning rate exponentially decays with lr \u2190 0.001 \u00d7 0.97epoch and lr \u2190 0.001\u00d70.9epoch in VINP-TCN+SA+S and VINP-oSpatialNet", "Settings": "The length of the CTF filter is set to L = 30. The fixed smoothing factor A in Eq. (22) is set to 0.7 to obtain a"}]}