{"title": "Object-centric Binding in Contrastive Language-Image Pretraining", "authors": ["Rim Assouel", "Florian Bordes", "Pietro Astolfi", "Michal Drozdzal", "Adriana Romero-Soriano"], "abstract": "Recent advances in vision language models (VLM) have been driven by contrastive models such as CLIP, which learn to associate visual information with their corresponding text descriptions. However, these models have limitations in understanding complex compositional scenes involving multiple objects and their spatial relationships. To address these challenges, we propose a novel approach that diverges from commonly used strategies, which rely on the design of hard-negative augmentations. Instead, our work focuses on integrating inductive biases into pre-trained CLIP-like models to improve their compositional understanding without using any additional hard-negatives. To that end, we introduce a binding module that connects a scene graph, derived from a text description, with a slot-structured image representation, facilitating a structured similarity assessment between the two modalities. We also leverage relationships as text-conditioned visual constraints, thereby capturing the intricate interactions between objects and their contextual relationships more effectively. Our resulting model not only enhances the performance of CLIP-based models in multi-object compositional understanding but also paves the way towards more accurate and sample-efficient image-text matching of complex scenes.", "sections": [{"title": "Introduction", "content": "Recent advancements in multi-modal representation learning have primarily been enabled by the introduction of CLIP (Radford et al., 2021). CLIP learns aligned image-text representations from Internet-scale data. Despite its success, CLIP exhibits limitations in understanding complex scenes composed of multiple objects (Kamath et al., 2023; Yuksekgonul et al., 2023a; Doveh et al., 2023b; Paiss et al., 2023). For instance, while capable of recognizing individual objects, CLIP struggles with interpreting spatial relationships among objects in the scene(e.g., \"the cat is to the left of the mat\" vs. \"the cat is to the right of the mat\") and adequately associating objects with their corresponding attributes (e.g., \u201ca red square and a blue circle\" v\u03c2. \"a blue square and a red circle\"). The process of acquiring this compositional understanding of the world is known as the binding problem in the literature, and may be decomposed into segregation, representation, and composition problems (Greff et al., 2020b).\nEfforts to improve the compositional understanding of CLIP-like models have largely relied on leveraging hard negative examples\u00b9, either in the text space (Kalantidis et al., 2020; Yuksekgonul et al., 2023b; Zhang et al., 2024b; Doveh et al., 2023b; Paiss et al., 2023) - to improve sensitivity to the order of words and subtle textual differences or the image space (Awal et al., 2024; Le et al., 2023; Zhang et al., 2024a) \u2013 to improve sensitivity to subtle visual differences. Although these methods have somewhat improved CLIP-like models' performance on scene compositionality benchmarks (Parcalabescu et al., 2022; Zhao et al., 2022; Yuksekgonul et al., 2023b; Hsieh et al., 2023b), they do not explicitly address the binding problem as they focus mainly on enhancing the model's representation capabilities with additional data, hindering their generalization to unseen scene compositions.\nYet, the literature on object-centric representation learning (Eslami et al., 2016; Greff et al., 2020a; Locatello et al., 2020; Wu et al., 2023; Seitzer et al., 2023) has long focused on devising methods to address the segregation"}, {"title": "Related Work", "content": "Contrastive Pretraining of VLMs. Vision-language models (VLMs) have made substantial strides in both the vision and multi-modal domains (Bordes et al., 2024). Modern VLMs are pretrained on vast, diverse and oftentimes noisy multi-modal datasets (Changpinyo et al., 2021; Schuhmann et al., 2022; Ilharco et al., 2021; Zeng et al., 2022), and have shown substantial improvements when applied to various zero-shot tasks. CLIP (Radford et al., 2021) presented a contrastive learning approach used for pretraining, which involves training the model to differentiate between similar and dissimilar image-text pairs. This approach encourages the model to learn a shared representation space for images and text, where semantically similar pairs are close together and dissimilar pairs are far apart. Following CLIP's lead, image-text contrastive learning has become a prevalent strategy for VLM pretraining (Liu et al., 2023; Cai et al., 2024; Liu et al., 2024a; Dai et al., 2023; Zhai et al., 2022b; Chen et al., 2022; Beyer et al., 2024; Fini et al., 2023). Contrastive vision-language pretraining spans numerous downstream applications, including zero-shot image classification (Zhai et al., 2022a; Radford et al., 2021; Metzen et al., 2024; Gao et al., 2021), text-to-image generation (Podell et al., 2023; Abdal et al., 2021; Ramesh et al., 2022; Saharia et al., 2022), as well as assessing text-image alignment (Moens et al., 2021; Cho et al., 2023). In this work, we are particularly interested in the ability of CLIP-based models to evaluate compositional text-image alignment.\nCompositional Understanding Benchmarks. Several benchmarks have been developed to assess the compositional understanding of VLMs. In this work, we focus on benchmarks structured as cross-modal retrieval tasks where the model needs to distinguish between correct and incorrect text descriptions given an image, and evaluations are based on accuracy metrics. The majority of these benchmarks (Zhao et al., 2022; Yuksekgonul"}, {"title": "Method", "content": "Our goal is to enhance CLIP-based architectures with object-centric binding and composition capabilities. Our method starts by extracting representations of distinct open-ended objects and relationships in a textual description, as well as representations of patches in an image. Next, a binding module matches the text representation of objects to the relevant image patches, producing a slot-centric representation of the image. Finally, a structured similarity score compares the slot-centric representation with the textual representations of different objects, and leverages the extracted relationships as constraints applied to the visual slots. Our key contributions lie in the design of the binding module and the proposal of the structured similarity score, which we detail in sections 3.1 and 3.2, respectively. Our approach relies on a scene-graph representation of the text modality. We assume the parser is given and orthogonal to our approach and discuss the choice of the parsing method in Appendix A.3.\nNotation. We denote as \\(x\\) an image of shape \\(R^{h\\times w\\times 3}\\) and as \\(x = [x_1,...,x_N] = E_{\\theta}(x) \\in R^{N\\times d}\\) its patch-level encoding, where \\(E_{\\theta}\\) is an image encoder - typically a pre-trained ViT (Dosovitskiy et al., 2020) - \\(N\\) is the number of patches and \\(d\\) the dimensionality of the patch embeddings. We denote as \\(t\\) the text description, or caption, associated with \\(x\\). We extract a scene graph. For example, the scene graph of \"A red apple to the left of a blue car\" will be represented with the set of nodes {\"red apple\", \"blue car\"} and the set of edges {(\"to the left of\", \"red apple\", \"blue car\")}. In practice, we represent \\(N\\) as a matrix of node features \\(N\\), where each row contains the embedding of a node in the graph. Moreover, we represent each \\(s^{i}\\) and \\(o^{i}\\) in the relationship tuples as indices referencing the nodes (rows) in \\(N\\)."}, {"title": "Binding Module", "content": "Our first contribution resides in the binding module. The idea is that when comparing the content of a caption and an image we do not want the features of different objects to interfere with each other but rather keep them separate at a representational level. The role of the binding module is thus to extract a slot-centric representation of an image where the content of the slots are pushed to represent the nodes of the associated scene graph.\nTo do so, we implement the binding module using a inverted cross-attention layer (Wu et al.). We normalize the attention coefficients over the queries' dimension in order to introduce a competition between queries to explain different parts of the visual input. We follow common practice and set the attention's softmax temperature to \\(\\sqrt{D}\\), with \\(D\\) being the dimensionality of the dot-product operation. Applying the softmax along the queries' dimension pushes all the candidate keys to be softly matched to at least one query. However, captions mostly describe specific parts of the image, and rarely capture all the visual information. Since we want only the relevant visual information to be captured by the queries, we add a set of default query tokens, stored in a matrix \\(Q_{default}\\), which participate in the competitive attention mechanism - with the goal of absorbing the visual information not captured in the caption. These default query tokens are dropped in the subsequent computation steps of our model (akin to registers in ViT backbones (Darcet et al., 2024)). We find the default query tokens crucial to stabilize the training our model.\nThe binding module computations are formalized as follows:\n\\[\\begin{aligned}\n&Q = W_qN,\\\\\n&K, V = W_kX, W_vX,\\\\\n&Q' = [Q; Q_{\\text{default}}],\\\\\n&\\text{Attn}(Q', K, V) = \\text{softmax}\\left(\\frac{Q' K^T}{\\sqrt{D}},\\text{dim}='Q'\\right) V,\\\\\n&S, S_{\\text{default}} = \\text{Attn}(Q', K, V).\n\\end{aligned}\\]\nHere, \\(W_q\\), \\(W_k\\), and \\(W_v\\) are the linear projection weight matrices for the queries, keys, and values, respectively, \\(S\\) are the visual slots, \\(S_{\\text{default}}\\) are the visual slots from default query tokens, which are discarded for subsequent steps, and \\([.]\\) denotes the concatenation operation.\nThus, the output of this binding module are the visual slots \\(S\\). Intuitively, these slots are pushed to represent the visual objects, or entities, that correspond to the nodes of the scene graph. Their object-centric learning is driven by the structured similarity that we detail in the next section."}, {"title": "Structured similarity score", "content": "Our second contribution resides in the introduction of a structured similarity score, whose goal is to promote the constraints imposed by the scene graph on the learnable visual slots. Our proposed structured similarity score is composed of an object scoring function and a relationship scoring function. The object scoring function assesses the presence of each node in the scene graph (objects present in the caption). We model this function as the sum of the cosine similarity between each textual node representation \\(N^{i}\\) and its assigned visual slot \\(S^{i}\\). The relationship scoring function encourages the relational constraints imposed by each edge in the scene graph and is defined as a learnable function \\(f_{\\theta}\\) of the relationship embedding \\(r^{i}\\), and the visual slot representations \\(s^{i}\\) and \\(s^{o}\\) corresponding to the subject and object of the relationship, respectively. We derive the overall structured similarity score over the visual slots \\(S\\) from an image \\(x\\) and a graph \\(G = (\\{N^{i}\\}_{i=1..M}, \\{(r^{i}, s^{i}, o^{i})\\}_{i=1..P})\\) such that: \\(S(x,G) = \\alpha \\sum_{i=1..M} \\text{cosine}(N^{i},S^{i}) + \\beta \\sum_{i=1..P} f_{\\theta}(r^{i},S^{s^{i}},S^{o^{i}})\\), where \\(\\alpha\\) and \\(\\beta\\) are learned parameters controlling the strength of each score. \\(M\\) and \\(P\\) are the number of nodes and relationships in the scene graph \\(G\\), respectively.\nWe define \\(f_{\\theta}\\) as follows:\n\\[f_{\\theta}(r, S^s, S^o) = \\text{cosine} \\left(r, f_s([r, S^s]) + f_o([r, S^o])\\right),\\]\nwhere \\([.]\\) denotes the concatenation of two vectors and \\(f_s\\) and \\(f_o\\) are MLPs that reduce the dimensionality of their inputs. Note that we model the relationship scoring function so that it keeps the same scale as the object scoring function and can take the order of the relationship into account."}, {"title": "Training", "content": "The model is trained using the following loss:\n\\[\\mathcal{L} = \\mathcal{L}_{itc} + \\mathcal{L}_{rel}.\\]\n\\(\\mathcal{L}_{itc}\\) is the image-text contrastive loss defined to minimize the distance between image and scene graph representations from paired text-image data while maximizing the distance between image and scene graph representations from unpaired text-image data as:\n\\[\\mathcal{L}_{itc} = - \\frac{1}{B} \\sum_{i=1}^B \\left( \\log \\frac{\\exp S(x_i, G_i)}{\\sum_{j=1}^B \\exp S(x_j, G_i)} + \\log \\frac{\\exp S(x_i, G_i)}{\\sum_{j=1}^B \\exp S(x_i, G_j)} \\right),\\]\nwhere \\(B\\) is the number of elements in the batch. Note that the \\(S\\) is the structured similarity score defined in Eq. 3.2. \\(\\mathcal{L}_{rel}\\) is the loss that pushes the model to learn a non-symmetric relationship scores:\n\\[\\mathcal{L}_{rel} = - \\frac{1}{B} \\sum_{i=1}^B \\log \\frac{\\exp S(x_i, G_i)}{\\exp S(x_i, G_i) + \\exp S(x_i, \\overline{G}_i) + \\exp S(x_i, \\tilde{G}_i)},\\]\nwhere \\(\\overline{G}\\) and \\(\\tilde{G}\\) are altered scene graphs. In \\(\\overline{G}\\), we swap the order of the subject and the object of a relationship, whereas in \\(\\tilde{G}\\), we randomly chose the relationship's subject and object from the nodes in the scene graph. We ablate the main components of OC-CLIP in Table 3 and give a more extensive ablation analysis in Appendix A.1"}, {"title": "Results", "content": "We evaluate OC-CLIP's inductive biases in 3 different settings:\n\u2022 Addressing CLIP's binding problem. We show the efficiency of OC-CLIP in addressing the binding problem compared to hard-negative based augmentation on a synthetic dataset. (Section 4.1).\n\u2022 Compositional understanding. We showcase OC-CLIP's compositional understanding on real-world object-centric attribute binding and spatial relationship understanding benchmarks (Section 4.2).\n\u2022 Scaling on noisy data. We show that OC-CLIP consistently outperforms a CLIP-based model in both zero-shot single object classification and zero-shot compositional understanding multi-object text retrieval, when training both models fully from scratch on larger-scale and noisy dataset (Section 4.3)."}, {"title": "Addressing CLIP's bag-of-words behavior", "content": "In this section, we aim to assess the efficiency and effectiveness of leveraging hard-negatives in OC-CLIP and CLIP-like models in addressing the binding problem. To do so, we use a synthetic dataset with a closed-set vocabulary, from which we can enumerate all possible object-attribute conjunctions and systematically evaluate the potential of CLIP-like models and OC-CLIP in addressing simple swap-attribute retrieval tasks under varying hard-negative sample sizes.\nDataset. We consider a controlled 3D environment based on PUG (Bordes et al., 2023) and build a dataset composed of a single textured animal, or pairs of animals, in different backgrounds. We use a combination of 4 textures, 20 animal classes, and 5 different backgrounds e.g., see example in Figure 2a. We follow prior benchmarks (Hsieh et al., 2023a) and perform a text-retrieval task between the correct caption and the associated negative caption. We give additional details about the subsets compositions in Appendix A.5.\nBaseline and OC-CLIP training. We finetune models on data splits from our synthetic data, while considering an increasing proportion of hard-negative samples. We consider a CLIP model initialized with OpenCLIP weights Ilharco et al. (2021). We also initialize OC-CLIP's text and vision backbones with OpenCLIP weights, but train OC-CLIP's binding module from scratch.\nResults. Our results, presented in Figures 2b and 2c, show that simply adding more hard-negatives to OpenCLIP's training plateaus and is not sample-efficient, as the swap-attribute binding performance always underperforms OC-CLIP trained on less data without any hard-negatives in a simple object-attribute binding task. On seen object pairs, with 70% of the possible pairs and 70% of their corresponding swap-attribute hard-negatives CLIP plateaus at 81% compared to OC-CLIP which solves the task at 97% on the same training data size and no hard-negatives. We hypothesize that the root cause of this issue lies in the representation format used in CLIP's original formulation, which relies on a single vector to capture complex semantic relationships. Our proposed method introduces inductive biases that allow the model to learn more structured representations, avoiding superposition of features (Greff et al., 2020b) and effectively mitigating the bag-of-words behavior."}, {"title": "Compositional Understanding", "content": "In this section, we verify that the observations made in the controlled environment presented in Section 4.1 also transfer to real-word datasets, thereby assessing the real-world compositional understanding of OC-CLIP.\nDatasets. We train OC-CLIP and finetune OpenCLIP in-domain on a set of datasets relevant for real-world compositional understanding. The training text descriptions representing positive samples are taken from COCO (Lin et al., 2014), Visual-Genome (VG) (Krishna et al., 2017) and GQA (Hudson and Manning, 2019). The latter annotates images coming from Visual Genome (Krishna et al., 2017) with objects and both spatial and non-spatial relationships, and thus contains a high representation of spatial prepositions. We evaluate the different models on the most challenging benchmarks representative of compositional understanding, ensuring that we validate both their attribute binding and spatial relationship understanding capabilities. In particular,"}, {"title": "Training OC-CLIP from scratch", "content": "In this section, we aim to assess the potential of OC-CLIP when trained fully from scratch from scene-graphs obtained from large scale non-human-curated captioning dataset.\nDatasets. We train both ViT-B-16 OpenCLIP model and OC-CLIP fully from scratch on increasingly large dataset sizes using CC3M (Sharma et al., 2018), CC12M (Changpinyo et al., 2021) and the combination of both datasets. We evaluate all models on ImageNet (Deng et al., 2009) zero-shot classification in this section, and report results on the ELEVATER suite (Li et al., 2022) in Appendix. We also evaluate zero-shot compositional understanding of the models on the challenging swap-object and swap-attribute splits of SugarCrepe, and on Winoground (Thrush et al., 2022).\nBaseline and OC-CLIP training. Both CLIP and OC-CLIP architectures are trained fully from scratch for 5, 15, or 25 epochs, using a batch size of 4096, a learning rate of 1.10-3, 2k steps of learning rate warm-up, and a cosine decay. As recommended by Mu et al. (2021), we use AdamW optimizer with 0.5 of weight decay and"}, {"title": "Ablations", "content": "In Table 3 we ablate the key design choices of our model. Specifically, we investigate two key components of the model: the use of competitive (inverted) cross-attention and the local graph contrastive loss. On the one hand, results show that removing the competitive cross-attention mechanism greatly affects fine-grained attribute binding (decreasing from 89.0 to 85.9). On the other hand, removing the local graph contrastive loss significantly impacts downstream relational understanding, with accuracy decreasing from 80.5 to 72.8. Adding attention layers helps relational understanding (boosting performance from 77.6 to 80.5), while adding more default tokens does not necessarily help with attribute binding. These findings highlight the importance of the main design choices behind OC-CLIP. More extensive ablations are presented in Appendix A.1."}, {"title": "Conclusion and limitations", "content": "Conclusion. In this paper, we proposed Object-Centric CLIP (OC-CLIP), a method to enhance the compositional scene understanding of CLIP-like models by leveraging advances from object-centric representation learning. Our approach adapts the slot-centric representation paradigm to CLIP and dynamically aligns each representational slot with the objects mentioned in the text description. This is achieved by the introduction of a binding module and a structured similarity score that allows to train OC-CLIP in a contrastive way. We evaluated our approach against common hard-negative augmentation strategies and demonstrated that OC-CLIP significantly enhances the binding of object-centric attributes and spatial relationships across a representative set of challenging real-world compositional image-text matching benchmarks. Notably, we reported an increase of +16.5% accuracy in the challenging swap-attribute split of SugarCrepe compared to OpenCLIP finetuned with in-domain data and drastically improved performance on COCO-spatial and GQA-spatial from the Whatsup benchmark, moving from random chance to more than 89%. Finally we show the scaling potential of OC-CLIP to be trained from scratch on a noisy dataset notably we report performance gain in zero-shot classfication (+12.8% in ImageNet 4) while maintaining a significant gap in zero-shot SugarCrepe swap attribute (+12.7%) and swap obj (+6.6%) splits.\nLimitations and Future Work. Our proposed Object-Centric CLIP (OC-CLIP) model has several limitations and avenues for future work. Notably, our approach relies on a parser to extract object-centric attributes and spatial relationships from text descriptions. While we have chosen an LLM-based parser, which is discussed in Appendix A.3, studying the different biases of LLM-based parser families could be interesting. A related promising direction for future research is also to explore the possibility of parsing scene graphs directly from Visual Language Models (VLMs), using both visual and textual inputs. Additionally, we plan to investigate the synergy between long-captioning and our scene graph-based training approach, aiming to study the complementary strengths of these two data-centric and model-centric paradigms."}, {"title": "Appendix", "content": "In this section we ablate and discuss some important design choice of OC-CLIP. We separately ablate and discuss :\n\u2022 The similarity score coefficients a and \u03b2 that control the weight of the objects and relations in the global graph-image similarity score.\n\u2022 Binding module inductive biases and their impact on compositional understanding performance.\n\u2022 Local Loss impact on downstream compositional understanding of relationships.\n\u2022 Layer selection with OpenCLIP backbone.\nSimilarity Score OC-CLIP's structured global similarity score is a combination of the object and relationship components respectively weighted by two learnt parameters a and \u03b2 balancing the different contributions. We let the model learn those parameters throughout the training. However, during preliminary experiments we tested a different combinations of initial coefficient within the [1.5, 1, 0.5, 0.1] grid and noticed that the model was always converging to a ~ 3 without any difference in the downstream compositional performance. We thus fix the initial coefficients to a = 1.5 and \u03b2 = 0.5 and treat them as parameters.\nDefault Token and Competitive Cross Attention In the binding module we propose to use an inductive biases to encourage the query tokens to attend to different groups of patches. In order to do so we use a competitive attention mechanism, the so called inverted cross attention common to many object-centric image encoder architecture (Locatello et al., 2020; Wu et al.). We found that the use of inverted cross attention impacts slightly the fine-grained attribute binning performance (see ATT performance in Table 3), -Comp Att model does not use any inverted cross attention and is rather implemented with a regular cross attention mechanism, the softmax being done along the keys dimensions.). The finegrained attribute understanding (ATT) is affected by the absence of competitive attention between query slots going from 89.0% to 85.9% accuracy.\nLocal Graph Contrastive Loss In designing the structured similarity score of OC-CLIP the relational component is formulated as the following cosine similarity f(r, S\u00b3, S\u00b0) = cosine(r, fs([r, S\u00b3]) + f\uff61([r, S\u00b0]). In theory both fs([r, S\u00b3]) and fo([r, S\u00b0]) can collapse to ignore the subject object visual representation. In order to prevent such collapse we propose to add a local graph contrastive loss that shares similarity with hard-negative based learning. We enforce the model to model with a higher similarity the graph composed of the same nodes but with either swapped object and subject indices or shuffle objects and subjects indices within the local graph. In both of those cases the relation component of the structured similarity score becomes (for a single relation graph) :\nswap G; cosine(r, fs([r, S\u00b3]) + f\uff61([r, S\u00b0])\nswap G; cosine(r, fs([r, S\u00b0]) + f\uff61([r, S*])\nshuffle G; cosine(r, fs([r, Si!=s]) + fo([r, Si!=0])\nThis prevents the model from collapsing because ground-truth G is distinguishable from G and G only if the visual representations are not ignored in the relationships components. As shown in Table 3, removing the local loss effectively impacts downstream relational understanding on SugarCrepe with a REL accuracy decreasing from 80.5 to 72.8 hence showing the effectiveness of the local graph contrastive loss.\nScoring dimensionality Our structured similarity score allows the text encoder to focus on encoding in-formation about individual objects and their relationships, rather than the entire scene configuration. To achieve this, we experimented with different dimensionality for both the object scoring bottleneck and the relationship scoring bottleneck. Specifically, each of these scores is designed as a cosine distance between a"}, {"title": "Experiments on CC3M/12M.", "content": "In the compositional understanding experiments we compare our approach with data-centric finetuning methods that do not add any additional parameters. These methods are expected to retain some of the general capabilities of the initial backbone. In contrast, our binding and relationship modules is trained from scratch, which means it may not generalize as well to unseen data and can only be expected to work well within the vocabulary domain it has been exposed to (eg. COCO/VG/GQA in our experiments setting). However an interesting question would be to asses whether such inductive biases and structured similarity object might have some sclaing potential on noisy and non human curated datasets such as CC12M (Changpinyo et al., 2021). To answer that question we propose to train both CLIP and OC-CLIP architectures from scratch on combinations of CC3M, CC12M and CC3M+12M and compare both of their general understanding and compositional downstream performance. In addition to the zero-shot evaluation, we also provide a computational analysis of the binding module to gain insights into its behavior and limitations."}, {"title": "Scene Graph Parsing Discussion", "content": "Comparison of different parsing methods Although the parsing method is not the core of our contribution we provide here a couple of qualitative and quantitative comparisons to motivate the choice of using an LLM to perform the parsing of the captions despite the pre-processing computational overhead it entails. We identify 3 families of parsing method that operate on text-only input and provide insights on their respective :\n\u2022 Automatic parsing methods: method based on hand-crafted rules about the semantics in order to extract tags and more complex dependency graphs. TagAlign also compares to nltk and justifies the choice of going to an llm-based method. We consider a representative of those automatic parsing methods based on spacy.\n\u2022 Finetuned factual scene graph parser trained in a supervised way to extract scene graph. We consider a representative of them, a state-of-the-art factual scene graph parser based on T5 model (Li et al., 2023b) trained to extract fine-grained scene graph information about the objects and relations in an input caption.\n\u2022 LLM-based, here we choose llama3-8b as a representative and leave the extensive analysisof the bias/cues of different llm families of model for future work.\nWe identified failures modes of automatic parsing and finetuned that are relevant to compositional under-standing of clip-like models and justify the use of an llm-based parsing method"}, {"title": "Additional Compositional Understanding results", "content": "Our main goal is to evaluate CLIP-like models compositional understanding in plausible and grammatically correct cases.  have identified exploitable textual biases in previous mainstream procedurally-generated hard negatives benchmarks like the COCO and Flickr set of ARO and VL-checklist. Specifically they show that procedurally generated hard negatives are either highly grammatically incorrect and can be identified by a blind model or by a good language model that can measure the plausibility of the caption. The SugarCrepe is thus designed to pfollow the same fine-grained taxonomy on attributes, objects, relationships as VL-checklist but ensures that the hard-negative are not distinguishable by a blind model. The main results of our paper thus focus on this benchmark. We however also give the performance of our model on the full ARO suite and VL-Checklist in"}, {"title": "PUG Dataset", "content": "In this section we describe in more details the content of the synthetic experiments, give more context on the motivation along with additional results.\nMotivation The rise of data-centric hard negative methods were motivated by the bag-of-words behaviour of CLIP noticed in \"simple swap-attribute\" retrieval tasks. Hard-negative methods propose to mitigate this behaviour by finetuning CLIP-like models on data points with minimal changes but"}, {"title": "Spatial Relation Understanding", "content": "In this section, we aim to evaluate the spatial relationship understanding capabilities of the models. To do so, we conduct controlled experiments using data splits where not all pairs of animals are seen during training. The relations considered in these experiments are \"left/right\" and \"above/below\". Hence, the task is to choose between the original caption of the form \"X left of Y\" and the caption with the swapped order \"Y left of X\". We consider the following generalization axes:"}, {"title": "Parsing", "content": "For the parsing of the training and testing data we used a llama-3-70b Instruct model with the following prompt:"}]}