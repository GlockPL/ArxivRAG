{"title": "FREE ENERGY RISK METRICS FOR SYSTEMICALLY SAFE AI: GATEKEEPING MULTI-AGENT STUDY", "authors": ["Michael Walters", "Rafael Kaufmann", "Justice Sefas", "Thomas Kopinski"], "abstract": "We investigate the Free Energy Principle as a foundation for measuring risk in agentic and multi-agent systems. From these principles we introduce a Cumulative Risk Exposure metric that is flexible to differing contexts and needs. We contrast this to other popular theories for safe AI that hinge on massive amounts of data or describing arbitrarily complex world models. In our framework, stakeholders need only specify their preferences over system outcomes, providing straightforward and transparent decision rules for risk governance and mitigation. This framework naturally accounts for uncertainty in both world model and preference model, allowing for decision-making that is epistemically and axiologically humble, parsimonious, and future-proof. We demonstrate this novel approach in a simplified autonomous vehicle environment with multi-agent vehicles whose driving policies are mediated by gatekeepers that evaluate, in an online fashion, the risk to the collective safety in their neighborhood, and intervene through each vehicle's policy when appropriate. We show that the introduction of gatekeepers in an AV fleet, even at low penetration, can generate significant positive externalities in terms of increased system safety.", "sections": [{"title": "1 Introduction", "content": "Rooted in physics, the Free Energy Principle (FEP), in tandem with Bayesian inference of world models, offers a compelling foundation in the Active Inference (ActInf) formulation of intelligent systems [Da +20; Fri+24; GB20; Hyl+24; KGT21; Lei22; MTB21; PPF22]. One of the earliest progenitors of this idea is the Helmholtz machine, proposed by Dayan, Hinton, Neal, and Zemel in 1995 [Day+95], connecting the statistical mechanics governing the Helmholtz Free Energy and perceptual processing. Here, treating the log-likelihood of perceptrons in a neural model as energy akin to statistical mechanics, learning proceeds as the minimization of variational free energy (VFE) through variational inference\u00b9. Fast-forward to the present day and the Bayesian Brain hypothesis has found popularity in neurosymbolic modeling, whereby perception and other decision/control mechanisms are driven by predictive (generative) models and hierarchical Bayesian uncertainty-resolving directives [Deu10; PF19]. For an enriching summary around FEP and its connections to Bayesian/Active Inference, see Gottwald and Braun [GB20].\nThe FEP in Active Inference can be applied in a few different ways [MTB21; GB20], and interpreted in many more [Da +20]. These interpretations are variations on a classic theme: exploitation vs. exploration. Whether it's accuracy vs. complexity, risk vs. ambiguity, intrinsic value vs. extrinsic value, model evidence vs. information gain, or energy vs. entropy, the mechanics of the FEP live in the tension of this duality.\nTo illustrate the rich connection between probabilistic modeling and the FEP, we begin with the common setup of an agent making observations of at time t, and wishing to infer the latent state of the world xt through actions at according to policy \u03c0 (which we will take as Markovian). The agent's uncertainty about xt given its observations is expressed as the posterior p(xt|ot) = p(ot, xt)/p(ot). With the standard assumption of the intractability of p(ot), Variational Inference prescribes we instead work with a tractable approximation, q(xt) that can be computed.\nTypically, the mismatch between p(x) and q(x) is quantified by the Kullback-Leibler divergence,\n$D_{KL}(q||p) = \\int q(x) \\ln \\frac{q(x)}{p(x)} dx.$"}, {"title": null, "content": "We will drop the subscript t going forward in most cases when it is irrelevant. The KL divergence is convex for fixed p. Thus, the problem is recast with a new proxy objective: the minimization of DKL (q|p) through inference on q.\nFinally, the KL divergence between the variational approximation of the true posterior $D_{KL}(q(x)||p(x|o))$ has an intrinsic connection to the log-evidence $ln p(o)$:\n$D_{KL}(q(x)||p(x|o)) = \\int q(x) \\ln \\frac{q(x)p(o)}{p(x, o)} dx$\n$= - \\int q(x) \\ln p(x, o) dx + \\int q(x) \\ln q(x) dx + \\int q(x) \\ln p(o) dx $\n$\\Rightarrow E_{q(x)}[\\ln q(x) - \\ln p(x, o)] + \\ln p(o).$\\                                                                                                                                                                    (1)\nIn line (1) we make use of the fact that p(o) is independent of q(x). Rearranging, we can express the evidence as\n$\\ln p(o) = D_{KL}(q(x)||p(x|o)) - E_{q(x)}[\\ln q(x) - \\ln p(x, o)]$\n$= D_{KL}(q(x)||p(x|o)) - F(q).$\nThe -F(q) term gives a floor for the evidence (since $D_{KL}(q||p) \\geq 0$), and as the evidence ln p(o) is fixed with respect to q(x), minimizing F(q) drives the floor up and minimizes the KL divergence between q and p.\nAs mentioned earlier, the free energy in statistical mechanics is, abstractly, the sum of an accuracy term (energy), and a complexity term (entropy). For example, for some distribution 4, the Helmholtz Free Energy,\n$F_H(\\phi) = \\langle E \\rangle_{\\phi} + \\frac{1}{\\beta}H[\\phi]$\nwhere inverse temperature \u1e9e plays a weighting factor between energy and entropy. It is this similarity in form why F(q) is also called the variational free energy (VFE):\n$F(q) = - E_{q(x)}[\\ln p(x, o)] + E_{q(x)}[\\ln q(x)]$\n$= - E_{q(x)}[\\ln p(x, o)] - H[q(x)].\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ $(2)\n\"", "Energy": "nThe entropic term is a form of Occam's razor, encouraging models to make fewer assumptions or have too many extraneous parameters. It also functions like a regularizer against overfitting to model evidence by the energy term. In the ActInf framework, agents are driven to reduce \u201csurprisal\u201d\u2014the discrepancy between their models and the world, i.e. VFE\u2014primarily through two means ([PPF22] \u00a72.6, [MTB21]):\n\u2022 (Perception) Updating world models to better fit the evidence.\n\u2022 (Action) Exploration and actions in the world to elicit desirable outcomes or reduce uncertainty.\nWith a generative model p(x, o), artificial agents can simulate potential futures and use the expected free energy to evaluate policies and inform their decisions."}, {"title": "1.1 Extending into the future", "content": "The VFE-based objective discussed thus far has focused on deriving a variational model q(x) through inference that both explains the data and is balanced by an entropic term. However, this falls short of how a fully equipped ActInf agent would operate intelligently: using preference-biased predicted futures to inform its actions. We defer the philosophical justification [PPF22], but in sum, incorporating a preference prior distribution \u03c0(o) over expected outcomes (or states p(x)) embeds the goal directives of the agent into the objective-elevating it from being just a Bayesian evidence-building machine.\nInference then proceeds towards minimizing the Expected Free Energy (EFE) across candidate policies, where quality of fit is judged by the expected log likelihood of desired observations, and exploration is encouraged through maximizing the divergence between the expected variational posterior and the expected variational prior 2.\n$EFE_t = E_{q(o_t,x_t|\\pi)}[\\ln q(x_t|\\pi) - \\ln p(o_t, x_t)]\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ $(3)\n$\\approx - E_{q(o_t,x_t|\\pi)}[\\ln p(o_t)] - E_{q(o_t|\\pi)} D_{KL}[q(x_t|o_t)||q(x_t|\\pi)]\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ $(4)\nwhere p(ot, xt) = p(Otxt)p(xt). Taking a temporal mean-field factorization of the variational posterior $q(x_{t:\\tau}, \\pi) \\approx q(\\pi) \\prod_{s=t}^{\\tau}q(x_s|\\pi)$ and generative model $p(o_{t:\\tau}, x_{t:}) \\approx \\prod_{s=t}^{\\tau}p(o_s)q(x_s|o_s)$, severs the temporal dependence between steps, meaning the optimal path is that with the lowest sum \u2211t EFEt.\nMillidge, Tschantz, and Buckley [MTB21] give considerable contemplation to the question of extending the VFE into the future and the natural origins of the EFE\u00b3."}, {"title": null, "content": "authors go on to introduce an additional FEP-based formulation, the Free Energy of the Future (FEF), which has an objective driven by the minimization of the entropic term, in stark contrast to epistemic maximization:\n$FEF_t = E_{q(o_t,x_t|\\pi)}[\\ln q(x_t|o_t) - \\ln p(o_t, x_t)]\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ $(5)\n$\\approx - E_{q(o_t,x_t|\\pi)}[\\ln p(o_tx_t)] + E_{q(o_t|\\pi)} D_{KL}[q(x_t|t)||q(x_t|\\pi)]\\ \\ \\ \\ \\ \\ \\ \\ \\ $(6)\nNote the epistemic terms between the EFE and FEF differ only in their sign. Encouraging the minimization of an information-seeking term seems anathema to an ActInf agent, yet minimizing the FEF satisfies the FEP-driven goals of 1) bounding the model evidence (surprisal), and 2) minimizing the divergence between a variational posterior and a target model (whether that is based on the true world distribution or a preference prior in the context of Active Inference)."}, {"title": "2 Cumulative Risk Exposure", "content": "We propose and showcase an arrangement that repurposes and reframes the VFE construction laid out above. The canonical Active Inference agent begins with a known preference prior that informs its actions as expected VFE computations. However, by obfuscating the preference prior from the agent or at least the true stakeholder preference prior, if we still want the agent to operate in an ActInf fashion with its own preference prior-we can help buffer against certain reward specification pitfalls, like reward hacking, etc. In essence, this defines a Gatekeeper (GK) arrangement, where the GK has access to the agent's policies and can compute a policy's expected free energy according to its hidden preference prior as a form of policy evaluation and risk metric. Expressing values as preference prior distributions allows for a wide range of preference structures, including risk-aversion, social preferences, and non-Markovian utility functions [SA23].\nThe free energy risk metric can be utilized as context prescribes, and we demonstrate a simple method whereby a risk threshold is defined as a point of criticality demanding gatekeeper intervention\u2074. To our knowledge, this is the first VFE-based gatekeeper model for agentic AI applications.\nWhen defining a risk metric, both the FEF and the EFE provide viable options. For contexts where exploration is discouraged, the FEF offers a better form since its objective is minimized through low-entropy futures. This may be the better choice for safety-critical applications where minimizing unexpected behavior is preferred. Conversely, in domains with significant structural uncertainty"}, {"title": "2.1 Adapting for observation-space", "content": "Often it is the case that a preference prior is expressed in terms of outcomes, not hidden states. Thus, it useful to express the VFE formulae in observation-space. From the definition of EFE in Eq. (3) (dropping the time-dependence),\n$E_{q(o,x|\\pi)}[\\ln q(x|\\pi) - \\ln p(o, x)] = E_{q(o,x|\\pi)}[\\ln q(x|\\pi) - \\ln p(o) - \\ln q(x|o)]$\n$= E_{q(o,x|\\pi)}[\\ln q(x|\\pi) - \\ln p(o) - \\ln q(o|x) - \\ln q(x)+ \\ln q(o|\\pi)]$\n$= E_{q(o,x|\\pi)}[-\\ln p(o) - \\ln q(o|x) + \\ln q(o|\\pi)]$\n$= - E_{q(o,x|\\pi)}[\\ln p(o)] - E_{q(x|\\pi)}[D_{KL}[q(o|2)||q(o|\\pi)]]$\nmaking use of the definitions $p(x, o) = q(x|o)p(o)$ and $q(x, o|\\pi) = q(x|\\pi)q(o|x) = q(o|\\pi)q(x|o)$, and Bayes' rule. Computationally, one can estimate these values through sampling of the variational prior and the produced observations. Similar decompositions can be achieved for the FEF:\n$E_{q(0,2/\\pi)} [\\ln q(x|o) - \\ln p(o, x)] = E_{q(0,x|\\pi)} [\\ln q(x|o) - \\ln p(o|x) - \\ln q(x|\\pi)]$\n$= E_{q(0,x|\\pi)} [\\ln q(0|x) + \\ln q(x|\\pi) - \\ln q(o|\\pi) - \\ln p(o|x) - \\ln q(x|\\pi)]$\n$- E_{q(0,x|\\pi)} [\\ln p(o|x)] + E_{q(x|\\pi)} [D_{KL}[q(o|x)||q(o|\\pi)]]$\nBetween the two decompositions, we see the sign flip on the epistemic term persist.\nFinally, the VFE risk formulations thus far are lacking a balancing variable that weights the epistemic and extrinsic components. In analogy with free energy formulations of thermodynamics, we can introduce an inverse \u201ctemperature\" to balance the terms of our risk equation. In abstract, the instantaneous risk at time t, for a variable set \u03a6 = [q, p, \u03c0] is\n$G_t(\\Phi) = \\langle E \\rangle_{\\Phi} \\pm \\frac{1}{\\beta}H[\\Phi],\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ $(7)\nwhere E and H are the energetic and entropic components5. Recall, the EFE and FEF are expected free energy forms,\""}, {"title": null, "content": "which can be y time-discounted in aggregation across time. We thus define the Cumulative Risk Exposure (CRE)\n$G_{\\sum}(\\phi,t) = \\sum_{t'} \\gamma^{t'} G_{t+t'}(\\phi),\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ $(8)\nthough we will commonly drop the time subscript in our discussions."}, {"title": "2.2 Preference prior construction", "content": "Choice of preference prior is context-dependent, but a natural form is the Boltzmann distribution over some loss function L:\n$p(\\mathcal{L}) = e^{-\\beta \\mathcal{L}}/Z, Z = \\sum_j e^{-\\beta \\mathcal{L}_j}.\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ $(9)\nWith this formulation, the inverse temperature term in Eq. (7), which serves to balance the extrinsic and intrinsic terms, equivalently operates on the extrinsic, preference-based term instead of the intrinsic term,\n$\\frac{1}{\\beta}G = - \\frac{1}{\\beta} E_{\\phi} E_q[\\ln p] = H$\n$\\Rightarrow \\beta E_q [\\mathcal{L}] + H + \\ln(Z).$\nConsequently, from Eq. (9) \u1e9e quantifies a tolerance to loss, scaling p accordingly, and can be thought of as a preference temperature of our system. Very strong preference biases create a \"low temperature\u201d (high \u03b2) system that is very energetically sensitive to preference alignment; conversely, weak preference bias creatures a smoothed out preference distribution that is more entropy dominated, with lower energetic sensitivity.\nFurther, the Boltzmann distribution has the property that the ratio of state probabilities\n$\\frac{p(\\mathcal{L}_1)}{p(\\mathcal{L}_2)} = exp(-\\beta(\\mathcal{L}_1 - \\mathcal{L}_2)).$\nThus, we can calibrate \u1e9e from a maximum and minimum loss range, and those corresponding stakeholder-assigned desirabilities,\n$\\ln (\\frac{p_{max}}{p_{min}}) = -\\beta(\\mathcal{L}_{max} - \\mathcal{L}_{min})$\n$\\Rightarrow \\beta = \\frac{ln (p_{min}/p_{max})}{\\mathcal{L}_{max} - \\mathcal{L}_{min}} > 0.$\n\u03b2 is non-negative since since by definition the desirability $p_{min} \\geq p_{max}$, and $\\mathcal{L}_{max} \\geq \\mathcal{L}_{min}$.\nThe inverse temperature has an interesting parallel with the Probability Dependency Graph framework, where a \u1e9e term represents the degree of confidence/belief in a distribution [Ric22]. In our construction, confidence in pp can factored into 1/8, but the inverse temperature carries a slightly different implication: one could be entirely confident in p but still value including entropic contributions."}, {"title": "2.3 Extending the approach", "content": "As discussed by Hyland et al. [Hyl+24], minimizing a joint free energy as a sum of individual agent free energies can avail game-theoretically optimized solutions that would otherwise not be played in selfish policies. Indeed, joint free energy minimization has been postulated as a potential core mechanism behind collective agency in biological systems [SL24; ML24]. It is also translatable to the Cooperative Inverse Reinforcement Learning paradigm [Had+24], as agents model the preferences of humans and themselves. In our AV experiment, the free energy of neighboring vehicle gatekeepers is aggregated before making decisions, and could for instance deter a vehicle from speeding up because to reduce the collective free energy, at the expense of reducing their own.\nExtending CRE and VFE-based metrics hierarchically affords a natural and mathematically straightforward approach to first-principles AI safety. Several contemporary AI safety proposals feature prolific construction of probabilistic models (themselves constructed from AIs, at least in part). \"Guaranteed Safe Al\" demands rigorous world modeling to construct formal safety guarantees [Dal+24; TO23]. Bayesian, \u201cScientist Als\" exert caution within uncertainty bounds according to their world models, aided by simulation, but are also expected to require potentially massive amounts of compute [Ben24; Ben+24]. Elsewhere, the Gaia Protocol of globally coordinated, amortized learning, depends on LLM-aided context-dependent model construction [KL23; KL24]. There is strong overlap in each of these pursuits, grounded in the creation and exploration of probabilistic world models, and the VFE framework outlined herein provides a natural language to 1) embed safety specifications into world models, 2) direct agentic learning and exploration in their accordance, while 3) taking actions that are in the collective interest through the minimization of the joint free energy.\""}, {"title": "3 Gatekeeping Experiment", "content": "We investigated the application of this principle in a simulated autonomous vehicle (AV) setting, using a pared-back simulator, highway-env [Leu18], which is built on top of gymnasium [Tow+24]. Code for this experiment is available on Github [Wal24], and a sample video can be found here.\nOur highway track featured autonomous vehicles with a variable number of these being gatekeeper controlled. We adopt (and abuse) terminology from theory-of-mind research to distinguish Alters and Egos as the two main types of vehicles on the road. Alters have a static policy and constitute the background traffic of our simulation, whereas Egos are the vehicles of interest that we optionally assign gatekeepers to, measure, etc. Our results find that the introduction of gatekeepers controlling Ego policies"}, {"title": "3.1 Rewards and Loss", "content": "Our reward score was constituted from three aspects: target speed, collisions, and defensive driving. Ego vehicles received a speed reward Rs in the form of a Gaussian centered on a target speed \u03c5\u03c4:\n$R_s(v) = \\alpha \\exp[-(v - \\upsilon_{\\tau})^2/2\\sigma^2],\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ $(10)\nwhere constants \u03b1, \u03c3, and \u03c5\u03c4 were heuristically chosen. The collision reward Rc was simply a constant based on collision state s Sc\n$R_c(s) = \\begin{cases} -\\kappa & \\text{if } s = s_c \\\\ 0 & \\text{otherwise} \\end{cases}$,\nwith 5 heuristically chosen appropriately to ascribe high disincentive.\nBraking distance-the distance it takes to come to a full stop-is a property that scales quadratically with speed [THH00]. This is an important property to capture, which we combine with the common sense that proximity is inherently more risky, to formulate our defensive-driving reward:\n$R_D(j) = R_{D,max} - \\lambda [\\sum_{i \\in V_j} \\frac{w(i, j)^2}{2 m d_{ij}} + \\zeta],\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ $(11)\n$w(i, j) = max(0, v_j - v_i) \\times H(x_i - x_j) + max(0, v_i - v_j) \\times H(x_j - x_i),$\nwith scalar \u03bb > 0, vehicle index i of vehicle j's neighbors (set V\u2081), lane differential m \u2208 {0, 1, 2, . .. }, and neighbor distance dij. w(i, j) returns the magnitude of relative speed between j and its neighbor, using the Heaviside binary function H to control for if a neighbor is in front or behind. If vehicles j and i are drifting apart, w(i, j) is 0. The constant \u03b6 adds an additional penalty for vehicle proximity. Since the terms are penalizing, we subtract the bulk from a max reward RD,max and truncate to the range RD(j) \u2208 [0, RD,max]. The final result is a function that 1) penalizes quadratically with relative speeds between neighbors, 2) penalizes with increased proximity x 1/dij, but 3) less so as lane differential increases."}, {"title": null, "content": "The fact that Rc is negative is appropriately handled in the reward normalization process. Loss was then simply the negative sum of rewards, and constituted our only observed variable,\n$\\mathcal{L} = -\\sum R.\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ $\nIt is worth highlighting here that the resulting improved road safety, as a consequence of gatekeeper decision-making, was achieved with this single aggregate scalar variable and did not require the suite of AV sensor inputs in its decision evaluation."}, {"title": "3.2 Risk formulation", "content": "Since our experiment was a fully observable environment, and we assert ex hypothesi that our loss and p formulations are sufficient and accurate, we can drop any entropic contributions. In this context, therefore, CRE is identical to time-discounted expected utility7. Additionally, whereas the extrinsic terms in EFE/FEF are expectations over the variational model q(x, \u03bf\u03c0), we can directly work with \u03c1(\u03bf, \u03b1\u03c0) since we have a fully observable environment, and use Monte Carlo methods to approximate p(o,x|\u03c0).\nThe removal of entropy simplifies the determination of our stakeholder tolerance parameter. Without exploratory requirements, the scale of beta is irrelevant\u2014as with energy in many other contexts, we are only concerned with relative values, not absolutes. In other applications, \u03b2 may be determined as a forced constraint: cost in dollars, quantity, etc.\nTaken together, our final CRE is the expected utility\n$G_{\\sum}(\\mathcal{L}) = - \\sum_{t'} \\gamma^{t'} E_{p(\\mathcal{L})}[\\ln p(\\mathcal{L})] = \\sum_{t'} \\gamma^{t'} E_{p(\\mathcal{L})}[\\mathcal{L}]\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ $(12)"}, {"title": "3.3 Policies", "content": "The highway-env library has an automated Intelligent Driving Model (IDMVehicle) [THH00] class, which employs a combination of deterministic logic to calculate acceleration and steering. Lane changes are determined in part according to the Minimizing overall braking induced by lane change \u201cMOBIL\u201d model [KTH07], which, as advertised, tries to reduce imposed braking in its lane-change selection.\nThis vehicle policy is deterministic and has no machine learning or sampling involved in its decision-making. How-"}, {"title": "3.4 Gatekeeper Policy Control", "content": "For online vehicles, gatekeepers anticipate upcoming risk through internal simulations, then toggle their vehicle's policy to Defensive in risky situations, or back to Hotshot when deemed safe enough. Using Hotshot as a nominal policy may seem odd, but it gives a stronger counterbalance to observe the phenomenon of interest.\nGatekeepers run NMC internal Monte Carlo trajectories at regular, frequent intervals in the world simulation to compute a CRE estimate, following Eq. (12). Values for a given trajectory's risk are accumulated out to an MC horizon \u03c4 = 10 steps. Each vehicle's actions are not in a vacuum. Sharing local observations and predictions by opening channels of communication through gatekeepers enhances decision-making through a collective intelligence. After computing individual CREs, we replace each with the average of their local neighborhoods and have online vehicles make policy decisions from this average.\nConverting from a unitless CRE value to a policy decision is not self-evident, and is open to the needs of the"}, {"title": "3.5 Results & Discussion", "content": "The ultimate goal here is better decision-making according to stakeholder preferences through simulated futures. To that end, our main measuring stick is the defined loss Land collision results. Two baselines were simulated across 1200 world runs, for the Defensive and Hotshot policies. In a given baseline, all 12 of the ego vehicles would stick to the defined policy throughout, and thus no CRE calculations were performed for GK operations. Realized rewards and loss values were still measured at each step, however.\nThough the Hotshot policy has a consistently higher speed reward, it suffers in the defensive reward compared to the Defensive policy, and incurs substantially more collisions (Fig. 1). Ultimately, the erratic, dangerous Hotshot behavior garners greater loss on average.\nWith the introduction of online gatekeepers, we aspire for the best of both worlds: intelligent policy selection that responds to environment conditions. We found a considerable signal in support of this, that became increasingly pronounced proportional to GK presence. At full GK strength, crash avoidance was significantly improved, while finding opportunity to excel in defensive driving and target speed.\nFor the most part, the Defensive Baseline is always going to be hard to surpass: It is expected to have the fewest crashes and the highest RD. Thus, gatekeepers need to perform comparatively well in those two dimensions while eking out gains in Rs\u2014which is at odds with RD and Rc. Nonetheless, the Online-12 configuration handled this remarkably well, especially for the first half of the simulation where it tracked Hotshot-level Rs while approaching Defensive-level RD. This superior performance combination was most strongly exhibited in the Loss minimum by Online-12 around Step 25 that substantially outperformed both baselines. From visual observations, the first half of the simulation is the more dynamic portion of the simulation, requiring egos to navigate around themselves and alters more (since they have a higher target speed than alters), versus the latter portion where the road approaches more of a steady-state. The selection of p* = 2 yielded modest policy switching activity, and the \"Defensive Fraction\" in Fig. 1 indicates that typically"}, {"title": "4 Conclusion", "content": "The Free Energy Principle, as one of the foundational underpinnings of Active Inference, draws powerful connections between physical energetic laws and intelligent action, with explanations for exploitation-exploration naturally emergent. Encoding stakeholder preferences via the preference prior provides a highly flexible means to direct agentic learning. The Cumulative Risk Exposure metric introduced leverages these foundations to create an interpretable, modular utility to score policies according to biased futures. The preference-temperature and tolerance mechanics outlined also introduce a conceptual and instructional foothold for usage.\nStakeholders and AI agents can employ this safety metric to anticipate upcoming high risk situations and respond intelligently, as demonstrated by our autonomous vehicle experiment, which saw increasingly superior driving performance proportional to online usage. This principle has immense potential across agentic applications as a quick and effective utility for gauging risk which, in contrast to simple loss measures, is biased towards stakeholder preferences, providing straightforward and transparent decision rules for risk governance and mitigation."}]}