{"title": "OPENMU: YOUR SWISS ARMY KNIFE FOR MUSIC UNDERSTANDING", "authors": ["Mengjie Zhao", "Zhi Zhong", "Zhuoyuan Mao", "Shiqi Yang", "Wei-Hsiang Liao", "Shusuke Takahashi", "Hiromi Wakaki", "Yuki Mitsufuji"], "abstract": "We present OpenMU-Bench, a large-scale benchmark suite for addressing the data scarcity issue in training multimodal language models to understand music. To construct OpenMU-Bench, we leveraged existing datasets and bootstrapped new annotations. OpenMU-Bench also broadens the scope of music understanding by including lyrics understanding and music tool usage. Using OpenMU-Bench, we trained our music understanding model, OpenMU, with extensive ablations, demonstrating that OpenMU outperforms baseline models such as MU-LLaMA. Both OpenMU and OpenMU-Bench are open-sourced to facilitate future research in music understanding and to enhance creative music production efficiency\u00b9.", "sections": [{"title": "1 INTRODUCTION", "content": "Multimodal Large Language Models (MLLMs) have successfully extended large language models (LLMs) by enabling them to perceive, process, and understand data in modalities beyond text (Tsimpoukelli et al., 2021; Liu et al., 2023b; Zhu et al., 2023; McKinzie et al., 2024; Zhang et al., 2023; Gong et al., 2024), such as images, videos, and audio. However, there has been limited effort (Gardner et al., 2024) focused on constructing MLLMs capable of effectively understanding the music modality or addressing Music Information Retrieval (MIR) tasks. MIR is a research field focusing on modeling, understanding and interpreting data relevant to music, aiming to improve the efficiency of music production (Serra et al., 2013). Conventional machine learning algorithms (Wang, 2003; Casey et al., 2008) sparked the success in music searching. Subsequently, deep learning models expanded the success to music tagging (Won et al., 2020), transcription (Gardner et al., 2021; Toyama et al., 2023) and representation learning (Castellon et al., 2021; Li et al., 2024; Won et al., 2024).\nWe aim to contribute to the MIR field by training an MLLM, dubbed OpenMU, for understanding music clips. Building on the versatile capabilities of LLMs and pretrained audio encoders, OpenMU effectively comprehends and reasons about input music clips, producing relevant answers accordingly. We also enable OpenMU to leverage well-established music tools to encourage synergies between OpenMU and creative practitioners through cooperation. OpenMU is expected to greatly improve music production efficiency. Creative practitioners can instruct OpenMU to describe a music clip's contents and features, saving minutes of time compared to listening to the full track.\nThe major obstacle we faced when training and evaluating OpenMU was the issue of data scarcity in the music modality (Serra et al., 2013; Seeger, 2003; Holzapfel et al., 2018). To address this issue, we construct OpenMU-Bench, a large-scale benchmark for training and evaluating MLLMs in music understanding. To construct OpenMU-Bench, we bootstrap new datasets using GPT-3.5, and leverage existing datasets when available. As a result, OpenMU-Bench comprises approximately one million training examples, covering various aspects of music understanding, such as music captioning, reasoning, multi-choice question answering, lyrics understanding and music tool using. To the best of our knowledge, no large-scale open-sourced benchmark comparable to OpenMU-Bench currently exists, and we hope it will advance future research and development of MIR.\nIn summary, our contributions include: (1) Proposing OpenMU for music understanding. OpenMU is an MLLM dedicated to the music modality, outperforming baseline models such as MU-LLaMA (Liu et al., 2024) in tasks like music captioning, reasoning, and multiple-choice question an-"}, {"title": "2 RELATED WORK", "content": "Understanding music goes beyond recognizing objective attributes of music such as tempo (B\u00f6ck et al., 2015; Sun et al., 2021) or instrumentation (Gururani et al., 2019; Zhong et al., 2023). It is also subjective and highly context-dependent, like determining music genres (Kereliuk et al., 2015) or moods (Bogdanov et al., 2019; Koutini et al., 2019). Researchers succeeded in understanding music by classifying music clips into predefined tags (Li et al., 2024; Won et al., 2024). Recently, music captioning (Manco et al., 2021) and reasoning (Gardner et al., 2024) tasks, where natural language descriptions are employed to describe music clips, have earned increasing attention. Also, the ability of selecting correct answers in multi-choice question answering is included in music understanding Weck et al. (2024). However, there has been limited exploration into enabling MLLMs to utilize external digital tools (i.e., established music tools) for music analysis. We hypothesize that a music understanding model can further boost the workflow of creative practitioners by deeply integrating the set of widely adopted music tools. Last but not least, lyrics information processing (Watanabe & Goto, 2020), such as semantic lyrics understanding (Zhang et al., 2022) enhances the understanding of a music clip. Therefore, we integrate it in OpenMU-Bench. Overall, we broaden the scope of music understanding by considering two extra aspects beyond music captioning and reasoning: Music tool using and lyrics understanding.\nFoundation models for music understanding. Multimodal LLMs (MLLMs) (Tsimpoukelli et al., 2021; Liu et al., 2023b; Zhu et al., 2023; McKinzie et al., 2024; Gong et al., 2024) fuse non-textual information into LLMs (Liang et al., 2022) to solve real-world tasks requiring the ability of perceiving data in different modalities. The scope of MLLMs is recently expanded to include music. MU-LLaMA (Liu et al., 2024) and MusiLingo (Deng et al., 2024) narrowed down their scope to music captioning and question answering (QA); other critical aspects of music understanding, e.g., key and chord recognition, are not covered. Perhaps the closest to ours is Llark (Gardner et al., 2024). However, neither the model itself nor the music understanding datasets from Llark have been released. None of these models is capable of using music tools, an important ability to interact with creators. In this paper, we propose OpenMU-Bench and OpenMU to advance the field of music understanding. OpenMU-Bench holistically measures various aspects of music understanding, while OpenMU achieves state-of-art performance on the benchmark. Both OpenMU-Bench and OpenMU are released to facilitate the future research and development in this field.\nMusic understanding datasets. The proliferation of LLMs has spurred the development of benchmarks designed to holistically measure the genuine capabilities of LLMs. Benchmarks have been designed for NLP tasks (BIG-bench authors, 2023; Hendrycks et al., 2021), and vision-language tasks (Liu et al., 2023c; Fu et al., 2023; Ye et al., 2023). MMMU (Yue et al., 2023) included the music modality into evaluation but at a very narrow scope (334 entries of sheet music). Researchers are striving to address the data scarcity challenge of music: Doh et al. (2023) introduced LP-MusicCaps, associating LLM-augmented captions with music clips from MusicCaps (Agostinelli et al., 2023). Similarly, Liu et al. (2024) developed MusicQA, containing QA and captioning tasks for music clips from MusicCaps, MagnaTagATune (Law et al., 2009b), and MTG-Jamendo (Bogdanov et al., 2019). Concurrently, Deng et al. (2024) proposed MusicInstruct, which targets QA and captioning for clips in MusicCaps. Weck et al. (2024) create MuChoMusic as a music understanding benchmark containing 1,187 multiple-choice questions for evaluation. Building on existing datasets, we construct OpenMU-Bench by additionally bootstrapping new datasets using GPT-3.5. OpenMU-Bench contains about one million examples for training and evaluation across various music understanding tasks. We also standardize evaluation metrics to ensure consistency\u00b2 in reporting results on OpenMU-Bench."}, {"title": "3 CONSTRUCTING OPENMU-BENCH", "content": "This section outlines the construction of OpenMU-Bench. We introduce the five types of tasks included in OpenMU-Bench and explain the dataset construction procedures for each type. In addition to incorporating existing music understanding datasets, we generate new annotations for music clips from datasets that do not contain natural language annotations. Our goal is to integrate as many datasets as possible to enable OpenMU-Bench to comprehensively and systematically evaluate music understanding models. Furthermore, we specify the recommended evaluation metrics to ensure consistent and fair benchmarking.\n3.1 OPENMU-BENCH TASK TYPES\nMusic captioning tasks a model with generating textual descriptions capturing musical contents and key features of a music clip. A music understanding model excels at captioning can improve the efficiency of music production by generating music descriptions in a short time, eliminating the needs of listening to the entire music track by creators. Music reasoning, as defined by Gardner et al. (2024), tasks the model with answering questions in two aspects. First, it examines the interaction between different elements of a music clip, such as how a fast tempo is likely to correspond with a high energy level. Second, it explores how the real-world can interact with the music clip, e.g., how a creator can increase the energy level of a music clip by using faster tempos.\nTool using. The MIR community has developed a wide range of music technology tools for various tasks, such as tempo estimation, key detection, chord recognition, and instrument identification\u00b3.\nLyrics understanding. Lyrics, which carry rich semantic content, are often used to convey moods and emotions (Watanabe & Goto, 2020). We incorporate a lyrics understanding task into OpenMU-Bench. We consider a model that excels at understanding lyrics to be capable of producing interpretations similar to those of humans, conditioned on the music input. This task is framed as a text generation problem.\nMultiple-choice questions. Text generation models are inherently difficult to evaluate due to their open-ended nature (Celikyilmaz et al., 2020). This challenge extends to LLM-based music under-"}, {"title": "3.2 INDIVIDUAL DATASETS", "content": "As introduced in \u00a72, a few datasets already exist for music understanding. We incorporate these datasets and create new annotations to ensure that OpenMU-Bench has both a large scale and broad coverage. We describe each of the datasets, along with the applied modifications aligning them with the OpenMU-Bench task types. We adhere to existing train/test splits of the datasets when available (c.f. \u00a7A.4); Appendix \u00a7A.2 details the preprocessing and annotating details of OpenMU-Bench; we highlight only the key information here.\nMusicCaps, created by Agostinelli et al. (2023), is pivotal for the music captioning task. It contains approximately 5.5K 10-second music clips sourced from AudioSet (Gemmeke et al., 2017), with corresponding gold-standard text captions written by professional musicians. We incorporate MusicCaps into OpenMU-Bench as part of the captioning task. LPMusicCaps & LPMusicMTT (Doh et al., 2023) extend MusicCaps and the MagnaTagATune (Law et al., 2009a) dataset by generating additional textual descriptions. The authors prompt GPT-3.5 to \u201cwrite\u201d, \u201csummarize\u201d, \u201cparaphrase\u201d, and \u201cpredict attributes\u201d new captions to the music clips. We integrate approximately 8K LPMusicCaps and 51K LPMusicMTT training captions into OpenMU-Bench. MusicInstruct (Deng et al., 2024) also extends MusicCaps by creating question-answer pairs for the MusicCaps clips using GPT-4. This dataset contains approximately 60K question-answer pairs, which are categorized into two versions: a short version (MI-short) focusing on musical content such as tempo and genre, and a long version (MI-long) that paraphrases the MusicCaps captions. We integrate MusicInstruct into OpenMU-Bench as a captioning task, and report performance on both versions separately.\nMusicQA, developed by Liu et al. (2024) by prompting MPT (MosaicML-NLP-Team, 2023), is employed to train their MU-LLaMA. MusicQA is composed of MusicCaps clips for pretraining, MagnaTagATune (Law et al., 2009a) clips for finetuning, and MTG-Jamendo (Bogdanov et al., 2019) clips for testing. We incorporate MusicQA-Finetune and MusicQA-Test into OpenMU-Bench, while MusicQA-Pretrain, which contains the test split of MusicCaps, is excluded to prevent potential train-test leakage (Deng et al., 2024). Following Liu et al. (2024), we separate MusicQA into captioning and reasoning parts.\nMusic4all, developed by Pegoraro Santana et al. (2020), consists of approximately 100K music clips with rich metadata, including attributes like energy, valence, and genre. Based on this metadata, we prompt GPT-3.5 to generate annotations for both the captioning and reasoning tasks. The prompts used for these annotations are provided in the Appendix \u00a7A.3. GTZAN, developed by Tzanetakis & Cook (2002), contains approximately 1K 30-second music clips, each labeled with genre tags and we create extra tempo tags with Madmom (B\u00f6ck et al., 2016). Based on these tags, we generate captioning and reasoning annotations with prompting. MusicNet (Thickstun et al., 2017) contains 1"}, {"title": "3.3 EVALUATION METRICS", "content": "OpenMU-Bench leverages common evaluation metrics for text generation tasks: captioning, reasoning, and lyrics understanding. BLEU-1, BLEU (Papineni et al., 2002), Meteor (Banerjee & Lavie, 2005), Rouge-1, and Rouge-L (Lin, 2004) measure an answer's textual overlap with the gold standard, while BertScore (Zhang et al., 2020) measures similarity in the semantic representation space of a pretrained BERT model. For all evaluations, we report the scores computed using the F-measure. We report accuracy for the task of multiple-choice questions."}, {"title": "4 MODEL ARCHITECTURE AND TRAINING DETAILS", "content": "4.1 MODEL ARCHITECTURE\nEncoding music clips. We use AudioMAE (Huang et al., 2022) to encode an input music clip into vector representations. Specifically, we use the \u201cViT-B AS-2M pretrained + finetuned\" version of AudioMAE, which is a Vision Transformer (Dosovitskiy et al., 2021) initially pretrained with a masked auto-encoding reconstruction loss (He et al., 2022), followed by finetuning on tagging tasks (Gemmeke et al., 2017), both using the AudioSet2M (Gemmeke et al., 2017) dataset. The choice of using AudioMAE over other music encoders, such as MERT (Li et al., 2024) or Jukebox-5B (Dhariwal et al., 2020; Castellon et al., 2021), is motivated by two primary reasons. First, more than half of the audio clips inAudioSet2M consist of music or musical instrument recordings, resulting in approximately 3,137 hours of music data (compared to the 910 hours in the MERT-95M-public model (Li et al., 2024)). Audio encoders pretrained on AudioSet have shown competitive performance in music tagging tasks (Koutini et al., 2021; Niizumi et al., 2022). Second, the size of the multimodal encoder is not a performance bottleneck (McKinzie et al., 2024). Instead, the smaller number of parameters in ViT-B (86M) facilitates more efficient training.\nLLM. We use the open-sourced Llama3-8B-instruct (Dubey et al., 2024) as our LLM. Compared to previous Llama models (Touvron et al., 2023), Llama3 has been trained on higher-quality datasets and at larger scales, achieving GPT-4-level performance (Achiam et al., 2023) on numerous tasks.\nMusic-language projector links the representation space of the music encoder with the LLM. Studies (McKinzie et al., 2024; Liu et al., 2023a) have shown that the architecture of the projector itself has little impact on downstream task performance, while the number of tokens from the multimodal"}, {"title": "4.2 TRAINING DETAILS", "content": "Dataset preprocessing. When processing the music clips, we limit their maximum length to 30 seconds and zero-pad those shorter than 30 seconds. All music clips are resampled to 16 kHz and then converted to a 128-bin Mel-spectrogram with a 25-ms hann window and 10-ms hop size. Consequently, each music clip is represented as a mel-spectrogram with a shape of $(3072, 128)$. Since AudioMAE is trained to encode inputs of up to 10 seconds, we segment each mel-spectrogram into three parts, encode them separately, and then concatenate the results. As a result, each 30-second music clip is encoded by 1536 tokens, with each token having a shape of $(1, 768)$.\nThroughout our experiments, we used between 8 and 16 A100 40GB GPUs, depending on the experimental setup (c.f. \u00a75). In all experiments, we set the maximum context length of the LLM to 2048 tokens. We utilized DeepSpeed ZeRO-3 (Rajbhandari et al., 2020) and FlashAttention2 (Dao et al., 2022) to enable fast and efficient training. It took approximately three days to train OpenMU on the captioning and reasoning subsets of OpenMU-Bench (around 1 million data examples).\nTraining setup of OpenMU-Bench largely follows the common practice of MLLM training (Yin et al., 2023; Liu et al., 2023b; McKinzie et al., 2024), consisting of:\nStage (1) Captioning. We train OpenMU to generate captions, conditioned solely on the input music clip. The goal of Stage (1) training is to align the representation spaces of AudioMAE and Llama3, with the only trainable module in this stage being the music-language projector. We use the captioning subset of OpenMU-Bench for training in this stage. A key configuration is the number of music tokens fed into the LLM, which we discuss in detail in \u00a75.1. The remaining hyperparameters largely follow Liu et al. (2023b) and are provided in the Appendix \u00a7A.1.\nStage (2) Instruction Tuning. After aligning the music and text representation spaces, Stage (2) training enables OpenMU to follow various instructions in the music domain, such as inferring music genres or reasoning about the content of a music clip. In this stage, LoRA adapters (Hu et al., 2022) are incorporated into OpenMU's LLM, followed by fine-tuning on OpenMU-Bench's captioning and reasoning tasks. We focus on two critical research questions in this stage. First, we extensively evaluate OpenMU's task performance with respect to its LoRA parameters (see $5.2). Second, we investigate in-depth OpenMU's use of music information. Given the large-scale pretraining data of OpenMU's LLM, we hypothesize that OpenMU might be able to make correct predictions for knowledge-intensive questions even without relying on musical information within a music clip. We test this hypothesis and show that in order to achieve higher performance, OpenMU indeed relies on information from the music clip, demonstrating OpenMU's genuine ability to understand music."}, {"title": "5 DESIGNING OPENMU AND DISCUSSIONS", "content": "In this section, we explore and discuss the critical factors involved in training OpenMU. We aim for these detailed analyses to contribute to the research and development of future foundation models for music understanding.\n5.1 NUMBER OF MUSIC TOKENS\nMcKinzie et al. (2024) illustrate that the number of image tokens is more significant than the architecture of the vision-language projector in vision-language MLLMs. To the best of our knowledge, no prior research has addressed this critical aspect in the context of training foundation models for music understanding. This is particularly important because music clips can often be lengthy, leading to a large number of music tokens. For instance, the AudioMAE encoder outputs 1536 tokens for representing a 30-second music clip. While using all available tokens ensures the maximum use of music modality information, it may hinder training efficiency and limit the utility of the context window of the LLM (2048 in our case). In this section, we extensively evaluate the impact of the number of music tokens when training OpenMU.\nFigure 2 displays the training trajectories (log-scale) of both Stage (1) and Stage (2) training, where we apply mean-pooling to every 2\u2013128 music tokens output by AudioMAE. For instance, mean-pooling every 8 tokens means using only 1536/8 = 192 tokens to represent the 30-second input music clip. The difference among mean-pooling 2, 8, and 32 tokens is small, suggesting that there may be redundancies in the representations of the encoded music clip. Although aggressively mean-pooling 128 tokens significantly reduces the overall training time (7.5 hours when pooling 128 tokens vs. 20 hours when mean-pooling 2 tokens), the setting results in a weaker convergence. As a result, we empirically focus on model variants with mean-pooling 8 tokens in the next sections to balance between model convergence and training efficiency."}, {"title": "5.2 LORA, TASK PERFORMANCE, AND MUSIC INFORMATION UTILITY", "content": "As introduced in \u00a74.2, Low-Rank Adaptation (LoRA; Hu et al. (2022)), is employed in Stage (2) training to efficiently adapt OpenMU's LLM for following instructions in the music domain. Given an LLM weight parameter matrix $W \\in \\mathbb{R}^{d \\times k}$, instead of directly modifying $W$, LoRA introduces and trains two matrices, $B\\in \\mathbb{R}^{d \\times r}$ and $A\\in \\mathbb{R}^{r \\times k}$ for adapting $W$ to a downstream task:\n$W \\leftarrow W + \\frac{r}{\\alpha} B A$.\nThe LLM weight matrix $W$ remains unchanged; the LoRA rank $r$ determines the number of trainable parameters by controlling the size of $B$ and $A$. The matrix multiplication result, $BA$, represents the changes introduced by adaptation to a downstream task, scaled by $\\frac{r}{\\alpha}$. Here, $\\alpha$ is a hyper-parameter, and typically $r < \\alpha$. For OpenMU, we fix $\\alpha = 128$ following Liu et al. (2023b;a), while varying the value of $r$. Intuitively, a smaller rank $r$ imposes a stricter bottleneck on $B$ and $A$, requiring the learned parameter differences, represented by $BA$, to rely on fewer trainable parameters to capture concise and genuine information about the downstream task, which are subsequently scaled by a larger $\\frac{r}{\\alpha}$. In contrast, a larger $r$ introduces more trainable parameters, which may be prone to learning shortcuts, redundant information, or noise (Geirhos et al., 2020) during adaptation to the downstream task, subsequently scaled by a smaller $\\frac{r}{\\alpha}$. In this section, we investigate how LORA configurations affect Stage (2) training, as well as reporting the evaluation results of OpenMU on OpenMU-Bench.\nOpenMU-Bench task performance. shows the performance of OpenMU variants on the captioning (left) and reasoning (right) tasks of OpenMU-Bench. For each evaluation, we report the macro-average performance of each OpenMU variant across all subtasks in OpenMU-Bench. Additionally,\nMusic information utility. Given the large-scale pretraining data of the LLM, which already contains rich knowledge about music, an MLLM may be able to answer questions about music without relying on the information in the music clip. Hence, this section addresses a key question: Does OpenMU genuinely utilize information from the input clip to understand the music more effectively? To investigate this, we evaluate OpenMU variants on MuChoMusic (Weck et al., 2024), a dataset containing multiple-choice questions focused on music understanding. Questions such as \"Which sub-genre of rock music would best classify this piece?\" require the model to select the"}, {"title": "6 OVERALL RESULTS", "content": "In this section, we compare OpenMU with MU-LLaMA, a widely used music understanding model, on OpenMU-Bench. For OpenMU, we use the variant of mean-pooling 8 tokens with LoRA parameters 128/16. For MU-LLaMA, we use the checkpoint released by Liu et al. (2024).\nMusic captioning and reasoning results are presented in Table 3 and Table 4, respectively. We observe that OpenMU consistently outperforms MU-LLaMA across various captioning and reasoning tasks. Interestingly, MU-LLaMA lags behind OpenMU on MusicCaps, despite the fact that the MusicCaps test set was used during MU-LLaMA's pretraining stage (Liu et al., 2024; Deng et al., 2024). We believe this is due to the small size of MusicCaps its effectiveness was likely overshadowed by the larger finetuning datasets used for MU-LLaMA.\nMU-LLaMA outperforms OpenMU on the MusicInstruct-short captioning task (Deng et al., 2024) and the MusicQA-test reasoning task (Liu et al., 2024) in terms of surface form matching metrics by a large margin. However, we found that the gold references in these two subsets are biased to contain"}, {"title": "7 CONCLUSION", "content": "We presented OpenMU-Bench, a large-scale benchmark suite containing approximately one million examples for training and evaluating LLM-based music understanding models. We construct OpenMU-Bench by creating new annotations as well as leveraging existing datasets. We trained our music understanding model, OpenMU, with extensive ablations and demonstrated that it outperforms baseline models such as MU-LLaMA. Both OpenMU and OpenMU-Bench are open-sourced to facilitate future research in music understanding and enhance the efficiency of creative music production. Future work may explore extending OpenMU to support multiple music clips as input and enable in-context learning for music understanding. Another promising direction is enabling OpenMU to integrate more MIR tools, combining the strengths of LLMs and established tools for deeper music understanding."}, {"title": "A APPENDIX", "content": "A.1 TRAINING DETAILS AND HYPERPARAMETERS\nIn this section, we describe the detailed settings and hyperparameters used for training OpenMU.\nAll experiments were conducted using 8-16 A100 40GB GPUs, with BF16 enabled to ensure stable training. We use DeepSpeed ZERO-3 (Rajbhandari et al., 2020) and Flash Attention 2 (Dao et al., 2022) to reduce the memory consumption. We utilized the Adam optimizer and a cosine learning rate scheduler, with a 30% warm-up ratio.\nFor Stage (1) training, we pretrained OpenMU for 15 epochs on the captioning subtask of OpenMU-Bench, which consists of approximately 275K pairs of music clips and corresponding captions. Stage (1) training took approximately 10 hours for the checkpoint we evaluated (i.e., mean-pooling every 8 music tokens as illustrated in Figure 2. The initial learning rate was set to 1e-3, with a batch size of 8 per GPU.\nFor Stage (2) training, we extended pretraining of OpenMU for 10 epochs on the captioning and reasoning subtasks of OpenMU-Bench, which comprise roughly one million training examples. The initial learning rate was set to 2e-5, with the same per-GPU batch size of 8. Stage (2) required approximately 40 hours due to the increased size of training data.\nFor the lyrics understanding subtask, we trained OpenMU for 20 epochs, reusing the hyperparameters from Stage (2). Similarly, for the tool using subtask, we reused the Stage (2) hyperparameters but reduced the number of epochs to 5 due to the smaller dataset size for this task.\nA.2 METADATA OF DATASETS\nIn this paper, we contribute to creating the large-scale benchmark suite OpenMU-Bench for music understanding.\nIn contrast to other modalities such as images, where rich natural language descriptions are widely available across the internet (Schuhmann et al., 2022), music clips are often accompanied by tags, such as genre, year, and instruments. We consider these tags to be a form of metadata for the music clips. When constructing OpenMU-Bench, we bootstrap captions and reasoning texts in natural language about the music clips based on this metadata by prompting GPT-3.5.\nA.3 PROMPTS AND DATASET FORMAT\nBased on the metadata of each music clip (\u00a7A.2), we prompt GPT-3.5 to generate examples for the music understanding tasks. Our prompts are adapted from those used by Gardner et al. (2024), with modifications tailored to the available metadata of different OpenMU-Bench subsets and subtasks."}, {"title": "A.4 DATASET SPLITS", "content": "In this section, we provide details about the train/test splits of the OpenMU-Bench subtasks. Specifically, for MusicCaps, MusicInstruct, LPMusicCaps, LPMusicMTT, MusicQA, MusicNet, BART-Fusion, and MuchoMusic, we follow the train/test splits proposed in the original papers. For GTZAN, we used the widely accepted filter-fault split (Kereliuk et al., 2015), and the split from MARBLE (Yuan et al., 2023) for MTT.\nFor Music4All, we start with the 800 music clips from BART-Fusion as the initial test set. We then expand this set by randomly sampling music clips until the total reaches 5,000. The remaining music clips and their annotations are used as training data. For MTG-Jamendo, we use annotations where the music clips from folds 90 to 99 of the original dataset (Bogdanov et al., 2019) serve as the test data, while the remaining clips and their annotations are treated as training data. For tool using, we randomly sample 80% examples for training and 20% for testing."}, {"title": "A.5 TOOLS", "content": "We define simple tools for solving MIR tasks such as tempo estimator. They are implemented as simple Python wrapper to the Madmom toolkit (B\u00f6ck et al., 2016), which has been widely used in MIR. For example, the tempo estimator can be implemented as:\nOpenMU then calls for such a tool to estimate the tempo, when being asked questions such as \u201cLet me know the tempo of this music clip.\" and replying with \"The music has tempo [EstimateTempo() \u2192 n] beats per minute.\"."}]}