{"title": "LEARNING EVOLVING TOOLS FOR LARGE LANGUAGE MODELS", "authors": ["Guoxin Chen", "Zhong Zhang", "Xin Cong", "Fangda Guo", "Yesai Wu", "Yankai Lin", "Wenzheng Feng", "Yasheng Wang"], "abstract": "Tool learning enables large language models (LLMs) to interact with external tools and APIs, greatly expanding the application scope of LLMs. However, due to the dynamic nature of external environments, these tools and APIs may become outdated over time, preventing LLMs from correctly invoking tools. Existing research primarily focuses on static environments and overlooks this issue, limiting the adaptability of LLMs in real-world applications. In this paper, we propose TOOLEVO, a novel framework designed to enhance the adaptive and reflective capabilities of LLMs against tool variability. By leveraging Monte Carlo Tree Search, TOOLEVO facilitates active exploration and interaction of LLMs within dynamic environments, allowing for autonomous self-reflection and self-updating of tool usage based on environmental feedback. Additionally, we introduce ToolQA-D, a benchmark specifically designed to evaluate the impact of tool variability. Extensive experiments demonstrate the effectiveness and stability of our approach, highlighting the importance of adaptability to tool variability for effective tool learning.", "sections": [{"title": "1 INTRODUCTION", "content": "Tool learning aims to augment large language models (LLMs) with tools and APIs\u00b9 (Schick et al., 2023; Qin et al., 2023a; Tang et al., 2023; Guo et al., 2024). This augmentation enables LLMs to interact with external environments and real-world applications, thereby expanding their capabilities to tackle a diverse array of complex tasks (Qiao et al., 2024b; Yang et al., 2024b; Lu et al., 2024) and having become a vital component in building LLM agents (Chen et al., 2024; Qian et al., 2024).\nA critical challenge often overlooked is the inherent dynamism of external environment. In the realm of tool learning, dynamic environments are primarily manifested as tool variability, which include changes in API names, parameters, or response formats. APIs are subject to continual evolution due to various factors, such as version updates, optimizations, and deprecations. This rapid and frequent"}, {"title": "2 PRELIMINARIES", "content": "2.1 TASK FORMULATION\nIn this paper, we consider the scenario of tool variability, which is both prevalent and demanding in practical applications. The input for this task consists of the task description D and the collected APIs Pc = {P1, P2, ..., Pn}. However, due to tool variability, the APIs actually deployed on the server, denoted as P\u2081 = {P1, P2, . . ., Pm}, may differ from Po in terms of API names, parameters"}, {"title": "3 METHODOLOGY", "content": "3.1 OVERVIEW\nAs illustrated in Figure 1 (right), we propose the TOOLEVO framework, which aims to actively immerse the LLM in dynamic environments with the help of MCTS. This enables the LLM to autonomously reflect on and update existing tool usage, rather than merely executing rigid tool invocations. Through actively exploring dynamic environments, the LLM accumulates trial-and-error experiences for self-improvement, enhancing its understanding and adaptability to tool variability.\n3.2 INTERACTION WITH DYNAMIC ENVIRONMENTS\nConsidering the risk that the API usage Pe provided in the prompt may become outdated over time, our TOOLEVO encourages the LLM to interact with dynamic environments through MCTS, thereby enhancing its adaptive and reflective capabilities regarding tool variability. Specifically, we customize the four key operations of MCTS as follows:\nSelection: During the selection process, we traverse the tree from the root to a leaf node using the PUCT algorithm (Rosin, 2011). This algorithm selects the most promising node to explore, striking a balance between exploring new states and exploiting known valuable states, which can be represented as:\nPUCT(s, a) = Q(s, a) + Cpuct \u00b7 P(s,a) \\frac{\\sqrt{N(s)}}{1+ N(s, a)}, (1)\nwhere Cpuct is a constant that controls the balance between exploration and exploitation; Q(s, a) and P(s, a) are the Q-value and prior probability of taking action a in state s, respectively. Additionally, N(s) and N(s, a) denote the visit counts for state s and for taking action a from state s, respectively.\nExpansion: Once an expandable leaf node is selected, the current state st is used as input to further expand the tree. Candidate actions are sampled using the policy \u03c0\u03bf (i.e., the LLM), as follows:\na^{1}_{t}, a^{2}_{t}, ..., a^{k}_{t} = \\pi_{\\theta}(a|s_{t}), (2)"}, {"title": "3.3 SELF-REFLECTION AND TOOL-UPDATE", "content": "With the customized MCTS described above, our TOOLEVO actively engages LLMs in both exploration and exploitation within dynamic environments. To address tool variability, we incorporate the self-reflection and tool-update module into MCTS based on environmental feedback. This allows the LLM not only to deal with invocation errors through self-reflection but also to autonomously summarize new tool usage through the tool-update module to update API usage Pe in the prompt, ultimately fostering a deeper understanding and robustness in the face of tool variability.\nSelf-Reflection In our TOOLEVO, the self-reflection module generates verbal reflections based on external environmental feedback, providing valuable insights for future corrections. It is important to note that, as shown in Figure 1, the self-reflection module and the LLM agent are the same LLM and we do not rely on a more powerful model, such as GPT-4. The external environment typically provides various API error messages, including invocation errors and deprecation errors (Robbes et al., 2012; Brito et al., 2018). The purpose of encouraging LLM to interact with the external environment through MCTS is to enable the model to effectively utilize the environmental feedback, rather than backtracking or stopping when encountering errors as in previous work (Qin et al., 2023b; Guo et al., 2024). Specifically, when an error message is encountered, we use this error state st as input, allowing the model to reflect on the cause of the error and try to solve it.\na^{1}_{ref}, a^{2}_{ref}, ..., a^{k}_{ref} = \\pi_{\\theta}(a|s_{t}); s_{t+1} = Cat(s_{t}, a_{ref}), (4)\nwhere aref is the reflective action based on the error state st. We will append these new reflective states st+1 after st instead of interrupting further exploration of this error state."}, {"title": "3.4 SELF-IMPROVEMENT FROM TRIAL AND ERROR", "content": "In our TOOLEVO, we employ MCTS to encourage the LLM to interact with dynamic environments, thereby collecting trial-and-error experiences (trajectories from the root node to the terminal node in the tree) involving self-reflection and tool updating. Through these experiences, we aim for the model to master the behavior of self-reflection and self-updating of tool usage in a dynamic environment, thereby enhancing adaptability to tool variability rather than merely learning the specific tool invocation. We evaluate the quality of these experiences based on whether the task is successfully completed, allowing the LLM \u03c0\u03b8 to improve itself through the successful experiences y+:\nL = arg min_{\\theta} \u2013 log \\pi_{\\theta}(y^{+}|D,Pc). (5)\nNotably, failed experiences can be further leveraged through preference learning (Rafailov et al., 2023). However, our work emphasizes the importance of interaction with dynamic environments. We focus solely on SFT for a fair comparison following the prior research (Zhuang et al., 2023b)."}, {"title": "4 TOOLQA-D", "content": "To the best of our knowledge, we are the first to investigate the impact of tool variability on tool learning, leading to the absence of benchmarks in this domain. For research purposes, we have constructed the first benchmark for tool variability, termed ToolQA-D, based on ToolQA (Zhuang et al., 2023b). There are several compelling reasons for developing this dataset based on ToolQA:\n\u2022 Modifiability: Most existing benchmarks (Qin et al., 2023b; Guo et al., 2024) provide APIs through platforms like RapidAPI (link), which limits our ability to control or modify these APIs. This constraint inhibits the implementation of tool variability. In contrast, ToolQA enables us to directly modify the API services, facilitating the exploration of dynamic tool behaviors.\n\u2022 Result-oriented: Unlike most existing benchmarks (Li et al., 2023; Tang et al., 2023; Du et al., 2024), ToolQA emphasizes results rather than the sequence of API invocations. This distinction is critical, as modifying the API in these benchmarks necessitates re-annotating the entire sequence of API invocations. By focusing on results, ToolQA eliminates the need for large-scale re-annotation, thereby facilitating a more flexible and efficient evaluation of tool variability.\n\u2022 Sophisticated yet Comprehensive: Although ToolQA comprises only 12 tools, it offers a diverse range of functionalities, including text tools, database tools, graph tools, code tools, and system tools. This comprehensive yet manageable set of tools stands in stark contrast to benchmarks that incorporate thousands of APIs (Qin et al., 2023b; Tang et al., 2023), where implementing tool variability for initial explorations can be exceedingly challenging. Consequently, ToolQA provides a balanced and practical platform for our pioneering research into tool variability.\n\u2022 Challenge: ToolQA poses significant challenges by encompassing 7 datasets across two levels of difficulty: easy and hard. These tasks cannot be accomplished solely with the internal knowledge of LLM; rather, they necessitate the integration of external knowledge through tool"}, {"title": "5 EXPERIMENTS", "content": "5.1 EXPERINMENTAL SETUP\nDataset Setup We conduct extensive experiments on ToolQA-D to investigate tool variability. In this work, we aim to demonstrate the importance of enabling LLMs to interact with dynamic environments during the training phase, rather than merely memorizing how to use the tools in a static environment. As shown in Table 1, we utilize Psin as our training environment while evaluating performance in three different environments of (1) Pc to demonstrate that self-improve in tool variability can still adapt to static environments effectively, even without specifically training on the provided tools. (2) Psin to demonstrate that, with the help of our TOOLEVO, LLM can reflect on and autonomously master the new tool usage through interactions with dynamic environments. (3) Psoon to demonstrate the generalizability of adaptability to tool variability, which is completely different from Psin. Notably, in all experiments, the LLM can only access Pe in the prompt. Both Psin and Psoop are agnostic to the LLM and can only be learned from environmental feedback.\nBaselines We compare our approach with typical tool-learning methods that only consider static environments to highlight the importance of accounting for tool variability. We compare our TOOLEVO with: (1) Proprietary models: ChatGPT, GPT-4 (Achiam et al., 2023), GPT-40, GPT-40-mini (OpenAI, 2024), and Claude-3.5-Sonnet (Anthropic, 2024); (2) Open-source models: Llama3-series (Dubey et al., 2024) and Qwen2-series (Yang et al., 2024a); (3) Static supervised fine-tuning (Static-SFT) method where the supervised data focuses exclusively on tool usage in static environments, as in previous studies (Zhuang et al., 2023b; Qin et al., 2023b; Wang et al., 2024).\nImplementation Details We briefly summarize the implementation details in this section, with further elaboration available in Appendix A.6. Through our TOOLEVO, we encourage the LLMs to interact with the dynamic environment of Psin and accumulate trial-and-error experiences, resulting in approximately 30k tool trajectories. Subsequently, we fine-tune these models on the collected experiences to enhance their adaptability within dynamic environments. For our experiments, we utilize Llama3-8B (Dubey et al., 2024) and Qwen2-7B (Yang et al., 2024a) as the base models. Note that the collection of tool trajectories and the training process are conducted separately for the different base models. For Static-SFT (Zhuang et al., 2023b), we collect the tool trajectories in a static environment (Pc) through our TOOLEVO and train it as our strong baseline. All methods perform greedy decoding and utilize 3-shot learning based on the REACT format (Yao et al., 2023) (see Appendix D). In all experiments, we only provide the API usage of Pe along with its demonstrations in the prompt, but the API usage deployed on the server may vary in different setups.\n5.2 MAIN RESULTS\nPerformance in Static Environment (Pc in Prompt and Pe on Server) We first compare the performance of our method in the static environment Pc, as shown in Table 2. It is noteworthy that our method does not undergo fine-tuning on the tool trajectories regarding Pc, while other baselines, including in-context learning and Static-SFT methods, benefit from corresponding demonstrations"}, {"title": "5.3 ANALYSIS ON TOOL VARIABILITY", "content": "In this section, we utilize the Llama3 series (Dubey et al., 2024) as base models to further investigate the detailed impacts of tool variability on performance, including changes in API names, parameters, and response formats. All changes are re-randomized by GPT-4 in this section (Appendix B.2).\nAPI Name As illustrated in Figure 4, we examine two key aspects of API name: textual variations and random insertion of special characters. Our findings are summarized as follows: (1) Compared with other models, Static-SFT model exhibits more fluctuations in the performance regarding tool variability. This instability can be attributed to the stereotype of API usage presented in the prompt being always correct, which arises from its focus on tool usage in static environments (Zhuang et al., 2023b; Guo et al., 2024). (2) Without any modification to the API names, the insertion of special characters poses significant challenges for tool-using abilities of LLMs. It is noteworthy that we"}, {"title": "5.4 ABLATION STUDY", "content": "In this section, we conduct a comprehensive ablation study to investigate the role of each component in our TOOLEVO. Since the tool update module is based on the self-reflection module, we conduct ablation experiments with the following setting: (1) \u201cw/o tool update\": During the interaction with the dynamic environment, we remove the tool update module. This means that after successfully invoking the new tools, the LLM does not summarize the usage of the new tools nor update Pe in the prompt. (2) \"w/o self-reflection\u201d: In this scenario, we remove self-reflection regarding invocation errors. However, reflection on deprecation errors is retained; otherwise, the model would struggle to handle tool variability effectively. All above ablation experiments are conducted in the OOD dynamic environment (PSOOD), as detailed in Table 5.\""}, {"title": "6 RELATED WORK", "content": "Tool Learning Recently, tool learning has demonstrated impressive potential for extending the capabilities of LLMs through various APIs (Schick et al., 2023; Qin et al., 2023b; Tang et al., 2023; Yang et al., 2023; Zhuang et al., 2023a; Huang et al., 2023; Shen et al., 2024; Yang et al., 2024b; Qiao et al., 2024b; Song et al., 2024; Qiao et al., 2024a; Sun et al., 2024). Current approaches can be divided into two categories: fine-tuning and in-context learning (ICL). Fine-tuning-based approaches (Parisi et al., 2022; Schick et al., 2023; Yang et al., 2023) construct high-quality tool chains for training LLM to use a specific set of tools, while ICL-based approaches (Zhuang et al., 2023a; Huang et al., 2023; Liang et al., 2024) directly incorporate sophisticated tool descriptions and usage demonstrations into the input context. However, existing work predominantly focuses on enabling LLMs to proficiently master API usage in static environments, often neglecting the inherent dynamic nature of the API ecosystem. Our experiments reveal that both outdated tool descriptions and demonstrations in the context, as well as stereotypes formed through fine-tuning, will lead to performance degradation in the face of tool variability. In this paper, we aim to bridge this gap and explore the impact of various API changes.\nMonte Carlo Tree Search MCTS, which integrates the principles of Monte Carlo sampling (Shapiro, 2003) with tree search, has emerged as a seminal search algorithm for decision-making processes. Its ability to effectively balance exploration and exploitation in complex environments is particularly noteworthy (Browne et al., 2012). The AlphaGo series (Silver et al., 2016; 2017; Schrittwieser et al., 2020) have demonstrated the efficacy of MCTS in the context of game-playing environments. In the realm of large language models, MCTS plays a critical role in various tasks, such as text generation (Zhang et al., 2023; Liu et al., 2023), mathematical reasoning (Zhu et al., 2023; Trinh et al., 2024) and so on. In this paper, we explore dynamic environments and leverage MCTS to enhance the interaction between LLMs and the environment, thereby accumulating trial-and-error experiences for tool variability."}, {"title": "7 CONCLUSION", "content": "In this paper, we have investigated the impact of tool variability on the tool-using capabilities of LLMs, which is often overlooked in existing studies. We find that existing methods that focus exclusively on static environments tend to reinforce stereotypes and are more susceptible to tool variability. To address these issues, we propose TOOLEVO, a MCTS-based framework that encourages LLMs to interact with dynamic environments and actively reflect on and update the usage of existing tools based on environmental feedback. This approach enables LLMs to understand tool variability through trial-and-error experiences, rather than merely memorizing invocation patterns of existing tools. Additionally, we have constructed ToolQA-D, the first benchmark specifically designed for evaluating the impact of tool variability. Through extensive experiments, we have demonstrated that TOOLEVO effectively handles tool variability, both in in-domain and out-of-domain settings."}, {"title": "APPENDIX", "content": "In this section, we provide a comprehensive overview of the implementation of our proposed method, including datasets, baseline methods, case studies and error analysis. For additional insights and more intricate details, we refer the reader to our supplementary materials.\nA IMPLEMENTATION DETAILS\nA.1 ACTION AND NODE DETAILS\nFollowing previous work (Zhuang et al., 2023b), we steer the model to perform an API invocation in the REACT format (Yao et al., 2023), which includes thought (textual analysis), tool invocation (APIs), and observation (environmental feedback). For the sake of clarity, we will consider Ot as part of at. Here is an example:\nREACT format of our Action at:\nThought: To answer this question, I should first load the database containing coffee price information. The database named 'coffee' seems to be the relevant one. (textual analysis)\nAction: LoadDB (API name)\nAction Input: {\"DBName\": \"coffee\"} (API parameters)\nObservation: We have successfully loaded the coffee database, including the following columns:\nDate, Open, High, Low, Close, Volume, Currency. (environment feedback, Ot)\nwhere the tool invocation consists of \"Action\" (specifying the API name) and \"Action Input\" (defining the parameters for the API). \u201cThought\u201d, \u201cAction\u201d, and \u201cAction Input\" are generated by the LLM and the \"Obvervation\" is obtained from the environment feedback.\nAdditionally, in our work, we define the Node in the Monte Carlo Tree as shown in Figure 1 (right) to record information, such as the action at, the Q-values, the visiting counts N, depth, and etc. Each node is defined to only contain a single action.\nA.2 DYNAMIC ENVIRONMENT\nAs previously discussed, we define the dynamic environment as everything excluding the LLM. In our setup, according to the real-world scenarios (Robbes et al., 2012; Brito et al., 2018), the dynamic environment will provide the following information:\n\u2022 Task Completion State: Upon task completion, the environment provides feedback indicating whether the task was successful (reward r = 1) or failed (reward r = -1). Specifically, we assess task completion state through the evaluation toolkit (Zhuang et al., 2023b). We assign rewards to the search path (tool trajectories) based on the task completion state.\nr=\\begin{cases}\n1 & \\text{if task is successful} \\\\\n-1 & \\text{if task is failed}\n\\end{cases} (6)\n\u2022 API Response: The API response refers specifically to the information returned following a successful API invocation. Different APIs provide various functionalities, thereby enabling LLMs to perform complex tasks.\n\u2022 API Error Message: API error message can be categorized into two main types: invocation errors and deprecation errors.\n\u2013 Invocation Errors: These errors arise from incorrect API names or parameter settings and are not related to API deprecation. To address such errors, the model must possess self-reflective capabilities, allowing it to adjust its input to successfully invoke the API.\n\u2013 Deprecation Errors: These indicate that an API has been removed in the current version, with a recommendation to utilize a newer API instead. To address such errors, the model must process tool update capabilities based on environment feedback."}, {"title": "A.3 DATASETS DETAILS", "content": "Table 6 provides detailed statistics for ToolQA-D. We use the dataset processed by Zhuang et al. (2023b) as our test set, and reprocess a batch of data according to the settings established by Zhuang et al. (2023b) as our training set and development set. We ensure that there is no overlap between the training, development, and test sets. Different datasets cater to different contexts and require various tools, making ToolQA-D particularly suitable for exploring tool variability in initial research\u00b2. For more API settings, please refer to Appendix B.\nA.4 CACHED ROLLOUT STRATEGY\nIn MCTS, estimating the expected return for each state has consistently been a focal point of research (Shapiro, 2003; Browne et al., 2012). AlphaGo (Silver et al., 2016) achieves efficient rollout"}, {"title": "A.5 TOOL UPDATE MODULE DETAILS", "content": "In our framework, we implement the tool update module as a system tool, referred to as UpdateTool [newtool_desc], which serves to update the descriptions of existing tools. This tool requires the model to autonomously summarize the usage of the new tool based on environmental feedback after successfully invoking the new tool. Following this, it updates the tool usage Pe in the prompt. The tool update module can be viewed as a memory mechanism (Zhang et al., 2024), where the model stores its own behavior and environment feedback into prompts to facilitate future tool invocations. This adaptive process not only enhances the model's efficiency but also contributes to its adaptability to tool variability. For further insights into self-reflection and tool update modules, we provide detailed case studies in Appendix C."}, {"title": "A.6 ADDITIONAL IMPLEMENTATION DETAILS", "content": "Parameter Details For a fair comparison, we utilize the same prompt for all methods, including our TOOLEVO and all baselines, as detailed in Appendix D. We only provide API usage of Pe in the prompt and use Psin as the deployed API usage on the server. To facilitate interaction with the dynamic environment and collect trial-and-error experiences, we construct 20 trees for each question. We employ 3-shot learning to guide the pretrained model (Dubey et al., 2024; Yang et al.,"}, {"title": "B MORE DETAILS IN TOOLQA-D", "content": "B.1 BENCHMARK DETAILS\nAs discussed in Section 4, we have constructed ToolQA-D, the first benchmark designed to investigate tool variability based on ToolQA (Zhuang et al., 2023b). We denote the original API usage of ToolQA as Pe and employ the GPT-4 to randomly modify Pe in terms of API names, parameters, and response formats, resulting in two new sets of API usage: Psin and PSOOD. Note that the prompt is constrained solely to the collected API usage Pc, which may be outdated, while Psin and PSOOD are deployed on the server that is agnostic to LLMs. Furthermore, the prompts do not contain any information about tool variability unless the model autonomously interacts with the external environment. In other words, we deliberately refrain from indicating in the prompts that the API may become deprecated. Moreover, in our ToolQA-D, we ensure that the modified APIs retain similar meanings and adhere to CamelCase conventions, aligning with real-world scenarios.\nSpecifically, we will maintain the system tools in a static condition (e.g., \"Finish\", which relies solely on the LLM and is independent on the external environment). We enable GPT-4 to randomly alter the names or parameters of the APIs, including textual variations and the random insertion of special characters. Here, we primarily focus on the underscore (\"_\"), as it is the most commonly encountered case. We will further investigate the impact of other special characters in Section 5.3. Additionally, we will slightly alter the functionality and the invocation patterns of the APIs. For instance, for \"RetrieveAgenda\", we will introduce the \"return_num\""}, {"title": "C CASE STUDY", "content": "C.1 CASES OF SELF-REFLECTION\nIn our TOOLEVO, we require the model to engage in self-reflection upon encountering errors, rather than backtracking or stopping reasoning as in previous work (Schick et al., 2023; Qin et al., 2023b). Self-reflection is reflected in the subsequent steps of thought and action, as shown in Equation 4. Here is an example of self-reflection:\nAn example of Self-Reflection\n<SOME PREVIOUS STEPS HERE ...>\nThought: The observation indicates that there was a type error when attempting to subtract the integer (1245) from the float (1240.0). This means that one of the values is not being properly converted to an integer before subtraction. I need to adjust my code accordingly. (There was a type error in the previous step, and the model starts to reflect on the cause"}, {"title": "C.2 CASES OF TOOL UPDATE", "content": "As discussed in Appendix A.5, we have transformed the tool update module into a system tool: UpdateTool. Here is an example of tool update module:\nAn example of Tool Update\n<SOME PREVIOUS STEPS HERE ...>\nThought: With the opening and closing prices for 2012-03-08, I can calculate the percentage change in coffee price. The formula to calculate percentage change is ((Closing Price - Opening Price) / Opening Price) * 100."}, {"title": "C.3 ERROR ANALYSIS", "content": "To facilitate future research, we conducted a comprehensive error analysis of performance in the PSOOD environment. We investigate gpt-40-mini and the Llama3 series, such as Llama3-72B-Instruct, Static-SFT (Llama3-8B) and our TOOLEVO (Llama3-8B), by randomly selecting 50 error samples for analysis, as detailed in Table 7. We have identified three main types of errors:\n\u2022 Invocation Error in New Tools: refers to the challenges that the model encounters when exploring the usage of new tools. Since only outdated API usage is provided in the prompt, the model is more prone to make mistakes when using new tools in a dynamic environment. There are mainly two reasons:\n\u2013 Invalid Invocation: refers to instances where the model encounters invocation errors while invoking new tools, such as incorrect API names or parameters, leading to failed tasks."}, {"title": "D PROMPTS", "content": "Following Zhuang et al. (2023b), we employ few-shot learning to guide the pretrained model in executing tool invocation, ensuring that the outputs adhere to the format of thought/action/action input/observation (Yao et al., 2023), as illustrated in the following example.\nYou are a powerful agent with excellent capabilities in using tools.\nAnswer the questions as best you can. You have access to the following tool:\n(1) RetrieveAgenda [keyword), which retrieves the agenda related to keyword.\n(2) RetrieveScirex[keyword], which retrieves machine learning papers' paragraphs related to keyword.\n(3) LoadDB (DBName), which loads the database DBName and returns the database. The DBName can be one of the following: flights/coffee/airbnb/yelp/agenda.\n(4) FilterDB [condition), which filters the database DBName by the column"}]}