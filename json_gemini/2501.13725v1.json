{"title": "You Only Crash Once v2: Perceptually Consistent Strong Features for One-Stage Domain Adaptive Detection of Space Terrain", "authors": ["Timothy Chase Jr", "Christopher Wilson", "Karthik Dantu"], "abstract": "The in-situ detection of planetary, lunar, and small-body surface terrain is crucial for autonomous spacecraft applications, where learning-based computer vision methods are increasingly employed to enable intelligence without prior information or human intervention. However, many of these methods remain computationally expensive for spacecraft processors and prevent real-time operation. Training of such algorithms is additionally complex due to the scarcity of labeled data and reliance on supervised learning approaches. Unsupervised Domain Adaptation (UDA) offers a promising solution by facilitating model training with disparate data sources such as simulations or synthetic scenes, although UDA is difficult to apply to celestial environments where challenging feature spaces are paramount. To alleviate such issues, You Only Crash Once (YOCOv1) has studied the integration of Visual Similarity-based Alignment (VSA) into lightweight one-stage object detection architectures to improve space terrain UDA. Although proven effective, the approach faces notable limitations, including performance degradations in multi-class and high-altitude scenarios. Building upon the foundation of YOCOv1, we propose novel additions to the VSA scheme that enhance terrain detection capabilities under UDA, and our approach is evaluated across both simulated and real-world data. Our second YOCO rendition, YOCOv2, is capable of achieving state-of-the-art UDA performance on surface terrain detection, where we showcase improvements upwards of 31% compared with YOCOv1 and terrestrial state-of-the-art. We demonstrate the practical utility of YOCOv2 with spacecraft flight hardware performance benchmarking and qualitative evaluation of NASA mission data. All code and datasets will be made open source upon publication.", "sections": [{"title": "1. Introduction", "content": "The detection of celestial surface terrain is critical for autonomous spaceflight applications such as Terrain Relative Navigation (TRN), Entry, Descent, and Landing (EDL), hazard analysis, and scientific data collection. Traditional methods are limited by the computational constraints of radiation-hardened systems, relying on extensive a priori imaging and offline processing by human operators [1], [2], [3], [4]. While effective in missions like the Mars Perseverance Rover landing [4] and OSIRIS-REx's touch-and-go [1], [2], these approaches extend mission timelines, increase costs, lack generalization, and execute at low processing rates. The recent advent of space-grade neural network accelerators [5] enables the deployment of state-of-the-art computer vision techniques, such as object detection, offering real-time performance [6] and robust recognition [7] to overcome these limitations.\nA major constraint in current object detection methods is their reliance on supervised learning, which requires extensive labeled data to detect and classify objects effectively. This dependency poses significant challenges for spaceflight applications, as celestial surface imagery is limited, and ground truth labels are often difficult or impossible to obtain, especially for missions targeting uncharted environments. In terrestrial vision tasks, Unsupervised Domain Adaptation (UDA) addresses similar issues by training models on accessible, distinct data distributions and adapting them to the target environment. This approach is particularly suited for space applications, where photorealistic simulations or lower-resolution mission data can serve as initial training sources.\nApplying UDA to celestial surface imaging is particularly challenging due to difficult illumination conditions, textureless regions, and feature space similarities inherent in unstructured space terrain [8], [9], [10], [11]. A promising method to address these challenges is Visual Similarity-based Alignment (VSA). Initially developed for two-stage detectors [12], VSA groups object features by visual appearance into instance-level clusters, reducing distribution shifts without relying on noisy or inaccurate class labels during training. However, two-stage detectors are generally too computationally expensive for spacecraft accelerators, requiring a one-stage solution for real-time operation [6]. Efforts to adapt VSA for one-stage models through the You Only Crash Once (YOCO [13]) framework have been largely successful but still face performance degradations in overly smooth or feature-similar terrains, such as those exhibited during high-altitude imagery of the Martian surface.\nIn this work, we present the next iteration of YOCO tailored to a broader range of one-stage architectures and propose an enhanced VSA formulation for real-time, onboard terrain detection in challenging planetary, lunar, and small-body environments. We target improvements in the presence of textureless regions and varying illumination conditions, integrating recent advances in image generation to mitigate distribution gaps. We investigate a comprehensive set of VSA techniques, including instance- and intra-feature-based clustering within adversarial and contrastive learning frameworks, identifying correlations between clustering methods and terrain types. Through rigorous analysis on simulated and real-world datasets, we achieve a performance improvement of over 31% compared to YOCOv1 and terrestrial state-of-the-art methods. Overall, we make the following contributions in this work:\n\u2022 We perform a deep characterization of the You Only Look Once (YOLO) model family for celestial surface terrain detection in UDA settings.\n\u2022 We benchmark eight YOLO model variants on NASA spacecraft hardware, determining the applicability toward in-situ, real-time operation.\n\u2022 We devise a novel formulation of VSA within YOLO, proposing improvements in perceptual feature comparisons and robust feature selection that bolster UDA performance.\n\u2022 We develop six datasets consisting of both simulated and real-world data emulating Mars, Moon, and Asteroid terrain surfaces for UDA evaluation.\n\u2022 We perform extensive quantitative evaluation, analyzing correlations between the type of VSA and celestial environment where we showcase improved UDA transfer by upwards of 31%."}, {"title": "II. Related Work", "content": "Celestial terrain detection has historically focused on lunar craters, and has evolved from classical computer vision methods to deep learning approaches. Convolutional Neural Networks (CNNs) have been applied for crater classification via edge detection [14] and synthetic data-driven cascaded networks [15], while U-Net architectures trained on Digital Elevation Maps (DEMs) [16] and infrared maps [17], [18] have enabled segmentation. The fusion of CNNs, image processing, and DEM data has been additionally successful [19], [20]. Object detection models like YOLOv3 [21], Faster R-CNN [22], and Mask R-CNN [23] were first used with curated datasets from previous missions. Limited research has addressed general multi-class terrain detection in varying environments, however, with the only such works employing density analysis for hazard labeling of SIFT features [24] and ensemble learning with multiple Faster R-CNNs [25]. The only approach studying UDA, YOCOv1 [13], focused on intra-feature clustering within the YOLOv3 architecture.\nThese methods face numerous practical challenges. Traditional image processing demands computational optimization [11], [6], and object detection models like YOLOv3 and Faster R-CNN cannot perform real-time inference on current spacecraft accelerators [6]. Synthetic training data often suffers from domain shift, while domain adaptation remains underexplored in space applications. Prior methods focus only on craters, offering limited generalization to other terrain such as dunes, mountains, and ridges. Multi-class scenarios further complicate dataset creation, where reliance on DEMs or hand labeling is both resource-intensive and non-transferable to novel, unexplored environments; a critical limitation to future missions to worlds such as Titan [26] and Enceladus [27]."}, {"title": "B. Visual Similarity-based Domain Adaptation", "content": "VSA was initially introduced within two-stage frameworks that align source and target domain proposals based on visual similarity clustering rather than class or pseudo-class labels [12]. Subsequent enhancements include online algorithms [28] and image-to-image translation for intermediate and style-consistent domain data [29]. Grounding pseudo-labels have been adopted [30] as well as mean teacher-student frameworks [31]. Transformers have been employed for improved adaptation [32], while auxiliary heads that compensate for discarded task-relevant features have enhanced performance [33].\nAdopting VSA into one-stage detectors has been challenging due to the lack of disparate region proposals, prompting the exploration of alternative network designs. Single unlabeled target images were utilized [34], along with adversarial alignment combined with self-training to address style gaps [35] and employing weak supervision [36] or style-invariant losses [37]. Instance-level domain alignment has been enhanced using self-attention and novel discriminator architectures [38] while merging target and source samples [39] and employing multi-scale adversarial learning [40] have also demonstrated success."}, {"title": "C. VSA on Celestial Terrain", "content": "Celestial terrain images pose significant challenges due to homogeneous regions, smooth surfaces, and a lack of distinct gradients or textures, especially at higher altitudes. These factors complicate traditional object detection pipelines, particularly in the presence of domain gaps. VSA offers a promising approach to address these challenges by enhancing network correlations across global terrain features, such as structure, appearance, shape, and differentiation from background uniformity. However, VSA studies remain sparse in this context. Previous Earth-based aerial VSA have lacked a focus on geological terrain [41], while YOCOv1 [13] is the only work targeting generalizable multi-class celestial terrain detection in UDA settings.\nYOCOv1 addresses celestial appearance challenges through intra-feature clustering, correlating activations responsible for detections and reducing domain shift in their appearance. This approach contrasts with earlier methods that extract object features before clustering, neglecting background features. While effective in single-class feature-rich scenes like terrestrial urban driving, these earlier methods struggle with multi-class celestial surfaces (e.g., Mars), where features such as craters, dunes, and mountains lack sufficient activation discernibility. Despite its conceptual strengths, YOCOv1's practicality is limited by the YOLOv3 architecture, which cannot achieve real-time execution on spacecraft hardware [6]."}, {"title": "III. Methodology", "content": "YOCOv1 marked a significant advancement in enhancing UDA performance for one-stage terrain detection but remains constrained by limitations that hinder its applicability to future missions. Building on the YOCOv1 framework, we propose an improved VSA strategy that accommodates a broader range of celestial environments and generalizes across YOLO architectures capable of real-time performance on current spacecraft hardware. In this section, we first describe the generalized YOLO architecture and modifications for UDA in subsection A, followed by our method for high-level global feature alignment in subsection B and improved visual similarity-based instance alignment in subsection C."}, {"title": "A. Network Architecture and UDA", "content": "The one-stage architecture used in YOCOv1, You Only Look Once (YOLO), can be viewed generally as three distinct sub-networks, including a backbone feature extractor, neck, and three scale (small, medium, large) detection heads. YOLO has seen many new versions in recent years (e.g., v5 [42], v6 [43], v8 [44]) but has largely adhered to this paradigm. Our integration of UDA begins with input images consisting of labeled source (S) and unlabeled target (T) data, where source images constitute any arbitrary (although assumed to be style-similar) training dataset and target images are unlabeled real-world images from the desired operating domain. With features F\u2208 RC\u00d7H\u00d7W extracted from the backbone, we denote features from source and target imagery as Fs and FT respectively and consider these features global (FG) as the backbone is a coarse processor. These global features are processed through the neck to produce local features Fl which in turn seed the regression of bounding box, classification, and confidence properties to yield source and target detection matrices Ds and DT. The objective of UDA is to minimize the discrepancy between source and target features both globally and locally, enabling invariance to the domain and yielding accurate target detections DT without any explicit target supervision. Our learning objective for this alignment at the global (image) and local (instance) levels is denoted by LImg(FF, FT) and Linst(FS, F+) respectively. The full learning objective of YOLO with image/instance UDA components is thus given by:\nL = LYOLO(Ds) + LImg(FF, FT) + LInst(FS, F+) (1)\nwhere LYOLO is the traditional YOLO supervised loss. We refer readers to the YOLO literature ([42], [43], [44]) for the full definition. The architecture with UDA components is visualized in Figure 2."}, {"title": "B. Global Alignment (LImg)", "content": "UDA at the global feature level is facilitated by an adversarial training regime. Adversarial training learns a discriminator that predicts the originating domain of the features, where the goal of the network is to fool this discriminator into making the wrong predictions and induce feature-domain ambiguity. Specifically, global features from source Fs and target FT domains are input into a discriminator D which learns to predict the originating domain d using a standard cross-entropy loss:\nLAdv(Fa) = -dlog(D(Fa)) \u2013 (1 \u2013 d) log(1 \u2013 D(Fa)) (2)\nwhere d = 0 for source (FF) and d = 1 for target (FF) data. To ensure the features from both domains become indistinguishable to the discriminator, the loss in Equation 2 is maximized with respect to Fa. This is achieved by using a Gradient Reverse Layer (GRL) [45], which applies a negative scaling factor to the gradients during backpropagation and reverses gradient flow. The GRL ensures that the discriminator minimizes the cross-entropy loss for domain classification, while the feature extractor maximizes it. The global image alignment loss LImg is given by:\nLImg(FF, FT) = LAdv(FF) + LAdv(FF). (3)"}, {"title": "C. Visual Similarity-based Local Alignment (Linst)", "content": "The alignment of detection instance features is the main driver of UDA performance in object detection. To facilitate this in the context of challenging celestial surface imagery, we first propose a general-purpose regularization scheme that constrains domain drift on pre-regression features, a process which we refer to as Perceptual Consistency (PC, subsubsection 1). Then, we examine two methodologies for minimizing object domain gap via VSA: Instance Clustering (subsubsection 2) and Feature Clustering (subsubsection 3)."}, {"title": "1. Perceptual Consistency Regularization", "content": "Previous studies [46], [47] have explored multi-scale discriminators to enhance adversarial training with feature matching. This approach extracts features from layers of the discriminator over each domain and aligns these features by minimizing the mean absolute error, which stabilizes the training process and improves performance by accounting for fine-grained details. As YOLO outputs (pre-regression) detection features at multiple scales directly, a similar regularization can be applied. We denote this regularization as Perceptual Consistency (PC) and define a Lpc loss as:\n|K|\n1\nLPC(F, F+) = \u03a3F - F\ni=1Wi (4)\nwhere the set of scales K = (large, medium, small), K 3 and Fl represent the features at scale i. Weights Wi = 2|K|-i rank the importance of each scale with an emphasis on smaller spatial resolutions."}, {"title": "2. Instance Clustering VSA", "content": "Instance Clustering VSA takes as input the instance-level features for both source (Fs) and target (FT) domains along with regressed detections Ds and Dr. We adopt the ViSGA formulation [12] to the one-stage setup, beginning by extracting instance-only features via the bounding box parameters, where each instance is flattened to yield fixed instance-feature embeddings z \u2208 RN\u00d7m. Hierarchical clustering is then employed over the set of source and target feature embeddings to create visually similar groupings; a flexible approach that dynamically assigns cluster centers instead of fixing a pre-determined amount as in K-Means. The average of each cluster is taken to construct a group-wise representative, which is input to instance discriminators and adversarially learned in a similar fashion to Equation 2.\nAn alternative to adversarial training is the application of max-margin contrastive losses found common in the metric learning literature. Here, we again follow the ViSGA [12] formulation and match embeddings from one domain to the nearest-neighbor (nn) embedding from the other domain and minimize the distance between their representations:\nnn(i) = argminjZT||zsi - ztj||\n(5)\n|zs-zt|\n|ZT|\ni\n+ \u03a3max{0,m-||zs - zt||2}\nj,j\u2260nn(i)\n(6)\nThe ability of hierarchical clustering to facilitate VSA is entirely dependent on the distinguishability of the instance embeddings. Poor disparity in the embedding space leads to loosely coupled groupings and unclear decision boundaries. This becomes incredibly prevalent in the context of celestial surface imagery, where we assume an already less distinctive feature space. To bolster instance distinction and downstream clustering performance, we propose to filter away features that may lead to ambiguous inter-similarities, and ensure that only strong, robust features that truly represent the terrain of interest persist into the embeddings. Our technique for Strong Feature Filtering (SFF) begins by ranking each channel via channel-wise attention [48] of the instance-level feature Fl to obtain feature weights Fw:\nFw = \u03c3(Conv1D(GAP(F))) (7)\nwhere Conv1D is a one-dimensional convolution and GAP is the Global Average Pooling operator [49]. The channel rankings Fw are then applied to Ff and sorted to yield ranked features F, where we select the Top-K highest ranks to obtain filtered features \u00ca."}, {"title": "3. Feature Clustering VSA", "content": "As an alternative to extracting instance embeddings, YOCOv1 claims that the activations present within the channels of Ff are a powerful identifier of detection class, and hypothesizes that forming clusters channel-wise will exhibit the same terrain distinctions with added residual background information that can assist alignment."}, {"title": "IV. Evaluation", "content": "Our evaluation begins with benchmarking YOLO version-size variants on spacecraft compute hardware to assess their feasibility for real-time, in-situ deployment, as detailed in subsection A. We then describe the datasets used for UDA evaluation, followed by implementation details and an analysis of the quantitative results in subsection B. Lastly, we conduct qualitative experiments using real-world data in subsection C."}, {"title": "A. Spacecraft Hardware Benchmarks", "content": "The YOLO model family offers various version and size variants, each optimized for different hardware configurations and performance needs. We profile many of these variants on a flight hardware analog, the TUL PYNQ-Z2 evaluation board, which features a Zynq-7020 SoC with a dual-core ARM Cortex-A9 processor and an Edge TPU USB Accelerator connected via USB 2.0. This setup mirrors the NASA SpaceCube Mini-Z [51]. Table I presents YOLO models capable of real-time operation on this platform, with performance metrics including model disk size (MB), number of parameters (M), floating point operations (FLOPs, G), and inference latency (ms) for both CPU and Zynq hardware.\nOf particular interest is the observation that only the nano (N), small (S), and medium (M) size models of YOLO v5, v6 (excluding medium), and v8 can achieve inference times under one second. Previous research [6] indicates that this significant drop in performance compared to desktop CPU architectures is primarily due to context switching and memory transfer bottlenecks between the co-processor (TPU) and ARM CPU. Large models, with substantial disk size (MB), require parameter streaming to the TPU for execution, which severely impacts inference time. However, we conclude that the majority of the evaluated variants are real-time capable and use these models as baselines for our UDA evaluation."}, {"title": "B. Quantitative Evaluation", "content": "For quantitative evaluation, we utilize six distinct datasets across three representative space environments: Mars, Asteroid, and Moon, and report the Mean Average Precision (mAP) at an Intersection-over-Union (IoU) threshold of 0.5 (mAP@0.5)."}, {"title": "1. Datasets", "content": "As no labeled instances of surface terrain imagery exist for Mars and Asteroid, we generate two distinct synthetic image datasets using the Blender 3D software with varying texture and material assets, referred to as Sim Mars and Sim Asteroid. Sim Mars contains ground truth labels for the classes Crater, Dune, and Mountain, while Sim Asteroid is a single-class dataset with Boulders. For the Moon, we use NASA mission-derived global mosaics of the lunar surface [52] at 474 and 100 meters-per-pixel (m/px) resolutions as source and target datasets, respectively, and produce image frames via random crops. We utilize the Robbins Moon Crater Database [53] to correlate known crater impact locations with each frame, generating ground truth bounding box labels. This setup reflects a realistic mission concept of operations, as the exploration of new bodies often involves gradually increasing data resolution. The 474 m/px (low resolution) data is used for training, while the 100 m/px (high resolution) data is used for testing. Each Moon dataset contains a single class, Crater."}, {"title": "2. Implementation Details", "content": "We utilize the Ultralytics [54] open-source implementation of all YOLO variants and implement our UDA components as plugins for ease of use and extensibility to any model available on the platform. We train all models from scratch without incorporating any pre-trained weights. Training was conducted on a single NVIDIA A6000 GPU for 50 epochs with a batch size of 32.\nWe study 11 loss variations across the models benchmarked in Table I. The baseline configurations are Source Only and Target Only, representing YOLO models trained exclusively on source or labeled target data, respectively. The Target Only model serves as an oracle, indicating performance without any data distribution gap. Nine additional models perform UDA, categorized based on the VSA types described in section III: adversarial instance clustering (Instance, Adversarial), contrastive instance clustering (Instance, Contrastive), and adversarial feature clustering (Feature, Adversarial). For Instance methods, we examine three loss configurations: (i) the original ViSGA [12] hierarchical clustering procedure adapted for YOLO (ViSGA), (ii) hierarchical clustering with PC loss (PC Only), and (iii) a combination of hierarchical clustering, PC, and SFF modules (PC SFF). For Feature methods, we explore three loss configurations: (i) YOCOvl's intra-feature hierarchical clustering [13] (YOCOvI), (ii) K-Means clustering with PC loss (PC K-Means), and (iii) PTAP's pixel pooling approach [48] (PTAP). For all Instance-based losses, we utilize agglomerative clustering with the maximum linkage, cosine as the distance function, and a cluster merging threshold of 0.1. For PC SFF and PTAP, we set the feature selection threshold to the top 50%."}, {"title": "3. Sim Mars", "content": "Accuracies for all model variants across the synthetic Mars scenes are shown in Table II, with the best-performing method for each architecture-size variant highlighted in green. Of first note, Instance clustering methods consistently outperform Feature clustering methods in most models, except for the YOLO 6-N variant where YOCOv1 achieves the highest performance. Within the Instance category, contrastive learning (Instance, Contrastive) is the most effective approach, achieving the highest accuracy in four out of eight models, while adversarial training (Instance, Adversarial) performs best in three models. This suggests that contrastive learning may better handle multi-class alignment, as discriminator networks struggle to accurately distinguish instances in the case of the Mars surface.\nAdditionally, a comparison of the adversarial and contrastive instance methods reveals that PC SFF outperforms ViSGA in most cases. Of the seven top-performing models in this category, PC SFF exceeds ViSGA in five. ViSGA only performs better with YOLO 5-M and 8-M. Notably, PC SFF shows strong generalizability, particularly in low-parameter models like YOLO 5-S, 6-S, and 8-S, with accuracies of 66.3%, 69.6%, and 66.6%, respectively.\nCompared to Source Only training, PC SFF provides significant performance improvements, particularly for YOLO 5-N, where it achieves an 11% accuracy increase, highlighting its effectiveness in making low-performing models viable through VSA with robust features."}, {"title": "4. Sim Asteroid", "content": "Similar results are observed in Table III for Sim Asteroid, where the proposed PC SFF technique outperforms the traditional ViSGA approach in instance clustering methods, surpassing it in four of eight cases. Notably, Perceptual Consistency alone performs best in two YOLO variants (5-N and 6-S), suggesting a potential detrimental learning effect in PC SFF's channel attention scoring mechanisms, particularly in YOLO 5-N, where performance drops by approximately 38% between PC Only and PC SFF. In contrast to Sim Mars, Feature clustering methods excel in three models, indicating their potential utility in single-class scenarios. ViSGA does not achieve the highest accuracy for any YOLO model, while YOCOv1 and adversarial PC SFF tie on YOLO 8-S. Among feature methods, K-Means with PC performs best in two instances, achieving the highest accuracy of 67% with the YOLO 6-N model. This suggests the effectiveness of intra-feature clustering in single-class scenes, where a K = 2 division of objects and backgrounds is highly effective.\nThe low performance of certain Source Only YOLO models underscores the challenging nature of the Asteroid dataset. For example, YOLO 5-N achieves only 5.4%, while YOLO 8-M and 8-N achieve just 25%. This highlights the critical importance of VSA techniques in addressing the highly similar and texture-repetitive feature space in UDA. Compared to Source Only training, PC Only results in a substantial accuracy improvement of about 48% for YOLO 5-N in the adversarial setting, with PTAP emerging as the next best-performing approach. This validates the effectiveness of pixel pooling for low-parameter models, where per-pixel object distinction is easier. Furthermore, significant improvements are observed in instances where ViSGA underperforms, such as with YOLO 8-S and Instance, Contrastive, where ViSGA achieves 22.1% accuracy and PC SFF achieves 53%, indicating an improvement of nearly 31%."}, {"title": "5. Moon", "content": "Results for the low-to-high resolution real-world lunar data adaptation are shown in Table IV. Firstly, low accuracy of the Target Only (oracle) models is observed, indicating erroneous or malformed ground truth. This can be attributed to the Robbins lunar crater database being incomplete, as it does not provide ground truth for every crater on the lunar surface. This discrepancy is evident in Figure 5, where predictions from the baseline YOLO 8-M appear more complete than the provided ground truth. Consequently, the mAP calculation underestimates the true detection capability of the models.\nDespite this limitation, a slight increase in performance is observed across all models when comparing Source Only to Target Only training, suggesting that the difference in resolution between source and target data constitutes a domain gap and justifies the necessity for UDA. Among these, the Instance, Contrastive category achieves the highest performance in six out of eight models. This result contrasts the Sim Asteroid experiment which is similarly single-class, where contrastive learning performed poorly. This demonstrates a general measure of the task difficulty, as we can assume it is easier to distinguish resolution differences within an embedding space as opposed to adversarial discriminators. Within the Instance, Contrastive methods, the original ViSGA implementation achieves the highest performance in only one instance, with the remaining improvements driven by PC Only (YOLO 6-S, 8-N) and PC SFF (YOLO 5-S, 5-M, 6-N). An additional noteworthy observation is that Source Only training retains the best performance on YOLO 5-N, a potential limitation in representation and learning capacity that prevents domain adaptation from discovering an effective alignment scheme."}, {"title": "C. Qualitative Results", "content": "In this section, we evaluate the qualitative performance on real-world data. Our real-world dataset for Mars is Mars HiRISE Landmarks [55], which showcases cropped instances of terrain features on the Martian surface. Our Asteroid data is prior mission imaging from OSIRIS-REx of the Asteroid 101955 Bennu [56]. For Moon, we utilize the same 100 m/px Moon dataset as in the quantitative experiments as this data is already real-world. We showcase YOLO 5-M Instance, Adversarial methods including ViSGA, PC Only, and PC SFF for Mars, the same methods although Instance, Contrastive and YOLO 8-M on Moon, and YOLO 6-S Feature, Adverarial methods including YOCOv1, PC K-Means, and PTAP on Asteroid. We train each setup with the same source datasets used in the quantitative experiments. All qualitative results have inference detection parameters set at \u2265 0.25 confidence and an IoU threshold of 0.7.\nFigure 6 showcases the qualitative results on Mars, with red representing the Crater class, blue representing the Mountain class, and green representing the Dune class. Training with Source Only data leads to numerous incorrect detections, such as false crater detections in the top row, mountain detections in blackened areas in the middle row, and misidentified sand and mountain features in the bottom row. ViSGA provides only marginal improvements, still yielding many erroneous detections. PC Only demonstrates enhanced performance over ViSGA, with improvements particularly evident in the first and second images (top and middle rows). PC SFF achieves the best qualitative results on this dataset, significantly reducing erroneous detections present in both the Source Only and ViSGA models. This configuration yields accurate new detections while simultaneously minimizing erroneous ones, as demonstrated in all three images.\nFigure 7 demonstrates qualitative results on the OSIRIS-REx imagery where red detections represent the single class Boulder. Similar to the results observed on the Mars dataset, Source Only training produces numerous erroneous and incorrect detections. YOCOv1 demonstrates improved detections in this challenging scene, as shown most decisively in the third example (bottom row). The PC K-Means method achieves relatively comparable performance, although issues such as overlapping and multiple detections of the same object are apparent in the first example where absent from YOCOv1. Additionally, in the third example, many subjectively correct detections are removed, highlighting the limitations of two-class clustering. The learnable pooling of PTAP strikes a balance between these extremes. Erroneous detections are significantly reduced while retaining high-quality detections, yielding more reliable and consistent results.\nFigure 8 portrays qualitative results for the Moon. In this case, we observe significantly stronger detection capabilities across all models, including Source Only. This improvement can be attributed to the less challenging gap in data distribution. The results mirror those from previous experiments, where Source Only exhibits some erroneous and overlapping detections. The ViSGA method provides a slight improvement, followed by further enhancements with the PC Only and PC SFF. PC SFF achieves the best subjective performance, demonstrating a marked reduction in erroneous detections while retaining high-quality, accurate detections. This method effectively eliminates many of the errors observed in both Source Only and ViSGA outputs. Overall, PC SFF shows very convincing results, detecting a roughly complete set of crater detections."}, {"title": "V. Conclusion", "content": "Novel techniques to Visual Similarity-based Alignment (VSA) for Unsupervised Domain Adaptation (UDA) in the context of one-stage, in-situ, and real-time terrain detection were proposed in this work. We demonstrate that the prior VSA implementation of intra-feature clustering found in YOCOv1 is largely insufficient for effective UDA performance with challenging space environments, and instrument a range of techniques that bolster the ability to learn domain-invariant properties of many classes of terrain. We performed an in-depth account of two techniques for general VSA, namely instance-based clustering and intra-feature clustering, and correlate the most prominent performance to the type of space environment, where we rigorously evaluate over Moon, Mars, and Asteroid scenes. Overall, our inception of perceptual consistency regularization and a robust feature selection mechanism ensures powerful domain transfer. We showcase quantitative performance on six datasets consisting of simulated and real-world data, observing VSA accuracy improvements upwards of 31% compared to YOCOv1 and terrestrial state-of-the-art. Furthermore, we qualitatively examine detections on real-world mission imagery and see a reduction in erroneous terrain detections and improved accuracy."}]}