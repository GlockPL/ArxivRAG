{"title": "Exploring the Knowledge Mismatch Hypothesis: Hallucination Propensity in Small Models Fine-tuned on Data from Larger Models", "authors": ["Phil Wee", "Riyadh Baghdadi"], "abstract": "Recently, there has been an explosion of large language models created through fine-tuning with data from larger models. These small models able to produce outputs that appear qualitatively similar to significantly larger models. However, one of the key limitations that have been observed with these models is their propensity to hallucinate significantly more often than larger models. In particular, they have been observed to generate coherent outputs that involve factually incorrect information and spread misinformation, toxicity, and stereotypes. There are many potential causes of hallucination, of which, one hypothesis is that fine-tuning a model on data produced by a larger model leads to a knowledge mismatch which contributes to hallucination. In particular, it is hypothesized that there is a mismatch between the knowledge that is fed to the model to fine-tune it and the knowledge that is already present in the graph. Fine-tuning the model on data that has such mismatch could contribute to an increased propensity to hallucinate. We show that on an unseen test set, a smaller model fine-tuned on data generated from a larger model produced more wrong answers when compared to models fine-tuned on data created by the small model, which confirms the hypothesis.", "sections": [{"title": "I. INTRODUCTION", "content": "A well-known limitation of large language models is their ability to hallucinate or generate factually incorrect statements. While large language models are often able to appear fluent, the responses generated by these systems have been observed to often produce statements that are misleading, factually incorrect and harmful. In the context of dialogue-based systems, it has been observed that models often produce responses that are not supported by the evidence available to the system. In the context of generative question and answering systems, models have been observed to provide factually incorrect responses. This is a key issue in the field of Large Language Models because of the potential harm that hallucination can pose to users. For instance, in medical applications, a medical summary generated from patient information could pose a risk to the patient if the generated outputs involve hallucination. In this case, a factually false recommendation generated by the model can lead to a life-threatening incident for the patient.\nIn recent times, this is even more concerning because of the rise of strong large language models that are often able to produce coherent and semantically convincing responses while at the same time, involving hallucination in their outputs. It has become increasingly difficult to tell the difference between the outputs of a model and text written by humans, especially in more general and generic topics and in prompts with short responses. This furthers the risk of hallucination in the outputs of models - the high level of coherence in the outputs of many recent models can easily fool many humans and even many experts.\nA key development in the field of open source models is the explosion of large language models created through fine-tuning with data from larger models. Most of these fine-tuned models are not only relatively small and easy to reproduce but are also relatively performant and are able to produce outputs that appear qualitatively similar to significantly larger models. However, one key limitation that many of these models share is their propensity to hallucinate significantly more often than larger models. While there has been a lot of previous work in addressing hallucination through further tuning via techniques such as Reinforcement Learning through Human Feedback (RLHF) and Reinforcement Learning through AI Feedback (RLAI), there has been relatively little work in analyzing the role of underlying training techniques such as fine-tuning.\nHallucination has a lot of possible reasons, both with respect to data and the model. Our goal in this paper is to study one of the possible reasons: fine-tuning data. In particular, we want to verify the hypothesis that fine-tuning a model on data produced by a larger model leads to a knowledge mismatch, which may contribute to hallucination. Our analysis finds that on an unseen test set, a smaller model fine-tuned on data generated from a larger model produced more wrong answers when compared to models fine-tuned on data created by the small model, which confirms the hypothesis. Our findings question the overall effectiveness of current fine-tuning practices, given"}, {"title": "II. ORIGINS OF HALLUCINATION", "content": "Hallucination is observed to have many potential origins and is observed to be rooted in both data and models. With respect to data, factually inaccurate data is known to be a key driver of hallucination - if the answer that the model was trained on was wrong, it is likely going to produce a wrong answer. However, even if the training data is factually accurate, hallucination can also still occur as there is simply no way to fully cover every single possible question in the training data. As a result, for questions that it has not seen, models are forced to generalize responses without being able to verify where it is accurate, relevant, or appropriate, which also contributes to hallucination. This issue also applies to other forms of problematic data, such as stereotypical data and biased data - due to the scale of the data used in the training of foundational models, it is impossible to fully or even significantly filter through and clean the training data. As a result, these issues rooted in problematic training data is likely to be present in many foundational models, which most fine-tuned model are based on and inherit from.\nAnother key reason source of hallucination is the prescence of duplicates in the training data, for which it is common for many training corpora to contain near-duplicate examples and long repetitive substrings. Not only does this lead to wasted compute resources in training, but it is observed to the model memorizing the common snippets. This can contribute to hallucination as the increased propensity to output memorized snippets may lead to the model generating them even in situations where it is not appropriate or factual to do so.\nFurthermore, Ji et al. note that large language models exhibit Innate Divergence. This is because by nature, some natural language generation (NLG) tasks do not always have factual knowledge alignment between the source input text and the target reference, especially in situations that value diversity in the generated output. For instance, it may be acceptable for open-domain dialogue systems to respond in a subjective style, as this can make generated dialogues more engaging and diverse. However, it has been observed that such dataset characteristics can lead to hallucinations. Such divergence can also lead to issues in the other direction, that is, the training data may contain pieces of information that are relatively informal and subjective, which a model may use in data generation even when prompted with a question that requires a factual answer.\nWith respect to models, Ji et al. point out that the training and inference of models are known to contribute to hallucination through a variety of factors, including Imperfect Representation Learning, Erroneous Decoding, Exposure Bias and Parametric Knowledge Bias.\nImperfect Representation Learning occurs when an encoder is unable to map inputs to the appropriate internal embeddings, which could influence the degree of hallucination. This is because the encoder has the key role of encoding input text into meaningful representations, and when encoders learn wrong correlations between different parts of the training data, it could produce an erroneous generation that diverges from the input, which contributes to hallucination.\nErroneous Decoding occurs when decoders focus on the wrong part of the encoded input source, leading to erroneous generation. It occurs also when decoders are designed in a way that improves the generation diversity, such as with changing top-p sampling, which is observed to be positively correlated with increased hallucination.\nExposure Bias occurs because of the discrepancy in de-coding between training and inference time, which can also contribute to hallucination. This is rooted in the common practice of training decoders with teacher-forced MLE training, where the decoder is encouraged to predict the next token conditioned on the ground-truth prefix sequences. However, during the inference generation, the model generates the next token conditioned on the historical sequences previously gen-erated by itself, which can contribute to hallucination through erroneous generation.\nParametric Knowledge Bias occurs because re-training of models on a large amount of data is known to result in the model memorizing knowledge in its parameters. While this is found to improve the performance in downstream tasks, it is also observed to contribute to hallucination, as models may prioritize parametric knowledge over the provided input. This also applies in generative question and answering, because if the phrasing of a question is too different from that of how knowledge was stored in the model, the intended answer may not be generated. Moreover, if the phrasing of a question is closer to that of another question that a model is more confident about, it may be the case that the model provides the answer for the other question and not the intended one, which can cause increased incorrect responses."}, {"title": "III. KNOWLEDGE MISMATCH HYPOTHESIS", "content": "The hypothesis examined in this paper is that there could be a mismatch between the knowledge that is fed to the model to fine-tune it and the knowledge that is already present in the graph, which could contribute to hallucination.\nAt a small scale, through fine-tuning, the model learns a simple function that extracts knowledge from the model and outputs token predictions. For instance, fine-tuning a model with multiple prompts similar to the following prompt: \"Q: Who is the leader of the United States? A: Joe Biden\", which has already been learned by the model during training, would eventually lead the model to learn a function that predicts the right answer.\nHowever, if we attempt to fine-tune the model on correct answers that have not been learned by the model, for instance, if the person creating fine-tuning instruction data knows about where Joe Biden was formerly a senator but the model does not, a kind of mismatch occurs. In this case, instead of training the model to output correct answers, this mismatch trains the model to guess answers for questions similar to the"}, {"title": "IV. TESTING THE HYPOTHESIS", "content": "To test the hypothesis, we take two language models of different sizes, a small model and a large model, we fine-tune the small model with data generated using the large model (we will explain later how such data is generated). Since the large model has more knowledge compared to the small model, the data generated by the larger model is more likely to mismatch with the knowledge of the small model. We then measure the effect of this mismatch on the amount of hallucination in the small model. If the hypothesis is correct, then the small model will hallucinate more, since it was trained with data that has a mismatch with its knowledge.\nMore precisely, we take two models of different sizes, a small model and a large model, and fine-tune each model to abstain when it doesn't know the answer (i.e., answer \"I don't know\"). This is done by taking a dataset of questions and generating responses to its questions via prompted completions to create a fine-tuning dataset for the small and large models.\nFor example, for the question \"Where was horse racing's Breeders' Cup held in 1988?\u201d:\n1) The model is first made to complete the prompt \"Question: Where was horse racing's Breeders' Cup held in 1988? Answer:\"\n2) Then, the generated response is compared with the list of correct answers provided by the dataset (a list of correct answers might be Churchill Downs, Louisville, Kentucky).\n3) If the generated response contains at least one correct answer, for instance, Churchill Downs, then it is marked as correct and the correct answer saved for the fine-tune response of the question. If it does not contain any correct answer, for instance, California, then it is marked as wrong and \"I don't know\" is saved as the fine-tune response of the question.\n4) The fine-tuning dataset is then created by using the questions and fine-tune response, which contains either correct answers or \"I don't know\". In doing this, the model is trained to abstain when it does not know the answer.\nWe then take the fine-tuning dataset created by the small model and the large model and use it to fine-tune the small model. Our hypothesis would suggest that the small model fine-tuned on the fine-tuning dataset generated by the large model will produce wrong answers more often."}, {"title": "A. Experimental Setup", "content": "a) Models: We use LLaMA models by to test the hypothesis. We use the LLaMA 7B model as our small model and the LLaMA 13B model as our large model.\nb) Data: We use TriviaQA: A Large Scale Dataset for Reading Comprehension and Question Answering as our source of question and answer data. We split the data 80/20 between the training set and the testing set.\nc) Fine-tuning: We use parameter-efficient fine-tuning to tune the model using fine-tuning code from Project Baize.\nd) Evaluation: We evaluate the model by making the fine-tuned small models generate responses to the unseen test split of the data."}, {"title": "B. Results", "content": "After evaluating the fine-tuned models on the unseen test set, we found that the small model fine-tuned on the data generated by the large model produces more wrong answers than the small model fine-tuned on data generated by the small model an average of 125% and median of 107% increase in wrong answers.\nThe difference between the two models can be seen in Table I. For example in the 5th row of the table, the small model (7B) fine-tuned on data generated by the small model, generates 426 wrong answers. The same model, when fine-tuned on data generated by the large model (13B), generates 1134 wrong answers (as shown in the 10th row). This confirms the hypothesis."}, {"title": "V. THE EFFECTS OF FINE-TUNING ON DATA OBTAINED FROM A LARGER MODEL", "content": "Our results show that a small model fine-tuned on data generated by a larger model provides significantly more wrong answers compared to small models fine-tuned on data generated by the small model. This suggests that fine-tuning on data generated by large models may have the unintended consequence of increasing the hallucination through an increase in factually incorrect responses in questioning and answering settings.\nWe also observe that there are fewer \"I don't know\" responses in the responses of the model fine-tuned on data generated by a larger model compared to that of the model fine-tuned on data generated by the smaller model. We believe that this is most likely due to the fact that the fine-tuning dataset created by the base 13B model has fewer \"I don't know\" responses compared to the one generated by the base 7B model because the base 13B model performs better than the base 7B model and thus had fewer wrong answers that were mapped to \"I don't know\".\nIn addition, we find that there are more correct answers in the model fine-tuned with the data generated by that larger model than that of the small model. This is in line with the evaluations of many of the small-scale larger model data fine-tune models released recently. This is likely due to the fact that despite causing some knowledge mismatch, fine-tuning still contributes to improved model performance."}, {"title": "VI. LIMITATIONS", "content": "Knowledge mismatch through model fine-tuning is only one of many potential reasons for large language model hallucination. As such, there may be other factors that may also contribute to model hallucination that our analysis does not discuss.\nGiven the resources available, we only tested the hypothesis on the 7B and 13B variants of the LLaMA model. While we believe that our hypothesis should likely still hold for other model combinations, we were not able to test this because of resource constraints.\nIt is also worth noting that while the TriviaQA dataset is a relatively large and widely used question and answering dataset, is not fully processed and cleaned. In particular, a few answers in the dataset are not correct, and some answers do not list all possible answers and answer formats. As such this may contribute to a few incorrect evaluations where the model was correct but the expected answer was incorrect."}, {"title": "VII. RELATED WORK", "content": "Most model evaluations, including many fine-tuned models trained using data from larger models generally report their hallucination as a key limitation and highlight the importance of using quality data in model tuning to achieve better performance.\nMost top Al research organizations also generally report model hallucination and the techniques that they use to lower and limit hallucination. Currently, there exist many attempts to create, leverage and analyze strategies to reduce hallucination, such as RLHF and RLAIF.\nFor instance, observe that making language models bigger does not inherently make them better at following a user's intent, as large language models can still generate outputs that are untruthful, toxic, or simply not helpful to the user. They note that large language models are not necessarily aligned with their users and propose RLHF, also known as Reinforcement Learning with Human Feedback, which finetunes models with human feedback to address the issue. They observe that in human evaluations outputs from a smaller model (InstructGPT 1.3B) that leverages RLHF are preferred to outputs from a model 100x the size (GPT3). They also find that InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets, which suggests that RLHF can contribute to reducing hallucination. and apply a similar strategy and also find less hallucination and more helpful, safe, and aligned models.\nAnother common technique to lessen hallucination and create more aligned models is RLAIF or Reinforcement Learning with Artificial Intelligence Feedback. implement this strategy through Constitutional AI, for which the only human oversight provided is a list of rules or principles, and the AI model is used to train itself through supervised training and reinforcement learning (RL). In the supervised phase, self-critiques and revisions are generated from the model, which are then revised and used to fine-tune the original model. In the RL phase, samples are generated from the finetuned model, which is fed to a model to evaluate which of the two samples is better, and then used to train a preference model from this dataset of AI preferences. They then engage in RLAIF by training with RL using the preference model as the reward signal. They find that doing this reduces harmfulness similar to human feedback.\nThere has also been some work in studying the effects of novel training and training data generation through techniques such as Contrastive Learning and Evol-Instruct.\n study the use of contrastive learning as a potential way to address hallucination. In particular, they propose Mixed Contrastive Learning (MixCL) to alleviate the hallucinations of LM-based dialogue agents. This technique explicitly samples the most confusing knowledge to the model and reduces its generation probability by contrasting it with the ground truth. They find that this technique significantly improves dialogue performance and lessens the hallucination of LMs, based on ablation studies and human evaluation.\n focus on creating better data for model fine-tuning. In particular, they propose Evol-Instruct as a technique to generate high-quality data for fine-tuning that can lead to more helpful models. They note that manually creating instruction data is not only time-consuming and labor-intensive but also difficult, as humans may struggle to produce high-complexity instructions. In their study, they propose a method to create instruction data with varying levels of complexity using LLMs instead of humans. Using an initial set of instructions, they use Evol-Instruct to rewrite them step by step into more complex instructions, then mix the generated instruction data to fine-tune a foundational model. They find that the resulting fine-tuned model is superior to top fine-tuned models of a similar size, including the Vicuna, which is fine-tuned on ShareGPT data. They also find that the fine-tuned model is even preferred over ChatGPT (a significantly larger proprietary model) in high-complexity tasks. This technique of Evol-Instruct can be said to be in line with the proposed hypothesis as it leverages the outputs generated by the same model for fine-tuning, which means that there should be significantly less knowledge mismatch in the fine-tuning, which may have contributed it its superior performance."}, {"title": "VIII. CONCLUSIONS", "content": "We have explored the hypothesis that a mismatch between the knowledge that is fed to the model to fine-tune it and existing knowledge in a graph can contribute to the propensity of a model to hallucinate. Through experiments on fine-tuning models on data generated from other models, we find that models fine-tuned on data generated by larger models produce more wrong answers on questions that it has not seen before. Studying potential causes of hallucination will hopefully inspire further work on better techniques to create more helpful, truthful, and harmless models."}]}