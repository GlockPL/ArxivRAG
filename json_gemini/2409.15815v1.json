{"title": "AsthmaBot: Multi-modal, Multi-Lingual Retrieval Augmented Generation For Asthma Patient Support", "authors": ["Adil Bahaj", "Mounir Ghogho"], "abstract": "Asthma rates have risen globally, driven by environmental and lifestyle factors. Access to immediate medical care is limited, particularly in developing countries, necessitating automated support systems. Large Language Models like ChatGPT (Chat Generative Pre-trained Transformer) and Gemini have advanced natural language processing in general and question answering in particular, however, they are prone to producing factually incorrect responses (i.e. hallucinations). Retrieval-augmented generation systems, integrating curated documents, can improve large language models' performance and reduce the incidence of hallucination. We introduce AsthmaBot, a multi-lingual, multi-modal retrieval-augmented generation system for asthma support. Evaluation of an asthma-related frequently asked questions dataset shows AsthmaBot's efficacy. AsthmaBot has an added interactive and intuitive interface that integrates different data modalities (text, images, videos) to make it accessible to the larger public. AsthmaBot is available online via asthmabot.datanets.org.", "sections": [{"title": "1. Introduction", "content": "In the past few decades, asthma rates have been on the rise globally, attributed not just to genetic factors but primarily to the influence of numerous environmental and lifestyle risk factors Nunes, Pereira and Morais-Almeida (2017). Asthma claims thousands of lives every year mainly due to lack of access to immediate and adequate medical care Mwangi, Ndashimye, Karikumutima and Ray (2020). However, a significant number of asthma-related fatalities are preventable by home remedies, exercise, treatments, and action plans which can help reduce the symptoms of asthma patients either by avoiding triggers or employing relieving remedies Mwangi et al. (2020); Murphy, McSharry, Hynes, Molloy et al. (2021). This shows the importance of easy and fast access to information in reducing the ramifications of asthma attacks. However, having an around-the-clock service by medical providers can be prohibitive in many ways, especially in developing countries, which motivates the need for an interactive automated medical support system for asthma patients.\nLarge language models (LLMs) have garnered considerable attention in recent years due to their generative abilities Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell et al. (2020); Chen, Tworek, Jun, Yuan, Pinto, Kaplan, Edwards, Burda, Joseph, Brockman et al. (2021); Wei, Xie and Ma (2021). Models such as ChatGPT Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin, Zhang, Agarwal, Slama, Ray et al. (2022), Gemini Team, Anil, Borgeaud, Wu, Alayrac, Yu, Soricut, Schalkwyk, Dai, Hauth et al. (2023), and Llama2 Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale et al. (2023) have paved the way for a new era of artificial intelligence (AI) where humans can interact with models in a mutually inclusive way. These models revolutionized multiple aspects of natural language processing (NLP) tasks (information retrieval, question answering, summarization, sentiment analysis etc) Ouyang et al. (2022); Team et al. (2023); Touvron et al. (2023). Recently, a plethora of works showed the proficiency of LLMs in question answering. Although their performance is encouraging they were found to generate plausibly sounding but factually incorrect responses, commonly known as hallucinations. In addition, LLMs can only remember the data that they trained on hindering the recency of their knowledge Huang, Yu, Ma, Zhong, Feng, Wang, Chen, Peng, Feng, Qin et al. (2023). These limitations can negatively affect critical domains such as healthcare.\nTo address these challenges new works Lewis, Perez, Piktus, Petroni, Karpukhin, Goyal, K\u00fcttler, Lewis, Yih, Rockt\u00e4schel et al. (2020); Gao, Xiong, Gao, Jia, Pan, Bi, Dai, Sun and Wang (2023) supply LLMs with pertinent"}, {"title": "2. Previous Literature", "content": "Traditionally, language models (LM) were developed to model the sequential nature of text Merity, Keskar and Socher (2018). These language models evolved from simple LSTM-based models Merity et al. (2018); Graves and Graves (2012) to transformer-based models such as BERT Kenton and Toutanova (2019). In general, these models were pre-trained in a general-purpose unsupervised task (e.g. next token prediction, next sentence prediction) and then fine-tuned to a specific task using supervised training. More recently, the surge in computational resources and increased efficiency of training techniques have allowed for a surmountable increase in transformer models' parameters, which coupled with a significant amount of available data on the web have allowed for the creation of LLMs Ouyang et al. (2022); Team et al. (2023). LLMs came with a new set of abilities that the traditional models never exhibited, like the emergence property, which allows LLMs to do tasks that they were not trained on. However, LLMs suffer from hallucinations. Hallucinations describe model outputs that are linguistically coherent but nonfactual Huang et al. (2023). To reduce the effects of these hallucinations retrieval augmented generation was proposed Lewis et al. (2020); Gao et al. (2023). In this setting an LLM is provided with a query relevant context that is extracted from factual sources to augment the limited knowledge of the LLM and increase the likelihood of having a factual answer. RAG systems have the following components: document collections (corpora), retrieval algorithms (retrievers), and backbone LLMs Gao et al. (2023). The document collection is a collection of text documents from various sources that can be general or domain-specific. Retrieval algorithms are tasked with retrieving relevant documents given a query. The backbone LLM is used to generate an answer to the query given retrieved documents augmented prompt. Additionally, the previous RAG system is based on text-only documents, recent works explored multi-modal RAG systems, which can retrieve not just text but also images Chen, Hu, Chen, Verga and Cohen (2022); Zhao, Chen, Wang, Jiao, Qin, Ding, Guo, Li, Li, Joty et al. (2023)."}, {"title": "2.1. Large Language Models (LLMs)", "content": "Traditionally, language models (LM) were developed to model the sequential nature of text Merity, Keskar and Socher (2018). These language models evolved from simple LSTM-based models Merity et al. (2018); Graves and Graves (2012) to transformer-based models such as BERT Kenton and Toutanova (2019). In general, these models were pre-trained in a general-purpose unsupervised task (e.g. next token prediction, next sentence prediction) and then fine-tuned to a specific task using supervised training. More recently, the surge in computational resources and increased efficiency of training techniques have allowed for a surmountable increase in transformer models' parameters, which coupled with a significant amount of available data on the web have allowed for the creation of LLMs Ouyang et al. (2022); Team et al. (2023). LLMs came with a new set of abilities that the traditional models never exhibited, like the emergence property, which allows LLMs to do tasks that they were not trained on. However, LLMs suffer from hallucinations. Hallucinations describe model outputs that are linguistically coherent but nonfactual Huang et al. (2023). To reduce the effects of these hallucinations retrieval augmented generation was proposed Lewis et al. (2020); Gao et al. (2023). In this setting an LLM is provided with a query relevant context that is extracted from factual sources to augment the limited knowledge of the LLM and increase the likelihood of having a factual answer. RAG systems have the following components: document collections (corpora), retrieval algorithms (retrievers), and backbone LLMs Gao et al. (2023). The document collection is a collection of text documents from various sources that can be general or domain-specific. Retrieval algorithms are tasked with retrieving relevant documents given a query. The backbone LLM is used to generate an answer to the query given retrieved documents augmented prompt. Additionally, the previous RAG system is based on text-only documents, recent works explored multi-modal RAG systems, which can retrieve not just text but also images Chen, Hu, Chen, Verga and Cohen (2022); Zhao, Chen, Wang, Jiao, Qin, Ding, Guo, Li, Li, Joty et al. (2023)."}, {"title": "2.2. Biomedical LLMs and RAG", "content": "NLP researchers found fertile ground for innovation in Biomedical NLP due to its knowledge-intensive nature. Medicine has benefited from the development of NLP techniques since the inception of the field Lee, Yoon, Kim, Kim, Kim, So and Kang (2020); Gu, Tinn, Cheng, Lucas, Usuyama, Liu, Naumann, Gao and Poon (2021). Recently, LLMs have demonstrated a considerable improvement in existing approaches to medical benchmarks in various tasks (named entity recognition, question answering, medical text summarization, medical diagnosis etc) Chen, Cano, Romanou, Bonnet, Matoba, Salvi, Pagliardini, Fan, K\u00f6pf, Mohtashami et al. (2023c); Chen, Sun, Liu, Jiang, Ran, Jin, Xiao, Lin, Chen and Niu (2023b). Although multiple LLMs trained on medical text have been proposed, they still suffer from the same hallucinations as the general domain LLMs. Consequently, RAG-based systems for medical question answering have been proposed Xiong, Jin, Lu and Zhang (2024); Miao, Thongprayoon, Suppadungsuk, Valencia and Cheungpasitporn (2024); Zakka, Shad, Chaurasia, Dalal, Kim, Moor, Fong, Phillips, Alexander, Ashley et al. (2024); Wang, Yang, Yao and Yu (2024). For example, Wang et al. (2024) showed that a RAG system with a 13B parameter backbone LLM can significantly outperform a 70B model (i.e. MediTron Chen et al. (2023c)). However, these RAG systems are not multi-modal or multi-lingual."}, {"title": "3. Methodology", "content": "AsthmaBot was conceptualised and designed to allow asthma patients to access relevant information interactively and adaptively. AsthmaBot backend is a multi-modal, multi-lingual retrieval augmented generation LLM, while the frontend is in the form of a chatbot (e.g. ChatGPT, Gemini etc) where questions and answers are saved so that the user can explore them."}, {"title": "3.1. Data Collections", "content": "To enrich LLM's responses with factual up-to-date information specific documents and media were selected. These elements satisfy certain criteria rather than just doing a web search every time a query is given by the user, which can leave room for pseudo-scientific sources. Our main data formats are PDF documents obtained by searching Google, images from Google images and videos from YouTube. Search for the elements is done via queries in the different search platforms. Queries can be general like \"asthma\" or more specific (e.g. FAQs). FAQs are obtained from the gina website\u00b9. These FAQs were later translated into different languages to obtain data from languages other than English. The PDF documents were downloaded manually while images from Google Images and YouTube videos were scraped automatically. After scraping the different images and videos we extracted the text from the source of the images and the video transcripts of each image and video respectively. We should note that we explored using image captioning to obtain image description but it was limited and didn't give importance to the context in which the image was used, consequently, we opted for the text in the webpage of the image source as an alternative."}, {"title": "3.2. Vector DB Building and Retrievers", "content": "A vector database indexes text, images, videos and other data modalities based on a textual description of what they contain. This textual description is transformed into a dense vector using a language model, which facilitates the process of semantic search. We built multiple independent vector databases where each containing different modalities in different languages. Although one database that contains all the aforementioned elements can be used, it causes certain limitations. First, having multiple stores makes parallelism possible, increasing the speed of the inference process. Second, the search process can be parameterized providing more structure and control to the LLM and reducing hallucinations."}, {"title": "3.3. Prompting and Backbone LLM", "content": "Prompting is a fundamental element of LLM inference as it has been shown that the quality and the design of the prompt affect the quality of the output of the LLM Chen, Zhang, Langren\u00e9 and Zhu (2023a). 1 shows the prompt that we used in AsthmaBot. The prompt contains three parameters the query, context and \"history\". The query refers to the user input, the context refers to the different text passages obtained by querying the different text vector DBs. The \"history\" refers to the chat history between the user and the LLM, composed of question-answer pairs. We used the Google Gemini LLM to infer the results of different queries."}, {"title": "3.4. Translation", "content": "Instead of opting for queries in the native language, we translate the queries to English, search the vector databases, prompt the model in English, and translate the model response to the native query language. This was done because we noticed that LLMs (Gemini and ChatGPT) have biases towards the English language and can produce significantly richer responses in English than in Arabic or French. We further illustrate this in the results section, where we query the LLM using Arabic and get a significantly worse result than if we query it using the described process. We opted for the Google Translate API since it gave better results and a faster response time compared to other translators."}, {"title": "3.5. Visual Interface", "content": "To ensure the accessibility of AsthmaBot to a wider user base we created an interactive interface similar to that of ChatGPT and Gemini with the added features of embedded videos, source documents and images. In addition, users can click on the images to be transferred to the source website of the image."}, {"title": "3.6. AsthmaBot's Inference Process", "content": "AsthmaBots' inference process can be summarized as follows:\n\u2022 Query: the process starts with a user inputting a query.\n\u2022 Language detection: a language detection module detects the language of the query to choose the right vector stores.\n\u2022 Query translation: if the detected language is not English then the query is translated to English."}, {"title": "4. Results", "content": "To evaluate the efficacy of AsthmaBot at answering asthma-related questions we extracted a list of frequently asked questions (FAQs) and their corresponding answers from different sources and compared the answers generated by AsthmaBot to the answers originally given to the FAQs. In addition, to evaluate the veracity of information in the videos and images we modify the AsthmaBot context to video transcript summary and image source summary, respectively. In"}, {"title": "4.1. Evaluation Procedure", "content": "To evaluate the efficacy of AsthmaBot at answering asthma-related questions we extracted a list of frequently asked questions (FAQs) and their corresponding answers from different sources and compared the answers generated by AsthmaBot to the answers originally given to the FAQs. In addition, to evaluate the veracity of information in the videos and images we modify the AsthmaBot context to video transcript summary and image source summary, respectively. In"}, {"title": "4.2. Evaluation Data", "content": "To evaluate AsthmaBot we collected a set of asthma related FAQs from various sources. We collected over 400 FAQs from over 30 sources. To evaluate AsthmaBot's multilingual capabilities we translated the FAQs using Google Translate API."}, {"title": "4.3. Evaluation Results", "content": "Table 3 shows the results of querying using AsthmaBot in multiple languages (English, Arabic, French) in multiple data modalities (text, images, videos). The table shows that RAG significantly improves the performance in question answering relative to the no RAG baseline. This is true for all modalities and languages. On the other hand, we notice that there are variations in performance between languages. This can be attributed to the richness of the documents that were used in RAG.\nTable 4 shows the results of experimenting with using English-only input to the LLM and using prompts in the native language of the query. The results show that the English-only prompts perform significantly better than the native language prompt. This can be attributed to the significant amount of English prompts that the LLM was trained on. This language bias limits the richness of LLM output in languages other than English."}, {"title": "5. Impact and Future Work", "content": "AsthmaBot integrates multiple features that are not included in many publicly available LLMs. We compared AsthmaBot to multiple publically available LLMs: ChatGPT2, Gemini 3, Perplexity AI4, YouChat. Table 5 compares the retrieval capabilities of different LLMs and AsthmaBot. Table 6 compares different LLMs generative capabilities."}, {"title": "6. Conclusion", "content": "In this paper, we have presented AsthmaBot, a multi-lingual, multi-modal Retrieval-Augmented Generation (RAG) system designed to provide automated support for asthma patients. Through the integration of curated documents, videos, and images, AsthmaBot offers personalized responses to asthma-related queries, empowering patients with valuable information. Our evaluation, based on diverse asthma-related FAQs, demonstrates AsthmaBot's enhanced performance compared to a non-RAG baseline, highlighting its effectiveness in providing relevant information. Moreover, AsthmaBot tackles the language biases inherent in Large Language Models (LLMs) by offering multi-lingual support. The visual interface of AsthmaBot further enhances user experience, presenting information in a comprehensive and accessible manner. Although this work focuses on applying the system to asthma it can be applied to any domain containing text, video and image information (e.g. law, entertainment, other medical conditions and concepts etc)."}]}