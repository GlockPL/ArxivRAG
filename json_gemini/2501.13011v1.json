{"title": "MONA: Myopic Optimization with Non-myopic Approval Can Mitigate Multi-step Reward Hacking", "authors": ["Sebastian Farquhar", "Vikrant Varma", "David Lindner", "David Elson", "Caleb Biddulph", "Ian Goodfellow", "Rohin Shah"], "abstract": "Future advanced AI systems may learn sophisticated strategies through reinforcement learning (RL) that humans cannot understand well enough to safely evaluate. We propose a training method which avoids agents learning undesired multi-step plans that receive high reward (multi-step \u201creward hacks\") even if humans are not able to detect that the behaviour is undesired. The method, Myopic Optimization with Non-myopic Approval (MONA), works by combining short-sighted optimization with far-sighted reward. We demonstrate that MONA can prevent multi-step reward hacking that ordinary RL causes, even without being able to detect the reward hacking and without any extra information that ordinary RL does not get access to. We study MONA empirically in three settings which model different misalignment failure modes including 2-step environments with LLMs representing delegated oversight and encoded reasoning and longer-horizon gridworld environments representing sensor tampering.", "sections": [{"title": "1. Introduction", "content": "When training an agent with reinforcement learning (RL) and imperfectly-specified rewards, the agent may engage in \"reward hacking\", where its behaviour is undesired but achieves a high reward (Amodei et al., 2016; Clark and Amodei, 2016). For example, large language models (LLMs) trained with RL from human feedback (Christiano et al., 2017) can become sycophantic, where an agent says what users likely want to hear (Sharma et al., 2023).\nAs AI systems become more powerful and are trained with longer horizons (Shani et al., 2024), reward hacking will likely become more sophisticated. Agents may learn to subvert evaluations to seem good without actually being good (Christiano, 2019), e.g., by obfuscating aspects of their actions that they expect we would dislike (Roger and Greenblatt, 2023). Longer task horizons make oversight harder, since they let the agent tamper with oversight tools, increase the decision-space and enter states that are less familiar to humans. This makes it important to mitigate long-horizon or multi-step reward hacking: i.e., reward hacking that requires more than one step.\nCurrently, most reward hacking is addressed via \"patching\": noticing bad behavior and changing the reward to stop incentivizing it. This only works if the overseer-whatever the source of the reward is-can detect the bad behavior. But agents with superhuman capabilities in narrow domains, like AlphaGo, already show that RL agents can learn strategies that are opaque to even the world's top experts (Silver et al., 2016). We could imagine that in the space of possible policies, there is a \u201cspotlight\u201d on strategies that human experts can understand-AlphaGo shows that RL agents will not stay in the spotlight. Scalable oversight (Amodei et al., 2016) aims to expand the spotlight by improving the ability to distinguish good from bad behavior, but it may not expand it enough to cover all strategies found by RL-trained agents.\nWe show how to address long-horizon reward hacking by ensuring the agent only learns strategies that the overseer understands but without assuming the overseer can detect bad behavior. Instead of expanding the spotlight to cover the agent's strategies, we instead restrict the agent to strategies within the spotlight. The intention is to reduce the more challenging problem of detecting and disincentivizing multi-step reward hacking to the simpler problem of single-step reward hacking. Myopic Optimization with Non-myopic Approval (MONA) does this by combining:\nMyopic optimization: Ordinary RL agents learn long-horizon strategies due to the expectation of future rewards. Since this can produce strategies outside the spotlight, we optimize only based on immediate or near-term rewards.\nNon-myopic approval: To guide the agent's forward behavior, the overseer evaluates future usefulness of the agent's actions and provides it as a reward to the agent."}, {"title": "2. Reward Specification and Hacking", "content": "Reward hacking is when an agent achieves a high reward in a way the system designer would not want if they understood (Krakovna et al., 2020).\nReward hacking is caused by a mismatch between the intentions of a system designer and the actual specification of the reward function. \"Winning\" is clearly defined for simple games like Chess. For more complex games like StarCraft, winning is still fairly well defined, though one might implicitly assume constraints such as limits on actions per minute (Korzekwa, 2019). In contrast, open-ended environments like dialogue or computer-use are so complex that it is hard to specify a reward function that matches one's intention (Dewey, 2014). Given a mismatch between what you want and what you reward, the policies with highest return will tend not to do quite what you wanted, in particular as RL learning becomes better (exploration, optimization, expressive power etc.).\nReward hacking has been observed in many RL systems (Krakovna et al., 2020) including LLM agents (Denison et al., 2024). Even when reward comes from online human evaluations, the most perceptive humans still have a limited ability to understand how good or bad an action is. For example, sycophancy is an instance of human ratings being a misspecified reward function (Sharma et al., 2023)\nA key insight of our paper is that reward hacking can be usefully split into two categories:\nSingle-step reward hacking: a policy takes an undesired action but receives a high reward.\nMulti-step reward hacking: a policy follows an undesired strategy over multiple steps but achieves a high return and at least two of the steps were jointly necessary but individually insufficient for the overall return.\nMulti-step reward hacking can be much harder to detect than single-step. The space of sequential action trajectories is potentially larger."}, {"title": "3. Myopic Optimization with Non-myopic Approval (MONA)", "content": "In this section, we describe how myopic-short-sighted-optimization can avoid multi-step reward hacking in reinforcement learning (RL) agents. But, to produce non-myopic agents, we show how to use rewards representing non-myopic approval without incorporating real-world feedback (which would reintroduce reward hacking). We call the method Myopic Optimization with Non-myopic Approval (MONA)."}, {"title": "3.1. Myopic Optimization", "content": "Ordinarily, an agent trained with RL is trained to maximize the expected sum of (discounted) current and future rewards. That is, the agent's policy should balance high immediate reward with trajectories that lead to high reward in the future. Concretely, RL aims to find a policy such that\n$\\pi(s_i) = argmax_{\\pi} (r(s_i, \\pi(s_i)) + E_{\\pi,\\tau}v(s_{i+1})) = argmax_{\\pi} [r(s_i, \\pi(s_i)) + E_{\\pi,\\tau} \\sum_{j=i+1}^{T} \\gamma^{j-i}r(s_j, \\pi(s_j))]$  (1)\nwhere the value v for any state is the expected sum of future rewards under the policy and transition distributions (possibly discounted by \u03b3).\nThe \"planning\" implicit in such an agent is that the RL training approximately solves the dynamic programming problem of optimising the full-trajectory expected return. An agent optimized myopically does not consider the expected value of the downstream trajectory but only the reward of the next action:\n$\\pi(s_i) = argmax_{\\pi}(r(s_i, \\pi(s_i)))$.   (2)\nThis is sometimes described as a greedy policy and is equivalent to discounting with \u03b3 = 0 or an effective horizon of T = 1. Myopic optimization stops dynamic programming from incorporating the future value of current actions. That is, it creates short-sightedness."}, {"title": "3.2. Non-myopic Approval", "content": "Myopic optimization limits planning caused by optimization. But agents trained myopically can still have policies that work over longer time horizons. This can happen even without deliberate intervention, just like LLMs trained with next-token prediction can produce coherent paragraphs.\nMoreover, if the reward incorporates information about anticipated future rewards, then a myopically optimized agent can learn controllable non-myopic behavior. We can distinguish part of the reward that represents the instantaneous reward $r^l$ from a second component $r^a$ representing the non-myopic approval: an overseer's estimation of the improvement to future prospects that came from taking that action.\nAdding non-myopic approval to the myopic optimization objectives results in the MONA objective:\n$\\pi(s_i) = argmax_{\\pi}(r^{mona} (s_i, \\pi(s_i)) = argmax_{\\pi} (r^l (s_i, \\pi(s_i)) + r^a(s_i, \\pi(s_i)))$.  (3)\nA key insight of this paper is that adjusting the proportion of an agent's foresight that comes from optimization relative to approval lets us mitigate multi-step reward hacking in favour of the easier-to-address single-step reward hacking."}, {"title": "3.3. Implementing Non-myopic Approval", "content": "Mathematically, non-myopic approval is a kind of reward. However, to mitigate multi-step reward hacking, there are constraints on what it can include: it should not depend on direct observations of the consequences of the agent's action, which rules out several ways to automatically generate step level feedback (Chen et al., 2024; Havrilla et al., 2024; Wang et al., 2024).\nThere are many good ways to construct non-myopic approval. It could be defined by a handcoded reward function; be modelled similarly to a reward model (Leike et al., 2018); be based on human preferences, scores, or feedback. Alternatively, variants of constitutional AI (Bai et al., 2022) can use AI approvers to identify steps that reflect epistemic or pragmatic virtues. For any of these, the agent could explain why its action is a good idea before the foresight approval is assigned. Doing this does not affect the underlying motivations for MONA but it can make single-step reward hacking more likely.\nSimilarly, foresight approval models can be high effort: prediction markets, tournaments, and entire research projects. These can be part of offline training even if they take much longer practical in real-time. The main thing they should avoid using is the actual outcome of an agent acting in the world. Last, some-"}, {"title": "3.4. Causal Incentives and MONA", "content": "Myopic optimization works because it makes the agent at any given time indifferent about the results of that same agent at a future time. This means that even if it \"knows\" how to do some kind of undetectable reward hacking it will not \u201cchoose\u201d to because it (at that time) does not \"care\" about the returns from a future action.\nWe can formalize this observation using causal influence diagrams (CIDs; Everitt et al., 2021a). Figure 2 compares the CIDs for MONA and ordinary RL. The key difference is that the MONA agent is not rewarded for future actions, but that the foresight is covered by a single approval node r.\nWe can formalize the claim that MONA removes the incentive to set up reward hacks for the future, by using the notion of instrumental control incentives introduced by Everitt et al. (2021a). Intuitively, an agent has an incentive to control a node X, if the agent could achieve a higher utility by controlling X directly.\nTheorem 3.1. When taking action $a_i$, a MONA agent has a control incentive on $s_i$, r, and $r^a$ but not $s_{i+1}, ..., s_T, r_{i+1}, ..., r_T$ while an ordinary RL agent has a control incentive over all $s_i, ..., s_T, r_i, ..., r_T$.\nIntuitively, this is because the future rewards do not affect the optimization objective. The proof in Appendix B.1 is straightforward given the CID. This incentive analysis shows that even with MONA the agent has incentives to manipulate the source of non-myopic approval, e.g., by making a human rater think the action will lead to desirable outcomes. However, the agent is no longer incentivized to set up multi-step reward hacks targeting high future approvals."}, {"title": "4. Results", "content": "We empirically demonstrate three main claims:\nOrdinary RL can cause multi-step reward hacking while MONA can prevent it without any additional supervision (Section 4.2).\nImproving the approval foresight improves the policy learned by MONA, but even noisy foresight can work well (Section 4.3).\nWhen approval feedback depends on outcomes, multi-step reward hacking can occur despite myopic optimization (Section 4.4)."}, {"title": "4.1. Case Studies for Misalignment", "content": "In very realistic environments, multi-step reward hacking materializes in complex and hard-to-predict ways, which makes studying the phenomena difficult. Therefore, we explore three environments designed to highlight important scenarios for the oversight of superhuman agents. These \u201cmodel organisms of misalignment\" (Hubinger et al., 2023) let us study phenomena in controllable settings. The first two are two-step environments using LLMs while the third lets us explore longer time-horizons than are tractable with LLMs in a toy gridworld setting.\nTest-driven Development. An LLM writes code to solve programming tasks stated in natural language. This model organism represents settings where human overseers can only partially assess task perfor-"}, {"title": "5. Related Work", "content": "Reward hacking is caused by the difficulty of reward specification (Dewey, 2014; Wen et al., 2024) and includes special cases like reward tampering (Everitt et al., 2017), user tampering (Carroll et al., 2022; Kasirzadeh and Evans, 2023) and wireheading (Everitt and Hutter, 2016). These behaviors have been demonstrated in large language models (Denison et al., 2024; Wen et al., 2024; Williams et al., 2024). The incentives to carry out reward hacking have been analysed using causal incentive diagrams (Everitt et al., 2021b; Farquhar et al., 2022) resulting in proposals for mitigating reward hacking using decoupled approval (Uesato-"}, {"title": "6. Limitations, Trade-offs, and Practical Advice for Using MONA", "content": "MONA helps by removing incentives for LLM agents to pursue multi-step reward hacking, without requiring the overseer to detect bad behavior from the agent.\nNevertheless, our work leaves significant unsolved problems. Crucially, MONA does not help when reward hacking only uses a single step. For example, in our test-driven development environment, there is a single-step reward hacking failure mode that does not need multi-step planning: on the second turn, the agent can write code that simply checks the expected test cases and hard-codes the expected test outputs one-by-one. However, single-step reward hacking is often easier to detect and prevent than multi-step. This is a pragmatic argument, and requires system designers to shrink the step size to be the largest unit that they can evaluate accurately.\nIn this section, we discuss practical considerations for using MONA and how to navigate tradeoffs between safety and performance."}, {"title": "6.1. When should we use MONA?", "content": "There may be a performance cost to using MONA. MONA can achieve superhuman performance, more than, for example, imitation learning (cf. Appendix B.2). However, almost by definition, it will achieve less reward than an agent trained with ordinary RL. AlphaGo's \u201cMove 37\u201d would be difficult, if not impossible, to achieve with MONA, because the foresight it involved was surprising to humans. MONA is therefore most valuable when outcome evaluations are imperfect and when we do not trust our ability to evaluate the safety and quality of the steps the agents take along the way."}, {"title": "7. Conclusion", "content": "As AI systems based on LLMs are increasingly trained using multi-step reinforcement learning, new kinds of reward hacking strategies will emerge. In the short term, these can be dealt with through regularization and by patching reward models whenever reward hacking is observed. In the longer term, however, this will stop working. These methods depend too much on being able to detect when reward hacking is happening, which will become much harder as Al systems learn strategies that humans do not understand.\nOur proposed method, MONA, combines myopic optimization with non-myopic approval, to reduce reward hacking even when the overseer cannot detect it. Myopic optimization limits the ability of dynamic programming to discover strategies that humans do not understand, while the non-myopic approval allows the agent to learn to use strategies that humans do understand. The resulting agents, in our experiments, are more capable as measured by ground truth reward and much safer, while being less capable as measured by the optimized reward.\nEven though we do not advise that agent designers use MONA for their current agents, which remain too weak and are used in relatively low-stakes ways, our work has implications for the ways these agents are trained. We show that current systems can already learn through RL to encode reasoning to defeat monitors in order to achieve a high outcome reward, so any system trained with a monitor-like reward should not use full-horizon RL unless it is willing to tolerate encoded reasoning. And we offer guidance on how to design step-level rewards that reduce the impact of outcome-reward on the learned process."}]}