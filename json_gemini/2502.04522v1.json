{"title": "ImprovNet: Generating Controllable Musical Improvisations with Iterative Corruption Refinement", "authors": ["Keshav Bhandari", "Sungkyun Chang", "Tongyu Lu", "Fareza R. Enus", "Louis B. Bradshaw", "Dorien Herremans", "Simon Colton"], "abstract": "Deep learning has enabled remarkable advances in style transfer across various domains, offering new possibilities for creative content generation. However, in the realm of symbolic music, generating controllable and expressive performance-level style transfers for complete musical works remains challenging due to limited datasets, especially for genres such as jazz, and the lack of unified models that can handle multiple music generation tasks. This paper presents ImprovNet, a transformer-based architecture that generates expressive and controllable musical improvisations through a self-supervised corruption-refinement training strategy. ImprovNet unifies multiple capabilities within a single model: it can perform cross-genre and intra-genre improvisations, harmonize melodies with genre-specific styles, and execute short prompt continuation and infilling tasks. The model's iterative generation framework allows users to control the degree of style transfer and structural similarity to the original composition. Objective and subjective evaluations demonstrate ImprovNet's effectiveness in generating musically coherent improvisations while maintaining structural relationships with the original pieces. The model outperforms Anticipatory Music Transformer in short continuation and in-filling tasks and successfully achieves recognizable genre conversion, with 79% of participants correctly identifying jazz-style improvisations. Our code and demo page can be found at https://github.com/keshavbhandari/improvnet.", "sections": [{"title": "I. INTRODUCTION", "content": "Music style transfer has recently emerged as a fascinating area of generative Al research, offering new possibilities for creating personalized compositions. Altering the melodic, rhythmic, and harmonic aspects of compositions can enable the crafting of music that feels both familiar and creative, tailored to reflect individual preferences or to evoke specific artist and genre styles.\nThe concept of neural style transfer was pioneered by [1], with the goal of blending \u201ccontent\u201d from one source with the \"style\" characteristics of another. In this framework, content refers to the underlying structure that remains in-tact, while style encompasses the features transferred from a different input. Since then, neural style manipulation has sparked widespread interest in various modalities. However, the core idea behind style transfer remains consistent across these applications: preserving the content or structure of one input while adopting the stylistic characteristics of another.\nIn recent years, the concept of \"style transfer\u201d has evolved, particularly in the realm of music processing. As seen in [2]\u2013[6], the definition now encompasses a wider and more general range of style transformations rather than deriving it from one or a few examples. In this context, the goal is to transform an input to match the stylistic characteristics of large datasets. According to [7], this can also be described as style conversion or translation. In our work, we adhere to the style conversion definition from the recent literature.\nIn this paper, we focus on the generation of expressive performance improvisations in the context of symbolically rep-resented classical and jazz solo piano pieces. An improvisation could be viewed as a special case of style conversion where the generated output either adopts a different style or retains the original one through meaningful modifications to the given content. The goal of improvisation is to introduce changes that maintain a recognizable link to the original, preserving its identity while exploring new variations within its stylistic representations. Hence, the musical content is generated with the aim of reflecting the essence of the original, either within the same style or a distinct one, depending on the user.\nWe interpret \"style\" throughout this paper as genre-specific elements, such as rhythmic, harmonic, and melodic properties that reflect a particular musical genre. On the other hand, \"content\" encompasses the foundational elements specific to a musical piece, including its core melody, rhythm, chord pro-gressions, and structure. According to this, our goal for cross-genre music improvisation is to adapt the content of a music piece to reflect the stylistic musical attributes representative of the target genre while preserving the core musical ideas of the original. For example, a classical to jazz cross-genre improvisation may preserve motivic fragments, tempo, and global form of the original piece while introducing elements such as dissonance, chromatic scales, irregular or syncopated rhythms, jazz harmonies, and melodies. Similarly, our goal for intra-genre improvisation (classical-to-classical or jazz-to-jazz improvisation) is to adhere to the genre of the original piece while modifying its content, generating a new variation that still references the essence of the initial composition.\nOur study addresses a gap in symbolic music research in"}, {"title": "II. RELATED WORK", "content": "Music style transformations come in various forms, depend-ing on how style is defined, how music is represented, and what the model is conditioned on. In this paper, we focus on expressive and controllable improvisational style in the sym-bolic domain, achieved through modifications to the rhythm, harmony, and melody of a solo piano performance conditioned on the target genre. Previous studies have focused on these tasks separately. For example, [13] addresses style transfer for long pop piano pieces, allowing users to customize musical attributes such as rhythmic intensity and polyphony at the bar level. Models such as [14], [15] harmonize a melody in the style of Bach chorales. Similarly, [16] proposes FolkDuet to fuse Chinese melodies with musical counterpoint in real time. [17] introduces FaderNets for generating musical variations by adjusting low-level attributes using latent regularization and feature disentanglement while enabling high-level feature control (e.g., arousal) through a VAE framework. [18], [19] focus on arranging accompaniments for solo piano melodies, offering control over aspects such as voice density. Studies such as [7], [20]\u2013[24] extend the controllable accompaniment arrangement task to multi-track generation. However, all of these approaches use highly quantized piano encodings, lim-iting the model's ability to produce expressive renditions.\nAn expressive performance rendition in symbolic music should be capable of human-like characteristics in timing, dynamics, and articulations, which go beyond the rigid quan-tization of notes. [25] was among the first works to generate expressive performance renditions by modelling a stream of midi note events. Recently, generating expressive performance renditions from symbolic score has been the focus of several works such as [26]-[32]. Among these, [31] is capable of generating controllable and expressive polyphonic piano vari-ations, but is limited to short fragments of music.\nSeveral works have explored genre style conversion, which is more closely related to this paper. [2] explores unsuper-vised monophonic melody style conversion using a statistical framework that integrates a music language model with an edit model, discovering musical styles through unsupervised grammar induction. [33] pioneered the use of GANs for genre conversion in symbolic polyphonic piano pieces by adapting the CycleGAN [34] framework with genre classifier discriminators. [35] enhanced this approach by incorporating a beta-VAE [36] for better disentanglement of style and content. Recently, [37] and [38] have advanced multi-instrument genre conversion through architectural and loss function-related modifications to the CycleGAN and VAE frameworks. How-ever, these GAN-based approaches face training instability, po-tential mode collapse, and challenging parameter tuning. Their piano-roll representations also limit expressive performance capabilities and lack user-controlled style conversion intensity, both key aspects of our work. Recent works on infilling and prompt continuation tasks, including [20], [24], [39]\u2013[41], also relate to our model's capabilities."}, {"title": "III. METHODS", "content": "We adapt a publicly available tokenizer called Aria\u00b9 to en-code expressive piano performances for ImprovNet. Although the Aria tokenizer can encode all MIDI instrument categories, we restrict it to solo piano and additionally change the order of tokens to assist with the harmonization task for which the generated logits are constrained, as shown later in section III-F4. As shown in Fig. 1, the Aria tokenizer uses a chunked absolute onset encoding. In this encoding, the pitch and velocity of the MIDI are merged into a single token, while the onset and duration remain as separate tokens. Most metadata, such as bars, key, and time signatures, is excluded. However, essential musical elements such as the usage of the sustain pedal are directly incorporated into the onset and duration tokens. Quantization is minimal: onset and duration values are rounded to the nearest 10 milliseconds, and velocity values are quantized to increments of 15 MIDI units (ranging from 0 to 120). The use of minimally quantized absolute onsets allows the music to be encoded in a manner that closely preserves the characteristics of a human-like performance rendition."}, {"title": "B. ImprovNet Overview", "content": "ImprovNet relies on a corruption-refinement training strat-egy similar to [44], to generate music in an improvisational style of the original composition. The entire piece is segmented into 5-second segments as seen in Fig. 1.\nDuring training (Fig. 2), a random segment is selected with context segments on its left and right. This segment is corrupted using one of the corruption functions described in Section III-D. The corrupted segment, combined with the original context, is fed into a transformer encoder along with conditional tokens for the genre and corruption type. The transformer decoder learns to reconstruct the original segment.\nDuring generation (Fig. 3), 5-second segments are pro-cessed iteratively. A chosen segment is corrupted and refined based on a probability similar to that of training. For cross-genre improvisation, refinement conditions on the target genre token. This process iterates from beginning to end several times, gradually shaping the piece into the target genre."}, {"title": "C. Formulation of Refinement Framework", "content": "Let S denote all 5-second segments separated by the (T) tokens. We can represent the encoding for a complete musical sequence as:\n$S = (s_1,...,s_N)$ (1)\nDuring training, we randomly sample a segment $s_i$ for $i < N$ from the sequence. To construct the input $S_{input}$, we include $(s_{i-L},...,s_{i-1})$ and $(s_{i+1},..., s_{i+R})$ for the $L$ and $R$ seg-ments of context around $s_i$ where $L$ and $R$ represent the lengths of the left context and right context, respectively. This gives us the input $S_{input}$ as follows:\n$S_{input} = (s_{i-L},..., s_{i-1}, s_i, s_{i+1},...,s_{i+R})$ (2)\nWhen segments prior or after $s_i$ are unavailable such as the beginnings and endings of the sequence, we simply remove them from $S_{input}$. We leave the left and right context segments as they are, but corrupt $s_i$ with a randomly chosen corruption function from $f_j \\in j$. The list of corruption functions is de-scribed in Section III-D. This process produces the corrupted segment $S_{corrupted} = f_j(s_i)$, which is then transformed by"}, {"title": "D. Corruption Functions", "content": "During training, ImprovNet uses nine different corruption functions $f_j$ to refine sequences relative to ground truth, conditioned on genre tokens.\n1) Pitch Velocity Mask: The pitch-velocity tokens within a $s_i$ segment are masked while the duration and onset tokens are kept intact.\n2) Onset Duration Mask: The onset and duration tokens are masked out. The goal is to add syncopation during the classical-to-jazz cross-genre improvisation task.\n3) Whole Mask: The entire $s_i$ segment is masked and replaced with a special \u201cwhole mask\u201d token. The model learns to generate all the notes omitted for this segment with respect to the context surrounding it. During training, when the whole mask corruption is randomly sampled, we further drop the right context segments $(s_{i+1},..., s_{i+R})$ 50% of the time for the short prompt continuation and short infilling tasks in section III-F2 and III-F3.\n4) Permute Pitch: We shuffle the MIDI pitches of all notes within a $s_i$ segment while preserving their velocity values.\n5) Permute Pitch Velocity: We shuffle both the pitch and velocity MIDI values for all notes within $s_i$.\n6) Fragmentation: Fragmentation retains a subset of notes within $s_i$, randomly keeping 20%-50% of the notes at the beginning. The model learns to generate missing notes to produce useful variations of the original segment.\n7) Incorrect Transposition: We randomly shift 50% of the notes in $s_i$ by $\u00b1$ 5 semitones while preserving their velocity, training the model to regenerate the correct pitches. This incorrect transposition, when conditioned on the jazz target genre, introduces jazzy melodies and harmonies, often transforming rapid semitone passages into chromatic scales.\n8) Note Modification: Note modification randomly omits 10%-40% of notes spaced at least 50 milliseconds apart, adding their duration to the preceding note. New notes are then inserted after notes longer than 500 milliseconds, with a probability of 10% - 40%. The pitches of these new notes are chosen within $\u00b1$ 5 semitones, and their velocities are randomly selected between 45 and 105 in increments of 15.\n9) Skyline: We use the Skyline [46] method for the extrac-tion of the melody by selecting the highest pitch values. Notes within 50 milliseconds are treated as chords to account for slight timing offsets in human performance, and their MIDI velocity is fixed at 90. Although not always melodic [47], Skyline serves as a corruption function to train ImprovNet to regenerate omitted notes, allowing harmonization with a monophonic melody, as discussed in Section III-F4, while learning dynamics for more expressive music."}, {"title": "E. Self-Supervised Pre-Training and Fine-Tuning", "content": "ImprovNet uses a self-supervised corruption refinement strategy, where segments are corrupted with predefined func-tions to help the model learn musical structures and re-lationships. This enables learning from weak genre labels without requiring richly annotated datasets, capturing genre-specific traits. ImprovNet was pre-trained on classical music"}, {"title": "F. Iterative Generation", "content": "In this section, we describe the tasks ImprovNet can perform through its iterative generation framework.\n1) Style-aware Improvisations: By conditioning ImprovNet on a genre label, users can generate two types of improvisa-tions. The first is cross-genre improvisation (CGI), where a genre label different from the original is used as conditioning. The second is intra-genre improvisation (IGI), where the genre label remains the same, preserving the original genre.\nUsers can also select different corruption functions with corresponding corruption rates ($\\alpha$) for each pass and adjust the context window size during refinement. Initial experiments show that CGI works well with smaller right context sizes (R, e.g., 1-2) or additional passes using progressively lower corruption rates compared to IGI. However, the optimal setup varies depending on the unique characteristics of each piece.\n2) Short Prompt Continuation: ImprovNet can generate short continuations (5-20 seconds) of a prompt using whole mask corruption while trimming the right context during the refinement process. For example, given a 20-second prompt (4 segments) at the start of a piece and a task to generate the next 15 seconds (3 segments), we adjust Equation 6 as follows:\nFor $q$ = 1, let $P_q : \\forall i \\in \\{5, 6, 7\\}$,\n$\\text{compute } s_i \\text{ as } S_r = r_\\theta(S_{c,g}[: i]),$ (7)\nwhere $S_{c,g}[: i] = (S_{i-L},..., S_{i-1}, S_{c,g})$. Here, the right con-text is trimmed to the index i to iteratively refine the sequence. The corrupted segment $s_{c,g}$ is generated using the whole-mask corruption method for the specific corruption function $f_{j=3}$ described in Equation 3. We find the coherency of the generated music starts deteriorating beyond 20 seconds, as the model isn't explicitly trained for long continuations.\n3) Short Infilling: ImprovNet can also perform short infill-ing (5-20 seconds) by considering both left and right contexts. For example, consider the scenario where the first 20 seconds and 40\u201360 seconds are provided as the left and right side contexts and 20\u201340 seconds need to be infilled by ImprovNet. The infilling process begins by generating segments $s_5$, $s_6$ and $s_7$ (20-35 seconds) as a continuation of the left context using the whole mask corruption. During this process, the right side context is not provided to ImprovNet as described in equation 7. For the final segment $s_8$ (35-40 seconds), the left and right contexts are included in the generation, as shown in Equation 6 to ensure a smooth transition to the segments after $s_8$.\n4) Harmonization: The skyline algorithm used as a corrup-tion function helps ImprovNet learn to generate notes below a MIDI pitch. During generation, when a monophonic melody is provided, we use the skyline corruption function for Equation"}, {"title": "A. Objective Evaluation", "content": "1) Style-aware Improvisation: Evaluating improvisations is a challenging task. We used two objective evaluation metrics to understand the impact of corruption functions, corruption rates, and number of passes on genre conversion and structural similarity with respect to the original composition.\nGenre Classifier: We train a genre classifier (see Section IV-B2) to predict the probability of the genre of the original and generated compositions.\nStructural Similarity Score: We derive a chroma-based structural similarity matrix from the audio of the original and generated compositions and compute the Pearson correlation between the two matrices. This indicates how similar the generation is to the original composition."}, {"title": "B. Subjective Evaluation", "content": "We conducted a subjective assessment of ImprovNet using a Qualtrics listening test with 28 participants, who spent ap-proximately 40 minutes providing quantitative and qualitative feedback on the generated examples. 18 participants had 3+ years of formal training in music theory (9 with 10+ years), and 20 had 3+ years of formal training on a musical instrument including voice (11 with 10+ years). 20 and 18 participants were familiar with jazz and classical genres, respectively.\nAs this work is the first expressive performance generation for genre-style-based improvisation and harmony generation tasks, we used the ground truth as the baseline to assess our"}, {"title": "VI. RESULTS", "content": "For the CGI task in Fig. 4a, each corrupted function refined over multiple passes increases the cross-genre (classical to jazz) probability score. The whole mask and onset-duration mask corruption functions have the highest increases in prob-ability, while the incorrect transposition and permutation func-tions have the lowest. We also see the sharpest decrease in the"}, {"title": "VII. DISCUSSION", "content": "ImprovNet exhibits strong capabilities to generate expres-sive style-aware improvisations, while offering users the ability to control the intensity of style transfer and musical structure, as seen in the objective and subjective results. Additionally, its versatility extends to short prompt continuation and infilling tasks, which achieve better performance compared to the AMT baseline. Moreover, the skyline corruption algorithm combined with the constraints over the logits allows ImprovNet to harmonize a given melody with respect to the musical genre.\nFor generating improvisations, there is no best corruption function or number of passes, as these depend on the unique characteristics of the piece and the user's preferences. Im-provNet can be controlled by the user by trying out various combinations of corruption functions at each pass with differ-ing corruption rates. At the global level, users can specify the target genre, context window size, and SSM based preservation ratio, indicating the original segments to be preserved during the refinement process. The specified number of passes pushes each segment towards a target genre as the context surrounding it moves toward it as well.\nImprovNet is able to add chromatic scales, jazz harmoniza-tions, and syncopations in cross-genre classical-to-jazz tasks. However, it has some limitations that we aim to address in future work. The harmonization task sometimes produces overly dense chords, which could be mitigated with additional conditional tokens. Similarly, the onset-duration mask occa-sionally produces irregular rhythms; we plan to refine this for a more consistent and stable swing rhythm in the future."}, {"title": "A. Generalizability and Link to Diffusion Models", "content": "The corruption-refinement framework used in ImprovNet highlights its potential applicability to broader domains, in-cluding large language models (LLM) or other unstructured data tasks, as a generalizable approach for iterative improve-ment. This method shares conceptual parallels with diffusion models, where corruptions can be interpreted as analogous to the noise applied during the diffusion process. While training diffusion models, noise is sampled from a predefined proba-bilistic distribution, while in ImprovNet, corruptions are sam-pled from a uniform distribution. The number of refinement passes in ImprovNet mirrors the iterative denoising steps of diffusion models. However, ImprovNet provides an additional advantage: users can specify the corruption function, allowing for greater controllability over the \"noise\" introduced, a level of customization not typically explored in diffusion models.\nA key distinction lies in the generation process: ImprovNet employs an autoregressive approach, where notes are gener-ated in an ordered sequence, and each segment is iteratively refined, leveraging its temporal context. In contrast, diffusion models generally predict an entire sequence or representation at once, guided by the iterative noise removal process."}, {"title": "VIII. CONCLUSION", "content": "This paper presents ImprovNet, a transformer-based ar-chitecture for generating expressive and controllable style-aware musical improvisations, leveraging a self-supervised corruption-refinement training strategy and an iterative gener-ation framework. The model demonstrates strong capabilities in generating both cross-genre and intra-genre improvisations while maintaining structural coherence with the original com-positions. ImprovNet's versatility extends beyond style-aware improvisations to include short prompt continuation, short infilling, and genre-conditioned harmonization tasks. Com-prehensive objective and subjective evaluations demonstrate ImprovNet's superior performance for the short prompt con-tinuation and short infilling tasks over the Anticipatory Music Transformer baseline. We set the baseline for future work on expressive style-aware improvisations and harmonization."}]}