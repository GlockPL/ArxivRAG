{"title": "BioRAGent: A Retrieval-Augmented Generation\nSystem for Showcasing Generative Query\nExpansion and Domain-Specific Search for\nScientific Q&A", "authors": ["Samy Ateia", "Udo Kruschwitz"], "abstract": "We present BioRAGent, an interactive web-based retrieval-\naugmented generation (RAG) system for biomedical question answer-\ning. The system uses large language models (LLMs) for query expan-\nsion, snippet extraction, and answer generation while maintaining trans-\nparency through citation links to the source documents and displaying\ngenerated queries for further editing. Building on our successful partic-\nipation in the BioASQ 2024 challenge, we demonstrate how few-shot\nlearning with LLMs can be effectively applied for a professional search\nsetting. The system supports both direct short paragraph style responses\nand responses with inline citations. Our demo is available online, and\nthe source code is publicly accessible through GitHub\u00b9.", "sections": [{"title": "1 Introduction", "content": "Professional search in biomedical domains requires complex query formulation to\nsupport the generation of evidence-based answers [5]. While LLMs have shown\nimpressive capabilities in question answering tasks, their tendency to hallucinate\nmakes it challenging to directly use them in professional settings [2]. RAG [4] has\nemerged as a promising approach to ground LLM responses in reliable sources\nand reduce hallucinations [8].\nBuilding on our participation in the BioASQ 2024 challenge [6], where we\ndemonstrated competitive performance using both commercial and open-source\nLLMs in a biomedical RAG setting [1], we present BioRAGent an interactive\nsystem that makes our approach accessible to users through a web interface."}, {"title": "2 System Overview", "content": "BioRAGent is implemented as a web application using the Gradio framework2,\nwhich provides a clean and responsive user interface. The underlying LLM used\nis Gemini 1.5 flash 002 from Google due to its speed and low cost\u00b3 [7]. The\nsystem architecture consists of four main components:"}, {"title": "2.1 Query Expansion", "content": "Given a biomedical question, the system uses few-shot (3-shot) learning with\nLLMs to generate an expanded query that incorporates relevant synonyms and\nrelated terms. The potential of LLMs generating queries for professional search\nhas been explored in related work, notably by Wang et al. (2023) [9]. Users\ncan inspect and modify the expanded query after execution, making the search\nprocess transparent and controllable. The system uses Elasticsearch as the un-\nderlying search engine with an index of the 2023 snapshot of PubMed articles 4.\nThe query syntax is the Elasticsearch query string query dsl that is supported\nby the used search endpoint."}, {"title": "2.2 Document Retrieval and Snippet Extraction", "content": "The expanded query is executed against an index of abstracts and titles of\nPubMed articles using the default English analyzer of Elasticsearch. The top 50\narticles ranked by the default BM25 based scoring of Elasticsearch are retrieved"}, {"title": "2.3 Answer Generation", "content": "The system generates two types of answers:\nA short paragraph style answer that is generated by grounding the model\nwith the retrieved snippets but doesn't force the model to use the retrieved\ninformation. This format was used in the BioASQ challenge.\nA paragraph style answer with inline citations to PubMed articles from the\nretrieved snippets in the form of PubMed IDs for every generated sentence.\nFor this answer format the model needs to use the retrieved snippets and\ncannot generate an answer if no relevant snippets were found. This format\nwas used in the TREC 2024 BioGen Track7."}, {"title": "2.4 User Interface", "content": "The interface provides:\nA question input field with a search button\nAn editable text box to display the Expanded query\nTwo answer text boxes with and without citations\nA list of retrieved snippets with direct PubMed links\nA screenshot highlighting the ability of the system to turn overly simple\nquestions into query terms that actually fit the target domain (scientific papers)\nis showcased in Figure 1."}, {"title": "3 Evaluation and Discussion", "content": "The core RAG components of BioRAGent were evaluated through our participa-\ntion in the BioASQ 2024 challenge [1]. Our system achieved competitive results\nacross different question formats and settings, winning multiple first and second\nplaces in the 12th. BioASQ challenges.\nWhile our system was most competitive in the question answering tasks of\nthe challenge (12B Phase A+, Phase B), in the document retrieval and snip-\npets extraction tasks other systems that also used dense and hybrid retrieval\ntechniques took the leading spots."}, {"title": "4 Conclusion and Future Work", "content": "BioRAGent demonstrates how SOTA LLM capabilities could be integrated into\nprofessional search systems while maintaining transparency and source attri-\nbution. The system makes our successful BioASQ approach accessible to users\nthrough an intuitive interface.\nWe hope that showcasing the usefulness of our rather simple in-context learn-\ning approaches, will encourage others to build upon and rapidly refine these\nmethods, advancing the state-of-the-art in scientific and biomedical RAG. Fu-\nture work will focus on expanding the interface for live prompt and few-shot\ntemplate editing; incorporating direct evaluation modules for BioASQ dataset\nverification and hallucination detection; and adding support for selecting and\nevaluating different LLMs."}]}