{"title": "Is Your Paper Being Reviewed by an LLM?\nInvestigating AI Text Detectability in Peer Review", "authors": ["Sungduk Yu", "Man Luo", "Avinash Madusu", "Vasudev Lal", "Phillip Howard"], "abstract": "Peer review is a critical process for ensuring the integrity of published scientific\nresearch. Confidence in this process is predicated on the assumption that experts\nin the relevant domain give careful consideration to the merits of manuscripts\nwhich are submitted for publication. With the recent rapid advancements in the\nlinguistic capabilities of large language models (LLMs), a new potential risk to the\npeer review process is that negligent reviewers will rely on LLMs to perform the\noften time consuming process of reviewing a paper. In this study, we investigate\nthe ability of existing AI text detection algorithms to distinguish between peer\nreviews written by humans and different state-of-the-art LLMs. Our analysis\nshows that existing approaches fail to identify many GPT-40 written reviews\nwithout also producing a high number of false positive classifications. To address\nthis deficiency, we propose a new detection approach which surpasses existing\nmethods in the identification of GPT-40 written peer reviews at low levels of false\npositive classifications. Our work reveals the difficulty of accurately identifying\nAI-generated text at the individual review level, highlighting the urgent need for\nnew tools and methods to detect this type of unethical application of generative AI.", "sections": [{"title": "Introduction", "content": "Recent advancements in large language models (LLMs) have enabled their application to a broad\nrange of domains, where LLMs have demonstrated the ability to produce plausible and authoritative\nresponses to queries even in highly technical subject areas. These advancements have coincided\nwith a surge in interest in AI research, resulting in large increases in paper submissions to leading\nAl conferences (Table S1). Consequently, workloads for peer reviewers of such conferences have\nalso increased significantly, which could make LLMs an appealing tool for lessening the burden of\nfulfilling their peer review obligations.\nDespite their impressive capabilities, the use of LLMs in the peer review process raises several\nethical and methodological concerns which could compromise the integrity of the publication process.\nReviewers are selected based on their expertise in a technical domain related to a submitted manuscript,\nwhich is necessary to critically evaluate the proposed research. Offloading this responsibility to an\nLLM circumvents the role that reviewer selection plays in ensuring proper vetting of a manuscript.\nFurthermore, LLMs are prone to hallucination and may not possess the ability to rigorously evaluate\nresearch publications. Therefore, the use of LLMs in an undisclosed manner in peer review poses a\nsignificant ethical concern that could undermine confidence in this important process."}, {"title": "Methodology", "content": "We used the OpenReview client API [2] to collect submitted manuscripts and their reviews for the\nICLR conference from 2019 to 2024. The total numbers of submission and reviews are summarized in\nTable S1. We use two LLMs to generate AI peer reviews for these manuscripts, OpenAI's GPT-40 [3]\nand the open-source instruction-tuned Llama-3.1 (70b) [4], to generate AI peer reviews. Manuscripts\nare converted from PDF to Markdown format, excluding the Bibliography and Acknowledgement\nsections to focus on the core content relevant for review. Prompts used for generating AI reviews\nare adapted from Lu et al. [5] with two major changes. First, recognizing that LLM-generated\ntext is sensitive to prompt variations, we introduce four distinct reviewer archetypes\u2014\"balanced,\"\n\"nitpicky,\" \"innovative,\" and \"conservative\"\u2014to simulate the diversity of real-world peer review\nscenarios. Second, we provide the LLMs with the ICLR 2022 reviewer guidelines [6] with a minor\nmodification. The ICLR guideline emphasize general instructions about how to approach peer review,\nrather than focusing on rubric and scoring scales as in the NeurIPS reviewer form. Our complete\nprompts are included in Appendix F. We collect a total of 16,000 AI-generated reviews for 500\nrandom submissions for each ICLR conference of year from 2021-2024. Each AI-generated review\ncontains five sections, and they were combined for detection tasks (see Appendix C for details)."}, {"title": "AI-Generated Text Detection Methods", "content": "Open-source AI text detection models. We utilize two supervised fine-tuned pretrained language\nmodels. The first is from [7], where the author collect a Human ChatGPT Comparison Corpus\n(HC3) consisting of question-answer pairs created by human experts and generated by ChatGPT.\nThe authors trained two Roberta-based models [8]: one detects whether answers are human or\nAI-generated using paired data, while the other evaluates single answers. We use the latter for our\nassessment. The second model is a Longformer [9]. As tested in [10], this model has demonstrated"}, {"title": "Results", "content": "AI review detectability. Table 1 compares the TPR of different detection methods on ICLR 2022\nreviews when calibrated for a FPR of 0.05 and 0.20. As there is no threshold to tune the sensitivity\nof the GPT-40 and Llama 3.1 Judge models, we report only the single TPR and FPR for the binary\nclassifications produced by these detectors. While GPT-40 is able to identify its own peer reviews\nover 80% of the time and those written by Llama 3.1 nearly 95% of the time, this comes at the cost\nof flagging 20% of all human reviews as AI-written. In contrast, the Originality AI API and our\nGPT-40 and Llama 3.1 Embedding models correctly identify nearly all GPT-40 written reviews at an\nidentical FPR. At a lower (and more practiacal) FPR of 0.05, our GPT-40 and Llama 3.1 Embedding\nmethods perform the best at detecting GPT-4o written reviews, identifying 97% and 95% of all such\nreviews (respectively). These methods also correctly identify 87-90% of Llama written peer reviews,\nwhile the Originality AI API identifies nearly all Llama 3.1 written reviews but is significantly\nworse at detecting GPT-40 written reviews. The RoBERTa and Longformer classifiers perform the\nworst among evaluated methods at both FPR levels. While we mainly present the results of 2022\nyear reviews, we provide additional results for ICLR reviews from 2021, 2023, and 2024 for other\ndetection models in Appendix B."}, {"title": "Conclusion", "content": "In this work, we showed that existing approaches for detecting AI text are poorly suited to the\nchallenging problem of identifying AI-generated peer reviews. While high detection rates are\npossible with existing methods, this comes at the cost of relatively high rates of falsely identifying\nhuman-written reviews as containing AI text, which must be minimized in practice. We proposed a\nnew approach which intentionally generates AI-written reviews for a given paper to serve as a basis"}, {"title": "F Prompts", "content": "F.1 Prompt for LLM judges\nSystem prompt:\nYou will be given a text. Your task is to determine if the text is either\nwritten by a human or generated by a large language model (LLM).\nFirst, read and analyze the given text rigorously.\nThen, if you think any portion of the text is written by AI (that is, large\nlanguage models), type \"AI\". Type \"human\" only if you think the entire text\nis written by a human.\nWrite your answer in the following JSON format, and output only the JSON\ncode block without any additional text or explanations outside of it:\n{\n}\njson\n\"Result\": \"<'human' or 'AI'>\",\n\"Rationale\": \"<Provide a clear and concise justification for your\ndecision. Ensure your explanation highlights specific features that\ninfluenced your decision. Do not use more than 100 words.>\"\nUser prompt:\nHere is the text you are asked to assess:\n{review}\nF.2 Prompt for generating reviews\nPrompts used for generating AI reviews were adapted from Lu et al. [5] with two major changes. First,\nwe introduced four archetypes of peer reviewers to generate a diverse set of reviews, capturing the\nvarying characteristics across different reviews. Second, instead of the NeurIPS reviewer guideline,\nwe provide the LLMs with the ICLR 2022 reviewer guideline\u00b9 with a minor modification. The ICLR\nguideline emphasize general instructions about how to approach the peer review, rather than focusing\non rubric and scoring scales as in the NeurIPS reviewer form.\nReviewer type prompt:\n\u2022 <balanced>\nYou provide fair, balanced, thorough, and constructive feedback,\nobjectively highlighting both the strengths and weaknesses of\nthe paper. You maintain a high standard for research in your\ndecision-making process. However, even if your decision is to\nreject, you offer helpful suggestions for improvement.\n\u2022 <nitpicky>\nYou are a perfectionist who meticulously examines every aspect\nof the paper, including minor methodological details, technical\nnuances, and formatting inconsistencies. Even if a paper presents\nnovel ideas or significant contributions, you may still recommend\nrejection if you identify a substantial number of minor flaws. Your"}]}