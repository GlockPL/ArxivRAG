{"title": "Rate, Explain and Cite (REC): Enhanced Explanation and Attribution in Automatic Evaluation by Large Language Models", "authors": ["Aliyah R. Hsu", "James Zhu", "Zhichao Wang", "Bin Bi", "Shubham Mehrotra", "Shiva K. Pentyala", "Katherine Tan", "Xiang-Bo Mao", "Roshanak Omrani", "Sougata Chaudhuri", "Regunathan Radhakrishnan", "Sitaram Asur", "Claire Na Cheng", "Bin Yu"], "abstract": "LLMs have demonstrated impressive proficiency in generating coherent and high-quality text, making them valuable across a range of text-generation tasks. However, rigorous evaluation of this generated content is crucial, as ensuring its quality remains a significant challenge due to persistent issues such as factual inaccuracies and hallucinations. This paper introduces two fine-tuned general-purpose LLM autoevaluators, REC-12B and REC-70B-specifically designed to evaluate generated text across several dimensions: faithfulness, instruction following, coherence, and completeness. These models not only provide ratings for these metrics but also offer detailed explanations and verifiable citations, thereby enhancing trust in the content. Moreover, the models support various citation modes, accommodating different requirements for latency and granularity. Extensive evaluations on diverse benchmarks demonstrate that our general-purpose LLM auto-evaluator, REC-70B, outperforms state-of-the-art LLMs, excelling in content evaluation by delivering better quality explanations and citations with minimal bias. It achieves Rank #1 as a generative model on the RewardBench leaderboard\u00b9 under the model name TextEval-Llama3.1-70B. Our REC dataset and models are released at https://github.com/adelaidehsu/REC.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have demonstrated impressive capabilities in generating high quality coherent text and are deployed in applications for various text-generation tasks (Brown et al., 2020; Chowdhery et al., 2022; OpenAI, 2023; Touvron et al., 2023). In order for LLMs to provide up-to-date information or to perform knowledge-intensive tasks (Lewis et al., 2020), Retrieval Augmented Generation (RAG) system (Chen et al., 2017; Borgeaud et al., 2021; Izacard et al., 2022; Guu et al., 2020) has been used widely. RAG involves first retrieving document chunks that are relevant to the generation task from an external knowledge source, and performing the generation using an knowledge-augmented prompt. However, it's of paramount importance to ensure that the generated text is reliable and can be trusted by a human as LLMs often suffer from factual incorrectness and hallucination of knowledge (Ji et al., 2022; Zhang et al., 2023; Shuster et al., 2021). Towards this goal, we propose the generated text needs to be evaluated along various dimensions shown below:\n\u2022 Faithfulness: Did the LLM generate factually correct response given the context? (Rating +\nExplanation task)\n\u2022 Instruction Following: Did the LLM generate response follow the instructions provided in the prompt? (Rating + Explanation task)\n\u2022 Coherence: Did the LLM generate a coherent response? (Rating + Explanation task)\n\u2022 Completeness: Did the LLM generate a complete response including all details? (Rating + Explanation task)\n\u2022 Citation: If the LLM generated factually correct response, can we provide evidence for where that response came from? (Citation task)\nIn this paper, we introduce fine-tuned models for the evaluation tasks listed above that can form the basis for all trust-related evaluations for generative AI applications. The model was fine-tuned to not only provide ratings for the metrics but also explanations + citations for why it rated the generation so. For instance, if the faithfulness metric is rated"}, {"title": "2 Related Work", "content": "Verifiable Text Generation Providing citations to attributable information source (Liu et al., 2023a; Gao et al., 2023) has been proposed as one of the approaches to mitigate hallucination and improve the factuality of LLM generation. Such work falls into the general RAG citation setting defined in our work, where the aim is to generate verifiable content through citing from the provided document chunks. Initial efforts fine-tune LLMs using either human-written examples (Nakano et al., 2021) or machine-generated examples verified by human (Menick et al., 2022), but their privately maintained training data limits further research. With the advent of more capable LLMs (OpenAI, 2023; Jiang et al., 2023), most existing work relies on zero-shot prompting or few-shot prompting to cite document chunks during generation (Kamalloo et al., 2023; Gao et al., 2023; Liu et al., 2023a), although the quality of their generated citations leaves large room for improvement (Malaviya et al., 2024). Other work utilizes an additional natural language inference (NLI) model to add citations (Gao et al., 2022; Chen et al., 2023). To improve upon prior work, we propose a fine-tuned LLM that can generate better-grounded responses supported with citations of various modes, with the fine-tuning dataset released to the public to facilitate future research and no additional NLI models needed.\nGeneral-purpose Autoevaluators Collecting human annotations to evaluate LLM is not only costly, time-consuming, but hard to replicate (Ouyang et al., 2022; Zheng et al., 2023; Chiang and Lee, 2023), as a result LLMs become a natural automated proxy to evaluate LLM capabilities on various benchmarks (Bai et al., 2024; Bubeck et al., 2023; Chiang et al., 2023; Fu et al., 2023; Liu et al., 2023b; Wang et al., 2023a). Existing LLM autoevaluators often judge LLM outputs by expressing \"preference\" over outputs from a reference model (Dubois et al., 2024; Li et al., 2023b; Yuchen Lin et al., 2024), or by providing rating and explanation according to a user-defined metric as a direct assessment (Li et al., 2023a; Wang et al., 2023b, 2024b). Closer to our work, Jiang et al. (2024) and Xu et al. (2023) fine-tunes LLMs to generate rating, explanation, and a detailed analysis to pinpoint errors in the response evaluated. Unlike prior work, our fine-tuned model is a more generalizable autoevaluator since it provides content"}, {"title": "3 Data Collection", "content": "To fine-tune our general-purpose LLM autoevaluator, we meticulously collect a multitask mixture of data encompassing a broad spectrum of LLM capabilities (Section 3.1), and we denote our collected dataset as REC-Data. Due to a lack of publicly available content quality citation datasets, we leverage synthetic data generation using LLM to curate such data (Section 3.2) to facilitate the research community. We perform rule-based automatic quality check followed by a unified task formatting to post-process our data for instruction fine-tuning (Section 3.3)."}, {"title": "3.1 Task Types", "content": "To enhance the explainability in the automatic evaluation of our LLM autoevaluator, while maintaining its general instruction-following capability, we gather datasets from a diverse range of task types (See detailed REC-Data distribution in Appendix B). These include:\n\u2022 Pairwise Evaluation: Compare two responses at the same time and express a preference according to evaluation criteria.\n\u2022 Pointwise Evaluation: Evaluate specific aspects of a response independently according to evaluation criteria and provide a rating.\n\u2022 Open-ended Evaluation: Evaluate a response independently and provide a free-form explanation, usually to support either pairwise or pointwise evaluation.\n\u2022 Citation: Evaluate a response alongside the context, and provide citations for verifiable answer attribution.\n\u2022 General Instruction: Generate a response as instructed (no evaluation tasks in this type), such as summarization and QA."}, {"title": "3.2 Synthetic data generation", "content": "We leverage synthetic data generation for some of the tasks, including pointwise evaluation and citation. For synthetic pointwise evaluation data, we leverage Mistral-7B to rate a response according to one of the 4 metrics listed in Section 1. For synthetic citation data generation, due to the challenging nature of citation tasks, we leverage a more powerful instruction-tuned LLM based on Llama-3.1-70B for both RAG and content quality citations. Example prompts used to generate the synthetic data is provide in Appendix C."}, {"title": "3.3 Post-processing and Unified Task format", "content": "Knowing that LLM-generated outputs is susceptible to hallucination, we perform automatic rule-based content quality check to ensure the quality of the synthetic data. During post-processing, we filter invalid synthetic LLM outputs that have either invalid output formats, or non-factual content cited by checking exact-match of the citation with the context. Our final REC-Data has in total around 140k datapoints, each containing a task prompt and a completion."}, {"title": "4 Model Training", "content": "The general-purpose LLM autoevaluator that we trained is available in two sizes, and they are denoted as REC-12B and REC-70B separately. REC-12B is instruction fine-tuned from Mistral-Nemo\u00b3, and REC-70B is instruction fine-tuned from Llama-3.1-70B (Team, 2024). We adopted supervised fine-tuning (SFT) to optimize the models. To accommodate GPU memory constraints, we made the following design choices: we filtered out examples where the combined length of the prompt and the response exceeded 6,144 tokens. We used Low-Rank Adaptation (LoRA) during fine-tuning (Hu et al., 2021), with a rank of $r = 256$ for REC-12B and $r = 64$ for REC-70B. Additionally, We applied 4-bit quantization for REC-70B during initialization. Both models were trained using a learning rate of $1 \\times 10^{-4}$, with data shuffled and for a single epoch. REC-12B was trained with a batch size of 2 and a gradient accumulation factor of 8, whereas for REC-70B we used a batch size of 1 with the same gradient accumulation factor. Both models were trained on eight NVIDIA H100 GPUs, each with 80GB of memory. The total training time was approximately 4.5 hours for REC-12B and 5.5 hours for REC-70B."}, {"title": "5 Evaluation", "content": "In this section, we evaluate our REC models on diverse benchmarks assessing LLMs' (1) RAG citation capability: ALCE (Gao et al., 2023), and ExpertQA (Malaviya et al., 2024); (2) content quality citation capability: human evaluation on ABCD summarization (Chen et al., 2021); (3) general capabilities: RewardBench (Lambert et al., 2024), and LLM-AggreFact (Tang et al., 2024a); (4) cognitive bias: COBBLEr (Koo et al., 2024), each of which is an important measure to a general-purpose LLM autoevaluator. We compare our results against 7 SOTA LLMs, including Misral-7B (Mistral-7B-Instruct-v0.2), Mistral-Nemo (Mistral-Nemo-Instruct-2407), Llama-3.1-70B (Llama-3.1-70B), Claude-3-Opus (claude-3-opus-20240229), GPT-3.5 (gpt-3.5-turbo), GPT-4 (gpt-4-turbo), GPT-4o (gpt-4o)."}, {"title": "5.1 RAG citation & Correctness", "content": "ALCE The ALCE (Automatic LLMs' Citation Evaluation) benchmark is designed to assess the ability of LLMs to generate text with accurate and relevant citations. ALCE addresses this by providing a framework that evaluates the quality of citations in text generated by LLMs, focusing on three key dimensions: fluency, correctness, and citation quality. The ALCE benchmark is built on three datasets: ASQA (a short-answer question dataset), QAMPARI (which provides lists of correct answers), and ELI5 (a long-form question-answer dataset). For each dataset, ALCE evaluates how well the model generates text that is not only fluent and accurate but also properly supported by citations. The evaluation includes metrics like citation precision (ensuring all cited sources are relevant) and citation recall (ensuring all necessary sources are cited). ALCE does not provide training data but instead measures citation performance through retrieval-based systems that simulate real-world information-seeking tasks, where the model retrieves relevant passages and uses them to support its answers.\nAs shown in Table 1, our REC models rank highly in terms of overall average performance, with REC-70B achieving an average score of 41.62, the highest among compared models, and REC-12B reaching 39.95. This highlights the robustness and balance of these models across fluency, correctness, and citation quality. REC offers a well-rounded solution across different types of questions, making it highly reliable for real-world applications where both citation quality and answer correctness are essential. Specifically, while GPT-4 excels in citation quality, the much smaller REC models outperform all the other competitors.\nExpertQA ExpertQA (Malaviya et al., 2024) is designed to evaluate LLMs' ability to generate accurate and well-attributed responses in technical and high-stakes fields, such as medicine and law. It was developed by involving experts from 32 different fields, who contributed 2,177 domain-specific questions based on their knowledge. In total, 484 participants helped curate these questions. LLMs were then used to generate responses to the questions, followed by human experts evaluating the quality of these responses on several criteria, in-"}, {"title": "5.2 Rating & Explanation & Citation", "content": "ABCD To evaluate our LLM autoevaluator's citation capability more comprehensively, besides examining its general RAG citation capability on QA tasks as reported in Section 5.1, we evaluate the content quality citation capability on a summarization task, the ABCD dataset (Chen et al., 2021), in this section.\nThe ABCD dataset (Chen et al., 2021) contains customer support conversation transcripts. We generate summaries by prompting Mistral-7B (Jiang et al., 2023) with: \u201cSummarize knowledge from transcripts after they've ended, including the customer issue and resolution.\" to form (dialogue, summary) pairs for evaluation. LLM autoevaluators then take as input the summarization task prompt hydrated with the dialogue, and the summary, to generate rating, explanation, and citations according to metrics defined in Section 1. Note that we use a held-out set of 50 examples in this experiment to make sure there exists no data overlap with the ABCD subsets used during training. However, unlike all the other evaluations included in this paper, this evaluation is in-distribution.\nDifferent from the general RAG citation benchmarks evaluated in Section 5.1, which contain human annotated citation groundtruth, there exists no well-established benchmarks to evaluate content quality citations. Hence, we seek human annotations to serve as a reference standard in this task. The final citation outputs were labeled by 12 machine learning experts where each has a graduate degree in computer science or related fields. They were asked to complete three tasks: rating correctness evaluation, explanation correctness evaluation, and to provide manually-written citations for the claims in explanation (See Appendix D for human labeling guideline).\nWe assign two labelers for each model and find the average inter-rater agreement is around 95%. For the correctness of rating and explanation, we report average results of the two labels, while for citations, we evaluate the intersection of the results of the two labelers against the machine-generated citations to compute F1 score as shown in Table 3. Due to the high quality summary generated by the summarizer LLM, most LLM autoevaluators obtain high rating and explanation accuracy. As for the citation task, both REC-12B and REC-70B models consistently outperform other models in F1 score."}, {"title": "5.3 General Capabilities", "content": "RewardBench The RewardBench dataset assesses LLMs' performance across several critical abilities using curated chosen-rejected response pairs (Lambert et al., 2024). The evaluation process involves determining the model's win percentage based on its ability to score the \u201cchosen\" response higher than the \"rejected\" one. Specifically, it's focused on testing chat performance, safety measures, and reasoning (both in terms of code and math skills), while controlling for potential biases such as overfitting to prior datasets.\nFirst we create prompts following the RewardBench evaluation format, where each prompt con-"}, {"title": "5.4 Bias Testing", "content": "COBBLEr Recent studies have found that LLM-as-a-Judge often exhibits cognitive biases, such as preferences for verbosity, egocentrism, bandwagon, and an overly authoritative tone (Wang"}, {"title": "6 Conclusions", "content": "LLMs excel in generating coherent and high-quality text for various tasks, but evaluating content quality is challenging due to issues like factual inaccuracies and hallucinations. This paper introduces two fine-tuned models-REC-12B and REC-70B-designed to evaluate generated text across several dimensions: faithfulness, instruction following, coherence, completeness. These models not only provide ratings for these metrics but also offer detailed explanations and verifiable citations, i.e., Ratings, Explanations, and Citations (REC), thereby enhancing trust in the content. To achieve this goal, a curated dataset for explanations and citations fine-tuning is proposed to facilitate future research, which is the first public dataset containing both content quality citations and RAG citations. The models support various citation modes, including Post-fix citation, Post-fix citation mode with snippet, Inline citation, and Inline citation mode with snippet to balance the trade-off between latency and granularity of citation. Extensive evaluations on diverse benchmarks\u2014ALCE, ExpertQA, ABCD summarization, RewardBench, LLM-AggreFact, and COBBLEr-demonstrate that the fine-tuned LLM autoevaluator, REC-70B, outperforms state-of-the-art models. It excels in content evaluation, delivering clear explanations and ensuring citation accuracy."}, {"title": "Limitations", "content": "Our work introduces the innovative Rate, Explain, and Cite (REC) model for trusted automated AI evaluations, offering significant benefits but with a few limitations that future research will address.\nFirst, our work is limited to the baselines we compare, as non-LLM citation methods, such as semantic similarity, were not included. Including such techniques could serve as valuable baselines to further validate our model's generalization and effectiveness. Second, the need for post-generation explanations and citations may introduce latency. Future work will compare latencies across models and explore optimizations to improve speed. We are also considering fine-tuning the snippet output using starting and ending character numbers to optimize for latency. Finally, even though the 12B base model is capable of generating multi-lingual output, our current evaluations are limited to English datasets. Expanding to multilingual evaluations is a key goal for future research."}, {"title": "Ethical Considerations and Risks", "content": "Our contributions present a novel general-purpose autoevaluator that supports various modes of citations, as well as a dataset with citations and explanations to support further research. There are a few ethical considerations for users of such technology. As with the use of fine-tuning and RAG, it is important to be aware of potential propagating biases and feedback loops. Biased data used for training and/or retrieval could lead to model outputs that reinforces such biases, impacting model quality and fairness. Although we extensively evaluated our LLM autoevaluator and found it to be safe, unbiased, and factual, users should consider whether such considerations hold true for the input source itself. Of note, even established sources such as knowledge banks may contain inaccurate or inconsistent information. Finally, though we aimed to be transparent and consistent for our data labeling tasks, using clear instructions and validations, note that data labeling tasks are inherently subject to the bias of labeler background and experiences."}, {"title": "A Various RAG Citation Modes that the REC Models Support", "content": "Here are example output templates of the different citation modes that our model supports and has been evaluated on. Users can easily process information from the JSON objects into the corresponding natural language format they desire. For all of these output styles, here's the example response and context that we've used to feed into the model.\n\u2022 Retrieved Context:\nID 1233\nPhotosynthesis is the process by which green plants and some other organisms utilize sunlight to synthesize their food. This remarkable process involves the conversion of carbon dioxide and water into glucose and oxygen, facilitated by chlorophyll. It serves as the foundation for energy production in these organisms and plays a crucial role in maintaining the balance of oxygen in our atmosphere.\nID 1422\nCellular respiration refers to a series of metabolic reactions that occur within the cells of organisms. This process transforms biochemical energy from nutrients into adenosine triphosphate (ATP), which is essential for cellular functions. During cellular respiration, waste products are released, and oxygen is a vital element needed for the process to function efficiently. This energy conversion is critical for sustaining life.\nID 4431\nDNA replication is the mechanism by which a cell duplicates its DNA in preparation for cell division. This process ensures that genetic information is accurately copied and passed on to daughter cells. The enzyme DNA polymerase is instrumental in DNA replication, playing a crucial role in maintaining genetic fidelity and continuity across generations.\n\u2022 LLM Generated Response to Cite:\nPhotosynthesis is a process that converts carbon dioxide and water into glucose and oxygen using sunlight and chlorophyll. This process occurs in green plants and certain other organisms."}, {"title": "A.1 Post-fix citation", "content": "{\n \"citations\": [\n {\n \"context_id\": \"1233\"\n }\n ]\n}"}, {"title": "A.2 Post-fix citation with snippet extracted verbatim from retrieved context", "content": "{\n \"citations\": [\n {\n \"context_id\": \"1233\",\n \"snippet\": \"Photosynthesis is the\n process by which green plants and\n some other organisms utilize\n sunlight to synthesize\n their food. This remarkable\n process involves the\n conversion of carbon\n dioxide and water into\n glucose and oxygen,\n facilitated by chlorophyll\"\n }\n ]\n}"}, {"title": "A.3 Inline citation with claims extracted verbatim from LLM generated response", "content": "{\n \"citations\": [\n {\n \"context_id\": \"1233\",\n \"claim\": \"Photosynthesis is a process\n that converts carbon dioxide\n and water into glucose and\n oxygen using sunlight and\n chlorophyll.\"\n }\n ]\n}"}, {"title": "A.4 Inline citation with snippet", "content": "{\n \"citations\": [\n {\n \"context_id\": \"1233\",\n \"claim\": \"Photosynthesis is a process\n that converts carbon dioxide\n and water into glucose and\n oxygen using sunlight and\n chlorophyll.\",\n \"snippet\": \"Photosynthesis is the\n process by which green\n plants and some other\n organisms utilize sunlight\n to synthesize their food. This\n remarkable process involves\n the conversion of carbon\n dioxide and water into\n glucose and oxygen,\n facilitated by chlorophyll\"\n }\n ]\n}"}, {"title": "A.5 Prompt for Inline Citation", "content": "Below is the prompt that we used to generate inline citations. This can be modified to generate responses in each of the different RAG citation modes shown in the sections above.\nYour task is to provide\ncitations for a generated response\nfrom an LLM for a RAG\n(Retrieval Augmented Generation)\napplication. You will be provided two\nsections below. The RETRIEVED CHUNKS\nsection is the context from\nwhere you will generate citations.\nThe LLM GENERATED ANSWER section\nis for the LLM generated answer\nfrom where you will generate claims.\n###\nRespond using the following JSON format:\nDesired format:\n{\n \"citations\":[\n {\n \"context_id\":\"<context_id>\",\n \"claim\": \"<claims identified from\n LLM GENERATED ANSWER>\"\n }\n ]\n}\n###INSTRUCTIONS\nIn order to generate citations,\nfollow these steps below:\n1. Review the \"LLM GENERATED ANSWER\"\nsection below and extract a set of claims,\nVERBATIM and set the \"claim\" in the output\njson.\n2. Your task is to generate a citation\nfor each of the claims identified\nin Step 1, by following the steps below:\ni. For each claim, identify the\nmost relevant chunk and the corresponding\n\"context_id\" from the \"RETRIEVED CHUNKS\"\nsection that supports the claim.\nii. For the identified chunk,\nset the \"context_id\" in the output\nto the corresponding context_id.\nSet \"context_id\" to \"None\" if none\nof the chunks in the \"RETRIEVED CHUNKS\"\nsection supports the claim.\n3. Ensure that you structure your\nresponse to adhere to the desired\noutput JSON format.\n###\nRETRIEVED CHUNKS:\n{retrieved_chunks}\n###\nLLM GENERATED ANSWER:\n{answer}\n###\nResponse (JSON only):"}, {"title": "B REC-Data Details", "content": "For general instruction tasks, we include summarization on ABCD (Chen et al., 2021), a dataset containing customer support dialogue transcripts, in five languages. For pointwise evaluation, we leverage HelpSteer2 (Wang et al., 2024c) and perform synthetic data generation (Section 3.2) on the (dialogue, summary) pairs from ABCD, and QA pairs from FloDial. For pairwise and open-ended evaluation tasks, we use Auto-j (Li et al., 2023a), Shepherd (Wang et al., 2023b), Skywork \u2074, HelpSteer2\u2075, OffsetBias\u2076, and Code Preference\u2077. For citations, we perform synthetic data generation (Section 3.2) on FloDial for both RAG citations and content quality citations, and on ABCD, Auto-j, Shepherd, and Email Thread Summary \u2078 for content quality citations.\nNote that it's possible to derive different tasks from the same dataset when different task prompts are assigned to an LLM. We provide a detailed breakdown of REC-Data in Figure 3."}, {"title": "C Synthetic Data Generation Prompt Example", "content": "C.1 Pointwise Evaluation (\"Faithfulness\nmetric\")\nYou will be given a source text given to another\nLLM, and that LLM's answer.\nYour task is to rate that LLM's answer\non one quality metric.\nPlease make sure you read and understand\nthese instructions carefully. Please\nkeep this document open while reviewing,\nand refer to it as needed.\nEvaluation criteria:\nmetric_name = 'Faithfulness'\nmetric_scale = 'Yes/No'\nmetric_description =\n\"\"\"\nThe generated answer only contains truthful\ncontent, and does not contain invented\nor misleading facts that are\nnot supported by the context.\n\"\"\""}, {"title": "C.2 Rating, Explanation, and Citation Evaluation Prompt", "content": "You will be asked to provide\nfeedback on one quality metric for a task\nprompt given to another LLM and that\nLLM's output. Given the task prompt as a\ncontext, your job is to give feedback on\nwhether there are any errors in the output,\nbased on the quality metric. If there\nare no errors in the output, please\nalso provide your feedback that there are\nno errors. Remember your main goal is to\nprovide feedback so that another LLM can\nuse your feedback to improve its generation.\nYou need to support statements in your\nfeedback that can be linked back to ANOTHER\nLLM'S TASK PROMPT with citations.\nPlease make sure you read and understand\nthese instructions carefully. Please keep\nthis document open while reviewing, and\nrefer to it as needed.\nPlease only generate your answer following\nthe Output JSON format. Do not generate\nanything else.\nEvaluation criteria:\n{metric_name} ({metric_scale})\n{metric_description}\nRespond using the following JSON format based\non the provided ANOTHER LLM'S TASK PROMPT and\nANOTHER LLM'S OUTPUT:\nDesired format:\n{\n \"answer\": <answer>,\n \"feedback\": <generated feedback>,\n \"statements\":[\n {\n \"statement_string\": <one statement\n extracted from feedback>,\n \"citations\":[\n {\n \"snippet\":<snippet>\n }\n ]\n }\n ]\n}"}, {"title": "D Instruction for Claim + Citations Labeling", "content": "We've asked an LLM to evaluate another LLM's 'Answer' for a summarization task given a 'Query' and 'Context'. The evaluator LLM has generated explanations for its evaluation with supporting citations, as shown in the 'metric_full_explain', 'Claim' (parts extracted from the explanation) and 'Citations' columns in a file separately.\nD.1 Instructions\nYou have two tasks to complete:\n1. Evaluate the quality of the 'Claim' gener-"}, {"title": "E Prompt for LLM-AggreFact Eval", "content": "Determine whether the provided claim is consistent\nwith the corresponding document. Consistency in\nthis context implies that all information presented\nin the claim is substantiated by the document.\nIf not, it should be considered inconsistent.\nDocument: {doc}\nClaim: {claim}\nPlease assess the claim's consistency with the\ndocument by responding with either \"yes\" or \"no\".\nAnswer (yes no only):"}]}