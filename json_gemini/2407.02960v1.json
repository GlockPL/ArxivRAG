{"title": "ObfuscaTune: Obfuscated Offsite Fine-tuning and Inference of Proprietary LLMs on Private Datasets", "authors": ["Ahmed Frikha", "Nassim Walha", "Ricardo Mendes", "Krishna Kanth Nakka", "Xue Jiang", "Xuebing Zhou"], "abstract": "This work addresses the timely yet under- explored problem of performing inference and finetuning of a proprietary LLM owned by a model provider entity on the confidential/private data of another data owner entity, in a way that ensures the confidentiality of both the model and the data. Hereby, the finetuning is conducted offsite, i.e., on the computation infrastructure of a third-party cloud provider. We tackle this problem by proposing ObfuscaTune, a novel, efficient and fully utility-preserving approach that combines a simple yet effective obfuscation technique with an efficient usage of confidential computing (only 5% of the model parameters are placed on TEE). We empirically demonstrate the effectiveness of ObfuscaTune by validating it on GPT-2 models with different sizes on four NLP benchmark datasets. Finally, we compare to a naive version of our approach to highlight the necessity of using random matrices with low condition numbers in our approach to reduce errors induced by the obfuscation.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) such as GPT-4 (Achiam et al., 2023) are increasingly used due to their state-of-the-art performance in diverse tasks and productivity benefits (Noy and Zhang, 2023). While LLMs excel in zero-shot and few- shot predictions with in-context learning (Mann et al., 2020), finetuning them on domain-specific data can significantly outperform foundation mod- els in tasks like chip design(Thakur et al., 2023; Wu et al., 2024; Liu et al., 2023).\nModel providers keep their proprietary models private due to the exorbitant costs of training them\u00b9. To enable their users to customize or apply the pro- prietary models to their data, model owners provide finetuning and inference services, e.g., OpenAI\n1 Training GPT-4 costed more than $100M (Knight, 2023)\nfinetuning API\u00b2 and GitHub Copilot\u00b3 respectively. Hereby, the users have to share their data with the model owners to use these services. Due to con- cerns of privacy leakage and competitive disadvan- tage, several users and commercial entities are not willing to share their private or confidential data. For e.g., Samsung banned the usage of ChatGPT after sensitive code was leaked (Ray, 2023). Hence, approaches that enable the inference and finetun- ing of proprietary LLMs of one stakeholder on the confidential/private data of another stakeholder in a privacy-preserving way are crucially needed.\nWe define the following requirement that poten- tial methods addressing this problem must fulfill: (a) Model confidentiality: prevent leakage of the proprietary model parameters, (b) Data confiden- tiality: prevent data leakage, (c) Utility: the perfor- mance and results of the inference and finetuning should be comparable with and without protection, (d) Efficiency: the computational time, memory footprint and communication should remain accept- able. To the best of our knowledge, no prior work fulfill all of these requirements simultaneously. In the following, we discuss different categories of prior works.\nPrior approaches based on differential privacy (DP) for inference (Igamberdiev and Habernal, 2023; Majmudar et al., 2022) and finetuning (Yu et al., 2021) focus on protecting the data. However, they do not provide any protection for the model parameters and incur significant utility losses (Req. (a) and (c) are not fulfilled). Another line of work uses cryptographic techniques, e.g., multi-party computation (MPC) and homomorphic encryption (HE) (Li et al., 2022; Liu and Liu, 2023). While the confidentiality of both the model and the data can be ensured, their substantial slowdown and communication costs are not suitable for real-time\n2https://platform.openai.com/docs/guides/fine-tuning\n3https://docs.github.com/en/copilot"}, {"title": "2 Method", "content": "We consider a problem setting involving three stakeholders: the model provider, the data owner and the cloud provider. The objective is to perform inference and finetuning of the proprietary LLM of the model provider on the confidential/private data of the data owner, in a way that ensures the confi- dentiality of both the model and the data. Due to the high computation and hardware costs required, we assume that the finetuning and/or inference is performed offsite, i.e., on the computational infras- tructure of the cloud provider. We assume that the cloud provider is honest-but-curious, i.e., they will perform their task correctly but will try to find extra information about the other parties assets and data.\nTo tackle this problem, we propose ObfuscaTune, an approach that addresses this problem by com- bining TEE and a simple yet effective obfuscation technique, ensuring model and data confidentiality while preserving utility. Following prior works, we consider the TEE as an isolated secure zone on a potentially adversary host where the data, code and computation processes used are inaccessible from outside (Hou et al., 2021; Huang et al., 2024).\n4 presents an overview of the ObfuscaTune approach, which we detail next.\nThe model protection is ensured as follows: the model provider sends the proprietary model to the TEE on the cloud provider infrastructure. Within the TEE, the highly parameterized attention and MLP layers are protected using our obfuscation technique that we detail later and then sent outside the TEE. Since large models do not fit inside the TEE, the model layers can be sent there batchwise to be protected before leaving it. The remaining low-parameterized layers, e.g., the input, output, normalization and dropout layers, are kept on the TEE. After these steps, all model parameters are protected, either by TEE or by the obfuscation, and the majority of model parameters are outside of the TEE. We note that the TEE is controlled by authen- tication that ensures that only the data owner can query the model. This prevents the cloud provider from querying the model to perform model steal- ing (Carlini et al., 2024) or embedding inversion attacks (Li et al., 2023; Morris et al., 2023).\nThe data protection in ObfuscaTuneis con- ducted as follows: The data owner sends an en- crypted batch of data directly to the TEE where it is first decrypted and then embedded using the model input layer. The resulting embedding is pro- tected by our obfuscation method before leaving the TEE. The text tokenization can be conducted either before or after transmitting the data on the data owner side or in the TEE, respectively.\nThe obfuscated feedforward pass through one transformer block is executed as follows: Outside the TEE, the obfuscated data embedding is passed through the obfuscated model layers yielding an ob- fuscated intermediate embedding that is sent back to the TEE. The latter is then de-obfuscated and passed through the corresponding model layers on the TEE, depending on the model architecture. Sub- sequently, the resulting embedding is obfuscated again and leaves the TEE to be fed to the next trans- former block. Finally, the output layer is applied in the TEE and the model output is sent back to the\n4Will be part of the additional page in the camera ready version upon paper acceptance."}, {"title": "3 Experimental evaluation", "content": "The conducted experiments aim to address the fol- lowing key questions: (a) What is the impact of ap- plying ObfuscaTune on utility, i.e., how do models finetuned with ObfuscaTune compare to the nor- mally finetuned models? (b) How does our obfus- cation method using orthogonal random matrices compare to naively using any random matrices?\nWe apply our method to GPT2 (Radford et al., 2019) models with different sizes, ranging from 117 million to 1.5 billion parameters. We imple- ment ObfuscaTune on top of the nanoGPT im- plementation (Karpathy, 2023). All our experi- ments perform LoRA-finetuning (Hu et al., 2022). Hereby, the LORA parameters are randomly ini- tialized and placed outside of the TEE. We apply LORA to all linear and attention layers. Further hyperparameters are specified in the appendix.\nIn each ObfuscaTune experiment, we use 2 GPU devices, one that is placed outside of TEE and an- other that simulates the TEE. We believe this is reasonable since high-end GPUs have TEE support (Apsey et al., 2023). We evaluate the finetuning with ObfuscaTune and with a naive version that uses any random matrices on 4 question-answering benchmark datasets, including WebQuestions (We- bQs) (Berant et al., 2013), OpenBookQA (OBQA) (Mihaylov et al., 2018), PIQA (Bisk et al., 2020) and SciQ (Welbl et al., 2017). We evaluate all models using 1m-eval-harness.\nTable 1 presents our main experimental results. We find that models finetuned with our method achieve a performance comparable to models fine- tuned without model and data protection. This observation is consistent across all model sizes and benchmark datasets. Besides, models that are finetuned with a naive method that uses arbitrary random matrices incur substantial utility loss due to the high accumulation of errors. Furthermore, we evaluate the impact of using random matrices with different condition numbers and empirically confirm that higher condition numbers deteriorate performance (Tab. 2, details in Appendix B).\nWe also measure the percentage of model param- eters present on TEE after model obfuscation to be 5.2% for GPT2-XL, which highlights a substantial efficiency increase compared to naively shielding the whole model inside the TEE. Finally, we mea- sure the runtime of the finetuning and find that using ObfuscaTune leads to a slowdown of 1.5x to 4.3x, for GPT2-small and GPT2-XL respectively. This is substantially lower than slowdowns yielded by cryptographic techniques, e.g., ca. 102 using MPC (Knott et al., 2021) and 105 using HE (Lou and Jiang, 2021) with significantly smaller models."}, {"title": "4 Conclusion", "content": "This work tackled the timely but underexplored problem of performing offsite inference and fine- tuning of a proprietary LLM owned by a model provider entity on the confidential/private data of another data owner entity, in a way that ensures the confidentiality of both the model and the data. Our proposed approach, ObfuscaTune, achieves this by combining a simple yet effective obfuscation technique with an efficient usage of confidential computing (only 5% of the model parameters are placed on TEE). Our extensive empirical evalua- tion on four NLP benchmark datasets and different models highlights the effectiveness of our method and emphasizes the importance of using random matrices with low condition numbers for preserv- ing high utility. In future work, we will investigate the effectiveness of our approach to RAG-systems."}, {"title": "5 Limitations", "content": "One potential limitation of our work is that despite testing on different models and datasets, we fo- cused on the same model architecture, i.e., GPT2. However, most of the other LLMs are composed on the same building blocks, which makes the appli- cation of our method to them straightforward. An- other limitation might be that while the slowdown incured by ObfuscaTune is substantially lower than other technologies, e.g., MPC and HE, it might still be unsuitable for some applications where effi- ciency has a higher importance than privacy"}, {"title": "A Hyperparameters", "content": "We train all models for 10 epochs. We perform validation at the end of every epoch and use early stopping with a patience of 3. We use a learning rate of 3e - 5 and a batch size of 1. We keep the other hyperparameters unchanged from (Karpathy, 2023). For LoRA, we use the hyperparameters: r = 16, \u03b1 = 32 and apply dropout with 0.05. We did not perform hyperparameter tuning, which highlights the robustness of our method. We did all experiments on middle-range GPUs. Each experiment took between less than 1 and 8 GPU hours, depending on he model size and dataset."}, {"title": "B Effect of the condition number", "content": "The condition number $\\kappa$ of a matrix $A$ is defined as $\\kappa(A) = \\frac{||A||}{||A^{-1}||} = \\frac{\\sigma_{max}(A)}{\\sigma_{min}(A)}$, where $M = max \\frac{||Ax||}{||x||}$ measures how much the mapping induced by that matrix can stretch vectors and $m = min \\frac{||Ax||}{||x||}$ measures how much it can shrink vectors. It determines how much a relative error in the input reflects on the out- put for solving linear systems, matrix inversion or matrix-vector multiplication (Golub and Van Loan, 2013). Such numerical errors get accumulated and increase with the number of sequential matrix mul- tiplication operations, i.e., the deeper the model the higher the accumulated error. We minimize the nu- merical errors by minimizing the condition number of the random matrix.\nIn this work, we consider the condition number w.r.t the $l_2$ norm. Since orthogonal matrices induce isometries, i.e $||Ax ||_2 = ||x||_2$ for all x, we get $\\kappa(A) = 1$ for every orthogonal matrix A. Note that singular matrices have the highest (worst) possible condition number, which is $\\infty$, since for a singular matrix A, m = min $\\frac{|| Ax||}{||x||} = 0$. On the other side, from the definition we see that the lowest possible $\\kappa$ is 1.\nLet $\\sigma_{max}(A)$ and $\\sigma_{min}(A)$ respectively be the largest and the smallest singular values of the ma- trix A. For the $l_2$-induced operator norm norm the following holds :\n$||A|| = max \\frac{|| Ax||}{||x||} = \\sigma_{max}(A)$.\nOn the other hand, for A square and non-singular\n$min \\frac{||Ax||}{||x||} = min \\frac{||y||}{||A^{-1}y||} = \\frac{1}{max \\frac{||A^{-1}y||}{||y||}} = \\frac{1}{||A^{-1}||} = \\frac{1}{\\sigma_{max} (A^{-1})} = \\sigma_{min}(A)$.\nFinally we get for every square and non-singular matrix A:\n$\\kappa(A) = \\frac{\\sigma_{max} (A)}{\\sigma_{min} (A)}$\nThe last equation makes it possible to generate ran- dom matrices R of a given predefined condition number $\\kappa(R)$. First we generate random matrices A and B using the standard normal distribution. We then apply QR-decomposition on A and B to"}]}