{"title": "OneEdit: A Neural-Symbolic Collaboratively Knowledge Editing System", "authors": ["Ningyu Zhang", "Zekun Xi", "Yujie Luo", "Peng Wang", "Bozhong Tian", "Yunzhi Yao", "Jintian Zhang", "Shumin Deng", "Mengshu Sun", "Lei Liang", "Zhiqiang Zhang", "Xiaowei Zhu", "Jun Zhou", "Huajun Chen"], "abstract": "Knowledge representation has been a central aim of AI since its inception. Symbolic Knowledge Graphs (KGs) and neural Large Language Models (LLMs) can both represent knowledge. KGs provide highly accurate and explicit knowledge representation, but face scalability issue; while LLMs offer expansive coverage of knowledge, but incur significant training costs and struggle with precise and reliable knowledge manipulation. To this end, we introduce OneEdit, a neural-symbolic prototype system for collaborative knowledge editing using natural language, which facilitates easy-to-use knowledge management with KG and LLM. OneEdit consists of three modules: 1) The Interpreter serves for user interaction with natural language; 2) The Controller manages editing requests from various users, leveraging the KG with rollbacks to handle knowledge conflicts and prevent toxic knowledge attacks; 3) The Editor utilizes the knowledge from the Controller to edit KG and LLM. We conduct experiments on two new datasets with KGs which demonstrate that OneEdit can achieve superior performance.", "sections": [{"title": "1 INTRODUCTION", "content": "The pursuit of empowering machines to master knowledge has remained a fundamental objective in the advancement of Artificial Intelligence (AI) systems. Over the years, researchers have devoted with various methods to enable machines to acquire knowledge, thereby supporting a wide range of tasks such as information retrieval [47], question answering [22], dialogue [60], reasoning [53], recommendation [44, 46], and domain specific applications [57]. Concretely, knowledge updating and management stand out as essential capabilities, empowering machines to adeptly adjust to new environments and tasks, thus facilitating lifelong learning [50, 52]. Early, Knowledge Graphs [20], as a form of symbolic knowledge representation, have garnered significant research interest from both academia and industry. KG is a structured representation of facts, composed of entities, relations, and semantic descriptions, which can be simply and precisely updated through symbolic manipulation. However, KG face challenges regarding scalability and the transferability of reasoning. On the other hand, Large Language Models (LLMs) have learned rich \"modaledge\" [14], potentially creating a kind of \"world model\u201d [49] and serving as parametric knowledge bases [36]. Based on the above hypothesis, researchers try to manupilate knowledge in LLMs and introduce knowledge"}, {"title": "2 RELATED WORK", "content": "Large Language Models. Typically, LLMs, such as GPT-4 [32] and LLAMA [43], usually denote Transformer-based models with hundreds of billions of parameters, trained on extensive text datasets [54]. Some work has suggested that LLMs can be regarded as parametrized knowledge bases [1, 25, 35, 37], as they are capable of recalling massive factual knowledge through prompt engineering [3, 6, 9, 39]. However, a major limitation of these parameterized knowledge bases is their inability to update stored information in real-time. Once pretrained, LLMs possess a fixed snapshot of knowledge reflecting the data they were trained on [27], and remain unable to incorporate new information unless retrained with updated datasets. This limitation diminishes the efficacy of LLMs in rapidly evolving sectors, such as current affairs, scientific developments, and cultural dynamics [11].\nKnowledge Graph. KGs are structured representations that map out complex networks of real-world entities and their interrelationships [35], facilitating advanced understanding and reasoning in NLP application with structured knowledge triples [4, 10, 13]. In KGs, symbolic knowledge leverages logical rules for reasoning, providing robust interpretability and precise inferential capabilities. Concretely, Knowledge Extraction (KE) [51] is essential for populating KGs from vast, unstructured datasets. This involves sophisticated NLP tasks such as Named Entity Recognition (NER), relation extraction, and entity resolution to accurately distill structured knowledge from texts [23].\nKnowledge Editing. The primary knowledge editing methods currently can be categorized into three groups [55]: meta-learning, locate-then-edit, and memory-based. Meta-learning methods employ external network to predict necessary gradient for editing, MEND [30] and MALMEN [41] utilizes a hyper-network to transform model gradients for updating model parameters. As to the locate-then-edit methods, ROME [28] and MEMIT [29] achieve edits by locating and modifying model parameters. For memory-based methods, the specific hidden states or neurons that store the edit knowledge are used for post-edit response, SERAC [31] leverages a scope classifier and trained sub-models for knowledge editing. Additionally, InstructEdit [42] focuses on general editing scenarios,"}, {"title": "3 SYSTEM DESGIN", "content": "As shown in Figure 2, OneEdit comprises three primary components: Interpreter, Controller, and Editor. The Interpreter serves as the interface for user interaction with OneEdit, responsible for discerning user intent. The Controller manages editing requests from various users, utilizing KG for conflict resolution and knowledge augmentation. The Editor primarily uses augmented knowledge triples from the Controller to edit KG and LLMs."}, {"title": "3.1 Knowledge Editing for LLMs", "content": "Suppose the original model is M and k is the knowledge that needs to be changed, by knowledge editing function E(), we obtain the post-edited model $M'$ which should override the prior erroneous memory about knowledge k while preserving other unrelated knowledge $k'$ as:\n$M' = E(M,k)$\n$M'(k) \\neq M(k)$\n$\\forall k' \\neq k, M'(k') = M(k')$\nWe hope by knowledge editing, we can:\nprecisely and generally manipulate knowledge in LLMs without impacting unrelated knowledge."}, {"title": "3.2 Neural-Symbolic Knowledge Editing", "content": "In this paper, we focus on simultaneously updating symbolic knowledge in KGs and parametric knowledge in LLMs (a.k.a., neural-symbolic knowledge editing), thus allowing different types of knowledge to complement each other and compensate for the challenges introduced by parameterized knowledge editing. Formally, a KG can be represented as G = (S, R, O), where S, R and O are sets of subjects, relation types, and objects. Each knowledge triple t"}, {"title": "3.3 Interpreter", "content": "The Interpreter functions as the interface between the user and the Controller, tasked with recognizing user intent expressed in natural language. If the user's intent is editing, the Interpreter converts the user's input into knowledge triples suitable for the KG and sends them to the Controller. If the user's intent is querying, the Interpreter takes no action and passes the input to the large language model for generation.\nWe conduct instruction fine-tuning on the MiniCPM-2B [18] to enable it to function as an Interpreter capable of distinguishing between editing intent and response intent. For generation intent data, we utilize the Alpaca dataset, as its instruction-following data involves everyday conversations without editing intent. We use the input from this instruction-following data as the model's generation intent data. For editing intent data, we manually created ten examples, using them as prompts for GPT-4 to generate similar but distinct editing intent data.\nWe combine the generation intent data and the editing intent data to train the model, enabling it to acquire both knowledge extraction and intent recognition capabilities."}, {"title": "3.4 Controller", "content": "In the Controller, the input is a knowledge triple. The Controller utilizes a KG as a knowledge base aligned with the model's parameters to identify and resolve conflicts between the input knowledge and the knowledge embedded in the model's parameters. After resolving conflicts, the KG is employed to augment the edited knowledge within the parameters of the large language model."}, {"title": "3.4.1 Conflict Resolution", "content": "In conflict resolution, we categorize conflicts into two types: coverage conflict and reverse conflict, which are the most common situations encountered in knowledge base management.\nCoverage Conflict. In practice, factual knowledge dynamically evolves in response to changes in the real world. For example, the answer to \"Who is the highest market value company in the United States?\" may vary significantly over a short period. To maintain consistency with the evolving real-world knowledge, it necessitates multiple coverage edits to the model. We formalize this situation as a pair of consecutive edits that contain the same subjects and relations but different objects:\nCoverage Conflict:\n$E_1 = (s, r, o) \\rightarrow (s, r, o_1)$\n$E_2 = (s, r, o_1) \\rightarrow (s, r, o_2)$\nHowever, research by [17] demonstrates that when modifying model parameters, the fundamental performance of the model inevitably degrades as the number of edits increases [5]. Additionally, [26] reveals that most current editing methods, such as FT, ROME, and MEMIT, leave residual knowledge from previous edits when repeatedly modifying the same piece of knowledge. This leads to the model's parameters storing multiple conflicting versions of the knowledge, ultimately causing Knowledge Distortion and affecting the model's expression of that knowledge.\nTo address this issue, we introduce the concept of edit rollback. When users perform knowledge editing on the LLM, the controller places the corresponding factual triple (s, r, o) into the KG for assessment. If there is no existing triple with the same subject and relation but a different object (s, r, o') in the KG, we write (s, r, o) into the KG, proceed with the corresponding knowledge editing, and store corresponding edit parameters in the edit cache (details in 3.5). If (s, r, o) already exists in the KG, no action is taken on the model. If (s, r, o') exists in the KG, we retrieve the edit cache concerning (s, r, o'), completely remove the previous edit within the model, rollback the edit concerning (s, r, o'), and re-edit to (s, r, o), updating the KG accordingly:\n$M' = E(M+ \\sum_{i=0}^{n} \\theta_i - \\sum_{i=0}^{m} \\theta_k), k \\in (0, n)$\nFurthermore, in the context of crowdsourced editing, if malicious edits are made to the model to produce harmful content, we can also identify and rollback those specific edits. Throughout this process, we perform at most one edit to the knowledge (s, r, o) and perfectly eliminate any previous edits, effectively minimizing performance loss in the model.\nReverse Conflict. When humans learn that Biden, not Trump, is the President of the United States, they naturally understand that the President of the United States is Biden. This generalization process is so seamless that it seems trivial. However, Berglund et al. [2] point out that large language models do not perform well in this regard, which is referred to as the \"reverse curse\". In knowledge editing, the reverse curse also exists. For instance, when we edit"}, {"title": "3.4.2 Knowledge Graph Augmentation", "content": "Li et al. [26] point out that editing a piece of knowledge can lead to the distortion of related parameterized knowledge within the language model. For example, after editing the knowledge of the United States President from Donald Trump to Joe Biden, the model might still answer \"The First Lady of the United States is Melania Trump\" (Trump's wife) when asked about the First Lady. Enhancing the edited knowledge to maintain the original knowledge structure within the model"}, {"title": "3.5 Editor", "content": "Even when performing knowledge editing on a 7B model, current major editing methods such as ROME, MEND, and GRACE require at least 30GB of VRAM and considerable editing time. In comparison, the memory overhead of storing model parameters after each edit is negligible. Based on this reality, we propose a space-for-time editing strategy, which involves storing the edit parameters after each knowledge editing. The edit parameters can then be utilized for subsequent edits or rollbacks, significantly reducing VRAM and time overhead.\n$M' = M + \\sum_{i=0}^{n} \\theta_i - \\sum_{j=0}^{m} \\theta_j$\nThe Editor is divided into two parts: the editor and the cache. In the editor part, we use EasyEdit [45], which provides a rich set of knowledge editing methods and supports a wide range of models, meeting our needs for knowledge editing. In the edit cache part, we have integrated the edit cache into EasyEdit. During each edit, we generate a unique edit key based on the knowledge and its corresponding edit parameters. Specifically, when using methods like ROME that directly modify model parameters, we store the parameters before the edit and the difference after the edit for the edited layers. For methods like GRACE that are based on adapters, we record the hidden states after each edit as the edit parameters. If the scenarios of coverage and rollback mentioned in Section 3.4.1 occur, we can directly use the stored edit parameters from the cache to quickly apply the edits or rollbacks by adding or subtracting them. This approach can reduce VRAM and time overhead."}, {"title": "4 EXPERIMENTS", "content": "Models and Baselines. We choose GPT-j-6B and Qwen2-7B as our base model. GPT-j-6B is a transformer model trained by Mesh Transformer JAX. The Qwen2-7B model, newly released by Alibaba, incorporates training corpora comprising multiple languages including English and Chinese. Both Qwen2-7B and GPT-J-6B are based on the transformer architecture and use causal language modeling objectives for pre-training. The baselines include methods for continual learning and model editing. We compare OneEdit against direct fine-tuning (FT) with an additional KL divergence loss, as well as ROME, MEMIT, and GRACE [15, 28, 29].\nMetrics. To evaluate the effectiveness of an editing method, we primarily consider three aspects: Reliability, Locality, and Portability. Reliability, as defined by Huang et al. [19], refers to a successful edit when the post-edit model $M'$ provides the edited target answer y for the prompt p. Reliability is measured as the average accuracy on the edited cases:\n$\\Ep,y~{(p,y)} 1 \\{ arg max M'(y | p) = y \\}$\nLocality, also referred to as Specificity in some works, denotes that editing should be implemented locally. This means that the post-edit model $M'$ should not alter the output of irrelevant examples in the out-of-scope set O(p, y). Therefore, locality is evaluated by the rate at which the post-edit model $M''s$ predictions remain unchanged compared to the pre-edit model M:\n$\\Ep,y~O(p,y) 1 \\{ M'(y | p) = M(y | p) \\}$\nPortability, as defined by Yao et al. [55], encompasses three components: Subject-Replace, One-hop, and Reverse. Portability is used to comprehensively evaluate the effectiveness of model editing in transferring knowledge to related content, termed robust generalization. Portability is calculated as the average accuracy of the"}, {"title": "4.1 Experiment Setting", "content": "edited model $M'$ when applied to instances in P(p, y):\n$\\Ep,y~P(p,y) 1 \\{ arg max M'(y | p) = y \\}$"}, {"title": "4.2 Dataset and Knowledge Graph Construction", "content": "Our experiments necessitate the integration of knowledge editing datasets with knowledge graphs. Due to the current lack of comprehensive datasets that combine knowledge graphs with knowledge editing, we decided to construct our own dataset for the experiments. We utilize two specific domains to demonstrate the feasibility and generality of OneEdit: American political figures and Academic figures. Our experimental dataset is based on Wikidata, zsRE, and GPT-4. Specifically, we first extracted approximately 500 relevant entities from Wikidata and used the factual knowledge corresponding to these entities to construct an initial knowledge graph. To ensure the completeness and accuracy of the knowledge graph, we meticulously verified and optimized each entity and relations. Additionally, we expanded their neighboring nodes using Wikidata, resulting in a high-quality knowledge graph. After constructing the knowledge graph, we used it as the foundation for building our experimental dataset. For the editing task, to ensure that the new knowledge was being edited into the model and not already present from pre-training, our editing data consisted of counterfactual information, which is opposite to the factual knowledge. We created counterfactual knowledge by replacing the ground truth object $o_t$ in the knowledge triples (s, r, $o_t$) with a new object $o_n$, and used manually written templates. This method ensures a high degree of consistency and relevance between the dataset and the knowledge graph."}, {"title": "4.3 Single-user Knowledge Editing Results", "content": "Our primary experimental conclusions are presented in Table 1. We observe that OneEdit performs comparably to other state-of-the-art methods in terms of reliability. When it comes to locality, OneEdit shows significant improvement over methods like ROME"}, {"title": "4.4 Multi-user Knowledge Editing Results", "content": "In this paper, we aim to simulate the scenario where multiple users collaboratively update the model. To achieve this, we adopt the sequential editing setup to simulate the process of multiple users updating the model [19, 59]. Specifically, we consider how to resolve conflicts when multiple users sequentially edit the same piece"}, {"title": "4.5 Knowledge Graph Augmentation Analysis", "content": "In OneEdit, the most critical parameter is the number of generation triples passed from the Controller to the Editor. However, OneEdit employs a nearest-neighbor strategy for selecting generation triples, which may lead to the exclusion of desired generalized knowledge, especially in dense knowledge graphs. This can result in multi-hop inference triples responsible for the edited knowledge being excluded from the generalized triples, causing a decrease in the One-Hop metric. In this section, we conduct experiemnts with varying the number of knowledge augmentation triples n to observe the resulting changes in the One-Hop metric for OneEdit (GRACE) and OneEdit (MEMIT) on the GPT-J-6B model.\nAs shown in Figure 3, we observe that when n is small, both OneEdit (GRACE) and OneEdit (MEMIT) underperform compared to their respective original methods, GRACE and MEMIT. We hypothesize that this discrepancy is due to the loss incurred during the conversion to triples. As n increases, multiple inference triples are incorporated into the edited sequence, and the values of both OneEdit (GRACE) and OneEdit (MEMIT) rise. However, when n becomes large, the results of OneEdit (GRACE) plateau, while the results of OneEdit (MEMIT) decline. We attribute this to the fact that GRACE has stricter rules for recalling the edited knowledge, whereas MEMIT's batch edit struggles to accurately recall the necessary edited knowledge from the extensive edited knowledge base."}, {"title": "4.6 Logical Rules Analysis", "content": "In Section 3.4.2, we have discussed how to obtain knowledge augmentation triples for the edited knowledge and then perform logical augmentation based on the semantic rules of their relationships. This process expands the triples and enhances the model's logical reasoning ability for the edited knowledge. To evaluate the model's logical reasoning ability, the most important metric is One-Hop, which assesses whether the model can use the newly edited knowledge to answer multi-hop reasoning questions related to the edited knowledge.\nIn this section, we demonstrate the effectiveness of adding logical rules in knowledge editing by comparing the One-Hop metric results with and without logical rules. Experimental results on Qwen2-7B and GPT-J-6B indicate that the model has poor logical generalization ability for edited knowledge, merely memorizing the edited knowledge mechanically without proper utilization. However, as shown in Figure 4, after adding logical rules and leveraging the logical reasoning advantages of symbolic knowledge to assist in modifying model parameters, the results significantly improve, with GPT-J-6B and Qwen2-7B showing improvements. This demonstrates that symbolic logical rules enhance the model's ability to utilize the edited knowledge effectively."}, {"title": "4.7 Computation Resource Analysis", "content": "In this section, we analyze the average time and memory overhead per edit associated with OneEdit. As shown in Table 3, experiments involving GPT-2 XL were conducted on an NVIDIA 3090, with the interpreter and editor deployed on separate GPUs. For GPT-J-6B and Qwen2-7B, experiments are similarly performed on an A800 machine, with both the interpreter and editor allocated to different GPUs. Compared to MEMIT and GRACE, OneEdit (MEMIT) and OneEdit (GRACE) require approximately 6GB of additional memory. This increase is primarily due to the memory overhead introduced by the interpreter. Regarding time overhead, we assess the editing duration with configurations of two and three users. Given the negligible time required for OneEdit's rollback process, we focuse solely on the time needed for a single edit. Our observations indicate that in scenarios with two and three users, OneEdit achieve a 40% and 70% reduction in time, respectively. This improvement is attributed to OneEdit's rollback mechanism, which enables the reuse of previous edits when repeatedly modifying the same piece of knowledge."}, {"title": "4.8 Case Study", "content": "In this part, we present two cases within OneEdit to specifically illustrate how our system addresses the two types of conflicts mentioned in Section 3.4"}, {"title": "4.8.1 Coverage Conflict Case", "content": "In the context of the coverage scenario, we present a real-world example: following the 2020 U.S. presidential election, the president changed from Trump to Biden. In the controller, we removed the knowledge parameter indicating \"the U.S. president is Trump\" and updated it with the new information that \"the U.S. president is Biden.\" However, if Trump were to win the election again in 2024, we could directly revert the knowledge that \"Trump is the U.S. president\" back into the model. This approach reduces the number of edits required to the model and maintains its baseline performance."}, {"title": "4.8.2 Reverse Conflict Case", "content": "In the reverse scenario, we also present a real-world example: Donald Trump divorced his wife Ivana Trump, who subsequently married Ricardo Mazzuchelli. If we only edit the model to reflect that \"Ivana Trump's husband is Ricardo Mazzuchelli\", it would lead to the absurd situation where \"Ivana Trump's husband is Ricardo Mazzuchelli\" while \"Donald Trump's wife is Ivana Trump\". However, the OneEdit controller module automatically constructs inverse relationships for such reversible scenarios. When \"Ivana Trump's husband is Ricardo Mazzuchelli\" reappears, it conflicts with the automatically constructed relationship \"Ivana Trump's husband is Donald Trump\" in the controller. This prompts the model to roll back the outdated knowledge and correctly update the inverse relationship of the new knowledge."}, {"title": "5 CONCLUSION AND FUTURE WORK", "content": "In this paper, we propose OneEdit, a neural-symbolic knowledge editing system that continuously updates symbolic knowledge in KG and the neural knowledge in LLM. OneEdit can advance domain-specific knowledge injection and alignment between the human semantic space and the latent semantic space of LLMs. In the future, we will extend the application scope of OneEdit to encompass a broader range of methods."}, {"title": "6 LIMITATIONS", "content": "Our work is still quite preliminary and has the following limitations: First, due to current computational power limitations, this system has only been tested on small pre-trained language models and small-scale KGs, which inevitably may affect its general capabilities. Currently, the natural language instructions that can be recognized are also quite limited. This system OneEdit is merely a prototype and there is significant room for improvement in the future. Additionally, we have only considered factual knowledge, with no support for commonsense, multimodal data, etc., and there is still substantial room for improvement in generalization capabilities. Furthermore, the system's security is still at a rudimentary stage, and measures to prevent malicious misuse that could lead to model tampering will need to be developed in the future."}]}