{"title": "Learning to Transfer Human Hand Skills for Robot Manipulations", "authors": ["Sungjae Park", "Seungho Lee", "Mingi Choi", "Jiye Lee", "Jeonghwan Kim", "Jisoo Kim", "Hanbyul Joo"], "abstract": "We present a method for teaching dexterous ma-nipulation tasks to robots from human hand motion demonstra-tions. Unlike existing approaches that solely rely on kinematicsinformation without taking into account the plausibility of robotand object interaction, our method directly infers plausiblerobot manipulation actions from human motion demonstra-tions. To address the embodiment gap between the humanhand and the robot system, our approach learns a joint motionmanifold that maps human hand movements, robot handactions, and object movements in 3D, enabling us to infer onemotion component from others. Our key idea is the generationof pseudo-supervision triplets, which pair human, object, androbot motion trajectories synthetically. Through real-world ex-periments with robot hand manipulation, we demonstrate thatour data-driven retargeting method significantly outperformsconventional retargeting techniques, effectively bridging theembodiment gap between human and robotic hands.", "sections": [{"title": "I. INTRODUCTION", "content": "Recent advances in imitation learning (IL) via expertdemonstrations have significantly improved dexterous ma-nipulation with multi-fingered robotic hands [1]-[5]. Thesedemonstrations typically come from robotic teleoperation,where a human teleoperates a robot hand using motioncapture gloves or a vision-based hand estimation module.While these methods provide physical plausibility of thecollected data, collecting demonstrations via teleoperationis often costly, time-consuming, and requires sophisticatedskills to operate the robot hardware, which can vary inperformance across operators due to the structural limitationsof the system. In contrast, demonstrations via human motioncapture offer a more natural and convenient alternative.Recent advances in vision-based methods promise moreaccessible solutions [6]\u2013[8], allowing users to perform taskscasually and potentially generating a larger volume of data.Motion capture data also provide rich hand information abouthow to manipulate the object, which is crucial for dexterousmanipulation.\nHowever, transferring human motion demonstrations torobots is not straightforward due to the embodiment gapbetween human and robotic hands. Differences in skeletalstructure, hand size, and forces that can be applied presentsignificant challenges in directly applying imitation learn-ing strategies. Consequently, traditional retargeting methodsthat transfer human demonstrations to robotic hands via,for example, kinematics-based [1], [5], [9] alignment oftenyield suboptimal results, leading to task failures. An idealretargeting should be generalizable to different hand/objectmotions to benefit from the scalability of human motion"}, {"title": "II. RELATED WORK", "content": "Robotic Teleoperation. In recent years, robotic teleoperationhas emerged as a common data source for robotics. Severalwork proposed a teleoperation system across different roboticplatforms, ranging from a dexterous robot hand [1]-[4],mobile robot [10], bimanual robot [11]-[14], and a humanoidrobot [15]-[17]. Such systems often consist of a perceptionmodule which estimates human teleoperator's motion and aretargeting algorithm that kinematically maps human motionto robot actions. While teleoperation data contain rich infor-mation for training robotic policy via imitation learning, ithas two major disadvantages: a necessity of hardware system(i.e., robot and teleoperation device such as VR devices [11],[18],exoskeleoton [12] and additional linkage system [19])and an embodiment gap between human teleoperator and therobot. Among the two, the latter often receives less attentionwhile significant. Specifically, as the human teleoperateorindirectly interacts with the environment through the system,the teleoperator should fill the embodiment gap via visualfeedback(i.e., see whether the robot is interacting with theenvironment in a desired way) during data collection. If the robot is not acting as intended, the teleoperator shouldimplicitly adapt the teleoperation strategy over time, makingteleoperation less scalable when it comes to complex manip-ulation tasks.\nRetargeting Human Hand to Robot Hand. Retargetinghuman's motion or interaction with the environment to thatof robot has been a challenging research topic [20]-[24]. Themost relevant to ours is retargeting human hand motion torobot hand. [1], [3], [5], [9] use an optimization algorithmvia kinematic loss, which aims to match certain humanjoints' position(e.g. fingertips) with that of correspondingrobot joints. Temporal consistency loss and self-collisionloss are also optionally taken into account. [2] learn anenergy model instead of performing optimization witha similar learning objective. While such methods work wellfor teleoperation, they may fail when the aim is to reproducethe interaction between human hand and the object, asmatching joint positions does not necessarily result in sameinteractions. To consider the interaction between human handand the object, [25] additionally considers contact heatmapgiven human hand and object mesh, so that the retargetedrobot hand can grasp the same object similar to human, whilebeing limited to grasping. [26], [27] also consider contactregions between human hand and object and try to matchit within target robot and same object, but requires humanexpert labels, such as the center of contact region within thetarget robot, corresponding points between human hand androbot, etc. Although matching contact regions may result ina more physically plausible robot motion along the object,it does not guarantee the contact region from human mocapto be perfectly matched, and even so, the embodiment gapbetween human hand and robot hand may result differentoutcomes to the target object. In short, prior work implicitlyassumes that matching kinematical constraints or contactregions would result in same robot-object interaction, whichmay not always hold. Our work differs from previous workthat we directly aim to reproduce the interaction withoutsuch assumptions. Specifically, we learn a retargeting modelwhich outputs robot actions that can achieve the same objecttrajectory given from human mocap data when executed.\nDexterous Manipulation. Several work have leveraged hu-man mocap data to learn a dexterous robot hand policy, either"}, {"title": "III. METHOD", "content": "We propose a learning-based framework that transforms human hand manipulation demonstrations into a sequence of robotic actions, enabling the robot hand to accurately imitate the same manipulation task. Specifically, our method takes as input a target object trajectory $O \\in \\mathbb{R}^{o\\times t}$ and a human hand demonstration $H \\in \\mathbb{R}^{h\\times t}$, and it outputs the corresponding plausible robot action trajectory $R\\in \\mathbb{R}^{r\\times t}$:\n$R = F(O, H)$ (1)\nwhere $t$ represents the number of timesteps, $o = 9$ (3 for position, 6 for rotation), $h = 81$ (3 for human wrist position, 6 for human wrist rotation, and 72 = 24 \u00d7 3 for each finger joint's 3D position w.r.t. human wrist frame), and $r = 25$ (3 for desired robot wrist position, 6 for desired robot wrist rotation, and 16 for desired robot hand joint angles), respectively. We use the 6D rotation representation [32].\nOur framework $F$ is built by learning the joint spatio-temporal manifold over $O$, $H$, and $R$ by training a convo-lutional autoencoder model [33], [34]:\n$(O, H, R) \\approx dec (\\Psi_{enc} (O, H, R))$, (2)\nwhere $\\Psi_{enc}$ and $\\Psi_{dec}$ are the 1-D temporal convolution encoder and decoder applied to the concatenated triplet $(O, H, R)$. The encoded bottleneck layer $\\Psi_{enc} (O, H, R) = L$ represents the manifold latent code modeling the correla-tions among human hand, robot action, and target object tra-jectory during manipulation. This learned manifold enables us to estimate missing components through an optimization-based framework. For example, given human hand motion $H$ and target object trajectory $O$, we infer the corresponding plausible robot hand trajectory $R$ by optimizing the latentcode $L$ as follows:\n$L^* = \\arg \\min_L || \\Psi_{dec}^{O} (L) \u2013 O ||^2,$ (3)\nwhere $\\Psi_{dec}^{O}$ is the object trajectory component decoded from the manifold latent code $L$, which is compared to the desired object trajectory $O$. Once the optimal $L^*$ is found, the desired robot hand motion $R$ can be computed as:\n$R = \\Psi_{dec}^{R}(L^*)$, (4)\n, by applying the decoder and extracting the robot handcomponent from the output, denoted as $\\Psi_{dec}^{R}$. Since thelatent code optimization Eq. 2 is performed via a gradientdecent method, choosing a good initial $L_{init}$ is important.To achieve this, we first estimate an initial robot handmotion $R_{init}$ from human motion $H$ using a conventionalInverse Kinematics (IK)-based optimization, by matchingrobot fingertip positions to human fingertip positions. Wethen use this as the input for the encoder to initialize thelatent code: $L_{init} = \\Psi_{enc}(O, H, R_{init})$.\nTo learn the manifold space over $(O, H, R)$, it is necessaryto collect paired data for the supervision, consisting ofhuman and robot hand motions that result in the same objectmanipulation. However, it is infeasible to obtain such dataset,as both demonstrations cannot be performed simultaneously.Our key insight is to synthesize plausible pseudo-ground-truth pairs, which we describe next."}, {"title": "A. Synthesizing Pseudo-GT Triplet DB.", "content": "To learn the manifold space described in Eq. 2, we needa dataset containing a set of triplets ${O_i, H_i, R_i}_{i=1}^n$. How-ever, collecting such a dataset is impractical. Our solutionis to synthesize corresponding human hand motion samples$H_i$ based on a collected robot teleoperation data ${O_i, R_i}$.Specifically, given a target object, we collect two separatedatasets: via human mocap manipulation demonstrations${O^M, H^M}$ and via teleoperation ${O, R_i}$. Then, webuild a framework $E$ to synthesize hand motion as follows:\n$H_i = E(O_i, R_i)$ (5)\nwhere our framework $E$ is composed of two modules:(1) a regressor $\\Omega$ to estimate an initial human hand motion$H_{init} = \\Omega(R_i)$, and (2) a manifold-based refinement process$\\Psi$ to improve the $H_{init}$ considering the object trajectory $O_i$,achieved via manifold learning similar to Eq. 2. We describeeach module below."}, {"title": "Learning The Manifold Space for ${O_i, H_i}$.", "content": "We first learnthe joint spatio-temporal manifold space over $O$ and $H$,similar to Eq. 2, by training an a temporal-convolutionalautoencoder model [33], [34] using the human mocap ma-nipulation dataset ${O^M, H^M}$:\n$(O, H) \\approx \\Psi_{dec} (\\Psi_{enc} (O, H))$ (6)\nwhere the bottleneck latent code $l = \\Psi_{enc} (O, H)$ capturesthe spatio-temporal correlation between object trajectory $O$and the corresponding hand motion $H$. Once trained, thismodel can be used to infer the corresponding hand motion$H_i$, given object trajectory $O_i$ by finding the optimal latentcode $l_i^*:$\n$l^* = \\arg \\min_l ||\\Psi_{dec}^{O}(l) \u2013 O_i||^2$ (7)\nwhere $\\Psi_{dec}^{O}$ is the object trajectory component decoded fromthe manifold latent code $l$. Once we obtain the optimal $l_i^*$,the desired human hand motion $H_i$ can be obtained as:\n$H_i = \\Psi_{dec}^{H}(l^*)$. (8)\nAs in optimization for Eq. 2, selecting a good initial latentcode $l_{init}$ is important. Thus, we first estimate the initial handmotion $H_{init}$ from robot action $R_i$, and apply it to the pre-trained encoder $l_{init} = \\Psi_{enc}(O, H_{init})$. However, unlike theprevious case where a traditional IK solver is used, here wetrain a neural regressor to estimate $H_{init}$."}, {"title": "Regressing Hand Motion from Robot Action.", "content": "One way toestimate the initial human hand motion $H_{init}$ from the pro-vided robot hand $R_i$ (obtained via teleoperation) is througha traditional Inverse Kinematics (IK), aligning correspondingfingertip and joint positions. However, we found this IKoptimization from robot hand to human hand unstable unlikethe opposite direction, human hand to robot hand, since thehuman hand typically has a higher degree of freedom withmore fingers. As a solution, we build a neural regressor$H_{init} = \\Omega(R)$, which we train on a dataset of paired humanand robot hand motions ${H^k, R^k_{IK}}$. To build the pairedDB, we compute the robot hand $R_{IK}^k$ from human hand $H^k$using traditional IK optimization, minimizing the differencebetween human fingertip and robot fingertip positions. Torepresent the human hand and robot hand, we include therotation and position of the wrist, and the position of thefinger joint and fingertips (24 positions for the human handand only 4 fingertips for the robot defined w.r.t. the wristframe). Then, we train the neural network regressor to predicthuman hand motion from robot action, $H_{init} = \\Omega(R).$\nOur regression model $\\Omega$ consists of 6 layers of multi-headself-attention [35] with 8 heads, each with an embeddingdimension of 256. $\\Omega$ operates per frame, converting eachrobot hand configuration to human hand pose."}, {"title": "Hand Motion Refinement.", "content": "While the initial estimate ofhuman hand motion $H_{init}$ produced by $\\Omega$ shows a certainlevel of visual plausibility, the quality is limited due to thelimited quality of the supervision derived from traditional IK,and, more importantly, its failure to account for interactionsbetween hand and object trajectories, by taking only robothand as the input. Our manifold-based optimization usingthe model of Eq 6 significantly enhances the quality ofhand motion synthesis, by capturing the relationship betweenhuman hand motion and object movement, resulting in thefinal output $H_i$.\nWhile motion manifold autoencoder Eq 6 is trained witha fixed window size, it can be applied to arbitrary lengths of${O_i, H_i}$ by applying it at each starting point in a slidingwindow fashion, and optimizing latent codes at all timewindow together via Eq 7, along with enforcing temporalconsistency of the overlapped output."}, {"title": "B. Hardware System Setup for Data Collection", "content": "To collect the required human mocap demo and robotteleoperation data, we build a multi-camera system with16 cameras paired with wearable motion capture devicesand gloves to capture 3D human body motion and objectmovements, as shown in Fig. 3 following the system of[36]. The 3D human body and hand motions cues areobtained from wearable mocap devices. The multi-camerasystem is used to track the 3D object movement by trackingthe attached aruco markers on the object and the gloves,as shown in Fig. 4, where the marker on the gloves arerequired to align the human motion and object in the same3D coordinate system. Multi camera system is synced andspatially calibrated. In our setup, objects, human hands,and robot arms and hands are located in the common 3Dcoordinate, with 30Hz capture frequency. After calibration,we collect human mocap demo and robot teleoperation datausing the same system. To perform robotic teleoperation, wedirectly use the teleoperator's wrist pose w.r.t. pelvis frameand hand joint angles acquired from the mocap device, as arobot action."}, {"title": "IV. EVALUATIONS", "content": "A. Experimental Setup\nWe choose three objects and corresponding tasks with dif-ferent characteristics to show the validity of our framework.Three objects and its corresponding tasks are as below.\nBottle. The robot must pick the bottle from a randomizedstarting location, and place it within the target locationwithout tipping it over . The bottle's diameter is sufficientlylarge such that a human hand cannot fully encompass it,whereas the robot hand is capable if enveloping the wholebottle due to larger hand size. Naively matching robot andhuman hand fingertips may result an unstable grasp. Bowl.\nThe robot must pick the bowl from a randomized startinglocation while maintaining upright rotation, and place itwithin the target location. The bowl's concave shape inducesa complicated contact interaction between human hand andobject hand, and a precise control is needed for the robot in"}, {"title": "B. Synthetic Paired Dataset Generation Model S", "content": "In this section, we verify our design choice for syntheticdataset generation pipeline.\nEffectiveness of human hand motion refinement. First, weevaluate the performance of human hand motion refinement,based on the following metrics.\nContact. Measured by the distance between human fingertipsor finger middle joints and closest object surface duringmanipulation. Middle joint refers to joint between distal andmiddle bone. Within all our tasks, human's all fingertips andmiddle joints are naturally in contact with the target objectduring manipulation."}, {"title": "C. Human-to-Robot Retargeting Model F", "content": "In this section, we evaluate our Retargeting Model $F$within the aforementioned three tasks, with the followingmetrics."}, {"title": "V. DISCUSSION AND LIMITATIONS", "content": "In this work, we developed a framework for learninga retargeting model which translates human mocap demoto a sequence of plausible robot actions for reproducingthe manipulation. Under a carefully designed pipeline, weachieve superior performance to baselines in multiple realworld dexterous manipulation tasks, even within noisy mocapdata. While our framework showed generalization capabili-ties along different trajectories within the same object, wehave separate models for each object. Moreover, we onlyuse object pose to represent each object. Learning a general,unified retargeting model along with rich representationinduced from mocap data (i.e., proximity between hand andobject) with a more scaled experiment will be an interestingfuture direction to pursue."}]}