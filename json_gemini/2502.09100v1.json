{"title": "Logical Reasoning in Large Language Models: A Survey", "authors": ["Hanmeng Liu", "Zhizhang Fu", "Mengru Ding", "Ruoxi Ning", "Chaoli Zhang", "Xiaozhang Liu", "Yue Zhang"], "abstract": "With the emergence of advanced reasoning models like OpenAI 03 and DeepSeek-R1, large language models (LLMs) have demonstrated remarkable reasoning capabilities. However, their ability to perform rigorous logical reasoning remains an open question. This survey synthesizes recent advancements in logical reasoning within LLMs, a critical area of Al research. It outlines the scope of logical reasoning in LLMs, its theoretical foundations, and the benchmarks used to evaluate reasoning proficiency. We analyze existing capabilities across different reasoning paradigms \u2014 deductive, inductive, abductive, and analogical and assess strategies to enhance reasoning performance, including data-centric tuning, reinforcement learning, decoding strategies, and neuro-symbolic approaches. The review concludes with future directions, emphasizing the need for further exploration to strengthen logical reasoning in AI systems.", "sections": [{"title": "1 Introduction", "content": "Logical reasoning is a fundamental challenge to artificial intelligence (AI) and natural language processing (NLP) [Newell and Simon, 1956; McCarthy and Hayes, 1981; McCarthy, 1959]. While early formal logic-based reasoning approaches faced limitations in scalability and adaptability [Pereira, 1982; Cann, 1993], data-driven models became the dominant method since the 1980s [McCarthy, 1989]. Recently, pre-trained Large Language Models (LLMs) and their emergent logical reasoning abilities have attracted increasing attention [Liu et al., 2023b; Xu et al., 2023]. Logical reasoning integrates LLMs with inference structuring, enabling multistep deduction and abstraction, and improving interpretability and reliability [Shi et al., 2021; Stacey et al., 2022; Rajaraman et al., 2023]. It also strengthens generalization, helping models handle novel scenarios beyond their training data [Haruta et al., 2020]. As LLMs become integral to domains like legal analysis and scientific discovery, ensuring the correctness and verifiability of their reasoning is increasingly vital. As a result, post-training LLM for reasoning has garnered a surge of interest in both industry and research[OpenAI, 2024; DeepSeek-AI, 2025; Muennighoff et al., 2025].\nDespite growing research, existing surveys [Plaat et al., 2024; Sun et al., 2023; Yu et al., 2024] often conflate logical reasoning with general-purpose heuristic strategies like Chain-of-Thought (CoT) [Xia et al., 2024]. There has been a lack of a literature review dedicated to LLM and formal symbolic logic. This survey provides a comprehensive review of logical reasoning in large language models (LLMs), with a focus on formal and symbolic logic-based reasoning rather than general heuristic approaches. We begin by defining logical reasoning in AI, distinguishing it from general-purpose reasoning, and categorizing key paradigms, including deductive, inductive, abductive, and analogical reasoning. Additionally, we analyze existing benchmarks and evaluation methodologies, identifying gaps in assessing symbolic inference, consistency, and robustness. We further explore state-of-the-art techniques for enhancing logical reasoning, such as instruction fine-tuning, logic-informed pre-training, reinforcement learning, inference-time decoding strategies, and hybrid neuro-symbolic methods. We examine recent advances in neuro-symbolic integration, along with applications of theorem provers, logic solvers, and formal verification frameworks in LLMs. Finally, we highlight open challenges in scalability, reasoning consistency, explainability, and efficiency, proposing future directions for multi-modal reasoning, hybrid architectures, and improved evaluation frameworks. The structure of the subsequent chapters is illustrated in Figure 1."}, {"title": "2 Logic in Artificial Intelligence", "content": "Logical reasoning is a cornerstone of artificial intelligence (AI), enabling machines to simulate human thought processes and solve complex problems. At its core, logical reasoning applies structured rules to derive conclusions from premises, providing a rigorous framework for decision-making and inference [Sun et al., 2023]."}, {"title": "2.1 History of Logic Reasoning Research", "content": "Logical reasoning can be traced back to ancient Greece, where Aristotle's syllogisms laid the foundation for classical logic. During the Middle Ages, scholars refined these theories, and in the 17th century, Leibniz's universal language and calculus ratiocinator bridged logic with mathematics, foreshadowing modern computational logic. The 19th century saw George Boole's Boolean algebra, which transformed logic into a mathematical framework, laying the foundation for digital computing.\nThe 20th century ushered in modern logic, with Russell and Whitehead's Principia Mathematica formalizing complex logical systems. By the mid-century, AI pioneers like John McCarthy leveraged logic for knowledge representation and automated theorem proving, leading to logic programming and knowledge bases. The 1970s introduced non-monotonic logic, enabling AI to handle commonsense reasoning. The 1980s saw logical reasoning integrate with knowledge representation, advancing expert systems for real-world applications. The 1990s saw the rise of knowledge graphs, structuring vast knowledge for complex reasoning tasks.\nIn the 21st century, neuro-symbolic approaches have merged deep learning with logical inference, resulting in tools like DeepLogic [Cingillioglu and Russo, 2019] and SAT-Net [Wang et al., 2019]. Logical reasoning remains a cornerstone of AI research, evolving from philosophy to modern computing. As AI advances, logical reasoning continues to shape intelligent systems, ensuring structured, interpretable, and robust decision-making."}, {"title": "2.2 Types of Logical Reasoning", "content": "Logical reasoning can be broadly categorized into four main types, each serving distinct purposes and applications:\nDeductive Reasoning. This type of reasoning derives specific conclusions from general principles or premises. It operates under the rule that if all premises are true and the reasoning is valid, the conclusion must also be true. For example, given the premises \"All apples are red\" and \"This fruit is an apple\" one can deduce that \"This fruit is red\" Deductive reasoning is fundamental in fields such as mathematics and formal logic, where certainty and rigor are paramount.\nInductive Reasoning. Unlike deductive reasoning, inductive reasoning draws general conclusions based on specific observations or evidence. While the conclusions are often considered probable, they are not guaranteed to be true. For instance, observing that all swans seen so far are white might lead to the inductive conclusion that \"All swans are white\" Inductive reasoning is widely used in scientific discovery and data-driven decision-making, where patterns and trends are inferred from empirical data.\nAbductive Reasoning. This form of reasoning seeks the most plausible explanation or cause for a set of observations, often in the presence of incomplete information. Abductive reasoning is particularly useful in diagnostic tasks and real-world problem-solving. For example, seeing wet spots on the street might lead one to infer that \"It has recently rained\" While abductive conclusions are not certain, they provide a practical basis for hypothesis generation and decision-making under uncertainty.\nAnalogical Reasoning. Analogical reasoning involves drawing comparisons between similar situations or domains to make inferences or solve problems. By identifying parallels between different scenarios, this type of reasoning enables creative problem-solving and knowledge transfer. For example, understanding that planets orbit the sun in elliptical paths might lead one to analogically reason that other celestial bodies, such as comets, exhibit similar orbital characteristics. Analogical reasoning is particularly valuable in fields like education, design, and innovation."}, {"title": "3 Tasks and Benchmarks", "content": "Logical reasoning datasets and benchmarks are essential for evaluating the reasoning capabilities of large language models (LLMs). These datasets can be categorized into three types based on their data sources:\nRule-based Datasets [Tafjord et al., 2021; Sinha et al., 2019] are automatically generated using logical rules, enabling large-scale data collection. However, ensuring diversity is crucial to avoid repetitive patterns and comprehensively evaluate reasoning capabilities.\nExpert-Designed Datasets [Han et al., 2024a] are constructed by domain experts, ensuring high precision and accuracy. Although typically smaller than crowd-sourced corpora, their meticulous design makes them indispensable for in-depth logical reasoning evaluation.\nExam-Based Datasets [Liu et al., 2021b; Yu et al., 2020; Wang et al., 2022] originate from standardized test questions (e.g., Chinese National Civil Service Exam, LSAT, GRE), offering high-quality, expert-crafted logic problems at scale. These datasets are widely used to evaluate reasoning in real-world scenarios.\nTable 1 summarizes important datasets for logical reasoning, which typically cover tasks such as Natural Language Inference (NLI) (\u00a73.1) and Machine Reading Comprehension (MRC) (\u00a73.2)."}, {"title": "3.1 Natural Language Inference (NLI)", "content": "NLI evaluates whether a hypothesis logically follows from a premise, directly assessing a model's reasoning ability. Labels typically fall into binary (Entailment, Non-entailment) or ternary (Entailment, Contradiction, Neutral) classifications. Some datasets use True and False labels instead.\nConTROL [Liu et al., 2021a] is derived from recruitment exams (e.g., bank entry, U.S. police selection), containing 8,325 entries with Correct, Incorrect, and Can't Say labels, corresponding to Entailment, Contradiction, and Neutral.\nFOLIO [Han et al., 2024a] is an expert-constructed dataset for First-Order Logic (FOL) reasoning, consisting of 1,351 entries labeled as True or False, making it a rigorous benchmark for formal logical inference.\nLogicNLI [Tian et al., 2021] contains 30K entries generated using logical rules, with Entailment, Contradiction, and Neutral labels. It isolates FOL-based inference from commonsense reasoning, enabling precise evaluation of reasoning accuracy and generalization.\nProofWriter [Tafjord et al., 2021] extends Rule-Taker [Clark et al., 2021] by introducing CWA (closed-world assumption) and OWA (open-world assumption) to handle negation and open-world reasoning. It includes Birds-Electricity (handcrafted domain theories) and ParaRules (crowdsourced paraphrased rules) for systematic evaluation of generalization across linguistic variations and real-world knowledge domains.\nLogicBench [Parmar et al., 2023] is a GPT-3-generated dataset covering 25 types of reasoning, including propositional logic, FOL, and non-monotonic logic. It consists of 1,270 test entries labeled as Yes or No."}, {"title": "3.2 Machine Reading Comprehension (MRC)", "content": "MRC evaluates logical reasoning by requiring models to answer questions based on a given passage. Tasks are commonly formatted as multiple-choice, span extraction, or free response, with multiple-choice QA being particularly effective due to its standardization.\nLogiQA [Liu et al., 2023a] is sourced from the Chinese Civil Service Exam, containing 15,937 entries in Chinese and English. It targets complex logical reasoning and is widely used for evaluating LLMs.\nReClor [Yu et al., 2020], derived from the GMAT, features 6,138 English entries with four-option multiple-choice questions.\nAR-LSAT [Wang et al., 2022] is based on the LSAT, containing 2,064 entries spanning ordering games, grouping games, and allocation games, each with five options.\nCLUTRR [Sinha et al., 2019] focuses on inductive reasoning, requiring models to infer kinship relationships in short narratives. It contains 6,016 entries, combining entity extraction and logical inference.\nGSM evaluates mathematical reasoning capabilities, comprising two datasets: GSM8K [Cobbe et al., 2021] (8.5K grade school math problems) and GSM-PLUS [Li et al., 2024a] (10,552), which is augmented with mathematical perturbations for robustness evaluation.\nLINGOLY [Bean et al., 2024] uses Linguistic Olympiad puzzles to evaluate in-context pattern identification and generalization in low-resource or extinct languages. It contains 1,133 problems across 6 formats and 5 difficulty levels, covering over 90 languages."}, {"title": "3.3 Benchmark Suites", "content": "Benchmark suites standardize evaluation and facilitate model comparison in logical reasoning research.\nGLORE [liu et al., 2023d] is a few-shot and zero-shot testing platform, including 17 test-only datasets to assess generalization in low-data scenarios.\nLogiGLUE [Luo et al., 2024] consists of 24 logical reasoning tasks, standardizing datasets into a sequence-to-sequence format for uniform input processing. It provides both test and training sets, enabling extensive model training and targeted evaluations.\nLogiTorch [Helwe et al., 2022] is a PyTorch-based library for natural language logical reasoning, offering 16 datasets, model architectures, and an accessible API for quick evaluation.\nBIG-bench [Srivastava et al., 2022] is a collaborative benchmark with 7 tasks dedicated to logical reasoning, such as Logic Grid Puzzle and Logical Fallacy Detection."}, {"title": "4 Evaluations", "content": "The rapid development of pre-trained language models (PLMs) necessitates rigorous evaluation of their logical reasoning capabilities. This section examines four reasoning paradigms-deductive, inductive, abductive, and analogical-while analyzing evaluation approaches and metrics."}, {"title": "4.1 Deductive Reasoning", "content": "Deductive reasoning, deriving specific conclusions from general premises, is crucial for automated theorem proving. Despite LLMs performing well on tasks like compositional proofs, standard benchmarks, and encoding entailment relationships, they struggle with extended reasoning, hypothetical sub-proofs without examples, generalization, and sensitivity to syntactic variations [Saparov et al., 2023; Yuan et al., 2023; Ryb et al., 2022]."}, {"title": "4.2 Inductive Reasoning", "content": "Inductive reasoning, which generalizes from specific instances to broader rules, is essential for tasks like hypothesis generation and pattern recognition. While Yang et al. [2024b] find that pre-trained models can serve as effective \"reason-ers,\" Bowen et al. [2024] show that even advanced LLMS struggle with simple inductive tasks in their symbolic settings. Similarly, Sullivan [2024] demonstrates that Transformer models, even after fine-tuning, fail to learn fundamental logical principles, indicating limited inductive reasoning capabilities."}, {"title": "4.3 Abductive Reasoning", "content": "Abductive reasoning, which seeks the most plausible explanations for observed phenomena, is crucial in fields like law and medicine. Del and Fishel [2023] highlights the challenges LLMs face in generating plausible hypotheses from incomplete information. In the legal domain, Nguyen et al. [2023] show that despite strong performance, models struggle with abductive reasoning, underscoring the complexity of this paradigm."}, {"title": "4.4 Analogical Reasoning", "content": "Analogical reasoning, which infers unknown information by comparing it with known information, is vital for tasks requiring creativity and knowledge transfer. Wijesiriwardene et al. [2023] introduced ANALOGICAL, a benchmark for long-text analogical reasoning. They find that as analogy complexity increases, LLMs struggle to recognize analogical pairs. Petersen and van der Plas [2023] show that models can learn analogical reasoning with minimal data, approaching human performance. However, Qin et al. [2024] question whether LLMs truly rely on analogical reasoning, discovering that random examples in prompts often achieve comparable performance to relevant examples."}, {"title": "4.5 Overall Analysis and Metrics", "content": "Liu et al. [2023b] evaluate GPT-4 and ChatGPT on benchmarks like LogiQA and ReClor, showing that while GPT-4 outperforms ChatGPT, both of them struggle with out-of-distribution tasks. Xu et al. [2023] introduce the NeuLR dataset and propose a framework evaluating LLMs across six dimensions: correctness, rigor, self-awareness, proactivity, guidance, and absence of hallucinations.\nMetrics for Evaluating Logical Reasoning. Traditional metrics like accuracy and F1 score are insufficient for assessing logical reasoning. Recent studies have introduced nuanced metrics such as consistency (invariance to logically equivalent inputs), generalization (performance on out-of-distribution data), and explainability (clarity of reasoning steps). Thatikonda et al. [2025] find that combining BERTScore with traditional metrics improves alignment with human judgments. Liu et al. [2024c] propose a framework for measuring logical consistency, showing that BERTScore aligns better with human rankings than LLM-based evaluators like GPT-4. Gandarela et al. [2024] emphasizes the need for metrics that account for the expressivity of logical theories, particularly in inductive reasoning."}, {"title": "5 Enhancement Methods", "content": "Enhancing LLMs' logical reasoning remains crucial. This section focuses on core strategies: Data-Centric Approaches (\u00a75.1), Model-Centric Approaches (\u00a75.2), External Knowledge Utilization (\u00a75.3), and Neuro-Symbolic Reasoning (\u00a75.4)."}, {"title": "5.1 Data-Centric Approaches", "content": "Data-centric approaches enhance LLMs' reasoning capabilities by utilizing meticulously curated training datasets. Formally, this can be expressed as:\nD* = arg max R(MD)\n        D\n(1)\nwhere:\n\u2022 D: training datasets.\n\u2022 MD: model trained on D.\n\u2022 R: performance evaluator (e.g., LLM-as-a-judge, rule-based metrics).\nThis formulation highlights the central role of dataset optimization in data-centric approaches. In practice, data-centric methods typically involve three types of datasets: expert-curated datasets, synthetic datasets, and LLM-distilled datasets.\nExpert-Curated Datasets. The FOLIO series [Han et al., 2024a; Han et al., 2024b] establish formal verification through FOL annotations, with P-FOLIO extending the complexity of reasoning chains for enhanced training. Lean-Dojo [Yang et al., 2023] provides 98k+ human-proven theorem pairs for mathematical reasoning. Additionally, Symbol-LLM [Xu et al., 2024a] systematically organizes 34 symbolic reasoning tasks to capture inter-symbol relationships across 20 distinct symbolic families.\nSynthetic Datasets. Rule-based synthetic data remains fundamental for data generation. RuleTaker [Clark et al., 2021] formalizes this through a three-phase pipeline: behavior formalization, example synthesis and linguistic equivalents generation. Similarly, Morishita et al. [2024] develops Formal Logic Deduction Diverse (FLD\u00d72), a synthetic dataset based on symbolic theory and previous empirical insights.\nLLM-Distilled Datasets. Researchers employ advanced models such as GPT-4 for intermediate reasoning step distillation. LogiCoT [Liu et al., 2023c] augments existing datasets with GPT4-generated reasoning chains, while LogicPro [Jiang et al., 2024] combines algorithmic problems with code solutions to create variable-guided reasoning data. To advance, Wang et al. [2024b] propose PODA, which generates contrastive analyses of correct/incorrect options through premise-oriented augmentation, enabling reasoning path differentiation via contrastive learning."}, {"title": "5.2 Model-Centric Approaches", "content": "Model-Centric approaches enhance LLMs' reasoning capabilities by optimizing model parameters and decoding strategies. The formal objective is:\n(0*, S*) = arg max R(M\u03b8, S)\n    \u03b8,S\n(2)\nwhere:\n\u2022 \u03b8: learnable model parameters.\n\u2022 M\u03b8: model with parameters \u03b8.\n\u2022 S: decoding strategy (e.g., chain-of-thought prompting, verification-based decoding).\n\u2022 R: reasoning performance metric.\nThis formulation highlights the joint optimization of model parameters \u03b8 and decoding strategy S. Practical implementations can be categorized as:\n\u2022 Instruction Fine-Tuning: optimizing \u03b8.\n\u2022 Reinforcement Learning: optimizing \u03b8.\n\u2022 Inference-Time Decoding: optimizing S.\nModel-Centric approaches focus on directly improving the model's reasoning capabilities by optimizing its internal mechanisms and decoding strategies, making them complementary to data-centric approaches.\nInstruction Fine-Tuning\nInstruction Fine-Tuning (IFT) adapts LLMs through supervised learning on task-specific instructions. For example, Liu et al. [2023c] design multi-grained instructions spanning diverse levels of abstraction and complexity. Similarly, Feng et al. [2024] IFT models to mimic logical solvers by replicating formal deduction reasoning processes. In addition, Xu et al. [2024a] implement two-stage symbolic fine-tuning through Injection (injecting symbolic knowledge) and Infusion (balancing symbol and NL reasoning).\nTo overcome IFT's over-fitting limitations, Wang et al. [2024b] enforce contrastive learning between factual/counterfactual paths with IFT. Further, Wang et al. [2024a] augment Llamas with a Program-Guided Learning Framework and logic-specific architecture adjustments.\nRecently, Muennighoff et al. [2025] propose s1, achieving test-time scaling through IFT on 1,000 meticulously crafted long CoT samples. Combined with budget-forcing technique, it significantly enhances the reasoning capability of a Qwen2.5-32B-Instruct model, allowing extrapolating beyond its performance without test-time intervention.\nReinforcement Learning\nReinforcement learning (RL) has become pivotal in optimizing large language models (LLMs), particularly since the breakthrough of Reinforcement Learning from Human Feedback (RLHF). Jiao et al. [2024] leverage RL for planning-based reasoning optimization, while Xi et al. [2024] develop R\u00b3, achieving process supervision benefits through outcome-only supervision.\nThe success of large-scale RL in OpenAI-01 [OpenAI, 2024] has inspired numerous studies. RL algorithms train 01-style models to enhance Chain-of-Thought (CoT) reasoning, addressing issues like formulaic outputs and limited long-form reasoning. For instance, Zhao et al. [2024] integrate CoT instruction fine-tuning with Monte Carlo Tree Search (MCTS) decoding for multi-path reasoning exploration. In contrast, Zhang et al. [2024] employ MCTS to generate code-reasoning data for instruction fine-tuning (IFT) and Direct Preference Optimization (DPO).\nA significant breakthrough comes from DeepSeek-R1 [DeepSeek-AI, 2025], which pioneers a novel RL strategy to enhance logical reasoning. DeepSeek-R1-Zero, trained purely through RL without IFT, demonstrates impressive reasoning capabilities but faces challenges in readability and language consistency. To address this, DeepSeek-R1 introduces minimal long-CoT IFT data as a cold start before RL, achieving a balance between usability and reasoning performance. By iteratively synthesizing high-quality reasoning data through RL, DeepSeek-R1 overcomes limitations imposed by human annotators, addressing issues such as mechanistic responses, repetitive patterns, and insufficient long-chain reasoning. This approach represents a potential paradigm shift in logical reasoning optimization, pushing the boundaries of what LLMs can achieve in structured reasoning tasks."}, {"title": "Inference-Time Decoding", "content": "We categorize logical reasoning enhancement methods during inference-time into inference-time scaling and constrained decoding.\nInference-time scaling employs computational augmentation without parameter updates. One common approach is decoding with structured outputs and modular workflows. GoT [Lei et al., 2023] creates structured reasoning nodes to improve complex multi-step logical reasoning. Similarly, Chain of Logic [Servantez et al., 2024] introduces a Decomposition-Recomposition structure for legal reasoning. In other contexts, researchers design more complex modular workflows for better performance [Creswell et al., 2023; Malon et al., 2024].\nAnother inference-time scaling approach involves stimulating autonomous reasoning, guiding LLMs to iteratively refine their answers. Maieutic Prompting [Jung et al., 2022] eliminates contradictions through recursive reasoning. Similarly, Logic-of-Thoughts [Liu et al., 2024a] and DetermLR [Sun et al., 2024] progressively approach the answers in an iterative style.\nConstrained decoding methods, on the other hand, focus on improving the controllability and reliability of reasoning processes. Neurologic [Lu et al., 2021] enforces predicate logic constraints, while Formal-LLM [Li et al., 2024b] integrates automata for constraining plan generation."}, {"title": "5.3 External Knowledge Utilization", "content": "LLMs often generate incorrect answers due to hallucinations when performing complex tasks such as logical reasoning, making it necessary to incorporate external knowledge to assist in producing accurate responses. Formally, the optimal integration of external knowledge can be formulated as a joint optimization problem:\n(M*, K*) = arg max R(M, K)\n            M,K\n(3)\nwhere:\n\u2022 M: the neural model, which includes both the model's parameters and its decoding strategies (generally, the model's parameters remain unchanged).\n\u2022 K: knowledge integration strategy, including knowledge source curation, structured knowledge representation, retrieval-augmented mechanisms, etc.\n\u2022 R: reasoning performance evaluator (e.g., factual accuracy, logical consistency).\nZayyad and Adi [2024] and Yang et al. [2023] extract data from Lean, a mathematical proof tool, to aid theorem proving. In contrast, \u201cLogic-Query-of-Thoughts\u201d (LQOT) [Liu et al., 2024b] decomposes complex logical problems into easier sub-questions before integrating knowledge graphs.\nIn reading comprehension, Ouyang et al. [2023] construct supergraphs to address complex contextual reasoning, while KnowRA [Mai et al., 2025] autonomously determines whether to accept external knowledge to assist document-level relation extraction."}, {"title": "5.4 Neuro-Symbolic Approaches", "content": "Neural-symbolic hybrid methods represent a burgeoning research area that aims to combine the powerful representational capabilities of deep learning with the precision and interpretability of symbolic reasoning.\nFormally, a neural-symbolic hybrid system aims to optimize both the neural model M and the symbolic solver P (where P represents the symbolic reasoning process) to maximize logical reasoning performance. The overall objective can be expressed as:\n(M*, P*) = arg max R(P(M(x))),\n              M,P\nwhere:\n\u2022 M: The neural model, which includes both the model's parameters and its decoding strategies. It maps the input x (e.g., natural language) into a symbolic representation z within a formal language L:\nz = M(x), z\u2208L.\n\u2022 P: The symbolic solver, which operates on the symbolic representation z produced by M to generate the final output y:\ny = P(z).\n\u2022 R: The reasoning performance metric, which evaluates the ability to perform logical reasoning tasks.\nThe optimization process involves two key directions:\n\u2022 Improving M: including refining the model's parameters and decoding strategies to produce symbolic representations that are both accurate and compatible with P.\n\u2022 Enhancing P: involving improving the symbolic solver's ability to process.\nBy jointly optimizing M and P, neural-symbolic hybrid systems aim to leverage the strengths of both neural networks and symbolic reasoning to achieve superior logical reasoning capabilities. It is worth noting that in earlier neural-symbolic pipelines, P is often implemented as a fixed external logical reasoning engine, and thus is generally not optimized. However, in advanced practice, LLMs are increasingly being used to perform the role of P, enabling diverse optimization.\nFundamentally, these methods involve translating problems into symbolic representations with LLMs, and external symbolic solvers solving them. For example, in LINC [Olausson et al., 2023], LLMs convert natural language (NL) into first-order logic (FOL) expressions, and utilize an external theorem prover for symbolic deductive inference.\nFurther efforts focus on improving NL-to-symbolic translation. One prevailing approach is directly optimizing translation through training [Yang et al., 2024a] or decoding strategies [Ryu et al., 2024], while the other depends on verification or correction mechanisms [Yang et al., 2024a; Pan et al., 2023].\nBuilding upon these, recent advancements address the traditional pipeline limitations by fully integrating LLMs into reasoning processes. Logic Agent (LA) [Liu et al., 2024a] replaces external solvers with rule-guided LLM inference chains, while LLM-TRes [Toroghi et al., 2024] implements self-contained verifiable reasoning without external symbolic solvers. SymbCoT [Xu et al., 2024c] coordinates translation, planning, solving and verification entirely through LLMs. Xu et al. [2024b] propose Aristotle, which further systematizes the symbolic reasoning pipeline through three LLM-driven components: Logical Decomposer, Logical Search Router, and Logical Resolver."}, {"title": "6 Discussion", "content": "The integration of logical reasoning into large language models (LLMs) remains a critical challenge, marked by persistent gaps between heuristic performance and formal logical rigor. Below, we analyze three unresolved tensions dominating the field and outline future directions.\nRobustness vs. Generalization. LLMs exhibit inconsistent performance in structured reasoning tasks such as deductive inference and abductive hypothesis generation. While models fine-tuned on datasets like FOLIO [Han et al., 2024a] excel in controlled settings, they struggle with adversarial perturbations or semantically equivalent rephrasings. This inconsistency arises from their reliance on surface-level statistical correlations rather than causal relationships, coupled with limited out-of-distribution generalization. A key question persists: can LLMs achieve human-like robustness without sacrificing cross-domain adaptability? Current methods prioritize narrow task performance, leaving real-world applicability uncertain.\nInterpretability vs. Performance. A central tension lies in balancing neural scalability with symbolic precision. Neuro-symbolic approaches like Logic-LM [Pan et al., 2023] and Symbol-LLM [Xu et al., 2024a] embed formal logic solvers into neural architectures, improving interpretability through step-by-step proofs. However, these methods face scalability bottlenecks with large knowledge bases or complex rule dependencies. Conversely, data-driven methods (e.g., instruction tuning on LogicBench [Parmar et al., 2024]) achieve broader task coverage but fail to generalize beyond syntactic patterns. How can we reconcile transparent reasoning with black-box model performance? Hybrid architectures offer promise but introduce computational overhead, limiting practical deployment.\nEvaluation Rigor. Existing benchmarks like LogiQA [Liu et al., 2021b] and ReClor [Yu et al., 2020] conflate reasoning ability with pattern recognition through multiple-choice formats. While efforts like NeuLR [Xu et al., 2023] curate \"neutral\" content to isolate reasoning from domain knowledge, they lack scope for holistic evaluation. Current metrics (e.g., accuracy, BLEU) fail to assess consistency (invariance to logically equivalent inputs) or soundness (adherence to formal proof structures). What defines a gold standard for logical reasoning evaluation? Benchmarks must prioritize systematic testing of core principles (e.g., transitivity, contraposition) over task-specific performance.\nFuture Directions. Addressing these challenges requires hybrid architectures that dynamically integrate neural and symbolic components, such as differentiable theorem provers, to balance scalability and precision. Equally important is the development of evaluation frameworks that stress-test models on perturbed logical statements (e.g., negated premises, swapped quantifiers) to isolate reasoning from memorization. Multimodal reasoning, which grounds inference in diverse modalities (text, images, code), presents untapped potential for enhancing robustness and interpretability. Finally, interdisciplinary collaboration\u2014leveraging insights from formal logic, cognitive science, and machine learning will be essential to design systems that reason with and about uncertainty. Until LLMs reliably disentangle logic from lexicon, their deployment in high-stakes domains will remain precarious. Bridging this gap demands rigorous benchmarks, scalable hybrid methods, and a redefinition of evaluation paradigms."}, {"title": "7 Conclusion", "content": "This survey synthesizes the rapid advancements and persistent challenges in logical reasoning for large language models (LLMs). While LLMs demonstrate impressive heuristic reasoning, rigorous logical inference\u2014spanning deductive, inductive, abductive, and analogical paradigms-remains inconsistent due to limitations in robustness, generalization, and interpretability. We analyzed strategies to enhance reasoning, including neuro-symbolic integration, data-centric tuning, reinforcement learning, test-time scaling, and other improved decoding methods, and highlighted benchmarks like FOLIO and LogiQA for systematic evaluation. Future progress hinges on hybrid architectures that unify neural and symbolic reasoning, robust evaluation frameworks, and scalable methods for cross-domain and multimodal inference. Addressing these challenges will advance LLMs toward reliable, interpretable reasoning critical for real-world applications."}]}