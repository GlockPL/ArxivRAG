{"title": "Prediction of Clinical Complication Onset using Neural Point Processes", "authors": ["Sachini Weerasekara", "Sagar Kamarthi", "Jacqueline Isaacs"], "abstract": "Predicting medical events in advance within critical care settings is paramount for patient outcomes and resource management. Utilizing predictive models, healthcare providers can anticipate issues such as cardiac arrest, sepsis, or respiratory failure before they manifest. Recently, there has been a surge in research focusing on forecasting adverse medical event onsets prior to clinical manifestation using machine learning. However, while these models provide temporal prognostic predictions for the occurrence of a specific adverse event of interest within defined time intervals, their interpretability often remains a challenge. In this work, we explore the applicability of neural temporal point processes in the context of adverse event onset prediction, with the aim of explaining clinical pathways and providing interpretable insights. Our experiments span six state-of-the-art neural point processes and six critical care datasets, each focusing on the onset of distinct adverse events. This work represents a novel application class of neural temporal point processes in event prediction.", "sections": [{"title": "Introduction", "content": "Timely interventions in critical care significantly impact patient outcomes, particularly in responding to adverse clinical events like cardiac arrest, pneumonia, and sepsis. However, predicting the onset of these events before they clinically manifest remains challenging due to the intricate clinical presentation of critically ill patients. Machine learning models offer predictive capabilities by capturing complex correlations within the dynamic patient conditions over time, assisting medical professionals in effectively anticipating the onset of such critical events [1,2,3,4,5].\nIn particular, these models employed various neural architectures, including recurrent architectures such as long-short-term memory (LSTM) [6], gated re- current units (GRU) [7], convolutional networks [8], and temporal convolution networks [9,10]. Moreover, several studies implemented boosted tree-based mod- els, including XGBoost [11] and random forest [12]. These models commonly utilize feature spaces, including a variety of patient biomarkers, encompassing vital signals, lab measurements, and demographic data (Moor et al., 2021)."}, {"title": "", "content": "While the predictive prowess of these models continues to advance, many state-of-the-art (SOTA) models offer probabilities for disease onset within a given timeframe but often fall short in terms of interpretability regarding these probabilities. Research on interpretable predictions has predominantly employed attention mechanisms [14] to discern feature importance [13]. Nonetheless, ad- dressing interpretability still remains challenging, given the inherently complex nature of neural architectures compounded by the high dimensional nature of patient biomarkers.\nBy shifting the problem's perspective to predicting a series of continuous-time events leading up to the occurrence of the adverse event of interest, there's poten- tial for a substantial enhancement in the interpretability of predictions regarding adverse event occurrence. This involves employing temporal point process (TPP) modeling to predict a sequence of continuous-time events.\nTemporal point processes serve as a powerful framework for understanding a series of events unfolding temporally. They allow capturing the timing, fre- quency, and dependencies of events in continuous time. More specifically, TPPs describe event timing, probability of occurrence, and temporal dependencies for a series of forthcoming events. Classical TPPs, such as Poisson processes [15] and Hawkes processes [16], have long served as foundational mathematical models and found extensive application on event series predictions in diverse domains such as traffic modeling [17], finance [18], and seismology [19]. Nevertheless, the strong parametric assumptions inherent in these models tend to limit their ability to capture the intricate complexities of real-world phenomena."}, {"title": "Method", "content": "Consider a fixed time duration [0, T] during which we observe an event sequence. Let N denote the total number of events occurring within this interval, happen- ing at times 0 < t1 < ...< tN. This event sequence can be represented as (t1, e1), ....(tN, eN), where ek \u2208 1,....K represents K distinct event types. If we denote the probability of an event of type k occurring within a time interval [t,t + dt) as pk(t|x[0,t)), then the probability of no event occurring during this interval would be 1-\\sum_{k=1}^K Pk (t|x[0,t)). The distribution of a TPP is defined by the intensity function \\lambda_k(t|x[0,t)) \u2265 0 for each event type k at each time t > 0 such that pk(t|x[0,t)) = \\lambda_k(t|x[0,t))dt. Hence, the next event in the sequence is determined by the maximum event intensity argmax(\\lambda_k(t|x[0,t))dt).\nNeural TPPs autoregressively generate event sequences determined by neural networks. For the i-th event, it computes the embedding of the event $e_i \\in R^D$ via an embedding layer, and the hidden state $h_i$ gets updated conditioned on $e_i$ and the previous state $h_{i-1}$. Then the prediction for the next event conditioned on the hidden state $h_i$ is drawn as below where $f$ denotes a recurrent encoder, which is either RNN-based or attention-based.\n$t_{i+1}, e_{i+1} \\sim P_\\theta(t_{i+1}, e_{i+1}|h_i), h_i = f(h_{i-1}, e_i)$ (1)"}, {"title": "", "content": "Both classical TPPs and neural TPPs use negative log-likelihood (NLL) as the training loss function. The NLL of a TPP given the event sequence e[0,T] is given in Equation (2), and the derivation can be found elsewhere [26].\n$\\sum_{i=1}^N log \\lambda_{e_i}(t_i|e_{[0,t_i]}) - \\sum_{t=0}^T\\sum_{k=1}^K  \\lambda_k(t|e_{[0,t]})dt$ (2)"}, {"title": "Adverse Event Onset with TPP", "content": "Next, we formulate the adverse event prediction problem in the context of TPP. Suppose we are given an event sequence S = {(ti,ei)}=1 of I events where each event ei \u2208 {adverse, amber \u2013 flag} and ei \u2208 {1, ..., K} where K is the number of different event types from an event pool with one adverse event and several amber flag events. An amber flag event describes an abnormal reading of patient biomarkers: vital signals or lab measurements. For instance, abnormal body temperature is translated as an amber flag event named \"thermoregulation dysfunction\". \nData preprocessing. Following the standard practices, as the first step, we split the data into train, test, and validation sets. Then, to feed the model with"}, {"title": "Experiments", "content": "In this section, we discuss the experiment setup: datasets, model implementation and assessment, and result analysis."}, {"title": "Setup", "content": "Model implementation. We conduct comprehensive evaluations of the fol- lowing six widely cited SOTA neural point processes that facilitate multi-type event predictions regarding their effectiveness in addressing the adverse event onset problem. Of these, three utilize Recurrent Neural Network (RNN) archi- tecture, while the remaining three leverage attention-based mechanisms.\nThree RNN-based models: Neural Hawkes Process (NHP) [24], Recurrent Marked Temporal Point Process (RMTPP) [23], Intensity-free [29]\nThree attention-based models: Transformer Hawkes Process (THP) [26], Self- attentive Hawkes Process (SAHP) [25], Attentive Neural Hawkes Process (A-NHP) [27]\nTo ensure consistency in implementation standards, all models were devel- oped in Pytorch, and we employed base models provided by EasyTPP [31] with the default hyperparameters for our experiments. However, instead of relying on the thinning algorithm utilized in EasyTPP, we opted to integrate a Mul- tilayer Perceptron (MLP) layer for event type and time prediction. Our code implementation can be accessed at NTPP-for-clinical-events.\nTraining objective. For the training objective, which is log-likelihood, we used the implementation given in EasyTPP. The integral in the second term is computed using the Monte Carlo approximation given by Mei & Eisner [24].\nNext event type and time prediction. For each event in the validation dataset, we try to predict the immediate next event ei+1 and the time at which the event is predicted to happen ti+1. For all the intensity-based models, we use an MLP layer with a softmax function at the end of the network to compute the next event time $f_t(t|h_i)$ and event intensities $\\Lambda_e((t_i+1)|h_i)$. For the IF"}, {"title": "", "content": "model, we use the conditional distribution of inter-event times to determine the next event time and type as implemented in EasyTPP.\nDatasets. We conducted experiments using five adverse event datasets ob- tained from the eICU Collaborative Research Database [30]. This database is a comprehensive collection of anonymized patient data from over 200,000 ICU admissions across various medical centers in the United States between 2014 and 2015. It encompasses a wide range of information, including patient demograph- ics, vital signs, laboratory measurements, and diagnostic records.\nEvent chains were constructed using vital signs, laboratory measurements, and diagnosis records. Abnormal readings of patient biomarkers served as indi- cators for the onset of amber flag events, as elaborated in Section 2. The onset of adverse events was determined from diagnostic details.\nThe five datasets comprise time-stamped positive samples, representing event sequences of patients who experienced the respective adverse event within 12 hours of onset, alongside negative samples, which correspond to event sequences of patients who did not encounter the adverse event. After pre-processing, the below positive event sequences are drawn from a cohort of 17014 pneumonia patients, 20256 sepsis patients, 4945 cardiac arrest patients, 17005 acute renal failure patients, 33054 respiratory failure patients, and 1621 cardiogenic shock patients."}, {"title": "Results and Analysis", "content": "In this section, we discuss the results of the experiments conducted with the aim of addressing the key research questions.\nFigure 4 elucidates Q1 by showcasing the negative log-likelihoods of the hold- out validation sets following a training duration of 100 epochs. Elevated values denote superior models, with IF consistently outperforming other intensity-based models across all scenarios because Monte Carlo integration in intensity-based models is prone to numerical approximation errors.\nAddressing Q2, Table 2 presents the accuracy of predicting the next event type, from event pools of 34 events each. Our aim here is to test all the models with a standard setting, as we discussed in Section 2, rather than optimizing each model's performance. In the standard setting, no model shows significantly better performance than the others, and the performance is seen to saturate among the models. While generally attention-based models perform superior to other models, results show that RNN-based models also make strong competitors to attention-based models in this case.\nRegarding Q3, Figure 5 illustrates the optimal transport distances (OTD) for short-term horizon (next 3 events) and long-term horizon (next 6 events) prediction for pneumonia, sepsis, and respiratory failure. OTDs were calculated using the function wsasserstein_distance from the scipy package in Python. OTD explains the cost required to transform the predicted sequence into ground"}, {"title": "Future Research", "content": "In this section, we explore the prospective avenues for future research stemming from the observed results.\nThis study serves as an inaugural experimental investigation utilizing the foundational NTPP models in adverse clinical event prediction, thereby opening up multiple new avenues for further exploration. A key observation is the con- vergence of performance across diverse and sophisticated model architectures, suggesting a saturation at comparable levels. This suggests a potential enhance- ment through richer input feature spaces, particularly given the coarse-grained, one-hot encoded nature of input sequences for NTPP models. Hence, an avenue of future research could be the development of models capable of incorporating patient-specific details, such as demographics and biomarker values, alongside event sequences. Furthermore, since certain event sequences may be infrequent because of the complicated clinical picture of critical care patients, exploring few-shot learning strategies within this context could prove advantageous.\nGenerating more comprehensive and informed amber flag event sequences tailored to the specific adverse event under consideration with clinician inputs would immensely benefit event sequence predictions preceding the adverse event, thereby presenting yet another promising avenue for future research."}, {"title": "Related work", "content": "Neural Temporal Point Processes (NTPP). With the ability of deep learn- ing models to handle sequential data, various architectures of neural point pro- cesses have emerged in recent years. Most of these models implement RNNs"}]}