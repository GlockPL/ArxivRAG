{"title": "FINE-TUNING CAN HELP DETECT PRETRAINING DATA FROM LARGE LANGUAGE MODELS", "authors": ["Hengxiang Zhang", "Songxin Zhang", "Bingyi Jing", "Hongxin Wei"], "abstract": "In the era of large language models (LLMs), detecting pretraining data has been increasingly important due to concerns about fair evaluation and ethical risks. Current methods differentiate members and non-members by designing scoring functions, like Perplexity and Min-k%. However, the diversity and complexity of training data magnifies the difficulty of distinguishing, leading to suboptimal performance in detecting pretraining data. In this paper, we first explore the benefits of unseen data, which can be easily collected after the release of the LLM. We find that the perplexities of LLMs perform differently for members and non-members, after fine-tuning with a small amount of previously unseen data. In light of this, we introduce a novel and effective method termed Fine-tuned Score Deviation (FSD), which improves the performance of current scoring functions for pretraining data detection. In particular, we propose to measure the deviation distance of current scores after fine-tuning on a small amount of unseen data within the same domain. In effect, using a few unseen data can largely decrease the scores of all non-members, leading to a larger deviation distance than members. Extensive experiments demonstrate the effectiveness of our method, significantly improving the AUC score on common benchmark datasets across various models.", "sections": [{"title": "1 INTRODUCTION", "content": "The impressive performance of large language models (LLMs) arises from large-scale pretraining on massive datasets collected from the internet (Achiam et al., 2023; Touvron et al., 2023b). But, model developers are often reluctant to disclose detailed information about the pretraining datasets, raising significant concerns regarding fair evaluation and ethical risks. Specifically, Recent studies reveal that the pretraining corpus may inadvertently include data from evaluation benchmarks (Sainz et al., 2023; Balloccu et al., 2024), making it difficult to assess the practical capability of LLMs. Besides, LLMs often generate text from copyrighted books (Grynbaum & Mac, 2023) and personal emails (Mozes et al., 2023), which could infringe on the legal rights of the original content creators and violate their privacy. Considering the vast size of the pretraining dataset and the single iteration of pretraining, it has been increasingly important and challenging to detect pretraining data, which determines whether a piece of text is part of the pretraining dataset.\nIn the literature, current works of detecting pretraining data primarily focus on designing scoring functions to differentiate members (i.e., seen data during pretraining) and non-members (unseen). For example, previous work shows that sequences leak in the training data tend to have lower perplexity (i.e., higher likelihood) than non-members (Li, 2023). Min-k% leverages the k% of tokens with minimum token probabilities of a text for detection, assuming that trained data tends to contain fewer outlier tokens (Shi et al., 2024). However, non-member data can obtain low perplexities by including frequent or repetitive texts, while members may contain rare tokens that result in high perplexities. This casts significant doubt on utilizing those scoring functions for detecting pretraining data. Consequently, this issue prompts us to present a preliminary attempt to enlarge the difference between members and non-members for pretraining datasets of LLMs.\nIn this work, we propose Fine-tuned Score Deviation (FSD), a novel and effective approach that improves the detection capabilities of current scoring functions in a specific domain (e.g., event data"}, {"title": "2 BACKGROUND", "content": "In this work, we focus on detecting pretraining data, the problem of detecting whether a piece of text is included in the pretraining data of a specific LLM. First, we formally define the problem setup and its challenges. Then, we introduce two commonly used methods for this task."}, {"title": "Pretraining data detection", "content": "Let $f$ be an autoregressive large language model (LLM) with trainable parameters $\\theta$ (e.g., LLAMA (Touvron et al., 2023a)) and $D$ denotes the associated pretraining data, sampled from an underlying distribution $P$. As model developers rarely provide detailed information about the pretraining dataset, we generally desire to identify if the LLM is trained on the given text for scientific and ethical concerns. Formally, the task objective is to learn a detector $h$ that can infer the membership of an arbitrary data point $x$ in the dataset $D$: $h(x, f_\\theta) \\rightarrow {0,1}$.\nUnlike the black-box assumption in previous works (Shi et al., 2024; Oren et al., 2024), we assume the access to fine-tune LLMs with custom datasets and the output probabilities of LLMs, which is realistic for open-sourced LLMs and many commercial APIs, such as GPT-40\u00b9. In addition, the detector can obtain a few data samples ${x}_i^{i=0}$ that belong to the same domain as the given sample $x$ and do not present in the training set. This can be achieved by collecting those contents (e.g., journal articles) published after the release of the LLM.\nThe task of pretraining data detection can be formulated as a binary classification: determining whether a given text $x$ is a member or non-member of the pretraining dataset $D$. Pretraining data detection can be performed by a level-set estimation:\n$h(x; f_\\theta) = \\begin{cases}\nmember & \\text{if } S(x; f_\\theta) < \\epsilon, \\\\\nnon-member & \\text{if } S(x; f_\\theta) \\geq \\epsilon,\n\\end{cases}$                                                                                              (1)\nwhere $S(x; f_\\theta)$ denotes a scoring function and $\\epsilon$ is the threshold determined by a validation dataset. By convention, examples with lower scores $S(x; f_\\theta)$ are classified as members of pretraining data and vice versa. In the following, we introduce two popular scoring functions for the task."}, {"title": "Scoring functions", "content": "For large language models, likelihood is typically used to estimate the uncertainty in generating new tokens. In particular, a high likelihood indicates that the model predicts tokens with high confidence. Given a piece of text $x = {x_1,x_2, ..., x_n}$, the likelihood of the next token $x_{n+1}$ is $p_\\theta(x_{n+1}|x_1,...,x_n)$. In general, a piece of text seen in pre-training tends to have more tokens with a high likelihood, whereas unseen texts have more tokens with a low likelihood.\nIn light of this, previous studies usually design likelihood-based scoring functions to detect pretraining data (Shi et al., 2024; Carlini et al., 2021; Li, 2023). For example, Perplexity is proposed to distinguish members and non-members, based on the observation that members tend to have lower perplexity than non-members (Li, 2023). Formally, The perplexity of $x$ is calculated as:\n$Perplexity(x; f_\\theta) = exp\\{-\\frac{1}{n}\\sum_{i=1}^{n} log p_\\theta(x_i | x_1,..., x_{i-1})\\}$                                                           (2)\nwhere $x = {x_1, x_2,...,x_n}$ is a sequence of tokens and $p_\\theta(x_i | x_1,..., x_{i-1})$ is the conditional probability of $x_i$ given the preceding tokens.\nInstead of using the likelihood of all tokens, Min-k% (Shi et al., 2024) computes the average probabilities of k% outlier tokens with the smallest predicted probability. The intuition is that a non-member example is more likely to include a few outlier words with low likelihoods than members. Formally, Min-k% is computed by:\n$Min-k\\%(x; f_\\theta) = \\frac{1}{|E|}\\sum_{x_i\\in Min-k\\%(x)} log p_\\theta(x_i | x_1,..., x_{i-1})$                                                  (3)\nwhere E is the size of the Min-k%(x) set.\nHowever, non-member data can obtain low perplexities by including frequent or repetitive texts, while members may contain rare tokens that result in high perplexities (See Figure 3a and 3b). This issue makes it challenging to distinguish members and non-members using those scoring functions, leading to suboptimal performance in detecting pre-training data. Thus, we present a preliminary attempt to utilize extra non-member data to enlarge the gap between members and non-members."}, {"title": "3 METHOD: FINE-TUNED SCORE DEVIATION", "content": "Recall the realistic assumption that detectors can obtain a few non-members that belong to the same domain as the given sample, we aim to explore how to utilize these extra non-members to improve"}, {"title": "3.1 MOTIVATION", "content": "In the analysis, we conduct experiments with WikiMIA (Shi et al., 2024), an evaluation benchmark that uses events added to Wikipedia after specific dates as non-member data. We use D to denote the non-member dataset that is accessible for detectors. To construct the dataset D, we randomly sample a subset with 100 examples from the non-member data of WikiMIA. In addition, we construct the test set with 630 examples each for both members and non-members. Throughout this subsection, we fine-tune LLaMA-7B (Touvron et al., 2023a) with LoRA (Hu et al., 2022) on the non-member dataset D. To illustrate the effects of fine-tuning, we compare the perplexity distribution of members and non-members from the pre-trained model and the fine-tuned model."}, {"title": "3.2 METHOD", "content": "Motivated by the previous analysis, we propose Fine-tuned Score Deviation (FSD), a general method that can improve the detection performance of current scoring functions in a specific domain. The key idea of our method is to enlarge the gap between seen and unseen data, by exposing the LLM to a few unseen data. With this in mind, we present the details of our approach step by step.\nConstruct fine-tuning dataset Given a piece of text x, the first step of our method is to collect a small amount of unseen data for the LLM within the same domain. Owing to the availability of public text data in enormous quantities, we can construct non-member datasets by comparing the LLM release date and data creation timestamp. For instance, we collect some events occurring post-2023 from Wikipedia as the auxiliary non-member dataset for fine-tuning LLaMA (Touvron et al., 2023a), since LLaMA was released in February 2023.\nFine-tuning with non-members To expose LLMs to unseen data, we perform fine-tuning on LLMs with the constructed fine-tuning dataset. As our goal is to reduce the perplexity of the unseen data, we employ self-supervised fine-tuning by predicting the next word or token in a given"}, {"title": "Fine-tuned Score Deviation", "content": "Recall that fine-tuning decreases the perplexity of non-members but almost maintains those of members, we propose to exploit the score deviation for detecting pretraining data. Given a new sample $x$, we calculate the score difference between the pre-trained LLM $f_\\theta$ and the fine-tuned LLM $f_{\\theta'}$, where $\\theta'$ denotes the parameters of LLM after fine-tuning. Formally, the new score of Fine-tuned Score Deviation (FSD) can be formulated as:\n$FSD(x; f_\\theta, f_{\\theta'}) = S(x; f_\\theta) \u2013 S(x; f_{\\theta'})$                                                                          (5)\nwhere $S(.)$ denotes an existing scoring function, such as Perplexity and Min-k%. With the proposed score, we can estimate the membership of $x$ through the level-set estimation (Eq. (1)). Examples with a large deviation score are considered as non-members and vice versa. In practice, we determine the threshold $\\epsilon$ by maximizing detection accuracy on a validation set, following the previous work (Shi et al., 2024). Our method is compatible with various scoring functions and consistently enhances their performance in detecting pretraining data, as presented in Table 1.\nBy way of the FSD score, we can obtain a clear distinction between members and non-members, establishing excellent performance in detecting pretraining data. To provide a straightforward view, we show in Figure 3 the score distribution between members and non-members using various scoring functions on WikiMIA (Shi et al., 2024). The results of ArXivTection (Duarte et al., 2024) are also presented in Appendix C.1. Our experiments validate that, compared to the perplexity and Min-k% scores, our FSD score significantly increases the gap between non-members and members, and as a result, enables more effective pretraining data detection."}, {"title": "4 EXPERIMENTS", "content": "In this section, we evaluate the effectiveness of our method for pretraining data detection across several benchmark datasets with multiple existing open-sourced models. We also apply FSD to copyrighted book detection in real-world scenarios and find it a consistently effective solution."}, {"title": "4.1 EXPERIMENTAL SETUP", "content": "Models We conduct extensive experiments on diverse open-sourced LLMs. For the main results, we use LLaMA-7B (Touvron et al., 2023a) as the LLM throughout our experiments. We also provide experiments on other models including Pythia-6.9B (Biderman et al., 2023), GPT-J-6B (Wang &\nKomatsuzaki, 2021), OPT-6.7B (Zhang et al., 2022), LLaMA-13B models (Touvron et al., 2023a),\nLLAMA-30B (Touvron et al., 2023a), and NeoX-20B (Black et al., 2022). Existing works (Shi et al.,\n2024; Ye et al., 2024) generally use these models as LLMs for performing the studies of pretraining data detection. The models are provided by Hugging Face 2.\nDatasets To verify the effectiveness of detection methods, we employ four common benchmark datasets for evaluations, including WikiMIA (Shi et al., 2024), ArXivTection (Duarte et al., 2024), BookTection (Duarte et al., 2024) and BookMIA (Shi et al., 2024). Previous works have demonstrated that model developers commonly use text content among those datasets for pre-training (Shi et al., 2024; Duarte et al., 2024; Ye et al., 2024). The datasets are provided by Hugging Face\u00b3, and detailed information of datasets is presented in Appendix A.\nBaseline methods We use four detection methods based on scoring functions as our baselines for evaluating the performance of methods on various datasets under diverse models. Those methods employ specific metrics based on the likelihood, followed by a comparison with a preset threshold to identify the given text's membership. Specifically, baseline methods include the example perplexity (Perplexity) (Li, 2023), the ratio of example perplexity and zlib compression entropy (Zlib) (Carlini et al., 2021), the ratio of the perplexity on the example before and after lowercasing (Lowercase) (Carlini et al., 2021) and detecting pretraining example through outlier words with low probability (Min-k%) (Shi et al., 2024).\nEvaluation metrics We evaluate the performance of detection methods for detecting pretraining data by measuring the following metrics: (1) AUC, the area under the receiver operating characteristic curve; (2) the true positive rate (TPR) when the false positive rate (FPR) of examples is 5% (TPR@5%FPR).\nImplementation details Our approach involves constructing the non-member dataset and fine-tuning the base model. For constructing the non-member dataset, we randomly sample 30% of the data from the entire dataset and select all non-members from this subset as the constructed fine-tuning dataset. The remaining 70% of the dataset is used for testing. We employ LoRA (Hu et al., 2022) to fine-tune the base model with 3 epochs and a batch size of 8. We set the initial learning rate as 0.001 and drop it by cosine scheduling strategy. We conduct all experiments on NVIDIA L40 GPU and implement all methods with default parameters using PyTorch (Paszke et al., 2019)."}, {"title": "4.2 MAIN RESULTS", "content": "Can FSD improve the performance of current scoring functions? We compare the performance of detection methods on WikiMIA and ArXivTection datasets across various large language models. The detailed information of dataset split is shown in Appendix B.1. Our results in Table 1 show that the FSD significantly improves the performance of all baseline methods on both datasets across diverse models. For example, our method improves the AUC score compared to the best baseline method Min-k%, increasing it from 0.62 to 0.91 on WikiMIA dataset from the OPT-6.7B model. Similarly, it improves the AUC score from 0.76 to 0.86 on ArXivTection dataset from the LLaMA-7B model. Moreover, we show that our method also remarkably improves the TPR@5%FPR score of all baseline methods in Table 11 of Appendix C.2."}, {"title": "5 DISCUSSION", "content": "Can members be used for fine-tuning? The key step of our method is to fine-tune the pre-trained model using a few non-members. One may also ask: can a similar effect be achieved by utilizing members as the fine-tuning dataset? In this ablation, we separately sample members and non-members from WikiMIA to construct fine-tuning datasets(Mem, Non). In addition, we randomly sample data from WikiMIA as another fine-tuning dataset (All). The details of implementation are presented in Appendix B.3. To investigate the impact of different fine-tuning datasets on pretraining data detection, we fine-tune the LLaMA-7B model with each of the fine-tuning datasets individually.\nOur results in Table 4 show that our method can improve the performance of baseline methods using members as the fine-tuning dataset. However, our method achieves inferior performance when using members for fine-tuning compared with non-members. Moreover, it is not realistic to construct a member dataset without accessing pretraining data in real-world scenarios. In addition, this is feasible for constructing non-members as a fine-tuning dataset based on the model release date and data creation timestamp. Overall, our method demonstrates superior performance when using non-members for fine-tuning, while ensuring applicability in real-world settings.\nIs our method affected by distribution difference? Existing works generally construct bench-mark datasets based on the LLM release date and data creation timestamp (Ye et al., 2024; Shi et al.,"}, {"title": "Is FSD effective with different fine-tuning methods?", "content": "To expose LLMs to unseen data, we employ LoRA to fine-tune the pre-trained model. The results demonstrate that our method achieves impressive performance for pretraining data detection when fine-tuning with LoRA. However, can a similar effect be achieved using different fine-tuning methods? To this end, we apply AdaLoRA (Zhang et al., 2023), IA3 (Liu et al., 2022), and LoRA to fine-tune LLaMA-7B with WikiMIA, respectively. The details of the dataset in our experiment can be found in Appendix B.1.\nIn Table 6, we report the AUC and TPR@5%FPR scores for pretraining data detection with our method and baseline methods. The results show that our method can improve the performance of baseline methods when using different fine-tuning methods. Although our FSD achieves inferior performance with AdaLoRA compared with IA3 and LoRA, it still significantly improves the performance of baseline methods. Overall, our method can be implemented with different fine-tuning methods and does not require a specific fine-tuning technique."}, {"title": "6 RELATED WORK", "content": "Pretraining data detection, which is an increasingly important topic for large language models, relates to a large amount of literature on membership inference attacks and data contamination. We discuss some of the relevant works to ours in two directions below."}, {"title": "Membership Inference Attacks", "content": "Our work mainly studies how to detect a given example in the pretraining data, which is consistent with the objective of membership inference attacks (MIAs) (Shokri et al., 2017; Truex et al., 2019). This task aims to determine whether a given data point is a member of training data. Metric-based attack methods, such as loss (Yeom et al., 2018), entropy (Salem et al., 2019), confidence (Liu et al., 2019) and gradient (Liu et al., 2023), infer membership of data by comparing the calculated metric value with a preset threshold. Previous works have generalized metric-based methods to large language models (Duan et al., 2024; Xie et al., 2024; Zhang et al., 2024; Mattern et al., 2023), by calculating the based-likelihood metric (e.g., perplexity) for membership inference. Recent works apply MIAs to pretraining data detection by designing likelihood-based scoring functions to measure the membership of data (Shi et al., 2024; Ye et al., 2024). In this work, we analyze the limitations of existing scoring functions for pretraining data detection, and design an effective method to improve their performance. In particular, this work is the first to explore the importance of collecting unseen data in pretraining data detection."}, {"title": "Data Contamination", "content": "Data contamination has been studied in the literature (Xu et al., 2024a; Magar & Schwartz, 2022; Balloccu et al., 2024), where training data may inadvertently include evaluation benchmark data, resulting in unauthentic evaluation results. Thus, it is important to assess the leakage of benchmark data into pretraining data (Zhou et al., 2023). On the one hand, model developers can remove evaluation benchmark data from training data by retrieval-based methods with access to pertaining data (Ravaut et al., 2024; Chowdhery et al., 2023). Specifically, those methods employ n-gram tokenization and string-matching for detecting data contamination (Brown et al., 2020; Touvron et al., 2023b; Team et al., 2023; Radford et al., 2019). On the other hand, researchers utilize prompting techniques (Golchin & Surdeanu, 2024), performance analysis (Ye et al., 2024; Debenedetti et al., 2024), model likelihood (Oren et al., 2024; Shi et al., 2024; Xu et al., 2024b) to detect potential contamination without access to the training data. Our work focuses on pretraining data detection, an area that is similar to data contamination. Different from data contamination detection, our FSD can also be applied to the detection of copyrighted resources in real-world scenarios."}, {"title": "7 CONCLUSION", "content": "In this paper, we introduce Fine-tuned Score Deviation (FSD), a novel detection method that can universally improve the performance of existing detection methods. To the best of our knowledge, our method is the first to utilize some collected non-members in the task of pretraining data detection. Our core idea behind FSD is to enlarge the gap between seen examples and unseen examples by exposing the LLM to a few unseen examples. In effect, unseen data have a larger score than seen examples when using FSD, which makes it more distinguishable between seen and unseen data. Extensive experiments demonstrate the effectiveness of our method for detecting pretraining data on common benchmark datasets across various models. In summary, the FSD is an effective approach for accurately detecting pretraining data of LLMs.\nLimitations Our method requires to collect a few examples that belong to the same domain but are not involved in the training. Generally, we can utilize the data content published after the release of the LLM. Therefore, our method is applicable for detecting benchmarks or copyrighted resources in a specific domain (e.g., math tests, magazines). The diversity of the test set may make it challenging to construct an effective auxiliary dataset of unseen data. In addition, the effectiveness of our method may reflect the distribution difference between members and non-members in existing popular benchmarks, which is inconsistent with the assumption of MIAs. It might be necessary to inspect the dataset splitting in the common benchmarks of detecting pretraining data."}, {"title": "A DETAILS OF DATASETS", "content": "Previous works construct benchmark datasets to evaluate the performance of detection methods for pretraining data detection. Following the prior literature, we conduct experiments on 4 benchmark datasets: WikiMIA (Shi et al., 2024) selects old Wikipedia event data as member data by leveraging the Wikipedia data timestamp and the model release date, since Wikipedia is a commonly pretraining data source. BookMIA (Shi et al., 2024), which contains excerpts from copyrighted books in the Books3 subset of the Pile dataset (Gao et al., 2020), can be used for detecting potential copyright infringement in training data. ArXivTection (Duarte et al., 2024) is a curated collection of research articles sourced from arXiv. BookTection (Duarte et al., 2024), which comprises passages from 165 books, is constructed based on BookMIA."}, {"title": "B EXPERIMENTAL DETAIL", "content": null}, {"title": "B.1 DATASET SPLIT", "content": "We report the performance of detection methods on WikiMIA and ArXivTection datasets across various large language models. To construct the fine-tuning dataset, we randomly split 30% of the dataset using a seed of 42 and select all non-members from this subset. The remaining 70% of the dataset is used for testing. The detailed information of the constructed dataset is shown in Table 7."}, {"title": "B.2 COPYRIGHTED BOOK DETECTION", "content": "To conduct experiments of copyrighted book detection on BookMIA and BookTection, we randomly split 30% of the dataset using a seed of 42 and select all non-members from this subset as the fine-tuning dataset. Subsequently, we randomly sample 500 members and non-members from the remaining 70% of the datasets, constructing a balanced validation set of 1,000 examples. The remaining data is used for testing. The detailed information dataset split is shown in Table 8."}, {"title": "B.3 FINE-TUNING WITH MEMBERS", "content": "To investigate the impact of model fine-tuning with different fine-tuning datasets on pretraining data detection, we construct three kinds of fine-tuning datasets. In this ablation, we sample members (Mem) and non-members (Non) from WikiMIA as fine-tuning datasets, respectively. In addition,"}, {"title": "B.4 TEMPORAL SHIFT", "content": "We show the temporal shift between members and non-members in the WikiMIA dataset, the illustration is presented in Table 10."}, {"title": "C DETAILED EXPERIMENTAL RESULTS", "content": null}, {"title": "C.1 FINE-TUNED SCORE DEVIATION", "content": "We show in Figure 5 the score distribution between members and non-members using various scoring functions on ArXivTection. The results also demonstrate that our FSD score significantly increases the gap between non-members and members compared to the perplexity and Min-k% scores, thus enabling more effective pretraining data detection."}, {"title": "C.2 DETAILED RESULTS OF EXPERIMENT", "content": "We report the TPR@5%FPR score for pertaining data detection in Table 11, 12, 13.\nCan FSD improve the performance of detection methods based on scoring functions? We compare the TPR@5%FPR score with our method and baselines on WikiMIA and ArXivTection datasets across various large language models in Table 11. The results show that our method significantly improves the TPR@5%FPR score of the baseline methods.\nIs FSD effective with different-sized models? We verify the performance of baselines and our methods from different-sized LLaMA models (7B, 13B, 30B) on WikiMIA. In Table 12, we show the TPR@5%FPR score from different-sized LLaMA models. The results demonstrate that our method is effective with different-size models.\nIs our method affected by distribution difference? We report the TPR@5%FPR score of baselines and our method on the original WikiMIA dataset, Deletion and Replacement. In Table 13, the results show that our method still improves the performance of baselines when mitigating the temporal shift between members and non-members."}]}