{"title": "Enabling Time-series Foundation Model for Building Energy Forecasting via Contrastive Curriculum Learning", "authors": ["Rui Liang", "Yang Deng", "Donghua Xie", "Fang He", "Dan Wang"], "abstract": "Advances in time-series forecasting are driving a shift from conventional machine learning models to foundation models (FMs) that are trained with generalized knowledge. However, existing FMs still perform poorly in the energy fields, such as building energy forecasting (BEF). This paper studies the adaptation of FM to BEF tasks. We demonstrate the shortcomings of fine-tuning FM straightforwardly from both the perspectives of FM and the data. To overcome these limitations, we propose a new contrastive curriculum learning-based training method. Our method optimizes the ordering of training data in the context of TSFM adaptation. Experiments show that our method can improve the zero/few-shot performance by 14.6% compared to the existing FMs. Our code and new TSFM will be available at <Anonymous Github Repo>.", "sections": [{"title": "1 INTRODUCTION", "content": "Building energy forecasting (BEF), i.e., energy consumption forecasting for a building, plays a crucial role in many downstream applications, such as occupant behavior modeling. Currently, the majority of BEF schemes are based on machine learning techniques. To achieve acceptable forecasting performance, the common practice is to develop specific models for individual buildings, yet it is hard to generalize at scale and requires significant effort. A growing promising paradigm is Foundation models [9]: large AI model trained on broad data such that it can be applied across a wide range of tasks, such as LLM in the NLP domain. Foundation models are capable of making inferences on a dataset with only a small fraction of training data, or even none at all, which corresponds to few-shot and zero-shot settings, respectively. Recently, various foundation models have been built in computer vision and language tasks to solve practical problems.\nThere are also some time-series foundation model (denoted as TSFM for simplicity) products available in 2024, such as IBM Granite [5] and Amazon Chronos [1], with a series of TSFMs\u00b9 which are trained on various data source such as weather, energy, medical, financial. From existing benchmarking [9], these TSFMs perform well in tasks, e.g., climate forecasting, where large-scale real datasets are adopted for pre-training. However, there are limited real-world data resources in building scenarios and many BEF works rely on the simulated dataset. This is because the building energy is related to occupant privacy or business confidentiality and thus leads to the building managers having a low willingness to share the energy data [16]. From the recent measurement study [11], the existing TSFMs can not achieve acceptable performance in BEF.\nThis paper is motivated by an essential question: Can we adapt the existing TSFMs to support the building energy forecasting tasks via the currently available data resources? We perform a preliminary evaluation of day-ahead BEF on a product-level TSFM (under zero-shot setting). Our analysis compares the forecasting accuracy of i) the original pre-trained FM and ii) the FM fine-tuned using the BEF dataset in a straightforward manner. This fine-tuning can be conducted using either real-world data alone (R) or a combination of real-world and simulated data (R+S)\u00b2. And half of the real buildings was allocated for the test set. As shown in Table 1, the improvement by fine-tuning is very limited, and the performance can not reach engineering purposes. Specifically, even after incorporating 900,000 simulated buildings into the real dataset, the accuracy improved by only 0.6%. These results provide two key insights: (1) There are no universal energy patterns for buildings, as patterns can vary significantly in complexity due to factors such as diverse occupancy behaviors and meteorological conditions. (2) Straightforward training or fine-tuning does not enhance the FM, even with a sufficiently large training set. This is because the knowledge embedded in the pre-trained FM is not easily quantifiable."}, {"title": "2 PRELIMINARIES", "content": "Building Energy Forecasting. BEF is a domain-specific task of time-series forecasting: under the rolling forecasting setting with a fixed size window with a length of L + T, we have the data sample ut = (xt, y\u207a) at time t, comprising past data xt = {1, ...,11} with a look-back window length L and future data y\u207a = {1+1, \u2026, L+T}, where I can be multi-dimension. Considering y in BEF task is single dimension and currently published TSFMs mainly support univariate forecasting [11], thus l\u2208 R in this paper. We omit the superscript t for simplicity later.\nCurriculum Learning. Motivated by the feature of human education, curriculum learning is a data-centric training strategy in which an ML model is trained on samples of increasing difficulty to smooth the training process and get better performance [14]. The two subtasks are: a Difficulty Measurer to measure and rank the difficulty of samples; and a Training Scheduler to decide the sequence of samples throughout the training process.\nContrastive Learning. Contrastive learning is to learn an embedding space to represent data samples, in which similar samples are grouped closer while dissimilar samples are pushed apart. The core is to construct positive pair (u, u\u207a) and negative pair (u, u\u00af) for an anchor sample u, i.e., to define the similarity, based on which to train the NN-based encoder with a contrastive loss function."}, {"title": "3 METHODOLOGY", "content": "Problem statement: given a pre-trained TSFM M, the existing/available BEF datasets D = {Dtrain, D'train}, where Dtrain = {u} is the real-world dataset, D'train = {u'} is the simulated dataset, and Dtrain <<< 1. Our objective is to adapt M to a new M' using D, D'train to minimize the loss of M' under zero/few-shot settings.\nWe propose a new Contrastive-aware Curriculum Learning (CCL) method to schedule the training process of M on D, with samples ordered as easy-to-difficult which is a common paradigm for ML model training, for example, [15] schedules the images from blur to clear to train the model. A unique challenge in our scenario is to measure the difficulty of the simulated data. We leverage contrastive representation to cope with this challenge, and the difficulty measurer and training scheduler designs are presented as follows."}, {"title": "3.1 Contrastive-aware Difficulty Measurer", "content": "The design of the difficulty measurer is usually based on model performance or data pattern analysis. Considering that the curriculum is for adapting an existing TSFM M, which has been pre-trained with various knowledge and patterns, we can directly make inference with the TSFM and use the performance as the difficulty score of samples in the real-world dataset Dtrain (Eq. 1). Here, L(, ) denotes the prediction error of the TSFM in terms of CV-RMSE.\n\nDM(u) = L(M(x), y), (1)\n\nFor the simulated dataset Drain, the key challenge is that the difficulty measurer for u is not suitable for u' because L(M(x'), y') introduces bias. We then leverage contrastive learning to predict the TSFM comprehension on the representation of u' and hence to determine the difficulty. We first define TSFM comprehension and contrastive pairs, based on which we introduce the contrastive model and how to estimate the difficulty of u'.\nDEFINITION 1. TSFM comprehension on samples. Let CM (u1,u2) = |DM(41) - DM(u2)| \u2208 R be the comprehension of a pre-trained TSFM M on two real sample u\u2081 and u2. Less value denotes a similar comprehension of M on u\u2081 and u2, and vice versa.\nDEFINITION 2. Contrastive pairs. We define positive and negative contrastive pairs for real u based on the value of CM(,). Given an anchor u, the pair (u, u\u207a) is positive if CM(u, u\u207a) < d since M shows similar comprehension on the two samples; otherwise, the pair is negative, as (u, u\u00af).\nNote that, contrastive pairs construction relies solely on real {u} and we set 8 to 0.01 based on our experimental analysis. It is a typical phenomenon that two dissimilar samples, which appear dissimilar in terms of time-series patterns (e.g., as measured by DTW similarity), may correspond to a similar TSFM comprehension, indicated by a low value of CM(,). This is related to the uncertain knowledge encapsulated by the TSFM."}, {"title": "3.2 Training Scheduler", "content": "After measuring the difficulty of samples, we leverage a linear continuous scheduler to select training samples at each epoch. Specifically, samples are first sorted by their difficulty. Then, a function \u03bb(t) = min(1, \u03bb\u03bf + (1 \u2212 \u03bb\u03bf) \u00b7 t/Tgrow) decides the percentage of the easiest samples to be used at the t-th epoch, where \u03bb\u03bf denotes the initial percentage of the easiest samples for training and Tgrow is the epoch when (t) grows to 1. Then, let D = {vi}=1, the training set at the t-th epoch is given by Dt = {v}(t). In order to let TSFMs fully explore patterns in building energy data, we set the number of training epochs larger than Tgrow, which means that TSFMs will be trained using the whole training set."}, {"title": "4 EVALUATION", "content": "4.1 Methodology\nTSFMs. We adopt two product-level TSFMs, which are Tiny Time Mixer (TTM) [5] from IBM and Chronos [1] from Amazon, for adapting to BEF tasks\u00b3.\nDatasets. Simulated and real-world public building energy datasets are used for experiments: (1) Buildings-900K [6]. This dataset contains hourly energy consumption time-series from 900k simulated buildings over two years. (2) Building Data Genome Project (BDG) [10]. This project aggregates 19 real-world building energy datasets from different locations around the world (totaling 1,636 buildings), where hourly energy meter data over a two-year period are collected for each building. (3) UCI Electricity [13]. This dataset collects electricity consumption data from 370 houses for four years, sampled at 15-minute interval.\nBaselines & Metrics. To explore improvement achieved by our method, we compare the TSFMs fine-tuned with our method (denoted as TSFM+CCL-FT) against: (1) the original pre-trained TSFMs (denoted as TSFM); and (2) the TSFMs directly fine-tuned without our method (denoted as TSFM+FT). Besides, we adopt three state-of-the-art time-series forecasting models adopted in BEF field for comparisons: LSTM [3] (a classical RNN architecture for handling sequential data), Autoformer [8] and Temporal Fusion Transformer (TFT) [7] (two transformer-based models tailored for time-series forecasting). For performance evaluation, we consider a standard metric in BEF field: the Coefficient of Variation of the\nRoot Mean Square Error (CV-RMSE): \u221a(\u03a3=1 Yi/n)\nSetup. We select five datasets from BDG together with the UCI dataset as the evaluation set since they cover most building types and climate conditions. In zero-shot setting, all data from the target building are used for testing. In few-shot setting, 10% of data are used for training and the remaining 90% of data are used for testing. The other 15 datasets from BDG and the simulated dataset Buildings-900K are used for TSFMs fine-tuning. The number of fine-tuning steps is set to 1000. The look-back window length and forecast horizon is set by default values of TSFMs during fine-tuning.\u2074 For evaluation, we set three forecast horizons, i.e., 24, 96, 192, as TSFMs can adapt to different horizons. The experiments are conducted on a Linux server with two NVIDIA GeForce RTX 4090 24GB GPUs."}, {"title": "4.2 Performance Result", "content": "Overall performance. We evaluate our method and baselines in zero-shot and few-shot settings under three forecast horizons in Table 2. Overall, TSFM+CCL-FT consistently outperforms TSFM and TSFM+FT in zero-shot setting, with average improvements in CV-RMSE at 18.3%, 11.3% for TTM and 12.6%, 8.5% for Chronos. Similar results are observed in few-shot setting where our method surpasses baselines by 14.9%, 11.9% for TTM and 12.7%, 8.9% for Chronos. Besides, as CV-RMSE < 0.3 is an industrial requirement defined by ASHRAE [2] for deployable forecasting models, we observe that on each dataset, there are cases where our method successfully reduces the error of pre-trained TSFMs to less than 0.3. Moreover, we find that although our CCL method enhances the performance of both TSFMs over their pre-trained version, the improvement ratio is higher on TTM. This is likely due to that TTM has a smaller model size and there are less energy datasets in its pre-training set such that the effect of our method is significant. Additionally, we notice that for Chronos, TSFM+FT only slightly outperforms TSFM by 4.3% which is consistent with our preliminary experiment result on TTM.\nNext, we compare TSFM+CCL-FT with three state-of-the-art baseline forecasting models. Here, TSFM+CCL-FT is evaluated under few-shot setting while the baselines are first trained using the first 50% of data from each test building and then tested on the remaining 50% of data. As shown in Table 3, we observe that Chronos fine-tuned with the CCL strategy outperforms the best of baselines on almost all datasets, with an improvement of 14.6% on average. For TTM, although its performance is improved with our method, it still lags behind the best baseline, particularly on the BDG-Rat and UCI datasets.\nAblation Study. To take a closer look at the contribution of the designed contrastive-aware difficulty measurer in our method, we implement a variant of our method named TSFM+CL-FT, which simply uses the performance of TSFMs as difficulty for both real and simulated samples. The zero-shot performance of our method and this variant is compared in Figure 3. We observe that TSFM+CCL-FT outperforms TSFM+CL-FT by 7.4% and 7.7% for TTM and Chronos, respectively. With further analysis, we find that TSFM+CL-FT is still better than TSFM-FT under the same experiment setting, which"}, {"title": "5 CONCLUSION", "content": "This work identifies a discrepancy between existing TSFMs and the in-use performance of TSFMs in a specific domain: building energy forecasting. To bridge this gap, we have introduced a new curriculum learning-based training method in this context, to identify the difficulty of both real-world and simulated building data, and based on this to manage the order of train set during the process of TSFM adaptation. The experiments show that the proposed curriculum design can greatly improve the zero/few-shot performance of two existing TSFM products among 1040 real buildings over three forecasting horizon settings. This work highlights the potential of curriculum learning to harness the generalized knowledge of FM tailored specifically for a specific domain."}]}