{"title": "Scalable Transformer for High Dimensional Multivariate Time Series Forecasting", "authors": ["Xin Zhou", "Weiqing Wang", "Wray Buntine", "Shilin Qu", "Abishek Sriramulu", "Weicong Tan", "Christoph Bergmeir"], "abstract": "Deep models for Multivariate Time Series (MTS) forecasting have recently demonstrated significant success. Channel-dependent models capture complex dependencies that channel-independent models cannot capture. However, the number of channels in real-world applications outpaces the capabilities of existing channel-dependent models, and contrary to common expectations, some models underperform the channel-independent models in handling high-dimensional data, which raises questions about the performance of channel-dependent models. To address this, our study first investigates the reasons behind the suboptimal performance of these channel-dependent models on high-dimensional MTS data. Our analysis reveals that two primary issues lie in the introduced noise from unrelated series that increases the difficulty of capturing the crucial inter-channel dependencies, and challenges in training strategies due to high-dimensional data. To address these issues, we propose STHD, the Scalable Transformer for High-Dimensional Multivariate Time Series Forecasting. STHD has three components: a) Relation Matrix Sparsity that limits the noise introduced and alleviates the memory issue; b) ReIndex applied as a training strategy to enable a more flexible batch size setting and increase the diversity of training data; and c) Transformer that handles 2-D inputs and captures channel dependencies. These components jointly enable STHD to manage the high-dimensional MTS while maintaining computational feasibility. Furthermore, experimental results show STHD's considerable improvement on three high-dimensional datasets: Crime-Chicago, Wiki-People, and Traffic. The source code and dataset are publicly available.", "sections": [{"title": "1 INTRODUCTION", "content": "Multivariate Time Series (MTS) forecasting involves predicting future values for multiple variables simultaneously based on historical time series data. This technique shows promise and applicability across various domains and is gaining attention due to its wide applications in forecasting natural disasters, government management, monitoring human health, and decision-making, etc. MTS differs from univariate time series by consisting of multiple time-dependent variates, each contributing to the dynamics of the dataset. Recently, the increasing amount of large-scale MTS data has led to two significant developments: the extension of time dimensions (long-time dimension) and a substantial increase in the number of channels (high-dimensional), each bringing new challenges to statistical forecasting methods. Statistical forecasting methods, are often designed according"}, {"title": "2 RELATED WORK", "content": "A channel-independent model treats each channel as an individual series, to forecast with just the given series. For the data whose channels may not have significant dependencies, channel-independent modeling simplifies the process. Statistical methods in this field are mostly global models or local univariate models, adapting to the features of data. One of the most prevalent approaches is Auto-Regressive Integrated Moving Average(ARIMA), which incorporates autoregression, differencing, and moving averages, providing a detailed understanding of time series dynamics. Another fundamental statistical method is Exponential Smoothing (ETS) applies weighting factors that decrease exponentially over time. ETS is widely used for smoothing time series data to identify trends and patterns. However, the computation speed and memory of the above methods limit their scalability for long, and high-dimensional data analysis.\nDNNs make a significant advance in handling sequential time series data. Recurrent Neural Networks (RNNs), particularly Long"}, {"title": "2.1 Channel-independent Model", "content": "A channel-independent model treats each channel as an individual series, to forecast with just the given series. For the data whose channels may not have significant dependencies, channel-independent modeling simplifies the process. Statistical methods in this field are mostly global models or local univariate models, adapting to the features of data. One of the most prevalent approaches is Auto-Regressive Integrated Moving Average(ARIMA) [3], which incorporates autoregression, differencing, and moving averages, providing a detailed understanding of time series dynamics. Another fundamental statistical method is Exponential Smoothing (ETS) [22] applies weighting factors that decrease exponentially over time. ETS is widely used for smoothing time series data to identify trends and patterns. However, the computation speed and memory of the above methods limit their scalability for long, and high-dimensional data analysis.\nDNNs make a significant advance in handling sequential time series data. Recurrent Neural Networks (RNNs), particularly Long"}, {"title": "2.2 Channel-dependent Model", "content": "The channel-dependent model recognizes and leverages the dependencies among channels, that is to forecast with both target series and auxiliary series. Recognizing these dependencies can lead to more accurate and explainable forecasts. The fundamental statistical method is Vector Auto-Regression (VAR), which captures linear dependencies among multiple time series. Despite a simple and effective framework for MTS analysis, VAR might not perform well with non-linear data. Besides, some statistical methods are used for channel dependency analysis in different aspects [49]: Correlation Analysis, Granger Causality, Mutual Information, Maximum Likelihood Estimation, Naive Transfer Entropy, Optimal Causation Entropy, and Thouless-Anderson-Palmer. The above methods are hard to scale for high-dimensional data due to slow computation.\nDNNs can be channel-dependent in linear and non-linear ways. Representative linear methods are Linear, Normalization Linear (NLinear), and Decomposed Linear (DLinear) [72]. They are linear model implemented in the Deep-learning framework. DLinear applies a linear model on the decomposed elements in different frequency domains of time series. With this, DLinear can precisely capture the seasonality, trend, and residual components. DeepVAR [43] is an RNN-based time series model with a Gaussian copula process output model. TST [73] is a pre-trained model for time series representation learning. STEP [46] learns both the spatial and temporal patterns based on TST. Crossformer [78] develops a transformer-based model to learn both relations cross time and cross dimension, separately. Graph Neural Networks (GNNs) are wildly used in forecasting [6, 9, 10, 12, 14, 15, 33, 35, 39, 45, 55, 63]. Ye et al. [68] model time series with a hierarchical graph structure to capture the"}, {"title": "3 INITIAL ANALYSIS", "content": "In this section, we conduct exploratory experiments and data analysis on a high-dimensional dataset: Crime-Chicago, a precursor to the model development phase. We employ Crossformer [78] and iTransformer [37], state-of-the-art channel-dependent transformer models, as our pre-test models. We start with a randomly selected series from the dataset as the target series. Then, we use the last 20% as the test sub-series. Subsequently, we sample different proportions of top K correlated series with target series as training data, where K \u2208 {0, 1, 5, 10, 15, 57, 114, 288, 576, 1154}, representing the proportions in {0%, 0.1%, 0.5%, 0.9%, 1.3%, 5%, 10%, 25%, 50%, 100%} respectively. Figure 1 shows the performance of training with the above data for Crossformer and iTransformer (iTransfor for short in Figure 1).\nPlease note that, both metrics in Figure 1 are metrics of errors which means that they both are the less the better. From Figure 1a-1b, we find a sharp metric downtrend at the very beginning and then was followed by an obvious rise. From Figure 1c-1d, we find when the proportion equals 50%, the performance is the best. This shows that for both channel-dependent models, for the target series, utilizing some series achieves the best performance compared with utilizing all series or none at all. We guess the reason is that utilizing 100% of the series as training data across channel-dependent models, which have full connections to all series, can potentially introduce noise stemming from unrelated connections. That means, a proper number of related series, may introduce useful patterns to forecasting. To confirm this, we perform the data analysis in Figure 2."}, {"title": "4 SCALABLE TRANSFORMER STHD", "content": "We are solving Multivariate Time Series Forecasting with a channel-dependent model for high-dimensional data. Following the common practice of existing works [42, 78], we use the sliding window to split each series into S subseries, with each containing an input and a horizon, when training, the loss computes a little overlap of series, which is similar to the common practice in NLP. Given the historical observation set of time series X = {X1,X2, ..., XM} \u2208 R^(M\u00d7T\u00d7(1+K)), which consists of M series with 1 target series, and K related series for each, all of them are in T time points. For example, in the Wiki-People prediction dataset, the series X is the traffic of each article, and K articles that show a similar temporal pattern. The output is the future \u03c4 time points' observations (Horizon) \u0423 = {Y1, Y2, ..., YM } \u2208 R^(M\u00d7\u03c4) for M series. After the sliding window process, the number of windows S = \u230a(T\u2212L)/s\u230b+2, where L is the window length, the stride is set to 1. Finally, the input X' \u2208 R^(M\u00d7S\u00d7L\u00d7(1+K)), the output Y' \u2208 R^(M\u00d7S\u00d7\u03c4).\nThe supervised learning pipeline is shown in Figure 3. First, STHD starts with the Relation Matrix Sparsity module, which refines the input data by focusing on the most relevant inter-channel relationships. According to relation matrix \u0393\u2208 R^(M\u00d7M), STHD gets K related series with 1 target series as input. This not only enhances the model's ability to capture channel dependencies but also alleviates the memory issue commonly encountered in handling high-dimensional data. This content is introduced in Section 4.1. Then, the refined data passes through the ReIndex module, which ensures efficient data processing and enhances training diversity, ultimately aiding in better model generalization. We introduce this content in Section 4.2. Finally, the 2-D Transformer, introduced in Section 4.3, with its capability to handle 2-D inputs, utilizes the processed and optimized data to capture the complex dependencies between channels over time. STHD predicts future y'. Collectively, these modules are optimized for computational efficiency, making STHD viable for high-dimensional MTS forecasting."}, {"title": "4.1 Relation Matrix Sparsity", "content": "Based on the assumption in Section 3 that not all inter-series relationships contribute equally to forecasting accuracy, STHD uses Pearson Correlation, an efficient method, to capture the extent of series co-variates. The impact of this component is twofold: it improves forecasting accuracy by reducing the influence of noise and irrelevant information and improves computational efficiency with DeepGraph as it allows the model to focus on the related series instead of all the series.\nReferring to ADLNN [49], which demonstrates the effectiveness of correlation matrix sparsity, we use correlation matrix sparsity to get K related series. However, the computation is very slow, especially for high-dimensional MTS data. To facilitate efficient relation computation and manage the relationships within the dataset, we use DeepGraph [53], a structure used for computing relations, to model high-dimensional MTS data by formulating individual series as a node and relation with another series as an edge. It enables the dynamic construction of a sparse correlation matrix by identifying and mapping significant inter-series relationships. The fast computation of pairwise correlation matrices in the DeepGraph framework is attributed to a combination of advanced data structures, robust scalability, and parallelization. Central to its efficiency are optimized data structures, tailored to handle complex and large datasets efficiently with full control over RAM usage. This is complemented by the parallelization and integration of Cython, which significantly boosts performance. To get the sparse correlation matrix \u0393, for each series, we keep the top K most related series, and set them as auxiliary series to assist target series forecasting. We compute ranks for each column of each matrix, to get the ranking matrix Rr. Through Rr, we select the top K series for each series."}, {"title": "4.2 ReIndex", "content": "The above training data X' = {x(1), x(2), ..., x(M\u00d7S)} contains M X S samples in shape of L \u00d7 (1 + K). For simplicity, we label the i-th sample as x^(i) \u2208 R^(L\u00d7(1+K)). If we follow existing work to sample batches in the S dimension, that is, each batch will be allocated b \u00d7 M \u00d7 (1 + K) data, and each with the length of L, as shown in the middle of Figure 4, where b denotes the batch size. According to existing transformer methods [42, 78], the average channel number is 200, that is M = 200. For such a relatively small number of channels, most of the DNNs including linear, CNNs, RNNs, and transformers should not have problems. However, in high-dimensional datasets, transformers may incur memory issues. In this work, to adapt transformer to high-dimension data, we design a more flexible method called ReIndex. Specifically, we sample the batch from the M \u00d7 S dimensions, as shown on the right-hand side of Figure 4. With ReIndex, the number of samples of each batch can be reduced from b \u00d7 M \u00d7 (1 + K) to b \u00d7 (1 + K), each with a length of L. This indicates that the memory usage and computational complexity of the attention map are largely decreased, so we can set more flexible batch sizes. By setting shuffle for the training data, ReIndex improves the diversity of each training batch from specific windows in all M series to any window in any series. With more diversified training examples, ReIndex is also able to improve the model's generalization ability apart from the improved memory"}, {"title": "4.3 2-D Transformer", "content": "We adapt the vanilla Transformer to encode the 2-D input in the shape of R^(L\u00d7(1+K)) as latent representations, where (1 + K) represents 1 target series and K auxiliary series.\nPatches have been proven to be effective in previous works [42, 78] due to their ability to learn long and short-term temporal dependencies. Here for each series, first we pad I repeated numbers that equal the last value at the end of the original series. Then we divide each series x of length L into P patches\nx^(i) = {x^(i)1, x^(i)2,...,x^(i)p } \u2208 R^(P\u00d7lx(1+K))\nwhere I is the length of a patch, P = \u230a(L\u2212l)/s\u230b + 2 is the number of patches for each series, s represents strides of getting patches.\nEncoder. First, we embed each patch x^(i) \u2208 R^(lx(1+K)) into the D-dimension latent space of transformer by a trainable linear function Wp \u2208 R^(D\u00d7l), and apply a sine and cosine position encoding for patches, Wpostem, to distinguish the temporal order,\nz^(i)p = Wp x^(i)p + Wpostem [:, : 1 + K]\nThen each sample with all the patches z^(i) \u2208 R^(D\u00d7(1+K)\u00d7P. Next, we apply positional encoding for channels, Wposcha, to distinguish the target series and auxiliary series, resulting in\nz^(i)d = z^(i) + Wposcha [:, : 1 + K]\nFinal output O^(i) = MultiHeadAtn(z^(i)d, z^(i)d, z^(i)d \u2208 R^((P\u00d7(1+K))\u00d7D)).\nAfter adding a residual connection, followed by layer normalization zi)' is passed through the feed-forward layer, comprising two 1-D convolutional layers Conv1() and Conv2().\nzi)' = LayerNorm2 (zi) + Conv2 (GeLU (Conv1 (zi))))\nwhere GeLU() is the Gaussian Error Linear Unit active function.\nThe attention mechanism is the most important element in each layer of the Transformer backbone. Existing works for exploring cross-time and channel relations are not enough. Channel-independent models like PatchTST use self-attention to explore time dependencies of different patches, thus ignoring the relation of channels; channel-dependent models like Crossformer use two-stage Attention, which explores relations separately. That is, they first explore time relations, then explore channel relations. Therefore, relations for patches from different series during different periods are ignored. In our work, we use self-attention to learn across the time and channels of target series and related series. Here we set attention head h \u2208 [1, H]. For each head, we transform zi)' into query matrix, key matrix, and value matrix.\nThen we get the attention output O^(i)h \u2208 R^((P\u00d7(1+K))\u00d7D", "R^((H\u00d7D": "D) represents the weight matrix of the subsequent linear transformation layer mapping the concatenated output back to the model's original dimensional space.\nDecoder. The decoder transforms shape of z^(i)d from R^(P\u00d7(1+K)\u00d7D) to R^((1+K)\u00d7D) that is suitable for the forecasting task. Specifically, the decoder first flattens the P dimension of z^(i)d, then applies a linear projection to it. Finally, we get the representation of (1+K) series. STHD gets the first vector as the representation of the target series, and then transforms it to y^(i) \u2208 R^\u03c4 that equals the horizon.\nFollowing common practice [42, 78], we use Mean Squared Error (MSE) and Adam optimizer to minimize the divergence of forecasting values and ground truth values. The loss in each series is gathered and averaged to get the overall loss L = 1/MS \u03a3(i=1 to MS) (y^(i) - y^(i))2, where y^(i) is the true value for the ith sample, y^(i) is the predicted value."}, {"title": "5 EXPERIMENTS", "content": "In this section, we report our extensive experiments on real-world high-dimensional MTS datasets to answer five questions: (1) How is the performance of our STHD approach in comparison with other counterparts for high-dimensional MTS forecasting tasks (for both effectiveness and memory issues)? (2) What is the impact of using related series? (3) How effective is the efficiency of DeepGraph? (4) How does the hyperparameter K influence the final performance? (5) How does ReIndex contribute to the forecasting performance? These questions can be matched to Section 5.3, Section 5.4, Section 5.5, Section 5.6, and Section 5.7 respectively."}, {"title": "5.1 Datasets", "content": "Three real-world high-dimensional MTS datasets are used for evaluation, namely Crime-Chicago, Wiki-People, and Traffic. Crime-Chicago and Wiki-People are larger in channel dimensions, the length is not very long. Traffic has relatively small channel dimensions, while it is longer than the other two datasets.\nCrime-Chicago extracted from the Chicago Police Department's Citizen Law Enforcement Analysis and Reporting System, reflects reported incidents of crime that occurred in the City of Chicago. We reserve the top 14 frequent crime types, aggregate other types of crime as one type across 77 communities, and process it as the monthly time series. Wiki-People records Wikipedia web traffic and was published on Kaggle competition. It is daily time series data. When processing, first we filter out the 27,786 series with missing data and 9,194 series not published in Wikipedia agent. Then we filter articles that do not have series on all access, which results in 16,117 articles (93,020 series). Finally, we selected the 'people' topic, which resulted in 1,013 articles (6,107 series). Traffic records the hourly time series of road occupancy rates measured by different sensors on San Francisco Bay area freeways."}, {"title": "5.2 Baselines and Experimental Settings", "content": "Baselines. We compare our STHD model with the following representative and state-of-the-art algorithms, including 1) channel-independent models: Linear, DLinear, Transformer, Non-stationary Transformer, PatchTST, TimesNet; 2) channel-dependent models: Crossformer, iTransformer; and 3) the Na\u00efve method, the most basic forecasting baseline that the forecasting value equal to the past value. All the methods apart from Na\u00efve are implemented on TSlib .\nParameter settings. All of models follow the same horizon length \u03c4\u2208 {6, 12, 18, 24} for Crime-Chicago, \u03c4\u2208 {28, 35, 42, 49} for Wiki-People, and \u03c4\u2208 {96, 192, 336, 720} for Traffic. We use long-term horizons on Traffic dataset to facilitate comparisons with existing studies, thereby simplifying the process of contrasting our findings with established research [42]. For the Traffic dataset, we follow most of the parameter settings as PatchTST, leaving the test batch size larger, at the same time, ensuring all the test samples are compared. Then we use grid-search to tune parameters, specifically, embedding size"}, {"title": "5.3 Results Comparison", "content": "Table 2 presents the performance of all comparison methods on the three real-world datasets, where several interesting findings are observed as follows. 1) As the simplest baseline - Na\u00efve, where the forecasting value equals its closest value. It is competitive with basic deep learning model: Linear and Transformer; 2) Linear-based methods, including Linear and DLinear, perform better than the traditional transformer-based models but worse than the patch-based transformer models in general; 3) Transformer-based channel-independent models, including Transformer, Non-stationary Transformer, TimesNet, and PatchTST demonstrate quite different performance on three datasets. PatchTST consistently performs better on three datasets, verifying the effectiveness of patches for capturing temporal dependencies on high-dimensional MTS data; 4) The transformer-based channel-dependent baselines, iTransformer performs better than Crossformer, even overall better than channel-independent models on Wiki-People. Crossformer shows a quite stable performance that can be ranked at the middle of all baselines on high-dimensional MTS data. However, during the evaluation process, it is very time-consuming to run both, making it hard to scale well on high-dimension MTS data due to its modeling of full relations of all channels; 5) Finally, our STHD earns an overall best performance on all three high-dimensional datasets, with improvements of 30.02%, 10.20%, and 5.43% on Crime-Chicago, Wiki-People, and Traffic respectively.\nTraining with a single A100 graphic card, CUDA out-of-memory error occurs when we use Crossformer at 720"}, {"title": "5.4 Related Series", "content": "To better demonstrate the impact of the related series, we made the following comparison: a) target series with top K related series as the input; b) target series with top K unrelated series as input to our method STHD; c) target series without any series. Figure 5 shows the result on Crime-Chicago. Yellow, blue, and grey bars represent experimental results from a), b), and c) separately. With related series, yellow bars are overall twice as good as the blue bars across all measures. Without related series, grey bars are overall better than blue bars, which demonstrates the importance of reducing the introduction of noise from unrelated series. To sum up, the related series significantly benefits the task, demonstrated by its improved performance on different measures."}, {"title": "5.5 Efficiency of DeepGraph", "content": "To verify the efficiency of DeepGraph, we compare the run time of computing correlations with the DeepGraph graph structure and computing without DeepGraph (i.e., We use the correlation method embedded in Nerd instead of DeepGraph). We compare them to the Wiki-People dataset considering its largest number of dimensions compared with the other two datasets.\nRunning time is shown in Figure 6, we can observe that Nerd in black bars is 120.2 seconds, while DeepGraph in red bars are 6.1 seconds. In this way, we achieve a 20 times speedup compared with a na\u00efve pair-wise computation."}, {"title": "5.6 Correlation Sparsity", "content": "Figure 7 shows the experimental results for the change of hyperparameter K on Crime-Chicago, Wiki-People, and Traffic, respectively. A similar trend with the change of K on all the datasets is observed. With the increase of K, the performance first obviously drop, and then slightly rise. The possible reason is that at the very beginning, the increased K means the model can benefit from real related series, while when K becomes too large, the series becomes less relevant which will introduce noise to the model."}, {"title": "5.7 ReIndex", "content": "Note that ReIndex is designed to alleviate the memory issue in computing the attention map and it makes the training stage scalable to larger batch sizes, as shown in Figure 4 and Appendix. However, we find that apart from this, ReIndex is also able to improve the model's effectiveness as shown in Table 3. In this table, the following comparison is conducted on the Crime-Chicago dataset: take the target series as the input, and use STHD with ReIndex STHDwt and without ReIndex STHDwto to forecast. The results show that STHDwt outperforms STHDwto across all measures across all horizons."}, {"title": "6 CONCLUSION", "content": "This paper proposed a scalable Transformer framework - STHD, designed specifically for tackling the intricate challenges of high-dimensional Multivariate Time Series (MTS) forecasting. Through the innovative incorporation of sparse correlation matrix, STHD significantly enhances forecasting performance by efficiently filtering out unrelated series. Furthermore, the introduction of the ReIndex strategy emerges as a dual-purpose solution, mitigating the constraints of parameter ranges in training transformer-based methods on high-dimensional MTS data, while at the same time enriching"}, {"title": "A DATA ANALYSIS AND PROCESSING", "content": "Before this work, none of the time series works used the Crime-Chicago dataset for MTS forecasting, so we conducted data analysis for Crime-Chicago. Figure 8 shows the correlation of different communities, the x-axis, and y-axis represent the community area. To draw this figure, we add up all crime types of the series from the same community. The color scale on the right indicates the strength and direction of the correlation coefficient: Dark blue represents a correlation coefficient close to 1, indicating a strong positive correlation; Lighter blue and white represent correlation coefficients closer to 0, indicating little or no linear correlation; The red shades (which are not prominently visible in this matrix) would indicate a negative correlation, with dark red being close to -1. Through the figure, we can clearly see different color distributions, which illustrates why STHD uses the correlation method."}, {"title": "B THEORETICAL ANALYSIS", "content": "Time. Relation Matrix Sparsity module, with its O(M2) complexity of M series, is the most computationally intensive part, primarily due to pairwise correlation computations. Because of that, we leverage parallel computing and efficient data structures to mitigate the computational load. The ReIndex module, while crucial for data optimization, earns the same complexity as other works. The Transformer-based model's primary complexity stems from its attention mechanism. Unlike PatchTST, which focuses solely on time dependency with a complexity of O(P2) with P patches within a subseries, Adapted Transformer's complexity is primarily dependent on the attention: O(((1+K)SP)2) for S subseries of each series. However, previous Sparsity and ReIndex steps effectively reduce the input size where K < 20, (1 + K) << M, mitigating the quadratic dependency. Improvements in accuracy for high-dimensional datasets also counterbalance this.\nMemory. Relation Matrix Sparsity uses O(MK) where K << M is the most significant correlation. For M time series, the naive method stores a full correlation matrix that requires O(M2). ReIndex reduces samples from b \u00d7 M \u00d7 (1 + K) to b \u00d7 (1 + K). Adapted Transformer: needs extra memory than other transformers for attention O((KS)2) and related layers."}, {"title": "C EXPERIMENTAL SETTING", "content": "All DNNs models including pre-test, ablation study on STHD, and baselines are implemented in PyTorch and trained on a single NVIDIA A100 GPU. The parameter configurations for the Crime-Chicago and Wiki-People are outlined in Table 4. For Traffic, we follow the setting of PatchTST. To allow each model to converge during training, we set the training epoch to 100, and use an early stop setting, which means if the delta of the validate set loss is less than e-7, the training will stop and begin to test."}, {"title": "D PERFORMANCE ON LOW-DIM DATASETS", "content": "While the STHD is designed with high-dimensional MTS forecasting, theoretically, it can be applied to low-dimensional series. Some further experiments on lower-dimensional data have been done on Weather.\nWe have the following findings: 1) channel-dependent model Crossformer performs well when the horizon equals 192 and 720 on MAE, which may contribute to less noise introduced in low-dimensional datasets; 2) patch-based method PatchTST and multi-frequency-based method TimesNets show their ability to capture long-term dependencies; 3) by sampling correlated series (K = 1), STHD still works on low-dimensional datasets."}]}