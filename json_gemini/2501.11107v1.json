{"title": "CHAOSEATER: FULLY AUTOMATING CHAOS ENGINEERING WITH LARGE LANGUAGE MODELS", "authors": ["Daisuke Kikuta", "Hiroki Ikeuchi", "Kengo Tajiri", "Yuusuke Nakano"], "abstract": "Chaos Engineering (CE) is an engineering technique aimed at improving the re- silency of distributed systems. It involves artificially injecting specific failures into a distributed system and observing its behavior in response. Based on the ob- servation, the system can be proactively improved to handle those failures. Recent CE tools realize the automated execution of predefined CE experiments. However, defining these experiments and reconfiguring the system after the experiments still remain manual. To reduce the costs of the manual operations, we propose CHAOSEATER, a system for automating the entire CE operations with Large Lan- guage Models (LLMs). It pre-defines the general flow according to the systematic CE cycle and assigns subdivided operations within the flow to LLMs. We assume systems based on Infrastructure as Code (IaC), wherein the system configurations and artificial failures are managed through code. Hence, the LLMs' operations in our system correspond to software engineering tasks, including requirement definition, code generation and debugging, and testing. We validate our system through case studies on both small and large systems. The results demonstrate that our system significantly reduces both time and monetary costs while complet- ing reasonable single CE cycles.", "sections": [{"title": "1 INTRODUCTION", "content": "Modern software-based services, such as streaming, e-commerce, and conversational AI platforms, are implemented as distributed systems, where each service is divided into smaller services accord- ing to specific functionalities. These small services (i.e., functions), along with the communication network that connects them, constitute the entire service. This design, known as microservice ar- chitecture (Bucchiarone et al., 2020), enables scalable and continuous deployment while supporting the integration of heterogeneous technologies. On the other hand, the complex dependencies among small services can lead to unexpected, chaotic behavior in the entire system from even minor fail- ures. However, proactively predicting and addressing such complex behavior is challenging.\nTo address this and improve the resiliency of distributed systems, numerous organizations, including Netflix, Amazon, and Microsoft, have recently adopted Chaos Engineering (CE) (Basiri et al., 2016; 2019). Its concept is that rather than predicting the chaotic behavior, let's observe it directly by artificially injecting the failures into the system. Based on the observation, we can proactively rebuild a new system that is resilient to the assumed failures. Systematically, CE cycles through four phases for a system:\n1. Hypothesis: Define steady states (i.e., normal behavior) of the system and injected failures. Then, make a hypothesis that the steady states are maintained in the system even when the failures occur.\n2. (Chaos) Experiment: Inject the failures into the system while logging the system's response behavior."}, {"title": "2 PROPOSED SYSTEM: CHAOSEATER", "content": "In this section, we describe the technical details of CHAOSEATER. Figure 1 shows its simplified system diagram. It takes as input instructions for the CE cycle (optional) and a folder containing K8s manifests and a Skaffold configuration file . In short, K8s manifests are system configuration files that define the resources (i.e., small services) that constitute a system, while a Skaffold configuration file defines the process to automatically deploy those re- sources in a K8s cluster. It then conducts a CE cycle for those inputs through five divided phases: pre-processing, hypothesis, experiment, analysis, improvement, and post-processing phases. Finally, it outputs a summary of the completed CE cycle and a modified folder containing K8s manifests that have been modified to satisfy the hypothesis defined in the hypothesis phase and their Skaffold con- figuration file.\nTo ensure that the LLMs perform as intended, our system fixes the general flow according to the systematic CE cycle. It then guides LLMs by assigning them subdivided CE operations within this flow. Hereafter, we define each LLM assigned a CE operation as an agent. Our system prepares prompt templates for each agent, which include placeholders that can be dynamically filled with text. Therefore, once a user inputs the data, prompts for each agent are dynamically generated ac- cording to that data, and the internal agents autonomously complete the flow (i.e., CE cycle). To facilitate data processing within our system, all agents output JSON data. This is achieved by in- structing agents in their input prompts to output text in JSON format, and then parsing the output text as JSON data. Our system uses the JSON output instruction and parser of LangChain . Our system has 18 agents, 21 system prompts, 26 user prompts, and two AI prompts. In the following sections, we describe the details of our system's internal process from input to output, breaking it down into the five phases. See Appendix A for the detailed agentic workflow and all our system's prompt templates."}, {"title": "2.1 PHASE 0: PRE-PROCESSING", "content": "Given user inputs, our system first deploys the user's system to the K8s cluster by running the Skaffold configuration file. Then, the agents sequentially process the user inputs as follows:\n1. Summarize each of the input K8s manifests separately.\n2. Identify potential issues for resiliency and redundancy in the K8s manifests.\n3. Assume a possible application of the K8s manifests.\n4. Summarize user instructions for the CE cycle if provided. At the same time, filter out suspi- cious prompts, e.g., jailbreak prompts.\nThis phase is for deploying the user's system and explicitly filling in the implicit context of the user's input. In the subsequent phases, this added context will also be provided as input."}, {"title": "2.2 PHASE 1: HYPOTHESIS", "content": "The hypothesis phase defines the system's resiliency for an assumed failure scenario. Following the principles of CE , our system first defines steady states and then defines failure injections.\nSteady-state definition Steady states are the expected, normal behaviors of a system. Each steady state is defined by a pair of a state value and a threshold, and a steady state is considered satisfied when the state value meets the threshold. Therefore, the state values must be measurable outputs of the system, such as the number of active resources, error rates, and response time. Given the pre-processed user inputs, the agents define steady states as follows:\n1. Select measurable states critical to maintaining the system's application. If any weak config- urations are identified from the K8s manifests, their related states are preferentially selected.\n2. Select tools to inspect the states. K8s API and k6 (Grafana Labs, 2021) are supported. Then, write the corresponding inspection scripts and inspect the current (normal) values of the states in the system by running the scripts.\n3. Define the thresholds for each state based on the inspected values (steady states must be sat- isfied under the current condition).\n4. Write unit-test scripts to validate whether each steady state is satisfied by adding threshold- based assertions to the corresponding inspection scripts."}, {"title": "2.3 PHASE 2: (CHAOS) EXPERIMENT", "content": "The experiment phase plans a chaos experiment to validate the hypothesis and executes it.\nExperiment planning To enable systematic planning, we divide a chaos experiment into three phases: pre-validation, failure-injection, and post-validation. In the pre-validation phase, VaC scripts are executed to ensure that the steady states are satisfied under normal conditions. In the failure-injection phase, fault injections are executed. If a steady state, such as response time, needs to be validated during fault injections, the corresponding VaC scripts are executed concurrently. In the post-validation phase, VaC scripts are executed to ensure that steady states have been recovered after the fault injections. Given the pre-processed user inputs and the hypothesis, the agents plan a chaos experiment by dividing it into these three phases as follows:\n1. Determine the duration of each phase.\n2. Determine the VaC scripts and failure injections to be executed in each phase. For each of them, specify the duration and grace period within a range that does not exceed the duration of the phase.\n3. Summarize the timeline of the chaos experiment (i.e., the order of each node) in detail. This summary is referred to when analyzing the experiment results.\nIn step 2, the agent outputs a list of dictionaries (i.e., schedule list) separately for each phase, with each dictionary containing three keys: name, grace_period, and duration. The name is either a steady state name or a failure type name, and each corresponds one-to-one with the VaC script and failure injection defined in the hypothesis. The grace_period is the waiting time from the start of each phase until the execution of the VaC script or failure injection, allowing flexible adjustment of the execution timing. The duration is the execution period after the grace period.\nBased on these schedule lists, our system configures the chaos experiment using the Chaos Mesh workflow. This workflow supports three types of nodes: failure node to execute failure injection,"}, {"title": "2.4 PHASE 3: ANALYSIS", "content": "After the chaos experiment is finished, our system mechanically checks whether the VaC scripts have passed. If all of them have passed, that means the current system configurations (i.e., K8s manifests) already satisfy the hypothesis. Therefore, our system finishes the current CE cycle at this point and moves to the extra phase. If at least one has failed, our system moves to the next improvement phase after analyzing the experiment results. In this analysis, given the K8s manifests, the timeline of the chaos experiments, and the list of failed scripts with their logs, the agent identifies the cause of the fails and then generates a report containing the causes and recommended countermeasures."}, {"title": "2.5 PHASE 4: IMPROVEMENT", "content": "The improvement phase reconfigures the K8s manifests to satisfy the hypothesis. Given the K8s manifests, the hypothesis, the experiment plan, and the improvement loop history, the agent recon- figures the K8s manifests so that all the VaC scripts pass in the chaos experiment. The improvement loop history stores the history of the experiment results, their analysis reports, and their reconfigu- rations, within the improvement loop. The history suppresses the repetition of the same reconfigu- ration. There are three reconfiguration modes: create, delete, and replace. The agent first selects the reconfiguration mode while specifying the file name, and then writes the reconfigured K8s manifest only for the create and replace modes. The file manager of our system then edits the folder from the previous improvement loop (in the first improvement, it corresponds to the user's input folder) according to the agent's output. Figure 5 illustrates these reconfiguration processes. The verification loop is also conducted here: the agent's output is debugged repeatedly until all the K8s manifests in the edited folder are correctly applied to the K8s cluster.\nImprovement loop After the reconfiguration, our system applies the reconfigured K8s manifests to the K8s cluster. Then, they will be validated again through the experiment and analysis phases. That is, as in the systematic CE cycle, our system also repeats the experiment, analysis, improvment phases until the hypothesis is satisfied. We define this loop as the improvement loop."}, {"title": "2.6 EXTRA PHASE: POST-PROCESSING", "content": "After the CE cycle is completed, our system finalizes its entire process by summarizing the com- pleted CE cycle. The agent summarizes the user's input and each of the four completed phases. Finally, our system provides the user with the summary of the completed CE cycle and the folder containing K8s manifests that have been reconfigured to satisfy the hypothesis defined in the hy- pothesis phase and their Skaffold configuration file."}, {"title": "3 CASE STUDY", "content": "In this section, we validate the entire process of our system through case studies on two different scale systems: NGINX and SOCKSHOP . NGINX is a small-scale system that consists of two K8s manifests (i.e., two resources): pod.yaml and service.yaml. The for- mer defines a Pod resource including a Nginx server, and the latter defines a Service resource routing TCP traffic to the Pod. To verify whether our system can improve the system when there are resiliency issues, we intentionally configure the resource with a non-resilient setting; we set restartPolicy to Never in Pod.yaml. With this configuration, once the Pod goes down, it will never restart, resulting in extended service outages. On the other hand, SOCKSHOP is a prac- tical and large-scale e-commerce system that consists of 29 manifests, which define the resources and databases for front-end pages, user information, order, payment, shipping, and so on. The num- ber of replicas of all the Deployment resources is originally set to one. However, this setting could lead to downtime of the single replica when it goes down. To narrow down this original resiliency issue to a single point, we increase the replicas for Deployment resources other than front-end-dep.yaml to two, while keeping a single replica for front-end-dep.yaml. This RELATIVELY reduces the redundancy/resiliency of the front-end resource. In this case study, we validate whether our system correctly identifies and addresses these resiliency issues through a reasonable CE cycle.\nLong-term experiments are not required for the resiliency issues here. Therefore, to save time, we input the following instruction along with the K8s manifests: \u201cChaos-Engineering experiment must be completed within 1 minute\". For SOCKSHOP, we additionally instruct on how to access its web page as follows: \u201cWhen using k6 in steady-state definition, always select a request URL from the following options (other requests are invalid): 1. http://front-end.sock-shop.svc.cluster.local/, 2. http://front-end.sock-shop.svc.cluster.local/detail.html?id=ID, ...\u201d. We use GPT-40 (gpt-40-2024- 08-06) as LLMs for our system. To improve the reproducibility of this case study, its temperature is set to zero with the seed fixed at 42. We run a single CE cycle for each system five times under the same settings. In the following, we first discuss the aggregated results obtained from multiple runs of single CE cycles: the time and monetary costs, the completion rate, and the reconfiguration rate. Then, we qualitatively validate the operations within the CE cycles conducted by our system. See also Appendix B for more details on the inputs and outputs for each system."}, {"title": "4 RELATED WORK", "content": "Chaos engineering Since Basiri et al. introduced its name, CE has gained attention and is currently employed in various services . The major research question of CE is how we can efficiently test meaningful failures to identify system vulnerability. To address this, various optimization methods for failure selection have been proposed  . In application, several automation tools have been developed from both the open-source software  and commercial sectors . While these technologies have advanced CE automation, its full automation has not yet been achieved due to the complexities of generative tasks, such as hypothesis formulation and reconfiguration. Our work is the first to fully automate CE with LLMs.\nLLMs for software engineering LLMs for coding have been actively explored from various as- pects: Pretraining models , prompt engineering , and evaluation . For more general SE tasks, LLMs that solve issues in GitHub repositories have also emerged . Since CE for software systems are regarded as SE, our work can be considered a part of this trend. Unlike existing SE benchmarks, CE requires autonomously setting goals and achieving them. Our work demonstrates the potential of LLMs to tackle such new complex SE tasks.\nLLMs for networking (NW) LLMs for networking have also been explored from various aspects in recent years: Datasets , benchmarks , fine-tuned models , an agent framework for NW-related tasks , and comprehensive evaluation . These works empirically demonstrate the promise of applying LLMs to the NW domain. In parallel with the research side, applications have also been developed, especially for software systems. They range from LLM-based IaC code generation  to diagnostic tools  and misconfiguration remediation . Despite the advancements of LLMs in the NW domain, their application to CE remains unexplored. Our work is the first to demonstrates the capabilities of LLMs in CE, which involves complex NW operations.\nConcurrent works In parallel with our work, some other projects have shown promising results in applying LLMs to CE. From a security perspective, Bedoya et al.  leverage LLMs to con- struct attack-defense trees, which assist security analysts in designing security chaos experiments. The major differences from ours lie in their use of LLMs as a supplementary support tool and de- signing the LLM workflow specialized to the hypothesis phase based on the methodology of security CE. AIOpsLab is an LLM agent-based framework for automating the operations of cloud (i.e., software) systems. It includes failure injection as a feature and supports both func- tional failures, such as system misconfigurations, and independent symptomatic failures, such as server downtime and network delays. However, while it provides cause analyses and solutions for functional failures, it is limited to identifying the presence of failure and pinpointing their locations for symptomatic failures. On the other hand, our system focuses on the symptomatic failures and provides more sophisticated CE for them. For example, it supports complex failure scheduling to simulate symptomatic failure scenarios, analyzes system behavior during symptomatic failure in- jection, and enables reconfigurations to address unexpected behavior due to the failures. While these concurrent works, including ours, share a similar goal, each focuses on a different aspect of CE. We hope to integrate the strengths of each work in the future to realize more comprehensive LLM-enhanced CE."}, {"title": "5 DISCUSSION", "content": "Broader impacts Numerous systems, including the increasing number of LLM applications in recent years, are built in the microservice architecture, and their number is expected to continue to grow in the future. By fully automating CE, it will be possible for anyone to build resilient systems. Moreover, it is expected to combine our system with other LLM systems, such as improving the resiliency of applications created by other LLM systems through our system. Although our system is not yet at a practical level, we believe that our system would be a good starting point toward such use cases. Even at its current level, our system can be sufficiently used as training materials (including both good and bad practices) for the Chaos Game Day, which is a training exercise for CE engineers.\nLimitations Our system currently has three limitations: 1) Limited deployment environment; Al- though CE should ideally be conducted in actual production environments, our system is currently only supported in development environments. 2) Limited to GPT-40 only; Our system's prompt tem- plates are highly tuned only for GPT-40. Therefore, other LLMs can not currently be used for our system. 3) Vulnerability discovery; In the case study, our system improved systems with relatively simple resiliency issues. However, for systems that already possess a certain level of resiliency, our system fails to find new hidden issues through a CE cycle. Given that this is a challenging task even for human engineers, our system is currently considered to perform at a level comparable to, or lower than, that of engineers. To find such issues, it is necessary to conduct multiple CE cycles over extended operational periods.\nFuture directions Given the current limitations above, we share several future directions for our system and the full automation of system resiliency improvement:\n1. Full automation of long-term multiple CE cycles; By using the system's output as input for the next CE cycle, we can automate multiple CE cycles even with our current system. However, we additionally need to develop techniques to manage the long-term history of completed CE cycles and continuous learning (if LLMs are fine-tuned).\n2. Support for various LLMs; As our system's prompt templates are tuned manually, support- ing various LLMs significantly increases their management costs. To address this, automatic prompt tuning is considered an effective solution. Our current prompt templates may be used as the seed prompts.\n3. Fine-tuning LLMs specifically for CE; Fine-tuning is necessary to improve the quality of CE cycles and expand supported LLM types. Our system's outputs may be used as the instruction- tuning data.\n4. Evaluation frameworks; There are no datasets and benchmarks for CE. New metrics should be also carefully proposed.\n5. Production deployment and security; If our system is deployed in production environments, further research on security will be necessary. This includes controlling more carefully the impact range of failures (i.e., blast radius), preventing our system from becoming a proxy for attacking production services, and proposing emergency response measures, such as a higher- level monitoring system that always monitors our system.\n6. Improvement for more complex systems; We need to incorporate the recent advances in LLMs x graph approaches to extract necessary sub-graphs from large system graphs. This sub-graph extraction is important to organize the agent's inputs in each phase."}, {"title": "6 CONCLUSION", "content": "In this paper, we proposed CHAOSEATER, a system for automating the entire CE workflow with LLMs. We presented the technical details of our system and validated it through case studies of small and large systems. The results demonstrated that our system successfully reduces the time and monetary costs while completing reasonable single CE cycles. In future work, we will improve our system following the future directions discussed above. We are also excited that other researchers and developers will propose related works in the same or different directions. We hope this paper helps promote the full automation of system resilience improvement."}]}