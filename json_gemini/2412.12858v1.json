{"title": "Efficient Speech Command Recognition Leveraging Spiking Neural Network and Curriculum Learning-based Knowledge Distillation", "authors": ["Jiaqi Wang", "Liutao Yu", "Liwei Huang", "Chenlin Zhou", "Han Zhang", "Zhenxi Song", "Min Zhang", "Zhengyu Ma*", "Zhiguo Zhang"], "abstract": "The intrinsic dynamics and event-driven nature of spiking neural networks (SNNs) make them excel in processing temporal information by naturally utilizing embedded time sequences as time steps. Recent studies adopting this approach have demonstrated SNNs' effectiveness in speech command recognition, achieving high performance by employing large time steps for long time sequences. However, the large time steps lead to increased deployment burdens for edge computing applications. Thus, it is important to balance high performance and low energy consumption when detecting temporal patterns in edge devices. Our solution comprises two key components. 1). We propose a high-performance fully spike-driven framework termed SpikeSCR, characterized by a global-local hybrid structure for efficient representation learning, which exhibits long-term learning capabilities with extended time steps. 2). To further fully embrace low energy consumption, we propose an effective knowledge distillation method based on curriculum learning (KDCL), where valuable representations learned from the easy curriculum are progressively transferred to the hard curriculum with minor loss, striking a trade-off between power efficiency and high performance. We evaluate our method on three benchmark datasets: the Spiking Heidelberg Dataset (SHD), the Spiking Speech Commands (SSC), and the Google Speech Commands (GSC) V2. Our experimental results demonstrate that SpikeSCR outperforms current state-of-the-art (SOTA) methods across these three datasets with the same time steps. Furthermore, by executing KDCL, we reduce the number of time steps by 60% and decrease energy consumption by 54.8% while maintaining comparable performance to recent SOTA results. Therefore, this work offers valuable insights for tackling temporal processing challenges with long time sequences in edge neuromorphic computing systems.", "sections": [{"title": "Introduction", "content": "Recognized as the third generation of neural networks (Maass 1997), spiking neural networks (SNNs) effectively mimic the dynamics of biological neurons with sparse and asynchronous spikes. This approach enables SNNs to achieve high computational performance with low energy consumption, making them a viable energy-efficient alternative to artificial neural networks (ANNs) (Fang et al. 2023).\nRecent studies (Kugele et al. 2020; Yao et al. 2021; Kim et al. 2023; Zhu et al. 2024) have demonstrated that the spiking mechanisms of SNNs excel in handling temporal data and event-driven applications, capitalizing on their inherent spatio-temporal encoding capabilities. These unique capabilities make SNNs exceptionally well-suited for processing constantly evolving dynamic content, such as speech and audio. While ANNs have demonstrated effectiveness in handling various acoustic features like Mel-frequency cepstrum coefficients (Logan et al. 2000), Mel spectrograms (Shen et al. 2018), and other audio representations in speech recognition tasks, their inherent architecture inevitably leads to high energy consumption (Yang et al. 2022). This poses a non-negligible obstacle when deploying ANNs in speech command recognition (SCR) in energy-restricted scenarios, such as mobile and wearable devices, where both high accuracy and low energy consumption are crucial. In response, the natural temporal dynamics and event-driven property inherent in SNNs not only align with the properties of audio but also cater to the urgent needs for efficiency and effectiveness in speech processing systems, offering a promising solution for developing energy-efficient neuromorphic technologies (Xu et al. 2023a).\nSince the inherent capabilities of SNNs align well with the requirements of SCR, an SNN-based model is essential to fully exploit these strengths. However, two critical challenges remain in developing such a high-performance and energy-efficient model for SCR. First, the efficient learning of contextual information across multi-level remains largely unexplored. While local context captures fine-grained details, it lacks an overall perspective, which will hinder the understanding of the command's intent and increase sensitivity to noise. Conversely, global context provides a broader understanding but may miss essential details needed to differentiate similar commands. Second, achieving a balance between high performance and energy efficiency has been largely overlooked. Recent SNN studies (Bittar and Garner 2022; Hammouamri, Khalfaoui-Hassani, and Masquelier 2024; Deckers et al. 2024) have leveraged the embedded time sequences as time steps. This approach aligns well with the dynamics of SNNs, as it avoids artificially increasing dimensions through repetition. However, directly using these time sequences can lead to higher accuracy but also increases the energy burden. A potential solution involves"}, {"title": "Related Works", "content": "There are primarily two major approaches to obtain a trained SNN. One is ANNs-to-SNN conversion (Deng and Gu 2021; Bu et al. 2022; Xu et al. 2023b), where a high-performance pre-trained ANN is transformed into an SNN by replacing its ReLU activation layers with spiking neurons (Rathi and Roy 2021). While the converted SNNs can achieve excellent performance, they often require a long-term simulation with hundreds or even thousands of time steps to accurately approximate the ReLU activations, which leads to large latency (Su et al. 2023; Guo et al. 2024). The other is direct training, the application of backpropagation-through-time (BPTT) on SNNs enables end-to-end training from scratch (Neftci, Mostafa, and Zenke 2019). Notably, this approach involves using surrogate gradient functions to approximate the derivatives of the non-differentiable Heaviside step function, which is essential for the spiking mechanism (Lee, Delbruck, and Pfeiffer 2016; Lee et al. 2020). Recent impressive works (Sengupta et al. 2019; Fang et al. 2021; Zhou et al. 2023; Yao et al. 2024) have dedicated efforts to translating de-facto standard models from ANNs into new architectures that conform to the calculation characteristics of SNNs.\nSpeech command recognition, also known as keyword spotting, demands accurate recognition and low energy consumption, and it is crucial for numerous edge computing applications (Warden 2018). Recent SNNs have demonstrated considerable promise in meeting these requirements. Several studies (Yang, Liu, and Li 2022; Dampfhoffer et al. 2022) have concentrated on enhancing network architecture to achieve high accuracy and energy efficiency in low-resource settings. Notably, further developments (Hammouamri, Khalfaoui-Hassani, and Masquelier 2024; Sun et al. 2023b; Deckers et al. 2024) involve novel methods for learning delays in SNNs, addressing the critical challenge of plastic delays for the timing of spike arrivals. Additionally, recent works (Bittar and Garner 2022; Zhang et al. 2024b; Deckers et al. 2024) have proposed innovative spiking neurons for better learning dynamic time series. Moreover, substantial efforts (Stewart et al. 2023; Cramer et al. 2020) have been made to provide novel artificial cochleas models or spike generation pipelines, contributing to the advancement of neuromorphic speech recognition systems.\nThe aforementioned works often rely on long time sequences to approximate hundreds or more time steps, leading to a tendency towards higher energy consumption. The challenge of optimizing the balance between energy consumption and performance is unresolved. Resolving this challenge could significantly improve the efficiency and deployment of neuromorphic technologies in power-sensitive environments, enhancing both energy sustainability and the accessibility of speech processing applications."}, {"title": "Methods", "content": "Spiking neurons are a crucial component of SNNs, which can serve as bio-plausible abstractions (Rathi et al. 2023). We adopt the Leaky Integrate-and-Fire (LIF) neuron, which has been widely proven effective (Roy, Jaiswal, and Panda 2019). The dynamics of a LIF neuron are described as:\n$H[t] = V[t - 1] + \\frac{1}{T}(X[t] - (V[t - 1] - V_{reset})), (1)$"}, {"title": "SpikeSCR Architecture", "content": "Spike/Spec Augment Module (SAM). As shown in Figure 1(b), we utilize the augmentation by SpecAugment (Park et al. 2019) with frequency mask and time mask for the Mel spectrogram of the input audio. For the spike trains, we utilize a method similar to EventDrop (Gu et al. 2021), which involves two operations: drop-by-time and drop-by-neuron, both performed at a fixed ratio to drop events in different dimensions randomly. Due to space constraints, the implementation details are provided in Appendix A.\nThe SEM is structured by sequentially stacking {Conv1D-BN-LIF}. Given the input X, the process of SEM can be formulated as,\n$SEM(X) = SN(BN(Conv(X))), (4)$"}, {"title": "KDCL", "content": "As illustrated in Figure 3, we propose an effective curriculum learning-based knowledge distillation method (KDCL) to optimize the trade-off between performance and energy consumption. We define the teacher model's initial learning from longer time steps as the easy curriculum. This knowledge is progressively transferred to student models with shorter time steps, defined as the hard curriculum. The learning process of KDCL is thoroughly described in Algorithm 1. This approach effectively enhances student model's performance by incrementally transferring valuable representations from long to short time steps.\nFor clarity, we will describe the knowledge distillation process within a single curriculum. It is straightforward to generalize this process to multiple curricula, since the same method is consistently applied in each curriculum setting. We denote S(x) and T(x) as the output logit vectors of the student and the teacher for input x, respectively. Formally, given a mini-batch, we first obtain the probability distribution Ps(x; \u03c4) of the S(x),\n$P_S(x; \\tau) = softmax (S(x)/\\tau), (11)$\nwhere \u03c4 denotes a temperature parameter (Kim et al. 2021). The probability distribution PT(x; \u03c4) of the T(x) is computed in the same manner. Further, we focus on the Kullback-Leibler (KL) divergence between the two probability distributions (Wang, Cheng, and Lim 2022; Yang and Chen 2023). The divergence $D_{KL}$ between PT(x; \u03c4) and Ps(x; \u03c4) is computed for each instance as Eq. (12). Finally, the weighted loss $L_{KD}$ is determined by averaging the $D_{KL}$ across all M instances in the batch:\n$D_{KL}(P_T||P_S) = \\sum_i P_T log (\\frac{P_T}{P_S}), (12)$"}, {"title": "Training Strategy", "content": "Note that the training of our model adopts the BPTT with surrogate gradients. We first introduce the input and output structure of SpikeSCR. Similar to the previous methods (Bittar and Garner 2022; Hammouamri, Khalfaoui-Hassani, and Masquelier 2024), the proposed models take inputs of size (B, T, N), where B is the batch size, T is the number of time steps, and N is the number of input neurons or frequency bins. The number of classes is denoted by Y. The classification head outputs are each neuron's synaptic potential through the time steps T. The method used to aggregate information from these time steps can be described as:\n$out_i[t] = softmax(s_i[t]) = \\frac{e^{s_i[t]}}{\\sum_{j=1}^Y e^{s_i[j]}}, (14)$\nwhere si[t] (t = 1, 2, ..., T) is the synaptic potential of neuron i at time t. Then the final outputs of the model after T time steps return the size (B, C), which can be defined as:\n$Y_i = \\sum_{t=1}^{T} out_i[t]. (15)$\nThe cross-entropy loss $L_{CE}$ for the classification task is calculated as Eq. (16). For the multi-task in a specified curriculum, the training objective $L_{MT}$ is denoted as Eq. (17),\n$L_{CE} = - \\frac{1}{N} \\sum_{n=1}^{N} log(softmax(y_n)[y_n]), (16)$\n$L_{MT} = \\lambda_1 L_{CE} + \\lambda_2 L_{KD}, (17)$\nwhere $L_{KD}$ is detailed in Eq. (13). We set \u03bb1 = 1.0 and \u03bb2 = 0.5, which is refined through experiments, to balance the gradients in the overall learning process."}, {"title": "Experiments", "content": "We evaluate our methods on two spiking datasets, SHD (Spiking Heidelberg Digits) and SSC (Spiking Speech Commands) (Cramer et al. 2020), and a non-spiking counterpart of SSC, GSC (Google Speech Commands V2) (Warden 2018). Both spiking datasets are constructed using artificial cochlear models to convert audio recording data to spikes. The SHD dataset consists of 10k recordings of 20 different classes, with spoken digits ranging from zero to nine in both English and German. SSC and GSC are much larger datasets, each containing 100k recordings. The task on SSC and GSC is the classification of all 35 different speech commands. We utilize the same preprocess methods (Hammouamri, Khalfaoui-Hassani, and Masquelier 2024) to all three datasets. For the two spiking datasets, the input neurons were reduced from 700 to 140 by using spatio-temporal bins across every 5 neurons. As for the temporal dimension, we used a discrete time-step (which means at each \u0394t interval, the number of spikes is extracted) and a zero right-padding to ensure the same time duration (used as the time steps) of all recordings in one batch. For the GSC dataset, we used the Mel spectrogram of the waveforms with 140 frequency bins and approximately the same time steps consistent with the SSC. The dataset statistics and experimental setups are detailed in Appendix B. The empirical curriculum setting in KDCL is described as follows. For the SHD dataset, KDCL training involves two curricula from easy to hard: 100 time steps and 40 time steps, as the smaller dataset risks overfitting with excessive time steps. For the larger SSC and GSC datasets, KDCL involves four curricula from easy to hard: 500 (400), 200, 100, and 40 time steps."}, {"title": "Main Results", "content": "We compare our method to recent SNN works on the SHD (20 classes), SSC, and GSC (both with 35 classes) benchmark datasets in terms of accuracy and model size, as shown in Table 1. Even without knowledge transfer, our models surpass recent SOTA approaches across three benchmarks with the same time steps, including those focusing on learning delays in SNNs and modeling neuron dynamics. Notably, SpikeSCR achieves an accuracy of 82.54% on the SSC dataset with 100 time steps, demonstrating significant improvements compared to the current method DCLS (Hammouamri, Khalfaoui-Hassani, and Masquelier 2024), which achieves 80.69% accuracy with more parameters (2.5M vs. 1.63M). Moreover, the application of KDCL results in substantial improvements, achieving competitive results of 93.60%, 80.25%, and 95.01% on the SHD, SSC, and GSC datasets, respectively, with the fewest time steps (40) among compared works. Comprehensive results across all datasets are detailed in Appendix C due to space constraints.\nWe further validate our models' capability for long-term learning over extended time steps, which is crucial for demonstrating the effectiveness of using a model with large time steps (such as 500 time steps) as the initial teacher model in subsequent KDCL procedures. As shown in Figure 4(a), we present the results of the SpikeSCR trained with either a single block or two blocks of SGLE from scratch on the SSC dataset with various time steps: 40, 100, 200 and 500. We also compare our results with the reproduced DCLS method. The accuracy of SpikeSCR clearly improves as the number of time steps increases, rising from 79.23% to 85.57%. In contrast, DCLS shows optimal performance with 100 time steps but then declines with more time steps. This comparison indicates that SpikeSCR is better suited for long-term learning, effectively leveraging both comprehensive global contexts and detail-oriented local contexts. Figure 4(b) further illustrates the energy consumption results. As the number of time steps decreases, the energy consumption of SpikeSCR declines significantly. Specifically, from 100 to 40 time steps, the energy consumption for one block (blue) decreases by 53.8% (from 0.0171mJ to 0.0079mJ), and for two blocks (red), it decreases by 54.8% (from 0.0314mJ to 0.0142mJ). Notably, SpikeSCR demonstrates a significant energy efficiency advantage with 40 time steps, compared to the DCLS model."}, {"title": "Conclusion", "content": "In this study, we first propose a fully spike-driven framework, SpikeSCR, aiming at enhancing speech command recognition. SpikeSCR effectively integrates position-embedded SSA and separable gated convolution to achieve global and local contextual representation learning. We demonstrate its efficient long-term learning capabilities, achieving excellent performance with extended time steps. To achieve a satisfactory trade-off between energy consumption and performance, we further introduce an effective knowledge distillation method, KDCL, where valuable representations learned from easy curriculum are progressively transferred to hard curriculum with minor loss. Our experimental results show that SpikeSCR outperforms SOTA methods across three benchmark datasets, achieving superior performance at the same time steps. Furthermore, through KDCL, SpikeSCR maintains comparable performance to SOTA results while reducing time steps by 60% and energy consumption by 54.8%."}, {"title": "Appendix A: Augments", "content": "We utilize SpecAugment (Park et al. 2019) with frequency masking and time masking for the Mel spectrogram of the input audio. The specific parameter settings for SpecAugment are detailed in Table 4. Additionally, we present a comparison of the augmented data in Figure 7, illustrating a \"Right\" command sample before and after data augmentation. For the spike trains in SHD and SSC, we utilize a method similar to EventDrop (Gu et al. 2021) involves two operations: drop-by-time and drop-by-neuron, both performed at a fixed ratio to drop events in different dimensions randomly. The specific parameter settings for EventDrop are detailed in Table 4. We also present a comparison of the augmented data in Figure 8, illustrating a \"Cat\" command sample before and after data augmentation."}, {"title": "Appendix B: Datasets and Experimental Setup", "content": "The dataset statistics of three benchmark datasets are shown in Table 5. The SHD dataset consists of recordings of spoken digits ranging from zero to nine in English and German language. There are only training and testing sets, with a total of 10,420 samples in SHD dataset. SSC is the spiking version of GSC. Both SSC and GSC have the same total number of samples, 105,829, which consist of a total of 35 word commands (Yes, No, Up, Down, Left, Right, etc), although the number of training and testing samples differ slightly. Overall, the number of samples of SSC and GSC are nearly ten times that of SHD. Therefore, we set two learning curricula for SHD and four learning curricula for SSC and GSC. Our work is implemented using the PyTorch-based Spiking-Jelly (Fang et al. 2023) framework. Additionally, we provide detailed experimental settings for the three datasets, as shown in Table 6. It is worth noting that to ensure the stability of the knowledge distillation process, we further employ a warming-up strategy, setting it for the initial 10 epochs."}, {"title": "Appendix C: Comprehensive Results", "content": "To provide a more comprehensive understanding of the impact of our SpikeSCR framework and KDCL method across different datasets, we present detailed evaluations in this section, which can showcase the robust adaptability of our proposed framework and method.\nFigure 4 presents a comparative analysis of model performance and energy consumption from short to long time steps on the SSC dataset. We further provide detailed numerical statistics in terms of time steps, parameters, energy and accuracy, showcasing in Table 7, and conduct a thorough analysis. Across all time steps, SpikeSCR outperforms the DCLS model, where (3L-2KC) represents a 3-layer feedforward network with dilated convolutions and 2 learnable kernel counts used for learning synaptic delays. With 40 time steps, SpikeSCR exhibits lower energy consumption"}, {"title": "Appendix D: Theoretical Calculation of Energy Consumption", "content": "The theoretical energy consumption of an SNN is usually calculated through multiplication between the number of MAC/AC operations and the energy consumption of each operation on predefined hardware (Panda, Aketi, and Roy 2020; Zhou et al. 2023; Yao et al. 2024; Zhang et al. 2024a). The number of synaptic operations (SOPs) is calculated as:\n$SOP' = fr^{l-1} \\times FLOP', (18)$\nwhere $fr^{l-1}$ is the firing rate of spiking neuron layer l - 1. FLOP\u00b9 refers to the number of floating-point MAC operations (FLOPs) of layer l, and SOP\u00b9 is the number of spike-based AC operations (SOPs). Assuming the MAC and AC operations are performed on the 45nm hardware (Horowitz 2014), \u0456.\u0435. $E_{MAC}$ = 4.6pJ and $E_{AC}$ = 0.9pJ, the energy consumption of SpikeSCR can be calculated as follows:\n$E = E_{AC} \\times (\\sum_{i=1}^{N} SOP'_{Conv} + \\sum_{j=1}^{M} SOP'_{SSA}), (19)$\n$E = E_{MAC} \\times (FLOP_{Conv})+E_{AC} \\times (\\sum_{i=2}^{N} SOP'_{Conv} + \\sum_{j=1}^{M} SOP'_{SSA}) (20)$\n$SOP'_{Conv}$ represents the SOPs of a convolution or linear layer, and $SOP'_{SSA}$ represents the SOPs of an SSA module, $FLOP_{Conv}$ represents the FLOPs of the first layer before encoding input frames into spikes. N is the total number of convolution layers and linear layers, and M is the number of SSA modules. During model inference, several cascaded linear operation layers such as convolution, linear, and BN layers, can be fused into one single linear operation layer (Zhou et al. 2024), still enjoying the AC-type operations with a spike-form input tensor.\nWe further explain the energy consumption calculation under two different input conditions. Specifically, when the input to SpikeSCR is in spike-form (such as the SHD and SSC datasets), it does not involve MAC operations, and thus its energy consumption is calculated as shown in Eq. 19. However, when the input to SpikeSCR is in real-valued form (such as the GSC dataset), the first Conv layer in the SEM involves MAC operations, and its energy consumption is calculated as shown in Eq. 20."}, {"title": "Appendix E: Code Available", "content": "We utilize the SpikingJelly framework (Fang et al. 2023) to train our model, available at https://github.com/fangwei123456/spikingjelly.\nFor comparison, we select the currently reproducible SOTA work DCLS (Hammouamri, Khalfaoui-Hassani, and Masquelier 2024), with code available at https://github.com/Thvnvtos/SNN-delays. Moreover, we employ the same data preprocessing methods as those used in the DCLS work to ensure consistency in our experimental setup.\nOur energy consumption calculation framework is based on syops-counter (Chen et al. 2023), with code available at https://github.com/iCGY96/syops-counter.\nOur organized code will be made publicly available on a common repository upon reaching the camera-ready version of this paper."}]}