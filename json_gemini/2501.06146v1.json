{"title": "xLSTM-SENet: xLSTM for Single-Channel Speech Enhancement", "authors": ["Nikolai Lund K\u00fchne", "Jan \u00d8stergaard", "Jesper Jensen", "Zheng-Hua Tan"], "abstract": "While attention-based architectures, such as Conformers, excel in speech enhancement, they face challenges such as scalability with respect to input sequence length. In contrast, the recently proposed Extended Long Short-Term Memory (xLSTM) architecture offers linear scalability. However, XLSTM-based models remain unexplored for speech enhancement. This paper introduces xLSTM-SENet, the first XLSTM-based single-channel speech enhancement system. A comparative analysis reveals that xLSTM-and notably, even LSTM-can match or outperform state-of-the-art Mamba- and Conformer-based systems across various model sizes in speech enhancement on the VoiceBank+Demand dataset. Through ablation studies, we identify key architectural design choices such as exponential gating and bidirectionality contributing to its effectiveness. Our best XLSTM-based model, xLSTM-SENet2, outperforms state-of-the-art Mamba- and Conformer-based systems on the Voicebank+DEMAND dataset.", "sections": [{"title": "1. Introduction", "content": "Real-world speech signals are often disrupted by noise, which degrades performance in tasks such as hearing assistive devices [1], automatic speech recognition [2], and speaker verification [3]. The process of removing background noise and enhancing the quality and intelligibility of the desired speech signal is known as speech enhancement (SE). SE has a wide range of applications, which drives the development of new and improved algorithms.\nSingle-channel SE methods leveraging deep learning encompass a range of architectures, such as deep neural networks [1], recurrent Long Short-Term Memory (LSTM) networks [4], convolutional neural networks (CNNs) [5, 6], generative adversarial networks (GANs) [7-10], and diffusion models [11-13]. Recently, Conformer-based models have demonstrated impressive SE performance, achieving state-of-the-art results [10, 14] on the VoiceBank+Demand dataset [15, 16]. However, models based on scaled dot-product attention, such as Conformers, face challenges with scalability with respect to input sequence length, and require a lot of data to train [17].\nHowever, recurrent neural networks (RNNs), particularly LSTMs [18], offer several advantages over attention-based models, including: (i) linear scalability, instead of quadratic, in terms of computational complexity with respect to the input sequence length, and (ii) reduced runtime memory requirements, as they do not require storage of the full key-value (KV) cache. In contrast, attention-based models such as Transformers and Conformers necessitate significant memory overhead for KV storage. Despite these advantages, LSTMs suffer from critical drawbacks: (i) inability to revise storage decisions, (ii) reliance on scalar cell states, constraining storage capacity, and (iii) memory mixing preventing parallelizability [19]. Consequently, LSTMs have been less utilized in recent deep learning-based SE systems, with only a few exceptions [4, 20].\nTo address the inherent limitations of attention-based models, Mamba [21], a sequence model integrating the strengths of CNNs, RNNs, and state space models, has recently emerged. Mamba has demonstrated competitive or superior performance relative to Transformers across various tasks, including audio classification [22] and SE [23]. Recently, the Extended Long Short-Term Memory architecture (xLSTM) [19] was proposed to overcome the limitations of LSTMs. By incorporating exponential gating, matrix memory, and improved normalization and stabilization mechanisms while eliminating traditional memory mixing, xLSTM introduces two new fundamental building blocks: sLSTM and mLSTM. The XLSTM architecture has shown competitive performance across tasks such as natural language processing [19], computer vision [24], and audio classification [25]. However, while xLSTM adds increased memory via matrix memory and an improved ability to revise storage decisions via exponential gating, the potential advantages of these additions over LSTM have yet to be assessed for SE.\nIn this work, we propose an xLSTM-based SE system (XLSTM-SENet), which is the first single-channel SE system utilizing xLSTM. The system architecture is illustrated in Figure 1. Systematic comparisons of our proposed XLSTM-SENet with Mamba-, and Conformer-based models across various model sizes, show that xLSTM-SENet matches the performance of state-of-the-art Mamba- and Conformer-based systems on the VoiceBank+Demand dataset [15, 16]. Intriguingly, we empirically show that LSTM matches or even outperforms XLSTM, Mamba, and Conformers on the VoiceBank+Demand dataset. Additionally, we perform detailed ablation studies to explore the design space of neural architectures, quantifying their impact on overall performance. Finally, our best XLSTM-based model, XLSTM-SENet2, outperforms state-of-the-art Mamba- and Conformer-based systems on the Voicebank+DEMAND dataset. Code is publicly available.\u00b9"}, {"title": "2. Method", "content": null}, {"title": "2.1. Extended long short-term memory", "content": "As mentioned, xLSTM [19] introduces two novel building blocks: sLSTM and mLSTM, to address the limitations of the original LSTM [18]. Following Vision-LSTM [24] and Audio XLSTM [25], we employ mLSTM as the main building block in our SE system. Unlike the sigmoid gating used in traditional"}, {"title": "2.2. xLSTM-SENet: speech enhancement with XLSTMs", "content": "Following SEMamba [23], we integrate XLSTM into the MP-SENet architecture [14] by replacing the Conformers in MP-SENet with xLSTM blocks as shown in Figure 1. We use the MP-SENet architecture since it facilitates joint denoising of magnitude and phase spectra, and has shown superior performance compared to other time-frequency (TF) domain SE methods [14]."}, {"title": "2.2.1. Model structure", "content": "Model overview: As shown in Figure 1, our proposed XLSTM-SENet architecture follows an encoder-decoder structure. Let T and F denote the time and frequency dimensions, respectively, and let  Y = Y_m \\cdot e^{jY_p} \\in C^{T\\times F}  denote the complex spectrogram, which is obtained by performing an STFT on the noisy speech waveform  y \\in R^{D} . Then  Y_p \\in R^{T\\times F}  is the wrapped phase spectrum, and by applying power-law compression [26] with compression factor  0 < c \\in R , we extract the compressed magnitude spectrum  (Y_m) \\in R^{T\\times F} . The stacked compressed magnitude spectrum and wrapped phase spectrum  Y_{in} = (Y_m) \\oplus Y_p \\in R^{T\\times F \\times 2} , where  \\oplus  is the concatenation operator, is encoded into a compressed TF-domain representation, which is subsequently fed to a stack of  N  TF-XLSTM blocks. Each TF-XLSTM block comprises a time and frequency XLSTM block, capturing temporal and frequency dependencies, respectively. Similar to SEMamba [23], we use a bidirectional architecture (Bi-mLSTM) for these blocks. Hence, the output  v  of the time and frequency xLSTM blocks is:\n v = Conv1D(mLSTM(\\varepsilon) \\oplus mLSTM(flip(\\varepsilon))), (10)\nwhere  \\varepsilon  is the input to the time and frequency xLSTM blocks, and mLSTM(\\cdot), flip(\\cdot), and Conv1D(\\cdot) is the unidirectional mLSTM, the sequence flipping operation, and the 1-D transposed convolution, respectively.\nFinally, the output of the TF-XLSTM blocks is decoded by both a magnitude mask decoder and wrapped phase decoder [27]. They predict the clean compressed magnitude mask"}, {"title": "3. Experiments", "content": null}, {"title": "3.1. Dataset", "content": "In this study, we perform experiments on the Voice-Bank+Demand dataset, which consists of pairs of clean and noisy audio clips sampled at 48 kHz. The clean audio samples come from the VoiceBank corpus [15], which comprises 11,572 audio clips from 28 distinct speakers for training, and 824 audio clips from 2 distinct speakers for testing. The noisy audio clips are created by mixing the clean samples with noise from the DEMAND dataset [16] at four signal-to-noise ratios (SNRs) during training ([0, 5, 10, 15] dB) and testing ([2.5, 7.5, 12.5, 17.5] dB). Two speakers from the training set are left out as a validation set."}, {"title": "3.2. Implementation details", "content": "Unless otherwise stated, experimental details and training configurations match those presented in MP-SENet [14] and SE-Mamba [23]. To reduce memory and computational resources, all models were trained on randomly cropped 2-second audio clips. Additionally, all audio clips were downsampled to 16 kHz, reducing computational complexity and ensuring compatibility with the wide-band PESQ metric [30]. When performing STFTs we set the FFT order, Hann window size, and hop size to 400, 400, and 100, respectively. We train all models for 200 epochs and select the checkpoint (saved every 1000th step) with the best PESQ score on the validation data. We fix  C= 64  channels and  N = 4  stacks of TF-XLSTM blocks in our XLSTM-SENet model for direct comparison with SEMamba and MP-SENet. All models are trained with a batch-size  B = 8  on four NVIDIA L40S GPUs, and the four layer XLSTM-SENet model takes approximately 3 days to train. This is the main limitation of xLSTM compared to Mamba and Transformers, which are roughly four times as fast to train [19]."}, {"title": "3.3. Evaluation metrics", "content": "We use the following commonly used evalutation metrics to assess SE performance: wide-band PESQ [30] and short-time objective intelligibility (STOI) [31]. To predict the signal distortion, background intrusiveness and overall speech quality, we use the composite measures CSIG, CBAK and COVL [32]. For all measures, a higher value is better. We train all models with 5 different seeds and document the mean and standard deviation."}, {"title": "4. Results and analysis", "content": null}, {"title": "4.1. Comparison with existing methods", "content": "We evaluate several architectural design choices for our xLSTM-SENet model while limiting the parameter count to that of SEMamba. We choose the best model based on validation performance. Our best model uses a bidirectional architecture and we have added biases to layer normalizations and projection layers as in [24]. The expansion factor is set to  E_f = 4 ."}, {"title": "4.2. Ablation study", "content": "mLSTM adds matrix memory and exponential gating to improve LSTM. To evaluate some of these improvements and our model architecture design choices, we perform ablations on the expansion factor  E_f  and on the biases in layer normalizations and projection layers. Additionally, we evaluate the effect of exponential gating by replacing it with sigmoid gating. Finally, we investigate the performance of a unidirectional architecture, by removing the transposed convolution, flipping and the second"}, {"title": "4.3. Comparison with LSTM", "content": "To compare the performance of xLSTM and LSTM for SE, we first replace the mLSTM layers in XLSTM-SENet with conventional LSTM layers (this models is referred to as: LSTM (layer)). Table 3 shows that this results in a performance decrease even though LSTM (layer) is approximately 11% larger. Then, we replace the entire mLSTM block with LSTM (denoted as: LSTM (block)) and double the number of layers N to match the parameter count of xLSTM-SENet. Table 3 shows that LSTM (block) matches XLSTM-SENet, which in Table 1 was shown to match the enhancement performance of state-of-the-art Mamba and Conformer-based systems."}, {"title": "4.4. Scaling experiments", "content": "Smaller models are preferred in real-world SE applications, like hearing aids, due to reduced computational complexity, facilitating their use in such devices. Additionally, it is of interest to explore the performance achieved by increasing model sizes. Hence, we perform a comparative analysis of XLSTM, Mamba, Conformer and LSTM across varying layer counts N. For LSTM, N is doubled to match the parameter counts of the XLSTM-, Mamba-, and Conformer-based models.\nFigure 2 shows that XLSTM, Mamba, and Conformer-based models perform similarly when scaled down, with LSTM out-performing them for  N = 1  and  N = 2 . When scaled up, all models achieve comparable performance."}, {"title": "4.5. XLSTM-SENet2", "content": "To provide a direct comparison of xLSTM and LSTM for SE, we propose XLSTM-SENet2, which is configured with an expansion factor  E_f = 2  and  N = 8  layers, resulting in approximately the same parameter count per layer for XLSTM-SENet2 and LSTM (block). Table 4 shows that XLSTM-SENet2 outperforms state-of-the-art LSTM-, Mamba-, and Conformer-based models."}, {"title": "5. Conclusion", "content": "This paper proposed an Extended Long Short-Term Memory-based method for speech enhancement (xLSTM-SENet). Experiments on the VoiceBank+Demand dataset show that XLSTM-SENet, and even LSTM-based models, rival existing state-of-the-art Mamba- and Conformer-based speech enhancement systems across several model sizes. We studied the importance of several architectural design choices, and demonstrated that the inclusion of exponential gating and bidirectionality is critical to the performance of the XLSTM-SENet model. Finally, empirical results show that our best XLSTM-based system, XLSTM-SENet2, outperforms state-of-the-art systems in speech enhancement on the VoiceBank+Demand dataset."}]}