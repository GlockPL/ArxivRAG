{"title": "REMEMBER, RETRIEVE AND GENERATE:\nUNDERSTANDING INFINITE VISUAL CONCEPTS AS\nYOUR PERSONALIZED ASSISTANT", "authors": ["Haoran Hao", "Jiaming Han", "Changsheng Li", "Yu-Feng Li", "Xiangyu Yue"], "abstract": "The development of large language models (LLMs) has significantly enhanced\nthe capabilities of multimodal LLMs (MLLMs) as general assistants. However,\nlack of user-specific knowledge still restricts their application in human's daily\nlife. In this paper, we introduce the Retrieval Augmented Personalization (RAP)\nframework for MLLMs' personalization. Starting from a general MLLM, we\nturn it into a personalized assistant in three steps. (a) Remember: We design\na key-value database to store user-related information, e.g., user's name, avatar\nand other attributes. (b) Retrieve: When the user initiates a conversation, RAP\nwill retrieve relevant information from the database using a multimodal retriever.\n(c) Generate: The input query and retrieved concepts' information are fed into\nMLLMs to generate personalized, knowledge-augmented responses. Unlike pre\nvious methods, RAP allows real-time concept editing via updating the external\ndatabase. To further improve generation quality and alignment with user-specific\ninformation, we design a pipeline for data collection and create a specialized\ndataset for personalized training of MLLMs. Based on the dataset, we train a\nseries of MLLMs as personalized multimodal assistants. By pretraining on large\nscale dataset, RAP-MLLMs can generalize to infinite visual concepts without ad\nditional finetuning. Our models demonstrate outstanding flexibility and generation\nquality across a variety of tasks, such as personalized image captioning, question", "sections": [{"title": "1 INTRODUCTION", "content": "Recently, the development of large language models (LLMs) has significantly enhanced their lan-\nguage processing and generating capabilities (Zhao et al., 2023b). Building on this foundation, the\nintegration of visual and textual ability through vision-language alignment brings powerful multi\nmodal LLMs (MLLMs) (Yin et al., 2023; OpenAI, 2023; Gemini-Team, 2024; Liu et al., 2023b;\nZhang et al., 2024; Han et al., 2024). MLLMs have shown significant improvement in various tasks,\nsuch as image description and question answering, highlighting their potential as human's assistants.\nHowever, their lack of user-specific knowledge continues to limit their effectiveness as personalized\nassistants in daily life.\nA qualified personalized assistant first needs to be able to recognize and remember user-related con-\ncepts, such as the dog named (Lala) adopted by the user. Although existing MLLMs have been\ntrained on large-scale datasets and possess strong recognition and classification capabilities, directly\ntransferring this knowledge to a user's personal concepts remains challenging. For instance, cur-\nrent leading MLLMs cannot remember your dog's name, even if you have mentioned it before, and\nthey lack awareness of your identity and preferences. Furthermore, the assistant should generate re-\nsponses tailored to the user's preferences and requirements. However, collecting extensive personal\ninformation to train a unique assistant for each user is impractical.\nTo address this issue, the personalization of MLLMs has become a topic of growing interest, with\nseveral approaches already being proposed. MyVLM (Alaluf et al., 2024) utilizes external classifica-\ntion heads to recognize specific concepts, and learns an embedding for each concept to personalize\nthe outputs of vision language models (VLMs). Another concurrent work, Yo'LLaVA (Nguyen\net al., 2024), learns a few special tokens to represent each concept. However, both approaches ne-\ncessitate continuous learning and updating of the model as new concepts emerge. This presents a\nchallenge in dynamic, ever-changing real-world scenarios, where the computing power of users'\npersonal devices is often limited, and all data must be stored locally for privacy concerns.\nTo address these challenges, we propose the Retrieval Augmented Personalization (RAP), designed\nto allow MLLMs to update their supported concepts without additional training. Specifically, our\nRAP works in three key steps. (a) Remember: RAP includes a designed database to help remember\neach concept via storing its image and basic information, e.g., name, avatar and other attributes.\n(b) Retrieve: When a user initiates a conversation, RAP will retrieve relevant information from the\ndatabase using a multimodal retriever. (c) Generate: The input query and retrieved concepts infor-\nmation are incorporated into the MLLM's input for personalized, knowledge-augmented generation.\nRAP requires only one image per concept with its basic information for personalization. It allows\nusers to make real-time adjustments to the model's outputs by modifying their personal databases,\neliminating the need for retraining. A more detailed comparison is presented in Table 1.\nAnother significant challenge is the lack of large-scale datasets for training MLLMs' personalized\ngeneration capabilities. To address this, we design a pipeline to collect extensive training data and\ncreate a comprehensive dataset, which enables to train MLLMs to effectively understand and utilize\nuser-related information for generation. Based on this dataset, we train LLaVA (Liu et al., 2023b)\nand Phi3-V (Rasheed et al., 2024) as novel personalized assistants and evaluate their performance\nacross various tasks, including personalized image captioning, question answering, and visual recog-\nnition. Experimental results demonstrate that our RAP-MLLMs excel in wide range of personalized\ngeneration tasks, showcasing excellent generation quality and flexibility."}, {"title": "2 RELATED WORK", "content": "Multimodal Large Language Models. Recently, numerous advanced large language models\n(LLMs) (Touvron et al., 2023; Zhang et al., 2023b; Chiang et al., 2023; Taori et al., 2023) have\nbeen proposed, showing remarkable performance in addressing a wide range of tasks. The rapid de\nvelopment of these LLMs has led to the emergence of multimodal LLMs (MLLMs) (OpenAI, 2023;\nGemini-Team, 2024; Liu et al., 2023b; Zhang et al., 2024; Han et al., 2024; Zhu et al., 2023), which\nexcel in general visual understanding and complex reasoning tasks. For instance, LLaVA (Liu et al.,\n2023b;a) and MiniGPT-4 (Zhu et al., 2023) align visual and language modalities through visual in\nstruction tuning, showcasing impressive capabilities in multimodal conversations. GPT4RoI (Zhang\net al., 2023c) and RegionGPT (Guo et al., 2024) enhance fine-grained understanding and reasoning\nfor specific regions by training on region-level instruction datasets. Despite these advancements in\ntasks such as image captioning and question answering, the lack of user-specific knowledge restricts\nthe generation of personalized content, which hinders the practical application of MLLMs in daily\nlife. In this work, we focus on the personalization of MLLMs, enabling them to remember and\nunderstand user-specific concepts, and generate personalized content tailored to user's preferences.\nPersonalization of MLLMs. In the realm of artificial intelligence, personalization typically refers\nto the process of tailoring a system, application, or model to meet the individual needs and prefer\nences (Yeh et al., 2023; Wo\u017aniak et al., 2024; Shi et al., 2024). Substantial efforts have been made\nto generate images of user's personal objects or in certain context (Ruiz et al., 2023; Kumari et al.,\n2023; Ham et al., 2024; Gal et al., 2022; Ye et al., 2023). For example, Dreambooth (Ruiz et al.,\n2023) employs transfer learning in text-to-image diffusion models via fine-tuning all parameters for\nnew concepts. In this paper, we mainly aim at enabling MLLMs to remember and understand user\nspecific concepts, and generate personalized language outputs. There are several works focusing on\nthe personalization of MLLMs, among which the most relevant works are MyVLM (Alaluf et al.,\n2024) and Yo'LLaVA (Nguyen et al., 2024). MyVLM introduces the task of personalizing VLMs.\nIt utilizes external classification heads to recognize specific concepts, and learns an embedding for\neach concept to personalize the outputs of VLMs. Yo'LLaVA personalizes LLaVA by extending its\nvocabulary and learning specific tokens for each concept. However, both approaches require con\ntinuous model updates as new concepts emerge, which presents challenges in dynamic real-world\napplications. In this work, we propose RAP framework for the personalization of MLLMs, enabling\nmodels to be trained once while continuously updating supported concepts without further training.\nRetrieval Augmented Generation. Retrieval-based methods for incorporating external knowledge\nhave proven effective in enhancing generation across a variety of knowledge-intensive tasks (Gao\net al., 2023; Zhao et al., 2023a; Asai et al., 2023; Xu et al., 2023; Yoran et al., 2023; Lin et al.,\n2023b). DPR (Karpukhin et al., 2020) introduces Dense Passage Retrieval, marking a shift from\nsparse to dense retrieval techniques. Later, MuRAG (Chen et al., 2022) proposes to use multimodal\nknowledge to augment language generation. Self-Rag (Asai et al., 2023) introduces special tokens to\nmake retrieval adaptive and controllable. ERAGent (Shi et al., 2024) presents a comprehensive sys\ntem for retrieval-augmented language models. With the advancements in MLLMs, RAG has been\nwidely applied to multimodal generative tasks. For instance, FLMR (Lin et al., 2023a) employs"}, {"title": "3 RETRIEVAL AUGMENTED PERSONALIZATION", "content": "Existing MLLMs typically align other modalities with language. For instance, LLaVA (Liu et al.,\n2023b) projects visual tokens into text space, and then generates subsequent tokens using an LLM.\nWhile these MLLMs perform well in various tasks, the lack of memory and comprehension of\npersonal concepts hinders effective user-specific responses. In this work, we mainly focus on per\nsonalizing MLLMs to generate tailored language responses, such as creating personalized captions\nfor user's images and answering questions about personal concepts. In this section, we detail the\nimplementation steps of our proposed Retrieval Augmented Personalization (RAP). Unlike previ\nous approaches that usually necessitate additional data collection and further training to learn new\nconcepts, our RAP does not require additional training as the user's database expands. By pretrain\ning on our dataset, our RAP-MLLMs can adapt to diverse users and infinite new concepts without\nfurther training. In section 3.1, we present the RAP framework that is applicable to various types of\nMLLMs, and then in section 3.2, we provide details of the proposed dataset."}, {"title": "3.1 RAP FRAMEWORK", "content": "Our RAP works in three main steps: Remember, Retrieve and Generate, as shown in Figure 2.\nRemember. The premise of personalization is that the model can remember personal concepts and\nrelevant information, such as the dog named (Lala) adopted by (A). To facilitate this, we construct\na database M to store these personal concepts, which comprises an avatar, a name, and a brief\ndescription for each concept. The key for each concept in the database is its visual feature, obtained\nby feeding its image into a pre-trained image encoder $E(\u00b7)$. Examples of our database are presented\nin Figure 2. When a user initiates a conversation, the input can be represented as $Q = (I,T)$,\nwhich may include both image $I$ and some textual instructions $T$. The first step involves identifying\npossible concepts within the input image that have been previously stored in the database. Previous\nmethods (Alaluf et al., 2024) typically need to learn an external classifier to determine whether a\nconcept appears in the input image, which requires a substantial amount of training data and can"}, {"title": "Retrieve", "content": "Identified region-of-interest will be used as query to retrieve from the database. For each\nrecognized component I, we feed the image crop into the image encoder E(\u00b7) to get its visual\nfeature $Q^{i} = E(I)$, which is a n-dimensional vector. Then we calculate the euclidean distance be-\ntween the visual feature and each key $k_j \u2208 M$, which is calculated as $Dist(Q^{i}, k_j) = ||Q_i \u2013 k_j ||$.\nThe Top-K image-text pairs ${(I_1, T_1), (I_2, T_2), \u00b7\u00b7\u00b7 (I_k, T_k)}$ with the lowest distances are selected.\nWe also introduce retrieval using concept names, such as (sks) for a unique concept. When the user\nmentions the name of an object documented in the database, our model retrieves its related informa-\ntion from the database. This also enables our model to respond to text-only queries effectively."}, {"title": "Generate", "content": "Each pair $M_j = (I_j,T_j)$ provides related information about a user's personal concept\nand will be incorporated into the input of the MLLM. Take LLaVA (Liu et al., 2023b) as an example,\nthe image $I_j$ is first encoded by a pre-trained vision encoder, such as CLIP (Radford et al., 2021),\nto obtain their visual tokens $Z_j$. These image tokens are then projected by a projector into language\ntokens $H$, which could be understood by the language model. Simultaneously, corresponding text\ninformation $T_j$ are transformed into text tokens $H^j$. During training, we keep parameters of both\nthe detector and retriever frozen, just train the MLLM's parameters $\\theta$. Given the length $L$ of the\noutput sequence, the probability of the target answers $X_a$ computed as:\n$p(X_a|I,T, M_1,\u2026 M_k) = \\prod_{i=1}^{L}p_{\\theta}(X_{a,i}|I,T, M_1,\u2026 M_k, X_{a,<i})$"}, {"title": "3.2 PERSONALIZATION DATASET", "content": "Most existing MLLMs struggle to generate personalized outputs even if additional concept informa-\ntion is provided, and there is currently no large-scale dataset for personalized training of MLLMs.\nTo this end, we design a pipeline for data creation and curate a novel dataset specifically for the per\nsonalized training and evaluation of MLLMs. We use Gemini-1.5 (Gemini-Team, 2024) to generate\nannotations for our dataset. An overview of our pipeline and dataset is presented in Figure 3."}, {"title": "4 EXPERIMENT", "content": "Implementation Details. We conduct experiments on LLaVA-1.5-13B (Liu et al., 2023b) and Phi3\nV-3.8B (Rasheed et al., 2024), resulting in two personalized MLLMs, RAP-LLaVA and RAP-Phi3\nV. We select YOLO-Worldv2 (Cheng et al., 2024) as the detector and construct a multimodal re\ntriever using Facebook AI Similarity Search (FAISS) (Johnson et al., 2021), employing a pre-trained\nCLIP ViT-L/14-336 (Radford et al., 2021) as the visual encoder. Due to the context length limitation\nof the backbone language model, for RAP-LLaVA and RAP-Phi3-V, we retrieve the 2 and 3 different\nconcepts with the highest similarity, respectively. More details can be found in Appendix C.\nTraining. In the training phase, we skip the recognition and retrieval procedures, instead perform\ninstruction tuning to train the MLLMs. We adhere to most settings from the original experiment\nof LLaVA (Liu et al., 2023b), except for using a maximum learning rate of 1e-4 and training for 1"}, {"title": "4.1 PERSONALIZED IMAGE CAPTIONING", "content": "In this section, we evaluate our models on generating personalized image captions with user's spe\ncific concepts. We extend the dataset introduced by MyVLM (Alaluf et al., 2024) via adding 16 new\nconcepts, which include both objects and humans, forming 8 concept pairs that appear together in\nimages. For each pair, there are 8-13 images used for testing. This multiple concepts setting presents\nadditional challenges for personalization.\nSettings. We compare our models with MyVLM and finetuning based method LLaVA-LORA (Hu\net al., 2022). We do not include Yo'LLaVA since it does not porvide open-sourced model. For\nLLaVA-LORA and MyVLM, the training dataset contains 1 image accompanied by 5 captions for\neach concept. This simulates the real-world challenge of collecting high-quality training data for\neach concept, which is both difficult and time-consuming. For LLaVA-LORA, we train it with cap\ntions of the training images for 3 epochs, applying low-rank adapters (Hu et al., 2022) and the same\nhyperparameters as our models. For MyVLM, following their training process, we first train the\nclassification head with the positive and 150 negative images, then train the corresponding concept\nembedding with the provided captions for each concept. For our models, we construct a database\nwhere each concept is represented by a cropped image and a personalized description. Details of"}, {"title": "Quantitative Evaluation", "content": "We employ recall, precision and the comprehensive metric F1-score as\nour evaluation metrics. Recall is calculated as the percentage of correct occurrences of target con\ncepts, while precision is the ratio of correct concept names to the total number of concept names pre\nsented. The experimental results are shown in Table 3. From the results, we find that the finetuning\nbased model LLaVA-LORA achieves higher performances than MyVLM. Notably, the classification\nheads of MyVLM exhibit higher error rates when the number of positive images is limited, leading\nto weaker performance. Our models demonstrate superior performance in both recall and precision\nmetrics, highlighting the advantages of our RAP-MLLMs in data efficiency."}, {"title": "Influence of Number of Learned Concepts", "content": "In real-world scenario, users' personal databases\ntypically expand over time. Next, we evaluate the performance of various methods with varying\nnumbers of learned concepts. We extend the database with hundreds of new concepts selected\nfrom RefCOCO dataset (Kazemzadeh et al., 2014), ensuring no overlap with the test dataset. For\nLLaVA-LORA and MyVLM, we provide images containing the target concepts along with their\ncaptions as training data, and we assess the models' performance on the original test dataset. The\nresults are presented in Figure 4. As the number of learned concepts increases, performance of all\nmethods declines. More learned concepts result in increased recognition errors, leading to a drop in\nperformance. Our RAP-MLLMs maintain the highest performance under different settings."}, {"title": "4.2 PERSONALIZED QUESTION ANSWERING", "content": "Settings. In this section, we evaluate different methods on the benchmark of personalized question\nanswering introduced by Yo'LLaVA (Nguyen et al., 2024), which contains both visual-based and\ntext-only questions about user's personal concepts. For each concept, we generate a description that\nserves as the concept's information in our database. For LLaVA-LORA, we feed these descriptions\nand corresponding images to train the model to describe the properties of concepts. Additionally, we\nincorporate text-only queries and answers to enhance the model's understanding of specific concepts\nfrom textual perspectives. The training dataset for Yo\u2019LLaVA and MyVLM consists of 5 positive\nimages with question answering pairs and 200 negative images for each concept. For GPT-4V\n(OpenAI, 2023), images and related information about the concepts mentioned in the questions are\nprovided as supplementary prompt. Additional details on the baselines are provided in Appendix C.\nResults and Analysis. The experimental results are provided in Table 4. LLaVA and LLaVA-LORA\nboth perform well in visual based question answering, because substantial information of the target\nconcept can be obtained from the images. However, their performance is quite poor when images\nof the target concept mentioned in the question are not available. MyVLM performs well in visual\nquestion answering but does not support text-only question answering. Yo'LLaVA excels in text\nonly question answering, but its performance is still limited by the insufficient information provided\nby the learned tokens of a concept. In contrast, our models demonstrate balanced performance\nin both visual and text-only question answering. By providing a single image, our RAP-LLaVA\nsurpasses baseline methods and achieves performance comparable to that of GPT-4V."}, {"title": "Visual Recognition", "content": "We also evaluate the models' recognition abilities for a more comprehensive\ncomparison. In this task, the MLLMs are required to determine whether a personal concept exists\nin an image. We query them with \"Is (sks) in the image? Answer with a single word or phrase.\u201d,\nwhere (sks) is replaced by corresponding concept name. For positive images, the desired response\nis \"Yes\" and \"No\" for negative. Results show that, without understanding of personal concepts,\nthe vanilla LLaVA consistently outputs negative responses. After training on the target concepts,\nLLaVA-LORA, MyVLM and Yo'LLaVA tend to give positive responses, but struggle to differentiate\nbetween concepts, resulting in weaker performance on negative images. Our models demonstrate\nexceptional performance in both positive and negative scenarios, achieving the best overall results."}, {"title": "4.3 COST OF PERSONALIZATION.", "content": "We further compare the costs of personalization. As shown in table 1, existing methods usually\nstruggle with continuous updates or have high demands for training data. For finetune-based method\nlike LLaVA-LORA, while they can achieve satisfactory performance, finetuning the model each\ntime a new concept emerges incurs substantial computational costs. MyVLM and Yo'LLaVA learn\nan embedding or some new tokens to represent the new concept without updating the pre-trained\nMLLM's parameters, however, they require multiple labeled images of the target concept and a large\nnumber of negative images, which poses significant challenges for data collection. In contrast, our\nRAP requires only 1 image with its related information provided by the user, achieving outstanding\nperformance across various personalized generation tasks. At the same time, by modifying images"}, {"title": "4.4 ABLATION STUDY.", "content": "Retriever. The recall rate of the retriever is crucial for a RAG system. We first assess the retriever's\nperformance on the personalized captioning dataset. We use the detection model to identify potential\nconcepts and retrieve the K concepts with the highest similarity from the database. The Top-K recall\nrates for varying values of K and database sizes N are presented in Figure 6. Results indicate that as\nthe database size increases, the retriever's performance declines, while a larger K generally enhances\nthe recall rate. Notably, even with 500 personal concepts to remember, the Top-5 recall rate is still\nable to surpass 90%, which guarantees the effectiveness of our RAP framework."}, {"title": "Generation Ability of MLLM", "content": "We skip the recognition and retrieval processes, providing the\nMLLM with relevant information of each concept present in the image to evaluate the generation\ncapability of the trained MLLM. The results, shown in Table 5, indicate that when relevant con\ncept information is supplied, our RAP-LLaVA achieves superior generation performance, obtaining\n100% precision without outputting irrelevant concepts as well as a higher recall rate."}, {"title": "Dataset Composition", "content": "We conduct experiments to assess contribution of each component in our\ndataset. First, we remove data generated through data augmentation and train the original LLaVA.\nThe results indicate a obvious decrease in the recall metric for image captioning, resulting in lower\noverall performance. We further exclude constructed negative samples from the dataset and retrain\nthe model, then we find that it performs poorly on precision metric. This suggests a diminished\nability to discriminate against noisy concepts not present in the image."}, {"title": "Multimodal Benchmark", "content": "We also evaluate our model's performance on several traditional mul\ntimodal benchmarks, including MMMU (Yue et al., 2024) and InfoSeek (Chen et al., 2023). We\nassess our models' performance both with and without external knowledge base. Details of the\nknowledge base are provided in Appendix C. We evaluate on the validation set of MMMU, and 5K\nquestions sampled from the validation set of InfoSeek. We use the official scripts to get the results,\nwhich are presented in Table 6. From the results, our RAP-LLaVA retains most general knowledge\nof the original LLaVA. It also equips the MLLM with the ability to retrieve information from an\nexternal knowledge base, demonstrating superior performance in knowledge intensive tasks."}, {"title": "5 CONCLUSION", "content": "In this paper, we introduce the RAP framework for personalizing MLLMs. This framework enables\nMLLMs to understand an infinite number of user-specific concepts, generate personalized captions\nand respond to user-related queries. To enhance the quality of the generated content and better"}, {"title": "F LIMITATION", "content": "Our proposed RAP framework is a retrieval-based method. The limitations of RAP mainly concern\nthe additional computational cost of generation and the precision of the retriever. While incorpo\nrating external information effectively generates more specific answers, it inevitably increases the\ncontext length for MLLMs, leading to additional computational overhead during the generation pro\ncess. We will further explore ways to mitigate this computational burden. Another limitation is the\npersonalization performance of our RAP-MLLMs depends on the retriever's capability This pro\nposes need for a robust multi-modal retriever that can discern intricate features to enhance retrieval\nprecision. Despite these limitations, RAP offers a timely solution for MLLM personalization. By\nretrieving from a user's specific database, RAP facilitates reliable and flexible personalized genera\ntion, which is valuable in practical applications."}, {"title": "A APPENDIX OVERVIEW", "content": "\u2022 Section B: Additional evaluations of our models.\n\u2022 Section C: More experiment details.\n\u2022 Section D: More details of RAP dataset.\n\u2022 Section E: Additional demonstrations.\n\u2022 Section F: Analysis on limitations of our work.\n\u2022 Section G: Examples of the personalized database."}, {"title": "B ADDITIONAL EVALUATION RESULTS", "content": "Ablation Studies. We conduct ablation experiments on the question answering and recognition\nbenchmark, experimental results are present in Table 7. The results further demonstrate that our\ndata augmentation and the constructed negative samples also contribute to the model's performance."}, {"title": "C MORE EXPERIMENTAL DETAILS", "content": "Implementation details. We utilize YOLO-Worldv2-X (Cheng et al., 2024) as the detection model,\nsetting detection classes to include all categories stored in the database to reduce the interventions\nfrom unrelated objects. We construct a multimodal retriever using Facebook AI Similarity Search\n(FAISS) (Johnson et al., 2021), employing a pre-trained CLIP ViT-L/14-336 (Radford et al., 2021)\nas the visual encoder. Each key in the database is generated by inputting the image of a concept\ninto the CLIP visual encoder, resulting in a 768-dimensional vector. Considering the restriction of\ncontext length of the backbone language model, we retrieve the 2 most similar images from the\ndatabase for each region of interest. And then, we select 2 and 3 different concepts with the highest\nsimilarity among all as supplementary inputs for RAP-LLaVA and RAP-Phi3-V, respectively.\nExternal knowledge base. For MMMU (Yue et al., 2024), we use 30K images paired with corre\nsponding captions from Wikipedia as the external knowledge base. During testing, we retrieve the\nthree most similar images based on the question's image and incorporate only the textual knowledge\nto the input. For InfoSeek (Chen et al., 2023), we randomly sample 5K questions from the validation\nset and construct a knowledge base containing 50K entities from Wikipedia database provided by\nthe authors, which includes all relevant entities associated with the questions. For each question, we\nretrieve the most similar entity and add only the textual knowledge to the input.\nBaselines. For MyVLM, we find that when the training data is very limited, it is quite hard for\nthe classification head to work effectively. Therefore, we use data augmentation to help improve\nits performance. Specifically, we crop the single image into several pieces containing the target\nconcept to improve the accuracy of classification heads. To distinguish between multiple possible\ndifferent concepts that may appear in the image, we use (sks1), (sks2). . . as concept identifiers. For\nYoLLaVA, as there is no open-source code or model available, we present its experimental results\nas reported in the original paper (Nguyen et al., 2024)."}, {"title": "D.1 DATASET COMPOSITION", "content": "\u2022 We provide a summary of the composition of our dataset in Figure 7, which visually represents\nthe distribution of different components.\n\u2022 Table 8 presents detailed numerical data for each part.\n\u2022 In Table 9, we specify the sources for each component of our dataset."}, {"title": "D.2 INSTRUCTIONS", "content": "In this section, we present the instruction templates used to create our dataset:\n\u2022 Table 20 contains instructions for visual grounding and recognition.\n\u2022 Table 21 includes example instructions for image captioning.\n\u2022 Table 22 presents example instructions for image description.\n\u2022 Table 23 presents example questions used for question answering synthesis."}, {"title": "E ADDITIONAL DEMONSTRATIONS", "content": "In this section, we provide more qualitative results obtained by various models.\n\u2022 In Table 10, we demonstrate how our models achieve real-time editing of concepts by modifying\nthe database.\n\u2022 In Table 11, we demonstrate the real-time addition of new concepts by updating the database.\n\u2022 In Table 12, we present qualitative results on personalized conversation of RAP-LLaVA."}]}