{"title": "What makes a good feedforward computational graph?", "authors": ["Alex Vitvitskyi", "Jo\u00e3o G.M. Ara\u00fajo", "Marc Lackenby", "Petar Veli\u010dkovi\u0107"], "abstract": "As implied by the plethora of literature on graph rewiring, the choice of computational graph employed by a neural network can make a significant impact on its downstream performance. Certain effects related to the computational graph, such as under-reaching and over-squashing, may even render the model incapable of learning certain functions. Most of these effects have only been thoroughly studied in the domain of undirected graphs; however, recent years have seen a significant rise in interest in feedforward computational graphs: directed graphs without any back edges. In this paper, we study the desirable properties of a feedforward computational graph, discovering two important complementary measures: fidelity and mixing time, and evaluating a few popular choices of graphs through the lens of these measures. Our study is backed by both theoretical analyses of the metrics' asymptotic behaviour for various graphs, as well as correlating these metrics to the performance of trained neural network models using the corresponding graphs.", "sections": [{"title": "1. Introduction", "content": "Modern deep learning workloads frequently necessitate processing of sequential inputs, such as words in a sentence (Sutskever et al., 2014), samples of an audio recording (Van Den Oord et al., 2016), partially ordered nodes in a graph (Thost & Chen, 2021), execution steps of an algorithm (Veli\u010dkovi\u0107 et al., 2022), or snapshots of edits made to temporal graphs (Rossi et al., 2020). In addition, many important self-supervised learning tasks require efficiently predicting future evolution of such inputs, with examples ranging from temporal link prediction (Huang et al., 2024) to next token prediction (Radford et al., 2018).\nIn regimes like this, it is important to be able to train such models scalably and without leaking ground-truth data about the future input parts that need to be predicted. As such,\nmany modern architectures resort to feedforward computational graphs, wherein information may only flow from older samples towards newer ones\u2014never in reverse! Applying such a graph - also known as a \u201ccausal mask\" (Srambical, 2024) - allows for scalable training over entire chunks of the input at the same time.\nThis naturally invites the question: what makes a good feedforward computational graph? Alternately worded, which considerations need to be taken into account when deciding which feedforward graph to use (see Figure 1)?\nHere we propose two suitable and complementary ways to measure suitability of feedforward graphs: mixing time-the speed at which input data converges towards a stationary distribution and minimax fidelity-the sharpness of the data is as it propagates through the graph. We supplement our measures with thorough theoretical derivations across several graph generators, and correlate them to empirical performance, paving the way to future studies in the area."}, {"title": "2. Motivation", "content": "We were inspired to seek an answer to this question, given that there is already a rich, extensive body of work on studying undirected computational graphs (commonly referred to as graph rewiring). Therein, important issues such as over-smoothing (Li et al., 2018; Oono & Suzuki, 2019; Keriven, 2022), oversquashing (Alon & Yahav, 2021; Di Giovanni et al., 2024) and under-reaching (Barcel\u00f3 et al., 2020) have all been identified, and related to the input graph topology. In response, a substantial amount of methods have been proposed to modify the provided input graph for improved diffusion (Gasteiger et al., 2019), curvature (Topping et al., 2022; Fesser & Weber, 2024), effective resistance (Arnaiz-Rodr\u00edguez et al., 2022), or reducing smoothing (Azabou et al., 2023) and commute time (Sterner et al., 2024).\nEventually, the focus of study expanded beyond \"how to modify an input graph to be better?\u201d towards \"what makes a good input graph?\", propelling the discovery of task-agnostic graphs that are guaranteed to have good topological properties. This question is well-studied in mathematics, where it had led to the advent of expander graphs (Kowalski, 2019); graphs with highly sparse topologies but incredibly good information propagation. Once expanders have been discovered in the context of graph machine learning, they have seen equal application among graph neural networks (Deac et al., 2022; Christie & He, 2023; Wilson et al., 2024) and graph Transformers (Shirzad et al., 2023; 2024).\nUnfortunately, to the best of our knowledge, the state-of-the-art is not as rich in the domain of feedforward graphs. This includes the domain of mathematics, wherein many important concepts have not been generalised to the directed case as they may rely on spectral properties of the graph structure (Chung, 1997), and many spectral properties are ill-defined on a feedforward graph. In terms of practical usage, most of the heavy lifting is done either by fully connected feedforward graphs or locally-connected sliding-window graphs. And while recent work has identified limitations of high-indegree feedforward graphs through over-squashing (Barbero et al., 2024) and dispersion (Veli\u010dkovi\u0107 et al., 2024), most such works do not actively offer a different computational graph structure, nor do they offer any principles that can be used to derive one. We seek to fill this gap.\nWhat's in a good metric? Both of the above papers make it clear at least if size generalisation is a desirable property-that it is beneficial to limit the in-degree of each node in the feedforward graph.\nA good metric should be able to help us, in the very least, compare against different graph distributions with same asymptotic in-degree budget.\nIt might also be helpful if the metric would be able to hint to practitioners at what point the in-degree budgets become\""}, {"title": "3. Theoretical foundations", "content": "In this work, we study feedforward graphs, $\\mathcal{G} = (\\mathcal{V}, \\mathcal{E})$, i.e., graphs where nodes are given by an ordered set of $n$ integers ($\\mathcal{V} = \\mathbb{Z}_n$) and edges are constrained to only go forwards; that is, $(a, b) \\in \\mathcal{E} \\Rightarrow a < b$. We will denote by $\\tau \\in \\mathcal{V}$ the sink vertex of that graph ($\\tau = n - 1$), a node which is used to drive decision-making, and which is not allowed to have any outward edges (except to itself).\nMuch like a degree in an undirected graph, a node $i$ in a feedforward graph has an indegree $d_i^{\\leftarrow}$ and an outdegree $\\delta_i^{\\rightarrow}$, counting the number of incoming and outgoing edges to/from node $i$.\nAs an illustrative example, two types of feedforward graph are commonly used for machine learning tasks today:\n*   The fully connected feedforward graph, which draws an edge between every allowed pair of nodes; $\\mathcal{E} = \\{(a,b) | a \\leq b\\}$. Indegrees and outdegrees in this graph are $d_i = i + 1$, $\\delta_i^{\\rightarrow} = (n - i)$.\n*   The locally connected feedforward graph, which draws an edge between all allowed pairs of nodes up to a distance $\\kappa \\geq 0$ apart: $\\mathcal{E} = \\{(a, b) | b - \\kappa \\leq a \\leq b\\}$. A special case of $\\kappa = 1$ is known as a line graph. Indegrees and outdegrees are $\\delta_i^{\\leftarrow} = \\delta_i^{\\rightarrow} = \\kappa + 1$.\nWe now outline three desirable properties which we will typically assume during our exploration:\nSelf-edges. We will always assume all self-edges are present in the graph \u2013 that is, $(i, i) \\in \\mathcal{E}$ for all $i \\in \\mathcal{V}$, unless otherwise stated. This is generally an important design decision for improving data retention.\nUnique sinks. As we will be tracking how easily information from each node can reach the designated sink node, $\\tau$, it is highly desirable that our graph generator does not create additional sink nodes of any kind (beyond the final node, of course). Any additional sink nodes imply an impossibility to reach the final sink from such nodes, and they are hence effectively excluded from decision making.\nSelf-similarity. While most of our theory will concern the flow of information specifically into $\\tau$, it is important to note that in real workloads, any node might be used as a sink node (over an appropriately sampled sub-input). As such, we focus our attention on self-similar feedforward graphs, wherein we can expect similar flow properties to all nodes (including ones in the middle)."}, {"title": "4. Mixing time: Tracking path complexity", "content": "We are now ready to work our way towards defining the first of our two measures, the averaged mixing time of the feedforward graph. This measure tracks how quickly information travels towards the sink, by carefully analysing the expected path length from each node to the sink. Clearly, the aim is to keep this value within reasonable upper bounds.\nFor a feedforward graph of $n$ nodes, $\\mathcal{G} = (\\mathbb{Z}_n, \\mathcal{E})$, let $A \\in \\mathbb{R}^{n \\times n}$ be its adjacency matrix, defined in the usual way:\n$A_{ij} = \\begin{cases} 1 & (j, i) \\in \\mathcal{E} \\\\ 0 & (j, i) \\notin \\mathcal{E} \\end{cases}$ (1)\nkeeping in mind that $A$ must be lower-triangular due to the feedforward constraint.\nIts walk matrix $W$ is given by\n$W_{ij} = \\begin{cases} 1/\\delta_j^{\\rightarrow} & (j, i) \\in \\mathcal{E} \\\\ 0 & (j, i) \\notin \\mathcal{E}. \\end{cases}$\nSo $W$ is the transition matrix for the (lazy) random walk where there is an equal probability of leaving vertex $j$ along any of its outgoing edges. This will be well defined as long as $\\delta_j^{\\rightarrow} > 0$ for all $j \\in \\mathbb{Z}_n$; we are guaranteeing this condition by requiring all self-edges within the graph.\nNote that for graphs where, for all $j \\in \\mathbb{Z}_n$, $\\delta_j^{\\rightarrow} = \\kappa$ for some constant, $\\kappa$, the walk matrix is related to the usual adjacency matrix $A$. Specifically\n$W = \\frac{1}{\\kappa} A.$\nHence, in this case, the spectrum of the walk matrix is obtained from the spectrum of $A$ by scalar multiplication.\nA probability distribution $\\pi\\in \\mathbb{R}^n$ on the vertices is station-ary if $W\\pi = \\pi$. In other words, $\\pi$ is unchanged by one step of the random walk.\nIt is a well-known result that a strongly connected graph has a unique stationary distribution. Here, a graph is strongly connected if, for any ordered pair of vertices, there is an oriented path joining them. However, the graphs we will consider will be feedforward and hence not strongly connected. In that case, we have the following useful result.\nLet $\\mathcal{G}$ be a feedforward graph with a unique sink vertex $\\tau$. Then there is a unique stationary distribution for $W$, namely $1_\\tau$, the probability distribution taking the value 1 at $\\tau$ and 0 elsewhere.\nProof. Certainly $1_\\tau$ is stationary. To see that it is unique, consider a distribution $\\pi$ other than $1_\\tau$. Let $i < n$ be its smallest vertex with $\\pi_i \\neq 0$. When applying the random walk matrix $W$, we can see that the probability of being at $i$ after one step is $\\pi_i/\\delta_i^{\\rightarrow}$. Since $\\delta_i^{\\rightarrow} > 0$, we deduce that $\\pi$ is not stationary.\nThere are several definitions of mixing time in the literature. Here is one. The mixing time is the smallest value of $t$ such that for any starting distribution $x$,\n$\\|W^t x - \\pi\\|_1 < 1/4.$\nHere, the $L^1$ norm is used on probability distributions, i.e., for two probability distributions $x$ and $y$,\n$\\|x - y\\|_1 = \\sum_i |x_i - y_i|.$\nThere is nothing special in the use of 1/4 here. Any fixed $0 < \\epsilon < 1/2$ would work. In our set-up, it is reasonable not consider a minimum over starting distributions $x$ but an 'averaged' version:\n$\\frac{1}{n} \\sum_i \\|W^t e_i - \\pi\\|_1,$\nwhere $e_i$ is the probability distribution concentrated on ver-tex $i$. If we let $\\Pi$ be the square matrix that has $\\pi$ in each column, then this is\n$\\frac{1}{n} \\|W^t - \\Pi\\|_1,$\nwhere now we are taking the $L^1$ norm on matrices:\n$\\|B\\|_1 = \\sum_{ij} |b_{ij}|.$\nWe call this quantity the averaged mixing time (D\u00edaz et al., 2024). We will now demonstrate its behaviour is as expected on two commonly used graph distributions.\nLet $\\mathcal{G}$ be the line graph from vertex 0 to vertex $n - 1$, as previously defined. Suppose that we start a random walk at vertex $n - i - 1$, i.e., the initial probability distribution is $e_{n-i-1}$. We want to consider the probabilty of not reaching the terminal vertex after $t$ steps. We can think of flipping a coin $t$ times and we step right only if we get a head. We reach the terminal vertex if and only if we get at least $i$ heads. So the probability of not reaching it in $t$ steps is\n$\\sum_{j < i} \\binom{t}{j} \\left(\\frac{1}{2}\\right)^t,$"}, {"title": "5. Minimax fidelity: Information sharpness", "content": "Mixing time provides an excellent estimate of how long will information need to travel in a graph-it is generally a good idea to keep it low. However, it is not the whole story: it is irrelevant if information from a given node travels fast to the sink if only a small proportion of it makes it through. To quantify the extent to which information sharply reaches the sink, we will be using the minimax fidelity metric.\nAdditionally, we require that every node must have a positive in-degree; that is, $d_i^{\\leftarrow} > 0$ for all $i \\in \\mathcal{V}$. Note that this will"}, {"title": "6. The FunSearch (FS) graph generator", "content": "Apparently, even after trying out many hand-crafted sparse graph generators, we have not been able to strike the right balance between mixing time and fidelity. So we turned our attention to evolutionary methods: as there's no clear method of constructing graphs which optimise the fidelity while keeping the mixing time within reasonable bounds, we used the FunSearch (Romera-Paredes et al., 2023) algorithm to produce graphs with good values for both metrics. The most promising result generated by FunSearch may be found in Figure 2 (Left), and it turns out to be an impactful motif."}, {"title": "7. Results", "content": "To supplement our findings with empirical performance metrics, we evaluate a graph attention network (Veli\u010dkovi\u0107 et al., 2018)-style model on three representative tasks:\n*   finding the highest valued node in a sequence (Veli\u010dkovi\u0107 et al., 2024);\n*   finding the second highest valued node in a sequence (Ong & Veli\u010dkovi\u0107, 2022);\n*   computing the parity value of a bitstring (Hahn, 2020).\nusing adjacency matrices sampled from our graph generators. The tasks are chosen to represent opposite extremes of sharpness required: for finding the maximum, exactly one node needs to propagate its input features to the sink \u2013 for"}, {"title": "8. Conclusions", "content": "In this work, we have embarked on a detailed study of feedforward computational graphs, attempting to chart a novel path that could enable for a principled discovery of useful feedforward blueprints. While it is apparent that analysing these graphs is substantially less straightforward than doing so with their undirected counterparts, we believe the outcomes to have been fruitful. Namely, we proposed two well-justified metrics of feedforward information propagation, used them to automatically discover an interesting graph generator, and demonstrated its strong performance on several carefully crafted benchmarks. We are hopeful that our work will inspire targeted follow-up studies in this space, which may usher in a new paradigm of graph rewiring."}, {"title": "A. Proof that minimax fidelity in a line graph decays to zero with increasing size", "content": "First let's determine an expression for $\\max_a \\frac{\\binom{k}{a}}{2^a}$ for a fixed $n$.\nNotice that this is equivalent to finding an expression for $\\max_a \\binom{k}{2a}$ and then replacing $k = n - 1$.\nWith a fixed $n$ we know that $a \\geq k$, otherwise the binomial term would simply be 0. Now let $f(a) = \\binom{k}{2a}$, and let's compute $\\frac{f(a + 1)}{f(a)}$:\n$\\begin{aligned}\n\\frac{f(a+1)}{f(a)} &= \\frac{\\binom{k}{a + 1}}{\\binom{k}{a}} \\\\\n&= \\frac{\\frac{k!}{(a + 1)!(k - a - 1)!}}{\\frac{k!}{a!(k - a)!}} \\\\\n&= \\frac{a!}{k!(a - k)!} \\frac{k!(a \u2013 k)!}{a!} \\\\\n&= \\frac{a + 1}{2a-k+1}\n\\end{aligned}$\nFrom that we know that $\\frac{f(a+1)}{f(a)} > 1 \\iff (a + 1) \\geq 2(a - k + 1) \\iff 2k - 1 \\geq a$. As such f(a) increases for a in [k, 2k - 1] and decreases for $a > 2k$. We can thus say that\n$\\max_a \\binom{2k}{a} = \\binom{2k}{2k}$\nNow let's compute our limit using Stirling's Approximation:\n$\\begin{aligned}\n\\lim_{k \\rightarrow \\infty} \\frac{\\binom{2k}{k}}{2^{2k}} &= \\lim_{k \\rightarrow \\infty} \\frac{\\frac{2k!}{k!k!}}{2^{2k}} \\\\\n&= \\lim_{k \\rightarrow \\infty} \\frac{\\sqrt{2\\pi(2k)} \\left(\\frac{2k}{e}\\right)^{2k} \\frac{1}{\\left(\\sqrt{2\\pi k} \\left(\\frac{k}{e}\\right)^{k}\\right)^2} \\frac{1}{2^{2k}} \\\\\n&= \\lim_{k \\rightarrow \\infty} \\frac{\\sqrt{2\\pi k}}{2^{2k}} \\left(\\frac{2k}{k}\\right)^{2k} \\\\\n&= \\lim_{k \\rightarrow \\infty} \\frac{1}{\\sqrt{\\pi k}} \\cdot \\frac{1}{2^{2k}} = 0\n\\end{aligned}$"}, {"title": "B. Proof of Theorem 6.1.", "content": "In order to help us reason about the mixing time, it will be very useful to compute it as a function of the starting level and block the random walker is in. That is, we will track $T(d, k)$ as the expected number of steps needed for a random walker to reach the sink node, assuming the walker is currently $d$ depth levels away from the deepest recursive level, and $k$ blocks away from reaching the final block in the current depth level.\nWe already know that $k$'s maximal value will be $\\lceil\\log n\\rceil$ for each depth level, due to the generator's parameters. For the maximal value of $d$, i.e. the total number of depth levels, we note that after each level, the number of nodes being considered is further subdivided by $\\lceil\\log n\\rceil$. This means that, after $l$ levels, the block size is $\\lceil\\frac{n}{\\log n^l}\\rceil$. No further subdivisions are possible once the block size reaches 1.\nSince we are mainly interested in the asymptotic number of depth levels, we make a simplifying assumption that both log $n$ is an integer and that n is divisible by each power of log $n$. In this situation, the total number of levels $D$ is such that $n = \\log^D n$.\nThis implies $D = \\frac{\\log n}{\\log\\log n}$, i.e., the number of depth levels is $O\\left(\\frac{\\log n}{\\log\\log n}\\right)$.\nWith this in mind, we aim to quantify $T\\left(\\frac{\\log n}{\\log\\log n}, \\log n\\right)$, once again under the simplifying assumption that all of these variables are integers. To do this, we will establish several upper bounds on $T(d, k)$, assuming pessimistic behaviour from the random walker.\nFirstly, when the walker is in the final level (d = 0), the number of nodes considered is 1, hence there are are no blocks left to traverse, and therefore $T(0, k) = 0$ for all $k$."}]}