{"title": "CausalChat: Interactive Causal Model Development and Refinement Using Large Language Models", "authors": ["Yanming Zhang", "Akshith Kota", "Eric Papenhausen", "Klaus Mueller"], "abstract": "Causal networks are widely used in many fields to model the complex relationships between variables. A recent approach has sought to construct causal networks by leveraging the wisdom of crowds through the collective participation of humans. While this can yield detailed causal networks that model the underlying phenomena quite well, it requires a large number of individuals with domain understanding. We adopt a different approach: leveraging the causal knowledge that large language models, such as OpenAI's GPT-4, have learned by ingesting massive amounts of literature. Within a dedicated visual analytics interface, called CausalChat, users explore single variables or variable pairs recursively to identify causal relations, latent variables, confounders, and mediators, constructing detailed causal networks through conversation. Each probing interaction is translated into a tailored GPT-4 prompt and the response is conveyed through visual representations which are linked to the generated text for explanations. We demonstrate the functionality of CausalChat across diverse data contexts and conduct user studies involving both domain experts and laypersons.", "sections": [{"title": "I. INTRODUCTION", "content": "Causal relationships are the building blocks of how we make sense of the world. They help us understand why things happen the way they do, from the simple cause- and-effect of a light switch to the complex interplay of factors influencing societal trends. We encounter causal facts in everyday life, whether in conversations about health choices or discussions on broader issues like health policies.\nBeneath this universal concept lies a deep philosophical debate between 18th-century thinkers David Hume and Im- manuel Kant. Hume, an empiricist, viewed causality as a mental habit formed by repeated experiences [1], while Kant, a transcendental idealist, saw it as an inherent concept imposed by the mind on sensory experience [2]. The Hume-Kant debate echoes in the modern discussion of causal inference from data versus text. Data-driven methods, like Hume's empiricism, rely on statistical analysis of observations, while text-driven meth- ods, aligned with Kant's idealism, extract causal knowledge from literature or people. Both have limitations: data-driven methods struggle with unobserved confounders, biases, and overfitting [3], while text-based methods face challenges like language ambiguity, lack of domain expertise, and difficulty distinguishing causation from correlation [4].\nIn this paper, we integrate perspectives from Kant and Hume in advancing causal analysis. To incorporate Kant's perspec- tive we leverage large language models (LLMs) trained on extensive text data, and so harness their rich causal knowledge to minimize ambiguities. Following Hume's approach, we also incorporate data when available. While this combined strategy is not entirely new [5], to our knowledge this approach has not been explored thus far with LLMs. It allows users to nav- igate and aggregate complex causal relationships, enhancing accessibility even for those without domain expertise.\nA widely known issue with LLMs like ChatGPT is that they can produce inaccurate information and hallucinations, even with well-constructed prompts [6]. To address this problem, we approach causal questions from every possible angle, en- hancing confidence in selected relationships while uncovering potential conflicts. A useful aspect of posing causal questions from diverse perspectives is that it sheds light on the broader context of the relationship. ChatGPT often highlights latent, confounding and mediator variables, which offer valuable insights. Taking an additional step to explicitly request these variables facilitates domain knowledge acquisition, supporting focused data collection and refinement of causal models.\nFinally, while multifaceted prompts like these offer valuable perspectives, they generate substantial textual data, which can potentially be overwhelming even with summarization. To aid human analysts in navigating this information, we introduce a set of interactive visualizations designed to efficiently convey key insights. This paper presents our approach and its appli- cation in developing and refining causal networks from text and data, involving active human participation.\nIn summary, our contributions are as follows:\n\u2022 a practical implementation of a recursive text/data-driven causal network development and refinement paradigm\n\u2022 a set of LLM prompts that interrogate a hypothesized causal relation from diverse perspectives, such as \u201cdoes high A cause low B\" and so forth\n\u2022 a suite of visual representations and charts to commu- nicate the relation's diverse causal perspectives and its latent variables, confounders, and mediators\n\u2022 a visual component capable of managing feedback loops in causal inference, extending into multiple causal net- works tailored for diverse decision-making processes\n\u2022 an explainable AI mechanism that links the visual repre- sentations back to the prompted LLM text\n\u2022 an interactive visual interface that puts the analyst into the loop of the recursive text/data-driven causal network development and refinement activities\n\u2022 a set of case studies and usage scenarios that demonstrate our ideas and studies with experts and laypersons to gain insight about the usage of our system"}, {"title": "II. RELATED WORK", "content": "Causal networks are widely used in various fields, such as epidemiology [8], healthcare [9], biology [10], and social sciences [11] to understand complex systems. In observational studies, causal networks are typically represented using di- rected acyclic graphs (DAGs), as DAGs can illustrate potential sources of confounding bias and selection bias [12], [13]. Constructing such a DAG is best achieved through the rig- orous process of randomized controlled trials based on first principles. However, this method often faces challenges, be it due to cost, ethical considerations, or practical limitations. Ad- ditionally, it is not easily scalable and may restrict the number of researchers who can engage in such studies. Alternatively, a more scalable and general approach involves deriving a causal DAG through one of three primary methods: analyzing data, analyzing text, or collaborative construction and crowdsourc- ing. Many of these approaches combine elements from more than one of these paradigms, and some allow human analysts to participate in the DAG development process [14]\u2013[16]."}, {"title": "A. Causal Network Discovery using Numerical Data", "content": "There are essentially two popular strategies for causal discovery. One approach involves enforcing the constraint that two statistically independent variables are not causally linked, followed by a series of conditional independence tests to construct a compliant DAG. Well-known algorithms for this method include the PC algorithm [17] and the Fast Causal Inference (FCI) algorithm [18]. Another strategy is to greedily explore the space of possible DAGs via Greedy Equivalence Search (GES) [19]. This entails score-based methods in which edges are iteratively added and removed from the graph to maximize a model fitness measure, such as the Bayesian Information Criterion (BIC) [20], [21].\nCausal discovery relies on four common assumptions: (1) the causal structure can be represented by a DAG, (2) all nodes are Markov-independent from non-descendants when conditioned on their parents, (3) the DAG is faithful to the underlying conditional independence, and (4) the DAG is suf- ficient, i.e., there is no pair of nodes with a common external cause. Unfortunately, these conditions are rarely entirely met, often due to selection/sampling bias in the data. Essentially, the phenomenon to be explained by the causal network is only partially captured by (1) the measured variables and (2) the observed data samples, and this leads the discovery algorithm astray. While the probability of obtaining a partially incorrect DAG can be reduced by using more data, it remains uncertain how much data is truly needed [22], [23].\nOur research tackles both of these bottlenecks: (1) the limitation of collected datasets in fully capturing all vari- ables essential for constructing a comprehensive causal model, which we address through ChatGPT-based variable ideation and relevance assessment, and (2) in the absence of data for newly discovered relations connecting ideated variables to the DAG, we use ChatGPT to generate plausible hypotheses for the strengths of these links based on its contextual understand- ing and patterns learned from vast sources."}, {"title": "B. Causal Network Discovery using Textual Data", "content": "While extracting causal relations from text documents is not a new endeavor [24], thanks to ChatGPT this process has become remarkably convenient with a simple prompt across various application domains. The literature on LLM assisted causal network learning is rapidly expanding. The earliest documented attempt using LLMs (specifically GPT- 3) for causal analytics was by Long et al. [25]. However, this was a preliminary study focused on optimizing prompts to reveal insights into the presence or absence of directed edges. More recently, K\u0131c\u0131man et al. [26] delved much deeper into the subject. They devised a comprehensive set of prompts for GPT-3.5 and GPT-4, generating yes/no responses to standard causal queries. While they demonstrated excellent success rates on benchmark datasets where causal truth was known, they did not explore utilizing GPT's output to gather additional causal and contextual knowledge. Similarly, subsequent papers (e.g., [27]-[29]) also did not explore visualizing the acquired information within explainable and trustworthy AI.\nSome studies have highlighted the limitations of using LLMs for causality analysis. While LLMs excel at discerning causality from empirical or commonsense knowledge, Jin et al. [30] demonstrated their significantly reduced effectiveness in deriving causality through pure causal reasoning\u2014something numerical algorithms like PC are specifically designed for. To test this, they assembled a substantial dataset comprising over 400,000 correlational statements in natural language and tasked the LLM with determining the causal relationship between variables. Their findings revealed that existing LLMs demonstrated performance is akin to random chance in this particular task. These findings are echoed by Ze\u010devi\u0107 et al. [31], who suggest that LLMs can serve as a valuable starting point for learning and inference, reaffirming their role as a tool for ideation and creativity. They can complement data-driven causal inference methods, such as PC, which is what one of the approaches we promote here in this paper."}, {"title": "C. Collaborative Causal Network Discovery with Crowds", "content": "In 2018, Berenberg and Bagrow [32] introduced a method- ology that harnessed the 'wisdom of the crowds' to construct a large causal network, utilizing the widely-used crowdsourcing platform Amazon Mechanical Turk. They devised a three-stage approach: in stage 1, workers proposed causes; in stage 2, they suggested effects for these causes; and in stage 3, they edited and refined longer causal pathways derived from the stage 2 results. The final causal network was then formed by amalgamating all worker-generated pathways, with more popular edges indicating stronger causal links. Salim et al. [33] adopted a similar approach, focusing on mining crowd"}, {"title": "III. METHODOLOGY", "content": "Fig. 1 illustrates the workflow of our method. Step 1 com- putes an initial causal model from the user-provided data. This model is displayed as a Completed Partially Directed Acyclic Graph (CPDAG) in a our visual causal analytics interface, where the model coefficients are computed through Structural Equation Modelling (SEM). Alternatively, users may skip this data-driven step and draw their own causal network. The model can then be refined based on the outcome of a ChatGPT prompt with regards to a certain relation indicated by the user we have used the GPT-4 API which at the time of this research was the most advanced OpenAI GPT version. In the remainder of this paper will refer to this API as 'GPT-4'.\nTo aid in the GPT-4 powered causal analytics activity, we provide various visualizations that summarize the gist of these responses, linked back to the corresponding GPT-4 response text. Finally, if a new variable or relation is introduced, the respective data may be uploaded (if available) from a suitable data repository and the model coefficients are updated. The workflow adopts a loop optimization structure, wherein the incorporation of new data and the updating of model coefficients aligns with Hume's empiricism, while the inferred causal knowledge provided by GPT-4 is consistent with Kant's epistemology.\nIt is worth noting that the study by Berenberg and Bagrow predates the emergence of LLMs. While the degree to which Large Language Models (LLMs), trained on extensive human- written text, tap into the 'wisdom of the crowds' remains uncertain, it is plausible to expect that LLM assistance would necessitate a significantly smaller crowd. As LLMs effectively encapsulate the viewpoints of a large crowd simultaneously, using a few-shot prompt programming approach can guide and constrain the response towards a pertinent answer [34]. We believe that our multifaceted prompt represents a significant step in this direction.\nYen et al. [35] developed an interactive system for col- laborative causal network construction. This system enabled users to articulate narratives to explain causal relationships they perceived, visualize the causal models of these using DAGs, and review and incorporate the causal diagrams and narratives of other users. A notable feature of their system was the 'Inspire Me' popup, which users could request when they needed fresh ideas on how to expand the network. They would then be presented with one of several pre-programmed thought-provoking questions related to causal relationships.\nThe purpose of this system was to investigate whether actively evolving and narrating a causal network, and learning from networks constructed by peers, could uncover blind spots in a person's causal reasoning and lead to a refinement of their own causal network. In a more recent development, the authors introduced an enhanced user interface called CrowdIDEA [5]. This version also included a data panel with visualizations and statistics. It is conceivable that an LLM could fulfill a similar collaborative role. For instance, in our system, users have the ability to explore any variable or variable pair and visualize suggested directions, confounders, and mediators, all ready to be seamlessly integrated into the emerging causal network."}, {"title": "A. Prompting for Direct Causal Relationships", "content": "We investigate causality at two levels: the first examines the existence of direct causal relations, while the second digs deeper to uncover latent variables, mediators, and confounders.\nThe prompts we utilize follow an optimized template (see supplemental material). Offering sufficient contextual infor- mation and guidance on expectations is crucial for prompt engineering [34], [36]. Below is an abstraction of the prompt.\nPrompt: You are an expert in <domain>. On a scale from 1 to 4, where 4 represents highly significant, 3 represents signif- icant, 2 represents doubtful, 1 represents not significant, rate the following cause-and-effect relationship: Does higher/lower A/B cause higher/lower B/A.\nThis generates 10 distinct prompts, 5 each for A and B taking opposite roles, and within each of these two sets there are 4 combinations of A and B being (higher, lower) plus one relation that just asks this for a general case. An example prompt is shown below, where (...) denotes further prompt specifications (see supplement for complete prompt):\nPrompt: You are an expert in public health. On a scale from 1 to 4, where 4 represents highly significant, 3 represents significant, 2 represents doubtful, 1 represents not significant, rate the cause-and-effect relationship: Does higher percent fair or poor health cause lower life expectancy...\nIncluding the domain hint 'public health' provides contextual information. GPT-4 can also infer the domain from the dataset attributes if it is told to do so. GPT-4's response to the prompt is to the point:\nResponse: Rating: 4"}, {"title": "B. Prompting for Confounders", "content": "This prompt template also distinguishes among the 4 com- binations that explore the effects of higher and lower levels plus one relation that just asks this for the general case. In the following we explain this template using the variables food environment index and violent crime rate as an example. Also here (....) denotes omissions (see supplement).\nPrompt: You are an expert in public health. Given the cause- and-effect relationship 'lower food environment index' causes 'higher violent crime rate' identify potential confounders based on the definition For each identified confounder, provide the following details in a tuple format: 1. Name of the confounder. 2. Strength of the confounder (options: weak, medium, strong). 3. Justification for its role as a confounder based on the definition provided.\nResponse: GPT-4 returned 2 'strong' confounders (Socioe- conomic Status, Residential Segregation) and 4 'medium' confounders (Substance Abuse and Mental Health Issues, Availability of Public Services, Racial and Ethnic Compo- sition, Neighborhood Disorganization). For each a detailed justification was given, such as \"Substance abuse and mental health issues can contribute to both a lower food environment index (due to prioritization of immediate needs over healthy food choices) and higher rates of violent crime, as these issues can lead to unstable social environments\"."}, {"title": "C. Prompting for Mediators", "content": "Also here we distinguished among the 4 level combina- tions and the general one. Using the same example as for the confounder, the (partial) mediator prompt is below (see supplement for complete prompt).\nPrompt: You are an expert in public health. Given the cause- and-effect relationship 'lower food environment index' causes 'higher violent crime rate' identify potential mediators based on the definition: Rather than a direct causal relationship between the independent variable and the dependent variable, the independent variable influences the mediator variable, which in turn influences the dependent variable. For each identified mediator, provide the following details in a tuple format: 1. Name of the mediator. 2. Strength of the mediator (options: weak, medium, strong). 3. Justification for its role as a mediator (...) 4. Specific conditions under which the mediator operates (...) 5. Direction of the mediator's effect ('positive' or 'negative') (...). The direction tells us how to intervene on the mediators to achieve the relationship....\nThe 'Direction' parameter (parameter 5) is crucial as it spec- ifies the way in which the level of a mediator should change to influence the effect variable as indicated. This guidance not only informs analysts about the type of intervention required but also sheds light on the underlying cause-effect mechanism.\nResponse: GPT-4 returned 1 'strong' mediator (Economic Disadvantage \u2191) and 4 'medium' mediators (Social Cohesion \u2193, Substance Abuse \u2191, Educational Attainment\u2193, Mental Health), where \u2193 \u2191 indicate the direction the mediator needs to have to support the effect. For each mediator a detailed justification was given, such as \"A lower food environment index may contribute to reduced social cohesion within a community, as limited access to nutritious food options can lead to increased stress and poorer overall health. Reduced social cohesion has been associated with higher rates of violent crime, as it may lead to weaker community bonds and less effective informal social control\"."}, {"title": "D. Prompting for Latent Factors", "content": "Unlike previous prompts, this prompt focuses on a single variable, emphasizing intervenable factors. It identifies action- able variables as points of intervention, enabling practitioners to influence the target variable through specific causal path- ways. An example of a latent factors prompt is provided below (see supplement for the full prompt).\nPrompt: Given the target variable primary care physicians rate, identify potential latent (intervenable) factors that might influence the target variable. Ensure that the identified latent factors can be actionable or intervenable to affect the target variable. Provide the following details for each latent factor: 1. Name of the latent factor. 2. Strength of the effect (weak, medium, strong). 3. Sign of the effect (positive, negative, or categorical). 4. Justification for its role as a latent factor.\nResponse: GPT-4 returned 1 'strong' positive latent factor (Reimbursement Rates), 2 'medium' positive latent factors (Medical Infrastructure Investment and Healthcare Policy Re- forms), 1 'strong' negative latent factor (Medical Student Debt), and 1 'medium' negative latent factor (Urbanization Incentives). For each latent factor a storyline rationale was given. For instance, Medical Student Debt can serve as a negative latent factor, as 'high levels of debt from medical education can deter graduates from entering lower-paying specialties like primary care.' Medical Student Debt can be addressed through governmental medical debt relief programs, which act as intervention points."}, {"title": "E. Visualizing the GPT-4 Generated Text Responses", "content": "While the inclusion of text helps justify the presence (or absence) of a causal relation, GPT-4 may generate excessive text. Even when instructed to summarize its findings, this abundance of information can overwhelm general users. Be- low, we present the visualizations we have designed to make browsing this information easier."}, {"title": "F. The Causal Debate Chart", "content": "Fig. 2 shows our summary visualization that contrasts the numerical outcomes of the 10 prompts designed to probe a direct causal relation. We call it Causal Debate Chart since it visually argues the strength of one variable being the cause of the other. The chart is a bidirectional bar chart where each side is headed by one of the two relation variables. In this case the left side is Percent Fair or Poor Health (PFPH) and the right side is Life Expectancy (LE). The x-axis is the score assigned by GPT-4 and the length of each bar is mapped to that score. The grey bars are for the general prompt while the other bars are colored in magenta if the cause was a higher or increasing level of the variable or in sky blue if the cause was a lower or decreasing level (see color legend on the top right)."}, {"title": "G. The Causal Relation Environment Chart", "content": "The Causal Relation Environment Chart is a diagram that visualizes the complete causal relation, consisting of the two main relational variables along with a list of mediators and confounders/covariates. An example for the general Percent Fair or Poor Health (PFPH) - Life Expectancy (LE) relation is shown in Fig. 3, where the confounders are colored in shades of red and the mediators are colored in shades of green \u2013 the shades indicate their strength (see color legend on the right). It is often the case that GPT-4 will identify similar mediators and confounders for the more focused (low, high) relations. But they vary in the sign. For example, to go from low PFPH to high LE, positive levels of the mediators are cited, such as good access to healthcare and good health habits, while to go from high PFPH to low LE the cited mediators are usually the opposite, like limited access to health care and poor health habits. Fig. 4 shows these two cases where the up and down arrows indicate the positive and negative levels, respectively.\nFig. 5 explores the unlikely relationship of high PFPH causing high LE. GPT-4 correctly identifies this as \"counter- intuitive\" but treats it as a 'hypothetical scenario'. It suggests that a mediating relationship would need to exist to achieve the desired high LE. These mediators represent potential points of intervention that policymakers could target to increase high LE, despite the high PFPH. For example, a policymaker might intervene by opening additional health clinics in areas with high PFPH, thereby increasing the mediator, Access to Healthcare, which in turn improves high LE."}, {"title": "H. The Latent Factors Chart", "content": "The Latent Factors Chart is designed to advise on supple- mentary variables that should be included in the analysis of the current causal graph. It highlights the variable of interest alongside other potential factors that may directly influence it. These latent factors are color-coded: different shades of yellow represent negative effects, while different shades of blue represent positive effects. By reviewing the provided justifications, users can verify the relevance of these latent factors and incorporate the most significant ones into the"}, {"title": "I. Model Tree", "content": "Although DAGs provide intuitive representations for causal graphs, their scalability is limited, with typical graphs contain- ing an average of 12 nodes, with most graphs having between 9 and 16 nodes [37]. To address the challenge of representing causal relationships involving dozens or even hundreds of variables while still using DAGs, we introduce the Model Tree (see Fig. 9(A)).\nThe Model Tree is an N-ary tree structure in which each node represents a distinct causal graph. The root node cor- responds to the global model, providing an overview of the entire system, while child nodes represent progressively more specific or local models. Users interact with the general model at the root and can select a subset of nodes to create a child node in the Model Tree. These child nodes inherit a subgraph"}, {"title": "J. Graphical Encoding Schemes and Dashboard", "content": "In the graphical encoding schemes, we sought to maintain clarity across different diagrams while ensuring consistent meaning for shapes and colors that represent similar concepts. As mentioned, in the causal diagram, red lines (varying in thickness) represent positive causal relationships, and green lines represent negative causal relationships. The variation in thickness reflects the strength of these relationships. Different shapes provide context: oval-shaped purple marks the outcome variable, cyan denotes a selected node, unfilled ovals signify causal nodes, and dotted shapes/lines indicate elements derived from GPT-4 that are not yet confirmed by data.\nIn the Causal Debate Chart, using magenta bars for increas- ing levels and sky blue bars for decreasing levels provides a clear visual contrast, ensuring that this chart remains visually distinct from the others, especially in its portrayal of variable levels. The Causal Environment Chart uses red shaded boxes for confounders and green shaded boxes for mediators, main- taining consistency with the causal diagram's color scheme but applying the colors to shaded boxes (instead of lines), which helps differentiate between the two types of charts.\nFinally, the Latent Factors Chart introduces blue shaded boxes to represent positive influence and yellow shaded boxes for negative influence. These colors were chosen to avoid over- lap with the red and green used in other charts, allowing the latent chart to stand apart while still adhering to a recognizable positive/negative color scheme.\nAll charts and GPT-4 justifications are accessible via an in- t"}, {"title": "IV. USAGE SCENARIOS", "content": "In this section, we demonstrate the capabilities of CausalChat by presenting two usage scenarios that employ real-world datasets."}, {"title": "A. Exploratory Causal Analysis: Automotive Engineering", "content": "In this study, we focus on Oscar, an automotive hobbyist who wants to gain more insight into automotive engineering. Oscar finds the AutoMPG dataset and reads it into CausalChat. He then employs the GES algorithm and obtains the pre- liminary causal graph shown in Fig. 7 (see caption for an explanation of the edge coloring). We now follow Oscar in his mission to audit and expand this causal graph.\nResolving undirected edges. Oscar identifies four blue edges that the GES algorithm could not resolve, possibly due to the dataset's limited coverage of the car domain. He employs CausalChat's GPT-4 suite to address these edges. For brevity, we shall focus on how he resolves the blue edge between Cylinders and Displacement, the procedure for the other blue edges is similar.\nOscar clicks on the blue edge and the system generates the Causal Debate Chart depicted in Fig. 8. It is immediately apparent that the bars representing Cylinders are notably longer than those for Displacement, for the general grey bars as well as for the red-red and the blue-blue bars and all at full strength - a classic pattern for a positive causation. Oscar contemplates converting the blue edge to a green directed link from Cylinders to Displacement. After confirming this direction from the GPT-4 justification panel (see an example in Fig. 9D), Oscar directs the edge as suggested. In a similar fashion he also directs the other blue undirected edges.\nAdding confounders and latent variables. Oscar is still curious about the relation of Displacement and Horsepower. He examines the corresponding Causal Relation Environment Chart (not shown) and identifies Engine Technology as a confounder. Furthermore, he also discovers a latent factor Material Choice which exerts a reducing effect on Car Weight. All of these interactions taken together give rise to the final causal graph shown in Fig. 9B. The edges for the two newly added variables are visualized as dotted lines to convey that their weights have not been calculated from data yet - all Oscar has are the strengths indicated by GPT-4. Inspecting the triad Displacement, Horsepower, and Engine Technology reveals that Horsepower is, in fact, a causal collider. It is influenced by both Displacement and Engine Technology, or dominated by one of the two. While in older cars displacement was the dominant factor determining horse- power, modern engine technology has altered this role. The negative causal effect of Engine Technology on Displacement further suggests that as engine technology advances, the need for displacement to elevate horsepower diminishes. In other words, higher values of engine technology have become more influential in determining horsepower than displacement alone. This interpretation aligns with the idea that modern engines, with advancements in technology, can achieve higher horse- power despite low displacement. In essence, by adding Engine Technology, Oscar has modernized the original causal network derived from the antiquated dataset of 1980s cars.\nAdding mediators. In the updated causal graph Oscar notices the antagonistic relationship of Car Weight and Horse- power which affect Time to 60 MPH in opposite ways. Having resolved the need for high Displacement also reduces the need for a high number of Cylinders which would cause high Car Weight. However, there may be other valid causes for high Car Weight not represented in the graph. Consequently, Oscar chooses to investigate potential ways to mitigate these factors. The Causal Debate Chart in Fig. 10 (left) illustrates that heavy cars take more time to reach 60 MPH (2nd bar pair) than lighter cars (5th bar pair), i.e., they have poor acceleration. These are the hard facts represented by the two long bars. But Oscar wants to innovate and is looking for a car that can be heavy yet quick off the mark, the condition represented by the 3rd pair of bars. Clicking on the left bar brings up the corresponding Causal Relation Environment Chart, Fig. 10 (right). It suggests that increasing the engine's twisting power, or torque, can improve acceleration and mitigate the effect of"}, {"title": "B. Causal Strategizing for Opioid Mortality Prevention", "content": "Here, we join Lena, an epidemiologist, on her mission to discover preventive measures against the widespread opioid epidemic afflicting numerous counties in the United States. She starts out with 9 socioeconomic variables that she feels are related to opioid mortality plus data on opioid mortality itself (the aforementioned opioid death dataset).\nInitial setup. As a first step, Lena utilizes her expertise to construct a foundational causal graph based on the available data, see Fig. 13a. However, she finds herself dissatisfied with the connection between Education Index and Opioid Dispensing Rate. Intuitively, she believes that enhanced edu- cation would raise awareness of the adverse effects associated with opioid dispensing. Yet, the green edge she initially drew suggests the opposite. To investigate, she opts to reassess this edge using the Causal Debate Chart shown in Fig. 14 (left).\nExploring doubts. Examining this chart, Lena comes to re- alize that while there is a slight inclination towards the current causality, none of the bars exhibit highly significant strength, indicating a raised likelihood of a confounder. She clicks on the left bar of the general (grey-colored) relation which brings up its Causal Relation Environment Chart. Indeed, two con- founders are suggested, with Socioeconomic Status being the strongest. The justification states that \"socioeconomic status can influence level of education and also lead to better access to healthcare facilities where more prescription opioids are dispensed\". This confirms Lena's initial apprehension about the direct edge, with the justification pointing to an important mediator between Socioeconomic Status and Opioid Dispens- ing Rate: Prescription. Thus, a mitigating policy intervention would be to tighten opioid prescription regulations.\nAddressing the target effect. Next, Lena sets out to identify actionable measures to help reduce opioid fatalities. She directs her attention to the edge from Primary Care Physician Rate to Opioid Dispensing Rate. Generally, the relationship is positive since opioid dispensing typically involves doctors\u00b9"}, {"title": "V. USER STUDY", "content": "We conducted a two-fold user study: (1) Obtaining feedback from domain experts to assess the practical applicability and logical coherence of our proposed framework; (2) Evaluating the usability and efficacy of our framework by assigning tasks to non-expert users."}, {"title": "A. Datasets", "content": "In these user studies, we utilized two real-world datasets related to public health. Each data point corresponds to a distinct county in the United States for the year 2019. The expert assessment utilized the opioid death dataset described in Fig. IV. The non-expert user study employed the Life Expectancy dataset detailed below.\nThe Life Expectancy dataset comprises 8 key variables sourced from the County Health Rankings & Roadmaps Database [39]: firearm fatality rate, violent crime rate, av- erage grade performance, high school graduation rate, food environment index, percent fair or poor health, primary care physician rate and debt income ratio for each of more than 3,000 US counties, All of these variables are recognized to affect demographic life expectancy either directly or indirectly."}, {"title": "B. Expert Assessment", "content": "We invited three domain experts, affiliated with our uni- versity, to evaluate CausalChat. These experts specialize in health policy (P1), exposure science (P2), and environmental epidemiology (P3). All possess solid background in causal inference and have a shared interest in utilizing quantitative approaches in public health modeling.\nEach of the three sessions took place via Zoom. We first introduced our interface, emphasizing the functions and signif- icance of each visual component. We ensured that the experts were familiar with how to navigate the system. Subsequently, they were tasked with refining a causal graph containing two unresolved edges and one misdirected edge, with the goal to achieve a valid causal graph, and eventually ideating additional factors. All were able to achieve these tasks. Throughout their interaction with the system, we encouraged them to articulate their thought process. Finally, we gathered their feedback on CausalChat, specifically soliciting suggestions for potential enhancements. In the following we grouped the verbal session outcomes into specific themes.\nOverall assessment. All three participants unanimously praised CausalChat as an exceptional ideation tool, noting its effectiveness in helping non-experts quickly grasp the most prominent causal relations between variables in a field potentially unfamiliar to them. Specifically, P2 highlighted the efficiency of the tool, noting, \"CausalChat enlightens non- expert users in a productive fashion. Users don't have to go through an exhaustive literature research process to obtain a fundamental understanding of a new field.\"\nValidation. P1 expressed that her expertise led her to anticipate the presence of Withdrawal Treatment as a mediator between opioid dispensing and opioid-related deaths. This anticipation was validated when she discovered Withdrawal Treatment listed as a significant mediator within the Causal Relationship Environment chart. She added that once identified conceptually, the proposed mediators, confounders, and other new variables can be statistically tested, enhancing confidence and adding accuracy beyond GPT-4's strength assessments.\nMaking access to domain science easier and more streamlined. The potential of our tool to provide expedited access to domain knowledge became evident when the experts confirmed CausalChat's adeptness in identifying confounders and mediators within an existing causal graph. P1 remarked that \"compared to the traditional method of studying and including all possible confounders, which demands relentless and tedious literature review, CausalChat offers a far more efficient solution by automatically and visually presenting all potential confounders. This feature significantly enhances accessibility to science\".\nUse as a research tool. P3 perceived CausalChat as an actual research planner. She said: \"I would use the framework for planning my research. With all these essential variables and their information visually represented, I can have a clear goal of what data I need to collect, what research papers are useful, and what academic areas I can potentially contribute to.\" She further noted that the synergy between the Causal Debate Chart and the Causal Relation Environment Chart not only highlighted well-studied areas but also suggested intervention strategies (such as mediators) and ways to mit- igate unrealistic relationships. Relatedly, P1 remarked that the framework effectively prompts domain experts to consider essential components that may be overlooked but are crucial for enhancing the outcome variable.\nPreferred alternative to pure data-driven causal analy- sis. P2, an expert in experimental-based causality, began his session by voicing strong skepticism with regards to existing methods for automated causal analysis. He contended that ob- servational data could only uncover correlations, not causation. According to his view external information is indispensable for directing causal edges, and even for forming them in the first place. He favors the alternative approach supported by CausalChat \u2013 taken by Lena in our second case study \u2013 which first constructs and refines a causal graph by leveraging the knowledge of GPT-4 and then estimates the causal effect of each edge using data.\nSatisfying the need for personalized causal models. P2 emphasized that socioeconomic variables are predominantly cross-sectional and can engender feedback loops, resulting in bidirectional edges. This poses a challenge to the assumption of the causal graph being a DAG, underscoring the importance of further expanding our model tree panel. In each distinct causal model, there exists a distinct data generating process where variables have a clear upstream and downstream direc- tion. An edge pointing from A to B in one model can point reversely in another. These two models will share a parent"}, {"title": "C. Non-Expert User Study"}]}