{"title": "Dialogue is Better Than Monologue: Instructing Medical LLMs via Strategical Conversations", "authors": ["Zijie Liu", "Xinyu Zhao", "Jie Peng", "Zhuangdi Zhu", "Qingyu Chen", "Xia Hu", "Tianlong Chen"], "abstract": "Current medical AI systems often fail to replicate real-world clinical reasoning, as they are predominantly trained and evaluated on static text and question-answer tasks. These tuning methods and benchmarks overlook critical aspects like evidence-based reasoning and handling distracting information. To bridge this gap, we introduce a novel benchmark that simulates real-world diagnostic scenarios, integrating noise and difficulty levels aligned with USMLE standards. Moreover, we explore dialogue-based fine-tuning, which transforms static datasets into conversational formats to better capture iterative reasoning processes. Experiments show that dialogue-tuned models outperform traditional methods, with improvements of 9.64% in multi-round reasoning scenarios and 6.18% in accuracy in a noisy environment. Our findings highlight dialogue tuning as a promising approach for advancing clinically aligned and robust medical AI systems.", "sections": [{"title": "1. Introduction", "content": "Large language models (LLMs) have achieved significant advancements in the medical domain, especially in disease analysis and diagnostic assessment (Singhal et al., 2023; Kwon et al., 2024; Li et al., 2023b;a; Chen et al., 2023; Peng et al., 2023), while evaluating these medical LLMs remains a significant challenge. Many medical LLM benchmarks are derived from medical examinations or research articles, framed as multi-choice question-answering and long-answer reasoning tasks, (Jin et al., 2021; Pal et al., 2022). Although these benchmarks have provided valuable insight into the capabilities of medical LLMs which enhanced AI performance, they primarily work in articulated environments that diverge from the complexities of actual diagnostic procedures (Chen et al., 2024a; Yao et al., 2024). The highly structured nature of such tasks fails to reflect the reasoning required in clinical scenarios, where systematic diagnostic investigation and step-by-step analysis are essential: (i) First, in real-world clinical scenarios, diagnostic reasoning progresses from patient history review, through analysis of clinical findings and test results, to a comprehensive diagnostic conclusion. This requires diagnostic hypotheses to be refined through a systematic evaluation of evidence. Crucially, this process involves not only identifying relevant evidence but also determining the next logical steps in gathering information. For instance, when presented with a set of clinical findings or test results, a doctor must identify what additional symptoms to inquire about, as well as what further examinations or tests the patient should undergo. This iterative, step-by-step approach conflicts with the single-round judgments or explanations emphasized by current multi-choice tasks. (ii) Furthermore, real-world diagnostics often involve navigating ambiguous and noisy information, extracting relevant evidence, and synthesizing it to arrive at accurate conclusions. Existing benchmarks fail to capture these critical aspects, instead focusing on straightforward pattern recognition or information retrieval. Based on these gaps, it is worth exploring alternative benchmark formats beyond standard question-answering tasks - formats that can better reflect systematic evidence-based analysis and step-by-step clinical reasoning.\nTo address these gaps, we first analyze existing benchmarks and observe that clinical information typically follows a natural sequence: from the patient background through examination findings to the final diagnosis. the sequential order mirrors real-world diagnostic processes. Based on these insights, we reformulate traditional question-answering tasks into evidence retrieval and ranking. In this new benchmark, models are required to identify and integrate relevant evidence to support their answers under two tasks: the one-round ranking task that requires the model to rank all evidence once, and the multi-round ranking employing an iterative approach that retrieves and ranks evidence over several steps, gradually constructing the necessary context for decision-making. This reformulation reflects the reasoning process that doctors use when combining and synthe-"}, {"title": "2. Related Work", "content": "Medical Large Language Models. LLMs have demonstrated considerable progress in the medical field, (Singhal et al., 2023; Chen et al., 2023; Wu et al., 2023; Zhang et al., 2024b; Gema et al., 2024; Han et al., 2023; Xie et al., 2024), achieving state-of-the-art performance in medical question-answering and summarization task. Medical LLMs are typically fine-tuned on medical examinations, scientific literature, clinical guidelines, and clinical notes from EHRs to enable them to excel in a wide range of medical text analysis and real-world clinical tasks. Moreover, bio-focused large language models are being tailored specifically to address the unique challenges of biomedical research and healthcare applications (Luo et al., 2022; Bannur et al., 2023). For instance, BioBERT (Lee et al., 2020) and PubMedBERT (Gu et al., 2021) are foundational models pre-trained on PubMed articles, enabling them to excel in natural language understanding tasks such as named entity recognition (NER) and relation extraction in the biomedical domain.\nConversation Datasets in the Medical Field. Previously,"}, {"title": "3. Methodology", "content": "In this section, we outline the steps taken to develop and evaluate our benchmark for medical reasoning. First, we introduce the concepts of monologue and dialogue formats. Next, we explain our proposed dialogue-tuning approach, which leverages this dialogue format to improve the model's reasoning capabilities. Finally, we describe how existing multi-choice and article-based tasks were reformulated into a dialogue format using a large language model (LLM). This step was crucial to simulate real-world doctor-patient interactions and emphasize logical reasoning."}, {"title": "3.1. Preliminary", "content": "Monologue Tuning. Monologue-style datasets primarily involve static, non-interactive formats such as multi-choice question answering and article-based tasks. Medical models often rely on biomedical literature for pretraining or fine-tuning, leveraging either the entire article or just the abstract. Moreover, fine-tuning on multi-choice question-"}, {"title": "3.2. Why We Need Dialogue Tuning?", "content": "Dialogue tuning is proposed as a more effective approach for capturing logical relationships, as the dialogue format inherently mirrors the reasoning process found in human doctor-patient interactions.\nInteractive Nature of Dialogue. The dialogue format enables an interactive and question-driven reasoning process that mirrors how doctors approach diagnostic reasoning in clinical practice. In real-world settings, doctors iteratively ask questions to gather relevant evidences, with each exchange adding context and reducing uncertainty. Thus, we formulate this process in terms of entropy reduction via useful evidence increasing.\nLet Et denote the Evidence set accumulated up to step t. With each new piece of information enew collected, the total information is updated as:\nEt+1 = Et + enew = Et Uenew, with Et \u2229 enew = \u00d8.\nIn this paper, we apply conditional entropy to describe the uncertainty about the final diagnosis D at step t as Ht = H(D | Et). Then we can define the reduction in entropy after incorporating the new information.\n$\\Delta$Ht = Ht - Ht+1\n= H(D | Et) \u2013 H(D | Et, enew)\n= [H(D,enew | Et) \u2013 H(enew | D,Et)]\n[H(D, enew | Et) - H(enew | Et)]\n= H(enew | Et) \u2013 H(enew | D,Et)\n= I(D; enew | Et)\nThe conditional mutual information I(D; enew|Et) is always non-negative, which reflects that $\\Delta$Ht always be positive, indicating that uncertainty decreases as new information is integrated. It reflects how new information systematically reduces diagnostic uncertainty, mirroring the way doctors refine their understanding in clinical settings. The step-by-step approach naturally organizes information into logical and systematic structures, clarifying the reasoning path. By training models in this format, we align their reasoning processes with real-world practices, promoting a structured understanding of how evidence supports conclusions.\nEmpirical Support for Dialogue Tuning. Our experiments demonstrate that dialogue tuning significantly outperforms multi-choice and article-based tuning in tasks designed with our benchmark. Key findings include: Dialogue-tuned models exhibit higher accuracy in reasoning tasks, particularly those requiring multi-step integration of evidence. They achieve better generalization to unseen cases, suggesting that the dialogue format enhances their capacity to reason beyond the training data. Dialogue-tuned models remain robust even when tackling challenging tasks or integrating noisy inputs, maintaining performance where other approaches falter."}, {"title": "3.3. Task Reformulation to Enable Dialogue Tuning", "content": "To better simulate real-world doctor-patient interactions, we reformulated existing multi-choice tasks and article-based"}, {"title": "4. Our Benchmark: Muddy Maze", "content": "To evaluate the reasoning abilities of models under realistic medical scenarios, we constructed a benchmark called Muddy Maze, designed to challenge models on logical reasoning and evidence-based decision-making. The benchmark captures the complexities of real-world diagnostic reasoning by doctors and includes multiple layers of difficulty and distracting information."}, {"title": "4.1. Dataset Sources", "content": "Muddy Maze is built using the following datasets, carefully chosen for their relevance and alignment with real-world medical tasks: \u25cf MedQA: Includes questions from USMLE Step 1, Step 2, and Step 3 exams. Step 1 questions requiring foundational medical knowledge and straightforward reasoning. Step 2 and Step 3 questions focusing on clinical reasoning and decision-making. \u25cf MedBullets Step 2/3: Contains high-quality questions for USMLE Step 2 and Step 3 preparation. \u25cf JAMA Challenge: Includes complex, real-"}, {"title": "4.2. Difficulty Levels Aligned with USMLE", "content": "The benchmark is divided into three difficulty levels to reflect the progressive challenges of medical training:\n\u2022 Basic Level. Based on MedQA Step 1 questions. Focuses on foundational medical knowledge with straightforward evidence identification.\n\u2022 Advanced Level. Combines MedQA Step 2 & Step 3 and MedBullets questions. Requires the integration of multiple pieces of evidence and reasoning through complex information.\n\u2022 Challenge Level. Derived from JAMA Challenge cases. Simulates challenging real-world clinical cases from a range of medical domains and is more unusual."}, {"title": "4.3. Noise Levels Aligned with the Real World", "content": "The benchmark can randomly select irrelevant evidence from the dataset and add it to the current evidence set to challenge the LLM's judgment. If the noise level is set to 0,"}, {"title": "4.4. Details of One-Round and Multi-Round Tasks", "content": "Traditional question-answering tasks were reformulated into evidence ranking tasks to emphasize reasoning:\n\u2022 One-Round Evidence Ranking. In this task, the model is provided with the background information, a question-answer pair, and a pool of evidence sentences. The objective is to determine the correct sequential order in which evidence should be added to the background information to logically support the transition from the question to the answer. The model must consider the dependencies between evidence sentences and infer how each piece contributes to building a complete reasoning chain in a single step. This task tests the model's ability to reason holistically and deduce the necessary evidence order efficiently.\n\u2022 Multi-Round Evidence Ranking. In this task, the"}, {"title": "4.5. Evaluation Metric", "content": "Multi-Hop Accuracy. metric evaluates the model's ability to both identify the correct evidence sentences and place them in the correct sequential order to support the reasoning chain. The metric is defined as:\nMulti-Hop Accuracy = $\\frac{\\sum_{i=1}^{N} \u2161(e_i = \\hat{e_i} / p_i = \\hat{p_i})}{N}$,\nwhere, N represents the total number of evidence sentences in the reasoning chain. Each ei is the i-th evidence sentence in the ground-truth reasoning chain, while \u00ea\u00bf is the corresponding evidence sentence selected by the model. Similarly, pi denotes the position of the i-th evidence sentence in the ground truth, and \u00eei is the position assigned by the model. The indicator function II returns 1 if both ei = \u00eai and pi = pi, and 0 otherwise.\nThis formula calculates the proportion of evidence sentences where the model correctly identifies both the content and its position in the reasoning chain, providing a comprehensive measure of reasoning accuracy.\nSingle-Wise Accuracy. metric evaluates the model's ability to predict correct pairs of consecutive evidence sentences in the reasoning chain. This metric measures how well the model captures the sequential relationship between evidence elements, ensuring that not only the individual evidence sentences but also their relationship in the chain is accurate. The metric is defined as:\nSingle-Wise Accuracy = $\\frac{1}{N-1} \\sum_{i=1}^{N-1} II((e_i, e_{i+1}) = (\\hat{e_i}, \\hat{e_{i+1}}) \\lor (e_i, e_{i+1}) = (\\hat{e_{i+1}}, \\hat{e_i}))$,\nwhere N represents the total number of evidence sentences in the reasoning chain. The pair (ei, ei+1) denotes the ground-truth pair of consecutive evidence sentences in the reasoning chain, while (\u00eai, \u00eai+1) represents the corresponding pair predicted by the model. The accuracy is calculated using an indicator function I, which returns 1 if (ei, ei+1) = (\u00eai, \u00eai+1) and 0 otherwise.\nThis metric highlights the model's ability to maintain the correct sequential relationships between evidence elements, ensuring consistency in multi-round and one-round reasoning tasks. However, the metric is relatively loose, as it accommodates bidirectional consistency to account for reasoning paths that may flow in either direction."}, {"title": "5. Experiment", "content": "In this section, we want to validate the effectiveness of our proposed dialogue tuning strategy. We would like to answer the following research questions:"}, {"title": "5.1. Experimental Setup", "content": "Dataset. Our benchmark integrates the MedQA-USMLE Test Set, Medbullets, and JAMA Clinical Challenge. Our fine-tuning is based on the MedQA-USMLE Train Set, which have around 10.2k question-asnwer pairs and approximately 12k PubMed articles. All dataset information shows in Figure 4.\nTestbed. We fine-tuned the Llama 3.2-3B and Qwen 2.5-3B models using 2 NVIDIA RTX 6000 GPUs, each with 49GB of memory. For Llama 3.1-8B model, we finetune them on 4 H100 80G. We running our benchmark also on the NVIDIA RTX 6000 49GB GPUs."}, {"title": "5.2. Experimental Details", "content": "Q1: What is the effect of dialogue tuning on reasoning compared to multi-choice and article-based tuning at different difficulty levels? A1: Improves reasoning across basic, advanced, and challenge levels.\nThe results (Table 1, Table 2) demonstrate that dialogue tuning significantly enhances the reasoning performance of"}, {"title": "6. Conclusion", "content": "In this work, we introduce a novel benchmark, Muddy Maze, designed to evaluate the logical reasoning and evidence-based decision-making capabilities of language models in realistic medical scenarios. Moreover, we demonstrate that dialogue tuning significantly improves model performance compared to multi-choice and article-based tuning, through extensive experiments. Models trained in a dialogue format better capture logical relationships and exhibit robustness even in more challenged multi-step reasoning. Our evaluation metric quantifies these improvements by measuring both evidence identification and sequential ranking accuracy.\nThis work highlights the importance of a dynamical approach to advancing reasoning in medical AI systems. Dialogue tuning aligns training with the step-by-step cognitive processes required for diagnostic decision-making, providing a framework for developing more understandable and reliable models.\nFuture Work. Future directions include expanding the benchmark to encompass a broader range of medical domains and modalities. We will also carefully check to see if the dialogue construction process will include the model's hallucination and see if the model's ability to reason remains consistent when the amount of noisy information rises sharply across different models."}, {"title": "Impact Statement", "content": "This work seeks to enhance medical AI systems' capacity to reason through clinical scenarios in ways that align more closely with real-world medical practice. While the advancements proposed in this study offer significant potential to improve healthcare delivery, a comprehensive discussion of their broader implications is essential.\nBenefits and Opportunities:\n\u2022 Enhanced medical AI systems could provide critical support for clinical decision-making, particularly in resource-constrained environments where specialist expertise is limited.\n\u2022 The proposed dialogue-based approach has the potential to create more interpretable and transparent medical AI systems, enabling clinicians to gain better insights into the underlying reasoning processes of LLMs..\n\u2022 The benchmark introduced in this work could aid in identifying potential failures of medical AI systems, facilitating safer deployment in clinical environments.\nPotential Risks and Mitigations:\n\u2022 Despite improvements in reasoning capabilities, the model should not be employed for autonomous medical decision-making without human oversight to prevent potential errors.\n\u2022 The reliance on synthetic medical conversations for training may not fully encapsulate the complexity and nuances of real doctor-patient interactions.\n\u2022 Strict adherence to patient privacy and medical data protection regulations is essential to ensure ethical and compliant system deployment.\nFuture Considerations:\n\u2022 Further evaluation of these systems across diverse patient demographics and medical conditions is necessary to ensure equitable performance.\n\u2022 Successful implementation in clinical settings will require close collaboration with healthcare providers and rigorous adherence to medical validation protocols.\n\u2022 Continuous monitoring and updates will be critical to maintaining high performance standards and ensuring adaptability to evolving medical knowledge.\nWe believe that a transparent discussion of these impacts is vital for the responsible development and deployment"}, {"title": "A. Dialogue Generation", "content": "A.1. Prompt Templates\nMulti-choice QA to Dialogue: Please convert the following paragraph into a doctor-patient dialogue. Ensure that all the information provided, including personal details, symptoms, examination findings, diagnosis, and treatment, is included. Most important is the final answer, \"{{Answer}}\", which must be included in the dialogue without any changes. Use natural conversational language to connect the details, but do not introduce any new information. The dialogue should not be too redundant:\n{{Document}}\nArticle to Dialogue: Please convert the following paragraph into a doctor-patient dialogue. Ensure that all the information provided, like personal details, symptoms, examination findings, diagnosis, and treatment, is included. Use natural conversational language to connect the details, but do not introduce any new information. The dialogue should not be too redundant:\n{{Document}}"}, {"title": "A.2. Task Reformulation Examples", "content": "This example demonstrates how we use the above prompt to guide LLaMA 3.1-8B in transitioning from multiple-choice question answering to a doctor-patient dialogue format.\nA 23-year-old pregnant woman at 22 weeks gestation presents with burning upon urination. She states it started 1 day ago and has been worsening despite drinking more water and taking cranberry extract. She otherwise feels well and is followed by a doctor for her pregnancy. Her temperature is 97.7\u00b0F (36.5\u00b0C), blood pressure is 122/77 mmHg, pulse is 80/min, respirations are 19/min, and oxygen saturation is 98% on room air. Physical exam is notable for an absence of costovertebral angle tenderness and a gravid uterus. Which of the following is the best treatment for this patient?\nDoctor: Hi there, how are you feeling today?\nPatient: I'm feeling okay, but I've been having some issues with burning when I urinate. It started a day ago and has been getting worse.\nDoctor: I see. Have you tried anything to help with that?\nPatient: Yeah, I've been drinking more water and taking cranberry extract, but it hasn't seemed to make a difference.\nDoctor: Okay. Can you tell me a little bit about your pregnancy? How far along are you?\nPatient: I'm 22 weeks pregnant.\nDoctor: Alright. Let's take a look at your vitals. Your temperature is 97.7, blood pressure is 122 over 77, pulse is 80, respirations are 19, and oxygen saturation is 98% on room air.\nPatient: That all sounds normal, I think.\nDoctor: Yes, it does. Now, let's take a look at you. I don't see any costovertebral angle tenderness, and your gravid uterus looks normal.\nPatient: What does that mean?\nDoctor: It just means that there's no sign of infection in your kidneys, and your pregnancy is progressing normally.\nPatient: Okay. So, what do you think is going on with the burning sensation when I urinate?\nDoctor: Based on what you've told me and what I've seen, I think you might have a urinary tract infection. We'll need to run some tests to confirm that, but in the meantime, I'd like to prescribe you some medication to help with the symptoms.\nPatient: What kind of medication?\nDoctor: I'd like to prescribe you Nitrofurantoin. It's a common antibiotic that's safe to take during pregnancy and should help clear up the infection.\nPatient: Okay, that sounds good. I'll take it.\nDoctor: Great. I'll write you a prescription and you can pick it up at the pharmacy. In the meantime, make sure to drink plenty of water and follow up with me in a few days to make sure the infection is clearing up. Sound good?\nPatient: Yeah, that sounds good"}, {"title": "A.3. Limitation", "content": "The dialogue generation process, which relies solely on LLaMA 3.1-8B, may introduce several significant biases that warrant careful examination. First, while the synthetic nature of these dialogues preserves medical accuracy, it may fail to capture the full spectrum of communication styles, cultural nuances, and socioeconomic factors that shape real doctor-patient interactions. Additionally, relying on a single large language model for dialogue generation could lead to model-specific biases, particularly in how it structures conversations and prioritizes certain types of medical information. Without explicit controls or considerations for diversity in the dialogue generation process, the resulting training data might produce medical LLMs that perform inconsistently across different patient populations and communication contexts."}, {"title": "B. Benchmark: Muddy Maze", "content": "B.1. Dataset Release\nAll benchmark datasets can be found in Muddy Maze."}, {"title": "B.2. Prompt Template", "content": "Here is the background information: \"{{prerequisit}}\"\nQuestion: {{question}}\nAnswer: {{answer}}\nBelow are several evidence sentences. Identify the {{groundtruth zoo length}}\nsentences that, if added to the background information, would support inferring\nthe answer based on the given question-answer pair. Please choose the sentence in\nlogical order!\n{{tagged maze}}\nProvide only the indices of the relevant sentences in brackets formatted like this:\n[ ], no more than {{groundtruth zoo length}} sentences.\nANSWER:\nHere is the background information: \"{{prerequisit}}\"\nQuestion: {{question}}\nAnswer: {{answer}}\nBelow are several evidence sentences. Based on the given question-answer pair,\nplease select which sentence should be added to the background information to\nsupport inference of the answer.\n{{tagged maze}}\nYou have {{groundtruth zoo length}} attempts in total to make a selection; this is\nyour {{i_th}} attempt. Please choose the sentence in logical order!\nProvide only the indices of the relevant sentences in brackets formatted like this:\n[]\nANSWER:"}, {"title": "B.3. Task Reformulation Examples", "content": "This is an example of transitioning from the traditional question-answering task to our benchmark. The results are generated using LLaMA 3.1-8B at the basic task level, with a noise level of 0, in a one-round setting."}]}