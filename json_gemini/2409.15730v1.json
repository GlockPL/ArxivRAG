{"title": "Learning Multiple Probabilistic Decisions from Latent World Model in Autonomous Driving", "authors": ["Lingyu Xiao", "Jiang-Jiang Liu", "Sen Yang", "Xiaofan Li", "Xiaoqing Ye", "Wankou Yang", "Jingdong Wang"], "abstract": "The autoregressive world model exhibits robust generalization capabilities in vectorized scene understanding but encounters difficulties in deriving actions due to insufficient uncertainty modeling and self-delusion. In this paper, we explore the feasibility of deriving decisions from an autoregressive world model by addressing these challenges through the formulation of multiple probabilistic hypotheses. We propose LatentDriver, a framework models the environment's next states and the ego vehicle's possible actions as a mixture distribution, from which a deterministic control signal is then derived. By incorporating mixture modeling, the stochastic nature of decision-making is captured. Additionally, the self-delusion problem is mitigated by providing intermediate actions sampled from a distribution to the world model. Experimental results on the recently released close-loop benchmark Waymax demonstrate that LatentDriver surpasses state-of-the-art reinforcement learning and imitation learning methods, achieving expert-level performance. The code and models will be made available at https://github.com/Sephirex-X/LatentDriver.", "sections": [{"title": "I. INTRODUCTION", "content": "Motion planning is a fundamental task in autonomous driving systems. Recently, with the introduction of several real-world data-driven benchmarks [1], [2], learning based planning methods have garnered significant attention from both industry and academia. However, navigating through various unfamiliar driving scenarios based on the vehicle's current observations remains extremely challenging. This difficulty arises from the complexity involved in understanding interactions among traffic participants and the unstructured nature of road environments. Most critically, it involves deriving appropriate actions from these observations.\nTo understand the environment, early methods [3], [4] typically modeled current dynamics using frameworks like PointNet [5] or BERT [6]. Recent works [2], [7]\u2013[9] have designed encoders based on successful implementations from motion forecasting [10], [11]. Despite their effectiveness, performance in out-of-distribution scenes remains suboptimal. In response, [12] developed an autoregressive world model with strong generalization capabilities for environmental understanding. As shown in Fig. 1(a), this model integrates with various planners, functioning as an interactive simulator that generates scores for each planner and selects the best one. However, performance is limited by the world model's imperfections and insufficient sparse signals for the planner. Additionally, the high training cost for a sufficiently accurate world model poses challenges. This raises the question: How can we leverage the world model's knowledge to aid planner learning at minimal cost?\nOne potential solution is to implicitly transfer knowledge from the world model to the planner and optimize them jointly [13]\u2013[15], as demonstrated in Fig. 1(b). However, these approaches fall short of fully utilizing the autoregressive model's potential. The first issue is the incomplete consideration of uncertainty, particularly regarding the ego vehicle's actions when interacting with the environment. Driving scenes are inherently stochastic, and decision-making should not be considered a single-modality problem. Multiple valid options may exist, with each option representing a different mode of the distribution. Another challenge is self-delusion. Learning in the autoregressive world model involves sequence prediction [16], with agent interactions framed as cascading conditional distributions of actions. The planner, however, must respond based on current observations rather than historical actions, exacerbating the 'copycat' phenomenon [7], [17] in imitation learning planners.\nTo address the above issues, we propose LatentDriver with a key insight: it hypothesize the distribution for actions and states is multi-probabilistic, as well as their combination. As illustrated in Fig. 1(c), the interaction between the world model and the planner is bi-directional and fully stochastic, with the final action derived from their mixture distribution. Specifically, we introduce the Multiple Probabilistic Planner (MPP), which models the ego vehicle's actions as a stochastic process through a mixture Gaussian distribution [18], [19]. The MPP is structured in a multi-layer transformer style, with each layer refining action distributions based on the Latent World Model's (LWM) output. Therefore, it naturally capturing the stochastic actions of the ego vehicle. To mitigate the self-delusion problem during joint optimization, actions sampled from an intermediate layer of the MPP serve"}, {"title": "II. RELATED WORKS", "content": "A. World Model for Planning in Autonomous Driving\nThe world model aims to capture the transition dynamics of the environment using data. Two primary approaches exist for integrating world models into planning. The first treats the world model as an accurate simulator [12], [20], where actions are selected based on the lowest cost through simulation. This method focuses on maximizing world model's precision; for example, Drive-WM [20] employs a diffusion model [21] to generate action-conditional realistic images, while GUMP [12] uses a GPT-style autoregressive model to learn dynamics from vectorized inputs. The effectiveness of this approach hinges on the simulator's accuracy and the associated training cost is significant. The second approach [13]\u2013[15], [22], treats world model learning as an auxiliary task, generating actions directly from the image feature space. DriveDreamer [13] and MILE [14] utilize VAE [23] and LSTM [24] to model transition dynamics, while ADriver-I [22] employs LLaVA [25]. However, these methods have not been validated in complex real-world closed-loop evaluations and are unsuitable for vectorized observation spaces.\nB. Imitation Learning Based Planner\nImitation learning based planners can be categorized by their observation space into end-to-end [26]\u2013[28] and mid-to-end methods (also known as mid-to-mid). End-to-end approaches directly learn driving policies from raw sensor data, while mid-to-end methods rely on post-perception outputs. Our method falls into the latter category. Early works [29]\u2013[31] validated their algorithms in real-world settings due to the absence of standardized benchmarks. More recent works, including PlanT [4] and Carformer [32], conducted experiments in the CARLA simulator [33] using object-centered representations. With the introduction of the nuPlan benchmark [1], subsequent studies [3], [7]\u2013[9], [34], [35] have leveraged this dataset for comprehensive evaluations. These methods encode vectorized observations using scene encoders like PointNet [5] or BERT [6], as well as motion forecasting models [10], [11]. However, their effectiveness in out-of-distribution scenarios is suboptimal, e.g., in close-loop simulation."}, {"title": "III. PROBLEM FORMULATION", "content": "At time step t, the objective for mid-to-end autonomous driving is to estimate actions $a_t$ based on the current post-perception results $O_t$. The training objective is $P(a_t | O_t)$.\nIn autonomous driving, a world model is to predict future states based on actions and observations. The states can be observations (world model) or extracted latent features (latent world model), here we focus on the latter one. Specifically, let $s_t$ be latent features at time step t, $O_{1:t}$ and $a_{1:t}$ be observations and ground truth actions from time step 1 to t, the training objective for latent world model is $P(s_{t+1} | O_{1:t}, a_{1:t})$.\nWe can derive planning-oriented training objective with latent world model accordingly as\n$P(a_t, s_{t+1} | O_{1:t}, a_{1:t-1}) = P(a_t | s_{t+1}, O_{1:t}, \\hat{a}_{1:t-1})P(s_{t+1} | O_{1:t}, \\hat{a}_{1:t-1})$.\nWe omit $a_t$ here as it is the variant needed to estimate. The planner should react based on the current observation $O_t$, but including historical actions $\\hat{a}_{1:t-1}$ can lead to a 'copycat' phenomenon [7], [17].\nTo address these issues, we propose using the estimation of actions for the world model instead of the real one. By introducing an intermediate estimation term $\\tilde{a}_{1:t}$, the planning-oriented objective becomes\n$P(a_t, s_{t+1}, O_{1:t} | O_{1:t}) = P(a_t, s_{t+1} | O_{1:t}, \\tilde{a}_{1:t})P(\\tilde{a}_{1:t} | O_{1:t}) = P(a_t | s_{t+1}, O_{1:t}, \\tilde{a}_{1:t})P(\\tilde{a}_{1:t} | O_{1:t}) P(s_{t+1} | O_{1:t}, \\tilde{a}_{1:t})$.\nWe formulate the action using a Gaussian Mixture Model (GMM), denoted as $A$ and the latent state as a Gaussian distribution, denoted as $s$."}, {"title": "IV. METHODS", "content": "To realize Eqn. 2, we propose LatentDriver, as illustrated in Fig. 2. The raw observation is first vectorized and then fed to a scene encoder. The intermediate action distribution is generated by an intermediate layer of the MPP. Upon receiving the intermediate action, the LWM predicts the next latent state and formulates it as a distribution. The action distribution and latent state distribution are then combined through subsequent layers of the MPP, resulting in a mixture distribution from which the final control signal is derived.\nA. Input Representation and Context Encoding\nAt each time step, the raw observation is vectorized using object-centered representation as described in [4], resulting in $O_t \\in \\mathbb{R}^{N \\times 6}$, where N represents the number of segments. The attributes of each segment are consistent with the definitions provided in [4]. We only consider context that within the Field of View (FOV) under the ego vehicle coordinate system, with a certain width $w_f$ and height $h_f$. To extract this object-centered vectorized scene information, we utilize BERT [6] as our scene encoder. Given a sequence of observations $O_{1:t}$, the collection of class token and environment"}, {"title": "B. World Model for Latent Prediction", "content": "We formulate the latent state prediction as a next token prediction task using autoregressive model as shown in Fig. 2. We refer latent state as latent state token here. The Latent World Model (LWM) is designed to predict the next latent state token using action tokens and previous latent state tokens (both generated by an adapter that takes A and h).\n1) Adapter: The adapter generates two types of tokens: action tokens and latent state tokens. For the action tokens, given actions, each dimension of input actions is independently mapped into a D-dimensional space via a linear layer. Consequently, considering waypoints as the action space, the action tokens at time step t is represented as $a_t = (a_{t,x}, a_{t,y}, a_{t,yaw}) \\in \\mathbb{R}^{3 \\times D}$. It is important to note that during training, the input actions for LWM is estimated from the planner, whereas during inference, it is derived from the actual executed historical action sequence. For the latent state tokens, we employ several stacked standard transformer cross-attention blocks that use M learnable queries (M < N) to encode $h_{1:t} \\in \\mathbb{R}^{N \\times D}$ into latent space [36], followed by a distribution head $\\phi$ to parameterize it as a Gaussian distribution $\\hat{s}_t \\sim N((\\mu_{\\phi} \\circ CrossAtt)(h_t), (\\sigma_{\\phi} \\circ CrossAtt)(h_t))$, where $\\mu_{\\phi}$ and $\\sigma_{\\phi}$ are multilayer perceptrons (MLPs). Thus, the latent state token (or latent state) is sampled from the latent state's distribution, $s_t = sample(\\hat{s}_t), s_t \\in \\mathbb{R}^{M \\times D}$.\n2) Latent world model: At each time step, input tokens are ordered as 'action - observation'. The input to an autoregressive transformer [16] is expressed as $(a_1, s_1, . . ., a_t, s_t)$. We utilize a factorized spatio-temporal position embedding to encode token positions. The output latent state token $s_{t+1}$ is modeled as a Gaussian distribution via a distribution head $\\theta$. Therefore the next latent state is sampled from distribution using $s_{t+1} = sample(\\hat{s}_{t+1} \\sim N(\\mu_{\\theta}(S_{t+1}), \\sigma_{\\theta}(S_{t+1})))$."}, {"title": "C. Multiple Probabilistic Planner", "content": "The multiple probabilistic planner is composed of Multiple Probabilistic Action (MPA) blocks, as shown in Fig. 2. Each layer will generate a action distribution and the ultimate control action for ego vehicle is provided by the action distribution of the last layer.\n1) Multiple probabilistic modeling for decision-making: Considering waypoints as the action space, the ground truth actions are denoted as $\\hat{a} = [a_x, a_y, a_{yaw}] \\in \\mathbb{R}^3$. Prior researches [18], [19] used Gaussian Mixture Models (GMM) for trajectory prediction, focusing on x, y. Recognizing the spatial relationship between $a_{yaw}$ and its horizontal actions $a_x, a_y$, we propose modeling $a_x, a_y$ with a Gaussian mixture distribution and $a_{yaw}$ with a Laplace distribution.\nSpecifically, we predict the probability $p$ and parameters $(\\mu_x, \\mu_y, \\sigma_x, \\sigma_y, \\rho)$ for each Gaussian component follows $G_i = MLP(q^j)$, where $G_i \\in \\mathbb{R}^{K \\times 6}$ represents the parameters of K Gaussian components $N_{1:K}(\\mu_x, \\sigma_x; \\mu_y, \\sigma_y; \\rho)$ and their associated probabilities $P_{1:K}$. The query content from the j-th layer is denoted as $q^j \\in \\mathbb{R}^{K \\times D}$.\nFor $a_{yaw}$, we use another network predicts its parameter $\\mu_{yaw}, L^j = MLP(q^j)$, where $L \\in \\mathbb{R}^{K \\times 1}$ is the parameter for each Laplace component $Laplace_{1:K}(\\mu_{yaw}, 1)$. The final action distribution in the j-layer is $A^j = [G^j, L^j] \\in \\mathbb{R}^{K \\times 7}$, where $[,]$ denotes concatenation. The actions is sample from mixture distribution by using the expectation of the Gaussian component with the highest probability, as well as the Laplace component following\n$k^* = arg\\underset{k\\in(1,K)}{max} p$,\n$\\hat{a} = sample(A^j) = (E(N_{k^*}), E(Laplace_{k^*})).$\n2) MPA block: The detail design of MPA block is shown in Fig. 2. Given action distribution $A^{j-1}$ and query content features $q^{j-1}$ from the previous layer, the query content feature is updated via a self-attention module. The action distribution is embedded into a token through an MLP, serving as a query position embedding for the cross-attention module to extract features from the latent world model and scene encoder outputs. The query content features and query position embedding are concatenated following practices in [19], [37]. For j = 1, where $A^0$ is unavailable, we initialize $q^0 \\in \\mathbb{R}^{K \\times D}$ as a learnable token. $q^0$ is initialized with zero following previous practices [38]. For j < I where $s_{t+1}$ is lack, we omit it in the key and value."}, {"title": "D. Loss Function", "content": "1) World model: Given $O_{1:t}$ and $\\hat{a}_{1:t}$, the training objective for the world model is to minimize the Kullback-Leibler (KL) divergence between the adapter's output $\\hat{s}_{2:t}$ and the estimated latent state distribution $S_{2:t}$\n$\\mathcal{L}_{world} = - \\sum_{i=2}^{t} D_{KL}(\\hat{s}_i || s_i)$.\n2) Planner: The predicted distribution for the ego vehicle's actions is formulated as\n$\\sum_{k=1}^{K} p_k \\cdot N_k(a_x - \\mu_x, \\sigma_x; a_y - \\mu_y, \\sigma_y; \\rho) \\cdot Laplace_k(a_{yaw} - \\mu_{yaw}, 1)$.\nWe use negative log-likelihood loss to maximize the likelihood of the ego vehicle's ground truth action a. Thus, the loss function for action is formulated as\n$\\mathcal{L}_{gmm} = -log(p_s) - log N_s (a_x - \\mu_x, \\sigma_x; a_y - \\mu_y, \\sigma_y; \\rho) - log Laplace_s (a_{yaw} - \\mu_{yaw}, 1)$,\nwhere $a_x = a_x - \\mu_x, a_y = a_y - \\mu_y, a_{yaw} = a_{yaw} - \\mu_{yaw}$, and $N$ refers to the selected positive component for optimization, as do Laplace, and $p_s$. The selection procedure will be discussed in Section V. Since each MPA block contains a GMM action head, the final loss is the average of Eqn. 6 across all decoder layers. The final loss for LatentDriver is formulated as\n$\\mathcal{L} = 0.001 \\times \\mathcal{L}_{world} + \\mathcal{L}_{gmm}$."}, {"title": "V. EXPERIMENTS", "content": "A. Environment and Dataset\nAll experiments are conducted using the recently released simulator Waymax [2] driven by WOMD dataset (v1.1.0) [39]. Each scenario has a sequence length of 8 seconds, recorded at 10 Hz. Agents are controlled at a frequency of 10 Hz, with a maximum of 128 agents per scenario. The training set comprises 487,002 scenarios, while the validation set includes 44,096 scenarios. The simulation will not be terminated until it reaches the maximum length of 8 seconds.\nB. Scene Categorization\nWe found the original metrics in Waymax [2] can not reflects the long tail problem due the lack of scenario type information. Therefore, we categorize the driving scenarios into five representative types: Stationary, Straight, Turning Left, Turning Right, and U-turn. The categorization is based on the pattern of the ego vehicle's expert trajectory, which reflects its intention. Specifically, given the route's maximum curvature $\\kappa$ and the heading difference $\\delta$ between the starting and ending locations, we classify Straight, Turning, and U-turn scenarios as follows\n$Scene = \\begin{cases} Turning & if (0.03 < \\kappa < 0.18 \\text{ and } \\delta > 0.2) \\text{ or } (0.1 < \\kappa < 0.18), \\\\ U-turn & if \\kappa \\geq 0.18, \\\\ Straight & otherwise. \\end{cases}$"}, {"title": "C. Evaluation Metrics", "content": "All experiments are conducted under close-loop evaluation following [2]. The Off-road Rate (OR) and Collision Rate (CR) follows the same description in [2]. The definition for Progress Ratio (PR) is consistent while the maximum value in this paper is 100%, as the future derivable area is not disclosed in WOMD. Other metrics we used is detailed as:\n\u2022 Arrival Rate under $\\tau$ (AR@$\\tau$). It determines if the ego vehicle has traveled $\\tau$% of the route safely. For example, AR@50 refers to the safe travel of 50% of the route. To avoid the metric dominated by PR, we report AR@[95:75] to compare different algorithms' performance,\n$AR@[95:75] = (AR@95+AR@90+\u2026\u2026+AR@75)/5$.\n\u2022 Mean Arrival Rate (mAR). It represents the average AR across all categorized scenarios,\n$mAR = (AR_{Straight} + \u2026 + AR_{U-turn})/5$.\nD. Implementation Details\n1) Hyperparameters: The FOV dimensions are set to $w_f = 80$ and $h_f = 20m$. The time step t is set to 2. For the scene encoder, we use a randomly initialized BERT-mini model with feature dimension D of 256. In the MPP, we use I = 1, the 1st layer to estimate the intermediate distribution and J = 3 for the final action prediction. The mode K of GMM is set to 6 following previous practice in trajectory prediction [19]. The LWM's cross-attention module has 4 layers, 4 heads per layer, and employs M = 32 learnable queries. For the autoregressive model, we use a randomly initialized GPT-2 [16] model with 8 layers and 8 heads for each layer. The encoder is pretrained using our re-implementation of PlanT [4] for faster convergence. We set the batch size to 2,500 and use the Adam optimizer. The learning rate is initialized at 2 \u00d7 10-4 and decays to 0 over 10 epochs using a cosine scheduler."}, {"title": "E. Performance Comparison", "content": "The comparative analysis with other methods on Waymax [2] is presented in Table I. Non-ego agents are controlled by the Intelligent Driver Model (IDM) [40], as implemented in previous works [2], [41]. The asterisk (*) on PR indicates that this metric cannot be fairly compared under some methods due to the absence of drivable future under 'Route'. This discrepancy is highlighted in gray. The mean and standard deviation are reported under three random seeds.\nThe first two rows represent scenarios where the ego vehicle is stationary and controlled by actions from the driving log, respectively, indicating the lower and upper boundaries of the benchmark. The dagger (\u2020) on PlanT [4] denotes our re-implementation to fit the dynamic space in this benchmark. Focusing solely on OR and CR is insufficient, as the risk is proportional to travel distance. Therefore, mAR and AR are more indicative of overall performance. Our method achieves the best results in mAR, AR, and PR across all approaches, demonstrating superior performance. Notably, our method's AR is only 2% lower than the Logged Oracle, indicating expert-level performance. However, the Logged Oracle's mAR is 8% higher than ours, suggesting that certain scenarios remain challenging. EasyChauffeur ranks second after our model in mAR and AR, followed closely by PlanT. While PlanT performs best in OR and CR, it has one of the shortest travel distances except Waymax-BC. Both PlanT and EasyChauffeur have mAR below 80%, indicating weaker performance in some long-tail scenarios."}, {"title": "F. Ablation Studies", "content": "In ablation studies, all non-ego agents is controlled using actions from expert demonstration for its lower evaluation time than IDM."}, {"title": "G. Visualization Results", "content": "We visualize the behaviors of four different methods across four typical and distinct driving scenarios in Fig. 4. In the straight scenario, other three methods collide with a turning vehicle due to hesitation at the intersection. In contrast, LatentDriver successfully navigates through by making a decisive decision. A similar outcome is observed in the U-turn scenario, where both PlanT and the GMM baseline fail, while EasyChauffeur manages the turn but ultimately goes off the road. During an unprotected left turn, PlanT and the GMM baseline fail to avoid contact with an oncoming vehicle. Although both EasyChauffeur and LatentDriver handle this situation better, EasyChauffeur collides with a pedestrian at the beginning. For the right turn, while all methods exhibit trajectory fluctuations, only EasyChauffeur and LatentDriver complete the turn safely, with the other two going off-road."}, {"title": "VI. CONCLUSION", "content": "In this paper, we propose LatentDriver to address the challenges of inadequate uncertainty modeling and self-delusion in autoregressive world model-enhanced planners. Our approach represents the environment's next states and the ego vehicle's next actions as a mixture distribution, which forms the basis for selecting the planner's final action. The LWM module predicts the distribution of the environment's next state, while the MPP module refines the ego vehicle's action using LWM's output. LatentDriver outperforms current state-of-the-art methods on the Waymax benchmark and achieves expert-level performance."}]}