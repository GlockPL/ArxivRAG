{"title": "TRUSTWORTHINESS OF STOCHASTIC GRADIENT DESCENT IN DISTRIBUTED LEARNING", "authors": ["Hongyang Li", "Caesar Wu", "Said Mammar", "Mohammed Chadli", "Pascal Bouvry"], "abstract": "Distributed learning (DL) leverages multiple nodes to accelerate training, enabling the efficient optimization of large-scale models. Stochastic Gradient Descent (SGD), a key optimization algorithm, plays a central role in this process. However, communication bottlenecks often limit scalability and efficiency, leading to the increasing adoption of compressed SGD techniques to alleviate these challenges. Despite addressing communication overheads, compressed SGD introduces trustworthiness concerns, as gradient exchanges among nodes are vulnerable to attacks like gradient inversion (GradInv) and membership inference attacks (MIA). The trustworthiness of compressed SGD remains underexplored, leaving important questions about its reliability unanswered.\nIn this paper, we provide a trustworthiness evaluation of compressed versus uncompressed SGD. Specifically, we conduct empirical studies using GradInv attacks, revealing that compressed SGD demonstrates significantly higher resistance to privacy leakage compared to uncompressed SGD. Moreover, our findings suggest that MIA may not be a reliable metric for assessing privacy risks in machine learning.", "sections": [{"title": "1 Introduction", "content": "DL is the method used to accelerate the training of deep learning models by distributing training tasks to multiple computing nodes [1]. However, as data scales continue to grow, the complexity of model gradients increases accordingly, for example, consider the training of deep learning on ImageNet [2], which contains over 14 million labeled images and topics with approximately 22,000 categories, leading to constraints on communication efficiency [3].\nGradient compression aimed at reducing communication overhead during gradient transmission between multiple nodes which enhances system computational efficiency [4, 5, 6], thus this has emerged as an effective optimization technique in distributed learning, especially when training complex models to process large-scale data. Among various gradient compression techniques, PowerSGD [6] and Top-K SGD [7] have emerged as prominent solutions for their ability to substantially reduce communication costs while preserving scalability and model accuracy in large-scale distributed learning. These two algorithms are particularly suitable for our study as they represent fundamental approaches to gradient compression: PowerSGD uses low-rank approximation, while TopKSGD leverages sparsification through threshold quantization. Both techniques are widely recognized for their practical effectiveness, especially when combined, to varying extents, with advanced features such as error feedback, warm start, all-reduce, making them ideal candidates of compressed SGD for assessing privacy risks in distributed deep learning systems. Although distributed deep learning systems share model gradients instead of raw data, they are still vulnerable to indirect privacy attacks[8]. One notable attack is GradInv, where an adversary attempts to reconstruct original training data or extract sensitive information from the shared gradients during training. GradInv has been demonstrated as an effective method for exposing private data, making it a significant privacy concern[8, 9]. Given this context, gradient reconstruction attacks are considered a powerful technique for evaluating the robustness of distributed algorithms. Another relevant attack is \u039c\u0399\u0391 [10, 11, 12], where an attacker infers whether a specific data point was part of the model's original training.\nAlthough previous studies, such as [8, 13, 14], have examined sparsified gradient techniques as potential defenses against passive attacks, their focus has been limited to sparsification alone under single attackers GradInv, without a comprehensive analysis of a broader range of compressed SGD algorithms.\nTo address this gap, I conducted an in-depth comprehensive evaluation with two attackers for two representative methods: PowerSGD, which leverages low-rank approximation, and the widely-used Top-K SGD, which employs threshold-based sparsification. This study aims to offer a rigorous analysis alongside empirical validation, providing insights into the robustness of these techniques in distributed learning settings. This study provides an empirical evaluation of the trustworthiness of compressed SGD algorithms in distributed learning. We assess their performance across various datasets, compression levels, and compare them with uncompressed SGD. Our results show that compressed SGD offers stronger resistance to gradient inversion attacks, indicating better privacy protection. We also find that MIAs show low sensitivity to both compressed and uncompressed SGD, suggesting MIA may not be an ideal privacy metric for these algorithms."}, {"title": "2 Related Work and Problem Setup", "content": ""}, {"title": "2.1 Gradient Compression", "content": "Several strategies have been proposed to address communication bottleneck problem in distributed deep learning. Quantization-based methods have gained attention for their simplicity and effectiveness, includes Quantized SGD (QSGD)[15, 16] which employs lossy compression by quantizing gradients before transmission between nodes. signSGD where only exchange the sign of each gradient vector [17], although this effectiveness is evident, it can have limited generalization capacity. To advanced this, EF-SGD compensates with Error Feedback, enhancing the robustness of signSGD [18]; Another line of work focuses on sparsification, by transmitting only the most critical parts of the gradients, [19] introduced a sparse SGD technique that sets 99% of small gradient updates to zero, transmitting only the sparse matrix, and more compressed methods of sparsification proposed by [5, 20, 7]; Considering the scalability challenges and the high computational cost of compression methods in large-scale federated learning, low-rank compression method like PowerSGD stands out as an excellent compression SGD algorithm proposed by Vogels et all., 2019 [6]."}, {"title": "2.2 Top-K SGD and PowerSGD", "content": "Top-K SGD reduces the communication load by selecting only the top K gradient elements with the largest magnitudes, forming a sparse representation. This approach maintains a close approximation to the full gradient, allowing the transmission of the most important information while discarding less critical components. The addition of error feedback further enhances Top-K SGD by accumulating any missed gradient information in subsequent updates. This algorithm is widely used in large-scale distributed learning due to its simplicity and effectiveness in reducing the volume of data exchanged with the server[7].\nPowerSGD offers an alternative approach by employing a low-rank matrix approximation to compress the gradient. Instead of transmitting the full gradient vector, PowerSGD represents it with most largest low-rank matrices, thereby capturing the primary directions of the gradient while ignoring smaller components. This compression mechanism captures the most significant directions of the gradient while discarding smaller components, significantly reducing the communication cost. Vogels et all. demonstrated that PowerSGD surpasses standard SGD in performance on a multiple GPUs setup, even on high-speed networks. By significantly reducing communication time 54% for ResNet18 on CIFAR10, also lowered overall training time by 24%, all while maintaining model accuracy within 1% of uncompressed SGD [6]. These make PowerSGD a superior choice compared to other compression-based SGD algorithms."}, {"title": "2.3 Passive Attackers: GradInv and MIA", "content": "In distributed learning, gradient inversion is a widely used attack technique that exploits gradient updates to infer model inputs. Prior works, such as [21] and [22], demonstrated that sharing gradients in shallow neural networks can lead to significant leakage of original information. Building on this, [8, 23] extended these techniques, showing that even in deep networks, shared gradients can effectively recover original image data. These studies underscore the privacy risks posed by gradient sharing during deep learning model training. This work benchmarks at the state-of-the-art cosine similarity gradient inversion algorithm from [23] to evaluate the trustworthy performance of compressed SGD methods in DDL with ResNet-18 [24]. Moreover, we select three common MIA methods-based on prediction confidence, loss values, and cross-entropy to evaluate the privacy risks of machine learning models. The prediction-based MIA method analyzes the model's output probabilities, asserting that models typically have higher prediction confidence for training data samples [10], the loss-based MIA method calculates the loss value for each sample, assuming that training data samples generally have lower losses [12], and the cross-entropy-based MIA method assesses the cross-entropy between the model's predicted distribution and the true labels; lower cross-entropy values indicate a better fit of the model to the sample, making it more likely to belong to the training set [11]."}, {"title": "2.4 Distributed Learning Setting", "content": "We consider a distributed learning setup with N clients, each having a local dataset Dw for w = 1, 2, . . ., N. The objective is to train a global model with parameters 0 \u2208 Rd by minimizing the average loss:\n$\\min \\frac{1}{N} \\sum_{w=1}^N E_{(x,y)\\sim D_w} [L(x, y; \\theta)]$, (1)\nwhere L(x, y; 0) is the loss function computed for the model parameters 0, input x, and corresponding label y from the local dataset Dw [25, 26]."}, {"title": "2.5 GradInv Description", "content": "The objective of the GradInv[23] method is to minimize Lrecon(x), such that the generated gradient VoL(x, y; 0) aligns as closely as possible with the:\n$\\min L_{recon} (x) = 1 - \\frac{(\\nabla_\\theta L(x, y; \\theta), g)}{||\\nabla_\\theta L(x, y; \\theta) || ||g||}$ (2)\nwhere VoL(x, y; 0) is the gradient of the loss with respect to model parameters 0 for input x, and y is the corresponding label (in some cases the label may be assumed to be known), and g is the given gradient, the adversary's goal is to minimize Lrecon (x) such that it approaches 0."}, {"title": "3 Theoretical Analysis", "content": "Here we provide a theoretical proof that demonstrates why gradient inversion attacks in PowerSGD and Top-K SGD fail to converge."}, {"title": "3.1 Convergence Analysis", "content": "PowerSGD: approximates the gradient matrix M \u2208 Rn\u00d7m using a low-rank representation M = PQT, d = n \u00d7 m where d is the dimensionality of the original gradient vector. Using singular value decomposition (SVD), M can be decomposed as:\n$M = \\sum_{i=1}^{full} \\sigma_i u_i v_i^T$ (3)\nwhere \u03c3\u03b5 are the singular values, and ui \u2208 R and vi \u2208 Rm are the corresponding singular vectors. The low-rank approximation M retains only the top-r singular values:\n$\\hat{M} = \\sum_{i=1}^{r} \\sigma_i u_i v_i^T$ (4)\nThe Frobenius norm of the error, which quantifies this loss of information, is given by:\n$||M - \\hat{M}||_F = \\sqrt{\\sum_{i=r+1}^{full} \\sigma_i^2}$ (5)"}, {"title": "Implications:", "content": "Due to the divergence in cosine similarity caused by compression, even if the adversary minimizes Lrecon(x), the best possible alignment between the reconstructed gradient and the true gradient is limited by the compression error. The discarded high-rank information in PowerSGD or the discarded gradient components in Top-K SGD prevent full recovery of the true gradient, thereby enhancing privacy protection. Of course, as the degree of compression decreases, more information is retained, which increases the possibility of successful gradient reconstruction and thereby reduces privacy protection."}, {"title": "4 Empirical Study", "content": "In this section, we present the empirical analysis of the GradInv on different SGD algorithms, including PowerSGD, Top-K SGD, and the original SGD. We include the experimental setup, attack results, and a discussion of the findings."}, {"title": "4.1 Experiments Setup", "content": "The experiments are conducted using the ResNet18 architecture trained on the three baseline datasets: CIFAR-10, CIFAR-100, and MNIST dataset, a widely-used benchmark for image classification tasks, each configuration evaluates 10 reconstructed image samples. We evaluated the vulnerability of different compression strategies under GradInv by comparing the Structural Similarity Index Measure (SSIM) values for the reconstructed data, ranges from 0 to 1, where a value of 1 indicates perfect similarity between two images, and 0 indicates no similarity. The algorithms compared in our analysis included:\n\u2022 PowerSGD: Evaluated at ranks 1, 2, 4, 30, 50.\n\u2022 Top-K SGD: With compression ratios equivalent to the corresponding ranks of PowerSGD.\n\u2022 Original SGD: As a baseline, without any compression."}, {"title": "4.2 Results and Analysis", "content": "Figure 1 displays the reconstructed images from GradInv and table 1 summarizes the SSIM values for reconstructed images, under different algorithms and compression settings across datasets. The metrics reported include the mean SSIM, standard deviation, and percentage of the SSIM relative to the baseline obtained with original SGD (set as 100 %). The results highlight how each compression method affects the quality of gradient inversion, providing insight into the privacy-preserving capability of each algorithm:"}, {"title": "5 Discussion", "content": "We conducted MIA following the experimental setup described in Empirical Study, focusing on three types of attacks: prediction-based attacks, loss-based attacks, and cross-entropy-based attacks. The results, as summarized in Table 2, show the success rates of these attacks across different datasets (CIFAR-10, CIFAR-100, MNIST) and algorithms (OriginalSGD, PowerSGD, Top-K SGD). Contrary to our initial hypothesis, compressing gradients, whether through PowerSGD or Top-K SGD, does not appear to significantly reduce the attack success rates. The differences between the compressed and uncompressed methods are subtle, and in most cases, the success rates of attacks remain very close to those observed with uncompressed gradients (OriginalSGD). For example, when evaluating attacks on the CIFAR-10 dataset, the success rates across compressed variants such as PowerSGD and Top-K SGD show only minor variations from the OriginalSGD baseline, particularly for lower compression ranks (e.g., PowerSGD Rank 1 and Top-K SGD Level 1). These small variations suggest that gradient compression does not inherently enhance resistance to MIA. Similarly, in the MNIST dataset, the success rates across all compressed methods are nearly identical to those of the uncompressed baseline.\nHowever, we observe that in certain configurations, gradient compression does show more noticeable effects. For instance, in the CIFAR-100 dataset, the use of Top-K SGD with higher compression levels (e.g., Level 30 and Level 50) leads to a more pronounced decrease in the attack success rates. This indicates that specific compression strategies and higher levels of compression can have a tangible impact on MIA resistance, although this effect is not consistently observed across all datasets and algorithms.\nThese findings suggest that the relationship between gradient compression and MIA success rates may be more complex than initially assumed. While black-box MIA techniques may be unreliable in detecting subtle privacy leakage, it is also possible that certain gradient compression methods retain sufficient information for adversaries to exploit. This aligns with recent studies that argue current privacy evaluation metrics: MIAs may be misleading [27, 28]."}, {"title": "6 Conclusion", "content": "This study examines the impact of gradient compression techniques, including PowerSGD and Top-K SGD, and uncompressed SGD on the effectiveness of GradInv and MIA in distributed learning across various datasets. Unlike previous work, we provide comprehensive empirical evaluations to assess the privacy risks of different SGD variants, addressing gaps in the understanding of these methods' trustworthiness.\nOur findings show that uncompressed gradients using Original SGD pose the highest privacy risk, as demonstrated by high SSIM values and detailed reconstructions. In contrast, gradient compression techniques significantly lower the quality of reconstructed images, thereby enhancing privacy protection. Notably, we find that MIA demonstrates low sensitivity to both compressed and uncompressed SGD, indicating that MIA may not be a reliable metric for assessing privacy risks in machine learning. We call for further investigation into the existing privacy evaluation methods for machine learning.\nFuture work will explore the effects of compression on other attack vectors, such as data poisoning and backdoor attacks, to gain a more complete understanding of the privacy implications of these methods. Additionally, we aim to build a stronger theoretical foundation for assessing the trustworthiness of distributed learning algorithms."}]}