{"title": "ADLM - stega: A Universal Adaptive Token Selection Algorithm for Improving Steganographic Text Quality via Information Entropy", "authors": ["Zezheng Qin", "Congcong Sun", "Taiyi He", "Yuke He", "Azizol Abdullah", "Normalia Samian", "Nuur Alifah Roslan"], "abstract": "In the context of widespread global information sharing, information security and privacy protection have become focal points. Steganographic systems enhance information security by embedding confidential information into public carriers; however, existing generative text steganography methods face challenges in handling the long-tail distribution of candidate word pools, which impacts the imperceptibility of steganographic information. This paper proposes a quality control theory for steganographic text generation based on information entropy constraints, exploring the relationship between the imperceptibility of steganographic texts and information entropy. By controlling the information entropy of the candidate word pool within a specific range, we optimize the imperceptibility of the steganographic text. We establish upper and lower bounds for information entropy and introduce an adaptive truncation method to balance semantic coherence and lexical diversity. Experimental results demonstrate that reasonably controlling the candidate pool size and information entropy thresholds significantly enhances the quality and detection resistance of steganographic texts, showcasing broad application potential in the field of natural language processing.", "sections": [{"title": "1 Introduction", "content": "Global information sharing has become an indispensable aspect of the information society [1,2]. Recently, the protection of information security and privacy has garnered increased attention. According to Shannon, there are three primary types of information security systems in cyberspace: encryption systems, privacy systems, and steganography systems [3]. While encryption and privacy systems safeguard the content of information, they also reveal its significance, which may introduce latent risks to the overall security framework. In contrast, steganography systems uniquely focus on embedding critical information within public carriers to obscure its presence, thereby enhancing its security [4].\nTheoretically, any carrier with redundant information space can be utilized to conceal internal secrets. This technology, known as steganography, involves embedding a secret message into public multimedia carriers\u2014such as text, images, audio, and video\u2014to create a steganographic carrier. This carrier can then be transmitted through public channels [5].\nAs one of the most crucial carriers of information, text plays a vital role in daily communication. Over the course of its historical development, text has evolved complex semantic coding rules aimed at improving communication efficiency, which has led to a reduction in semantic ambiguity and information redundancy. However, these factors present challenges in generating high-quality steganographic text carriers [6].\nTo generate high-quality steganographic text carriers, previous studies have proposed methods for carrier selection and modification [7]. While these methods have addressed the complex semantic encoding rules of the original text, reducing semantic ambiguity and improving carrier quality, they fail"}, {"title": "2 RELATED WORK", "content": ""}, {"title": "2.1 Text selection based steganography", "content": "Li et al. (2021) proposed a method for dynamically expanding the sentiment dictionary by applying conjunction rules to identify candidate sentiment words and adding words with high similarity scores to the dictionary. Experimental results show that these conjunction rules effectively identify sentiment words, significantly reducing the likelihood of detection by statistical analysis when used for sentiment word substitution in steganography [14]. Mu et al. (2010) addressed challenges related to varying vocabulary, syntax, and semantics in steganographic carriers, allowing for the correction of grammatical errors and enhancing the coherence of the generated covert text by using templates [15]. Bennett and Krista (2004) demonstrated that information can be concealed in text files by strategically placing punctuation marks, such as periods and commas, in appropriate positions [16].\nAlthough these methods overcome the complexity of semantic encoding rules, reduce semantic ambiguity, and produce higher-quality steganographic carriers, they fail to effectively address the issue of low information redundancy in the original text, which results in a lower embedding rate for secret information."}, {"title": "2.2 Text generation based steganography", "content": "In the early stages, text generation-based steganographic techniques utilized Markov models to estimate the conditional probability distribution of each candidate word in the generated text, selecting appropriate words based on the encoded secret information [17]. However, due to the limitations of Markov models, the generated steganographic texts only conformed to the probability characteristics of the training dataset, making them vulnerable to detection.\nWith advancements in Natural Language Processing (NLP), Yang et al. (2018) developed a steganographic algorithm that combines Recurrent Neural Networks (RNNs) with Huffman coding. This approach first trains a language model using an RNN and then employs Huffman coding to encode the vocabulary [9]. Building on this foundation, Kang et al. (2020) introduced a Long Short-Term Memory (LSTM) network with keyword attention, which enhances the generation of steganographic texts by improving their semantic quality and increasing resistance to steganalysis [18].\nFurthermore, the FREmax method [13] adjusts the probability distribution of tokens using frequency factors to make it more aligned with human-generated text. However, while this method increases the generation probability of low-frequency tokens in the long tail, it may not fully capture the complex distribution patterns of these tokens as observed in human text. In addition to these methods, other researchers have explored text generation steganography techniques based on Generative Adversarial Networks (GANs) [19] and Automatic Dialogue Systems [20].\nAlthough generation-based steganographic techniques can embed arbitrary secret information during text generation, these methods often fail to fully consider the overall probability distribution of the candidate pool when constructing the vocabulary, as well as the impact of the candidate pool size on the quality of the steganographic text. As a result, the generated steganographic texts may suffer from semantic ambiguity or lack coherence in vocabulary usage."}, {"title": "3 The Control Theory of Imperceptibility for Steganographic Text Based on Information Entropy Bounds", "content": "The core of steganographic text quality includes two aspects: imperceptibility and semantic coherence. Imperceptibility refers to the difficulty of detecting hidden information within the generated text, while coherence refers to the natural fluency of the text's semantics. This paper hypothesizes a quantitative relationship between the quality of steganographic text and the information entropy distribution of words in the candidate word pool. By controlling the information entropy within a specific threshold range, we can optimize text quality.\nCore Hypothesis: The information entropy of words in the candidate word pool is closely related to the quality of steganographic text. There exists an optimal entropy value interval [Hmin, Hmax] that allows the generated text to achieve the best states of imperceptibility and coherence.\nIn the process of generating steganographic text, information entropy H(X) is used to measure the randomness and diversity of word selection in the candidate word pool. Controlling the information entropy within a"}, {"title": "3.1 Connection Between Information Entropy and Semantic Coherence", "content": "Let the candidate word pool be denoted as V, and the probability of each word x appearing in the pool is p(x). The information entropy of the word pool is given by:\n$H(X) = - \\Sigma_{x \\epsilon V}p(x) log p(x)$ (1)\nThe entropy value reflects the randomness of word selection in the candidate word pool. A higher entropy value indicates that the distribution of candidate words is more dispersed, while a lower entropy value suggests that word selection is concentrated among a few instances.\nWhen information entropy H(X) is too high, the selection of words in the candidate word pool tends toward a uniform distribution, resulting in excessive diversity of the vocabulary distribution. In this case, the generated text may lack semantic coherence, as the generated words may be independent of one another, leading to semantic dispersion.\nConversely, when information entropy H(X) is too low, word selection becomes highly concentrated on high-frequency words, increasing the repetition rate and resulting in generated text that is stiff and lacks natural fluency."}, {"title": "3.2 The Relationship Between Information Entropy and Imperceptibility", "content": "The imperceptibility of steganographic text can be measured by the similarity between the hidden information and normal text. If the word distribution of the generated steganographic text is not significantly different from that of normal text, it becomes more difficult to detect the hidden information, resulting in better imperceptibility."}, {"title": "3.3 Theoretical Derivation of Information Entropy Bounds", "content": "To determine reasonable Hmin and Hmax, we can analyze the behavior of information entropy under different distributions by examining extreme cases of probability distributions.\nMaximum Information Entropy Hmax in Extreme Cases First, consider the case where the selection of each word in the candidate word pool"}, {"title": "3.4 Derivation of Reasonable Bounds for Information Entropy", "content": "While Hmax = log|V| and Hmin = 0 represent theoretical extremes, in practical applications, excessively high or low information entropy can degrade text quality. Therefore, the reasonable entropy value interval [Hmin, Hmax] should be restricted between these extreme entropy values.\nTo further derive a reasonable threshold range, we assume that the semantic coherence of the generated text is inversely proportional to information entropy, while the diversity of vocabulary is positively correlated with information entropy. Specifically, we define:"}, {"title": "4 PROPOSED APPROACH", "content": "In current text steganography, balancing the size of candidate pools during secret information embedding presents a significant challenge. To address this issue and enhance the quality of the generated steganographic text, improvements to the embedding algorithm are necessary. Inspired by the work of Zhu et al. [21], our approach involves truncating the predicted words output by the generative model based on their information entropy at each time step, using a predetermined threshold. Truncation is an algorithm that reduces the vocabulary space of a text steganography generation model to a"}, {"title": "4.1 The model to generate the steganographic text", "content": "Due to the superior performance of pre-trained models in Natural Language Generation (NLG) tasks [22], this paper employs the pre-trained model GPT-2 XL [23] to generate the steganographic text.\nGPT-2 XL has enhanced capabilities for text generation and understanding. Its larger parameter size enables it to produce smoother and more coherent text, and it excels in handling complex language structures and contexts, capturing subtle contextual nuances more accurately."}, {"title": "4.2 Adaptive truncation of candidate set process", "content": "This section will explain the rationale behind using information entropy to effectively truncate the size of the candidate set, describe how information entropy is employed to appropriately manage this truncation, and outline the processes of information embedding and extraction.\nThe candidate pool of a steganographic text generation model, when used without intervention, typically exhibits a long-tail distribution, often resulting in word repetition in the generated steganographic text [13]. While this behavior aligns with the objective of maximizing likelihood during text generation, it does not necessarily ensure high-quality steganographic text. Additionally, the model may sample from low-probability tokens that are semantically unrelated to the already generated text, leading to incoherence. Therefore, effective truncation of the model's output words is essential for improving the quality of steganographic text generation.\nTo address this, we use information entropy to truncate the candidate set. Information entropy, a concept from information theory, measures the uncertainty or randomness of a random variable. Higher entropy indicates greater uncertainty, while lower entropy suggests less uncertainty [24]. Thus, the information entropy of the candidate pool at each time step can indicate the state of information within the pool. By analyzing the information entropy across all time steps, a reasonable range of entropy typically suggests good vocabulary diversity and semantic coherence in the steganographic text. Conversely, higher or lower entropy may indicate excessive repetition or that the generated text fails to meet the criterion of imperceptibility [24].\nFor the discrete variable distribution of the candidate pool at each time step. For each time step, the information entropyH(X), of the model output words discrete variable distribution is\n$H(X) \u2264 log |V|$ (8)\nIn order to reduce the impact of the vocabulary dimension of the text generation model, we use the minimum-maximum method to normalize the entropy. For each time step of the model output words discrete variable distribution X, the confidence Conf(X) is defined as the minimum-maximum scale of entropy, and its value range is [0,1]:\n$Conf(X) = 1 + \\frac{\\Sigma_{x \\epsilon V}p(x) log p(x)}{log |V|}$ (9)\n$\\Sigma_{x \\epsilon V} p(x) log p(x)$ can be divided into two parts. The first part is $\\Sigma p_i log p_i$,\nwhere k represents the number of model output words selected in the candidate pool at the current time step, which is 0 at the beginning. The second part is the unknown part, which introduces the maximum uncertainty and is\nexpressed as $(1 \u2013 \\frac{Pi}{|V|}) log \\frac{Pi}{|V|}$. That is, the maximum influence of the\nunselected words in the candidate pool at the current time step.\nIn order to determine the reasonable size of the candidate pool, we define\nthe candidate pool size at the current time step as k and the confidence state"}, {"title": "4.3 Secret Information Embedding Algorithm", "content": "The ADLM-stega method proposed in this paper embeds secret messages during the text generation process. The core idea is to encode candidate words at each step and determine the length of the binary bit stream that can be embedded based on the size of the candidate pool. The corresponding code word is then selected according to this binary bit stream. The specific process is as follows:\n1. The sender determines the prefix p and the threshold \u025b based on the pre-agreed key. The sender encodes the secret message into a binary bit stream B.\n2. The generative text model performs the text generation task using the prefix p.\n3. The sender establishes a candidate pool. If n = 1, it cannot embed the secret message, so the sender updates p = p + xt and returns to Step 2. If n > 1, the sender adds all truncated candidate words to the pool.\n4. The sender determines the length of the binary bit stream that can be embedded based on the candidate pool size. Let m denote the pool size at time t (where m > 1). The bit flow rate is k = [log2 m].\nThe sender compares the remaining unembedded message length lrh"}, {"title": "4.4 Secret Information Extraction Algorithm", "content": "During the hidden text extraction process, the receiver uses the same model and key to extract the encoded secret message. The specific process is as follows:\n1. The receiver determines the prefix p and threshold e from the shared key and treats the hidden text as a word sequence.\n2. The generative text model generates text with prefix p and threshold \u0395.\n3. The receiver creates a candidate word pool as the sender does.\n4. The receiver determines the binary bit stream segment length k based on the pool size, selects and encodes words, and updates the prefix p accordingly.\n5. The process is repeated until all secret messages are extracted."}, {"title": "5 EXPERIMENTAL RESULTS AND ANALYSIS", "content": "In this section, we begin by conducting experiments and analyses to evaluate the quality of the generated steganographic text. Subsequently, we investigate the relationship between the size of the candidate pool and the threshold. We then assess the resistance of our method to steganalysis. Finally, we perform ablation studies on the model to further validate its robustness.\nThe experiments are conducted using the GPT-2XL model for generating steganographic texts. The proposed method is implemented in PyTorch and executed on an RTX 3080 GPU."}, {"title": "5.1 The imperceptibility and semantic coherence of steganographic text.", "content": "First, the regulation of information entropy H(X) during the steganographic text generation process significantly impacts text quality. Theoretically, after controlling with information entropy thresholds, the generated text should approach optimal values in perplexity PPL and vocabulary distribution D, specifically:\n$PPL \u2248 PPL_{min},  D \u2248 D_{opt}$ (18)\nThe objective of the experiments is to compare the perplexity and vocabulary distribution of the text generated within different ranges of information"}, {"title": "5.2 Relationship Between the Size of the Candidate Pool and the Threshold", "content": "The size of the candidate pool in the model is influenced by the threshold value. To explore the relationship between candidate pool size and threshold, we generated 200 samples with varying thresholds and calculated the average candidate pool size for each threshold. The results are presented in Figure 4.\nWe observe that the average candidate pool size remains relatively stable when the threshold is set to 0.001 and 0.002, but decreases rapidly as the threshold increases beyond these values. This behavior indicates that the distribution of the candidate pool follows a long-tail distribution pattern."}, {"title": "5.3 Anti-Detectability", "content": "The primary objective of steganography is to securely transmit secret information, making it essential to evaluate the anti-detection capability of the proposed method [25]. We assess anti-detection ability using accuracy (ACC) and recall (R) metrics for the steganalysis detection method."}, {"title": "5.4 Ablation Experiment", "content": "From the results presented in Table 2, we draw the following conclusions. First, as the embedding rate increases, the steganographic text becomes more detectable. Second, our method demonstrates superior anti-detection ability compared to the other two methods. Third, the steganalysis results further corroborate the findings from the imperceptibility experiments."}, {"title": "6 Conclusion", "content": "The ADLM-stega method proposed in this paper effectively improves the quality of stego-texts by adaptively adjusting the candidate set size, addressing the shortcomings of existing methods in handling long-tail distribution issues. Firstly, we demonstrate the impact of candidate set size on the quality of stego-texts, proving that an appropriate candidate set size is crucial for generating high-quality stego-texts. Secondly, we analyze the relationship between the candidate pool size and the truncation algorithm threshold, revealing the long-tail effect in text steganography. Finally, ADLM-stega excels in terms of concealment and resistance to detection, validating its effectiveness in stego-text generation. Experimental results show that this method can generate semantically consistent and naturally flowing stego-texts across various embedding rates, with strong detection resistance, providing an effective solution for the field of stego-text generation. Future work will focus on utilizing fine-tuning techniques to improve generative steganography models to further enhance the security of covert communications."}]}