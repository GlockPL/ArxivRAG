{"title": "A Robust Prototype-Based Network with Interpretable RBF Classifier Foundations", "authors": ["Sascha Saralajew", "Ashish Rana", "Thomas Villmann", "Ammar Shaker"], "abstract": "Prototype-based classification learning methods are known to be inherently interpretable. However, this paradigm suffers from major limitations compared to deep models, such as lower performance. This led to the development of the so-called deep Prototype-Based Networks (PBNs), also known as prototypical parts models. In this work, we analyze these models with respect to different properties, including interpretability. In particular, we focus on the Classification-by-Components (CBC) approach, which uses a probabilistic model to ensure interpretability and can be used as a shallow or deep architecture. We show that this model has several shortcomings, like creating contradicting explanations. Based on these findings, we propose an extension of CBC that solves these issues. Moreover, we prove that this extension has robustness guarantees and derive a loss that optimizes robustness. Additionally, our analysis shows that most (deep) PBNs are related to (deep) RBF classifiers, which implies that our robustness guarantees generalize to shallow RBF classifiers. The empirical evaluation demonstrates that our deep PBN yields state-of-the-art classification accuracy on different benchmarks while resolving the interpretability shortcomings of other approaches. Further, our shallow PBN variant outperforms other shallow PBNs while being inherently interpretable and exhibiting provable robustness guarantees.", "sections": [{"title": "Motivation and Context", "content": "Two principal streams exist in the field of explainable machine learning: (1) post-processing methods (post-hoc approaches) that try to explain the prediction process of an existing model, such as LIME and SHAP (see Marcinkevi\u010ds and Vogt 2023, for an overview), and (2) the design of machine learning methods with inherently interpretable prediction processes (Rudin 2019). While the former could create non-faithful explanations due to only approximating the output distribution of a black box model without explaining its internal logic, it is claimed that inherently interpretable methods always generate faithful explanations (Rudin 2019). According to Molnar (2022), a model is called interpretable if its behavior and predictions are understandable to humans. Moreover, when the provided explanations lead to a correct interpretation of the model, this interpretation enriches the user (or developer) with an understanding of how the model works, how it can be fixed or improved, and whether it can be trusted (Ribeiro, Singh, and Guestrin 2016).\nA well-known category of interpretable models for classification tasks is (shallow) Prototype-Based Networks (PBN) such as LVQ (e. g., Biehl, Hammer, and Villmann 2016). These models are interpretable because (1) the learned class-specific prototypes\u00b9 are either from the input space or can be easily mapped to it; belonging to the input space helps summarize the differentiating factors of the input data and provides trusted exemplars for each class, (2) the dissimilarity computations are given by human comprehensible equations such that differences between inputs and learned prototypes can be understood, (3) the classification rule based on the dissimilarities is intelligible (e. g., winner-takes-all principle); see Bancos et al. (2020) for an interpretability application. Despite being interpretable, these models also face limitations: (1) The number of parameters becomes large on complex data since the prototypes are class-specific and are defined in the input space.\u00b2 (2) The classification performance is behind that of deep neural architectures as the dissimilarity functions and the classification rules are straightforward to ensure interpretability (Villmann, Bohnsack, and Kaden 2017).\nTo fix these limitations, researchers investigated the integration of prototype-based classification heads with deep neural feature extractors to build deep interpretable PBNs and designed numerous architectures such as ProtoPNet (Chen et al. 2019), ProtoPool (Rymarczyk et al. 2022), CBC (Classification-By-Components; Saralajew et al. 2019), and PIPNet (Nauta et al. 2023). The generated results of these models are impressive as they achieve state-of-the-art classification accuracy on fine-grained image classification, and some show a good performance in rejecting Out-Of-Distribution (OOD) examples (e. g., PIPNet). The high-level structure of these models follows the same principles (see Fig. 1): (1) embedding of the input data in a latent space by a Neural Network (NN), denoted as feature extractor backbone; (2) measuring the dissimilarity (or similarity) between the embedding and the latent prototypes; (3) prediction computation after aggregating the dissimilarities by a shallow model (realizes the classification rule), denoted as classification head. In this paradigm, the differences between the"}, {"title": "Review of Deep Prototype-based Networks", "content": "In the following section, we review the differences between deep PBNs and show their relation to RBF networks. Thereafter, we discuss the interpretability of these methods using the established relation. Later, we explain why PBNs are suitable for OOD detection and analyze the role of negative reasoning.\nDifferences between the architectures and their relation to RBF networks. Fig. 1 shows the general architecture of most deep PBNs. We use the shown building blocks to characterize existing approaches in Tab. 1 along the following dimensions:\n\u2022 Backbone: Single, multiple, or Siamese feature extractor, and whether the method has been tested without a feature extractor (shallow model).\n\u2022 Latent prototypes: Whether the prototypes are defined in the input or the latent space and if they are back-projected to training samples (Chen et al. 2019). This dimension also indicates if prototypes are class-specific.\n\u2022 Similarity: The used similarity function. RBF refers to the standard squared exponential kernel. If a different nonlinear function is used to construct the RBF, it is specified in parenthesis. Note that all RBFs use the Euclidean norm.\n\u2022 Linear layer constraints: The constraints on the final linear prediction layer or the stated approach to compute the output if no linear output layer is used. The $l_1$ regularization is only applied to connections that connect similarity scores (slots, etc.) with incorrect classes.\n\u2022 Single loss term: Whether multiple loss terms are used.\n\u2022 Main contribution: The primary contribution of the proposed architecture compared to previous work.\nConsidering Fig. 1, we realize that the head of a deep PBN is an RBF network if a linear layer is used for prediction. Combined with a feature extractor, we obtain deep RBF networks (e. g., Asadi et al. 2021). Notably, the first deep PBN is LeNet5, where RBF heads are used to measure the similarity between inputs and the so-called \"model\" (prototype) of the class. Starting with ProtoPNet, the existing architectures (see Tab. 1) build on each other (except for CBC and ProtoAttend), and almost all use an RBF network with some constraints or regularizers as classification heads. Consequently, changes between the architectures are incremental, and concepts persist for some time once introduced. Recently, researchers abandoned the idea of back-projecting prototypes and started using dot products instead of RBF functions, which implicitly defines prototypes as convolutional filter kernels (PIPNet, LucidPPN).\nOn the interpretability of deep PBNs. Using the definition of interpretability in Sec. 1 and the relation to RBF networks, we discuss the interpretability of deep PBNs. First, it should be noted that RBF networks and shallow PBNs learn representations in the input space (centroids and prototypes, respectively), and both use these representations to measure the (dis)-similarity to given samples. At the same time, these two paradigms differ in two aspects: (1) RBFs' usage of non-class specific centroids and (2) PBNs' usage of the human-comprehensible winner-takes-all rule instead of a linear predictor over the prototypes.\nThe first aspect overcomes the Limitation (1) mentioned in Sec. 1 without harming the interpretability. The second"}, {"title": "Classification-by-Components Networks", "content": "We now review the original CBC architecture and show its limitations. Based on that, we propose our CBC-simply denoted as CBC and the old version is denoted as original CBC-that overcomes these limitations and realizes a strong link to RBF networks. Then, we show how a CBC can be learned efficiently and derive robustness lower bounds.\nReview of the original CBC method. Components are the core concept of the original CBC, where a component is a pattern that contributes to the classification process by its presence (positive reasoning; the component must be close) or absence (negative reasoning; the component must be far) without being tied to a specific class label. A component can also abstain from the classification process, which is called indefinite reasoning (modeled via importance). The original CBC is based on a probability tree diagram to model the interaction between the detection of components in input samples and the usage of detection responses to model the output probability (called reasoning). The probability tree, Fig. 2, employs five random variables: c, the class label; k, the component; I, the importance of a component (binary); R, the requiredness for reasoning (binary); D, the detection of a component (binary). The probability tree constructs the following: $P(k)$, the prior of the k-th component to appear; $P(I|k, c)$ and $P(R|k, c)$ are the importance and the requiredness probabilities of the k-th component for the class c; $P(D|k, x)$, the detection probability of the k-th component in the input x. $P(\\bar{D}|k, x)$ is the complementary probability, that is, not detecting the k-th component in x. An agreement A is a path in the tree (see solid lines in Fig. 2) that depicts the positive influence of the k-th component on class c by either being detected (D) and required (R) or not detected ($\\bar{D}$) and not required ($\\bar{R}$). The output probability $p_c(x) = P(A|I, x, c)$ for class c is derived from the agreement A using the following expression:\n$\\frac{\\sum_{k} (P(R,I|k, c) P (D|k,x) + P (\\bar{R},\\bar{I}|k,c) P (\\bar{D}|k,x)) P (k)}{\\sum_{k} (P(R, I|k, c) + P (\\bar{R},\\bar{I}|k,c)) P (k)}$       (1)\nThe defined probabilities and components are learned by minimizing the margin loss (maximizing the probability gap)\n$\\min \\max_{c'\\neq y} [p_{y}(x) - p_{c'}(x) + \\gamma, 0]^+$    (2)\nwith $\\gamma\\in [0, 1]$ being the margin value, y being the correct class label of x, and c' being any class label other than y. The model can be used without or with a feature extractor (see Fig. 1); that is, the distance computation occurs in the learned latent space. An original CBC without a feature extractor realizes an extension of traditional PBNs, overcoming Limitation (1) while posing new difficulties.\nThe architecture is difficult to train as it often converges to a bad local minimum (see Sec. 4), and the explanations can be counterintuitive. To see this, note that $P (\\bar{I}|k, c) + P (R,I|k, c) + P (\\bar{R},\\bar{I}|k,c)  = 1$ for each k. Thus, one can scale the reasoning probabilities $P(R,I|k,c)$ and $P (\\bar{R},\\bar{I}|k,c)$ in Eq.(1) by any factor $a > 0$ as long as $P(R,I|k,c), P (\\bar{R},\\bar{I}|k,c)  \\in [0, 1]$ remains valid without changing the output probability $p_c(x)$. Assuming that $p_c(x) = 1$, this result can be obtained from nearly zero reasoning probabilities, giving confident predictions from infinitesimal reasoning evidence. This contradicts the design principle of the original CBC approach, as $p_c(x) = 1$ should only be generated if the model is certain in its reasoning. At the same time, this result implies that the optimal output ($p_c(x) = 1$) is not unique with a wide range of flawed feasible solutions, thus causing the model to converge to bad local minima.\nOur extension of the original CBC method. In CBC, both problems mentioned above are caused by the indefinite reasoning probability $P (\\bar{I}|k, c)$ together with the component prior P(k). These probabilities model the extent to which a component is used in the classification process; hence, they both serve the same purpose, as confirmed by fixing P(k) to be uniform in the original CBC. Removing $P (\\bar{I}|k, c)$ from the model eliminates the problematic model's tolerance towards scaling by a factor a. Still, it causes missing support for allowing components to remain irrelevant (to abstain), as explained by Saralajew et al. (2019) in Figure 1. Similarly, allowing the prior P(k) to be trainable does not generalize to cover the property of class-specific component priors.\nWe now present our modification to the original CBC to overcome the difficulties. We propose to remove the impor-"}, {"title": "Experiments", "content": "In this section, we test our CBC and the presented theories: (1) We analyze the accuracy and interpretability of our CBC and compare it to PIPNet. (2) We compare shallow CBCs with other shallow models, such as the original CBC. (3) To demonstrate our theorems, we analyze the adversarial robustness of shallow PBNs. Note that all accuracy results are reported in percentage; we train each model five times, and report the mean and standard deviation.\nInterpretability and performance assessment: Comparison with PIPNet. We evaluate the performance of CBC in comparison with PIPNet and the state-of-the-art deep PBN ProtoPool and ProtoViT (CaiT-XXS 24; best-performing"}, {"title": "Discussion and Limitations", "content": "While we refrain from claiming that our deep model is fully interpretable, we believe it offers partial interpretability, providing valuable insights into the classification process, especially in the final layers. In contrast, the shallow version is inherently interpretable.\nCompared to other deep PBNs, our model uses only a single loss term and neither forces the components to be close to training samples nor to be apart from each other. This is beneficial as it simplifies the training procedure drastically since no regularization terms have to be tuned. Even if we only use one loss term, our model converges to valuable components. However, interpreting these components is complex and requires expert knowledge. As a result, especially for deep PBNs, the interpretation could be largely shaped by the user's mental model, highlighting the importance of quantitative interpretation assessment approaches\u2014something that is still lacking in the field. Additionally, by optimizing the single loss term, our model automatically learns sparse component representations without the issue of the learned representation being excessively sparse (see the additional PIPNet experiments in Appx. D.1).\nDuring the deep model training, we observed that the CBC training behavior can be sensitive to pre-training and initializations. Further, training huge shallow models was challenging, especially when optimizing the robust loss: The model did not leverage all components as they often converged to the same point or failed to use all reasoning vectors if multiple reasoning vectors per class were provided. Additionally, training exponential functions (the detection probability) is sensitive to the selection of suitable temperature values. When we kept them trainable and individual per component, sometimes they became so small that the components did not learn anything even if the components had not converged to a suitable position in the data space. The same happened when we tried to apply exponential functions on top of a"}, {"title": "Conclusion and Outlook", "content": "In this paper, we harmonize deep PBNs by showing a solid link to RBF networks. We also show how these models are not interpretable and only achieve partial interpretability in the best case. Inspired by these findings, we derive an improved CBC architecture that uses negative reasoning in a probabilistically sound way and ensures partial interpretability. Empirically, we demonstrate that the proposed deep PBN outperforms existing models on established benchmarks. Besides, the shallow version of our CBC is interpretable and provably robust. The shallow CBC is an attractive alternative to established models such as GLVQ as it resolves known limitations like the use of class-specific prototypes.\nOpen questions still exist and are left for future work: For example, a modification that prevents components from converging to the same point, along with the integration of spatial knowledge (to avoid global max-pooling), could improve deep PBNs, a challenge that the original CBC partially addressed. Moreover, in our evaluation, we focused on the assessment of our approach using image datasets, which is currently the commonly used benchmark domain for PBNs. However, future work should investigate the application of our approach to other domains, such as time series data. Moreover, to stabilize the training of the detection probability, one should explore the strategies proposed by Ghiasi-Shirazi (2019) or analyze the application of other detection probability functions (note that our theoretical results generalize to exponential functions with an arbitrary base). Finally, it is unclear why all shallow models, including non-robustified ones, exhibit good empirical robustness."}, {"title": "Derivation of the Tangent Distance and Extension to Restricted Versions", "content": "The tangent distance is a transformation-invariant measure. Instead of learning an individual prototype, it learns an affine subspace to model the data manifold of a given class (Haasdonk and Keysers 2002; Hastie, Simard, and S\u00e4ckinger 1995). Its effectiveness was demonstrated multiple times. Given an affine subspace that models the data and an input sample, the tangent distance is defined as the minimal Euclidean distance between the affine subspace and the input sample:\n$d_T (x, S) = \\min_{\\Theta \\in R^r}  d_E (x, w+W\\Theta)$,\nwhere $S = {w+W\\Theta | \\Theta \\in R^r}$ is an r-dimensional affine subspace with W being an orthonormal basis (i. e., $W^TW = I$). It can be shown that the minimizer $w^* (x)$ is given by\n$w^* (x) = w + WW^T (x - w),$    (11)\nwhich is the best approximating element. Using this result, the tangent distance becomes\n$d_T (x, S) = ||x - w - WW^T (x-w) ||_E$\n$= || (I-WW^T) (x-W) ||_E$\nNote that $I - WW^T$ is an orthogonal projector and, hence, is idempotent, which implies\n$d_T (x,S) = \\sqrt{(x-w)^T (I-WW^T) (x - w)}.$       \nThis equation can be used as a dissimilarity measure in classification learning frameworks where the affine subspace is learned from data (Saralajew and Villmann 2016). Moreover, this equation can be efficiently implemented and even generalized to sliding operations (similar to a convolution that uses the dot product) on parallel computing hardware. If the measure is used to learn the affine subspaces, it is important that the basis matrix is orthonormalized after each update step or that a proper encoding is applied. For instance, the former can be achieved by a polar decomposition via SVD and the latter by coding the matrices as Householder matrices (Mathiasen et al. 2020). After learning the affine subspaces, W captures the invariant class dimensions, which are dimensions that are invariant with respect to class discrimination. Moreover, the vector w represents a data point similar to an ordinary prototype, a point that represents the surrounding data as well as possible.\nThere are also extensions of this dissimilarity measure that constrain the affine subspace. For example, one can define a threshold $\\gamma > 0$ and modify Eq. (10) to\n$d_{CT} (x, S) = \\min_{\\Theta \\in R^r, ||\\Theta|| \\leq  \\gamma} d_E (x, w+W\\Theta)$,\nwhich constrains the r-dimensional affine subspace to an r-dimensional hyperball. The solution for this distance is\n$d_{CT} (x,S) = d_E(x, w^* (x)) + (\\max {0, d_E (w, w^* (x)) - \\gamma})^2$.\nLike before, this measure can be efficiently implemented so that it is possible to learn these hyperballs from data, which are like affine subspaces that know the neighborhood they are approximating."}, {"title": "Derivation of the Robust Lower Bounds", "content": "In the following, we prove the presented theorems. For this, we prove a lemma that simplifies assumptions such as a class-independent temperature. Then, using the lemma, we prove Thm. 1. Later on, based on Thm. 1 and the proven lemma, we prove Thm. 2 and 3.\nRobustness lower bound for component-independent temperature and a specific incorrect class\nLemma 4. The robustness of a correctly classified sample x with class label y with respect to another class c' and temperature $\\sigma_k = \\sigma$ for all components in the detection probability Eq. (5), where the distance is any distance d (\u00b7,\u00b7) induced by a norm ||\u00b7||, is lower bounded by\n$||\\epsilon^* || \\geq \\sigma ln (\\frac{B+\\sqrt{B^2-4AC}}{2A}) > 0,$    (12)\nwhen A \u2260 0, where\n$A = ((r_y - 1) \\odot b_y - r_{c'} \\odot b_{c'})^T d (x)$,\n$B = (1 - r_y)^T b_y - (1 - r_{c'})^T b_{c'}$,\n$C = (r_y \\odot b_y - (r_{c'} -1) \\odot b_{c'})^T d(x)$."}, {"title": "Proof of Thm. 1", "content": "We now prove Thm. 1 by using Lem. 4. For completeness we restate the theorem:\nTheorem. The robustness of a correctly classified sample x with class label y is lower bounded by\n$||\\epsilon^* ||  > \\kappa_{min} \\ln(\\frac{B+\\sqrt{B^2-4AC}}{2A})$ when A \u2260 0, where\n$\\kappa  =: \\min_{\\sigma_k} \\sigma_k$, \nProof. The proof follows the technique used to prove Lem. 4 with the following changes: To account for a component-wise $\\sigma$, we lower (upper) bound Eq. (14) and Eq. (15) again, respectively,\n$exp(-d(x+\\epsilon,w_k)/\\sigma_k > exp(d(x,w_k)/\\sigma_k) exp(-||\\epsilon||/\\sigma_k)$      (25)\n$exp(d(x+\\epsilon,w_k)/\\sigma_k) = exp(d(x,w_k)/\\sigma_k) exp(-||\\epsilon||/\\sigma_{min})$      (26)\n$exp(-d(x+\\epsilon,w_k)/\\sigma_k) < exp(d(x,w_k)/\\sigma_k) exp(||\\epsilon||/\\sigma_k)$     (27)\n$\\leq exp(-d(x+\\epsilon,w_k)/\\sigma_k) < exp(d(x,w_k)/\\sigma_k) exp(||\\epsilon||/\\sigma_{min})$        (28)\nwhere $\u03c3_k$ is the component-wise temperature, and $\u03c3_{min} = min\\{\u03c3_1 , ..., \u03c3_K\\}$. Consequently, the lower bound for the correct class becomes\n$p_y (x+  \\epsilon) \\geq (r_y \\odot b_y)^T d(x) exp(-z_{min}) + (1 - r_y)^T  b_y + ((r_y - 1) \\odot b_y)^T d (x) exp (z_{min})$,\nand the upper bound of the output probability for an incorrect class becomes\n$p_c (x+  \\epsilon) \\geq (r_{c'}  \\odot b_{c'}) d(x) exp(z_{min}) + (1 - r_{c'}) b_{c'} + ((r_{c'} - 1) \\odot b_{c'}) d (x) exp (-z_{min})$,\nwhere $z_{min} = \\frac{||\\epsilon||}{\\sigma_{min}}$."}, {"title": "Proof of Thm. 2", "content": "We now prove Thm. 2 by extending the proof of Thm. 1. For completeness we restate the theorem:\nTheorem 5. If we use the standard RBF kernel (squared norm), then Eq. (8) becomes $||\\epsilon^*|| > -\\beta + \\sqrt{\\beta^2+8} > 0 with \u03ba = \\frac{\u03c3_{min}}{3}$ and $\\beta = max d (x, w_k)$.\nThe theorem states that the derived bound also holds for the frequently used squared exponential kernel (Gaussian RBF), which implicitly projects data into an infinite-dimensional space. In principle, any squared distance induced by a norm can be used.\nProof. The proof follows the technique used to prove Thm. 1 with the following changes: We modify the initial lower bounds of the distances. In Lem. 4, we used\n$d(x+8,wk) \\leq ||E|| + d (x, wk)$ \nto derive the result. If we square this inequality, we get\n$d^2 (x+8,wk) \\leq ||E||^2 + d^2 (x,wk) +2 ||E|| d (x, wk) $.  \nAmong all $d (x, w_k)$ there exists a maximum $\u03b2 = \\max_kd (x, w_k)$. Using this result, we get\n$d^2 (x+\\epsilon,wk) \\leq (||\\epsilon||^2+2||\\epsilon|| \\beta) + d^2 (x,wk)$ \nand we further we relax the bound to\n$d^2 (x+\\epsilon,wk) \\leq (3||\\epsilon||^2 +2||\\epsilon|| \\beta) + d^2 (x, wk) $.  \nWith this, we get for the lower bound of the detection probability Eq. (14)\n$exp(-d^2 (x+\\epsilon,w_k)/ \\sigma) > exp(-d^2 (x,w_k)/ \\sigma) exp(-\\frac{3 |E||^2 + 2\\beta ||E||}{\\sigma}) = exp(-z)$ \nSimilarly, we can derive the following result for the squared triangle inequality of $d (x,wk) \\leq ||E|| + d(x+8,wk)$: \n$d^2 (x, wk) \\leq ||E||^2 + d^2 (x+\\epsilon,wk) + 2 ||E|| d (x+\\epsilon,wk)$\n$< ||E||^2 + d^2 (x+\\epsilon,wk) +2 ||E|| (||E|| +d (x, wk))$  \n$= ||E||^2 + d^2 (x+\\epsilon,wk) + 2 ||E||^2 + 2 ||E|| d (x,wk)$\n$= ||||^2 + d^2 (x+8,wk) + 2 ||E||^2 + 2\\beta ||E||$\n$= (3||E||^2+2\\beta ||E||) +d^2 (x+8,wk)$.\nThis implies that\n$d^2 (x + \\epsilon,wk) \\geq - (3 ||\\epsilon||^2 + 2\u03b2 ||\\epsilon||) + d^2 (x, wk)$ \nand for the upper bound of the detection probability Eq. (15)\n$exp(-\\frac{d^2 (x+\\epsilon,w_k)}{\\sigma}) < exp(-\\frac{d^2 (x,w_k)}{\\sigma}) exp(\\frac{3 |E||^2 + 2\\beta ||E||}{\\sigma}) = exp(z)$ \nBy using the results of Lem. 4, we get for the root Eq. (23)\n$3 ||\u03b5\u0301||^2  + 2\u03b2 ||\u03b5\u0301|| = \u03c3 ln (B+\u221aB2-4AC).     = ||\u03b5\u0301|| > 0.   (32)\nfor the valid solution as the negative part would lead to a negative ||6\u0301||. By following the additional proof steps of Thm. 1, we get the result."}, {"title": "Proof of Thm. 3", "content": "We now prove Thm. 3 by extending the proof of Thm. 1. For completeness we restate the theorem:\n1\nTheorem. If we use the tangent distance in the RBF of Eq. (5), then Eq. (8) holds with $\\kappa = \\frac{\u03c3_{min}}{2}$ and ||\u00b7|| being the Euclidean norm.\nSee Appx. A for information about the tangent distance.\nProof. The proof follows the technique used to prove Thm. 1 with the following changes: We modify the initial lower bounds of the distances. The goal is to derive a lower bound for\n$d_T (x+\\epsilon,S) = d_E (x+ \\epsilon ,w^* (x+  \\epsilon)),$\nwhere $w^* (x)$ is the best approximating element with respect to x, see Eq. (11). Similar to before (see Eq. (14)) we apply the triangle inequality to derive\n$d_E (x+\\epsilon , w^* (x+  \\epsilon)) \u2264 ||E||E+d_E (x, w^* (x +  \\epsilon)).$\nNow, we upper bound dE (x, w\u2217 (x + )) by applying the triangle inequality again:\n$d_E (x, w^* (x+\\epsilon)) \u2264 d_E (x, w^* (x)) + d_E (w^* (x), w^* (x+e))$ = dT (x, S) + dE (w\u2217 (x), w\u2217 (x+e)).\nThe expression $d_E (w^* (x), w^* (x +  \\epsilon))$ can be upper bounded by ||\\\\:\\\n$d_E (w^* (x), w^* (x+\\epsilon)) = d_E (w+WW^T (x-w), w+ W^T (x + \\epsilon- w))$  \n$= ||w+WW^T (x-w) \u2013 w \u2013 W^T (x +  E-w)||E$\n$= ||WW^T||E$\n$= | E T W W T W E$\n\nNext, we use the fact that the Euclidean norm is compatible with the spectral norm and that the spectral norm of WT is 1 (because of the orthonormal basis assumption):\n$d_E (w^* (x), w^* (x +  \\epsilon)) = ||W^T||E  || E||E =  || E||E.$\nFinally, combining the results we get\n$d_T (x+\\epsilon,S) \u2264 2 ||\\epsilon||E + d_T (x, S).$\nSimilarly, we can derive\n$d_T (x+\\epsilon,S) \u2265 \u22122 ||\\epsilon||F + d_T (x, S).$\nUsing these two results in Eq. (14) and Eq. (15), we conclude that the robustness lower bound for the tangent distance is given by\n||e *|| E \u2265\\frac{\\sigma_{min}}{2}ln  (\u221a)"}]}