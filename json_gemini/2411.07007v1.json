{"title": "NON-ADVERSARIAL INVERSE REINFORCEMENT LEARNING VIA SUCCESSOR FEATURE MATCHING", "authors": ["Arnav Kumar Jain", "Harley Wiltzer", "Jesse Farebrother", "Irina Rish", "Glen Berseth", "Sanjiban Choudhury"], "abstract": "In inverse reinforcement learning (IRL), an agent seeks to replicate expert demonstrations through interactions with the environment. Traditionally, IRL is treated as an adversarial game, where an adversary searches over reward models, and a learner optimizes the reward through repeated RL procedures. This game-solving approach is both computationally expensive and difficult to stabilize. In this work, we propose a novel approach to IRL by direct policy optimization: exploiting a linear factorization of the return as the inner product of successor features and a reward vector, we design an IRL algorithm by policy gradient descent on the gap between the learner and expert features. Our non-adversarial method does not require learning a reward function and can be solved seamlessly with existing actor-critic RL algorithms. Remarkably, our approach works in state-only settings without expert action labels, a setting which behavior cloning (BC) cannot solve. Empirical results demonstrate that our method learns from as few as a single expert demonstration and achieves improved performance on various control tasks.", "sections": [{"title": "1 INTRODUCTION", "content": "In imitation learning (Abbeel & Ng, 2004;\nZiebart et al., 2008; Silver et al., 2016; Ho & Ermon,\n2016; Swamy et al., 2021), the goal is to\nlearn a decision-making policy that reproduces\nbehavior from demonstrations. Rather than\nsimply mimicking the state-conditioned action\ndistribution as in behavior cloning (Pomerleau,\n1988), interactive approaches like Inverse Rein-\nforcement Learning (IRL; Abbeel & Ng, 2004;\nZiebart et al., 2008) have the more ambitious\ngoal of synthesizing a policy whose long-term\noccupancy measure approximates that of the\nexpert demonstrator by some metric. As a re-\nsult, IRL methods have proven to be more ro-\nbust, particularly in a regime with few expert\ndemonstrations, and has lead to successful de-\nployments in real-world domains such as au-\ntonomous driving (Bronstein et al., 2022; Vinitsky et al., 2022; Igl et al.). However, this robust-\nness comes at a cost: approaches to IRL tend to\ninvolve a costly bi-level optimization.\nSpecifically, modern formulation of many IRL\nmethods (e.g., Garg et al., 2021; Swamy et al.,\n2021) involve a min-max game between an ad-\nversary that learns a reward function to maxi-"}, {"title": "2 RELATED WORK", "content": "Inverse Reinforcement Learning (IRL) methods typically learn via adversarial game dynamics,\nwhere prior methods assumed the base features are known upfront (Abbeel & Ng, 2004; Ziebart\net al., 2008; Syed & Schapire, 2007; Syed et al., 2008). The advent of modern deep learning archi-\ntectures led to methods (e.g. Ho & Ermon, 2016; Swamy et al., 2021; Fu et al., 2018) that do not\nestimate expected features, but instead learn a more expressive reward function that captures the dif-\nferences between the expert and the the agent. The class of Moment Matching (MM; Swamy et al.,\n2021) methods offers a general framework that unifies existing algorithms through the concept of\nmoment matching, or equivalently Integral Probability Metrics (IPM; Sun et al., 2019). In contrast\nto these methods, our approach is non-adversarial and focuses on directly addressing the problem\nof matching expected features. Furthermore, unlike prior methods in Apprenticeship Learning (AL;\nAbbeel & Ng, 2004) and Maximum Entropy IRL (Ziebart et al., 2008), our work does not assume\nthe knowledge of base features. Instead, SFM leverages representation learning technique to extract\nrelevant features from the raw observations. The method most similar to ours is IQ-Learn (Garg\net al., 2021), a non-adversarial approach that utilizes an inverse Bellman operator to directly esti-"}, {"title": "3 PRELIMINARIES", "content": "Reinforcement Learning (RL; Sutton & Barto, 2018) typically considers a Markov Decision\nProcess (MDP) defined by M = (S,A,T,r,\u03b3, Po), where S and A denote the state and action\nspaces, T: S \u00d7 A \u2192 \u2206(S) denotes the transition kernel, r : S \u00d7 A \u2192 [-1,1] is the reward\nfunction, \u03b3 is the discount factor, and Po is the initial state distribution. Starting from the initial\nstate so ~ Po an agent takes actions according to its policy \u03c0 : S \u2192 \u0394(A) producing trajectories\nT = {80,A1,81,...}. The value function and action-value are respectively defined as V\u2122(s) =\n\u0395\u03c0[\u03a3\u03c4\u03bf \u03b3r(St, At)|So = s] and Q\" (s, a) = \u0395\u03c0[\u2211t=o\u03b3+r(St, At)|So = s, A\u2081 = a] where \u03b3 \u0395\n[0, 1) represents the discount factor. The performance is the expected return obtained by following\npolicy \u03c0 from the initial state, given by J(\u03c0) = Eso~ Po [\u0395\u03c0 [\u2211t=0 Ytr(St, At) | So = s]], and can\nbe rewritten as J(\u03c0) = Es0~Po [V\" (80)].\nthe expected occupancy of future states for a given policy. For tabular state spaces, temporal-\ndifference learning can be employed to estimate the SR. Successor Features (SF; Barreto et al.,\n2017) generalize the idea of the successor representation by instead counting the discounted sum of\nstate features \" (s,a) = \u0395\u03c0[\u03a3t=oY*$(St, At)|So = s, A\u2081 = a] after applying the feature map-\nping : S \u00d7 A \u2192 Rd. The SR is recovered when $ is the identity function, with $ typically serving\nas a form of dimensionality reduction to learn SFs in continuous or large state spaces. In practice,\nSFs can be estimated via temporal difference learning (Sutton, 1988) through the minimization of\nthe following objective,\n$L_{SF}(\\theta;\\theta) = E_{(s,a,s')\\sim D} [||\\phi(s, a) + \\gamma\\psi_{\\bar{\\theta}} (s', \\pi(s')) - \\psi_{\\theta} (s, a)||^2],\\tag{1}\nwhere the tuple (s, a, s') is sampled from dataset D and \\bar{\\theta} denotes a parametric SF model.\nThe parameters \\theta denote the \u201ctarget parameters\u201d that are periodically updated from \u03b8 by taking\na direct copy or through a moving average. For tasks where the reward function can be ex-\npressed as a combination of base features & and a weight vector w \u2208 Rd such that r(s,a) =\n$(s,a)Tw, the performance of a_policy can be rewritten as Q\u2122 (s,a) = \u03c8\u03c0 (\u03c2, \u03b1) \u03a4 w and\nJ(\u03c0) = Eso~Po, \u03b1~\u03c0(so) [\u03c8\" (80, \u03b1)]Tw (Barreto et al., 2017).\""}, {"title": "4 SUCCESSOR FEATURE MATCHING (SFM)", "content": "In this section, we will describe SFM \u2013 a state-only non-adversarial algorithm for matching expected\nfeatures between the agent and the expert. SFM distinguishes itself in two crucial manners; namely,\nit derives a feature-matching imitation policy by direct policy optimization via policy gradient ascent,\nand it learns base features simultaneously during training.\nThe key intuition behind SFM is that successor feature matching in the l2-norm can be accomplished\ndirectly via policy gradient ascent\u2014this allows us to leverage powerful actor-critic algorithms for\nIRL, as intuited by equation 3. Towards this end, we define a potential function defined as the Mean\nSquared Error (MSE) between the expected features of the expert and the agent, given by\n$U(\\mu) = \\frac{1}{2}||\\psi^{\\pi_{\\mu}} - \\hat{\\psi}^E||_2,\\tag{4}\nwhere \u03c0\u03bc is a policy parameterized by \u03bc, $\\hat{\\psi}^{\\pi_{\\mu}} = E_{s \\sim P_0, a \\sim \\pi(s)} [\\psi^{\\pi_{\\mu}} (s,a)]$ and $\\hat{\\psi}^E =$\n$E_{s \\sim P_0, a \\sim \\pi_E(s)} [\\Psi^E (s, a)]$ represents the expected SF of agent and expert conditioned on the ini-\ntial state distribution P\u2081. Note that \u2207U(\u03bc) = (\u03c8\u03c0\u03bc \u2013 \u03c8\u0395) \u03a4\u2207\u00b5\u03c04. Interpreting \u2122 as a value\nfunction for a vector-valued reward (the base features), it becomes clear that the latter term is sim-\nply a vector of standard policy gradients. This suggests a method for matching the expert successor\nfeatures with a simple actor-critic algorithm. With the state-only base feature function $ : S \u2192 Rd,\nthe expected features of the expert can be estimated using the demonstrations. Here, the SF for the\nexpert using M demonstrations {7\u00b2 = {s\u2081, a, ..., s, a}}\u2081 of length T is obtained by\n$\\hat{\\psi}^E = \\frac{1}{MT}\\sum_{i=1}^M\\sum_{t=1}^T \\gamma^t \\phi(s_t).$\\tag{5}\nTo optimize the objective defined in (4), our method SFM is jointly trained with two components-\npolicy optimizer and base feature function. In subsection 4.1, we describe the policy optimization\ncomponent which comprises of an actor to predict actions and a network to predict the SF. Interest-\ningly, the training loop of SFM closely resembles that of any familiar actor-critic methods, avoiding\nchallenges such as bi-level optimization. In subsection 4.2, we describes the base feature function\ncomponent and discuss how they are adaptively updated using unsupervised RL methods. Since the\nbase feature function is also updated during training, this would change E at each step and thereby\nthe objective in (4). Here, E is updated with Exponential Moving Average (EMA) to stabilize\nlearning. We provide an overview of the training procedure of SFM in Algorithm 1."}, {"title": "4.1 POLICY OPTIMIZATION", "content": "Taking inspiration from off-policy actor-critic methods for standard RL tasks (Fujimoto et al., 2018;\n2023; Haarnoja et al., 2018), SFM maintains a deterministic actor and a network to estimate the SF\nfor agent's policy. Here, instead of having a critic to estimate the expected returns, the agent has a\nnetwork to predict the expected features. The network to predict SF of the agent is a parameterized\nand differentiable function with parameters @ and is denoted by yo. To obtain actions for a given\nstate, SFM maintains a deterministic actor \u03c0\u03bc with parameters \u03bc. To learn policies without an\nadversarial game for this task, we propose to optimize this non-linear objective where our method\nleverages the prowess of the Deterministic Policy Gradient (DPG) (Silver et al., 2014) algorithm.\nThe network to estimate the SF e is updated using 1-step TD error as described in (1) where we\nuse a state-only base-feature function. To update the actor network \u03c0\u03bc, we first show how SFM\nestimates the SF of the current policy under the initial state distribution.\nProposition 1. Let B denote a buffer of trajectories sampled from arbitrary stationary Markovian\npolicies in the given MDP with initial state distribution Po. For any deterministic policy \u03c0,\n$\\hat{\\psi}^{\\pi} := E_{s \\sim P_0} [\\psi^{\\pi} (s,\\pi(s))] = (1 - \\gamma)^{-1}E_{(s_t,s_{t+1}) \\sim B} [\\psi^{\\pi} (s_t, \\pi(s_t)) - \\gamma\\psi^{\\pi} (s_{t+1},\\pi(s_{t+1}))].\\tag{6}$\nThe proof of proposition 1 is deferred to Appendix A. Proposition 1 presents a method for estimating\nthe SF for the agent conditioned on the initial state distribution. The proposed derivation can utilize\nsamples coming from a different state-visitation distribution and uses an off-policy replay buffer in\nthis work. Similar to standard off-policy RL algorithms (Fujimoto et al., 2018; 2023; Haarnoja et al.,\n2018), SFM maintains a replay buffer B to store the transitions and use it for sampling. This buffer\nallows us to make good use of all state transitions for the purpose of estimating the initial-state\nsuccessor features with temporal difference learning.\nNote that the potential function defined in (4) depends only on the initial state distribution and does\nnot specify a way of updating the actor for any other state. By substituting (6) into (4), we express\nthe potential function representing the gap between the expected features of the agent and those of\nthe expert in terms of features at states visited by the agent (and not just initial states). Thus, we\ndefine the loss LG for the actor, which we call the SF-Gap loss, according to\n$L_G(\\mu) := \\frac{1}{2} ||(1 - \\gamma)^{-1}E_{s,s'\\sim B}[\\psi_{\\theta}(s, \\pi_{\\mu}(s)) - \\gamma\\psi_{\\theta}(s', \\pi_{\\mu}(s'))] - \\hat{\\psi}^E||_2^2\\tag{7}$"}, {"title": "4.2 BASE FEATURE FUNCTION", "content": "We described in section 3 that SF depends on a base feature function 4 : S \u2192 Rd. In this work, SFM\nlearns the base features jointly while learning the policy. Base feature methods are parameterized\nby pairs feat = ($, f) together with losses Lfeat, where \u00a2 : S \u2192 Rd is a state feature map, f is an\nauxiliary object that may be used to learn 4, and Lfeat is a loss function defined for $ and f. Below,\nwe briefly outline the base feature methods considered in our experiments.\nRandom Features (Random): Here, & is a randomly-initialized neural network, and f is discarded.\nThe network & remains fixed during training (Lfeat = 0).\nInverse Dynamics Model (Pathak et al., 2017, IDM): Here, f : Rd \u00d7 Rd \u2192 A is a function that\ntries to predict the action that lead to the transition between embeddings of consecutive states. The\nloss Lfeat is given by the IDM loss LIDM,\n$L_{IDM}(\\theta_{feat}) = E_{(s,a,s')\\sim D} [||f(\\phi(s), \\phi(s')) - a||_2^2], \\theta_{feat} = (\\phi, f).\\tag{9}$\nForward Dynamics Model (FDM): Here, f : Rd \u00d7 A \u2192 S is a function that tries to predict the\nnext state in the MDP given the embedding of the current state and the chosen action. The loss Lfeat"}, {"title": "5 EXPERIMENTS", "content": "Through our experiments, we aim to analyze (1) how well SFM performs relative to competing\nnon-adversarial and state-only adversarial methods at imitation from a single expert demonstration,\n(2) the robustness of SFM and its competitors to their underlying policy optimizer, and (3) which\nfeatures lead to strong performance in SFM. Our results are summarized in Figures 2, 4, and 6,\nrespectively, and are discussed in the remainder of this section.\nUltimately, our results confirm that SFM indeed outperforms its competitors, achieving state-of-the-\nart performance on a variety of single-demonstration tasks, and even surpassing the performance\nof agents that have access to expert actions."}, {"title": "5.1 EXPERIMENTAL SETUP", "content": "We evaluate our method on the 10 environments from the DeepMind Control (DMC) (Tassa et al.,\n2018) suite. Following the investigation in (Jena et al., 2020) which showed that the IRL algorithms\nare prone to biases in the learned reward function, we consider infinite horizon tasks where all\nepisodes are truncated after 1000 steps in the environment. For each task, we collected expert\ndemonstrations by training a TD3 (Fujimoto et al., 2018) agent for 1M environment steps. In our\nexperiments, the agent is provided with a single expert demonstration which is sampled at the start\nand kept fixed during the training phase. The agents are trained for 1M environment steps and we\nreport the mean performance across 10 seeds with 95% confidence interval shading and Rliable"}, {"title": "5.2 RESULTS", "content": "Quantitative Results Figure 2 presents the Rliable plots (Agarwal et al., 2021) obtained across\nDMC environments. We observe that the proposed method SFM learns to solve the task with a\nsingle demonstration and significantly outperforms the offline method BC (Pomerleau, 1988) and"}, {"title": "6 LIMITATIONS", "content": "One limitation of SFM is that the algorithm is currently tied with a particular choice of RL solvers,\ni.e. deterministic policy gradients. We believe our approach can be extended to a broader set of\nsolvers that optimize both deterministic and stochastic policies. Secondly, our method only works\nwith state-only base feature functions. We believe future work can lift this assumption by doing on-\npolicy over a learned world model. Finally, while SFM is simpler than IRL methods, it still doesn't\ntheoretically alleviate the exploration problem that IRL methods encounter. A promising direction\nof future work would be to combine SFM with mechanisms like reset distribution (Swamy et al.,\n2023) or hybrid IRL (Ren et al., 2024) to improve computational efficiency."}, {"title": "7 DISCUSSION", "content": "We introduced SFM-a novel non-adversarial method for IRL that requires no expert action labels-\nv\u00eda a reduction to a deterministic policy gradient algorithm. Our method learns to match the expert's\nsuccessor features, derived from adaptively learned base features, using direct policy optimization\nas opposed to solving a minimax game. Through experiments on several standard imitation learn-\ning benchmarks, we have shown that state-of-the-art imitation is achievable with a non-adversarial\napproach, thereby providing an affirmative answer to our central research question.\nConsequently, SFM is no less stable to train than its online RL subroutine. This is not the case with\nadversarial methods, which involve complex game dynamics during training. Much like the rich\nliterature on GANS (Goodfellow et al., 2014; Gulrajani et al., 2017; Kodali et al., 2018), adversarial\nIRL methods often require several tricks to stabilize the optimization, such as gradient penalties,\nspecific optimizers, and careful hyperparameter tuning.\nBeyond achieving state-of-the-art performance, SFM demonstrated an unexpected feat: it is excep-\ntionally robust to the policy optimization subroutine. Notably, when using the weaker TD3 policy\noptimizer, SFM performs almost as well as it does with a strong state-of-the-art TD7 optimizer.\nThis is in stark contrast to the baseline methods, which performed considerably worse under the\nweaker policy optimizer. As such, we expect that SFM can be broadly useful and easier to deploy\non resource-limited systems, which is often a constraint in robotics applications."}, {"title": "A PROOFS", "content": "Before proving Proposition 1, we begin by proving some helpful lemmas. First, we present a simple\ngeneralization of a result from Garg et al. (2021).\nLemma 1. Let \u00b5 denote any discounted state-action occupancy measure for an MDP with state\nspace S and initial state distribution Po, and let V denote a vector space. Then for any f : S \u2192 V,\nthe following holds,\n$E_{(s,a)\\sim\\mu} [f(s) - E_{s'\\sim P(:\\s,a)}[f(s')]] = (1 - \\gamma)E_{s\\sim P_0} [f(s)].$\nProof. Firstly, any discounted state-action occupancy measure \u03bc is identified with a unique policy\n\u03c0h as shown by Ho & Ermon (2016). So, \u03bc is characterized by\n$\\mu(dsda) = (1 - \\gamma)\\pi^{\\mu} (da | s) \\sum_{t=0}^{\\infty}\\gamma^tp_t(ds),$\nwhere pt(S) = Pr\u3160\u03bc(St \u2208 S) is the state-marginal distribution under policy \u03c0\u201c at timestep t.\nExpanding the LHS of the proposed identity yields\n$E_{(s,a)\\sim\\mu} [f(s) - (1 - \\gamma)E_{s'\\sim P(:\\s,a)}[f(s')]]$\n$= (1 - \\gamma) \\sum_{t=0}^{\\infty}\\gamma^tE_{s\\sim p_t} [f(s)] - \\gamma E_{(s,a)\\sim\\mu}E_{s'\\sim P(:\\s,a)} [f(s')]$\n$= (1 - \\gamma) \\sum_{t=0}^{\\infty}\\gamma^tE_{s\\sim p_t} [f(s)] - (1 - \\gamma) \\sum_{t=0}^{\\infty}\\gamma^{t+1}E_{s\\sim p_t} E_{a\\sim \\pi^{\\mu} (:\\s)} E_{s'\\sim P(:\\s,a)} [f(s')]$\n$= (1 - \\gamma) \\sum_{t=0}^{\\infty}\\gamma^tE_{s\\sim p_t} [f(s)] - (1 - \\gamma) \\sum_{t=0}^{\\infty}\\gamma^{t+1}E_{s\\sim p_{t+1}}[f(s)]$\n$= (1 - \\gamma) \\sum_{t=0}^{\\infty}\\gamma^tE_{s\\sim p_t} [f(s)] - (1 - \\gamma) \\sum_{t=1}^{\\infty}\\gamma^tE_{s\\sim p_t} [f(s)]$\n$= (1 - \\gamma)E_{s\\sim P_0}[f(s)],$\nsince p = Po (the initial state distribution) for any \u03bc.\nIntuitively, we will invoke Lemma 1 with f denoting the successor features to derive an expression\nfor the initial state successor features via state transitions sampled from a replay buffer.\nProposition 1. Let B denote a buffer of trajectories sampled from arbitrary stationary Markovian\npolicies in the given MDP with initial state distribution Po. For any deterministic policy \u03c0,\n$\\hat{\\psi}^{\\pi} := E_{s \\sim P_0} [\\psi^{\\pi} (s,\\pi(s))] = (1 - \\gamma)^{-1}E_{(s_t,s_{t+1}) \\sim B} [\\psi^{\\pi} (s_t, \\pi(s_t)) - \\gamma\\psi^{\\pi} (s_{t+1},\\pi(s_{t+1}))].\\tag{6}$\nProof. Suppose B contains rollouts from policies {\u03c0\u03ba}}=1 for some N \u2208 N. Each of these policies\n\u03c0\u03b5 induces a discounted state-action occupancy measure \u03bc\u03ba. Since the space of all discounted state-\naction occupancy measures is convex (Dadashi et al., 2019), it follows that \u00b5 = \u03a3\u03ba=1 \u03bc\u03b5 is itself\na discounted state-action occupancy measure.\nConsider the function f : S \u2192 Rd given by f(s) = \u03c8", "P(": "s_t,a_t)} [f(s_t) - \\gamma f(s_{t+1})]$\n$= E_{(s_t,a_t)\\sim \\mu,s_{t+1}\\sim P(:\\s_t,a_t)}[f(s_t) - \\gamma f(s_{t+1})]$\n$= E_{(s,a)\\sim \\mu} [f(st) - E_{s'\\sim P(:\\s,a)}[f(s')]]$\n$= (1 - \\gamma)E_{s\\sim P_0}[f(s)],$\nwhere the final step invokes Lemma 1, which is applicable since u is a discounted state-action\noccupancy measure. The claim then follows by substituting f(s) for \u03c8", "by": "n$\\nabla_{\\mu}L_G(\\mu) = \\frac{1}{2}\\sum_{i=1}^{d}2 (\\sum_{t=0}^{d}(1 - \\gamma)^{-1}E_{s,s'\\sim B}[\\psi_{\\theta}(s, \\pi_{\\mu}(s)) - \\gamma\\psi_{\\theta}(s', \\pi_{\\mu}(s'))] - \\hat{\\psi}^E) \\nabla_{\\mu} \\{\\sum_{t=0}^{d}(1 - \\gamma)^{-1}E_{s,s'\\sim B}[\\psi_{\\theta}(s, \\pi_{\\mu}(s)) - \\gamma\\psi_{\\theta}(s', \\pi_{\\mu}(s'))] - \\hat{\\psi}^E}\\}$\n$\\ = \\sum_{i=1}^{d}2(1 - \\gamma)^{-1}E_{s,s'\\sim B}[\\psi_{\\theta_i}(s, \\pi_{\\mu}(s)) - \\gamma\\psi_{\\theta_i}(s', \\pi_{\\mu}(s'))] - \\hat{\\psi}^E_i\\} \\sum_{i=1}^{d}(1 - \\gamma)^{-1}E_{s,s'\\sim B}[\\nabla_{\\mu}\\psi_{\\theta_i}(s, \\pi_{\\mu}(s))\\]\n$\\ = \\sum_{i=1}^{d}2(1 - \\gamma)^{-1}E_{s,s'\\sim B}[\\psi_{\\theta_i}(s, \\pi_{\\mu}(s)) - \\gamma\\psi_{\\theta_i}(s', \\pi_{\\mu}(s'))] - \\hat{\\psi}^E_i\\} \\sum_{i=1}^{d}(1 - \\gamma)^{-1}E_{s,s'\\sim B}[\\nabla_{\\alpha}\\psi_{\\theta_i}(s, \\alpha)]_{\\alpha = \\pi_{\\mu}(s)}\\]\nHere, we defined zi = (1 \u2013 \u03b3)\u00af\u00b9Es,s'\u223cB[Yo(s,\u03c0\u03bc(s)) \u2013 \u03b3\u03c8\u0113(s', \u03c0\u03bc(s'))] \u2013 \u0177\u0395. This completes\nthe proof.\nProposition 3. Let \u20ac > 0 and let \u00b5 be a policy parameter such that || - ||2 < \u20ac. Suppose the\nexpert policy is optimal for the reward function r(s) = w$(s) for base features $(s) \u2208 Rd and\n||w||2 \u2264 B for B < \u221e. Then it holds that J(\u03c0\u03b5) \u2013 J(\u03c0\u03bc) \u2264 \u0392\u03b5.\nProof. Notably, we have that J(\u03c0) = w\u00af"}, {"title": "B IMPLEMENTATION DETAILS", "content": "Since SFM does not involve estimating a reward function and cannot leverage an off-the-shelf RL\nalgorithm to learn a Q-funtion, we propose a novel architecture for our method. SFM is composed\nof 3 different components- actor \u03c0\u03bc, SF network o, base feature function & and f. Taking inspi-\nration from state-of-the-art RL algorithms, we mantain target networks for both actor and the SF\nnetwork. Since, SF network acts similarly to a critic in actor-critic like algorithms, SFM comprises\nof two networks to estimate the SF (Fujimoto et al., 2018). Here, instead taking a minimum over\nestimates of SF from these two networks, our method performed better with average over the two\nestimates of SF. To implement the networks of SFM, we incorporated several components from the\nTD7 (Fujimoto et al., 2023) algorithm. Moreover, unlike MM (Swamy et al., 2021), SFM did not\nrequire techniques like gradient penalty (Gulrajani et al., 2017), the OAdam optimizer (Daskalakis\net al., 2017) and a learning rate scheduler."}, {"title": "B.1 NETWORK ARCHITECTURE", "content": "The architecture used in this work is inspired from the TD7 (Fujimoto et al., 2023) algorithms for\ncontinuous control tasks (Pseudo 2). We will describe the networks and sub-components used below:\nEncoder: The encoder comprises of two sub-networks to output state and state-action embedding,\nsuch that z = fv(s) and zsa = gv (z\u00b3, a). The encoder is updated using the following loss:\n$L_{Encoder} (f_v, g_v) = (g_v (f_v(s), a) - |f_v(s')|_x)^2$\n$= (z^{sa} - |z^s|_x)^2,$\nwhere s, a, s' denotes the sampled transitions and | . |\u00d7 is the stop-gradient operation. Also, we\nrepresent z = ft(s), zsa = g\u2081(\u017e\u00ba, a), \u017e\u00ba = ft(s), and sa = g(, a).\nSF network: Motivated by standard RL algorithms (Fujimoto et al., 2018; 2023), SFM uses a pair\nof networks to estimate the SF. The network to estimate SF are updated with the following loss:\n$L_{SF}(\\psi_{\\theta_i}) = ||target - \\psi_{\\theta_i} (z^{sa}, z^s, s, a)||_2,$\ntarget = $(s) + \\frac{1}{2} * clip([\\psi_{\\theta_1}(x) + \\psi_{\\theta_2}(x)], \\psi_{min}, \\psi_{max}),$\nx = [s'a', ', s', a']\na' = \u03c0\u03bc(8', s') + \u20ac, where \u0454 ~ clip(N(0, \u03c3\u00b2), \u2212c, c).\nHere, instead of taking the minimum over the SF networks for bootstrapping at the next state (Fuji-\nmoto et al., 2018), the mean over the estimates of SF is used. The action at next state a' is samples\nsimilarly to TD3 (Fujimoto et al., 2018) and the same values of (z, zsa) are used for each SF net-\nwork. Moreover, the algorithm does clipping similar to TD7 (Fujimoto et al., 2023) on the predicted\nSF at the next state which is updated using target (equation 16) at each time step, given by:\n$\\psi_{min} \\leftarrow min(\\psi_{min}, target)$\n$\\psi_{max} \\leftarrow min(\\psi_{max}, target)$\nPolicy: SFM uses a single policy network which takes [z, s] as input and is updated using the\nfollowing loss function described in section 4.\nUpon every target_update_frequency training steps, the target networks are updated by cloning\nthe current network parameters and remains fixed:\n$(0_1,0_2) \\leftarrow (0_1, 0_2)$\n$\\mu_{\\Theta} \\leftarrow \\mu_{\\Theta}$\n$(V_1, V_2) \\leftarrow (V_1, V_2)$\n$(V_1, V_2) \\leftarrow (V_1, V_2)$\nMoreover, the agent maintains a checkpointed network similar to TD7 (Refer to Appendix F of\nTD7 (Fujimoto et al., 2023) paper). However, TD7 utilizes the returns obtained in the environment"}, {"title": "C HYPERPARAMETERS", "content": "In Table 1, we provide the details of the hyperparameters used for learning. Many of our hyper-\nparamters are similar to the TD7 (Fujimoto et al., 2023) algorithm. Important hyperparameters\ninclude the discount factor y for the SF network and tuned it with values y = [0.98, 0.99, 0.995"}]}