{"title": "Robustness and Cybersecurity in the EU Artificial Intelligence Act", "authors": ["Henrik Nolte", "Miriam Rateike", "Mich\u00e8le Finck"], "abstract": "The EU Artificial Intelligence Act (AIA) establishes different legal\nprinciples for different types of Al systems. While prior work has\nsought to clarify some of these principles, little attention has been\npaid to robustness and cybersecurity. This paper aims to fill this\ngap. We identify legal challenges and shortcomings in provisions\nrelated to robustness and cybersecurity for high-risk Al systems\n(Art. 15 AIA) and general-purpose AI models (Art. 55 AIA). We show\nthat robustness and cybersecurity demand resilience against perfor-\nmance disruptions. Furthermore, we assess potential challenges in\nimplementing these provisions in light of recent advancements in\nthe machine learning (ML) literature. Our analysis informs efforts\nto develop harmonized standards, guidelines by the European Com-\nmission, as well as benchmarks and measurement methodologies\nunder Art. 15(2) AIA. With this, we seek to bridge the gap between\nlegal terminology and ML research, fostering a better alignment\nbetween research and implementation efforts.", "sections": [{"title": "1 Introduction", "content": "The European Union (EU) recently adopted the Artificial Intelli-\ngence Act (AIA)\u00b9 which creates a legal framework for the develop-\nment, deployment, and use of \"human-centered and trustworthy\nartificial intelligence (AI)\u201d (Art. 1 AIA). The AIA outlines desirable\n\"ethical principles\u201d of AI systems (Rec. (27)) and, i.a., imposes some\nof these as legally binding requirements for high-risk AI systems\n(HRAIS), e.g, Al systems intended to be used to take university ad-\nmission decisions, or to evaluate individuals' creditworthiness, and\nfor general-purpose AI models (GPAIMs), e.g., multimodal large lan-\nguage models. While the AIA is recognized as being one of the first\nlegally binding regulatory frameworks for AI [16], it has faced crit-\nicism for its imprecise and incoherent terminology [7, 50], which\nwill complicate its practical implementation. Previous work has ex-\namined the AIA and its legislative history to clarify terms like ex-\nplainability [8, 71, 98] and fairness [27]. So far, little attention has\nbeen paid to robustness and cybersecurity. Only Al systems classi-\nfied as high-risk (HRAIS) must meet the robustness and cybersecu-\nrity requirements set out in Art. 15 AIA. This paper thus focuses\non requirements for HRAIS. To provide a clearer understanding,\nwe compare these requirements with requirements for specific Al\nmodels, namely for GPAIMs with systemic risk, in Art. 55 A\u0399\u0391."}, {"title": "2 An ML Perspective on Robustness and Cybersecurity", "content": "ML research on robustness focuses on mitigating undesired changes\nin model outputs when deploying models in real world scenar-\nios [86]. This issue is explored across different applications such as\ncomputer vision [28, 29, 93] and natural language processing [15,\n49]. Unintended changes in model outputs can occur due to adver-\nsarial or non-adversarial factors affecting the ML model, its input\n(test) data, or its training data [20, 95].\nPerturbations of input (test) data often present a difficult chal-\nlenge. While a model's output may be as expected\nwhen using \"safe\" test data from the original population distribu-\ntion, unintended changes can occur when perturbed examples are\nprovided as input to the ML model.\nNon-adversarial (or natural) robustness often addresses changes\nin ML model outputs due to distribution shifts in input data [35, 95].\nThese changes occur when the distribution from which the test\ndata is sampled differs from that of the training data [29, 93]. For\ninstance, alterations in data collection methods, such as upgrading\nto a new X-ray machine, can modify the format or presentation\nof images [14, 34]. Importantly, distribution shifts can also result\nfrom feedback loops, where the ML model's outputs influence the\ndata distribution, creating a cycle from the model's output back\nto its input [24, 112]. Such an effect can be found, for example, in\nmovie recommendation systems, where user's preferences change\nover time in response to the ML system's suggestions, thereby\ninfluencing future recommendations [73]. Other forms of research\non non-adversarial robustness investigate the robustness of ML\nmodels to noise, which frequently occurs in real-world data sets [66,\n83].\nAdversarial robustness refers to the study and mitigation of model\nevasion attacks using adversarial examples. These are data samples\ntypically drawn from the original population distribution and then\nmodified by an adversary-often in ways that are difficult or even\nimpossible to detect through human oversight-with the intent of\naltering a model's output [92]. This phenomenon can occur across\nvarious models and data types. For instance, in the image domain,\nsmall pixel perturbations in input images can lead to significant\nchanges in a model's output [92]. In a broader sense, adversarial\nrobustness also encompasses the study and mitigation of other\nforms of adversarial attacks that attempt to extract the model or\nreconstruct or perturb the training data set [19, 62].\nFrom a technical standpoint, adversarial robustness is one as-\npect of cybersecurity. Research on cybersecurity focuses on devel-\noping defenses that protect computer systems from attacks com-\npromising their confidentiality, integrity, or availability [26]. This\nencompasses aspects like data storage, information access and mod-\nification, and secure data transmission over networks [84]. Unlike\nrobustness, cybersecurity is not a stand-alone concept in ML, but\nis discussed more broadly as both a tool for ensuring cybersecu-\nrity and a potential source of cybersecurity risks. ML algorithms\ncan be employed to detect and mitigate cybersecurity threats [84],\nbut can also introduce specific vulnerabilities that adversaries may\nexploit, such as data poisoning or adversarial attacks [81, 82]."}, {"title": "3 Background on the AIA and Art. 15 AIA", "content": "AIA. The AIA creates harmonized rules for certain Al systems in\norder to incentivize the use of such systems in the EU market and\nprevent regulatory fragmentation between member states (Rec. (1)).\nIt is formally structured into recitals (Rec.), articles (Art.), and an-\nnexes. Recitals are not strictly legally binding and outline the ratio-\nnale behind the articles, articles delineate specific binding obliga-\ntions, and the annexes provide additional details and specifications\nto support the articles [45]. Art. 3(1) AIA defines an Al system as\n\"a machine-based system that is designed to operate with varying\nlevels of autonomy and that may exhibit adaptiveness after deploy-\nment, and that, for explicit or implicit objectives, infers, from the in-\nput it receives, how to generate outputs [...] that can influence phys-\nical or virtual environments\". These AI systems are regulated differ-\nently based on their perceived risk level [7, 90]: Those posing unac-\nceptable risks, such as social scoring, are prohibited or subject to\nqualified prohibitions; HRAIS, such as those used in medical devices,\nare allowed but must comply with certain requirements and un-\ndergo pre-assessment; other Al systems are subject only to specific\ntransparency and information obligations. Among these categories,\nonly HRAIS must fulfill the robustness and cybersecurity require-\nments under Art. 15 AIA. According to Art. 16(a) AIA, providers\nof HRAIS must ensure compliance with these requirements. An AI\nsystem is considered a HRAIS if it is either a safety component of a\nproduct or a product itself regulated under specific legislation, such\nas medical devices, machinery, or toys (Art. 6(1) AIA, Annex I), or\nif it poses a significant risk of harm to the health, safety, or fun-\ndamental rights of individuals in specific areas, such as education,\nemployment, or law enforcement (Art. 6(2) and (3) AIA, Annex III).\nIn addition to AI systems, the AIA establishes a separate regime\nof legal requirements in chapter V of the AIA for a very specific\ntype of AI models, namely GPAIM (e.g., multimodal large language\nmodels, see also Section 6). GPAI models are Al models that can\nperform tasks that they were not originally trained for [38], such as\nlarge language models [33, 67], or large text-to-image models [78].\nOther types of Al models are not regulated and mentioned in the\nAIA. These are models that are created with a specific objective\nand can only accomplish tasks they are trained to perform (e.g.,\ntranslation, classification).\nThe AIA does not directly define specific technical requirements.\nInstead, it sets out 'essential requirements' that AI systems must\ncomply with, and which are concretized by so-called technical stan-\ndards. The regulatory concept of relying on standardization is a\nwell-established process in EU product legislation and is referred\nto as the New Legislative Framework [37]. It traditionally involves\nthe participation of stakeholders, such as providers of AI systems.\nThe European Commission has already issued a standardization"}, {"title": "4 The Purpose of Art. 15 \u0391\u0399\u0391", "content": "Art. 15(1) AIA states that high-risk AI systems must achieve an \"ap-\npropriate level of accuracy, robustness and cybersecurity\" and must\nfunction consistently in this respect throughout their lifecycle. It is\none of seven provisions in the AIA that sets specific requirements\nfor high-risk AI systems. These requirements aim to ensure product\nsafety and a \"consistent and high level of protection of public inter-\nests as regards health, safety and fundamental rights\" (Rec. (7)). Par-\nticularly, Art. 15(1) AIA outlines specific product-related require-\nments for AI systems, detailing how they must be \"designed and\ndeveloped to achieve trustworthy AI. This is emphasized by the\n2019 Ethics Guidelines for Trustworthy AI developed by the AI In-\ndependent High-level Expert Group (AI IHEG) on AI [1] appointed\nby the European Commission which can be seen as the conceptual\nbasis of the AIA (Rec. (27)). These guidelines state that technical\nrobustness ensures that AI systems \"reliably behave as intended\nwhile minimizing unintentional and unexpected harm, and prevent-ing unacceptable harm\".\nThe question arises as to why it is even necessary to regulate\naccuracy, robustness and cybersecurity of AI systems. One could\nassume that it is in the best interest of an economic actor to fulfill\nthese requirements in the best possible way to gain a market advan-\ntage. The European Commission's impact assessment of the AI Act\nrecognizes this thought with respect to accuracy and robustness,\nstating that \"an economic operator [...] would anyway have to en-\nsure that their product actually works\" [31, Annex 4]. However, the\nimpact assessment clarifies that \"is important that these require-\nments are included in the regulatory framework so that substan-\ndard operators need to improve their procedures\" [31, Annex 4].\nTherefore, the purpose of the specific requirements set out in\nArt. 15 AIA is to achieve the overarching objective of ensuring the\ntrustworthiness of AI systems (see Art. 1(1) AIA) and to advance\nthe cybersecurity agenda of the EU."}, {"title": "5 Requirements for High-Risk AI Systems", "content": "In this section, we provide an analysis of the overarching challenges\nof implementing Art. 15 AIA, followed by a discussion\nregarding the robustness requirement in Art. 15(4) AIA and the cybersecurity requirement in Art. 15(5) AIA.\n5.1 General Challenges of Art. 15 AIA\nWe identify four legal challenges related to Art. 15 AIA that may\narise in its practical implementation. First, there is no clear delin-\neation of the legal terms of robustness and cybersecurity and its\ncounterparts in ML literature. Second, while the AIA mandates\ncompliance for entire Al systems, the ML literature primarily fo-\ncuses on models, which may pose practical challenges for imple-\nmentation. Third, while accuracy is specified as a requirement in\nArt. 15 AIA, the provision does not clarify its role in measuring\nrobustness and cybersecurity. Fourth, the terms 'lifecycle' and 'con-\nsistent' performance are not defined, leaving ambiguity about how\nsuch performance can be practically ensured.\nRobustness and Cybersecurity. The robustness requirement in\nArt. 15(4) AIA addresses \"errors, faults, or inconsistencies\u201d that may\ninadvertently occur as the system interacts with its real-world envi-\nronment. In contrast, the cybersecurity requirement in Art. 15(5) \u0391\u0399\u0391\ntargets deliberate attempts \"to alter the use, outputs, or perfor-\nmance\" of an Al system \"by malicious third parties exploiting the\nsystem's vulnerabilities\u201d. Both robustness and cybersecurity re-\nquirements aim to ensure that HRAIS perform consistently and\nare resilient against any factors that might compromise this per-\nformance. They, however, address different threats to consistent\nperformance: robustness requires protection against unintentional\ncauses, whereas cybersecurity protects against intentional actions.\nWhile robustness is a new term in EU legislation and not explicitly\ndefined in the AIA, the term cybersecurity has already been defined\nin the EU Cybersecurity Act (CSA). Art. 2(1) CSA provides a broad\ndefinition of cybersecurity, covering all \u201cthe activities necessary to\nprotect network and information systems, the users of such sys-\ntems, and other persons affected by cyberthreats\". According to the\nCSA, a cyber threat is any potential circumstance, event or action\nthat could damage, disrupt or otherwise adversely impact network\nand information systems, the users of such systems and other per-\nsons (Art. 2(8) CSA). Importantly, the CSA does not distinguish be-\ntween intentional or unintentional cyberthreats as causes of harm.\nRather, both are explicitly included in the scope of the CSA (see,\ne.g., Art. 51(1)(a) and (b) CSA). However, the AIA artificially splits\nthe CSA's concept of cybersecurity by designating unintentional\ncauses as a matter of robustness and restricting cybersecurity to\nintentional actions. This creates a conflict when aligning the AIA's\nrequirements with the CSA's definition of cybersecurity that may\nlead to regulatory ambiguity. Specifically, Art. 42(2) AIA considers\nHRAIS with CSA certification or conformity declarations as com-\npliant with cybersecurity requirements in Art. 15 AIA. This sug-\ngests that the CSA definition of cybersecurity applies to the AIA,\neven though it inherently covers both types of causes.\nWe explore how these legal terms could be understood within\nthe ML domain proposing a simple model as an explanatory heuris-\ntic (see Figure 2). In ML, robustness refers to maintaining consistent\nmodel performance in real-world scenarios [86]. ML research dis-\ntinguishes between different types of robustness. Non-adversarial\nrobustness in ML refers to a model's ability to maintain performance\ndespite data shifts or noise [35, 66, 83, 95]. This aligns with the legal\nterm robustness in the AIA. Adversarial robustness in ML refers to\nthe model's resistance to intentional perturbations aimed at alter-\ning predictions [92]. This aspect aligns more with the legal concept\nof cybersecurity. The cybersecurity requirement in Art. 15(5) \u0391\u0399\u0391\naims to ensure Al systems' integrity, confidentiality, and availabil-\nity, protecting them from threats like unauthorized access, adver-\nsarial manipulation, data modification, Denial-of-Service attacks,\nand theft of sensitive information (e.g., model weights). However,\nother scenarios within the ML domain may also fall under the rele-\nvant legal terms. For example, language model jailbreaks exploit\nAl vulnerabilities to bypass safety constraints [97, 101]. This aligns\nmore closely with the notion of cybersecurity in protecting against\nmisuse of Al systems.\nOur findings are supported by an historic analysis of the legisla-\ntion process. As outlined in Section 4, the AIA builds on the Ethics\nGuidelines for Trustworthy AI [1]. In the guidelines, the principle\nof 'technical robustness and safety' includes resilience against at-\ntacks, but does not mention cybersecurity. The White Paper on Ar-\ntificial Intelligence [30], which elaborates on these guidelines, still\nlists resilience to attacks against AI systems under \"robustness and\naccuracy\" without differentiating those terms from cybersecurity.\nThe first official draft of the AIA by the European Commission\nwas the first official document to distinguish between these three\nterms and assigned \"resilience against attacks\" to cybersecurity\nrather than robustness. Rec. (27), which refers to the IHGE guide-\nlines, seems to be a remnant of this development process. It de-\nmands under the term 'technical robustness' that Al systems should\nbe resilient \"against attempts to alter the use or performance of the\nAl system\", essentially asking for adversarial robustness.\nSystem vs. Model. The AIA regulates AI systems, but not AI mod-\nels, with the only exception being GPAIMs. ML research, in contrast,\noften focuses on developing technical solutions for ML models. This\nraises the question of whether solely relying on technical solutions\nfor ML models is enough to ensure the compliance of a HRAIS with\nArt. 15 AIA-or whether additional measures are needed. Rec. (97)\nspecifies that an Al model is an essential component of an AI sys-\ntem. Additional components can include, i.a., user interfaces, sen-\nsors, databases, network communication components, or pre- and\npost-processing mechanisms for model in- and outputs (Rec. (97),\n[43]). All these individual components should contribute to the\noverall robustness of the AI system, particularly in scenarios where\nsome components may fail. This is illustrated by Art. 15(4)(ii) AIA,\nwhich states that robustness may be ensured through technical re-\ndundancy solutions, including \u201cback-up or contingency plans\". Fur-\nthermore, Art. 15(5)(iii) AIA stipulates that the cybersecurity of AI\nsystems shall be achieved through technical solutions that, \"where\nappropriate\", target training data, pre-trained components, the AI\nmodel or its inputs. This binding provision suggests that at least\nthese different components of the AI system are required to be as-\nsessed individually for their appropriateness in mitigating cyberse-\ncurity attacks. Thus, Art. 15 AIA should not be understood as re-\nquiring a single, unified assessment of the requirements. Instead, it\nmust be interpreted as mandating that each component, including\none or more ML models, be assessed individually. The assessment\nof the AI system's overall performance is then derived from an ag-\ngregation of the individual performance results [47]. This requires\nan interdisciplinary approach that draws on expertise from fields\nsuch as ML, engineering, and human-computer interaction. To es-tablish a common understanding, it can prove beneficial to formally\ndescribe the evaluation process of an entire Al system, including po-\ntential challenges, such as interdependencies of technical solutions.\nRole of Accuracy. Art. 15(1) AIA mandates that HRAIS shall\n\"achieve an appropriate level of accuracy\". This is important be-\ncause trade-offs between different desiderata can exist, such as be-\ntween robustness and accuracy (see Appendix E). While accuracy\nis not defined in the AIA, Annex IV No. 3 AIA states that accuracy\nis an indicator of the capabilities and performance limits of an AI\nsystem. Accordingly, accuracy should be measured in at least two\nways: i) separately for \"specific persons or groups of persons on\nwhich the system is intended to be used\", and ii) the overall ex-\npected accuracy for the \"intended purpose\" of the AI system. In ML,\nthe metric accuracy typically describes the overall proportion of\ncorrect predictions out of the total number of predictions made [12].\nHowever, the term can also describe the objective of \"good perfor-\nmance\" of an Al system and, depending on its specific purpose, can\nalso be evaluated using different metrics, such as utility [22] and\nf1-score [91]. Art. 15(3) AIA explicitly references 'accuracy and the\nrelevant accuracy metrics', indicating that accuracy is understood\nas an objective that can be measured with various metrics, leaving\nthe choice of the relevant metric to the provider. The selection of\nthe metric should consider various factors, including the specific\npurposes of the ML model, dataset-specific circumstances (e.g., im-\nbalanced data) and the particular model type (e.g., classification,\nregression). Technical standards and guidelines by the EU Commis-\nsion should clarify how AI systems' accuracy should be measured.\nIn ML, robustness is often measured using an accuracy metric.\nTypically, this involves comparing the accuracy (or error rates) eval-\nuated on an unperturbed dataset from the original distribution with\nthe accuracy on a perturbed test set (e.g., sampled from the shifted\ndistribution or containing adversarial samples) [36, 41, 93]. The\nsmaller the difference between these two accuracy results, the better\nthe robustness. The choice of the accuracy metric thus has an impact\non the measurement of robustness. As a result, the ML model may\nappear more robust under some accuracy metrics than others. The\nselection of favorable metrics has been studied in fair ML under the\nterm fairness hacking [6, 58, 89]. Without entering into the debate,\nwe note that there is an ongoing discussion in the ML literature\nabout the existence and characteristics of a trade-off between robust-\nness and accuracy. While some research showed that enhancing ro-\nbustness leads to a drop in test accuracy [76, 96, 111], others believe\nthat robustness and accuracy are not conflicting goals and can be\nachieved simultaneously [77, 107]. Technical standards and guide-\nlines by the EU Commission should provide instructions on how AI\nsystem providers should choose an appropriate 'accuracy' measure,\nespecially when it is used to assess robustness in subsequent steps.\nConsistent Performance Throughout the Lifecycle. Al systems\nmust perform \"consistently\u201d in terms of accuracy, robustness, and\ncybersecurity \"throughout their lifecycle\" (Art. 15(1) AIA). Perfor-\nmance is the \"ability of an AI system to achieve its intended purpose\"\n(Art. 3(18) \u0391\u0399\u0391). However, i) the term 'lifecycle' is not defined, creat-\ning ambiguity about whether it differs from the term 'lifetime' used\nin Art. 12(1) AIA and Rec. (71); ii) the concept of 'consistent' perfor-\nmance is unclear, and it is not specified how it should be measured.\nFirst, 'lifecycle' and 'lifetime' could be understood as synonyms\n[57]. On the other hand, the term 'lifetime' could be understood\nto refer specifically to the active period of the AI system in opera-\ntion [60], while 'lifecycle' could encompass a broader view of all\nphases from product design and development to decommission-\ning [40]. In this case, however, it is unclear how accuracy, robust-\nness and cybersecurity should be ensured beyond the operational\nphase (e.g., during development). Art. 2(8) AIA clarifies that these\nrequirements do not have to be met during the test and develop-\nment phase of the HRAIS-unless the system is tested under real\nworld conditions. However, the use of the term 'lifecycle' might be\ninterpreted to suggest that the requirements of Art. 15 AIA should\nnot only be assessed when the system is ready for deployment but\nalso be considered during design process itself.\nSecond, it is unclear what 'consistent' performance means and\nhow it should be measured. In the ML literature, a model's variabil-\nity in performance over time is often measured using the variance\nof a metric such as accuracy or robustness [3, 44, 79]. The variance\nof a metric over a time interval indicates its deviation from its mean\nwithin this interval. For instance, high variance in robustness in-\ndicates significant fluctuations in robustness levels between two\npoints in time, whereas low variance indicates similar levels of ro-\nbustness over time. A low variance could therefore be understood\nas a consistent performance. In practice, performance can vary\ndue to factors, such as random initializations of weights or input\ndata sampling. These types of variations are unavoidable. Defining\nlevel of variance considered 'consistent' is challenging as it is de-\npendent on the context. Technical standards and guidelines by the\nEU Commission should clarify how to measure a consistent perfor-\nmance with respect to accuracy, robustness, and cybersecurity, and\nprovide guidance on determining the required level of consistency."}, {"title": "5.2 Robustness Art. 15(4) \u0391\u0399\u0391", "content": "We now turn to challenges specific to Art. 15(4) AIA. Art. 15(4)(i) AIA\nstates that \"technical and organisational measures shall be taken\"\nto ensure that Al systems are \"as resilient as possible regarding\nerrors, faults or inconsistencies that may occur within the sys-\ntem or the environment\". Art. 15(4)(ii) AIA specifies that robust-\nness can be achieved through technical redundancy solutions, and\nArt. 15(4)(iii) AIA requires addressing feedback loops in online\nlearning with possibly biased outputs.\nInconsistent Terminology. The term robustness is used inconsis-\ntently throughout the AIA. Art. 15(1) and (4) AIA refer to robust-\nness, whereas the corresponding Rec. (27) and Rec. (75) both men-\ntion technical robustness. One could argue that technical robust-\nness is synonymous with robustness. The term 'technical robust-\nness' in Rec. (27) may be a remnant of the legislative process that\nbuilt on the 2019 Ethics Guidelines for Trustworthy AI [1] devel-\noped by the AI IHEG, which introduced the principle of 'technical\nrobustness and safety'. These guidelines are explicitly referenced by\nRec. (27). Nevertheless, it remains unclear why Rec. (75) also refers\nto 'technical robustness'. It could be that the wording in Rec. (75)\nis borrowed from Rec. (27). Alternatively, one could argue that ro-\nbustness in Art. 15(1) and (4) AIA is not limited to technical aspects,\nbut additionally includes some form of non-technical robustness.\nThe latter could refer to organizational measures that must be im-\nplemented to ensure robustness (Art. 15(4)(i) AIA). Technical stan-\ndards and guidelines by the EU Commission should clarify what\naspects robustness encompasses.\nRequired Level of Robustness. The AIA creates ambiguities re-\ngarding the required level of 'robustness'. Art. 15(1) AIA mandates\nthat Al systems must achieve an \"appropriate level\" of robustness.\nArt. 15(4) AIA, however, demands that AI systems shall be \"as re-\nsilient as possible\" to \"errors, faults, or inconsistencies\", suggest-ing a stricter requirement. This discrepancy initially appears am-\nbiguous, as it is unclear whether HRAIS must simply meet an ap-\npropriate standard of robustness or strive for the highest possible\nlevel. However, the \"appropriate\" level stated in Art. 15(1) AIA can\nbe understood as a general principle, which is further specified by\nArt. 15(4) AIA. Therefore, appropriate with respect to robustness is\nto be understood as 'as resilient as possible'.\nWhen determining the appropriate level of robustness of a spe-\ncific HRAIS, the intended purpose of the system and the generally\nacknowledged state of the art (SOTA) on AI and AI-related tech-\nnologies must be taken into account (Art. 8(1) AIA). Art. 9(4) \u0391\u0399\u0391\nacknowledges that one of the objectives of the required risk man-\nagement is to achieve an \"appropriate balance in the implementa-\ntion of measures to fulfil\" requirements. Art. 9(5) AIA further ac-\nknowledges the permissibility of a residual risk, meaning that the\nmeasures adopted under the risk management system are not ex-\npected to eliminate all existing risks, but rather to maintain these\nresidual risks at an 'acceptable' level. The risk management sys-\ntem is a continuous iterative process (Art. 9(1) AIA). This means\nthat the appropriate level of robustness of HRAIS must be regularly\ndetermined and updated, taking into account its purpose and the\nSOTA while balancing it with other requirements.\nFeedback Loops. Art. 15(4)(iii) AIA states that Al systems must\nbe explicitly developed in such a way that they \"duly address\" feed-\nback loops and \"eliminate or reduce\" the risks associated with them.\nAccording to Rec. (67), feedback loops occur when the output of\nan Al system influences its input in future operations, an under un-\nderstanding that aligns with the concept as found in the ML litera-\nture. Feedback loops are a well-studied problem manifesting in var-\nious forms [68], with the most common issues being a distribution\nshift [73] or a selection bias [44, 56]. Importantly, in this context,\nthe risk of 'biased outputs' in feedback loops (Art. 15(4)(iii) AIA)\nis often studied in the literature on fairness in ML rather than in\nthe literature on robustness in ML, which traditionally constitute\ndifferent research fields and communities [51].\nAn important aspect of Art. 15(4)(iii) AIA is that it applies specif-ically to Al systems that learn online. Online learning ML models\niteratively learn from a sequence of data and continuously update\ntheir parameters over time [42]. This adaptiveness is reflected in\nArt. 3(1) AIA as a factual characteristic of an Al system. The prob-\nlem with feedback loops in online learning is that newly collected\ntraining data can become biased, e.g., due to selection bias, which\noccurs when the data collected is not representative of the overall\npopulation [54, 110]. This can distort model predictions and rein-\nforce existing biases, ultimately impacting the model's accuracy\nand fairness [3, 44, 79]. Offline models, in contrast, are trained on\na fixed dataset all at once [42]. Offline models can also carry risks\nwhen feedback loops are present: The outputs of an ML model can\ninduce a distribution shift through their interaction with the en-\nvironment [24, 55, 112]. Since an offline ML model is not updated,\ndistribution shifts can influence their performance over time and\npossibly lead to fairness concerns [55]. Although Art. 15(4) A\u0399\u0391\ndoes not explicitly address feedback loops in offline systems, HRAIS\nare not exempt from addressing them. Since they can impact the\nmodel's accuracy, feedback loops in offline systems may still need\nto be addressed to comply with Art. 15(1) AIA."}, {"title": "5.3 Cybersecurity Art. 15(5) \u0391\u0399\u0391", "content": "We now turn to legal challenges specific to Art. 15(5) AIA. Art.\n15(5)(i) AIA states that AI systems shall be resilient against attempts\nto \"alter their use, outputs, or performance by exploiting system\nvulnerabilities\". Art. 15(5)(ii) AIA specifies that technical solutions\naiming to ensure resilience against such malicious attempts \"shall\nbe appropriate to the relevant circumstances and the risks\". Finally,\nArt. 15(5)(iii) AIA mandates specific measures \"to prevent, detect,\nrespond to, and control for attacks\" exploiting AI-specific vulnera-\nbilities. This section examines the key aspects of compliance with\nArt. 15(5) AIA. However, a mentioned above, providers have an ad-\nditional pathway for demonstrating compliance with its cybersecu-\nrity requirements, namely a certification under the CSA [13].\nRequired Level of Cybersecurity. Art. 15(5)(ii) AIA mandates that\ntechnical solutions must be \"appropriate to the relevant circum-\nstances and the risks\", but this needs further clarification. The AIA\nspecifically addresses only three kinds of risks: health, safety, and\nfundamental rights (Rec. (1)). Risks associated with these aspects\ncan be identified and managed through a risk management system\nthat must be put into place as stipulated by Art. 9 AIA. Relevant\ncircumstances are any known and foreseeable circumstances that\nmay have an impact on cybersecurity.\nMandating a cybersecurity level that is 'appropriate to the rele-\nvant circumstances' acknowledges that complex ML models gen-\nerally cannot be expected to be fully resistant to all types of ad-\nversarial attacks. This has two major reasons: First, it is impossi-\nble to anticipate all types of possible attacks. This is acknowledged\nby Art. 9(5) AIA which states that measures adopted under the\nrisk management system are not expected to remove all existing\nrisks. Second, complete protection against a specific attack cannot\nbe guaranteed, especially as adversaries continuously adapt their\nstrategies to overcome possible defense mechanisms [46, 104]. The\nappropriateness of a certain performance level must consider the\nintended purpose of the system and the generally acknowledged\nSOTA (see Art. 8(1) AIA). The measures to ensure cybersecurity\nadopted are not expected to eliminate all existing risks, but the\noverall residual risk must be acceptable (see Art. 9(1) and (4) AIA).\nThus, when determining the appropriateness of technical solutions,"}, {"title": "6 Requirements for General-Purpose AI Models With Systemic Risk", "content": "In the previous section, we examined HRAIS requirements. To fur-\nther elucidate them, we study GPAIMs with systemic risk, high-\nlighting similarities and differences. The AIA establishes legal re-\nquirements for GPAIMs, such as multimodal large language mod-\nels, which can perform tasks beyond their original train-\ning objective. GPAIM can be standalone or embedded in an\nHRAIS, with the latter requiring compliance with both GPAIM\nand HRAIS requirements. The AIA distinguishes between GPAIM\nwith systemic risks and those without. Art. 3(65) AIA defines 'sys-\ntemic risk' as the risk that is specific to the high-impact capabilities\nof GPAIMs that have a \"significant impact\" on the market, public\nhealth, safety, security, fundamental rights, or society. GPAIMS\nwithout systemic risks are exempt from robustness and cybersecu-\nrity obligations (Art. 53 AIA ff.).\nCybersecurity Requirements. Art. 55(1)(d) AIA mandates \"an ade-\nquate level of cybersecurity protection\" for GPAIMs with systemic\nrisk. Rec. (115) further details this cybersecurity requirement. It\nmandates cybersecurity protection against \"malicious use or attacks\"\nand lists specific adversarial threats, such as \"accidental model leak-\nage, unauthorised releases, circumvention of safety measures\", \"cy-berattacks\", or \"model theft\". Notably, several of these threats have\ndirect counterparts in the ML literature on adversarial robustness\nand privacy for large generative models, such as the circumvention\nof safety measures (jailbreaking) or model theft [52, 100, 108"}]}