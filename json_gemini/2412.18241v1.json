{"title": "An Automatic Graph Construction Framework based on Large Language Models for Recommendation", "authors": ["Rong Shan", "Jianghao Lin", "Chenxu Zhu", "Bo Chen", "Menghui Zhu", "Kangning Zhang", "Jieming Zhu", "Ruiming Tang", "Yong Yu", "Weinan Zhang"], "abstract": "Graph neural networks (GNNs) have emerged as state-of-the-art methods to learn from graph-structured data for recommendation. However, most existing GNN-based recommendation methods focus on the optimization of model structures and learning strategies based on pre-defined graphs, neglecting the importance of the graph construction stage. Earlier works for graph construction usually rely on specific rules or crowdsourcing, which are either too simplistic or too labor-intensive. Recent works start to utilize large language models (LLMs) to automate the graph construction, in view of their abundant open-world knowledge and remarkable reasoning capabilities. Nevertheless, they generally suffer from two limitations: (1) invisibility of global view (e.g., overlooking contextual information) and (2) construction inefficiency. To this end, we introduce AutoGraph, an automatic graph construction framework based on LLMs for recommendation. Specifically, we first use LLMs to infer the user preference and item knowledge, which is encoded as semantic vectors. Next, we employ vector quantization to extract the latent factors from the semantic vectors. The latent factors are then incorporated as extra nodes to link the user/item nodes, resulting in a graph with in-depth global-view semantics. We further design metapath-based message aggregation to effectively aggregate the semantic and collaborative information. The framework is model-agnostic and compatible with different backbone models. Extensive experiments on three real-world datasets demonstrate the efficacy and efficiency of AutoGraph compared to existing baseline methods. We have deployed AutoGraph in Huawei advertising platform, and gain a 2.69% improvement on RPM and a 7.31% improvement on eCPM in the online A/B test. Currently AutoGraph has been used as the main traffic model, serving hundreds of millions of people.", "sections": [{"title": "1 INTRODUCTION", "content": "Recommender systems (RSs) have become increasingly indispens-able to alleviate the information overload problem [12, 16] and match users' information needs [20, 43, 60] for various online ser-vices [18, 57]. In the past decades, researchers have proposed vari-ous advanced deep learning methodologies to incorporate various model structures [42, 46, 89] and auxiliary information [39, 74, 90] for recommendation. Among them, graph neural network (GNN) based methods turn out to be the state-of-the-art algorithms in min-ing the complex topological distributions from graph-structured relational data for recommender systems [13, 17, 79].\nHowever, most of the existing GNN-based recommendation methods primarily focus on optimizing the model structures [59, 66] and learning strategies [34, 73] based on the pre-defined graphs, generally neglecting the importance of the graph construction stage. The quality of the graph structure is the foundation for the entire graph learning process, and can directly influence the model's abil-ity to capture the underlying relationships and patterns within the data [53, 88]. Earlier works for graph construction usually employ specific rules (e.g., click as linkage, or entity extraction) or human efforts via crowdsourcing (e.g., relational annotation for knowledge graphs). They are either too simplistic to model the sophisticated semantic signals in the recommendation data, or too labor-intensive to be scalable for large-scale scenarios.\nNowadays, large language models (LLMs) emerge as a promising solution for automatic graph construction in view of their vast amount of open-world knowledge, as well as their remarkable language understanding and reasoning capabilities [80]. Recent at-tempts have proposed various innovative prompt-based techniques, e.g., chain-of-thought prompting [87], multi-turn conversation [72], and proxy code generation [3], to estimate the relational linkage between nodes for graph construction. Although these works au-tomate the graph construction stage with the help of LLMs and largely save the intensive human labor, they still suffer from the fol-lowing two limitations, especially when faced with the large-scale user/item volumes in industrial recommender systems.\nFirstly, existing LLM-based graph construction methods fail to cap-ture the high-quality topological structure among nodes due to the invisibility of global view. The reasonable assessment of node connections should comprehensively consider the global view of the entire dataset, including but not limited to node attributes, graph schema, and contextual information [15, 80]. For example, as depicted in Figure 1(a), suppose we have three item nodes with fea-tures: $i_1 = \\{f_1, f_2, f_3, f_8\\}$, $i_2 = \\{f_1, f_2, f_3, f_5\\}$, and $i_3 = \\{f_1, f_6, f_7, f_8\\}$. With a simple local-view pairwise comparison, it seems that item $i_1$ is more similar to item $i_2$ since they have more overlapped features. But once we acquire the global information that $f_5$ serves as a rare feature with fairly low frequency and others are commonly fre-quent ones, the association between $i_1$ and $i_3$ will be significantly enhanced and the connection between $i_1$ and $i_2$ could be in turn reduced. Nevertheless, as shown in Figure 1(b), due to the context window limitation of LLMs and the large-scale users/items in RSs, it is hard to incorporate all the important information into the prompt, which will be truncated and incomplete. Therefore, the information utilized by these methods can only be partial, but never global. Such local-view information can thereby lead to inferior topological graph structures.\nSecondly, existing works generally suffer from the construction inefficiency issue due to the massive invocations of LLMs. While LLMs provide support for in-depth semantic analysis and complex topological structure mining, their intrinsic expensive inference cost poses a significant challenge to the efficiency of graph construc-tion algorithms. Most works instruct LLMs to infer the similarity scores between nodes in a pairwise manner [58], and result in a time complexity of $O(N^2)$, which is impractical for real-world sce-narios where the number of users/items N can easily reach million or even billion level [43]. Although several works propose to con-duct downsampling [61] or heuristic pre-filtering [87] to reduce the number of calls of LLMs, they generally sacrifice the graph quality and thereby introduce noise to the constructed graph. Therefore, it is crucial to design an efficient yet effective LLM-automated graph construction method for large-scale industrial applications.\nTo this end, we propose AutoGraph, an automatic graph con-struction framework based on large language models for recommen-dation. Specifically, AutoGraph consists of two stages: quantization-based graph construction and graph-enhanced recommendation. In the quantization-based graph construction stage, we first leverage LLMs to infer the user preference and item knowledge, which is encoded as semantic vectors. Such a pointwise invocation manner (i.e., invoking LLMs for each single user/item separately) improves the efficiency by reducing the calls of LLMs to $O(N)$ complexity. Then we propose latent factor extraction for users and items based on vector quantization techniques. By incorporating the latent fac-tors as extra nodes, we build a graph with a global view of in-depth semantics, providing more comprehensive and informative insights through the topological structure. In the graph-enhanced recommen-dation stage, we propose metapath-based message propagation to aggregate the semantic and collaborative information effectively on the constructed graph, resulting in the graph-enhanced user/item representations. These representations can be integrated into arbi-trary recommender systems for enhancement.\nThe main contributions of this paper are as follows:\n\u2022 To the best of our knowledge, we are the first to introduce vector quantization for graph construction based on LLMs in recom-mendation, which addresses the two key limitations of existing methods, i.e., invisibility of global view and inefficiency.\n\u2022 We propose a novel AutoGraph framework, which achieves both effectiveness and efficiency. We extract the latent factors of LLM-enhanced user/item semantics based on vector quantization, which are involved as extra nodes for global-view graph con-struction. Moreover, metapath-based message propagation is designed to aggregate the semantic and collaborative informa-tion for recommendation enhancement.\n\u2022 AutoGraph is a general model-agnostic graph construction frame-work. It is compatible with various recommendation models, and can be easily plugged-in for existing recommender systems."}, {"title": "2 PRELIMINARIES", "content": "Given the user set $U$ and item set $I$, each user $u \\in U$ has a chrono-logical interaction sequence $S_u = [i^1_u, ..., i^{L_u}_u]$, where $i^l_u \\in I$ is the $l$-th item interacted by the user $u$ and $L_u$ is the length of the interaction sequence. Besides, each user $u$ has a profile of multiple features, such as user ID and age, while each item has multiple attributes, such as item ID and genre. We can denote them as $F_u = \\{f^u_l\\}^{F_u}_{l=1}$ and $F_i = \\{f^i_l\\}^{F_i}_{l=1}$, where $F_u$ and $F_i$ denote the number of features for the user and item respectively. A typical recommendation model learns a function $\\Phi$ to predict the preference score of a user $u$ towards a target item $i$, which can be formulated as:\n$score = \\Phi(S_U,F_u,F_i),                                              (1)$\nwhere the model can be optimized for downstream tasks like click-through-rate estimation [43, 44], or next item prediction [46, 69].\nAs for graph-enhanced recommendation, we will further con-struct and incorporate a graph $G = \\{V, \\mathcal{E}\\}$ into the model:\n$score = \\Phi(S_U, F_u, F_i, G),                                                 (2)$\nwhere $V = \\{U, I\\}$ is the set of user nodes $U$ and item nodes $I$, and the edge set $\\mathcal{E}$ usually contains three types of edges [17, 25]:\n\u2022 User-Item Edge $e_{u-i}$ denotes that the item $i$ is positively inter-acted by user $u$, e.g., click or purchase.\n\u2022 User-User Edge $e_{u-u}$ indicates the relationship between each pair of users, e.g., social network.\n\u2022 Item-Item Edge $e_{i-i}$ represents the similarity between each pair of items, possibly measured by their attributes and contents.\nNote that there are many graph variants or special cases based on such a basic formulation. For example, many works simply focus on a specific homogeneous graph (e.g., user social networks [84]), or the bipartite user-item interaction graph [71] for recommendation. Knowledge graphs further introduce the entity nodes and diverse relation edges for item semantic modeling. In this work, we extend such a basic graph formulation with newly introduced latent factor nodes for both users and items, which will be discussed in Section 3."}, {"title": "3 METHODOLOGY", "content": "3.1 Overview of AutoGraph\nAs illustrated in Figure 2", "stages": 1, "61": ".", "41": "and thereby needs to be enriched for graph construction to better cap-ture in-depth semantics. To this end"}, {"61": ".", "as": "n$v^u_j = Encoder(LLM(T^u_j)) \\in \\mathbb{R"}, {"75": ".", "55": "for latent factor extraction. We quantize the semantic vectors v using T codebooks denoted as $\\{C^t\\"}, {"as": "n$m^t = arg\\text{ }min_k ||r^t - c^t_k||^2,                                                                                                                               (4)$\n$r^{t+1} = r^t - c^t_{m^t}."}]}