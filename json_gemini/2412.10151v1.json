{"title": "VLR-Bench: Multilingual Benchmark Dataset for Vision-Language Retrieval Augmented Generation", "authors": ["Hyeonseok Lim", "Dongjae Shin", "Seohyun Song", "Inho Won", "Minjun Kim", "Junghun Yuk", "Haneol Jang", "KyungTae Lim"], "abstract": "We propose the VLR-BENCH, a visual question answering (VQA) benchmark for evaluating vision language models (VLMs) based on retrieval augmented generation (RAG). Unlike existing evaluation datasets for external knowledge-based VQA, the proposed VLR-BENCH includes five input passages. This allows testing of the ability to determine which passage is useful for answering a given query, a capability lacking in previous research. In this context, we constructed a dataset of 32,000 automatically generated instruction-following examples, which we denote as VLR-IF. This dataset is specifically designed to enhance the RAG capabilities of VLMs by enabling them to learn how to generate appropriate answers based on input passages. We evaluated the validity of the proposed benchmark and training data and verified its performance using the state-of-the-art Llama3-based VLM, the Llava-Llama-3 model. The proposed VLR-BENCH\u00b9 and VLR-IF\u00b2 datasets are publicly available online.", "sections": [{"title": "1 Introduction", "content": "The search for external knowledge is very important for VLMs because it is often impossible to find answers directly from images in response to user queries (Marino et al., 2019). Previous studies attempted to incorporate external knowledge into VLMs. Among these efforts, dense passage retrieval (Karpukhin et al., 2020) has been used to search for documents related to queries in an attempt to solve this problem (Luo et al., 2021; Gao et al., 2022). However, as Lin and Byrne (2022) pointed out, these models face challenges in determining whether the retrieved documents are useful for answering queries.Following this, the proposed RA-VQA (Lin and Byrne, 2022) introduced an approach that simultaneously conducts searches and question-answering to overcome these drawbacks. However, since the study primarily focused on the RAG configuration, evaluating how the VLM utilized the search results remained challenging.\nTo address these issues, we propose a Vision Language-RAG Benchmark (VLR-BENCH) and training data to evaluate the Retrieval-Augmented Generation (RAG) capabilities of VLMs (Lewis et al., 2021). VLR-BENCH consists of 300 datasets composed of problems that are difficult to solve without external knowledge. The data were structured as an image-query-passage-output, and unlike conventional VQA datasets, each dataset contained five distinct passages. Only two passages contained direct information that could resolve the queries. This allows us to test the ability, which has been lacking in previous research, to determine which passages are useful for answering queries.\nIn this study, we developed the VLR Instruction Following (VLR-IF) training data for VLM RAG based on the data generation method proposed by LLaVA (Liu et al., 2024) and assessed its utility. We validated the proposed VLR-BENCH and VLR-IF training data based on the following three research questions: (1) Does the proposed VLR-BENCH require external knowledge retrieval to be solved? (2) How does the proposed training data impact external knowledge utilization? (3) How effectively can public VLMs and commercial models resolve queries that require retrieval?\nIn this study, we conducted a baseline performance evaluation of VLR-BENCH using the most recently released vision language models in the LLAVA-LLAMA-3 series (Contributors, 2023) and GPT-40 (OpenAI et al., 2024). The contributions of this study can be summarized as follows:\n\u2022 We propose multilingual RAG evaluation data, VLR-BENCH, and training data, VLR-IF, for the VLMs.\n\u2022 Through in-depth analysis, we prove the actual effect of our dataset."}, {"title": "2 Related Work", "content": "VLM Benchmark Datasets. In the VLM benchmark, OK-VQA (Marino et al., 2019) is a key open-domain VQA dataset that uses external knowledge from Wikipedia. Subsequently, A-OKVQA (Schwenk et al., 2022) and S3VQA (Jain et al., 2021), which included justifications for answers, were derived from OK-VQA. Additionally, datasets targeting specific domains have appeared; for instance, K-VQA (Shah et al., 2019), which intensively utilizes personal information, and ViQUAE (Lerner et al., 2022), which uses object information, were proposed as evaluation datasets. Furthermore, VQA models utilizing knowledge graphs have been proposed, notably GQA (Hudson and Manning, 2019), which uses scene graph knowledge and its multilingual expansion (xGQA (Pfeiffer et al., 2022) and BOK-VQA (Kim et al., 2024)). In a different context, datasets providing passages for evaluating the RAG capabilities of VLMs have recently emerged. Notable examples include InfoSeek (Chen et al., 2023) and Encyclopedic VQA (Mensink et al., 2023). These datasets provide passages or entire documents, resulting in performance variations based on the document retrieval ability. Detailed information on these external knowledge-based VLM benchmark datasets, as well as their differences from the proposed VLR-BENCH, can be found in Appendix D.3."}, {"title": "3 Proposed RAG Dataset for VLMs", "content": "Benchmarks related to the use of external knowledge by VLMs, as discussed in Section 2, particularly InfoSeek and Encyclopedic VQA, typically provide single gold-standard evidence to resolve queries. However, real-world RAG-based systems generate answers by incorporating multiple retrieved results (e.g., Top-5). A significant challenge arises when plausible but incorrect information is retrieved as external knowledge. Therefore, when VLM models use RAG, it is essential to evaluate (1) how accurately external knowledge is retrieved and (2) the model's ability to generate correct answers despite the existence of incorrect information. In this context, we propose VLR-BENCH, which simultaneously considers the correct selection of external knowledge and answers-generated by VLMs."}, {"title": "3.1 VLR-Bench Dataset", "content": "VLR-BENCH was constructed to evaluate whether VLMs can use the correct external knowledge to generate accurate responses to query. We constructed a parallel corpus of 300 datasets: 150 based on general knowledge and 150 based on cultural data from English, Chinese, and Korean. Detailed examples of the data are provided in Appendix A.\nImage Selection. Images are crucial within this dataset. The diversity of categories among the selected images is essential for depicting a range of external knowledge. Considering these factors, we manually curated 150 images from BOK-VQA, developed explicitly for open-world QA purposes.\nWe manually extracted 150 images from the 10 categories proposed by BOK-VQA, with 15 images each from the object-centric, atmosphere-centric, and relation-centric categories. In addition, We collected 150 images of different languages' cultural backgrounds from Wikimedia Commons under the same conditions as BOK-VQA.\nQuestion Selection. The question selection process used GPT-40 to receive recommendations for high-quality question-answer pairs. We input images into GPT-40 and requested them to generate ten queries, two essential pieces of external knowledge required to resolve these queries, and descriptive answers. To ensure the validity of the model verification, we imposed the following conditions: (1) The generated data should consist of question-answer pairs that cannot be resolved with the image alone. (2) Image information should not be explicitly evident in the questions to ensure that queries cannot be resolved using external knowledge."}, {"title": "Generation of Additional External Knowledge", "content": "VLR-BENCH consists of five pieces of external knowledge. Among these, two are directly referenced when generating answers for the actual images and questions, referred to as \u2018Gold Passage', which were already reviewed in the previous stage. Two of the five passages relate to the theme of the image or question but diverge from the central theme of the answer, termed \u2018Silver Passage'. The last one, unrelated to the image and the question, is designated as 'Bronze Passage'. At this stage, we generated two silver passages and one bronze passage. Three annotators directly reviewed the data derived through this process for the question-answer pairs, external knowledge, and descriptive answers. Specifically, errors in the generated external knowledge or knowledge with unclear sources were replaced with new information by annotators (see Appendix A.2). Finally, each annotator extracted the two essential keywords necessary to resolve the questions. Each sample comprises five elements: an image, a query, five pieces of knowledge, a descriptive answer, and two keywords. Examples of the data are shown in Figure 1."}, {"title": "3.2 VLR-IF Dataset", "content": "To address the proposed benchmark, we designed instruction-following data to enhance the utilization of external knowledge using VLMs. As previously proposed, we generated data using the same GPT-based method for question-external knowledge-answer creation. Initially, we randomly selected 9K COCO (Lin et al., 2015) images and generated a 'valid passage' related to each image. Subsequently, we randomly extracted external knowledge from different data samples for use as 'invalid passages', thus contrastively constructing datasets using a combination of valid and invalid passages. The VLR-IF dataset was constructed in parallel for three languages: English, Chinese, and Korean, with each language comprising 32K data samples. The specific process for constructing the datasets is described in Appendix B.2."}, {"title": "4 Experiments and Analysis", "content": "We selected the top-performing models for each language for our experiments. Table 2 presents the base models, pre-training volumes, and visual instruction tuning (VIT) training volumes for the models used in this experiment. The VLR-BENCH task involves generating long-form answers to the given queries. As described in Section 3, two keywords were manually annotated for each query. Therefore, these keywords in the model-generated long-form answers allow for some degree of quantitative evaluation, defined as the keyword-matching score (KMS). We considered a response correct only when both answer keywords were accurately identified. However, because the KMS performance may improve as the generated response lengthens, it is used as a reference indicator rather than an exact performance measure. To compensate for this, a comprehensive evaluation should be conducted using metrics that account for sentence length, such as Rouge (Lin, 2004), BLEU (Papineni et al., 2002), and BERT-Score (Zhang* et al., 2020)."}, {"title": "4.1 Experiment Results", "content": "Diversity in Performance Evaluation. Upon examining the English KMS performance in the With Passage section of Table 1, it can be observed that the performance of LLAVA1.5 closely mirrors that of GPT-40. This raises the question of whether LLAVA1.5 truly makes accurate predictions. The answer is no. The task involves generating long-form answers, and LLAVA1.5 often directly outputs the received external information, resulting in lengthy responses. Although such responses achieve high KMS performance, they also contain external knowledge irrelevant to the query, leading to lower BLEU and Rouge scores.\nThe Impact of Use of External Knowledge. VLR-BENCH allows for evaluations in scenarios where external knowledge is provided, as each problem is accompanied by five pieces of external knowledge. Table 1 presents the VLR-BENCH evaluation results based on the availability of external knowledge for each model. Notably, the performance of the X-LLAVA model dropped by an average of 37.72% for R-2 in English compared to when external knowledge was provided. These results suggest that the VLR-BENCH dataset contains queries that require external knowledge.\nThe Impact of VLR-IF Training. We conducted experiments to assess the utility of the VLR-IF data using the baseline LLAVA-LLAMA-3 and its version enhanced by VLR-IF training. According to the results in Table 1, the model trained with the VLR-IF data showed a 22.67% performance improvement over the baseline model when external knowledge was provided. This significant enhancement suggests that the VLR-IF training data effectively boosts the ability to select and utilize external knowledge. Finally, we examined whether VLR-IF could positively impact other evaluation datasets, using the InfoSeek (Chen et al., 2023) benchmark as a reference. The results indicated a 3.6% performance improvement with the application of VLR-IF (see Appendix C.3).\nComparing GPT-40 with Open Models. We conducted experiments to test if GPT-40 could solve VLR-BENCH problems without external knowledge. The results from the \"Without Passages\" section in Table 1 show that GPT-40 outperformed QWEN-VL-CHAT by an average of 17.33 points without external knowledge. However, with passages provided, the performance gap narrowed to an average of 7.36 points. This indicates that VLR-BENCH is a challenging benchmark without external knowledge, and open-source models can improve with passage-retrieval capabilities."}, {"title": "4.2 Analysis", "content": "In this section, we present an in-depth analysis to determine whether the VLR-BENCH is a suitable dataset for evaluating model's ability to utilize information. To this end, we measured the BERT-score and Rouge scores (R-1, R-2, R-L) between passage types, questions, and ground-truth outputs. The results presented in Table 3 show that the ground truth output correlates most strongly with the Gold - Silver - Bronze Passage in descending order. This trend substantiates the effective use of gold passages in deducing answers to the VLR-BENCH, indicating that the appropriate utilization of externally sourced knowledge through images is crucial for answering queries. On the other hand, an examination of the passages and question results reveals no clear trend, as Bronze's Rouge-1 score is higher than Silver's, suggesting that selecting suitable external knowledge based solely on the query can be challenging. This implies that understanding the images is necessary."}, {"title": "5 Conclusion", "content": "In this study, we propose VLR-BENCH for evaluating RAG-based VLMs, and VLR-IF for performance enhancement. The proposed benchmark differs from existing external knowledge-based VLM evaluation datasets in the following ways. (1) It consists of problems that are difficult to solve without external knowledge. (2) It includes five different passages, allowing the test of an ability not covered in previous research to determine which passages are useful for answering queries. The training data were designed as multilingual evaluation data that could simultaneously assess English, Chinese, and Korean, enhancing their utility."}, {"title": "6 Limitations", "content": "In this study, we proposed a benchmark and corresponding training data to evaluate the RAG capabilities of VLMs. The benchmark allows for the evaluation of both retrieval and generation abilities. However, there are still two issues that remain:\nAbsence of Image Search Capability. Ultimately, the ability to perform image searches is crucial for accurately assessing the performance of the VLR-Bench. As mentioned in Table 1, the superior performance of GPT-40 over other public language models originates from the presence or absence of image search capabilities. Unfortunately, this study did not consider methods related to image search.\nLack of Diversity in Responses Due to Training Data Construction Costs. The method proposed in this study enabled the construction of training data at a very low cost. However, applying the same method to other languages still incurs costs, particularly when building test data, which can be expensive. Due to these cost constraints, annotation was performed by a single individual. While there could be multiple correct answers to the short-answer core keywords, due to budget limitations, responses were collected from only one person. Nevertheless, the final test data underwent a secondary review process to ensure data quality."}]}