{"title": "Towards measuring fairness in speech recognition: Fair-Speech dataset", "authors": ["Irina-Elena Veliche", "Zhuangqun Huang", "Vineeth Ayyat Kochaniyan", "Fuchun Peng", "Ozlem Kalinli", "Michael L. Seltzer"], "abstract": "The current public datasets for speech recognition (ASR) tend not to focus specifically on the fairness aspect, such as performance across different demographic groups. This paper introduces a novel dataset, Fair-Speech, a publicly released corpus to help researchers evaluate their ASR models for accuracy across a diverse set of self-reported demographic information, such as age, gender, ethnicity, geographic variation and whether the participants consider themselves native English speakers. Our dataset includes approximately 26.5K utterances in recorded speech by 593 people in the United States, who were paid to record and submit audios of themselves saying voice commands. We also provide ASR baselines, including on models trained on transcribed and untranscribed social media videos and open source models.", "sections": [{"title": "1. Introduction", "content": "The performance of current speech recognition (ASR) systems has improved significantly over the last few years, with the emergence of new modeling techniques and considerable amounts of training data. However, most of the improvements are targeted for overall word error rate (WER). The evaluation sets being used tend to lack information associated with the demographic characteristics, such as ethnicity, geographic variation and whether utterances come from native or non-native English speakers. Also, most numbers are reported in aggregate, without giving a more clear picture of the potential gaps between different demographic groups. While there have been many studies showing that ASR systems do not perform equally well for all demographic and accent groups [1, 2, 3, 4, 5, 6], the number of open sourced datasets that can be used for evaluation of such characteristics is limited.\nIn this paper we introduce a new ASR dataset, Fair-Speech. Our dataset includes approximately 26.5K utterances in recorded speech by 593 people in the U.S. who were paid to record and submit audio of themselves saying commands. They self-identified their demographic information, such as age, gender, ethnicity, geographic location and whether they consider themselves native English speakers, together with their first language.\nThe verbal commands included in this dataset are categorized into seven domains, primarily serving voice assistant use cases music, capture, utilities, notification control, messaging, calling, and dictation that can support researchers who are building or have models in those areas. In response to prompts that relate to each of these domains, dataset participants provided their own audio commands. Some examples of prompts were asking how they would search for a song or make plans with friends, including deciding where to meet. Providing broad prompts to guide the speakers is better than simply asking participants to read text prompts, since that tends to make the audios sound less natural: people would make different kinds of pauses than in natural speech and entities might also not be pronounced properly, if the participants are not familiar with them.\nOur dataset includes the audio and transcription of participants' utterances, together with their self-identified labels across the different demographic categories. The intent is for this to be used for evaluating the performance of existing ASR models. The data user agreement prevents a user from developing models that predicts the value of those labels, but one may measure the performance of different models as a function of those labels.\nBy releasing this dataset, we hope to further motivate the AI community to continue improving the fairness of speech recognition models, which will help everyone have a better experience using applications with ASR."}, {"title": "2. Previous work on ASR Fairness", "content": "As voice recognition systems have become more integrated into daily lives, especially through the use of voice assistants, there has been a considerable amount of research showing that those systems exhibit biases when it comes to the performance of the ASR models. For example, [4] studied the ability of different ASR systems to transcribe structure interviews of black and white speakers, finding that all of them exhibited substantial racial disparities. The study was done on Corpus of regional African American language [7], a collection of socio-linguistic interviews with dozens of black individuals who speak African American Vernacular English, and also on Voices of California [8], which is a compilation of interviews recorded in both rural and urban areas of California. Prior studies also showed disparities across accents and socio-economic status of the speakers in [5], race and gender bias in [9, 10, 11, 12], regional and non-native accent [6].\nThere is also some recent work on how some of these demographic biases can be mitigated, such as in [3, 13, 14, 15]. However, some are using in-house datasets, which are difficult to use for comparison.\nThere are a number of open sourced datasets that can be used for measure the fairness of ASR systems across different demographic groups. Apart from the two mentioned above [7, 8], there is also the Artie Bias corpus [16], a curated set of the Mozilla Common Voice corpus, which contains demographic tags for age, gender, accent. Casual conversations dataset [17] has associated tags for gender, age and skin tone, while the ICSI meeting corpus [18] has associated information on gender, age,"}, {"title": "3. Corpus contents", "content": "The verbal commands included in this dataset are categorized into seven domains, primarily serving voice assistant use cases music, capture, utilities, notification control, messaging, calling, and dictation. In response to prompts that relate to each of these domains, dataset participants provided their own audio commands. Our dataset includes the audio and transcription of participants' utterances. The audio is mobile collected. The intention of this dataset is to be used as an evaluation tool, to uncover gaps or biases in ASR models.\nThis dataset was constructed with the recordings of paid participants who have explicitly provided their consent for their recordings to be used in research, together with the associated demographic information. This ensures that the dataset aligns with the ethical standards e.g. for data collection, respects the privacy and autonomy of the participants, but also promotes transparency and other key ethical considerations in responsible data collection practices."}, {"title": "3.1. Breakdown by gender for demographic categories", "content": "In Figure 1 we show the breakdowns by gender for each of the demographic categories. While for some categories there is a good balance between the different genders, for example in the 18 - 22 sub-group, for some there are many more utterances coming from male than from female speakers, such as in the Black or African American sub-group, or the other way around, such as in the Asian, South Asian or Asian-American sub-group. This is important to take into account when analyzing results. Therefore, in section 5.1 we'll take into account confounding factors and speaker variability when showing WER gaps between different sub-groups."}, {"title": "3.2. Data transcription", "content": "To produce transcription useful for ASR evaluation, all data was verbatim transcribed. Colloquial words were kept as spoken, as well as repeated words. Numbers were spelled out as spoken and entities were transcribed with capital letter. All the other text is lower-cased, without punctuation. Since this is a voice command dataset, each audio contains utterances from a single speaker. The length of the utterances vary. Each recording lasts an average of 7.36 seconds. The maximum length of an utterance is around 1 minute.\nHigh-quality annotations were achieved through a multi-pass human transcription, resolution, curation, and scoring. The multi-pass annotation by external vendors ran until a three-way agreement. The 10% without agreement was resolved by internal linguistic engineers. Data curation started from a side by side audit comparing annotations with ASR hypotheses coming from models with different sizes. The 650 rows with low confidence were further audited by cultural-specific linguistic vendors. All audited data was reviewed by multiple internal teams for cross-functional validation. Finally, 200 random rows were scored by an internal linguistic expert to obtain the annotation WER (1.47%)."}, {"title": "4. Speech recognition experiments", "content": "To provide some ASR baselines on the ASR fairness dataset on, we trained a series of recurrent neural network transducer (RNN-T) [19] models and also used open-sourced models.\n\u2022 1. Video model, supervised only data: an RNN-T model with an emformer encoder [20], LSTM predictor and a joiner, having approximately 50 million parameters in total. The input feature stride is 6. Encoder network has 13 emformer layers, each with embedded dimension of 480, 4 attention heads, FFN size of 2048. Prediction network is an LSTM layer with 256 hidden units and dropout 0.3. Joint network has 1024 hidden units and a softmax layer of 4096 units for blank and wordpieces. The model was trained on 29.8K hours of English video data that is completely de-identified before transcription. It contains a diverse range of speakers, accents,"}, {"title": "5. Results", "content": "We computed WER on the Fair-Speech dataset using the four models described above. We also noted the relative gap between the groups with the lowest and highest WER in each category. As can be seen in Tables 2, there are gaps across all demographic groups. Some of the key observations:\n\u2022 The data used in training can have a significant impact on the WER, particularly when it comes to bias between different demographic groups. The relative gap across all the demographic categories for all evaluated models is in double digits for most of the categories. For Whisper models the gap is larger than 40% across all dimensions.\n\u2022 Adding more data in training can significantly improve the performance of the model, such as when semi-supervised data was added for the video models. Interestingly however, for geographic variation, even though the WER improves with more data, the relative gap becomes wider. The linguistic variation difference almost disappears when more data is added in the training, making the dataset more diverse. Also for ethnicity, while the supervised model has some large gaps between different groups, after adding semi-supervised data the gaps decrease and the group with the highest WER actually changes.\n\u2022 As expected, having a larger model improves the accuracy across all the categories, as can be seen with the two Whisper models. However, the relative gap in the linguistic variation sub-group is actually increasing for the larger model.\n\u2022 Data might not be enough to achieve a fair model. All the models shown here were trained on more than 1 million hours of data. However, they exhibit significant gaps across each of the demographic sub-groups. Thus, new modeling techniques are needed to focus on improving the performance for all people. Also, during evaluation, a particular focus needs to be given to demographic breakdowns in addition to overall model accuracy.\nThere can be many nuances when interpreting these WER"}, {"title": "5.1. Understanding the WER gaps", "content": "When analyzing the results, we employed a model-based approach to measure fairness, using mixed-effects Poisson regression to interpret any WER differences between subgroups of interest, as described in [22]. This helps by taking into account nuisance factors, unobserved heterogeneity across speakers and helps tracing the source of WER gaps between different subgroups. For this analysis, we used the video semi-supervised model.\nWe apply the model-based approach, where we fit a mixed-effects Poisson regression with the demographic group we focus on (age, gender etc.) as the fixed effect and speaker label as a random effect.\nWhen computing the fairness measurement of speech recognition accuracy among different subgroups of the factor $f(.)$, the model is described as follows in [22]:\n$T_i \\stackrel{i.i.d.}{\\sim} N(0, \\sigma^2)$ (1)\n$C_{ij} | d_{ij} \\stackrel{i.i.d.}{\\sim} Poisson(d_{ij})$ (2)\n$\\log(d_{ij}) = \\log(N_{ij}) + \\mu_{f(i)} + r_i + 0 X_{ij}$ (3)\nwhere the utterance-level index of subscription notation ij represents the jth utterance from the ith speaker, $r_i$ denotes the speaker-level random effect that is independently sampled from a Gaussian distribution with mean 0 and variance o\u00b2 which is learnable. $\\mu_{f(i)}$ is used to denote the fixed effect for the factor $f(.)$ of primary interest, since typically it is at speaker level.\nThe bootstrap method [23] is applied to compute the 95% confidence interval (CI) of the ratio. If the CI does not include the value of one effect, we assume that there is a statistically significant result.\nResults are shown in Table 3. For each demographic category, we do pairwise comparison across all subgroups. The rows in bold indicate that the WER diference between two subgroups are statistically significant, while the rest are insignificant.\n\u2022 For age, when we compare the 18 - 22 group, which has the lowest WER, with the other groups, we see statistically significant differences to groups 31-45 and 46-65, but not to group 23 - 30.\n\u2022 For gender, there is a statistically significant difference between Female and Male speakers, which is in line with sociolinguistic theory, that establishes that women tend to speak in a more standard way than men [24].\n\u2022 For ethnicity, when we do the pairwise comparisons, there are statistically significant differences between the Black or African American subgroup and all the other subgroups. This is in line with previous research on racial disparities. [12] found that this is due to phonological, phonetic or prosodic characteristics of African American Vernacular English, rather than the grammatical or lexical characteristics.\n\u2022 In terms of geographic variation, there are statistically significant differences between Low and Medium and Medium and Affluent.\n\u2022 For linguistic variation, even though the difference in WER is quite small between the two sub-groups, we see statistically significant differences between people whose first language is English and those who have a different first language."}, {"title": "6. Conclusion", "content": "In this paper, we introduced a new ASR dataset, Fair-Speech, that has metadata attached for different demographic groups (age, gender, ethnicity, geographic and linguistic variation) and can be used for fairness evaluation when developing speech recognition models. We also run baseline analysis on different models and found that there are statistically significant gaps across the different sub-groups for each demographic category. The datasets, with transcripts and metadata, are all released to the external community. We hope that they will help in evaluating and improving the fairness of speech recognition models."}]}