{"title": "WildOcc: A Benchmark for Off-Road 3D Semantic Occupancy Prediction", "authors": ["Heng Zhai", "Jilin Mei", "Chen Min", "Liang Chen", "Fangzhou Zhao", "Yu Hu"], "abstract": "3D semantic occupancy prediction is an essential part of autonomous driving, focusing on capturing the geometric details of scenes. Off-road environments are rich in geometric information, therefore it is suitable for 3D semantic occupancy prediction tasks to reconstruct such scenes. However, most of researches concentrate on on-road environments, and few methods are designed for off-road 3D semantic occupancy prediction due to the lack of relevant datasets and benchmarks. In response to this gap, we introduce WildOcc, to our knowledge, the first benchmark to provide dense occupancy annotations for off-road 3D semantic occupancy prediction tasks. A ground truth generation pipeline is proposed in this paper, which employs a coarse-to-fine reconstruction to achieve a more realistic result. Moreover, we introduce a multi-modal 3D semantic occupancy prediction framework, which fuses spatio-temporal information from multi-frame images and point clouds at voxel level. In addition, a cross-modality distillation function is introduced, which transfers geometric knowledge from point clouds to image features. Dataset will be released at https://github.com/LedKashmir/WildOcc", "sections": [{"title": "I. INTRODUCTION", "content": "The target of semantic occupancy prediction task is to predict the semantic labels of each occupied voxel at a distance, and making accurate and reliable 3D semantic occupancy prediction is challenging [1]\u2013[3]. As the ultimate target of autonomous driving, liberating driver implies that the car could autonomously drive in any road environment. Therefore, besides in on-road environments, it is necessary to study autonomous driving in off-road environments [4]. Off-road environments are rich in irregular objects, such as grass, bushes and trees [5]. Therefore, it is a challenge to reconstruct 3D scene in complex off-road environments. Because the 3D semantic occupancy representation could provide more comprehensive information than 2D represen-tation [2], studying 3D semantic occupancy prediction in off-road environments is of great importance.\nAt present, there are growing researches on 3D seman-tic occupancy perception in on-road environments, such as TPVFormer [6], SurroundOcc [3] and OpenOccupancy [2]."}, {"title": "II. RELATED WORK", "content": "A. 3D Semantic Occupancy Prediction\nRecently, 3D semantic occupancy prediction is brought into focus. Several benchmarks [2], [3], [8], [9] are released. In the field of autonomous driving, among camera-based methods, MonoScene [10] is firstly proposed to predict semantic occupancy from a single camera. SurroundOcc [3] extracts multi-scale image features from multi-cameras. PanoOcc [11] utilizes voxel queries to align spatial and temporal information. LiDAR-based method, such as UDNet [12], extracts voxel features by 3D U-Nets [13]. Among multi-modal methods, OpenOccupancy [2] proposes a bench-mark derived from nuScenes and proposes several baselines. Co-Occ [14] fuses features from LiDAR and camera with volume rendering regularization [15]. However, methods above are all in on-road environments. To authors' knowl-edge, there are no researches on 3D semantic occupancy perception in off-road environments. Because the 3D se-mantic occupancy representation provides more comprehen-sive information than 2D representation [16], we propose a benchmark WildOcc and a framework OFFOcc for 3D semantic occupancy prediction in off-road environments.\nB. Knowledge Distillation\nThe initial target of knowledge distillation [17] is to compress model, by the way of transferring knowledge from teacher model to student model. Due to its effectiveness, knowledge distillation has been applied to semantic occu-pancy prediction [18], [19]. SCPNet [18] transfers semantic knowledge from a multi-frame point clouds network to a single-frame point clouds network, the teacher and student employ the same architecture. MonoOcc [19] utilizes knowl-edge distillation to transfer information from the teacher, the teacher model adopt a pre-trained larger backbone to extract rich features from multi-frame images. However, methods above transfer knowledge in the same modality. To utilize rich geometric information from LiDAR, our method proposes a cross-modality knowledge distillation strategy to transfer knowledge from LiDAR branch to camera branch.\nC. Off-road Perception\nDatasets of autonomous driving can be divided into two categories: on-road and off-road. On-road datasets are exten-sively used for autonomous driving research [8], [20], [21]. However, off-road datasets are relatively scarce. DeepScene [22] is the first public off-road dataset, RELLIS-3D [5] uti-lizes multi-modal sensors to improve autonomous navigation in off-road conditions. ORFD [4] emphasizes traversability analysis, a crucial aspect for detecting freespace in off-road environments. Current off-road perception tasks mainly focus on freespace detection [4] and segmentation [5], there are few researches on reconstructing off-road environments. Because reconstructing scenes can provide more compre-hensive perceptual information, it is important to employ 3D semantic occupancy task into off-road environments. Therefore, we design a benchmark WildOcc with accurate dense occupancy ground truth for off-road environments. Furthermore, a framework OFFOcc is introduced for off-road 3D semantic occupancy prediction."}, {"title": "III. PIPELINE OF WILDOCC", "content": "A. Multi-frame Point Clouds Aggregation\nFollowing SurroundOcc [3], given $P_{ego}^i$ as point clouds segment in ego coordinates of i-frame. We compute the world coordinates of i-frame as $P_i = T_{ui} P_{ego}^i$, where $T_{ui}$ is transform matrix consists of calibrated and ego-poses information. Then we concatenate multi-frame point clouds segments in world coordinates as $P_w = concat[P_i, ..., P_u]$, n represent the number of frames in the sequence. Finally, the points of current frame are calculated, as $P = T_{1i} \u00b7 P_w$, where $T_{1i}$ represents transformation from world coordinates to current coordinates.\nB. Coarse-to-fine Reconstruction\nDue to the interspace and uneven distribution of dense point clouds $P$, it is necessary to use Poisson Reconstruction [23] converting $P$ to a mesh. However, in our experiments, the reconstruction strategy of SurroundOcc [3], which recon-structs the whole scene directly, causes relatively big errors in off-road environments,\nAccording to the semantic category of each point, we first divide points $P$ into ground points $P_g$ and non-ground points $P_{ng}$. Because ground elements have a relatively simple"}, {"title": "C. Semantic Labeling", "content": "After achieving dense voxels $V_{geo}$, the occupancy annota-tions still have no semantic information. The annotations of point clouds from Rellis-3D [5] provides semantic informa-tion. Different from the method in SurroundOcc [3], which searches the nearest points for each voxel and assign the semantic label to it. We utilize KNN algorithm [24] to find the k-nearest points of each voxel center, and assign the most frequent category as semantic label of the voxel, which could reduce the interference of noise points. After this process, all voxels in $V_{geo}$ would get their semantic category. Compared with method in SurroundOcc [3], our method reduces the possibility of errors."}, {"title": "IV. OFFOCC METHOD", "content": "The framework of OFFOcc, consists of camera branch, LiDAR branch, and multi-modal branch. A spatio-temporal alignment module is proposed to utilize tem-poral information. Moreover, we introduce a cross-modality distillation function to transfer knowledge from LiDAR to camera. Specially, the camera branch and LiDAR branch can work independently, which ensures that the system still works when a single sensor is out-of-work. This design is necessary in off-road environments which are hard and unsafe.\nA. Task Definition\nThe target of 3D semantic occupancy prediction is to generate the geometric and semantic representation of a 3D scene. Following the definition in 3D semantic occupancy prediction tasks [2], [9], we take $X_t$ and $X_{t-1}, ..., X_{t-k}$ as input, where $X_t = {L_t,I_t}$ represents current frame's LiDAR sweeps $L_t$ and image $I_t$, $X_{t\u22121}, ..., X_{t-k}$ denotes the data from previous frames, and k represents the number of historical frames. The goal is to predict the occupancy labels $O(X_t) \u2208 R^{H,W,D}$, where $H,W, D$ represents the volume size, and $O(X_t)$ includes semantic classes and occupancy state (\"empty\u201d or \u201cnot-empty\u201d) of each voxel.\nB. Camera Branch\nImage Encoder & 2D-to-3D Transformation\nAfter alignment, we concatenate voxel features of previous frames and current voxel features $V_i^t$ together as features of image voxel space $V_I$. Then we utilize a voxel"}, {"title": "C. LiDAR Branch", "content": "As illustrated in Fig. 3, the LiDAR branch firstly utilizes voxelization [29] and LiDAR encoder [30] to obtain the voxel features. Although the LiDAR sweep contains from multi-frame point clouds, it is aggregated without alignment. Because Spatio-Temporal Alignment is designed for features at voxel level, it is suitable to use this module for aligning previous LiDAR voxel features as well, which is the same as alignment of camera branch. $V_L$ denotes the voxel features after alignment, where $V_L \u2208 R^{C,H,W,D}$. Then we use a voxel encoder as well, which outputs the point voxel features $F_L \u2208 R^{C,H,W,D}$. The size of point voxel features $F_L$ is same with image voxel features $F_I$. The unified representation of image voxel features and point voxel features makes cross-modality distillation possible.\nD. Multi-modal Branch\nThe target of multi-modal branch is to improve the per-formance by utilizing all modalities, therefore we use the adaptive fusion module [2] to dynamically fuse image voxel features $F_I$ and point voxel features $F_L$.\n$F_M = \u03c3(W) \u2299 F_I + (1 \u2212 \u03c3(W)) \u2299 F_L,$\nwhere W is a trainable parameter matrix output by 3D con-volution, \u2299 means element-wise product and \u03c3 is Sigmoid function.\nE. Cross-modality Distillation\nDue to the complexity of off-road environments, sensors are at a risk of being damaged. One of our goals is to ensure the system still works when a single sensor is out-of-work, therefore it is necessary to enhance the ability of single modality. Because of the inherent characteristics of image and point clouds, point clouds help better utilize geometric structures in image. Due to the sparsity of point clouds, transferring detailed context from images is difficult. Therefore, we set the LiDAR branch as teacher network and the camera branch as student. To constrain the feature similarity only on occupied voxels, we propose a soft-supervision loss function as followed:\n$L_{distill} = \\frac{1}{HWD} \u03a3_{x}^{H}\u03a3_{y}^{W}\u03a3_{z}^{D} F(x, y, z)$,\n$F(x, y, z) = M_{x,y,z} \\frac{f_I(x, y, z) \u00b7 f_L(x, y, z)}{||f_I(x, y, z)||_2 ||f_L(x, y, z)||_2}$,\nwhere $f_I(x, y, z) \u2208 R^C$ is the features indexed as $(x, y, z)$ in the image voxel features $F_I$, $f_L(x, y, z)$ is from LiDAR voxel features $F_L$ as well, and $M_{x,y,z} = 1$ if the annotation of $(x, y, z)$ is occupied (non-empty and non-noise), otherwise $M_{x,y,z} = 0$.\nF. Training Loss\nIn the method above, we use multiple loss functions to supervise the network. For the knowledge transferring, we introduce distillation loss $L_{distill}$. For the occupancy prediction, we use cross-entropy loss $L_{ce}$ and lovasz-softmax loss $L_{ls}$ [2]. The total function as follows:\n$L_{total} = L_{ce} + L_{ls} + \u03bb L_{distill} + L_d$"}, {"title": "A. Dataset", "content": "Experiments are conducted on WildOcc dataset, which is proposed in this work. WildOcc is split into a training set with 7500 frames, a validation set with 1250 frames, and a test set with 1250 frames. The method is evaluated on the test set of WildOcc. Because the data collector is a diminutive mobile robot [5] with monocular camera and LiDAR, we set the occupancy prediction range as [0, 20m] for X axis, [-10m, 10m] for Y axis and [-2m, 6m] for Z axis. The shape of occupancy is 100 \u00d7 100 \u00d7 40 with voxel size of 0.2m. The test set has 7 semantics categories.\nB. Evaluation Metrics\nFollowing on-road occupancy benchmarks [2], [3], [9], the setting of off-road evaluation metrics is the same with these benchmarks:\n$IoU = \\frac{TP}{TP + FP + FN}$,\n$mIoU = \\frac{1}{N} \u03a3_{i=1}^{N} \\frac{TP_i}{TP_i + FP_i + FN_i}$,\nwhere TP, FP, FN represent the number of true positive, false positive and false negative voxel prediction. N is the number of semantic classes.\nC. Implementation Details\nWildOcc is derived from 5 scenes of Rellis-3D [5]. In each scene, consecutive 2000 frames of point clouds are collected to form WildOcc with 10000 frames. For each scene, the first 1500 frames are set as training set, the middle 250 frames are set as validation set and the last 250 frames are set as test set. During multi-frame aggregation, we stitch consecutive 400 frames at once. The depth parameters of coarse and fine Poisson Reconstruction are set as 8 and 13 separately. k is set to be 15 in the period of semantic labeling.\nIn the camera branch, raw images are resized into 896 \u00d7 1600. ResNet101 [25] and FPN [26] are utilized as the image encoder. To generate dense depth maps, LiDAR points are projected onto the image, followed by performing depth completion [32] to make dense result. For LiDAR branch, the input are 10 LiDAR sweeps. We voxelize the sweeps and utilize a voxel encoder to extract features. The spatio-temporal alignment aggregates 4 frames (including current frame) with a 0.5s time interval. And the structure of occupancy head is the same in each branch. In training period, the LiDAR branch is trained firstly and frozen as teacher network after achieving the best performance. When training the camera branch, distillation function is utilized to transfer knowledge. And the hyper-parameters A is set as 0.8 in our experiments. Finally, the multi-modal branch is trained. Each branch of the model is trained for 20 epochs, with a batch size of 4 across 4 RTX 6000Ada GPUs. We utilize the AdamW [33] optimizer with a weight decay of 0.01 and an initial learning rate of 3e-4.\nD. Main Results\nComparisons with several on-road methods on various modalities\nTable II demonstrates C-OFFOcc achieves the highest mIoU among camera-only methods. With geometric knowledge from LiDAR branch, C-OFFOcc has a increase of 2.2% IoU on the tree category compared to FB-Occ [1] and 1.4% IoU on the bush cat-egory compared to OccFormer [31]. These two categories contains rich geometric and height information.\nE. Ablation Study\nTo verify the effectiveness of temporal alignment and com-ponents combination, we conduct several ablation studies. The Temporal Alignment column represents whether to use spatio-temporal alignment. In our experiment setting, we only set camera branch as the student network. The Cross-Modality Distillation column represents whether to employ knowledge distillation.\nBy comparing row 1 and row 2, row 3 and row 4 in Table III, the performance increases 1.2 mIoU and 0.5 mIoU respectively.\nIn Table IV, the results between different distillation func-tions demonstrate our soft-supervision distillation function improve the performance of cross-modality distillation as well.\nVI. CONCLUSION\nIn this paper, we propose the off-road 3D semantic oc-cupancy benchmark WildOcc. A new pipeline is designed to generate dense occupancy annotations, and the dense annotations is closer to real scenes. In addition, we introduce a method OFFOcc to predict occupancy. The camera branch and LiDAR branch can work independently. For the camera branch and LiDAR branch, we design a spatio-temporal alignment module to utilize temporal information from his-torical frames at voxel level. A cross-modality distillation function is proposed to transfer geometric information from LiDAR to camera. Owing to these improvements, our model OFFOcc achieves high performance on off-road benchmark WildOcc. There still exists some limitations in our work. The small occupancy prediction range (20m) is one of current limitation, mainly due to the use of a diminutive mobile robot. The relatively small frame count is another. In the future work, we plan to construct an off-road dataset with equal distribution and large size, collected by a vehicle."}]}