{"title": "MULTISCALE BYTE LANGUAGE MODELS\nA Hierarchical Architecture for Causal Million-Length Sequence Modeling", "authors": ["Eric Egli", "Matteo Manica", "Jannis Born"], "abstract": "Bytes form the basis of the digital world and thus\nare a promising building block for multimodal\nfoundation models. Recently, Byte Language\nModels (BLMs) have emerged to overcome tok-\nenization, yet the excessive length of bytestreams\nrequires new architectural paradigms. Therefore,\nwe present the Multiscale Byte Language Model\n(MBLM), a model-agnostic hierarchical decoder\nstack that allows training with context windows of\n5M bytes on single GPU in full model precision.\nWe thoroughly examine MBLM's performance\nwith Transformer and Mamba blocks on both uni-\nmodal and multimodal tasks. Our experiments\ndemonstrate that hybrid architectures are efficient\nin handling extremely long byte sequences dur-\ning training while achieving near-linear genera-\ntional efficiency. To the best of our knowledge,\nwe present the first evaluation of BLMs on visual\nQ&A tasks and find that, despite serializing im-\nages and the absence of an encoder, a MBLM\nwith pure next token prediction can match cus-\ntom CNN-LSTM architectures with designated\nclassification heads. We show that MBLMs ex-\nhibit strong adaptability in integrating diverse\ndata representations, including pixel and image\nfilestream bytes, underlining their potential to-\nward omnimodal foundation models. Source code\nis publicly available at: https://github.\ncom/ai4sd/multiscale-byte-lm.", "sections": [{"title": "1. Introduction", "content": "From the perspective of a traditional computational linguist,\nthe success of Language Models (LMs) in NLP is a suc-\ncess story of replacing inductive biases with data-agnostic\ncomputation blocks. Yet, (sub)-word tokenization has re-\nmained a cornerstone of any LM workflow, inducing strong\nassumptions about the structure of text and hampering out-\nof-distribution learning. In contrast, tokenization-free mod-\nels reduce the preprocessing overhead (Xue et al., 2022) by\nutilizing bytes as a universal encoding format and enable\nseamless adaptation to diverse languages and modalities\n(Wang et al., 2020; Li et al., 2019). Additionally, byte-\nlanguage models (BLMs) pre-trained on mixed-modality\ndatasets exhibit performance comparable to models trained\non specific modalities (Wu et al., 2024). To address the\nchallenges of long sequences and the computational over-\nhead associated with byte-level granularity, prior work has\naimed to mitigate the quadratic complexity of autoregressive\nTransformers with computationally more efficient, hierar-\nchical Transformers (Yu et al., 2023; Pagnoni et al., 2024)\nor Mamba models optimized for fast inference (Wang et al.,\n2024). However, these approaches depend on modality-\nspecific optimizations and model-specific features, which\nlimit their generalization and scalability.\nAs shown in Figure 1, we here introduce the Multiscale\nByte Language Model (MBLM), a model- and modality-\nagnostic architecture for causal byte language modeling.\nMBLMs extend the MegaByte hierarchy (Yu et al., 2023) to\nan unlimited number of stages, and predict the next byte of\na large input bytestream by refining input sequence repre-\nsentations through a hierarchy of generic decoder models,\nwhile enabling precise control over stage parallelism. By\nintegrating Transformer decoders (Vaswani et al., 2017)\nwith Mamba (Wang et al., 2024; Dao & Gu, 2024), we\ndemonstrate that hybrid hierarchies minimize computational\nrequirements during both training and inference, outper-\nforming other architectures when handling input sequences\ncomprising hundreds of thousands of bytes. MBLMs pro-\nvide granular control over the trade-off between parallelism\nand compute time by selectively checkpointing intermediate\nactivations, enabling efficient training on sequences span-\nning up to 5M bytes on single GPU. Our approach supports\nmodality-independent pre-training and fine-tuning on mul-\ntimodal downstream tasks. In a novel application of byte"}, {"title": "2. Related work", "content": "The MBLM builds upon the design principles of MegaByte\n(Yu et al., 2023), a causal byte language model featuring\na hierarchical architecture of two Transformer decoders,\nenabling subquadratic self-attention and context windows\nup to 1.2M bytes. MegaByte processes patch representa-\ntions of the input sequence with a global decoder, refines\nthese representations, and feeds them into a local model\nthat autoregressively predicts individual bytes. Incorporat-\ning the Mamba architecture (Gu & Dao, 2023; Dao & Gu,\n2024) at the byte level, MambaByte (Wang et al., 2024)\ndemonstrated superior performance over MegaByte in a\nFLOP-controlled setting across various datasets. As an alter-\nnative to the fixed-size patching used in MegaByte, Pagnoni\net al. (2024) proposed the Byte Latent Transformer (BLT),\nwhich dynamically segments bytes into patches based on\nthe entropy of the next byte. BLT demonstrated that byte\nlanguage models can be efficiently scaled, achieving per-\nformance comparable to a subword-based LLama 3 model\n(Grattafiori et al., 2024) at the 8B parameter scale.\nHowever, none of these approaches have demonstrated the\ncapability to handle multimodal inputs, which is arguably\nthe most inherent strength of byte-level models. As shown\nby Wu et al. (2024) with bGPT, extending pre-training to\ninclude binary data from mixed modalities facilitates effec-\ntive cross-modality knowledge transfer. This reinforces the\nhypothesis that byte-level models uniquely capture features\nand patterns in ubiquitous bytestreams, irrespective of the\noriginal data format. Nevertheless, limited focus has been\nplaced on architectures capable of translating multimodal\ninputs into multimodal outputs. Perceiver IO (Jaegle et al.,\n2022) addresses this by mapping inputs of arbitrary size into\na latent space using a latent array that encodes the semantics\nof the input. The latent representation is iteratively refined\nthrough a series of attention modules and subsequently de-\ncoded into outputs of arbitrary shape via an output query\narray. Due to the encoder and decoder attention modules\nscaling linearly with the input and output size, and most of\nthe computation occurring in the latent attention modules,\nPerceiver IO can efficiently handle extremely large input\nand output dimensions. Yet, PerceiverIO explores bytes only\nto represent text and thus, to date, we still lack applications\nof BLMs on multimodal tasks like visual Q&A."}, {"title": "3. Methods", "content": "3.1. MBLM\nThe MBLM module consists of N causal decoder mod-\nels $M_{i<N}$ that are stacked hierarchically. The first $N - 1$\nstages $M_1,..., M_{N-1}$ contain global models, while the\nfinal stage $M_N$ contains the local model. Each model\n$M_i$ operates on inputs with a hidden state of dimension\n$D_i \\in \\mathbb{R}$ and a patch/context size $P_i \\in \\mathbb{R}$. Inputs to an\nMBLM module are sequences of B batches, each of length\nL. The vocabulary V consists of 256 tokens for byte-level\nsettings. Similar to MegaByte (Yu et al., 2023), MBLMs\nscale through input length compression and aim to operate\non sequences of length $L_{max} = \\prod_{i=1}^{N} P_i$.\nPATCH EMBEDDER\nMBLMs employ a patch embedder that inputs a discrete\nsequence $x \\in \\mathbb{R}^{B \\times L}$, embeds each element, adds positional\nencodings and then chunks it into patches for each stage.\n1. Embed the bytes in x for each stage i:\n$x_{emb} \\in \\mathbb{R}^{B \\times L \\times D_N} = E_{emb}(x) + E_{pos}(x)$ (1)\nAlthough we always use $D_N$ as the embedding dimen-\nsion, each model learns its own byte embedding. $E_{emb} \\in$\n$\\mathbb{R}^{V \\times D_N}$ and $E_{pos} \\in \\mathbb{R}^{P_i \\times D_N}$ are thus specific to each stage.\n2. Reshape $x_{emb}$ to a nested sequence of patch embeddings\n$P_{emb}$ for all i:\n$x_{emb} \\xrightarrow{\\text{reshape}} P_{emb}$ $P_{emb} \\in \\mathbb{R}^{B \\times P_1 \\times ... \\times P_N \\times D_N}$ (2)\nIf L cannot be factored into $P_1 \\times ... \\times P_N$, the inner se-\nquence lengths $P_2$ to $P_N$ are padded with a padding token.\nIn case $L > L_{max}$ and no positional embeddings are ap-\nplied, we additionally allow $P_1$ to be larger than specified,"}, {"title": "3.2. Stage Models", "content": "All previous work on hierarchical models has been limited\nto two stages with Transformer decoders as stage block (Yu\net al., 2023; Wu et al., 2024). Training a model on this objec-\ntive requires careful patchification. Specifically, within the\nsame patch, no information to future tokens must be leaked,\nwhich MBLM achieves by offsetting the inputs between\nstages with trainable start tokens. A derived requirement\nfor the models contained within an MBLM is therefore\nthat they are autoregressive. Hierarchical architectures for\nTransformer decoders have historically aimed to reduce\nthe quadratic cost of self-attention. However, we argue\nthat even models with linear scaling properties like Mamba\n(Gu & Dao, 2023) can benefit from compression through\npatchification. Mamba is a type of state space model (SSM)\nwith a selection mechanism. SSMs describe the evolution\nof a physical system over time and are parametrized by\nthe system matrix $A \\in \\mathbb{R}^{N \\times N}$, input and output matrices\n$B \\in \\mathbb{R}^{N \\times 1}$ and $C \\in \\mathbb{R}^{1 \\times N}$ as well as the step-size $\\Delta \\in \\mathbb{R}$,\nwhich defines the resolution of the input. Continuous-time\nSSMs define a function-to-function map:\nState equation : $h'(t) = Ah(t) + Bx(t)$ (9a)\nOutput equation : $y(t) = Ch(t) + Dx(t)$ (9b)\nDx(t) can formally be omitted because it constitutes a skip-\nconnection. To discretize the continuous-time system de-\nfined in Equation 9, Gu & Dao (2023) apply the zero-order\nhold discretization rule, resulting in the discrete parameters\n$\\bar{A} = exp(\\Delta, A)$ and $\\bar{B} = (\\Delta\\bar{A})^{-1}(exp(\\Delta\\bar{A}) - I) \\cdot \\Delta B$.\nThe discrete SSM can be computed like a recurrent neural\nnetwork (RNN) that independently maps each channel D\nof an input $x \\in \\mathbb{R}^D$ at a time step t to an output $y \\in \\mathbb{R}^D$\nthrough a higher dimensional latent state $h \\in \\mathbb{R}^N$ (Gu et al.,\n2022). Moreover, the recurrence can be unrolled given the\ninitial state $h_0$ and vectorized into the SSM convolutional\nkernel K (Gu et al., 2022) for a sequence length L:\n$K_{\\mathbb{R}^L} = (CB, C\\bar{A}B, \\dots, C\\bar{A}^{L-1}B)$ (10a)\n$y = K * x$ (10b)\nThe use of a convolutional kernel for efficiency requires\nthat (1) the model is linear time-invariant (LTI), meaning\nthat $\\bar{A}, \\bar{A}$ and B are fixed for all time steps and (2) $\\bar{A}$ is\nstructured, with the most popular form of structure being\ndiagonal (Gu et al., 2024). This class of SSM is called\nstructured SSM (Gu et al., 2022). In contrast, Mamba is\na selective SSM that makes the parameters $\\bar{A}$, B and C\nfunctions of the input via linear projections. To compute\nthe time-varying parameters efficiently, the model cannot\nuse a convolution, which assumes a fixed kernel. Instead, it\nleverages a parallel associative scan (Harris et al., 2024) as\npart of a hardware-aware algorithm with linear complexity\nthat computes the model during training when the entire"}, {"title": "3.3. Datasets & Evaluation", "content": "We evaluate the performance of MBLMs in terms of lan-\nguage modeling on the Project Gutenberg (PG19) dataset\n(Rae et al., 2019). PG19 contains 28,752 English-language\nbooks, or 11.6 GB of text, which were published before\n1919. We select this dataset for comparability to prior art (Yu\net al., 2023; Wang et al., 2024) and because, on average,\neach book is around 411 KB, which allows long-range lan-\nguage modeling on consecutive bytes in the same document.\nFor the multimodal evaluation, we train models on CLEVR\n(Johnson et al., 2016), a labelled dataset for a visual question\nanswering. CLEVR contains 70,000 synthetically generated\nRGB images containing 3D shapes, 28 unique answers and\nroughly 700,000 questions requiring perceptual abilities\nsuch as recognizing or counting objects, inferring relation-\nships or making comparisons, with an example provided in\nFigure 3. Additional statistics are included in Appendix A.\nWe use bits-per-byte (BPB) (Gao et al., 2020) as the primary\nevaluation metric for byte-level modeling and report word-\nlevel perplexities (PPL) to facilitate comparisons with future\nwork. BPB, related to perplexity, quantifies the average\nnumber of bits needed to encode each byte of data and can\nbe seen as a compression measure where a lower value\nindicates a higher probability of correctly predicting the"}, {"title": "4. Results", "content": "For modeling long byte sequences, we take advantage of\nMBLMs' ability to combine different models at each stage\nand combine Transformer decoder with Mamba-2 models in\ndifferent constellations. All models are referenced by their\ndimensionality; 1D MBLMs contain only a single stage with\neither a Transformer decoder or Mamba-2 model. In our\nimplementation, a single-stage hierarchy operates with nu-\nmerical equivalence to the model when used independently\nof the hierarchy.\n4.1. Scaling Byte Language Models\nAs the first three-stage (3D) hierarchical model of its kind,\nan MBLM comprising a global Mamba followed by two\nTransformer decoders can process byte sequences of 5 mil-\nlion bytes during training on a single A100 80 GB GPU\nwith standard automatic mixed precision. After just over 15\nhours of training this 350M-parameter model processed 100\nGB of UTF-8 bytes and achieved 2.448 BPB on the PG19\ntest set (Figure 4).\nBy employing a multiscale hierarchy with MBLMs, we tar-\nget training sequence lengths that ordinary, non-hierarchical\n(1D) sequence models cannot process without exhausting\nGPU memory. As shown in Table 1, the same Transformer\ndecoder backbone scales to twice the sequence length when\nincorporated into a two- or three-stage MBLM, owing to op-\ntimized computational efficiency through input compression.\nNaturally, since MBLMs scale by compressing the input\nsequence, regular 1D models outperform hierarchical mod-\nels when the sequence fits into memory. This underscores\nthat hierarchical architectures are specifically designed for\nextremely long-sequence modeling."}, {"title": "5. Discussion", "content": "In this work, we introduced the Multiscale Byte Language\nModel (MBLM), a hierarchical, model-agnostic architecture\ncapable of scaling to byte sequences as long as 5 million\nbytes on a single GPU. The MBLM hierarchy operates\nin stages, with independent autoregressive models at each\nstage. Byte sequences are divided into patches, embedded,\nand refined as they pass through the hierarchy, culminating\nin a local model that autoregressively predicts bytes within\neach patch. This approach enables efficient processing of\nvery long byte sequences through compression.\nOur language modeling experiments demonstrated that\nMBLMs can handle unprecedented sequence lengths. While\nMamba-based hierarchies performed best, hybrid models\ncombining Mamba for global stages and Transformer de-\ncoders for local stages achieved an optimal balance between\nperformance and computational efficiency. Hybrid models\nalso converged faster and exhibit near-linear generational\nefficiency during inference. The novel evaluation of byte-\nlanguage models on the task of visual question answering\nrevealed that autoregressive models can perform compet-\\itively to CNN baselines, even with a language modeling\nhead and when learning from a mixed-modality byte stream.\nWe recommend extending MBLM evaluations to tasks re-\nquiring long contexts, such as multimodal document sum-\nmarization or needle in a haystack tasks and investigating\ntheir performance when scaled to billions of parameters.\nThe MBLM architecture, available on GitHub and as a PyPi\npackage, provides a modular and flexible framework for fur-\nther development. Its scaling capabilities can be enhanced\nthrough features like tensor parallelism or model sharding,\nwhich can seamlessly integrate into the hierarchy regard-\nless of the stage models used. With the right technical\nextensions, we believe MBLMs are well-suited to process\nsequences spanning tens of millions of bytes. These oppor-\ntunities position MBLMs as a strong foundation for tackling\nmillion-scale byte sequence modeling and driving future\ninnovations in hierarchical architectures."}, {"title": "Impact Statement", "content": "This paper presents work whose goal is to advance the field\nof Machine Learning. Our work is particularly focused\non improving context window length in language models\nwhich may allow future algorithms to sift through larger\namounts of data in one shot. There are many potential\nsocietal consequences of such work, none which we feel\nmust be specifically highlighted here."}]}