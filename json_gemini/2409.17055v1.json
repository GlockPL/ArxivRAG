{"title": "DRIM: Learning Disentangled Representations from Incomplete Multimodal Healthcare Data", "authors": ["Lucas Robinet", "Ahmad Berjaoui", "Ziad Kheil", "Elizabeth Cohen-Jonathan Moyal"], "abstract": "Real-life medical data is often multimodal and incomplete, fueling the growing need for advanced deep learning models capable of integrating them efficiently. The use of diverse modalities, including histopathology slides, MRI, and genetic data, offers unprecedented opportunities to improve prognosis prediction and to unveil new treatment pathways. Contrastive learning, widely used for deriving representations from paired data in multimodal tasks, assumes that different views contain the same task-relevant information and leverages only shared information. This assumption becomes restrictive when handling medical data since each modality also harbors specific knowledge relevant to downstream tasks. We introduce DRIM, a new multimodal method for capturing these shared and unique representations, despite data sparsity. More specifically, given a set of modalities, we aim to encode a representation for each one that can be divided into two components: one encapsulating patient-related information common across modalities and the other, encapsulating modality-specific details. This is achieved by increasing the shared information among different patient modalities while minimizing the overlap between shared and unique components within each modality. Our method outperforms state-of-the-art algorithms on glioma patients survival prediction tasks, while being robust to missing modalities. To promote reproducibility, the code is made publicly available at https://github.com/Lucas-rbnt/DRIM.", "sections": [{"title": "1 Introduction", "content": "Cancer diagnosis and prognosis are usually determined by an ensemble of data obtained from various sources including MRI, histopathology slides, and molecular profiles. Combining these modalities allows to derive a panoply of biomarkers, offering a deeper understanding of the tumor landscape. Thereby, it enables clinicians to provide patients with the utmost support throughout their pathology [12]. In fields beyond medicine, there are several deep learning models and techniques capable of merging multimodal data [15,25,31,18]. However, in a medical context, the prevalent challenge of integrating incomplete heterogeneous modalities steers deep learning techniques towards simpler multimodal or even unimodal-based models [30,2]. While some multimodal approaches rely on late fusion [27] i.e. averaging outputs from each modality, most popular methods focus on merging modalities in the latent space [29,8,5,9,21]. Indeed, MultiSurv [29] explores the utility of straightforward fusion methods such as mean, sum, product, and maximum, testing their capacity to handle missing modalities within survival prediction tasks. Although simple, another fusion technique that has showcased its effectiveness in the medical domain involves the concatenation of the representations [27]. Finally, [31] introduced a parameterized approach called Tensor, which leverages the outer product of representations. This method was later applied in [9] to combine histology slides and genomic data. Nonetheless, this method falls short in naturally accommodating missing modalities and faces a critical limitation as the amount of parameters it requires increases exponentially with each added modality. This excessively high number of parameters hinders its effectiveness for combining multiple modalities as it drastically limits the representation dimensions.\nBeyond mere fusion, leveraging auxiliary losses on representations to induce specific behaviors is a strategy adopted in previous works [8,5,32]. Hence, [8,32] embed a correlation function within representations to promote consistency. Conversely, [5] adopts a divergent strategy, denoted multimodal orthogonalization (MMO), striving for their representations to minimize redundancy and overlapping information across modalities.\nIn this paper, unlike to prior work, we make the natural assumption that information contained in a modality splits into patient-related aspects shared across modalities and unique features specific to each modality. Our key contributions are: (i) employing a pair of encoders for each modality one shared, one unique. Optimizing the shared encoder can be viewed through the lens of Information Bottleneck [28], ensuring that representations are rich in patient-specific information yet minimally captures the modality's characteristics. Concurrently, unique encoders aim to reduce mutual information between their representation and the corresponding shared one. (ii) Leveraging these disentangled representations, we introduce a dual-scale fusion approach: it initiates with the aggregation of shared information, later combined with unique information to craft a comprehensive representation pertinent for survival analysis. (iii) We employ an attention-based, scalable and parameterized fusion that intuitively models interactions between representations and natively manages missing modalities.\nGliomas, with their diverse prognosis biomarkers, range of grades, and complexity as described in the WHO 2021 classification [24], present a compelling use case for showcasing DRIM's ability to effectively capture and disentangle intricate information."}, {"title": "2 Method", "content": "Consider a training minibatch of N samples and M modalities, with $x^m = {x_i^m}_{i=1}^N$ representing data from the N samples for a specific modality m. For each modality m, let $o_m : x^m \\rightarrow s^m \\in \\mathbb{R}^{N\\times d}$ and $g_m : x^m \\rightarrow u^m \\in \\mathbb{R}^{N\\times d}$ denote the shared and unique encoders respectively, with $s^m$ and $u^m$, being the representations of the m-th modality for the i-th sample in the minibatch. DRIM makes it possible to learn disentangled representations while being flexible to a range of tasks. Indeed, the DRIM-Surv approach (Fig. 1b) operates by fusing representations at two distinct scales to perform supervised end-to-end survival training. Alternatively, DRIM-U provides a wholly unsupervised option for learning meaningful shared and unique representations as illustrated in Fig. 1c. In both scenarios, the DRIM loss can be dissected into three distinct components as illustrated in Eq. 1: a task-related term, a shared term, and a unique term.\n$L_{DRIM} = L_T + L_{sh} + L_u$ (1)"}, {"title": "2.1 Shared Loss", "content": "To maximize mutual information between shared representations $s^m$, traditional methods optimize the InfoNCE objective [25] for each modality pair [15,8], leading to a computational complexity of O(M\u00b2). Thereby, in Eq. 2, we propose a new well-suited shared cost function (see Fig. 1a). Inspired by supervised contrastive learning [22] wherein class labels facilitate generalization to multiple positive pairs, we stipulate that each representation $s^m$ finds positive pairs among representations stemming from distinct modalities yet originating from the same patient. Thus, all shared representations are concatenated to create the matrix $S = [s_1^1, s_2^1, \\cdots s_1^M, s_2^M, \\cdots s_{N\\cdot M}^1, s_{N\\cdot M}^M]^T \\in \\mathbb{R}^{N\\cdot M\\times d}$. Let $\\mathcal{J} := {1, ..., N \\cdot M}$ represent the indices of representations, $\\mathcal{B}(j) = {b \\in \\mathcal{J} : b \\ne j, \\kappa(b) = \\kappa(j)}$ define the set of its positive pairs, where $\\kappa(j)$ maps j to the patient it belongs to, $\\tau\\in \\mathbb{R}^+$ is a scalar temperature parameter. The shared loss term is formulated as follows:\n$L_{sh} =  \\frac{1}{N \\cdot M} \\sum_{j \\in \\mathcal{J}}  - \\frac{1}{\\vert \\mathcal{B}(j) \\vert} \\sum_{k \\ne j} log  \\frac{exp((s_j^m \\cdot s_k^m)/\\tau)}{\\sum_{b \\in \\mathcal{B}(j)} exp((s_j^m \\cdot s_b^m)/\\tau)} $ (2)"}, {"title": "2.2 Unique Loss", "content": "Until now, we have successfully derived shared representations ${s^m}_{m=1}^M$. The unique representation $u^m$, extracted by $g_m$, parameterized by $\\psi_m$, should highlight features specific to its modality m without encompassing the information contained in $s^m$. To this end, we use an adversarial objective [16], akin to the approach outlined in [26], to minimize the mutual information $I(s^m, u^m)$. A discriminator $D_m$, parameterized by $\\Theta_m$ is trained for each modality m to differentiate representations sampled from the joint distribution from those sampled from the product of the marginals as shown in Fig. 1a. Simulating samples from the product of the marginals $P_{s_u}P_u$ involves shuffling the batch of shared representations before pairing with unique ones as in [4]. This adversarial objective outlined in Eq. 3 drives the minimization of the Jensen-Shannon divergence $D_{JS}(P_{S,U}||P_{S}P_{U})$, as evidenced in [16].\n$L_u(\\Theta,\\Psi) =  \\sum_{m=1}^M  \\mathbb{E}_{P_{S,U}} [log(1 - D_m(s^m, u^m); \\Theta_m)] +  \\mathbb{E}_{P_{S}P_{U}} [log D_m(s^m, u^m); \\Theta_m]] +  \\mathbb{E}_{r_{S,V}} [log D_m(\\tilde{s}^m, \\tilde{u}^m);\\psi_m]] $ (3)"}, {"title": "2.3 Task Loss", "content": "Merely separating shared and unique components does not guarantee the relevance of the learned unique representations. The training of $g_m$ might still collapse, risking a meaningless latent space. Hence, to ensure we capture of pertinent information, each unique encoder is tied to a specific task. Indeed, in DRIM-Surv, unique encoders are also linked to a survival task, compelling them to identify modality-specific relevant features. To delve deeper, let ${x_i, t_i, d_i}_{i=1}^N$ denote our minibatch where $t_i$ represents the time of last follow-up and $d_i$ indicates if the event was observed (\u03b4\u1d62 = 1) or censored (\u03b4\u1d62 = 0). Time is divided into P equidistant fixed intervals, and a neural network $f(x) = [h_1,\\cdots,h_p] \\in [0,1]^P$ aims to estimate the conditional hazard probability for each time interval. Let also $v : \\mathbb{R} \\rightarrow {1,\\ldots, P}$ be a function that maps a follow-up t to the the interval in which it lies. The survival loss function used is an extension of [13] provided by [23] allowing predicted hazards to be non-proportional.\n$L_T = - \\frac{1}{N} \\sum_{i=1}^N \\sum_{k=1}^{N_u(t_i)} 1(v(t_i)=k, d_i=1) log h_k + (1 - 1(v(t_i)=k, d_i=1)) log (1 - h_k)$ (4)\nMoreover, with the DRIM-U alternative, each $g_m$ is also optimized to encode and decode inputs through a specific decoder $R^m (g^m(x^m)) = x^m$, minimizing reconstruction error $L_T = \\sum_{m=1}^M L_m$ and $L_m = \\sum_i ||x_i^m - \\hat{x}^m||_2$ as in Fig. 1c."}, {"title": "2.4 Multimodal Fusion", "content": "In DRIM-Surv, we merge representations on two scales: first, $h_s$ fuses shared representations into a global shared vector $s \\in \\mathbb{R}^{N\\times d}$; then, this is combined with unique representations via $h_u$ to form a unified patient embedding, depicted in Fig. 1b. In both fusion, a masked transformer processes each representation as a sequence token, with a classification token <cls> added at the beginning, akin to a ViT approach [11]. The output of the transformer for this token acts as the fused representation, as shown in Fig. 1b. Specifically, $h_u$ aggregates the M unique representations plus the global shared ones through $h_u([u^1,\\ldots,u^M,s]) = softmax(\\frac{QK^T}{\\sqrt{d_v}})V$ where $Q_{cls} = q_{cls}W_Q$, $K = C W_K$ and $V = CW_V$ are linear transformations of the learnable query token <cls>, and representations $C = [<cls>,u^1,\\ldots, u^M, s]$ respectively and D the projection dimension. In practice, we use multiple attention heads. This fusion, termed MAFusion, offers scalability, computational efficiency, robust handling of missing modalities through masking and provides interpretability via attention maps."}, {"title": "3 Experiments", "content": "This study leverages multimodal glioma data from TCGA [7], specifically GBM and LGG collections, across four modalities: DNA methylation, (DNAm), RNA sequencing (RNA), histopathological slides (WSI) and MRI. MRI data was completed from the BraTS dataset [3], selecting only TCGA-listed patients. Focusing on patient with at least two modalities, we kept 881 samples, finding coverage rates of 95% for DNAm, 89% for WSI, 75% for RNA, and 18% for MRI.\nGenomics (DNAm and RNA): DNA methylation data were processed into 25,978 Beta values using the same method as in [29]. RNA data were processed to retain protein-coding genes with variance over 0.1, resulting in 16,304 FPKM-UQ normalized genes, then log-transformed and z-score normalized. Both genomics encoders comprise six 1D convolutional layers with channels increasing from 1 to 256. Each layer applies GELU activation [20], batch normalization, and dropout, ending with Adaptive Average Pooling to produce a fixed-size embedding.\nWSI: Slides were subsampled, and a mask designed to isolate tissue zones was extracted using an OTSU filter. From these, 1000 high-resolution patches of size 256x256 were randomly selected, of which 100 were chosen based on a custom HSV intensity score (Fig. S1). A SimCLR technique [10] was employed to train a ResNet34 [19] on the chosen patches. Then, the WSI encoder, a single-layer transformer with 8 attention heads and no positional encoding, projects 10 randomly selected patch representations into a meaningful embedding.\nMRI scans: Segmentation masks were used to crop a 64 \u00d7 64 \u00d7 64 voxel region surrounding the tumor in Gd-T1w and FLAIR MRI scans. A SimCLR approach [10], similar to the WSI one, is used to train a MO\u039d\u0391\u0399 ResNet10-3D [6] across the BraTS dataset [3]. In our DRIM experiments, we froze the encoder weights and added a 256-unit linear layer with dropout and GELU activation [20], followed by a dense layer.\nImplementation details: Models were trained using an AdamW(lr=1e-3, wd=1e-2) optimizer over 30 epochs with a Cosine Annealing Scheduler and a batch size of 24. For all experiments, the representation size was set to 128, except in those involving Tensor [31], where it was reduced to 32 due to memory constraintsapproaching 34,500 million parameters at a dimension of 128 (Table S1). The final layer of each model consisted of 20 units with sigmoid activation. The MAFusion utilized 16 attention heads of size 64 and a dense layer of size 128. In DRIM experiments, discriminators were also optimized via AdamW (lr=1e-3, wd=3e-4), and the loss coefficient y is set empirically to 0.8. For the DRIM-U evaluation, encoders were pre-trained for 30 epochs as detailed in Section 2 and then frozen. Subsequently, a DRIM-Surv module was fine-tuned on top of these representations over 10 epochs for survival prediction.\nEvaluation: The dataset is divided into a training set and a test set, with the latter comprising 20% of the entire dataset. We use five-fold cross-validation on the training set, with performance metrics reported as mean standard deviation on the test set from models trained on 4 (out of 5) training folds. Evaluation metrics include the Concordance Index (C-index) [1], Integrated Brier Score (IBS) [14,17], and Integrated Negative Binomial Log-likelihood (INBLL) [17,23]."}, {"title": "4 Results and discussions", "content": "The data in Table 1 demonstrate that the DRIM-Surv approach outperforms several other methods across all three assessed metrics. This superiority highlights the strong ability of our method to accurately discriminate patient outcomes (C-index) as well as to reliably forecast survival probabilities over time (IBS, INBLL). Notably, DRIM-Surv not only slightly exceeds the performance of Tensor [31] in all metrics, but it also offers advantages in terms of scalability to additional modalities, reduced training time, and a significantly lower parameter count (Table S1 and Table S2). Indeed, except for the new encoders parameters, incorporating an additional modality into the DRIM framework leaves its parameter count unchanged, while the parameter total for Tensor [31] would escalate from 38 millionalready near 70 times larger than those of our MAFusionto exceed one billion (Table S1). The sensitivity analysis on the y coefficient illustrates that, beyond the neural network architecture, our method effectively learns to disentangle the representations of each modality, thereby enhancing the reliability of the final prognosis (Table S3). Finally, our unsupervised DRIM-U approach also gives convincing results simply by being fine-tuned on a few epochs, validating the flexibility of our method."}, {"title": "4.2 Stratifying high-risk vs low-risk patients", "content": "Additionally, we evaluate the proficiency of our model to stratify patients into high-risk and low-risk categories, a crucial step towards advancing personalized medicine. Concretely, for a given model, patients are divided into two groups based on their cumulative predicted hazards: those in the top half are classified as high-risk, while those in the bottom half are considered low-risk. Survival curves for each group are derived using Kaplan-Meier estimators, with a log-rank test evaluating their differences and stratification efficacy. For DRIM-Surv, The logrank test yields a p-value of 2.9e-23, against 4.2e-22 for the Tensor [31] w/ MMO [5] method, emphasizing the ability of our model to distinguish survival outcomes between the two risk groups (Fig. S2)."}, {"title": "4.3 Robustness across varied input modality combinations", "content": "One of the primary benefits of our approach lies in its ability to smoothly manage missing modalities during inference, avoiding the use of zero-filled tensors required by other methods [31,5,27]. Indeed, DRIM goes beyond simple arranged modality combination; it adeptly models their interactions and efficiently extracts crucial information, showcasing advanced data understanding."}, {"title": "5 Conclusions and perspectives", "content": "We propose DRIM, an approach for disentangling data representations from highly heterogeneous and incomplete sources. Our method not only enhances the forecasting of patient outcomes through an adept handling of input modalities but also demonstrates improved scalability and flexibility with the addition of new modalities, unlike classic multimodal techniques. This versatility ensures its applicability to a broad spectrum of datasets featuring diverse modalities and tasks. Finally, we aspire that the ongoing efforts to disentangle learned representations of modalities will pave the way for deeper analysis and enhanced interpretability. This development promises to unlock precise, individualized patient assessments, thereby bridging the gap to personalized medecine and practical clinical adoption."}]}