{"title": "Distilling Vision-Language Foundation Models: A Data-Free Approach via Prompt Diversification", "authors": ["Yunyi Xuan", "Weijie Chen", "Shicai Yang", "Di Xie", "Luojun Lin", "Yueting Zhuang"], "abstract": "Data-Free Knowledge Distillation (DFKD) has shown great potential in creating a compact student model while alleviating the dependency on real training data by synthesizing surrogate data. However, prior arts are seldom discussed under distribution shifts, which may be vulnerable in real-world applications. Recent Vision-Language Foundation Models, e.g., CLIP, have demonstrated remarkable performance in zero-shot out-of-distribution generalization, yet consuming heavy computation resources. In this paper, we discuss the extension of DFKD to Vision-Language Foundation Models without access to the billion-level image-text datasets. The objective is to customize a student model for distribution-agnostic downstream tasks with given category concepts, inheriting the out-of-distribution generalization capability from the pre-trained foundation models. In order to avoid generalization degradation, the primary challenge of this task lies in synthesizing diverse surrogate images driven by text prompts. Since not only category concepts but also style information are encoded in text prompts, we propose three novel Prompt Diversification methods to encourage image synthesis with diverse styles, namely Mix-Prompt, Random-Prompt, and Contrastive-Prompt. Experiments on out-of-distribution generalization datasets demonstrate the effectiveness of the proposed methods, with Contrastive-Prompt performing the best.", "sections": [{"title": "1 INTRODUCTION", "content": "The emergence of deep neural networks has led to rapid advances in deep learning [5, 6]. These models can be applied to a wide range of downstream tasks, but their ever-growing model parameters inhibit applications on resource-constraint edge devices. Knowledge distillation [19, 21, 36] is one of the most prevalent paradigms to solve this problem. Despite the promising progress, these approaches rely heavily on the original data, which is not always available due to copyrights and privacy concerns. To overcome this challenge, a few attempts have been made to distill knowledge without using real data, known as Data-Free Knowledge Distillation (DFKD) [14, 15, 25, 34]. Specifically, DFKD adopts a two-step paradigm, including image synthesis and knowledge distillation. Assumed that prior data distribution is implicitly encoded in the teacher model. The surrogate images are first generated by inverting the pre-trained teacher model, which are then utilized in knowledge distillation as an alternative to the original data.\nNevertheless, none of the existing works explore DFKD under distribution shifts, which are commonly encountered in real-world applications [26, 32]. It naturally comes to the demand for a lightweight generalizable model. From another perspective, the recent pioneer Vision-Language Foundation Models [23], e.g., CLIP [20, 29], have been witnessed remarkable success in zero-shot out-of-distribution (OOD) generalization to numerous domain-agnostic downstream tasks, yet suffering from cumbersome parameters and the inaccessible original dataset. In this paper, we take a further step to explore a more ambitious version of DFKD in OOD scenarios to customize a generalizable student from these foundation models which can be robust to any domain shift. We refer to this task as Data-Free Knowledge Distillation from Vision-Language Foundation Models (DFKD-VLFM). Unlike conventional DFKD approaches, the knowledge of publicly-available pre-trained foundation models do not need to be fully utilized. For example, CLIP [29] has the ability to recognize various category concepts, but for downstream tasks in some constrained scenarios, only a limited category space is required. Hence, as shown in Fig.1, we attempt to create a lightweight student model inheriting CLIP's generalizablity for domain-agnostic downstream tasks, based on the specific category concepts required for the tasks.\nDespite several studies [13, 25, 34] experimenting with DFKD on ImageNet, it remains a challenge to invert such a large-scale model to synthesize surrogate training data. Recently, the developed text-to-image models [8, 30] have made significant strides in image generation, yielding high-quality images based on text prompts. In this paper, we adopt one of the most representative text-to-image models, VQGAN-CLIP, as a baseline framework to explore DFKD-VLFM. To create a generalizable student, synthetic data should cover the distribution of downstream tasks as much as possible. As shown in Fig. 3, in the context of vision-language foundation models, diversifying text prompts can serve as a bridge to diversify synthesized images. However, hand-crafting prompt engineering is laborious and empirical. To this end, we aim to develop a Prompt Diversification method that can generate diverse text prompts and implicitly drive the synthesis of diverse surrogate images (Fig. 2), leading to a generalizable student. Three attempts have been made in this paper:\nMix-Prompt. It is intuitive to build a dictionary of style words sourced from the Internet and randomly sample various styles from the dictionary to construct diverse text prompts. To enrich the styles, we create a series of novel styles via a random convex interpolation among different styles in the text embedding space.\nRandom-Prompt. Albeit interpolating the style dictionary, it may fail to yield instances with different postures, appearances and breeds. To create novel styles of the target category, we replace the explicit style word with a pseudo word, which is generated randomly in the textual embedding without using the style dictionary.\nContrastive-Prompt. To further ensure inter-sample diversity, we introduce contrastive learning on the instance level to optimize text prompts. Specifically, the generated text prompt is forced to differ from those in the mini-batch and the historical ones saved in the memory bank."}, {"title": "2 RELATED WORKS", "content": "2.1 Vision-Language Foundation Models\nThe field of AI has seen a surge of interest in vision-language pre-training, which aims to develop generic cross-modal representation to address various downstream tasks. CLIP [29] and ALIGN [23] use image-text contrastive learning with noisy text supervision on billion-level training data to learn cross-modal representation, achieving impressive zero-shot capability. Following their footsteps, DeCLIP [22], SLIP [27] and FILIP [33] have further advanced this field by modifying the optimization objectives. Florence [35] trains on a vast amount of data and uses a unified model to solve extensive vision tasks. However, these models usually consist of an enormous number of parameters, limiting their practical applications. Additionally, the forbidden access to the billion-level training data precludes conventional knowledge distillation. Also, in most constrained scenarios, it is too redundant for a student to acquire all of the teacher's knowledge.\nIn this paper, we propose a new task called DFKD-VLFM that focuses on transferring the generalizable knowledge from the foundation models to customize lightweight models with a given task-specific label space.\n2.2 Data-Free Knowledge Distillation\nAs the growing concerns about data privacy, DFKD [3, 13-15, 25, 34] has been a flourishing topic in recent years. This approach extracts knowledge from a pre-trained teacher by synthesizing images that adhere to the teacher network statistics through enforcing the BN regularization [34]. The student is then optimized by knowledge distillation upon the synthetic dataset. While DeepInversion[34] generates class-conditional images from random noise, GAN [17]-based adversarial training has become a dominant scheme for DFKD with a generator trained from scratch [3, 14, 15]. CMI [15] argues that data diversity is beneficial for distillation performance and introduces contrastive learning to increase instance discrimination. Nevertheless, the out-of-domain (OOD) problem is unstudied in the conventional DFKD, in which the student is evaluated on the dataset with the same data distribution of training data. In this paper, we address a more rigorous DFKD problem, named DFKD-VLFM, where a pioneering vision-language pre-trained model (e.g., CLIP [29]) is inverted to achieve a generalizable student. Despite the promising performance of GAN-based DFKD methods, inverting a model pre-trained on a large-scale dataset remains challenging. Inspired by the remarkable advantages of recent text-to-image models [8, 30], we take VQGAN-CLIP [8] as a baseline framework. As demonstrated that a myriad of various images can be generated by the text-to-image models with diverse text prompts, in this paper, we propose three types of prompt diversification to achieve generalizable knowledge transfer."}, {"title": "3 METHOD", "content": "3.1 Preliminary\nWe first give the definition of DFKD-VLFM: given a pre-trained vision-language foundation model (e.g., CLIP) as the teacher \\(T(\u00b7, \u03b8_\u03c4)\\) and a few words of the target category \\([CLS_1, ..., CLS_n]\\), the objective of DFKD-VKFM is to develop a student \\(S(\u00b7, \u03b8_s)\\) that can recognize the target categories and can be generalized to any domain-agnostic downstream task without using any real data. As an enhanced version of DFKD, the process of DFKD-VLFM can be similarly decoupled into an image synthesis step and a knowledge distillation step. Unless explicitly specified, we use CLIP as the pre-trained teacher model in this paper. During image synthesis, as mentioned above, we replace the plain generator of the conventional DFKD with the pre-trained VQGAN \\(G(\u00b7)\\) to achieve high-fidelity image synthesis. Given a category-specific text prompt \\(t_m\\), VQGAN generates images by optimizing the latent code \\(z_n\\), guided by the semantic relevance score derived from CLIP. Formally,\n\\[I_n = E_{img}(G(z_n)), n \u2208 [1, N],\\]\n\\[T_m = E_{txt}(t_m), m \u2208 [1, M],\\]\n\\[L_{z_n} = D(I_n, T_m),\\]\nwhere \\(E_{img}(\u00b7)\\) and \\(E_{txt}(\u00b7)\\) denote the image encoder and the text encoder of CLIP, mapping the synthesized image and the text prompt to the image embedding \\(I_n\\) and the text embedding \\(T_m\\), respectively. Similar to [8], \\(D(\u00b7, \u00b7)\\) is the squared spherical distance for text-image semantic relevance measurement. For balance training, we synthesize \\(N\\) images for each category, where \\(N\\) is the total number of the synthesized images and \\(M\\) is the number of task-specific categories. With the synthesized images, the student can be trained by knowledge distillation, transferring the task-specific knowledge from CLIP:\n\\[Y_n = [D(I_n, T_1), ..., D(I_n, T_M)],\\]\n\\[L_{es} = L_1(S(G(z_n), \u03b8_s), Y_n),\\]\nwhere \\(S(\u00b7, \u03b8_s)\\) is the customized student.\nConventional DFKD implicitly relies on the premise that the test data involves little domain shift, which is vulnerable in real-world scenarios. In contrast, DFKD-VLFM aims to develop a generalizable student model and delivers it to domain-agnostic downstream tasks. To achieve this, it is crucial to extract comprehensive knowledge of the task-specific label space from CLIP by generating surrogate images that cover a wide range of data distribution. However, since the given information is only in the form of category words, the most intuitive method is to create numerous prompt templates by hand, such as \"[CLS] in the style of [STYLE]\". However, this approach is laborious, empirical, and poses a high risk of model collapse. To address this issue, we propose to find a new word that represents the novel \"style\" of a category and make three attempts at Prompt Diversification.\n3.2 Prompt Diversification\n3.2.1 Mix-Prompt. It is intuitive to source the words or phrases about image style from the Internet and build a hand-crafted word dictionary. By stitching different style words with the category word, we can achieve different sentences of a category. However, the limited size of the dictionary still poses a risk of mode collapse. Hence, we extend it to a broader space via a random convex interpolation. Specifically, we mix-up two different text embedding (\\(T_{m1}\\) and \\(T_{m2}\\)) of the same category:\n\\[T_m = \u03bbT_{m1} + (1 - \u03bb)T_{m2},\\]\nwhere \u03bb is the coefficient randomly sampled from a Beta distribution. \\(T_m\\) is the corresponding category text embedding of the novel style.\n3.2.2 Random-Prompt. In real-world scenarios, it is essential to synthesize data that covers a wide range of domain distributions. However, it is difficult to capture the entire semantic concept of a category using a limited-scale word dictionary, even when expanded by random convex interpolation. To address this issue, we propose a pseudo word to describe a category with an abstract concept, rather than relying solely on a concrete concept of image style in the Mix-Prompt approach.\nBefore introducing Random-Prompt, we first review the pre-processing step of the text encoder. Similar to BERT [9], the input string is converted into a set of tokens, each of which is associated with a corresponding token embedding once the text encoder has been trained. This creates a fixed codebook, where each word can be converted into an embedding vector \\(V\\) through index-based lookup in the codebook \\(C \u2208 R^{K\u00d7D}\\). Here \\(K\\) represents the codebook size and \\(D\\) is the token feature dimension. This step is referred to as token embedding in Fig.4. After this, all individual token embeddings of an input string are combined to construct a sentence embedding, also known as text embedding in this paper. Therefore, the text encoder \\(E_{txt}\\) can be decomposed into a token embedding \\(E_{token}\\) and a sentence embedding \\(E_{sentence}\\):\n\\[E_{txt} = E_{sentence} \u25e6 E_{token}\\]\nAs such, we designate a place-holder string, H\u2217, as the pseudo word to represent the novel style. This allows us to generate infinite styles by randomly generating a continuous virtual token embedding \\(V\\) for H\u2217. The corresponding token embedding for H\u2217 is denoted as \\(V = E_{token}(H^*)\\). Assuming that the token embedding space follows a Multivariate Gaussian distribution, we estimate the statistics of the distribution using token embeddings from the codebook C and generate V by randomizing from this distribution:\n\\[\u03bc_d = \\frac{1}{K} \\sum_{j \u2208 [1,K]} C_{j,d}\\]\n\\[\u03c3_d^2 = \\frac{1}{K} \\sum_{j \u2208 [1,K]} C_{j,d}^2 - \u03bc_d^2\\]\nGiven the mean \u00b5 and the standard deviation \u03c3, the token em-bedding \\(V_{\u2217}\\) can be randomized by Gaussian sampling and the text embedding \\(T_m\\) for image synthesis is achieved:\n\\[V_{*,d} = \u03bc_d + \\epsilon \u00b7 \u03c3_d, \\epsilon_d \u223c N(0, 1),\\]\n\\[T_m = E_{sentence}([V_m], V_*),\\]\nwhere \\([V_m]\\) is the token embedding of the target category name and the frozen template. And \\(N(0, 1)\\) is the standard Gaussian distribution. \\(\u03f5\\) is a scale scalar which can enlarge \\(\u03c3\\) so as to enrich diversification range.\n3.2.3 Contrastive-Prompt. Given the extremely vast visual concept space in large-scale vision-language pre-trained models, Mix-Prompt and Random-Prompt may not yield distinguishable instances, even with the infinite virtual styles generated. For example, a dog can appear in various styles, poses, and backgrounds, and even belong to different breeds. We conjecture that generating each text prompt independently lacks instance-level diversity, making it insufficient to extract generalizable knowledge from such models. To further diversify the data, we propose a learnable prompt optimized by instance-level contrastive learning [4]. The core idea is to enlarge the distance between the current prompt and the historical generated ones. Specifically, we adopt a memory bank mechanism to store the historical ones. The details of this process are illustrated in Fig. 4. Initialized by Random-Prompt or Mix-Prompt, we develop a contrastive loss in the text embedding space \\(T_m\\), back-propagating to optimize V in the token embedding space:\n\\[L_v = -log(\\frac{exp(D(T_m, T_m)/\u03c4)}{\u2211_{b\u2208[1,B]} exp(D(T_m, T_b)/\u03c4)}),\\]\nwhere \\(D(\u00b7, \u00b7)\\) is the cosine similarity function and \u03c4 is the temperature parameter. The positive pairs are comprised of the current text prompts and the augmented counterparts \\(\\{T'_m\\}\\), being optimized to be closer. The negative samples \\(T_b\\) are constructed by different text prompts from the current batch and the memory bank, being pulled apart. B is the number of negative samples. In this paper, we adopt a commonly used text augmentation method - randomly shuffling the text letter of the frozen template \u201cin the style of\u201d to construct the positive samples. Since the word of the target category is kept fixed, the semantic information can be preserved when some letters of the frozen template are shuffled."}, {"title": "4 EXPERIMENTS", "content": "In the following sections, we aim to demonstrate the practicality of DFKD-VLFM, where a generalized student is tailored for domain-agnostic downstream tasks without access to real data. We conduct experiments across a wide range of popular domain generalization (DG) datasets using two fundamental image classification settings: 1) Zero-shot learning, to assess whether the knowledge of CLIP can be effectively transferred upon synthetic datasets. 2) Few-shot learning, to evaluate whether the distilled student can serve as a promising pre-trained model for downstream tasks, even surpassing publicly available pre-trained models with the same architecture in an agnostic domain. Further ablation studies and analyses are conducted to validate the efficacy of the proposed approaches.\n4.1 Experiment Settings\n4.1.1 Datasets. We use four popular DG datasets to evaluate the out-of-distribution generalization ability of the students on image classification tasks, including PACS [24] (Art Painting (A), Cartoon (C), Photo (P) and Sketch (S)), VLCS[12] (Caltech101[16], LabelMe [31], SUN[7], VOC2007[11]), ImageCLEF-DA[2] (Caltech (C), ImageNet (I) and Pascal (P)) and VisDA[28] (Synthetic and real).\n4.1.2 Implementation details. We adopt ViT-B/32 [10] as the visual encoder and a 12-layer transformer as the textual encoder of CLIP. VQGAN is utilized for surrogate images generation. The pre-trained weights of CLIP\u00b9 and VQGAN\u00b2 are downloaded from the official Github page of the corresponding papers and are kept fixed throughout the training process. Without any specific statements, we use a ResNet18 [18] as the default student model for ablation studies. Specifically, we search from the Internet to develop a style dictionary with the size of 86 to develop Mix-Prompt. As for Contrastive-Prompt, the embedding of the pseudo word is initialized with Random-Prompt and optimized by a contrastive loss. The surrogate images are generated with the resolution 224 \u00d7 224. We report the mean classification accuracy over five runs with random seeds. Please refer to the Appendix for more details.\n4.2 Zero-shot Classification\nSince CLIP exhibits strong zero-shot performance, it is of interest to determine whether the generalizable knowledge can be transferred to a lightweight student using synthetic data. In this section, we evaluate the performance of the distilled student on several datasets with distinct domain shifts to verify its generalizability. Notably, all the methods described in this section are trained without any real-world data (zero-shot). Given the word \"CLS\" of the target categories, CLIP conducts zero-shot classification with the vanilla prompt \"A [CLS]\". The baseline synthesizes the surrogate data using the same vanilla prompt of zero-shot CLIP and performs knowledge distillation upon the synthetic dataset. Table 1-3 summarizes the quantitative results, where the student can inherit task-specific knowledge from CLIP to some extent. Introducing Prompt Diversification consistently outperforms the baseline setting by a large margin across all datasets, indicating the importance of diversity in surrogate datasets. Notably, Random-Prompt without prompt engineering can achieve comparable results to Mix-Prompt with elaborate prompt engineering, demonstrating the convenience of Random-Prompt. Contrastive-Prompt achieves the best performance among the proposed three Prompt Diversification methods, especially in the PACS dataset with large domain gaps, where the exhaustive search of diverse styles driven by the contrastive learning paradigm is believed to be responsible for the remarkable improvement. To further evaluate the effectiveness of our method, in Table 4, we conduct experiments on VisDA, a larger dataset, where Contrastive-Prompt still achieves satisfactory results.\n4.3 Few-shot Fine-tuning\nIn this section, we investigate the effectiveness of the distilled model in few-shot setting, which is expected to perform a fast adaptation to the real data distribution as another evidence of the effective generalizable data-free knowledge transfer. Specifically, we conduct few-shot experiments on each domain, following the common few-shot protocols of training on 1, 2, 4, 8, and 16 samples per category, with the remainder of the full dataset serving as the test set. All results represent the average of five runs with random seeds. Performance comparisons are drawn between the Contrastive-Prompt-driven distilled models and ImageNet pre-trained models sharing identical network architectures.\nBased on the average performance across domains in all datasets, as illustrated in Fig. 5, we observe that the grafted student from CLIP is a strong few-shot learner that can quickly adapt to target data with only a few samples and surpass the ImageNet pre-trained models by a substantial margin. These appealing results can support our assumption that large pre-trained vision-language models can serve as effective teachers for customizing specific students for edge devices, inheriting significant generalizability from the vision-language foundation models. Thus, it is feasible to create a more promising pre-trained classifier with arbitrary structure rather than the ImageNet pre-trained models.\n4.4 Comparison with DFKD Methods\nThis section provides an elaborate comparison between the pro-posed DFKD-VLFM task and the DFKD approaches [1, 14, 15, 34]. To this end, we perform experiments on PACS and present both quantitative and qualitative comparisons in Table 5 and Fig. 6, respectively. It is important to note that BN regularization \\(L_{bn}\\) is a crucial component of DFKD [34], which regularizes the feature statics (\\(\u00b5_i (g(z)), \u03c3_i (g(z))\\)) of synthesized data in the ith BN layer conformed to the original ones (\\(\u00fb_i, \u00f4\u00b2\\)) stored in the pre-trained teacher model. However, it is not applicable to Transformer models. For a fair comparison, we use Resnet-50 as the visual backbone for all experiments in this section.\nThe results presented in Table 5 demonstrate that the student model guided by Contrastive-Prompt consistently outperforms the DFKD methods, and the inclusion of \\(L_{bn}\\) leads to a significant drop in performance. We attribute this poor performance of DFKD to several factors. Firstly, the use of high-fidelity synthetic images significantly facilitates knowledge transfer. As shown in Fig.6, the images generated by VQGAN are more realistic and retain the identifying category features, which promotes knowledge transfer from the teacher to the student. On the other hand, although DFAD employs a generator, it is trained from scratch, which increases the risk of mode collapse, as demonstrated in prior research [15, 25]. Secondly, \\(L_{bn}\\) has an inferior effect on synthetic data. CLIP is pre-trained on a large-scale dataset sourced from the Internet, which has a significant domain shift with respect to the test images that are often complex scenes with multiple objects and textual descriptions. As shown in Fig.6(b)-(d), synthetic data generated by \\(L_{bn}\\) follows the data distribution of web images, which are often corrupted with textual characters, leading to a degradation in performance of the student model. Furthermore, the prior space of the well-pretrained CLIP and VQGAN are fixed during the image generation. Note that \\(L_{bn}\\) is only related to the priori of CLIP, which enforces the statistics of the synthesized images to conform to the prior statistics stored in the pre-trained CLIP [34]. As a result, imposing \\(L_{bn}\\) narrows the fixed broad prior space and reduces the diversity of the surrogate synthesized dataset, resulting in poorer performance of the distilled student. In contrast, Fig. 6(a) depicts more diverse images with fewer textual characters (only one among ten images shown), which implicitly validates the effectiveness of the proposed DFKD-VLFM framework."}, {"title": "5 CONCLUSION", "content": "In this paper, we study an important and novel task of creating a generalizable student model from the powerful vision-language foundation models that can be applied to domain-agnostic downstream tasks for zero-shot inference or few-shot fine-tuning. To the best of our knowledge, this is the first attempt to tackle this task. To facilitate the generalization ability transferring from the foundation models to the student model, we propose three novel Prompt Diversification methods, i.e., Mix-Prompt, Random-Prompt and Contrastive-Prompt, to promote diverse text prompts, which serves as a bridge to facilitate diverse image synthesis. We hope our work will stimulate further interesting research in this area."}, {"title": "A MORE DETAILS ABOUT EXPERIMENT SETTINGS", "content": "A.1 Datasets\nFour benchmark datasets are utilized for evaluating the generaliza-tion ability of the distilled student.\nPACS [24] is a commonly used domain generalization bench-mark, consisting of four domains with different types of im-age, i.e., Art Painting (A), Cartoon (C), Photo (P) and Sketch(S). There are totally 9991 images of 7 categories.\nVLCS[12] is collected from four photographic datasets, i.e.,Caltech101 [16], LabelMe[31], SUN[7], VOC2007[11], consist-ing of 10,729 images from five categories.\nImageCLEF-DA[2] is comprised of twelve categories fromthree domains, including Caltech (C), ImageNet (I) and Pascal(P), and each class has 50 images.\nVisDA[28] is a large-scale dataset designed for domain gen-eralization, which focuses on the domain shift between syn-thetic and real data. It contains over 280,000 images acrosstwelve categories.\nA.2 More Implementation Details\nPrompt Diversification. We study three types of prompt di-versification methods: 1) Mix-Prompt: We search from the Internetto develop a style dictionary with the size of 86, containing thewords/phrases to describe image styles. Please refer to Table 8for more details about the style dictionary. \u03bb is randomly sam-pled from a Beta distribution Beta(a, a) and a is set 1.0 by default.2) Random-Prompt: We replace the embedding of the style wordwith a vector sampled from the estimated Gaussian distribution. 3)Contrastive-Prompt: The embedding of the style word is initializedwith Random-Prompt and optimized by a contrastive loss for 100epochs with Adam optimizer. The learning rate is 0.1 and the batchsize for training is 256. The number of negative instances sampledfrom the memory bank for each batch is set 2048.\nSurrogate Image Synthesis. Adam optimizer with 500 itera-tions is used to synthesize images with the resolution 224 \u00d7 224.The batch size is 8 and the learning rate is 0.15.\nKnowledge Distillation. The distillation process is comprisedof two steps: warm-up and fine-tuning. We utilize the pre-trainedmodel as the feature extractor and learn the classifier with 150epochs for warm-up. Then we fine-tune the whole framework witha 0.1 times learning rate of warm-up step for 150 epochs. The stu-dent network is trained with an SGD optimizer with a momentum0.9, and the learning rate of warm-up step is set 0.1. All the experi-ments are run on a single NVIDIA V100 GPU. We report the meanclassification accuracy over five runs with random seeds.\nB ANALYSIS OF STYLE DICTIONARY\nSince the large-scale visual-language foundation models are trainedon abundant images annotated with different text descriptions, itis insufficient to invert diverse images of a visual concept usinga simple text prompt. To diversify the text prompts with minimalhuman effort, as shown in Table 8, we have devoted to sourcing thewords/phases about image style from the Internet and form a styledictionary with the size of 86. As is mentioned in the section ofMix-Prompt in the paper, we randomly sample two styles from thedictionary which are then mixed up in the text embedding spacefor novel style generation. Moreover, it is worth emphasizing thatthe style words appeared in the test datasets are not included inthe style dictionary so as to mimic the domain-agnostic scenarios.For example, the domain descriptions of PACS (Photo, Art painting,Cartoon and Sketch) are not included in the style dictionary. Inthis way, we can evaluate the generalization ability of the distilledmodel on the unknown target domains.\nC MORE VISUALIZATION OF THE SYNTHESIZED IMAGES\nFig. 9 gives a more sufficient qualitative visualization of the effec-tiveness of the proposed Contrastive-Prompt, which enjoys highfidelity and style diversity benefited from the well-pretrained VQ-GAN and various text prompts. Since a given category can appearin abundant contexts in the agnostic downstream tasks, the diver-sity of the surrogate synthesized datasets plays a vital importance.Fig. 9 gives a qualitative demonstration of the proposed Contrastive-Prompt in diversifying the text prompts. As shown in the forth-rowin Fig. 9, it is able to render the concept of bird in not only differ-ent styles, but also in different breeds and different scenes, whilepreserving the identifying feature of a bird. We owe this superiorproperty to the contrastive learning performed in the text embed-ding space, which pulls each instance apart to exhaustively extracttarget knowledge from the pre-trained CLIP.\nD MORE ABLATION STUDIES\nIn this section, more ablation studies are carried out on PACS tofurther evaluate the efficacy of the proposed methods. The setting isexactly the same as the main body of the paper, in which ViT-B/32is adopted as the visual backbone of CLIP and ResNet18 is adoptedas the student model.\nD.1 Feature Visualization.\nFig.10 shows the t-SNE visualization of the features extracted bythe distilled model on Art painting domain of PACS. When adaptedto the novel domain, we can see the features extracted by theContrastive-Prompt one are more compact than the others. Thisgives another evidence that Contrastive-Prompt helps the studentlearn discriminative features. Given 16 shots for fine-tuning, thefeatures can be fast adapted to the target domain.\nD.2 Case Study.\nWe take the visual concept \"dog\" as an example for case study.In Fig.11(a), the proposed Prompt Diversification methods canhelp synthesize more diverse data than the baseline counterpart,among which Contrastive-Prompt can provide the most diverseimages. Fig.11(b) represents different domains of the downstreamtasks, which is agnostic during Data-Free Knowledge Distillation.When Generalized to these agnostic domains, Contrastive-Promptperforms the best. Especially in the Sketch domain, the baselinemethod only achieves 7.14% accuracy, while Contrastive-Promptcan achieve 84.81% accuracy, which is superior by a large margin."}]}