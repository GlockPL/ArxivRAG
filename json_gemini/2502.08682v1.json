{"title": "On the Role of Pre-trained Embeddings in Binary Code Analysis", "authors": ["Alwin Maier", "Felix Wei\u00dfberg", "Konrad Rieck"], "abstract": "Deep learning has enabled remarkable progress in binary code analysis. In particular, pre-trained embeddings of assembly code have become a gold standard for solving analysis tasks, such as measuring code similarity or recognizing functions. These embeddings are capable of learning a vector representation from unlabeled code. In contrast to natural language processing, however, label information is not scarce for many tasks in binary code analysis. For example, labeled training data for function boundaries, optimization levels, and argument types can be easily derived from debug information provided by a compiler. Consequently, the main motivation of embeddings does not transfer directly to binary code analysis.\nIn this paper, we explore the role of pre-trained embeddings from a critical perspective. To this end, we systematically evaluate recent embeddings for assembly code on five downstream tasks using a corpus of 1.2 million functions from the Debian distribution. We observe that several embeddings perform similarly when sufficient labeled data is available, and that differences reported in prior work are hardly noticeable. Surprisingly, we find that end-to-end learning without pre-training performs best on average, which calls into question the need for specialized embeddings. By varying the amount of labeled data, we eventually derive guidelines for when embeddings offer advantages and when end-to-end learning is preferable for binary code analysis.", "sections": [{"title": "1 INTRODUCTION", "content": "Deep learning has been a driving force behind several advances in computer security. In particular, the ability of neural networks to distill information from highly complex data, such as assembly code, has led to a number of learning-based methods for binary code analysis. These methods allow, for example, to locate function boundaries [3, 30, 33], differentiate optimization levels [6, 31], assess code similarity [26, 32, 37, 39], reconstruct arguments [7, 18, 19], and detect aliases in memory [17]. While the approaches differ in the architecture of the neural networks used, most share a key component: an embedding. This learned vector representation originates from the area of natural language processing and provides geometric access to the data's structure and semantics, forming a versatile basis for solving different learning tasks.\nOver recent years, several methods have emerged for crafting embeddings tailored to binary code analysis, including Asm2Vec [11], Instruction2Vec [21], and PalmTree [23]. Additionally, dedicated approaches such as Gemini [36] and SAFE [25] have been specifically designed for generating function embeddings used to detect similar functions. The underlying rationale for these embeddings lies in their pre-training on large collections of unlabeled code, which allows for encoding general characteristics the data and learning a versatile representations for various downstream tasks. Over time, these embeddings have become a gold standard for applying deep learning to binary code analysis [2, 4, 19, 29, 38, 39].\nAlthough natural language processing bears similarities with code analysis, the availability of labeled data differs between the two domains. For natural language text, tedious manual labeling is often unavoidable to create examples for supervised learning, rendering pre-trained embeddings indispensable. In contrast, for many tasks of binary code analysis, labeled data can be easily generated from debug information provided by a compiler. For example, labels for function boundaries, optimization levels, and argument types can be extracted during the compilation process and enable constructing large-scale training sets with label information automatically. As a result, the necessity of pre-trained embeddings in natural language processing does not naturally apply to tasks in binary code analysis, where end-to-end learning is often possible.\nIn this paper, we investigate the role of pre-trained embeddings for binary code from a critical perspective. For this investigation, we construct a labeled evaluation corpus of 1.2 million functions from the Debian distribution, totaling about 129 million x86 instructions. This corpus allows us to systematically evaluate the capabilities and limitations of five widely used embeddings for assembly code, namely Word2Vec [28], Asm2Vec [11], Instruction2Vec [21], and Palm Tree [23]. In particular, we evaluate the performance of each embedding in different configurations on five common downstream tasks of binary code analysis: compiler detection, optimization level identification, function argument prediction, argument type reconstruction and code similarity detection.\nOur results provide a new view on pre-trained embeddings in binary code analysis: First, we observe that the embeddings hardly differ in performance if sufficient training data is available, so that differences discussed in prior work are not noticeable [23]. Even a random instruction embedding provides a reasonable performance in our experiments. Second, we surprisingly find that end-to-end"}, {"title": "2 A PRIMER ON PRE-TRAINING", "content": "Let us start by introducing some background on the concept of pre-training and embeddings for assembly code, before critically reflecting on their role in binary code analysis."}, {"title": "2.1 Training with a Headstart", "content": "When humans learn a new task, such as a playing a music instrument, they usually do not start from scratch but are able to built upon prior knowledge. For example, someone who played the violin is probably faster in learning to play the cello than someone without prior experience. This insight into the human learning process has also been employed in the machine-learning domain and is commonly referred to as transfer learning.\nTo be more specific, a learning model trained for a certain task can help create a second model for a different task. The first task is commonly referred to as pre-training task and the second as downstream task. The rationale underlying this transfer is that the second model may perform better when building on the knowledge of the first one. Since this transfer learning typically revolves around improving the performance of downstream task, we refer to the pre-training as task-agnostic (with respect to the downstream task), while the downstream training is task-specific.\nPre-training is considered especially beneficial in cases where high quality labelled data is expensive to create but unlabelled data is widely available. In natural language processing and computer vision, for example, vast amounts of unlabeled data are available on the Internet and can be employed for pre-training. Since the pre-training task and the downstream task do not need to share the same training objective, it becomes possible to use unsupervised learning for pre-training on a large unlabeled dataset and perform supervised learning on a smaller labeled one. A well-known example of this strategy is the unsupervised learning of input representations. In this case, the pre-training task learns a vector representation of the data, denoted as embedding, which serves as input for the subsequent downstream tasks, as shown in Figure 1(a)."}, {"title": "2.2 Instruction Embeddings", "content": "We proceed to investigate the different embeddings that have been proposed for binary code analysis. To this end, we first give a formal definition and then provide a recap of recent approaches for embedding assembly code.\nSimply put, embeddings are functions mapping data of the input domain to a vector representation. This vector can then be used for subsequent machine-learning tasks. The main reason behind using such a representation is, that it does not simply encode a word as a low-dimensional vector but is also able to express relationships among different words. For example, synonyms may be mapped to vectors within close proximity, while opposites are a mapped far apart from each other. Since these are appealing characteristics for machine learning, embeddings are widely used for natural language tasks [1, 10, 14] and recently for binary code analysis, where instructions take up the role of words.\nBefore introducing these embeddings, we first need to formally define what an instruction is. In particular, we refer to an instruction as the disassembled representation of a machine code operation for a given architecture. Generally, an instruction consists of an operation type denoted as mnemonic (M) and a group of operands ($O_1, ..., O_n$). Typically, the operand of an instruction can have one of the following types:\n(1) Register. The operand is a processor register, where each register has a unique identifier.\n(2) Immediate value. The operand is an address value or a numerical constant.\n(3) Memory access. The operand is a memory access expression which usually consists of several components.\nWhile the specific structure of the operands and the instruction depends on the architecture and the syntax flavor, the general structure remains the same. To give an example for a specific case, Figure 2 shows a sequence of five instructions for the x86-64 architecture in Intel syntax. For the instruction in line three, the mnemonic is highlighted in red and the operands in yellow. The first operand is a register operand, while the second one is a memory access expression. In this specific case, \u2460, \u2461, \u2462 and \u2463 point to the components of the memory access: the base register, index register, scale and displacement value, respectively. Although these instructions actually describe program semantics, it is easy to see that they can be interpreted as words (token) or phrases, similar to natural language text.\nEmbeddings for binary code differ in how they characterize the content of the instructions. While most embeddings build on tokenization schemes for operations and operands that are architecture independent, some approaches utilize specific knowledge about instruction semantics and execution behavior. The embeddings considered in our work fall into the first category, with the exception of Instruction2Vec, which incorporates knowledge about the structure of x86-specific memory access expressions.\nTo describe common instruction embeddings jointly, we phrase the previous description more formally. To this end, let $I$ be the set of all instructions. The embedding method can then intuitively be described as a function $\\Psi$ mapping an instruction $I \\in I$ to a real valued vector of dimension d:\n$\\Psi$: I$\\rightarrow R^d$\nAlthough sufficient for most cases, this definition is not able to capture methods which create context-dependent embeddings, such as PalmTree. That is, for a sequence of instructions S = ($I_1$, ..., $I_m$), the embedding for one instruction also depends on other instructions within the same sequence. To also capture such embedding methods we extend our definition to sequences of instructions. Let S be the set of all instruction sequences S = $\\bigcup_{i}I^i$. This allows for the following embedding function definition\n$\\Phi$: S$\\rightarrow R^{d \\times i}$\nwhere a whole instruction sequence is mapped to a matrix in which each column represents the embedding vector of the respective instruction. With this definition it is possible to map the same instruction to different vector representations dependent on the surrounding instructions.\nThe remainder of this section introduces the six instruction embeddings used in our evaluation.\n(a) Word2Vec. This embedding [25] is a variant of the classic embedding introduced by Mikolov et al. [28], which has been originally designed for natural language text. To learn a vector representation of assembly instructions, one can directly transfer this idea to binary code by considering instructions sequences as sentences and instructions themselves as words. The resulting embedding is based on the word embedding function $\\phi$ of the original model and yields the function\n$\\Psi(I) = \\begin{cases} \\phi(I), \\text{if I is in the vocabulary}\\\\  \\vec{0} \\text{ otherwise.}  \\end{cases}$\nThat means out-of-vocabulary (OOV) tokens are mapped to the zero vector. For the considered Word2Vec based embeddings approach, full instructions are considered as a single token and no further tokenization is conducted. However, immediate values greater than 5000 are normalized, i.e. replaced by the word IMM. In line with Massarelli et al. [25], our implementation uses the skip-gram algorithm [27] with negative sampling [28]. A similar instruction embedding based on a Word2Vec model has also been used by Chua et al. [8] for learning function type signatures.\n(b) Instruction2Vec. The Instruction2Vec embedding [22] utilizes a classic model as well. As opposed to Word2Vec, it is trained over individual instruction components gathered from instruction sequences. Based on the component-based embedding $\\phi$ that maps a component to a m dimensional space, an embedding for instructions is assembled. The embedding vector of an instruction I can be divided into 9 slots of size m. Slot 1 contains the embedding vector of the operation (M), slot 2 up to slot 5 contains the embedding of the first operand ($O_1$), and slot 6 to slot 9 contains the embedding"}, {"title": "2.3 Function embeddings", "content": "In contrast to embeddings at the instruction level, function embeddings aim to generate a comprehensive vector representation of an entire binary function. Before we present different embeddings for binary functions, we formally describe our notion of a binary function and define the embedding function.\nA binary function is represented by its control-flow graph and comprising basic blocks, i.e., continuous sequences of disassembled instructions interconnected by directed edges that signify the flow of control. Formally, let C = (V, N : V$\\rightarrow {x : x \\subset V}$) be a control-flow graph with a set of basic blocks V, and a function N that maps each basic block to its successors.We can then define an embedding function using the set of all control flow graphs C as\n$\\Omega$: C$\\rightarrow R^d$\nthat maps an arbitrary control-flow graph C of a binary function to a d-dimensional vector. This embedding function outputs a singular vector representation, effectively condensing the intricate structure of a binary function into a point within a continuous vector space, enabling streamlined comparison and analysis of functions in binary code."}, {"title": "3 AN EMBEDDING BENCHMARK", "content": "The cornerstone of our evaluation is a vast open corpus of labeled assembly code designed for various downstream tasks. Prior to delving into the assessment of the embeddings under consideration, we provide an introduction to these tasks (Section 3.1) and outline our pipeline for generating large-scale datasets automatically to train embeddings and learn models (Section 3.2)."}, {"title": "3.1 Downstream Tasks", "content": "For our evaluation, we select five common downstream tasks by which we measure the performance of the different embeddings. Each task addresses a different challenge in binary analysis [6, 8, 11, 25, 31, 36]. Note that we are not proposing new solutions for these tasks but rather recreate existing experiments to investigate the role of pre-trained embeddings. Table 1 offers a summary of the five downstream tasks, presenting the number of trainable parameters for the learning model and the parameters exclusive to the embedding layer in our end-to-end approach, along with their corresponding dimensions. Notably, Task T5, focused on detecting similar code, does not require an additional learning model as it relies solely on the embedding vectors. It's worth highlighting the substantial variation in the number of model parameters across tasks, providing an opportunity for experimentation with learning tasks of varying complexities in the domain of binary code analysis.\nCompiler and optimization options. The first two tasks deal with the detection of compilers (Task T1) and the identification of optimization options (Task T2). Both tasks have been previously investigated by Pizzolotto and Inoue [31] and Chen et al. [6]. Our implementation is based on the network introduced by Pizzolotto and Inoue. They propose a shallow network as learning model that consists of a single LSTM layer with a terminal dense output layer. The LSTM layer has an output dimension of 256 and performs the actual learning. To handle the different classes of each task, we use 2 output nodes for the compiler detection (GCC and CLang) and 4 nodes for the identification of optimization options (00, 01, 02, 03). Different from the approach by Pizzolotto and Inoue [31] we use disassembled instructions as input as opposed to the raw bytes of machine code instructions.\nFunction type signatures. The other two downstream tasks deal with the recovery of function type signatures. We choose the implementation published by Chua et al. [8] and reuse their network architecture. According to their method, we create two tasks for this problem. The first task predicts the number of function parameters (Task T3) and the second task aims at determining the data type of the first parameter (Task T4), such as int or char *.\nThe learning model consists of three sequential GRU layers configured with dropouts to avoid overfitting. The final layer has 10 nodes for the prediction of the number of arguments and 7 nodes for the prediction of the argument types [see 8].\nFunction Similarity. In addition to assessing the efficacy of function embeddings, we explore two distinct approaches for learning embeddings for function similarity detection. The first approach is based on Gemini [37], which leverages a graph neural network. Another method for identifying similar binary functions is SAFE [25], which is founded on a self-attentive neural network architecture."}, {"title": "3.2 Mining Debian Packages", "content": "For the training of instruction embeddings with unlabeled data and the automatic construction of a labeled dataset for training the downstream tasks, we deploy two data processing pipelines. The first pipeline processes Debian binary packages and emits the pre-trained embeddings. The second pipeline uses Debian packages as well, but also requires an embedding model to generate the labeled dataset used to train and evaluate the downstream tasks. Both processes are depicted in Figure 3.\nWe manually build each package from source for the x86-64 architecture using eight compilation combinations. Each compilation combinations uses a different compiler (GCC or CLang) and one of four optimization levels (00 to 03). Note that building a single Debian source package can produce multiple binary packages for the same compilation combination.\nIn total, we extract 1,293,205 functions consisting of 129,487,277 instructions from 480 binary Debian packages. More details are provided in Table 2, where we list the number of binary packages (with and without variants due to compilation combinations), functions, and instructions for each source package. Since the dataset generation processes binary Debian packages our dataset can be expanded by adding more packages. It is also possible to create datasets for different architectures.\nTraining and test splits. We split the dataset in training and test data at the level of Debian source packages. Consequently, all functions from a single source package will be placed either in the training set or test set. We never spread functions from one source package over training and test data. We believe that this is an important detail as functions can be shared not only between the binaries of a packages, but also between binary packages of a single source package. While functions may also be shared across source packages, we argue that these cases are rare and do not necessarily lead to overfitting. Instead, they reflect scenarios in which source code is simply reused, for example, through the bad practice of copying and pasting code snippets. Our data split procedure is different from the approaches used by Chua et al. who divide each open source project into training and test data or Pizzolotto and"}, {"title": "4 EXPERIMENTS", "content": "Equipped with a large corpus of labeled code for different downstream tasks, we are finally ready to compare the strengths and weaknesses of the considered embeddings. To this end, we design five basic experiments, each addressing a different aspect of pre-trained embeddings.\nExperimental instances. For every experiment, we evaluate multiple instances of the downstream tasks. In this context, an instance is defined by the learning model of the task (T1-T4), the deployed embedding and the available supervised training data. For example, the learning model for task T1 paired with a Word2Vec embedding and all training data from Table 2 is one instance of the compiler identification task. The performance of each instance is then evaluated using the test data from Table 2. With this experimental setup, we can directly evaluate the capabilities of the embeddings, simply by comparing the instances considered in one experiment.\nPerformance measures. As an evaluation criterion, we uee the accuracy metric for tasks T1-T4, aligning with its prominent usage as the primary measure in the original publications [8, 31]. Task T5 employs the area under the ROC curve, a widely recognized metric for assessing binary classifiers, providing a comprehensive view of the trade-off between true-positive and false-positive rates. The ROC curve (receiver operating characteristic curve) [15] visually represents the performance of a classification model or detection system across various classification thresholds, plotting the true positive rate against the false positive rate."}, {"title": "4.1 Baseline Experiment", "content": "To begin, we introduce a general baseline experiment. That is, we design an experiment to test our implementations of the various embeddings and downstream tasks for correctness. We create five different instances for each combination of downstream task and embedding type and train them on all training data. Each instance is based on a different random seed in order to detect possible variations. We further use this experiment to revise the peculiarities of the downstream task and discuss the natural limits on the classification performance and identify variations in the test data. The results are presented in Section 5.1 and complemented in Appendix A.1."}, {"title": "4.2 Experiment 1: Size of Labeled Data", "content": "The objective of the first experiment is to investigate the effect of the amount of labeled data on the accuracy of the employed embeddings. Moreover, we consider the changes in training time. The rationale behind this experiment is the following: Unlike pre-trained embeddings, an end-to-end embedding is build from scratch and can only be trained with supervised data.\nFor each combination of downstream application and embedding type, we create nine different groups of instances. The groups differ in the amount of labeled data used for training. In contrast, the instances within one group are trained with the same number of functions, but the function sets are disjoint. On that account, we partition the training data T into n disjoint subsets $t_i$, i = 1, . . ., n, i.e., T = $\\bigcup_{i=1}^{n}t_i$. We refer to each set $t_i$ as a shard (of the training data). For instance, if n = 1 the same data that is used for pre-training the embeddings is used to train the instances. As n gets larger less supervised data is available for training the instances, e.g., for n = 28 less than 1% of the data is used. Doing so, we mimic scenarios where more unlabeled then labeled data is available. We conduct this experiment for n = $2^i$, i = 0, . . . 8. In total, we train 511 ($\\Sigma_{i=0}^{8}2^i$) instances distributed over nine instance groups for all 24 combinations of downstream task and embedding type.\nNote that the pre-trained embeddings remain unaffected by the value of n. They are consistently trained on the entire training set, while only the learning model of the downstream task is trained on reduced data, that is, it is trained on a single shard of the training data. In contrast, experimental instances that use end-to-end learning have only this single shard to infer all weights for the embedding layer as well as the learning model. Hence, the end-to-end approaches face a significant penalty for larger values of n. Our primary findings of this experiment are elaborated on in Section 5.2, while Appendix A.2 offers supplementary insights."}, {"title": "4.3 Experiment 2: Computational Expense", "content": "In the second experiment, we investigate the computational overhead of end-to-end learning, assuming pre-trained models are readily available and thus only end-to-end-learning incurs an additional run-time overhead. Using the setup from the previous section, we focus on the required epochs, that is, the number of training cycles needed for each instance.\nRecall that we work with groups of instances that exist for each task and embedding pair, where group is defined by the size of the labeled data. For example, one group uses the entire training data while for another group it is split into multiple shards. In this case,"}, {"title": "4.4 Experiment 3: Embedding Dimension", "content": "The goal of the next experiment is to analyze the impact of the embedding dimension on the general accuracy of an instance. In case of an end-to-end approach the embedding dimension is an easy to change parameter of the network architecture. It can be tuned like any other hyper-parameter. By contrast, the dimension of a pre-trained embedding is fixed. If one wants to deploy a pre-trained embedding for some application, the user needs to pick the embedding dimension in advance. In other cases, a particular pre-trained embedding might only exist for a specific dimension. For that reason, it is worthwhile to determine if it is possible to identify a recommended value for a default embedding dimension that yields strong results across different downstream tasks.\nWe create several pre-trained embeddings with different embedding dimensions. We pick dimensions of the range from 1 to 256. Starting with the smallest possible dimension for each embedding we subsequently increase the dimension in each step. Some embeddings come with a restriction in regard to the dimension: The embedding dimension of Instruction2Vec is a multiple of 9 and the smallest possible dimension of Asm2Vec is 2. PalmTree has a fixed dimensionality of 128. Again, all instances are trained on the entire dataset. The results are summarized in Section 5.3."}, {"title": "4.5 Experiment 4: Sequence Length", "content": "In the last experiment, we seek to find a viable sequence length for the downstream tasks. We aim at determining a good compromise between training time and the general accuracy of an instance. This trade-off enables us to train and evaluate thousands of instances. To this end, we truncate each function sequence at various positions and observe the general accuracy of the instances. Starting with only the first instruction of each function, we double the sequence length subsequently. We stop at a maximum of 256 instructions per function. Figure 13 shows a bar plot of the distribution of function lengths. Most functions are rather short. Only few functions are longer than 256 instructions. These functions are accumulated in the last bar of the plots. We use the PalmTree embedding for this experiment, as it is the only context-dependent one. We report the result in Section A.3 of the appendix."}, {"title": "5 RESULTS AND DISCUSSION", "content": "We will now delve into the outcomes of the five experiments outlined in Sections 5.1 to 5.4. The setup for each specific experiment can be found in Section 4, where their objectives and designs are described. Supplementary findings are also provided in the appendix for further reference."}, {"title": "5.1 Baseline Experiment", "content": "The outcome of the baseline experiment is summarized in Table 4 and part of Figure 5. Table 4 lists the general accuracy of experimental instances that are trained on the entire dataset, that is, the same amount of data is available to pre-training and end-to-end learning. Our results are in line with the original publications and show that our re-implementations are correct. [23, 31]\nIn Figure 5, it can be observed that when utilizing 100% of the training data, the area under the ROC curve ranges from 0.79 to 0.92 for Gemini and 0.79 to 0.90 for SAFE. It is essential to highlight that our dataset differs from the evaluation data previously employed for function similarity [11, 26, 37] rendering direct comparisons challenging. Despite these discrepancies, discernible patterns emerge: models using Word2Vec and PalmTree demonstrate optimal performance, while the end-to-end approach yields comparable results to Asm2Vec. Conversely, ada-002 exhibits the least favorable performance in this experiment.\nApart from the reproduced performance, we observe no notable differences between the embeddings. This result is particularly striking for the random embedding that performs as well as the specialized instruction embeddings. We conclude that the embedding type is not as important when sufficient labeled data is available for the downstream task. Furthermore, we observe that the results are stable across five runs with different random seeds. This is quantified by the standard deviation provided in Table 4. The deviation is at most 0.01. This suggests that the used optimization algorithms are well calibrated and the networks capacities are adequate for the data basis. As a result, the training algorithm is able to find model weights that produce stable results for each downstream task."}, {"title": "5.2 Experiment 1: Size of Labeled Data", "content": "The preceding results suggest that the amount of labeled data available is a key factor in the success of end-to-end learning. Therefore, we turn to the first experiment in which this quantity is varied. The corresponding results for tasks T1-T4 are shown in Figure 4, and Figure 5 shows the results for task T5.\nThe plots show the performance of the embeddings trained with different amounts of labeled data. This amount is defined by the number of shards used to partition the training data. For example, if 8 shards have been generated from the training data, only $\\frac{1}{8}$ or 12.5% of the labeled data is available for supervised training of the learning models or, in the case of task T5, the embedding models."}, {"title": "5.3 Experiment 2: Computational Expense", "content": "While we've noted advantages of end-to-end learning in binary code analysis, they do come with a prize: Figure 6 shows the training time for tasks T1-T4. It stands out that the end-to-end approach needs the most epochs for training. When training on more labeled data, we notice a significant drop in the number of epochs. Especially for tasks T3 and T4, where the end-to-end learner has a comparable training time once the full dataset is used. The epochs needed for instances with pre-trained embeddings remain relatively constant. We conclude that pre-trained embeddings have acquired relevant knowledge about the instructions, which helps training instances for each downstream application with fewer epochs.\nAn alternative perspective is presented for task T5 in Figure 7. Here, we maintain a consistent number of training samples per epoch, regardless of the available data. Consequently, training the embedding models for Gemini and SAFE with a larger volume of supervised data necessitates more epochs to cycle through all samples. Our observations indicate a general trend: the training algorithm effectively reduces the loss over more epochs when more data is available and concludes earlier with less data, yielding less favorable results, as depicted in Figure 5. Among the different embedding models, those based on PalmTree demonstrate the fastest training time while delivering one of the best results. Notably, the Gemini model with the Instruction2Vec embedding trains slowly, contrasting with its relatively short training time for SAFE. Unlike tasks T1-T4, the training time of end-to-end-based models is not as significantly impacted negatively; however, it is still noticeably slower.\nWe can conclude here that learning-based methods for code analysis can profit from a pre-trained embedding as the training process becomes faster. However, this is only relevant in scenarios where a complex network architecture is involved and the training time per epoch is high or the hardware resources are limited. Note that the prediction time is not affected, which is much more relevant in practice. We want to stress that we do not address the training time of the pre-trained embeddings, since at least in theory those embeddings are only trained once, but used for many tasks."}, {"title": "5.4 Experiment 3: Embedding Dimension", "content": "It remains to investigate the role of the embedding dimension in our analysis, focusing specifically on tasks T1-T4 and the instruction embeddings. The results of this experiment are depicted in Figure 12 in the appendix. The plot shows the shift of the general accuracy of instances trained with different dimensions. Recall that the lowest dimension of Asm2Vec is 2 and Instruction2Vec's embedding dimension is a multiple of 9. The PalmTree embedding, that we use as an off-the-shelf embedding, has a fixed dimension of 128.\nThis experiment shows that the end-to-end approach adapts better to lower embedding dimensions. On average, instances with an end-to-end embedding attain the best performance already with 16 dimensions. The pre-trained embedding types tend to require higher dimensionality. As a general rule, a dimension of 128 suffices for our tasks. This confirms the observation made by Li et al.\nIn summary, we have seen that an end-to-end embedding can get good results out of lower embedding dimensions. All pre-trained embedding types require a higher dimensionality to yield equal results. Our explanation is that pre-trained embeddings are more universal while an end-to-end embedding is always tailored towards the downstream task. Hence, an end-to-end embedding does not require the same complexity as pre-trained embeddings."}, {"title": "5.5 Discussion", "content": "Our experiments shed new light on the role of pre-trained embeddings in binary code analysis. We show that an end-to-end approach can compete with pre-trained embeddings if sufficient labeled data is available. If training time is not a constraint, end-to-end learning can obtain the same performance even with smaller embedding dimension. For pre-trained embeddings to be beneficial, the labeled data must be orders of magnitude smaller than the unlabeled data. This raises the question of whether these embeddings are actually relevant in practice, since the learning model of the downstream task requires labeled data in any case.\nMoreover, our evaluation reveals that the embedding process is not crucial for the downstream task, as the success of the random embedding striking illustrates. We credit this finding to task-agnostic relations among instructions that are not as advanced as the connections between words in natural language. Without a given analysis task, an instruction embedding cannot carry the same amount of information as a natural word embedding. This makes instruction embeddings less useful and also explains why the differences between our pre-trained embeddings are negligible.\nOverall, our empirical analysis demonstrates that a general benefit of pre-training does not exist for binary code analysis in practice. This is contradictory to previous research in our domain [e.g., 23], and the common strategy to favor pre-trained embeddings over end-to-end learning. Interestingly, research in other fields also arrived at this observation [35] and questioned the role of pre-training if sufficient labeled information is available. Thus, our analysis refutes the intuition that pre-training is a generally beneficial and therefore mandatory step in designing methods for binary analysis.\nBased on our observations, we work out the following recommendations for deep learning in binary code analysis."}, {"title": "6 CONCLUSION", "content": "This paper investigates the role of pre-training on binary code analysis. To this end, we compare four pre-trained embeddings, an end-to-end approach, and an random instruction embedding under different downstream tasks. Our results show that binary analysis tasks with sufficiently labeled data do not benefit from pre-trained embeddings. Instead, conventional end-to-end learning provides the best performance on average. Only if the labeled data is artificially reduced, we can observe an advantage of pre-training.\nOur results have consequences for applying deep learning in other tasks of binary code analysis. First, an end-to-end setup is typically easy to deploy along with the learning model. Hence, if labeled data is available, it should be the first option when developing a new approach. Second, when labeled data is scarce or the training time is constrained, pre-trained embeddings are a reasonable solution. For example, we could show that Palm Tree often provides good results when we capped the available labeled data.\nOverall, our study highlights an interesting aspect of interdisciplinary research. The benefits of embeddings in one research area can only be transferred to another if the experimental setup is limited by similar constraints. Once these constraints are lifted from a setup, the performance improvement may disappear. Therefore, we recommend that practitioners develop learning-based approaches judiciously and, when in doubt, prefer well-known learning concepts over the latest inventions."}]}