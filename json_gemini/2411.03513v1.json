{"title": "Change Is the Only Constant: Dynamic LLM Slicing based on Layer Redundancy", "authors": ["Razvan-Gabriel Dumitru", "Paul-Ioan Clotan", "Vikas Yadav", "Darius Peteleaza", "Mihai Surdeanu"], "abstract": "This paper introduces a novel model compression approach through dynamic layer-specific pruning in Large Language Models (LLMs), enhancing the traditional methodology established by SliceGPT. By transitioning from constant to dynamic slicing, our method leverages the newly proposed Layer Redundancy (LR) score, which assesses how much change each layer changes its input by measuring the cosine similarity of the input to the output of the layer. We use this score to prune parts of individual layers based on redundancy in such a way that the average pruned percentage for all layers is a fixed value. We conducted extensive experiments using models like Llama3-8B and Mistral-7B on multiple datasets, evaluating different slicing bases and percentages to determine optimal configurations that balance efficiency and performance. Our findings show that our dynamic slicing approach not only maintains but, in many cases, enhances model performance compared to the baseline established by constant slicing methods. For instance, in several settings, we see performance improvements of up to 5% over the SliceGPT baseline. Additionally, a perplexity decrease by as much as 7% was observed across multiple benchmarks, validating the effectiveness of our method. The code, model weights, and datasets are open-sourced at https://github.com/RazvanDu/DynamicSlicing.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs), characterized by their massive scale, often consist of billions to trillions of parameters, enabling them to perform a wide range of complex tasks with remarkable proficiency. However, the deployment of these models poses significant challenges, primarily due to the extensive computational resources requirements. As the scale of these models grows, so does the urgency to develop more efficient methods for their deployment. This has led to increased interest in model compression techniques that aim to reduce the computational burden without substantially sacrificing performance. Techniques such as knowledge distillation, quantization, or pruning variants have emerged as viable solutions, each offering a different approach to streamlining model architecture and operations. In this paper, we improve the work on model pruning introduced by SliceGPT, a pruning technique via a constant slicing percentage of each layer. While this approach reduces computational demands and maintains a level of performance, it does not account for the varying significance of different layers within the network. We propose a more nuanced, dynamic pruning method that adapts the degree of pruning based on the individual characteristics and contributions of each layer. Our method aims to optimize both the efficiency and the efficacy of the pruning process by preserving more functionality in critical areas of the model, leading to better performance and less degradation in tasks.\nMore specifically, we develop a new metric, namely Layer Redundancy (LR) score, to quantify the impact of each layer on the model's overall performance. This evaluation is essential, as it guides the order in which layers are pruned, ensuring that the most influential layers are preserved while less critical layers are removed. Our approach involves generating slicing functions tailored to the importance of each layer, allowing for a dynamic and informed pruning strategy. Our results from the extensive empirical studies across various datasets and base models show a substantial improvement in model accuracy across all datasets tested, accompanied by a notable reduction in perplexity. In order to thoroughly evaluate the effectiveness of our proposed dynamic slicing pattern, we also analyzed the median accuracy and perplexity across"}, {"title": "2 Related Work", "content": "Recent advancements in model compression techniques have markedly improved the efficiency of deploying LLMs while striving to retain their performance.\nThe field has seen a variety of approaches including knowledge distillation, quantization, pruning, low-rank adaptation or hybrid variants, each designed to address the growing computational and memory requirements of these models.\nInnovative approaches such as LLM-Pruner and LaCo (Layer Collapse) offer novel perspectives on model pruning. LLM-Pruner focuses on structured pruning by identifying and removing dependency groups within the model, aiming to minimize dependency on the original training corpus while preserving linguistic capabilities. Similarly, LaCo presents a layer-wise pruning strategy where subsequent layers collapse into preceding ones, achieving notable size reduction while maintaining good performance. A third approach explores the potential of simple layer-pruning strategies combined with parameter-efficient finetuning (PEFT), demonstrating minimal performance loss even when half of the model's layers are removed.\nAmong the innovative strategies in LLM optimization, SliceGPT emerges as a significant breakthrough in model compression. Developed to address the intensive computational and memory demands of deploying LLMs, SliceGPT employs a unique post-training sparsification technique. Although effective in practice, previous research has illustrated that the order in which layers are removed plays a critical role in model performance. This insight led us to explore variable slicing percentages across different layers, challenging the constant slice for all layers. Initial attempts by the creators of SliceGPT to implement this through spectral analysis of layers did not yield a reliable method for determining the optimal percentage to be removed from each layer, as spectral analysis only tells part of the story and doesn't correlate with how much can be sliced out of a layer."}, {"title": "3 Method", "content": "Building on concepts introduced in a recent, unpublished study, we propose a novel metric for assessing layer usefulness. This metric quantifies the extent to which each layer modifies its input by measuring the cosine similarity between the input and output. Specifically, we define this as the Layer Redundancy (LR) score:\n$LR(L_i) = \\frac{L_iL^O}{\\|L_i\\||\\|L^O\\|}$ (1)\nIn Eq 1 $L$ refers to the input of layeri, while $L^O$ refers to the output of layeri. Intuitively, the higher cosine similarity between inputs and outputs leads to higher LR score, implying that the layer is more redundant. To evaluate the score for all layers, we use the full validation set of the PG-19 data set, we pass the data as context through the LLMs and we sum up how much each layer processed its input according to the cosine similarity score. At the end we normalize the values linearly so that the min$(LR(L_i)) = 0$ and max$(LR(L_i)) = 1$, guaranteeing that they range from 0 to 1. Intuitively, a LR score closer to 0 corresponds to lesser redundancy while higher LR score (close to 1) would mean high redundancy. To the best of our knowledge, our work is the first one to use a per layer importance/redundancy score to prune variable parts of layers out (explained in the following section), instead of removing whole layers or removing a fixed constant portion of the layer."}, {"title": "3.2 Defining a per-layer percentage", "content": "Our goal is to have a function that slices variable sized parts of each layers based on their LR score while keeping the overall average of sliced out parts across all layers to be a fixed percentage of the LLM parameters specified by the user. For example, layers with higher LR values (or redundancy) can be sliced more compared to layers with lower LR score (or redundancy). To achieve this, we applied several transformations on the previously defined LR score such that we have control over how big of an impact we want the redundancy to have on the sliced percentage.\nThe first step is to control the average of the LR score so that we can then control the average pruned percentage of the LLM. Concretely, we denote the average desired slicing percentage as $S_p$ (Slice Percentage). In order to investigate what happens as we go further away from a constant slice for each layer, we will also define $S_B$ (Slice Base) to be a fixed constant value that will be guaranteed to be sliced from each layer such that $S_B <= S_p$. As $S_B = S_p$, the slicing becomes constant as presented by Ashkboos et al. (2024). We experimented with different values of $S_B$ to see the effect of layer redundancy on the LLM's performance. The next step is to scale the LR function so that its average is $S_P - S_B$, thus once we add the base percentage for each layer ($S_B$) the total average will be $S_P - S_B + S_B = S_P$. This can be achieved by multiplying each LR value by the ratio of $S_P - S_B$ divided by the mean of the function. This is denoted as Slice per Layer Redundancy (SLR) and shown in Eq 2.\n$SLR(L_i) = LR_i \\cdot \\frac{S_P - S_B}{\\frac{1}{n} \\sum_{i=1}^n LR_i}$ (2)\nAt the end, the Final Slice (FS) for the layer Li, is the sum of SLR and SB as shown in Eq 3.\n$FS(L_i) = SLR(L_i) + S_B$ (3)"}, {"title": "3.3 Slicing parts of layers", "content": "The methodology in SliceGPT utilizes a specialized version of Principal Component Analysis (PCA) for efficient data reduction. It projects the data matrix X onto a lower-dimensional subspace using the eigenvectors Q and a deletion matrix D. The reduced matrix Z is computed as XQD, where D selectively omits certain components from Q, resulting in a compressed representation Z. The approximate reconstruction X is obtained by $ZDTQ^T$, minimizing the reconstruction error $\\|X \u2013 \\hat{X}\\|^2$. Unlike SliceGPT, we control the dimension of the matrix D on a per-layer basis to achieve our dynamic slice."}, {"title": "4 Experiments", "content": "The first step of the process is to evaluate how redundant each one of the layers is using the procedure described above. We have done this for Llama3-8B and Mistral-7B using the full validation split of the pg-19 data-set. This will give us a LR score for each layer that we then need to process into a slicing pattern. Furthermore, we evaluate Llama3-8B and Mistral-7B using different Slice Base ($S_B$) values in increments of 2%. Intuitively as we decrease the base percentage of the layers we have more extreme slicing patterns. In all our experiments we compare with the constant slice as a baseline. We experiment with Slice Percentages ($S_p$) of 30%, 35%,"}, {"title": "5 Results", "content": "We will first explore how the accuracy is affected by our dynamic slicing. As observed in Figure 2, for the Llama3-8B model, the accuracy is improved across all of the 5 evaluated datasets, and the perplexity decreases by as much as 1.4 while pruning the exact same amount from the model. We also see huge accuracy improvements from 52% to 57% on Winogrande which has a baseline accuracy of 50%. An important finding in all cases is that the perplexity decreases and task accuracy (mostly) increases when we started to decrease the SB (until a certain point) showcasing LLMs benefit more from dynamic slicing as proposed in our work.\nTo estimate a good SB in our proposed dynamic slicing method, we evaluated the accuracy at the SB point that achieves the minimum perplexity on Wikitextv2, thus using it as a calibration data set. The results shown in Table 1 indicate that our method outperforms SliceGPT on average for all pruning ratios and models explored. We also show even better results using the median accuracy over all SB values (Table 3) and mean accuracy values (Table 4), leading us to believe that there are even better ways to choose an SB value than the minimum perplexity."}, {"title": "5.1 Analyses", "content": "To highlight strengths of pruning with our dynamic slicing, we also show comparison with Short-GPT where entire set of layers are pruned or removed. Please note that techniques such as ShortGPT that have shown to be effective, often provide lesser flexibility in pruning ratio as entire set of layers are removed. For example, removing 9 (least important) layers out of 32 layers results in a pruning ratio of 28.1% as shown in Table 2.\nAs shown in Table 2, our dynamic slicing with SliceGPT outperforms ShortGPT in majority of the cases even with higher pruning ration i.e., 28.1% vs. 30%, 34.3% vs. 35%, and 37.5% vs. 40%. Especially for higher pruning ratio (i.e., 35% or 40%), our dynamic slicing based SliceGPT approach outperforms ShortGPT with a larger margin across the four classification datasets shown in Table 2. Importantly, removing set of layers completely as in ShortGPT lead to very high perplexity values suggesting high degradation in text generation quality. On the other hand, our proposed dynamic slicing technique with SliceGPT results in exponentially better perplexity score highlighting benefits of pruning only parts of LLM layers instead of removing entire layers.\nAdditionally, our variable slicing scaled from layer importance can be easily extended and merged with techniques like ShortGPT by simply removing the least important layers completely and slicing moderately important layers using our proposed approach. This hybrid method leveraging the strengths of both techniques, could potentially enhance model efficiency and performance. We leave this for exploration in future work."}, {"title": "6 Conclusions", "content": "In conclusion, we propose a novel dynamic slicing strategy that shows considerable improvements in accuracy when compared against a fixed slicing method that prunes the same amount of parameters on average. We also show that layer redundancy is a powerful metric when removing percentages of layers, and also that there is room for improvement, leading to possible future work in the field."}, {"title": "Limitations", "content": "While our study introduces significant advancements in the dynamic pruning of Large Language Models, there are several limitations that are worth discussing:\n\u2022 The effectiveness of our method has been demonstrated predominantly on the Llama3-8B and Mistral-7B models. However, its performance may vary with other architectures, especially those with different layer configurations or learning dynamics.\n\u2022 We only experiment with one method to choose the SB value (that gives lowest perplexity) and there can be other methods for estimating SB which we leave it to more focused future works.\n\u2022 Limited computational resources have constrained our ability to test on larger models. We believe that exploring the efficacy of our dynamic pruning method on more extensive architectures could provide valuable insights into its scalability and performance."}, {"title": "Ethical Considerations", "content": "Ethical considerations are central to our development of a dynamic pruning method for large language models. Our research strives to reduce the computational costs and environmental impact of deploying large-scale models, aligning with the ethical responsibility to promote environmental sustainability and minimize negative consequences such as excessive energy consumption. This not only makes LLMs more accessible but also supports broader societal needs by enabling more efficient processing solutions that respect both individual rights and community values. By enhancing the efficiency of these models, we may enable populations with limited computational resources to harness the power of advanced NLP tools.\nFurthermore, our methodology emphasizes the importance of using data sets that are free from harmful content. By using data sets that do not contain harmful data, we aim to ensure that the resulting models avoid biased outputs or content that could reduce their utility in practical applications. Ensuring the integrity and appropriateness of pruned models is essential, as these models often play significant roles in decision-making processes across various sectors. In this context, our approach is designed to be transparent and responsible, providing clear documentation and rigorous evaluation to maintain the reliability and fairness of the models.\nAlso, we have selected a color scheme that prioritizes accessibility, ensuring the visuals are clear and discernible to individuals with color vision deficiencies. This inclusive approach reflects our commitment to making our research accessible to a wider audience, including those with varying visual abilities."}, {"title": "A Appendix", "content": ""}, {"title": "A.1 Experimental details", "content": "For all our experiments we used 4 NVIDIA A100 GPUs, with 80GB of VRAM each, and running all of the data sets on one slicing pattern using one NVIDIA A100 takes around 30 minutes, leading to a total time of 10 hours for a plot that has 20 possible SB values. We use a total of 1000 samples for each data set in all our configurations. Also, calculating the LR score can take upwards of 30-40 minutes per model on 1 NVIDIA A100 GPU, but that only has to be computed once."}, {"title": "A.2 Tables with mean/median results", "content": "For a detailed evaluation, Table 3 compares our technique in the median setting against the constant slicing proposed by SliceGPT, and Table 4 provides a comparison in the average setting."}, {"title": "A.3 Llama3-8B and Mistral-7B in various scenarios", "content": "Visual representations of our experiments are shown across several figures: Figure 3 and Figure 4 illustrate the performance of Llama3-8B with 30% and 35% of the network sliced, respectively, comparing it to the baseline accuracy achieved by SliceGPT. Similarly, Figures 5, 6, and 7 display the results for Mistral-7B with 30%, 35%, and 40% of the network sliced, again benchmarked against SliceGPT's constant slice accuracies. In each figure, the red line denotes the baseline accuracy set by SliceGPT for the respective slicing percentages."}]}