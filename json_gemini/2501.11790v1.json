{"title": "Benchmarking Large Language Models via Random Variables", "authors": ["Zijin Hong", "Hao Wu", "Su Dong", "Junnan Dong", "Yilin Xiao", "Yujing Zhang", "Zhu Wang", "Feiran Huang", "Linyi Li", "Hongxia Yang", "Xiao Huang"], "abstract": "With the continuous advancement of large language models (LLMs) in mathematical reasoning, evaluating their performance in this domain has become a prominent research focus. Recent studies have raised concerns about the reliability of current mathematical benchmarks, highlighting issues such as simplistic design and potential data leakage. Therefore, creating a reliable benchmark that effectively evaluates the genuine capabilities of LLMs in mathematical reasoning remains a significant challenge. To address this, we propose RV-Bench, a framework for Benchmarking LLMS via Random Variables in mathematical reasoning. Specifically, the background content of a random variable question (RV question) mirrors the original problem in existing standard benchmarks, but the variable combinations are randomized into different values. LLMs must fully understand the problem-solving process for the original problem to correctly answer RV questions with various combinations of variable values. As a result, the LLM's genuine capability in mathematical reasoning is reflected by its accuracy on RV-Bench. Extensive experiments are conducted with 29 representative LLMs across 900+ RV questions. A leaderboard for RV-Bench ranks the genuine capability of these LLMs. Further analysis of accuracy dropping indicates that current LLMs still struggle with complex mathematical reasoning problems.", "sections": [{"title": "1 Introduction", "content": "The emergence of large language models (LLMs) has led to impressive results across a wide range of applications, including machine translation (Zhang et al., 2023; Zhu et al., 2024), text summarization (Liu et al., 2024d), and question answering (Kamalloo et al., 2023). With advancements in LLMs' reasoning capabilities (Huang and Chang, 2023), their performance on complex tasks suchas code generation (Hong et al., 2024b,a), planning (Huang et al., 2024a), and, particularly, mathematical reasoning and computation (Romera-Paredes et al., 2024), has become a central focus within the LLM research community (Zhao et al., 2023). Domain-specific studies of LLMs in mathematical reasoning (Ahn et al., 2024) highlight their strong potential for addressing real-world challenges. As this area continues to be a prominent focus of LLM research, numerous promising methods (Luo et al., 2023; Xu et al., 2024b) and benchmarks (Fang et al., 2024) have been developed to enhance LLMs' performance on mathematical tasks.\nHowever, are current evaluations of LLMs in mathematical reasoning truly reliable? Fig. 1 illustrates a discrepancy within the well-known MATH (Hendrycks et al., 2021b) dataset. In our pilot experiments, powerful LLMs like GPT-40 (Achiam et al., 2023) perform well on math problems in standard benchmarks but experience a significant drop in accuracy when answering questions with the same content but different numerical combinations of variables (Mirzadeh et al., 2024), as detailed in Sec. 4.4. This discrepancy raises two potential concerns about the current evaluation framework: 1) The design of standard benchmarks may be overly simplistic for contemporary LLMs, as they typically only evaluate performance on fixed-variable problems. The LLMs may not genuinely understand the problem but instead \"guess\" the correct answer (Dong et al., 2024); 2) The problems in widely-used benchmarks might be \u201cmemorized\u201d by LLMs due to potential data leakage during training, allowing the models to achieve high accuracy solely on the original problems (Ni et al., 2024). The latter concern presents a significant challenge in assessing the genuine capabilities of LLMs (Deng et al., 2024).\nThe advanced study presents an in-depth analysis suggesting that the probabilistic modeling of LLMs during the reasoning process obscures the fact that they are not genuinely capable of formal reasoning (Shi et al., 2023; Jiang et al., 2024). Additionally, the ongoing improvements in model performance have drawn research focus to potential issues such as data contamination and overfitting during LLM training (Xu et al., 2024a). Given that mathematics is a foundational topic applicable across a wide range of semantic scenarios, the increasing popularity and prevalence of math datasets like GSM8K (Cobbe et al., 2021), MathQA (Amini et al., 2019), and MATH (Hendrycks et al., 2021b) raise the risk of potential data contamination. Although recent studies on contamination detection (Chern et al., 2024; Ni et al., 2024) can signal unreliable results during the evaluation phase, they fail to reflect the genuine performance of LLMs, as data leakage likely occurs during the training phase and remains non-intervenable (Kapoor and Narayanan, 2023).\nThe above situation raises a critical issue: current benchmarks may not truly reflect the performance of LLMs (Balloccu et al., 2024; Mirzadeh et al., 2024). In this context, effectively benchmarking LLMs for genuine mathematical reasoning capabilities remains a significant challenge. To address this, we propose RV-Bench in this study as a solution for benchmarking LLMs in mathematical reasoning using random variables questions (RV questions), providing a genuine and effective evaluation framework that addresses concerns 1) and 2) mentioned above.\nSpecifically, we construct question functions based on the original problem from two selective mathematical data sources: MATH (Hendrycks et al., 2021b) and LeetCode-Math\u00b9. These functions generate instantiated questions with random variables and their corresponding answers. The RV questions are then collected to evaluate LLMs' mathematical reasoning capabilities. Unlike existing math benchmarks (Cobbe et al., 2021; Hendrycks et al., 2021b), RV-Bench includes questions with a wide range of variable combinations, rather than fixed ones. Furthermore, RV-Bench provides \"unseen\" questions, allowing LLMs to demonstrate their genuine performance even if the model has been contaminated by certain benchmarks (Mirzadeh et al., 2024). To achieve high accuracy in this framework, an LLM must fully understand the problem-solving process to correctly answer the RV questions, effectively reflecting its genuine capabilities in mathematical reasoning.\nIn summary, our contributions are listed in the following:\n\u2022 We propose RV-Bench, a framework that benchmarks LLMs using RV questions, providing a comprehensive evaluation of their genuine mathematical reasoning capabilities, regardless of potential data leakage in standard benchmarks during model training.\n\u2022 We compare the accuracy of LLMs on RV questions with their accuracy on original problems. The observed drop in accuracy reveals the limitations of current LLMs, suggesting a potential reliance on original solutions. Additionally, we highlight the domain dependence of LLMs in mathematical reasoning, raising concerns about the reliability of current LLM evaluations in this field.\n\u2022 We introduce the random variables framework for genuinely evaluating model performance to the LLMs community. Our work will be fully open-source to support further research."}, {"title": "2 Related Work", "content": "The rapid development of LLMs has significantly advanced the evaluation of their capabilities (Chang et al., 2024). Well-designed benchmarks such as MMLU (Hendrycks et al., 2021a), GLUE (Wang et al., 2018), MMLU-Pro (Wang et al., 2024b), SuperGLUE (Wang et al., 2019), CommonSenseQA (Talmor et al., 2019), and"}, {"title": "2.2 LLMs in Mathematical Reasoning", "content": "Mathematical reasoning is a task that effectively showcases the capabilities of LLMs and has garnered significant attention within the community, achieving remarkable advancements (Ahn et al., 2024). Techniques such as continual pre-training (Lin et al., 2024), fine-tuning (Yuan et al., 2023), and reinforcement learning (Wang et al., 2024a) are extensively employed to enhance LLMs' mathematical reasoning. This has led to the development of math-specific LLMs like Qwen-Math (Yang et al., 2024b), DeepSeek-Math (Shao et al., 2024), and MetaMath (Yu et al., 2024). Consequently, these methodologies have attained near-perfect performance on complex mathematical reasoning datasets (Fourrier et al., 2024). OpenMath (Toshniwal et al., 2024) highlighted the importance of question diversity during fine-tuning, achieving a 96% accuracy on the grade-school arithmetic reasoning benchmark GSM8K (Cobbe et al., 2021). Likewise, o1 (Zhong et al., 2024) improved performance on the high-school-level mathematical competition benchmark MATH (Hendrycks et al., 2021b), attaining nearly 95% accuracy.\nMost recently, GSM-Symbolic (Mirzadeh et al., 2024) uncovered a limitation regarding LLMs' genuine capabilities in arithmetic reasoning by evaluating them with diverse questions generated from symbolic templates of GSM8K. The study revealed that LLMs tend to replicate reasoning steps observed during training, rather than genuinely reasoning through specific problems, which poses a critical challenge to the current evaluation methodologies for LLMs. To further address this issue, our proposed RV-Bench employs \u201cunseen\u201d RV questions to effectively assess LLMs' mathematical reasoning. We provide a re-ranking and comparison with the original MATH benchmark, demonstrating that LLMs still face significant challenges in understanding and solving complex mathematical reasoning questions, thereby revealing their genuine capabilities."}, {"title": "3 RV-Bench", "content": "As exemplified in Fig. 2, a question function in RV-Bench is illustrated. In this section, we detail the process of constructing RV-Bench, spanning from the data sources to the annotation process."}, {"title": "3.1 Data Sources", "content": "The proposed RV-Bench comprises question functions constructed based on the problems collected from two selective data sources: the MATH (Hendrycks et al., 2021b) test set and the LeetCode-Math branch.\nMATH is a well-known dataset that covers 12,500 challenging mathematics problems targeted at high-school mathematics competitions. It includes annotated answers with full step-by-step reasoning processes, frequently used to enhance LLMs' capabilities in complex mathematical reasoning. Following the processing settings of the MATH-split in another widely-adopted dataset, PRM800K (Lightman et al., 2024), we construct 110 question functions by uniformly selecting problems at random from the test split for the following process LeetCode is a widely recognized platform providing coding and algorithmic problems for users to practice coding skills (Coignion et al., 2024). As part of the coding questions, LeetCode-Math includes algorithmic questions whose content is designed based on mathematical reasoning and computation. Our motivation for selecting LeetCode as one of our data sources is derived from its original focus on coding problems. By transforming these problems into mathematical reasoning formats, we ensure these problems are unlikely to have been encountered during the LLMs' training. This transformation effectively showcases genuine model performance, independent of potential data leakage in coding formats. Through a careful review of all candidate solutions for each question, we construct 90 question functions by reformatting the question content with random variables, selected at random.\nConsequently, the question functions in RV-Bench are randomly sampled from their respective data sources, maintaining similar distributions of difficulty, type, and bias as the original problems."}, {"title": "3.2 Question Functions Construction", "content": "As illustrated in Fig. 2, the complete question function consists of three modules: initialization, solution, and generation. These modules are responsible for instantiating the random variables, solving the questions with any variable combinations, and generating the QA pairs for RV-Bench, respectively.\nIn this section, we detail the construction of the question function by thoroughly describing the annotation process for each module.\nPreliminaries. To ensure the quality of our benchmark, we recruited 10 candidate annotators, all of whom are graduate students with strong backgrounds in mathematics and computer science (majoring in one domain and achieving an \"A\" grade in the other through examination). Candidates were required to solve a selection of mathematical and coding problems (100 in total) sampled from the MATH test set and LeetCode for qualification purposes. Ultimately, six candidates were chosen to serve as the professional annotators for RV-Bench.\nFor a specific problem, as exemplified in Fig. 2, the annotator initially reviews the problem meticulously to ensure a thorough understanding of the original content and the text-based solution provided. If the problem is found to be unclear or insufficiently comprehended by the annotator, it is subjected to a post-calibration process for further discussion, details of which are provided in Sec. 3.4. Problems that are well comprehended proceed to the annotation process as described below."}, {"title": "Step 1: Identify and Initialize the Variables.", "content": "For a problem meeting the criteria, the annotator begins by identifying the variables. Typically, key numbers, names, and equations are potential candidates for random variables. In practice, a problem may contain multiple variables, but to maintain a consistent difficulty level, only those variables that align with the original problem's intent are selected for randomization. As illustrated in Tab. 1, the original problem involves calculating the probability for a specific sum with two dice. The number of dice and the target sum, which are integral to the problem's intent, are identified as variables, highlighted in green and yellow, respectively. The description \"6-sided dice\u201d which serves as a characteristic of the objective content, is not identified as a variable and is shown in gray. Once the variables are identified, the annotator locates their numerical and symbolic elements in the original problem and replaces them with slots.\nNext, the annotator assigns a range to the identified variables, accompanied by semantically related variable names to define the initialization module. For variables that involve interdependent calculations, their ranges must be mutually constrained to ensure the question remains solvable (details are discussed in Sec. 3.3). For example, in Tab. 1, the range of the target sum depends on the number of dice. To initialize the question function, each variable is assigned a random value selected from its range."}, {"title": "Step 2: Construct the General Solution.", "content": "For RV questions, the numerical outcomes will vary with different combinations of variable values. Therefore, a general solution must be constructed to solve the problem irrespective of these values.\nIn the case of problems from the MATH test set, the provided solutions include detailed text-based step-by-step problem-solving process. The annotator is required to convert these text-based solutions into code implementations. In certain instances, the annotator may need to revise the code implementation because some text-based solutions are coupled tightly to the specific problem and lack generalization. The process of constructing a general solution for LeetCode-Math problems differs slightly. For each problem, some code-based solutions are available from the community2, which have been validated through successful execution. The annotator must identify and comprehend a Python solution and transform it into the appropriate format, therefore defining the solution module based on the community-provided solutions.\nOnce the variable combination has been initialized in Step 1, the solution module takes it as input and returns a correct corresponding answer. This module undergoes further validation for correctness and effectiveness through calibration among annotators, as detailed in Sec. 3.4."}, {"title": "Step 3: Generate the QA Pairs.", "content": "Following the completion of the previous steps, where the initialized variables and the general solution for various variable combinations were defined, annotators proceed to Step 3. In this step, annotators utilize the problem content with slots identified in Step 1 to define the generation module. This involves filling the slots with randomized variable values and formatting the output into question-answer (QA)pairs. Importantly, the original problem may include content that is extraneous to the problem (e.g., restrictions related to computational environments). Annotators are required to remove these irrelevant sections to ensure the question content focuses solely on the pertinent details. All generated questions from different question functions are compiled into a comprehensive question set for RV-Bench."}, {"title": "3.3 Conditions", "content": "To maintain the difficulty level of the RV questions consistent with the original problems, we incorporate difficulty control conditions when defining the random ranges for variables. We establish three conditions/criteria for setting the random range in Step 1: 1) The fluctuation range of variables should remain uniform across different questions; 2) Variables that significantly affect the problem's complexity may be fixed as constants; 3) The random range for simpler questions can be broader, whereas for more challenging questions, it should be narrower to prevent considerable variations in difficulty. By controlling the difficulty level, we ensure that LLMs are fairly compared on RV questions relative to the original problem, minimizing performance differences that could arise from variations in difficulty."}, {"title": "3.4 Calibration and Post-Filtering", "content": "Following the annotation process, we undertake a calibration and post-filtering step (Li et al., 2024) to enhance the consistency and objectivity of the question functions in RV-Bench. During Step 1, any problematic question that is not well-comprehended is promptly subjected to calibration and discussion. Confusing problems are collaboratively reevaluated and re-entered into the annotation process. If a problem cannot guarantee solvability or generalization for random variables, it is removed from the dataset. After all question functions have been annotated, a cross-calibration process is conducted. Annotators review each other's annotations, verifying the correctness of the question functions and testing the runtime of the solution module across a broad spectrum of variable combinations. This runtime testing helps identify potential issues, such as exceeding maximum recursion depth, to ensure that each unique variable combination remains correctly solvable. Additionally, question functions derived from LeetCode that do not closely relate to mathematical reasoning or computation are filtered out."}, {"title": "4 Experiments & Analysis", "content": "In this section, we present our experimental findings on benchmarking LLMs via random variables:\n\u2022 First, we introduce the RV-Bench leaderboard, ranked across various LLMs. We employ four carefully designed metrics to highlight the capabilities and characteristics of LLMs from different perspectives.\n\u2022 Next, we compare the accuracy of RV-Bench questions with their original corresponding problems, presenting the accuracy drop across different model sizes and families.\n\u2022 Finally, we validate the consistency of RV-Bench using various random seeds on representative LLMs, demonstrating the effectiveness of the generated questions in RV-Bench."}, {"title": "4.1 Experimental Setups", "content": "Datasets. After the calibration and post-filtering processes, RV-Bench consists of 183 question functions, with 108 derived from the MATH test set and 75 from LeetCode-Math. To evaluate LLM performance on both RV and original versions, we sample the corresponding original problems from the MATH test set (MATH-Sp) and LeetCode-Math (LeetCode-Sp). Specifically, we define the problem that is instantiated with input and output provided by examples found below the official description as the original problem in LeetCode3.\nFor each question function, five instantiated RV questions are generated. In total, 540 RV questions are generated for the MATH question functions (MATH-RV) and 375 for the LeetCode question functions (LeetCode-RV). This question set is utilized in the main experiments described in Sec. 4.3 and Sec. 4.4. To assess the consistency of RV-Bench (Sec. 4.5), we create 50 question sets4, each consisting of one question generated by each MATH-RV function.\nEvaluation Metrics. We define four metrics for RV-Bench evaluation:\n1. Exact Match Accuracy (EM): This metric measures the correctness of each question through strict string matching.\n2. Group Accuracy@n (GA@n): This metric indicates that all n RV questions are answered correctly. Given the stringent condition of \"all correct\" for LLMs, this metric essentially represents a lower bound and is unlikely to approach the infimum, due to the inherent randomness of neural networks (Zhuang et al., 2022).\n3. Complete Accuracy (CA): This metric assesses whether the original problem is answered correctly and at least 80% (rounded up) of the RV questions are also correct. It reflects the proportion of questions where the LLMs demonstrate full understanding and can correctly answer both the RV and original versions.\n4. Original Only Ratio (OOR): This metric evaluates whether the original problems from the standard benchmark are answered correctly, while at least 20% (rounded down) of the RV questions are incorrect. It indicates the proportion of questions where the LLMs exhibit partial understanding, able to answer only the original version correctly but not the RV version.\nImplementations. Following the evaluation setup of LLaMA-3 (Dubey et al., 2024), we employ 4-shot prompting using example problems from Minerva (Lewkowycz et al., 2022) during inference on MATH-RV and MATH-Sp. Similarly, for LeetCode-RV and LeetCode-Sp, we select four"}, {"title": "4.2 Model Selection", "content": "The selected models include a diverse range of popular and well-recognized LLMs, covering various model sizes and families to draw comprehensive conclusions across different aspects. Given the current focus on open-source LLMs, we select widely-used representative models such as LLaMA (Dubey et al., 2024), Qwen (Bai et al., 2023; Yang et al., 2024a), Phi (Abdin et al., 2024), Yi (Young et al., 2024), Gemma (Team et al., 2024), DeepSeek (Liu et al., 2024a), and Mistral (Jiang et al., 2023) to evaluate their genuine mathematical capabilities. We also include math-specific open-source models tailored for mathematical domain expertise: Qwen-Math (Yang et al., 2024b), Mathstral, and DeepSeek-Math (Shao et al., 2024). For proprietary LLMs, we incorporate well-known models like GPT-3.5 (Brown et al., 2020), GPT-40 (Achiam et al., 2023), Claude-3 (Anthropic, 2023), GLM-4-Plus (GLM et al., 2024), and DeepSeek-V3-Chat (Liu et al., 2024b). Finally, given the rising interest in large reasoning models (LRMs) in both academia and industry, we also include QwQ (Team, 2024; Yang et al., 2024a) and GLM-Zero-Preview."}, {"title": "4.3 Main Results", "content": "Tab. 4 provides a comprehensive overview of the performance of various LLMs on our proposed RV-Bench. Based on the metric definitions in Sec. 4.1, it is apparent that, in most cases, the order of metric values for the same model typically follows EM > CA > GA@5. LLMs with genuine reasoning capabilities are expected to perform exceptionally well across these three metrics, while maintaining a preferably lower score in OOR. As a result, the generally lower values of GA@5 and CA, particularly in LeetCode-RV, imply that while models are capable of correctly answering certain instances of questions, they tend to struggle with consistency across multiple instantiated RV questions sharing the same background. This observation suggests that current LLMs still face challenges in thoroughly understanding and solving complex mathematical reasoning problems, particularly those of specific typeswithin this domain (Liu et al., 2024c).\nNotably, the performance of nearly all LLMs on LeetCode-RV is significantly inferior compared to MATH-RV. A plausible explanation is the higher difficulty level of LeetCode-RV questions, as these algorithmic math problems introduce additional complexities requiring enumeration reasoning and problem-solving skills beyond basic mathematical calculations. Another potential factor is the relative abundance of math-related training data in domains similar to MATH-RV (e.g., MATH'S training split), contrasted with the scarcity of data resembling LeetCode-RV. As noted in Sec. 3.1, LeetCode-related data is predominantly used for enhancing coding skills rather than mathematical reasoning, making it unlikely for an LLM to incorporate these questions during training. The true reason behind this phenomenon is not fully explored in this paper, and further targeted experiments are necessary to provide conclusive explanations (Zhao et al., 2023).\nIn detail, DeepSeek-V3-Chat (Liu et al., 2024b) achieves the best performance, excelling not only in exact match accuracy with 73.55% but also in GA@5, CA, and OOR metrics, showcasing its robust genuine mathematical reasoning capabilities. Additionally, LRMs like QwQ-32B (Team, 2024) and GLM-Zero-Preview, despite their moderate size of approximately 30B, demonstrate strong and reliable mathematical reasoning abilities like large-scale LLMs. Their performance, enhanced by computational scaling during inference, even surpasses advanced models such as GPT-40. Furthermore, the Phi-4 model (Abdin et al., 2024), with just 14B parameters, achieves remarkable results, outperforming GPT-40 and further validating the efficacy of synthetic data. The Qwen2.5 family (Yang et al., 2024a), including the 72B and 7B (also math-specific (Yang et al., 2024b)) versions and even the 3B version, exhibits robust mathematical reasoning capabilities across nearly all metrics."}, {"title": "4.4 Accuracy Dropping", "content": "Fig. 3 illustrates the accuracy decline of various LLMs when transitioning from the original problems in standard benchmarks to RV questions in RV-Bench. Each bar represents the accuracy drop for a specific LLM, with blue indicating the MATH domain and red indicating the LeetCode domain. Most models exhibit a substantial decrease in performance, with some experiencing drops exceeding 40%. By comparing performance in MATH-RV and MATH-Sp, this phenomenon suggests a potential reliance on memorization of specific problems and replicating reasoning steps observed during training, rather than genuine understanding. Moreover, by examining the model rank highlighted in the yellow box in Fig. 3, we observe a general trend: the lower a model's rank, the greater its accuracy drop on RV questions. This indicates that lower-ranked LLMs not only tend to perform poorly on RV questions but also exhibit larger accuracy discrepancies compared to their performance on the original problems.\nAn intriguing observation is the notable disparity in accuracy drops between the MATH and LeetCode domains. As shown in Fig. 3, while the accuracy drop from the standard benchmark to RV-Bench generally decreases in the MATH domain, it does not always exhibit a similar reduction in the LeetCode domain. For example, the LLMs with rank ID 22 (Llama-3.1-8B-It (Dubey et al., 2024)) and 18 (Gemma-2-27B-It (Team et al., 2024)) display a significant disparity in accuracy drop between the MATH and LeetCode domains. This highlights that some models exhibit greater robustness to random variable perturbations in the MATH domain but are more sensitive in the LeetCode domain. This suggests that the mathematical reasoning capabilities acquired by these models during training are heavily domain-dependent, rather than representing a general domain-agnostic reasoning capability. This raises concerns about the reliability"}, {"title": "4.5 Consistency with Randomization", "content": "To ensure the effectiveness of the proposed RV questions and the repeatability of the results. Fig. 4 examines the consistency of model performance across 50 RV questions set generated by different random seeds. Each subplot shows the accuracy distribution for a specific LLM, fitted with an approximate normal distribution curve. Although there are variations in some question sets, the models exhibit relatively consistent performance across various randomization. For example, GPT-4o achieves a mean accuracy of 76.26% with a standard deviation of \u00b13.46, while DeepSeek-v3-Chat attains 85.35% with \u00b11.66, presenting consistent reliability across runs. These findings suggest that despite fluctuations from different random seeds, the RV questions maintain a consistent difficulty level, reinforcing the validity of RV-Bench in accurately reflecting LLMs genuine reasoning capabilities."}, {"title": "5 Conclusion", "content": "In this paper, we introduced RV-Bench, a novel framework designed to benchmark the genuine mathematical reasoning capabilities of LLMs using random variable questions. Unlike existing benchmarks that rely on fixed-variable problems, RV-Bench evaluates the LLM's capability to understand and solve mathematical problems across a wide range of variable combinations, making it a more reliable and effective evaluation framework. Through extensive experiments, we demonstrated that current LLMs, despite showing high performance on standard benchmarks, often experience a significant drop in accuracy when faced with random variable instantiations. This drop in accuracy highlights the limitations of current models, suggesting that they may rely on memorization rather than genuine understanding of the underlying mathematical principles. Our analysis also revealed that the domain-specific nature of LLMs' reasoning abilities raises concerns about the reliability of current benchmarks, which often fail to capture the full scope of model capabilities. The RV-Bench leaderboard provides a transparent ranking of models based on their genuine mathematical reasoning abilities, serving as a valuable resource for future research and development."}]}