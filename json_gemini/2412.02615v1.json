{"title": "Projection Abstractions in Planning Under the Lenses of Abstractions for MDPs", "authors": ["Giuseppe Canonaco", "Alberto Pozanco", "Daniel Borrajo"], "abstract": "The concept of abstraction has been independently developed both in the context of AI Planning and discounted Markov Decision Processes (MDPs). However, the way abstractions are built and used in the context of Planning and MDPs is different even though lots of commonalities can be highlighted. To this day there is no work trying to relate and unify the two fields on the matter of abstractions unraveling all the different assumptions and their effect on the way they can be used. Therefore, in this paper we aim to do so by looking at projection abstractions in Planning through the lenses of discounted MDPs. Starting from a projection abstraction built according to Classical or Probabilistic Planning techniques, we will show how the same abstraction can be obtained under the abstraction frameworks available for discounted MDPs. Along the way, we will focus on computational as well as representational advantages and disadvantages of both worlds pointing out new research directions that are of interest for both fields.", "sections": [{"title": "Introduction", "content": "Abstractions are tools that help agents to simplify their reasoning when solving problems. In the context of AI Planning this translates into automatically building a new, simpler problem that can be solved fast. This solution is used to guide and can possibly accelerate the search for a solution in the original task, representing the true goal of the agent. The most common Planning abstractions are defined through aggregation functions that disregard a subset of the state-space components, i.e., projections (Edelkamp 2002; Helmert et al. 2007). Furthermore, these kinds of abstractions usually assume that the planning domain model does not contain actions with conditional effects. If that is not the case, the planning domain model can be re-compiled in order to satisfy this assumption. On the one hand, this implies that abstractions can be computed directly from the implicit definition of the planning task without the need of explicitly representing the transition graph, consequently providing a significant computational advantage. On the other hand, it constraints us to problems that are not heavy on conditional effects, otherwise the number of actions would scale with the state space size and a (P)PDDL (Ghallab et al. 1998; Younes and Littman 2004) representation of the task may not be feasible in practice (e.g., Atari 2600 Games (Bellemare et al. 2013)).\nAbstractions are not confined only to the Planning field; they have, indeed, concurrently emerged in the context of discounted Markov Decision Processes (MDPs) (Puterman 2014). A discounted MDP is the most commonly used model in the context of Reinforcement Learning (RL) (Sutton and Barto 2018), a field that tackles sequential decision making problems analogous to the ones addressed by AI Planning. Here, abstractions have the same overarching objective as in Planning, i.e., reducing the state space size of the original problem to transform the search for a solution into an easier task. However, techniques in this field commonly work on the aggregated state space producing a solution to be used directly on the original problem. This is in contrast w.r.t. Planning abstractions that, instead, usually guide the search for a solution through the computation of a possibly admissible heuristic function. Furthermore, stochastic processes over the aggregated state space are Partially Observable MDPs (POMDPs) (Bai, Srivastava, and Russell 2016) and a Markovian approximation of a non-Markovian process may result in poor performance. There are three abstraction frameworks within discounted MDPs: Weighting Function Abstractions (WFAs) (Li, Walsh, and Littman 2006); Abstract Robust MDPS (ARMDPs) (Petrik and Subramanian 2014); and Abstract Bounded Parameter MDPS (ABPMDPs) (Givan, Leach, and Dean 2000). These techniques make no assumption on the aggregation function, but build abstractions via leveraging the transition and reward functions of the original task. This implies the utmost flexibility in defining how states of the original problem get aggregated into abstract states, but it is computationally intensive. Finally, the abstraction frameworks for discounted MDPs do not make any assumption about the conditional effects of actions. This, as already mentioned, comes at the cost of increased computational complexity, even though it allows more expressive power.\nConsidering the above discussion, there are many commonalities and differences for the same concept between the two fields. Therefore, in this paper, we will relate and unify projection abstractions in Planning (Classical and Probabilistic) with abstractions as they are conceived for discounted MDPs. Given a Planning abstraction, we will show"}, {"title": "Background", "content": "A common way tackle sequential decision making problems is through Planning techniques. A STRIPS (Fikes and Nilsson 1971) Planning task is defined as a tuple $P = (F, A, I, G, C)$, where F is the set of propositions, A is the set of actions, $I \\subseteq F$ is the initial state, $G \\subseteq F$ represents the goal to be achieved, and $C : A \\rightarrow \\mathbb{N}^+$ is the function associating a cost to each action. Every action $a \\in A$ is defined by a set of preconditions PRE(a) and effects EFFa(e) with $e \\sim P_a = (p_{a,1},..., p_{a,n_a})$ a categorical distribution over the $n_a$ possible effects associated to action a. If $n_a = 1$ for each a then we are in the Classical Planning setting and we drop the dependency on e referring to the effects as EFF(a). The effects of an action are split into $DEL_a(e)$ and $ADD_a(e)$, and, in a set-based representation, given a state $s \\in S$ the next state $s' = (s \\setminus DEL_a(e)) \\cup ADD_a(e)$. A binary vector representation of the state can be constructed as well using the propositions in F as components of a state $s \\in S$ (overloading the notation for simplicity's sake, it will be clear when referring to one or the other from the context). In this case, we will represent the effects of an action a as $EFF_a(e) = f_a(e)$ in the case of Probabilistic Planning and EFF(a) = f(a) in the case of Classical Planning. In both formulations an action is applicable in state s iff PRE(a) is satisfied in s. In the binary vector representation, applying action a in state s, we obtain $s' = s + EFF_a(e)$, where add effects are intended as adding 1 to the respective component of s and delete effects subtract 1 instead. A policy $\\pi$ is a decision rule telling us what action to execute given the state we are in (it may be stochastic or deterministic). It constitutes a solution to the planning problem P if, starting from the initial state, it reaches the goal with probability 1 as the interactions tend to infinity. The policy is optimal if its expected cost is minimal. If P is deterministic, then the concept of plan suffices. A plan is defined as a sequence of actions $\\omega = (a_1,...,a_n)$ and it is applicable in state $s_0$ if there exists a sequence of states $(s_1,..., s_n)$ such that at is applicable in $s_{t-1}$ and $s'_t = (s \\setminus DEL(a)) \\cup ADD(a)$ (or $s_t = s_{t-1} + f(a_t)$ in a binary vector representation). A plan is a solution to P if it is applicable in I and G is satisfied in $s_n$. Furthermore, it is optimal if its cost, defined as $C(\\omega) = \\sum_{t=1}^{n} C'(a_t)$, is minimal among all alternative plans costs.\nAnother formalism to model sequential decision making problems is represented by discounted MDPs. A discounted MDP is defined by a tuple $M = (S, A, T,R, \\gamma, \\rho)$, where S is the state space, A is the action space, $T(s'|s, a)$ is the Markovian state transition function (a probability distribution over the next state given that we applied action a in state s), R is the reward function, $\\gamma \\in [0, 1)$ is the discount factor, and $\\rho$ is the initial state distribution. An optimal solution, in this setting, is represented by a policy $\\pi$ maximizing the expected cumulative discounted reward $E_{\\tau \\sim p_{\\pi}}[R(\\tau)]$, where $R(\\tau) = \\sum_{t=0}^{\\infty} \\gamma^t R(s_t, a_t)$, $\\tau = (s_0, a_0, s_1, ...)$ is a trajectory, and $p_{\\pi}$ is the distribution over trajectories induced by the policy $\\pi$, the transition function T, and the initial state distribution $\\rho$.\nIn both the above described models, state-space abstractions may be formalized via an aggregation function $a : S \\rightarrow \\hat{S}$, where S is the state space of the problem at hand, $\\hat{S}$ is the abstract state space, and $|\\hat{S}| < |S|$. The reduction in the state space size is what is supposed to give the computational advantage."}, {"title": "Abstractions in Planning", "content": "Although there exist more general frameworks to build planning abstractions such as Cartesian abstractions (Seipp and Helmert 2018) or Merge-and-shrink (Helmert et al. 2007), in this work we will focus on projections, which form the basis for Pattern Database (PDB) heuristics (Culberson and Schaeffer 1998; Edelkamp 2002).\nDefinition 1 (Projection Abstractions). Let $P = (F, A, I, G, C)$ be a planning task. Let $F' \\subset F$ be a pattern. A projection $F'$ of the task P generates an abstract task $\\hat{P} = (F', \\hat{A}, \\hat{I}, \\hat{G})$ where:\n$ \\hat{I} = I \\cap F'$\n$\\hat{G} = G \\cap F'$\n$\\hat{A} = \\cup_{a \\in A} (PRE(a) \\cap F', EFF(a) \\cap F')$ \nIn the case of a binary vector representation, the above intersection operation that gives us the abstraction can be substituted with a projection a(s) = s', where s' is produced by keeping only the components within the pattern F'.\nPDBs are obtained by abstracting all but a part of the problem (the pattern), yielding a problem that is small enough to be solved optimally for every state. The results are computed offline and stored in a table called the pattern database. During search, the heuristic estimate of the given state is computed by accessing the value of the abstract state in the pattern database. Heuristics estimates from different abstractions can be combined by taking their maximum, or, under certain conditions, their sum (Haslum et al. 2007)."}, {"title": "Abstractions in MDPS", "content": "We will now review the main abstraction frameworks for discounted MDPS: WFAS, ARMDPs, and ABPMDPS.\nGiven an aggregation function $a : S \\rightarrow \\hat{S}$ that arbitrarily aggregates states of the original problem into abstract states, a set-valued function $a^{-1} : \\hat{S} \\rightarrow 2^S$ defining the abstract class for any abstract state $\\hat{s} \\in \\hat{S}$ as $a^{-1}(\\hat{s}) = \\{s \\in S : a(s) = \\hat{s}\\}$, and a probability distribution $w_{\\hat{s}} : a^{-1}(\\hat{s}) \\rightarrow [0, 1]$ for any abstract state $\\hat{s}$, then:\nDefinition 2 (Weighting Function Abstractions). Given an MDP $M = (S,A,T,R,\\gamma,\\rho)$, a WFA is a new MDP $M_\\omega = (\\hat{S}, A, T_\\omega, R_\\omega, \\gamma)$ such that the transitions and the reward functions are defined in the following way w.r.t. M:\n$T_\\omega(\\hat{s}'|\\hat{s}, a) = \\sum_{s \\in a^{-1}(\\hat{s})} w_{\\hat{s}}(s) \\sum_{s' \\in a^{-1}(\\hat{s}')} T(s'|s, a)$\n$R_\\omega(\\hat{s}, a) = \\sum_{s \\in a^{-1}(\\hat{s})} w_{\\hat{s}}(s) R(s, a)$\nA WFA performs a weighted average of all the transition probabilities (or the rewards, respectively) associated to the states in the abstract class represented by $\\hat{s}$. The weighting function is the same for all the actions applicable in $\\hat{s}$. For what concerns ARMDPs, we have to define a richer weighting function $\\xi_{s,a}: a^{-1}(\\hat{s}) \\rightarrow [0, 1]$, then:\nDefinition 3 (Abstract Robust MDPs). Given an MDP $M = (S, A, T, R, \\gamma, \\rho)$, an ARMDP is a new MDP $M_\\xi = (\\hat{S}, A, T_\\xi, R_\\xi, \\gamma)$ such that the transition and reward functions are defined as:\n$T_\\xi(\\hat{s}'|\\hat{s}, a) = \\sum_{s \\in a^{-1}(\\hat{s})} \\xi_{s,a}(s) \\sum_{s' \\in a^{-1}(\\hat{s}')} T(s'|s, a)$\n$R_\\xi(\\hat{s}, a) = \\sum_{s \\in a^{-1}(\\hat{s})} \\xi_{s,a}(s) R(s, a)$\nThanks to a more expressive weighting function that now will be different for any action applicable in an abstract state, ARMDPs can provide better approximations of the original MDP. Finally:\nDefinition 4 (Abstract Bounded Parameter MDPs). An ABPMDP is a family of MDPs $M_\\iota = (S, A, T_\\iota, R_\\iota, \\gamma)$ such that the transition and reward functions are defined as:\n$T_\\iota(\\hat{s}'|\\hat{s}, a) = [\\min_{s \\in a^{-1}(\\hat{s})} \\sum_{s' \\in a^{-1}(\\hat{s}')} T(s'|s, a), \\max_{s \\in a^{-1}(\\hat{s})} \\sum_{s' \\in a^{-1}(\\hat{s}')} T(s'|s, a)]$\n$R_\\iota(\\hat{s}, a) = [\\min_{s \\in a^{-1}(\\hat{s})} R(s, a), \\max_{s \\in a^{-1}(\\hat{s})} R(s, a)]$\nThis last abstraction framework provides a family of possible abstract MDPs selecting transition and reward functions within the specified intervals. Solving this family of abstractions means searching for the optimal policies in the least and most favorable MDP belonging to the family. Finally, as shown by Congeduti and Frans 2022, WFAs $\\subset$ ARMDPS $\\subset$ ABPMDPS."}, {"title": "The Relation Between Abstractions in Classical Planning and MDPS", "content": "After introducing the necessary background, we can turn our attention to the relationship between abstractions in Classical Planning and MDPs. Starting from two equivalent problems, one represented as a Classical Planning task and another one as an MDP, and given an abstraction over the planning task, we will discuss how to obtain the equivalent abstraction, if feasible, in the context of the MDP formalism\nProposition 1 (Absence of ambiguity in the abstraction transition graph). $\\forall s_1, s_2 \\in a^{-1}(\\hat{s})$ such that a is applicable in $s_1$ and $s_2$, then $a(s'|s_1,a) = a(s'|s_2,a) = \\hat{s}'$.\nProof. Since $s_1, s_2 \\in a^{-1}(\\hat{s})$, then a($s_1$) = a($s_2$) = $\\hat{s}$. Additionally, $s'_{s_1,a} = s_1 + f(a)$ and $s'_{s_2,a} = s_2 + f(a)$. This implies:\n$a(s'_{s_1,a}) = a(s_1) + a(f(a)) = \\hat{s} + a(f(a))$\n$= a(s_2) + a(f(a)) = a(s'_{s_2,a}) = \\hat{s}'$.\nThe above property avoids non-deterministic transitions in the abstract transition graph, where, starting from a state $\\hat{s}$ and applying an action a, we may move both to $\\hat{s}'$ and $\\hat{s}''$.\nIn order to obtain an abstraction equivalent to the one in the Classical Planning context, we need to require the abstraction to be connection preserving and deterministic. Connection preserving intuitively means that if there is a"}, {"title": "Connection Preserving", "content": "Definition 5 (Connection Preserving). Given and MDP M, and $C_{\\hat{s}} = \\{s : s \\in a^{-1}(\\hat{s}), \\sum_{s' \\in a^{-1}(\\hat{s}')} T(s'|s, a) > 0\\}$, an abstraction is connection preserving whenever $C_{\\hat{s}} \\neq \\emptyset \\leftrightarrow \\hat{T}(\\hat{s}'|\\hat{s}, a) > 0$ and $C_{\\hat{s}} = \\emptyset \\Leftrightarrow \\hat{T}(\\hat{s}'|\\hat{s}, a) = 0$.\nThe above property is of interest for abstractions in general, because it produces an abstraction that faithfully represents connections between abstract classes of the underlying MDP.\nDefinition 6 (Deterministic Abstraction). An abstraction is deterministic if and only if:\n$\\hat{T}(\\hat{s}'|\\hat{s}, a) = \\begin{cases} 1 \\\\ 0 \\end{cases}$\nAll the three different frameworks to construct abstractions for MDPs are connection preserving under some constraints over $\\omega$, $\\xi$, or $\\iota$.\nProposition 2 (Connection Preserving WFAs). WFAs are connection preserving whenever $\\forall \\hat{s}, \\hat{s}', a$ such that $C_{\\hat{s}} \\neq \\emptyset$ there exists $s \\in C_{\\hat{s}}$, for which $w_{\\hat{s}}(s) > 0$.\nProof. $\\Rightarrow$ Assumption: $C_{\\hat{s}} \\neq \\emptyset$. Furthermore, by assumption of the proposition, it exists $s \\in C_{\\hat{s}}$ for which $w_{\\hat{s}}(s) > 0$. For this $s \\in C_{\\hat{s}}$, it also holds that $\\sum_{s' \\in a^{-1}(\\hat{s}')} T(s'|s, a) > 0$ by definition of $C_{\\hat{s}}$, hence $\\hat{T}(\\hat{s}'|\\hat{s}, a) > 0$.\n$\\Leftarrow$ Assumption: $\\hat{T}(\\hat{s}'|\\hat{s}, a) > 0$. Hence, $\\sum_{s \\in a^{-1}(\\hat{s})} w_{\\hat{s}}(s) \\sum_{s' \\in a^{-1}(\\hat{s}')} T(s'|s, a) > 0$ that implies it exists $s \\in a^{-1}(\\hat{s})$ such that $w_{\\hat{s}}(s) > 0 \\wedge \\sum_{s' \\in a^{-1}(\\hat{s}')} T(s'|s, a) > 0$. This means that $C_{\\hat{s}} \\neq \\emptyset$.\n$\\Rightarrow$ Assumption: $C_{\\hat{s}} = (\\emptyset)$. Therefore, by definition of $C_{\\hat{s}}, \\forall s \\in a^{-1}(\\hat{s})$ then $\\sum_{s' \\in a^{-1}(\\hat{s}')} T(s'|s, a) = 0$.\n$\\Leftarrow$ Assumption: $\\hat{T}(\\hat{s}'|\\hat{s}, a) = 0$. This implies that $\\forall s \\in a^{-1}(\\hat{s})$ either $\\sum_{s' \\in a^{-1}(\\hat{s}')} T(s'|s, a) = 0$ or if $\\sum_{s' \\in a^{-1}(\\hat{s}')} T(s'|s, a) > 0$ then $w_{\\hat{s}}(s) = 0$. The second case cannot happen because it implies that $C_{\\hat{s}} \\neq \\emptyset$, hence it had to exist $s \\in C_{\\hat{s}}$ such that $w_{\\hat{s}}(s) > 0$ meaning that $\\hat{T}(\\hat{s}'|\\hat{s}, a) > 0$, contradicting the assumption we started from. Therefore, $C_{\\hat{s}} = \\emptyset$.\nProposition 3 (Connection Preserving ARMDPs). ARMDPs are connection preserving whenever $\\forall \\hat{s}, \\hat{s}', a$ such that $C_{\\hat{s}} \\neq \\emptyset$ then exists $s \\in C_{\\hat{s}}$, for which $\\xi_{\\hat{s},a}(s) > 0$.\nProof. The proof is analogous to the one above substituting $\\omega$ for $\\xi$.\nProposition 4 (Connection Preserving ABPMDPs). ABPMDPS are connection preserving whenever $\\forall \\hat{s}, \\hat{s}', a$ such that $C_{\\hat{s}} \\neq \\emptyset$ then if $\\min_{s \\in a^{-1}(\\hat{s})} \\sum_{s' \\in a^{-1}(\\hat{s}')} T(s'|s, a) = 0$, it must be excluded from the interval.\nProof. $\\Rightarrow$ Assumption: $C_{\\hat{s}} \\neq \\emptyset$. Then, by definition of $C_{\\hat{s}}$, it exists $s \\in a^{-1}(\\hat{s})$ such that $\\sum_{s' \\in a^{-1}(\\hat{s}')} T(s'|s, a) > 0$, implying $\\max_{s \\in a^{-1}(\\hat{s})} \\sum_{s' \\in a^{-1}(\\hat{s}')} T(s'|s, a) > 0$. By the assumption of the proposition 0 will be excluded as a lower bound of the interval, hence $\\hat{T}(\\hat{s}'|\\hat{s}, a) > 0$.\n$\\Leftarrow$ Assumption $\\hat{T}(\\hat{s}'|\\hat{s}, a) > 0$. This only happens if $\\max_{s \\in a^{-1}(\\hat{s})} \\sum_{s' \\in a^{-1}(\\hat{s}')} T(s'|s, a) > 0$, that, in turn, implies $C_{\\hat{s}} \\neq \\emptyset$.\n$\\Rightarrow$ Assumption: $C_{\\hat{s}} = (\\emptyset)$. Then it does not exists $s \\in a^{-1}(\\hat{s})$ such that $\\sum_{s' \\in a^{-1}(\\hat{s}')} T(s'|s, a) > 0$. This further implies that $\\hat{T}(\\hat{s}'|\\hat{s}, a) = [0, 0] = 0$.\n$\\Leftarrow$ Assumption $\\hat{T}(\\hat{s}'|\\hat{s}, a) = 0$. If the interval would be of the following form: $\\hat{T}(\\hat{s}'|\\hat{s}, a) = [0, c]$, with $c > 0$ we would discard zero according to the assumption of the proposition. The last case that remains is $\\hat{T}(\\hat{s}'|\\hat{s}, a) = [0, 0] = 0$ that implies it does not exists $s \\in a^{-1}(\\hat{s})$ such that $\\sum_{s' \\in a^{-1}(\\hat{s}')} T(s'|s, a) > 0$, further implying $C_{\\hat{s}} = \\emptyset$."}, {"title": "Weighting Function Abstractions", "content": "For what concerns WFAs, other than requiring the condition stated in Proposition 2, we need to require the obtained abstraction to be deterministic, see Definition 6. Since there may be actions in any state of the original MDP that are not applicable (i.e., the transition function has zero probability for all next states), then for the abstraction to be deterministic we have to further require that:\n$\\sum_{s \\in C_{\\hat{s}}} w_{\\hat{s}}(s) = 1 \\forall a, \\hat{s}, \\hat{s}' with \\hat{s} \\neq \\hat{s}'$.\nThe condition $\\hat{s} \\neq \\hat{s}'$ is added to remove self loops in the abstraction that are useless. In the context of Planning, we can remove all the actions whose effects are entirely annihilated by the aggregation function.\nUnfortunately, there are cases were the condition reported in Eq. (1) cannot be satisfied. Let us imagine an abstract"}, {"title": "Abstract Robust Markov Decision Processes", "content": "In the previous section, we have seen how WFAs, in some cases, may not be able to preserve connectivity w.r.t. the original MDP. This was due to the fact that we were forced to use the same weighting function $w_{\\hat{s}}$ for multiple, different actions. In the context of ARMDPs, this is no longer a problem. Indeed, the requirement in Eq. (1) becomes:\n$\\sum_{s \\in C_{\\hat{s}}} \\xi_{\\hat{s},a}(s) = 1\\forall a, \\hat{s}, \\hat{s}' with \\hat{s} \\neq \\hat{s}'.\nHaving one weighting function per action allows us to avoid over-restrictive requirements on the weighting function itself as it was happening for WFAs. Now, let us check that the condition reported in Eq. (2) allows us to represent the planning abstraction. If $C_{\\hat{s}} = 0$ then $\\hat{T}(\\hat{s}'|\\hat{s}, a) = 0$ because $\\forall s \\in a^{-1}(\\hat{s})$ then $s' \\in a^{-1}(\\hat{s}') \\hat{T}(s'|s, a) = 0$. Otherwise, if $C_{\\hat{s}} \\neq \\emptyset$ then $\\sum_{\\hat{s}' \\in a^{-1}(\\hat{s}')} T(s'|s, a) = 1 \\forall s \\in C_{\\hat{s}}$ due to the fact that the original MDP is deterministic. Now, we can conclude that:\n$\\hat{T}(\\hat{s}'|\\hat{s}, a) = \\sum_{s \\in a^{-1}(\\hat{s})} \\xi_{\\hat{s},a}(s) \\hat{T}(s'|s, a) = $\n$\\sum_{s \\in a^{-1}(\\hat{s})} \\xi_{\\hat{s},a}(s) \\sum_{s' \\in a^{-1}(\\hat{s}')} T(s'|s, a) = $\n$\\sum_{s \\in C_{\\hat{s}}} \\xi_{\\hat{s},a}(s) = 1$\nis deterministic and connection preserving, hence equivalent to the planning abstraction. Finally, for what concerns the reward function, given an abstract class $\\hat{s}$ and an action a such that $C^{\\text{SG}}_{\\hat{s}} = \\{s : s \\in a^{-1}(\\hat{s}) \\wedge \\sum_{\\hat{s}' \\in a^{-1}(\\hat{s}')\\cap SG} \\hat{T}(s'|s, a) > 0\\} \\neq \\emptyset$, we require:\n$\\sum_{s \\in C^{\\text{SG}}_{\\hat{s}}} \\xi_{\\hat{s},a}(s) = 1 with \\hat{s} \\neq \\hat{s}'$.\nThe above equation still satisfies Eq. (2) and implies a unitary reward when reaching an abstract goal. This further requirement is necessary only for those abstract classes that have states connected to some goals. If the aggregation function a does not affect the goal, and if applying an action a in a state $s_1 \\in a^{-1}(\\hat{s})$ the agent lands on $s_1 \\in SG$, then all the states $s' \\in a^{-1}(a(s_1))$ belong to SG. This implies that $a^{-1}(a(s_1)) \\cap SG = a^{-1}(a(s_1))$ and the additional condition stated in Eq. 5 is superfluous."}, {"title": "Abstract Bounded Parameter Markov Decision Processes", "content": "Since the family of abstractions represented by ABPMDPs contains ARMPDs, we can obtain an abstraction that is equivalent to the planning one. In order to do so, it is sufficient to select:\n$\\hat{T}_\\iota(\\hat{s}'|\\hat{s}, a) = \\max_{s \\in a^{-1}(\\hat{s})} \\sum_{s' \\in a^{-1}(\\hat{s}')} T(s'|s, a)$\n$\\hat{R}_\\iota(\\hat{s}, a) = \\max_{s \\in a^{-1}(\\hat{s})} R(s, a)$.\nThe above definition preserves connectivity. Indeed, it satisfies the condition required by Proposition 4. Furthermore, due to the fact that the underlying MDP is deterministic, the abstraction keeps on being deterministic choosing the maximum for the abstract transition function. The reward function $\\hat{R}(\\hat{s}, a)$ will be 1 if there exists an $s \\in a^{-1}(\\hat{s})$ such that $s$ through the action a lands on a goal. We can conclude that projection abstractions in Classical Planning are positioned according to Figure 4 w.r.t. abstractions in MDPs."}, {"title": "From Classical Planning to MDPs: an Example", "content": "In order to make all the above concepts more concrete, in this section, we will consider an instance of a simplified Logistics domain (Torralba and Croitoru 2018) in Classical Planning, fix an abstraction of it, and then obtain the same abstraction through ARMDPs. The considered simplified Logistics problem instance has 2 locations, namely L and R, two trucks, namely A and B, and one package P. The package starts at location L and needs to reach location R.\nA representation of the transition graph for the above described problem and its abstraction is shown in Figure 5. The actions in PDDL are reported in Listing 1. Their abstractions can be obtained by removing all the"}, {"title": "The Relation Between Abstractions in Probabilistic Planning and MDPs", "content": "Recently", "form": "s' = s + f_a(e)$", "implies": "n$a(s'_{s_1", "that": "n$\\sum_{s' \\in a^{-1}(\\hat{s}')} \\hat{T}(s'| s_1, a) = \\sum_{s' \\in a^{-1}(\\hat{s}')} \\hat{T}(s'| s_2, a)$\n$\\forall s_1, s_2 \\in a^{-1}(\\hat{s}),$\nwhere a is applicable. This implies that among all the states s belonging to a certain abstract class $a^{-1}(\\hat{s})$, where action a is applicable, it does not matter what state we chose as representative of the abstract transition function for the abstract state $\\hat{s}$ and action a. Indeed through the lenses of the aggregation function they will all transition to the same abstract next states with the same probabilities. Even though Equation 6 was proved first by Kl\u00f6\u00dfner et al. 2021, they do not explicitly link this fact to the absence of conditional effects and linearity of the aggregation function (used in Proposition 5).\nUnder the above described"}]}