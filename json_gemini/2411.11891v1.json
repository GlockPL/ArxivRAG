{"title": "Survey on Semantic Interpretation of Tabular Data: Challenges and Directions", "authors": ["Marco Cremaschi", "Blerina Spahiu", "Matteo Palmonari", "Ernesto Jimenez-Ruiz"], "abstract": "Tabular data plays a pivotal role in various fields, making it a popular format for data manipulation and exchange, particularly on the web. The interpretation, extraction, and processing of tabular information are invaluable for knowledge-intensive applications. Notably, significant efforts have been invested in annotating tabular data with ontologies and entities from background knowledge graphs, a process known as Semantic Table Interpretation (STI). STI automation aids in building knowledge graphs, enriching data, and enhancing web-based question answering. This survey aims to provide a comprehensive overview of the STI landscape. It starts by categorizing approaches using a taxonomy of 31 attributes, allowing for comparisons and evaluations. It also examines available tools, assessing them based on 12 criteria. Furthermore, the survey offers an in-depth analysis of the Gold Standards used for evaluating STI approaches. Finally, it provides practical guidance to help end-users choose the most suitable approach for their specific tasks while also discussing unresolved issues and suggesting potential future research directions.", "sections": [{"title": "1 Introduction", "content": "Tables are widely used and play a crucial role in creating, organising, and sharing information. A notable example of their significance as ways to organise human knowledge can be found in the oldest sample of writing on paper (on papyrus), dating back to around 2500 BC, in which Merer, an Egyptian naval inspector, documents his daily activities in a table (Fig. 1) [156]."}, {"title": "1.1 STI: key definitions, impact and challenges", "content": "In its most agreed and complete formalisation, the STI process considers two inputs: i) a relational table, which is usually assumed to be well-formed and normalised (i.e., the table has a grid structure, where the first row may contain the table headers and any other row contains values), as in Fig. 2; and ii) a reference Knowledge Graph (KG) with its vocabulary (i.e., a set of symbols denoting concepts, datatypes, properties, instances - also referred to as entities in the following) as in Fig. 3). The output of the STI process is a semantically annotated table, i.e., a table where its elements, typically values, columns, and column pairs, are annotated with symbols from the KG vocabulary. The exact specification of the annotations expected as the output of the STI process may differ in the proposed approaches. Here we discuss a canonical definition of a semantically annotated table to provide a first understanding of key STI tasks, inspired by the SemTab Challenge, where the annotation process has been better formalised with a community-driven effort.\nTo discuss this canonical definition, we use the example reported in Fig. 4.\nGiven:\n\u2022 a relational table T (Fig. 2);\n\u2022 a Knowledge Graph and its vocabulary (Fig. 3).\nTis annotated when:\n\u2022 each column is associated with one or more types from the KG [Column-Type Annotation (CTA)]; e.g., the column Name in the Fig. 2 is annotated with the type Mountain; the column Height is annotated with datatype xsd:integer;\n\u2022 each cell in \"entity columns\" is annotated with an entity identifier or with NIL, if no entity in the KG corresponds to the cell value [Cell-Entity Annotation (CEA)]; e.g., the cell Le Mout Blanc is annotated with Mont-Blanc; the cell Hoht\u00e4lli is annotated with NIL since it has not yet been included in the KG;\nMachine interpretation of tabular data is challenging because of the limited context available to resolve semantic ambiguities, the layout of tables that can be difficult to handle, and the incompleteness of KGs in general.\nKey challenges involved in the annotation process include:\n\u2022 Dealing with the heterogeneity of domains and data distributions: the tables may cover information that refers to very different domains (e.g., Geography vs Sports); the specificity of the table content may vary significantly (from a table with basic information about most famous mountains, like Fig. 2 to table that contains the composition of the rocks of this mountains5).\n\u2022 Dealing with limited contextual information: if compared with similar interpretation and disambiguation tasks on the textual document, the presence of contextual clues to support the interpretation and annotation of table elements may be limited and very diverse depending on the data sources; for example, table headers are often missing. Tables in open data portals may be described by metadata, while tables published on web pages may have some surrounding text.\n\u2022 Detecting the type of columns: in a table, there can be columns that contain references to named entities (NE-columns) and columns that contain strings, numbers, dates, and, in general, instances"}, {"title": "1.2 Specific contributions and structure of the manuscript", "content": "In the presentation of this comprehensive survey on STI we make the following more specific contributions, which highlights the main differences with previous surveys published on the same topic (see also Section 2.1 for a more detailed comparison):\n\u2022 A new taxonomy to organise and compare reviewed approaches comprising 31 specific attributes;\n\u2022 A new systematic literature review on 88 STI approaches published until October 2024, including latest approaches based on LLMs;\n\u2022 Analysis of existing tools that support STI and a comparison between their functionality features;\n\u2022 Analysis of the Gold Standard (GS) used to evaluate STI approaches;\n\u2022 A guide that can help researchers and practitioners locate STI approaches most suited to their tasks;\n\u2022 Highlight and discuss open issues and future research directions."}, {"title": "2 Scope and Methodology", "content": "This Section highlights the differences between this survey and other similar surveys in the STI field. The second part describes the methodology we applied to our systematic literature review, based on the well-established PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) method. Details on the systematic review results serve as a basis for the comprehensive analysis in the following Sections."}, {"title": "2.1 Differences from other STI surveys", "content": "STI approaches have been analysed in a few surveys [88, 177, 21, 106]; [88, 177, 21] have been published before the explosion in volume of STI related-works, also as a consequence of the SemTab challenge. Most recently, Liu et al. [106] aims to complement these surveys by providing a new classification of STI approaches reflecting the heterogeneity of tabular data and the resulting new challenges. We aim to update and extend previous surveys by introducing a new classification schema of STI approaches and discussing new research directions in improving such systems. Nevertheless, our analysis encompasses not only recent works but also older ones, allowing us to derive comprehensive guidelines (Section 10) for selecting approaches based on specific user needs. Furthermore, we can identify and highlight the unresolved issues which are yet to be addressed.\nTo provide clarity, we highlight the following differences with the previously published surveys:\n\u2022 Survey scope: through a rigorous snowballing approach, we collected a comprehensive list of STI approaches that allowed us to discover 88 works. Moreover, our survey includes works of a wider timeline (2007-2023);\n\u2022 Taxonomy: considering the comprehensive list of all the works in this field allowed us to specify and classify STI systems using different orthogonal dimensions. In this survey, we identify 31 dimensions;\n\u2022 Processes: providing a better understanding of the entire processes of STI by shedding light on each step;\n\u2022 Deeper investigation: examining a wider range of approaches enabled us to delve deeper into the field, thus, helping researchers and practitioners to better understand and inspire improved or novel approaches. Similarly to [106], we delve into a more comprehensive comparison of the evaluation process;\n\u2022 Opportunity discovery: uncovering research opportunities of the existing approaches. For instance, unlike [177] and [106], we provide a more detailed analysis of the potentials of the available STI approaches;\n\u2022 Additional sections: including other important elements, e.g., delving deep into tools and GS, this survey provides the complete landscape of the STI process."}, {"title": "2.2 Methodology", "content": "The objective of this systematic review is to provide a synthesis of the state of knowledge and suggestions for future research. The PRISMA method has been designed to provide detailed reporting guidelines for such reviews to ensure a comparable and comprehensive result. This method typically encompasses three stages: i) identification, ii) screening, and iii) selection. In this section, we provide a short overview of the methodology employed to conduct this survey. Please refer to Appendix A.1 for more details.\nIn the identification stage, to efficiently search in different databases for related works, we defined a set of 16 keywords related to semantic table interpretation. These keywords were ranked based on relevance by five researchers. We conducted searches on platforms including Scopus, Web of Science, DBLP, and Google Scholar, covering the period from 2007 to May 2023. We also employed a snowballing technique to include recent publications referencing key works.\nInstead, in the screening stage, two experts manually reviewed the identified papers, focusing on the semantic table interpretation phases of the approaches and their relevance. A categorization process was performed based on title, abstract, and keywords. Specific criteria were used, including generic and specific annotation tags, to determine relevance.\nIn the selection stage, publications included in this survey were required to be directly related to semantic table interpretation, published in English, and peer-reviewed. Using the specified keywords defined within the PRISMA method, 134 papers were initially identified, which were reduced to 111 after the screening process. Manual annotation and further screening led to the exclusion of 17 papers, resulting in a total of 88 approaches discussed in the survey."}, {"title": "3 Taxonomic analysis of STI Approaches", "content": "Introducing a taxonomy of features that characterise different STI approaches8 as to main objectives: i) defined STI more precisely by describing the tasks and subtasks, ii) allows us to comprehensively understand the various approaches and their unique contributions to the field. Fig. 6 depicts a high-level taxonomy of the features of STI approaches. The features are organised into dimensions. By analysing each dimension individually, we will present the main characteristics of the approaches proposed so far. The reader should note that many dimensions are orthogonal; thus, an approach may be classified into multiple other ones. To ensure that the information presented in the survey was verified and complete, we contacted the authors via email and received 45 responses."}, {"title": "4 Sub-tasks", "content": "The taxonomy dimension SUB-TASKS refers to the completeness of the approach concerning the sub-tasks of the STI process. Some approaches implement just a few sub-tasks, while others consider the implementation of all sub-tasks: i) Data Preparation (Section 4.1), ii) Column Classification (Section 4.2), iii) Datatype Annotation (Section 4.3), iv) Subject Detection (Section 4.4), v) Entity Linking (Section 4.5), vi) Type Annotation (Section 4.6), vii) Predicate Annotation (Section 4.7), and viii) NIL Annotation (Section 4.8)."}, {"title": "4.1 Data Preparation", "content": "Data Preparation is usually the first sub-task in an STI pipeline. This sub-task transforms the raw data into a format suitable for analysis. Data Preparation plays a crucial role in STI as it ensures that the data is appropriately structured and ready for analysis, enabling accurate interpretation and extraction of meaningful insights. It involves transforming the values within cells to a standardised format, ensuring consistency, and facilitating subsequent sub-tasks by eliminating variations in representation [56].\nTables consist of numerous cells with literal values that cannot be directly linked. Literal values encompass various types, including numeric quantities, dates, and coordinates. Multiple data preparation sub-tasks can be employed to clean and format these data types. For instance, numeric quantities within a column can be converted to a joint base unit. For example, values such as 10kg, 100g, and 34t, representing weights, can be interpreted and converted to kilograms. The date is another frequently encountered literal type, often appearing in diverse formats such as \u201c4 October 1983\u201d, \u201c4-10-1983\u201d, \u201cOct 4, 1983\", \"October 4, 1983\", \"1983/10/4\u201d, or \u201c1983.10.4.\u201d Normalising numeric and date values can be challenging. However, it can significantly improve subsequent sub-tasks in the pipeline. Moreover, table cells sometimes contain extraneous values, such as text in brackets or special characters, which can confuse entity-linking algorithms and result in poor annotations. Hence, omitting such values can enhance the reliability and accuracy of the final results.\""}, {"title": "4.2 Column Classification", "content": "Classifying table columns entails categorising each as a Named Entity (NE) or a Literal (LIT). NE-columns contain values representing entities such as names, locations, or organisations. In contrast, LIT-columns contain values representing literal data types such as numbers, dates, or geo-coordinates. Many existing approaches utilise prior datatype classification to determine the type of columns. By classifying columns, subsequent semantic analysis and data manipulation become more feasible. These approaches assign specific types (e.g., number, date, geo-coordinate) by employing: i) Regex matching [180, 55, 35, 38, 14, 13, 34, 9, 36, 120, 143, 11], ii) statistical analysis [116, 85, 65], or iii) Machine Learning (ML) techniques [178, 50]. Additionally, some approaches prioritise entity linking; in this scenario, unlinked columns are then classified as LIT-columns.\nSome approaches explicitly consider Column Classification as a distinct sub-task [116, 55, 180, 85, 35, 14, 13, 38, 34, 65, 1, 3, 2, 4, 9, 120, 30, 26, 78, 77, 75, 36, 178, 50, 11], while many others implicitly perform Column Classification by identifying cell or column datatypes [70, 71, 22, 43, 134, 138, 135, 154, 129, 143, 91, 7, 64, 50, 158, 123, 30]. TableMiner+ [180] utilises Regex patterns to identify empty, date, number, and long text columns, categorising them as LIT-columns, while the rest are considered NE-columns. Efthymiou et al. [55] applies a column sampling to classify literal columns. Later, [35, 38, 14, 13, 34, 9, 36, 120, 11] adopt a similar technique, employing additional Regex patterns and using majority voting to determine column types. In addition, MTab [120] combines the use of Regex with SpaCy14 pre-trained model to perform column classification. Kim et al. [91] consider text, number, and date as possible types while bbw [143] uses Regex to predict number, time, name and string datatypes.\nRegarding the statistical analysis, Mulwad et al. [116] developed a domain-independent and ex-tensible framework where it is possible to implement components to detect literal values; when all cells in a column are literal, the column is considered as \u201cNo-annotation\u201d. Kacprzak et al. [85] considers as numeric the columns with at least 50% of numerical values, else the columns are classified as NE. Similarly, also [65] classifies columns as either \u201ccharacter\u201d or \u201cnumeric\u201d.\nIn the ML techniques, Zhang et al. [178] proposes a column header classification model which was trained on the T2D dataset. The TURL [50] approach uses an additional type embedding vector to differentiate NE columns."}, {"title": "4.3 Datatype Annotation", "content": "In many approaches, the Datatype Annotation of LIT-columns is tightly linked to the Column Classifi-cation. Similarly to the Column Classification sub-task, approaches exploit three methods: i) statistical analysis [70, 71, 30, 119, 135, 129, 162, 85, 29, 122, 25], ii) Regex matching [138, 134, 43, 116, 135, 154, 129, 180, 35, 38, 34, 9, 36, 143, 14, 13, 120, 123, 64, 11], or iii) ML techniques [155, 175, 158, 63, 50, 178, 55, 27, 120, 123, 64]. In this sub-task it is possible to add an additional category related to approaches that use iv) other methods [94, 152, 154, 153].\nSeveral approaches employ statistical analysis by applying a set of rules to classify LIT and NE columns [70, 71]. Such rules concern how cell content is represented, i.e., the amount of text and numbers/units. Inspired by Taheriyan et al. [154], [129] adopts statistical hypotheses as a metric for column annotation. Hierarchical clustering is employed by Neumaier et al. [119] to construct a background KG using DBpedia. The nearest neighbours classification is applied to predict the most probable data type for a given set of numerical values, also considering distribution similarity. NUMBER [85] is inspired from previous works [119, 129]. The evaluation process involves two main aspects. Firstly, the similarity of value distributions is assessed by comparing them to the properties in a target KG using a KS test15. Secondly, the relative difference is computed between numerical values in a column and the numerical values of properties linked to the entities. MTab4Wikidata [122], column types are determined after the cells have already been linked to entities. LinkingPark [29, 30] uses precomputed statistics for numeric datatypes, such as range, mean, and standard deviation and considers datatypes that match the corresponding ranges as potential candidates. p-type [25] proposes a model built upon Probabilistic Finite-State Machines (PFSMs). In contrast to the standard use of Regex, PFSMs have the advantage of generating weighted posterior predictions even when a column of data is consistent with more than one type of model.\nRegex is used to check if the content of the cells can be classified, for instance, as pH, temperature, time, date, number, geo coordinates, iso8601 date, street address, hex colour, URL, image file, credit card, email address, IP address, ISBN (International Standard Book Number), boolean, id, currency, and IATA (International Air Transport Association) codes. If the number of occurrences of the most frequent RegexTypes detected exceeds a given threshold, the column will be annotated as LIT-column, and the most frequent RegexType will be assigned to the column under analysis. Then, to select the datatype to annotate the column, some approaches imply a mapping between RegexType and Datatype. [134, 116, 43, 138, 135, 154, 180, 35, 143, 38, 34, 14, 178, 9, 36, 13, 11] utilise some or the entire list of the Regex defined above to identify the datatype of the column under analysis.\nThe last method for Datatype Annotation regards ML techniques. Meimei [155] utilises embeddings by modelling a table with a Markov random field and employing multi-label classifiers to find the correct annotation, similar to InfoGather+ [175] which focuses only on numerical values. Another approach [63], models different latent structures within the data and employs a Conditional Random Field (CRF) to perform semantic annotation in different domains (e.g., weather, flight status, and geocoding). The approach ColNet [27] utilises a Convolutional Neural Network (CNN) trained on positive and negative samples to differentiate between different column types.\nOther approaches combine Regex with ML techniques; MTab[120] classifies columns as NE or LIT using Duckling Regex16 and SpaCy (a pre-trained classificator) with majority voting. Numeric columns are classified using a neural numerical embedding model (EmbNum [121]) through representation vectors for numerical attributes without prior assumptions on data distribution. In contrast [123], used only SpaCy to identify LIT-columns. Tab2KG [64] uses the Dateparser library\u00b97 for classification in numeric, spatial, boolean or text. Later the columns are further classified into more specific categories using Regex combined with the method described in [7], which implies classifying four kinds of numbers: nominal, ordinal, interval, and ratio. Then fuzzy c-means is used for classification.\nSeveral other approaches in the field of STI have been proposed, each with its unique methodology. These approaches, including works such as [157, 105, 117, 151, 118, 131, 164, 49, 57, 115, 183, 142, 19, 58, 56, 108, 176, 28, 74, 114, 98, 124, 147, 155, 158, 174, 12, 59, 89, 103, 161, 172, 69, 170, 146, 163, 181, 107, 149], do not involve semantic classification or Datatype Annotation sub-tasks for the columns. In [118], the concept of literal annotation was discussed as a potential area for future works, which was later implemented in [116]. The approach MAGIC, discussed in [146], uses entity embeddings with neighbour nodes up to a depth of 2 without explicitly distinguishing between entities and literals. However, it is plausible that this approach can also be extended to include Datatype Annotation. Specific approaches focus on the"}, {"title": "4.4 Subject Detection", "content": "As previously illustrated (Section 3), the S-column is the column among the NE-columns that all the others refer to. Some approaches define it as a \"key\" column that includes entity-based mentions that could potentially be consulted in a KG, containing a large number of unique values.\nGenerally, approaches might employ one of the following techniques for Subject Detection: i) heuris-tic approaches, ii) statistical analysis, or iii) ML techniques.\nRegarding heuristic approaches, one common method is based on the column position within the table. For example, some approaches designate the leftmost column as the S-column [55, 1, 91, 3, 2, 4, 30, 50]. [151] instead, involves identifying columns with specific labels as the S-column, such as columns labelled as \"title\" or \"label\". Another method for Subject Detection is to consider column detection after linking entities. For instance, Zhang et al. [176, 178] use the column with the highest number of linked entities as an indicator for Subject Detection. Similarly, the approach proposed by Heist et al. [69], considers subject-predicate-object relations between the table's columns and identifies the S-column as the one with the highest number of entities in the subject position.\nThe use of statistical analysis to identify the S-column is found in TableMiner+ [180]; it uses a set of rules based on the number of words, the capitalisation, and the mentions of months or days in a week. In MantisTable [35, 38, 34, 9, 36, 11] the subject is selected among the NE-columns using a calculated as a set of indicators such as the average number of words in each cell, the fraction of empty cells in the column, the fraction of cells with unique content and the distance from the first NE-column. The same indicators are also used in [14]. DAGOBAH2019 [26], TAKCO [98] and MTab2021 [123] consider the fraction of cells with unique content and the column position in the table.\nSome works in the literature [162, 58] use ML techniques. TAIPAN [58] selects SVM and Decision Tree as the best classifiers for this sub-task and uses as features the ratio of cells with disambiguated entities in a column and the number of relations between the columns. It is worth noting that [162] also employs an SVM using features dependent on the name and type of the column and the values in different column cells.\nTab2KG [64] uses a graph-based approach for subject detection, but the paper lacks details about the implementation.\nEventually, there are many approaches that do not perform S-column detection [70, 71, 157, 105, 117, 118, 63, 94, 131, 164, 22, 43, 49, 57, 116, 115, 134, 175, 183, 142, 152, 19, 135, 138, 119, 129, 154, 153, 56, 85, 108, 27, 28, 74, 114, 120, 124, 147, 155, 158, 174, 12, 29, 59, 65, 78, 89, 103, 122, 143, 161, 172, 13, 77, 146, 163, 170, 181, 107, 75, 149]."}, {"title": "4.5 Entity Linking", "content": "Entity Linking, also known as named entity linking or entity resolution, is an NLP task that involves linking named entities mentioned in the text to their corresponding entities in a KG. The goal is to identify and disambiguate the entities mentioned in the text and connect them to unique identifiers.\nIn Entity Linking, a named entity refers to a specific named person, organisation, location, event, or other well-defined entity. For example, in the sentence \"Barack Obama was born in Hawaii\", the named entity \"Barack Obama\" can be linked to the corresponding entry in a KG, such as dbr:Barack_Obama in DBpedia o Barack Obama (Q76) in Wikidata. For this sub-task, approaches can be grouped into three step within entity liking: i) mention detection [19], ii) candidate generation, and iii) entity disambiguation [105, 151, 117, 118, 164, 116, 183, 19, 138, 55, 56, 180, 26, 27, 35, 98, 114, 120, 124, 147, 158, 1, 12, 29, 38, 34, 59, 78, 91, 122, 143, 161, 178, 3, 9, 13, 77, 123, 146, 170, 4, 30, 36, 75, 50, 107, 11].\nMention detection refers to the identification and extraction of mentions from tabular data. It involves recognising specific pieces of information within a table representing entities. Detecting mentions in a table can involve various techniques, such as NLP methods, Named Entity Recognition (NER) methods, or pattern-matching algorithms. This sub-task analyses the table's content, column headers, and other contextual information to accurately identify and classify the mentions.\nThe TabEL [19] approach identifies potential mentions within a given cell that can be associated with entities in a KG. TabEL identifies the longest phrases within the cell's text content with a non-zero"}, {"title": "4.6 Type Annotation", "content": "The Type Annotation sub-task involves assigning a specific type from a reference KG to each NE-column. Various approaches have been developed to address this sub-task, focusing on leveraging Entity Linking techniques, partially or comprehensively, to identify the most frequent column type. This is particularly crucial in unsupervised classification scenarios. Additionally, some approaches consider the information provided by column headers to determine the most suitable type.\nThe most used methods to annotate NE-columns are: i) majority voting [70, 71, 117, 118, 162, 134, 183, 138, 58, 56, 180, 12, 14, 29, 59, 91, 122, 143, 161, 123, 30, 151, 116, 35, 147, 158, 38, 34, 9, 36, 1, 3, 2, 4, 98, 181, 11], ii) Term Frequency-Inverse Document Frequency (TF-IDF) [105, 131, 135, 154, 124, 26], iii) statistical methods [120, 77, 78, 69, 49, 75], iv) machine learning [27, 65, 146, 163, 170, 50, 64, 28, 74, 174, 149], or v) other [94, 175, 152, 22, 155, 13, 164, 114, 89, 178].\nMost research studies on column-type prediction utilise a common strategy called majority voting. This method involves determining the most frequently occurring type in a column and deciding based on the majority. This pure decision-making method is applied by [70, 71, 117, 151, 58, 118, 162, 134, 183, 138, 56, 180, 12, 14, 59, 29, 91, 122, 143, 161, 123, 170, 30]. Some approaches go beyond simple majority voting and incorporate additional mechanisms to address specific situations. For instance, [116, 35, 147, 158, 38, 34, 9, 36, 98, 181] set a threshold to prevent annotating a column when there is insufficient confidence about the type. The approach in [134] applies a deduplication process to remove duplicate types within a column. After deduplication, the type frequencies in the column are summed using a logarithmic function which measures the overall frequency and importance of the types present in the column. In Kruit et al. [98], majority voting across types is also used to select candidate entities for individual rows using Loopy Belief Propagation (LBP). This approach highlights how CTA and EL are interconnected. Also, for LinkingPark [29, 30], the primary method used is majority voting to obtain the most common (i.e., most frequent) type. In case multiple candidates have the same frequency, the type selected for annotation is the most specific in the KG. For bbw [143], majority voting is the primary selection algorithm. However, when multiple types have equal frequency, it selects the first common ancestor type in the KG. JenTab [1, 3, 2, 4] uses various techniques, including the Least Common Subsumer (LCS), direct parents (i.e., majority voting), and popularity. Following the ontology hierarchy, the LCS represents the most specific type, obtained by excluding types occurring in less than 50% of the cells. Zhou [181] selects the annotation based on level 2 and level 3 DBpedia classes."}, {"title": "4.7 Predicate Annotation", "content": "The Predicate Annotation sub-task can be challenging", "parts": "NE relations", "into": "i) ruleset [70, 71", "50": "iii) majority voting [105, 151, 117, 118, 162, 94, 116, 115, 22, 138, 58, 26, 35, 114, 158, 98, 147, 38, 14, 91, 122, 143, 172, 78, 13, 146, 69, 77, 30, 36, 75, 11", "4": "and v) embedding [28, 163, 149", "71": ".", "142": "consists of searching for exact matches of subject and object pairs in a large corpus to retrieve possible properties. Similarly, TURL [50", "105": "defined a feature vector based on the occurrence and frequency of relations between entities. The most frequently occurring relation is the selected one. A similar method is applied in [151, 117, 118, 162, 22, 116, 115, 26, 35"}]}