{"title": "Generating Realistic Adversarial Examples for Business Processes using Variational Autoencoders", "authors": ["Alexander Stevens", "Jari Peeperkorn", "Johannes De Smedt", "Jochen De Weerdt"], "abstract": "In predictive process monitoring, predictive models are vulnerable to adversarial attacks, where input perturbations can lead to incorrect predictions. Unlike in computer vision, where these perturbations are designed to be imperceptible to the human eye, the generation of adversarial examples in predictive process monitoring poses unique challenges. Minor changes to the activity sequences can create improbable or even impossible scenarios to occur due to underlying constraints such as regulatory rules or process constraints. To address this, we focus on generating realistic adversarial examples tailored to the business process context, in contrast to the imperceptible, pixel-level changes commonly seen in computer vision adversarial attacks. This paper introduces two novel latent space attacks, which generate adversaries by adding noise to the latent space representation of the input data, rather than directly modifying the input attributes. These latent space methods are domain-agnostic and do not rely on process-specific knowledge, as we restrict the generation of adversarial examples to the learned class-specific data distributions by directly perturbing the latent space representation of the business process executions.\nWe evaluate these two latent space methods with six other adversarial attacking methods on eleven real-life event logs and four predictive models. The first three attacking methods directly permute the activities of the historically observed business process executions. The fourth method constrains the adversarial examples to lie within the same data distribution as the original instances, by projecting the adversarial examples to the original data distribution.", "sections": [{"title": "I. INTRODUCTION", "content": "ACHINE Learning (ML) and Deep Learning (DL) methods have become central to a wide range of applications such as loan application processes [2], criminal justice [26] and, disease diagnostics [25]. Similarly, there has been a growing interest in using ML and DL techniques within Process Mining (PM), a field focused on analyzing event data generated during the execution of business processes. PM enables organizations to uncover, monitor, and improve real-world processes by extracting knowledge from modern information systems. Through data mining and machine learning techniques, PM reveals valuable insights into the processes of organizations, such as discovering process models, detecting inefficiencies, and predicting future behavior.\nPredictive Process Monitoring (PPM) is a subfield of PM that focuses on providing insights into ongoing business processes, including predicting the remaining time, the next event, or the outcome of a process. The latter field is also known as Outcome-Oriented Predictive Process Monitoring (OOPPM) and aims to predict the future state of ongoing cases. For instance, in a loan application process, all the events record specific activities-referred to as control flow attributes-leading up to the acceptance or rejection of the loan request. The primary goal of OOPPM is therefore to address critical questions, such as whether the loan application will be approved. This means that it is required to monitor and analyze a vast amount of data from complex systems, such as financial transactions or patient admissions. In this context, it is essential for predictive models to effectively handle malicious, faulty, noisy, or unpredictable inputs. Adversarial testing has emerged as a valuable approach for assessing model robustness under such challenging data conditions [3], [23], [28], [30], [32], [42], addressing trustworthiness concerns in critical decision-making contexts [3], [30].\nThis work extends research [30], [32] on the vulnerability of OOPPM methods to input perturbations. In [30], we already assessed how adversarial noise affects OOPPM predictive performance and explanation robustness across training and testing data. In this work, we introduced adversarial noise to the input data by perturbing dynamic event attributes in business process data, similar to subtle pixel changes in images that can significantly influence predictions. For example, in a loan application, modifying dynamic activity attributes like the Credit Score and Income Level affected the outcome predictions. The limitation of this work was that we avoided permuting the control flow attributes, as changes to activities and their position within a case are inherently noticeable due to the discrete nature of sequences. Even small modifications, such as substituting activities with those in nearby positions in the embedding space, such as moving Credit Check before Document Verification, can lead to unrealistic scenarios, as the sequence of events in a business process is often critical to its execution. In [32], we addressed the prior limitation of avoiding perturbations to control flow attributes with the use of manifold learning, by projecting the generated adversarial examples to the learned class-specific data distributions, which means that the adversarial examples are more likely to preserve the essential characteristics of the data. However, this method does not guarantee that the adversarial examples are indistinguishable from the original business process cases, as multiple activity changes across various event positions may have happened."}, {"title": "II. BACKGROUND", "content": "This section provides an overview of the preprocessing steps needed for OOPPM purposes and discusses key concepts from the field of adversarial machine learning and manifold learning."}, {"title": "A. Predictive Process Monitoring", "content": "An event log L consists of events grouped by cases, forming process execution traces. An event e from the event universe is represented as a tuple e = (c,a,t,d,s), where c\u2208C denotes the case ID, a \u2208 A represents the activity (i.e., the control flow attribute), and t \u2208 R indicates the timestamp. Each event also includes event-related attributes, or dynamic attributes, which can vary during the course of the case and are represented by d = (d1,d2,...,dma). Attributes that have the same value for every event with the same case identifier are called static attributes, represented by s = (s1,s2,...,Sms). A trace is a sequence of events \u03c3\u03b5 = [c1, c2, ..., ci, ..., en], with c as the case ID and i as the index in the trace, ordered by event timestamps. An event ei in case j of the event log L is denoted as ei,j = (cj, ai,j, ti,j, di,j, 8j). Each trace has a class label y(\u03c3\u03b5) \u2208 V, with y = {0,1} in the case of a binary outcome. This class labeling depends on the needs and objectives of the process owner [38].\nTo progressively learn from different stages of traces, a prefix log L is extracted from the event log L, encompassing all prefixes from the complete traces \u03c3. A trace prefix of case c with length l is defined as \u03c3\u03b5\u03b9 = [e1,2,..., ei], where 1 \u2264 \u03c3c."}, {"title": "B. Adversarial Machine Learning", "content": "In the context of machine learning, addressing vulnerabilities against adversarial attacks can be an important tool for mitigating the risks posed by adversaries who exploit model weaknesses, or test the ability to reliably perform on possible out-of-domain samples or unseen in-domain samples that were not present during training.\nThis latter perspective on robustness ensures that machine learning models can generalize effectively across a variety of different inputs.\nAdversarial Machine Learning (AML) focuses on examining the vulnerability against adversarial attacks, and their ability to withstand both adversarial perturbations and unexpected variations in data. In [39], a taxonomy of adversarial defenses and attacks is introduced to the field of AML, providing a structured framework for categorizing various vulnerabilities and defense mechanisms. However, this paper focuses exclusively on adversarial attacks. The adversarial attacks are divided into three categories: location-specific, knowledge-specific and intent-specific. Poisoning (evasion) attacks are when the training (testing) data is perturbed to mislead the ML model into incorrect predictions. In this paper, we only focus on evasion attacks. Next, adversarial attacks can be categorized into two knowledge-specific types: white-box attacks and black-box attacks.\nIn white-box attacks, the attacker has full access to the model, including its architecture, parameters, and training data. This allows them to exploit this detailed information to craft highly targeted attacks. In contrast, black-box attacks are conducted with limited access, relying only on the output or predictions of the model without insight into its internal workings. In this paper, we employ both black-box methods (e.g., the regular attack [30], which use only the output of the model to guide the attack), and white-box methods (e.g., the gradient-based attack, which leverage full access to the internal gradients, see Section III). Additionally, attacks can also be divided into targeted and non-targeted attacks. Non-targeted attacks, such as regular adversarial examples, are designed to induce misclassification without steering the prediction toward a specific class. On the other hand, targeted attacks, like manifold-based attacks [32], aim to intentionally mislead the model into predicting a specific, often predefined, class or outcome. Note that, as we are in a binary classification setting, our non-targeted regular adversarial attack will result in the same outcome as it would in a targeted fashion. In this paper, we focus exclusively on adversarial attacks on the test data (evasion attacks) to ensure consistent and comparable results. By keeping the training dataset and model unchanged throughout the experiments, we eliminate variability introduced by attacking the training process (poisoning attacks). This approach allows for a direct comparison of performance across different attack strategies and methods on the same test dataset, ensuring that observed differences arise from the nature of the attacks rather than changes in the training configuration.\nTwo types of adversarial attacks have already been introduced to OOPPM [30], the last event attack (A1) and the all events attack (A2). In [30] the A1 attack targets the last event in the trace, modifying its dynamic attributes, resulting in a trace . In this paper, however, we focus exclusively on permuting the activity values within each trace. This approach contrasts with [32], where both the activity and resource attributes can be permuted. In this paper, the final event therefore looks like this: . In [30] and [32], the A2 attack modifies the dynamic attributes of all events in the trace, resulting in , where . In this paper, we consider the A2 attack as: , where . Valid adversarial attacks must adhere to the label invariance assumption, which states that the true class label of a data point remains unchanged even if the input data have been perturbed or manipulated. In other words, a theoretically perfectly robust and reliable model (or classifier) should not alter its predicted label in response to changes (attacks) made to the input."}, {"title": "C. Manifold Learning", "content": "The manifold hypothesis [5] posits that high-dimensional data can be effectively represented on a lower-dimensional manifold, an idea central to various methods aimed at ensuring adherence to the data distribution [7], [14], [22], [27], [40], [41]. Generative models such as Variational Autoencoders (VAE) and Generative Adversarial Networks (GANs) are used in manifold learning to map low-dimensional latent vectors to high-dimensional data while adhering to an assumed prior distribution, typically Gaussian [16]. We refer to this process as manifold learning.\nUnlike a vanilla (non-variational) autoencoder that focuses on reconstructing the input x through the encoding process of an encoder e to generate a latent representation z = e(x), VAEs frame this as a probabilistic challenge. The objective of a VAE is to maximize the Evidence Lower Bound (ELBO), balancing the reconstruction loss (\u201chow accurately can we reconstruct the original data point?\u201d) with the Kullback-Leibler (KL) divergence (\u201chow closely does the learned latent variable distribution match the chosen prior distribution?\u201d). In VAE training, the prior distribution P is often the standard normal distribution, and the learned latent variable distribution Qis the distribution approximated by the VAE encoder.\nThe reconstruction loss is typically measured by the Negative Loss-Likelihood (NLL) loss:\nL(0) NLL = \u2013 \u03a3\u03a5\u03c3\u03b5\u00b7log(\u0177\u03b8,\u03c3\u03b5)\n+ (1 \u2212 yo)log(1 \u2013 \u0177\u03b8,\u03c3\u03b5)\nThe notation \u0177\u03b8,\u03c3\u03b5 is used to emphasize the dependency of the predicted probability on the model parameters 0. The KL divergence is the sum taken over all components of the distributions (i.e. the number of latent variables in the latent space, r, and r \u2264 p), where r \u2264 p):\nLKL(P||Q) = \u03a3\u03a1(\u03c3\u03b5)\u00b7 log\n\u03a1(\u03c3\u03b5)\nQ(\u03c3\u03b5)\nIn the context of OOPPM, class-specific manifolds are learned by creating manifolds specifically for the prefixes of each class label, as introduced in [32]. The Long Short- Term neural network (LSTM) VAE is thus trained using an LSTM-based architecture on these class-specific prefixes, which leverages the sequential information inherent in process data. By focusing on the prefix data for each outcome class separately, the model more effectively captures the specific temporal dynamics and uncovers the underlying structure or manifolds of high-dimensional data specific to certain classes, while discarding noise and redundancy. Within this setup, the encoder networks compute a low-dimensional latent representation of data, and the decoder network reproduces samples from the latent space. By imposing a prior over the latent distribution p(z), the variational aspect of the VAE regularizes the encoder and structures the latent space."}, {"title": "III. GENERATING ADVERSARIAL EXAMPLES FOR OUTCOME-ORIENTED PREDICTIVE PROCESS MONITORING", "content": "In this section, we detail four attack strategies-regular, projected, latent sampling, and gradient steps that together yield eight distinct adversarial attack methods. We explain how each attack strategy addresses the challenges posed by the sequential nature of process data. The regular and projected strategies operate directly within the original input space, representing input space attacks. These two strategies employ three distinct approaches for attacking individual events within the data sequence, resulting in six methods. In contrast, the latent sampling and gradient steps strategies generate adversarial examples within the latent space, categorized as latent space attacks. Together, these approaches result in eight unique adversarial attack methods."}, {"title": "A. Adversarial Example Generation", "content": "Algorithm 1 presents a general pipeline for generating adversarial examples, applicable across multiple attack strategies. This process begins with an iteration over prefixes (line 3), followed by the application of specific attack techniques (lines 4- 20), which is further described in the subsequent subsections. Next, we select the adversarial prefix that is closest to the original in latent space, using pairwise Euclidean distances to measure the proximity between the latent representation of the original trace and the latent representation of the generated adversarial prefix (lines 22-29).\nInput Space Attacks: The regular attack strategy, illustrated in Figure 3, follows the approach introduced in [30]. In this strategy, we directly attack the original trace, where the resulting adversarial example is the modified trace after applying the attack. We distinguish between three types of regular attacks: the last event attack (A1), which modifies only the last event of the trace (introduced in [30]); the all event attack (A2), which modifies every event (introduced in [30]); and the novel k event attack (A3) targets a selected subset of k events within the trace. The k event attack is similar to the Al and A2 attacks but additionally considers the positional constraints of activities within a case, making the attack more context-aware and reducing unrealistic perturbations that might otherwise break the realistic flow of the process. Specifically, this attack accounts for the fact that certain activities can only be executed (and therefore recorded as events) at specific positions within a process execution, performing perturbations only when an activity has already occurred in a similar position within the case. The A3 attack therefore only modifies activity values based on tuples of (index, activity) extracted from the event log. This allows us to randomly select k tuples, where each selected activity of the tuple being permuted if (1) the activity differs from the original and (2) the index has not already been modified, ensuring relevance to frequently occurring activities and contextual appropriateness.\nFormally, given a trace oc = [e1, C2, ..., Ci, ..., en], the A3 attack produces a modified trace:\n043 = [e43, 243, ..., eA3, ..., eA3],\nwhere for each modified event e43, we have:\nA3\ne43 (ci, a 43, ti) if ei is selected for modification\n((Ci, ai, ti) otherwise.\nIn contrast, the projected attack strategy, also shown in Figure 3, builds on the concept of on-manifold attacks introduced in [32]. In this approach, we learn class-specific encoders and decoders that approximate the data distribution for each class by training them on class-specific prefixes. Then, after attacking the original trace, we project the adversarial example onto the class-specific data distribution. This approach adheres to the label invariance assumption, as the generated adversarial examples for a particular class remain within the boundaries of the data distribution of that class. This is in contrast to the traditional attacks that might result in unrealistic adversarial examples that would never occur in practice. An important consequence of this projection is that the length of the adversarial prefix may differ from the original prefix length, i.e. projecting an adversarial example onto this manifold can result in either longer or shorter adversarial examples after decoding. This is due to our approach of masking out the padding tensor (see section IV-C). What we could have done to prevent this is, adding a constraint for preserving a similar prefix length relative to the original trace. Another option could be to enforce sparsity in the generated examples. Enforcing sparsity would mean explicitly encouraging the generated examples to differ minimally from the original trace, typically by modifying only a small number of attributes to preserve as much of the original sequence as possible. Nonetheless, our primary objective in this paper is to generate successful and realistic adversarial examples rather than to produce adversarial examples of identical lengths.\nLatent Space Attacks: Different works argue that adding noise in the latent space allows for the generation of more natural adversarial examples, compared to adding noise in the input space. Many works argue that rather than permuting unimportant background pixels (in the case of image-based adversarial attacks), adversarial examples should be generated through perturbations in the latent space [12], [27]. The first novel attacking method we propose is the latent sampling attack, displayed in Figure 4, and it is a black-box method that exploits the stochasticity introduced by the variational inference of the LSTM VAE manifold to create adversarial examples. This technique ensures that the instance stems from the same mean and variance as the original instance. Specifically, the reparameterization trick generates z = \u03bc + \u03b5\u00b7\u03c3, where z is the latent variable, u is the mean, o is the standard deviation, and e is a random variable sampled from a standard normal distribution. This approach ensures that the adversarial example is generated from the same latent space representation, making the generated adversarial examples more realistic.\nConversely, the gradient steps attack, displayed in Figure 4, is a white-box approach that utilizes the gradients of the classifier to iteratively move across the decision boundary. In each iteration, the algorithm calculates the gradients of the classifier with respect to the input trace, and it identifies the most impactful directions in the latent space to apply perturbations, creating adversarial examples that are likely to deceive the model while remaining close to the original trace in latent space. In this paper, we adapt the REVISE counterfactual generation method [14] to handle the sequential structure of business process executions. The adapted algorithm is tailored to create latent space representations based on the one-hot encoded structure of activity sequences, and the gradient-based optimization minimizes Euclidean distance in the latent space. This ensures adversarial examples with minimal perturbations by default. However, despite its effectiveness in creating adversarial examples, the gradient steps attack is primarily useful for benchmarking purposes and not for practical use in real-world settings. The key reason for this limitation lies in the white-box nature of the attack: it requires full access to the model, including the gradients of the classifier. In real-world scenarios, adversaries typically do not have this level of insight into the model, and thus cannot use such an attack strategy. Similarly to the projected attack, the length of the"}, {"title": "B. Closest Adversarial Example Selection", "content": "In VAEs, the latent space is generally modeled as a multivariate Gaussian distribution. The encoder learns the mean and variance of the distribution, allowing the latent points to be smoothly distributed within a Euclidean space, often viewed as a continuous low-dimensional manifold [4]. This design enables the encoder to map inputs to specific points in the latent space, where the Euclidean distance between points reflects the similarity between their latent representations."}, {"title": "C. Class Label Prediction of Adversarial Examples", "content": "We then use a classifier to predict the label outcome of the adversarial examples. An attack is considered successful if the predicted label of the adversarial example differs from the original label of the prefix trace."}, {"title": "IV. EXPERIMENTAL SETUP", "content": "In this section, we describe the different event logs and their specifications, the benchmark models commonly used for OOPPM purposes, and the hyperoptimization settings and implementation details of various setups."}, {"title": "A. Event Logs", "content": "This research utilizes four distinct real-life event logs available at the 4TU Centre for Research Data website\u00b9, which are frequently employed in the domain of Outcome-Oriented Process Prediction and Monitoring (OOPPM) [11], [17], [19], [38], [43]. The event logs are segmented using Linear Temporal Logic (LTL) rules as detailed in [38], establishing objectives for the processes. The segmentation is based on the labelling functions defined by the four LTL rules, resulting in four separate binary prediction tasks."}, {"title": "B. Benchmark Models", "content": "Four different predictive models, which are commonly used in OOPPM [17], [38], e.g. Logistic Regression (LR), XGBoost (XGB), Random Forest (RF), and an LSTM neural network, are used to benchmark the findings. These models serve as the most representative of the statistical, machine learning, and deep learning facilities. LR is a transparent predictive model that is commonly used for classification, and both XGB and RF are popular tree-based ensemble models due to their fast convergence and high predictive accuracy. Finally, a long short-term memory LSTM neural network is used due to its popularity in OOPPM research [18], [44]."}, {"title": "C. Implementation", "content": "In this section, we describe the implementation details and assumptions made in this work.\n1) Train-test Split: A temporal split is used to divide the event log into training and testing cases [38]. The cases are ordered by start time, with the first 80% used for training the predictive model and the remaining 20% for evaluating its performance. This split is done at the level of completed traces, ensuring all prefixes of the same trace stay in the same set (either train or test). To avoid overlap, events in the training cases that occur during the test period are discarded.\n2) Preprocessing Steps: Then, we extract prefix logs and do the necessary preprocessing steps as detailed in II. For the machine learning models, aggregation encoding [38] is used to encode the data for the machine learning algorithms. Note that aggregation encoding is unique to process data. On the other hand, the deep learning models are built to work with sequential models and therefore do not need the use of such a sequence encoding mechanism.\n3) Model Training and Optimization: There are two different types of models that need to be trained: a classifier and class-specific LSTM VAEs. The classifier is used to predict the outcome of the instances, whereas the LSTM VAEs are used to project or generate the adversarial examples.\nThe classifier is trained on the extracted prefix loss, The LSTM VAE, on the other hand, needs additional steps. First, to ensure that the LSTM VAE could learn the ending points of traces, we added an artificial End Of Sequence (EoS) token to all the (prefix) traces. During decoding, whenever the LSTM VAE produced the first EoS token, we masked the subsequent tokens with padding values. To prevent the model from learning irrelevant information from the length of the traces or the padding tokens, we masked the padding token value during backpropagation. It is important to note that this masking and the use of the EoS token were not necessarily for training and fitting the predictive models. Additionally, we provided the option to remove duplicate traces in the event logs, as duplicates are prevalent when only considering control flow. Furthermore, we allowed for the removal of ambiguous duplicates-identical traces with different labels, since such cases could mean that a trace serves as its own adversarial example. The implementation of the LSTM VAE is based on the work done in [31].\n4) Reproducibility of the Results: The implementation of the benchmark setting is provided in the GitHub repository, as we have transformed the extensive counterfactual generation benchmark named CARLA [21], for the use of adversarial example generation."}, {"title": "V. EXPERIMENTAL EVALUATION", "content": "In this section, we evaluate our work based on the answers provided to the research questions.\nThe first research question RQ1 can be answered with the use of section III, by showing how the two novel methods, i.e. the latent sampling attack and the gradient steps attack ensure that the generated adversarial examples conform to the original data distribution while adhering to real-world process constraints. Importantly, the proposed approaches circumvent the need for domain-specific knowledge by leveraging latent representations derived from generalizable process mining models.\nThe second research question RQ2 evaluates and compares the performance of eight distinct attack strategies\u2014both black-box and white-box-using metrics such as attack success rates and their implications on OOPPM methods. To address this, we calculate the success rate, as this is one of the most commonly used evaluation metrics to evaluate the vulnerability against adversarial examples [46]. The success rate on attacks is calculated by counting the percentage of adversarial examples that have a flipped prediction. A prediction is successfully flipped if the probability decreases/increases until it is lower/higher than the predefined optimal threshold for that classifier.\nFirst, the classifiers can be ranked by their average AUC scores as follows: LR with 80.24, LSTM with 82.35, RF with 83.93, and XGB with 84.54. In contradiction with the findings of [17], the DL model (LSTM) does not outperform the classical ML ensemble methods. First, we attribute the slight underperformance of the LSTM to the results for the event log BPIC2015 (2) and SEPSIS (1). Note that the results for the SEPSIS logs are volatile in general. Second, the performance of the DL is not fully exploited due to the trace cutting, as the DL models are better capable of handling long-term dependencies.\nThe attack methods, ranked from least to most successful, are as follows:\n(1) Last Event (Regular) with a success rate of 12.17%\n(2) 3 Event (Regular) with a success rate of 13.83%\n(3) Gradient Steps with a success rate of 33.17 %\n(4) All Event (Regular) with a success rate of 38.05%\n(5) Last Event (Projected) with a success rate of 45 %\n(6) All Event (Projected) with a success rate of 49.04%\n(7) 3 Event (Projected) with a success rate of 50.08 %\n(8) Latent Sampling with a success rate of 50.17%.\nThe first insight from the ranking is that the Latent Sampling method achieves the highest success rate. Additionally, the projected attack method introduced in [32] outperforms the regular attack method that was introduced in [30]. Increasing the number of iterations beyond the default 1500 could potentially further enhance success rates. Second, based on Figure 8, the LSTM seems to be the most vulnerable OOPPM model, as it consistently exhibits the highest average success rates across most attack methods. Furthermore, the Latent Sampling attack method remains the most effective across all the four OOPPM methods. By contrast, the three regular attacking methods-Last Event (Regular), All Event (Regular), and 3 Event (Regular) have the lowest success rate. Finally, Figure 7 reveals an interesting pattern: for most adversarial example generation methods (excluding Last Event (Regular) and 3 Event (Regular)), the number of successful adversarial examples increases with prefix lengths up to 30. This finding is counterintuitive, as one might expect that permutations in shorter prefixes would have a greater impact on the prediction. In Figure 8, we present the normalized frequency of successful adversarial attacks across different trace prefix lengths, averaged over the event logs, for various classifiers and attack methods. We focus on prefix length as it serves as an important factor in business process analysis, influencing the amount of information available to a predictive model at each point in a process. Longer prefixes typically provide a more comprehensive context, which can make models less sensitive to small perturbations, impacting the success rate of adversarial attacks. The results for the Last Event (Regular) attack method indicate a negative relationship between trace length and attack success rate, suggesting that classifiers become less vulnerable to adversarial attacks when the activity in the last event of longer traces is permuted, compared to shorter ones. In contrast, for the projected, latent sampling, and gradient steps attack methods, there is a clear positive relationship between prefix length and success rate, as the normalized frequency generally increases with longer prefixes. A similar trend is observed for the RF and XGB models under the All Event (Regular) attack method, but this pattern does not hold for the LR and LSTM models."}, {"title": "B. Results per Event Log Group", "content": "In Table III, we present the results for the three groups of event logs, averaged per group.\nThe Euclidean distance represents the distance in the latent space, where lower values indicate that adversarial examples are more perceptibly indistinguishable from the original instances. Interestingly, the Euclidean distance in the latent space for regular attack strategies (apart from the All Event (Regular)) is lower than those for projected strategies, such as Last Event (Projected), All Event (Projected), and 3 Event (Projected). This is counterintuitive, as one might expect that projection-based attacks, designed to be close in the latent manifold, would yield adversarial examples that are actually closer. This contradicts previous findings in [32].\nThe EMD captures the distance between the original prefix and its adversarial example to provide a meaningful measure of similarity between probability distributions, which, in the context of adversarial examples, reflects how close the generated adversarial example is to the original prefix trace in terms of its underlying data distribution. The EMD serves as a measure of distance in the input space, with lower values indicating adversarial examples that are more perceptually indistinguishable from the original instances. In adversarial machine learning, this is crucial because we want adversarial examples to remain realistic and perceptually similar to the original input while still achieving the goal of deceiving the model.\nAnother interesting remark is that, in general, the EMD distances of the adversarial examples are lower for the projected attacking strategy compared to the regular attacking strategies. For the event log group BPIC2012, we can see that the regular attacking strategy methods have the lowest average distance in the latent space (Euclidean distance), but also the highest EMD value of the adversarial examples after decoding. To understand this intuitively, we need to understand how the decoder functions. First, minimizing distances in the latent space encourages adversarial examples to stay close to the original representations at an encoded level. However, this does not guarantee that the decoded outputs will remain perceptually close to the original inputs. This is because the"}, {"title": "C. Cluster Profiles of Attacks", "content": "Instead of relying solely on statistical analysis, we define these clusters as profiles that capture common patterns in the nature of adversarial changes. To group attacks by their unique characteristics, we apply thresholds derived from quartile values, calculated after normalizing both the EMD and DL Edit scores according to the prefix length. This normalization ensures comparability across traces of varying lengths. The quartile-based thresholds provide a straightforward yet effective method for distinguishing between attack cluster profiles.\nThe resulting profiles categorize attacks into four primary groups: Subtle, Aggressive, Sequence Perturbations, Distribution Shift. Attacks that do not exhibit sufficiently distinctive characteristics are classified as Others. Below, we detail the criteria used to define and classify each profile.\nAggressive Aggressive attacks are characterized by significant changes to the trace. Specifically, an attack is classified as aggressive if the DL Edit distance and EMD distance fall within the third quartile, indicating extensive changes to the trace and its overall distribution. While these attacks achieve moderate success rates (around 40%), their broad and noticeable changes to the trace make them relatively easier to detect. It is clear to see that the All Event (Regular) category is predominantly represented within this cluster, as these attacks indeed target all the events in a trace.\nSubtle Attacks Subtle attacks are those that make only minimal changes to the original trace. An attack is considered subtle if both the DL Edit distance and the EMD value fall within the first quartile. These minimal changes reflect the conservative nature of such strategies and their focus on remaining undetected by only performing small perturbations that are less likely to be detected in the event log. As expected, the Last Event (Regular) and 3 Event (Regular) profiles are best represented by this classification. These attacks are the least successful attacks, with a success rate of below 20%.\nSequence Perturbations. Sequence Perturbations are changes that specifically target the order, dependencies, and relationships between events in the trace. Attacks with a high DL Edit are adversarial examples with a DL Edit distance in the third quartile, but also with an EMD value that is lower than the median value. This kind of attack primarily targets the sequence order of events without altering the underlying distribution of event types, and they are highly successful, with a success rate of up to 60 percent. This group is mostly represented by the All Event (Regular) attack.\nDistribution Shift. Distribution shift attacks are attacks that made changes to the trace that, while not necessarily altering individual events in an obvious or direct way, lead to a significant shift in the overall structure or distribution of events. Attacks with a high EMD value are adversarial examples with an EMD distance in the third quartile. These attacks are not that successful, with a success rate close to 20 percent. This group is mostly represented by the 3 Event (Regular) attack and the Last Event (Regular) attack.\nOthers. These attacks can be characterized by a combination of small changes in sequence structure (as indicated by a low to moderate DL) and slight shifts in event distribution (reflected by a low to moderate EMD), yet they are still successful. Although these attacks modify the sequence structure and event distribution of the trace, they are subtle enough to the extent that they would not be easily detectable. Despite these modest changes, the attacks remain successful, with a success rate of up to 50%.\nIn conclusion, the Others cluster represents the most desirable attack profile, as these attacks generated adversarial examples with a low to moderate EMD distance and low to moderate DL Edit value, while maintaining high success rates. The fact that these attacks introduce more or less small changes in both sequence structure and event distribution, yet remain effective, suggests they exploit vulnerabilities that are not easily detected. The attack methods most representative of this category include the Last Event (Projected), the All Event (Projected), the 3 Event (Projected) and the Latent Sampling attacks. These methods showcase how moderate perturbations to the trace can yield successful results without major alterations to the original process flow. This showcases the need for projected and latent sampling-based approaches, as they can be effective without needing any particular engineering which resorts to strong insights into the sequential information of the processes."}, {"title": "VI. RELATED WORK", "content": "Predictive process monitoring is an important field that involves monitoring and analyzing process data extracted from business information systems. In recent years, researchers have proposed various solutions to tackle different prediction tasks, such as predicting the most probable next event or suffix of a case [35], estimating the remaining time [8], or predicting the outcome [38]. For a more comprehensive overview of the field, we refer to the systematic literature review of Neu et al. [20].\nRecent works have already issued the lack of reliability of deep learning models in the context of predictive process monitoring [24], [29] with issues such as the compromised faithfulness of post-hoc explanations [29]. Incremental adaptations to predictive models have also been suggested to address prediction stability over time [24]."}, {"title": "B. Adversarial Machine Learning", "content": "Deep neural networks are powerful tools for learning complex tasks, but their adoption in high-stake decision-making is often limited due to their lack of robustness against adversarial examples [10", "34": ".", "45": ".", "30": ".", "32": "the idea was to explore these adversarial attacks as a means to enhance robustness against such threats. The field of AML is also focused on improving robustness model and generalization [15", "33": [36], "37": [42], "47": "."}]}