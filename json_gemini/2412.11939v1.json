{"title": "SEAGraph: Unveiling the Whole Story of Paper Review Comments", "authors": ["Jianxiang Yu", "Jiaqi Tan", "Zichen Ding", "Jiapeng Zhu", "Jiahao Li", "Yao Cheng", "Qier Cui", "Yunshi Lan", "Xiang Li"], "abstract": "Peer review, as a cornerstone of scientific research, ensures the integrity and quality of scholarly work by providing authors with objective feedback for refinement. However, in the traditional peer review process, authors often receive vague or insufficiently detailed feedback, which provides limited assistance and leads to a more time-consuming review cycle. If authors can identify some specific weaknesses in their paper, they can not only address the reviewer's concerns but also improve their work. This raises the critical question of how to enhance authors' comprehension of review comments. In this paper, we present SEAGraph, a novel framework developed to clarify review comments by uncovering the underlying intentions behind them. We construct two types of graphs for each paper: the semantic mind graph, which captures the author's thought process, and the hierarchical background graph, which delineates the research domains related to the paper. A retrieval method is then designed to extract relevant content from both graphs, facilitating coherent explanations for the review comments. Extensive experiments show that SEAGraph excels in review comment understanding tasks, offering significant benefits to authors.", "sections": [{"title": "1 Introduction", "content": "In recent years, the number of academic publications has grown exponentially, creating a vast \"sea of papers\u201d (Bornmann and Mutz, 2015; Lin et al., 2023). Traditionally, authors rely on the peer review process to receive feedback on their manuscripts (Lee et al., 2013; Bj\u00f6rk and Solomon, 2013). However, the review cycle typically requires several months or even longer, which is time-consuming (Horbach and Halffman, 2018). Meanwhile, the large volume of submissions results in uncertain review qualities (Bornmann and Mutz, 2015), often resulting in ambiguous or overly brief comments that are challenging to explain (Verma et al., 2022). For example, the statement that \"the method is limited\" is very vague without any details provided. In such cases, authors may be confused about which parts of the paper to be revised, leading to the difficulty in effectively addressing the reviewers' issues or fully identifying the shortcomings of the paper. If they receive detailed suggestions, they may benefit more during this review process. Therefore, it is essential to understand the vague comments with more details provided, thereby helping the authors to improve their paper quality (as shown in Fig. 1).\nCurrently, Large Language Models (LLMs) have shown powerful text comprehension and generation capabilities (Achiam et al., 2023; Wei et al., 2022), offering new directions for revealing the underlying intentions behind each review comment. A straightforward approach is to provide LLMs with both the comment and the corresponding paper. Yet, it is usually difficult to feed an entire paper into LLMs for identifying key points, as review comments typically focus on specific aspects rather than the entire paper. Another alternative approach is using RAG (Retrieval-Augmented Generation) (Cheng et al., 2024; Jiang et al., 2023), which enhances reasoning by retrieving the most relevant passages from lengthy texts based on the query. Nevertheless, the information retrieved by RAG tends to be fragmented, lacking clear logic (Cao et al., 2024). In contrast, review comments are given based on the coherent logical structure formed when reviewers read the paper, which cannot be easily captured by fragmented segments. Recently, the success of GraphRAG (Edge et al., 2024), which splits lengthy texts into discrete chunks and hierarchically connects them, has inspired new directions. Similarly, papers are inherently structurally organized with sections and subsections provided. Therefore, we can format papers as structured graphs, from which logical chains can be extracted to facilitate a deeper understanding of review comments.\nIn this paper, we propose SEAGraph, a novel framework designed to uncover the intentions behind paper reviews and enhance the understanding of review comments. We construct two distinct graphs for each reviewed paper: a semantic mind graph and a hierarchical background graph. Building upon the principles of the mind map (D'Antoni and Zipp, 2006), which employs visuospatial orientation to integrate information, we introduce the semantic mind graph to facilitate deeper semantic connections and organization of key knowledge points. In addition, the hierarchical background graph connects various related papers based on the themes of the paper, thereby simulating its research context. After the construction of the two graphs, we design a tailored retrieval method to extract the most relevant content from both graphs in response to each review comment. The extracted content is subsequently fed into LLMs to generate coherent and logical arguments that explain the reviewer's comments. Overall, our contributions are summarized as follows:\n\u2022 We introduce a novel framework SEAGraph, which pioneers the field of review comment understanding.\n\u2022 We construct a semantic mind graph and a hierarchical background graph for a paper, capturing its deep semantics and related domain knowledge.\n\u2022 We conduct extensive experiments to validate the effectiveness of our framework, which can help authors improve the quality of their papers."}, {"title": "2 Related Work", "content": "2.1 Retrieval-Augmented Generation\nRAG (Retrieval-Augmented Generation) improves the generation performance of LLMs by incorporating external knowledge (Lewis et al., 2020). Initially, naive RAG approaches follow a process including indexing, retrieval, and generation (Li et al., 2022). Advanced RAG frameworks focus on enhancing the retrieval quality during the pre-retrieval and post-retrieval phase like query rewriting, query expansion, and chunk reranking (Ma et al., 2023; Peng et al., 2024b; Zheng et al., 2023). Furthermore, some modular RAG approaches put forward new modules or pipelines to enhance the retrieval capability and alignment with task-specific requirements (Yu et al., 2022; Shao et al., 2023). Despite these advancements, RAG faces challenges in handling query-focused summarization tasks when queries target entire text corpora (Cao et al., 2024). GraphRAG emerges as an innovative solution to address this challenge (Peng et al., 2024a). Edge et al. (2024) establish logical relationships between segments by connecting chunks or communities through a hierarchical structure. Wu et al. (2024a) and Sepasdar et al. (2024) construct specialized knowledge graphs, extending GraphRAG to the medical and soccer domains. In this work, we construct two logically connected graphs for the paper, leveraging the strengths of GraphRAG to better address the review comment understanding tasks.\n2.2 Large Language Models in Peer Review\nRecently, Large Language Models (LLMs) have made remarkable progress in text generation tasks (Zhao et al., 2023; Ouyang et al., 2022; Luo et al., 2024), prompting researchers to explore new opportunities in the field of peer review (Li et al., 2024b; Checco et al., 2021). A significant focus has been placed on generating automated reviews to enhance the quality of academic papers (Gao et al., 2024). For example, Liu and Shah (2023) and Liang et al. (2023) customize prompts to guide GPT-4 in generating scientific feedbacks. Yu et al. (2024a) and Wei et al. (2023) respectively employ supervised fine-tuning and continuous pretraining to align LLMs with human reviews. Expanding on this research, Jin et al. (2024) employ LLMs to simulate the entire review process, thereby revealing the impact of various factors on academic evaluation. Subsequently, Ye et al. (2024) highlight the risks associated with using LLMs for automated peer reviewing. Building on these insights, Yu et al. (2024b) delve into the challenge of distinguishing between peer reviews authored by humans and those generated by LLMs. However, both human reviews and generated reviews may suffer from issues such as ambiguity or brevity, causing confusion for authors. Our work aims to leverage LLMs to understand the intent of review comments, thereby assisting authors in polishing their papers."}, {"title": "3 SEAGraph", "content": "This section provides a detailed description of each module in SEAGraph, and the overall framework is in Figure 2.\n3.1 Review Comments Understanding Task\nPaper Processing. Our dataset consists of the PDF versions of academic papers and their corresponding review comments. We begin by utilizing Nougat (Blecher et al., 2023) as the parser, a model built on the Visual Transformer architecture specifically tailored for extracting information from academic documents. Then, we construct the Semantic mind graph and the Hierarchical background graph for each reviewed paper, denoted as $G_s(V_s, E_s)$ and $G_H(V_H, E_H)$, where V and E represent nodes and edges, respectively.\nReview Comments Extraction. We input the entire review into LLM to extract individual comments under three textual sections: \u201cStrengths, \"Weaknesses\u201d, and \u201cQuestions.\" Then, all review comments are defined as a query set Q, where $q \\in Q$ represents a single comment.\nOur Goal. Our goal is to retrieve subgraphs from two graphs Gs and GH, which represent evidence relevant to a review comment q. These pieces of evidence, along with the comment, are then provided to LLMs to help understand the details of the review. By generating a logical chain, SEAGraph enables authors to better understand reviewers' perspectives and proceed with subsequent research more effectively.\n3.2 Semantic Mind Graph Construction\nA paper is structurally organized into different sections, while key points of the paper may be scattered across various parts. The entire paper can be structured like a mind graph, where content progressively branches out from different paragraphs. Our goal is to construct a semantic mind graph to model the writing logic of a paper. We next detail the main steps.\nPaper Chunking. We first use the Spacy library (AI, 2017) to break a full paper down to sentence level, allowing us to assess the relevance between sentences and decide whether adjacent sentences should be merged into chunks. Specifically,"}, {"title": "4 SEAGraph", "content": "we utilize Sentence-BERT (Reimers and Gurevych, 2019) to encode both sentences and chunks, and design a semantic relevance measure to determine whether the current chunk is related to the next sentence. Subsequently, we place the first sentence $s_1$ into the initial chunk $C_{current}$. For each subsequent sentence $s_i$, we compute the embedding similarity between $s_i$ and $C_{current}$. If the similarity exceeds a threshold $\\theta_1$, we merge the sentence into the current chunk by:\n$C_{current} \\leftarrow C_{current} \\cup s_i \\quad if \\quad f(h_{s_i}, h_{C_{current}}) \\geq \\theta_1$,\nwhere $h_{s_i}$ and $h_{C_{current}}$ represent the embeddings of the i-th sentence and the current chunk, respectively. Here, $f(\\cdot, \\cdot)$ is the similarity function (e.g. cosine similarity). If the similarity is less than the threshold, we end the current chunk and start a new one: $C_{new} \\leftarrow s_i$. Then we repeat the above steps to divide a paper into chunks. Additionally, a maximum chunk size is also set to prevent excessive imbalance in the length of different chunks. Finally, the chunk nodes of the paper can be represented as $V_s = \\{C_a, C_1, C_2, ... C_n \\}$, where $C_a$ denotes the abstract node of the paper. In this way, the content within the same chunk is closely related.\nChunk Linking. Given the segmented chunks, the next step is to link them based on their contextual and semantic relationships. Generally, chunks within the same (sub)section often share the same topic. For example, the \"Method\u201d section describes the paper's research methodology, while the \u201cExperiments\" section validates the proposed method through experimental design. Therefore, adjacent chunks in a sequential order are probably highly correlated and we call this contextual correlation. We can establish connections between them by setting $e_{C_a, C_1} = 1$ and $e_{C_i, C_{i+1}} = 1, \\forall i \\in [1, n - 1]$ where e denotes the edge between two chunk nodes. This approach also helps mitigate issues that sentences with high semantic relevance may be split into different chunks due to chunk size constraints.\nFurther, the authors may not simply follow a linear mind in organizing the paper. As shown in Figure 3, the \"Introduction\u201d section of a paper often lays the groundwork for understanding the problem, which is further elaborated in the \u201cMethod\" section with detailed descriptions of the proposed approach or framework. Subsequently, the \u201cExperiments\" section validates these methods through practical evaluations. If these segments are extracted individually, they can still form a coherent logic. We call this semantic correlation. To capture their correlations, we compute the semantic similarities between different chunks and set a threshold $\\theta_2$ to connect highly similar chunks:\n$e_{C_i, C_j} = 1 \\quad if \\quad f(h_{C_i}, h_{C_j}) \\geq \\theta_2$ (1)\nFinally, the paper is transformed into a semantic mind graph, where linked chunks represent either contextual proximity or semantic similarity.\n3.3 Hierarchical Background Graph Construction\nTo effectively review a paper, a reviewer not only needs a deep understanding of the content but also a solid grasp of the knowledge in corresponding fields. Therefore, we construct a background graph with hierarchical relationships to simulate reviewers' domain knowledge. This graph is organized into a three-layer structure: the themes of the reviewed paper, the abstracts of relevant papers, and the semantic mind graph for each individual paper.\nCited Paper Search. We first locate the cited papers in the \u201cRelated Work\" section of the reviewed paper, which represent the existing research achievements in the field. Subsequently, we extract the publication details of these papers from the \"Reference\" section.\nTheme Summarization. We next crawl the PDFs of referenced papers from Google Scholar, parse them into markdown format, and extract the abstracts and titles of each paper. Then, we feed them into LLM to summarize multiple themes and assign corresponding papers to each theme. In this way, we obtain a theme set related to the reviewed paper, denoted as $T = \\{t_1, t_2, ..., t_m \\}$ and t refers to the descriptive summarization of a theme node.\nComplementary Papers. The authors may not always reference all foundational or cutting-edge papers in the field. Therefore, we aim to enrich the paper by incorporating both fundamental and recent studies within the research domain. Based on the extracted themes, we search and crawl the most popular and recent papers related to these themes from Google Scholar to enrich the background graph in terms of breadth and timeliness. After identifying the relevant papers related to the paper, we apply the method from Section 3.2 to construct a semantic mind graph for each of them.\nHierarchical Linking. Now, for a reviewed paper, we construct its hierarchical background knowledge graph based on theme nodes, abstract nodes, and semantic mind graphs. The first level includes multiple theme nodes, each corresponding to a thematic description that encapsulates the research topics. The second level connects these theme nodes to abstract nodes, where each abstract serves as a concise summary of a paper, representing its key ideas and maintaining a direct association with its respective theme. The third level extends from the abstract nodes to semantic mind graphs, which provide fine-grained information, offering a deeper insight into the paper's structure and details. This hierarchical design clearly delineates the logical progression from themes to papers and further to detailed information, forming a systematic framework for representing the research background.\n3.4 Semantic Mind Graph Retrieval\nWe next introduce retrieving the semantic mind graph based on review comments and obtaining the relevant supporting texts. Given a review comment as a query, we first calculate the probability distribution of the query over the semantic mind graph by calculating the textual similarity between the query and each chunk node $c_j$:\n$P(c_j) = \\frac{f(h_q, h_{c_j})}{\\sum_{i=1}^{n} f(h_q, h_{c_i})}$, (2)\nwhere $P(c_j)$ represents the probability distribution over the nodes in the semantic mind graph, $f(\\cdot, \\cdot)$ is the similarity function and $h_q$ denotes the embedding of the query.\nThen we iteratively retrieve chunk nodes that can help explain the review comments. We start by randomly sampling k chunk nodes based on the probability computed in Eq. 2 and add them to an empty node set V. After that, we explore the one-hop neighbors of these newly sampled chunk nodes and add them to V. Given a chunk node $c_i \\in V$, suppose chunk node $c_j$ as its one-hop neighbor and we calculate:\n$score_i(h_{c_j}) = \\alpha \\cdot P(c_j) + f(h_{c_i}, h_{c_j})$, (3)\nwhere $f(h_{c_i}, h_{c_j})$ denotes the cosine similarity between the embeddings of the two chunks and $\\alpha$ is a hyperparameter to control term balance. In Eq. 3, the score is used to measure the relevance between query and chunk $c_j$. In particular, the second term calculates the similarity between chunk nodes $C_i$ and $c_j$, which indirectly reflects the relation between query and chunk $c_j$ via chunk $c_i$. After the scores are computed, we select chunk nodes with the highest scores and further add them into V. We repeat the above process to retrieve more chunk nodes. Finally, all the chunk nodes in V constitutes a subgraph that is relevant to the given review comment.\n3.5 Hierarchical Background Graph Retrieval\nTo further mine the background knowledge related to a paper, we conduct an in-depth hierarchical background graph retrieval based on the review comment and the corresponding semantic mind subgraph obtained in the previous section. The hierarchical retrieval process refines from (1) theme level to (2) abstract level, and finally to (3) chunk level, ensuring background knowledge obtained at different levels of granularity. We apply scoring method in Eq. 3 to the three levels of nodes and select the nodes with higher scores.\nWe first begin the retrieval process at the theme level, aiming to extract themes related to a review comment. For example, if a reviewer questions whether the proposed method shares similarities with certain techniques in the fields of computer vision (e.g., contrastive learning in CV), we retrieve the corresponding descriptions of theme nodes to align with the reviewer's concerns from a broader perspective.\nBased on the theme-level information, we then proceed to the abstract-level retrieval, focusing on papers related to the identified themes. Note that abstracts summarize key research questions, methodologies, and conclusions, providing a concise yet comprehensive overview. Therefore, abstract-level"}, {"title": "4 Experiments", "content": "4.1 Experimental Setup\nDatasets. We select 20 papers and their corresponding reviews from ICLR over the past five years through OpenReview\u00b9. Each paper was reviewed by 3 to 8 reviewers, with a total of 82 reviewers. We divide the review comments and exclude those longer than 200 characters, which are considered relatively complete. In the end, 284 shorter comments were retained. These papers cover diverse fields such as Artificial Intelligence (AI), Machine Learning (ML), Natural Language Processing (NLP), and Computer Vision (CV). The selected papers exhibit a wide range of citation counts, including both highly cited and less-cited works, as well as recent studies and earlier publications.\nBaseline Methods. To validate the effectiveness of SEAGraph in terms of graph construction and retrieval, we compare it with the following two categories of baseline methods: (1) Direct inference methods: DirectInfer takes the review comment and the full parsed paper as input to directly reason about the understanding of each review comment. (2) RAG-based methods: RAG-naive computes the similarity between each review comment and every chunk of the paper, selecting the top-k chunks to combine with the review comment as input; RAG-SMG utilizes only the construction and retrieval of the Semantic Mind Graph; RAG-HBG relies solely the construction and retrieval of the Hierarchical Background Graph.\nWe conduct experiments using the same open-source base model Ministral-8B-Instruct-24102 and the same prompt (Table 3) as input to LLMs.\nEvaluation Protocol. Since the task of reivew comments understanding lacks a definitive ground truth and exhibits significant diversity in generated content, we design two evaluation methods: human evaluation and automated evaluation. For human evaluation, we engage 40 experts from various fields, each responsible for one paper to assess the quality of the generated understanding of the review comments. For automated evaluation, given the powerful text comprehension capabilities of LLMs to play as a judge (Li et al., 2024a), we employ gpt-40-2024-11-20\u00b3 as the evaluation model to provide objective judgments for the task.\n4.2 Main experiments\nEvaluation Metric. There are four main assessment metrics for human and automated evaluation: (1) Relevance: Assesses the alignment between the provided evidence and the review comments. (2) Clarity: Evaluates how clearly and effectively the information is presented for ease of understanding."}, {"title": "5 Conclusion", "content": "In this paper, we present SEAGraph, a novel framework designed to bridge the gap between reviewers' comments and authors' understanding. By constructing two distinct graphs\u2014the semantic mind graph, which captures the authors' thought process, and the hierarchical background graph, which encapsulates the research background\u2014the framework effectively models the context of a reviewed paper. The well-designed retrieval method ensures that relevant content from both graphs is used to generate coherent and logical explanations for review comments. SEAGraph not only enhances the clarity of reviews but also empowers authors to understand reviewer concerns more effectively, improving the quality of academic publications.\nIn a nutshell, we sincerely hope that our work does empower authors to not only gain a deeper understanding of reviews feedback but also elevate the quality of their papers, ultimately expediting both the advancement of research and the efficiency of the submission process."}, {"title": "Limitations", "content": "SEAGraph is designed to assist authors in comprehending review comments during the peer review process, with a particular emphasis on the review-comment understanding stage. Here we elaborate on some of these constraints, along with intriguing future explorations.\nRebuttal mechanism. The rebuttal mechanism, where authors respond to reviewers' concerns and engage in further discussion, also plays a critical role in improving the paper (Jin et al., 2024). The success of multi-agent systems in executing complex tasks presents a promising opportunity (Wu et al., 2023, 2024b). In future research, we will explore simulating the rebuttal process through multi-agent communication, aiming to further bridge the understanding gap between reviewers and authors in papers and comments, thus advancing the rebuttal mechanism.\nEnhancing Pipeline Stability. As an integrated pipeline, SEAGraph involves a relatively complex process. Certain components, such as the processes of searching for and downloading relevant papers, are dependent on network conditions. To address this, we aim to continuously refine and optimize the underlying code, ensuring the robustness and stability of these technical operations while improving their overall efficiency.\nPrivacy Challenges in Benchmarking. We plan to establish a standardized benchmark for the field of peer review to help standardize and advance research practices in this area. Currently, platforms such as OpenReview provide a wealth of publicly available papers and review data, offering valuable resources for studying the mechanisms and effectiveness of peer review. However, there are certain limitations to the use of these data, primarily concerning privacy protection. While the identities of authors are ostensibly anonymized, the peer review process allows senior roles such as Program Chairs and Area Chairs to access the actual identities of both authors and reviewers. This potential breach of privacy may pose challenges to the objectivity, fairness, and ethical considerations of related research.\nEvaluation Metrics. Although human evaluation and GPT-based evaluation can reflect the strengths and weaknesses of a model to some extent, they often involve significant subjectivity and lack consistency. This issue becomes particularly pronounced in open-ended generation tasks, where differences in standards and preferences among evaluators may lead to inconsistent results. Therefore, establishing a comprehensive, unified, and reproducible quantitative standard is crucial for more objective and fair assessment of model performance. Such a standard not only helps to minimize the influence of human bias but also provides more actionable feedback for subsequent model optimization and improvement."}, {"title": "Ethics Statement", "content": "This work seeks to assist authors in better comprehending review comments. We do not intend to suggest that some reviews are inherently of low quality or unhelpful. Instead, we appreciate that clearer and more comprehensible review comments can more effectively fulfill the primary objective of the peer review process\u2014namely, to offer objective evaluations and constructive feedback aimed at improving the manuscript. Recently, some academic conferences have introduced AI-assisted review bots to standardize reviewers' feedback. Through this work, we aim to benefit authors by enhancing their understanding of review comments, while also encouraging reviewers to consider the clarity of their feedback and strive for higher-quality reviews. Ultimately, we seek to foster a healthier and more harmonious academic interaction environment."}, {"title": "A More details of SEAGraph", "content": "Prompt. In Table 3, we present the instruction designed to generate content for understanding review comments that conform to the specified format based on the retrieved content. We require LLMs to output several evidence before the summary.\nThe specific meaning of metrics. The metrics for evaluating the generation of understanding review comments and the retrieval content are shown in Table 4 and Table 5, respectively.\nSEAGraph results Figure 6 shows an example of the explanation for a review comment generated by SEAGraph. For privacy concerns, both the review comment and the generated content have been processed. As shown in the figure, the review comment points out that the paper only conducts experiments in its own designed experimental settings, and suggests that comparing the proposed method with other libraries would help demonstrate its validity. SEAGraph locates, through the retrieval of the semantic mind graph, that the paper mentions only part of the workloads in the \"Experiments\u201d and \u201cRelated Work\" sections, highlighting that, although the method is effective, it does not compare with some of the libraries or algorithms mentioned. It also points out in the \"Conclusion\" that the method's validity could be verified by comparing it with more real-world applications. Additionally, SEAGraph, through the retrieval of hierarchical background knowledge, mentions other papers that have conducted such comparisons in their experiments. Finally, in the summary, SEAGraph effectively consolidates the logic of the entire review comment, highlighting the missing experiments in the paper and referring to other papers' experimental settings. This example demonstrates SEAGraph's ability to generate explanations for review comments by constructing two graphs and retrieving relevant chunks.\nB Hierarchical Background Graph Retrieval\nIn the Hierarchical Background Graph, to make background knowledge more aligned with the reviewed paper and the review comments, we conduct a retrieval process for external knowledge based on the review comments and semantic mind subgraph identified in Section 3.4. First, we compute the representation of the retrieved semantic mind subgraph V using a pooling operation to obtain the subgraph representation. First, we obtain the subgraph representation by applying a pooling operation to the node representations of the retrieved semantic mind subgraph.\n$h_V = \\frac{1}{|V|} \\sum_{c_i \\in V} c_i$ (4)\nOn this basis, we design a three-layer retrieval framework, including theme-level, abstract-level, and chunk-level retrieval, to capture information at varying levels of granularity. The specific retrieval formulas are as follows:\n(1) Theme level.\n$score(h_{t_i}) = \\alpha_t \\cdot f(h_{t_i}, h_q) + \\beta_t \\cdot f(h_{t_i}, h_{ss})$\n(2) Abstract level.\n$score(h_{a_i}) = \\alpha_a \\cdot f(h_{a_i}, h_q) + \\beta_a f(h_{a_i}, h_{ss}) + \\gamma_a \\cdot f(h_{a_i}, h_{t_a})$\n(3) Chunk level.\n$score(h_{c_i}) = \\alpha_c \\cdot f(h_{c_i}, h_q) + \\beta_c \\cdot f(h_{c_i}, h_{ss}) + \\gamma_c f(h_{c_i}, h_{t_a}) + \\delta_c \\cdot f(h_{c_i}, h_{a_c})$\nHere, \u03b1, \u03b2, \u03b3, and 8 represent the hyperparameters, while ti, ai, and ci correspond to the text of the theme description, the abstract, and the chunk, respectively. Additionally, hq denotes the embedding of the query.\nC Supplement to Automated Evaluation\nIn Figure 7, we show a pairwise comparison of the text generated by the five methods in the automated evaluation. Compared to Figure 4 in the main text, here we show the results for samples of all lengths. The values in the heatmap represent the win rate of the method shown on the vertical axis over the method on the horizontal axis. From the figure, we can see that SEAGraph outperforms in most cases, only slightly falling short on the Relevance metric when compared to Infer. On the other hand, RAG-SMG lags slightly behind RAG-naive in terms of relevance, likely because longer review comments are already sufficiently detailed, leading to a minor disadvantage in argument relevance. However, both SEAGraph and RAG-SMG demonstrate superior performance on other metrics, proving that the evidence they provide are more effective and better support reasoning."}]}