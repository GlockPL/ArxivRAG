{"title": "Mental Disorders Detection in the Era of Large Language Models", "authors": ["Gleb Kuzmin", "Petr Strepetov", "Maksim Stankevich", "Ivan Smirnov", "Artem Shelmanov"], "abstract": "This paper compares the effectiveness of traditional machine learning methods, encoder-based models, and large language models (LLMs) on the task of detecting depression and anxiety. Five datasets were considered, each differing in format and the method used to define the target pathology class. We tested AutoML models based on linguistic features, several variations of encoder-based Transformers such as BERT, and state-of-the-art LLMs as pathology classification models. The results demonstrated that LLMs outperform traditional methods, particularly on noisy and small datasets where training examples vary significantly in text length and genre. However, psycholinguistic features and encoder-based models can achieve performance comparable to language models when trained on texts from individuals with clinically confirmed depression, highlighting their potential effectiveness in targeted clinical applications.", "sections": [{"title": "Introduction", "content": "The problem of detecting mental disorders and patient emotions through text analysis and machine learning has been of increasing interest to researchers over the past decade (Graham et al., 2019; Zhang et al., 2022; Calixto et al., 2022; Mayer et al., 2024). In fact, advances in data science and natural language processing methods offer promising opportunities for screening, monitoring, early detection, and prevention of negative outcomes of mental disorders. Although there are studies that work with interviews (Morales and Levitan, 2016; Ringeval et al., 2017) and offline texts (Lynn et al., 2018; Stankevich et al., 2019), in most cases the material for such research comes from social media (Guntuku et al., 2017; Garg, 2023). These studies tend to focus on but are not limited to, conditions such as depression, anxiety, stress, suicidality, post-traumatic stress disorder, and anorexia. Unsurprisingly, the methods considered for predicting mental state from text fell into traditional machine learning, using hand-crafted linguistic features, and various forms of deep learning (Zhang et al., 2022). The deep learning approach is often more accurate, especially when there are enough data samples, while traditional machine learning produces more interpretative results.\nIn this paper, we compare the performance of linguistic features, encoder-based models, and large language models (LLMs) on the task of identifying mental disorders. We consider two types of mental states, depression and anxiety, and several datasets in Russian that differ in text format and in the way a pathology is detected.\nWe are particularly interested in evaluating the performance on the essay dataset because it consists of texts written by patients with clinically diagnosed depression and texts written by volunteers who were considered healthy (Stankevich et al., 2019). It is worth pointing out the problem of dividing data into texts written by people with a certain mental pathology. In most cases, social media-based studies use self-report, affiliation, or questionnaire data to determine mental health status. However, there is a lack of work using clinically validated data, which can vary widely (Chancellor and De Choudhury, 2020; Ernala et al., 2019). We believe that verified training data from essays can significantly contribute to building models for the social media domain.\nThis paper addresses the following research questions:\n\u2022 RQ1: What is the most prominent technique for predicting depression and anxiety: traditional machine learning, encoder-based models, or recent LLMs?\n\u2022 RQ2: Do models trained on a dataset of essays in which depression was defined by a clinical diagnosis generalize to social media, where the depression status is defined by a questionnaire?\nOur main contributions are the following:\n\u2022 We outperformed the existing state-of-the-art depression detection method on one dataset and established classification baselines on three previously unexamined anxiety datasets.\n\u2022 We conducted a thorough comparison of various groups of models on the depression and anxiety detection tasks in Russian, which could be used for the practitioners in this field for future experiments.\n\u2022 We investigated the transferability of models from the task with clinical diagnosis as a target to the task with a target obtained as a questionnaire result to alleviate the lack of clinically validated data in the mental disorder detection task."}, {"title": "Related Work", "content": ""}, {"title": "Traditional and Advanced ML Methods", "content": "Researchers have employed diverse methodologies and techniques in a comprehensive exploration of depression and anxiety detection across various social media platforms. Tadesse et al. (2019) consider the Reddit users' dataset, comparing single and combined feature learning for depression detection. N-gram features, Linguistic Inquiry and Word Count (LIWC) dictionary features, and topics from Latent Dirichlet Allocation are considered, showcasing the effectiveness of combined features on the multi-layer perceptron with a remarkable 91% accuracy and 0.93 F1 score.\nIn (Shah et al., 2020), the authors leverage NLP methods for depression detection of Reddit users based on their posts. GloVe, Word2Vec, and FastText embeddings, as well as handcrafted statistical metadata features and features from the LIWC dictionary, are used for text representation. The two-headed model, combining BiLSTM for embedding transformation and a fully connected layer for meta-features, demonstrates superior results with Word2Vec embeddings and meta-features achieving a noteworthy 0.81 F1 score. Additionally, the authors use Early Risk Detection Error and Latency metrics to take the time of classification into account.\nThe study (Owen et al., 2020) considers depression and anxiety detection for Twitter posts. SVM on TF-IDF vectors and GloVe embeddings, along with BERT (Devlin et al., 2019) and ALBERT (Lan et al., 2019), are explored, showcasing the nuanced performance of BERT on a balanced dataset and better performance of SVM on an unbalanced one.\nIn a comprehensive survey (Babu and Kanaga, 2021), the importance of emoticons in texts in sentiment analysis for depression detection is underscored. The study covers 101 publications, emphasizing the effectiveness of combining deep learning algorithms, with CNN+LSTM yielding the highest precision. Moreover, multi-class classification, categorizing sentiments into subclasses based on polarities, provides more precise results than binary and ternary classification.\nIn (Tejaswini et al., 2022), the authors introduce the FCL (Fasttext+CNN+LSTM) model, comparing it with LSTM and CNN models on Glove and Word2vec embeddings. They also use datasets containing Reddit and Twitter posts. FCL outperforms other models in identifying depression in both small and large datasets.\nAssessing the anxious Twitter posts caused by the COVID-19 pandemic, (Jeong et al., 2023) delves into anxiety detection using BERT trained on the Korean language, achieving a 0.67 macro F1-score and 0.91 accuracy and establishing a correlation between the anxiety index and COVID-19 waves.\nThe study (Ansari et al., 2023) focuses on identifying depression in social media datasets (CLpsych, Reddit, eRisk) through various text classification methods, combining sentiment lexicons with deep learning pipelines. Authors utilize sentiment lexicons with logistic regression and LSTM with attention on Glove embeddings, comparing them and combining them into ensembles. Ensemble models outperform hybrid lexicon- and DL-based models on all datasets, achieving 75% accuracy and 0.77 F1 score on the Reddit dataset."}, {"title": "Large Language Models", "content": "According to the systematic review (Omar and Levkovich, 2024), most studies about depression detection focus on BERT-based models, indicating the field's early stages in adopting newer technologies like GPT-4 and Google's Gemini. However, LLMs are demonstrating significant potential in enhancing the capabilities of depression detection systems.\nThe Chat-Diagnose approach (Qin et al., 2023) integrates diagnostic criteria from the Diagnostic and Statistical Manual of Mental Disorders (DSM-5) into prompts and uses the Chain of Thoughts technique to deliver explainable diagnoses via an LLMs-augmented system based on ChatGPT/GPT-3. This method demonstrates state-of-the-art results on Twitter and Weibo depression datasets by employing zero-shot and few-shot learning, using prompts with selected social media content alongside predictions from a Time2VecTransformer model to enhance diagnostic accuracy.\nAnother study (Hadzic et al., 2024) compares the effectiveness of fine-tuned BERT with GPT-3.5 and GPT-4 in the depression detection task. The authors use Patient Health Questionnaire-8 scores for classifying transcribed audio data from the Distress Analysis Interview Corpus, KID, and a simulated dataset. With scores separated into depressive and non-depressive groups, the zero-shot method for GPT-4 outperforms GPT-3.5 and BERT across all datasets, highlighting the potential of LLMs in depression detection.\nAdditionally, research (Wang et al., 2024) on eRisk 2023 and eRisk 2021 datasets explores detecting depression symptoms and classifying depression severity using LLMs. Utilizing Beck's Depression Inventory to form queries related to depression symptoms and the Universal Sentence Encoder for text embeddings, the study creates two datasets containing top-1 and top-5 ranked texts for each query. LLMs fine-tuned with QLORA, including Llama-2-13b-chat, SUS-Chat-34B, and Neural-chat-7b-v3, are used for classification into four levels of depression severity. The top-five dataset performs better overall, with the combined model neural-chat+SUS-Chat_top5 excelling in Average Hit Rate and Average Closeness Rate, while neural-chat_top5 and SUS-Chat_top5 achieve the highest Average DODL and Depression Category Hit Rate among single models.\nThe DORIS (Lan et al., 2024) system addresses the challenges of detecting depression through social media posts from the Sina Weibo Depression Dataset. The authors use GPT3.5-Turbo-1103 for annotating high-risk texts according to the DSM-5 depression scale, also LLM is used to summarize critical information from users' historical mood records (mood courses). The final model based on XGBoost is learned on features from annotations and gte-small-zh model vector representations of post histories and mood courses. It shows an improvement in the AUPRC metric by 0.036 compared to baseline methods. This study underscores the system's potential for early detection and intervention, providing a valuable tool for mental health assessment on social media platforms.\nWe are the first to examine and compare three generations of the discussed models for depression and anxiety detection tasks in Russian. Unlike other works, we used different models from each group and carefully compared the results of the models between the groups on five datasets, aiming for a general recommendation on the best models to use in practice."}, {"title": "Data", "content": "Classes in all datasets were represented in the binary format: a healthy class and a pathology class.\nThe general description of the datasets used in our study is shown in Table 1."}, {"title": "Depression-Essays", "content": "To compile this dataset, subjects were asked to write a short essay (up to 5,000 characters) on the topic of \"Myself, others, world\u201d (Stankevich et al., 2019). A total of 557 essays were collected, where 110 of them were written by people with clinical depression. The essays written by people with clinically validated depression were provided by the Mental Health Research Center, Moscow, Russia. The label DE (Depression-Essay) is further used to describe this dataset. The best-reported performance on this data reaches 73% F1 for the depression class in the cross-validation evaluation with the random forest model trained on n-grams and psycholinguistic features."}, {"title": "Depression-Social Media", "content": "The other depression dataset under consideration was created from data collected to study the problem of identifying depression in social network users (Ignatiev et al., 2022). The best-reported classification performance using textual data reaches only 65% F1 for the depression class with a logistic regression model trained on psycholinguistic features. This dataset contains text messages from the social network VKontakte and results from the Beck Depression Inventory (BDI) questionnaire (Beck et al., 1987). The healthy class included users with a scale score of 10 or less, and the pathological class between 30 and 63.\nWe have made some changes to the original dataset. For each subject, all messages were combined for a period of 170 days prior to the date of the questionnaire screening, and the total text was limited to 6,000 characters. Such restrictions were imposed to bring the final texts from this dataset closer to the format of the essay dataset in terms of total text length for each subject and to account for the fact that the relevance of depression screening becomes less over a period of more than six months. The label DSM (Depression-Social Media) is used hereafter to describe this dataset."}, {"title": "Anxiety-Letter and Picture Description", "content": "The RusNeuroPsych corpus (Litvinova and Ryzhkova, 2018) was used as the data for creating the anxiety detection model. This corpus was prepared to study the relationships between a person's text, personal traits, mental status, and demographic characteristics. To compile it, subjects were asked to write an informal letter to a friend and give a textual description of a picture, and then complete a series of psychological questionnaires. Among the questionnaires, the Hospital Anxiety and Depression Scale (HADS) (Bjelland et al., 2002) was used. Thus, the data were divided using the results of this questionnaire, where the healthy class included subjects with a score of 7 and below, and the pathology class included all subjects with a score of 8 and above. Since the corpus contained 2 different types of text, it was divided into 2 separate datasets for the experiments. It is worth noting that the results of the HADS questionnaire also allow us to divide subjects into groups based on the presence and absence of depression, but a number of subjects with high depression scale scores was less than 20, which prevented us from conducting experiments to train classification models. The labels AL (Anxiety-Letter) for letters and AD (Anxiety-Description) for picture description are further used to describe these datasets. To the best of our knowledge, no classification experiments have been performed on this data before."}, {"title": "Anxiety-COVID Comments", "content": "In addition, we used a corpus of subjects' comments on the COVID-19 pandemic situation (Medvedeva et al., 2021). Subjects were asked to complete a series of questionnaires and write a free-form commentary describing their attitudes towards the world situation around the pandemic and self-isolation. Among the questionnaires used was the SCL-90-R symptom questionnaire (Derogatis, 1983), the anxiety scale from which was used to form 2 groups: the healthy group, with an anxiety scale score below the 33rd percentile, and the pathology group, with an anxiety scale score above the 66th percentile. The label AC (Anxiety-Covid) is used hereafter to describe this dataset. To the best of our knowledge, no classification experiments have been performed on this data before."}, {"title": "Method", "content": ""}, {"title": "Linguistic Features", "content": ""}, {"title": "Psycholinguistic Features", "content": "The linguistic features used in our research were calculated using the tool described in (Smirnov et al., 2021). This tool allows the calculation of morphological, syntactic, and vocabulary parameters of texts, including various psycholinguistic coefficients. A total of 113 features were used. A detailed description of the features used and the features obtained with this tool on our data is given in the Huggingface repository\u00b9."}, {"title": "Classification Setup", "content": "As a classification baseline, we use AutoML system auto-sklearn (Feurer et al., 2015) and learn it on psycholinguistic features and on n-grams. Auto-sklearn classifier employs a Bayesian optimizer which considers 15 classification algorithms, 14 feature preprocessing methods, and 4 data preprocessing methods. Also, the classifier utilizes a"}, {"title": "Encoder Models", "content": "We employ classification with encoder models to compare methods, based on psycholinguistic features, with more modern solutions. As we target classification on Russian corpora, we consider models pretrained on multilingual or Russian datasets. As a simple baseline, we used a base version of multilingual BERT, a model with 110 million parameters, first introduced in (Devlin et al., 2019). Another considered baseline is RuBERT (Kuratov and Arkhipov, 2019), a pretrained Russian version of BERT-base with the same amount of parameters.\nWe also finetuned several more recent models, such as RuBioROBERTa (Yalunin et al., 2022) which is a ROBERTa, pretrained on Russian language biomedical texts, and RuRoberta-large - a bigger version of ROBERTa, also pretrained on Russian language datasets. For all of the four models, we optimized hyperparameters using Bayesian optimization from HuggingFace (Wolf et al., 2019) framework.\nThe used hyperparameter grid and optimal parameters, alongside with used checkpoint of models, are presented in Appendix A."}, {"title": "Large Language Models", "content": "We conducted experiments with LLMs in various settings. First of all, we evaluated models by 0-shot and 5-shot prompting, considering only normalized probabilities for tokens \u201c0\u201d or \u201c1\u201d in the first generated token, as it was done in MMLU (Hendrycks et al., 2021). We will refer to these settings as \"O-shot MMLU\u201d and \u201c5-shot MMLU\" correspondingly. Secondly, we employed 0-shot and 5-shot prompting with bigger generation lengths and matched the generated answer to one of the possible classes. Finally, we conducted the finetuning of the models using LORA (Hu et al., 2022).\nWe selected a set of relatively small (less than 9B parameters) self-hosted open-source models, either tuned for the Russian language or showing good multilingual capabilities. To the first group belongs SaigaLlama3 8B, a version of Llama 3 8B Instruct (Dubey et al., 2024) tuned on several Russian datasets, as well as models from Vikhr family (Nikolich et al., 2024). We used Vikhr 7B Instruct 0.4, Vikhr 7B Instruct 5.4, and Vikhr Gemma 2B Instruct. The former two are based on Mistral 7B (Jiang et al., 2023) with vocabulary adaptation for the Russian language followed by additional pretraining and instruction tuning. The latter one is based on Gemma2 2B Instruct (Team et al., 2024), additionally trained on Russian data. We also used multilingual models, such as Gemma2 2B Instruct and Gemma2 9B Instruct, as well as Qwen2 7B Instruct (Yang et al., 2024). As we are conducting experiments on sensitive data and with private datasets, we did not consider remotely-hosted models (such as GPT4, Claude) due to the possibility of data leaking.\nThe full information about used prompts, training hyperparameters, and versions of used models, are presented in Appendix A."}, {"title": "Results", "content": ""}, {"title": "Classification Results on Depression and Anxiety Datasets", "content": "The data were divided into training and test samples in an 80% by 20% ratio, with stratification by target variable reduced to binary form. All classification reports in this study show results on the test data. The classification report for the best models from each group is presented in Table 2."}, {"title": "DE Dataset", "content": "The best scores of the F1 for the pathology class and F1-macro in this experiment are achieved on the essay dataset (DE) by the finetuned LLM model with 88.4% F1-macro and 81.1% F1 score for the pathology class. The model trained on the linguistic features shows similar results with 85.8% F1-macro and 77.0% F1-pathology.\nIn general, the three best results on the DE dataset were achieved by finetuned models, which can be linked to the bigger dataset size and to the longer texts in the dataset, which, in turn, are crucial for supervised finetuning. For all other datasets methods without finetuning significantly outperform SFT and LoRA."}, {"title": "DSM Dataset", "content": "The best result achieved by Vikhr 7B IT 0.4 model, evaluated in 5-shot, with 66.1% F1-macro. The traditional machine learning methods, as well as encoder models, performed poorly with nearly 53% F1-macro on linguistic features.\nComparing the results between the DE and DSM datasets, a significant difference in classification quality can be observed. This may be due to several factors. First of all, we can note the sample size, which was significantly smaller in the DSM dataset. Anyway, almost all machine learning models, regardless of the selected features, can show low accuracy on a small amount of data. On the other hand, if we refer to the study (Ignatiev et al., 2022), where less stringent restrictions on text volume and temporal proximity to questionnaire screening dates were applied to the same raw data, the results were still not very high: about 60% F1-macro for psycholinguistic features and TF-IDF features.\nSecondly, the problem may lie in the format of the texts from the social network. The texts in DSM are concatenated together with a collection of posts from users' personal pages, and they mostly lack coherent logic and cohesion in the resulting text. Even with the 6,000 character limit, the standard deviation in word count in this data is on the order of 300 words, while in DE the similar value is around 100. Thus, DSM is a much noisier dataset, which can strongly affect the quality of classification with psycholinguistic markers, where the values of some markers can be affected by the volume of text analyzed, even considering their normalization with respect to the text volume.\nFinally, the way in which the target class of pathology is defined can be of great importance. Although a widely used and validated psychological questionnaire was used for the DSM dataset, it is not possible to directly compare its results with a clinically confirmed diagnosis. The same findings can be outlined in work that criticizes the way in which social media users are labeled for mental illness by indirectly affiliating or self-identifying mental ill-health (Ernala et al., 2019). In favor of the significance of this factor is also the fact that TF-IDF-based features performed significantly better on DE data than on DSM, although, unlike psycholinguistic markers, they do not have an initial specialization for detecting signs of mental ill-health.\nTo take into account all of the aforementioned considerations, we applied LLMs to this task and showed, that the bigger model is able to partially overcome these issues. While RuBERT and the model trained on linguistic features both show comparatively low results, the evaluated in 5-shot format model performs significantly better. The same outcomes are observed in the experiments with anxiety datasets."}, {"title": "AL, AD, and AC Datasets", "content": "Turning to the anxiety detection task, the difference between the AutoML-based and encoder-based models is only noticeable on the picture description (AD) dataset, where the accuracy of the RuROBERTa model was 55% F1-macro. Overall, it can be said that none of the non-LLM models performed well in this task. However, LLMs performed significantly better on these tasks. Models with LoRA in general perform better than encoder-based and AutoML-based models, but the best performance is achieved by 0-shot and 5-shot prompting. Again, this can be linked to the complexity of the domain and the small amount of training data, which makes it hard to finetune a model.\nIn general, the LLMs outperform other models on all datasets, but on the DE and DSM, the gap between various types of models is smaller, leaving the usage of non-LLM models reasonable for some specific cases."}, {"title": "Classification on D-all and A-all Datasets", "content": "We combined the depression and anxiety datasets into pooled datasets D-all (DE and DSM) and A-all (AL, AD, and AC). Classification performance of the best models on D-all and A-all combined data presented in Table 3.\nThe LLMs performed better than other models on both D-all and A-all. It is also noticeable that methods with model tuning work better on D-all, while prompting shows better results on A-all. It can be explained with significant genre and length variations on A-all datasets, so the general-purpose models perform better than finetuned ones due to the complexity of the data."}, {"title": "Classification of DSM Data with Best Models from DE", "content": "The classification results of models that demonstrated the best performance on the DE dataset applied to DSM test data are shown in Table 4.\nThis experiment shows that models trained on essays from depressed subjects cannot be used to detect depression from social media texts from users who have taken a depression questionnaire and received a high score. Similar findings were shown in (Ernala et al., 2019), but with a reverse logic of the experiments.\nSuch results may also be just due to the fact that essay texts and collections of social network posts are very different in genre. A sample of social network users with clinically diagnosed depression would be needed to clarify this issue."}, {"title": "Conclusion", "content": "In this paper, we have investigated the effectiveness of traditional machine learning methods and LLMs on the task of detecting depression and anxiety. The results obtained in our work establish the new state-of-the-art on five Russian-language datasets.\nOur investigation shows that psycholinguistic features can produce results at the level of encoder-based models when texts from individuals with clinically diagnosed depression are used for training. BERT models, in turn, perform better on noisy text data, where examples from the training sample may vary widely in text length or genre. LLM-based models performed best on all five different mental health datasets. Even without fine-tuning, LLMs usually demonstrate relatively high performance.\nIn response to RQ1, the experimental results indicate that LLM-based models have high potential for detecting mental disorders from texts.\nIn response to RQ2, the findings reveal that models trained on essays from depressed individuals are not effective for detecting depression in social media texts from users who have completed a depression questionnaire and scored high."}, {"title": "Limitations", "content": "The main limitation of this paper is that it is impossible to share all the raw texts from used datasets, as they are all distributed under different terms.\nThe amount of sample data used for predicting anxiety is small, which does not allow us to adequately judge the possibility of predicting anxiety in Russian text using machine learning methods. Although the results on all the anxiety datasets used show poor accuracy, perhaps a very different scale of data is needed for this task. The paper does not discuss the differences between the results inside one group of the models in detail as this was not the aim of the paper.\nWe conducted the experiments only for the Russian due to the poor availability of related datasets for other languages. However, the used methods in general are language agnostic so the results could be extended to other languages. The overall result about LLMs as the best-performing models matches similar studies for English on closed data."}, {"title": "Ethical Considerations", "content": "The problem discussed in this paper is the sensitive issue of mental health. To avoid any possible harm, we did not fully open-source used datasets. All shared data does not contain any information that names or uniquely identifies individuals, nor does it contain offensive content. The examined models for the detection of mental disorders do not aim to replace a professional physician, on the contrary, these models are intended to support a human expert."}]}