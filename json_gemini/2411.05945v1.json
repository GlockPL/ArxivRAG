{"title": "NEKO: Toward Post Recognition Generative Correction\nLarge Language Models with Task-Oriented Experts", "authors": ["Yen-Ting Lin", "Chao-Han Huck Yang", "Zhehuai Chen", "Piotr Zelasko", "Xuesong Yang", "Zih-Ching Chen", "Krishna C Puvvada", "Szu-Wei Fu", "Ke Hu", "Jun Wei Chiu", "Jagadeesh Balam", "Boris Ginsburg", "Yu-Chiang Frank Wang"], "abstract": "Construction of a general-purpose post-\nrecognition error corrector poses a crucial ques-\ntion: how can we most effectively train a model\non a large mixture of domain datasets? The\nanswer would lie in learning dataset-specific\nfeatures and digesting their knowledge in a sin-\ngle model. Previous methods achieve this by\nhaving separate correction language models, re-\nsulting in a significant increase in parameters.\nIn this work, we present Mixture-of-Experts as\na solution, highlighting that MoEs are much\nmore than a scalability tool. We propose a\nMulti-Task Correction MoE, where we train the\nexperts to become an \u201cexpert\u201d of speech-to-text,\nlanguage-to-text and vision-to-text datasets by\nlearning to route each dataset's tokens to its\nmapped expert. Experiments on the Open ASR\nLeaderboard show that we explore a new state-\nof-the-art performance by achieving an aver-\nage relative 5.0% WER reduction and substan-\ntial improvements in BLEU scores for speech\nand translation tasks. On zero-shot evaluation,\nNeKo outperforms GPT-3.5 and Claude-Opus\nwith 15.5% to 27.6% relative WER reduction\nin the Hyporadise benchmark. NeKo performs\ncompetitively on grammar and post-OCR cor-\nrection as a multi-task model.", "sections": [{"title": "1 Introduction", "content": "Human recognition (Biederman, 1987; Juang and\nFurui, 2000; Kanwisher et al., 1996) capabilities\nspan multiple modalities, including speech recog-\nnition, visual patterns, and extensions to semantic\nand textual interpretations. These faculties, how-\never, are not infallible and often incorporate mis-\nrecognition errors. Despite these imperfections,\nhumans efficiently communicate using speech, lan-\nguage, or facial expressions.\nFor instance, two non-native speakers (Lev-Ari,\n2015; Valaki et al., 2004) can often achieve mutual\nunderstanding through this imperfect recognition"}, {"title": "2 Related Work", "content": "Language Modeling and Generative Error Cor-\nrection Neural correction LMs have been widely\nused for end-to-end (E2E) models for text error\ncorrection or normalization for both ASR (Jelinek,\n1990; Irie et al., 2016; Zhang et al., 2019a,b; Guo\net al., 2019) and OCR (Plamondon and Srihari,\n2000; Sabir et al., 2017; Fogel et al., 2020). These\nmodels often use beam search to generate new esti-\nmates, and can usually handle text normalization\nand denormalization or spelling errors. To utilize\nthe abundant textual data to improve ASR, neu-\nral LMs are integrated to E2E models using shal-\nlow fusion or rescoring over acoustic model' like-\nlihood confidence (Kannan et al., 2018; Salazar\net al., 2020; Yang et al., 2021).\nIn addition to using only textual hypotheses for\ncorrection, deliberation models (Hu et al., 2020;\nWang et al., 2022; Hu et al., 2023a) utilizes both\naudio and text for generating new ASR hypothe-\nses. While the aforementioned methods have drawn\nmuch attention in the past, recent advances focus\non using pretrained textual LLMs to benefit E2E\nASR (Song et al., 2023; Hu et al., 2023c), or sim-\nilar to deliberation, employing both speech and\ntext for speech understanding by prompting (Gong\net al., 2023) and joint speech and language founda-\ntion models (SLMs) (Wang et al., 2023). On the\nother hand, text-to-text based GEC (CHEN et al.,\n2023; Yang et al., 2023; Radhakrishnan et al., 2023;\nChan et al., 2023) have been recently introduced\nto ASR, ST, image captioning, and video summa-\nrization to set up superior performance 1. These\nGEC approaches take the text output as an input\nto large language models (LLMs), enabling them\nto perform zero-shot typo correction by genera-\ntively refining the final recognition results (CHEN\net al., 2023; Yang et al., 2023; Radhakrishnan et al.,"}, {"title": "3 Method", "content": "3.1 Mixture-of-Experts (MoE)\nOur method, NEKO, is based on a Transformer ar-\nchitecture (Vaswani et al., 2017) with modifications\nsimilar to those described in Jiang et al. (2023). The"}, {"title": "3.2 Task-Oriented Expert Assignment", "content": "The key idea of NEKO is to assign each expert to a\nspecific task during training. Given a set of tasks\nT = {T1, T2, ..., Tm}, we define a mapping func-\ntion f : T \u2192 {1, 2, ..., n} that assigns each task\nto a unique expert. During training, for an input\ntoken x from task T\u2081, we deterministically route x\nto the expert f (T) in addition to the top-1 expert\nselected by the gating network. This ensures that\neach expert learns task-specific features while still\nallowing for knowledge sharing through the gating\nnetwork. Formally, the output of the MoE layer for\nan input token x from task Ti during training is:\ny = G(x)f(T\u2081) \u00b7 Ef(T)(x)+G(x) top1. Etop1(x), \n(3)\nwhere top1 = arg maxj\u2260f(T\u2081) G(x)j is the index\nof the top-1 expert selected by the gating network,\nexcluding the task-specific expert f(Ti).\nDuring inference, we do not assume knowl-\nedge of the specific task an input token belongs\nto. Instead, we route each token to the top-K\nexperts selected by the gating network based on\ntheir predicted probabilities. This approach allows\nthe model to leverage the task-specific knowledge\nlearned by the experts during training while still"}, {"title": "3.3 Training Objective", "content": "We train NEKO on a mixture of error correc-\ntion datasets D = {D1, D2, ..., Dm}, where each\ndataset Di corresponds to a specific task Ti. The\ntraining objective is to minimize the negative log-\nlikelihood of the target sequences:\nm\nL= -\u03a3\u03a3logp(y|x, T\u2081), (4)\ni=1 (x,y)\u2208Di\nwhere x is the input sequence (e.g., ASR hy-\npotheses, OCR output), y is the target sequence\n(e.g., ground-truth transcription, corrected text),\nand p(yx, Ti) is the probability of the target se-\nquence given the input sequence and the task\nprompt (Figure 3.) By jointly training on mul-\ntiple error correction datasets with task-oriented\nexpert assignment, NEKO learns to capture task-\nspecific features while allowing for knowledge\nsharing across tasks through the shared gating net-\nwork and other model components."}, {"title": "4 Experiments", "content": "4.1 Training and Evaluation Datasets\nASR To assess the ability to handle diverse\nand noisy real-world speech, we use the Open\nASR Leaderboard (Gandhi et al., 2022; Srivastav\net al., 2023) for ASR evaluation, which comprises\nnine diverse datasets spanning various domains\nand speaking styles. These include LibriSpeech\n(Panayotov et al., 2015), Common Voice 9 (Ardila\net al., 2020), VoxPopuli (Wang et al., 2021), TED-\nLIUM (Hernandez et al., 2018), GigaSpeech (Chen\net al., 2021), SPGISpeech (O'Neill et al., 2021),\nEarnings-22 (Del Rio et al., 2022), and AMI (Car-\nletta, 2007; Renals et al., 2007), as one most rep-\nresentative benchmark due to its scale and data\ndiversity. We include the training set of above 8\ndatasets for NeKo training. We use the word error\nrate as the evaluation metric for ASR.\nST and MT For the speech translation error cor-\nrection task, we use the subset of the HypoTranslate\ndataset (Hu et al., 2024b) for training and evalua-\ntion. This dataset includes translation results from\nFLEURS (Conneau et al., 2022), CoVoST-2 (Wang\net al., 2020), and MuST-C (Di Gangi et al., 2019),\ncovering a range of languages such as Spanish,\nFrench, Italian, Japanese, Portuguese, Chinese, and"}, {"title": "4.2 Task-Specific Recognition Systems and\nBaselines", "content": "ASR We compare against state-of-the-art ASR\nmodels, Distil-Whisper-V2-Large(Gandhi\net al., 2023),\nWhisper-V2-Large,\nWhisper-V3-Large (Radford et al., 2022),\nCanary(NVIDIA, 2024) without applying GEC\nmethod. A end-to-end ASR-LLM, SALM (Chen\net al., 2024b), is also compared. For all Cascaded\nASR+GEC Methods, the task-specific system is\nthe Canary model. This model transcribes the\nspeech data and generate 5-best hypotheses for\neach utterance using temperature-based sampling\n(Ackley et al., 1985) with p = 0.3. This allows us\nto capture a diverse set of potential transcriptions\nfor each utterance, which can then be fed into our\nerror correction model."}, {"title": "4.3 Post-recognition LLMs Setup", "content": "We implement NEKO using the Transformer archi-\ntecture (Vaswani et al., 2017) and fine-tune both\ndense and MoE models for comparison. For dense\nmodels, we fine-tune Gemma 2B (Team et al.,\n2024) and Mistral 7B (Jiang et al., 2024b). For\nMoE models, we fine-tune Gemma 8x2B2 and Mix-\ntral 8x7B without applying our task-oriented expert\nassignment. We explore the Branch-Train-Mix ap-\nproach (Sukhbaatar et al., 2024), which involves\nbranching from the Mistral 7B model to an 8x7B\nMoE model as one competing setup. To investigate\nthe scalability of our method, we design NEKO to\nthree different sizes of MoE models: Gemma 8x2B,\nMixtral 8x7B, and Mixtral 8x22B. We further com-\npared low-rank adaptation (LoRA(Hu et al., 2021))\nwith full fine-tuning (FFT) on 8x7B MoE setup.\nFor MoE models, we use top-k routing as pro-\nposed in (Lepikhin et al., 2021) to balance the"}, {"title": "4.4 Post-recognition Generative Correction\nResults", "content": "ASR We first evaluate the zero-shot ability of\nNEKO on unseen domain compared to two general-\npurpose LLMs, including GPT-3.5 Turbo and\nClaude-Opus\u00b3. With a task-specific recognition\nbaseline of Whisper-V2-Large (third column) in\nNEKO-MOE shows the best zero-shot\nability with a relative 22.3% average WER reduc-\ntion. GPT-3.5 Turbo and Claude-Opus have relative\n4.3% and 7.3% of zero-shot improvements, where\nNEKO consistently outperform their 5-shot ASR\ncorrection."}, {"title": "5 Conclusion", "content": "In this work, we proposed NEKO, a multi-task\nGER approach that leverages task-oriented MoEs\nto handle diverse tasks. NEKO assigns each ex-\npert to a specific dataset during training, enabling\nthe experts to capture task-specific features while\nallowing knowledge sharing through the gating net-\nwork. Our results show that task-oriented expert\nassignment is a promising approach for multi-task\nlearning in error correction and other natural lan-\nguage processing tasks. By aligning experts with\ndatasets, NEKO can effectively capture the nuances\nand specificities of each task while benefiting from\nthe shared knowledge learned by the gating net-\nwork and other model components. Future work in-\ncludes exploring more advanced expert assignment\nstrategies, such as dynamically assigning experts\nbased on the input characteristics. Investigating\nthe interpretability of the learned expert represen-\ntations and routing decisions would be important\nnext step upon the open NEKO models."}, {"title": "Limitation", "content": "We aim to provide a transparent and comprehensive\nunderstanding of the current scope of NEKO, and\npave the way for future research to further improve\nthe NEKO model.\nDataset Diversity and Size and Assumptions in\nError Distribution This study addresses a mix-\nture of error correction tasks, including ASR, ST,\nOCR, and TEC, using representative task-specific\ndatasets such as LibriSpeech for ASR, CoVOST\nfor ST, ICDAR 2019 for OCR, and CoNLL-2014\nfor TEC. While these datasets are widely recog-\nnized benchmarks, they may not cover all possible\nerror correction scenarios, particularly those in-\nvolving more complex or less common error types\nfound in real-world data. This setup assumes that\nthe error distributions in the training datasets are\nrepresentative of those in real-world applications.\nConsequently, the performance of NEKOmight be\noverestimated for certain types of data not covered\nby these benchmarks, affecting the generalizabil-\nity of the results to more diverse and noisy real-\nworld scenarios. Future research should include a\nbroader range of datasets, particularly those with\nmore diverse and challenging error types, and in-\nvestigate methods to dynamically adapt to varying\nerror distributions, possibly through online learn-\ning (Yasunaga et al., 2021) or domain adaptation\ntechniques (Khurana et al., 2021), to better evaluate\nthe robustness and generalizability of the model.\nEthical and Societal Considerations The study\ndoes not extensively address the ethical and soci-\netal implications of deploying NEKO in real-world\napplications. There could be unintended conse-\nquences, such as biases in error correction or mis-\nuse of the technology in sensitive applications. Fu-\nture work should include a thorough analysis of the\nethical and societal impacts of the model, along\nwith strategies to mitigate potential negative conse-\nquences. This could involve incorporating fairness\nand bias detection mechanisms (Liu et al., 2022)\ninto the model to ensure responsible and ethical\ndep\nBoarder Impacts The NEKO model's applica-\ntion of MoE for multi-domain and multi-task error\ncorrection has the potential to significantly enhance\nautomated system's performance across various do-\nmain"}, {"title": "A Appendix", "content": "Prompt Format We provide detailed correction example per [TASK] and actual prompt format of\nINPUT: used in the our experiments for qualitative studies as shown in Figure 3. For instance, each\ntask will have a specific task-activation prompt format, where ASR, ST, and MT would be based on the\nsampling or beam search results. On the other hand, OCR and TEC will use input texts for end-to-end\nmapping.\nCorrection Examples We randomly select post-recognition example by NEKO. In Figure 4, a long\nform ASR output has been selected and it remain the top 1-best correction with NEKO. or the ST and\nMT correction result in Figure 5 and in Figure 6, although the post-NEKO corrected output does not\nperfectly align with the ground truth, it boosts the general semantic meaning, as reviewed by native\nspeakers. Meanwhile, the OCR and TEC correction results in Figures 7 and 8 demonstrate various\ntypes of corrections, such as pattern-wise character misrecognition and understanding-based coherence\nimprovements.\nAdditional Discussion on Human Recognition from Speech and Text Inputs Human recognition\n(e.g., speech, optical character, text translation) and has naturally evolved to excel at recognizing and\nunderstanding speech in a wide range of real-world scenarios (He et al., 2019; Deng et al., 2013). However,\nthe field of automatic speech recognition (ASR) has traditionally concentrated on training and evaluating\nmodels on specific datasets (Chan et al., 2016; Watanabe et al., 2017). These models have shown limited\nadaptability to new environments (Yang et al., 2021; Du et al., 2016; Hu et al., 2024a), leading to decreased\naccuracy and practicality in real-world settings. Recognizing the challenges posed by single dataset models\nand the availability of diverse datasets collected over time, unified models are being developed that merge\ninformation from multiple datasets into a single framework (Barrault et al., 2023a). While Grammatical\nError Correction (TEC) has been actively explored (Yang et al., 2023), ASR error correction is distinct\ndue to the arbitrariness of spoken language (Aks\u00ebnova et al., 2021), requiring efforts from both speech,\nNLP, and cognitive science communities as one human recognition example shown in Figure 9.\nTask-Oriented Inference for Mixture of Expert Models During inference, the Neko-model utilizes\ntop-2 expert routing, instead of just top-1. Our pilot studies showed that top-1 routing indeed led to worse\nperformance due to limited knowledge sharing.\nUsing more than two experts (e.g., top-3 or higher) diverged from the training setup and increased\ninference costs (ranging from 23.5% to 75.5%) without significant gain (i.e., a relative difference of less\nthan 0.06%).\nFuture Model Maintenance Plan and ASR Community For ASR tasks, we used Canary-v0, Whisper-\nseires, and SeamlessM4T to decode textual hypotheses data. For Whisper, we included it as a widely-used\nbaseline, but our key comparisons are to other GEC methods also using Whisper (e.g. GenTranslate). Open\neco-system, including ESPnet (Watanabe et al., 2018) and SpeechBrain (Ravanelli et al., 2021) models,\nare also our interests to be adapted as first-pass ASR in the open code base. This will provide a more\ncomprehensive evaluation across model types. In general, NeKo's post-ASR correction improvements are\nconsistent across datasets and first-pass models, suggesting the benefits generalize beyond model-specific\n(i.e., Canary, Whisper, or SeamlessM4T) 's strengths as the initial medical term correction results shown\nin Figure 10."}]}