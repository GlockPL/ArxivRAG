{"title": "Scaling Smart: Accelerating Large Language Model Pre-training with Small Model Initialization", "authors": ["Mohammad Samragh", "Iman Mirzadeh", "Keivan Alizadeh Vahid", "Fartash Faghri", "Minsik Cho", "Moin Nabi", "Devang Naik", "Mehrdad Farajtabar"], "abstract": "The pre-training phase of language models often begins with randomly initialized parameters. With the current trends in scaling models, training their large number of parameters can be extremely slow and costly. In contrast, small language models are less expensive to train, but they often cannot achieve the accuracy of large models. In this paper, we explore an intriguing idea to connect these two different regimes: Can we develop a method to initialize large language models using smaller pre-trained models? Will such initialization bring any benefits in terms of training time and final accuracy? In this paper, we introduce HyperCloning, a method that can expand the parameters of a pre-trained language model to those of a larger model with increased hidden dimensions. Our method ensures that the larger model retains the functionality of the smaller model. As a result, the larger model already inherits the predictive power and accuracy of the smaller model before the training starts. We demonstrate that training such an initialized model results in significant savings in terms of GPU hours required for pre-training large language models.", "sections": [{"title": "Introduction", "content": "Modern language models are very large, and training them is expensive [Kaplan et al., 2020, Rae et al., 2021, Hoffmann et al., 2022]. Experimenting with such models can be time-consuming and financially burdensome due to the high monetary cost. For instance, training a 12-billion-parameter model requires approximately 72,000 GPU hours [Biderman et al., 2023]. The total training cost from scratch can be expensive given current pricing of public cloud compute [Sevilla et al., 2022, Cottier et al., 2024]. Moreover, training can fail for reasons such as improper learning rate tuning, hardware failures, or loss divergence [Narayanan et al., 2021, Dubey et al., 2024]. Even with careful planning, robust engineering, and thorough testing to mitigate these failure risks, the monetary cost remains staggering.\nWhile small language models are less costly to train and impose lower financial and environmental burdens during research and development, they often lack the desired level of accuracy. This situation leaves industries and businesses that prioritize performance with no choice but to scale up and utilize larger models. However, to address the prohibitive costs of training large language models from scratch, one effective strategy is to begin with a small language model and gradually expand its parameter capacity. This approach, known as model growth in contemporary literature, explores scaling up models from modest beginnings [Chen et al., 2015, Du et al., 2024]."}, {"title": "Methodology", "content": "Our goal is to design an oracle called HyperCloning that transfers the knowledge from a small pretrained language model to a larger model that requires training. To ensure the effectiveness of HyperCloning, we established several design goals:\n\u2022 Expansion Dimension: The larger network should have larger hidden dimensions compared to the smaller network, while maintaining the same number of layers in both networks.\n\u2022 Function Preservation: After converting the smaller model to its equivalent larger model, the logits in the final layers of both networks should match.\n\u2022 Low Compute Overhead: The conversion process from the smaller model to the larger model should be straightforward, avoiding heavy computations or iterative updates.\n\u2022 Unchanged Training Loop: For ease of deployment, the training loop should remain unchanged. The only modification should be in the network initialization.\nIn contrast to the mainstream model expansion approaches that increase the depth [Gong et al., 2019, Samragh et al., 2023, Yang et al., 2020, Karp et al., 2024, Li et al., 2023, Wang et al., 2023a], the first criteria targets a complementary techniques that can be accompanied by any of these methods to provide a full recipe for model scaling. Width scaling can be beneficial for increased model accuracy, robustness, and inference efficiency, compared to solely increasing depth. The second criterion gives the model a warm-start by ensuring that the larger model performs at least as well as the pretrained smaller model in the begining of training, leading to faster convergence and better final accuracy. As we'll see, our approach, also satisfies the third and fourth criteria, which are essential for maintaining efficiency and facilitating adoption in LLM training. These differentiate HyperCloning with expansions methods that use techniques such as distillation to transfer knowledge [Xu et al., 2024, Zhong et al., 2023], as they usually require changing the training setup.\nLet $x_s \\in \\mathbb{R}^d$ be a hidden representation in the source (small) network. We achieve $x_D \\in \\mathbb{R}^{nd}$, the n-fold cloned version of $x_s$, by stacking n copies of $x_s$ and denote it as $x_D = [x_s,...,x_s]^T$. The main idea of HyperCloning is to establish the destination (large) network such that its hidden representations are cloned versions of the source (small) network. Consider a linear (fully connected) layer in the source network with weight parameter $W_s$ and bias parameter $b_s$. The goal of HyperCloning is to obtain the weight $W_D$ and bias $b_D$ in the target network such that the input and output vectors in the target model are cloned versions of those in the source network. Depending on which of the input/output dimensions are expanded, there could be three different cases shown in Figure 3. Please refer to Appendix A for more specific details on the initializations for linear layers, attention layers, normalization layers, and positional embeddings."}, {"title": "Experiments", "content": "Model Architectures. We perform experiments with three open-source benchmarks: OPT [Zhang et al., 2023], Pythia [Biderman et al., 2023], and OMLO [Groeneveld et al., 2024]. We choose OPT-350M, Pythia-460M, and OLMO-1B as the base pretrained models. Using HyperCloning, we then construct three larger architectures as destination networks: OPT-1.3B, Pythia-1.4B, and OLMO-2.9B. Refer to Appendix B for more information about model architectures, training dataset, and training hyperparameters."}, {"title": "Results Overview", "content": ""}, {"title": "Comparison to Random Initialization", "content": "In this section, we compare the training convergence of the studied models in two scenarios: (i) random initialization, which is the standard process for training language models, and (ii) initialization with HyperCloning from a base model. In both cases, all other hyperparameters were kept identical, including learning rate, optimizer type, number of GPU nodes, batch size, context size, and order of training data.\nWe compute the models' accuracy using the Harness framework [Gao et al., 2023], an open-source and widely-used tool for LLM evaluation. Accuracies are measured over 10 different tasks, and the final accuracies for both random initialization and HyperCloning are presented in Figure 4. As shown, HyperCloning significantly improves the accuracy of the models after convergence.\nAdditionally, we measure the average accuracy over the 10 tasks and present its trend during training in Figure 2, which we present early in the paper. As observed, HyperCloning enables the network to reach the final accuracy of the random initialization baseline much faster, with a speedup ranging from 2.2x to 4x across different model types. The better final accuracy and faster convergence achieved by HyperCloning can be attributed to the transfer of knowledge from the base model. For example, the base model for the OLMO architecture was already trained on 2.4T tokens, and this knowledge was transferred to our model before training started. Note that the base models are freely available; we simply downloaded them from HuggingFace. In practice, HyperCloning can leverage previously trained models, thus offering a cost-saving advantage. Consequently, the model initialized with HyperCloning begins with high accuracy and can converge to a better solution with significantly fewer training tokens (i.e., 250B tokens rather than 2.4T).\nOne notable observation is that models initialized with HyperCloning tend to exhibit catastrophic forgetting at the beginning of training. This is evident in the training curve for the OLMO benchmark. However, our experiments show that with sufficient training, this forgetting can be compensated for. Despite the initial catastrophic forgetting, HyperCloning still outperforms random initialization by a large margin. Understanding the underlying causes of catastrophic forgetting, identifying strategies to mitigate it, and exploring why HyperCloning continues to outperform random initialization despite its occurrence are valuable avenues for future research. We believe these areas hold great potential for further enhancing our method."}, {"title": "Analyzing HyperCloning: Weight Symmetry", "content": "Recall that for an n-fold cloning, the target weights in the target network are initialized with blocks of source weights normalized by n. This means that the weights in the target network have a standard deviation that is $\\frac{1}{n}$ of the standard deviation of the weights in the source network. Compared to existing work [Wang et al., 2023b, Chen et al., 2015], Our approach beneficial because it maintains the standard deviation requirement proposed by [Glorot and Bengio, 2010].\nHowever, HyperCloning initializes parts of the weight parameters as duplicates of one another. In [Wang et al., 2023b], authors note this phenomenon and raise the concern that the duplicated neurons/weights may continue being the same throughout training and not learn any independent information. Intuitively, this means that the model's parameters are not fully utilized immediately after HyperCloning is applied. In our work, however, we note that this phenomenon does not happen, likely due to the randomness introduced by different mechanisms such as dropout. Here we analyze these weight patterns and study their evolution during training. First, let us define a metric to assess the amount of symmetry in a cloned matrix. Looking at Figure 3, case 3, each row of a 2-fold cloned matrix contains two identical horizontal vectors. We can compute the cosine similarity of these two vectors as a measure of similarity. By repeating this computation for all rows, we can obtain an average cosine similarity. This average cosine similarity serves as a metric indicating how similar the vectors in the matrix are.\nFor an n-fold cloning, the target weights in the target network are initialized with blocks of source weights normalized by n. Consequently, the weights in the target network have a standard deviation that is $\\frac{1}{n}$ of the standard deviation of the source network weights. This approach aligns with the standard deviation requirement proposed by [Glorot and Bengio, 2010] and offers benefits over existing methods like those in [Wang et al., 2023b] and [Chen et al., 2015].\nHowever, our method, HyperCloning, initializes parts of the weight parameters as duplicates of each other. As noted by [Wang et al., 2023b], this duplication raises concerns that the duplicated neurons or weights might not learn independently, potentially limiting the model's capacity to utilize all parameters effectively. Nonetheless, we observe that this issue does not occur in our implementation, likely due to the randomness introduced by techniques such as dropout.\nTo analyze the evolution of these weight patterns during training, we define a metric to assess the symmetry in a cloned matrix. In the 2-fold cloning scenario depicted in Figure 3, case 3, each row of the matrix contains two identical horizontal vectors. We measure the cosine similarity between these vectors for each row and calculate the average cosine similarity across all rows. This metric provides an indication of the similarity between the vectors in the matrix.\nFigure 5 shows the evolution of cosine similarities for several selected layers in our studied networks. Initially, the cosine similarities of all layers are 1, showing a complete symmetry in the weights. As training progresses, we observe that the cosine similarity decays in most layers. This suggests that the model is utilizing its effective parameter space during training. While this analysis provides insights into the evolution of model weights, further studies are worthwhile in the future."}, {"title": "Analyzing HyperCloning: Principal Components", "content": "Another way to analyze the convergence of HyperCloning is by examining the ranks of the weight matrices. Consider the weight matrices shown in Figure 3. Due to the replicating nature of our cloning algorithm, it is evident that the rank of the cloned matrix is at most equal to the rank of the base matrix. Essentially, the rank of the cloned matrix is half of its maximum possible value at initialization. This implies that, while the model has reasonable accuracy at initialization, it is not fully utilizing its capacity for making predictions. The concern is that the model might continue underutilizing this capacity even after training is completed. We demonstrate that this does not occur.\nIn Figure 6, we show the eigenvalues of several weight matrices within our OLMO-2.9B model before and after training, for both the randomly initialized model and the model initialized with HyperCloning. It can be seen that half of the singular values of the before training model initialized with HyperCloning are zero, whereas the randomly initialized model does not exhibit this behavior. However, after training, the model initialized with HyperCloning achieves similar high-rank weights to those in the randomly initialized model."}, {"title": "Alternative Expansion Methods", "content": "In our original formulation for the expanded weights, we proposed $W_L = \\begin{bmatrix} W_s & W_s \\\\ \\frac{W_s}{2} & \\frac{W_s}{2} \\end{bmatrix}$. However, this is not the only weight parameter configuration that can satisfy function preservation. In this part of our analysis, we empirically evaluate several strategies for initializing $W_L$ as follows:\n\u2022 Symmetric: $W_L = \\begin{bmatrix} W_s & W_s \\\\ W_s & W_s \\end{bmatrix}$\n\u2022 Diagonal: $W_L = \\begin{bmatrix} W_s & 0 \\\\ 0 & W_s \\end{bmatrix}$\n\u2022 Noisy symmetric: $W_L = \\begin{bmatrix} \\frac{W_s + \\eta_1}{2} & \\frac{W_s - \\eta_1}{2} \\\\ \\frac{W_s + \\eta_2}{2} & \\frac{W_s - \\eta_2}{2} \\end{bmatrix}$, where $\\eta_1$ and $\\eta_2$ are random noise tensors of the same shape as $W_s$.\n\u2022 Noisy diagonal: $W_L = \\begin{bmatrix} W_s + \\eta_1 & -\\eta_1 \\\\ -\\eta_2 & W_s + \\eta_2 \\end{bmatrix}$.\nNote that all of the above weight expansion strategies are function-preserving. Figure 7 shows the accuracy of each instantiation method. The noise values ($\\eta_1$ and $\\eta_2$) in these experiments are selected such that the signal-to-noise ratio is 10 dB. All cloning methods outperform random initialization. The diagonal variant achieves the smallest accuracy boost, likely due to the presence of zero values in the expanded weight matrices. The noisy diagonal version performs slightly better than diagonal; however, the symmetric and noisy symmetric methods stand out as the best. With symmetric expansion, the benefits of noise addition are minimal. Therefore, we opt for the noise-free version of the method to avoid having to tune an extra hyper-parameter, the signal-to-noise ratio."}, {"title": "Effect of base model accuracy", "content": "Next, we study the effect of the base model's performance on the target model's performance. For this study, we use different checkpoints from the OPT-350M base model, trained with 16, 32, and 64 billion tokens, respectively. We initialize the target OPT-1.3B model with each of these checkpoints. Another baseline is random initialization, bringing the total number of comparison baselines to four. We observe the training convergence in Figure 8. As seen, initializing with the base model improves accuracy compared to random initialization when any of the base checkpoints are used for cloning. Among the cloned networks, those initialized with a more accurate base network achieve better accuracy, especially at the beginning of the training. However, as training continues, the differences between the curves become smaller."}, {"title": "Effect of base model size", "content": "Next, we demonstrate the effect of the base model's size on the target network's convergence. We create the target model by doubling the hidden dimension size of OPT-1.3B, resulting in a model we call OPT-5.3B. This architecture can be initialized in two ways using HyperCloning: (i) with OPT-1.3B using 2-fold cloning, or (ii) with OPT-350M using 4-fold cloning. The convergence of these candidates, along with the network initialized randomly, is shown in Figure 9. As observed, initializing with either OPT-350M or OPT-1.3B achieves faster convergence compared to random initialization, with OPT-1.3B providing better convergence than OPT-350M. This is because OPT-1.3B is larger and more accurate than OPT-350M, thereby offering a superior initialization."}, {"title": "Related Work", "content": "A comprehensive study on related work in the network growth literature is available in [Du et al., 2024], which examines various growth strategies, including depth and width expansion. Their innovative approach to depth growth involves initializing a larger model by repeating block weights. This approach is also supported by the findings of other research work [Gong et al., 2019, Samragh et al., 2023, Yang et al., 2020, Karp et al., 2024, Li et al., 2023, Wang et al., 2023a]. For instance, [Samragh et al., 2023] demonstrates that, due to the presence of residuals in transformer architectures, blocks can be removed or duplicated to achieve superior initialization compared to random methods. In terms of width growth, [Du et al., 2024] explored several strategies, including directly copying weights, projecting weights to a larger dimension, initializing new weights to zero, and randomly initializing weights. Notably, both depth and width scaling strategies in their study do not preserve function properties. They concluded that depth growth achieves the best accuracies, while non-function-preserving width growth results in poorer performance. While their work provides valuable insights, our research takes a different direction by focusing on a function-preserving transformation in the width dimension. Further studies are necessary to fully understand the benefits of depth versus width scaling and function-preserving versus non-function-preserving transformations.\nWidth expansion was initially introduced by [Chen et al., 2015] for convolutional neural networks and later explored for BERT-style transformer models in [Chen et al., 2021]. Our work builds on these foundations by generalizing width expansion techniques to decoder-style transformers, which are increasingly utilized in modern large language models. Specifically, we extend the width expansion method to include attention layers, define essential cloning functions for position embeddings, and validate our approach through experiments on larger-scale models and datasets. These contributions advance the applicability of width expansion in contemporary transformer architectures.\nIn [Shen et al., 2022], the authors introduce a width expansion technique where the non-diagonal elements of the expanded weight matrices are initialized to zero. Our ablation studies indicate that this diagonal initialization can lead to slower convergence compared to our symmetric initialization method. In contrast, [Wang et al., 2023b] discuss that the symmetry of neurons in an expanded network suggests these neurons may not contribute independently to the model's learning. However, our experiments demonstrate that the symmetry in weights naturally breaks during training, potentially due to random operations such as dropout.\nWe further propose a function-preserving noise addition mechanism to intentionally break the symmetry in weights. Our findings show that this noise addition improves the model's convergence rate. Additionally, we analyze the eigenvalues of the expanded network's weights after training and find that their distribution closely resembles that of a network trained from scratch. This result suggests that the expanded network effectively utilizes its parameter space during learning, comparable to a network trained from scratch."}, {"title": "Conclusion", "content": "This paper introduces HyperCloning, a novel initialization strategy designed to transfer weights from a smaller, pretrained source model to a larger target model. The transfer process in HyperCloning is straightforward, effective, and preserves the model's functionality. By using this method, we achieve faster convergence and better final accuracy during language model training. In our experiments, HyperCloning accelerates training by 2-4 times. Additionally, we conducted ablation studies to explore the impact of the source model's architecture and different weight-cloning techniques on the target model's convergence."}, {"title": "Cloning Details", "content": "In this section we explain the cloning process for different layer types in detail. For simplicity, we consider a 2-fold expansion of the network but the method can be generalized to a generalized n-fold expansion."}, {"title": "Cloning Linear Layers", "content": "In general, there can be three different expansion cases for a Linear Layer show in Figure 3:\n\u2022 Case 1: Only the input is expanded: $x_D = \\begin{bmatrix} x_s \\\\ x_s \\end{bmatrix}$ and $y_D = y_s$. This may occur at any linear layer whose outputs are not expanded such as the unembedding layer.\n\u2022 Case 2: Only the output is expanded: $x_D = x_s$ and $y_D = \\begin{bmatrix} y_s \\\\ y_s \\end{bmatrix}$. This may occur at any linear layer whose inputs are not expanded such as the embedding layer.\n\u2022 Case 3: Both input and output are expanded: $x_D = \\begin{bmatrix} x_s \\\\ x_s \\end{bmatrix}$ and $y_D = \\begin{bmatrix} y_s \\\\ y_s \\end{bmatrix}$. This may occur at hidden linear layers which may include attention and/or feed-forward layers.\nThe expanded weight parameter is formed by stacking the original pretrained matrix in both rows and columns and normalizing the values by $\\sqrt{n}$, where n is the expansion factor in the input dimension. The expanded bias vector is created by repeating the original bias values n times. This formulation ensures that the outputs of the expanded linear layer are cloned versions of the original linear layer's outputs. More specifically:\n\u2022 Case 1: We initialize $W_D = \\begin{bmatrix} \\frac{W_s}{2} + \\eta_1 & \\frac{W_s}{2} - \\eta_1 \\end{bmatrix}$ and $b_D = b_s$, where $\\eta_1$ is a random tensor with reasonable magnitude. We then have:\n$y_D = W_Dx_D + b_D = \\begin{bmatrix} \\frac{W_s}{2} + \\eta_1 & \\frac{W_s}{2} - \\eta_1 \\end{bmatrix} \\begin{bmatrix} x_s \\\\ x_s \\end{bmatrix} + b_s = y_s$\n\u2022 Case 2: We initialize $W_D = \\begin{bmatrix} \\frac{W_s}{2} \\\\ \\frac{W_s}{2} \\end{bmatrix}$ and $b_D = \\begin{bmatrix} \\frac{b_s}{2} \\\\ \\frac{b_s}{2} \\end{bmatrix}$. We then have:\n$y_D = W_Dx_D + b_D = \\begin{bmatrix} \\frac{W_s}{2} \\\\ \\frac{W_s}{2} \\end{bmatrix} x_s + \\begin{bmatrix} \\frac{b_s}{2} \\\\ \\frac{b_s}{2} \\end{bmatrix} = \\begin{bmatrix} W_sx_s + b_s \\\\ W_sx_s + b_s \\end{bmatrix} = \\begin{bmatrix} y_s \\\\ y_s \\end{bmatrix}$\n\u2022 Case 3: We initialize $W_D = \\begin{bmatrix} \\frac{W_s + \\eta_1}{2} & \\frac{W_s - \\eta_1}{2} \\\\ \\frac{W_s + \\eta_2}{2} & \\frac{W_s - \\eta_2}{2} \\end{bmatrix}$ and $b_D = \\begin{bmatrix} \\frac{b_s}{2} \\\\ \\frac{b_s}{2} \\end{bmatrix}$, where $\\eta_1$ and $\\eta_2$ are a random tensors with reasonable magnitudes. We then have:\n$y_D = W_Dx_D + b_D = \\begin{bmatrix} \\frac{W_s + \\eta_1}{2} & \\frac{W_s - \\eta_1}{2} \\\\ \\frac{W_s + \\eta_2}{2} & \\frac{W_s - \\eta_2}{2} \\end{bmatrix} \\begin{bmatrix} x_s \\\\ x_s \\end{bmatrix} + \\begin{bmatrix} \\frac{b_s}{2} \\\\ \\frac{b_s}{2} \\end{bmatrix} = \\begin{bmatrix} y_s \\\\ y_s \\end{bmatrix}$"}, {"title": "Cloning Attention Layers", "content": "When cloning attention layers, there are two possibilities to expand a multi-head attention:\n\u2022 Expanding the dimension of each attention head: When increasing the head dimension, each of the query/key/value matrices can be treated as individual linear layers and expanded as explained in Figure 3. Let $q_s$ and $k_s$ represent the query and key values in the small network. Then the corresponding query and key values in the expanded network would be:\n$q_D = \\begin{bmatrix} q_s \\\\ q_s \\end{bmatrix}$ and $k_D = \\begin{bmatrix} k_s \\\\ k_s \\end{bmatrix}$\nThe attention computed in the small network is:\n$\\alpha_s = \\frac{q_sk}{\\sqrt{d}}$\nIn the expanded layer, the attention is computed as:\n$\\alpha_D = \\frac{q_Dk_D}{\\sqrt{2d}} = \\frac{q_sk + q_sk}{\\sqrt{2d}} = \\sqrt{2}\\alpha_s$\nTo make $\\alpha_D$ equal to $\\alpha_s$, we should scale the query value by $\\frac{1}{\\sqrt{2}}$. More generally, the expanded query weights should be scaled by $\\sqrt{\\frac{d_s}{d_D}}$, where $d_s$ and $d_D$ are the head dimensions in the original and extended layers, respectively.\n\u2022 Expanding the number of attention heads: This case is straightforward. We can simply duplicate the attention heads.\nIn both cases, the fully connected layer that follows the attention layer will also be expanded to increase the hidden representation's dimensionality."}, {"title": "Cloning Layer Norm", "content": "let $x_s$ be a hidden representation vector in the small network. Applying Layer Norm over this vector computes the following:\n$l(x_s) = \\frac{x_s - E(x_s)}{\\sqrt{var(x_s) + \\epsilon}} \\gamma_s + \\beta_s$\nWhen cloning layer norm, we expand the affine parameters (if any) as $\\beta_D = \\begin{bmatrix} \\beta_s \\\\ \\beta_s \\end{bmatrix}$ and $\\gamma_D = \\begin{bmatrix} \\gamma_s \\\\ \\gamma_s \\end{bmatrix}$. We then have:\n$l(x_D) = \\frac{\\begin{bmatrix} x_s \\\\ x_s \\end{bmatrix} - E(\\begin{bmatrix} x_s \\\\ x_s \\end{bmatrix})}{\\sqrt{var(\\begin{bmatrix} x_s \\\\ x_s \\end{bmatrix}) + \\epsilon}} \\begin{bmatrix} \\gamma_s \\\\ \\gamma_s \\end{bmatrix} + \\begin{bmatrix} \\beta_s \\\\ \\beta_s \\end{bmatrix} = \\begin{bmatrix} l(x_s) \\\\ l(x_s) \\end{bmatrix}$\nIn the above derivation, we used the fact that $E(\\begin{bmatrix} x_s \\\\ x_s \\end{bmatrix}) = E(x_s)$ and $var(\\begin{bmatrix} x_s \\\\ x_s \\end{bmatrix}) = var(x_s)$.\nIn general, repeating the weights and biases in the layer norm n-times will ensure that the output of the expanded layer norm is a cloned version of the output from the original layer norm. Similar argument is true for batch normalization, RMS normalization, and group normalization."}, {"title": "Cloning Positional Embedding Layers", "content": "For positional embedding, we need to define the n-times cloned equivalents. Let $P_s(x_{s,i}) \\in \\mathbb{R}^d$ denote the positional embedding of a pretrained small network. The n-times cloned positional embedding is defined as follows:\n$P_D(x_{D,i}) = \\begin{bmatrix} P_s(x_{s,i}) \\\\ \\vdots \\\\ P_s(x_{s,i}) \\end{bmatrix}$\nIn essence, the positional embedding of the expanded network is created by repeating the positional embedding of the small network n times. In our codebase, we define Pytorch equivalents of the expanded positional embedding layers when necessary."}, {"title": "Architectures and Training Details", "content": "The architectures of our etudied networks are summarized in Table 1. Among the target models, OPT-1.3B and Pythia-1.4B are already available through HuggingFace, providing us with a good baseline for comparison. OLMO-2.9B was not trained by the authors of [Groeneveld et al., 2024], and we are the first to train and evaluate it. We obtain the weight checkpoints of the base models from the HuggingFace repositories, except for OPT-350M, for which we train our own base model with 30B tokens. This is because the HuggingFace OPT-350M model has extra linear layers after the embedding layer and before the unembedding layer, which the target OPT-1.3B model does not have. With these benchmarks, we emulate three different scenarios:\n\u2022 OPT: The training dataset is the same for both the base and target model. The base model is trained with a relatively small number of tokens (30B).\n\u2022 Pythia: The dataset used for training the base model (Pile) is not available to train the target model. We use a different dataset (DOLMA) for training the target model. The base model was trained with a moderate number of tokens (250B).\n\u2022 OLMO: The training dataset is the same for both the base and target model. The base model is trained with a large number of tokens (2.4T).\nFor all experiments, we use the DOLMA dataset provided by the authors of [Groeneveld et al., 2024]. This dataset includes several open-source datasets and totals up to 2.4 trillion tokens. However, our training jobs do not process this many tokens due to the extensive cost. To ensure fair representation of all sub-datasets within DOLMA, we shuffled the data shards. The seed for random shuffling is kept the same across all our experiments to eliminate the impact of data ordering on our conclusions.\nFor all of our experiments, we use the AdamW optimizer with a weight decay of 0.05, $\\beta_1 = 0.9$ and $\\beta_2 = 0.999$. We use gradient accumulation with 16 steps to increase our effective batch size and the zero_2 gradient update algorithm [Rajbhandari et al., 2020] to reduce memory footpring. We apply a learning rate warm-up over 25,000 iterations to reach the maximum learning rate. Afterward, we decay the learning rate to 1/10th of its value until 2,500,000 iterations, after which the learning rate is kept constant. Our models are trained on 64 GPUs with varying batch sizes, context sizes, and learning rates summarized in Table 2."}]}