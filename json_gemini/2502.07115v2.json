{"title": "Online Scheduling for LLM Inference with KV Cache Constraints", "authors": ["Patrick Jaillet", "Jiashuo Jiang", "Chara Podimata", "Zijie Zhou"], "abstract": "Large Language Model (LLM) inference, where a trained model generates text one word at a time in response to user prompts, is a computationally intensive process requiring efficient scheduling to optimize latency and resource utilization. A key challenge in LLM inference is the management of the Key-Value (KV) cache, which reduces redundant computations but introduces memory constraints. In this work, we model LLM inference with KV cache constraints theoretically and propose novel batching and scheduling algorithms that minimize inference latency while effectively managing the KV cache's memory.\nWe analyze both semi-online and fully online scheduling models, and our results are three-fold. First, we provide a polynomial-time algorithm that achieves exact optimality in terms of average latency in the semi-online prompt arrival model. Second, in the fully online case with a stochastic prompt arrival, we introduce an efficient online scheduling algorithm with constant regret. Third, we prove that no algorithm (deterministic or randomized) can achieve a constant competitive ratio in fully online adversarial settings. Our empirical evaluations on a public LLM inference dataset, using the Llama-70B model on A100 GPUs, show that our approach significantly outperforms benchmark algorithms used currently in practice, achieving lower latency while reducing energy consumption. Overall, our results offer a path toward more sustainable and cost-effective LLM deployment.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) [Brown et al., 2020, Chowdhery et al., 2023, OpenAI, 2023, Kaplan et al., 2020, Wei et al., 2022] represent a significant advancement in artificial intelligence, enabling machines to generate human-like text across various languages and contexts. Trained on vast datasets, these models are becoming critical for applications such as chatbots [Anthropic, 2023, Character, 2021, OpenAI, 2019, 2023], search engines [Microsoft, 2023, Google, 2023, Komo, 2023, Perplexity, 2022, You.com, 2020], code assistants [Amazon, 2023, GitHub, 2021, Replit, 2018], and healthcare services [Cascella et al., 2023, Peng et al., 2023, Sallam, 2023].\nLLM Inference and the KV Cache. Despite their transformative potential, LLMs pose substantial computational challenges, particularly during the inference process where inputs are processed to generate responses. In LLM inference, a \u201cprompt\u201d is the input text provided to initiate a model's response generation. These prompts are broken down into smaller units called \u201ctokens\", which may"}, {"title": "Scheduling Optimization in LLM Inference.", "content": "In this work, we enhance the computational efficiency of LLM inference by optimizing its scheduling policy. Scheduling for LLM inference differs from classical scheduling problems primarily because of the bottlenecks introduced by the KV cache (i.e., the linear memory usage growth, and the KV cache's dynamic processing capacity).\n\u00b9We provide a more detailed background section on LLM inference in Section 2. This section explains in detail how a single worker processes inference for a single request and how batching is used to handle multiple requests efficiently."}, {"title": "1.1 Results Roadmap", "content": "In this work, we make the following contributions.\n(1) Mathematical model for online batching and scheduling in LLM inference with blow-up memory usage. While many studies have focused on improving LLM inference efficiency from an engineering perspective [Agrawal et al., 2023, Kwon et al., 2023, Patel et al., 2023, Pope et al., 2023, Sheng et al., 2023, Yu et al., 2022, Zhong et al., 2024], there is a lack of mathematical models and theoretical analyses in this domain. To address this gap, we propose a model (Section 3) for optimizing the batching and scheduling policy in LLM inference and develop scheduling algorithms with provable performance guarantees. Batching entails selecting which requests to process concurrently, while scheduling determines their timing.\n(2) Polynomial-time optimal algorithm for the semi-online setting. In Section 4, we first consider a simplified case as a warm-up, where all prompt requests are available at the beginning of the interaction. Although the arrivals are not online in this case, the prompts and their corresponding output tokens must still be processed sequentially; we term this scenario the semi-online setting. In this context, minimizing average end-to-end latency can be formulated as an integer programming problem. We develop a polynomial-time scheduling and batching algorithm without solving the integer programming, proving that it is exactly optimal.\n(3) Constant regret online algorithm for the stochastic setting. In Section 5, we examine an online stochastic arrival model over a discrete time horizon [0,T], where, in each period $t\\in [T]$, the number of arrivals follows an unknown distribution with bounded support. Each prompt request generates an uncertain number of output tokens according to an unknown probability distribution. In this stochastic setting, we measure the performance of our algorithm with a notion of regret defined as the expected difference between the average latency of an online algorithm and that of an offline algorithm with full hindsight, which has access to the complete arrival sequence and"}, {"title": "1.2 Other Related Work", "content": "Online Scheduling Problem. In standard online scheduling problems, a decision-maker needs to decide the optimal timing for job processing as jobs arrive sequentially. A common approach is to analyze the stochastic arrival model, where job arrivals are probabilistically distributed [Lattanzi et al., 2020]. This framework has inspired a range of algorithms that analyze the initial portion of arrivals to predict patterns, subsequently adjusting scheduling decisions based on this understanding [Balkanski et al., 2016, Vee et al., 2010, Devanur and Hayes, 2009, Cole and Roughgarden, 2014]. However, in practice, the arrival process may not follow a stationary distribution due to demand fluctuations. An alternative approach is the adversarial arrival model, where an adversary selects the worst-case input. This framework provides theoretical guarantees on an algorithm's performance under the worst case. Previous research, including work by Bartal et al. [1992], Karger et al. [1996], Albers [1997], Rudin III and Chandrasekaran [2003], has investigated online scheduling in adversarial contexts. Some studies extend beyond processing individual jobs, incorporating both batching and scheduling [Lucier et al., 2013, Im and Moseley, 2013, Liu and Lu, 2015, Li et al., 2020], where similar job types are grouped into batches and processed using greedy approaches. Yet, the unique demands of LLM inference, such as the fact that tokens need to be processed in a specific order, and the blow-up memory in the KV cache limit the applicability of these standard heuristics.\nLLM Inference. LLM inference is a developing field, and, to date, theoretical guidelines for optimizing LLM inference remain underexplored. However, numerous engineering-oriented studies are emerging within systems research. For instance, in scheduling, Patel et al. [2023] proposed \"Splitwise,\" and Zhong et al. [2024] introduced \u201cDistServe,\u201d; both systems split processing into initial prompt inputs for generating first output tokens and then continue with output tokens to produce additional tokens. Regarding batching approaches, works like Yu et al. [2022], Agrawal et al. [2023, 2024b] examine methods for statically or dynamically grouping pending requests for batch execution. Liu et al. [2024] boosts LLM inference efficiency by introducing multi-head latent attention, which reduces the KV cache through low-rank key-value joint compression. Agrawal et al. [2024a] introduce a publicly available simulator that models LLM inference processing times across different hardware configurations. This enables researchers to conduct LLM inference experiments without requiring access to expensive GPUs, facilitating methodological advancements and broader experimentation in the field. Zhu et al. [2023] assume the prompts are drawn from a fixed distribution and model the LLM inference problem as an online learning problem. However, the benchmark in Zhu et al. [2023] is the static optimal cache that is fixed over T and they derive O(VT) regret. In contrast, our algorithm compares against the stronger hindsight optimal algorithm that can be dynamic, and we derive a O(1) regret algorithm."}, {"title": "2 Background: Online Optimization in LLM Inference", "content": "This section provides some background in LLM inference under a single computational worker."}, {"title": "2.1 LLM Inference Process on a Single Request", "content": "We first demonstrate how a single GPU worker processes an incoming prompt, using the example prompt \"What color is the sky?\" from Lienhart [2023]. The workflow is illustrated in Figure 2."}, {"title": "2.2 Batching and Scheduling during the LLM Inference Process", "content": "In Subsection 2.1, we examined how a single worker processes individual requests. However, when multiple requests are in the queue, batching prompts together rather than handling them one-by-one improves GPU efficiency. This section explores the decision-making involved in batching and scheduling multiple requests.\nFigure 3 illustrates the online scheduling and batching process for two distinct prompts, P1 (\u201cWhat color is the sky?\") and P2 (\u201cHow are you doing?\"), during LLM inference. Initially, P1 is processed within its own batch. When P2 arrives, it must wait, as simultaneous processing of prompts is limited by worker availability. Once P1 is processed and generates its first output token, \"The,\u201d the worker batches the token and the prompt P2 together and processes. After P2 produces its first output token, \"I,\" and the output token \"The\" produces its next token \"sky,\" both tokens, \"sky\" and \"I,\" are then batched together for efficient token processing, facilitating the subsequent generation of \u201cis\u201d for P1 and \"am\" for P2."}, {"title": "2.3 Comparison with Previous Scheduling and Batching Problems", "content": "Though the scheduling problems have been studied extensively in the previous literature on computing resource allocation (e.g. Pinedo [2012]), our model is fundamentally different from the previous scheduling models. To be specific, due to the autoregressive LLM inference process described in 2.1, the memory size required by a prompt keeps growing during the inference procedure, which is fundamentally different from the previous scheduling models where the size (or the resource occupation) of a job is fixed. Therefore, even for the same scheduling policy, the performance will be different for the LLM inference scheduling model and the previous scheduling model with a fixed job size, which implies that previous approaches to find or approximate the optimal policies for classical scheduling models with a fixed job size cannot work for our model.\nTo see this point, suppose that we are following the principle of processing the shortest job first and there are two prompts P1 and P2, whose sizes eventually grow to $t_1$ and $t_2$. If $t_1 + t_2 > M$, where $M$ is the memory limit of the KV cache, then the two prompts cannot be processed at the same time in the classical scheduling model. However, this may not be the case for our LLM inference model since the sizes are varying over time. Suppose that the initial size for both prompts is the same, denoted by $s$, and it satisfies that $s \\leq t_1 < M/2$. The size of both jobs increases by 1 at each round until reaching $t_1$ and $t_2$. Then, we can process P1 and P2 at the same time until the time $t_1 - s$ where the size of both jobs increases to $t_1$, since the memory requirement for the two jobs is upper bounded $2t_1 < M$. After time $t_1 - s$, though the size of P2 may be larger than $t_1$, the prompt P1 has finished processing the required memory can be released to further processing P2."}, {"title": "3 Model", "content": "We study a batching and scheduling problem within a discrete time horizon for a single computational worker (GPU). The worker has a memory constraint $M > 0$.\u00b2 Let $I$ denote the instance consisting of unprocessed prompt requests arriving over the discrete time horizon. Following a popular, practical approach Agrawal et al. [2023], Ratner et al. [2022], Yen et al. [2024], An et al. [2024] that input prompts can be divided into equally sized small chunks, we assume that each prompt request is characterized by its size $s > 0$, defined as the number of words in the prompt chunk. Additionally, we assume that each prompt size is negligible relative to $M$,\u00b3 i.e., $s/M = \\epsilon \\rightarrow 0$. Each request $i$ has an associated response length $o_i$, indicating the number of tokens in its response for all $i \\in [n]$. We assume that $o_i$ can be accurately predicted upon the arrival of request $i$.\nPrompt Processing. Each prompt request is processed online and undergoes two primary phases on the GPU worker:\n1. Prompt Phase: The prompt is processed, and the initial output token is generated. During this phase, the memory required for prompt $i$ is $s$, accounting for the storage of key and value matrices for all tokens in the prompt within the KV cache.\n2. Token Phase: Subsequent tokens are produced sequentially.\u2075 In this phase, the memory needed to process the $j$th token of prompt $i$ ($j \\in [o_i]$) is $s + j$. This increment accounts for each new token's key and value, which adds 1 to the existing KV cache memory. Consequently, the peak memory usage for processing the final token of prompt $i$ reaches $s + o_i$. After the completion of the last token, the KV cache clears all related memory usage $s + o_i$ for that prompt.\n\u00b2M depends on the power of the GPU and the complexity of the large language model in use; we provide exact formulas for computing it in Section 7.\n\u00b3In practice, when performing inference on long-input prompts (s large), more powerful GPUs with larger KV cache capacity (M) are used to prevent integer allocation issues that arise when KV cache approaches its limit.\n\u2074Methods for high-accuracy prediction are introduced in Section 2.1. In the state-of-the-art approach, the prediction is accurate up to 80% Zheng et al. [2024].\n\u2075Detailed dynamics for processing a single prompt request are described in Section 2.1."}, {"title": "Batch Processing.", "content": "A batch may include any unprocessed prompt or output token of different requests. When a prompt request is added to a batch, its first output token is generated post-processing. Similarly, adding an output token to a batch results in the generation of the subsequent token upon batch completion. We assume each batch's processing time as one unit of time. The batching constraint ensures that for all ongoing prompt requests (those not fully processed or pending final output tokens), the total memory usage at any given moment does not exceed $M$. Formally, for each time period $t$, define $S(t)$ as the set of prompts that have been processed but whose final output tokens are pending. For each $i \\in S(t)$, let $a_i(t)$ represent the index of the output token of request $i$ awaiting processing at time $t$. Hence, for all $t$, it holds that: $\\sum_{i\\in S(t)} (s + a_i(t)) \\leq M$.\nPrompt Arrival Process. In this paper, we consider three different arrival models of the unprocessed prompts assigned by the global scheduler:"}, {"title": "Semi-Online Arrival.", "content": "We first propose a semi-online arrival model as a warm-up. Specifically, at $t = 0$, the worker receives $n > 0$ unprocessed prompt requests from the global scheduler. Since all prompts arrive at $t = 0$, the decision-maker has complete knowledge of $o_i, \\forall i \\in [n]$ at this initial time. Although the arrival process is fully offline, the sequential processing of prompts and tokens introduces elements of online decision-making, hence, we classify this as a semi-online model. We use this arrival model as a stepping stone to our fully online models."}, {"title": "Online Stochastic Arrival.", "content": "In this arrival model, at each round $t \\in [T]$, the number of arriving requests independently follows an unknown distribution $D$ with bounded support. We define $\\lambda > 0$ as the arrival rate, which is equal to the expectation of $D$. For each prompt request $i$, the output length $o_i$ is drawn from a probability distribution characterized by the cumulative distribution function (CDF) $F(.)$. Upon the arrival of request $i$, the decision-maker is still able to accurately predict the value of $o_i$. Both $\\lambda$ and $F$ need not be known."}, {"title": "Online Adversarial Arrival.", "content": "The final arrival model we consider is the online adversarial one. In this model, an adversary determines the online arrival process, including the total number of prompt requests $n$, which can be any positive integer. The adversary also decides the arrival time of each prompt and assigns an output length $o_i \\in [1, M - s]$ to each prompt request $i$. Once a prompt $i$ is revealed, the decision-maker gets access to the value of $o_i$.\nEvaluation Metrics. Our performance evaluation metric is the end-to-end latency, which is computed as: given an algorithm $A$, for each prompt request $i$ arriving at time $t_i$, the end-to-end latency for this request is $c_i(A) - t_i$, where $c_i(A)$ is the time the last output token for prompt $i$ is processed. We use $TEL(I; A)$ to represent the total end-to-end latency of algorithm $A$ for a request sequence $I \\in \\Omega$. Here, $\\Omega$ denotes the set of all possible request sequences, which includes every possible sequence of prompt sizes, and number of output generated tokens. Then, the total end-to-end latency across all requests from sequence $I$ is: $TEL(I; A) = \\sum_{i=1}^n C_i (A) - t_i$. We use $AEL(T; A) = TEL(I; A)/n$ to denote the averaged end-to-end latency of algorithm $A$ for a request sequence $I \\in \\Omega$. Our goal is to design an algorithm $A$ minimizing the average end-to-end latency."}, {"title": "4 Semi-Online Setting", "content": "In this section, we study the semi-online prompt arrival model, where $n$ prompts arrive at time $t = 0$. Assume that $\\{o_i\\}_{i\\in[n]}$ is a non-decreasing sequence. This is without loss of generality since"}, {"title": "Algorithm Overview.", "content": "At each round $t$, let $R(t)$ represent the set of all requests that have not yet been processed, while $S(t)$ denotes the set of requests that are currently in progress but not yet completed (i.e., some output tokens have been generated, but not all of them). Our algorithm prioritizes processing requests in $S(t)$ first. After processing all the requests currently in $S(t)$, there may still be unused memory in the KV cache, so our algorithm chooses subset of requests, $U(t) \\subset R(t)$ to add to the batch in order to maximize memory utilization and minimize total latency. To be more precise, our algorithm tries to process as many requests as possible within each batch; for that, it maximizes the number of requests in $U(t)$, provided that they satisfy memory constraints.\nSpecifically, for a subset $U \\subset R(t)$, let $t_{max}(U) := \\max_{i\\in U}\\{t+o_i\\}$ represent the maximum completion time for all requests in $U$ if they are added to the batch at time $t$. To ensure $U$ is feasible, the KV cache memory limit must not be exceeded at any $t' \\in [t, t_{max}(U)]$. This requires that:\n$\\sum_{i \\in S(t)} (s + t' - p_i) \\mathbb{1} \\{o_i\\geq t'-p_i\\} + \\sum_{i \\in U} (s + t' - t) \\cdot \\mathbb{1} \\{o_i\\geq t'-t\\} < M, \\qquad (1)$\nwhere $p_i$ is the starting time to process request $i$. The first sum accounts for memory usage from ongoing requests in $S(t)$, while the second captures new requests in $U$. As long as this inequality is satisfied for all $t' \\in [t, t_{max}(U)]$, $U$ is feasible to add to the batch. Thus, our selection rule is:\n$U^{(t)} = \\arg \\max_{U \\subset R^{(t)}} \\{|U|: \\text{inequality (1) is satisfied for all } t' \\in [t, t_{max}(U)]\\}. \\qquad (2)$\nTo maximize the number of requests in the batch at time $t$, we should add requests in ascending order of their indices because requests with shorter output lengths consume less memory over time, allowing more requests to fit within the memory We continue adding requests in this order, checking the feasibility condition of inequality (1). Importantly, we only need to check this constraint at the completion times of ongoing or new requests, specifically $p_j + o_j$ for $j \\in S(t) \\cup U(t)$. This is because (i) memory usage potentially peaks at these completion times, as a request's memory demand increases until it finishes, and (ii) since memory usage varies linearly between start and end times, satisfying the constraint at these peak points ensures feasibility throughout the interval. The complete algorithm is detailed in Algorithm 1."}, {"title": "ALGORITHM 1: Semi-Online Scheduling and Batching Algorithm", "content": "Initialize $R^{(1)} = \\{1,2,\\dots,n\\}$, and $S^{(1)} = ()$;\nfor $t$ in $1,2,\\dots,T$ do\n| If $t \\geq 2$, update $S^{(t)} = S^{(t-1)} \\cup U^{(t-1)}$;\n| If $t \\geq 2$, update $R^{(t)} = R^{(t-1)} \\setminus U^{(t-1)}$;\n| Set $U^{(t)} = \\emptyset$;\n| for each request $i \\in R^{(t)}$ in ascending order of $o_i$ do\n| | Set a time list $t' = p_j + o_j$ for $j \\in S^{(t)} \\cup U^{(t)}$;\n| | if all inequalities in (1) hold for time $t'$ then\n| | | Add request $i$ to $U^{(t)}$;\n| | else\n| | | Break the for loop;\n| Process the requests in $U^{(t)}$;"}, {"title": "Proposition 4.1.", "content": "Given that the memory limit of the KV cache is M, Algorithm 1 has a computational complexity of O(M\u00b2) at each round $t \\in [T]$.\nTheorem 4.2. Algorithm 1 is optimal in terms of average end-to-end latency.\nProof Sketch. The proof of Theorem 4.2 is a proof by contradiction. Assuming there exists an alternative schedule S that achieves a lower total latency than Algorithm 1 while satisfying all memory constraints, S could be derived by swapping a finite number of jobs in the schedule generated by Algorithm 1. Through combinatorial constructions, we demonstrate that any such swap that respects the memory constraints cannot reduce the latency."}, {"title": "5 Fully Online Stochastic Setting", "content": "In this section, we leverage the semi-online setting in order to tackle the fully online stochastic one. In this setting, there is a discrete time horizon [1,T], and both the arrival process and the output length of each prompt follow some unknown probability distributions. To measure the performance of our online algorithm, we consider the hindsight optimal algorithm as a benchmark. Define $OPT(I)$ and $\\overline{OPT}(I)$ as the total and averaged end-to-end latency achieved by a hindsight optimal algorithm respectively, which has complete knowledge of the arrival process and the output length of each request from the beginning. Note that $OPT(I)$ and $\\overline{OPT}(I)$ can be achieved as the solution to an integer program (see Appendix B for more information).\nOverloaded vs Stable System. For the hindsight optimal algorithm, the order of the expected value $E[OPT(I)]$ with respect to $T$ depends on whether the system is overloaded or stable. Recall that $\\lambda = E[D]$ is the arrival rate. Given a fixed distribution $F$ for output lengths, there exists a threshold $\\mu^* > 0$ such that: if $\\lambda > \\mu^*$, then we say that the system is overloaded, meaning the number of pending prompts grows unboundedly as $T$ increases; if $\\lambda < \\mu^*$, then we say that the system is stable, meaning that the queue of waiting prompts remains constant. Here, $\\mu^*$ can be interpreted as the average processing rate, which depends on the memory limit $M$, the distribution $F$ of output lengths, and the scheduling algorithm. Its exact value is challenging to compute due to variations in the number of requests the worker can handle over time, which are influenced by the KV cache structure. In real-world LLM inference, high demand often places the system in an overloaded state. In such cases, we have $E[OPT(I)] = O(T^2)$ since most requests experience waiting times of $O(T)$. Therefore, we have $\\overline{E[OPT(I)]} = O(T)$. Although any scheduling algorithm may cause the average latency to increase when $T$ scales up, our objective is to minimize the rate of this growth. Conversely, in a stable setting, $E[OPT(I)] = O(1)$. However, it is not trivial to guarantee that the expected average latency under an online algorithm also remains $O(1)$ since the system may be stable under the hindsight optimal policy but become overloaded under the online algorithm. Therefore, the primary objective is to ensure that the online algorithm maintains system stability. We measure an algorithm's performance in terms of regret."}, {"title": "Definition 5.1 (Regret).", "content": "The regret of an online algorithm $A$ for LLM inference is:\n$Regret(A) = E_{I \\in \\Omega} [AEL(I; A) - \\overline{OPT}(I)]$"}, {"title": "Remark 5.2.", "content": "In an overloaded system, since both $TEL(I; A)$ and $OPT(I)$ scale as $O(T^2)$, it follows that $AEL(I; A)$ and $\\overline{OPT}(I)$ are both $O(T)$. Consequently, achieving constant regret implies that $AEL(I; A)$ and $\\overline{OPT}(I)$ share the same leading coefficient of $T$, with a gap of $O(1)$. This ensures that, regardless of how large $T$ becomes, the average latency gap remains constant."}, {"title": "5.1 Near Optimal Online Algorithm", "content": "In this section, we introduce a near optimal online algorithm (Algorithm 2) and show that its regret is $O(1)$. Algorithm 2 works as follows: at each round, it first prioritizes all requests that have already started processing. Then, for all pending requests, similar to the semi-online optimal algorithm (Algorithm 1), it sorts the requests by their output length in ascending order. It then adds the requests to the batch one by one, checking at each step whether adding the new request would pass the memory check in Equation (1) or not."}, {"title": "ALGORITHM 2: Online Scheduling and Batching Algorithm", "content": "Input: Memory capacity $M$, time horizon $T$\nOutput: Schedule for processing requests\nfor each round $t = 1$ to $T$ do\n| // Continue processing requests already in progress\n| Prioritize running requests that are currently processing, $S^{(t)}$.\n| // Handle waiting requests\n| Let $R^{(t)}$ be the set of waiting requests at time $t$.\n| Set $U^{(t)} = \\emptyset$\n| for each request $i \\in R^{(t)}$ in ascending order do\n| | Set a time list $t' = p_j + o_j$ for $j \\in S^{(t)} \\cup U^{(t)}$\n| | if all inequalities in Equation (1) hold for round $t'$ then\n| | | Add request $i$ to $U^{(t)}$\n| | else\n| | | Break the for loop\n| Process the requests in $U^{(t)}$"}, {"title": "Algorithm 2", "content": "makes decisions efficiently and, importantly, does not require knowledge of $\\lambda$ or $F$, which are often difficult to estimate in practice. The following theorem demonstrates that, regardless of whether the system is overloaded or stable, Algorithm 2 achieves a constant regret that remains bounded as $T$ increases. Consequently, this algorithm is near-optimal in the overloaded setting and ensures stability when the hindsight optimal solution is stable."}, {"title": "Theorem 5.3.", "content": "The regret of Algorithm 2 is upper bound by $C$, where $C$ is a constant independent of $T$.\nProof of Theorem 5.3. We use a telescoping approach to bound the regret. Let $L^{OFF} (S^{(t)}, R^{(t)})$ represent the total latency incurred by the offline optimal solution from round $t$ to round $T$, where $S'(t)$ is the set of jobs currently being processed and $R'(t)$ is the set of jobs waiting to be processed. The total latency incurred by the hindsight optimal offline algorithm over the entire horizon can"}, {"title": "be expressed as a telescoping sum:", "content": "$L^{OFF}(S^{(1)}, R^{(1)}) = \\sum_{t \\in [T]} (L^{OFF} (S'(t), R'(t)) - L^{OFF} (S'(t+1), R'(t+1))). \\qquad (3)$\nNote that we can substitute $S'(t)$ and $R'(t)$ with any sets representing the system's state at time $t$, as long as we remain consistent for terms $(t)$ and $(t + 1)$. Therefore, we replace $S'(t)$ and $R'(t)$ with $S_{ON}^{(t)}$ and $R_{ON}^{(t)}$, which represent the sets of processing and waiting requests, respectively, under our online algorithm (Algorithm 2):\n$L^{OFF}(S^{(1)}, R^{(1)}) = \\sum_{t \\in [T]} (L^{OFF}(S_{ON}^{(t)}, R_{ON}^{(t)}) - L^{OFF} (S_{ON}^{(t+1)}, R_{ON}^{(t+1)})). \\qquad (4)$\nLet $L^{ON}(S^{(t)}, R^{(t)})$ denote the total latency incurred by the online algorithm from round $t$ to round $T$, given the sets $S^{(t)}$ and $R^{(t)}$. Using the same telescoping approach, we obtain:\n$L^{ON} (S^{(1)}, R^{(1)}) = \\sum_{t \\in [T]} (L^{ON}(S_{ON}^{(t)}, R_{ON}^{(t)}) - L^{ON}(S_{ON}^{(t+1)}, R_{ON}^{(t+1)})) = \\sum_{t \\in [T]} l_t(S^{(t)}, R^{(t)}), \\qquad (5)$\nwhere $l_t(S^{(t)}, R_{ON}^{(t)})$ represents the latency incurred by Algorithm 2 from $t$ to $t + 1$. Regardless of whether each request in $S_{ON}^{(t)}$ and $R_{ON}^{(t)}$ is processed between $t$ and $t + 1$, all requests in these sets contribute an additional latency of 1. Thus, it follows that\n$l_t(S_{ON}^{(t)}, R_{ON}^{(t)}) = |S_{ON}^{(t)} \\cup R_{ON}^{(t)}|.$"}, {"title": "Then, the regret of Algorithm 2 can be decomposed as", "content": "$Regret(A) = E [AEL(I; A) - \\overline{OPT}(I)]$\n$= \\frac{1}{T} E[L^{ON} (S^{(1)}, R^{(1)}) - L^{OFF} (S^{(1)}, R^{(1)})]$\n$= \\frac{1}{T} \\sum_{t \\in [T]} E [l_t(S_{ON}^{(t)}, R_{ON}^{(t)}) - L^{OFF} (S_{ON}^{(t)}, R_{ON}^{(t)}) + L^{OFF} (S_{OFF}^{(t+1)}, R_{OFF}^{(t+1)}) - l_{t+1}(S_{OFF}^{(t+1)}, R_{OFF}^{(t+1)}))] \\qquad (Eqs. (4), (6))$\n$= \\frac{1}{T} \\sum_{t \\in [T]} E (l_t(S_{ON}^{(t)}, R_{ON}^{(t)}) - l_t(S_{OFF}^{(t)}, R_{OFF}^{(t)})) + L_{t+1}^{OFF} (S_{ON}^{(t+1)}, R_{ON}^{(t+1)})) - L_{t+1}^{OFF} (S_{OFF}^{(t+1)}, R_{OFF}^{(t+1)})) \\qquad (7)$\nwhere $S_{OFF}^{(t+1)}$ and $R_{OFF}^{(t+1)}$ denote the system state at time $t + 1$ obtained by running the offline optimal algorithm starting from the online state $S_{ON}^{(t)}, R_{ON}^{(t)}$ at time $t$. The third equation holds because (regardless of the algorithm used), the total latency between $t$ and $t + 1$ is $|S \\cup R|$(Eq. (6)).\nWe now analyze the term\n$CP_{t+1} = E (L_{t+1}^{OFF}(S_{ON}^{(t+1)}, R_{ON}^{(t+1)})) - L_{t+1}^{OFF} (S_{OFF}^{(t+1)}, R_{OFF}^{(t+1)})).$\nWe refer to $CP_{t+1}$ as the compensated value for coupling the offline state with the online state at time $t$, and we aim to establish an upper bound for $CP_{t+1}$ for each $t \\in [T]$.\nNote that if the online and offline algorithms make the same batching decision at time $t$, then $S_{ON}^{(t+1)} = S_{OFF}^{(t+1)}$ and $R_{ON}^{(t+1)} = R_{OFF}^{(t+1)}$, so the compensated value is 0. However, if the online and"}, {"title": "since OFF is the offline optimal algorithm in terms of latency."}]}