{"title": "Context-Aware Command Understanding for Tabletop Scenarios", "authors": ["Paul Gajewski", "Antonio Galiza Cerdeira Gonzalez", "Bipin Indurkhya"], "abstract": "This paper presents a novel hybrid algorithm designed to interpret natural human commands in tabletop scenarios. By integrating multiple sources of information, including speech, gestures, and scene context, the system extracts actionable instructions for a robot, identifying relevant objects and actions. The system operates in a zero-shot fashion, without reliance on predefined object models, enabling flexible and adaptive use in various environments. We assess the integration of multiple deep learning models, evaluating their suitability for deployment in real-world robotic setups. Our algorithm performs robustly across different tasks, combining language processing with visual grounding. In addition, we release a small dataset of video recordings used to evaluate the system. This dataset captures real-world interactions in which a human provides instructions in natural language to a robot, a contribution to future research on human-robot interaction. We discuss the strengths and limitations of the system, with particular focus on how it handles multimodal command interpretation, and its ability to be integrated into symbolic robotic frameworks for safe and explainable decision-making.", "sections": [{"title": "I. INTRODUCTION", "content": "In Human-Robot Interaction (HRI), understanding commands where humans communicate with robots through both speech and gestures is a critical and challenging problem. This capability is essential for seamless human-robot collaboration, particularly in environments such as manufacturing, healthcare, and service robotics, where intuitive and flexible interaction is required. The complexity arises from the need to accurately interpret multimodal input, such as natural language and visual cues, while addressing ambiguity, variability, and noise in real-world settings. Speech commands may be imprecise, object locations may be unclear, and gestures may vary between individuals, making it difficult for a robot to reliably parse and act upon human intentions. In addition, integrating these different modalities into a coherent understanding requires sophisticated coordination between natural language processing, computer vision, and decision-making processes, all of which must operate in real time.\nTo address this problem, we introduce a hybrid algorithm for command interpretation in tabletop scenarios, where a human interacts with a robot by providing instructions through verbal commands and gestures such as pointing at objects. Our approach seamlessly integrates a procedural control flow with multiple state-of-the-art deep learning models to handle the core tasks involved in understanding and executing these commands. The procedural component orchestrates the interaction between Al models, manages the data flow, and facilitates decision-making, while deep learning models are responsible for handling complex sub-tasks, including speech recognition, text-based reasoning, and object detection. Notably, our solution operates in a fully zero-shot manner, meaning it does not rely on pre-modeled databases or prior knowledge of the objects involved. This allows it to function effectively across a wide range of different environments and tasks.\nThis system is designed to integrate with a larger robotic framework, providing distilled, actionable information to other planning modules (see Figure 1). It alleviates the complexity of interpreting natural language commands by extracting grounded, concrete information about the objects involved and the actions to be performed on them. Additionally, objects are segmented to aid in tasks like object tracking and manipulation, such as grasping.\nIn addition to the algorithm, we are releasing a small dataset of video recordings used to evaluate our system. This dataset can facilitate further research in the area of HRI, as tabletop scenarios represent a relatively underexplored domain in the literature. In these settings, the robot focuses on the objects on the table rather than the entire person, making many existing HRI systems and algorithms unsuitable for such tasks.\nThe main contributions of this work include:\n\u2022 Introduction of a novel hybrid algorithm for command interpretation in tabletop scenarios. To our knowledge, this is the first algorithm that comprehensively handles human command understanding by integrating information from multiple sources, such as speech, pointing gestures, and scene context.\n\u2022 Integration of multiple deep learning models and assessment of their readiness for deployment in robotic systems.\n\u2022 Release of a test dataset consisting of video recordings of a human giving instructions to a robot in a tabletop scenario."}, {"title": "II. RELATED WORK", "content": "Understanding human commands is essential for a service robot, as it must first comprehend what it needs to do before executing any action. However, command understanding is closely interconnected with other aspects of robotics such as scene understanding, robot morphology, and even the design choices in robot software. For example, a robot without end effectors would focus on navigation rather than grasping tasks. Similarly, if a robot relies on a large unified deep learning model [1] - a trend that is rapidly evolving - then that model would be responsible for understanding commands and perceiving the world around it. In contrast, a symbolic reasoning system [2] would use specialized modules for command understanding, as presented in this paper.\nSymbol-based robotic frameworks, such as those described in [3], [4], [5], [6], [7], [8], [9], [10], which our system is designed to integrate with, offer the advantage of explainable decision-making [11], [12]. This transparency enhances safety, making them more suitable for service robotics compared to end-to-end learning approaches, where decision processes are often opaque.\nSignificant research has already been conducted on this topic, but the problem is far from solved. In this research, we focus on natural human-robot communication through voice and gestures. Combining natural language understanding with additional information from gestures allows for the development of intuitive and robust human-robot interaction systems. However, human communication involves subtle nuances and variations that pose significant challenges for intelligent robotic systems to handle effectively.\nNotable related work includes an indoor navigation system for telepresence robots developed by Kumar et al. [13], which combines language grounding and pointing gesture recognition to identify target locations in indoor environments. Although its approach to integrating language and pointing gestures to determine a destination is conceptually similar to ours, their focus is on indoor navigation, where the target location may not be immediately visible. Their system analyzes the whole body of the user, whereas we focus on tabletop scenarios where only the hands of the human instructor are visible and all relevant objects are present in the scene.\nSeveral systems aim to understand human commands through natural language and gestures. For instance, in [14], natural language commands are parsed into semantic graphs before grounding, while more complex cognitive systems, such as [15], involve robots learning concepts from scratch. The challenge of command understanding is not limited to robotics; it also applies to general voice and gesture-driven computer interfaces, a problem that is nearly as old as personal computers themselves [16]. However, the specific requirements of the environment of each system dictate the solutions employed, making direct comparisons to our method difficult.\nA related system in [17] for tabletop scenarios enables users to create instructions for a robot to push objects around a table using a multimodal user interface (UI) that incorporates both speech and gestures. However, it is limited to pushing tasks and relies on a highly specialized graphical user interface (GUI), requiring specific equipment and training. In contrast, our method supports a wider range of tabletop tasks and eliminates the need for specialized interfaces, using only natural communication as the medium between human and machine."}, {"title": "III. METHODOLOGY", "content": "Our proposed algorithm follows a structured decision-making process that leverages deep learning models (Table I) to extract key information from the data. The table details the specific models and their respective versions used in our approach. In the following sections, we provide a comprehensive explanation of the entire process.\nFigure 2 presents a graphical overview of the information flow between the key components of the algorithm. Due to its complexity and length, the full algorithm is not included in this paper. However, the source code is available for public access and can be reviewed or utilized by anyone interested\u00b9.\nThe pseudo-code for the algorithm is also available next to the actual implementation\u00b2.\n\nA. Input processing\n\nThe algorithm begins with a video input that contains a single command. As the first step the audio is separated from RGB frames.\n\nB. Audio transcription\n\nThe audio from the video is processed using the Whisper model [24] for automatic speech recognition (ASR), configured for English language recognition. Whisper outputs a word-level time-stamped transcription in JSON format.\n\nC. Textual command understanding\n\nThe transcription is passed to the Phi-3 model [21], tasked with extracting the core components of the command. The model is prompted to identify the key elements: the object of interest (noun), the requested action (verb or verb phrase), and the target of the action (noun). Additionally, it determines whether the object and target are explicitly named or referenced through deixis (e.g., \u2018here', 'this').\nTo perform this extraction, we issue two sequential prompts to the language model. The first prompt directs the model to identify and extract the object, action, and target from the Whisper transcription. The second prompt refines the output, adding information on whether the object and target are concrete items.\n\nThe first prompt:\nUser will provide you a transcription JSON from Whisper. Extract from it the object (noun + optional adjectives), the action (verb or phrase), and the target (noun + optional description). If the target has a description of 'next to', 'between', 'near', etc; it should be included in the target description. If the action is a phrasal verb, put it whole. Return the result as JSON with the keys 'object', 'action', and 'target'. If you can't find any of these, leave the value empty. Be as concise as possible. Example: {'object': {'text': 'mug', 'timestamp': [1.04, 1.36]}, 'action': {'text': 'put on top', 'timestamp': [1.5, 1.76]}, 'target': {'text': 'laptop', 'timestamp': [2.24, 2.46]}}. Choose only one interpretation and write just one valid JSON object.\n\nThe second prompt:\nRefine your own output to include information whether the object and the target are concrete objects like 'apple' or not concrete like 'here'. Add appropriate 'concrete' flag to your generated JSON.\n\nD. Video processing\n\nOnce the object and target are identified, the corresponding frames of the video are extracted using the timestamps generated during the ASR stage. A frame is selected at the point when the speaker finishes the relevant word, allowing analysis of the instructor's pointing gestures and object detection. Frames are extracted separately for both the object and the target.\n\nE. Pointing gesture understanding\n\nIn both frames, hand detection is performed using the Mediapipe Hand Landmarker [25]. The pointing vector is defined as the vector of the index finger, specifically from the base of the finger to its tip (from landmark 5 to landmark 8). The detector is configured to recognize up to two hands. The z-coordinate, which indicates the depth or distance from the camera, is also estimated by the Mediapipe model. To determine which hand is making the pointing gesture, the z-coordinate of the index finger is used, with the hand closer to the camera assumed to be the one pointing.\nThe subsequent processing of the object and target diverges and is outlined in the following subsections.\n\nF. Object handling\n\nIf the object is referenced by name, the name (as extracted by the Phi-3 model) is used as a prompt (followed by a period, as this increases the accuracy of the detector) for the GroundingDINO object detector [18]. If multiple objects are detected, the algorithm selects the one whose center of the bounding box is closest to the line extending from"}, {"title": "G. Target handling", "content": "the pointing vector of the finger. This line represents the direction of the instructor's point gesture, and the proximity of the center of the bounding box to this line determines the selected object.\n\nIf the target is a reference, its name is used as a prompt for the object detector. In cases where multiple objects are detected, the one closest to the extended line from the pointing finger is selected, following the same approach used for the object.\nThe target is then evaluated to determine whether it refers directly to an object or to a position relative to another object. This is done by searching for spatial relation terms such as 'left', 'right', 'beside', 'between', 'in front of', or 'behind' in the action and target names. If the target directly refers to an object, the detected object is used as the result.\nIn the case where the target refers to a position relative to another object, indicating an empty space on the table, the following steps are performed. First, the object detector is used to segment the entire table, which is then divided into cells using the Voronoi diagram [26]. The object detector is also used to detect existing objects (by using an 'objects' prompt) on the table. The centers of these objects, along with the center of the previously detected target, are used to mark Voronoi cells as occupied if they intersect with any objects. This process creates a segmented map of the table in which certain areas are marked as occupied by objects. An empty cell is then searched for in the direction indicated by the spatial term used by the demonstrator. This results in identifying a specific cell as the target of the command.\nIf the target is purely deictic (that is, referenced by pointing), the system first attempts to detect a 'container' in the image. If a container is found, it is used as the target. If multiple containers are detected, the one closest to the pointing line is chosen. If no container is found, the target is assumed to be an empty space indicated by the pointing gesture. To determine the exact location, the table is detected and segmented using the Voronoi diagram, similar to the previous case. However, in this instance, depth information is also used to identify the cell at which the finger is likely pointing. This produces a 3D pointing vector along with 3D Voronoi cells. Depth information is estimated using the DINOv2 model [27]."}, {"title": "H. Compiling the command representation", "content": "In the final stage, the exact pixels that correspond to both the object and the target are segmented using the SegmentAnything model (SAM)[28]. The model uses the center point of each bounding box as a prompt to perform the segmentation.\nThe segmented image regions, along with the identified object, action, and target names, are then returned as the final output of the algorithm."}, {"title": "I. Applying the command representation", "content": "The algorithm provides the exact segmentation of the pixels of both the object and the target (from past frames), which a robot can potentially use to identify and track these elements in real-time camera streams. This information enables the robot to locate and interact with the object and its designated target in the current scene.\nUsing the textual representation of the action, the robot can apply the appropriate task-specific skills. For simple actions such as 'put it' or 'bring it', the robot can perform simple pick-and-place operations. However, more complex actions, such as 'pour', can require the robot to perform specialized movements and utilize advanced manipulation skills. The combination of visual and textual data equips the robot with the necessary context to perform a wide range of tasks effectively."}, {"title": "IV. EVALUATION", "content": "In natural human communication, even in constrained environments like the tabletop scenario considered in this paper, numerous variations can occur. A human may use different words to refer to the same objects, rely on pointing gestures, or use relative spatial references. Our proposed algorithm is designed to handle these variations by integrating both speech and gestures to accurately interpret commands. Additionally, it leverages the latest deep learning models (see Table I) in a zero-shot manner, enabling the system to handle a wide range of object and location names without prior training. Table II outlines the different scenarios our algorithm is expected to manage. The complete test dataset used for the evaluation of our approach is available online\u00b3.\nThe human may refer to objects by name (reference) or by pointing (deixis), using substitute words like 'this' or 'that'. Objects on the scene can include distractors, items that look similar to the intended object. Similarly, target locations can be identified by name if they refer to specific objects, or through deictic expressions. Additionally, a target may not be a concrete object but rather a designated area. This area can be indicated either absolutely (by pointing) or relatively in reference to another object on the scene. Targets, like objects, may also have distractors, such as other targets of the same class. In all cases, the scene may contain additional clutter from unrelated objects.\nThe evaluation was conducted by running the algorithm on several examples recorded using a mobile phone camera. These recordings were segmented into individual commands and converted into mp4 format. The examples encompass all the variations the algorithm was designed to address. Table III provides a summary of a representative subset of the scenarios used in the evaluation, highlighting the diversity of commands and conditions tested. Results from select test scenarios are shown in Figure 1, with automatically generated annotations overlaid on frames from the actual footage.\nThe performance of the algorithm was assessed by comparing its interpretation of the command with the ground truth, i.e., the intended action of the person giving the command. To isolate the robustness of the procedural aspects of the algorithm from the limitations of the underlying deep learning models, multiple takes of the same command were sometimes tested. This approach helped mitigate errors such as misinterpreted words in voice commands, ensuring that detection inaccuracies did not skew the overall assessment."}, {"title": "V. RESULTS", "content": "Overall, the algorithm's output closely matched the intentions of the human instructor in almost all instances, demonstrating that the procedural logic performed as expected. However, several detection errors arose due to limitations in the underlying deep learning models. The most problematic was the MediaPipe hand detector, which was highly sensitive to motion blur and often failed to detect the presence of the hand. Another issue was the Whisper ASR model, which occasionally mistranscribed words, for example, consistently transcribing 'bowl' as 'ball'. It is worth noting that the experimenters were non-native English speakers, which may have contributed to these errors. On the other hand, the GroundingDINO, SegmentAnything, and DINOv2 models performed reliably, with rare mistakes that could often be corrected using gesture information.\nThe algorithm itself stands to benefit from future advancements in deep learning models as they can easily be updated. However, at present, the performance of these models remains the most significant limiting factor in applying hybrid algorithms, like the one proposed in this paper, to real-world robotics. While the algorithm effectively combines multiple models to enhance accuracy, it also tests their limits with specific classes of examples."}, {"title": "VI. CONCLUSIONS & FUTURE WORK", "content": "In this paper, we presented a command understanding algorithm designed for tabletop scenarios, capable of interpreting natural language commands combined with pointing gestures. The algorithm demonstrated high accuracy in extracting objects, actions, and targets, provided there were no recognition errors. Its procedural logic proved robust, effectively integrating multiple deep learning models to handle a wide range of input variations.\nHowever, the evaluation also revealed significant challenges posed by recognition errors in the underlying deep learning models. The MediaPipe Hand Landmarker struggled with motion blur, and Whisper ASR occasionally mistranscribed words, particularly when non-native English speakers were involved. Despite these issues, models such as GroundingDINO, SegmentAnything, and DINOv2 performed reliably, contributing to the success of the algorithm in most cases. It is important to note that the high accuracy of the algorithm was observed primarily when detection errors were minimal or rectified by contextual information, such as gestures.\nThe flexibility of the algorithm allows it to benefit from future improvements in deep learning models, which can be replaced or upgraded without altering the core framework. As these models evolve, particularly in speech recognition and hand detection, the practical deployment of this system in real-world robotic applications will become increasingly feasible.\nWe plan to focus future work on improving the robustness of the algorithm against recognition errors and to explore its ability to handle continuous unconstrained data streams. The outputs could also be integrated into a larger robotic system to perform the requested actions. With these improvements, the algorithm has the potential to significantly advance human-robot interaction in complex real-world environments.\nAnother potential improvement to the current system is to add support for languages other than English. This enhancement would primarily involve configuring the transcription (ASR) and command extraction (LLM) models to handle multiple languages. The rest of the algorithm, particularly the decision-making component, would remain unchanged.\nLooking ahead, our aim is to further refine our approach and explore additional avenues to enhance the system's adaptability, scalability, and integration into real-world applications, ultimately moving closer to creating intuitive, human-like robotic assistants."}]}