{"title": "Generating a Low-code Complete Workflow via Task Decomposition and RAG", "authors": ["Orlando Marquez Ayala", "Patrice B\u00e9chard"], "abstract": "AI technologies are moving rapidly from research to production. With the popularity of Foundation Models (FMs) that generate text, images, and video, AI-based systems are increasing their complexity. Compared to traditional AI-based software, systems employing FMs, or GenAI-based systems, are more difficult to design due to their scale and versatility. This makes it necessary to document best practices, known as design patterns in software engineering, that can be used across GenAI applications.\nOur first contribution is to formalize two techniques, Task Decomposition and Retrieval-Augmented Generation (RAG), as design patterns for GenAI-based systems. We discuss their trade-offs in terms of software quality attributes and comment on alternative approaches. We recommend to AI practitioners to consider these techniques not only from a scientific perspective but also from the standpoint of desired engineering properties such as flexibility, maintainability, safety, and security.\nAs a second contribution, we describe our industry experience applying Task Decomposition and RAG to build a complex real-world GenAI application for enterprise users: Workflow Generation. The task of generating workflows entails generating a specific plan using data from the system environment, taking as input a user requirement. As these two patterns affect the entire AI development cycle, we explain how they impacted the dataset creation, model training, model evaluation, and deployment phases.", "sections": [{"title": "I. INTRODUCTION", "content": "Since the introduction of ChatGPT two years ago, more and more systems based on Generative AI (GenAI) are being shipped to consumers. The underlying technology is a Foundation Model (FM), more commonly a Large Language Model (LLM), able to generate text, images, and, more recently, video.\nDue to current limitations in FMs, such as their black-box nature and sensitivity to data distribution shifts [1], [2], building systems leveraging their capabilities is particularly challenging. Integrating AI models into software systems already adds a new set of complex design choices to software engineering [3]-[7]. This complexity has increased as the range of possible GenAI-based systems has grown significantly due to their increasing abilities to generate almost any kind of output. For instance, whereas in the past AI models added classification or regression capabilities to software, nowadays systems include more sophisticated features such as code completion, explanation, and search [8].\nThe software engineering community has recognized the need for new design patterns for AI-based systems to provide blueprint solutions to the challenges involved in integrating AI models [4], [9], [10]. In particular, patterns for architectural design decisions are well-documented [5], [11]\u2013[13].\nGenerative AI requires adapting existing design patterns for AI-based systems and adding new ones, as problems specific to generative models such as hallucination (safety) or large number of output tokens (scalability) can affect software systems in new, unexpected ways [14]. It is also not trivial to engineer these systems to allow adding features rapidly (modifiability) and to allow them to interact effectively with other non-AI components (interoperability).\nAs AI practitioners are responsible for building the GenAI models (fine-tuning or prompting), they need to be aware of the impact, positive and negative, that modern techniques have on software engineering [15]. However, the typical process to build AI-based systems still sees a rift between scientists and engineers, where the former develop the models and the latter bring the models into production [5], [16].\nThe consequence is that models are built with little regard to the desired quality attributes of the overall system, as the main concern of model building is to maximize metrics to prove that the solution works. This may increase the time to deploy the AI-based system into production due to possible redesign of datasets and model rebuilding if the pipeline proves to be too slow, unsafe, etc. Therefore, it is necessary to discuss how common AI techniques affect software engineering.\nTo help bridge this gap between AI and software engineering, the first part of this paper formalizes two common techniques as design patterns since their utilization impacts the entire engineering effort, crucially the architectural design of the system. Their impact goes beyond constructing the model; hence it is important to discuss how they affect the flexibility, maintainability, safety, and security of the system, among other commonly desired software quality attributes [17].\nTask Decomposition is an application of divide and conquer to the AI domain. This technique allows breaking a machine learning (ML) task into sub-tasks, which can be more easily solved. It has been proposed for a variety of tasks such as speech processing [18], film trailer generation [19], and abstract visual reasoning [20].\nRetrieval-Augmented Generation (RAG) is a well-known technique to allow an FM to interact with data from its"}, {"title": "II. RELATED WORK", "content": "There is a rich body of literature on software engineering for ML, ranging from case studies [6], [7] to surveys drawn from academic and gray literature [5], [10], [16]. Some papers contribute a list of design patterns for AI-based systems [4], [9]. A limitation of literature reviews is that they necessarily lack detail, and thus their description of design patterns are at a high-level, divorced from the context in which they are applied. Moreover, most of the existing list of design patterns for ML dates from the pre-generative AI era, when the AI components were relatively simple compared to current uses of\nare nascent efforts underway to document the new set of challenges FMs add to software engineering [14]. Our paper fits into this line of work by discussing how FMs can be used in a real-world application with Task Decomposition and RAG.\nTask Decomposition as a technique to solve complex ML tasks is not new. It is related to AI design patterns such as Two-Phase Predictions, AI pipelines, and Multi-Layer Pattern [4]. In the GenAI literature, it is related to the Coordination connector, where the FM coordinates tasks between AI and non-AI components [26], and it can be implemented with a Chain of FMs where each FM is responsible for executing a sub-task that is part of a larger task [27]. We expand on this work by discussing the software quality attributes that this technique offers to GenAI-based systems.\nMost modern GenAI systems employ an implementation of Retrieval-Augmented Generation, as a way to improve the output quality of FMs. RAG can be used in an iterative, recursive, or adaptive fashion [21], and to adapt FMs to specialized domains [28]. It can be seen as an implementation of the Strategy pattern in the context of AI [4], where the strategy is made up of two interfaces: one to generate text and one to retrieve suggestions used in the generation process. The generation is affected by the quality of the retrieved results, whereas the retrieval is affected by the quality of the query, which can be user-provided or FM-generated. As with Task Decomposition, we formalize its usage as a design pattern and discuss its impact on software quality attributes.\nLastly, automatically generating workflows is an important task for enterprise systems, as building them generally requires expert domain knowledge. Workflows provide significant value to end-users because they automate repeatable processes in several domains such as IT (e.g., acquiring an IT asset) or HR (e.g., onboarding new employees). There exist several approaches to generate them, such as having an LLM generate an initial plan, then asking users to correct the generated plan, followed by asking the LLM again to regenerate the workflow and its details [29]. In our use case, we generate a very detailed plan with environment artifacts without any additional input from the user besides the initial requirement. The workflow is generated in a code-like representation that can be rendered in a web user interface."}, {"title": "III. GENAI DESIGN PATTERNS", "content": "We consider Task Decomposition and RAG as design pat-terns because they are reusable solutions to the problem of integrating GenAI components into larger software systems. While they affect all phases of the AI development cycle, they crucially affect the architecture of the system.\nInspired by the template used to document design patterns in the seminal work by Gamma et al. [30], we describe these two techniques following most of the template's sections, and motivate them using classical software quality attributes [17]."}, {"title": "A. Task Decomposition", "content": "Intent: Break a complex ML task into simpler sub-tasks.\nMotivation:\n1) Scalability: FMs can output large number of tokens per input, creating performance risks as the output size increases. Asking the FM to generate output for smaller sub-tasks helps limit scalability issues.\n2) Modularity: By reducing the granularity of the problem, different FMs can solve different parts of the problem, improving the system maintainability.\n3) Functional correctness: As with humans, breaking a problem into sub-problems can aid the FM increase the quality of its output.\n4) Time behaviour: Generating output at the sub-task level reduces the time it takes for end-users to receive a response.\n5) Modifiability: As the model can handle more functionality due to more labeled data or better FMs, more sub-tasks can be added without breaking the overall system design.\n6) Testability: Evaluating simpler sub-tasks instead of one complex task makes testing FMs easier.\n7) Analysability: It is easier to diagnose problems in the generated output if generation takes place incrementally via sub-tasks.\nApplicability: Task Decomposition is suitable for ML tasks that require complex structured output or output that can be structured, such as code generation or essay writing. In the former, when generating a class, the AI model can first generate the signatures of the functions of the class, and then generate the content of each function. In the latter, the AI model can first generate the leading sentence of each paragraph, and then generate each paragraph. In these cases, asking the model to generate the entire output at once may result in lower correctness and/or delays in responses.\nStructure: Figure 1 shows sample structures.\nParticipants: One or more FMs which execute the sub-tasks, and another component (AI or non-AI) to orchestrate how the sub-tasks are distributed.\nCollaborations: The orchestrator component divides the sub-tasks among FMs so that the main task is completed. This component can be another FM (e.g., Al agents) or traditional software code (i.e., non-AI component) that calls the FMs deterministically. There also needs to be a mechanism to create the sub-task input, which can come from FMs or non-AI components.\nConsequences:\n1) Decomposition can lead to lower response times and improved system scalability as the size of the FM output is decreased significantly. However, the overall system response time may increase due to the task orchestration.\n2) By having specialized FMs or specialized prompts for the same FM, the overall correctness and modu-"}, {"title": "B. Retrieval-Augmented Generation", "content": "Intent: Provide environment data to an FM so that it can adjust its output according to real and available input.\nMotivation:\n1) Security: FMs can introduce security risks if allowed to call functions or tools available in the environment [32], [33]. By suggesting environment data in the input to the FMs, they can interact with the environment in a more secure way.\n2) Safety: Retrieving and suggesting facts to an FM during generation can decrease the amount of hallucination in their output.\n3) Modularity: To simplify the ML task for the FM, it is desirable to separate how the task is solved from what data is needed to solve it. Requiring the FM to memorize all the data it needs for the task in its model weights is unfeasible.\n4) Interoperability: Modern AI systems require FMs to interact with artifacts or knowledge available in the environment in which they run.\n5) Functional correctness: The knowledge embedded in the model weights of an FM is limited to the data it has received during training. By augmenting its knowledge with information from the environment, it can increase its correctness.\n6) Self-descriptiveness: GenAI-based systems tend to be black boxes; showing the retrieval results gives users more visibility on the system's inner workings.\n7) Testability: RAG allows evaluating the retriever and the indexed data sources independently of the FM.\n8) Analysability: When errors arise, RAG allows a diagnostic of whether the FM made an error due to lack of AI capabilities, or due to inappropriate environment data available to it.\nApplicability: RAG is useful when an FM needs to access up-to-date knowledge from its environment, allowing it to generalize to scenarios or input data not seen during its training. When FMs are not grounded to the environment in which they run, they have a higher likelihood of producing answers that are plausible yet false. When models tend to hallucinate, it is a good idea to use RAG.\nStructure: Figure 2 shows sample structures.\nParticipants:\n1) A retriever, normally another AI model, used to encode the data to be retrieved as well as the search query. A simpler keyword search mechanism can be"}, {"title": "IV. WORKFLOW GENERATION: A CASE STUDY", "content": "Having discussed Task Decomposition and RAG at length, we report how our team built a real-world GenAI application called Workflow Generation. We structure our reporting as a descriptive case study [25], where the objective is to show how these design patterns result in a well-engineered AI system.\nThe context of this case study is a Platform-as-a-Service enterprise company that automates the creation and execution of workflows in domains such as IT and HR. The core team responsible for the GenAI application is made up of a dozen individuals, including software engineers, data labelers, quality engineers, and AI applied researchers. The application\nRQ1. How does Task Decomposition and RAG impact data labeling, model training, model evaluation, and model deployment?\nRQ2. What are the trade-offs involved when using these two techniques?\nL.E. Lwakatare et al. [6] grouped the software engineering challenges common to commercial AI-based systems in four categories: Assemble dataset, create model, train and evaluate model, and deploy model. Hence, RQ1 is meant to address how these techniques help deal with these challenges in the GenAI era, in terms of software quality attributes.\nRQ2 aims to elicit discussion on limitations of these techniques as well as on alternative approaches that were not chosen to build the application. It is easier to understand trade-offs when real-world constraints and requirements are considered.\nThe sources of information for this case study are mainly of first degree, as the authors of this paper were key contrib-utors in the entire development cycle. Third degree sources,"}, {"title": "A. ML Task Definition", "content": "A workflow automates a repeatable process by listing the steps that need to be executed in a given order. Each of these steps translates to code, but end-users avoid writing code by building a workflow; this is referred to as low-code.\nEach environment, or installation of the enterprise platform, defines a set of steps $S = \\{s_1, s_2, ..., s_n\\}$. Each step $s$ has a set of inputs $I_s = \\{i_1, i_2, ..., i_n\\}$ and a set of outputs $O_s = \\{o_1, o_2, ..., o_n\\}$. Inputs and outputs have a name and a value, but the name of an input as well as the name and value of outputs are deterministic (i.e., they do not need to be generated by the FM). Only the value of inputs change depending on what the workflow does.\nA workflow $w$ is an ordered list of steps $S_w = (w_1, w_2, ..., w_n)$ where every element in $S_w$ comes from set $S$. In $w$, the inputs of $w_n$ can use the outputs of $w_m$ where $m < n$, that is, the outputs of previous steps can be used in future steps. Importantly, the input values of $w_n$ can also use environment artifacts such as database table names, table columns and row values.\nThe ML task then consists of generating workflow $w$ from a natural language text such as When a P1 incident is created, look up the user assigned to the incident and if the user has a manager, send an email reminding them of the incident."}, {"title": "B. Data Labeling", "content": "While labeling is expensive, it is necessary for achieving automatic evaluation. When prompting does not yield good results, the last resort is to fine-tune an FM with labeled"}, {"title": "C. Model Training", "content": "The greatest impact of Task Decomposition and RAG is on model training, or model fine-tuning in our case, as how models are trained determine how they will interface with the rest of the system (i.e., model inputs and outputs). Using samples such as the one shown in Figure 4, we shipped a first version of the application that could generate only outlines, with low rates of hallucination thanks to a simple"}, {"title": "D. Model Evaluation", "content": "Evaluating the model is as important as data collection. It is generally acknowledged that testing GenAI-based systems is particularly challenging due to the open nature of generation [42]. There is generally more than one generated answer for the same input and retraining FMs can result in different output for the same previously tested input [14]. The simplest yet time-consuming way to evaluate FMs is to use humans knowledgeable of the ML task, while the fastest yet potentially inaccurate approach is to use the strongest available FM as a LLM-as-a-Judge. As using LLMs for evaluation does not always correlate with human evaluations [43] and introduces non-determinism into the evaluation process, we created our own metric that is deterministic and explainable.\nThe insight behind our metric, called Flow Similarity, is that workflows can be represented as trees, similar to how a computer program can be represented as an abstract syntax tree. Once we have the expected and generated workflows as trees, we can use the tree edit distance algorithm [44] to compute a similarity score. We still require humans to give us a expected workflow per user requirement, but we can automate the evaluation.\nTask Decomposition also helps the analysability of the system since we had three groups of evaluation metrics to report to our stakeholders: correctness considering only the outline, correctness considering the entire workflow, and correctness considering only step inputs (e.g., Flow similarity when populating the look_up_record step). We rank the steps by how well the FM can populate its inputs and focus on the bottom of the list.\nUsing RAG also offered testability benefits as we evaluated the retriever separately using recall metrics for every kind of artifact that we needed to retrieve from the environment. We analyzed the errors per kind of artifact to determine that retrieving column names is very difficult as the annotations can be of the form set it to false. In this case, the pronoun it refers to a previously mentioned column name. Quality can be improved by including more context in the query in addition to the annotation, such as adding the requirement.\nOverall, Task Decomposition and RAG contribute to the functional correctness of the system, mainly because they allow testing and improving different parts independently: sub-tasks and the retriever. By systematically testing the different parts, we can more easily discover failure modes. This is similar to the benefits that modularity brings to building traditional software systems as engineers can devote their attention to the underperforming parts.\nHowever, it has been previously noted that ML modularity is not the same as software engineering modularity. Improve-ments in a sub-task may not necessarily translate to overall system improvements as the rest of the system may not have been tuned according to the improved sub-task output [7].\nWhile using these two patterns enhanced the testability of the GenAI components, we still required significant end-to-end testing."}, {"title": "E. Deployment", "content": "The vast size of FMs in number of model parameters creates significant performance and deployment challenges [14]. A flexible architecture can help the deployment process by offering choices. With Task Decomposition, since each sub-task has its own level of ML difficulty and is handled separately, they can be handled by a separate FM to maximize performance.\nFor instance, the most complex sub-tasks can be assigned to the most powerful FM available and the simpler sub-tasks to smaller FMs, since larger models generally produce better output quality [45]. If the FMs are deployed on local servers, then the smaller FMs can be deployed on smaller, older GPUs such as V100s, reserving the more modern GPUS, A100s and H100s, for larger FMs. Or, if higher quality is needed, the most complex task can be given to a very strong cloud-served FM such as GPT-40, while the simpler sub-tasks can be served in local servers. To further improve resource utilization, as long as the use case permits it, the sub-tasks can be batched so they can be solved concurrently, as depicted in Figure la.\nIn our case, due to fine-tuning, we were able to obtain good quality with a small FM having only 7 billion parameters. Due to its small size, we had the same FM execute both sub-tasks on a H100 GPU. Unless computing resources are lacking, it is preferable to have the same FM execute all sub-tasks in order to reduce the deployment complexity.\nOur RAG encoder is also very small having only 100 million parameters, but it achieves good retrieval quality due to fine-tuning. RAG provides good interoperability as the same encoder (and indexes) can be used for other use cases. As Workflow Generation is not the only GenAI capability in the company's platform, the encoder was deployed as an AI component that can be used in other use cases such as Code Generation. Note that this small retriever could run on CPU due to its small size if we did not have GPUs at our disposal.\nRAG however makes the deployment more complex as there are two models to deploy: the FM and the retriever encoder. If the Tool Usage pattern is used, only the FM would be deployed. However, the tools or APIs that the FM calls during generation would need to be fixed unless RAG is used to tell the FM what is available in the environment. Current industry practice shows that it is difficult to ship real-world applications with low hallucination rates and with up-to-date knowledge without a RAG implementation."}, {"title": "V. DISCUSSION", "content": "The presented case study gives enough detail to answer RQ1 and RQ2. Task Decomposition and RAG resulted in a modular, flexible, secure, and testable system, at the expense of added complexity especially in model training. Breaking down tasks into sub-tasks requires some experimentation and AI knowledge, but it can yield faster prototyping and an iterative path to releasing GenAI applications.\nData labeling still constitutes a significant percentage of the overall human labour in any ML project [14], [46]. Task Decomposition and RAG can add to the labeling effort as"}]}