{"title": "SKIN CANCER MACHINE LEARNING MODEL TONE BIAS", "authors": ["James Pope", "Md Hassanuzzaman", "Mingmar Sherpa", "Omar Emara", "Ayush Joshi", "Nirmala Adhikari"], "abstract": "Background: Many open-source skin cancer image datasets are the result of clinical trials conducted\nin countries with lighter skin tones. Due to this tone imbalance, machine learning models derived\nfrom these datasets can perform well at detecting skin cancer for lighter skin tones. Though there\nis less prevalence of skin cancer with darker tones, any tone bias in these models would introduce\nfairness concerns and reduce public trust in the artificial intelligence health field.\nMethods: We examine a subset of images from the International Skin Imaging Collaboration (ISIC)\narchive that provide tone information. The subset has more light (83.3%) than dark (16.7%) images\nand more benign (74.7%) than malignant (25.3%) images. These imbalances could explain a model's\ntone bias. To address this, we train models using the imbalanced dataset (3,623 images) and a\nsampled balanced dataset (~500 images) to compare against. The datasets are used to train a deep\nconvolutional neural network model to classify the images as malignant or benign. We then evaluate\nthe models' disparate impact, based on selection rate, relative to dark or light skin tone.\nResults: Using the imbalanced dataset, we found that the model is significantly better at detecting\nmalignant images in lighter tone (selection rate 27.5%) images versus darker tones (selection rate\n15.9%). This results in a disparate impact of 0.577. Using the balanced dataset, we found that the\nmodel is also significantly better at detecting malignant images in lighter (selection rate 50.0%)\nversus darker tones (selection rate 34.2%). This results in a disparate impact of 0.684. Using the\nimbalanced or balanced dataset to train the model still results in a disparate impact well below the\nstandard threshold of 0.80 which suggests the model is biased with respect to skin tone.\nConclusion: The results show that typical skin cancer machine learning models can be tone biased.\nThese results provide evidence that diagnosis or tone imbalance is not the cause of the bias. These\nresults provide evidence the models are learning tone related features. Other techniques will be\nnecessary to identify and address the bias in these models, an area of future investigation.", "sections": [{"title": "Introduction", "content": "In the past decade numerous skin image datasets have been publicly released Cassidy et al. [2022] and used for\ndeveloping models to predict skin diseases, including skin cancer [Jain et al., 2021, Liu et al., 2020]. However, most of\nthe datasets are sampled from lighter skin tone populations [Wen et al., 2022]. This raises concerns about the developed\nmodels being biased towards lighter skin tones.\nRecent work on machine learning bias proposes methods to detect and mitigate model bias [Green et al., 2024]. Machine\nlearning models utilise relationships between the input features and the target feature to make predictions. There are\nconcerns when the input features include or are strongly correlated with protected features (Barocas et al. [2023]), also\nknown as protected characteristics. These protected features include race, age, gender, nationality, and religion. In\ncertain applications it is acceptable to use these features to improve model predictions, such as healthcare applications\nBonham et al. [2016]."}, {"title": "Material and Methods", "content": "In this section we first provide details about the study approach and datasets used. We consider model bias and\nevaluation measures. We then present details about the deep CNN model used in the study followed by the experimental\nsetup."}, {"title": "Study design and population", "content": "The study here is retrospective design that harness the work of deep convolutional neural network model to classify the\nimages as malignant or benign. We used the machine learning approach where we trained the model using the images\nfrom publicly available International Skin Imaging Collaboration (ISIC) dataset. The model is trained to perform the\nspecific classification of the images as malignant or benign. The images data comprises from patients globally, ensuring\na wide representation of demographics across the world. The diversity in skin images allow for generalisation of our\nmodel across different population. Patient information such as name, age, sex and ethnicity are anonymised in the ISIC\ndataset, where we ensure the research ethical standards. The images with skin tone came from the following institutes\n\u2022 1517 Hospital Italiano de Buenos Aires\n\u2022 1459 Sydney Melanoma Diagnostic Center at Royal Prince Alfred Hospital, Pascale Guitera\n\u2022 709 Memorial Sloan Kettering Cancer Center\nThough this does not provide global representation, it does provide a more diverse patient population. Future work will\nbe to use 81,000 images that will include an even larger patient population."}, {"title": "Dataset Ethics", "content": "Our study uses images publicly available under Creative Commons licenses from the International Skin Imaging\nCollaboration website Codella et al. [2018]. To the best of our knowledge, the images used in this study do not\nhave personally identifiable information and are in compliance with current US Health Insurance Portability and\nAccountability Act (HIPPA) laws and EU General Data Protection Regulations."}, {"title": "Dataset", "content": "The International Skin Imaging Collaboration has over 20 datasets from around the world and included ~81,155\ndermoscopic images with the diagnosis of {malignant,benign}. Of this, there are 3,623 images that also have the"}, {"title": "Evaluation Metrics", "content": "In this section we first define terms used in the confusion matrix, followed by common evalaution metrics, and finish\nwith a discussion of which metrics we chose."}, {"title": "Confusion Matrix Definition", "content": "For a given image, a binary skin cancer classification model will predict either benign or malignant and can be compared\nagainst the true diagnosis for the image. For binary classification, the more general terms are positive and negative. We\ndefine positive to indicate malignant and define negative to indicate benign. With these definitions, we further define\nthe following terms.\n\u2022 true positive (TP) the classifier predicts malignant and the true diagnosis is malignant,\n\u2022 true negative (TN) the classifier predicts benign and the true diagnosis is benign,\n\u2022 false positive (FP) the classifier predicts malignant and the true diagnosis is benign,\n\u2022 false negative (FN) the classifier predicts benign and the true diagnosis is malignant.\nGiven a large number of images to predict, the first step in evaluating the model is to compute number of true positives,\ntrue negatives, false positives, false negatives. These four terms are usually presented in a confusion matrix with the\ntrue predictions on the diagonal and the false predictions off-diagonal. This presentation is useful and we use it in our 3\nsection."}, {"title": "Model Evaluation Metric", "content": "The confusion matrix is used to compute several metrics that can be used to evaluate the performance of the model.\nGiven the confusion matrix, the accuracy is defined as follows.\naccuracy = \\frac{TP+TN}{TP+TN+FP + FN}                               (1)\nThe accuracy includes all terms and measures the number of times the model is correct versus the number of predictions.\nFor a balanced dataset, where the number of malignant images is roughly equal to the benign images, accuracy is a useful\nmetric to evaluate the performance of a classifier. However, when there are significantly more benign than malignant\nimages in the datasets, known as a class imbalance, accuracy is not best choice. This is because a trivial majority class\nclassifier, that just predicts benign, can achieve a high accuracy. Other scores consider different combinations of TP,\nTN, FP, and FN (such as precision, recall, and f1) and are also often used (for both balanced and imbalanced datasets).\nFor our purposes, we are concerned with the disparate impact (also derived from TP, TN, FP, and FN). However,\nregarding accuracy, the model needs to perform better than the majority class classifier. For the imbalanced dataset, the\nmajority class classifier would achieve an accuracy of 74%. Any model that cannot achieve a higher accuracy would\nindeed be considered poor. A model that is learning to differentiate between malignant and benign should result in an\naccuracy higher than the majority class percentage."}, {"title": "Model Training", "content": "The number of epochs (i.e. how long to train the model) can greatly affect the accuracy of a model. During each epoch\nthe training loss is generally expected to decrease. A model is considered trained when the training loss ceases to\ndecrease. The model is said to have learned all it can from the dataset. The training stops using some criteria. We decide\nto stop when the training loss generally stops decreasing. This stopping criteria is generally termed early stopping (this\ncan also combined with differences between the training and validation losses). Importantly, this approach is often used\nand would be typical of a deployed model. We provide a detailed bias analysis using the trained models."}, {"title": "Evaluating Model Bias", "content": "Many statistical criteria have been proposed to determine whether a model is biased towards certain protected features.\nWe narrow our focus to the following three defined by Barocas, et al. Barocas et al. [2023], and detailed in Table 1.\n\u2022 Independence\n\u2022 Separation\n\u2022 Sufficiency\nWhere random variable A is a protected feature, Y is the binary classifier, and Y is the target variable. We also represent\npositive as a 1 and negative as a 0. We choose to use the Independence criterion, also known as disparate impact, as it\ncaptures the notion of equal selection. Considering Y = 1 as selection, the condition requires all groups to be accepted\nequally. Allowing a and b to be swapped, the equation is often relaxed to meet the following ratio where e is often taken\nto be 0.2 Feldman et al. [2015].\nDisparate Impact = \\frac{selection rate for group a}{selection rate for group b} = \\frac{P{Y = 1 | A = a}}{P{Y = 1 | A = b}} >1-\u03b5 \\qquad(2)"}, {"title": "Model Bias Solutions", "content": "There are numerous proposed solutions to addressing model bias including regulatory and algorithmic. Here we only\nconsider algorithmic solutions to meet a criteria. The algorithmic solutions generally fall into one of three techniques.\n\u2022 Preprocessing of the data\n\u2022 Modify the model's weights during training\n\u2022 Postprocessing (reweighing) the model predictions\nEach has known strengths and weaknesses and will be explored in future work. We provide here to facilitate our later\ndiscussion."}, {"title": "Model Architecture", "content": "Many models have been proposed for image classification, notably those based using Convolutional Neural Networks\nsuch Residual and Inception architectures He et al. [2016]. The canonical CNN model has a series of convolutional\nblocks, then a flatten layer, followed by series of linear blocks, and finally the prediction / output layer Lecun et al.\n[1998]. To allow more inspection of the model, we define a custom CNN architecture for training and classifying images\nas either malignant or benign. We note that we do not expect this custom model to outperform current techniques but it\nshould outperform a majority class classifier."}, {"title": "Hyper-parameter Tuning", "content": "Deep learning architectures have a number of hyper-parameters that can have a significant effect on the model's\nprediction and computational training performance. These hyper-parameters are often selected manually using the\nmodeller's intuition or, more recently, using automated search methods. The presented architecture has the following\nhyper-parameters. The ranges explored are given in the braces.\n\u2022 Number of convolutional blocks [2,6]\n\u2022 Number of units for each convolutional layer [16,256]\n\u2022 Number of linear blocks [2,6]\n\u2022 Number of units for each linear layer [16,256]\n\u2022 Percentage for each dropout layer [0.2,0.5]\n\u2022 Learning rate step size [0.1, 0.00001]\n\u2022 Optimiser {Adam, SGD, RMSProp}\nThrough a combination of automated (using the Optuna framework Akiba et al. [2019]) and manual hyper-parameter\ntuning we found three convolutional blocks and two linear blocks provided the best results. This also informed our\nselection of blocks, units, and dropout percentage. The tuning process also selected the Adam optimiser with a learning\nrate of 0.00001."}, {"title": "Experimental Setup", "content": "Figures 5 and 5 depict the experimental setup. The given input dataset is first randomly split into a training and\nvalidation set (2/3 and 1/3 respectively). For each epoch the training set is used in the forward pass through the model\nto produce predictions. The predictions are compared against the validation set and results computed, to include the\ntraining loss, accuracy. and selection rates. The backward pass then updates the model's weights. This process is\nrepeated each epoch with the results saved to be plotted and compared."}, {"title": "Imbalanced Dataset Results", "content": "To provide some indication that the disparate impact is not just spurious, we add a control feature to compare against.\nThe control feature has random values and, therefore, not correlated with the diagnosis (or skin tone). We would expect\nthe control to have a disparate impact between 0.8 and 1.2. Anything outside (above or below) these thresholds would\nindicate bias.\nWe trained the model for approximately 200 epochs. We compute the disparate impact for the tone and control features.\nWe also compute the training loss to provide an indication the model is learning. The tone and control disparate impact\nare shown from 0.0 to 1.3. The training loss is also (conveniently) shown in the same range. Figure 7 shows the results\ncomputed for each epoch. We see significant variation in the early epochs when the model is not well trained. After\nabout 50 epochs the control remains within the expected, unbiased range. We can clearly see that the tone disparate\nimpact is consistently well below the threshold. Between 150 to 200 epochs, it appears the model has stopped learning\n(the training loss flattens)."}, {"title": "Balanced Dataset Results", "content": "We trained the balanced model for approximately 350 epochs. This was trained for more epochs than the imbalanced\ncase as it took more epochs before the training loss flattened (around epoch 300). Figure 7 shows the results computed\nfor each epoch. Similar to the imbalanced results, we see significant variation of the disparate impact in the early\nepochs. After 50 epochs the control remains within the expected range but the tone remains below the 0.80 threshold,\nthough not as significant as with the imbalanced disparate impact."}, {"title": "Model Accuracy Results", "content": "Table 6 shows the accuracy for the balanced and imbalanced models (technically the models trained using the imbalanced\nand balanced datasets) computed using Equation 1. Note that the accuracy is derived from the last epoch after training\nloss has stopped decreasing. We compare against the majority class classifier derived from Figures 2 and 3."}, {"title": "Discussion", "content": ""}, {"title": "Disparate Impact for Imbalanced and Balanced Models", "content": "The balanced model resulted in a disparate impact of 0.71. The imbalanced model resulted in a disparate impact of\napproximately 0.58. Regardless of the relative number of malignant versus benign diagnosed images or relative number\nof light versus dark tone images, the disparate impact indicates the model is better at selecting light tone images versus\ndark tone images. The disparate impact for both imbalanced and balanced datasets is below the 0.80 threshold indicating\nmodal bias.\nThe results suggest that the balanced model results may have less bias than the imbalanced model. However, there is\nclearly randomness in the disparate impact as the model's are trained each epoch. We did repeat these experiments"}, {"title": "Dataset Size, Class Imbalance, and Model Accuracy", "content": "We chose to address the class imbalance by under sampling benign diagnosis and then under sampling light tone images.\nOf the 3,623 images, this left roughly 500 images to train and test the models. Deep learning models typically require\nlarge amounts of data to achieve high accuracy. Our model's accuracy was not significantly larger than the majority\nclass classifier, likely due to the fact this there was minimal training data.\nWith the 3,623 images, the model accuracy was reasonable. We are careful to note that many high accuracy models\ninclude other features, such as exposure to the sun, location of the lesion, etc. Tschandl et al. [2018].\nOne solution to increase the number of images is to develop a tone classifier to label the skin tone of the remaining\n78,000 images. Another, possibly more controversial, approach would be to use generative AI approaches to create\nmore malignant and dark images."}, {"title": "Model Bias Evaluation", "content": "As discussed in Section 2.4.4, there are several criterion to evaluate model bias. It can be shown that satisfying one of\nthe three criteria presented by Barocas, et al. Barocas et al. [2023], will violate one or both the other criterion. We\nchose to evaluate the model using Independence (via the disparate impact measure). If we used one of the solutions\nmentioned in Section 2.5 to meet this criteria, we would necessarily violate the Separation criteria. Unfortunately, this\nmeans we cannot simply declare our model \"free from bias\" given we satisfy one of these criteria. Nevertheless, not\nmeeting any criteria or being completely unaware of a model's bias is less desirable.\nBalancing the dataset for tone and diagnosis is considered a data preprocessing solution to address the bias. Though this\ndid not work, we believe further data preprocessing techniques are important to consider. This motivates exploration of\nother techniques (outlined in Section 2.5) to mitigate the model bias."}, {"title": "Skin Tone Definition", "content": "We somewhat arbitrary decided to delineate light and dark tone using the Fitzpatrick skin types. We note there are no V\nor VI skin types in ISIC archive. There are other definitions of skin tone and future work will explore defining light and\ndark using these definitions Mukwende et al. [2020]."}, {"title": "Model Accuracy Comparison", "content": "The model accuracy results were compared against the majority class classifier to ensure the model is learning how\nto differentiate the diagnosis versus just guess randomly or guessing the majority class (i.e. benign). Though both\nmodels performed better than the majority classifier, the model trained on the balanced dataset performed relatively\nbetter. However, these accuracy results are well below state-of-the-art Brinker et al. [2018] for more sophisticated deep\nneural networks trained on much larger datasets. We believe this is due to the relatively small datasets used to train\nour models. This work was limited due to the lack of skin type annotations. Future work will consider these more\nsophisticated models with larger datasets derived using a tone classifier."}]}