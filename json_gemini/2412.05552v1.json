{"title": "SAME: Learning Generic Language-Guided Visual Navigation with State-Adaptive Mixture of Experts", "authors": ["Gengze Zhou", "Yicong Hong", "Zun Wang", "Chongyang Zhao", "Mohit Bansal", "Qi Wu"], "abstract": "The academic field of learning instruction-guided visual navigation can be generally categorized into high-level category-specific search and low-level language-guided navigation, depending on the granularity of language instruction, in which the former emphasizes the exploration process, while the latter concentrates on following detailed textual commands. Despite the differing focuses of these tasks, the underlying requirements of interpreting instructions, comprehending the surroundings, and inferring action decisions remain consistent. This paper consolidates diverse navigation tasks into a unified and generic framework we investigate the core difficulties of sharing general knowledge and exploiting task-specific capabilities in learning navigation and propose a novel State-Adaptive Mixture of Experts (SAME) model that effectively enables an agent to infer decisions based on different-granularity language and dynamic observations. Powered by SAME, we present a versatile agent capable of addressing seven navigation tasks simultaneously that outperforms or achieves highly comparable performance to task-specific agents.", "sections": [{"title": "1. Introduction", "content": "Acquiring the capability to understand natural language commands and navigate in unfamiliar environments constitutes a fundamental competency for embodied intelligence. In recent years, a great variety of navigational tasks has emerged, each defined by distinct navigation objectives, from broad, high-level goals to detailed, low-level directives, highlighting exploration and instruction-following, respectively. However, these tasks are mostly formulated as isolated research problems, and the specialized methods developed for each are typically not generalizable to others (Figure 1a). For example, structured memory tailored for efficient target object exploration, contextual guidance for vague in-"}, {"title": "2. Background", "content": "In this section, we begin by introducing the formulation of language-guided visual navigation tasks under varying levels of language granularity. We employ DUET [16] to illustrate a general cross-modality navigation model for language-guided navigation. Through a series of contrastive experiments, we analyze the interconnections among these navigation tasks, understanding their underlying contradictions and providing a proof of concept for our approach."}, {"title": "2.1. Navigation Tasks Formulation", "content": "Given an instruction represented by a sequence of L word embeddings W = {wi}L i=1, an agent navigates on a predefined undirected graph G = (V, E), where V represents the navigable nodes and E represents the connectivity edges. The agent is expected to execute a sequence of actions {s0, a0, s1, a1, ..., sT, aT } to navigate to the target position vT , as specified by the instruction W. Each action at transit the agent from the current state st = (vt, \u03b8t, \u03c6t) to st+1 = (vt+1, \u03b8t+1, \u03c6t+1) which includes its spatial location vt \u2208 V , heading angle \u03b8t, and elevation angle \u03c6t, and generate a new visual observation Ot. Additionally, the agent maintains a record of the state history ht and adjusts the conditional transition probability between states, defined as St = T(st+1 | at, st, ht, Ot, W ), where T denotes the conditional transition probability distribution. We categorize the language instruction W into three classes by granularity as:\n\u2022 Fine-grained VLN: W describes the sequence of actions {s0, a0, s1, a1, ..., st, aT } step-by-step.\n\u2022 Coarse-grained VLN: W refers to an out-of-sight target at vT , e.g., \"the cold tap in the first bedroom on level two\".\n\u2022 Zero-grained VLN: W refers to a single term indicating the target (e.g., an object category in OBJECTNAV).\nMultimodal Navigation Policy At each step, the agent receives a local visual observation Ot = {O360 i=1 }, consisting of 36 view images, and a language instruction W. These are encoded separately by a vision encoder and a language encoder into visual feature \u00d4t and language feature W. DUET [16] incorporates \u00d4t and agent's state st to obtain node embedding Vt and maintain a topological map Gt = {Vi}i\u2264t as navigation history, details are provided in the supplementary. A local cross-modal encoder is utilized"}, {"title": "to excite visual features conditioned on language features:", "content": "CrossAttn(\u00d4t, W) = Softmax ( \u00d4TWWT \u221ad )VW. (1)\nThe output embedding from the final layer of view Ot is denoted as \u00d4t. Similarly, a parallel global cross-modal encoder is implemented to encode language-conditioned map Gt = CrossAttn(Gt, W). Denote the output embedding of node Vt as vt, the navigation score given by the local and global cross-modal encoder is calculated as:\nsi l = FFN(\u00f4l t ), sg = FFN9(vl i ),\nsi = \u03c3otsi l + (1 \u2212 \u03c3t)sg i (2)\n(3)\nwhere FFN is a two-layer feed-forward network and \u03c3 is a learnable parameter.\nDatasets We select three typical datasets for our contrastive experiments based on instruction granularity:\n\u2022 R2R [6]: The fine-grained VLN task which consists of 22k human-annotated navigational instructions. On average, an instruction contains 32 words, and the ground-truth path is formed by 7 steps, totaling 10 meters.\n\u2022 REVERIE [80]: The coarse-grained VLN task inherits the trajectories from R2R but provides high-level instructions that describe a target object. On average, instructions contain 21 words, and the length of ground-truth paths ranges from 4 to 7 steps.\n\u2022 OBJECTNAV-MP3D [8]: We use the standard split of 11 validation scenes from the Habitat OBJECTNAV dataset [92] in MP3D [9], which consists of 21 goal categories. We utilized human demonstration from Habitat-Web [87] as training data, details are discussed in Section 2.2."}, {"title": "Evaluation Metrics", "content": "We follow 5 standard metrics in VLN literature to assess the agent performance, including Trajectory Length (TL), Navigation Error (NE), Success Rate (SR), normalized inverse of the Path Length (SPL) [5], normalized Dynamic Time Warping (nDTW) [40]."}, {"title": "2.2. What are the Conflicts in Navigation Multi-tasks Learning?", "content": "To successfully train a versatile navigation agent, it is essential to understand the underlying contradictions that prevent unified model learning and to gain insights into the best practices for learning from diverse data sources. Following the discussion in Section 1, we unify the training data by transferring 70k human demonstration OBJECTNAV data from Habitat-Web into trajectories in the discrete environment.\nData Transformation in Discrete Environment The original trajectories in OBJECTNAV are constructed as sequences of continuous viewpoint positions, averaging 243"}, {"title": "Fine-grain Language Understanding Benefits Target-oriented Navigation", "content": "We conduct multi-task training on DUET [16] initialized from LXMERT [97] using various data mixtures and report held-out inference results for datasets not included in the training. To address the visual gap between Habitat-rendered images and Matterport3Drendered images, we map the R2R and REVERIE trajectories onto G* and employ Habitat-rendered images for this experiment. The results are presented in Table 1.\nOur findings reveal several key insights: (1) Mixing training data across different tasks reduces performance compared to training on individual tasks requiring higher-level language understanding capacity. For example, in the R2R task, incorporating additional data results in a 2-3% drop in success rate (SR), while in REVERIE, combining OBJECTNAV data leads to a substantial SR decrease of 6-7%, even lower than the zero-shot performance achieved when training exclusively with R2R data (39% SR vs. 35% SR). (2) Training with fine-grained human-annotated instruction-trajectory pairs proves advantageous for OBJECTNAV, yielding a 2-4% SR improvement. (3) Models trained exclusively on VLN data achieve strong zero-shot performance on OBJECTNAV (above 40% SR); however, models trained only on OBJECTNAV data perform poorly on tasks that demand sophisticated language understanding (with only ~ 15% SR). Additionally, models trained solely on R2R (fine-grained VLN) data achieve 39% SR on REVERIE (coarse-grained VLN) and 55% SR on OBJECTNAV. This suggests that when the target is visible or only minor exploration is needed, R2R-trained models can successfully infer the location. These observations lead to two"}, {"title": "main conclusions:", "content": "1. Fine-grained language understanding improves target-oriented navigation, as visual-semantic understanding is enhanced through learning vision-language alignment; however, models trained exclusively on target-oriented data lack the language comprehension required to follow complex instructions.\n2. Training with simple data mixing is insufficient to achieve optimal performance for tasks demanding both exploration and detailed instruction interpretation (coarse-grained VLN)."}, {"title": "3. Mixture of Experts for Versatile Language-guided Visual Navigation", "content": "The insights from Section 2.2 motivate the need for a method to manage conflicts that arise during multi-task learning. To address this, we propose a new State-Adaptive Mixture of Experts (SAME) approach. SAME employs multiple specialized expert networks E = {f1, ..., fN} which could be switched during each step in a navigation episode conditioned on the agent's state by a routing mechanism R. In this way, we differentiate the learning of distinct navigation skills, such as exploration and instruction-following, while facilitating the sharing of common navigational knowledge like visual semantic understanding."}, {"title": "3.1. MoE Formulation", "content": "A subset of experts is activated in a sparsely gated MoE layer during each forward pass. The router predicts the probability of each expert being assigned:\nP (xi) = Softmax (R (xi))\n(4)\nR(xi) = Wxr,\n(5)\nwhere xi is the routing feature extracted from the input x, W \u2208 Rd\u00d7N is a trainable layer, d is the hidden dimension and N is the number of experts. The weighted sum of the outputs from experts with top-k routing scores noted as set T is computed as the output:\nMoE(x, xi) = \u2211 i\u2208T P(xr)i fi(x) (6)\nTypically a load balancing loss [27] is implemented to encourage an even distribution of input across the N experts:\nLbalance = \u03bb N \u2211i=0 F2 i Di . (7)\nFi = 1 K \u2211K k=1 {argmax P(xi) = i} , (8)\nD = 1 K \u2211K k=1 P(xi). (9)\nwhere F represents the fraction of inputs processed by each expert fi, and D represents the fraction of router probability allocated for expert fi.\nTask-wised MoE and Token-wised MoE Recently, most research on Mixture of Experts (MoE) has focused on transformer-based Large Language Models (LLMs), where MoE operates at the token level to process each individual input token, as illustrated in Figure 2. Concurrently, MoE is also extensively explored in computer vision multi-task learning, where expert modules are routed at the task level. These two routing features are formulated as follows:\nxtoken i = xi (10)\nxtask = Etask = W\u03a6T (11)\nwhere xi is the i-th token in input x, and Etask is the task embedding for task with index T.\nBeyond assigning each task T to specific experts through a hard assignment, it might be helpful for an agent to select appropriate navigation skills based on the language instruction. To this end, we formulate a language-aware expert selection mechanism, represented as follows:\nxtext = WCLS (12)\nwhere WCLS denotes the [CLS] token for text feature W."}, {"title": "3.2. State-Adaptive Experts Selection", "content": "Our initial experiments with the above token-wise and task-wise MoE in multi-task navigation learning yielded suboptimal results (to be presented in \u00a73.3), prompting us to reconsider a more feasible MoE formulation for the sequential decision-making process in navigation. We observed that the density of language information the agent receives which must align with visual observations to determine an action can vary significantly across different timesteps in different tasks. In other words, the agent's state interpretation should be inherently generalizable to address distinct navigation problems. In light of this, we introduce SAME, a multimodal State-Adaptive expert selection mechanism:\nxmulti = Wm ( 1 L \u2211L i=1 \u00d4i \u2016 WCLS ) (13)\nwhere a linear layer Wm \u2208 R2h\u00d7h is implemented to merge the concatenation of the mean visual feature \u2211Li=1 \u00d4i over different views and the text [CLS] token WCLS .\nBesides, we also investigate the effect of adding task information for expert selection:\nxtext_task = xtext + Etask (14)\nxmulti_task = xmulti + Etask (15)"}, {"title": "3.3. Comparison on MoE Routing", "content": "Following the above, we can see the key to distinguishing specific and shared navigational knowledge learning is learning the routing mechanism in MoE. In this section, we investigate the routing strategies for a versatile language-guided navigation policy.\nExperiment Setup We conduct contrastive experiments to compare the routing mechanism, Matterport3D-rendered images [6] are used for R2R and REVERIE. MoE experts are deployed to the visual queries (will be discussed in Section 3.4), and results are shown in Table 2. As discussed in Section 2.2, we demonstrate that learning fine-grained language understanding can enhance target-oriented navigation learning, motivating us to take advantage of the powerful vision-language-action pre-train conducted in VLN [14, 16, 32, 34, 74, 81, 106]. Throughout the subsequent experiments, we initialized our model from ScaleVLN [106], which scales the VLN data from 14K to 4.9M by generating synthetic data to perform pertaining."}, {"title": "VLN Pretrain Benefits Multiple Navigation Task Learning", "content": "As shown in Table 2, there is a significant improvement on all tasks when initializing with VLN pre-trained weights compared to directly performing multi-task tuning on SAME initialized from general vision-language pretrain LXMERT [97] (w/o Pretrain), featured by a ~ 15% SR increase on R2R."}, {"title": "SAME Facilitates Navigation Skill Sharing", "content": "Comparing different MoE routing types, we highlight a significant improvement of ~ 3% SR and ~ 2% SPL in SAME routing on REVERIE. We hypothesize that this improvement is due to the nature of the coarse-grained VLN task, where the agent must alternate between exploration, in cases where language guidance lacks detail, and adhering closely to language instructions when precisely localizing the target. SAME routing enables a flexible selection of experts to manage these distinct navigation behaviors based on the current observation and language input, allowing the agent to learn transferable knowledge across tasks. This flexibility also results in the SAME agent achieving the highest average SPL of 48.31% across all tasks.\nCompared to the multi-task-tuned DUET (w/o MoE) under the same experimental conditions, all MoE methods show a significant performance increase (3-5% SR) on the REVERIE task, with no notable performance drops on other tasks. This aligns with our findings in Section 2.2, suggesting that coarse-grained VLN performance is influenced when co-trained with other language instruction types, likely due to model overfitting to the R2R task. Our proposed MoE approach addresses this issue effectively.\nFurthermore, we examine hard assignments with specific experts dedicated to different tasks by directly routing through task embeddings. This approach results in per-"}, {"title": "3.4. Which Part of the Navigation Policy Learns Different Navigation Behaviour?", "content": "Since the initial application of MoE in transformer architectures [27, 54, 127], MoE has acted as an enhancement for Feed-Forward Network (FFN) modules within these models. Concurrently, some studies have integrated multi-head attention layers with MoE to further enhance performance while managing computational costs. In this section, we analyze the impact of MoE applied at different components of the transformer model, specifically focusing on the FFN, visual queries Wq, textual key Wk, and value Wv with SAME under the same experiment setup described in Section 3.3. The results are shown in Table 3.\nMoE applied to different components yields varying performance across tasks. Notably, the best overall performance is observed when applying MoE to the visual query, achieving the highest SPL on all tasks with fewer parameters. This suggests that utilizing MoE at the visual query level within the cross-attention layer is particularly effective. This effectiveness likely stems from the multimodal policy's control over diverse navigation behaviors within the cross-attention layer, where specialized visual query experts allow the agent to more accurately determine the next action by adjusting attention scores over visual embeddings from multiple viewpoints. While FFN-based MoE enhances performance compared to non-MoE models, it is surpassed by MoE configurations that integrate with attention layers, highlighting the crucial role of cross-modal attention in successful action selection.\nIn summary, the experimental results indicate that employing MoE with visual query experts and routing based on multimodal features from visual observations and language instructions (SAME) allows the agent to dynamically adapt to environmental visual changes while staying aligned with language guidance, thereby enhancing robust performance across different language-guided navigation tasks."}, {"title": "4. Experiments", "content": "In this section, we conduct multi-task tuning based on the previous discussion. We conduct a thorough evaluation of SAME across seven major navigation benchmarks, complemented by a series of ablation studies on training details to establish best practices for effective multi-task training in language-guided navigation.\nDatasets Besides the R2R, REVERIE, and OBJECTNAVMP3D, we include 4 other datasets for evaluation.\n\u2022 RxR-EN [52]: English split of the RxR dataset, which contains longer instructions compared to R2R and non-shortest trajectory from starting point to ending point.\n\u2022 CVDN [99] requires the agent to comprehend the conversation history and infer the correct next actions based on the dialogue context. For evaluation, we use the standard metric, Goal Progress (GP), which calculates the average difference between the completed trajectory length and the remaining distance to the goal.\n\u2022 SOON [125]: Similar to REVERIE, the instructions describe target rooms and objects, with an average length of 47 words. The expert paths vary in length from 2 to 21 steps, with an average of 9.5 steps.\n\u2022 R2R-CE [50]: Transfering the discrete trajectories in R2R to continuous 3D scans rendered by Habitat [92], allowing an agent to navigate freely in open space while requiring interaction with obstacles.\nImplementation Details We build upon the DUET architecture, and replace all the visual query layers in the cross-attention with MoE layers. We adopt SAME routing for each MoE layer. We initialize from the pre-trained weights of ScaleVLN [106] and utilize CLIP ViT-B/16 [84] as the visual encoder. We use ScaleVLN, R2R, RxR-EN, CVDN, REVERIE, SOON, and Habitat-Web as the training data with a sampling ratio of 10:1:1:1:1:1:2 without mixing different data in a batch. We follow [3, 16, 44] to utilize DAgger [90] algorithm to obtain interactive supervision from the simulator. The training objective L = LDAG + \u03bbLbalance is the combination of DAgger loss and MoE load balancing loss in Equation 7, balanced by coefficient \u03bb = 0.8. The model is fine-tuned using AdamW optimizer [71] with a learning of 1 \u00d7 10\u22125 for 20k iterations with a batch size of 16 on a single 80G NVIDIA A100 GPU."}, {"title": "4.1. Comparison with State-of-the-Art Models", "content": "Comparison in Discrete Environment We first compare our method with current SOTA methods in the discrete MP3D environment [6]. The datasets are organized in Table 4 by language instruction granularity and complexity, ranging from fine-grained and complex to coarse-grained and simple, from left to right. We classify previous methods into two categories, the first one is a separate model for each task, which fine-tunes a distinct set of parameters for"}, {"title": "Effect of Training Schema", "content": "We investigate different training schemas for SAME, specifically the training algorithm and data mixing strategy. For the training algorithm, we compare training with imitation learning only on teacher actions [6] to training with DAgger, where at each time step, an agent performs an action sampled from the predicted probability of its action space and minimizes the loss between the sampled action and the ground truth. This method"}, {"title": "Effect of MoE Routing Balance Coefficient", "content": "We investigate the effect of MoE routing balance loss coefficient \u03bb in Table 7. When \u03bb is large, the balancing loss would force the model to evenly select different experts for samples in the same batch, when \u03bb decreases, such constraint is weakened. The model trained with X = 0.2 in row 1 performs worse than all other variants, demonstrating the significance of the balancing loss in MoE training."}, {"title": "5. Related Work", "content": "5.1. Vision-and-Language Navigation\nThe development of a navigation agent capable of interpreting and acting upon unrestricted linguistic instructions to navigate through unfamiliar, photorealistic environments has been a longstanding goal within the field of Vision-and-Language Navigation [7, 35, 52, 80, 99, 118, 125]. Approaches to this problem have primarily addressed two main areas: (1) Vision-Language Alignment: Some studies [14, 32, 34, 36, 57, 60, 74, 81, 119] leverage generic visual-linguistic representations [18, 59, 61, 96, 97]. Others incorporate additional supervision through data augmentation"}, {"title": "5.2. ObjectGoal Navigation", "content": "Approaches to learning to understand visual semantics and perform object goal navigation [9, 22, 23, 25, 29, 4648, 85, 110, 115] could be categorized into two streams: (1) Modular Pipelines with Learned Modules [10, 11, 33, 86, 87, 108]: This paradigm integrates learning into specific modules by leveraging explicit scene representations like semantic map [11, 33, 86], or simply employ object detectors or segmentors [75, 87]. (2) End-to-end Learning with RL or IL [1, 88, 109, 112, 114]: these methods benefit from visual representation [76, 113], auxiliary task [114], and data augmentation [75] to generalize to unseen environments."}, {"title": "5.3. Mixture of Experts", "content": "Mixture of Experts (MoE) [41, 43, 93] models utilize multiple specialized experts along with a routing network that dynamically assigns tasks based on their complexity. Task-oriented MoE enhances the model's capacity to learn both specific and shared knowledge in computer vision, without significantly increasing computational costs [19, 31, 68, 77, 89, 94]. Sparsely activated MoE is widely employed in LLMs to reduce computational costs while enabling the training of gigantic models with trillions of parameters through sparse activation [27, 54, 93]. MoE assigns different experts for instruction tuning in recent LLMs [21, 42, 111], optimizing the learning process by focusing on specific tasks within the overall model architecture. To address task conflicts and enhance generalization in unseen tasks, some methods [19, 31, 105] employ various strategies to optimize the selection and aggregation of expert outputs in MoE."}, {"title": "6. Conclusion", "content": "This paper unifies a diverse range of navigation tasks within a cohesive language-guided navigation framework. It examines the fundamental challenges of sharing common knowledge while leveraging task-specific capabilities in navigation learning. We propose the State-Adaptive Mixture of Experts (SAME), which enables an agent to make decisions by integrating multi-granularity language inputs and dynamic observations. We believe SAME can guide the learning towards versatile language-guided navigation agents."}, {"title": "A. OBJECTNAV and VLN Training", "content": "Besides learning shareable knowledge and task-specific skills from the model design of SAME, another challenge under the unified language-guided navigation framework is to determine the most effective approach to facilitate the learning of agents' language comprehension capacity and grounding it in action prediction. Analyzing the rationality within the contrasting research focuses on ObjectGoal Navigation and VLN offers insights into this challenge. Specifically, we observe that the primary differences lie in the training data and training methods used. In this section, we discuss and make strategic decisions in SAME training regarding these two aspects, to address the above challenge. \nWhen language instructions are minimal, the task reduces to OBJECTNAV, where the learning objective is the semantic affinity between the target object and the visual perception, and leveraging episodic memory for strategic exploration without redundant revisits, since no extract information is provided from the language instruction. From the data aspect, it is proven to be effective to learn strategical exploration through human demonstration, and data collection is done in several works [87, 88]. From the training aspect, OBJECTNAV combines learning where and how to move, incorporating semantic perception and low-level collision avoidance (FORWARD, TURN_LEFT, TURN_RIGHT, STOP) within a continuous environment [92].\nOn the contrary, VLN requires higher-level language understanding, where the agents not only need to understand the visual semantics of the environment but also need to align past observation and action sequence with"}, {"title": "B. DUET Revisit", "content": "SAME builds upon the design of the Dual-scale Graph Transformer (DUET) [16]. DUET incorporates a text encoder to process instructions and employs both global and local branches to facilitate cross-modal reasoning at coarse and fine scales."}, {"title": "B.1. Text and Visual Embedding", "content": "DUET's text encoder leverages a 12-layer transformer, initialized with LXMERT [97]. For visual embedding, each node's visual observation comprises 36 view images, covering 12 horizontal and 3 vertical directions. To differentiate between these views, a directional embedding Eang is added to the visual features , which are extracted by the vision encoder. Since DUET incorporates all 36 view images to construct the spatial observation, navigable adjacent nodes are only visible in a subset of these views, referred to as navigable views. To account for this, a navigable embedding Enav is also included. The final visual embedding is processed by a 2-layer transformer to encode spatial relationships between views, producing panoramic view embeddings:\nSelfAttn (  +  + Eang + Enav)."}, {"title": "B.2. DUET Local Branch", "content": "This section focuses on the local branch of DUET, which predicts actions based on the current node's instruction and egocentric observation. Unlike the global branch, no graph-level information is utilized beyond local observations."}, {"title": "B.2.1. Local Visual Embedding", "content": "The panoramic view embedding is augmented with two types of location embeddings. The first represents the relative location of the current node with respect to the starting node, encoding long-distance directional relationships. The second represents the egocentric directions of adjacent views at the current node, enabling actions such as turn right."}, {"title": "B.2.2. Local Cross-Modal Encoding", "content": "The local branch employs a standard 4-layer cross-modal transformer to capture relationships between vision and language. During action prediction, a mask is applied to exclude unnavigable views and action logits are computed only for the navigable views at the current node."}, {"title": "B.3. DUET Global Branch", "content": "This section introduces the global branch of DUET, which tasks the topological map representation Gt and encoded language instruction W to predict actions by selecting any nodes on the graph."}, {"title": "B.3.1. Node Embedding", "content": "For each node on the graph, two additional encodings are applied: a location encoding Eloc and a navigation step encoding Estep. The location encoding represents the egocentric"}, {"title": "B.3.2. Global Cross-Modal Encoding", "content": "The encoded node features and word embeddings are processed through a 4-layer graph-aware cross-modal transformer, which is composed of the following two key components, as illustrated in Figure 2.\nCross-Attention Layer This layer models the relationships between the global map and the instruction, enabling cross-modal alignment. SAME examine applying State-Adaptive MoE on the visual query or textual key and value Wv in this layer.\nGraph-Aware Self-Attention Layer (GASA) Unlike standard self-attention mechanisms which rely solely on visual similarity, the GASA module incorporates the graph's structural information to refine attention computation, formulated as follows:\nGASA(V) = Softmax (   ( Et))V, (17)\nwhere A(Et) represents the spatial affinity matrix, comprised of pairwise distances among all observed nodes. By incorporating this spatial context, GASA ensures that the model prioritizes spatially or topologically proximate nodes, which are often more contextually relevant than visually similar but distant nodes.\nEach block in the global branch concludes with a Feed-Forward Network (FFN). Additionally, SAME explores applying the State-Adaptive MoE mechanism to this FFN, as depicted in Figure 2 of the main paper."}, {"title": "C. Full Results on All VLN Tasks", "content": "We show the full results of SAME on all the tested VLN benchmarks in Table 8."}]}