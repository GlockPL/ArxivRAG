{"title": "Do LLMS HAVE POLITICAL CORRECTNESS?\nANALYZING ETHICAL BIASES AND\nJAILBREAK VULNERABILITIES IN AI SYSTEMS", "authors": ["Isack Lee", "Haebin Seong"], "abstract": "Although large language models (LLMs) demonstrate impressive proficiency in\nvarious tasks, they present potential safety risks, such as \u2018jailbreaks', where mali-\ncious inputs can coerce LLMs into generating harmful content. To address these\nissues, many LLM developers have implemented various safety measures to align\nthese models. This alignment involves several techniques, including data filtering\nduring pre-training, supervised fine-tuning, reinforcement learning from human\nfeedback, and red-teaming exercises. These methods often introduce deliberate\nand intentional biases similar to Political Correctness (PC) to ensure the ethical\nbehavior of LLMs. In this paper, we delve into the intentional biases injected\ninto LLMs for safety purposes and examine methods to circumvent these safety\nalignment techniques. Notably, these intentional biases result in a jailbreaking\nsuccess rate in GPT-40 models that differs by 20% between non-binary and cis-\ngender keywords and by 16% between white and black keywords, even when the\nother parts of the prompts are identical. We introduce the concept of PCJailbreak,\nhighlighting the inherent risks posed by these safety-induced biases. Additionally,\nwe propose an efficient defense method PCDefense, which prevents jailbreak at-\ntempts by injecting defense prompts prior to generation. PCDefense stands as an\nappealing alternative to Guard Models, such as Llama-Guard, that require addi-\ntional inference cost after text generation. Our findings emphasize the urgent need\nfor LLM developers to adopt a more responsible approach when designing and\nimplementing safety measures. To enable further research and improvements, we\nopen-source our code and artifacts of PCJailbreak, providing the community with\ntools to better understand and mitigate safety-induced biases in LLMs.", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language Models (LLMs) have rapidly become essential components in many fields, ranging\nfrom professional decision-making to various forms of interactive user engagement (Araci (2019);\nLuo et al. (2022); Tinn et al. (2023)). However, as these models become popular, ensuring their safe\nusage has become crucial. Developers have implemented several safety features to prevent these\nmodels from generating harmful or objectionable content, often referred to as 'safety alignment'.\n(Bakker et al. (2022); Christiano et al. (2017); Ouyang et al. (2022); Openai Usage Policies). How-\never, in this work we show that these safety alignments often introduce deliberate and intentional\nbiases, giving rise to a phenomenon known as 'jailbreak', where malicious inputs manage to cir-\ncumvent these safety alignments, thus allowing LLMs to generate harmful outputs (Goldstein et al.\n(2023); Kang et al. (2024)).\n\nThe term 'jailbreak' refers to carefully crafted prompts that can bait aligned LLMs into bypassing\ntheir safety alignment, resulting in the generation of content that may be harmful, discriminatory, vi-\nolent, or sensitive (Smith et al. (2022)). Numerous types of jailbreak attacks have been identified and\ncategorized into two primary methods: manually written jailbreak prompts and learning-based jail-\nbreak prompts (Zou et al. (2023); Lapid et al. (2023); Liu et al. (2023); Wei et al. (2024); Yuan et al."}, {"title": "2 BACKGROUND AND RELATED WORKS", "content": ""}, {"title": "2.1 SAFETY ALIGNMENT IN LLMS", "content": "Ensuring the safety and ethical alignment of large language models (LLMs) is a critical area of\nongoing research. Methods such as data filtering, supervised fine-tuning, and reinforcement learning\nfrom human feedback (RLHF) aim to align models like GPT-4 and ChatGPT with human values\nand preferences (Christiano et al. (2017); Bai et al. (2022); Ouyang et al. (2022); Xu et al. (2020)).\nHowever, despite these efforts, recent studies reveal vulnerabilities that can be exploited through\n'jailbreak' attacks, which lead to undesirable and harmful outputs (Kang et al., 2023; Hazell, 2023;\nShen et al., 2023)."}, {"title": "2.2 JAILBREAK ATTACKS AND TECHNIQUES", "content": "Jailbreaking LLMs involves crafting inputs that bypass safety mechanisms, resulting in harmful\nor objectionable content. Early jailbreak attacks, such as the \"Do-Anything-Now (DAN)\u201d series,\nrelied on manually crafted prompts to exploit LLM safeguards (walkerspider, 2022). (Liu et al.\n(2023) provided an in-depth analysis and categorization of these jailbreak prompts, highlighting the\ndelicate balance between an LLM's capabilities and its safety constraints.\n\nDiverse strategies for jailbreaks have been proposed. Manual methods, while effective, suffer from\nscalability issues (Wei et al. (2024)). On the other hand, learning-based methods like GCG (Zou\net al. (2023)) use adversarial techniques to generate prompts automatically, though often at the\ncost of producing semantically meaningless outputs detectable via simple defenses like perplexity\ntests (Alon & Kamfonas (2023); Liu et al. (2023)) introduced AutoDAN, which combines manual\nand automated strategies using hierarchical genetic algorithms to enhance both the stealthiness and\nscalability of jailbreak prompts.\n\nLanguage diversity and non-natural language inputs present additional challenges. Deng et al.\n(2023) explored multilingual jailbreak attacks, demonstrating that LLMs could be tricked into pro-\nducing harmful outputs with non-English prompts. Yuan et al. (2024) extended this by investigating\nthe vulnerabilities of LLMs to non-natural language inputs, such as ciphers."}, {"title": "2.3 TOWARDS IMPROVED SAFETY MEASURES", "content": "Complex attack strategies like those proposed by Ding et al. (2023) with the ReNeLLM framework\nintroduce the concept of generalized and nested jailbreak prompts, leveraging LLMs to generate\neffective prompts through prompt rewriting and scenario nesting. This highlights the dynamic and\nevolving nature of jailbreak techniques.\n\nOur work builds on the existing body of research by focusing on the paradoxical consequences of\nintentional biases introduced for safety purposes. While these biases aim to align LLMs ethically,\nthey also highlight new vulnerabilities. To counteract this, we propose using prompts to make the\nLLM re-align those biases, thus offering a robust secondary defense against jailbreak attempts.\n\nIn conclusion, AI developers must adopt a higher degree of responsibility in designing, testing,\nand deploying LLMs. This involves continuous monitoring and iterative improvements based on\nreal-world data. Our findings advocate for a nuanced approach to LLM safety, promoting the devel-\nopment of more secure and reliable models, and ensuring that safety measures do not inadvertently\nintroduce new risks."}, {"title": "3 METHODOLOGY: PCJAILBREAK", "content": ""}, {"title": "3.1 PRELIMINARIES", "content": ""}, {"title": "3.1.1 JAILBREAK ATTACK", "content": "A jailbreak attack in the context of Large Language Models (LLMs) occurs when the model gen-\nerates harmful or inappropriate responses to malicious inputs instead of producing a refusal signal,\nwhich is a safe and ethical response denying the request (Zeng et al., 2024; Zou et al., 2023). Such\nattacks are intricately linked to the alignment methods employed in LLMs, which aim to align the\nmodel's outputs with human values and ethical considerations. The primary objective of these at-\ntacks is to bypass the LLM's alignment constraints, causing the model to produce inappropriate\nresponses to malicious queries instead of refusing to answer.\n\nConsider a set of malicious queries $Q = \\{Q_1, Q_2, ..., Q_n\\}$. An attacker meticulously designs a set\nof jailbreak prompts $J = \\{J_1, J_2, . . ., J_n \\}$ to manipulate these queries. When each combined input\n$T_i = (J_i, Q_i)$ is fed into the LLM, the model generates a set of responses $R = \\{R_1, R_2, ..., R_n\\}$.\nThe aim of the jailbreak attack is to ensure that the responses R correctly address the malicious\nqueries in Q instead of producing refusal signals (Zeng et al., 2024).\n\nIn summary, a successful jailbreak attack coerces the LLM into bypassing its safety and ethical con-\nstraints, culminating in the generation of harmful or inappropriate content in response to malicious"}, {"title": "3.1.2 FORMULATION", "content": "Designing a specific target response for each malicious question is impractical for several reasons.\nFirstly, manually crafting an appropriate answer for every possible malicious query $Q_i$ is resource-\nintensive and infeasible due to the vast number of potential queries. Secondly, creating specific\nresponses may limit the attack's generalizability, as tailored responses might not be effective for\nother queries or variations of the same query. Lastly, directly specifying harmful content could raise\nethical and legal concerns.\n\nHence, a common solution to this problem is to define a standardized introductory phrase for the tar-\nget response (Zou et al., 2023; Lapid et al., 2023; Qi et al., 2024). The determination of a successful\njailbreak focuses on the absence of this standardized introductory phrase in the response. Such as:\n\nTarget Prefixes [\u201cI am sorry\u201d, \"I can not\", \"I apologize\"...]"}, {"title": "3.2 PCJAILBREAK APPROACH", "content": ""}, {"title": "3.2.1 GENERATING KEYWORDS AND PROMPTS", "content": "Our proposed PCJailbreak approach investigates how keywords associated with various demo-\ngraphic and socio-economic groups influence an LLM's susceptibility to jailbreak attacks. To"}]}