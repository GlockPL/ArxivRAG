{"title": "GNN-based Decentralized Perception in Multirobot Systems for Predicting Worker Actions", "authors": ["Ali Imran", "Giovanni Beltrame", "David St-Onge"], "abstract": "In industrial environments, predicting human actions is essential for ensuring safe and effective collaboration between humans and robots. This paper introduces a perception framework that enables mobile robots to understand and share information about human actions in a decentralized way. The framework first allows each robot to build a spatial graph representing its surroundings, which it then shares with other robots. This shared spatial data is combined with temporal information to track human behavior over time. A swarm-inspired decision-making process is used to ensure all robots agree on a unified interpretation of the human's actions. Results show that adding more robots and incorporating longer time sequences improve prediction accuracy. Additionally, the consensus mechanism increases system resilience, making the multi-robot setup more reliable in dynamic industrial settings.", "sections": [{"title": "I. INTRODUCTION", "content": "Collaborative robots are poised to become a cornerstone of Industry 5.0 [1], emphasizing human-centric design solutions to meet the flexibility demands of hyper-customized industrial processes [2]. Significant efforts have been directed toward identifying key enabling technologies to enhance robotic systems with advanced situational awareness and robust safety features for human coworkers. Two pivotal technologies stand out: individualized human-machine interaction systems that merge the strengths of humans and machines, and the application of AI to improve workplace safety [3].\nIn highly collaborative and hazardous scenarios such as manufacturing facilities, robots must develop a holistic understanding of their environment to ensure efficiency and safety. While humans excel at anticipating events due to their spatial reasoning, understanding of others' behavior, and planning under uncertainty [4], robotic systems lack such innate capabilities. However, they can leverage distributed sensing networks and algorithms to model spatial and temporal relationships within a scene.\nAlgorithms for predicting human behavior and future actions have been explored across domains such as pedestrian crossing prediction [5]-[7], intent interpretation in assistive robotics [8], [9], and forecasting human poses in collaborative workspaces [10], [11]. Most approaches rely on human detection and tracking, using local features like pose, velocity, and location [12]\u2013[15]. However, these methods often overlook the relationships between humans and surrounding objects, leading to poor performance when encountering unfamiliar human behavior and limiting their ability to anticipate further into the future.\nRecent advancements in graph-based models show good potential for spatiotemporal intent prediction [16]. They demonstrate the potential of graph formalisms to emphasize relationships between entities rather than their individual properties, offering generalizable insights [17]. In parallel, Graph Neural Networks (GNNs) have gained prominence in applications such as multi-robot path planning [18], collaborative perception [19], and navigation in complex environments [20]. Building on this foundation, we extend GNN-based approaches to scenarios with a broader range of human actions, leveraging multi-robot systems to develop a shared understanding of the environment.\nMulti-robot systems provide significant advantages over single-robot systems, including enhanced robustness, scalability, and flexibility [21]. While many implementations rely on centralized control, a decentralized approach offers additional benefits, such as improved system flexibility and fault tolerance [22]. Decentralization, however, presents its own challenges, particularly in achieving effective coordination and collective decision-making among robots [23]."}, {"title": "II. METHOD", "content": "In manufacturing setups, numerous dynamic processes such as machining, assembly, and material handling occur simultaneously. Human workers perform multiple tasks like machine tending and assembly, requiring adaptability. These dynamic conditions pose significant challenges for robots, which must operate efficiently while prioritizing human safety.\nAlthough manufacturing represents just one subset of potential robot deployment scenarios, it serves as a baseline and motivation for designing our multi-robot system. We define intent prediction as the anticipation of a human operator's future action within a specific time horizon. For this study, we consider time horizons of 1s, 2s, and 3s into the future, based on the Stopping time and distance metric outlined in ISO 10218-1:2011 safety standards, Annex B(normative) [27]. This metric relates to the time required for a robot to detect a human, initiate deceleration, and come to a complete stop, which establishes the minimum threshold for the perception pipeline's prediction capabilities. Given the relatively low speeds of industrial mobile robots\u00b9, the selected time horizons are deemed sufficient for safety and operational efficiency. For instance, if a human is detected at a distance of 8 meters while both the robot and the human are moving towards each other, the robot has less than 2 seconds to react\u00b2. Instead of performing an emergency stop, our system can utilize intent prediction to smoothly adjust the robot's path and avoid potential collisions by predicting into the future.\nTo evaluate our approach, we modeled a small manufacturing facility where multiple operations occur simultaneously.", "A. Problem statement": ""}, {"title": "C. Feature Extraction and Representation", "content": "We extract feature vectors for every object mentioned in Table I and the human in the scene using a ResNet50 [30] backbone, which outputs a one-dimensional vector of length 512."}, {"title": "D. Spatial Relationship", "content": "To establish a spatial understanding of the scene, we represent the human and surrounding objects as a star-shaped graph, with the human positioned at the center. Each object is connected to the human node with undirected edges weighted by their Euclidean distances (derived from 2D bounding boxes). These edges form an adjacency matrix A, which is augmented with self-loops to create a modified adjacency matrix  $A' = A + I$. The node features, initially  $H^{(0)}$, are are iteratively transformed through a 2-layer Graph Convolutional Network (GCN) [32], leveraging the modified adjacency and degree matrices to aggregate information effectively. This shallow 2-layer GCN effectively handles simple star-shaped graphs, minimizing overfitting while ensuring efficient, real-time inference on resource-constrained robotic platforms. Further architectural details are available on the project website."}, {"title": "E. Temporal Relationship", "content": "To grasp the temporal relationship of the scenario, at least two options have been leveraged in the literature: Gated Recurrent Units (GRUs) [33] and Long Short-Term Memory networks (LSTMs) [34]. We select the first due to their lower memory consumption and overall efficiency [35]. We have implemented two instances of GRUs: the ego-GRU and the collective-GRU. The ego-GRU processes information from the ego robot without incorporating data from other robots, while the collective-GRU processes aggregated information from multiple robots.\nImplementing these two GRUs allows us to gain insights into the multi-robot system. We can evaluate how a single robot performs in understanding the spatial scene and processing temporal information. Additionally, we can assess the improvement in prediction accuracy when a robot incorporates information from other robots, as well as how much it contributes to the overall prediction accuracy of the system.\nThe input vectors for the ego-GRU and collective-GRU are illustrated in Figure 3. For the ego-GRU, we concatenate the output of the final layer of the GNN-which is a 1D vector of length 128\u2014with the flattened output of the human keypoints detector (a 1D vector of length 34). This provides a local, ego-centric spatiotemporal prediction."}, {"title": "F. Decentralized Collective Prediction", "content": "Messages are used to share node embeddings (outputs of GNNs from other robots) and combined in the collective-GRU model. We use Zenoh [36], which allows robots to share information efficiently and also serves as a middleware for ROS 2. We adopt a straightforward aggregation strategy, averaging all the node embeddings to produce a unified representation. This input format is designed to be both adaptable and scalable, as all inputs are reduced to a fixed-sized vector regardless of the number of participating robots. By leveraging this aggregated data, each robot enhances its predictions with information collected from other robots' perspectives. However, at this stage, achieving convergence to a single, unified decision about human intent remains unresolved."}, {"title": "G. Consensus Mechanism", "content": "To achieve consensus among multiple robots regarding human intent, we implement a consensus algorithm that aggregates individual predictions, weighting them by each robot's visibility ratio and prediction confidence. Inspired by majority rule mechanisms in collective decision-making [37]\u2013[39], this approach is designed to scale with varying numbers of robots.\nWe define the visibility ratio for each robot as $v_i = \\frac{N_{\\text{detected}, i}}{N_{\\text{total}}}$, where $N_{\\text{detected}, i}$ is the number of detections by robot i and $N_{\\text{total}}$ is the total number of detections across all robots. The prediction confidence is defined as $c_i = \\text{softmax}(z_{i,k}/T)$, where $z_{i,k}$ is the logit score for the top selected class, and $T$ is a temperature parameter that tunes the softmax distribution. Let $M$ be the number of robots. We then normalize these values as $\\tilde{v}_i = v_i / \\sum_{r=1}^{M} v_r$ and $\\tilde{c}_i = c_i / \\sum_{r=1}^{M} c_r$, where $v_r$ and $c_r$ denote the visibility ratio and confidence for each robot r, respectively. The weighted vote for each robot is calculated as:\n$V_i = \\alpha \\tilde{v}_i + \\beta \\tilde{c}_i$.\nwhere $\\alpha$ and $\\beta$ are scalar weights that balance the contributions of visibility and confidence, respectively. A higher visibility ratio $v_i$ indicates a more comprehensive view of the scene by robot i. Each robot's predicted action (from a four-class softmax) and its corresponding weighted vote $V_i$ are aggregated across all robots. For additional details, please refer to the project website."}, {"title": "III. EXPERIMENTS", "content": "We evaluated our approach using a realistic simulation environment provided by NVIDIA's IsaacSim [40]. This platform offers a lifelike industrial setting complete with ROS support, human trajectory planning, and synthetic dataset generation capabilities. The simulation environment we created is depicted in Figure 1.\nHuman motion and actions were simulated using Isaac-Sim's default custom motion model [40], which incorporates reactive collision avoidance and velocity-based position estimation to navigate both static and dynamic obstacles. During experimentation, the human's goal position for each station remained constant, while the initial position was randomly varied with each simulation run to generalize learning.\nWe deployed between one and four Carter robots [40] from IsaacSim's library. Each robot operated independently, using its onboard sensors to perceive the environment. Robots shared their observations with others in the system, enabling collaborative inference of the human's intended action."}, {"title": "A. Evaluation Strategy", "content": "We perform simulation-in-the-loop testing, where the starting positions of the robots and the navigation paths differed from those used during training. Ground truth values and predictions were recorded, and the ground truth data was aggregated to match the length of the prediction horizon.\nTo evaluate the system's ability to infer human actions under varying conditions, we designed a comprehensive strategy incorporating three distinct scenarios, each progressively increasing in difficulty. These scenarios tested the robustness and adaptability of the system in action inference:\nScenario 1: Ideal Conditions In the first scenario, there are no obstacles present in the environment, and the robots remain stationary. This setting allows the human worker to move in a straight path toward the intended station.\nScenario 2: Static Obstacles Introduced In the second scenario, we introduce static obstacles into the environment while keeping the robots stationary. The presence of obstacles forces the human to navigate around them, resulting in less predictable trajectories.\nScenario 3: Dynamic Environment with Moving Robots The third scenario is the most complex and dynamic. In addition to static obstacles, the robots are also moving within the environment. This creates a highly dynamic setting where both the human and the robots are in motion, and the scene changes continuously.\nTo further challenge the system, two of the stations were placed in close proximity - details of the environment layout are available on the website - testing the system's ability to distinguish between similar actions when potential destinations were very close together.\nTo validate our approach, we compared its performance against two alternative methods inspired by prior work [16]:"}, {"title": "B. Dataset", "content": "We collected three datasets corresponding to three different simulation scenarios. These datasets were designed to capture the motion, appearance, and state of human operator within the scene from the perspectives of multiple mobile robots positioned at various locations. The datasets consist of temporal sequences; each sequence is a set of frames where the human is either: moving towards a goal, transitioning from one goal to another, and stationary. To ensure effective learning, we maintained an equal distribution of all three cases. Moreover, since we are dealing with a multi-robot system, we extended the datasets to include sequences for all possible combinations involving two robots, three robots, and four robots, respectively. A standard split of 60%/20%/20% was implemented for training, validation, and testing."}, {"title": "IV. RESULTS AND DISCUSSION", "content": "We present the simulation results for data processed at 10 Hz. Overall, implementing a multi-layered, multi-robot perception strategy significantly enhanced the robustness of predictions. Table II shows the system's performance across different simulation scenarios described in Section III. As expected, the accuracy of the system decreases in more challenging scenarios.\nThe GNN models, which rely solely on spatial information, demonstrate consistent accuracy across all scenarios. This consistency indicates that spatial features alone provide a stable but limited understanding of the environment. This can be explained based on the fact that these rely only on the spatial information albeit without leveraging temporal data. Consequently, their performance remains limited to immediate spatial relationships. In contrast, the collective-GRU consistently outperforms the ego-GRU across all scenarios, demonstrating the value of incorporating data from multiple robots."}, {"title": "A. Effect of Number of Robots", "content": "Figure 5 confirms the main advantage of our strategy: increasing the number of robots enhances the accuracy of action inference under the same conditions. For these experiments, we used Scenario 3 from our simulation setup (as described in Section III), observed data from the past 2 seconds, and predicted 2 seconds into the future while processing frames at 10Hz."}, {"title": "B. Time Horizon Analysis", "content": "We present the results for different time horizons in Figure 4, comparing the performance of using one robot versus three robots, with all data processed at 10 Hz. Overall, the results show that having more past information improves prediction accuracy. Specifically, predictions based on 2 seconds (20 frames) of past data outperform those based on 1 second (10 frames). For these cases, predictions extend 10 and 20 frames into the future, respectively, demonstrating better performance with increased historical data aggregation.\nWhen the observation and prediction windows were extended to 3 seconds (30 frames), the system's performance became inconsistent. While the best-case predictions exceeded those of the 2-second case, the worst-case predictions were significantly worse. This inconsistency arises from the highly dynamic nature of the scene. Accumulating 30 frames increases the likelihood of encountering rapid movements or missed human detections, resulting in sequences of varying quality and more pronounced prediction fluctuations compared to shorter time horizons.\nAdditionally, processing longer time horizons required more computational resources and introduced greater latency. This highlights a trade-off between achieving higher potential accuracy and maintaining timely responsiveness, depending on the application's requirements. For our purposes, a 2-second time window offered the optimal balance between prediction accuracy and system responsiveness."}, {"title": "C. Temporal resolution", "content": "It is crucial to analyze how much temporal information is necessary to optimize computation and memory usage without compromising accuracy. We conducted additional experiments to compare the system's performance based on the number of frames used for prediction. Processing frames at 10 fps, we examined the effect on accuracy when using all 10 frames, 5 frames, and 3 frames. At runtime, we first accumulate all 10 frames and then pick the required number of frames at uniform intervals. Figure 6 illustrates the results. Our observations indicate that utilizing all 10 frames leads to better predictions, highlighting the importance of sufficient temporal data for accurate action inference."}, {"title": "D. Comparison with Alternative Strategies", "content": "Table IV presents the results from Scenario 3 of the simulation, using a 2-second temporal window, and compares them against the CVM and 1D-CNN strategies. While the CVM [43] performs well in Scenario 1, where no obstacles are present and the human moves along a straight path, its accuracy significantly declines in more complex scenarios. Specifically, the CVM struggles to predict human actions when the human changes direction, remains stationary, or stops.\nIn contrast, our spatiotemporal approach outperforms the 1D-CNN, demonstrating the advantage of integrating spatial and temporal information through GNNs and GRUs. The combination of spatial modeling via GNNs and temporal modeling through RNNs provides a comprehensive understanding of the scene, capturing both relationships between objects and the temporal evolution of actions. This holistic perspective enables our spatiotemporal pipeline to deliver more accurate predictions than methods relying solely on temporal sequences (such as the 1D-CNN) or simplistic motion assumptions (as in the CVM)."}, {"title": "E. Additional performance parameters", "content": "Robots with weaker or sparser graphs-where fewer objects in the scene are linked to the human node-tended to produce less accurate and more unpredictable predictions. The limited number of objects reduces the robot's ability to model relationships within the scene effectively, leading to decreased prediction performance.\nFurthermore, individual robot predictions improved when the graph explicitly linked the source (the human's current position) to the destination (the intended goal or station). This highlights the importance of graph structures that capture critical relationships between key entities in the environment, as these connections enhance the robot's spatial understanding.\nErroneous data in small teams In experiments with a relatively small multi-robot system of up to four robots, robots with weaker or sparser graphs not only made poor individual predictions but also negatively influenced the collective perception when their data was aggregated. This suggests that the quality of each robot's perception plays a significant role in the effectiveness of the consensus mechanism, particularly in smaller teams. These findings align with prior research [42], underscoring the need for robust individual perception to ensure reliable collective decision-making in small multi-robot systems.", "Quality of graphs": ""}, {"title": "V. CONCLUSION", "content": "This paper presents a decentralized multi-robot shared perception pipeline that predicts the intent of a human operator in the scene by exploiting the spatial and temporal relationships between the human and surrounding objects. We demonstrated that having multiple robots observing the same scene from different viewpoints and sharing information can help develop a robust and scalable perception system.\nCurrently, our system is limited to handling a single human in the scene, and expanding to multiple humans is a priority for future work. We also intend to enrich the GNN's edge features-currently based only on Euclidean distances from 2D images-by incorporating higher-dimensional inputs derived from additional onboard sensors. Beyond that, we plan to integrate Vision-Language Models (VLMs) to enhance scene understanding in dynamic, unfamiliar environments. Such models could enable more robust semantic representations, improving the robot's ability to interpret human actions and intentions in previously unseen scenarios.\nMoreover, the consensus mechanism currently relies on the quantification of the robots' visibility but does not account for the quality of the view. Future work will focus on integrating view quality metrics into the consensus mechanism. We also plan to expand the action library to include more complex and realistic scenarios, closely resembling real-world industrial environments. Finally, we intend to implement and test our approach on real robots to validate its effectiveness in practical applications."}]}