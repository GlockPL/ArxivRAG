{"title": "Process Reward Model with Q-Value Rankings", "authors": ["Wendi Li", "Yixuan Li"], "abstract": "Process Reward Modeling (PRM) is critical for complex reasoning and decision-making tasks where\nthe accuracy of intermediate steps significantly influences the overall outcome. Existing PRM approaches,\nprimarily framed as classification problems, employ cross-entropy loss to independently evaluate each\nstep's correctness. This method can lead to suboptimal reward distribution and does not adequately\naddress the interdependencies among steps. To address these limitations, we introduce the Process Q-value\nModel (PQM), a novel framework that redefines PRM in the context of a Markov Decision Process.\nPQM optimizes Q-value rankings based on a novel comparative loss function, enhancing the model's\nability to capture the intricate dynamics among sequential decisions. This approach provides a more\ngranular and theoretically grounded methodology for process rewards. Our extensive empirical evaluations\nacross various sampling policies, language model backbones, and multi-step reasoning benchmarks show\nthat PQM outperforms classification-based PRMs. The effectiveness of the comparative loss function is\nhighlighted in our comprehensive ablation studies, confirming PQM's practical efficacy and theoretical\nadvantage. Our codes can be found at https://github.com/WindyLee0822/Process_Q_Model", "sections": [{"title": "1 Introduction", "content": "Process reward modeling (PRM) plays a crucial role in tasks where the quality of intermediate steps is\npivotal to achieving the final outcome [1]. In complex problem-solving scenarios, such as mathematical\nreasoning or multi-step decision-making [2, 3, 4], the accuracy and effectiveness of each intermediate action\ncan significantly influence the overall success. Unlike outcome reward models (ORM) [5], which focus solely\non the final result, PRM provides detailed feedback at each stage of the process. By capturing the value of\nintermediate steps, PRM allows for a deeper understanding of how each action contributes to the overall\ngoal. This granular approach supports the development of more sophisticated and reliable systems that can\nnavigate complex tasks with greater accuracy.\nExisting research typically frames PRM as a classification problem [6, 2, 1, 7], where each intermediate state\nis classified as correct or incorrect. Specifically, for a trajectory {x, a\u2081,a\u2082,..., aH} where x, a, H represent\na question, a reasoning step, and the trajectory horizon, a reasoning state si = (x, a\u2081:i\u22121) comprises the\ninstruction & and text pieces previously generated (e.g. reasoning steps in reasoning tasks). Current research\nuses cross-entropy loss to maximize the probability p(ci|si) for each reasoning state, where ci is the label\nindicating whether si is correct. While this approach has shown empirical success, it has notable limitations.\nClassification-based methods treat each state independently and do not account for the dependencies and\nnuances among states within a trajectory. This can lead to suboptimal reward assignments, as these methods\noften ignore the relative importance of different steps and their influence on the overall process. Furthermore,\nthese approaches lack theoretical grounding on how they approximate the desired reward function.\nTo address the challenges, we propose a novel framework\u2014Process Q-value Model (PQM)\u2014which frames\nPRM as a Q-value ranking problem. This framework allows us to capture the interdependencies among\nstates and provides a more nuanced evaluation of each step's contribution to the overall process. Specifically,\nour framework is grounded in the Markov Dynamic Process, where each action aH is a text piece generated\nbased on the current state sH = (x, a\u2081:h\u22121). The LLM policy \u03c0(aH|x, a\u2081:h\u22121) maps the observed state to\na distribution over the action space. The process reward model intuitively scores each action aH based"}, {"title": "2 Preliminaries", "content": "LLMs for reasoning. Large language models have demonstrated impressive abilities on challenging reasoning\ntasks across a wide range of math, science, and coding challenges. Chain of thought [9] and related techniques\n[10, 11, 12, 13] have emerged as dominant methods, linking the question and the final answer by a series\nof intermediate reasoning steps. For a given question x and its corresponding answer y, extensive studies\n[9, 14, 11, 12, 13] have shown that prompting LLMs to arrive at solutions via intermediate steps {a\u2081,a\u2082,...}\ncan produce more interpretable and accurate results. To generate the final answer, each intermediate step is"}, {"title": "3 PQM: Process Reward Model with Q-Value Rankings", "content": "In this section, we introduce our framework PQM, which frames process reward modeling as a Q-value ranking\nproblem. In what follows, we first define a Q-value function for reasoning tasks, which implicitly defines a\nreward function for each intermediate step (Section 3.2). Then, we derive the desirable Q-value rankings among\nintermediate reasoning steps (Section 3.3), by which we can train PRMs to approximate the intermediate\nQ-values by a comparison-based loss (Section 3.4). Lastly, we demonstrate that classification-based PRMs\ncan be viewed as a special case within our theoretical framework (Section 3.5)."}, {"title": "3.1 Deterministic MDP for LLMs", "content": "Formulations of MDP. A standard Markov Dynamic Process can be formulated as M = (S, A, T, r, p,H),\nwhere S is the state space, A is the action space, T : S \u00d7 A \u2192 \u2206(S) is the transition kernel, r is the reward\nfunction, p denotes the initial state distribution, and H is the maximal number of interaction steps. A policy\nin MDPs, denoted by \u03c0 : S \u2192 \u0394(A), maps each state to a distribution over actions. The interaction between\nthe environment M and the agent can be described as follows. Initially, the starting state s\u2081 is sampled from\nthe initial distribution p. At each step t, the agent observes the current state st and selects an action at based\non its policy. The environment then transits to the next state st+1, which is sampled from the distribution"}, {"title": "3.2 Defining Q-function Implicitly Defines a Reward Function", "content": "Recall that the state-action value Q(s, a) [19, 20, 21] typically represents the expected benefit of taking a\nspecific action a to achieve a correct answer. In the context of reasoning tasks, we define the Q-value function\nas the success probability of achieving the correct final answer. Specifically, the Q-value function is defined as\nQ* (a1:t-1, at) := \u03c3\u207b\u00b9 (E\u2090\u209c\u208a\u2081:H~\u03c0(.|\u2090\u2081:\u209c)(I(x, a\u2081:H)),\nwhere is a policy, H is the maximum step number, \u03c3 is the sigmoid function and \u03c3\u207b\u00b9 is its inverse function\nto ensure Q\u2208 R. I is an indicator function, which equals 1 if the trajectory reaches the correct answer of x,\nand 0 otherwise. For simplicity, we also denote Q(a\u2081:t\u22121, at) as Qt when there is no ambiguity."}, {"title": "3.3 Optimal Q-Value Ranking", "content": "In this subsection, we derive the Q-value rankings among intermediate reasoning steps, by which we can later\ntrain PRMs to approximate the intermediate Q-values by a comparison-based loss.\nWe start by introducing a few lemmas that are useful for deriving our main Theorem 3.5. For any two actions\nan, am s.t. n < m in a solution trajectory \u0442 = (x, a\u2081, a\u2082,...\u0430\u043d), we have\nP*(\u03c4|a1:n) = P*(a1:m|a1:n)P*(\u03c4|a1:m) + P*(a1:m|a1:n)P*(\u03c4|a1:m),\nP*(\u03c4|a1:n) = P*(a1:m|a1:n)P*(\u03c4|a1:m) + P*(a1:m|a1:n)P*(\u03c4|a1:m),\nwhich directly follows the Bayesian factorization. P*(a1:m|a1:n) denotes the possibility that policy generate\ncorrect state a1:m conditioned on a wrong state a1:n. For a solution \u03c4 = (x, a\u2081, a\u2082,...\u0430\u043d), recall the Q function\nin Eq.2, we define Q*(a1:t-1, at) = \u03c3(Q* (a1:t\u22121, at)) = P*(T|a1:t) where \u03c3 is the sigmoid function. Since \u03c3 is\na monotonically increasing function, hence when Q*(a1:m-1, am) > Q*(a1:n-1, an) for any two steps am, an,\nwe have Q*(a1:m\u22121, am) > Q*(a1:n\u22121, an). Then we can obtain the following lemma."}, {"title": "3.4 Comparative Loss Function for Optimizing Q-Value Rankings", "content": "Given the optimal Q-value ranking derived in Theorem 3.5, we now propose a new comparative loss that\ntrains RPM to approximate the intermediate Q-values. While the ranking relationship can be captured by the\nclassical Plackett-Luce (PL) ranking model [23, 24], there are significant limitations when using the canonical\nPL loss directly in this context. The standard PL loss is designed to handle general ranking scenarios without\naccounting for the varying degrees of discrepancy within the ranking. However, in our case, the Q-value gaps\nbetween correct and incorrect steps are often highly pronounced (cf. Lemma 3.4), leading to a situation\nwhere the standard PL model may not adequately capture the importance of these differences. As discussed\nin Section 4.3, this results in suboptimal performance, since the PL loss does not differentiate sufficiently\nbetween steps that are only marginally different in rank versus those with substantial Q-value gaps."}, {"title": "3.5 Classification-based PRM is a special case of Q-value approximators", "content": "We show that the previous classification-based PRM can be cast as a special case of our framework under\ncertain conditions. To illustrate this, consider an extreme scenario where the assumptions outlined in\nAssumption 3.1 are satisfied, namely, when P*(at+1|a1:t) \u2192 1 and P*(at+1|a1:t) \u2192 1. According to the Q-\nfunction definition provided in Eq. 2 and leveraging Bayesian Factorization, it follows that classification-based\nPRMs approximate Q-value rankings under these conditions."}, {"title": "4 Experiments", "content": "Datasets and metrics. Following previous research [6, 1, 7], we evaluate PRMs based on their verification\nability through best-of-n sampling. The metric, BON@n, assesses the correctness of the most preferred\ntrajectory selected by the PRM from n candidates for each question. During the evaluation, the PRM first\nscores every step within each trajectory. Consistent with prior studies [6], the final score of a trajectory is\ndetermined by the minimum score of its individual steps. The test corpus includes 128 solutions for each\nquestion from GSM-Plus [25] and MATH500 [8] datasets. These solutions are sampled from three policy\nmodels with strong performance in math tasks with different scales: MetaMath-Mistral-7B [3], MuggleMath-\n13B [26], Llama-3-70B-Instruct [27]. We utilize the existing off-shelf corpus, Math-Shepherd [6], as our\ntraining corpus.\nBaselines and implementation details. Consistent with prior works [6, 1], we evaluate the performance\nof PRM by comparing it against the outcome reward model (ORM). We also compare our comparative loss\nwith the BCE loss, which is employed in Math-Shepherd. Additionally, some research [28, 29] adopt more\nstrict MSE loss to minimize the distance between the predicted value and the label. We implement MSE loss\nwith two versions: 0-1 label and iterative Monte Carlo Tree Search (MCTS) to estimate the continuous label\nfor MSE loss as in [28]. For the model architecture, we adopt general reward model frameworks, incorporating\na value head on top of the Deepseek-7B-base LLM [2]. This value head projects the latent representation\nof the model into a scalar value, facilitating the evaluation of intermediate steps and trajectories. More\ndetailed implementation information, including specific configurations and experimental setups, can be found"}, {"title": "4.2 Main Results", "content": "Verification performance across different policy models. Experimental results are shown in Table 1.\nOur proposed PQM demonstrates significant performance improvements over all baselines. Firstly, PQM\noutperforms the outcome reward model, which is consistent with prior findings that process-based methods\nprovide a more nuanced evaluation of intermediate steps. Moreover, when compared to classification-based\nPRM models using BCE or MSE loss, PQM shows a notable advantage. For example, when verifying\nsolutions sampled from the Llama-3-70B-Instruct model, PQM improves the accuracy from 39.8% (BCE) to\n51.4%, a direct 11.6% improvement on the challenging MATH500 benchmark. This result underscores the\neffectiveness of PQM in capturing the relative quality of different steps within a trajectory, addressing the\nlimitations of BCE loss which treats each step independently without considering their interdependencies.\nPQM outperforms MSE loss with either 0-1 label or MCTS search. Compared to 0-1 label, MCTS search\nrequires more computational resources but only leads to marginal performance enhancement. This may stem\nfrom its Q-value definition with sophisticated heuristics, and theoretically biased estimation of Q-values in\nMCTS. Other results on both the MATH500 and GSM-Plus datasets across three policy models further confirm\nthe efficacy of PQM. In these benchmarks, PQM consistently outperforms existing methods, demonstrating\nsuperior performance across different policy scales and test sets, validating the efficacy of ranking-based\nprocess reward modeling.\nPQM performance can be boosted by self-consistency [10]. By sampling multiple trajectories\nand then selecting the final answer that appears most frequently, self-consistency can further enhance the"}, {"title": "4.3 Further Studies", "content": "In ablation studies, we keep most of the experimental settings consistent with the main experiments, except\nthat we use data with a length of less than 512 tokens, totaling 390k data out of 440k data, to save the\ntraining cost. The detailed hyperparameters are shown in Appendix B.\nImpact of margin \u00a7. In this ablation, we investigate how the margin ( in our loss function influences the\nperformance. We implement several variations with ( = 0,2,4,8,16. The experimental results are shown in\nTable 3, along with loss curves in Figure 5 (Appendix). Our experiments reveal that has a minimal effect\non the convergence of training, as the loss curves for all \u03b6 values flatten out after approximately 200 steps.\nHowever, the choice of \u03b6 impacts the effectiveness of our method. As shown in Table 3, extreme values of\n\u03b6-either too large or too small-lead to suboptimal performance. Specifically, \u03b6 values of 2,4,8 yield the\nbest results, whereas \u03b6 values of 0 and 16 perform less effectively. When \u03b6 is too large, the comparative\nloss overweighs the discrepancy between the correct steps and wrong steps while neglecting the ascending\nrelationship among Q-values of correct steps. Conversely, when \u03b6 is too small, the loss function fails to\nadequately capture Q-value discrepancies, leading to suboptimal performance. These findings align with\nour theoretical expectations and underscore the importance of choosing an appropriate \u03b6 to balance the\ncomparative loss and capture meaningful Q-value distinctions."}, {"title": "5 Related Works", "content": "Process Reward Models. Process supervision [18, 32], represented by PRMs, can provide more precise\nfeedback, which is easier for humans to interpret, and more directly rewards models in step-by-step reasoning\ntasks. Most existing research [1, 6, 2, 7] formulates PRM as a classification problem, where the process\nreward is modeled as the probability of correctness of each step. We show that the prior approach can be cast\nas a special case under our theoretical framework. Due to the labor-intensive nature of dense annotations,\nseveral recent methods have introduced automatic annotation strategies [6, 7, 33]. In these approaches,\na step is deemed correct if a valid completion can be sampled from the LLM policy within k trials, see\ndetails in Appendix A. Generally, the subsequent steps after the first error are all treated as wrong steps in\nthis line of methods. Additionally, [28, 29] estimate the Q-value of intermediate steps by iterative Monte"}, {"title": "6 Conclusion", "content": "In this paper, we introduce the Process Q-value Model (PQM), a new approach to model process rewards\nvia optimization Q-value ranking. Unlike existing classification-based methods, which treat intermediate\nsteps independently, PQM captures the interdependencies among steps. To effectively optimize the Q-value\nrankings, we propose a margin-based comparative training objective and validate its effectiveness through\ncomprehensive experiments. Our results demonstrate that PQM significantly outperforms previous baselines,\nachieving an 11.6% accuracy improvement when verifying solutions generated by LLama-3-70B-Instruction\non the MATH500 dataset, and consistently delivering robust results across various backbone scales, policy\nmodels, and datasets. We hope our work inspires more future investigation on process reward modeling that\nbetter captures the complexities of multi-step reasoning processes."}, {"title": "F Limitations & Future Works", "content": "Notably, as shown in [7], the data quality of Math-shepherd is restricted due to the noise introduced by\nautomatic annotation. We believe that more advanced datasets could significantly improve the verification\ncapabilities of PRMs. As shown in Appendix C, there remains a considerable gap between the current\nperformance of PRMs and the ceiling performance, underscoring the need for further advancements in PRM\ntechniques. Additionally, while PRMs have garnered increasing research attention, there is a lack of systematic\nstudies on corresponding online RL algorithms that leverage PRMs. The potential for fully utilizing PRMs in\nthis context remains largely unexplored."}]}