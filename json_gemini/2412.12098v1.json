{"title": "MAXINFORL: BOOSTING EXPLORATION IN REINFORCEMENT LEARNING THROUGH INFORMATION GAIN MAXIMIZATION", "authors": ["Bhavya Sukhija", "Stelian Coros", "Andreas Krause", "Pieter Abbeel", "Carmelo Sferrazza"], "abstract": "Reinforcement learning (RL) algorithms aim to balance exploiting the current best strategy with exploring new options that could lead to higher rewards. Most common RL algorithms use undirected exploration, i.e., select random sequences of actions. Exploration can also be directed using intrinsic rewards, such as curiosity or model epistemic uncertainty. However, effectively balancing task and intrinsic rewards is challenging and often task-dependent. In this work, we introduce a framework, MAXINFORL, for balancing intrinsic and extrinsic exploration. MAXINFORL steers exploration towards informative transitions, by maximizing intrinsic rewards such as the information gain about the underlying task. When combined with Boltzmann exploration, this approach naturally trades off maximization of the value function with that of the entropy over states, rewards, and actions. We show that our approach achieves sublinear regret in the simplified setting of multi-armed bandits. We then apply this general formulation to a variety of off-policy model-free RL methods for continuous state-action spaces, yielding novel algorithms that achieve superior performance across hard exploration problems and complex scenarios such as visual control tasks.", "sections": [{"title": "1 INTRODUCTION", "content": "Reinforcement learning (RL) has found numerous applications in sequential decision-making problems, from games (Silver et al., 2017), robotics (Hwangbo et al., 2019; Brohan et al., 2023), to fine-tuning of large language models (Ouyang et al., 2022). However, most widely applied RL algorithms such as PPO (Schulman et al., 2017) are inherently sample-inefficient, requiring hundreds of hours of environment interactions for learning. Off-policy methods like SAC (Haarnoja et al., 2018), REDQ (Chen et al., 2021), and DroQ (Hiraoka et al., 2022) offer a more sample and compute efficient alternative and have demonstrated success in real-world learning (Smith et al.,"}, {"title": "2 BACKGROUND", "content": "A core challenge in RL is deciding whether the agent should leverage its current knowledge to maximize rewards or try new actions in pursuit of better solutions. Striking this balance between exploration-exploitation is critical. Here, we first introduce the problem setting, then we discuss two of the most commonly used exploration strategies in RL: e-greedy and Boltzmann exploration."}, {"title": "2.1 PROBLEM SETTING", "content": "We study an infinite-horizon Markov decision process (MDP, Puterman, 2014), defined by the tuple (S, A, p, \u03b3, r, \u03c1), where the state and action spaces are continuous, i.e., S C Rds, AC Rda, and the unknown transition kernel p : S\u00d7S\u00d7A \u2192 [0, \u221e) represents the probability density of the next state St+1 \u2208 S given the current state st \u2208 S and action at \u2208 A. At each step t in the environment, the agent observes the state st, samples an action at from the policy \u03c0 : A\u00d7S \u2192 [0, \u221e), at ~ \u03c0(\u03b1|st), and receives a reward r : S \u00d7 S \u00d7 A \u2192 [-max, max]. The agent's goal is to learn a policy \u03c0* that maximizes the y discounted reward w.r.t. the initial state distribution so ~ \u03c1.\n$$\u03c0^* = \\arg \\max_{\\pi \\in \\Pi} J(\\pi) = \\arg \\max_{\\pi \\in \\Pi} E_{s_0,a_0,...}  \\sum_{t=0}^{\\infty} \\gamma^t r_t$$"}, {"title": "2.2 E-GREEDY AND EXPLORATION", "content": "The e-greedy strategy (Kearns & Singh, 2002; Mnih, 2013; Van Hasselt et al., 2016) is widely applied in RL to balance exploration and exploitation, where the RL agent follows this simple decision rule below to select actions\n$$a_t = \\begin{cases} a \\sim \\text{Unif}(A) & \\text{with probability } \\epsilon_t \\\\ \\arg \\max_{a \\in A} Q^*(s_t, a) & \\text{else,} \\end{cases}$$\nHere Q* is the estimate of the optimal state-action value function. Therefore, at each step t, with probability et, a random action at ~ Unif(A) is sampled, else the greedy action at = maxa\u2208 A Q* (st, a) is picked. Lillicrap (2015); Fujimoto et al. (2018) extend this strategy to continuous state-action spaces, where a deterministic policy \u03c0\u03b8 is learned to maximize the value function and combined with random Gaussian noise for exploration."}, {"title": "2.3 BOLTZMANN EXPLORATION", "content": "Boltzmann exploration is the basis of many RL algorithms (Sutton, 2018; Szepesv\u00e1ri, 2022). The policy distribution \u03c0 for Boltzmann is represented through\n$$\\pi(a|s) \\propto \\exp{\\left(\\alpha^{-1}Q^{\\pi}(s,a)\\right)},$$\nwhere a is the temperature parameter that regulates exploration and Q\u03c0 is the soft-Q function. We neglect the normalization term Z-1(s) in the definition for simplicity. As a \u2192 0, the policy greedily maximizes Q\u03c0 (s, a), i.e. it exploits, and as a \u2192 \u221e the policy adds equal mass to all actions in A, effectively performing uniform exploration. Intuitively, Boltzmann exploration can be interpreted as a smoother alternative to e-greedy, with a serving a similar role to e in controlling the degree of exploration. Cesa-Bianchi et al. (2017) show that the standard Boltzmann exploration is suboptimal even in the simplest settings. They highlight that a key shortcoming of Boltzmann exploration is that it does not reason about the uncertainty of its estimates.\nOverall, both e-greedy and Boltzmann exploration strategies are undirected. They fail to account for the agent's \"lack of knowledge\" and do not encourage risk- or knowledge-seeking behavior. The agent explores by sampling random action sequences, which leads to suboptimal performance, particularly in challenging exploration tasks with continuous state-action spaces."}, {"title": "2.4 INTRINSIC EXPLORATION WITH INFORMATION GAIN", "content": "Intrinsic rewards or motivation are used to direct agents toward underexplored regions of the MDP. Hence they enable RL agents to acquire information in a more principled manner as opposed to the aforementioned naive exploration methods. Effectively, the agent explores by selecting policies that maximize the y-discounted intrinsic rewards. A common choice for the intrinsic reward is the information gain (Cover & Thomas, 2006; Sekar et al., 2020; Mendonca et al., 2021; Sukhija et al., 2024b). Accordingly, for the remainder of the paper, we center our derivations around using information gain as the intrinsic reward. However, our approach is flexible and can also be combined with other intrinsic exploration objectives, such as RND (Burda et al. (2018), see Appendix D).\nWe study a non-linear dynamical system of the form\n$$s_{t+1} = f^*(s_t, a_t) + w_t.$$\nHere St+1 = [st+1,rt] represents the next state and reward, f* represents the unknown dynamics and reward function of the MDP and wt is the process noise, which we assume to be zero-mean i.i.d., 02-Gaussian. Note this is a very common representation of nonlinear systems with continuous state-action spaces (Khalil, 2015) and the basis of many RL algorithms (Pathak et al., 2019; Kakade et al., 2020; Curi et al., 2020; Mania et al., 2020; Wagenmaker et al., 2023; Sukhija et al., 2024a). Furthermore, it models all essential and unknown components of the underlying MDP; the transition kernel and the reward function."}, {"title": "Approximating information gain", "content": "Given a dataset of transitions Dn = {(si, ai, \u0161i)}=0, e.g., a replay buffer, we learn a Bayesian model of the unknown function f*, to obtain a posterior distribution p(f* Dn) for f*. This distribution can be Gaussian, e.g., Gaussian process models (Rasmussen & Williams, 2005) or represented through Bayesian neural networks like probabilistic ensembles (Lakshminarayanan et al., 2017). As opposed to the typical model-based RL setting, similar to Burda et al. (2018); Pathak et al. (2017; 2019), our learned model is only used to determine the intrinsic reward. The information gain I(\u0161'; f*|s, a, Dn), reflects the uncertainty about the unknown dynamics f* from observing the transition (s, a, \u0161'). Moreover, let o(s, a|Dn) = [\u03c3;(s,a)]j\u2264ds+1 denote the model epistemic uncertainty or disagreement of f*. Sukhija et al. (2024b, Lemma 1.) show that\n$$I(\\mathbf{s}'; f^* | s, a, D_n) = H(\\mathbf{s}' | s, a, D_n) - H(\\mathbf{s}' | s, a, f^*, D_n) \\leq \\sum_{j=1}^{d_s+1} \\log \\left(1 + \\frac{\\sigma_{n-1,j}^2(s_t, a_t)}{\\sigma^2} \\right)$$\nData dependence of intrinsic rewards Information gain and other intrinsic rewards depend on the data Dn, making them inherently nonstationary and non-Markovian. Intuitively, underexplored areas of the MDP become less informative once visited (c.f., Prajapat et al. (2024) for more details). However, in RL, intrinsic rewards are often treated similarly to extrinsic rewards, a simplification that works very well in practice (Burda et al., 2018; Sekar et al., 2020). We take a similar approach in this paper and omit the dependence of I on Dn and use I(s'; f*|s, a) from hereon for simplicity."}, {"title": "3 MAXINFORL", "content": "In this section, we present our method for combining intrinsic exploration with classical exploration strategies. While MAXINFORL builds directly on Boltzmann exploration, we begin by illustrating its key ideas in the context of an e-greedy strategy, due to its mathematical simplicity and natural distinction between exploration and exploitation steps. The insights gained from this serve as motivation for developing our main method: MAXINFORL with Boltzmann exploration algorithms, which we evaluate in Section 4."}, {"title": "3.1 MODIFYING E-GREEDY FOR DIRECTED EXPLORATION", "content": "We modify the e-greedy strategy from Section 2.2 and learn two critics, Qextrinsic and Qintrinsic, where Qextrinsic is the state-action value function of the extrinsic reward r and Qintrinsic the critic of an intrinsic reward function 'intrinsic, for instance, the information gain (see Eq. (5)). Unlike traditional e-greedy exploration, we leverage intrinsic rewards to guide exploration more effectively by selecting actions that maximize Qintrinsic, leading to more informed exploration rather than random sampling. At each step t, we pick a greedy action that maximizes Qextrinsic with probability 1 et, while for exploration, the action that maximizes the intrinsic critic is selected, i.e., at = maxa\u2208A Qintrinsic (st, a).\n$$a_t = \\begin{cases} \\arg \\max_{a \\in A} Q_{\\text{intrinsic}}(s_t, a) & \\text{with probability } \\epsilon_t \\\\ \\arg \\max_{a \\in A} Q_{\\text{extrinsic}}(s_t, a) & \\text{else,} \\end{cases}$$\nWe call the resulting exploration strategy \u20ac-MAXINFORL. This approach is motivated by the insight that in continuous spaces, intrinsic rewards cover the state-action spaces much more efficiently than undirected random exploration, making them more effective for exploration in general (Aubret et al., 2023; Sekar et al., 2020; Sukhija et al., 2024b). In Appendix A, to give a theoretical intuition of our approach, we study e-MAXINFORL in the simplified setting of multi-armed bandit (MAB). We show that as more episodes are played, it gets closer to the optimal solution, i.e. has sublinear-regret."}, {"title": "3.2 MAXINFORL WITH BOLTZMANN EXPLORATION", "content": "In Section 3.1, we modify e-greedy to sample actions with high intrinsic rewards during exploration instead of randomly picking actions. Motivated from the same principle, we augment the distribution of Boltzmann exploration with the intrinsic reward I (s'; f*|s, a) to get the following\n$$\\pi(a|s) \\propto \\exp{\\left(\\alpha^{-1}Q^{\\pi}(s,a) + I(\\mathbf{s}'; f^*|s, a)\\right)} .$$\nThe resulting distribution encourages exploration w.r.t. information gain, with a playing a similar role to \u20ac in Eq. (6). Therefore, Eq. (7) can be viewed as a soft formulation of Eq. (6). Effectively, instead of randomly sampling actions, for large values of the temperature, we pick actions that yield high information while maintaining the exploitative behavior for smaller temperatures. This distribution is closely related to the epistemic risk-seeking exponential utility function from K-learning (O'Donoghue, 2021) and probabilistic inference in RL (Tarbouriech et al., 2024). As we show in the following, this choice of parameterization results in a very intuitive objective for the policy. Given the previous policy old and Qold, akin to Haarnoja et al. (2018), we select the next policy new through the following optimization\n$$\\pi^{\\text{new}} = \\arg \\min_{\\pi \\in \\Pi} D_{\\text{KL}}\\left(\\pi(\\cdot|s) \\propto \\frac{\\pi_{\\text{old}}(s, \\cdot) + \\exp{\\left(\\alpha^{-1}(Q^{\\text{old}}(s,a) + I(\\mathbf{s}'; f^*|s, a))\\right)}}{Z(s)}\\right)$$\n$$= \\arg \\max_{\\pi \\in \\Pi} E_{a \\sim \\pi(\\cdot|s)} [Q^{\\text{old}}(s, a) - \\alpha \\log(\\pi(a|s)) + \\alpha I(\\mathbf{s}'; f^*|s, a)]$$\n$$= \\arg \\max_{\\pi \\in \\Pi} E_{a \\sim \\pi(\\cdot|s)} [Q^{\\text{old}}(s, a)] - \\alpha H(\\mathbf{s}', a|s),$$\nhere in the last line we used that Ea\u223c\u03c0(:|s)[\u2212log(\u03c0(a|s)) + I(\u0161'; f*|s, a)] = H(a|s) + H(\u0161'|a, s) - H(\u0161'|s, a, f*) = H(\u0161', a|s) \u2013 H(w). Hence, the policy new trades off maximizing the value function with the entropy of the states, rewards, and actions. This trade-off is regulated through the temperature parameter a. We provide a different perspective to Eq. (8) from the lens of control as inference (Levine, 2018; Hafner et al., 2020) in Appendix C.\nSeparating exploration bonuses MAXINFORL has two exploration bonuses; (i) the policy en- tropy, and (ii) the information gain (Eq. (5)). The two terms are generally of different magnitude and tuning the temperature for the policy entropy is fairly well-studied in RL (Haarnoja et al., 2018). To this end, we modify Eq. (8) and introduce two individual temperature parameters 01 and 02 to separate the bonuses. Furthermore, since information gain does not have a closed-form solution in general, akin to prior work (Sekar et al., 2020; Sukhija et al., 2024b), we use its upper bound Iu(s, a) (Eq. (5)) instead.\n$$\\pi^{\\text{new}}(.|s) = \\arg \\max_{\\pi \\in \\Pi} J^{\\text{old}}(\\pi_s) = E_{a \\sim \\pi(\\cdot|s)} [Q^{\\text{old}}(s, a) - \\alpha_1 \\log(\\pi(a|s)) + \\alpha_2 I_u(s, a)]$$\nFor a1, we can either use a deterministic policy with 1 = 0 like Lillicrap (2015) or auto-tune 01 as suggested by Haarnoja et al. (2018). Notably, for a2 = 0 we get the standard max entropy RL methods (Haarnoja et al., 2018). Therefore, by introducing two separate temperatures, we can treat information gain as another exploration bonus in addition to the policy entropy and combine it with any RL algorithm.\nAuto-tuning the temperature for the information gain bonus Haarnoja et al. (2018) formulate the problem of soft-Q learning as a constrained optimization.\n$$\\pi^*(\\cdot|s) := \\arg \\max_{\\pi \\in \\Pi} E_{a \\sim \\pi} [Q^{\\pi}(s, a)] \\text{ s.t., } H(a|s) \\geq H$$\n$$:= \\arg \\max_{\\pi \\in \\Pi} \\min_{\\alpha_1 \\geq 0} E_{a \\sim \\pi} [Q^{\\pi}(s, a) - \\alpha_1 (\\log(\\pi(a|s)) + H)] .$$"}, {"title": "4 EXPERIMENTS", "content": "We evaluate MAXINFORL with Boltzmann exploration from Section 3.2 across several deep RL benchmarks (Brockman, 2016; Tassa et al., 2018; Sferrazza et al., 2024) on state-based and visual control tasks. In all our experiments, we report the mean performance with standard error evaluated over five seeds. For the state-based tasks we combine MAXINFORL with SAC (Haarnoja et al., 2018) and for the visual control tasks with DrQ (Yarats et al., 2021) and DrQv2 (Yarats et al., 2022). In the following, we refer to these algorithms as MAXINFOSAC, MAXINFODRQ, and MAXINFODRQV2, respectively. To further demonstrate the generality of MAXINFORL, in Appendix D, we provide additional experiments, where we combine MAXINFORL with REDQ (Chen et al., 2021), OAC (Ciosek et al., 2019), DrM (Xu et al., 2024), use RND (Burda et al., 2018) as"}, {"title": "5 RELATED WORKS", "content": "Naive Exploration Naive exploration approaches such as e-greedy or Boltzmann are widely applied in RL due to their simplicity (Mnih, 2013; Schulman et al., 2017; Lillicrap, 2015; Haarnoja et al., 2018; Hafner et al., 2023). In particular, the maximum entropy framework (Ziebart et al., 2008) is the basis of many sample-efficient model-free deep RL algorithms (Haarnoja et al., 2018; Chen et al., 2021; Hiraoka et al., 2022; Yarats et al., 2021). However, these methods often perform suboptimally, especially in challenging exploration problems such as those with sparse rewards or local optima (cf., Section 4). Effectively, the agent explores the underlying MDP by taking random sequences of actions. In continuous spaces, this makes sufficiently covering the state and action space exceptionally challenging. Moreover, even in the simplest setting of MAB in continuous"}, {"title": "6 CONCLUSION", "content": "In this work, we introduced MAXINFORL, a class of model-free off-policy RL algorithms that train an agent to trade-off reward maximization with an exploration objective that targets high entropy in the state, action, and reward spaces. The proposed approach is theoretically sound, simple to implement, and can be combined with most common RL algorithms. MAXINFORL consistently outperforms its baselines in a multitude of benchmarks and achieves state-of-the-art performance on challenging visual control tasks.\nA limitation of MAXINFORL is that it requires training an ensemble of forward dynamics models to compute the information gain, which increases computation overhead (c.f. Appendix E Table 1). In addition, while effective, the constraint in Equation (10) also requires maintaining a target policy.\nThe generality of the MAXINFORL exploration bonus and the trade-off we propose with reward maximization are not limited to the off-policy model-free setting, which we focus on here due to their sample and computational efficiency. Future work will investigate its applicability to other classes of RL algorithms, such as model-based RL, where the forward dynamics model is generally part of the training framework. In fact, in this work we only use the forward dynamics model for the intrinsic rewards. Another interesting direction is to include samples from the learned model in the policy training, which may yield additional gains in sample efficiency. Lastly, extending our theoretical guarantees from the bandit settings to the MDP case is also an interesting direction for future work."}, {"title": "A ANALYZING E-GREEDY FOR MULTI-ARMED BANDITS", "content": "In this section, we enunciate a theorem that shows that our modified e-greedy approach from Section 3.1 has sublinear regret in the simplified setting of multi-arm bandits."}, {"title": "A.1 SUBLINEAR REGRET FOR E-MAXINFORL", "content": "This exploration-exploitation trade-off is also fundamental in multi-armed bandits (MAB) (Lattimore & Szepesv\u00e1ri, 2020), where the task is to optimize an unknown objective function J : \u04e8 \u2192 [-Jmax, Jmax], with \u04e8\u2282 Rd being a compact set over which we seek the optimal arm 0* = arg maxoee J(0). In each round t, we choose a point t \u2208 and obtain a noisy measurement yt of the function value, that is, yt = J(0t) + wt. Our goal is to maximize the sum of rewards t=1 J(0t) over T learning iterations/episodes, thus to perform essentially as well as 0* (as quickly as possible). For example, O can be parameters of the policy distribution \u03c0e and our objective maximizing the discounted rewards as defined in Eq. (1), i.e., max\u04e9\u2208\u04e9 J(\u03c0\u04e9). This formulation is the basis of several sample-efficient RL algorithms (Calandra et al., 2016; Marco et al., 2016; Antonova et al., 2017; Berkenkamp et al., 2021; Sukhija et al., 2023).\nThe natural performance metric in this context is the cumulative regret defined as RT = t=1J(0*) \u2013 J(0t). A desirable asymptotic property of any learning algorithm is that it is no-regret: limy\u2192\u221e RT/T = 0. In the following theorem, we show that under standard continuity assumptions on J, i.e., we can model it through Gaussian process regression (Rasmussen & Williams, 2005) with a given kernel k(\u00b7, \u00b7) : \u04e8\u04e8 \u00d7 \u04e8 \u2192 R+, our exploration strategy from Eq. (7) with the model epistemic uncertainty as the intrinsic objective has sublinear regret."}, {"title": "Theorem A.1.", "content": "Assume that the objective function J lies in the RKHS Hk corresponding to the kernel k(0, 0), ||J||k \u2264 B, and that the noise wt is zero-mean o-sub Gaussian. Let et = t2a-1 for \u03b1 \u2208 (1/4, 1/2), \u03b4 \u2208 (0, 1] and consider the following e-greedy exploration strategy\n$$\\theta_t = \\begin{cases} \\arg \\max_{\\Theta \\in \\Theta} \\sigma_t(\\theta) & \\text{with probability}\\epsilon_t \\\\ \\arg \\max_{\\Theta \\in \\Theta} \\mu_t(\\theta) & \\text{else,} \\end{cases}$$\nwhere ut is our mean estimate of J and ot the epistemic uncertainty (c.f., (Rasmussen & Williams, 2005) for the exact formula). Then we have with probability at least 1 \u2013 8\n$$R_T < 0 \\left(J_{\\max}T^{\\frac{2\\alpha-1}{2\\alpha}} + 2\\Gamma_T(k)T^{1-\\alpha}\\right),$$\nwhere \u0413\u0442(k) is the maximum information gain (Srinivas et al., 2012).\nThe maximum information gain \u0413\u0442(k) measures the complexity of learning the function J w.r.t. the number of data points T and depends on the choice of the kernel k, for common kernels such as the RBF and linear kernel, it grows polylogarithmically with T (Srinivas et al., 2012; Vakili et al., 2021), resulting in a sublinear regret bound. Intuitively, by sampling informative actions sufficiently often, we will improve our estimate of J and thus gradually suffer less regret."}, {"title": "A.2 PROOF OF THEOREM A.1", "content": "From hereon, let Zt ~ Bernoulli(et) be a Bernoulli random variable. The acquisition function for e-greedy GP bandit from Theorem A.1 is defined as:\n$$\\Theta_t = \\begin{cases} \\arg \\max_{\\Theta \\in \\Theta} \\sigma_t(\\theta) & \\text{if } Z_t = 1 \\\\ \\arg \\max_{\\Theta \\in \\Theta} \\mu_t(\\theta) & \\text{else} \\end{cases}$$\nLet Qt = t1 Zs Es. In the following, we analyze the sequence Q1:t. Note that Qt \u2208 [0, t] for all t and\n$$E[Q_t | Q_{1:t-1}] = E[Z_t - \\epsilon_t] + E[Q_{t-1} | Q_{1:t-1}]$$\n$$= Q_{t-1}.$$\nTherefore, Q1:t is a martingale sequence, or equivalently, {Zs - \u20acs} s\u22081:t is a martingale difference sequence.\nIn the following, we use the time uniform Azuma Hoeffding inequality from Kassraie et al. (2024) to give a bound on t=1 Zs."}, {"title": "B PROOF OF THEOREM 3.1", "content": "Lemma B.1. Consider the Bellman operator T\u2122 from Eq. (12), and a mapping Q\u00ba : S \u00d7 A \u2192 R and define Qk+1 = T\u03c0Qk. Furthermore, let the assumptions from Theorem 3.1 hold. Then, the sequence Qk will converge to the soft-Q function of \u3160 as k \u2192 \u221e.\nProof. Consider the following augmented reward\nr\u03c0(s,a) = r(s, a) + Es'ls,a [Ea'~\u03c0(:\\s') [-a\u2081 log(\u03c0(a's')) + a2I(\u0161\"; f*|s', a')]],\nwhere \u0161\" is the next state and reward given (s', a'). Then from Eq. (12), we have\n$$T^{\\pi}Q(s,a) = r_{\\pi}(s, a) + \\gamma E_{s',a'|s,a}[Q(s', a')].$$\nFurthermore, r is bounded since r is bounded and the policy entropy and the information gain are bounded for all \u03c0\u2208 I by assumption. Therefore, we can apply standard convergence results on policy evaluation to show convergence (Sutton, 2018).\nLemma B.1 shows that the Q updates satisfy the contraction property and thus converge to the soft-Q function of \u03c0. Next, we show that the policy update from Eq. (8) leads to monotonous improvement of the policy.\nLemma B.2. Let the assumptions from Theorem 3.1 hold. Consider any \u03c0old \u2208 II, and let new denote the solution to Eq. (8). Then we have for all (s, a) \u2208 S \u00d7 A that Q\u00bald (s, a) < Qnew (s, a).\nProof. Consider any s \u2208 S\n$$V_{\\pi^{\\text{old}}}(s) = E_{a \\sim \\pi^{\\text{new}}(\\cdot|s)} [Q_{\\pi^{\\text{old}}}(s, a) - a_1 \\log \\pi^{\\text{new}}(a|s) + a_2 I(s'; f^*|s, a)]$$\n$$> E_{a \\sim \\pi^{\\text{old}}(\\cdot|s)} Q_{\\pi^{\\text{old}}}(s, a)$$\n$$= V_{\\pi^{\\text{old}}}(s)$$\nNext consider any pair (s, a) \u2208 S \u00d7 A\n$$Q_{\\pi^{\\text{old}}}(s, a) = r(s, a) + \\gamma E_{s'|s,a}[V_{\\pi^{\\text{old}}}(s')]$$\n$$\\leq r(s, a) + E_{s'|s,a}[E_{a' \\sim \\pi^{\\text{new}}(\\cdot|s')} [Q_{\\pi^{\\text{old}}}(s', a') - a_1 \\log \\pi^{\\text{new}}(a'|s') + a_2 I(s''; f^*|s', a')]]$$\n$$< Q_{\\pi^{\\text{new}}}(s, a)$$\nwhere we have repeatedly expanded Q\u2122 on the RHS by applying the soft Bellman equation and the bound in Eq. (15).\""}, {"title": "C MAXINFORL FROM THE PERSPECTIVE OF KL-MINIMIZATION", "content": "We take inspiration from Hafner et al. (2020) and study the Boltzmann exploration formulation from Section 3.2 from the action perception and divergence minimization (APD) perspective. We denote with T = {st,at,rt}t>0 the trajectory in state, reward, and action space. The goal of the RL agent is to maximize the cumulative reward w.r.t. the true system f*. Therefore, a common and natural choice for a target/desired distribution for the trajectory \u315c is (Levine, 2018; Hafner et al., 2020)\n$$p^*(\\tau) = \\prod_{t \\geq 0} p(s_{t+1}, r_t | s_t, a_t, f^*) \\prod_{t \\geq 0} \\pi(a_t | s_t) .$$\nHowever, f* is unknown in our case and we only have an estimate of the underlying distribution p(f) from which it is sampled. Given a state-pair (st, at), we get the following distribution for (St+1, rt).\n$$p(s_{t+1}, r_t | s_t, a_t) = \\int p(s_{t+1}, r_t | s_t, a_t, f) dp(f) = E_f [p(s_{t+1}, r_t | s_t, a_t, f)].$$\nFor a given policy \u03c0, we can write the distribution of its trajectory via T\u2122 as\n$$p^{\\pi}(\\tau) = \\prod_{t \\geq 0} p(s_{t+1}, r_t | s_t, a_t) \\pi(a_t | s_t)$$\n$$= p(s_1) \\prod_{t \\geq 0} E_f [p(s_{t+1}, r_t | s_t, a_t, f)] \\pi(a_t | s_t) .$$\nA natural objective for deciding which policy to pick is to select such that its resulting trajectory T\u2122 is close (in-distribution) to T*. That is,\n$$\\pi^* = \\arg \\min_{\\pi} D_{KL} (p^{\\pi}(\\tau) | p^*(\\tau)).$$\nIn the following, we break down the term from Eq. (18) and show that it results in the MAXINFORL objective."}, {"title": "D ADDITIONAL EXPERIMENTS", "content": "We present additional experiments and ablations in this section.\nCovergence of intrinsic reward coefficient \u5165 for MAXINFOSAC: In Fig. 8 we report the tuned intrinsic reward coefficient A for SACEIPO and MAXINFOSAC (a2 for MAXINFOSAC). As shown in Fig. 5 in Section 4", "REDQ": "In Fig. 9 we combine MAXINFORL with REDQ. In all our tasks", "tasks": "In Fig. 10 we compare MAXINFODRQV2 with DrQv2 and MAXINFODRQ. For MAXINFODRQV2", "reward": "We evaluate MAXINFOSAC with RND as in- trinsic reward. Moreover, we initialize a random NN model and train another model to predict the output of the random model as proposed in Burda et al. (2018). We train an ensemble of NNs to learn the random model and use their disagreement as the intrinsic reward"}]}