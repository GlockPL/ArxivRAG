{"title": "Position: LLMs Can be Good Tutors in Foreign Language Education", "authors": ["Jingheng Ye", "Shen Wang", "Deqing Zhou", "Yibo Yan", "Kun Wang", "Hai-Tao Zheng", "Zenglin Xu", "Irwin King", "Philip S. Yu", "Qingsong Wen"], "abstract": "While recent efforts have begun integrating large language models (LLMs) into foreign language education (FLE), they often rely on traditional approaches to learning tasks without fully embracing educational methodologies, thus lacking adaptability to language learning. To address this gap, we argue that LLMs have the potential to serve as effective tutors in FLE. Specifically, LLMs can play three critical roles: (1) as data enhancers, improving the creation of learning materials or serving as student simulations; (2) as task predictors, serving as learner assessment or optimizing learning pathway; and (3) as agents, enabling personalized and inclusive education. We encourage interdisciplinary research to explore these roles, fostering innovation while addressing challenges and risks, ultimately advancing FLE through the thoughtful integration of LLMs.", "sections": [{"title": "1. Introduction", "content": "Foreign Language Education (FLE) has long been a cornerstone of global education and a critical component of K-12 curricula, equipping students with the linguistic and cultural competencies necessary for an interconnected world. However, traditional FLE methods often fall short in addressing the diverse needs of learners (Hou, 2020). Challenges such as limited personalization, scalability constraints, and the lack of real-time feedback are particularly pronounced in large classroom settings (Ehrenberg et al., 2001), where individual attention is scarce, leaving many students disengaged or struggling to keep pace. Addressing these shortcomings requires innovative approaches that not only enhance the quality of instruction but also adapt to the unique learning trajectories of students (Eaton, 2010)."}, {"title": "2. Background", "content": "language instruction. By examining these roles, we aim to demonstrate how LLMs can address the limitations of traditional FLE methods while advancing our understanding of intelligent tutoring systems. Additionally, we discuss potential challenges, ethical considerations, and future directions (Section 7) for integrating LLMs into FLE, offering a technical guideline for researchers and educators to harness their transformative potential. We also describe the paradigm shift of leveraging AI for FLE, starting from the last century, as one of our contributions in Section 3."}, {"title": "2.1. Foreign Language Education", "content": "FLE (Watzke, 2003; Rashov, 2024) has long been a cornerstone of global communication and cultural exchange. Traditional methods often emphasize grammar rules, vocabulary memorization, and repetitive practice, supplemented by limited opportunities for real-world application. Such approaches are often constrained by the availability of skilled teachers, the diversity of learners' needs, and the lack of personalized feedback (Williams et al., 2004). To bridge the gaps, many technologies for FLE have been proposed (Al-husaiyan, 2024), focusing on solving specific tasks instead of describing the whole picture of foreign language tutoring. While intelligent language tutoring systems (Schwind, 1990; C Angelides & Garcia, 1993) have the potential to create adaptive environments, attention to this field is relatively less compared to other subjects like science and mathematics. One key reason lies in the inherent complexity of lan-"}, {"title": "2.2. Large Language Models", "content": "The potential of LLMs in education (Alhafni et al., 2024), particularly in FLE (Gao et al., 2024; Karata\u015f et al., 2024), is immense. Benefiting from large-scale pre-training on extensive corpora, LLMs have demonstrated emergent abilities including (1) in-context learning (Dong et al., 2022), which allows the model to adapt to new tasks and provide contextually relevant responses based on a few examples provided during the interaction; (2) instruction following (Zeng et al., 2024), which enables the model to process and execute complex user instructions with high accuracy; and (3) reasoning and planning (Huang et al., 2024b), which allows the model to generate coherent, structured, and context-aware outputs, even for tasks that require multi-step thinking.\nHowever, these fundamental capabilities, while impressive, are not sufficient to fully meet the unique demands of FLE. Teaching a foreign language requires more than generating grammatically correct sentences or providing accurate translations; it demands a nuanced understanding of pedagogy, learner psychology, and cultural context. Maurya et al. (2024) propose an evaluation taxonomy that identifies eight critical dimensions for assessing AI tutors. These dimensions can be broadly categorized into two groups. (1) Problem-solving abilities assess the technical capabilities of LLMs to perform tasks relevant to FLE. (2) Pedagogical alignment abilities evaluate how well the LLM aligns with effective teaching and learning principles. Pedagogical alignment includes the model's ability to adapt to the learner's proficiency level, provide scaffolded feedback, foster engagement, and maintain motivation. While LLMs can"}, {"title": "3. Paradigm Shift", "content": "The development of AI models for FLE can be broadly categorized into four successive generations as shown in Figure 3: (1) rule-based models, (2) statistical models, (3) neural models, and (4) large language models. We leave the detailed description in Appendix B.\nOur position. We foresee next-generation LLMs with deeper alignment to pedagogical principles and stronger guardrails to mitigate misinformation and bias. Future models may integrate multimodal data (e.g., text, image, video, speech) and employ meta-learning to adapt in real time to diverse learner profiles. These improvements will reinforce the position that LLMs can evolve into even more effective tutors for FLE."}, {"title": "4. LLMs as Data Enhancers", "content": "Education is a high-stake area where any form of LLMs' hallucination could cause devastating harm to humans' cognition activities (Ho et al., 2024). One of the hallucination causes is from data (Huang et al., 2023). Therefore, high-quality and diverse data resources (Long et al., 2024) are critical to ensuring the reliability of incorporating LLMs into FLE. The 1) creation, 2) reformation, and 3) annotation of educational materials are crucial to delivering effective and engaging teaching. Traditional resource development methods often lack the scalability, adaptability, and personalization necessary to meet the diverse needs of learn-"}, {"title": "4.1. Data Creation", "content": "Creating pedagogically sound and learner-specific data is a cornerstone of personalized learning. However, manually creating such resources is time-consuming and often fails to address the wide range of learner needs (Cochran et al., 2022). LLMs can revolutionize this process by generating tailored and diverse educational content or responses on demand (Zha et al., 2023; Cochran et al., 2023).\nEducational Materials Generation. A primary use of LLMs in data creation is the generation of educational questions aligned with specific learning objectives. Due to their superior contextual understanding, classic rule-based approaches have largely been eclipsed by neural network-based techniques (Kurdi et al., 2020; Rathod et al., 2022; Mulla & Gharpure, 2023). LLMs can produce answer-aware (whose target answer is known) or answer-agnostic (whose answer is open) (Zhang et al., 2021), resulting in more nuanced exercises and assessments (Xiao et al., 2023).\nStudent Simulation. Simulating the learner's perspective is crucial for designing adaptive instructional materials. Traditional surveys and standardized tests often fail to capture the complexity of dynamic learner behaviors (K\u00e4ser & Alexandron, 2024). In contrast, LLM-based approaches enable high-fidelity, context-aware student simulations (Liu et al., 2024d; Yue et al., 2024), generating synthetic learners who exhibit realistic mastery levels and evolving behaviors. For instance, Generative Students (Lu & Wang, 2024) create simulated learners with various competency levels, while EduAgent (Xu et al., 2024) integrates cognitive priors to model complex learning trajectories and behaviors better.\nDiscussion. While LLMs excel at generating educational content, current approaches mainly focus on question creation, leaving many areas of FLE underexplored. Essential tasks like generating culturally rich reading materials, context-dependent writing prompts, or dynamic comprehension exercises are still lacking in diversity and depth. Additionally, the student simulations created by LLMs often fail to reflect long-term learning trajectories or the intricacies of individual learning progress, leading to challenges in creating truly adaptive systems."}, {"title": "4.2. Data Reformation", "content": "In addition to creating new content, LLMs can adapt existing materials to better align with current needs. This process, commonly referred to as data reformation, involves"}, {"title": "4.3. Data Annotation", "content": "While Data Creation focuses on generating learner-specific data, it often prioritizes diversity and adaptability over precision. The approach is particularly useful for tasks with large label spaces (Ding et al., 2024). In contrast, Data Annotation emphasizes producing high-quality, meticulously labeled data that is essential for tasks requiring accuracy and consistency. Unlike data creation, annotated data often undergoes rigorous validation to ensure its accuracy and relevancy (Artemova et al., 2024).\nAnnotation Generation. LLMs can be central to generating a variety of annotations, including categorical labels, rationales, pedagogical feedback, and linguistic features such as discourse relations. Recent prompt engineering and fine-tuning techniques have further expanded LLMs' annotation capabilities. For instance, Ye et al. (2024) leverage GPT-4 to annotate structured explanations for Chinese grammatical error correction, while Samuel et al. (2024) examine GPT-4 as a surrogate for human annotators in low-resource reading comprehension tasks. Likewise, Siyan et al. (2024) deploy GPT-4-Turbo for audio transcript annotations. However, inconsistencies across LLMs (T\u00f6rnberg, 2024) remain a serious challenge, posing risks to educational reliability.\nAnnotation Assessment. Although LLM-based annotation generation is efficient, it also raises critical issues of bias, calibration, and validity-particularly in low-resource language contexts (Bhat & Varma, 2023; Jadhav et al., 2024). Automated or semi-automated evaluation strategies have emerged to address these quality concerns. For example, LLMs-as-Judges (Li et al., 2024a;b; Gu et al., 2024) reduce human overhead by automating evaluation, an approach increasingly explored in education-focused applications (Chiang et al., 2024; Zhou et al., 2024). However, purely automated frameworks can still propagate errors or bias."}, {"title": "5. LLMs as Task Predictors", "content": "Task-Based Language Learning (TBLL) (Nunan, 1989; Willis, 2021) as a methodological approach is one of the critical factors of modern foreign language teaching. LLMs have demonstrated remarkable capabilities in understanding and generating human language, making them well-suited for addressing numerous tasks in FLE. These tasks can be broadly categorized into three types based on their nature and the role of LLMs: 1) Discriminative, 2) Generative, and 3) Mixed of the above two roles."}, {"title": "5.1. Discriminative Task Predictors", "content": "Discriminative tasks in FLE primarily involve classifying learner inputs or grading their future performance. Below are some applications that are still calling for improvements:\nAutomated Assessment. The task aims to automatically grade students' assignments, including essay scoring (Se\u00dfler et al., 2024; Li & Liu, 2024; Syamkumar et al., 2024), short answer grading (Schneider et al., 2023; Henkel et al., 2024), and spoken language evaluation (Gao et al., 2023; Fu et al., 2024). LLMs can process learners' submissions to judge grammar, lexical diversity, coherence, and even spoken fluency, providing instant feedback. This scalability is particularly appealing for large classes, where human evaluators are often overwhelmed and unable to provide timely, personalized critique (Mizumoto & Eguchi, 2023).\nKnowledge Tracing. Given sequences of learning interactions in online learning systems, Knowledge Tracing identifies and tracks students' evolving mastery of target skills (Shen et al., 2024b; Xu et al., 2023). LLM-based methods have been explored in cold-start scenarios (Zhan et al., 2024; Jung et al., 2024), offering strong generalization by inferring latent learner states from limited data. These approaches can support adaptive learning pathways, giving personalized recommendations based on predicted performance and knowledge gaps."}, {"title": "5.2. Generative Task Predictors", "content": "Generative tasks involve producing new content or responses. LLMs are known to be adept in these tasks due to their natural language generation capabilities.\nGrammatical Error Correction and Explanation. In foreign language writing, errors often reveal learners' gaps in grammar and vocabulary (Hyland & Hyland, 2006). LLMs can detect and correct these errors (Bryant et al., 2023; Ye et al., 2023), offering concise explanations (Ye et al., 2024) that reinforce language rules. By streamlining error detection and pedagogically framing corrections, learners deepen their linguistic understanding.\nFeedback Generation. Quizzes and exercises remain vital in FLE for practice and targeted remediation (Rashov, 2024). LLMs enhance this process by delivering prompt, personalized feedback that pinpoints strengths and addresses weaknesses (Borges et al., 2024). This scalability enables learners to self-regulate and refine their skills without relying solely on human graders (Stamper et al., 2024).\nSocratic Dialogue. Moving beyond straightforward Q&A, Socratic questioning promotes critical thinking and self-reflection (Paul & Elder, 2007). SocraticLM (Liu et al., 2024b), for example, aligns an LLM with open-ended, inquiry-based teaching principles, guiding learners through iterative exploration rather than prescriptive correction. In theory, this fosters deeper conceptual understanding and active learner engagement."}, {"title": "5.3. Mixed Task Predictors", "content": "Mixed tasks integrate discriminative and generative elements, requiring LLMs to evaluate learner inputs and generate meaningful feedback or suggestions. These tasks are particularly valuable in fostering an interactive and adaptive learning experience, as they bridge the gap between evaluation and instruction.\nAutomated Assessment with Feedback. While discriminative systems for essay scoring and speech evaluation primarily focus on assigning grades, LLMs extend these capabilities by simultaneously generating formative feedback (Katuka et al., 2024; Stahl et al., 2024b). For example, an LLM can evaluate the coherence and lexical diversity of a written assignment, then offer specific revision strategies. In speaking practice, it can measure fluency and pronunciation accuracy while suggesting drills to refine intonation or stress patterns. Through this combination of scoring and tailored advice, learners gain a deeper understanding of their strengths and areas for improvement.\nError Analysis. Error Analysis systematically uncovers and categorizes learners' missteps, from syntactic lapses in writing to flawed pronunciations in speaking (James, 2013; Erdo\u011fan, 2005). LLMs functioning in a mixed capacity can classify these errors and generate corrective guidance, providing revised sentences, clarifications of grammatical rules, or remediation exercises for identified weaknesses (Myles, 2002; Mashoor & Abdullah, 2020). Such insight facilitates targeted interventions that enhance language proficiency across modalities, including reading and listening."}, {"title": "6. LLM-empowered Agent", "content": "In this section, we delve into the potential of LLMs as intelligent agents in FLE. LLMs can act as catalysts for personalized learning, addressing the long-standing scalability, adaptability, and inclusivity challenges in traditional teaching paradigms."}, {"title": "6.1. Fundamental Abilities", "content": "This section highlights five key abilities of LLM-empowered agents that enable them to function as adaptive tutors.\nKnowledge Integration. LLMs excel at merging structured educational knowledge graphs (Abu-Rasheed et al., 2024; Hu & Wang, 2024) with unstructured textual data (Li et al., 2024c; Modran et al., 2024), providing rich, contextualized information on linguistic constructs and cultural nuances. Their ability to perform real-time knowledge editing (Wang et al., 2024d; Zhang et al., 2024a) ensures learners receive content aligned with evolving language usage, addressing the inherent limitations of static materials.\nPedagogical Alignment. LLMs require embedding with pedagogical principles to facilitate genuine learning experiences (Carroll, 1965; Taneja, 1995). Recent work incorporates theoretical frameworks, such as Bloom's taxonomy (Bloom et al., 1956), to guide LLMs in systematically addressing different cognitive levels (Jiang et al., 2024b). Approaches like Pedagogical Chain of Thought (Jiang et al., 2024b) and preference learning (Sonkar et al., 2024; Rafailov et al., 2024) focus on aligning model responses with educational objectives.\nPlanning. By assisting in crafting teaching objectives and lesson designs, LLMs can handle complex tasks such as differentiated instruction (Hu et al., 2024). LessonPlanner (Fan et al., 2024) has been proposed to assist novice teachers in preparing lesson plans, with expert interviews"}, {"title": "6.2. Applications", "content": "Although still in its early stages, LLM-empowered agents have already started to show promising applications in FLE.\nClassroom Simulation. Classroom simulation leverages LLM-empowered agents to recreate complex, interactive learning settings without the logistical hurdles of organizing physical classrooms (Zhang et al., 2024b). By simulating virtual students and tutors, researchers can study pedagogical strategies at scale, generate diverse learner interactions, and refine teaching techniques. Moreover, this virtual data can be used to fine-tune LLMs for specific educational contexts and learner profiles (Liu et al., 2024b), offering a cost-effective and adaptable approach to language instruction.\nIntelligent Tutoring System (ITS). LLM-based agents have demonstrated the capacity to provide dynamic, personalized tutoring experiences (Kwon et al., 2024), effectively identifying learner weaknesses through large-scale linguistic"}, {"title": "7. Challenges and Future Directions", "content": "While we posit that LLMs have the potential to revolutionize FLE, realizing their full promise requires addressing key challenges. This section offers a concise overview of these challenges, followed by directions that could guide future research and deployment.\nEnsuring Reliability and Mitigating Hallucinations. LLMs may produce hallucinations (Huang et al., 2023) that can mislead learners and undermine pedagogical goals. This risk intensifies in high-stakes educational environments, where trust and correctness are paramount. Efforts to increase data quality (Long et al., 2024), combine model outputs with structured domain knowledge, and employ rigorous validation mechanisms (including human oversight) are crucial for minimizing such detrimental outcomes.\nAddressing Bias and Ethical Considerations. As LLMs inherit biases from their training data, these systems may produce culturally insensitive or unfair responses, poten-"}, {"title": "8. Alternative Views", "content": "8.1. Task-Specific or Language-Specific Models as Better Alternatives\nSome argue that specialized or language-specific models, including classical ML systems with carefully engineered features, can outperform general-purpose LLMs in narrowly defined tasks (e.g., phonetics or grammar drills (Fang et al., 2023)). By focusing on limited objectives, such models avoid the computational overhead and potential inaccuracies of LLMs, which aim to handle a broader range of inputs and contexts (Shen et al., 2024a).\nCounterargument. While specialized models may excel in isolated tasks, they lack the flexibility required for comprehensive FLE, which involves cultural nuances, conversations, and evolving learner needs. In contrast, LLMs can be fine-tuned for specific goals while still offering broader linguistic competence (Song et al., 2024a). Additionally, relying on multiple specialized models can be resource-intensive, whereas a well-configured LLM provides a unified framework that balances specialization and scalability.\n8.2. Concerns About Over-Reliance on LLMs\nCritics warn that over-reliance on LLMs may lead to problems such as generating misleading outputs (Nahar et al., 2024), reducing human interaction, and over-standardizing teaching methods. These issues could undermine the interpersonal and motivational aspects of language learning.\nCounterargument. These risks highlight the need for balanced integration rather than the replacement of human tutors. LLMs can complement educators by automating repetitive tasks, allowing teachers to focus on individualized"}, {"title": "9. Conclusion", "content": "This position paper emphasizes the transformative potential of LLMs in FLE, positioning them as valuable tutors to complement traditional teaching methods. Through their roles as data enhancers, task predictors, and agents, LLMs can provide adaptive learning experiences across the core skills of listening, speaking, reading, and writing. This paper encourages interdisciplinary exploration to FLE."}, {"title": "Impact Statement", "content": "This paper positions LLMs as promising tutors in FLE. By leveraging LLMs, we aim to enhance inclusivity, personalization, and overall accessibility. However, potential risks\u2014such as bias, privacy issues, and over-reliance on AI tutoring\u2014necessitate careful oversight, which have been discussed in the paper. In particular, ensuring equitable data representation, model usage transparency, and human agency preservation remains critical. While careful integration with human expertise is essential, the potential for LLMs to revolutionize FLE is significant, paving the way for more inclusive, efficient, and engaging educational practices. We encourage continuing dialogue and interdisciplinary collaboration to responsibly integrate LLMs into educational ecosystems and mitigate potential harms."}, {"title": "A. Literature Review", "content": "We provide an overview of LLM-centric research of FLE presented in Figure 4."}, {"title": "B. Four Phases of Research Roadmap", "content": "Stage 1: Rule-based Models (1960s\u20131990s). Early solutions relied on handcrafted linguistic rules to process language in tightly constrained scenarios (Grosan et al., 2011; C Angelides & Garcia, 1993). Classical platforms like PLATO (Hart, 1981) and Systran (Toma, 1977) operated effectively for highly structured tasks (e.g., grammar drills) but struggled with complex, context-dependent interactions.\nStage 2: Statistical Models (1990\u20132010s). With the increased availability of digitized corpora, methods such as the early version of Google Translate (Och, 2006) and Dragon NaturallySpeaking (Blair, 1997) pioneered statistical pattern mining. These approaches leveraged large datasets to infer linguistic rules probabilistically, improving scalability yet still lacking deeper semantic understanding.\nStage 3: Neural Models (2010s\u20132020s). The advent of deep learning architectures (e.g., RNNs (Yu et al., 2019) and Transformers (Vaswani, 2017)) enabled more robust context modeling, sparking transformative applications like Grammarly (Fitria, 2021) and Duolingo (Vesselinov & Grego, 2012). These systems offered enhanced personalization and feedback, significantly augmenting learners' writing and reading comprehension.\nStage 4: Large Language Models (2020s\u2013Present). Today's LLMs (e.g., ChatGPT (Achiam et al., 2023)) combine massive pre-training corpora with generative capabilities, achieving impressive results in multi-turn dialogue, individualized scaffolding, and multimodal integration. Tools such as Khanmigo (Anand, 2023) demonstrate LLMs' potential for real-time conversational practice, dynamic content creation, and inclusive educational support at scale."}]}