{"title": "S4DL: Shift-sensitive Spatial-Spectral Disentangling Learning for Hyperspectral Image Unsupervised Domain Adaptation", "authors": ["Jie Feng", "Tianshu Zhang", "Junpeng Zhang", "Ronghua Shang", "Weisheng Dong", "Guangming Shi", "Licheng Jiao"], "abstract": "Unsupervised domain adaptation techniques, extensively studied in hyperspectral image (HSI) classification, aim to use labeled source domain data and unlabeled target domain data to learn domain invariant features for cross-scene classification. Compared to natural images, numerous spectral bands of HSIs provide abundant semantic information, but they also increase the domain shift significantly. In most existing methods, both explicit alignment and implicit alignment simply align feature distribution, ignoring domain information in the spectrum. We noted that when the spectral channel between source and target domains is distinguished obviously, the transfer performance of these methods tends to deteriorate. Additionally, their performance fluctuates greatly owing to the varying domain shifts across various datasets. To address these problems, a novel shift-sensitive spatial-spectral disentangling learning (SDL) approach is proposed. In S\u2074DL, gradient-guided spatial-spectral decomposition is designed to separate domain-specific and domain-invariant representations by generating tailored masks under the guidance of the gradient from domain classification. A shift-sensitive adaptive monitor is defined to adjust the intensity of disentangling according to the magnitude of domain shift. Furthermore, a reversible neural network is constructed to retain domain information that lies in not only in semantic but also the shallow-level detailed information. Extensive experimental results on several cross-scene HSI datasets consistently verified that SDL is better than the state-of-the-art UDA methods.", "sections": [{"title": "I. INTRODUCTION", "content": "Hyperspectral image (HSI) is obtained by capturing information from the reflection of light from an object or scene at hundreds of different wavelengths. Unlike widely-adopted RGB images, each pixel in HSI not only contains visible light information, but also covers near-infrared, short-wave infrared, mid-infrared and long-wave infrared information, which enables HSIs to capture richer spectral information and detect more accurately [1].\nHowever, factors like lighting, seasonal variations, atmospheric conditions, and differences in sensors lead to an inevitable domain shift between HSI of different scenes, undermining the assumption of independent and identically distributed data. This limitation hinders the transferability and generalization of traditional classification models to other scenes. In response to these challenges, unsupervised domain adaptation (UDA) for HSI has been introduced [2]\u2013[4], which seeks to apply knowledge from a labeled source domain to an unlabeled target domain. While the target task and label spaces of the training and test sets remain identical, their feature distributions differ yet are related. Therefore, the model needs to learn domain-invariant features while dealing with the target task to achieve cross-scene HSI classification.\nUDA is one of the most effective solutions for cross-scene HSI classification by extracting domain-invariant features. Inspired by disentangling learning, domain disentangling methods explicitly separate domain-invariant feature and domain-specific feature while maintaining the model transferability by seeking for the alignment on the domain-invariant features between the source and target domains [5]\u2013[7]. Based on this framework, existing methods introduce style information [8] and instance information [9] in disentangling stage to facilitate the feature disentanglement. Nevertheless, for handling cross-scene HSI classification, it is necessary to capitalize on the domain information in a large number of spectral bands embedded in HSIs.\nIn HSI, there may be a significant domain gap between domains. This is due to the spectral uncertainties of targets, which can be remarkably significant due to the complexity of spectral information and variations in the imaging environment. As a result, the extracted features can become confused across different spectral channels. This channel confusion makes it difficult to consistently extract invariant features, ultimately affecting the transferability of the model across domains. Thus, the domain gap induced by spectral variations undermines the stability and effectiveness of domain-invariant feature extraction. The variation and accuracy of this phenomenon have been scrutinized through different methods\nFurthermore, domain shifts in HSIs are caused by various factors including imaging time, imaging geographical location, imaging season, etc. [11], [12], therefore the degree of domain shifts is diverse in different scenes and different training stages. Traditional fixed alignment strategy across different datasets and training phases may result in insufficient transfer or negative transfer in HSIs.\nIn this paper, we propose a novel framework, named shift-sensitive spatial-spectral disentangling learning network (SDL), which aims to enhance the disentanglement of domain-invariant features from domain-specific features. Firstly, a gradient-guided spatial-spectral decomposition is designed to quantify the domain information of each channel based on the gradient of domain classification, and subsequently generate tailored masks to decouple domain-invariant and domain-specific channels. Secondly, a shift-sensitive adaptive monitor is incorporated to cope with various degrees of domain shift in various datasets and training stages. This detector continually monitors the inter-domain variance and dynamically fine-tunes the alignment strategy by using exponential moving average (EMA) strategy. Finally, a reversible feature extractor (RFE) is constructed to retain domain information lying in low-level features by preserving and embedding low-level features together with semantic features for alignment.\nOur contributions are summarized as follows:\n1) We propose a novel joint disentangling unsupervised\ndomain adaptation framework for cross-scene HSI clas-\nsification by collaboratively decoupling both spectral\nand spatial dimensions simultaneously, and RFE is in-\ntroduced for enhancing the fine-grained spatial informa-\ntion at high-level feature maps, which jointly leads to\nimproved transferability to different scenes.\nassociated with domain classifier provides a direct mea-\nsure of domain-specific information for each channel,\nallowing for continuous monitoring and dynamic disen-\ntangling domain-invariant channels for classification.\n3) To address the variations in the extent of domain gap\nacross different scenes and different training phases, we\npropose an adaptive domain shift detector that dynami-\ncally modifies the model's alignment strategy according\nto the scale of domain shifts during training, enabling\nit to be suitable for different datasets with various\ndomain shifts, thus enhancing the model's generalization\ncapabilities."}, {"title": "II. RELATED WORKS", "content": "The primary goal of UDA is to leverage the extensive knowledge gained from a source domain, characterized by abundant annotated training examples, for application in target domains that exclusively utilize unlabeled data. To achieve such a goal, a variety of methods are built by matching the statistical distribution differences [13]\u2013[18], aligning marginal or joint distribution [19]\u2013[23] or adopting self-training schemes [24]\u2013[26] and consistency regularizations [27]\u2013[29].\nRecently, domain disentangling defines a new UDA paradigm by separating domain-invariant and domain-specific features into distinct feature subspaces, while maintaining the orthogonality between the shared subspace for domain-invariant information and the private subspace for the domain-specific information [5], [7], [30]. This design promotes the transfer of domain-invariant features to downstream tasks and discards the harmful domain-specific features, which guarantees the model's ability to transfer and generalize by performing domain alignment in the shared subspace. Specifically, Bousmalis et al. [5] proposed Domain Separation Networks (DSN), firstly introduced disentangled representation learning to domain adaptation, extracting source-specific features, target specific features and domain-invariant features by private source encoder, private target encoder, and shared encoder, respectively. Then, a shared decoder is used to reconstruct the original images. These three encoders are decoupled by the orthogonal loss. Building on DSN, Lee et al. [8] attempted to disentangle individual features by content and style, then translate domains by style transformations. In order to narrow intra-domain and inter-domain gaps, Zhou et al. [6] proposed self-adversarial disentangling to learn domain-invariant features in a domain-specific dimension. However, these disentangling strategies have not paid attention to the domain information in channels. Nevertheless, there is plenty of spectral information in HSIs, which causes the insufficiency of existing disentangling methods, leading to a performance decline in cross-scene classification of HSIs."}, {"title": "B. Domain Adaptation for Hyperspectral Image Classifcation", "content": "In domain adaptation of HSI classification, previous works focus on learning more discriminative features in an unsupervised way, especially in the target domain. For example,"}, {"title": "C. Channel-wise Feature Enhancement", "content": "Work that explicitly models the importance weights of channels is also relevant to this paper, for example, the squeeze-and-excitation mechanism [33] and the channel attention mechanism [34]. These methods have been applied across different domains, including semantic segmentation [35] and image super-resolution [36]. Contrary to these aforementioned methods that designed singularly for feature extraction within a specific domain, our method diverges in two aspects. Firstly, in terms of channel importance generation, instead of the SE module or an attention matrix, we quantify the domain information of each channel explicitly through the gradient of the domain classification. Secondly, in terms of utilization, as opposed to their emphasized on enhancing feature extraction within a singular domain, we employ it for feature disentanglement. This involves the explicit decomposition of domain-invariant and domain-specific channels, which aim to amplify the inter-domain transfer ability of the model."}, {"title": "III. METHODOLOGY", "content": "To handle the insufficient disentangling and the stationary alignment strategies in the existing methods, we propose a novel shift-sensitive spatial-spectral disentangling learning network, namely S\u2074DL. Our model comprises three main components: the reversible feature extractor (RFE), the gradient-guided spatial-spectral decomposition (GSSD), and the shift-sensitive adaptive detector (SSAM).\nAs illustrated in Fig. 2, our S4DL deploys a siamese architecture for feature extraction. For a given pair of images from source and target domains, their corresponding feature maps are extracted using a shared backbone. For preventing the vanishing of low-level information at high-level features, we substitute the conventional CNN backbone with the RFE. The obtained feature maps are then fed to the domain-invariant extractor, and each feature map $F$ is disentangled into a domain-invariant feature map $F^{di}$ and its supplementary domain-specific counterpart $F^{ds}$, such that $F = F^{di} + F^{ds}$. For quantifying the domain information across diverse channels, the proposed GSSD is attached to $(F^{di}, F^{ds})$ for further refining the obtained domain-invariant and domain-specific components along the spectral dimension, with the assistance of the gradient back-propagated from a domain discriminator $D$. For enhancing the adaptivity of our GSSD to fluctuative domain shifts across scenes, the proposed SSAM is injected for dynamically adjusting the proposed GSSD. Finally, the obtained domain-invariant feature is fed to the classification head."}, {"title": "A. Gradient-guided Spatial-Spectral Decomposition", "content": "With extended spectrum coverage and dense spectral sampling interval, HSIs provide rich channel dimensional information, compared to natural images. While existing UDA methods are dominantly constructed on an over-simplified encoder for extracting domain-invariant features and their domain-specific counterpart, the underlying structure and distribution along the channel dimension are overlooked. As summarized in Fig. 1, it is evident that, without proper treatments for handling the channel information, considerable variances are observed over the obtained domain-invariant features, which hinders model transferability across domains. To this end, we highlight that a stronger channel disentangling mechanism is key to extracting domain-independent features for hyperspectral image domain adaptation. In this work, we propose a novel GSSD module, where refinements along the channel dimension are attended to the decoupled domain-specific and domain-invariant features. Since it is non-trivial to conduct such refinements with no explicit supervision available, we dive into the gradients from a domain classifier for additional guidance, leading to improved domain-invariant features with minimized channel variance.\nSpecially, for an input image from either the source domain or the target domain, let $F \\in R^{H\\times W\\times C}$ denote its corresponding feature map from the backbone, where $H$ and $W$ are its height and width, and $C$ is the number of channels. This feature map is fed to the domain invariant encoder (DIE) and decomposed to a domain-invariant component and its domain-specific counterpart, denoted by $F^{di}$ and $F^{ds}$, respectively. Notably, $F^{ds} = F \u2013 F^{di}$. Instead of directly passing $F^{di}$ and $F^{ds}$ for down-stream tasks, we introduce two $C$-dimensional binary channel filters, termed as the domain-invariant kernels $u \\in R^{C}$ and the domain-specific kernels $v \\in R^{C}$. Then, the refined domain-invariant feature $\\tilde{F}^{di}$ and domain-specific feature $\\tilde{F}^{ds}$ can be obtained by attending these filters to each pixel location at $F^{di}$ and $F^{ds}$, respectively. This process can be achieved by applying $1 \\times 1$ depth-wise convolution filters over $F^{di}$ and $F^{ds}$ with the kernels constructed from $u$ and $v$, \n$\\tilde{F}^{di} = DWConv_{1\\times 1}(F^{di}, u), \\\\ \\tilde{F}^{ds} = DWConv_{1\\times 1}(F^{ds}, v),$\nwhere $DWConv_{1\\times 1}(\\cdot,\\cdot)$ refers to the depth-wise convolution operator with a kernel of size of $1 \\times 1$. Intuitively, these kernels play important roles for filtering out non-disentanglable channels, however, the selection of proper kernels remains an open problem, due to the inaccessibility of supervision on both kernels.\nAs each channel contributes to feature in-variance by varying significance, we prefer to gain guidance from this fact for quantifying the amount of domain information in each channel. To this end, a domain discriminator $D$ is employed for determining the domain labels of the domain-invariant features from source and target domains, and our domain-invariant filter is estimated on the contribution of each channel toward accurate domain prediction. For a given domain-invariant feature map $F^{di}$ obtained by DIE, it is globally pooled into a feature vector $P^{di} \\in R^{C}$, which is then passed to the domain discriminator $D$. For measuring the contribution of a feature channel toward accurate domain prediction, we define a channel domain discriminability metric of $F^{di}$ as\n$w^{di}_c = \\frac{P^{di}_c}{\\frac{\\partial D(P^{di})}{\\partial P^{di}_c}}, \\forall c \\in \\{1, 2, 3, ..., C\\},$ \nwhere a greater $w^{di}_c$ naturally implies the $c$-th channel contains more domain-discriminative information. Our domain-invariant filter $u$ is designed for suppressing those channels with top $K$ $w^{di}_c$ scores as\n$u_c =\\begin{cases}\n0 & \\text{if } c\\in \\text{argsort}(-w^{di})[:K] \\text{ and } w^{di}_c > 0 \\\\\n1 & \\text{otherwise}\n\\end{cases},$\nwhere the kernel elements of a top portion of channels are set as 0. argsort($\\cdot$)$[:K]$ is used to find the indices of the smallest $K$ elements. The number of suppressed channels $K$ is a fraction of the total channel number, $K = C \\times r$, where $r$ denotes the suppressing ratio. This design helps ease the domain-invariant feature refinement among channel dimension by filtering out channels with strong contributions to domain-specific information.\nAt the same time, as each channel contributes to specific features by varying contributions, we use the shared domain discriminator $D$ to provide guidance for quantifying the amount of domain information in each channel. Based on the gradient of each channel obtained from different domain labels, the contribution of each channel to domain classification is estimated, thereby our domain-specific filter $v$ is generated. The domain-specific feature map $F^{ds}$ extracted by DIE is processed through GAP, after which $P^{ds} \\in R^{C}$ is input into $D$. The channel domain discriminability metric of $F^{ds}$ is defined"}, {"title": "B. Shift-Sensitive Adaptive Monitor", "content": "In cross-scene HSIs, domain shifts might be caused by various factors such as differences in imaging time, location, seasons, and sensors. Furthermore, the scale of domain shift between extracted features in source domain and target domain fluctuates dynamically with model training. Consequently, the scale of these domain shifts across different scenes and different training stages may differ greatly. Fixed domain alignment for different scenes and different training stages might induce negative transfer. Therefore, it is necessary to dynamically measure the scale of domain shift and adjust the alignment strategy, making it suitable for various scenes.\nIn SSAM, the scale of the domain shift is defined by measuring the distribution of the channel variance between source and target domains during the training process. Then, depending on the scale of the domain shift, the extent of alignment is dynamically adjusted by updating the mask ratio of domain-invariant features and domain-specific features.\nSpecifically, $\\mu_e$ is used to represent the scale of the domain shift. A larger $\\mu_e$ indicates a greater disparity of feature values across different domains, suggesting a larger domain shift and the need for a more aggressive alignment strategy. Conversely, a smaller value of $\\mu_e$ implies a lesser disparity in feature values between different domains, indicative of a smaller domain shift, and thus calling for a more gentle alignment strategy. Therefore, $\\mu_e$ is designed to update the mask ratio $r_e$ for the alignment strategy. In order to map $\\mu_e$ into the [0, 1] range, a shifted Sigmoid function is designed to establish the mapping.\n$r'_e = \\frac{1}{1+ e^{-k(\\mu_e-s)}}$,\nwhere $r'_e$ is the temporary mask ratio for $r_e$ of the $e^{th}$ epoch, $k$ and $s$ represent the slope and offset adjustment parameters of the Sigmoid function, which are employed to yield a smooth output by mapping $\\mu_e$ to an appropriate range.\nThe calculation of $\\mu_e$ begins by measuring the channel variance between the source and target domains, and then computing the average of these variances, as shown below:\n$\\mu_e =\\frac{1}{C}\\sum_{c=1}^{C}(\\frac{1}{n_s + n_t}\\sum_{k=1}^{n_s}(P^{di}_{sk,c} - \\bar{P^{di}_{c}})^2+\\sum_{k=1}^{n_t}(P^{di}_{tk,c} - \\bar{P^{di}_{c}})^2)$,\nwhere $n_s$ and $n_t$ are the numbers of source samples and target samples, and $\\bar{P^{di}_{c}}$ is the mean of channel variance, which is defined as follows:\n$\\bar{P^{di}_{c}} = \\frac{1}{n_s + n_t}(\\sum_{k=1}^{n_s} P^{di}_{sk,c} + \\sum_{k=1}^{n_t}P^{di}_{tk,c}),$\nTo preserve the stability of the training process, the EMA method is employed to update the mask ratio $r_e$ of the $e^{th}$ epoch as\n$r_e = (1 - m) \\cdot r_{e-1} + m \\cdot r'_e.$"}, {"title": "C. Reversible Feature Extractor", "content": "In HSIs, domain-invariant information and domain-specific information present in not just in high-level semantic features but also low-level features such as the corners of buildings and the texture and details of plants. However, existing methods primarily align high-level semantic features, neglecting the alignment of low-level features. For example, DAN [14] only aligns feature distribution in the last few layers, while"}, {"title": "D. Loss Function of S\u2074DL", "content": "S4DL consists of three main components: RFE, GSSD and SSAM. All these components are updated by end-to-end training through Eq. 12. The overall loss $L_{total}$ of SDL is defined as follows:\n$L_{total} = L_{cls} + \\lambda_1 L_{ortho} + \\lambda_2 L_{dom},$\nwhere $L_{cls}$ is the cross-entropy loss for the labeled source domain [42]. $L_{ortho}$ is the orthogonal loss computed between $F^{di}$ and $F^{ds}$ to enhance their differentiation [5]. $L_{dom}$ is the domain classification loss [19], [20], and $\\lambda_1$ and $\\lambda_2$ are hyperparameters that control the weight of the loss terms."}, {"title": "IV. EXPERIMENTAL RESULTS AND ANALYSIS", "content": "For performance evaluation, three challenging HSI datasets, Houston, HyRANK and S-H, are selected, and the performance on these datasets is examined in terms of class-specific accuracy, overall accuracy (OA), and Kappa coefficient.\nHouston. The Houston dataset is compose of Houston-2013 [45] and Houston-2018 [46], captured by different sensors in 2013 and 2018 over the University of Houston, Texas, USA. Houston-2013 contains 349 \u00d7 1905 pixels with"}, {"title": "B. Implementation Details", "content": "For a fair comparison, the input patch size is set as 11 \u00d7 11 for all the methods, and Z-score normalization is conducted"}, {"title": "C. Main Results", "content": "For validating the effectiveness of our SDL, a Support Vector Machine (SVM) baseline without any domain adaptation and 10 top-performing UDA methods are selected for comparison. On all datasets, we collect the average and variance of the reported evaluation metrics from 10 rounds of experiments by each method.\nAmong the selected UDA methods, DDC [13], DAN [14], JAN [15] and DSAN [43] are statistics matching methods, where both DDC and DAN use Maximum Mean Discrepancy [48] loss for adaptation, JAN uses Joint Maximum Mean Discrepancy loss, and DSAN uses Local Maximum Mean Discrepancy loss, with the number of kernels of DAN, JAN, and DSAN being 5. DANN [19] and MCD [22] are domain adversarial methods that share a discriminator architecture identical to S4DL. ST [24], a semi-supervised method, operates with a confidence threshold set to 0.7. DSN [5] is a domain disentangling method, utilizing a uniform backbone for the shared encoder, the private target encoder, and the private source encoder. SCLUDA [3] and TSTNet [2] are recent corss-scene HSI classification methods, and we reproduce the results by following their original setups.\nHouston. As summarized in Table. I, the proposed S\u2074DL achieves the highest OA and Kappa scores. More specifically, our S4DL outperforms the existing top-performer TSTNet by 1.8% and 5.8% in term of OA and Kappa, respectively. Notably, on the categories of Grass stressed, Trees, Non-residential buildings and Road, the accuracy has increased by 30.1%, 1.4%, 16.7% and 11.3%, respectively. Compared with DSN that is based representation disentangling without channel decomposition, the proposed S\u2074DL is higher by 12.0% in OA and 12.7% in Kappa scores owing to the adaptive disentangling strategy in channel dimensions. Qualitatively, with enhanced ability to capture domain-invariant features, our SDL tends to generalize better to unseen scenes and produce classification with reduced false alarms.\nHyRANK. Table. II shows the cross-scene classification results in HyRANK dataset. Compared with suboptimal TSTNet, the proposed S4DL has improved by 1.9% in OA and 5.3% in Kappa scores. Compared with the third best DSN,\nour S4DL has improved by 4.0% in OA and 4.0% in Kappa scores, which confirm that our method can separate domain-invariant and domain-specific features more comprehensively, thereby further aiding the learning of discriminative features. Meanwhile, among all DA methods, our S\u2074DL achieves the best results on most 12 categories. In addition, it is difficult for most algorithms to correctly classify the Fruit trees and Rocks and Sand, while our S4DL improves these categories by up to 33.9% and 70.1%, and by at least 6.0% and 1.7% respectively.\nS-H. As shown in Table. VI, compared with other methods, our S4DL exhibits improvements of at least 1.0% in OA and 2.0% in Kappa, respectively. Especially compared with TSTNet, S\u2074DL maintains a high accuracy with improvements of 10.7% in OA and 16.2% in Kappa. When compared with DSN, our method has improved the OA by 1.3% and the Kappa score by 1.8%, verifying that our S\u2074DL can alleviate the channel confusion caused by the phenomenon of same objects with different spectra in cross-scene HSIs."}, {"title": "D. Ablation Study", "content": "To verify the effectiveness and contribution of each component to the overall performance by our S4DL, we conducted ablation studies on the three selected datasets."}, {"title": "E. Feature Visualization", "content": "To further assess the alignment performance, we use t-SNE to reduce dimensionality and visualize the distribution of the input data and the domain-invariant feature $\\tilde{F}^{di}$ extracted by SDL on the Houston dataset, as shown in Fig. 10.\nTable. VIII shows the role of $F^{di}$ and $F^{ds}$ branches in GSSD and their impact on model performance. It can be seen that the channel decomposition alone in either $F^{di}$ or $F^{ds}$ improves the model's performance. This confirms the premise that rich domain information exists in the spectral dimension. When $F^{di}$ and $F^{ds}$ channels are decomposed at the same time, the model performance is best. With the addition of GSSD, the model's cross-scene classification ability is significantly enhanced because the joint disentangling strategy can comprehensively decouple different domain-invariant and domain-specific features, ensuring the model's transferability.\n$\\tilde{F}^{di}$or $\\tilde{F}^{di}$ is represented in blue, and the distribution of target domain data or $\\tilde{F}^{di}$ is represented in orange. All the data is mapped to 2D by t-SNE method.\nIt can be clearly observed that in the original samples, there is a significant domain shift between the distributions of the source domain and the target domain. Interestingly, following feature extraction by S\u2074DL, there is some overlap in $\\tilde{F}^{di}$ from different domains, and the distribution of identical categories in $\\tilde{F}^{di}$ tends to be consistent. This denotes that the features of both the source and target domain in $\\tilde{F}^{di}$ align to the same feature space, effectively alleviating the domain shift."}, {"title": "F. Parameter Tuning", "content": "In S4DL, the slopes k and the offsets s determine the initial value and speed of mask ratio $r_e$ updated in SSAM. Therefore, this determines the intensity of channel disentangling, thus the model is rather sensitive to the choice of k and s. To analyse parameter sensitivity of S\u2074DL on three datasets, the grid search is conducted for different parameters. The search range for k is 0.5, 1, 1.5, 2, 2.5, and the search range for s is 0, 1.25, 2.5, 3.75, 5. It can be seen that when s is fixed and k is in the interval [0.5, 1.5], OA rises as k increases. This is because as k gradually increases, the intensity of model disentangling can be updated more quickly. While k is in the interval [1.5, 2.5], OA decreases with the increase of k. This is because as k becomes too large, the large fluctuation of the disentangling intensity causes the training process to be unstable, thereby deteriorating the model performance.\nCorrespondingly, when k is fixed within the interval [0, 2.5], the model performance improves with the increase of s. This is because when s increases, the value of the mask ratio correspondingly decreases, which ensures the learning of discriminative features. Within k is the interval [2.5, 5], the model performance decreases with the rise of s. This is because when the mask ratio is too low, the model cannot completely disentangle the domain-invariant channel and the domain-specific channel, thereby reducing the transfer performance."}, {"title": "G. Model Efficiency", "content": "The number of parameters and FLOPs of different methods are listed in Table. IX to compare the computational complexity of S4DL with other methods. ResNet18 [49] is chosen as the backbone. The experimental environment and parameter settings, such as patch size, are consistent with Section. IV. B. The number of parameters and FLOPs of DDC, DAN, JAN, and DSAN are the same because these methods have the same"}, {"title": "V. CONCLUSION", "content": "In this paper, a novel and efficient shift-sensitive joint disentangling learning framework S\u2074DL is proposed for cross-scene HSI classification. For the quantitation and separation of domain invariant and domain-specific information in spatial-spectral dimension, S4DL constructs the GGSD. The dynamic feature decomposition allows the model to extract more comprehensive domain invariant features for cross-domain classification. For adaptation to the various scales of domain shift of different datasets and different training stages, S4DL designed the SSAM that adjusts the disentangling strategy in real-time, improving the model's generalization on different datasets. For preservation of domain information in low-order features, S4DL ensembles the RFE to retain and embed low-level features while extracting high-level features. Experimental results on three commonly used cross-scene HSI datasets demonstrate that the proposed S\u2074DL achieves better transfer performance than many other state-of-the-art methods."}]}