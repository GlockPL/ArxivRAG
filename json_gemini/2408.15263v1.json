{"title": "S4DL: Shift-sensitive Spatial-Spectral Disentangling Learning for Hyperspectral Image Unsupervised Domain Adaptation", "authors": ["Jie Feng", "Tianshu Zhang", "Junpeng Zhang", "Ronghua Shang", "Weisheng Dong", "Guangming Shi", "Licheng Jiao"], "abstract": "Unsupervised domain adaptation techniques, extensively studied in hyperspectral image (HSI) classification, aim to use labeled source domain data and unlabeled target domain data to learn domain invariant features for cross-scene classification. Compared to natural images, numerous spectral bands of HSIs provide abundant semantic information, but they also increase the domain shift significantly. In most existing methods, both explicit alignment and implicit alignment simply align feature distribution, ignoring domain information in the spectrum. We noted that when the spectral channel between source and target domains is distinguished obviously, the transfer performance of these methods tends to deteriorate. Additionally, their performance fluctuates greatly owing to the varying domain shifts across various datasets. To address these problems, a novel shift-sensitive spatial-spectral disentangling learning (SDL) approach is proposed. In S\u2074DL, gradient-guided spatial-spectral decomposition is designed to separate domain-specific and domain-invariant representations by generating tailored masks under the guidance of the gradient from domain classification. A shift-sensitive adaptive monitor is defined to adjust the intensity of disentangling according to the magnitude of domain shift. Furthermore, a reversible neural network is constructed to retain domain information that lies in not only in semantic but also the shallow-level detailed information. Extensive experimental results on several cross-scene HSI datasets consistently verified that SDL is better than the state-of-the-art UDA methods.", "sections": [{"title": "I. INTRODUCTION", "content": "HYPERSPECTRAL image (HSI) is obtained by capturing information from the reflection of light from an object or scene at hundreds of different wavelengths. Unlike widely-adopted RGB images, each pixel in HSI not only contains visible light information, but also covers near-infrared, short-wave infrared, mid-infrared and long-wave infrared information, which enables HSIs to capture richer spectral information and detect more accurately [1].\nHowever, factors like lighting, seasonal variations, atmospheric conditions, and differences in sensors lead to an inevitable domain shift between HSI of different scenes, undermining the assumption of independent and identically distributed data. This limitation hinders the transferability and generalization of traditional classification models to other scenes. In response to these challenges, unsupervised domain adaptation (UDA) for HSI has been introduced [2]\u2013[4], which seeks to apply knowledge from a labeled source domain to an unlabeled target domain. While the target task and label spaces of the training and test sets remain identical, their feature distributions differ yet are related. Therefore, the model needs to learn domain-invariant features while dealing with the target task to achieve cross-scene HSI classification.\nUDA is one of the most effective solutions for cross-scene HSI classification by extracting domain-invariant features. Inspired by disentangling learning, domain disentangling methods explicitly separate domain-invariant feature and domain-specific feature while maintaining the model transferability by seeking for the alignment on the domain-invariant features between the source and target domains [5]\u2013[7]. Based on this framework, existing methods introduce style information [8] and instance information [9] in disentangling stage to facilitate the feature disentanglement. Nevertheless, for handling cross-scene HSI classification, it is necessary to capitalize on the domain information in a large number of spectral bands embedded in HSIs.\nIn HSI, there may be a significant domain gap between domains. This is due to the spectral uncertainties of targets, which can be remarkably significant due to the complexity of spectral information and variations in the imaging environment. As a result, the extracted features can become confused across different spectral channels. This channel confusion makes it difficult to consistently extract invariant features, ultimately affecting the transferability of the model across domains. Thus, the domain gap induced by spectral variations undermines the stability and effectiveness of domain-invariant feature extraction. Therefore, it is crucial to reduce domain shifts existing in the spectral dimension of HSIs.\nFurthermore, domain shifts in HSIs are caused by various factors including imaging time, imaging geographical location, imaging season, etc. [11], [12], therefore the degree of domain shifts is diverse in different scenes and different training stages. Traditional fixed alignment strategy across different datasets and training phases may result in insufficient transfer or negative transfer in HSIs.\nIn this paper, we propose a novel framework, named shift-sensitive spatial-spectral disentangling learning network (S\u2074DL), which aims to enhance the disentanglement of domain-invariant features from domain-specific features. Firstly, a gradient-guided spatial-spectral decomposition is designed to quantify the domain information of each channel based on the gradient of domain classification, and subsequently generate tailored masks to decouple domain-invariant and domain-specific channels. Secondly, a shift-sensitive adaptive monitor is incorporated to cope with various degrees of domain shift in various datasets and training stages. This detector continually monitors the inter-domain variance and dynamically fine-tunes the alignment strategy by using exponential moving average (EMA) strategy. Finally, a reversible feature extractor (RFE) is constructed to retain domain information lying in low-level features by preserving and embedding low-level features together with semantic features for alignment.\nOur contributions are summarized as follows:\n1) We propose a novel joint disentangling unsupervised domain adaptation framework for cross-scene HSI classification by collaboratively decoupling both spectral and spatial dimensions simultaneously, and RFE is introduced for enhancing the fine-grained spatial information at high-level feature maps, which jointly leads to improved transferability to different scenes.\n2) In feature disentangling, the gradient-based calculation associated with domain classifier provides a direct measure of domain-specific information for each channel, allowing for continuous monitoring and dynamic disentangling domain-invariant channels for classification.\n3) To address the variations in the extent of domain gap across different scenes and different training phases, we propose an adaptive domain shift detector that dynamically modifies the model's alignment strategy according to the scale of domain shifts during training, enabling it to be suitable for different datasets with various domain shifts, thus enhancing the model's generalization capabilities."}, {"title": "II. RELATED WORKS", "content": "A. Unsupervised Domain Adaptation\nThe primary goal of UDA is to leverage the extensive knowledge gained from a source domain, characterized by abundant annotated training examples, for application in target domains that exclusively utilize unlabeled data. To achieve such a goal, a variety of methods are built by matching the statistical distribution differences [13]\u2013[18], aligning marginal or joint distribution [19]\u2013[23] or adopting self-training schemes [24]\u2013[26] and consistency regularizations [27]\u2013[29].\nRecently, domain disentangling defines a new UDA paradigm by separating domain-invariant and domain-specific features into distinct feature subspaces, while maintaining the orthogonality between the shared subspace for domain-invariant information and the private subspace for the domain-specific information [5], [7], [30]. This design promotes the transfer of domain-invariant features to downstream tasks and discards the harmful domain-specific features, which guarantees the model's ability to transfer and generalize by performing domain alignment in the shared subspace. Specifically, Bousmalis et al. [5] proposed Domain Separation Networks (DSN), firstly introduced disentangled representation learning to domain adaptation, extracting source-specific features, target specific features and domain-invariant features by private source encoder, private target encoder, and shared encoder, respectively. Then, a shared decoder is used to reconstruct the original images. These three encoders are decoupled by the orthogonal loss. Building on DSN, Lee et al. [8] attempted to disentangle individual features by content and style, then translate domains by style transformations. In order to narrow intra-domain and inter-domain gaps, Zhou et al. [6] proposed self-adversarial disentangling to learn domain-invariant features in a domain-specific dimension. However, these disentangling strategies have not paid attention to the domain information in channels. Nevertheless, there is plenty of spectral information in HSIs, which causes the insufficiency of existing disentangling methods, leading to a performance decline in cross-scene classification of HSIs.\nB. Domain Adaptation for Hyperspectral Image Classifcation\nIn domain adaptation of HSI classification, previous works focus on learning more discriminative features in an unsupervised way, especially in the target domain. For example,"}, {"title": "III. METHODOLOGY", "content": "To handle the insufficient disentangling and the stationary alignment strategies in the existing methods, we propose a novel shift-sensitive spatial-spectral disentangling learning network, namely S\u2074DL. Our model comprises three main components: the reversible feature extractor (RFE), the gradient-guided spatial-spectral decomposition (GSSD), and the shift-sensitive adaptive detector (SSAM).\nAs illustrated in Fig. 2, our S4DL deploys a siamese architecture for feature extraction. For a given pair of images from source and target domains, their corresponding feature maps are extracted using a shared backbone. For preventing the vanishing of low-level information at high-level features, we substitute the conventional CNN backbone with the RFE.\nThe obtained feature maps are then fed to the domain-invariant extractor, and each feature map $F$ is disentangled into a domain-invariant feature map $F^{di}$ and its supplementary domain-specific counterpart $F^{ds}$, such that $F = F^{di} + F^{ds}$. For quantifying the domain information across diverse channels, the proposed GSSD is attached to $(F^{di}, F^{ds})$ for further refining the obtained domain-invariant and domain-specific components along the spectral dimension, with the assistance of the gradient back-propagated from a domain discriminator $D$. For enhancing the adaptivity of our GSSD to fluctuative domain shifts across scenes, the proposed SSAM is injected for dynamically adjusting the proposed GSSD. Finally, the obtained domain-invariant feature is fed to the classification head.\nA. Gradient-guided Spatial-Spectral Decomposition\nWith extended spectrum coverage and dense spectral sampling interval, HSIs provide rich channel dimensional information, compared to natural images. While existing UDA methods are dominantly constructed on an over-simplified encoder for extracting domain-invariant features and their domain-specific counterpart, the underlying structure and distribution along the channel dimension are overlooked. It is evident that, without proper treatments for handling the channel information, considerable variances are observed over the obtained domain-invariant features, which hinders model transferability across domains. To this end, we highlight that a stronger channel disentangling mechanism is key to extracting domain-independent features for hyperspectral image domain adaptation. In this work, we propose a novel GSSD module, where refinements along the channel dimension are attended to the decoupled domain-specific and domain-invariant features. Since it is non-trivial to conduct such refinements with no explicit supervision available, we dive into the gradients from a domain classifier for additional guidance, leading to improved domain-invariant features with minimized channel variance.\nSpecially, for an input image from either the source domain or the target domain, let $F \\in R^{H\\times W \\times C}$ denote its corresponding feature map from the backbone, where $H$ and $W$ are its height and width, and $C$ is the number of channels. This feature map is fed to the domain invariant encoder (DIE) and decomposed to a domain-invariant component and its domain-specific counterpart, denoted by $F^{di}$ and $F^{ds}$, respectively. Notably, $F^{ds} = F - F^{di}$. Instead of directly passing $F^{di}$ and $F^{ds}$ for down-stream tasks, we introduce two $C$-dimensional binary channel filters, termed as the domain-invariant kernels $u \\in R^{C}$ and the domain-specific kernels $v \\in R^{C}$. Then, the refined domain-invariant feature $\\tilde{F}^{di}$ and domain-specific feature $\\tilde{F}^{ds}$ can be obtained by attending these filters to each pixel location at $F^{di}$ and $F^{ds}$, respectively. This process can be achieved by applying 1 \u00d7 1 depth-wise convolution filters over $F^{di}$ and $F^{ds}$ with the kernels constructed from $u$ and $v$,\n$$\\tilde{F}^{di} = DWConv_{1\\times1}(F^{di}, u),$$\n$$\\tilde{F}^{ds} = DWConv_{1\\times1}(F^{ds}, v),$$\nwhere $DWConv_{1\\times1}(\\cdot,\\cdot)$ refers to the depth-wise convolution operator with a kernel of size of 1 \u00d7 1. Intuitively, these kernels play important roles for filtering out non-disentanglable channels, however, the selection of proper kernels remains an open problem, due to the inaccessibility of supervision on both kernels.\nAs each channel contributes to feature in-variance by varying significance, we prefer to gain guidance from this fact for quantifying the amount of domain information in each channel. To this end, a domain discriminator $D$ is employed for determining the domain labels of the domain-invariant features from source and target domains, and our domain-invariant filter is estimated on the contribution of each channel toward accurate domain prediction. For a given domain-invariant feature map $F^{di}$ obtained by DIE, it is globally pooled into a feature vector $P^{di} \\in R^{C}$, which is then passed to the domain discriminator $D$. For measuring the contribution of a feature channel toward accurate domain prediction, we define a channel domain discriminability metric of $F^{di}$ as\n$$w_c^{di} = \\left| \\frac{P_c^{di}}{\\frac{\\partial D(P^{di})}{\\partial P_c^{di}}} \\right|, \\forall c \\in \\{1, 2, 3, ..., C\\},$$\nwhere a greater $w_c^{di}$ naturally implies the $c$-th channel contains more domain-discriminative information. Our domain-invariant filter $u$ is designed for suppressing those channels with top $K$ $w_c^{di}$ scores as\n$$u_c = \\begin{cases}\n0 & \\text{if } c \\in argsort(-w^{di})[:K] \\text{ and } w_c^{di} > 0 \\\\\n1 & \\text{otherwise}\n\\end{cases},$$\nwhere the kernel elements of a top portion of channels are set as 0. $argsort(\\cdot)[:K]$ is used to find the indices of the smallest $K$ elements. The number of suppressed channels $K$ is a fraction of the total channel number, $K = C \\times r$, where $r$ denotes the suppressing ratio. This design helps ease the domain-invariant feature refinement among channel dimension by filtering out channels with strong contributions to domain-specific information.\nAt the same time, as each channel contributes to specific features by varying contributions, we use the shared domain discriminator $D$ to provide guidance for quantifying the amount of domain information in each channel. Based on the gradient of each channel obtained from different domain labels, the contribution of each channel to domain classification is estimated, thereby our domain-specific filter $v$ is generated. The domain-specific feature map $F^{ds}$ extracted by DIE is processed through GAP, after which $P^{ds} \\in R^{C}$ is input into $D$. The channel domain discriminability metric of $F^{ds}$ is defined correspondingly to measure the contribution of each channel in $F^{ds}$ towards accurate domain classification.\n$$w_c^{ds} = \\left| \\frac{P_c^{ds}}{\\frac{\\partial D(P^{ds})}{\\partial P_c^{ds}}} \\right|, \\forall c \\in \\{1, 2, 3, ..., C\\},$$\nwhere a greater $w_c^{ds}$ implies the $c$-th channel in $F^{ds}$ contains more domain-discriminative information in the same way. Conversely, the domain-specific filter $v$ is designed for suppressing those channels with the smallest $K$ absolute scores of $w^{ds}$ as\n$$v_c = \\begin{cases}\n0 & \\text{if } c \\in argsort(|w^{ds}|)[:K], \\\\\n1 & \\text{otherwise}\n\\end{cases},$$\nwhere $|\\cdot|$ represents the absolute value operation to avoid confusing incorrect domain-specific information with domain-invariant information, and the kernel elements of the smallest $K$ absolute values of channels are set to 0. This further decomposition of $F^{ds}$ suppresses domain-invariant channels and preserves domain-specific channels, which helps remove domain-specific feature along the channel dimension by identifying channels with little contribution to domain-specific information.\nThrough the secondary extraction of the channel dimension by preserving or suppressing each channel in $F^{di}$ and $F^{ds}$, it has expanded the gap between domain-invariant features and domain-specific features, promoting the generalization ability of the model.\nB. Shift-Sensitive Adaptive Monitor\nIn cross-scene HSIs, domain shifts might be caused by various factors such as differences in imaging time, location, seasons, and sensors. Furthermore, the scale of domain shift between extracted features in source domain and target domain fluctuates dynamically with model training. Consequently, the scale of these domain shifts across different scenes and different training stages may differ greatly. Fixed domain alignment for different scenes and different training stages might induce negative transfer. Therefore, it is necessary to dynamically measure the scale of domain shift and adjust the alignment strategy, making it suitable for various scenes.\nIn SSAM, the scale of the domain shift is defined by measuring the distribution of the channel variance between source and target domains during the training process. Then, depending on the scale of the domain shift, the extent of alignment is dynamically adjusted by updating the mask ratio of domain-invariant features and domain-specific features.\nSpecifically, $\\mu_e$ is used to represent the scale of the domain shift. A larger $\\mu_e$ indicates a greater disparity of feature values across different domains, suggesting a larger domain shift and the need for a more aggressive alignment strategy. Conversely, a smaller value of $\\mu_e$ implies a lesser disparity in feature values between different domains, indicative of a smaller domain shift, and thus calling for a more gentle alignment strategy. Therefore, $\\mu_e$ is designed to update the mask ratio $r_e$ for the alignment strategy. In order to map $\\mu_e$ into the [0, 1] range, a shifted Sigmoid function is designed to establish the mapping.\n$$r_e' = \\frac{1}{1+ e^{-k(\\mu_e-s)}},$$\nwhere $r_e'$ is the temporary mask ratio for $r_e$ of the $e^{th}$ epoch, $k$ and $s$ represent the slope and offset adjustment parameters of the Sigmoid function, which are employed to yield a smooth output by mapping $\\mu_e$ to an appropriate range.\nThe calculation of $\\mu_e$ begins by measuring the channel variance between the source and target domains, and then computing the average of these variances, as shown below:\n$$\\mu_e = \\frac{1}{C} \\sum_{i=1}^{C}(\\frac{1}{n_s + n_t}(\\sum_{k=1}^{n_s}(P_{sk,c}^{di} - \\overline{P^{di}_{c}})^2 + \\sum_{k=1}^{n_t}(P_{tk,c}^{di} - \\overline{P^{di}_{c}})^2)),$$\nwhere $n_s$ and $n_t$ are the numbers of source samples and target samples, and $\\overline{P^{di}_{c}}$ is the mean of channel variance, which is defined as follows:\n$$\\overline{P^{di}_{C}} = \\frac{1}{n_s + n_t}(\\sum_{k=1}^{n_s} P_{sk,c}^{di} + \\sum_{k=1}^{Nt} P_{tk,c}^{di}),$$\nTo preserve the stability of the training process, the EMA method is employed to update the mask ratio $r_e$ of the $e^{th}$ epoch as\n$$r_e = (1 - m) \\cdot r_{e-1} + m \\cdot r_e'.$$\nC. Reversible Feature Extractor\nIn HSIs, domain-invariant information and domain-specific information present in not just in high-level semantic features but also low-level features such as the corners of buildings and the texture and details of plants. However, existing methods primarily align high-level semantic features, neglecting the alignment of low-level features. For example, DAN [14] only aligns feature distribution in the last few layers, while"}, {"title": "IV. EXPERIMENTAL RESULTS AND ANALYSIS", "content": "A. Datasets\nFor performance evaluation, three challenging HSI datasets, Houston, HyRANK and S-H, are selected, and the performance on these datasets is examined in terms of class-specific accuracy, overall accuracy (OA), and Kappa coefficient.\nHouston. The Houston dataset is compose of Houston-2013 [45] and Houston-2018 [46], captured by different sensors in 2013 and 2018 over the University of Houston, Texas, USA. Houston-2013 contains 349 \u00d7 1905 pixels with 144 spectral bands at a spatial resolution of 2.5 meters, and the Houston-2018 dataset contains 210 \u00d7 954 pixels with 48 spectral bands, offering a finer spatial resolution of 1 meter. The overlapped 48 spectral bands are collected from both images. Following [2], 210\u00d7954 pixels from Houston 2013 are selected as the source domain, and Houston 2018 is used as the target domain. Pixel-wise annotations of 7 categories\nHyRANK. The HyRANK dataset [47] covers two hyperspectral scenes, Dioni and Loukia. Both of them are captured by the EO-1 Hyperion hyperspectral sensor. The source domain, Dioni, consists of 250\u00d71376 pixels and 176 bands, and the target domain, Loukia, comprises 249\u00d7945 pixels and 176 bands. The annotations for 12 categories are provided, and please see Table II for more details on the number of samples of these categories. Fig. 5 presents the pseudo-color images and their corresponding ground truth maps.\nS-H. The Shanghai-Hangzhou dataset was acquired using the EO-1 Hyperion hyperspectral sensor, which features 220 spectral bands. The source domain, the Hangzhou scene, comprises 590\u00d7360 pixels, while the target domain, the Shanghai scene, includes 1660\u00d7260 pixels. After the removal of bad bands [2], 198 bands are remained. The annotations on three categories of land covers are provided, which are Water, Land/Building and Plant. Table III summarizes the number of samples and a visualization on the images and their corresponding ground truth maps are provided in Fig. 6.\nB. Implementation Details\nFor a fair comparison, the input patch size is set as 11 \u00d7 11 for all the methods, and Z-score normalization is conducted prior to putting the data into the network. Adaptive moment estimation(Adam) is utilized as the optimization scheme. We adopted a plateau strategy for learning rate decay, applying a decay factor of 0.1 and a patience of 2. In S4DL, the offset k of the shifted Sigmoid function is 1.5, and the slopes is 2.5 on three datasets, which will be discussed in detail in Section F. All the models were trained 10 times using different random seeds, and the averaged results are recorded. All the experiments were conducted by PyTorch 2.0 on NVIDIA GeForce RTX 3090 GPU.\nC. Main Results\nFor validating the effectiveness of our SDL, a Support Vector Machine (SVM) baseline without any domain adaptation and 10 top-performing UDA methods are selected for comparison. On all datasets, we collect the average and variance of the reported evaluation metrics from 10 rounds of experiments by each method.\nAmong the selected UDA methods, DDC [13], DAN [14], JAN [15] and DSAN [43] are statistics matching methods, where both DDC and DAN use Maximum Mean Discrepancy [48] loss for adaptation, JAN uses Joint Maximum Mean Discrepancy loss, and DSAN uses Local Maximum Mean Discrepancy loss, with the number of kernels of DAN, JAN, and DSAN being 5. DANN [19] and MCD [22] are domain adversarial methods that share a discriminator architecture identical to S4DL. ST [24], a semi-supervised method, operates with a confidence threshold set to 0.7. DSN [5] is a domain disentangling method, utilizing a uniform backbone for the shared encoder, the private target encoder, and the private source encoder. SCLUDA [3] and TSTNet [2] are recent corss-scene HSI classification methods, and we reproduce the results by following their original setups.\nHouston. As summarized in Table. I, the proposed S\u2074DL achieves the highest OA and Kappa scores. More specifically, our S4DL outperforms the existing top-performer TSTNet by 1.8% and 5.8% in term of OA and Kappa, respectively. Notably, on the categories of Grass stressed, Trees, Non-residential buildings and Road, the accuracy has increased by 30.1%, 1.4%, 16.7% and 11.3%, respectively. Compared with DSN that is based representation disentangling without channel decomposition, the proposed S\u2074DL is higher by 12.0% in OA and 12.7% in Kappa scores owing to the adaptive disentangling strategy in channel dimensions. Qualitatively, with enhanced ability to capture domain-invariant features, our SDL tends to generalize better to unseen scenes and produce classification with reduced false alarms. As visualized in Fig. 7, our S4DL exhibits more precise classification on Non-residential buildings located at the bottom of the image, with significantly reduced noise. The superior performance on this dataset implies that S\u2074DL demonstrates a robust capability in capturing domain-invariant information during the training process.\nHyRANK. Table. II shows the cross-scene classification results in HyRANK dataset. Compared with suboptimal TST-Net, the proposed S4DL has improved by 1.9% in OA and 5.3% in Kappa scores. Compared with the third best DSN, our S4DL has improved by 4.0% in OA and 4.0% in Kappa scores, which confirm that our method can separate domain-invariant and domain-specific features more comprehensively, thereby further aiding the learning of discriminative features. Meanwhile, among all DA methods, our S\u2074DL achieves the best results on most 12 categories. In addition, it is difficult for most algorithms to correctly classify the Fruit trees and Rocks and Sand, while our S4DL improves these categories by up to 33.9% and 70.1%, and by at least 6.0% and 1.7% respectively. From Fig. 8, it can be observed that our S4DL successfully differentiates between easily confused Sparse Sclerophyllous Vegetation and Rocks and Sand in the lower left corner. This correct classification by our S4DL in an area with significant inter-domain and minimal inter-class differences demonstrates its ability to effectively extract key discriminative features through suitable domain alignment.\nS-H. As shown in Table. VI, compared with other methods, our S4DL exhibits improvements of at least 1.0% in OA and 2.0% in Kappa, respectively. Especially compared with TSTNet, S\u2074DL maintains a high accuracy with improvements of 10.7% in OA and 16.2% in Kappa. When compared with DSN, our method has improved the OA by 1.3% and the Kappa score by 1.8%, verifying that our S\u2074DL can alleviate the channel confusion caused by the phenomenon of same objects with different spectra in cross-scene HSIs. The visualization in Fig. 9 shows that S\u2074DL effectively extracts the details and edge information. For example, in the Land/Building of upper half of the image, S\u2074DL retains the integrity of topology structure while reducing domain shifts.\nD. Ablation Study\nTo verify the effectiveness and contribution of each component to the overall performance by our S4DL, we conducted ablation studies on the three selected datasets.\nDANN, which has the same $L_{dom}$ as the proposed S\u2074DL, is selected as the baseline model to verify the effectiveness of each module. As presented in Table. VII, the generalization performance of the baseline model is relatively weak.\nWhen solely using GSSD, in order to eliminate the impact of different mask ratios, we experimented with the fixed mask ratio rat {0%, 5%, 10%, 15%, 20%} and recorded the highest value as the result. The integration of GSSD results in substantial improvements across all the metrics for the three datasets. The OA improved by 5.3%, 5.5% and 4.2%, and the Kappa improved 4.7%, 5.9% and 5.8%, respectively. This demonstrates that models without suitable adaptive strategies struggle with cross-scene HSI classification.\nAfter confirming the effectiveness of GSSD, we further incorporate SSAM to verify the impact on model performance by dynamically adjusting the disentangling strategy according to the scale of domain shifts between different datasets and different training stages. As shown in Table. VII, subsequent integration of SSAM leads to varying degrees of improvement on the three selected datasets. The OA scores are improved by 1.9%, 1.5% and 2.7% on three datasets, and the Kappa improved by 1.9%, 1.6% and 3.7%, respectively. The varying degrees of improvement across different datasets demonstrates the capacity of SSAM to dynamically modulate the intensity of alignment in relation to the scale of the domain shift inherent in each dataset and training stage.\nMeanwhile, the usage of RFE has improved the baseline by preserving and embedding domain information in low-level features. On this new baseline, adding GGSD and SSAM can further improve the classification performance. Ultimately, the model reaches its peak performance when all the modules are activated.\nTable. VIII shows the role of $F^{di}$ and $F^{ds}$ branches in GSSD and their impact on model performance. It can be seen that the channel decomposition alone in either $F^{di}$ or $F^{ds}$ improves the model's performance. This confirms the premise that rich domain information exists in the spectral dimension. When $F^{di}$ and $F^{ds}$ channels are decomposed at the same time, the model performance is best. With the addition of GSSD, the model's cross-scene classification ability is significantly enhanced because the joint disentangling strategy can comprehensively decouple different domain-invariant and domain-specific features, ensuring the model's transferability.\nE. Feature Visualization\nTo further assess the alignment performance, we use t-SNE to reduce dimensionality and visualize the distribution of the input data and the domain-invariant feature $\\tilde{F}^{di}$ extracted by SDL on the Houston dataset, as shown in Fig. 10. Fig. 10a, 10c, 10e and 10g depict the distributions for three different classes, while Fig. 10b, 10d, 10f and 10h display the distributions of $\\tilde{F}^{di}$. The distribution of source domain data or $F^{di}$ is represented in blue, and the distribution of target domain data or $\\tilde{F}^{di}$ is represented in orange. All the data is mapped to 2D by t-SNE method.\nIt can be clearly observed that in the original samples, there is a significant domain shift between the distributions of the source domain and the target domain. Interestingly, following feature extraction by S\u2074DL, there is some overlap in $\\tilde{F}^{di}$ from different domains, and the distribution of identical categories in $\\tilde{F}^{di}$ tends to be consistent. This denotes that the features of both the source and target domain in $\\tilde{F}^{di}$ align to the same feature space, effectively alleviating the domain shift.\nF. Parameter Tuning\nIn S4DL, the slopes k and the offsets s determine the initial value and speed of mask ratio $r_e$ updated in SSAM. Therefore, this determines the intensity of channel disentangling, thus the model is rather sensitive to the choice of k and s. To analyse parameter sensitivity of S\u2074DL on three datasets, the grid search is conducted for different parameters. The search range for k is 0.5, 1, 1.5, 2, 2.5, and the search range for s is 0, 1.25, 2.5, 3.75, 5. Fig. 11 shows the change trend of classification results of S\u2074DL with different parameters on three datasets. It can be seen that when s is fixed and k is in the interval [0.5, 1.5], OA rises as k increases. This is because as k gradually increases, the intensity of model disentangling can be updated more quickly. While k is in the interval [1.5, 2.5], OA decreases with the increase of k. This is because as k becomes too large, the large fluctuation of the disentangling intensity causes the training process to be unstable, thereby deteriorating the model performance.\nCorrespondingly, when k is fixed within the interval [0, 2.5], the model performance improves with the increase of s. This is because when s increases, the value of the mask ratio correspondingly decreases, which ensures the learning of discriminative features. Within k is the interval [2.5, 5], the model performance decreases with the rise of s. This is because when the mask ratio is too low, the model cannot completely disentangle the domain-invariant channel and the domain-specific channel, thereby reducing the transfer performance.\nG. Model Efficiency\nThe number of parameters and FLOPs of different methods are listed in Table. IX to compare the computational complexity of S4DL with other methods. ResNet18 [49] is chosen as the backbone. The experimental environment and parameter settings, such as patch size, are consistent with Section. IV. B. The number of parameters and FLOPs of DDC, DAN, JAN, and DSAN are the same because these methods have the same"}, {"title": "V. CONCLUSION", "content": "In this paper, a novel and efficient shift-sensitive joint disentangling learning framework S\u2074DL is proposed for cross-scene HSI classification. For the quantitation and separation of domain invariant and domain-specific information in spatial-spectral dimension, S4DL constructs the GGSD. The dynamic feature decomposition allows the model to extract more comprehensive domain invariant features for cross-domain classification. For adaptation to the various scales of domain shift of different datasets and different training stages, S\u2074DL designed the SSAM that adjusts the disentangling strategy in real-time, improving the model's generalization on different datasets. For preservation of domain information in low-order features, S4DL ensembles the RFE to retain and embed low-level features while extracting high-level features. Experimental results on three commonly used cross-scene HSI datasets demonstrate that the proposed S\u2074DL achieves better transfer performance than many other state-of-the-art methods."}]}