{"title": "Foundation Model Engineering: Engineering Foundation Models\nJust as Engineering Software", "authors": ["Dezhi Ran", "Mengzhou Wu", "Wei Yang", "Tao Xie"], "abstract": "By treating data and models as the source code, Foundation Models\n(FMs) become a new type of software. Mirroring the concept of\nsoftware crisis, the increasing complexity of FMs making FM crisis\na tangible concern in the coming decade, appealing for new theo-\nries and methodologies from the field of software engineering. In\nthis paper, we outline our vision of introducing Foundation Model\n(FM) engineering, a strategic response to the anticipated FM crisis\nwith principled engineering methodologies. FM engineering aims\nto mitigate potential issues in FM development and application\nthrough the introduction of declarative, automated, and unified\nprogramming interfaces for both data and model management, re-\nducing the complexities involved in working with FMs by providing\na more structured and intuitive process for developers. Through\nthe establishment of FM engineering, we aim to provide a robust,\nautomated, and extensible framework that addresses the imminent\nchallenges, and discovering new research opportunities for the\nsoftware engineering field.", "sections": [{"title": "OVERVIEW, MOTIVATION, AND AIMS", "content": "Foundation Models [2] (in short as FMs) are becoming a new type\nof software. An analogy between traditional software and FMs is\ndepicted in Figure 1, highlighting the parallel roles of their core\ncomponents. In FM development, data and models play the critical\nrole akin to source code in conventional software development.\nDevelopers curate datasets, such as sets of example input-output\npairs, to articulate the specifications for the desired FM. They then\nchoose an appropriate network architecture and employ model\ntraining techniques such as backpropagation [47], to effectively\n\"compile\" the dataset and network architecture into the targeted\nFM.\nGiven the analogy between FMs and traditional software coupled\nwith the increasing complexity of FM development (depicted in\nFigure 2), we envision that an FM crisis, mirroring the concept of\nthe software crisis [7], will surface as a tangible concern over the\ncoming decade. This potential crisis can be evidenced across four\nmajor aspects:\n\u2022 Increasing FM complexity. Since the invention of Trans-\nformer [52], the complexity of FMs (represented by the num-\nber of parameters) has been increasing at an exponential\nrate. This growing complexity raises significant challenges\nnot only in training and managing these FMs but also in\nmanaging the datasets that underpin them. Specifically, as\ndatasets grow in size to unprecedented levels, the tasks of\ncleaning, labeling, and managing data at such a scale become\nincreasingly challenging.\n\u2022 Continuous FM evolution. FMs are in a state of continu-\nous and fast evolution, driven by the integration of new data,\nthe emergence of new requirements, and the implementa-\ntion of bug fixes. For example, the average time between\nupdates for OpenAI's models is 2 weeks [39], more frequent\nthan that of Linux, whose mainline kernels are updated ev-\nery 9-10 weeks [27]. Beyond the updates to the base FM,\ndevelopers often finetune these models within their specific\napplication domains. When the base FM updates, particularly\nsecurity-related fixes, it necessitates a corresponding update\nto the finetuned FMs. It is challenging to efficiently and cost-\neffectively manage the evolution of these finetuned FMs,\ncatching up with the base model updates while maintaining\ntheir domain-specific enhancements.\n\u2022 Diverse customization demand. The era of FMs is char-\nacterized not only by their technological advancements but\nalso by the broad spectrum of customization demands. On\nHugging Face [11], there are 254,871 models with Trans-\nformer architecture, 127,462 datasets, and over 170,000 \u0391\u0399\napp demos across various application domains. This diverse\necosystem of models, datasets, and applications is a clear\nindication that the future of LLMs lies in their ability to be\ncustomized. During the customization, ensuring data pri-\nvacy, model interpretability, and bias elimination requires\ninnovations of data management and model management.\n\u2022 Multi-agent collaboration. The development of FMs in-\nvolves multi-agent collaboration among data scientists, model\ndevelopers, and application domain experts. This collabora-\ntion introduces significant hurdles, primarily due to differing\nobjectives, terminologies, and methodologies across disci-\nplines. The specialized language used by data scientists and\nengineers may not align with the domain-specific knowledge\nand practical insights of application domain experts, poten-\ntially leading to misunderstandings and delays in project\ntimelines. Innovative techniques are needed for minimizing\nthe barriers posed by interdisciplinary work and effective\ncollaboration.\nInspired by the role that software engineering has played in\naddressing software crisis [7], this paper outlines our vision of\nfoundation model engineering (depicted in Figure 3), a strategic re-\nsponse to the anticipated FM crisis. Foundation model engineering\naims to mitigate potential issues in FM development and applica-\ntion through the introduction of declarative, automated, and unified\nprogramming interfaces for both data and model management, re-\nducing the complexities involved in working with FMs by providing\na more structured and intuitive process for developers. The key\ncomponents of our proposed foundation model engineering include:\n\u2022 Data management with weak supervision. Recognizing\nthe importance of high-quality data in training effective FMs,\nwe advocate for advanced data management strategies that\nleverage weak supervision. This approach allows for the ef-\nficient labeling and curating of vast datasets by combining\nlimited amounts of labeled data with large quantities of un-\nlabeled data, using algorithms to infer labels and improve\ndata quality. This method significantly reduces the time and\nresources required for data preparation.\n\u2022 Model management with workflows and continuous in-\ntegation. Effective model management is essential for the\nscalable and sustainable development of FMs. We envision\nthe implementation of workflows that encompass model de-\nvelopment, training, evaluation, and deployment processes,\nautomating routine tasks and ensuring optimal practices.\nWe also envision a distributed version control system like\nGit [29] to track the update of FMs, manage model branches,\nand resolve conflicts of model updates.\n\u2022 Programmatic FM development with declarative speci-\nfications. To further simplify the engineering of FMs, we\nenvision unified and declarative APIs that abstract away\nthe underlying complexities of model and data management.\nThese APIs allow developers to specify what they want to\nachieve in a high-level language, without needing to provide\ndetailed instructions on how to accomplish these tasks. This\nnot only makes the process more accessible to a wider range"}, {"title": "ENVISION OF FM ENGINEERING", "content": "2.1 Overview of Foundation Model Engineering\nFigure 3 presents the overview of the envisioned Foundation Model\nEngineering (in short as FME), which manages resources, abstracts\ncommon operations, and provides APIs for related developers to\nengineer data and model in declarative ways.\nData management. In the landscape of FME, data management\nemerges as a key element. The ease with which machines generate\nmassive volumes of data presents a unique challenge, especially in\nensuring full coverage across enormous public or private informa-\ntion sources where collection complexities multiply. The criticality\nof data relevance cannot be overstated; amidst the deluge, distin-\nguishing valuable data for analysis and decision-making becomes\ncrucial. Moreover, the effort to process and refine this raw data into\na form that is both accessible and actionable is a formidable task\nthat FME tackles head-on.\nAt the core of FME's strategy is the fundamental principle that\nacquiring the appropriate data is critical to the success of the entire\noperation. It is widely recognized that simple models built on well-\ncurated datasets often surpass their complex counterparts that are\nfed skewed or incomplete data. An integral part of this approach\nincludes rigorous auditing and inspection of data pipelines, which\nsafeguards data integrity and ensures that processing aligns with\npredefined goals and compliance requirements.\nOnce raw data is collected, the next phase involves transforming\nthis data into meaningful signals within FME. This transformation\nis a multi-step procedure that commences with data wrangling\nfor cleaning and structuring, followed by aggregation to distill\nsummary insights. Anomaly detection algorithms sift through to\nhighlight irregularities, while pattern-matching and linear regres-\nsion inform on current trends and future directions. The process\nreaches its end with the deployment of advanced machine learn-\ning models that extract complex patterns and forecasts, thereby\ndeepening the comprehension of the data's story.\nFME maintains the data assets for model training and finetuning.\nData developers can declare data labelling and cleaning functions\nwith high-level intention descriptions and weak-supervision, such\nas specifying exemplar data-label pairs. FME takes the schema as\ninput, selects an appropriate model for parsing and understanding\nthe schema (typically a code generation model or SQL generation\nmodel), and generates data labeling or cleaning functions to perform\nthe data labelling or cleaning at scale. In addition to data labeling,\nFME also implements access control to data assets to assure the\ndata access conforms to legal policies as well as user intentions.\nModel management. FME champions a novel paradigm for ma-\nchine learning, paralleling the collaborative dynamism of open-\nsource software development. This initiative seeks to transform\nthe lifecycle of foundation models from static entities to evolving\nconstructs, continuously refined through community contributions.\nUnlike traditional open-source software, which thrives on collective\ninputs and evolution, foundation models often see their develop-\nment halt post-release. To bridge this gap, FME is cultivating a\nculture where machine learning models are not just released but\nare actively developed, enhanced, and adapted through collabora-\ntive efforts.\nCentral to this culture shift is the strategic facilitation of efficient\nchange communication and contribution integration, steering clear\nof the impracticalities of transferring voluminous parameters char-\nacteristic of contemporary models. Leveraging insights like Fisher\ninformation [31], FME focuses on pinpointing and updating specific\nsubsets of parameters. This targeted approach enables substantial\nperformance enhancements without the heft of large-scale data\ntransfers.\nIn the spirit of collaborative software engineering, the explo-\nration of merging contributions from disparate sources stands as a\ntestament to the power of collective intelligence. By integrating the"}, {"title": "Overview of Data Management", "content": "diverse expertise of independently crafted models, FME paves the\nway for new capabilities and amplified performance in specialized\ntasks.\nThe push towards modularity in machine learning is informed by\nthe \"mixture of experts\" architectural paradigm [9, 37, 55, 57], where\nspecialized sub-networks synergize through adaptive routing, facili-\ntating the backpropagation of discrete choices. Such an arrangement\nempowers models to assimilate domain-specific knowledge with\nunprecedented efficiency.\nLastly, to facilitate the version control and integration of updates,\nthe concept of git for models, is being developed [20]. This system\naims to track parameter updates, support efficient merging, and\nintegrate seamlessly with existing workflows, marking a significant\nstep towards a more collaborative machine learning ecosystem.\nFME also automates the model finetuning process, where de-\nvelopers specify tasks and declare accessible data assets. The sys-\ntem intelligently selects suitable models, identifies finetuning data\nfrom the available pool, and executes finetuning with auto-adjusted\nhyperparameters. Upon completion, FME merges the redundant\nmodels, resolves conflicts during merging, and returns the merged\nmodel for subsequent usage.\n2.2 Data Management\nAs shown in Figure 4, we envision an integrated environment and\ndecarative interfaces that support data management with minimal\nhuman efforts. Data is the user interface to \"program\" FMs, and a\nuser-friendly interface is expected to fulfill four major requirements.\nFirst, FMs are trained on vast datasets, comprising billions of words\nand documents from the Internet, books, articles, and more. The\nquality, diversity, and size of the training data directly affect the\nLLM's ability to understand and perform tasks ranging from dialog\nsystem to code generation. Second, labelled data are required to\nadapt FMs to specific downstream tasks or non-functional proper-\nties such as human preference alignment [40]. Third, data sourcing,\naccess control and unlearning are crucial for data management to\nconform to legal policies and user privacy requirements. Finally, the\ngoal is to fulfill the above requirements in an automated streamline\nwith minimal human efforts.\nTo fulfill the preceding requirements, we envision that the data\nmanagement module of FME consist of the following four modules.\nData cleaning. The data cleaning module, critical in ensuring\ndata integrity and quality for FMs, embraces a high-level, declara-\ntive framework. Within this framework, users specify the desired\ncharacteristics of clean data, guiding the module to automatically\npinpoint and rectify inaccuracies, inconsistencies, and redundan-\ncies in the datasets according to these specifications. The module\nis enriched with an interactive feedback loop. This crucial feature\nallows users to review and validate the automated cleaning actions\nundertaken by the system, facilitating an iterative refinement pro-\ncess. Through this dynamic interaction, users can fine-tune the\ncleaning criteria, ensuring it resonates with the unique aspects of\nthe data and their specific needs. By employing automated tools and\nalgorithms, this module can detect anomalies, filter out irrelevant\nor sensitive information, and standardize data formats. This step is\ncrucial for reducing noise in the training data, thereby enhancing\nthe FM's learning efficiency and effectiveness in understanding\ncomplex language patterns and generating coherent outputs.\nData labelling. Data labelling focuses on annotating the train-\ning data with informative tags or labels that define the context\nor the desired outcome of the data. This is particularly important\nfor supervised learning tasks where the FM needs to recognize\npatterns or generate responses based on specific inputs. Leverag-\ning automated data labeling strategies with human specifications,\nestablish high-level rules or heuristics, facilitating the automatic\ncreation of labels over large datasets. This method is strengthened\nby incorporating specialized knowledge through labeling functions\nand using weak supervision techniques to utilize a broad range of\ninformation sources for deducing labels. In addition, the module\niteratively enhances label accuracy by leveraging model-driven in-\nsights to rectify initial ambiguities or errors introduced by heuristic\nrules or noisy labels. This module enables the customization of FMs\nfor specialized applications, from sentiment analysis to personal-\nized content creation, by aligning the model's outputs with human\npreferences and task-specific requirements.\nAccess control. Data sourcing must comply with copyright laws,\nprivacy regulations, and ethical standards. The use of publicly avail-\nable data, proprietary datasets, and user-generated content requires\ncareful examination. Access control mechanisms are implemented\nto manage who can view or use the data, ensuring that only au-\nthorized users or systems have the ability to access or modify the\ndatasets. By incorporating robust authentication, authorization, and\nauditing processes, the access control module safeguards sensitive\ninformation and prevents unauthorized data breaches, thereby fos-\ntering trust in the LLM's development and deployment processes.\nWeak supervision. The weak supervision module aims to reduce\nthe reliance on extensively labeled datasets by utilizing less precise\nlabels that can be generated more easily or derived from heuristic\nrules and external knowledge sources. This approach allows for the\nrapid scaling of training data while managing resource constraints\nand minimizing manual labelling efforts. Through advanced algo-\nrithms and models that can learn from weakly supervised data,\nthis module supports the efficient training of FMs across a broader\nrange of tasks and domains, accelerating the model's adaptability\nand performance improvement."}, {"title": "Model Management", "content": "As shown in Figure 5, we develop an integrated environment and\ndistributed version control system that supports the development,\nevolution and deployment of FMs.\nNowadays, model updates rarely start from retraining from\nscratch but instead involve incremental fine-tuning where only\na small fraction of parameters are changed. Considering that dif-\nferent applications may finetune their own FMs from the same\npretrianed FM, maintaining separate FMs for different application\nencounters similar problems as code cloning problems common in\ntraditional software engineering, reducing software reliability and\nmaintainability. Given that different users have different fine-tuning\ndata, leading to different or even conflict parameter updates, main-\ntaining these FMs can be challenging for developers to manually\ncheck and update the FMs.\nInspired by Git [29], we envision a distributed version control\nsystem of foundation models (shown in Figure 6), providing plat-\nform support for the community-driven development, evolution,\nand continuous integration of FMs, The system offers a unified\nsoftware development and version management abstraction for\nboth new applications and the supporting platform.\nModel selection. The model selection module empowers users to\nidentify the most suitable FMs for their specific applications based\non performance benchmarks, compatibility, and previous usage\noutcomes. For example, the Composition of Experts (CoE) [48] is\nused in model selection to aggregate multiple specialized models\nto improve overall performance and accuracy. For example, in a\nCoE system, there could be distinct models expert in language\nunderstanding, image recognition, and sentiment analysis. When\nfaced with a complex task that involves understanding text within\nimages and gauging sentiment, the CoE framework would select\nand combine the outputs of these expert models. This modular\napproach allows for targeted fine-tuning of each expert model,\nensuring that the collective output is both accurate and efficient in\nhandling the task at hand.\nModel finetuning. The model finetuning module offers a flexible\nand user-friendly toolkit for customizing FMs to meet the unique\ndemands of diverse applications. It simplifies the process of apply-\ning incremental updates, adjusting parameters, and integrating new\ndata, thereby enabling personalized model optimization without\nthe need for extensive machine learning expertise. The finetuning\nmodule provides an intuitive toolkit, enabling users to tailor Foun-\ndation Models for diverse needs with ease. It supports collaborative,\nlarge-scale, distributed learning environments, where contributors\nwork on separate data without sharing, ensuring data privacy and\nownership while optimizing model performance through a central\nminimal-computation repository. This distributed setup allows for\nparallel training tasks on shared servers, maintaining parameter\nownership and preventing task interference, streamlining the path\nto personalized model optimization.\nModel merging. The process of model merging after finetuning in-\nvolves a collaborative and incremental approach, similar to practices\nin open-source software development. Addressing the challenges of\ndivergent fine-tuning efforts, the model merging module incorpo-\nrates sophisticated algorithms to harmonize changes from multiple\nsources. This method allows for efficient communication of updates\nbetween contributors and a central repository, focusing on updat-\ning only a selected subset of parameters based on their significance,\nas determined by measures like Fischer information [31, 50]. This\napproach ensures that updates are manageable in size, reducing\ncommunication costs and complexity. The ultimate goal is to com-\nbine the strengths of independently trained models, preserving\nthe benefits of each, to enhance the overall performance and ca-\npabilities of the merged model in a distributed and collaborative\nlearning environment. This ensures consistency, mitigates conflicts,\nand maintains the integrity of FMs across different applications,\nsignificantly reducing the maintenance burden and promoting col-\nlaborative improvements.\nModel deployment. The model deployment module streamlines\nthe process of rolling out updated or newly finetuned FMs into\nproduction environments. The deployment process begins with\nuser requests, which are systematically queued. These requests are\nthen managed by a deployment setup, often running on a Kuber-\nnetes environment, where the FM is loaded into memory within\ncontainers organized into pods. Within this deployment, there are\ntwo primary types of model weights: the base model weights, which\nform the core parameters of the FM, and adapter weights, which\nare a smaller set of parameters that allow for model fine-tuning.\nOne of the central challenges in deploying fine-tuned FMs is\nthe significant resource requirements, particularly in terms of GPU\nusage. Each new user or task traditionally necessitates a new pod\nand GPU, leading to potentially excessive resource consumption.\nHowever, a crucial insight is that most of the model weights across\ndifferent deployments remain identical, with variations primarily\nin the adapter weights. This realization opens up possibilities for\nsharing the base model across multiple adapters, thereby optimizing\nresource use.\nAddressing this challenge, we envision a system with several in-\nnovative components. Firstly, it employs dynamic loading of adapter\nweights, allowing the system to serve multiple user requests by\nonly loading the necessary adapter weights alongside the base\nmodel weights into memory. This approach significantly reduces\nthe need for additional resources per user or task. Secondly, the\nsystem incorporates a multi-tier weight cache to manage adapter\nweights efficiently. This cache includes a GPU cache for actively\nused adapters, a CPU cache for adapters awaiting activation, and\nan idle tier for adapters not currently in use, with their weights\nstored on ephemeral disk storage for potential future requests.\nAnother key innovation that we can adopt here is continuous\nmulti-adapter batching, an extension of the continuous batching\nconcept [43]. This technique allows the system to process requests\nfrom multiple adapters together in the same batch, significantly\nimproving throughput and efficiency. The batching algorithm cen-\ntral to this system prioritizes adapters based on request timestamps\nand employs a cycle time parameter to manage the swapping of\nadapters in and out of the active set, striking a balance between\nthroughput and latency. The model deployment module ensures\nsmooth transition, minimizes downtime, and facilitates continu-\nous delivery, allowing developers and users to leverage the latest\nadvancements with ease and confidence.\nTogether, these components form a comprehensive ecosystem\nthat not only simplifies the management of FMs but also acceler-\nates the pace of innovation, fostering a collaborative and dynamic\nenvironment for the advancement of intelligent applications."}, {"title": "STATE OF THE PRACTICE", "content": "3.1 Data-Centric Machine Learning\nKaplan et al. [21] reveals that improving in model architectures\nusually offer limited benefits. In contrast, the efficacy of data utiliza-\ntion is becoming the cornerstone of advancing model performance\ngiven the increasing dataset scale enabled by self supervised learn-\ning techniques [6, 44].\nData cleaning. Data cleaning is the process of addressing errors,\nduplications, and incompleteness in datasets by modifying, adding,\nand deleting data. Holoclean [46] utilizes a variety of methods\nincluding heuristic rules (such as integrity constraints), external\nknowledge, and quantitative statistics to integrate multiple data\nsources into a probabilistic model, identifying and correcting er-\nrors in datasets. Picket [28] employs self-supervised deep learning\nmodels to identify and remove corrupted data without the need\nfor human supervision. Neutatz et al. [38] found that the benefits\nof data cleaning largely depend on the application, leading to the\nproposal of end-to-end, application-driven, holistic data cleaning\napproaches. ActiveClean[24] combines data cleaning with active\nlearning, prioritizing the cleaning of data that could potentially\nimpact model performance in specific application domains.\nData programming with weak supervision. The need for large\nlabeled datasets to optimally train modern machine learning mod-\nels presents a major bottleneck, due to the high costs and time\nneeded for expert manual annotation. In response, active learn-\ning strategies[49] streamline this process by selectively engaging\nexperts to label data of maximal utility-such as instances at the\nfringes of classification models-thereby amplifying model efficacy\nwith diminished input. Parallelly, semi-supervised learning[51] c\u0430\u0440-\ntalizes on a modest quantum of labeled data supplemented by\nunlabeled data to enhance model accuracy, effectively economiz-\ning on the need for extensive labeled datasets. Weak supervision\napproaches harness cost-effective techniques for gathering lower-\nquality labeled data through avenues like Crowdsourcing[22], Dis-\ntant Supervision[36], or heuristic rules, markedly alleviating the\nreliance on manual labeling. The Snorkel system[45] enables users\nto employ labeling functions that encapsulate heuristic methods,\ncombines different weak supervision sources to generate a proba-\nbilistic distribution of labels.\n3.2 Incremental Model Training\nParameter-Efficient Fine-Tuning (PEFT). The extensive param-\neter set of FMs renders their fine-tuning both computationally ex-\npensive and storage-intensive. PEFT techniques offer a solution by\nfine-tuning of a fraction of their parameters, The compact nature of\nspecialized PEFT modules facilitates their dissemination within the\ncommunity, as evidenced by the availability of over 20,000 adapters\non the Hugging Face Model Hub, all of which are based on the PEFT\nframework [32].\nPEFT strategies are predominantly categorized into three distinct\napproaches. First, additive methods entail the integration of addi-\ntional parameters into the original Large Language Model (LLM)\narchitecture, with the fine-tuning process focusing exclusively on\nthese new parameters. Notably, Houlsby et al. [15] introduced fully-\nconnected networks as adapter modules within the transformer\narchitecture, after the attention and Feed-Forward Network (FFN)\nlayers. Prompt tuning[25] and prefix tuning[26] incorporate task-\nspecific vector sequences at the input layer and throughout various\nlayers of the LLM, respectively, with fine-tuning achieved through\nthe adjustment of these vector parameters. Second, selective methods\ninvolves the selective training of a subset of the LLM's parameters.\nBitFit[56] fine-tune only the bias parameters, while DiffPruning [12]\nemploys an L0-norm to train a sparse weight matrix. Freeze and\nReconfigure [53] and FishMask[50] identify and train crucial model\nparameters based on L1-distance and Fisher information, respec-\ntively. Third, reparametrization-based methods modifies the original\nmodel's parameter matrix into a more tractable low-rank format\nfor training purposes. Intrinsic SAID [1] utilizes the FastFood trans-\nform for reparameterizing model weight updates. LoRa[16] decom-\npose the weight matrix updates into products of low-rank matrices,\nwith KronA[10] employing Kronecker product for matrix factoriza-\ntion. AdaLoRa [59] adopts Singular Value Decomposition (SVD) for\nparameter matrix decomposition, prioritizing resources based on\nthe significance of different weight matrices. Additionally, innova-\ntive integrations such as SparseAdapter [14], MAM Adapters[13],\nUniPELT[33], Compacter [23], and S4[3] amalgamate various PEFT\nmethodologies to enhance efficiency and adaptability.\nModular training for multi-task learning. Ilharco et al. [18]\nintroduces the concept of a task vector to delineate the shifts within\nthe model's parameter space consequent to fine-tuning for a specific\ntask. He further elucidates that performing arithmetic operations on\nthis task vector facilitates the processes of forgetting, constructing\nmulti-task models, and task analogies. In the quest to refine model\nmerging capabilities leveraging the task vector, Matena and Raffel\n[34] applies Fisher information weighting, Jin et al. [19] frames the\nchallenge as an optimization quandary and resolves it via linear\nregression, while Yadav et al. [54] mitigates interference by eliminat-ing superfluous parameters and reconciling symbol discrepancies.\nChoshen et al. [4] and Don-Yehiya et al. [8] engage in iterative\nfine-tuning of the Fundamental Model (FM) across diverse tasks,\nsubsequently averaging the weights to enhance the FM. Further, an\namalgamation of PEFT techniques with model merging strategies\nis explored [5, 17, 30, 41, 42, 58]. Ponti et al. [42] posits that each\ntask is linked to a spectrum of skills, with each skill mirrored by\nan adapter, and devises a routing function to allocate skills per\ntask. AdapterSoup [5] employs task textual similarity and clustering\ntechniques to identify auxiliary tasks conducive to the target task,\nfacilitating the merger of pertinent adapters. LoRaHub [17] adopts a\ngradient-free optimization strategy to fine-tune LoRa model merg-\ning, guided by few-shot examples of the target task.\nA noteworthy trend in recent research [9, 37, 55, 57] is the de-\nvelopment of mixture-of-experts models to tackle the multi-task\nconundrum. Within such models, a selective activation of a subset\nof experts is triggered at each layer contingent on the input, thereby\nfocusing inference and training efforts solely on the activated ex-\nperts. This approach has been shown to yield superior performance,\nparticularly when models are extended to tasks beyond their initial\ntraining scope."}, {"title": "RESEARCH OPPORTUNITIES", "content": "To fulfill our vision of FM engineering, numerous research oppor-\ntunities arise, which can be categorized into three-fold.\n4.1 Declarative FM Engineering\nDeclarative FM engineering represents a paradigm shift in the devel-\nopment of Large Language Models, emphasizing the specification\nof what the model should achieve rather than how it achieves it.\nThis approach, rooted in the principles of declarative programming,\noffers a more intuitive and efficient methodology for designing,\ntraining, and deploying FMs. It invites a wealth of research oppor-\ntunities aimed at simplifying the complex process of FM engineer-\ning, making it more accessible and adaptable to a broader range of\napplications and developers.\nHigh-level model specification languages. Developing high-\nlevel, domain-specific languages for FM engineering that allow\ndevelopers to specify the desired outcomes, constraints, and be-\nhaviors of the model in an abstract manner. Research in this area\ncould focus on creating intuitive syntax and semantics that encap-\nsulate the complexities of neural network architectures, training\nprocedures, and data processing pipelines.\nAutomated model synthesis. Building on high-level specifica-\ntions, automated model synthesis involves research into algorithms\nand systems capable of translating these abstract descriptions into"}, {"title": "CONCLUSION", "content": "In conclusion, this paper envisions foundation model engineer-\ning, aiming to streamline the development of foundation models\nthrough innovative infrastructure software and methodologies. By\nsimplifying data and model management and emphasizing auto-\nmated, declarative interfaces, we envision a future where LLMs are\nmore accessible, efficient, and ethically developed. This approach\nnot only promises to accelerate innovation within the machine\nlearning field but also ensures that the profound benefits of LLMs\ncan be leveraged across various sectors, contributing positively to\nsocietal advancement. As we advance, collaborative and consci-\nentious efforts will be key to realizing the full potential of these\ntechnologies in a responsible and beneficial manner."}]}