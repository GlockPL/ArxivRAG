{"title": "IndicMMLU-Pro: Benchmarking Indic Large Language Models on Multi-Task Language Understanding", "authors": ["Sankalp KJ", "Ashutosh Kumar", "Laxmaan Balaji", "Nikunj Kotecha", "Vinija Jain", "Aman Chadha", "Sreyoshi Bhaduri"], "abstract": "Known by more than 1.5 billion people in the Indian subcontinent, Indic languages present unique challenges and opportunities for natural language processing (NLP) research due to their rich cultural heritage, linguistic diversity, and complex structures. IndicMMLU-Pro is a comprehensive benchmark designed to evaluate Large Language Models (LLMs) across Indic languages, building upon the MMLU Pro (Massive Multitask Language Understanding) framework. Covering major languages such as Hindi, Bengali, Gujarati, Marathi, Kannada, Punjabi, Tamil, Telugu, and Urdu, our benchmark addresses the unique challenges and opportunities presented by the linguistic diversity of the Indian subcontinent. This benchmark encompasses a wide range of tasks in language comprehension, reasoning, and generation, meticulously crafted to capture the intricacies of Indian languages. IndicMMLU-Pro provides a standardized evaluation framework to push the research boundaries in Indic language AI, facilitating the development of more accurate, efficient, and culturally sensitive models. This paper outlines the benchmarks' design principles, task taxonomy, and data collection methodology, and presents baseline results from state-of-the-art multilingual models. As a publicly available resource, IndicMMLU-Pro is set to contribute significantly to advancements in Indic language-based technologies and serve as a valuable tool for the NLP community.", "sections": [{"title": "1 Introduction", "content": "With over 1.5 billion speakers, Indic languages constitute a substantial component of the world's linguistic tapestry, showcasing the incredible diversity of the Indian subcontinent, where languages from the Indo-Aryan and Dravidian families have evolved over centuries, shaped by a shared cultural and historical context. As an integral part of daily life, these languages facilitate communication and accessibility in various aspects of society, including education, government, media, healthcare, and social services. Further, by developing language-specific benchmarks and conducting research on Indic languages, we can increase the downstream impact on these languages on inclusion, ultimately leading to better language support and enhanced accessibility for the deaf and hard-of-hearing communities across the region who use Indic sign languages (Ananthanarayana et al., 2021b). This, in turn, can enable greater social inclusion, improved education and employment opportunities, and more effective participation in civic life for sign language users.\nThe disparity in Natural Language Processing (NLP) research and resources across languages is striking with a survey of existing literature showing how resources and research on Indic languages lagging (KJ et al., 2024a). In their work, (Joshi et al., 2020) revealed a stark imbalance, where a mere 28 percent of languages are considered \"winners,\" while a staggering 88 percent are \"left behind.\" This disparity is exemplified by the contrast between English and Bengali, languages with comparable speaker populations (Lane, 2019). Despite this, English dominates Bengali regarding available resources, with hundreds of times more visibility on platforms like the Linguistic Data Consortium, Wikipedia, and academic publication venues of significance.\nThus, Indic languages have historically received less attention in the field of NLP compared to more globally dominant languages (Das et al., 2024). The disparity in NLP resources for Indic languages has been attributed to their remarkable linguistic diversity, intricate morphology, and the scarcity of annotated datasets (Kakwani et al., 2020a; Marreddy et al., 2022). Nevertheless, driven by the increasing demand for NLP applications in Indic languages and the prospect of groundbreaking technological innovations (Ananthanarayana et al., 2021a), there is an urgent need for rigorous evaluation benchmarks to assess the performance of AI models in this domain accurately.\nTo address this gap, we introduce IndicMMLU-Pro, a benchmark built upon the principles of the recently released MMLU-Pro (Wang et al., 2024) by Tiger Labs. IndicMMLU-Pro adapts the robust multi-task principles to the unique context of Indic languages, providing a comprehensive evaluation framework that assesses the linguistic understanding, reasoning abilities, and generative capabilities of AI models.\nThis paper makes three key contributions. Primarily, we introduce IndicMMLU-Pro, a novel benchmark for evaluating AI models across a wide range of tasks and multiple Indic languages. Next, the design principles, task taxonomy, and data collection methodology of IndicMMLU-Pro are presented in detail, to ensure that the benchmark accurately captures the complex linguistic and cultural characteristics of Indic languages. Furthermore, we establish baseline results on IndicMMLU-Pro using state-of-the-art multilingual models, laying the groundwork for future research and development of Indic languages-based Al models."}, {"title": "2 Methodology", "content": "To create IndicMMLU-Pro, a benchmark for Indic languages equivalent to MMLU Pro, we adopted a process similar to the prior work on IndicMMLU, organizing our approach into two main steps: dataset creation and baseline benchmarking."}, {"title": "2.1 Dataset Creation", "content": "Our goal was to provide MMLU-Pro in nine Indic languages: Hindi, Bengali, Telugu, Marathi, Tamil, Gujarati, Urdu, Kannada and Punjabi. To achieve this, we used IndicTrans2 (Gala et al., 2023), a state-of-the-art machine translation model specifically designed for Indic languages.\nAs shown in Figurel, we use IndicTrans2 to convert the questions and corresponding options from the original English MMLU Pro dataset into each of the target languages. This approach allowed us to maintain the structure and content of the original benchmark while adapting it to the linguistic characteristics of Indic languages.\nThe experimental settings for IndicTrans2 were as follows:\n\u2022 Model Size: 1B parameters\n\u2022 Quantization: None\n\u2022 Batch Size: 8\nWe applied these settings consistently across all nine language translations to ensure uniformity in the translation process."}, {"title": "2.2 Quality Assurance", "content": "To maintain the integrity and accuracy of the translated content, we implemented a rigorous quality assurance process:\n\u2022 Back-translation: For a subset of the data, we performed back-translation to English and compared it with the original text to identify any significant discrepancies.\n\u2022 Validation: After back-translation, we verified the back-translated dataset with the original MMLU-Pro on numerous metrics, such as chrF++, BLEU, METEOR, TER & SacreBLEU to ensure the dataset's quality and consistency, also described in Section 3. This multi-metric evaluation provides a comprehensive assessment of the translation's accuracy and fluency."}, {"title": "2.3 Dataset Structure", "content": "The resulting IndicMMLU-Pro dataset maintains the same structure as the original MMLU-Pro dataset, with separate subsets for each of the nine Indic languages. The dataset retains the original categories and task types from MMLU Pro, thus ensuring identical usage and facilitating comprehensive evaluation in various domains and cognitive skills."}, {"title": "2.4 Dataset Availability", "content": "The IndicMMLU-Pro dataset is publicly available on the Hugging Face Hub (KJ et al., 2024b). This allows for easy access and reproducibility of our results. Researchers and practitioners can directly use or adapt this dataset for their studies and applications in the processing of Indic languages."}, {"title": "2.5 Baseline Benchmarking", "content": "To establish baseline performance metrics for the IndicMMLU-Pro benchmark, we evaluated state-of-the-art multilingual language models: GPT-40, GPT-40-mini, Llama-3.1-8B-Instruct, IndicBERT (Kakwani et al., 2020a), IndicBART (Dabre et al., 2021), RemBERT (Chung et al., 2020), MuRIL (Khanuja et al., 2021), and XLM-RoBERTa (Conneau et al., 2020), Navarasa, Airavata (Gala et al., 2024), OpenHathi, TamilLlama (Balachandran, 2023), and MahaMarathi. These models were chosen due to their demonstrated capabilities in handling multiple languages and their specific design for the inclusion of Indic languages in their training data. The benchmarking process was conducted as follows:\n\u2022 Model Selection: To provide a benchmark of performance over the various languages contained in IndicMMLU-Pro, we leverage the following language models with Indic language understanding capabilities.\nIndicBERT and IndicBART: Specifically designed for Indic languages, offering robust performance in this domain.\nRemBERT: A multilingual model with strong performance across diverse languages, suitable for cross-lingual tasks.\nMuRIL: A multilingual model with a focus on Indian languages, providing comprehensive coverage of Indic languages.\nXLM-ROBERTa: A large-scale multilingual model is known for its cross-lingual performance and ability to handle multiple languages efficiently.\nGPT-40 and GPT-40-mini: State-of-the-art models with advanced capabilities in handling multilingual tasks.\nLlama-3.1-8B-Instruct: A model designed for multilingual (Hindi) instruction tasks.\nNavarasa, Airavata, OpenHathi, TamilLlama, and MahaMarathi: Models specifically designed or fine-tuned for Indic languages, enhancing their performance in this domain.\n\u2022 Data Preparation: We used the test split of the IndicMMLU-Pro dataset for each of the nine Indic languages. The data was preprocessed to match the input format required by each model.\n\u2022 Evaluation Process: We used accuracy as the primary metric, calculated as the percentage of correct predictions across all tasks in the benchmark. We acknowledge the importance of using multiple evaluation metrics to garner a comprehensive understanding of text data (Bhaduri et al., 2024a), as different metrics can capture distinct aspects of model performance (Bedemariam et al., 2025). Nevertheless, for this study, accuracy served as a suitable baseline metric. Notably, our evaluation was conducted separately for each language, enabling language-specific performance analysis and facilitating a more nuanced understanding of model strengths and weaknesses across diverse linguistic contexts.\n\u2022 Computational Resources: The benchmarking was performed on a cluster of NVIDIA A100 GPUs, with each model evaluation taking approximately 24 hours per language."}, {"title": "2.6 Proofreading and Setup", "content": "We conducted a proofreading exercise on 9,000 sentence pairs (English and Indic), where 1,000 of these pairs were from the test split of each of the nine languages in the IndicMMLU-Pro dataset. These sentences pairs were stratified equally across the 14 categories and their respective 4 sources in each of the nine languages. These sentence pairs were shuffled randomly across all Indic languages to eliminate any bias and ensure there was no preservation for each source and category after the selection process. The three criteria that were defined for proofreading scores were:\n\u2022 Semantic Accuracy and Correctness: ensuring the translation conveys the exact meaning of the original text without errors, omissions, or additions.\n\u2022 Fluency and Readability: check if the translation reads naturally, smoothly, and is easy to understand in the target language.\n\u2022 Linguistic and Stylistic Appropriateness: ensuring the tone, style, and language fit the purpose, audience, and cultural context of the text.\nA total of 13 experts, representing all 9 Indic languages, participated in the proofreading exercise. The expert distribution ensured that each language had at least one expert, with four languages having an additional expert to facilitate workload sharing. Using a standardized evaluation framework, each expert assessed sentence pairs based on three criteria, assigning scores on a 5-point scale (1 = lowest, 5 = highest). Comprehensive guidelines, accompanied by illustrative samples, were provided to all experts to ensure consistency in scoring."}, {"title": "3 Results", "content": "Table 3 presents the evaluation metrics for the IndicMMLU-Pro dataset using back-translation techniques. The chrF++ (Popovi\u0107, 2017) and BLEU (Papineni et al., 2002) scores are provided for each of the nine Indic languages (Bengali, Gujarati, Hindi, Kannada, Marathi, Punjabi, Tamil, Telugu, and Urdu). These metrics assess the quality and accuracy of the translated content in the IndicMMLU-Pro benchmark, providing insights into the dataset's linguistic fidelity across different Indic languages.\nTable 1 and 2 present the accuracy scores (in percentages) of various pre-trained language models evaluated on the IndicMMLU-Pro benchmark. The benchmark covers nine major Indic languages: Bengali, Gujarati, Hindi, Kannada, Marathi, Punjabi, Tamil, Telugu, and Urdu. The scores reflect the models' ability to handle diverse linguistic challenges specific to Indic languages."}, {"title": "3.1 Overall Performance", "content": "Performance Range: The accuracy scores across all models and languages now fall within a broader range of 9.90% to 44.80%. This wider range reflects the significant performance differences between the newer, more advanced models (like GPT-40) and the previously evaluated models.\nModel Comparison: GPT-40 consistently outperforms all other models across all languages, achieving the highest scores ranging from 38.46% to 44.80%. GPT-40 mini follows as the second-best performer with scores ranging from 25.75% to 35.08%. Among the previously evaluated models, XLM-ROBERTa remains the top performer, consistently outperforming other models across most languages with scores ranging from 11.92% to 13.16%.\nLanguage-Model Variability: There is now a much more pronounced variability in performance across different languages and models. This suggests that language-specific characteristics and model architectures play significant roles in performance outcomes."}, {"title": "3.2 Language-wise Performance", "content": "Breaking down the performance for each language reveals interesting patterns:\n\u2022 Hindi: GPT-40 leads with 44.80%, followed by GPT-40 mini (32.33%). Among other models, Navarasa (12.43%) and XLM-ROBERTa (12.33%) perform best.\n\u2022 Bengali: GPT-40 achieves 44.38%, with GPT-40 mini at 31.11%. XLM-ROBERTa (12.68%) and Navarasa (12.08%) lead among other models.\n\u2022 Telugu: GPT-40 scores 41.34%, GPT-40 mini 26.78%. XLM-ROBERTa (12.62%) outperforms Navarasa (11.77%) and TamilLlama (11.53%).\n\u2022 Marathi: GPT-40 reaches 42.20%, GPT-40 mini 27.13%. RemBERT (12.93%) performs best among earlier models, with Navarasa at 11.88% and MahaMarathi at 11.60%.\n\u2022 Tamil: GPT-40 mini shows strong performance at 35.08%, close to GPT-40's 38.46%. Navarasa (12.38%) slightly outperforms XLM-ROBERTa (12.34%), with TamilLlama at 11.54%.\n\u2022 Gujarati: GPT-40 scores 41.77%, GPT-40 mini 28.29%. IndicBART (12.14%) and RemBERT (12.13%) perform well among earlier models.\n\u2022 Urdu: GPT-40 achieves 44.18%, GPT-40 mini 31.13%. XLM-ROBERTa (12.53%) leads among other models.\n\u2022 Kannada: GPT-40 scores 38.97%, GPT-40 mini 25.75%. XLM-ROBERTa significantly outperforms other models (13.16%) among earlier models.\n\u2022 Punjabi: GPT-40 reaches 40.60%, GPT-40 mini 26.25%. XLM-ROBERTa (12.59%) performs best among other models."}, {"title": "3.2.1 Cross-linguistic Analysis", "content": "Indo-Aryan languages (Hindi, Bengali, Punjabi, Gujarati, Urdu, Marathi) and Dravidian languages (Kannada, Tamil, Telugu) exhibit distinct performance patterns.\n\u2022 Performance Patterns: GPT-40 and GPT-40 mini consistently outperform all other models across both language families.Among other models, XLM-ROBERTa generally performs best across both language families. Navarasa shows competitive performance, particularly for the Dravidian language\n\u2022 Language-specific Models: TamilLlama and MahaMarathi show promise but fail to surpass the performance of top multilingual models like XLM-ROBERTa and Navarasa.\n\u2022 Consistency: GPT-40 and GPT-40 mini demonstrate relatively consistent performance across different languages, suggesting robust multilingual capabilities. Navarasa also shows consistency, particularly among Dravidian languages.\n\u2022 Script-based Patterns: Languages using the Devanagari script (Hindi, Marathi) exhibit similar performance patterns, while Urdu (using the Perso-Arabic script) shows more varied performance.\n\u2022 Performance Gap: A significant performance gap exists between GPT-40/GPT-40 mini and other models, highlighting the advantages of larger, more advanced models in handling the complexities of Indic languages."}, {"title": "3.3 Dataset Quality Assessment", "content": "In order to heuristically assert the quality of Indic language datasets, we convert the dataset from the respective Indic language back to English again by leveraging IndicTrans2 as in Section 2.1. The English translations are then scored by the following metrics:\n\u2022 chrF++ (Popovi\u0107, 2017) (threshold > 50%): A language and token agnostic, character n-gram F1 score. We use n=6, as it is the standard set in the chrF++ paper.\n\u2022 BLEU (Papineni et al., 2002) (threshold > 25-30%): A score obtained by measuring the precision of n-grams of the candidate translation compared to the reference translation, with a brevity penalty to penalize short translations.\n\u2022 METEOR (Banerjee and Lavie, 2005) (threshold = 50-60%): A metric that evaluates translation quality by considering precision and recall, along with a harmonic mean of unigram matches, with additional features such as stemming and synonymy matching.\n\u2022 TER (Snover et al., 2006) (threshold < 40-50%): A score that measures the number of edits required to change a system-generated translation to exactly match a reference translation.\n\u2022 SacreBLEU (Post, 2018) (threshold > 25-30%): A variant of BLEU that ensures standard reference text processing and tokenization.\nTable 3 provides insights into the quality of the IndicMMLU-Pro dataset through back-translation evaluation metrics for three languages: Hindi, Gujarati, and Tamil. The metrics include chrF++, BLEU, METEOR, TER, and SacreBLEU scores.\nHindi shows the highest overall quality with a chrF++ score of 78.06, BLEU of 0.59, METEOR of 0.56, TER of 42.27, and SacreBLEU of 59.07. Gujarati follows closely with very similar scores: chrF++ of 77.67, BLEU of 0.58, METEOR of 0.55, TER of 43.09, and SacreBLEU of 58.28. Tamil, while still demonstrating good quality, shows slightly lower scores across all metrics: chrF++ of 74.32, BLEU of 0.54, METEOR of 0.52, TER of 46.41, and SacreBLEU of 53.64.\nThe high chrF++ scores (above 74) for all three languages indicate good overall translation quality and semantic preservation. The BLEU and SacreBLEU scores, ranging from 0.54 to 0.59 and 53.64 to 59.07 respectively, suggest reasonably good translation quality, though there's room for improvement. The METEOR scores (0.52-0.56) also indicate good semantic similarity between the original and back-translated texts.\nThe Translation Error Rate (TER) scores, ranging from 42.27 to 46.41, suggest that a moderate amount of editing is required to match the reference translation, with Tamil requiring slightly more editing than Hindi or Gujarati.\nIt is important to note that data for the other six languages (Bengali, Punjabi, Kannada, Telugu, Urdu, and Marathi) is missing from Table 2. This limitation in the dataset quality assessment makes it challenging to draw comprehensive conclusions about the overall quality of the IndicMMLU-Pro dataset across all nine languages.\nIn summary, the available metrics indicate good translation quality for Hindi, Gujarati, and Tamil, with Hindi showing the highest quality across all metrics. However, the lack of data for the remaining languages highlights the need for a more comprehensive evaluation of the entire dataset.\nTo better illustrate the meaning of chrF++ scores in practice, let's examine sample translations for Hindi (highest score), Gujarati (middle score), and Tamil (lowest score) as shown in Figure 2, 3, 4 respectively.\nHindi (chrF++: 78.06): The chrF++ score for Hindi is the best among the three languages indicating a high structural and semantic similarity between the original text and the back-translated text.\nGujarati (chrF++: 77.67): While slightly lower, this score still represents a high-quality translation with minor variations.\nTamil (chrF++: 74.32): This lower score might reflect slight changes in word choice or structure, but the core meaning is preserved.\nThese examples demonstrate that even with the lowest chrF++ score in our dataset (74.32 for Tamil), the translations maintain good semantic fidelity. The differences in scores often reflect nuances in word choice, sentence structure, or slight variations in conveying the same meaning, rather than significant errors in translation. This analysis supports our earlier observation that the IndicMMLU-Pro dataset maintains good overall translation quality across the evaluated languages.\nPreliminary assessments of the other languages (Bengali, Punjabi, Kannada, Telugu, Urdu, and Marathi) suggest similar trends in translation quality, with chrF++ scores ranging from 73 to 79. Complete metrics for these languages are being compiled and will be included in future publications."}, {"title": "3.4 Cosine Similarity Scores", "content": "We evaluated the semantic similarity between Indic languages from the IndicMMLU-Pro dataset and the English MMLU-Pro dataset using the LaBSE (Language-agnostic BERT Sentence Embedding) model. It loads both datasets and processes all the samples from each. For each pair of corresponding questions, it generates embeddings for the questions and their multiple-choice options using the LaBSE model.\nThe script then calculates cosine similarity between these embeddings to measure how semantically similar Indic languages and their english versions are. It computes two main metrics: average question similarity, and average choice similarity. The question and choice similarities indicate how close the meanings are between the two languages. This analysis helps assess how accurately Indic languages in the IndicMMLU-Pro represent the original English MMLU-Pro in terms of meaning and structure, which is crucial for ensuring the quality and consistency of multilingual datasets in machine learning and natural language processing tasks."}, {"title": "4 Related Work", "content": "The study of multilingual language models has been significantly advanced by efforts to create and utilize datasets that encompass a wide range of languages. Indic languages, with their diverse scripts and rich morphological structures, present unique challenges and opportunities for multilingual NLP research.\nMultilingual datasets have been crucial in developing language models capable of understanding and generating text in multiple languages. The WikiAnn dataset, for instance, provides a valuable resource for named entity recognition across many languages, including several Indic languages (Pan et al., 2017). The IndicNLP Corpus (Kunchukuttan, 2020), developed as part of the IndicNLP Library, provides extensive monolingual corpora for various Indic languages. This dataset has been instrumental in training and evaluating models specifically designed for these languages. IndicCorp (Joshi et al., 2022), a large-scale dataset of Indic languages, and the IndicNLPSuite(Kakwani et al., 2020b), which includes pre-trained models and linguistic resources, have further enhanced the development of multilingual NLP models. These resources have provided a solid foundation for various NLP tasks such as machine translation, text classification, etc.\nIndicGLUE(Kakwani et al., 2020c) is a comprehensive benchmark designed to evaluate the performance of NLP models on a variety of tasks across multiple Indic languages. It includes tasks such as sentiment analysis, natural language inference, and question answering, making it a valuable resource for assessing the capabilities of multilingual models.\n(Ahuja et al., 2023) introduce Megaverse, a comprehensive benchmark designed to evaluate large language models across a variety of languages, modalities, models, and tasks. This benchmark aims to provide a holistic assessment of language models' performance, emphasizing their capabilities and limitations in handling diverse linguistic and contextual scenarios. Megaverse is instrumental in identifying the strengths and weaknesses of current models and guiding future improvements in multilingual and multimodal language processing.\n(Holtermann et al., 2024) developed MultiQ, a benchmark aimed at evaluating the elementary multilingual capabilities of large language models. MultiQ focuses on assessing basic language understanding and generation tasks across multiple languages, providing insights into the foundational multilingual capabilities of these models. This benchmark is critical for identifying gaps in language model training and performance, particularly for less-represented languages.\n(Aggarwal et al., 2022) introduce IndicXNLI, a benchmark specifically designed to evaluate the natural language inference capabilities of multilingual models for Indian languages. This benchmark includes a diverse set of inference tasks, reflecting the linguistic richness and complexity of Indic languages. IndicXNLI is crucial for advancing the understanding and processing of Indian languages in NLP, providing a standard for evaluating inference models in this context.\n(Kumar et al., 2022) develop the IndicNLG benchmark, which provides multilingual datasets for a variety of natural language generation (NLG) tasks in Indic languages. This benchmark addresses the unique challenges posed by the linguistic diversity of Indic languages, offering datasets for tasks such as machine translation, summarization, and text generation. IndicNLG is instrumental in improving the performance and capabilities of NLG models for Indic languages, fostering advancements in multilingual NLP.\nMMLU (Massively Multilingual Language Understanding) (Liang et al., 2020; Hu et al., 2020) is a comprehensive benchmark designed to evaluate the performance of language models across multiple languages and a wide range of tasks. This benchmark has been instrumental in assessing the capabilities of multilingual models and has provided insights into their strengths and limitations."}, {"title": "5 Key Findings and Implications of IndicMMLU-Pro", "content": "IndicMMLU-Pro represents a significant step forward in the development and evaluation of AI models for Indic languages. Through this comprehensive benchmark, we have shed light on the current capabilities and limitations of state-of-the-art multilingual models in handling the linguistic diversity and complexity of the Indian subcontinent. This section details implications based on some of our key findings."}, {"title": "5.1 Performance Across Models", "content": "5.1.1 GPT-40 Dominance\nGPT-40 consistently outperformed all other models across all languages, with accuracy scores ranging from 38.46% to 44.80%. This suggests that large, advanced language models have significant potential for handling Indic languages."}, {"title": "5.1.2 Performance Tiers", "content": "A clear hierarchy emerged among the models:\n\u2022 Elite Tier: GPT-40\n\u2022 Advanced Tier: GPT-40 mini\n\u2022 Specialized Tier: XLM-ROBERTa, Navarasa, and other specialized models\n\u2022 Foundation Tier: Earlier models like IndicBERT, IndicBART, RemBERT, and MURIL"}, {"title": "5.1.3 Specialized vs. General Models", "content": "While GPT-40 and GPT-40 mini showed superior performance, specialized models like XLM-ROBERTa and Navarasa consistently outperformed other models in the foundation tier. This highlights the importance of both scale and specialized training for Indic languages."}, {"title": "5.2 Language-Specific Insights", "content": "5.2.1 Performance Variability\nSignificant variability in model performance was observed across different Indic languages, emphasizing the need for language-specific approaches in NLP."}, {"title": "5.2.2 Script and Language Family Impact", "content": "Languages using similar scripts (e.g., Devanagari for Hindi and Marathi) showed similar performance patterns. Urdu, using a different script, exhibited more varied performance across models."}, {"title": "5.2.3 Indo-Aryan vs. Dravidian Languages", "content": "While performance varied across both language families, some models showed more consistency in Dravidian languages, suggesting potential differences in how models handle these distinct language groups."}, {"title": "5.3 Dataset and Evaluation Quality", "content": "5.3.1 Translation Quality\nBack-translation evaluation metrics for Hindi, Gujarati, and Tamil demonstrated good overall translation quality and semantic preservation, with chrF++ scores above 74 for all three languages."}, {"title": "5.3.2 Metric Discrepancies", "content": "Differences between chrF++ and BLEU scores suggest that while meaning is generally preserved, there may be variations in phrasing and structure compared to the original English text."}, {"title": "5.3.3 Comprehensive Evaluation Need", "content": "The lack of evaluation metrics for six out of nine languages highlights the need for more thorough assessment across all included Indic languages."}, {"title": "5.4 Methodological Insights", "content": "5.4.1 Benchmark Design\nThe adaptation of MMLU-Pro to create IndicMMLU-Pro demonstrates a successful approach to developing comprehensive, multi-task benchmarks for specific language groups."}, {"title": "5.4.2 Translation Approach", "content": "The use of IndicTrans2 for dataset creation, combined with rigorous quality assurance processes, provides a template for developing high-quality multilingual datasets."}, {"title": "5.4.3 Evaluation Metrics", "content": "The use of multiple metrics (chrF++, BLEU, METEOR, TER, SacreBLEU) for assessing translation quality offers a more nuanced understanding of dataset quality.\nThese findings and implications highlight the significant advancements made by IndicMMLU-Pro in evaluating and understanding the performance of language models across Indic languages, while also pointing to crucial areas for future research and development in multilingual NLP."}, {"title": "5.5 Future Directions", "content": "Based on our findings, we propose several key areas for future research and development:\n\u2022 Data Collection: There is a pressing need for more high-quality, diverse datasets across all Indic languages, particularly for low-resource languages. This will in turn enable more robust model training and evaluation.\n\u2022 Model Development: Future research should focus on developing models that can better handle the unique linguistic features of Indic languages, including their complex morphology and script diversity. This may involve innovative architecture designs or novel pre-training techniques.\n\u2022 Cross-lingual Transfer: Exploring techniques to improve knowledge transfer between related Indic languages will help boost performance, especially for low-resource languages. This could involve leveraging linguistic similarities or developing more effective multilingual training strategies.\n\u2022 Task-specific Fine-tuning: Developing strategies for effective fine-tuning of large multilingual models on specific language tasks lead to significant performance improvements (Balne et al., 2024). Thus future work in this area may include investigating optimal fine-tuning techniques or developing Indic-specific pre-training tasks.\n\u2022 Evaluation Metrics: Further refinement of evaluation metrics to account for the linguistic and cultural nuances of Indic languages is necessary for more accurate performance assessment. Existing metrics often overlook the linguistic and cultural nuances inherent to Indic languages.\nIndicMMLU-Pro sets a new standard for evaluating AI models in the context of Indic languages. By providing a comprehensive, multi-task, and multilingual benchmark, it enables researchers and developers to assess and improve the capabilities of their models across a wide range of Indic languages and domains. As the field of NLP continues to advance, we hope that IndicMMLU-Pro will catalyze increased research and development of technologies based on Indic languages. This benchmark not only highlights the current state of the art but also points the way forward for creating more linguistically diverse, culturally sensitive, and capable AI systems that can serve the vast and diverse population of the Indian subcontinent.\nTo foster a more comprehensive understanding of Indic language models, it is essential to intentionally integrate multi-disciplinary perspectives into engineering and science workstreams (Bhaduri et al., 2024b). Collaborations between engineers, scientists, and experts from social sciences can provide nuanced insights into the complex interplay between language, culture, and technology (Mackenzie et al., 2024). By embracing interdisciplinary approaches and refining evaluation metrics, we can work towards a more inclusive and accurate assessment of Indic language models, ultimately driving progress in natural language processing for diverse languages and communities.\nIn conclusion, while significant challenges remain in the field of Indic language NLP, IndicMMLU-Pro provides a robust framework for measuring progress and guiding future research efforts. As we continue to push the boundaries of what's possible in multilingual AI, benchmarks like IndicMMLU-Pro will play a crucial role in ensuring that technological advancements benefit all language communities, fostering greater inclusivity and accessibility in the digital age."}]}