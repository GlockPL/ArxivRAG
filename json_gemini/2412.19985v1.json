{"title": "The 5th International Verification of Neural Networks Competition (VNN-COMP 2024): Summary and Results", "authors": ["Christopher Brix", "Stanley Bak", "Taylor T. Johnson", "Haoze Wu"], "abstract": "This report summarizes the 5th International Verification of Neural Networks Competition (VNN-COMP 2024), held as a part of the The 7th International Symposium on AI Verification (SAIV), that was co-llocated with the 36th International Conference on Computer-Aided Verification (CAV). VNN-COMP is held annually to facilitate the fair and objective comparison of state-of-the-art neural network verification tools, encourage the standardization of tool interfaces, and bring together the neural network verification community. To this end, standardized formats for networks (ONNX) and specification (VNN-LIB) were defined, tools were evaluated on equal-cost hardware (using an automatic evaluation pipeline based on AWS instances), and tool parameters were chosen by the participants before the final test sets were made public. In the 2024 iteration, 8 teams participated on a diverse set of 12 regular and 8 extended benchmarks. This report summarizes the rules, benchmarks, participating tools, results, and lessons learned from this iteration of this competition.", "sections": [{"title": "1 Introduction", "content": "Deep learning based systems are increasingly being deployed in a wide range of domains, including recommendation systems, computer vision, and autonomous driving. While the nominal performance of these methods has increased significantly over the last years, they largely lack formal guarantees on their behavior. However, in safety-critical applications, including autonomous systems, robotics, cybersecurity, and cyber-physical systems (CPS), such guarantees are essential for certification and reliability.\nWhile the literature on the verification of traditionally designed systems is wide and successful, neural network verification remains an open problem, despite significant efforts over the last years. In 2020, the International Verification of Neural Networks Competition (VNN-COMP) was established to facilitate comparison between existing approaches, bring researchers working on this problem together, and help shape future directions of the field. VNN-COMP has been held annually since then [11, 54, 16, 15]. In 2024, the 5th iteration of the annual VNN-COMP\u00b9 was held as a part of the 7th International Symposium on AI Verification (SAIV) that was collocated with the 36th International Conference on Computer-Aided Verification (CAV).\nThis 5th iteration of the VNN-COMP continues last year's trend of increasing standardization and automatization, aiming to enable a fair comparison between the participating tools"}, {"title": "2 Rules", "content": "Terminology An instance is defined by a property specification (pre- and post-condition), a network, and a timeout). For example, one instance might consist of an MNIST classifier with one input image, a given local robustness threshold e, and a specific timeout. A benchmark is defined as a set of related instances. For example, one benchmark might consist of a specific MNIST classifier with 100 input images, potentially different robustness thresholds e, and one timeout per input.\nRun-time caps Run-times are capped on a per-instance basis, i.e., any verification instance will timeout (and be terminated) after at most X seconds, determined by the benchmark proposer. These can be different for each instance. The total per-benchmark runtime (sum of all per-instance timeouts) may not exceed 6 hours per benchmark. For example, a benchmark proposal could have six instances with a one-hour timeout, or 100 instances with a 3.6-minute timeout, each. To enable a fair comparison, we measure the startup overhead for each tool by running it on a range of tiny networks and subtract the minimal overhead from the total runtime.\nHardware To allow for comparability of results, all tools were evaluated on equal-cost hardware using Amazon Web Services (AWS). Each team could decide between a range of AWS instance types (see Table 1) providing a CPU, GPU, or mixed focus.\nScoring The final score is aggregate as the sum of all benchmark scores. Each benchmark score is the number of points (sum of instance scores discussed below) achieved by a given tool, normalized by the maximum number of points achieved by any tool on that benchmark. Thus, the tool with the highest sum of instance scores for a benchmark will get a benchmark score of 100, ensuring that all benchmarks are weighted equally, regardless of the number of constituting instances.\nInstance score Each instance is scored is as follows:\n\u2022 Correct hold (property proven): 10 points;\n\u2022 Correct violated (counterexample found): 10 points;\n\u2022 Incorrect result: -150 points (penalty increased compared to 2022);\n\u2022 Timeout / Runtime Error / Unknown: 0 points.\nHowever, the ground truth for any given instance is generally not known a priori. In the case of disagreement between tools, we, therefore, place the burden of proof on the tool claiming that a specification is violated, i.e. that a counterexample can be found, and deem it correct exactly if it produces a valid counterexample.\nThe provided counterexamples were supposed to define both the input and the resulting output of the networks. However, for some tools and instances, the output definition was"}, {"title": "3 Participants", "content": "We list the tools and teams that participated in the VNN-COMP 2024 in Table 2 and reproduce their own descriptions of their tools below.\n3.1 \u03b1,\u03b2-CROWN\nTeam Co-leaders: Huan Zhang (UIUC) and Zhouxing Shi (UCLA); Team members: Duo Zhou (UIUC), Jorge Chavez (UIUC), Xiangru Zhong (UIUC), Hongji Xu (Duke, working as an intern supervised by Prof. Huan Zhang at UIUC), Kaidi Xu (Drexel), Hao Chen (UIUC)\nThe team acknowledges (ordered by last names) Christopher Brix (RWTH Aachen), Sanil Chawla (UIUC), Qirui Jin (University of Michigan), Suhas Kotha (CMU), and Zhuolin Yang (UIUC) who were involved into the development of the verifier during 2023 - 2024 but did not directly work on any benchmarks of the competition.\nDescription \u03b1,\u03b2-CROWN (alpha-beta-CROWN) is an efficient neural network verifier based on the linear bound propagation framework and built on a series of works on bound-propagation-based neural network verifiers: CROWN [87], auto_LiRPA [83], \u03b1-CROWN [84], \u03b2-CROWN [74], GCP-CROWN [86], GenBaB [58], BICCOS [90]. The core techniques in a,\u1e9e-CROWN combine the efficient and GPU-accelerated linear bound propagation method with branch-and-bound methods specialized for neural network verification.\nThe linear bound propagation algorithms in \u03b1,\u03b2-CROWN are based on our auto_LiRPA library [83], which supports general neural network architectures (including convolutional layers, pooling layers, residual connections, recurrent neural networks, and Transformers) and a wide range of nonlinear functions (e.g., ReLU, tanh, trigonometric functions, sigmoid, max pooling and average pooling), and is efficiently implemented on GPUs with Pytorch and CUDA.\nWe jointly optimize intermediate layer bounds and final layer bounds using gradient ascent"}, {"title": "3.2 CORA", "content": "Team Lukas Koller, Tobias Ladner, Matthias Althoff (Technical University of Munich)\nDescription CORA [7] enables the formal verification of neural networks, both in open-loop as well as in closed-loop scenarios. Open-loop verification refers to the task where properties of the output set of a neural network are verified, e.g. correctly classified images given noisy input, as also considered at VNN-COMP. In closed-loop scenarios, the neural network is used as a controller of a dynamic system, e.g., controlling a car while keeping a safe distance over some time horizon.\nThis is realized using reachability analysis, mainly using polynomial zonotopes [43, 47], allowing a non-convex enclosure of the output set of a neural network. Moreover, CORA can also train robust neural networks [44], which requires an efficient batch-wise propagation of zonotopes through a neural network on a GPU. This can also be used during verification by efficiently propagating the splitted sets batch-wise."}, {"title": "3.3 Marabou", "content": "Team Haoze Wu (Stanford University), Clark Barrett (Stanford University), Guy Katz (He-brew University of Jerusalem)\nDescription Marabou [38, 79] is a user-friendly Neural Network Verification toolkit that can answer queries about a network's properties by encoding and solving these queries as constraint satisfaction problems. It has both Python/C++ APIs through which users can load neural networks and define arbitrary linear properties over the neural network. Marabou supports many different linear, piecewise-linear, and non-linear [81, 76] operations and architectures (e.g., FFNNs, CNNs, residual connections, Graph Neural Networks [78]).\nUnder the hood, Marabou employs a uniform solving strategy for a given verification query. In particular, Marabou performs complete analysis that employs a specialized convex optimization procedure [82] and abstract interpretation [63, 78]. It also uses the Split-and-Conquer algorithm [80] for parallelization."}, {"title": "3.4 NeVer2", "content": "Team Setefano Demarchi, Armando Tacchella (University of Genova), Elena Botoeva (University of Kent)\nDescription NeVer2 [23] is an open-source, cross-platform tool aimed at designing, training, and verifying neural networks. It seamlessly integrates popular learning libraries with our verification backend, offering their functionalities also via a graphical interface.\nNeVer2 relies on the pyNever [33] Python API, which provides the verification capability employing an abstraction-refinement algorithm, which uses symbolic bounds propagation to compute stable and unstable neurons and an iterative refinement procedure to grow a search tree for proving the safety or the unsafety of a verification query, which is expressed using star sets [70].\nThe algorithm propagates the abstraction provided by symbolic propagation and, if there is an intersection with the unsafe post-conditions, checks whether there is a counter-example to state the unsafety of the network, or to refine the abstraction branching on unstable ReLU neurons. The next refinement target is decided based on the presence of unstable neurons in early layers, or by the approximation area of the linear relaxation. In the form it is presented, this behaves as a complete algorithm. However, it can be easily turned into an incomplete one by incorporating some early stopping criteria, e.g., a timeout, or the maximum depth/number of refined neurons in a branch. Currently, NeVer2 supports only feed-forward architectures with ReLU layers."}, {"title": "3.5\nnnenum", "content": "Team Ali Arjomandbigdeli (Student), Stanley Bak (Supervisor) (Stony Brook University)\nDescription The nnenum tool [10] uses multiple levels of abstraction to achieve high-performance verification of ReLU networks without sacrificing completeness [9]. The core verification method is based on reachability analysis using star sets [70], combined with the ImageStar method [67] to propagate stes through all linear layers supported by the ONNX runtime, such as convolutional layers with arbitrary parameters. The tool is written in Python 3 and uses GLPK for LP solving. New this year, we added support for single lower and single upper bounds propagation in addition to zonotopes, similar to the DeepPoly method or the CROWN approach. We also added an option to use Gurobi instead of GLPK for LP solving."}, {"title": "3.6 NNV", "content": "Team Diego Manzanas Lopez (Vanderbilt University), Samuel Sasaki (Vanderbilt University), Taylor T. Johnson (Vanderbilt University)\nDescription The Neural Network Verification (NNV) Tool [72, 49] is a formal verification software tool for deep learning models and cyber-physical systems with neural network components written in MATLAB and available at https://github.com/verivital/nnv. NNV uses a star-set state-space representation and reachability algorithm that allows for a layer-by-layer computation of exact or overapproximate reachable sets for feed-forward [70], convolutional [67], semantic segmentation (SSNN) [71], and recurrent (RNN) [69] neural networks, as well as neural network control systems (NNCS) [68, 72] and neural ordinary differential equations (Neural ODEs) [52]. The star-set based algorithm is naturally parallelizable, which allows NNV to be designed to perform efficiently on multi-core platforms. Additionally, if a particular safety property is violated, NNV can be used to construct and visualize the complete set of counterexample inputs for a neural network (exact-analysis). For this competition, updated from last year's, we tailor the solver approach depending on the benchmark at hand, although all follow a similar flow. First, we perform a simulation-guided search for counterexamples for a fixed number of samples. If no counterexamples are found (i.e., demonstrate that the property is SAT), then we utilize an iterative refinement approach using reachability analysis to verify the property (UNSAT). This consists of performing reachability analysis using a relax-approximation method [71], if not verified, then a less conservative approximation based on zonotope pre-filtering approach [66], and finally using the exact analysis when possible [67] until the specification is verified or there is a timeout. Based on the benchmark to evaluate, the initial reachability analysis may be any of the overapproximation methods or the exact method, based on the complexity of the benchmarks (size of network, input, etc)."}, {"title": "3.7 NeuralSAT", "content": "Team Hai Duong and Thanhvu Nguyen (George Mason).\nDescription NeuralSAT [26, 27] integrates conflict-driven clause learning (CDCL) in SAT/SMT-solving with an DNN abstraction-based theory solver for infeasibility checking. The figure on the right gives an overview of NeuralSAT, which implements the DPLL(T) framework used in modern SMT solvers such as Z3 and CVC. The design of NeuralSAT is inspired by the core algorithms used in SMT solvers such as CDCL components (light shades) and theory solving (dark shade). The tool is written in Python and uses Gurobi for LP solving. Unlike many other modern VNN verification tools, Neural-SAT does not require parameter tuning and works out of the box, e.g., the tool runs on the wide-range of benchmarks in VNN-COMPs without any tuning.\nNew: In VNN-COMP'24, NeuralSAT has been improved with neuron stability, parallel beam search, and restarting strategies, which improved its performance significantly [27]."}, {"title": "3.8 PyRAT", "content": "Team Augustin Lemesle, Julien Lehmann, Tristan Le Gall (CEA-List)\nDescription PyRAT (Python Reachability Assessment Tool) [48] is an abstract interpretation based tool to verify the safety and robustness of neural networks. PyRAT implements several abstract domains such as Intervals, Zonotopes, Constrained Zonotopes, or Polyhedras to efficiently compute the reachable states for different architectures of neural networks such as dense, convolutional, residual or recurrent neural networks. It supports multiple non-linear activation functions (ReLU, Sigmoid, Softmax, Floor, ...) with precise abstractions while maintaining floating-point soudness. PyRAT is correct in that the output bounds reached will always be a sound over-approximation of the results and complete for ReLU based networks in that it will always give a true or false result given enough time. Depending on the benchmark, different verification modes and domains can be selected. For smaller networks and problems, PyRAT can leverage branch and bound strategies on the inputs with heuristics like ReCIPH [28]. While on larger ReLU networks, PyRAT will use branch and bound strategies on the ReLU neurons in conjunction with fast GPU computation."}, {"title": "4 Benchmarks", "content": "In this section, we provide an overview of all scored benchmarks, reproducing the benchmark proposers' descriptions. Artifacts for all benchmarks are available in the repository6.\n4.1 CGAN\nProposed by Feiyang Cai, Ali Arjomandbigdeli, Stanley Bak (Stony Brook University)\nMotivation While existing neural network verification benchmarks focus on discriminative models, the exploration of practical and widely used generative networks remains neglected in terms of robustness assessment. This benchmark introduces a set of image generation networks specifically designed for verifying the robustness of the generative networks.\nNetworks The generative networks are trained using conditional generative adversarial networks (CGAN), whose objective is to generate camera images that contain a vehicle obstacle located at a specific distance in front of the ego vehicle, where the distance is controlled by the input distance condition. The network to be verified is the concatenation of a generator and a discriminator. The generator takes two inputs: 1) a distance condition (1D scalar) and 2) a noise vector controlling the environment (4D vector). The output of the generator is the generated image. The discriminator takes the generated image as input and outputs two values: 1) a real/fake score (1D scalar) and 2) a predicted distance (1D scalar). Several different models with varying architectures (CNN and vision transformer) and image sizes (32x32, 64x64) are provided for different difficulty levels.\nSpecifications The verification task is to check whether the generated image aligns with the input distance condition, or in other words, verify whether the input distance condition matches the predicted distance of the generated image. In each specification, the inputs (condition distance and latent variables) are constrained in small ranges, and the output is the predicted distance with the same center as the condition distance but with slightly larger range."}, {"title": "4.2 NN4Sys", "content": "Proposed by the \u03b1,\u03b2-CROWN team with collaborations with Cheng Tan, Haoyu He and Shuyi Lin at Northeastern University.\nApplication The benchmark contains networks for database learned index, video streaming learned adaptive bitrate, and learned cardinality estimation which map inputs from various dimensions to 1-dimension outputs.\n\u2022 Background: learned index, learned cardinality, and learned adaptive bitrate are all instances in neural networks for computer systems (NN4Sys), which are neural network based methods performing system operations. These classes of methods show great potential but have one drawback\u2014the outputs of an NN4Sys model (a neural network) can be arbitrary, which may lead to unexpected issues in systems.\n\u2022 What to verify: our benchmark provides multiple pairs of (1) trained NN4Sys model and (2) corresponding specifications. We design these pairs with different parameters such that they cover a variety of user needs and have varied difficulties for verifiers. We describe benchmark details in our NN4SysBench report: http://naizhengtan.github. io/doc/papers/nn4sys23lin.pdf.\n\u2022 Translating NN4Sys applications to a VNN benchmark: the original NN4Sys applications have some sophisticated structures that are hard to verify. We tailored the neural networks and their specifications to be suitable for VNN-COMP. For example, learned index [46] contains multiple NNs in a tree structure that together serve one purpose. However, this cascading structure is inconvenient/unsupported to verify because there is a \"switch\" operation-choosing one NN in the second stage based on the prediction of the first stage's NN. To convert learned indexes to a standard form, we train a monolithic (larger) NN.\n\u2022 A note on broader impact: using NNs for systems is a broad topic, but many existing works lack strict safety guarantees. We believe that NN Verification can help system developers gain confidence to apply NNs to critical systems. We hope our benchmark can be an early step toward this vision.\nNetworks This benchmark has twelve networks with different parameters: two for learned indexes, four for learned cardinality estimation and six for learned adaptive bitrate. The learned index uses fully-connected feed-forward neural networks. The other two-the learned cardinality and the learned adaptive bitrate has a relatively sophisticated internal structure. Please see our NN4SysBench report (URL listed above) for details\nSpecifications For learned indexes, the specification aims to check if the prediction error is bounded. The specification is a collection of pairs of input and output intervals such that any input in the input interval should be mapped to the corresponding output interval. For learned cardinality estimation and learned adaptive bitrate, the specifications check the prediction error bounds (similar to the learned indexes) and monotonicity of the networks. By monotonicity specifications, we mean that for two inputs, the network should produce a larger output for the larger input, which is required by cardinality estimation or adaptive bitrate."}, {"title": "4.3 LinearizeNN", "content": "Proposed by Ali Arjomandbigdeli, Stanley Bak (Stony Brook University).\nMotivation Assuming having a neural network controller approximation with a piecewise linear model in the form of a set of linear models with added noise to account for local linearization error. The objective of this benchmark is to investigate the neural network output falls within the range we obtain from our linear model output plus some uncertainty.\nThe idea of this benchmark came from one of our recent paper [8] in which we approximated the NN controller with a piecewise linear model, and we wanted to check if the neural network output falls within the range we obtained from our linear model output plus some uncertainty.\nNetworks The neural network controller we used in this benchmark is an image-based controller for an Autonomous Aircraft Taxiing System whose goal is to control an aircraft's taxiing at a steady speed on a taxiway. This network was introduced in the paper \"Verification of Image-based Neural Network Controllers Using Generative Models\" [39]. The neural network integrates a concatenation of the cGAN (conditional GAN) and controller, resulting in a unified neural network controller with low-dimensional state inputs. In this problem, the inputs to the neural network consist of two state variables and two latent variables. The aircraft's state is determined by its crosstrack position (p) and heading angle error (\u03b8) with respect to the taxiway center line. Two latent variables with a range of -0.8 to 0.8 are introduced to account for environmental changes.\nBecause in this case the output spec depends on both the input and output and considering the VNN-LIB limitation, we added a skip-connection layer to the neural network to have the input values present in the output space. We also added one linear layer after that to create a linear equation for each local model.\nSpecifications As mentioned earlier, the aim of this benchmark is to examine whether the neural network output stays within the range defined by the linear model's output, including a margin for uncertainty.Given input x \u2208 X and output Y = fNN(x), the query is of the form: Amat \u00d7 X + b + Uu < Y < Amat \u00d7 X + b + Uub for each linear model in its abstraction region."}, {"title": "4.4 ml4acopf", "content": "Proposed by Haoruo Zhao, Michael Klamkin, Mathieu Tanneau, Wenbo Chen, and Pascal Van Hentenryck (Georgia Institute of Technology), and Hassan Hijazi, Juston Moore, and Haydn Jones (Los Alamos National Laboratory).\nMotivation Machine learning models are utilized to predict solutions for an optimization model known as AC Optimal Power Flow (ACOPF) in the power system. Since the solutions are continuous, a regression model is employed. The objective is to evaluate the quality of these machine learning model predictions, specifically by determining whether they satisfy the constraints of the optimization model. Given the challenges in meeting some constraints, the goal is to verify whether the worst-case violations of these constraints are within an acceptable tolerance level.\nNetworks The neural network designed comprises two components. The first component predicts the solutions of the optimization model, while the second evaluates the violation of each constraint that needs checking. The first component consists solely of general matrix multiplication (GEMM) and rectified linear unit (ReLU) operators. However, the second component has a more complex structure, as it involves evaluating the violation of AC constraints using nonlinear functions, including sigmoid, quadratic, and trigonometric functions such as sine and cosine. This complex evaluation component is incorporated into the network due to a limitation of the VNNLIB format, which does not support trigonometric functions. Therefore, these constraints violation evaluation are included in the neural network.\nSpecifications In this benchmark, four different properties are checked, each corresponding to a type of constraint violation:\n1. Power balance constraints: the net power at each bus node is equal to the sum of the power flows in the branches connected to that node.\n2. Thermal limit constraints: power flow on a transmission line is within its maximum and minimum limits.\n3. Generation bounds: a generator's active and reactive power output is within its maximum and minimum limits.\n4. Voltage magnitude bounds: a voltage's magnitude output is within its maximum and minimum limits.\nThe input to the model is the active and reactive load. The chosen input point for perturbation is a load profile for which a corresponding feasible solution to the ACOPF problem is known to exist. For the feasibility check, the input load undergoes perturbation. Although this perturbation does not exactly match physical laws, the objective is to ascertain whether a machine learning-predicted solution with the perturbation can produce a solution that does not significantly violate the constraints.\nThe scale of the perturbation and the violation threshold are altered by testing whether an adversarial example can be easily found using projected gradient descent with the given perturbation. The benchmark, provided with a fixed random seed, is robust against the simple projected gradient descent that is implemented."}, {"title": "4.5 ViT", "content": "Proposed by the \u03b1,\u03b2-CROWN team.\nMotivation Transformers [73] based on the self-attention mechanism have much more complicated architectures and contain more kinds of nonlinerities, compared to simple feedforward networks with relatively simple activation functions. It makes verifying Transformers challenging. We aim to encourage the development of verification techniques for Transformer-based models, and we also aim to benchmark neural network verifiers on relatively complicated neural network architectures and more general nonlinearities. Therefore, we propose a new benchmark with Vision Transformers (ViTs) [24]. This benchmark is developed based on our work on neural network verification for models with general nonlinearities [58].\nNetworks The benchmark contains two ViTs, as shown in Table 4. Considering the difficulty of verifying ViTs, we modify the ViTs and make the models relatively shallow and narrow, with significantly reduced number of layers and attention heads. Following [60], we also replace the layer normalization with batch normalization. The models are mainly trained with PGD training [50], and we also add a weighted IBP [31, 59] loss for one of the models as a regularization."}, {"title": "4.6 LSNC", "content": "Proposed by the \u03b1,\u03b2-CROWN team.\nMotivation We develop a benchmark for the problem of verifying the Lyapunov stability of NN controllers in nonlinear dynamical systems within a region-of-intrest and a region-of-attraction. This is important for providing stability guarantees that are essential for safety-critical applications with NN controllers. It is also a useful application of neural network verification as recently demonstrated in [85, 57], and we refer readers to those works for more details on the problem.\nNetworks and Specifications Models are adopted from [85]. We adopt two models for the 2D quadrotor dynamical system with state feedback and output feedback, respectively. Each"}, {"title": "4.7 Collins-RUL-CNN", "content": "Proposed by Collins Aerospace, Applied Research & Technology (website).\nMotivation Machine Learning (ML) is a disruptive technology for the aviation industry. This particularly concerns safety-critical aircraft functions, where high-assurance design and verification methods have to be used in order to obtain approval from certification authorities for the new ML-based products. Assessment of correctness and robustness of trained models, such as neural networks, is a crucial step for demonstrating the absence of unintended functionalities [29, 41]. The key motivation for providing this benchmark is to strengthen the interaction between the VNN community and the aerospace industry by providing a realistic use case for neural networks in future avionics systems [40].\nApplication Remaining Useful Life (RUL) is a widely used metric in Prognostics and Health Management (PHM) that manifests the remaining lifetime of a component (e.g., mechanical bearing, hydraulic pump, aircraft engine). RUL is used for Condition-Based Maintenance (CBM) to support aircraft maintenance and flight preparation. It contributes to such tasks as augmented manual inspection of components and scheduling of maintenance cycles for components, such as repair or replacement, thus moving from preventive maintenance to predictive maintenance (do maintenance only when needed, based on component's current condition and estimated future condition). This could allow to eliminate or extend service operations and inspection periods, optimize component servicing (e.g., lubricant replacement), generate inspection and maintenance schedules, and obtain significant cost savings. Finally, RUL function can also be used in airborne (in-flight) applications to dynamically inform pilots on the health state of aircraft components during flight. Multivariate time series data is often used as RUL function input, for example, measurements from a set of sensors monitoring the component state, taken at several subsequent time steps (within a time window). Additional inputs may include information about the current flight phase, mission, and environment. Such highly multi-dimensional input space motivates the use of Deep Learning (DL) solutions with their capabilities of performing automatic feature extraction from raw data.\nNetworks The benchmark includes 3 convolutional neural networks (CNNs) of different complexity: different numbers of filters and different sizes of the input space. All networks contain only convolutional and fully connected layers with ReLU activations. All CNNs perform the regression function. They have been trained on the same dataset (time series data for mechanical component degradation during flight).\nSpecifications We propose 3 properties for the NN-based RUL estimation function. First, two properties (robustness and monotonicity) are local, i.e., defined around a given point. We provide a script with an adjustable random seed that can generate these properties around input"}, {"title": "4.8 VGGNET16", "content": "Proposed by Stanley Bak, Stony Brook University\nMotivation This benchmark tries to scale up the size of networks being analyzed by using the well-studied VGGNET-16 architecture [62] that runs on ImageNet. Input-output properties are proposed on pixel-level perturbations that can lead to image misclassification.\nNetworks All properties are run on the same network, which includes 138 million parameters. The network features convolution layers, ReLU activation functions, as well as max pooling layers.\nSpecifications Properties analyzed ranged from single-pixel perturbations to perturbations on all 150528 pixles (L-infinity perturbations). A subset of the images was used to create the specifications, one from each category, which was randomly chosen to attack. Pixels to perturb were also randomly selected according to a random seed."}, {"title": "4.9 Traffic Signs Recognition", "content": "Proposed by M\u0103d\u0103lina Era\u015fcu and Andreea Postovan (West University of Timisoara, Romania)\nMotivation Traffic signs play a crucial role in ensuring road safety and managing traffic flow in both city and highway driving. The recognition of these signs, a vital component of autonomous driving vision systems, faces challenges such as susceptibility to adversarial examples [64] and occlusions [88], stemming from diverse traffic scene conditions.\nNetworks Binary neural networks (BNNs) show promise in computationally limited and energy-constrained environments within the realm of autonomous driving [35]. BNNs, where weights and/or activations are binarized to \u00b11, offer reduced model size and simplified convolution operations for image recognition compared to traditional neural networks (NNs).\nWe trained and tested various BNN architectures using the German Traffic Sign Recognition Benchmark (GTSRB) dataset [4]. This multi-class dataset, containing images of German road signs across 43 classes, poses challenges for both humans and models due to factors like perspective change, shade, color degradation, and lighting conditions. The dataset was also tested using the Belgian Traffic Signs [1] and Chinese Traffic Signs [2] datasets. The Belgium Traffic Signs dataset, with 62 classes, had 23 overlapping classes with GTSRB. The Chinese Traffic Signs dataset, with 58 classes, shared 15 classes with GTSRB. Pre-processing steps involved relabeling classes in the Belgium and Chinese datasets to match those in GTSRB and eliminating non-overlapping classes (see [55] for details).\nWe provide three models with the structure in Figures 1, 2, and 3. They contain QConv, Batch Normalization (BN), Max Pooling (ML), Fully Connected/Dense (D) layers. Note that"}, {"title": "4.10 CIFAR100", "content": "Proposed by the \u03b1,\u03b2-CROWN team.\nMotivation This benchmark is reused from VNN-COMP 2022 with a reduced complexity (only two out of the four models with medium sizes are retained). See details in Section 4.5 of the report of VNN-COMP 2022 [54].\nNetworks We provide two ResNet models on CIFAR-100 with different model widths and depths (input dimension 32 \u00d7 32 \u00d7 3, 100 classes):\n\u2022 CIFAR100-ResNet-medium: 8 residual blocks, 17 convolutional layers + 2 linear layers\n\u2022 CIFAR100-ResNet-large: 8 residual blocks, 19 convolutional layers + 2 linear layers (almost identical to standard ResNet-18 architecture)"}, {"title": "4.11 TinyImagenet", "content": "Proposed by the \u03b1,\u03b2-CROWN team.\nMotivation This benchmark is reused from VNN-COMP 2022. See details in Section 4.5 of the report of VNN-COMP 2022 [54].\nNetworks We provide a ResNet for TinyImageNet (input dimension 64 \u00d7 64 \u00d7 3, 200 classes):\n\u2022 TinyImageNet-ResNet-medium: 8 residual blocks, 17 convolutional layers + 2 linear layers\nSpecifications We randomly select 200 images from the TinyImageNet test set with a verification timeout of 100 seconds for each of the two models. A filtering procedure has been adopted similar to the CIFAR100 benchmark."}, {"title": "4.12 TLL Verify Bench", "content": "Proposed by James Ferlez (University of California, Irvine)\nMotivation This benchmark consists of Two-Level Lattice (TLL) NNs, which have been shown to be amenable to fast verification algorithms (e.g. [30]). Thus, this benchmark was proposed as a means of comparing TLL-specific verification algorithms with general-purpose NN verification algorithms (i.e. algorithms that can verify arbitrary deep, fully-connected ReLU NNs).\nNetworks The networks in this benchmark are a subset of the ones used in [30, Experiment 3]. Each of these TLL NNs has n = 2 inputs and m = 1 output. The architecture of a TLL NN is further specified by two parameters: N, the number of local linear functions, and M, the number of selector sets. This benchmark contains TLLs of sizes N = M = 8, 16, 24, 32, 40, 48, 56, 64, with 30 randomly generated examples of each (the generation procedure is described in [30, Section 6.1.1]). At runtime, the specified verification timeout determines how many of these networks are included in the benchmark so as to achieve an overall 6-hour run time; this selection process is deterministic. Finally, a TLL NN has a natural representation using multiple computation paths [30, Figure 1], but many tools are only compatible with fully-connected networks. Hence, the ONNX models in this benchmark implement TLL NNs by \"stacking\" these computation paths to make a fully connected NN (leading to sparse weight matrices: i.e. with many zero weights and biases). The TLLnet class (https://github.com/jferlez/TLLnet) contains the code necessary to generate these implementations via the exportONNX method."}, {"title": "4.13 ACAS Xu", "content": "Networks The ACASXu benchmark consists of ten properties defined over 45 neural networks used to issue turn advisories to aircraft to avoid collisions. The neural networks have 300 neurons arranged in 6 layers, with ReLU activation functions. There are five inputs corresponding to the aircraft states, and five network outputs, where the minimum output is used as the turn advisory the system ultimately produces.\nSpecifications We use the original 10 properties [37"}]}