{"title": "MonoSOWA: Scalable monocular 3D Object detector Without human Annotations", "authors": ["Jan Skvrna", "Lukas Neumann"], "abstract": "Detecting the three-dimensional position and orien-\ntation of objects using a single RGB camera is a foundational\ntask in computer vision with many important applications.\nTraditionally, 3D object detection methods are trained in a fully-\nsupervised setup, requiring vast amounts of human annotations,\nwhich are laborious, costly, and do not scale well with the ever-\nincreasing amounts of data being captured.\nIn this paper, we present the first method to train 3D object\ndetectors for monocular RGB cameras without domain-specific\nhuman annotations, thus making orders of magnitude more\ndata available for training. Thanks to newly proposed Canonical\nObject Space, the method can not only exploit data across a\nvariety of datasets and camera setups to train a single 3D\ndetector, but unlike previous work it also works out of the\nbox in previously unseen camera setups. All this is crucial for\npractical applications, where the data and cameras are extremely\nheterogeneous.\nThe method is evaluated on two standard autonomous driving\ndatasets, where it outperforms previous works, which, unlike our\nmethod, still rely on 2D human annotations. The source code and\nthe model is going to be published soon.", "sections": [{"title": "I. INTRODUCTION", "content": "Monocular 3D object detection is a key component of\nmany important systems, ranging from robotics to autonomous\ncars. 3D object detection methods are typically trained in\na fully-supervised setup, requiring vast amounts of human\nannotations, which are laborious, costly, and do not scale well\nwith ever-increasing amounts of data that ideally should be\nused for training. To make matters even worse, especially\nin autonomous driving, new cameras and new camera setups\nemerge constantly, which typically implies the need to collect,\nlabel and train on a new dataset as camera parameters such as\nfocal length change.\nIn this paper, we address both of these challenges \u2013 costly\nhuman labels and changing camera setup \u2013 by introducing a\nnew method to train a 3D rigid object detector in a Canoni-\ncal Object Space without domain-specific human labels. Our\nmethod allows us to exploit large amounts of training data\nreadily available because all we need as input for training are\nunlabelled driving video sequences. Furthermore, our method\ndoes not require data from additional sensors such as LiDAR.\nIt makes our method extremely applicable in practice because\nas such our method can directly exploit data collected by a\nmajority of currently manufactured vehicles, which typically\nonly have a single camera. This is in contrast with some\nmethods which require LiDAR (and human annotations) [1],\n[2], because for them specialised vehicles are needed to collect\nthe data, and this severely limits the variety of geographies,\nenvironments, and traffic situations being captured.\nDrawing inspiration from monocular depth literature [3],\nthe proposed Canonical Object Space (COS) allows us to\naggregate and learn from data coming from different cameras,\nwhich is a crucial pre-condition to being able to effectively\nharvest otherwise very heterogeneous video sequences. More-\nover, unlike previous methods such as VSRD [4], it also allows\nfor the deployment of our 3D object detector in new unseen\ncamera setup without needing to train or fine-tune for the new\nsetup. This is again extremely useful in practice, because new"}, {"title": "II. RELATED WORK", "content": "a) Monocular Depth Estimation: Monocular depth esti-\nmation has shown significant improvements in recent years.\nMiDAS [5] employs affine-invariant losses to enable training\non multiple datasets. ZoeDepth [6] combines training on\nmultiple datasets with relative losses and further fine-tunes\non the target metric dataset. Depth Anything [7] focuses\non the affine-invariant depth estimation using self-supervised\npre-training. For metric depth estimation, which is relevant\nfor object detection, it fine-tunes on the target dataset such\nas KITTI [8]. Metric3D [3], [9] focuses on metric depth\nestimation as it uses a Canonical Camera Transformation\nModule to further improve zero-shot performance, together\nwith DINOv2 [10] or ConvNext [11] as the backbone and\nDPT [12] as the predictor. CamConv [13] models cameras\ninside the network, but as shown in [3], this leads to poor\nresults.\nb) Fully-supervised Monocular 3D Object Detection:\nEarly methods [14]\u2013[16] were based on lifting the image\ninto pseudo-LiDAR and then detecting objects in the gener-\nated point cloud. SMOKE [17], on the other hand, directly\npredicts 3D bounding boxes from images. MonoDTR [18]\nand MonoDETR [19] use an end-to-end depth-aware/guided\ntransformer [20] architecture to directly predict 3D bounding\nboxes. SSD-MonoDETR [21] introduces scale awareness into\nthe model. MonoATT [22] variably assigns tokens to areas\nof higher importance. MonoFlex [23] uses an ensemble of\ndirect depth and keypoint-based predictions and decouples the\ndetection of truncated and untruncated vehicles. MonoCD [24]\nenhances the ensemble with a complementary depth predictor.\nHowever, all the aforementioned methods require 3D human\nannotations during their training.\nc) Weakly-supervised 3D Object Detection: Most previ-\nous works on weakly-supervised 3D detection have focused\non training 3D detectors for LiDAR. VS3D [25] pioneered in\nthis area by using point density in LiDAR scans to identify\nobjects, and detections are further refined by a student-teacher\nnetwork, where the teacher is trained on images only. Zakharov\net al. [2] employs a 2D off-the-shelf detector accompanied\nby a novel Signed Distance Fields (SDF)-renderer within the\nDeepSDF framework [26] and Normalized Object Coordinate\nSpace (NOCS). The method is trained on a synthetic dataset\nand further fine-tuned on real datasets.\nWS3D [27], [28] employs Bird's Eye View (BEV) human-\ncentered clicks as weak supervision to train a network for\ndetecting vehicles, arguing that these are much easier to obtain\nthan 3D bounding boxes. Furthermore, they use a small subset\nof human ground-truth annotations to train a second network\nto predict the spatial dimensions of the detected vehicles.\nMcCraith et al. [29] employ direct fitting of generic templates\ninto LiDAR scans segmented by Mask-RCNN [30] and share\nthe information between frames by requiring consistency be-\ntween frames while simultaneously discounting outliers. TCC-\ndet [31] uses direct fitting of generic templates on aggregated\npoints corresponding to each instance. During the training of\nthe 3D detector, TCC-det appends two additional losses for\nfiner alignment. VG-W3D [32] encodes the image and LiDAR\nscan separately and then enforces alignment between 2D and\n3D on multiple feature levels to predict precise 3D bounding\nboxes. In contrast to the aforementioned methods, VSRD [4]\nfocuses on 3D detection in camera, and does not require\nLiDAR scans. However, it uses 2D human ground-truth masks\nfor each instance, representing them as a surface in Signed\nDistance Fields (SDF). Furthermore, the SDF are rendered\ninto masks through their proposed volumetric-aware silhouette\nrendering, enabling end-to-end training. Each instance is op-\ntimized over multiple frames. While VSRD shows promising\nresults in both weakly-supervised and semi-supervised train-\ning, it requires 2D instance masks, and rendering of each\nframe takes approximately 15 minutes. In contrast, our method\ndoes not require any human annotations at all and is\napproximately 100 times faster."}, {"title": "III. METHOD", "content": "Traditionally, deep networks that predict 3D bounding boxes\nof vehicles (and other agents) from a single image are trained\nusing 3D annotations, which are manually created by human\nannotators. They typically rely on both camera image and\nLiDAR data for correct distance estimate in creating their\nlabels. However, this process is very laborious and costly,\nlimiting the amount of training data available to train such\nmethods. Moreover, it requires capturing the scene using both\na camera and a LiDAR sensor, which is a significant limitation\nsince the majority of production vehicles these days do not\nhave LiDAR on board \u2013 as a result, data collection is only\nlimited to vehicles dedicated to such a purpose, and this again\nlimits the variety and diversity of data being used to train these\nmethods."}, {"title": "A. Auto-labelling pipeline", "content": "The process of automatic label creation assumes that images\nare captured in a sequence, which is a natural way to capture\ndata in driving scenarios. It also assumes that the camera setup\nis known (both intrinsics and extrinsics) for each sequence.\nHowever, the camera setup might be and actually often is\ndifferent (see section III-B). Lastly, it assumes approximate\ndata about ego-vehicle's motion is available (e.g. from GPS\nor an Inertial Measurement Unit\nIMU), which again is\ncommonplace.\nBecause labelling a 7-DOF pose of objects jointly is a\nvery challenging problem, we instead infer individual pose\nparameters separately in a sequence of steps.\na) Detecting objects in pseudo-LiDAR:\nThe auto-\nlabelling process begins by inferring a metric depth map\n$D\\in R^{h\\times w}$ for each camera image $I \\in R^{3\\times h\\times w}$, where h\nand w denotes image height and width, using an off-the-shelf\nmonocular depth estimator. We opted to use Metric3D [3],\nas it is zero-shot, it does not limit our method in terms of\ndata distribution and has shown great results for generalization.\nThe choice of a particular monocular metric depth estimator\nis, however, not crucial. For each pixel and its corresponding\ndepth, a 3D point cloud is generated as\n$\\begin{aligned}\nX_{u,v} &= \\frac{D_{u,v} \\cdot (u - c_x)}{f_x}\\\\\nY_{u,v} &= \\frac{D_{u,v} \\cdot (v \u2013 c_y)}{f_y}\\\\\nZ_{u,v} &= D_{u,v}\n\\end{aligned}$ (1)\nwhere $u \\in [0, h]$ and $v \\in [0, w]$, c is the principal point and f\nis thefocal length. We denote the pseudo lidar point cloud as\n$P_i\\in R^{3\\times h\\times w}$, where i stands for the frame index.\nNext, inspired by [29], we employ an off-the-shelf 2D object\nimage detector to detect the objects we are interested in (ve-\nhicles) and their instance segmentation masks in each camera\nimage. Given an instance segmentation mask $M_{i,j} \\in R^{h\\times w}$\nof an object j in a frame i, we find object point cloud $P_{i,j}$ in\nthe point cloud as\n$P_{i,j} = \\{p \\in P_i | T(p) \\in M_{i,j}\\}$ (2)\nwhere T(p) denotes the projection of the 3D point p into\nimage coordinates (using camera calibration, which is known).\nThe $P_{i,j}$ tends to contain outliers, as either the masks\nor the metric depth estimation is not perfect. To remove\nthe outliers, an ensemble of the following methods is em-\nployed: HBOS [33], Z-scores, statistical outlier rejection in\nOpen3D [34], HDBSCAN [35] and DBSCAN [36]. If at least\ntwo of these methods classify a point as an outlier, it is\nremoved. To compensate for the effect of the ego-vehicle\nmovement, we transform all $P_{i,j}$ into the reference frame\ncoordinate space, given the poses from IMU data."}, {"title": "b) Temporal consistency:", "content": "In order to exploit temporal\nconsistency across multiple frames of the sequence, we need to\nfirst establish correspondences of individual object instances\nbetween frames. Because we have inferred approximate 3D\npoint cloud of each object instance $P_{i,j}$ in each frame, we\ntrack each object in 3D world coordinate system.\nFor each object j in frame i, we approximate its location\n$L_{i,j}$ as the median of its point cloud $P_{i,j}$. The tracking is\ninitialized in n frames before the reference (current) frame, and\nfor each frame, it matches the instances based on locations in\nthe current frame $L_{i,j}$ and the predicted future locations from\nthe last frame into the current frame $L_{i,j}$. For the location\nprediction $L_{i,j}$, a simple physics-aware motion model is used\n$L_{i,j} = L_{i-1,j} + \\frac{1}{3} \\sum_{k=1}^{3} L_{i-k,j} - L_{i-k-1,j}$ (3)\nTo match two object instances between frames, it is required\nthat the instances are both their nearest neighbours and that\nthe distance between them is lower than a set threshold $T_{dist}$.\nOtherwise, the instance is considered as a new object. After\nthis step, each object instance is represented by extracted\npoints $P_{i,j}$ and locations $L_{i,j}$ in each frame the instance is\npresent. The tracking sequentially processes all the frames up\nto n after the reference frame."}, {"title": "c) Movement Classification:", "content": "The next step is to classify\ninstances as either stationary or moving, as different principles\nof temporal consistency exploitation are applied to those\nclasses. Note that all object instances actually appear moving\n\u2013 they change location in relative terms, as the ego-vehicle\nitself is driving while data is captured.\nIn order to classify a vehicle as stationary or moving,\nwe propose the following novel classification procedure. For\neach instance, all locations $L_{m...n,j}$ are taken and differences\nbetween all adjacent locations are computed, where m is the\nfirst frame, where the instance is present and n is the last.\nFrom those differences, the standard deviation $o$ is calculated\nas\n$\\begin{aligned}\n\u03c3_j &= \\sqrt{\\frac{1}{2}\\frac{1}{V}}\\cdot \\sqrt{\\frac{1}{n-m}\\sum_{i=m+1}^n (\\mu_j - D_{i,j})^2}\\\\\n\\mu_j &= \\frac{1}{n-m}\\sum_{i=m+1}^n D_{i,j}\\\\\n\\end{aligned}$ (4)\n$D_{i,j} = L_{i,j} - L_{i-1,j}$ (5)\nFurthermore, the actual distance d and expected distance \u00e2\ncaused by the $o$ is calculated as\n$d_j = ||L_{m,j} - L_{n,j}||_2  d_j = (n - m) \\cdot ||o_j||_2$ (6)\nThe z- and p-scores are calculated to know if the difference\nbetween the $d_{real}$ and $d_{exp}$ is statistically significant or not\n$z_j = \\frac{d_j - \\hat d_j}{\u03c3_j}$   $p_j = 1 - \u03a6(z_j)$ (7)\nTo classify the instance as moving, the p-score is required\nto be lower than the threshold $T_m$ and simultaneously, the d\ndistance must be higher than a given threshold.\nAs all $P_{i,j}$ are in the reference frame, we can aggregate\npoints for stationary instances by simply concatenating the\npoints\n$A_j = \\bigcup_{i=m}^{n} P_{i,j}$ (8)\nAggregating points for moving instances poses a significant\nchallenge, as the $P_{i,j}$ differs significantly between frames. For\nthat reason, for moving instances, only the trajectory and $P_{i,j}$\nare employed."}, {"title": "d) Orientation and size estimation:", "content": "Finding oriented 3D\nbounding boxes is a 7-degrees of freedom (DOF) problem.\nInstead of solving it directly, our method divides it into three\ndisjunctive problems: orientation, location and dimensions.\nOrientation estimation is different for moving and stationary\ninstances. For moving instances, the yaw of the object is\nestimated by directly computing the angle between adjacent\nlocations as\n$\\theta_{i,j} = atan2(\\frac{L_{i,j}(y) - L_{i-1,j} (y)}{L_{i,j}(x) - L_{i-1,j} (x)})$ (9)\nIt is worth noting again that all locations are transformed into\nthe reference coordinate space. To make it more robust, yaws\nfor up to 5 locations before and after the reference frame\nare predicted, and then the median is taken out of those yaw\npredictions.\nFor stationary instances, we adapt the 3D box fitting al-\ngorithm of Zhang et al. [37]. The algorithm first flattens all\npoints into BEV, and then it iterates over all possible angles\n\u03b8\u2208 0, \u03c0. For each angle, two perpendicular axes to each other\nare computed, and all points are projected into both axes C1\nand C2. Further, the criterion is computed, and @ with the\nminimal loss is chosen."}, {"title": "Algorithm 1 Saturated Closeness Criterion", "content": "Require: k\n1: function SATURATEDCLOSENESS (C1, C2)\n2:  cmax \u2190 P90(C1), cmin \u2190 P10(C1)\n3:  cmax P90(C2), cmin \u2190 P10(C2)\n4:  D1 \u2190 arg minv\u2208 {cmax-C1,C1-cmin} ||0||2\n5:  D2 \u2190 arg min\u2208 {cmax-C2,C2-cmin} ||0||2\n6:  D1 \u2190 \u03c3(\u03b1. D1)\n7:  D2 \u2190 \u03c3(\u03b1. D2)\n8:  L-0\n9:  for i = 1 to length(D1) do\n10:  d\u2190 min(D1(i), D2(i))\n11:  L-L+d\n12:  end for\n13:  return L\n14: end function\nAs mentioned, the algorithm chooses the yaw with a mini-\nmal loss. It creates two hypotheses, as it doesn't differentiate\nbetween the front and back of the car. It is worth noting that\nthe novel Saturated Closeness Criterion does not significantly\nincrease the computational requirements.\nTo make the estimation more robust, we take both the Aj\nand multiple Pij. Our method chooses frames so the object\ninstance is the closest possible to the ego-vehicle, but at the\nsame time, it is still fully in view to benefit from the fact that\nthe pseudo-lidar is the most accurate for close predictions.\nThe algorithm outputs a single spatial dimension for a single\nframe. If any of the values exceed the expected dimensions\nof an instance, the output is replaced by a prior estimate of\nthe dimensions of a generic instance. Also, it is necessary to\ntake into account that estimating the spatial dimensions of an\ninstance from a single image can be an ill-posed problem; for\nexample, if an instance is seen from the back, it is not possible\nto estimate length correctly. Thus, such cases are detected,\nand again, the output is replaced by a prior car dimensions\nestimate."}, {"title": "e) Position refinement:", "content": "The yaw bij and an approximate\n3D position Li,j are already known from the previous steps.\nIn order to get a more fine-grained estimate of position Li,j,\nwe apply small perturbations (up to 2 meters) along x and z\naxes and use Template Fitting Loss (TFL) [31] as the criterion\nto select the final position. In this stage, we also determine\nwhether the car is facing towards or away from the ego-vehicle\nby testing two hypotheses for vehicle orientation \u2013 0 and 0+ \u03c0\nand again selecting the orientation with the lowest TFL. As the\norientation, location and dimensions are known at this step,\n3D pseudo labels are created and further used to train the\nMonocular 3D Detector."}, {"title": "B. Canonical Object Space (COS)", "content": "Camera focal length is a crucial parameter for precise met-\nric depth estimation, but monocular 3D detectors, however,\nhave no direct way to compensate for changing focal length\nbecause the same object at the same distance from the camera\nwill have different size in the image, depending on the focal\nlength (see fig. 5).\nAssuming an object of size $S^{real}$ with a distance D from\nthe camera in world coordinates, the apparent size (\u2248 number\nof pixels) in the image $S^{img}$ is given as\n$S^{img} \\approx \\frac{S^{real} \\cdot f_1}{D} \\quad  \\frac{S^{img}}{=} \\frac{S^{real} \\cdot f_2}{D}$ (11)\nwhere f1 and f2 denote the focal length of camera capturing\nthe same scene.\nTaken to the extreme, for the exact same object image (same\nsize $S^{img}$), the network needs to predict arbitrarily different\ndistances in world coordinates due to the focal length change.\nThis is clearly impossible, and this phenomenon confuses the\nnetwork both in training as well as at inference time. When\ncombining multiple datasets that were not taken with a single\ncamera or even a single dataset that used different cameras\nto capture the dataset [38], it is a pre-requisite to be able to\naccommodate focal length differences.\nInspired by Metric3D [3] where depth maps are normal-\nized into a uniform space, our method presents Canonical\nObject Space (COS) where3D bounding boxes are trained to\nbe invariant to changes in the focal length by choosing a\nsingle canonical focal length and then transforming the labels\n(objects) into COS. Instead of transforming whole images or\ndepth maps, as in Metric3D, only the x,y,z coordinates of\nall pseudo-ground truth labels are transformed. To transform\nlabels, the scaling parameter w\u2081 is calculated as\n$w_i = \\frac{f_C}{f_i}$ (12)\nwhere $f_i$ stands for the focal length of the frame i and f\nis the canonical focal length. Given the scaling parameter $W_i$,\nthe object instance (x, y, z) is transformed into COS as\n$\\begin{aligned}\nx^C &= x \\cdot w_i\ny^C &= y \\cdot w_i\nz^C &= z \\cdot w_i\n\\end{aligned}$ (13)\nThe model then operates in the COS and is directly supervised\nby the transformed labels (objects). During inference, all\npredictions are transformed from COS by dividing the depth\nprediction by scaling parameter w; of the inference frame j\n$\\begin{aligned}\nx &= \\frac{x^C}{W_j}\ny &= \\frac{y^C}{W_j}\nz &= \\frac{z^C}{W_j}\n\\end{aligned}$ (14)\nThis very simple transformation enables the network to train\non multiple datasets where focal length differs and also allows\nthe network to work in unseen camera setup (focal length)\nbecause $w_j$ is calculated at inference time (see table III - 0%\nfine-tuning row).\nNote that in the training process, it is necessary to adapt\nfor data augmentations that affect the perceived focal length\nof the image, such as image scaling, and for the fact that\nthe detector resizes the image into a constant input size by\nadjusting the perceived focal length of the image subject to\nthe applied augmentations and the resizing."}, {"title": "IV. EXPERIMENTS", "content": "a) Datasets: We use two public datasets for the evaluation\nKITTI [8] and KITTI-360 [39]. KITTI dataset setup has\nfour front-facing cameras, IMU and LiDAR. KITTI-360 setup\nuses two front-facing cameras and two fish-eye cameras facing\nsides, IMU and LiDAR. The cameras and their parameters\ndiffer in both datasets. For the KITTI dataset, the same training\n(3712 samples) and validation (3769 samples) splits as [2],\n[29], [31], [40] are used. For the KITTI-360 dataset, the\ntraining (6 sequences, 44178 frames), validation (2 sequences,\n1115 frames) and testing (1 sequence, 2459 frames) splits from\nVSRD [4] are used. It is worth noting that, unlike VSRD, we\nare able to use all frames of the training set as our method is\nmuch faster.\nb) Implementation:\nWe used off-the-shelf MViT2-\nHuge [41] in the Detectron2 framework [42] trained on MS-\nCOCO [43] as the 2D object detector and Metric3DV2-\ngiant [9] for metric depth estimation. We use our pseudo-labels\nto train MonoDETR [19] monocular 3D object detector, using\nAdamW [44] optimiser with learning rate and weight decay\nequal to 0.0002 and 0.0001, respectively, while keeping other\nhyper-parameters as in [19]. The canonical focal length is\nequal to 500. Aggregation is done over 100 frames, the p-\nscore threshold for stationary/moving classification is 0.0001,\nand the minimum threshold for $d_{real}$ is 5 meters. The steepness\nparameter a in the Saturated Closeness Criterion is 10."}, {"title": "A. Results", "content": "a) KITTI-360.: The first evaluation and comparison with\nprior work is on KITTI-360 [39] as shown in table I. Our\nmethod shows outstanding results on both the Bird's eye view\n(BEV) and 3D at 0.5 IoU as it outperforms current state-\nof-the-art weakly-supervised method VSRD [4], despite the\nfact our method does not use any human labels.\nIn the 0.3 IoU metric both for BEV and 3D, our method\noutperforms both WeakM3D [1] and Autolabels [2] by a sig-\nnificant margin and it achieves a competitive AP as VSRD [4].\nWe believe that this is likely caused by the detection failures,\nclass ambiguities or that KITTI 2D bounding boxes are amodal\nas using consistent ground-truth 2D labels significantly helps\nVSRD.\nb) Cross-dataset evaluation:\nThanks to the proposed\nCanonical Object Space, it is possible to train on multiple\nmerged datasets. We, therefore, combine both KITTI and\nKITTI-360 datasets training subsets, generate our pseudo-\nlabels, train a single model and evaluate it on the KITTI\nvalidation subset (see table II). We show that generating\npseudo-labels for two different datasets and then performing\ntraining on the joint significantly improves average precision\nboth for 0.3 and 0.5 IoU on BEV and 3D. The gain comes\nfrom the additional amount of data present in the KITTI-360\ndataset."}, {"title": "B. Ablations", "content": "a) Canonical Object Space: As seen in table IV, disabling\nCOS and training on both datasets jointly decreases the aver-\nage precision of both BEV and 3D. It may be surprising that\nwhen training on both KITTI+K360, the model is not broken\ncompletely - we speculate that the network is able to learn\nto classify from which of the two datasets the image comes\nfrom and adjusts its outputs accordingly; such an approach\nobviously does not generalize well. Note that only in this\nablation, we use human labels to eliminate any effects of\npseudo-labels.\nb) Number of frames used in aggregation: table V shows\nthe ablation study on the number of frames used in the aggre-\ngation process. Perhaps surprisingly, aggregating over longer\nsequences does not yield better performance. The aggregation\ncan recover from the error caused by Metric3D [3], [9] as in\nfig. 4. We speculate that the distant car poses a significant\nchallenge for the detector itself, therefore creating precise 3D\nlabels for such objects does not translate to better 3D detection\naccuracy."}, {"title": "c) Steepness parameter in Saturated Closeness Criterion:", "content": "In this ablation, we explore possible values for the steepness\nparameter a in the Saturated Closeness Criterion, which\ncontrols what pseudo-LIDAR points are discarded as outliers.\nAs shown in table VI, a equal to 10 achieves the best\nperformance."}, {"title": "V. CONCLUSION", "content": "A novel method for training monocular 3D detectors with-\nout domain-specific human annotations was presented. The\nmethod exploits temporal consistency in video sequences to\nautomatically create 3D labels of objects (cars) and, as such,\ndoes not require human labels or additional sensors such as\nLiDAR. The method is able to compensate for camera focal\nlength differences to significantly improve its scalability across\nmultiple datasets, and furthermore, it works out of the box in\npreviously unseen camera setups.\nThe method is evaluated on two large-scale public datasets,\nwhere despite using no human labels, it outperforms prior\nwork by a significant margin both for BEV and 3D when\nusing the stricter 0.5 IoU evaluation. Last but not least, the\nmethod shows its versatility by also being a powerful pre-\ntraining method for fully-supervised monocular 3D detection,\nwhere the final accuracy increases by approximately 6 percent\npoints over traditional object detector.\nThe main limitation is detecting objects far away, which is\nnot primarily due to the auto-labelling procedure but due to\nthe inherent limitation of the trained monocular detector to\ninfer the distance of objects which are only a couple of pixels\nhigh."}]}