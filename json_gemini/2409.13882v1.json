{"title": "Tabular Data Generation using Binary Diffusion", "authors": ["Vitaliy Kinakh", "Slava Voloshynovskiy"], "abstract": "Generating synthetic tabular data is critical in machine learning, especially when real data is limited or sensitive. Traditional generative models often face challenges due to the unique characteristics of tabular data, such as mixed data types and varied distributions, and require complex preprocessing or large pretrained models. In this paper, we introduce a novel, lossless binary transformation method that converts any tabular data into fixed-size binary representations, and a corresponding new generative model called Binary Diffusion, specifically designed for binary data. Binary Diffusion leverages the simplicity of XOR operations for noise addition and removal and employs binary cross-entropy loss for training. Our approach eliminates the need for extensive preprocessing, complex noise parameter tuning, and pretraining on large datasets. We evaluate our model on several popular tabular benchmark datasets, demonstrating that Binary Diffusion outperforms existing state-of-the-art models on Travel, Adult Income, and Diabetes datasets while being significantly smaller in size.", "sections": [{"title": "Introduction", "content": "The generation of synthetic tabular data is a critical task in machine learning, particularly when dealing with sensitive, private, or scarce real-world data. Traditional generative models often struggle with the inherent complexity and diversity of tabular data, which typically encompasses mixed data types and complex distributions.\nIn this paper, we introduce a method to transform generic tabular data into a binary representation, and a generative model named Binary Diffusion, specifically designed for binary data. Binary Diffusion leverages the simplicity of XOR operations for noise addition and removal, fundamental components of probabilistic diffusion models. This method eliminates the need for extensive preprocessing and complex noise parameter tuning, streamlining the data preparation process.\nOur approach offers several key advantages. First, by converting all columns into unified binary representations, the proposed transformation removes the necessity for column-specific preprocessing commonly required in handling mixed-type tabular data. Secondly, the Binary Diffusion model itself is optimized for binary data, utilizing binary cross-entropy (BCE) loss for predictions during the training of the denoising network.\nWe evaluate our model on several popular tabular benchmark datasets, including Travel tej [2021], Sick Smith et al. [1988], HELOC lia [2018], FICO [2018], Adult Income Becker and Kohavi [1996], California Housing Pace and Barry [1997], nug [2017], and Diabetes Strack et al. [2014], Kaggle [2021] tabular datasets. The Binary Diffusion model outperforms existing state-of-the-art models on Travel, Adult Income and Dianetes datasets. Additionally, our model is significantly smaller in size compared to contemporary models and does not require pretraining on other data modalities, unlike methods based on large language models (LLMs) such as GReaT Borisov et al. [2022]."}, {"title": "Related Work", "content": "TVAE (Tabular Variational Autoencoder) adapts the Variational Autoencoder (VAE) framework to handle mixed-type tabular data by separately modeling continuous and categorical variables. CTGAN (Conditional Tabular GAN) employs a conditional generator to address imbalanced categorical columns, ensuring the generation of diverse and realistic samples by conditioning on categorical data distributions. CopulaGAN integrates copulas with GANs to capture dependencies between variables, ensuring that synthetic data preserves the complex relationships present in the original dataset Xu et al. [2019].\nGReaT (Generation of Realistic Tabular data) Borisov et al. [2022] leverages a pretrained auto-regressive language model (LLM) to generate highly realistic synthetic tabular data. The process involves fine-tuning the LLM on textually encoded tabular data, transforming each row into a sequence of words. This approach allows the model to condition on any subset of features and generate the remaining data without additional overhead.\nExisting data generation methods show several shortcomings. Models such as CopulaGAN, CTGAN, and TVAE attempt to generate columns with both continuous and categorical data simultaneously, employing activation functions like softmax and tanh in the outputs. These models also require complex preprocessing of continuous values and rely on restrictive approximations using Gaussian mixture models and mode-specific normalization. Additionally, large language model-based generators like GReaT need extensive pretraining on text data, making them computationally intensive with large parameter counts with potential bias from the pretraining data.\nThe proposed data transformation and generative model address these shortcomings as follows: (i) by converting all columns to unified binary representations; (ii) the proposed generative model for binary data, with fewer than 2M parameters, does not require pretraining on large datasets and offers both fast training and sampling capabilities."}, {"title": "Data transformation", "content": "To apply the Binary Diffusion model to tabular data, we propose a invertible lossless transformation T, shown on the Figure 1, that converts tabular data columns into fixed-size binary representations. The transformations is essential for preparing tabular data for the Binary Diffusion model, enabling it to process and generate tabular data without the need for extensive preprocessing. This approach ensures that the data retains its original characteristics.\nThe transformation method converts each column of the table into a binary format. For continuous data, this process includes applying min-max normalization to the columns, followed by converting these normalized values into a binary representation via 32-bit floating-point encoding. For categorical data, binary encoding is used. The encoded columns are concatenated into fixed-size rows.\nThe inverse transformation $T^{-1}$ converts the binary representations back into their original form. For continuous data, the decoded values are rescaled to their original range using metadata generated during the initial transformation. For categorical data, the binary codes are mapped back to their respective categories using a predefined mapping scheme."}, {"title": "Binary Diffusion", "content": "Binary Diffusion shown in Figure 2 is a novel approach for generative modeling that leverages the simplicity and robustness of binary data representations. This method involves adding and removing noise through XOR operation, which makes it particularly well-suited for handling binary data. Below, we describe the key aspects of the Binary Diffusion methodology in detail.\nIn Binary Diffusion, noise is added to the data by flipping bits using the XOR operation with a random binary mask. The amount of noise added is quantified by the proportion of bits flipped. Let $x_0 \\in {0,1}^d$ be the original binary vector of dimension d, and $z_t \\in {0,1}^d$ be a random binary noise vector at timestep t. The noisy vector $x_t$ is obtained as: $x_t = x_0 \\oplus z_t$, where $\\oplus$ denotes the XOR operation. The noise level is defined as the fraction of bits flipped in $z_t$ in the mapper $M_t$ at step t, with the number of bits flipped ranging within [0, 0.5] as a function of the timestep.\nThe denoising network $q_{\\theta}(x_0, z_t | x_t, t, y_e)$ is trained to predict both the added noise $z_t$ and the clean-denoised vector $x_0$ from the noisy vector $x_t$. We employ binary cross-entropy (BCE) loss (1) to train the denoising network. The loss function is averaged over both the batch of samples and the dimensions of the vectors:\n$L(\\theta) = \\frac{1}{B} \\sum_{b=1}^B [L_x(x_0^{(b)}, \\hat{x}_0^{(b)}) + L_z(z_t^{(b)}, \\hat{z}_t^{(b)})]$\n$\\phantom{L(\\theta)} = \\frac{1}{B} \\sum_{b=1}^B [\\frac{1}{d} \\sum_{i=1}^d -x_{0i}^{(b)} log(\\hat{x}_{0i}^{(b)}) + (1 - x_{0i}^{(b)}) log(1 - \\hat{x}_{0i}^{(b)})$\n$\\phantom{L(\\theta)} = \\frac{1}{B} \\sum_{b=1}^B [\\frac{1}{d} \\sum_{i=1}^d - \\hat{z}_{ti}^{(b)} log(\\hat{z}_{ti}^{(b)}) + (1 - z_{ti}^{(b)}) log(1 - \\hat{z}_{ti}^{(b)})] $        (1)\nwhere B is the batch size, $\\theta$ represents the parameters of the denoising network, $x_0^{(b)}$ and $\\hat{x}_0^{(b)}$ are the b-th samples of the true clean vectors and the predicted clean vectors, respectively. Similarly, $z_t^{(b)}$ and $\\hat{z}_t^{(b)}$ are the b-th samples of the true added noise vectors and the predicted noise vectors, respectively. $y_e = E_y(y)$ denotes the encoded label y, one-hot encoded for classification and min-max normalized for regression. $L_x$ and $L_z$ denotes binary cross-entropy (BCE) loss. The indices i and b correspond to the i-th dimension of the vectors and the b-th sample in the batch, respectively.\nDuring training (Figure 2 left), we use classifier-free guidance Ho and Salimans [2022]. For classification tasks, the conditioning input class label y is a one-hot encoded label $y_e$. For regression tasks, y consists of min-max normalized target values y, allowing the model to generate data conditioned on specific numerical outcomes. In unconditional training, we use an all-zeros conditioning vector for classification tasks and a value of -1 for regression tasks to indicate the absence of conditioning.\nWhen sampling (Figure 2 right), we start from a random binary vector $x_t$ at timestep t = T, along with the conditioning variable y, encoded into $y_e$. For each selected timestep in the sequence [T, ..., 0], denoising is applied to the vector. The denoised vector $\\hat{x}_0$ and the estimated binary noise $\\hat{z}_t$ are predicted by the denoising network. These predictions are then processed using a sigmoid function and binarized with a threshold. During sampling, we use the denoised vector $\\hat{x}_0$ directly. Then, random noise $z_t$ is generated and added to $\\hat{x}_0$ via the XOR operation: $x_t = \\hat{x}_0 \\oplus z_t$. The sampling algorithm is summarized in Algorithm 1."}, {"title": "Results", "content": "We evaluate the performance of Binary Diffusion on widely-recognized tabular benchmark datasets, including Travel tej [2021], Sick Smith et al. [1988], HELOC lia [2018], FICO [2018], Adult Income Becker and Kohavi [1996], California Housing Pace and Barry [1997], nug [2017], and Diabetes Strack et al. [2014], Kaggle [2021]. For classification tasks (Travel, Sick, HELOC, Adult Income, and Diabetes), classification accuracy was used as metric, while mean squared error (MSE) was used for the regression task (California Housing). Following the evaluation protocol established in Borisov et al. [2022], we employed Linear/Logistic Regression (LR), Decision Tree (DT), and Random Forest (RF) as downstream models to assess the quality of the synthetic data. The datasets were split into training and test sets with an 80/20 split. The generative models were trained on the training set, and the test set was reserved for evaluation. To ensure robustness, 5 sets of synthetic training data were generated, and the results are reported as average performances with corresponding standard deviations. Table 1 shows the detailed results. Binary Diffusion achieved superior performance compared to existing state-of-the-art models on the Travel, Adult Income, and Diabetes datasets. Notably, Binary Diffusion maintained competitive results on the HELOC and Sick datasets, despite having a significantly smaller parameter footprint (ranging from 1.1M to 2.6M parameters) compared to models like GReaT, which utilize large language models with hundreds of millions of parameters. Binary Diffusion does not require pretraining on external data modalities, enhancing its efficiency and reducing potential biases associated with pretraining data. In the regression task (California Housing), Binary Diffusion demonstrated competitive MSE scores. Additionally, Binary Diffusion offers faster training and sampling times, as detailed in Appendix C. Implementation details are summarized in Appendix D."}, {"title": "Conclusions", "content": "This paper proposed a novel lossless binary transformation method for tabular data, which converts any data into fixed-size binary representations. Building upon this transformation, we introduced the Binary Diffusion model, a generative model specifically designed for binary data that utilizes XOR operations for noise addition and removal and is trained using binary cross-entropy loss. Our approach addresses several shortcomings of existing methods, such as the need for complex preprocessing, reliance on large pretrained models, and computational inefficiency.\nWe evaluated our model on several tabular benchmark datasets, and demonstrated that Binary Diffusion achieves state-of-the-art performance on these datasets while being significantly smaller in size compared to existing models. Our model does not require pretraining on other data modalities, which simplifies the training process and avoids potential biases from pretraining data. Our findings"}, {"title": "Sampling algorithm", "content": "Algorithm 1 Sampling algorithm.\n1: $x_t \\leftarrow$ random binary tensor\n2: $y \\leftarrow$ condition/label\n3: $y_e \\leftarrow$ apply condition enxoding\n4: $threshold \\leftarrow$ threshold value to binarize  \\triangleright Default 0.5\n5: $q_{\\theta}(x_0, z_t | x_t, t, y_e) \\leftarrow$ pre-trained denoiser network\n6: for t $\\in$ {T, . . ., 0} do   \\triangleright Selected timesteps\n7:  $\\hat{x}_0, \\hat{z}_t \\leftarrow q_{\\theta}(x_0, z_t | x_t, t, y_e)$\n8:  $\\hat{x}_{0i} \\leftarrow \\sigma(\\hat{x}_{0i}) > threshold$   \\triangleright Apply sigmoid and compare to threshold\n9:  $z_+ \\leftarrow get\\_binary\\_noise(t)$   \\triangleright Generate random noise\n10:  $x_t \\leftarrow \\hat{x}_0 \\oplus z_t$    \\triangleright Update $x_t$ using XOR with $z_t$\n11: end for\n12: return $x_t$"}, {"title": "Evaluation models hyperparameters", "content": "During evaluation, we follow the evaluation proposed in Borisov et al. [2022]. The hyperparameter configuration of the evaluation models for the ML efficiency experiments are provided in Table 2."}, {"title": "Runtime comparison", "content": "We compare the training and sampling times, the number of training epochs, batch sizes, and peak VRAM utilization of generative models. The results, including the number of training epochs and batch sizes required for each model to converge, are summarized in Table 3. Specifically, for TVAE, CopulaGAN, and CTGAN, we employed the default batch size of 500 and trained for 200 epochs; for Distill-GReaT and GReaT, we used a batch size of 32 and trained for 200 epochs; and for Binary Diffusion, a batch size of 256 and 500 epochs were utilized to ensure model convergence. For this study, we utilized the Adult Income dataset. All experiments were conducted on a PC with a single RTX 2080 Ti GPU, an Intel Core i9-9900K CPU 3.60 GHz with 16 threads, 64 GB of RAM, and Ubuntu 20.04 LTS as the operating system."}, {"title": "Implementation details", "content": "Denoiser Architecture. We use a similar denoiser architecture across all datasets, which takes as input a noisy vector $x_t$ of size d, a timestep t, and an input condition y. The input size d corresponds to the size of the binary vector in each dataset. The input vector $x_t$ is projected through a linear layer with 256 output units. The timestep t is processed using a sinusoidal positional embedding, followed by two linear layers with 256 output units each, interleaved with GELU activation functions Hendrycks and Gimpel [2016]. The input condition y is processed through a linear projector with 256 output units. The outputs of the timestep embedding and the condition projector are then combined via element-wise addition. This combined representation is subsequently processed by three ResNet blocks that incorporate timestep embeddings. Depending on the size of the binary representation for each dataset, the number of parameters varies between 1.1 million and 1.4 million.\nTraining and Sampling Details. We trained the denoiser for 50,000 steps using the Adam optimizer Kingma and Ba [2014] with a learning rate of $1 \\times 10^{-4}$, a weight decay of 0, and a batch size of 256. To maintain a distilled version of the denoiser, we employed an Exponential Moving Average (EMA) with a decay rate of 0.995, updating it every 10 training steps. This distilled model was subsequently used for sampling. During training, we utilized classifier-free guidance with a 10% probability of using a zero token. The diffusion model was configured to perform 1,000 denoising steps during training. Given the relatively small size of our models, we opted for full-precision training. All training parameters are summarized in Table 4."}, {"title": "Effect of sampling steps", "content": "We empirically observed that model performance, measured by accuracy for classification tasks and mean squared error (MSE) for regression tasks, deteriorates as the number of sampling steps increases. Notably, for regression tasks, linear regression models show significantly poorer performance with an increasing number of sampling steps. For our analysis, we utilized an Exponential Moving Average"}]}