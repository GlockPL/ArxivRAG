{"title": "Equitable Federated Learning with Activation Clustering", "authors": ["Antesh Upadhyay", "Abolfazl Hashemi"], "abstract": "Federated learning is a prominent distributed learning paradigm that incorporates collaboration among diverse clients, promotes data locality, and thus ensures privacy. These clients have their own technological, cultural, and other biases in the process of data generation. However, the present standard often ignores this bias/heterogeneity, perpetuating bias against certain groups rather than mitigating it. In response to this concern, we propose an equitable clustering-based framework where the clients are categorized/clustered based on how similar they are to each other. We propose a unique way to construct the similarity matrix that uses activation vectors. Furthermore, we propose a client weighing mechanism to ensure that each cluster receives equal importance and establish $O(\\frac{1}{\\sqrt{K}})$ rate of convergence to reach an $\\epsilon$-stationary solution. We assess the effectiveness of our proposed strategy against common baselines, demonstrating its efficacy in terms of reducing the bias existing amongst various client clusters and consequently ameliorating algorithmic bias against specific groups.", "sections": [{"title": "1 Introduction", "content": "With the advent of distributed learning paradigms, Federated Learning (FL) emerged as a promising mechanism for collaborative learning among diverse clients. In FL, the diversity of the clients gives rise to both system and statistical heterogeneity [1,2]. The system heterogeneity is attributable to the different computational capabilities of the participating clients, uplink and downlink communication channel bandwidths, and faults. Statistical heterogeneity exists due to the variability in the data distribution coming from the different clients. In our work, our primary aim is to tackle the statistical heterogeneity in the data distribution arising due to the technological, cultural, and other biases in the data generation process native to each client.\nIn a distributed learning setting, algorithms use the participating clients' local updates and perform global weighted aggregation. The goal is to create a global model that performs equally well on all clients. This leads to a notion of group fairness [3] that incentivizes the participation of diverse groups of clients [4], thus mitigating the bias in performance during the learning process. The end goal of such algorithms is to ensure that the generated model is not biased towards any particular group of clients and performs well for all of them. This is achieved through various techniques, such as FL, where the model is trained on the data from all participating clients without compromising the privacy of individual clients. Ultimately, the success of distributed learning algorithms depends on their ability to balance the needs and preferences of all participating clients to generate a global model that is accurate, efficient, and fair. A celebrated FL method, FedAvg [1], showcases success in the case of IID distribution of data among the clients. However, its performance in the case"}, {"title": "2 Related Work", "content": "Centralized and Consensus-based Methods. Fairness has long been a concern in machine learning. Traditional centralized machine learning approaches use pre-processing and post-processing techniques to ensure fairness since the central server typically has access to the data [6\u201310]. However, these methods do not suit paradigms like FL, which prioritize data locality and privacy. Achieving fairness in FL remains a critical research area. This paper focuses on traditional FL settings where collaborative model training happens across multiple clients while preserving the data privacy. Researchers have proposed extensions to FedAvg [1], including model-level regularization [2, 11], feature alignment between local and global models [12], and momentum-based updates at the server and client levels to reduce variance [13,14]. Despite these advancements,\nFL methods often train a global model that performs well on average across all clients, neglecting individual group performance. This limitation calls for new methods that accommodate groups' demands.\nClustering and Fairness in Federated Learning. Preserving privacy by not sharing data or sensitive client information with the server or other clients is a crucial aspect of FL, posing challenges for developing learning algorithms. Some work [15\u201317] prioritize client selection to boost performance using local validation losses. We demonstrate client discrepancies using the global model's performance at the end of each epoch, though this information is not essential for our algorithm's efficiency. Other approaches [18,19] group the clients based on their representations using similarity matrices or submodular sets. These methods assume a fixed number of clients per round, deviating from FL standards. Approaches like [20,21] prioritize highly contributing clients, undermining the consensus-based nature of FL. The method in [22] minimizes the maximum loss across all data samples to avoid bias towards any data distribution, differing from our server side debiasing approach using activation vector information. Additionally, [23] proposes domain adaptation for client groups with similar characteristics, whereas we focus on disjoint client groups participating in the collaboration. Other works such as [24] aim to mitigate the bias between the data samples with a client's data to promote group fairness, whereas we focus on fairness among groups of clients where the goal is to create equitable learning scenarios among different groups. A recent study [25] also focuses on ensuring fair performance for both groups of clients and individual clients. However, prior knowledge of the client groups is required, while our approach automatically clusters the clients based on the activation vectors. Another concurrent work [26] orthogonal to ours uses the bias's gradients in the neural network's last layer to construct clusters and then perform heterogeneity-aware client sampling. This work poses an immense computational overhead, requiring computing the similarity matrix amongst all clients to sample the participating clients. Other clustering-based works [27, 28] perform clustered federated learning to promote personalization, which is not the objective of our work."}, {"title": "3 Background and Preliminaries", "content": "In this section, we start by defining the standard terminologies introduced in [1,2] and proceed towards extending the idea to the setting used in our paper.", "subsections": [{"title": "3.1 Federated Learning Setup", "content": "FL involves a central server collaborating with $n$ clients, each maintaining its unique data distribution $D_i$ and sample size $N_i$. The central server aims to train a machine learning model using data from its clients without accessing their local data directly, thus preserving data locality. The"}]}, {"title": "4 Equitable Clustering", "content": "In this section, we present our novel approach named Equitable-FL, which aims to tackle the issue of algorithmic bias in FL. Our solution involves implementing a server-side debiasing mechanism that leverages the activation vectors (see Definition 1) to identify and cluster clients based on their similarities. By doing so, we are able to update how the central server aggregates the local model updates received from clients. This approach ensures that the participation across each group of clients is more equitable, minimizing the potential for bias to occur during the learning process.", "subsections": [{"title": "4.1 Problem statement and Motivation", "content": "In Federated Learning (FL), the $k$-th communication round's model aggregation involves computing the weighted average of the model updates from each client, given by $w_{k+1} = \\sum_{i\\in S_k} p_i w_{k,E}^i$, where $S_k$ is the set of clients in round $k$. Algorithms such as those proposed by McMahan et al. (2017) and Li et al. (2020) weigh clients differently based on factors like the number of data samples, $N_i$, a client possesses. In this scenario, a client's weight is proportional to their data sample size, $p_i = \\frac{N_i}{\\sum_{i\\in S_k} N_i}$.\nAlternatively, clients can be weighed uniformly, irrespective of their data sample size, with each client's weight being $\\frac{1}{n}$. Accurately weighing each client's contribution is crucial for fair and unbiased distributed learning. Simply weighing clients based on data sample size can lead to biases,"}, {"title": "4.2 Framework: Equitable-FL", "content": "Our proposed framework, Equitable-FL, addresses these limitations. First, we define what an activation vector is in our context.\nDefinition 1 (Activation vector). An activation vector within a neural network refers to the output generated by any given layer after it undergoes transformations like linear combinations (involving inputs, weights, and biases) and activation through a non-linear function. This output captures critical features of the input data, which are vital for the following layers in tasks like classification or prediction. For our purpose, we use the activation vectors, specifically $a_{k,E}^i$, that are generated as outputs of the pre-final layer of the model architecture for $E$ \u2013 th local epoch and"}, {"title": "4.3 Convergence Analysis", "content": "In this section, we discuss the standard assumptions that we make in order to provide theoretical guarantees on the convergence of the proposed method.\nAssumption 1 (Smoothness). $l(x, w)$ is $L$-smooth with respect to $w$, for all $x$. Thus, each $f_i(w)$ ($i \\in [n]$) is $L$-smooth, and so is $f(w)$.\n$||\\nabla f_i(w_1) - \\nabla f_i(w_2)|| \\leq L||w_1 \u2013 w_2||; \\text{ for any } i, w_1, w_2$.\nThe assumption stated in Assumption 1 is frequently used while analyzing the convergence of algorithms that employ gradient-based optimization. This assumption has been referenced in several publications such as [14,30,31]. It aims to limit abrupt changes in the gradients.\nAssumption 2 (Non-negativity). Each $f_i(w)$ is non-negative and therefore, $f^* \\eqslantless \\min f_i(w) \\geq 0$.\nThe assumption stated in Assumption 2 is usually fulfilled by the loss functions that are employed in practical applications. Nevertheless, if a loss function happens to have a negative value, this assumption can still be met by introducing a constant offset.\nAssumption 3 (Bounded Variance). The variance of the stochastic gradient for all clients $i = 1,...,n$ is bounded, where $B_{k,\\tau}^{(i)}$ represents the random batch of samples in client $i$ for $\\tau$th local iteration.\n$\\mathbb{E}[||\\nabla \\tilde{f_i}(w; B_{k,\\tau}^{(i)}) - \\nabla f_i(w)||^2] \\leq \\sigma^2$.\nAssumption 3 is often utilized to assess the convergence of gradient descent-based algorithms, as demonstrated in various works, including [31-33]. However, some other studies assume uniformly bounded stochastic gradients, where $\\mathbb{E}[||\\nabla \\tilde{f_i}(w); B)||^2] < \\sigma^2$. This assumption is stronger than Assumption 3 and is also shown to be untrue for convex loss functions in [33].\nAssumption 4 (Existence of Clusters). Assuming a system comprising $C$ clusters to which all $n$ clients are allocated, this assumption aligns with the inherent system partitions, for instance, clients segmented by diverse demographic regions. Each cluster, denoted by $\\gamma_q$, encapsulates a subset of participants, with $q$ signifying the specific cluster. In the course of the $k$th communication round, a selection of $r$ clients is made to partake, and they are subsequently distributed into $C$ clusters by our algorithm, ensuring a minimum representation of one client per cluster $q$.\nRemark 4. We ensure a minimum of one participant per cluster for theoretical analysis purposes, though this constraint is relaxed during experimental execution. Notably, Assumption 4 is not a stringent requirement, as the scenario where a cluster lacks participant representation is deemed excessively pessimistic."}]}, {"title": "5 Experiments", "content": "In this section, we present the findings of our framework and compare it with several baselines. We evaluate our algorithm and baselines on an extensive suite of datasets in FL with varying client partition and cluster sizes to show its efficacy.", "subsections": [{"title": "5.1 Datasets and Model Architecture", "content": "We conduct deep learning experiments on datasets such as MNIST [35], CIFAR-10, CIFAR-100 [36], and FEMNIST [37,38]. These datasets are standard datasets used in FL experimentation. To showcase the effectiveness of our algorithm, we partition the data in a non-IID fashion. In this partition, we try to emulate the clustering scenario by creating groups among clients and giving them only specific labels. We train a simple CNN model architecture and ResNet-18 [39] for all these datasets. In Table 1, we show how we have planted the clusters. For example, in the case of the MNIST dataset, there are $n$ = 10 clients divided into $C$ = 2 clusters where $r$ = 4 clients are participating in each round. We now describe the data partition and the model architecture for the datasets mentioned above.\nMNIST. In this case, we train a simple neural network on the MNIST dataset. The total number of clients is $n$ =10, and the data is distributed among these 10 clients. As we described earlier, the division of data is in a non-IID fashion. Since we do not know the true distribution of data, we created the non-IID and an inherent clustering scenario by distributing the data based on classes. In particular, we ensure that there are two sets of clients where the data possession is entirely orthogonal. So, the first 4 out of the 10 clients get the images from the first 4 out of 10 classes, and the rest classes go to the remaining 6 clients. A client has 800 images of each class, which leads to a client from the first category having 3200 images and a client from the second category having 4800 images, respectively. This approach leads to the formation of two clusters with heterogeneity in terms of the number of samples in each cluster and the nature of samples present in each cluster. From a practical perspective, we tried to emulate the partial client participation scenario. We conducted experiments with 40% of the total participants. The model architecture consists of three fully connected layers, with the first layer accepting flattened input images of size 28\u00d728 (784"}, {"title": "5.2 Experiment Setup", "content": "Baselines. We evaluate our proposed approach against several seminal works in the FL area for tackling client heterogeneity. The baselines include algorithms that tackle client heterogeneity, such as Fedprox [2], and client selection to improve the representation of clients, such as Cluster2 [18]. Other algorithms that we compare against propose different model aggregation strategies for groups of clients to promote group fairness, i.e., FairFed [24] and GIFAIR-FL-Global [25]. We also compare"}, {"title": "5.3 Main Results", "content": "In this study, we evaluate the performance of our algorithm against established baselines. To ensure comprehensive results, we conducted experiments on various datasets, including the widely used MNIST, CIFAR-10, CIFAR-100, and FEMNIST datasets. The datasets are distributed among clients such that the clients form clusters. Our findings demonstrate that Equitable-FL consistently outperforms other baselines in reducing client disagreement in highly heterogeneous settings, as illustrated in Figure 3. Additionally, we show that the effectiveness of our framework in mitigating bias is independent of the model architecture used. The average accuracy and Acc are presented in Tables 2 and 3 as well as in Figures 4 and 5. Our results indicate that the Equitable-FL significantly outperforms other baselines, except the FEMNIST dataset (regarding average accuracy). Overall, our results suggest that Equitable-FL is a promising solution for addressing challenges associated with distributed machine learning."}, {"title": "5.4 Abalation study", "content": "We evaluate our proposed framework on the CIFAR-100 dataset using the ResNet-18 architecture. During the experiment, we vary the number of clusters, $C$, the number of local epochs, $E$, and the proximal term, $\\mu$. In Table 4, we show that with correct cluster assignment, i.e., $C$ = 3, our framework significantly reduces the disagreement among clients as well as improves the test accuracy. Additionally, we show in Table 4 that with increasing local iterations, our algorithm consistently manages to reduce the disagreement among clients, mitigating the effect of client drift. In Table 5, we show that if we increase the $\\mu$, the disagreement or the effect of client drift will"}]}, {"title": "6 Conclusion", "content": "In this paper, we presented an equitable learning framework for FL, which reduces the bias against a diverse set of participants. We utilize the side information offered by the activation vectors to cluster the clients into groups based on their similarity and use this to propose a weighing mechanism that promotes fairness. Additionally, we established a rate of convergence to reach a stationary solution for Equitable-FL. We visualized the efficacy of our proposed framework on various vision datasets and showed that it consistently outperforms the baseline in mitigating bias.\nLimitation and Future works: As previously mentioned, privacy is a cornerstone of Federated Learning (FL). However, activation vectors can lead to information leakage if the server is compromised. Despite this risk, privacy-preserving mechanisms exist to enable secure learning without compromising privacy [40, 41]. This paper aims to demonstrate the effectiveness of using activation vectors to cluster client groups and develop a fair weighting mechanism for these groups. We also propose that this clustering approach can be applied to personalized federated learning [42,43]. This method effectively clusters client groups and ensures fair solutions for each group, maintaining fairness for all participants."}, {"title": "Appendix / supplemental material", "content": null, "subsections": [{"title": "A Theorems and Lemmas", "content": "Theorem 2. (Detailed proof of Theorem 1) For $\\eta_kLE \\leq \\frac{1}{2}$, $\\eta_k\\mu E \\leq \\frac{1}{2}$, $\\mu < 1$, and let Assumptions 1, 2, 3, and 4 hold true for Equitable-FL (refer Algorithm 1). In Equitable-FL, set $\\eta_k = O(\\frac{1}{\\sqrt{K}})$ for all k. Define a distribution P for k $\\in$ {0,...,K \u2013 1} such that\n$P(k) = \\frac{\\zeta^k}{\\sum_{k=0}^{K-1} (1+\\zeta)^{(K-1-k)}}$ where $\\zeta := \\eta^2E^2 (9\\eta_kL^2E + 4\\eta_k\\mu E + 6L (1 + \\frac{4\\eta^2\\mu^2E^2}{18}))$. Sample $k^*$ from P uniformly. Then:\n$\\mathbb{E}[||\\nabla f(w_{k^*})||^2] = O(\\frac{\\sigma^2}{\\sqrt{K}})$\nProof. From Lemma 1, for $\\eta_kLE < \\frac{1}{2}$, $\\eta_k\\mu E \\leq \\frac{1}{2}$, we upper bound the per-round progress as:\n$\\mathbb{E} [f(w_{k+1})] \\leq \\mathbb{E} [f(w_{k})] - \\frac{\\eta_kE(1-\\mu)}{2} \\mathbb{E} [||\\nabla f(w_{k})||^2] + \\eta_k^2E^2 (9\\eta_kL^2E + 4\\eta_k\\mu E + 6L ( 1 + \\frac{4\\eta^2 \\mu^2 E^2}{18})) + E (L (1+\\frac{2\\eta_k}{3}) + \\frac{8\\eta_kL^2E^3}{9} + \\frac{4\\eta_k\\mu E (1 + \\eta_k\\mu E)}{9} )\\frac{1}{3C} (L^2 + \\frac{C}{Ya^2}) \\sigma^2$\nNow using L-smoothness and 2 of $f_i$'s, we get:\n$\\frac{1}{C} \\sum_{q=1}^{C} (\\frac{1}{|\\gamma_q|}) \\sum_{i\\in [q]} \\mathbb{E}[||\\nabla f_i(w_k)||^2] \\leq \\frac{2L}{C} \\sum_{q=1}^{C} (\\frac{1}{|\\gamma_q|}) \\sum_{i\\in [q]} (\\mathbb{E}[f_i(w_k)] - f_i)$\n$\\frac{1}{C} \\sum_{q=1}^{C} (\\frac{1}{|\\gamma_q|}) \\sum_{i\\in [q]} \\mathbb{E}[||\\nabla f_i(w_k)||^2] \\leq 2L\\mathbb{E}[f(w_k)] - \\frac{2L}{C} \\sum_{i\\in [q]} f_i$\n$\\frac{1}{C} \\sum_{q=1}^{C} (\\frac{1}{|\\gamma_q|}) \\sum_{i\\in [q]} \\mathbb{E}[||\\nabla f_i(w_k)||^2] \\leq 2nL\\mathbb{E}[f(w_k)]$\nUsing eq. (11) in eq. (10), we get for a constant learning rate of $\\eta_k = \\eta$:\n$\\mathbb{E} [F(W_{k+1})] \\leq (1 - \\frac{\\eta E(1-\\mu)}{2} + \\eta^2E^2 (9\\eta L^2E + 4\\eta \\mu E +6L (1 + \\frac{4\\eta^2 \\mu^2 E^2}{18}))) \\mathbb{E} [f(w_{k})]\n+ \\eta^2E(L (1+\\frac{2\\eta}{3}) + \\frac{8\\eta L^2E^3}{9} + \\frac{4\\eta \\mu E (1 + \\eta \\mu E)}{9} )\\frac{1}{3C} (L^2 + \\frac{C}{Ya^2}) \\sigma^2$\nFor clarity, define $\\zeta := \\eta^2E^2 (9\\eta L^2E + 9\\eta L^2E + 4\\eta \\mu E + 6L (1 + \\frac{4\\eta^2 \\mu^2 E^2}{18}))$ and $\\zeta_2 := \\eta^2E (L (1+\\frac{2\\eta}{3}) + \\frac{8\\eta L^2E^3}{9} + \\frac{4\\eta \\mu E (1 + \\eta \\mu E)}{9} ) + \\frac{1}{3C} (L^2 + \\frac{C}{Ya^2})$ Then unfolding the recursion from k = 0 to k = K \u2212 1, we get:\n$\\mathbb{E}[f(w_{k})] \\leq (1 + \\zeta_1) \\mathbb{E} [f(w_{k})] - \\frac{\\eta E(1-\\mu)}{2} \\sum_{k=0}^{K-1} \\Pi E(1-\\mu)(1+\\zeta)^{(K-1-k)} \\mathbb{E} [||\\nabla f(w_{k})||^2] + \\eta^2 E\\zeta_2 \\sigma^2 \\sum_{k=0}^{K-1} (1+\\zeta)^{(K-1-k)}$\nLet us define $p_k := \\frac{\\zeta^k}{\\sum_{k=0}^{K-1} (1+\\zeta)^{(K-1-k)}}$. Then, set $\\mu < 1$ and re-arranging eq. (13) using the fact that $\\mathbb{E}[f(w_{k})] \\geq 0$, we get:\n$\\sum_{k=0}^{K-1} Pk\\mathbb{E}[||\\nabla f(w_{k}) ||^2] \\leq \\frac{2(1+\\zeta)\\mathbb{E} [f(w_{k})]}{\\eta E(1 - \\mu) \\sum_{k=0}^{K-1} (1+\\zeta)^{k'}} + \\frac{2\\eta \\zeta_2 \\sigma^2}{(1 - \\mu)^2}$\n$= \\frac{2\\zeta f (w_0)}{\\eta E(1 - \\mu) (1 - (1+\\zeta)^{-K})} + \\frac{2\\eta \\zeta_2 \\sigma^2}{(1 - \\mu)}$\nwhere the eq. (15) follows by using the fact that $\\sum_{k=0}^{K-1} (1+\\zeta)^k = \\frac{1 - (1+\\zeta)^K}{(1+\\zeta) -1}$. Now,\n(1 + $\\zeta_1)^{-K} < 1 - \\zeta K + \\frac{\\zeta^2 K(K+1)}{2} < 1 - \\zeta K + \\zeta^2 K^2 \\implies 1 \u2013 (1 + $\\zeta)^{-K} > \\zeta K(1 \u2013 \\zeta K)$. Plugging this in eq. (15), we have for $\\zeta K < 1$:\n$\\sum_{k=0}^{K-1} Pk\\mathbb{E}[||\\nabla f(w_{k}) ||^2] \\leq \\frac{2\\zeta f (w_0)}{\\eta EK(1 - \\mu)(1 - \\zeta K)} + \\frac{2\\eta \\zeta_2 \\sigma^2}{(1 - \\mu)}$\nNow, note that the optimal step size will be $\\eta = O(\\frac{1}{\\sqrt{K}})$. Then $\\sum_{k=0}^{K-1} Pk\\mathbb{E}[||\\nabla f(w_{k}) ||^2] = O(\\frac{\\sigma^2}{\\sqrt{K}})$"}]}]}