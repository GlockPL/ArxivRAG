{"title": "Mitigating Vanishing Activations in Deep CapsNets Using Channel Pruning", "authors": ["Siddharth Sahu", "Abdulrahman Altahhan"], "abstract": "Capsule Networks outperform Convolutional Neural Networks in learning the part-whole relationships with viewpoint invariance, and the credit goes to their multidimensional capsules. It was assumed that increasing the number of capsule layers in the capsule networks would enhance the model performance. However, recent studies found that Capsule Networks lack scalability due to vanishing activations in the capsules of deeper layers. This paper thoroughly investigates the vanishing activation problem in deep Capsule Networks. To analyze this issue and understand how increasing capsule dimensions can facilitate deeper networks, various Capsule Network models are constructed and evaluated with different numbers of capsules, capsule dimensions, and intermediate layers for this paper. Unlike traditional model pruning, which reduces the number of model parameters and expedites model training, this study uses pruning to mitigate the vanishing activations in the deeper capsule layers. In addition, the backbone network and capsule layers are pruned with different pruning ratios to reduce the number of inactive capsules and achieve better model accuracy than the unpruned models.", "sections": [{"title": "1 Introduction", "content": "The human brain learns part-whole relationships quite well with viewpoint invariance while recognizing shapes. Inspired by this philosophy, Hinton et al. [1] proposed the concept of 'capsules' that can recognize implicit entities by computing intrinsic visual information in multiple dimensions of the appearance manifold. The proposal was novel to the artificial neural network community because the capsules generate vector outputs that contain diverse spatial information instead of the scalar output produced by neurons of the artificial neural networks.\nConvolutional neural networks (CNNs) are widely used to recognize objects and patterns in images. They are computationally efficient and show strong translational equivariance due to convolution operations and sparse weight sharing. However, CNNs fail to generalize their knowledge to novel viewpoints, contributing to poor performance in shape recognition and overlapping object"}, {"title": "2 Background", "content": "Pruning is widely applied to CNN models to reduce computation cost and memory usage without significantly compromising the model's accuracy. Pruning can be performed in two different ways: structured and unstructured. On the one hand, structured pruning ensures that the network maintains its topology post-pruning and usually involves removing the filters or entire convolutional layers. [16] introduced filter pruning in CNNs and showed that the training accuracies of models like VGG-16 and ResNet-56 are not much affected even after pruning 34. 2% and 27. 6% of their tuning parameters, respectively.\nOn the other hand, unstructured pruning minimizes the model parameters by removing the non-contributing weights without paying any importance to the topology of the pruned network. The weight pruning performed on AlexNet and VGG-16 models showed impressive compression ratios of 35% and 49%, respectively, without impacting model accuracies [17]. Nevertheless, it may not reduce the computational complexity of convolutional layers due to the introduction of irregular sparsity after pruning [16]. [18] identified that structured pruning results in lower model memory requirements and faster inference time than unstructured pruning. For this reason, structured pruning is adopted as the method of choice in this paper to prune Capsule Networks.\nTo improve the structured filter pruning performance, [15] presented a data-driven CCM loss calculated using the correlation coefficient matrix between different feature maps in a network layer. CCM loss ensures the model learns stronger linear relations between the feature maps during training and compresses shared information into fewer useful channels for effective filter pruning. The authors also used the CHannel Independence-based Pruning (CHIP) technique, introduced by [14], to prune the less important channels in CNN models. CHIP incorporates inter-channel information to prune the convolutional neural networks efficiently. It identifies channel importance using nuclear norms on their feature maps and selects channels that have feature maps with less information for pruning. The authors' experiments show that CHIP produces high pruning performance while preserving or slightly increasing the model accuracy. Thus, this paper uses CHIP with CCM loss to prune the backbone network and the primary capsule layer during model training."}, {"title": "3 Design and Methodology", "content": "A simple CapsNet [5] is a shallow network consisting of three layers: a convolutional layer, a primary capsule layer, and a digit capsule layer. The first is a 2D convolutional layer to extract the feature maps from an input image. The second is a primary capsule layer (Primary Caps), which is also a convolutional layer, and the output of this layer contains n-dimensional vectors for all the individual capsules in any channel, unlike the scalar-valued feature maps generated by the 2D convolutional layer. The final layer is called the digit capsule layer (DigitCaps), as it contains as many capsules as the number of digit classes. The DigitCaps has a transformation matrix trained to map the output of all the capsules in the PrimaryCaps to the digit class capsules and uses a dynamic routing algorithm to produce the final vector output for each digit class capsule."}, {"title": "3.1 Model Architecture", "content": "Our model is similar to the CapsNet model proposed by [5] with two main differences. First, we add more convolutional layers to the backbone to increase its feature extraction ability and use a different loss to facilitate structured filter pruning performance similar to [15]. Since the backbone network is responsible for detecting local features in the input image and passing it to the Primary Caps layer through the generated feature maps, we wanted to give our network more capabilities to detect these local features. Therefore, instead of using a 2D convolutional layer for the backbone network, we used three convolutional layers. Each layer has 3\u00d73 convolution kernels, a padding of 1, ReLU activation and batch normalization. Furthermore, we used a stride of 1 for the first and third convolutional layers and 2 for the second convolutional layer.\nThe PrimaryCaps layer is another convolutional layer with a kernel of 3\u00d73, stride of 2, zero padding, and squashing activation. It has C\u00d7D output channels, where C is the number of capsules, and D is the capsule's dimension in the layer. The layer generates the convolutional output of width and height, denoted by W and H, respectively, as shown in Fig. 1. Thus, the layer produces the W\u00d7H\u00d7C output capsules of D-dimension. The ClassCaps layer is the last layer of the CapsNet architecture used in this paper and is similar to the DigitCaps [5]. Its"}, {"title": "3.2 Dynamic Routing", "content": "The dynamic routing algorithm is at the core of our CapsNet, which routes the outputs of the lower-layer capsules to the appropriate capsules in the layer above using an iterative process as presented in equation (1), where $b_{ij}$ is the logits calculated between capsules i in the lower layer and capsules j in the layer above for the rth iteration.\n$b_{ij}^{(r+1)} = b_{ij}^{(r)} + \\hat{u}_{j|i} \\cdot squash (s_j)$ (1)\n$\\hat{u}_{j|i} = W_{ij}u_i$ (2)\nThe logits are initialized to zeros and iteratively updated by adding the product of $\\hat{u}_{j|i}$ and squashed $s_j$. $\\hat{u}_{j|i}$ is obtained by multiplying capsule output $u_i$ from the lower layer with the transformation matrix $W_{ij}$ of the layer above, shown"}, {"title": "3.3 Loss Function", "content": "This paper uses a combination of two loss functions for model training: the capsule margin loss [5] and the CCM loss [15]. The total loss function is calculated by subtracting a small portion of the total CCM loss from the total capsule margin loss, as shown in equation (5), where the first loss term is for the total margin loss and the second is for the CCM loss.\n$L_{total} = \\sum_{n}^{N} Loss_{margin\\; n} - \\alpha \\sum_{n}^{N} Loss_{ccm}^{(l)}$ (5)\nThe total margin loss of the model is obtained by adding the individual margin losses of all the N class capsules in the Class Caps layer. For each class capsule n, a separate margin loss is calculated using the equation (6), where $||v_n||$ is the norm of the M-dimensional output vector of the nth class capsule, $T_n$ is 1 when the class n is present in the capsule output and 0 otherwise, $t_{pos}$ = 0.9, $t_{neg}$ =\n0.1, and $\\alpha$ = 0.5.\n$Loss_{margin \\; n} = T_nmax(0, t_{pos} - ||v_n||)^2 + \\lambda(1 \u2013 T_n)max(0, ||v_n|| \u2013 t_{neg})^2$ (6)\n$Loss_{ccm}^{(l)} = \\frac{1}{C^{(l)} \\times C^{(l)}} \\sum_i \\sum_j  corr (f_i^{(l)}, f_j^{(l)})|$ (7)\nThe total CCM loss is calculated by adding the CCM losses of all the convolutional layers in the backbone network and the Primary Caps. This loss enables the convolutional layer to learn linear relationships between the feature maps"}, {"title": "3.4 Pruning", "content": "In order to mitigate the vanishing activations observed in the deep CapsNet, the convolutional channels in the backbone network and the Primary Caps layer are pruned. Moreover, for the effective pruning operation, the model is trained with CCM loss before pruning, which is essential to ensure that enough linear redundancy exists among the channels of a convolutional layer before their channel independence (CI) scores are calculated using the CHIP algorithm.\nA channel CI score reflects the effect of omitting that channel feature map from the set of all feature maps of a layer. According to the CHIP algorithm [14], the nuclear norm of all channels in a convolutional layer is calculated first. Then, one channel is selected for masking at a time, and the nuclear norm of all the channels, along with the masked channel, is re-calculated. The CI score of the selected channel c is computed as the difference between the two nuclear norms, as outlined in equation (8), where $f^{(l)}$ represents the feature maps of convolutional layer 1, $f_c^{(l)}$ denotes the feature map of channel c in convolutional layer 1, and $M_c$ is a masked matrix of similar size as $f^{(l)}$, containing ones in all rows except the cth row, which is filled with zeros. The masking of the cth row in $f^{(l)}$ is carried out by element-wise multiplication of $f^{(l)}$ with $M_c$. However, to obtain the final score, the CI scores of all selected channels are averaged across all batches in the training dataset.\n$CI(c) = || f^{(l)} ||_* - || f^{(l)} \\odot M_c||_*$ (8)\nThe CI scores for all the channels in all the convolutional layers of the backbone network and the Primary Caps layer are computed. The lower CI score of any channel in the layer signifies the least independent channel and contains less information, so it can be pruned without impacting the network. Hence, the CI scores for all the channels of convolutional layers are then sorted in ascending order, and the channels with the lowest CI scores are selected for pruning from the ordered list of batch-averaged CI scores.\nThe number of channels to be pruned is decided using a pruning ratio. However, to ensure enough channels are available after pruning to rebuild the Primary Caps layer with a pre-defined capsule dimension D, the channels in all the convolutional layers and the PrimaryCaps layer are pruned in multiple of D. For example, if the number of channels is 12 and the pruning ratio is 0.25, ideally,"}, {"title": "3.5 The Interplay between Vanishing Activation and Pruning", "content": "We built and trained a base CapsNet (BaseCapsNet) model with a backbone network consisting of three convolutional layers, a Primary Caps layer, and a ClassCaps layer over multiple epochs using the total margin loss given by equation (6). The model with the highest accuracy is selected and then undergoes re-training with the total loss function defined in equation (5), which includes both margin and CCM losses because CCM supports structure pruning. As illustrated in equation (8), CI scores are calculated for all the convolutional channels in the network using the CHIP algorithm, and the channels with the lowest CI scores are then identified to be pruned.\nAfter setting the pruning ratio, the lowest-scored channels of the convolutional layers in both the backbone network and the Primary Caps layer of the best pre-trained model are pruned. Then, the model is rebuilt using the remaining channels of the backbone network and the primary capsules with a new ClassCaps layer, which ensures the rerouting of the pruned Primary Caps to appropriate ClassCaps. The pruned CapsNet model is then retrained over multiple epochs - using only the total margin loss given by equation (6). The model with the highest test accuracy, named the best-pruned model, is saved for later comparison of the performance of different pruning ratios. The process of setting the pruning ratio, carrying out model pruning, followed by model rebuilding, model training, and validation, is repeated multiple times for different pruning ratios of choice.\nTo recreate the vanishing activations issue typically seen in the CapsNet [13], Intermediate Caps layers are added one by one between the Primary Caps and the Class Caps layers. At first, one Intermediate Caps layer is inserted, and the model is trained using the total margin loss. After training, if the validation accuracy is quite reasonable and changes considerably over the epochs, an additional Intermediate Caps layer is inserted, and the new model is trained. During model training, the outputs of the capsules in an Intermediate Caps layer are averaged over all the input images, and then the Frobenius norm is calculated along the capsule dimension to obtain individual capsule activations. When the activation value is less than or equal to 0.01 for a capsule, it is pronounced dead.\nThe Intermediate Caps layers are continuously added until the model accuracy reduces to a meagre value (~ 10%) over multiple epochs. When a very low non-recovering validation accuracy is obtained, the model is assumed to be broken due to infinitesimal capsule activations in the Intermediate Caps layers. Once a model breaks due to vanishing activations, the backbone network and"}, {"title": "4 Experimental Results and Discussions", "content": "Six different base model configurations without any IntermediateCaps layers are pruned with seven different pruning ratios (PR) to analyze the effect of pruning on model performance. The model accuracies obtained for different pruned model configurations are summarized in Table 1. Three important inferences can be made from the table. Firstly, for all six model configurations, when the pruning ratio is increased from 0.125 to 0.75, the model accuracy increases at the beginning, reaches a peak and then starts plummeting. For instance, the unpruned [C=10, D=8] model accuracy is 73.92% (C is the number of capsules and D is the dimension of the capsules). As the pruning ratio increases, the model accuracy also increases and reaches a peak accuracy of 74.71% at a pruning ratio PR=0.5. Then, the accuracy falls to 73.84% (which is below the unpruned model accuracy) when the PR is 0.75. Thus, the pruning ratio must be chosen wisely to achieve the best model performance.\nSecondly, keeping the capsule dimensions D fixed when the number of capsules C is doubled, the model accuracy always increases irrespective of the pruning ratio set. It is evident that when D is 8 and C is changed from 10 to 20, the model accuracy goes up from 74.26% to 78.71% at PR=0.125. Similarly, when D is 12 and C is doubled from 8 to 16, the model accuracy improves from 75.44% to 78.43% at PR=0.375.\nThe third and last point is when the number of capsules C is fixed, and the number of capsule dimensions is doubled, the model accuracy increases de"}, {"title": "4.1 Effect of Intermediate Capsule Layers", "content": "Up to ten IntermediateCaps layers (I) are added for four different model configurations to assess the advantages of going deeper with CapsNet. Adding Intermediate Caps layers to CapsNet models increases the model accuracy for most of the models up to five IntermediateCaps layers, which is evident in Table 2. On another note, after adding seven IntermediateCaps layers to CapsNet[C=20, D=8], the validation accuracy of the model drastically drops to 10% showing the model's incapability to learn further. Thus, the CapsNet[C=20, D=8] model is selected for the in-depth evaluation of the vanishing activations."}, {"title": "4.2 Exploring Vanishing Activations", "content": "To view the vanishing activations in the CapsNet[C=20, D=8] models with 6 and 7 Intermediate Caps layers, heatmaps are plotted for the capsule activations of the Intermediate Caps layers as shown in Fig. 2. Unlike the case of the CapsNet[C=20, D=8] with I=6 Intermediate Caps layers, where most of the capsule activations in layers 5 and 6 are close to 0.01, for the CapsNet[C=20, D=8] with I=7 Intermediate Caps layers, all the capsule activations in all the Intermediate-Caps layers are found to be zero. When the model breaks in the case of I=7, all the capsules in the Intermediate Caps layers are dead, resulting in a poor validation accuracy of 10%. Increasing the number of intermediate capsule layers beyond seven does not help recover the dead capsules, and hence, validation accuracy remains at 10% thereafter. In addition, Fig. 3 shows how the percentage of dead capsules increases for CapsNet[C=20, D=8, I=6] over the training epochs while validation accuracy mostly stays around 76% beyond epoch 6."}, {"title": "4.3 Going deeper with pruning", "content": "From Table 3, it can be inferred that the validation accuracies of the Base-CapsNet[C=20, D=8] pruned with pruning ratios (PR) of 0.125, 0.25 and 0.375 are better than the validation accuracy achieved without pruning. Hence, 0.125, 0.25 and 0.375 pruning ratios are only used to overcome the vanishing activations problem."}, {"title": "4.4 Other Experiments", "content": "Different Kernel Sizes. Table 4 presents the accuracies obtained by the BaseCapsNet built with different convolutional layers for the backbone network and the Primary Caps. The data in Table 4 shows that maximum accuracy is achieved for kernel size 5 and 32 8D capsules. However, training the [C=32, D=8]model with a kernel size of 5 and multiple intermediate layers became computationally expensive due to resource constraints. As a result, models with at most 20 capsules and a kernel size of 3 are selected for this project."}, {"title": "5 Conclusion and Future Work", "content": "In this paper, Capsule Networks with varying numbers of capsules and capsule dimensions are pruned using six different pruning ratios. Channel pruning not only reduced the number of model parameters and computational complexity but also significantly increased the model accuracy. When up to 10 intermediate layers are added to four different unpruned models to assess the benefits of building deeper Capsule Networks, a slight improvement in the model accuracies is observed. For a CapsNet with a large number of capsules, it is found that when the number of capsule dimensions is reduced, most of the capsule activations shrink to zero. For example, the model with 20 capsules of 8 dimensions breaks down after adding 7 intermediate layers, whereas the model with 10 capsules of 8 dimensions does not break even after adding 30 intermediate layers. Finally, in this paper, CCM loss-based channel pruning is used to mitigate the vanishing capsule activations in deeper Capsule Networks, revealing that a pruned model has fewer dead capsules than a deep unpruned model. In conclusion, increasing the pruning ratio allows for building deeper Capsule Networks without compromising model performance. In the future, bigger models can be built to generalize channel pruning's contribution to mitigating vanishing activations, as this paper already shows for relatively small Capsule Networks. Additionally, it would be exciting to see how other pruning techniques mitigate vanishing activations in deeper Capsule Networks. The source code for this project is available on the GitHub link."}]}