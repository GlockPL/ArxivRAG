{"title": "LUNAR: LLM Unlearning via Neural Activation Redirection", "authors": ["William F. Shen", "Xinchi Qiu", "Meghdad Kurmanji", "Alex Iacob", "Lorenzo Sani", "Yihong Chen", "Nicola Cancedda", "Nicholas D. Lane"], "abstract": "Large Language Models (LLMs) benefit from training on ever larger amounts of textual data, but as a result, they increasingly incur the risk of leaking private information. The ability to selectively remove knowledge from LLMs is, therefore, a highly desirable capability. In this paper, we propose LUNAR, a novel unlearning methodology grounded in the Linear Representation Hypothesis. LUNAR operates by redirecting the representations of unlearned data to regions that trigger the model's inherent ability to express its inability to answer. LUNAR achieves state-of-the-art unlearning performance while significantly enhancing the controllability of the unlearned model during inference. Specifically, LUNAR achieves between 2.9x to 11.7\u00d7 improvements on combined 'unlearning efficacy' and 'model utility' score ('Deviation Score') on the PISTOL dataset across various base models. We also demonstrate, through quantitative analysis and qualitative examples, LUNAR's superior controllability in generating coherent and contextually aware responses, mitigating undesired side effects of existing methods. Moreover, we demonstrate that LUNAR is robust against white-box adversarial attacks and versatile in handling real-world scenarios, such as processing sequential unlearning requests.", "sections": [{"title": "1. Introduction", "content": "Large Language Models (LLMs) exhibit exceptional natural language generation capabilities, supporting diverse applications (Brown et al., 2020; Achiam et al., 2023) and often achieving near-human quality (Abdali et al., 2024). However, their outputs are not always appropriate, as they may include personally identifiable information (PII), exhibit bias or toxicity, or contain inaccurate or outdated information (Kotek et al., 2023; Motoki et al., 2023; Bender et al., 2021; Wen et al., 2023; Nasr et al., 2023; Barrett et al., 2023). Furthermore, there is a growing need for LLMs to be tailored to local or private datasets while complying with regulations such as the General Data Protection Regulation (GDPR), which enforces the 'right to be forgotten'. Addressing these challenges through extensive data management or model retraining is impractical due to the enormous size of training datasets and the substantial computational costs involved (RD). Consequently, it is increasingly critical to develop efficient LLM unlearning methods to address privacy, security, and commercial concerns effectively (Conneau et al., 2019; Wenzek et al., 2019).\nExisting unlearning methods often claim success by producing outputs that deviate from the ground truth for the data designated for forgetting while maintaining accuracy on the remaining data (Liu et al., 2024). However, they do not always achieve a desirable balance of these two objectives, as evidenced by insufficient unlearning or significant degradation in the model utility (Qiu et al., 2024).\nMoreover, while existing unlearning methods focus on balancing unlearning efficacy and retained model utility, they often neglect other undesirable side effects (Yao et al., 2023; Wang et al., 2024; Liu et al., 2024; Blanco-Justicia et al., 2025), including hallucinations, rigid and monotonous responses (as opposed to dynamic and contextually aware ones), or the generation of nonsensical outputs when being prompted with unlearned data (Figure 1(a)). We term this problem a lack of controllability. We define controllability as the unlearned model's ability to explicitly convey its inability to respond while ensuring that the generated text remains dynamic, contextually aware, and coherent. We advocate for incorporating controllability as a key criterion in the evaluation of LLM unlearning methods in future studies.\nAdditionally, widely adopted unlearning methods, whether gradient-ascent-based (Jang et al., 2022; Yao et al., 2023; Liu et al., 2022) or preference-optimization-based (Rafailov et al., 2024; Zhang et al., 2024b), are associated with high computational costs (Sec.3.3), particularly as LLMs scale up. These limitations pose significant barriers to the broader adoption of such methods in real-world scenarios.\nTo address the limitations of current LLM unlearning methods, we propose a novel method LUNAR. It leverages recent"}, {"title": "2. Background", "content": "Transformers We focus on transformer architecture and let Z denote an input space (e.g., sequences of tokens), $c \\in$ N+ the number of classes (e.g., vocabulary size), $Y = R^C$ the output logit space, and $d \\in N+$ the hidden dimension. We consider the following functions $q : Z \\rightarrow Y$:\n$q = v \\circ h_L$, where $h_L : Z \\rightarrow R^d$, $h_l = \\beta_l \\circ h_{l-1}$, where $l \\in [L]$ \nwhere $L \\in N+$ is the number of residual blocks (i.e., layers), $\u03b7 : Z \\rightarrow R^d$ is the token embedding, and $circ$ denotes repeated functional composition. The residual blocks $\u03b2_l : R^d \\rightarrow R^d$ for $l \\in [L]$ and the output decoding module $v : R^d \\rightarrow Y$ are defined as:"}, {"title": "3. LUNAR", "content": "In this section, we introduce LUNAR method and layer selection in Secs.3.1 and 3.2, and conclude with an analysis of LUNAR's memory and computational costs. Algorithm pseudo-code can be found in Appendix A."}, {"title": "3.1. Unlearning via Neural Activation Redirection", "content": "Previous works (Panickssery et al., 2023; Marks & Tegmark, 2023) have shown that contrastive features can be delineated by computing the 'steering vector': $r = a(x) \u2013 \\bar{a}(y)$ (i.e. the difference in mean residual stream activations a between pairs of positive x and negative y examples of a particular behavior). These steering vectors have significant implications for influencing model behavior. For instance, a 'steering vector' computed out of contrastive harmful versus harmless prompts can be added to the residual stream activations of harmful prompts to circumvent the model's safety guardrails (Arditi et al., 2024a).\nHowever, given the remarkable ability of transformer architectures to aggregate information and capture abstract representations through high-dimensional residual stream activations, particularly in intermediate layers (Grosse et al., 2023; Dwivedi & Bresson, 2020), we conjecture that it is not strictly necessary for two features to be explicitly contrastive in a human-comprehensible sense to compute and utilize 'steering vectors'. Instead, those can be employed more generally to map a shared hidden feature underlying one group of prompts (i.e., the source feature abstracted by the transformer in intermediate layers) to another group of prompts (i.e., the target feature). We term this process 'Activation Redirection'. This mapping can effectively trigger the model to resemble the behavior associated with the target feature.\nIn the context of LLM unlearning, the objective is to create an unlearned model that closely mimics the behavior of a retrained model, which explicitly and lively communicates its inability to respond to prompts related to the forget set. To achieve this, we redirect the activations of forget set across all token positions to activations representing the state of inability as follows:\n$a'(x) \\leftarrow a(x) + r_{UV}^{(l)}$\nwhere $r_{UV}^{(l)}$, the unlearning vector (UV) as a linear intervention in the residual stream activations, is defined as below:\n$r_{UV}^{(l)} = \\frac{1}{|D_{ref}|} \\sum_{x \\in D_{ref}} a^{(l)}(x) - \\frac{1}{|D_f|} \\sum_{x \\in D_f} a^{(l)}(x)$ \nIn Eq. 5, $D_f$ is the forget set and $D_{ref}$ is a set of reference prompts associated with the target feature. In one instance, provided the base model is safety-aligned, $D_{ref}$ can be the prompts that activate the model's internal safety mechanisms to state its inability to positively engage with the unlearned queries. This approach differs from previous unlearning methods by leveraging the model's existing guardrails to produce controlled outputs for the forget set. Alternatively, we observed that the latest LLMs are capable of stating 'a lack of knowledge' to fictitious prompts (such as 'What is the capital of the country $7\\&a\\#!'). As such, Dref can be a set of fictitious prompts. This is particularly useful when the base model lacks the safety guardrails to be activated.\nAs outlined before, the unlearning task necessitates a dual focus: ensuring effective forgetting performance while preserving the model's utility. In this sense, the training objective is two-fold. First, it ensures the activations of the forget set are redirected to the perturbed activation according to Eq. 4. Second, it restricts the retain set from moving away from the original activation space. Therefore, the loss can be defined below:\n$L_{LUNAR} = \\begin{cases} \\sum_{x\\in D_f} ||a^{(l)}(x) - a'_f(x)||^2, \\text{ if } x \\in D_f \\\\ \\sum_{x\\in D_r} ||a^{(l)}(x) - a^{(l)}_r(x)||^2, \\text{ if } x \\in D_r \\end{cases}$\nIn light of the pivotal role of MLP layers in storing knowledge within transformer-based LLMs (Meng et al., 2022) functioning as memory banks where values are stored and accessed by $W_{out}$ through subject tokens acting as keys we focus on the down-projection of MLPs as the target for weight optimization in the unlearning process. We, therefore, set the residual stream activations in Eq.4 and 5 as the MLP output and optimize the weights of the down-projection layer using Eq. 6 while keeping the rest frozen."}, {"title": "3.2. Layer Selection", "content": "In the transformer architecture, different residual blocks (i.e., layers) exhibit distinct generalization patterns (e.g., intermediate layers accumulate information and capture more abstract representations) (Grosse et al., 2023). As a result, activation redirection is intuitively most effective when performed in the middle layers of the model. To identify the exact layer, two primary objectives are considered: (1) the model should most effectively state its inability to respond, and (2) the response should correctly convey its reason.\nTo assess the first objective, prior work computes a binary refusal score by string-matching common 'refusal substrings' (e.g., \"I'm sorry\u201d or \u201cAs an AI\u201d) (Robey et al., 2023; Lermen et al., 2023; Liu et al., 2023) or uses the probability of 'I' as the first token as a proxy for refusal (Arditi et al., 2024a). However, the substring-matching approach may fail to evaluate the lexical or semantic coherence of the responses (Huang et al., 2023; Meade et al., 2024; Qi et al., 2023), while we found the token-probability method can lead to gibberish-like responses of multiple 'I's as the probability of 'I' increases. Thus, we propose an alternative by computing the cosine similarity (\u03b41) between the sentence-level embeddings of the generated responses and desired responses (e.g., 'I apologize that I don't have access to this information that you are asking for').\nAdditionally, to ensure the responses correctly convey the intended reason, we simultaneously minimize the cosine similarity (\u03b42) between the embeddings of the response and reasons unrelated to the unlearning task (e.g., harmfulness, danger, or unethicality). Overall, we select the layer that maximizes (\u03b41 - \u03b42), thereby ensuring effective convey of the unlearned model's inability to respond in a coherent and contextually appropriate manner."}, {"title": "3.3. Memory and Computational Costs", "content": "The cost of unlearning methods is critical for determining their adoption. Unlike previous proposals that require retraining the full model or a reduced representation while keeping the full model frozen, e.g., LoRA-based methods, LUNAR only requires training a single down-projection layer. As such, LUNAR's memory footprint is represented by the frozen full model during procedures 1 and 2 (see Algorithm 1) and a single dense layer during procedure 3. This extreme reduction of the trainable parameters goes beyond a lower impact on the memory, resulting in significant computational efficiency. In practice, reducing the memory footprint allows for the use of more data examples per step, which results in higher throughput (Mao et al., 2024).\nThe number of trainable parameters for a LoRA-based method are $N_{LORA} = 2 \\cdot L \\cdot (d_{model} \\cdot r + r \\cdot d_{ff})$, where L is the number layers, $d_{model}$ is the input dimension of a feed- forward network, $d_{ff}$ is the hidden dimension of the feed-forward network, and $r$ is the low rank of LORA. For LUNAR applied on K layers using LoRA modules, the number of trainable parameters are $N_{LUNAR} = K \\cdot (d_{model} \\cdot r + r \\cdot d_{ff}) = \\frac{2K}{L} N_{LORA}$. Since $K \\leq L$ or, in most cases, $K = 1$, $N_{LUNAR} < N_{LORA}$.\nAs in previous works (Kaplan et al., 2020) assuming standard optimization conditions, the computational cost per token (FLOPs/token) $C$ for training an LLM is estimated as $C \\approx 6N$, where $N$ is the total number of trainable (non-embedding) parameters. Fully frozen blocks in a transformer, like those used in LoRA, execute only the forward pass contributing for $C_{fwd} \\approx 2N$ FLOPs per token. The LORA modules execute both forward and backward passes for the total cost of $C_{LORA} = 6N_{LORA}$. LUNAR during the first two procedures (see Algorithm 1) executes a complete forward pass on the full frozen model for each of the K layers sampled at the cost of $C_{LUNAR|1,2} = 2NK$ FLOPs per token. For training the K down-projection layers (using their LoRA modules) during the third step of LUNAR (see Algorithm 1), the FLOPs per token can be estimated as $C_{LUNAR|3} = 6 \\cdot K \\cdot (d_{model} r + r \\cdot d_{ff}).$"}, {"title": "4. Analytical Solution and Convergence Study", "content": "In transformer architectures, the down-projection layer functions as a fully connected layer without activation functions. By framing the optimization objective for this layer with $L_{LUNAR}$, a closed-form solution can be derived analytically, implying its convergence.\nLet n and m denote the number of tokens in the forget set and the retained set, respectively. The input dimension of the selected down-projection layer is represented by p, while q be the output dimension. Hidden states before the down-projection layer are therefore $H_f = [h_{1,f}, h_{2,f}, ..., h_{n,f}] \\in R^{n \\times p}$ for the forget set and $H_r = [h_{1,r}, h_{2,r}, ..., h_{m,r}] \\in R^{m \\times p}$ for the retained set, where $h_{i,f}$ and $h_{i,r}$ are p-dimensional vectors representing each token in the forget and retained set respectively. Let the original MLP output activations be $A_{origin} = [a_{1,f}, a_{2,f}, ..., a_{n,f}] \\in R^{n \\times q}$ and $A_{origin} = [a_{1,r}, a_{2,r}, ..., a_{m,r}] \\in R^{m \\times q}$. LUNAR introduces a redirection in the activation space for the forget set, resulting in $A_f = [a_{1,f} + r_{UV}, a_{2,f} + r_{UV}, ..., a_{n,f} + r_{UV}]$, while the activations for the retained set remain unchanged, i.e., $A_r = [a_{1,r}, a_{2,r}, ..., a_{m,r}].$\nThe objective is to optimize the weights of down-projection layer $W_{out}$ to minimize the distance between the redirected MLP output and the original output, as follows:\n$W = \\text{arg min}_W ||[H_f, H_r]W - [A_f, A_r]||_2$\nOne can show that there exists a unique solution in the following form: (Proofs of the closed-form solution B.1 and"}, {"title": "5. Experiment", "content": "We propose our method as a novel, robust, and efficient alternative for LLM unlearning. In this section, we conduct experiments to evaluate LUNAR's performance, focusing on the following research questions:\nRQ1 Does LUNAR improve unlearning performance while maintaining model utility? (Sec.5.2)\nRQ2 Does LUNAR improve the controllability of LLM unlearning via generating dynamic, contextually aware and coherent responses? (Sec.5.2)\nRQ3 IS LUNAR versatile in handling real-world applications, including unlearning data from different training stages and handling sequential unlearning tasks? (Secs.5.3 and 5.4)\nRQ4 IS LUNAR robust against various adversarial attacks, both white-box and black-box? (Sec.6)"}, {"title": "5.1. Experimental Setup", "content": "Datasets We evaluate LUNAR on two LLM unlearning benchmark datasets: TOFU (Maini et al., 2024) and PISTOL (Qiu et al., 2024). These datasets are specifically tailored for studying LLM unlearning in a controlled environment, featuring fictitious entities to mitigate confounding risks with data from the pre-training corpus. In addition to evaluating unlearning synthetic PISTOL or TOFU data from fine-tuned models (SFT data), we also examine LUNAR'S effectiveness on unlearning pre-trained data from the base model by utilizing a factual dataset provided by (Maini et al., 2024), which consists of common knowledge (e.g., 'Who wrote the play Romeo and Juliet').\nTo conduct activation redirection, we use either harmful prompts dataset (Arditi et al., 2024b) to activate the model's internal safety guardrails or an unverifiable prompts dataset that we composed using GPT-4 consisting of 200 questions about fictitious objects (e.g., imaginary countries, creatures, laws, etc.) to which the base model responds with apologies and an acknowledgment of its inability to provide an answer.\nMetrics. We evaluate unlearning effectiveness by assessing the forget efficacy (how much the unlearned model's outputs deviate from the forget data) and model utility"}, {"title": "5.2. Unlearning Performance", "content": "Table 2 shows that LUNAR achieves SOTA unlearning performance, as evidenced by lower deviation scores (up to 11.7x reduction vs. LUNAR (base) on the PISTOL dataset with Gemma-7B model) and higher refusal quality scores. Additionally, examples in Table 1 and Appendix E.1 further visualize LUNAR's superior controllability, significantly reducing hallucinations and improving the coherent expression of its inability to respond within the conversational context."}, {"title": "5.3. Unlearning Pre-trained Data from Base Models", "content": "We observe that modern LLMs exhibit, to some extent, an ability to express a lack of knowledge when prompted with fictitious or unverifiable questions. This ability is often significantly stronger in pre-trained models compared to SFT models. While unlearning SFT data is more effective through the redirection of residual stream activations to those of harmful features, unlearning pre-trained data is equally effective by redirecting the activations of the forget set to those either associated with the harmful prompts or unverifiable prompts. The effectiveness of LUNAR in unlearning pre-trained data is presented in Table 3."}, {"title": "5.4. Unlearning Sequentially", "content": "Another practical scenario in LLM unlearning deployment involves private data being removed incrementally over time, as unlearning requests arrive sequentially. Table 9 (Appendix E) demonstrates that LUNAR is robust to handle sequential unlearning, whereas baseline methods exhibit brittleness when unlearning additional data on top of an already unlearned model. LUNAR consistently achieves strong results across different models, comparable to the performance observed in single-round unlearning."}, {"title": "6. Robustness Study on LUNAR", "content": "Given the nascent nature of LLM unlearning, its evaluation scope may sometimes be overly optimistic on the efficacy of unlearning methods, particularly as some simple yet effective attacks can yield unexpected results, revealing information that should have been successfully unlearned (Thaker et al., 2024; Liu et al., 2024; Zhang et al., 2024c). Therefore, we advocate for incorporating robustness studies in future research to ensure the integrity of the unlearning process.\nIn this section, we show the robustness of LUNAR through three white-box attacks, which operate under strong assumptions that the attacker at least possesses full knowledge of the model weights. Such attacks are of less concern if LUNAR is performed on closed-source models. We also show LUNAR is robust against prompt paraphrase attack which could target both white and black-box scenarios. Notably, when combining LoRA with LUNAR, we merge the updated parameters back into the model to prevent adversarial attackers from bypassing the unlearning process by simply removing the modified model weights."}, {"title": "6.1. Layer Skip Attack", "content": "The layer skip attack is designed to bypass the layers where MLPs have been optimized for activation redirection. We hypothesize that this attack is effective due to the ensemble nature of transformer architectures (Veit et al., 2016; Chen et al., 2024), where the final prediction can theoretically be interpreted as an ensemble of diverse computational paths. Empirically, a number of recent works use layer skipping to accelerate inference speed (Chen et al., 2020; Fan et al., 2020; 2024; Elhoushi et al., 2024). In this work, however, we repurpose layer skipping to evaluate the robustness of our unlearning method against malicious attacks.\nTo address this, we perform activation redirection on the top-K layers identified through the layer selection process. For Llama2-7B model, selecting top-3 layers is an effective defense with the ROUGE-1 score only increasing marginally to c.0.1 (Table 4), indicating a minimal recovery of un-"}, {"title": "6.2. Reverse Direction Attack", "content": "This attack strategy assumes a 'white-box' attacker has full knowledge of the layer selection and the exact Unlearning Vectors (UVs) $r_{UV}^{(l)}$ used in the unlearning process. In this case, the attacker performs reverse engineering in order to recover pre-unlearning activations by ablating the UV from post-unlearning activations of the selected layer. This is achieved by doing: $a_{attack}^{(l)}(x) \\leftarrow a_{unlearned}^{(l)}(x) - r_{UV}^{(l)}$.\nWe report the attack results in Table 4, demonstrating that it is ineffective against the LUNAR unlearned model. We hypothesize that this robustness arises because the activation region corresponding to the refusal behavior is significantly broader than the specific activation region associated with the forget set. As for the forget set, knowledge is more precise, and even a small divergence from the correct answer can result in incorrect responses. This broader region, combined with the stochastic nature of the unlearning optimization process, prevents the loss from fully converging to zero during training. As a result, reversing the activation redirection process fails to map the activations back to their original state, thereby rendering the attack ineffective."}, {"title": "6.3. Quantization Attack", "content": "Unlearning methods tend to be subject to 'utility' constraints, which require that the model retain as much of its original functionality and performance as possible on data that was not marked for unlearning. As recently observed by Zhang et al. (2024c), since the original models are finely converged, methods from the GA and PO families"}, {"title": "6.4. Prompt Paraphrase Attack", "content": "A common limitation in evaluating existing unlearning methods is their focus on accuracy degradation for queries directly related to the forget set. However, effective unlearning must generalize to similar samples sharing characteristics with the forget set, ensuring the process extends beyond specific instances to broader concepts. This enhances robustness, particularly against paraphrasing attacks (Thaker et al., 2024; Yao et al., 2023). To evaluate this, we compiled a set of paraphrased prompts from the PISTOL dataset using GPT-4 and ran inference on the LUNAR unlearned model. Table 4 demonstrates that paraphrased prompts fail to extract unlearned information from the LUNAR unlearned model, showcasing its robustness against such attacks."}, {"title": "7. Related Works", "content": "Machine Unlearning Machine unlearning is gaining recognition for its significance and potential, yet it remains a relatively under-explored field. Recent studies (Chen & Yang, 2023; Jang et al., 2022; Ilharco et al., 2022; Zhang et al., 2023) have begun to address aspects of text generation within this context. Prior research (Qiu et al., 2024; Maini et al., 2024) has highlighted the limitations of current unlearning methods, noting their extreme sensitivity to hyperparameter tuning and a lack of robustness in structural unlearning. These challenges complicate their deployment in practical, real-world applications. Moreover, several survey papers (Liu et al., 2024; Nguyen et al., 2022) have"}, {"title": "8. Conclusion", "content": "We propose LUNAR, a simple and effective LLM unlearning method that balances unlearning efficacy and model utility while mitigating common side effects, particularly the lack of controllability. LUNAR achieves SOTA unlearning performance, enabling models to coherently and contextually express their inability to respond. Additionally, through theoretical analysis, we prove the existence of a closed-form solution for LUNAR that ensures convergence and demonstrates its significant computational efficiency gains. Empirical analysis further demonstrates LUNAR's robustness against adversarial attacks and its versatility in addressing real-world applications."}, {"title": "Impact Statement", "content": "This paper is motivated by the social consequences of recent advances in the field of machine learning and large language models (LLMs). LLMs have made significant strides by pre-training on and memorizing vast amounts of textual data. However, this process can raise privacy concerns and potentially violate data protection regulations. Consequently, the ability to efficiently remove data related to individual users from these models, without compromising their predictive quality, is becoming increasingly important.\nWe aim to provide a better and more efficient method to tackle this problem and enhance privacy considerations in this field.\nOverall, we believe the potential positive social benefits of our work in LLM unlearning outweigh the potential negatives, which stem primarily from misuse."}, {"title": "Appendix", "content": "A. Algorithm\nAlgorithm 1 LUNAR: Unlearning via Neural Activation Recalibration\nRequire: Let Df be the forget set; D\u2081 be the retained set; Dref be the reference dataset.\nProcedure 1: Compute Unlearning Vectors (UV)\nGiven Df and Dref, calculate activation mean\nProcedure 2: Layer Selection\nSelect the layer (according to Sec. 3.2) where activation redirection is most effective in producing controlled outputs that accurately express the model's inability to respond while correctly conveying the underlying reason, and store selected layers in set L\nProcedure 3: Optimize MLP down-projection in the selected layer to implement the desired recalibration\nB. Proofs\nLemma B.1. Let [Hf, Hr] \u2208 Rm\u00d7n (with m > n). The Gram matrix [Hf, H\u2084]T[Hf, H\u2084] is invertible if and only if the columns of [Hf, Hr] are linearly independent.\nProof. Let G = [Hf, H\u2084]T[Hf, Hr] be a Gram matrix, where G \u2208 Rn\u00d7n and Gij = <[Hf, Hr]i, [Hf, Hr]j), the inner product of column vectors [Hf, Hr]i and [Hf, Hr]j.\nSuppose G is not invertible, then there exists a nonzero vector v \u2208 Rn such that:\nB.1. Close-form solution of weight optimization\nWe have shown in Section 3.1 that the activation recalibration is equivalent to solving the following optimization problem:"}, {"title": "C. Experiments Setup", "content": "C.1. Dataset\nWe evaluate LUNAR and all baseline methods using the PISTOL dataset (Qiu et al., 2024) and TOFU dataset (Maini et al., 2024), both of which are designed specifically for assessing unlearning methods in LLMs.\nC.2. Metrics\nWe assess LUNAR and all baseline methods in terms of both the Unlearning Effectiveness and Refusal Quality."}, {"title": "D. Unlearning method baselines", "content": "We experiment with several unlearning methods summarized in the survey paper (Liu et al., 2024; Maini et al., 2024), each of which is introduced in detail in the section.\nGA-based methods. A major branch of LLM unlearning methods is built on the concept of performing Gradient Ascent (GA) on the forget data (Jang et al., 2022; Yao et al., 2023), which is mathematically equivalent to applying Gradient Descent on the negative cross-entropy loss function (Eq. 11). The objective of GA is to maximize the likelihood of mispredictions for samples in the forget set, effectively reducing the model's ability to recall or generate the unlearned information.\nPreference optimization-based methods. DPO (Rafailov et al., 2024) is a preference alignment method that aligns the model to avoid disclosing information from the forget set by computing loss using question-answer pairs. Unlike GA and its variants, DPO does not employ gradient ascent. Drawing inspiration from DPO, NPO (Zhang et al., 2024a) focuses on generating only negative responses to given instructions, without providing any positive or informative answers.\nHyperparameters. We combine both forget dataset and retain dataset and randomly select from them to form a mini-batch for all of LUNAR training. All baseline unlearning methods exhibit high sensitivity to learning rate tuning, necessitating"}, {"title": "E. Additional Experimental Results", "content": "E.1. TOFU Examples of Responses Post-Unlearning\nThe table below provides examples of responses generated after applying LUNAR and baseline methods on Llama2-7B fine-tuned with the TOFU dataset. These examples demonstrate that LUNAR significantly enhances the coherence and contextual awareness of responses compared to baseline methods."}, {"title": "E.3. Examples of Responses After Layer-Skip Attack", "content": "Table below presents the ground truth answers and the generations produced by the LUNAR-unlearned model after applying the Layer Skip attack on Llama2-7B and Gemma-7B models fine-tuned with the PISTOL dataset. While the post-attack model correctly guessed some binary-choice questions, the unlearned model remains largely non-usable on the forget set, as evidenced by inaccurate responses"}]}