{"title": "A Unified Debiasing Approach for Vision-Language Models across Modalities and Tasks", "authors": ["Hoin Jung", "Taeuk Jang", "Xiaoqian Wang"], "abstract": "Recent advancements in Vision-Language Models (VLMs) have enabled complex multimodal tasks by processing text and image data simultaneously, significantly enhancing the field of artificial intelligence. However, these models often exhibit biases that can skew outputs towards societal stereotypes, thus necessitating debiasing strategies. Existing debiasing methods focus narrowly on specific modalities or tasks, and require extensive retraining. To address these limitations, this paper introduces Selective Feature Imputation for Debiasing (SFID), a novel methodology that integrates feature pruning and low confidence imputation (LCI) to effectively reduce biases in VLMs. SFID is versatile, maintaining the semantic integrity of outputs and costly effective by eliminating the need for retraining. Our experimental results demonstrate SFID's effectiveness across various VLMs tasks including zero-shot classification, text-to-image retrieval, image captioning, and text-to-image generation, by significantly reducing gender biases without compromising performance. This approach not only enhances the fairness of VLMs applications but also preserves their efficiency and utility across diverse scenarios. The code is available on GitHub.", "sections": [{"title": "1 Introduction", "content": "Vision-Language Models (VLMs) have revolutionized the way we handle multimodal tasks by enabling simultaneous processing of text and image data. Models such as CLIP [31] and XVLM [43] serve as foundation models [41] for various downstream tasks demonstrating the remarkable versatility of these systems such as zero-shot classification and text-to-image retrieval. Additionally, models like BLIP [25] and CoDi [37] enhance this spectrum by facilitating tasks such as image captioning and text-to-image generation. These examples highlight the diverse capabilities of VLMs in adapting to various specific multimodal interactions.\nDespite their remarkable capabilities, VLMs have a critical bias issues, often skewing the model outputs in ways that reflect societal stereotypes [21] such as gender [46] or racial [45] biases in assigning professions or describing scenarios. For example, studies have identified biases in multi-class zero-shot classification [17], where models might disproportionately associate certain professions with specific genders. Similarly, biases in text-to-image retrieval [18, 39] can lead to the preferential retrieval of images that reinforce stereotypical narratives. The implications of these biases extend to image captioning [45, 20] and text-to-image generation [10], where the descriptive and generative capacities of VLMs may perpetuate and even amplify existing societal prejudices. These issues highlight a critical need for effective debiasing strategies that can ensure the fairness of VLMs applications."}, {"title": "2 Related Work", "content": "Recent research has focused significantly on identifying and evaluating bias in VLMs. Agarwal et al. [2] and Wolfe et al. [40] observed that CLIP [31] embeddings exhibit substantial racial and gender biases. Additionally, Chuang et al. [11] highlighted that text prompts can capture spurious features in visual representations, exacerbating the bias. Kim et al. [24] further investigated how the association between sensitive attributes and specific keywords contributes to bias issues in downstream tasks. These biases lead to unfair outcomes in zero-shot binary classification and text-to-image retrieval, as noted by [13, 18]. Moreover, Slyman et al. [35] extend the bias in zero-shot classification in multi-class setting using the FACET dataset [17] and its evaluation metric. Seth et al. [34] suggested a new metric for fairness in text-to-image retrieval, considering the gender distribution in the query set. Beyond zero-shot classification and text-to-image retrieval, other downstream tasks also reveal biases. For instance, Hirota et al. [20] and Zhao et al. [45] investigated bias in image captioning, where specific genders or races are disproportionately represented leading to generate biased caption. Similarly, Cho et al. [10] raised concerns about bias in text-to-image generation and suggested evaluation methods. Finally, Sathe et al. [32] emphasized the need for a unified evaluation approach for various downstream tasks to address these pervasive biases comprehensively.\nAs bias issues have arisen, many debiasing methods have been proposed. Zhang and R\u00e9 [44] trained an adapter on frozen representations to debias spurious correlations in zero-shot binary classification. Additionally, Chuang et al. [11], Adila et al. [1], and Berg et al. [6] suggested methods to manipulate input prompts or text tokens for debiasing spurious correlations in VLMs' encoders, covering downstream tasks such as zero-shot binary classification and text-to-image retrieval. For image captioning, Hirota et al. [20] proposed a fine-tuning method for both the encoder and decoder to mitigate biases. For text-to-image generation, Kim et al. [23] recommended a de-stereotyping prompt design to address biases. Some methods, such as DeAR [34] and CLIP-clip [38], manipulate frozen representations without training the entire model and can be generalized across various tasks, as discussed in Section 4. Dehdashtian et al. [13] also aimed to debias frozen representations from both the image and text encoders, though this approach requires class labels and text-image pair datasets, which are challenging to define and obtain across various downstream tasks. To address these limitations, we propose the first unified debiasing method for VLMs across various downstream tasks, which demonstrates outstanding performance compared to the extensions of other debiasing methods, such as DeAR and CLIP-clip."}, {"title": "3 Bias Analysis in VLMs", "content": "Multi-class zero-shot classification leverages the capability of VLMs that train image and text encoders jointly. Specifically, the predicted class is determined by providing text prompts about classes to the text encoder and selecting the class with the highest cosine similarity to the encoded image. For instance, the text prompt \u201ca photo of a/an [CLASS NAME]\" is used for the zero-shot classification. Bias issues arise from the difference in accuracy between genders for a given class. For example, in the class Carpenter, the accuracy for male carpenter images and female carpenter images might differ, as VLMs are likely to associate the concept of \"male\" more strongly with \"carpenter.\" Therefore, bias is defined by the average demographic disparity, which is determined by the gender disparity in recall for each class following [17],"}, {"title": "3.1 Zero-shot Classification", "content": "ADPmean = 1/|K| \u2211_{k\u2208K} |P(\u0176 = k|a = 1) \u2013 P(\u0176 = k|a = 0)|,\nwhere \u0176 is the predicted class, and k \u2208 K represents each class in the multi-class classification, and a \u2208 {0,1} is the sensitive attribute. The overall accuracy is used as an evaluation metric for the performance of VLMs. Lower ADP indicates fair classification across the sensitive attribute, while higher overall accuracy denotes higher classification ability. As a baseline for zero-shot classification,"}, {"title": "3.2 Text-to-Image Retrieval", "content": "Text-to-image retrieval leverages the matching ability of image and text encoders. For a given ground truth caption, images in the query set are retrieved by sorting them according to the cosine similarity between the image embeddings and the text embedding of the caption. Bias in retrieval arises when the gender distribution in the retrieved set is skewed to a certain gender. For example, for a gender-neutral caption such as \u201ca person in a suit is hurrying across the street.\u201d VLMs retrieve male images more frequently than females by associating the concept of 'suit' and 'male'.\nThe evaluation metric for fairness in text-to-image retrieval is defined as Skew@M, as suggested in [34]. Let pa = Na/N for a \u2208 {0,1}, where Na is the number of images for each gender in the original dataset and N is the total number of images. For M retrieved images, calculate pa = Ma/M, where Ma is the number of images for each sensitive attribute in the retrieved set. The metric is then defined as Skewa = log(pa/pa) for each sensitive attribute a indicating if a particular gender is retrieved more frequently. The final evaluation metric, Skew, is defined by averaging the maximum Skew values over the set of text prompts T,\nSkew = 1/|T| \u2211_{t\u2208T} max Skew_a"}, {"title": "3.3 Image Captioning", "content": "Image captioning aims to generate a caption given an image. Fairness issues arise from differences between the gender mentioned in the caption and the ground truth gender of the subjects in the image [20], as VLMs may prefer certain genders for particular subjects. For example, as shown in Figure 1, image captioning models tend to associate contexts and genders, such as (athlete, male) and (long hair, female).\nFor the evaluation metric, we first detect the gender in the generated caption by its pronoun. Specifically, for an image in the query set, we measure the gender mismatch rate for a k-th image,\nIk = { 1 if (original gender) \u2260 (detected gender) {0 0 if (original gender) = (detected gender) or (neutral detected gender)\nwhere the misclassification rate for each gender is defined as MRM = 1/|M| \u2211_{k\u2208M} Ik, MRF = 1/|F| \u2211_{k\u2208F} Ik, and MR0 = 1/|D| \u2211_{k\u2208D} Ik, with M, F, and O indicating male, female, and overall, respectively. Although the overall misclassification rate is used in [20], it cannot perfectly reflect fairness. For example, in two different situations where (MRM, MRF, MRo) are (3.0%, 3.0%, 3.0%) and (0.0%, 6.0%, 3.0%), respectively, the overall misclassification rates are the same, but the rates for each gender are not fair. To address this, we derive a Composite Misclassification Rate, defined as MRC = \u221aMR\u00b2 + (MRF \u2013 MRM)2, which can be minimized when both the overall misclassification rate and the disparity in misclassification rates between genders are low.\nOn the other hand, the caption's quality is measure by METEOR [4] and SPICE [3]. METEOR measures the balance between precision and recall of n-grams in generated captions, incorporating synonyms, while SPICE focuses on the semantic content of captions by comparing sets of propositional semantic tuples extracted from candidate and reference captions. (See Appendix B for details.) Considering fair image captioning, the evaluation metric for caption quality should account for both"}, {"title": "3.4 Text-to-Image Generation", "content": "VLMs can be used for text-to-image generation, creating an image given a text prompt. However, bias issues in text-to-image generation arise in two ways: generating an image with the wrong gender despite of a specific gender is given in the prompt, or generating a biased gendered image for a neutral text prompt. We utilize the input prompt from [10] to generate images for captions: \"a photo of G who works as a/an P,\u201d where gender set G = {a man, a woman, a person} and profession set P = {accountant, engineer,... }. We use 83 professions as in [10].\nFor the evaluation, we first detect the gender of the generated image by utilizing BLIP-2 [26] by asking \"Does the person look like a male or a female?\". For gender-specific prompts, a fair text-to-image generation model should produce exact gender images irrespective of the profession. Thus, we measure the mismatch rate: 1/|D| \u2211_{k\u2208D} Ik, where D is a set of prompts for either the male or female subgroup. We adapt the composite misclassification rate, MRc, as the same as the image captioning to verify a balanced outcome between mismatch rates for each gender. For neutral prompts, a fair model should produce an equal number of images of each gender across all professions. The bias metric thus measures the skewed distribution of the model:\nSkew = 1/|P|\u2211_{PEP} max(Np.m, Np.f)/10                 (1)\nwhere Np,m and Np,f are the numbers of detected genders for each profession, and C = 10 is the number of generation for each prompt. In text-to-image generation, lower values of both MRC and Skew indicate fairness. We use SDXL [28] and CoDi [37] as baselines for text-to-image generation."}, {"title": "4 Proposed Method", "content": "Let XD be a debiasing dataset with the sensitive attribute label YD. Let XQ be a query dataset that the user is interested in and wants to evaluate in debiased downstream tasks. For a frozen component in VLMs g, whether it is an encoder or decoder, or processes image or text, we obtain the frozen representations ZD = g(XD) and ZQ = g(XQ), respectively."}, {"title": "4.1 Selective Feature Imputation for Debiasing", "content": "For the debiasing representation ZD, Selective Feature Imputation for Debiasing (SFID) uses a RandomForest [8] f to predict the sensitive attribute YD. Given the interpretability of RandomForest, it is capable of providing feature importance for predictions, allowing us to identify which features in the frozen representation are relevant to the sensitive attribute. Additionally, RandomForest is known for not requiring hyperparameter tuning [29] and for its computational efficiency [7], making it an easily implementable choice. As the objective of SFID is to lead VLMs' components to produce a fair outcome, free of bias regarding a sensitive attribute, SFID prunes the important features that show higher relevance to the sensitive attribute. This procedure is beneficial as it considers the dependency between features, whereas methods like CLIP-clip [38] extract feature importance by measuring the mutual information between each feature and the sensitive attribute, assuming each feature is independent.\nHowever, simply dropping features cannot maintain the dimensionality of the representation, which is crucial for using the embedding for generation tasks. For example, in CLIP-CAP [27], the decoder (GPT-2 [30]) takes the image representation from the CLIP ViT-B/32 [31] image encoder as input, and the input dimension of GPT-2 is fixed. Therefore, we must maintain the dimension of the representation after pruning to utilize the pre-trained decoder but approaches like filling with zero-values or Gaussian noise may mislead the semantic meaning of the representation, as described in Figure 4 and ablation study in Section 5.3.\nTo address this, SFID replaces important features with ambiguous features through Low Confidence Imputation (LCI). LCI is defined as the average of the features in low-confidence samples from the validation set as determined by RandomForest. RandomForest is known for its robustness against overfitting and can provide reliable confidence levels, identifying which samples are more ambiguous with low confidence. These low-confidence samples are likely to be 'hard to identify the sensitive attribute,' implying they are free of biased features.\nWe visualize how the important features are correlated to social biases by showing GradCAM [33] visualizations, as presented in Figure 2. For example, the more important features highlight human faces, while the least important features are correlated with the image background. SFID imputes"}, {"title": "4.2 Comparison with Other Debiasing Approaches", "content": "As described in Section 1, two existing methods, DeAR [34] and CLIP-clip [38], utilize debiasing datasets with gender labels and pre-trained models to debias embeddings. While DeAR is specifically designed for image encoders and CLIP-clip for both image and text encoders, their methodologies could potentially be extended to text and image decoders, as well as other encoders. In contrast, Prompt-Debias [11] is designed primarily for text encoders, using pre-defined text prompts to achieve debiasing. These three approaches are selected for comparison in our analysis."}, {"title": "4.3 High-Confidence Strategy in Text-to-Image Generation", "content": "In SFID, we utilize low-confidence samples to obtain imputation values that mute bias-related information within the embedding. However, in text-to-image generation, users may sometimes wish to specify the gender of the generated image through gender-specific prompts, such as \u201ca photo of a man who works as a nurse.\" In these cases, rather than muting the bias-related features, we can impute features from samples classified with high confidence by RandomForest to retain the desired gender. For text-to-image generation, we report results using both low-confidence (LC) imputation and high-confidence (HC) imputation approaches, respectively."}, {"title": "5 Experimental Result", "content": "We use the FairFace dataset [22] for image inputs and the Bias-in-Bios dataset [12] for text inputs as our debiasing datasets, denoted as XD. Each dataset is split into training and validation sets. A RandomForest classifier is applied to a 2D-shaped embedding representing all training samples, with low-confidence samples selected from the validation set. All hyperparameters and model settings for each baseline follow the default configurations provided in their respective open-source repositories. Detailed experimental settings, along with evaluation metrics and query datasets, are described in Section 3.\nFor a fair comparison, we conduct 10 experiments using different subsets and report the mean and standard deviation for the text-to-image retrieval task. For zero-shot classification and image captioning, we employ 1000 bootstrapping iterations to calculate the confidence intervals. For text-to-image generation, images are generated using 10 random seeds. The mismatch rates are reported as the mean and standard deviation over 10 runs, while the Skew for the Neutral prompt is reported as a single value based on 10 runs. Further analysis of this evaluation metric can be found in Appendix D."}, {"title": "5.2 Result Analysis", "content": "As shown in Table 2, SFID effectively mitigates biases in CLIP RN50, CLIP ViT-B/32, and XVLM for multi-class zero-shot classification on the FACET dataset and text-to-image retrieval on the Flickr30K dataset. Specifically, the fairness metrics such as ADP in zero-shot classification and Skew@100 in text-to-image retrieval outperform existing debiasing methods, including DeAR, CLIP-clip, and Prompt-Debias without compromising performance metrics such as accuracy and recall.\nIn Table 3 for the image captioning task, SFID consistently improves both the overall misclassification rate and the composite misclassification rate, outperforming other debiasing methods. This indicates that SFID not only reduces the likelihood of gender misclassification but also balances the misclassification rate across genders.\nAs shown in Table 4 for text-to-image generation, SFID demonstrates improvements in both overall and composite mismatch rates. Notably, SFID with the high-confidence strategy (HC) shows a significant reduction in mismatch rates, though SFID with the low-confidence strategy (LC) also achieves a considerable level of improvement. This suggests that the debiased generative model adheres more closely to the intended gender specified by the user, rather than associating certain genders with professions in a biased manner, outperforming other methods. In particular, DeAR with SDXL tends to produce only one gender, resulting in a higher overall mismatch rate and an increase in Skew. Although there were improvements in Skew for neutral prompts, the score remains notably high, indicating that further refinements are necessary for this task."}, {"title": "5.3 Ablation Study", "content": "To verify the impact of the low confidence imputation and the hyperparameter 7, we conduct an ablation study using XVLM, one of the state-of-the-art foundation models. As shown in Table 5, our low confidence imputation strategy demonstrates outstanding performance compared to zero and Gaussian imputation. Moreover, the hyperparameter 7 shows optimal performance at 7 = 0.7, thereby we set T = 0.7 across the experiments."}, {"title": "6 Conclusion", "content": "In conclusion, our study addresses the critical issue of bias in Vision-Language Models (VLMs) by introducing the Selective Feature Imputation for Debiasing (SFID) method. This approach effectively reduces biases across various downstream tasks, including zero-shot classification, text-to-image retrieval, image captioning, and text-to-image generation, without compromising performance. Additionally, SFID does not require extensive hyperparameter tuning or costly retraining, making it cost-efficient. By training a debiasing dataset separate from the test query set, SFID demonstrates its transferability and maintains zero-shot capability. Furthermore, SFID's ability to generalize across different VLM components, such as encoders and decoders, highlights its versatility in diverse multimodal contexts. Experimental results show that SFID outperforms existing debiasing methods, making it a versatile and efficient solution. This advancement paves the way for fairer and more reliable applications of VLMs in diverse multimodal scenarios, promoting both fairness and practicality in real-world applications."}, {"title": "A Details of the Proposed Method", "content": ""}, {"title": "A.1 Selection of Number of Pruned Feature", "content": "In SFID, the number of features to be pruned, denoted as k, is a critical hyperparameter. To determine an appropriate value for k, we analyzed the feature importance by plotting Figure 6, which shows the sorted feature importance ranks. The plot reveals that the first few features have significantly higher importance, stabilizing around the top 100 features for all components. Therefore, we set k = 50."}, {"title": "A.2 Extension to Decoder", "content": "In some scenarios, embeddings may not be in a 2D shape, which poses challenges for further processing. For instance, in a decoder, outputs are tensors with shapes such as Z\u2208 RNXS\u00d7C for text decoder and Z \u2208 RN\u00d7C\u00d7H\u00d7W,for image decoders, where N is the number of samples, S is the sequence length, C is the number of channels, H is the height of the feature, and W is the width of the feature. These non-2D shaped tensors are not suitable for extracting feature importance via RandomForest, which requires 2D data Z \u2208 RN\u00d7C. To address this, we transform the data into a 2D shape by averaging over the sequence length or applying global average pooling,\nZij = 1/S \u2211_{k=1}^S Zi,k,j or Zij = 1/HW \u2211_{k=1}^H\u00d7W \u03a3Zi,j,k,l.\nThis averaging is applied solely for extracting feature importance indices Sand imputation values \u03bcj."}, {"title": "B Evaluation Metric for Image Captioning", "content": "METEOR measures the balance between precision and recall of n-grams in generated captions, incorporating synonyms. Let P and R be the precision and recall of matches between the generated caption and ground truth, including exact, synonym, and paraphrase matches: METEOR = Fmean (1 - Pen) where Fmean = 1/(1/P + 1/R) is a harmonic mean, and Pen = 0.5\u00d7 ( number of chunks / number of matches).\nSPICE [3] focuses on the semantic content of captions by comparing sets of propositional semantic tuples extracted from candidate and reference captions. SPICE is the F1 score of precision and recall between the tuples of generated captions and ground truth."}, {"title": "C The Impact of k in CLIP-clip", "content": "CLIP-clip [38] requires a hyperparameter k to determine the number of pruned features. We selected k for CLIP-clip based on empirical results with XVLM, as shown in Figures 7 and 8. The value k = 60 performs best in text-to-image retrieval for the Flickr30K dataset, compromising slightly on accuracy and recall. Meanwhile, SFID demonstrates the best trade-off at k = 50, corresponding to Appendix A.1."}, {"title": "D Confidence Interval in Text-to-Image Generation", "content": "We conduct text-to-image generation 10 times with different seeds and use a unified evaluation metric to measure the skewed distribution across 10 different generations for each neutral prompt:\nSkew = 1/|P|\u2211_{PEP} max(Np.m, Np.f)/10\nwhere Np,m and Np, f are the numbers of detected genders for each profession, P is a profession set. For example, if a model generates the same gender for a class 9 times out of 10, the Skew value for"}, {"title": "E Multiple Sensitive Attribute", "content": "We extend our analysis to more complex bias scenarios in Vision-Language Models (VLMs), involving multiple sensitive attributes. Specifically, we conduct additional experiments that focus on racial bias, considering more than two sensitive attributes to capture a broader spectrum of bias patterns.\nFirstly, we adopted the FairFace dataset for training the attribute classifier, as it contains seven racial attributes: East Asian, Indian, Black, White, Middle Eastern, Latino Hispanic, and Southeast Asian. Given that RandomForest can handle multiple classes, SFID is also applicable in this context. During the evaluation stage for zero-shot classification, we used the FACET dataset, which contains 'skin tone' labels instead of race. We categorized the skin tone attributes into three categories: \u2018lighter,' 'middle,' and 'darker.' In this setting, the biased zero-shot classifier tends to produce higher accuracy by associating certain skin tones with specific professions (e.g., bartender with lighter skin, trumpeter with darker skin).\nTo mitigate this bias, SFID demonstrates its effectiveness even though the attributes in the training set and test set do not exactly match, i.e. race in FairFace and skin tone in FACET. We adopted an evaluation metric inspired by [16] and [14], which measures the maximum discrepancy across the sensitive attributes for each class. This metric is defined as follows:\nDPc = max max_{i\u2208S j\u2208S\\{i}} |P(y = c | a = i) \u2013 P(y = c | a = j)|\nwhere i, j \u2208 S, c is a class, and S denotes the set of multiple sensitive attributes. We consider the mean and maximum values of DP to evaluate the bias across the classes. By applying this metric, we can effectively measure and demonstrate the reduction in bias achieved by SFID, despite the differences in sensitive attributes between the training (debiasing) and test sets. The results in Table 6 show that even with limited data and racial bias present only in the image dataset, SFID"}, {"title": "F Computational Resource", "content": ""}]}