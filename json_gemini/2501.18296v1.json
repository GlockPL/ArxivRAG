{"title": "Extending the design space of ontologization practices: Using bCLEARer as an example", "authors": ["Chris Partridge", "Andrew Mitchell", "Sergio de Cesare", "John Beverley"], "abstract": "Our aim in this paper is to outline how the design space for the ontologization process is richer than current practice would suggest. We point out that engineering processes as well as products need to be designed \u2013 and identify some components of the design. We investigate the possibility of designing a range of radically new practices, providing examples of the new practices from our work over the last three decades with an outlier methodology, bCLEARer. We also suggest that setting an evolutionary context for ontologization helps one to better understand the nature of these new practices and provides the conceptual scaffolding that shapes fertile processes. Where this evolutionary perspective positions digitalization (the evolutionary emergence of computing technologies) as the latest step in a long evolutionary trail of information transitions. This reframes ontologization as a strategic tool for leveraging the emerging opportunities offered by digitalization.", "sections": [{"title": "I. INTRODUCTION", "content": "In ontology engineering there is, in theory at the very least, a tight coupling between ontologies and ontologization, the process that produces them. Our aim in this paper is to suggest that the design space for the ontologization process is wider than a look at many of the current methodologies would indicate. We leave\nTo illustrate this at a general level, we partition the space along two dimensions: the levels of generality and digitalization."}, {"title": "A. Structure of the paper", "content": "In the next section, we provide a broad picture of the ontologization process and introduce two current mainstream ontologization methodologies to act as a baseline for comparisons. In the third section we introduce our outlier methodology, bCLEARer. In the fourth section we build the contextual scaffolding, firstly situating digitalization in an evolutionary perspective, then situating ontologization within digitization. In the fifth section, we situate bCLEARer in this evolutionary perspective. In the sixth section, we illustrate from within the evolutionary perspective some of the outlier design choices that bCLEARer has made."}, {"title": "II. A BROAD PICTURE OF THE ONTOLOGIZATION PROCESS", "content": "In ontology engineering one would expect a tight coupling between ontologies and ontologization, the process that produces them. One can characterize this as a product-process distinction, a fundamental concept in traditional engineering. The engineering mindset differentiates the product (the output) and the process (the method or system used to produce the output) and expects both to be engineered. And, as part of the engineering, to both be quality managed, hence quality assurance (process) and quality control (product)."}, {"title": "A. Top-level ontologies and ontologization", "content": "In top-level ontology (engineering) work, the process is often an invisible relation of the product. An example of this is provided by the main standard, ISO 21838-1:2021 \u2013 Information technology: Top-level ontologies (TLO) \u2013 Requirements [1]. While this references the ontology of processes, the standard makes no mention of the ontologization process itself. Hence, unsurprisingly, the standards based upon it do not mention the ontologization process either.\nSome top-level ontologies have associated documentation for the ontologization process. The top-level Descriptive Ontology for Linguistic and Cognitive Engineering (DOLCE) has a related analysis tool OntoClean [2], but this falls far short of an ontologization process. The Basic Formal Ontology (BFO) has a book [3] on the ontologization process that assumes the BFO top-level ontology. We look at this text in more detail below. The BORO Foundational Ontology has a closely intertwined bCLEARer ontologization process described in [4] and [5]. There are a variety of domain level ontologization processes that we discuss later in this paper."}, {"title": "B. The case for engineering the ontologization process", "content": "The importance of engineering the process is reflected in the often-quoted dictum that: the quality of the process determines the quality of the product.\nFor a historical background to this, from the wider history of innovation, see Mokyr's The Past and the Future of Innovation [6] or his A Culture of Growth [7]. He argues that history shows that technological progress cannot rely on artisanal skills alone, it needs to be supplemented with formal and systematic (that is, engineered) knowledge.\nWithin engineering, this idea was explored, analyzed and championed in manufacturing in the second half of the 20th century by quality management pioneers such as W. Edwards Deming [8] and Joseph Juran [9]. This led to a rich variety of designs including movements such as Total Quality Management and its successors Lean Manufacturing, and Six Sigma. These developed a fertile range of ways of managing manufacturing processes. An example is the Plan-Do-Check-Act (PDCA) Cycle used to design, implement, and refine processes on a small scale \u2013 which fits well with the Kaizen philosophy of continuous improvement, where processes are regularly reviewed and improved incrementally. This has spread to some other domains. For example, one can see Kaizen-like principles being used in the Agile software development methodology.\nWithin the ontology engineering community, one does not find a comparatively rich selection of designs and range of ways of managing the ontologization processes \u2013 a kind of process design poverty. This is despite a few interesting innovative examples such as the ROBOT tool (https://robot.obolibrary.org/extract.html). Especially for top-level ontologies, there appears to be more 'theory' for, and so more attention on, the design of the final product 'the 'ontology' - than the process \u2013 'ontologization' \u2013 that produces it.\nFrom an engineering perspective, this imbalance looks unhealthy. One could argue that this poverty arises from the process being relatively new and under-researched, unlike, for example, top-level ontology which can build upon a rich heritage."}, {"title": "1) Process design poverty in logic", "content": "Interestingly, a similar poverty of design process has been pointed out in logic, which is a key part of the last stages of the \u2018ontologization' process. Novaes [10] makes a product-process distinction for logic, distinguishing the formal product from its formalization process, noting an almost exclusive focus on the former in contemporary logic:\n\"As a discipline, logic is arguably constituted of two main sub-projects: formal theories of argument validity on the basis of a small number of patterns, and theories of how to reduce the multiplicity of arguments in non-logical, informal contexts to the small number of patterns whose validity is systematically studied (i.e. theories of formalization). Regrettably, we now tend to view logic \u2018proper'exclusively as what falls under the first sub-project, to the neglect of the second, equally important sub-project.\"\nShe discusses two historical theories of argument formalization, from Aristotle and Medieval Logic that have more balance. Both \"illustrate this two-fold nature of logic, containing in particular illuminating reflections on how to formalize arguments (i.e. the second sub-project).\" She suggests reflecting on these should lead to a broader conceptualization of what it means to formalize.\nGiven how much the ontology (engineering) product builds on formal logic \u2013 inheriting many of its (cultural) practices \u2013 this may contribute to the poverty in ontology engineering. This suggests that developments in the formalization process could be recruited by and enrich ontologization's approach to formalization."}, {"title": "C. Comparing different ontologization processes", "content": "In this section, we briefly look at some current mainstream methodologies that guide the ontologization process to provide a basis for comparison with bCLEARer. This gives us a rough benchmark on common practices. A caveat: we are not claiming that this selection reflects all the work that is happening in this area. Rather, we are aiming for examples that lend themselves to our broad comparison.\nIn this section we restrict ourselves to ontologization to help make a clear comparison. This is even though, as we touch upon later from a bCLEARer perspective, there are interesting features in the methodologies guiding the processes in other software related domains, such as:"}, {"title": "1) The 'Ask-an-Expert' approach", "content": "We selected the OntoCommons report D.4.2 [18] as our basis for AaE. It suits our purposes as it not only describes its own approach (the LOT methodology) but documents other similar approaches (including Gr\u00fcninger & Fox [13], METHONTOLOGY, On-To-Knowledge, DILIGENT, NeOn, RapidOWL, SAMOD and AMOD). Together these provide many good examples of the \u2018Ask-an-Expert' (AaE) approach, which has its roots in Artificial Intelligence (AI) and knowledge representation.\nThis process is largely a rationalist armchair exercise \u2013 in the sense that there is little empirical content. The input for the process is domain experts as this quote illustrates:\n\"The goal of the ontology implementation activity is to build the ontology using a formal language, based on the ontological requirements identified by the domain experts.\" [13, p. 28]\nAcross all the approaches reviewed, there is a similar information pathway from a level of digitalization perspective. In the early stages there is an underlying focus on natural language (from a levels of digitalization view, speech), sometimes organized into (natural language) competency questions [13] \u2013 as this quote illustrates:\n\"If domain experts have no knowledge about ontology data generation and querying, we recommend writing the requirements in the form of natural language sentences.\" [13, p. 22]\nThe methodology's input to the information pathway is the brains of experts via speech into documented (unstructured) natural language. Then the methodology broadly separates concerns [21]: separating the confirmation of content from its formalization \u2013 and chooses to address the first concern before the second.\nWe will revisit this point later, but it is important to note that this separation and ordering choice assumes that reaching content agreement prior to the formalization process won't negative impact the final product. In this design architecture, the first stage is a confirmation of content which uses mostly (unstructured) natural language which is organized and agreed as a statement of the requirements of the ontology. The second stage takes the natural language and formalizes them.\nThe early pathway is not always or entirely natural language, as there is a mention of the possibility of using more structured information in the shape of a \u201ctabular technique\u201d using \"3 types of tables: Concepts, Relations, Attributes\". Formalization (structured information) only really enters the process in the later stages of the pathway in ontology implementation, after the requirements (expressed in natural language) are collected.\nThe paper notes that there is optionally a conceptualization stage, where an interim concept model based upon the requirements may be built. Interestingly, it suggests that \u201cdiagraming tools such as MS Visio or draw.io, as well as non-digital tools as pen and paper or a blackboard\" may be used to build this."}, {"title": "2) The \u2018Top-Down-Classification' approach", "content": "We take Building ontologies with Basic Formal Ontology [3] as the baseline for the 'Top-Down-Classification' (TDC) approach. This provides a clear example with a concise summary of how it aims to construct an ontology (this shows why it deserves the top-down-classification nickname). This process is also largely a rationalist armchair exercise, one that has roots in biological classification and philosophy. It is a common approach to developing top-level ontologies in Information Systems (IS).\n\"Overview of the Domain Ontology Design Process\nOntology is a top-down approach to the problem of electronically managing scientific information. This means that the ontologist begins with theoretical considerations of a very general nature on the basis of the assumption that keeping track of more specific information (for example, about specific organs, genes, or diseases) requires getting the very general scientific framework underlying this information right, and doing so in a systematic and coherent fashion. It is only when this has been done that the detailed terminological content of a specific science such as cell biology or immunology can be encoded in such a way as to ensure widespread accessibility and usability.\u201d [3, p. 49]\nThis informal view is then structured into a step-by-step process in a table \u2013 see below."}, {"title": "3) Process Comparison", "content": "One can make a rough assessment of the engineering maturity of these methodologies. As the quotes above hint at, they are currently collections of \"ad hoc rules\" with simple heuristics. There is no background context to act as a foundation to guide the engineering of the process design \u2013 certainly no common context. Hence, they are, from an engineering design perspective, at an early stage of development. There is still plenty of scope for them to undergo the kind of serious engineering re-design Deming and Juran undertook for manufacturing.\nBoth approaches have several features in common, ones that differentiate them from bCLEARer. From the perspective of digitalization, in both cases, their early processes for establishing the base ontology focus their efforts on working with unstructured (pre-digitalization) natural language related to human understandability, with less focus on machine understandability. In both approaches the ontologization happens before the formalization. The (unstructured) ontological information is captured and regimented in natural language first and then formalized.\nOne can broadly divide information into levels of generality. From a syntactic 'data' perspective, these are the natural levels: metadata, schema and data. For our purposes here, these are a good enough rough proxy for the semantic levels; top, the most general, middle and the most specific bottom level \u2013 typically, particulars.\nWe can use this perspective to show how the two approaches showcased differ and agree. They differ in their approach to the metadata level. The 'top-down-classification' approach works by framing the middle level in terms of the relevant top-level structure so the middle schema level is framed by this metadata. The 'ask-an-expert' approach works explicitly at the schema level \u2013 focusing on the domain. In principle, there is no reason why the \u2018ask-an-expert' approach could not start with the metadata level, or the \u2018top-down-classification' approach could not ignore the metadata level and work at the schema level. The two approaches agree on their approach to the bottom data level. They both ignore it (a significant omission, we return to below).\nOne can relate the design of the digitalization and generality features. If one chooses to design a process to work with humans and unstructured information, one needs to recognize that one is building in scaling constraints that block the processing of large amounts of information. One way around this is, of course, to work with the data level indirectly through the schema level. bCLEARer takes a different route and aims to automate the process so removing these scaling constraints."}, {"title": "III. BCLEARER - HISTORY", "content": "In this section, we provide more context with a brief background history of bCLEARer and its place in BORO (an acronym for \u2018Business Object Reference Ontology'). BORO's development and deployment started in the late 1980s. This early work is described in Business Objects [4]. BORO's focus was then, and is now, on enterprise modelling; more specifically, it aims to provide the tools to salvage and reuse the semantics from a range of enterprise systems building a single ontology with a common foundation in a consistent and coherent manner.\nBORO was originally developed to address a particular need for a solid legacy re-engineering process. This naturally led to the development of methodology for re-engineering existing information systems, currently named bCLEARer \u2013 where the capital letters are an acronym for Collect, Load, Evolve, Assimilate and Reuse. This was co-developed with a closely intertwined top-level ontology (the BORO Foundational Ontology). Hence, the term BORO on its own can refer to either of, or both, the mining methodology and the ontology.\nOur focus here is on the bCLEARer methodology which is used to systematically unearth reusable and generalized ontological business patterns from existing data. Most of these patterns were developed for enterprises and successfully applied in commercial projects within the financial, defense, and energy industries.\nbCLEARer has evolved organically over the last three decades both in response to the evolutionary pressures of experience as well as exploiting the opportunities provided by evolving digital technology. An early version of the methodology is described in [4] \u2013 with a detailed description in Part 6. At the time this was developed, the late 80s and early 90s, the technology support was immature, so while the core process was systematized it was not fully automated. Over the last decade, as appropriate technology has emerged, the core process has been fully automated into a data pipeline. Later versions of this are described in several places, including [22]. There are also open-source examples on GitHub (https://github.com/boro-alpha).\nbCLEARer (and its associated top-level ontology) have, over the years, been configured to exploit a variety of situations ranging from its original legacy system migration to application migration to developing requirements and quality controlling existing systems. A common feature of all these projects has been the initial collection of one or more datasets (where this may include both structured and unstructured data though structured data is preferred) and its regimented evolution to a more digitally aligned state."}, {"title": "IV. SITUATING DIGITALIZATION AS INFORMATION EVOLUTION", "content": "We have established that (engineers have learnt that) the quality of the final product depends upon the engineering quality of the design of the process. We have also suggested that the mainstream ontologization processes are very lightly engineered with a weak background context. This indicates that there is an opportunity to develop a more engineered ontologization process. What is less clear is what form this engineering should take.\nOver the last three decades, the evolution of bCLEARer's practices was initially driven by experience. This pragmatic, experiential approach is supported by many including Aristotle [23] who said, \u201cfor the things we have to learn before we can do them, we learn by doing them\u201d. Reflecting upon the process has always been a central part of the practice. However, in the last decade, as the practice has matured, questions about the broader context has naturally arisen and this has led to a much better understanding of how the practice should be engineered.\nThis better understanding has merged in large part from a recognition that for foundational issues, setting the right context can have a bigger impact on improvements than the quality of the problem-solving processes. This is not a new idea; it is already established in many fields. In the context of professional practice, Donald A. Sch\u00f6n's \u201cThe Reflective Practitioner,\u201d [24] raised the concept of problem setting as a crucial part of having a successful outcome. In design thinking, David Kelley emphasizes the importance of problem framing in his book \"The Art of Innovation\" [25] explaining that reframing the problem often opens the door to more creative and effective solutions. In systems thinking, Peter Senge's \"The Fifth Discipline\" [26] differentiates between problem identification and problem solving. For him effective problem solving involves identifying leverage points-places within a system where a small change can lead to significant, long-term improvements.\nIn each of these cases, the initial stage involves developing a clear understanding of the underlying causes and interconnections of the entire complex system. Problem solving then involves developing interventions that address the root causes.\nIn the case of ontologization there is a ready-made context that can provide the perspective needed \u2013 this is evolutionary theory. If one positions ontologization as an essential part of a much wider more pervasive phenomena, the latest information evolutionary step \u2013 digitalization (the emergence of computing technologies) then a new picture emerges. From a short-term perspective, this recapitulates the relatively recent steps of printing and writing. From a long-term perspective, this fits into an evolutionary trail of information transitions that spans life on earth. Within such perspectives, ontologization can be understood as a tool for exploiting the emerging opportunities offered by digitalization.\nWe can then position the bCLEARer practices into the overall evolutionary context. This leads, in turn, to a clearer picture of the specific evolutionary pressures within the practice. From the start, bCLEARer has been framed in terms of information engineering, evolution and revolution. The narrative in [4] was the evolution of information paradigms. For much of its early life the focus of bCLEARer's evolution has been pragmatic practice adapting to the evolutionary pressures presented by actual ontologization with only a modicum of reflection upon the nature of the process. In the last decade or so there has been more reflection on what these practices might reveal. We are reaching a conclusion that the best way to understand the design of the process is make the originally evolutional framing much richer \u2013 to firstly more clearly frame the process as digitalization and secondly to show the digitalization as part of a wider trend that frames evolution in terms of information."}, {"title": "1) Situating digitalization as information innovation", "content": "Digitalization is an information transition \u2013 and it turns out information transition is a ubiquitous pattern which can be used to frame the whole of macro-evolution. This provides a reassuringly broad context where digitalization is the latest in a long history of information transitions.\nMaynard Smith and Szathm\u00e1ry [27], [28], [29] suggest that macro-evolution can be characterized as a series of information transitions and that one can frame the whole of macro-evolution in these terms:\n\"... that evolution depends on changes in the information that is passed between generations, and that there have been 'major transitions' in the way that information is stored and transmitted, starting with the origin of the first replicating molecules and ending with the origin of language.\"\nAnd these changes in information transmission, the passing of \"information between generations\", are central to evolution. Maynard Smith and Szathm\u00e1ry provide a table of seven major transitions, where the third is the \"genetic code\" and the seventh is \"language\". Each transition not only transforms life but also transforms the way life evolves \u2013 and so, in a sense, evolution evolves through transformations in information transmission. They also suggest that each of these transitions has accelerated and expanded evolution enabling more complex entities to emerge quicker."}, {"title": "a) Evolutionary transitions in information transmission", "content": "This Lamarckian \u2018targeting'and\u2018constructing' enables further evolutionary transitions in information transmission. Obvious examples from symbolic evolution are the emergence of writing and printing information technologies. These involve fundamental \"changes in the way information is stored and transmitted\" where writing involved changes in structure and printing changes in economics. These both clearly led to further innovations in information evolution. Ironically, this reveals CRISPR technology, where DNA is selectively modified, as a Lamarckian genetic evolution.\nWhile the transitions clearly involve the use of new technology, closer examination (see, for example, Ong [32] and Olson [33]) reveals they depended upon the coevolution of human behavior and external information technologies. Where both the emergence and exploitation of the technology depends upon intertwined coevolution with human behavior."}, {"title": "b) Domestication as an example of coevolution", "content": "A more familiar example may help us to appreciate the nature of coevolution \u2013 the domestication of plants and animals. This has been studied as a distinctive coevolutionary relationship between domesticator and domesticate in a range of research [34], [35]. In this domain, it is easy to see that both parties (the domesticators and domesticates) coevolve in the sense of contributing to the relationship. Zeder [34, p. 3191] describes domestication as a:\n\u201c... relationship in which one organism assumes a significant degree of influence over the reproduction and care of another organism in order to secure a more predictable supply of a resource of interest.\u201d\nOur relationship with the new digital forms of information technology can be described in a similar way. One where we domesticate our computer systems controlling their breeding."}, {"title": "c) A new\u2018digital' form of information transmission", "content": "In The Selfish Gene, Dawkins [36] introduced an idea. He distinguished between genes as \u201creplicators\u201d that pass on copies of themselves through generations and organisms as \u201cvehicles\" or \u201csurvival machines\u201d constructed by genes to survive in the environment and so ensure their continued replication.\nOne could adopt a 'promiscuous ontology', one that regards computer systems as a form of life subject to evolution [37], [38], [39]. If so, then computers can easily be seen as \u201cvehicles\u201d for the information they carry and replicate as well as things we domesticate and breed. Furthermore, the emergence of these individuals then passes the Smith and Szathm\u00e1ry test in so far as it radically 'changes the way information is stored and transmitted' directly between these individuals and with humans.\nOne could be less adventurous and instead see the computer systems as an extension of humans [40]. In this view, computer systems are replicators rather than vehicles \u2013 they are part of the apparatus transmitting information rather than \u201csurvival machines\" in a computer ecosystem. Even on this view, they pass the Smith and Szathm\u00e1ry test. So, either way, one can see them as providing an opportunity for an information transition [20].\""}, {"title": "2) Narrower context \u2013 Lamarckian choices for co-evolution", "content": "If, as looks likely, history repeats itself and this digital transition follows the pattern of most previous transitions, then it is likely to involve the coevolution of human behavior and digital technology. The energy enterprises currently devote to their digitalization efforts show an intuitive understanding that some kind of directed effort needs to be made. From our evolutionary perspective, we can see this as our culture starting to co-evolve with the new technology \u2013 looking to construct the Lamarckian variations that will exploit this opportunity. What is less clear is which Lamarckian constructed variations are likely to lead to significant success \u2013 in the language of evolution to be able to exploit the natural selection pressures well.\nMaybe history has a clue. A common historical narrative is that technology drives change, that it is the emergence of a new technology that initiates the associated cultural change. Careful study reveals a more interrelated pattern of co-evolution between technology and culture. Where cultural change often prepares the ground for technological innovation and then feeds off it and feeds further innovation. Olson [33] provides a relevant example, explaining how cultural developments in Western Europe from the 9th century onwards played a key role in the invention of moveable type in the 15th century. This technological innovation then laid the ground for developments in Western science in the 16th and 17th centuries. So maybe the coevolution of culture and digital technology evolution can give us some clues on where to target Lamarckian variations.\nIt is well-accepted that work in logic and mathematics laid the foundation for computing. If we look at the culture of this work, then we can see some trends that help us to target variations to help the co-evolution. Well before the introduction of digital computers, Frege [41] uses the analogy of a microscope and the eye to explain how his formal language compares with ordinary informal language, noting that it provided a superior sharpness of resolution. Carnap [42] talks about 'rational reconstruction' (rationale Nachkonstrucktion). Quine [43] says that one doesn't merely clarify commitments that are already implicit in unregimented language; rather that one often creates new commitments by regimenting. Quine notes that paying attention to the ontological commitment often leads to radical 'foreign' differences [44, pp. 9\u201310]: \u201cOntological concern is not a correction of lay thought and practice; it is foreign to the lay culture, though an outgrowth of it\" Adding \"There is room for choice, and one chooses with a view to simplicity in one's overall system of the world.\u201d Lewis [45, pp. 133-5] following the theme of \u2018outgrowth', argues that the differences are a result of taking the lay common sense seriously, by trying to make it simpler and consistent."}, {"title": "3) The ontologization value chain", "content": "The bCLEARer methodology, which is the topic of this paper, has through experience refined these historical intuitions, developing a view of the digitalization process as a network of transformation processes. One way to characterize this is as a value chain, where each transformation adds value. We briefly outline what a value chain is below and then describe the factorization of digitalization into component transformations."}, {"title": "a) Recruiting the value chain view", "content": "In manufacturing, Porter's value chain [46] provides a useful tool for broadly characterizing processes as a system of transformations. The system is provided with inputs which feed into a network of transformation processes. This network feeds into the output. The characterization is recursive. Each transformation process can be seen as a sub-system with its own value chain.\nWe recruit a lightweight version of this tool to characterize ontologization. Under this view, at the broadest level, the ontologization process starts with pre-ontologization information and is transformed using an ontologization process into an ontology. It adds value by transforming the pre-ontologization information into a formal ontology. This reveals an information pathway that starts with the pre-ontologization inputs, undergoes transformations and is output as a formal ontology. Different methodologies have different intermediate transformations and so different information pathways. While the inputs and outputs remain similar, the value-chain transformations differ."}, {"title": "b) Factoring digitalization into *computerized and *ontologized", "content": "We firstly factorize digitalization into two types of digital transitions that it has found useful to target (and construct). These are *computerization and *ontologization. We use the \u2018*' prefix convention to indicate our specialized use of the term and differentiate it from the many other senses in which it is used.\n*Computerization is the process of converting relatively unstructured information into formally structured data. It implies something more than the digitization mentioned earlier, which just aims at bare computer readability. Formally structured data refers to information that is organized into a highly defined and predictable form, typically within a fixed schema or format. So, a scan of an engineering drawing in, say, PDF format would be digitized but not *computerized, as there is no direct way for the computer to read the components of the drawing. Whereas an engineering drawing in a CAD format, such as native DWG, would be *computerized, as the information in the drawing is explicit in its structure and can be read directly by a computer. *Computerization is intended to be a pragmatic distinction and while there are borderline cases, there are also cases that clearly fall into the pre-*computerization and *computerization camps.\n*Ontologization is the process of converting relatively semantically unorganized information into information organized into a common ontological structure. Typically, the information is used in a domain, and there is a level of semantic precision needed for it to be fit for purpose. The *ontologization organizes the information into a common ontological structure that is sufficiently fine-grained to capture the requisite semantic precision.\nOne way of characterizing *ontologization is that it develops an explicit picture of ontological commitment [47], [48]. There is a long tradition of seeing this process as a transformation that reveals a deeper structure.\nCurrently, most information systems being digitized have no precise explicit ontological commitment. So, in practice, the *ontologization is often a regimentation [43] and rational reconstruction [42] of what the ontological commitment would be given some preferred top-level ontology. In bCLEARer's case, the top-level ontology is the BORO Foundational Ontology [5]."}, {"title": "c) *Computerization transitioning from implicit to explicit formal structure", "content": "The bCLEARer methodology has further identified a factorization of the *computerization transition into two sub-transitions: surface-*computerization and deep-*computerization, corresponding to two levels of *computerization."}, {"title": "d) Inter-process dependency", "content": "Obviously, there is an order to the surface- and deep-*computerization process. One surface-*computerizes information before *deep-computerizing it. Theoretically, at least, the *computerization and *ontologization processes would seem to be sufficiently independent that one could undertake either one without the other \u2013 implying that there is a choice in which to do before the other.\nHowever, the bCLEARer experience is that there are strong pragmatic reasons for undertaking the full *computerized transition before undertaking the *ontologization transition [48], [20]. Our experience has been that the formalization process inherent in *computerization is best done with raw unaltered data, straight from the operational 'wild'. This is because we found that in cases where the *ontologization process was carried out on pre-*computerization information, it often obfuscated structure that *computerization needed \u2013 making the overall process significantly harder. Hence, in bCLEARer we see *ontologization as primarily a process for refining already *computerized data."}, {"title": "V. BCLEARER'S BROAD STRUCTURE", "content": "The bCLEARer process has been modularized (see [53, App. B], [54]) into a component architectural pattern. We describe this in the first section.\nIn the earlier comparison of current methodologies, we assessed them relative to two levels: generalization and digitalization. We now describe how bCLEARer addresses these levels in the second and third sections below. In the final, fourth, section we look at whether it is better to surface-*computerize (using bCLEARer) in vitro or in vivo."}, {"title": "1) bCLEARer's Pipeline Component Architecture Framework", "content": "The bCLEARer process has a pipeline (pipe-and-filter) architecture [54], a prevalent approach for data transformation. This architecture consists of a sequence of processing components, arranged so that the output of each component is the input of the next one creating a 'flow'. The pipeline architecture has, as the 'pipe-and-filter' name suggests, a series of pipe and filter components, where pipes pass data to and from filters that transform the data the pipeline flow. The architecture can be nested, in that filters can encapsulate a sub-pipeline process.\nThis generic architectural pattern is refined into a more constrained pattern for bCLEARer's more specific needs. It must include the components of the ontologization process in a structure where the specific arrangement of components can be dictated by the needs of the project and this arrangement can flexibly evolve over time, potentially into a radically different shape.\nTypically, it is divided into three broad levels:"}, {"title": "a) The bCLEARer stage types", "content": "While the contents of the individual thin slices and bUnits level vary from project to project depending upon their needs, as well as evolving over time, the bCLEARer stage types are a more stable architectural feature. The design of these types is motivated by the 'separation of concerns' [21", "56": "."}]}