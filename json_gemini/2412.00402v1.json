{"title": "DroidCall: A Dataset for LLM-powered Android Intent Invocation", "authors": ["Weikai Xie", "Li Zhang", "Shihe Wang", "Rongjie Yi", "Mengwei Xu"], "abstract": "The growing capabilities of large language models in natural language understanding significantly strengthen existing agentic systems. To power performant on-device mobile agents for better data privacy, we introduce DroidCall, the first training and testing dataset for accurate Android intent invocation. With a highly flexible and reusable data generation pipeline, we constructed 10k samples in DroidCall. Given a task instruction in natural language, small language models such as Qwen2.5-3B and Gemma2-2B fine-tuned with DroidCall can approach or even surpass the capabilities of GPT-4o for accurate Android intent invocation. We also provide an end-to-end Android app equipped with these fine-tuned models to demonstrate the Android intent invocation process. The code and dataset are available at https://github.com/UbiquitousLearning/DroidCall.", "sections": [{"title": "1. Introduction", "content": "The advent of large language models (LLMs) revolutionizes natural language processing, enabling machines to understand and generate human-like language with unprecedented accuracy. In the realm of mobile computing, this advancement presents a significant opportunity for developing intelligent mobile agents (Li et al., 2024; Zhang et al., 2024b; Wen et al., 2024; Wang et al., 2023a). Specifically, these agents can leverage the rich ecosystem of built-in intents (int, 2024) provided by both the operating system and third-party applications on Android devices. These intents serve as a fundamental mechanism for inter-app communication and function invocation, such as sending messages, making phone calls, or triggering specific app features. By harnessing LLMs, mobile agents can interpret diverse and complex user instructions, seamlessly mapping them to the appropriate intents, and therefore automating user interaction with mobile devices.\nOn-device LLMs are necessary for building mobile agents due to privacy and latency constraints (goo, 2024; Lu et al., 2024b; Yin et al., 2024; Xu et al., 2023b; Yuan et al., 2024). Since user data are processed locally, sensitive information remains on devices, thereby mitigating risks associated with data transmission over networks. Moreover, on-device inference eliminates the need for constant internet connectivity. Various on-device LLM inference optimizations significantly reduce response time (Xu et al., 2024b; Yi et al., 2023a; Xu et al., 2024a), leading to a more responsive and fluid user experience.\nHowever, our investigations reveal a critical challenge: Existing device-affordable LLMs lack the capability of accurate intent invocation. For example, Llama3.2-1B (Dubey et al., 2024) only succeeds in 31.5% and 60.5% of the tasks in zero-shot and few-shot scenarios, respectively. This limitation is not due to inherent deficiencies in the models themselves but stems from the absence of specialized datasets tai-"}, {"title": "2. Related Work", "content": "2.1. LLM-based Agents\nLLMs have emerged as a significant advancement in the field of artificial intelligence, marking a new era in natural language processing and understanding. Among them, OpenAI's GPT series (Achiam et al., 2023) has ushered AI into the era LLMs, which have begun to enter the public eye and develop rapidly. Subsequently, numerous open-source LLMs (Yang et al., 2024; Team, 2024; Bai et al., 2023; Dubey et al., 2024; Liu et al., 2024a; Zhu et al., 2024; GLM et al., 2024) have emerged, gradually approaching and even rivaling the capabilities of GPT-4, which has empowered developers and researchers alike to harness the power of these advanced models to implement a variety of applications. Furthermore, models such as GPT-4V have endowed LLMs with visual capabilities (Yang et al., 2023b; Lu et al., 2024a; Wang et al., 2024c; Liu et al., 2024b), enabling them to undertake a broader and more complex array of tasks.\nBy employing some prompting techniques, such as React (Yao et al., 2022), Plan and Solve (Wang et al., 2023b), ReWOO (Xu et al., 2023a), it is possible to guide LLMs in planning for specific tasks. These approaches enable the models to use tools and interact with the external environment, thus enhancing their capabilities to perform more intricate tasks. Based on LLMs and innovative prompting methods, a variety of agents such as AutoGPT (Yang et al., 2023a), MetaGPT (Hong et al., 2023) and HuggingGPT (Shen et al., 2024b) which can serve as assistants to humans have emerged.\n2.2. Mobile Device Control Agents\nSignificant efforts have been made in controlling mobile devices using agents. Early work (Venkatesh et al., 2022; Wang et al., 2023a; Wen et al., 2024) design UI representations to bridge the gap between GUIs and natural language, enabling models to understand mobile screens. Later, with the advent of multimodal LLMs, agents become capable not only of processing textual inputs but also of receiving images, audio, or video as inputs. This enhancement allows them to perceive the external environment more effectively and accomplish more complex tasks. Work such as AppAgent (Yang et al., 2023c) and Mobile Agent (Wang et al., 2024b;a) integrate visual capabilities to implement agents on mobile devices.\nHowever, most existing agents have certain limitations. (1) Most of them utilize cloud-side LLMs such as GPT-4. Applications on edge devices prioritize user privacy protection, and implementing edge agents through invoking cloud-side LLMs cannot effectively safeguard user privacy. Additionally, agents cannot be used under poor network conditions. Our work addresses these issues by deploying SLMs on edge devices to control Android devices, which effectively avoids the aforementioned problems. (2) Existing agents heavily rely on simulating human actions to operate mobile devices, such as through tap and swipe gestures. In this study, we envision agents directly interfacing with mobile devices through intent invocation as a more efficient and accurate approach to replace potentially tedious and error-"}, {"title": "3. DroidCall Dataset and Workflow", "content": "In this section, we introduce the overall workflow of DroidCall, which comprises three key phases as shown in Figure 2: Function Predefinition, Data generation, Fine-tuning and Evaluation. In \u00a73.1, we first introduce Android intent, a key mechanism of Android. Based on the common intents in Android, we manually predefine 24 functions that can assist users in performing some common operations on Android. In \u00a73.2, we detail our method for generating the DroidCall dataset, the first open-sourced dataset for Android intent invocation. Our method requires minimal human supervision and can be easily extended. In \u00a73.3, we describe how we fine-tune LLMs and evaluate their performance. \u00a73.4 shows an end-to-end demonstration of device control using fine-tuned LLMs with DroidCall.\n3.1. Collecting Android Intents\nIn Android development, an intent is a fundamental messaging object used to request an action from another app component. Despite its simplicity, the intent system plays a crucial role in facilitating communication between com-"}, {"title": "3.2. Dataset Generation", "content": "In this section, we present a detailed description of the DroidCall dataset generation process. We first introduce the key components utilized in data generation: the sampler, collector, LLM and filter components. Subsequently, we elaborate on the critical phases of data generation: function predefinition, seed data generation, and data generation. The entire dataset generation process leverages GPT-4-turbo as the underlying language model. \n3.2.1. KEY COMPONENTS OF GENERATION PIPELINE\nSampler, LLM, Filter, and Collector are essential components of the data generation pipeline.\nSampler. A sampler is the component capable of taking multiple data sources as input (such as lists, jsonl files, etc.), sampling the required data from each data source according to a specific sampling strategy, and organizing the sampled data into a particular format as intended by the user for output.\nLLM. LLM serves as the engine for data generation. We employ the self-instruct (Wang et al., 2022) paradigm for data generation, which involves integrating data sampled by the sampler into specific prompt templates and then handing it over to the LLM for data generation. Therefore, the LLM is the core component of data generation, and its capabilities directly impact the quality of the generated data. In this paper, GPT-4-turbo is utilized as the LLM for data generation.\nFilter. A filter is used to process the output from the LLM after it has been generated, such as extracting structured data from the output of LLMs, discarding data that does not meet the required format, and eliminating data that is highly similar to existing data. In the framework, the LLM can be processed by a series of custom filters, offering a high degree of flexibility.\nCollector. A collector is the component designed to coordinate and utilize the aforementioned three components, acting as the manager of the entire data generation pipeline. After obtaining data from the sampler, the collector integrates the data into specified prompt templates, generates raw data through the LLM, processes the data generated by the LLM with a series of filters, and ultimately collects the results obtained.\n3.2.2. FUNCTIONS PREDEFINITION\nThe automated extraction of intents from the Android Open Source Project (AOSP) (AOS, 2024) source code is a complex endeavor due to the multitude of intents and the dynamic nature of the Android platform. Given the complexities involved in the automated extraction of intents, our methodology diverges from such approaches.\nWe have predefined 24 functions that cover common operations on Android and utilize common intents within these functions for their specific implementations. Subsequently, we will teach the LLM to operate Android by learning to use these predefined functions. These predefined functions act as an interface between the LLM and the intents, concealing the specific details of the intents from the LLM. This approach also circumvents the issue of different versions of Android causing the LLM's learned intent knowledge to become obsolete. Once the LLM has learned these functions, we only need to implement them on different versions of Android, and the LLM will be able to do the intents invocation on different versions of Android through the functions without needing to know the specifics of the intents. In particular, these functions can perform common operations on Android, which include:\n\u2022 Scheduling Assistant: Help users to set an alarm/timer, insert an event on calendar.\n\u2022 Contact Management: Add contacts, make phone calls.\n\u2022 Common Operations: Internet search, search on maps, open camera for taking photos or recording videos, open various settings.\n\u2022 Messaging Services: Compose text messages or emails.\nIn our framework, the method for predefining functions is the same as that for defining ordinary Python functions. We only need to write out the function signatures and provide the Google-style docstrings (Goo, 2024) for the function. Subsequently, we can automatically extract structured information describing the function from the function signature and the docstring. The extracted data have the format shown in Listing 1.\n3.2.3. DATA GENERATION\nWe follow the self-instruct paradigm (Wang et al., 2022; Taori et al., 2023) to build our data generation pipeline. We have two stages to generate data.\nSeed Generation Stage. When leveraging LLMs for synthetic data generation, incorporating high-quality examples in the prompt is crucial. This guidance helps maintain the data's quality and aligns it closely with human standards. These examples are called seed. It is often the case that these seeds are manually written and verified, which can be very labor-intensive and time-consuming. To avoid the time-consuming and labor-intensive task of manually writing seed data, we automatically generate a series of seed data"}, {"title": "3.3. Fine-tuning SLMs with DroidCall", "content": "Models. We fine-tuned a series of SLMs using the DroidCall dataset, including PhoneLM-1.5B (Yi et al., 2024), Qwen2.5-1.5B, Qwen2.5-3B (Yang et al., 2024; Team, 2024), Llama3.2-1B, Llama3.2-3B (Dubey et al., 2024), MiniCPM3-4B (Hu et al., 2024), Phi3.5-3.8B (Abdin et al., 2024) and Gemma2-2B (Team et al., 2024).\nModeling function-calling tasks. We regard function calling as an instruction following task, where the model's input consists of user query, available function descriptions, and task instructions. The output of the model is a specific representation for calling a function.\nIf a unified input-output format is designed for fine-tuning models from different vendors, there would be issues: different models use different formats instruction tuning. If we use a unified format for fine-tuning, this format may have a gap compared to the format used during the model's instruction tuning. This could potentially affect the model's performance when it comes to function calling. Most current models have undergone fine-tuning specifically for chat, which typically involve three roles: system, user, and assistant. So we can reuse model's own chat template to do function calling. Specifically, we put user query and available function descriptions in system prompt and user prompt and put the function calling result in assistant output. By adopting this approach, we can equip the model with the capability for function calling while avoiding a significant gap between the data used for fine-tuning and the knowledge the model has already acquired.\nSetups. We formatted the DroidCall dataset into the chat format described above, resulting in 10K training samples. We then fine-tuned the model using LoRA (Hu et al., 2022), with a LoRA rank of 8 and a LoRA alpha of 16. Additionally, we employed a linear learning rate scheduler, setting the learning rate to 1.41e-5 and the warmup ratio to 0.1. We train for 24 epoch and pick the best checkpoint. Details of prompt format are provided in Appendix B."}, {"title": "3.4. Putting It All Together", "content": "Using the DroidCall dataset, we equip SLMs with certain capabilities for Android intent invocation. To verify its effectiveness, we developed an Android application. The design of our demo is shown in Figure 5. This demo consists of two important components. One is the retriever, which is used to retrieve the most relevant functions. To implement this retriever, we utilized GTE (Li et al., 2023) to create word embeddings for the function descriptions and stored them in ObjectBox (obj, 2024), a vector database. When a user query arrives, we employ GTE for word embedding and retrieve the most relevant functions from ObjectBox, thus we have a simple and effective retriever. Another important component is a model capable of intent invocation, which takes in the user's query along with the functions retrieved by the retriever and outputs the function calls that can fulfill the user query. In our demo, we used PhoneLM-1.5B (Yi et al., 2024) fine-tuned on the DroidCall dataset as this model. It is worth noting that all of our model inference processes are completed on mobile phones. We utilized mllm (Yi et al., 2023b), a fast and lightweight multimodal LLM inference engine designed for mobile and edge devices, to carry out the inference for both GTE and PhoneLM. Finally, we have a demo that can help us completing common tasks on Andrid devices."}, {"title": "4. Experiments", "content": "We first explored the impact of different prompt designs on model fine-tuning. After considering both the length of the prompts and the model's performance after fine-tuning, we selected an appropriate format for our prompts. Subsequently, through experimentation, we demonstrated that using the DroidCall dataset is more effective than using a general function calling dataset in scenarios aimed at Android intent invocation. Finally, we presented a series of results showcasing the effectiveness of models fine-tuned with the DroidCall dataset.\nMetrics. To quantitatively assess the efficacy of function calling within our model, we introduce two distinct metrics: Accuracy and Soft Accuracy.\n\u2022 Accuracy. This metric evaluates the model's ability to precisely replicate the ground-truth function calls associated with a user query. A sample is deemed correct if and only if the model's output perfectly matches the ground truth in terms of both function identity and parameter values. Mathematically, Accuracy (Acc) is formalized as the ratio of the number of perfectly predicted samples ($N_{perfect}$) to the total number of samples ($N_{total}$):\n$Acc = \\frac{N_{perfect}}{N_{total}}$\n\u2022 Soft Accuracy. This metric offers a nuanced evaluation of the model's performance, especially when it produces function calls that are partially correct. For each function call, a score is assigned based on the proportion of accurately predicted parameters ($P_{correct}$) relative to the total number of parameters ($P_{total}$). Soft Accuracy ($Acc_{soft}$) is then computed as the mean of these scores across all function calls:\n$Acc_{soft}= \\frac{1}{F} \\sum_{i=1}^{F} \\frac{P_{correct, i}}{P_{total, i}}$\nwhere F denotes the total number of function calls.\nIt is important to note that the parameters of some functions are not straightforward to compare directly for correctness, such as parameters like title or subject. Semantic consistency is sufficient for these parameters; they do not need to match exactly to be considered correct. For such parameters, we employ models from the RoBERTa (Liu et al., 2019) series to compare semantic similarity. If the similarity exceeds a set threshold, it is considered correct. We set the threshold to 0.75.\nWe use the 200 data entries from the test split of DroidCall to evaluate SLMs. It is worth noting that in real-world applications, we need to use a retriever to retrieve the functions that are likely to be used. But in our work, we are not focused on the retriever. So when testing the Acc and Accsoft, we use a fake retriever that always retrieves the ground-truth functions."}, {"title": "4.1. Effect of Different Prompts", "content": "In \u00a7 3.3, we mentioned that the model input includes several key components: user query, available function descriptions, and task instructions. The model output is a specific representation for calling a function. The user query is provided by the user and is beyond our control. However, we can design the remaining three parts and observe how models perform after fine-tuning using different designs.\njson A minimalist and straightforward design is to directly use JSON data as available function descriptions and a specific representation for calling a function. We opt for JSON formatting due to its simplicity.\ncode Another approach is to leverage the prevalence of code data in large language models' pre-training. We hypothesize that using docstrings as available function descriptions and adopting a Python function call format for the specific representation for calling a function may yield superior results. This is because such data closely resembles the coding examples encountered during pre-training, potentially enhancing the model's comprehension and performance.\nshort In the above two formats, we provided a detailed description of the tasks the model is required to complete as the task instructions. However, this approach significantly increases the length of the prompt. We posit that for models undergoing fine-tuning, task instructions may not be essential. Through the fine-tuning process, models can learn to perform the function calling task without the need for explicit task instructions in the prompt. Consequently, we experimented with removing the task instructions from the previous formats, which we denote as json_short and code_short.\nWe experimented with fine-tuning the Qwen2.5-1.5B-Instruct model using four different types of prompts mentioned above. We selected nine checkpoints throughout the entire fine-tuning process to test for accuracy. As can be observed from the figure, the final outcome indicates that the json format performed slightly better. However, the upward trend among all four formats is consistent, and the code_short format is essentially on par with the code and json formats. Additionally, we tested the average of input tokens for the model under four different prompt formats on the DroidCall test dataset, as shown in Table 1. The code_short format has a significantly smaller number of tokens compared to the other prompts. After comprehensive consideration, we ultimately chose the code_short format for subsequent fine-tuning experiments."}, {"title": "4.2. Effectiveness of DroidCall", "content": "To verify that the DroidCall dataset can achieve better results in the task of controlling Android phones through Android Intent invocation, we compared the performance of the Qwen2.5-1.5B-Instruct and PhoneLM-1.5B models after fine-tuning on the DroidCall dataset and xlam-function-calling-60k (Liu et al., 2024d), a general function calling dataset.\nTo eliminate the influence of prompt design, we formatted both the xlam-function-calling-60k and DroidCall datasets using the code_short format. The xlam-function-calling-60k dataset comprises 60k data points, while DroidCall contains 10k. To ensure an equivalent number"}, {"title": "4.3. Performance of Different SLMs", "content": "To test the Android intent invocation capabilities of some existing SLMs tailored for the edge scenario and further verify the effectiveness of DroidCall, we tested the Acc and"}, {"title": "5. Conclusion", "content": "In this paper, we introduce DroidCall, a novel dataset specifically engineered to enhance the Android intent invocation capabilities of LLMs. Our approach diverged from conventional cloud-based models, focusing instead on on-device deployment to address privacy concerns inherent in mobile environments. In our work, we (1) build a highly customizable and reusable data generation pipeline, (2) construct DroidCall, a first-of-its-kind open-sourced dataset for Android intent invocation based on the pipeline, (3) fine-tune a series of models tailored for edge devices, enabling them to approach or even surpass the performance of GPT-4o in the specific task of intent invocation, (4) implement an end-to-end demo with mllm. Our work demonstrates the potential applications of small models on the edge. We have open-sourced all the code of the data generation, fine-tuning, and evaluation."}, {"title": "A. Data Generation Prompts", "content": "At the beginning of data generation, we first generate seed data. The prompt used to generate seed is shown as following:\nI need your help to generate some function calling datasets. I will provide you with a tool description, and you need to\ngenerate queries and corresponding answers based on this tool, i.e., the answers that call the tool to resolve the user's\nquery. Here are my requirements:\n1. For queries, try to use different vocabulary and syntax to ensure query diversity. Queries can be long or short,\ncomplex or concise. In short, try not to generate similar queries; I want to ensure query diversity.\n2. The language of the queries should be as diverse as possible. This means a query can be a command, a question, or\na request with detailed descriptions, etc.\n3. The generated queries should cover all possible uses of the tool as much as possible, meaning the coverage of\nvarious parameters should be comprehensive, ensuring the tool can be used to complete various forms of work.\n4. The generated queries should be solvable using the given tools.\n5. For the queries you generate, you should provide answers using the tool, i.e., give the tool used and the values for\neach parameter.\n6. When providing parameters, if a parameter has required=False, you may omit its value.\n7. The generated data must be presented in the format given in my example.\n8. The parameter values generated with function call generated must be values that can be inferred from the user's\nquery; YOU CANNOT FABRICATE PARAMETERS THAT CANNOT BE OBTAINED FROM THE USER'S\nREQUEST.\n9. Attach each answer with an id starting from 0. And if a tool should use the respone from another tool, you can\nreference it using #id, where id is the id of the tool.\nfollowing are some examples:\n$examples\nNow I will give you a tool, and you help me generate 15 query-answer pairs.\nREMEMBER TO GENERATE THE RESULT IN JSON FORMAT LIKE THE EXAMPLE ABOVE REMEMBER\nNOT TO FABRICATE PARAMETERS FOR TOOLS. PARAMETERS SHOULD BE INFERED FROM USER\nQUERY.\ntool: $tool\nIn the prompt above, $examples will be replace by random samples sampled from xlam-function-calling-60k (Liu et al.,\n2024d). Below is an example:\ntool: {\n\"name: \"...\",\n\"description\": \"...\",\n\"arguments\": {\n}\n}\nresponse: {\n\"query\": \"...\",\n\"answers\": [\n{\n}\n]\n}\n$tools will be replace by json formatted predefined function, below is an example:"}, {"title": "B. Function Calling Prompts", "content": "In \u00a7 4.1, we've mentioned that we have tested 4 format of prompt: json, code, json_short and code_short. To unify our\nfine-tuning, we use chat to do function calling thus we only need to design the part of system, user and assistant using chat\ntemplate.\nIn json or code format, the system prompt would be:\nYou are an expert in composing functions. You are given a query and a set of possible functions. Based on the query,\nyou will need to make one or more function calls to achieve the purpose. If none of the function can be used, point it\nout. If the given question lacks the parameters required by the function, also point it out. Remember you should not\nuse functions that is not suitable for the query and only return the function call in tools call sections.\nin json_short or code_short the system prompt would be:\nYou are an expert in composing functions.\nThe user part of json or code is:\nHere is a list of functions that you can invoke:\n$functions\nShould you decide to return the function call(s), Put it in the format of\n$format_description\n$example\nIf there is a way to achieve the purpose using the given functions, please provide the function call(s) in the above\nformat. REMEMBER TO ONLY RETURN THE FUNCTION CALLS LIKE THE EXAMPLE ABOVE, NO OTHER\nINFORMATION SHOULD BE RETURNED.\nNow my query is: $user_query\n$functions is the functions descriptions provided by retriever, in code or code_short format, it would be like:\nName:\nsend_email\nDescription:\nCompose and send an email with optional attachments."}]}