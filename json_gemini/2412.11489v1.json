{"title": "HGSFusion: Radar-Camera Fusion with Hybrid Generation and Synchronization for 3D Object Detection", "authors": ["Zijian Gu", "Jianwei Ma", "Yan Huang", "Honghao Wei", "Zhanye Chen", "Hui Zhang", "Wei Hong"], "abstract": "Millimeter-wave radar plays a vital role in 3D object detection for autonomous driving due to its all-weather and all-lighting-condition capabilities for perception. However, radar point clouds suffer from pronounced sparsity and unavoidable angle estimation errors. To address these limitations, incorporating a camera may partially help mitigate the shortcomings. Nevertheless, the direct fusion of radar and camera data can lead to negative or even opposite effects due to the lack of depth information in images and low-quality image features under adverse lighting conditions. Hence, in this paper, we present the radar-camera fusion network with Hybrid Generation and Synchronization (HGSFusion), designed to better fuse radar potentials and image features for 3D object detection. Specifically, we propose the Radar Hybrid Generation Module (RHGM), which fully considers the Direction-Of-Arrival (DOA) estimation errors in radar signal processing. This module generates denser radar points through different Probability Density Functions (PDFs) with the assistance of semantic information. Meanwhile, we introduce the Dual Sync Module (DSM), comprising spatial sync and modality sync, to enhance image features with radar positional information and facilitate the fusion of distinct characteristics in different modalities. Extensive experiments demonstrate the effectiveness of our approach, outperforming the state-of-the-art methods in the VoD and TJ4DRadSet datasets by 6.53% and 2.03% in RoI AP and BEV AP, respectively.", "sections": [{"title": "Introduction", "content": "3D object detection is a critical task in autonomous driving, focusing on accurately determining the location, dimensions, and orientation of surrounding objects (Mao et al. 2023; Ma et al. 2023; Ghasemieh and Kashef 2022; Aung et al. 2024). Multiple sensors, such as camera, radar, and LiDAR, have been widely used for object detection with distinct data structures and properties. To achieve accurate and effective object detection, both semantic information, provided by the camera, and positional information, offered by radar or LiDAR, are crucial (Wu et al. 2024).\nHowever, compared with LiDAR, radar point clouds exhibit more pronounced sparsity degrading the detection performance, yet potential solutions for this issue are quite limited. Meth-"}, {"title": "Related Works", "content": "To further improve detection performance, an increasing number of studies focus on leveraging complementary information from different modalities through fusion approaches. Although a straightforward concatenation of features from various modalities can yield some improvement (Liu et al. 2023), challenges arise due to the limited angle resolution of radar and the absence of depth information in images, leading to feature misplacement. Therefore, developing effective strategies for feature fusion across modalities and mitigating the misalignment of features have emerged as critical issues that require immediate attention.\nSingle-Modality 3D Object Detection\nExisting camera-based detection methods typically require transforming the image features from Perspective View (PV) to BEV to ensure consistency between the input feature space and the output space. The transformation can be categorized into splatting and sampling. Splatting methods (Philion and Fidler 2020) project each pixel of the image to 3D space along the corresponding 3D rays and place image features to voxels passed by 3D rays. Sampling methods (Harley et al. 2023) project the center of voxels to images, and then sample the voxel features based on the positions they fall on the image features.\nOn the other hand, both radar and LiDAR can provide input for point-based object detection. Several previous works (Li, Luo, and Yang 2023; Meng et al. 2023; Hu, Kuai, and Waslander 2022; Li, Wang, and Wang 2021) convert the LiDAR point cloud into voxels to realize regular shapes. Then, feature extraction is usually conducted on these regular tensors. Unlike LiDAR, conventional automotive radar provides additional physical information, such as velocity and Radar Cross Section (RCS), but with sparser points and lower angle resolution, making it challenging to perform object detection on radar alone (Dreher et al. 2020; Popov et al. 2023; Ulrich et al. 2022). The emergence of 4D imaging radar eases these issues with more radar points and elevation angle (Dong et al. 2020; Liu et al. 2024a; K\u00f6hler et al. 2023). RadarMFNet (Tan et al. 2022b) conducts 3D object detection using a multi-frame 4-D radar point cloud to handle the sparsity in radar point clouds and shows that incorporating temporal and spatial features can improve detection capabilities. Moreover, in RPFA-Net (Xu et al. 2021) pillar-based design is employed to alleviate the influence of error in elevation angle. In addition to point cloud-based radar detection methods, recently, methods based on raw radar echo signals have also received more attention (Liu et al. 2024b; Paek, Kong, and Wijaya 2022; Rebut et al. 2022).\n3D Object Detection with Multi-Modality Fusion\nRecent advancements in 3D object detection focus on fusing image-based and point-based sensors to enhance system robustness and accuracy (Jiao et al. 2023; Zhang et al. 2024a; Yan et al. 2023; Yang et al. 2022). Notably, BEVFusion (Liu et al. 2023) introduces a technique that builds detection schemes for image and LiDAR in a unified BEV space, improving robustness in scenarios. The advancement of radar enables it as a key point-based sensor in autonomous driving (St\u00e4cker et al. 2022; Kim et al. 2023a). FUTR3D (Chen et al. 2023) employs transformer-based query mechanisms to integrate features from camera, radar, and LiDAR in autonomous driving, presenting a robust fusion approach."}, {"title": "Method", "content": "In this paper, we introduce a radar-camera fusion network named HGSFusion (Hybrid Generation and Synchronization), designed to fully leverage the potential of radar and facilitate the integration of camera and radar data for 3D object detection. In particular, the proposed Radar Hybrid Generation Module (RHGM) generates denser radar points with estimated points falling into masks, also known as foreground points. During the generation process, different probability distributions are employed to mitigate the impact of angle errors brought by DOA estimation. Subsequently, features from both image and radar are extracted by separate backbones and transformed into one unified Bird's Eye View (BEV) space. Then, the Dual Sync Module (DSM) utilizes spatial sync to enhance image features with position information in radar features and modality sync to alleviate the influences of image features under adverse lighting conditions. Extensive experiments conducted on VoD and TJ4DRadSet datasets achieve state-of-the-art (SOTA) performance, verifying the effectiveness and robustness of the proposed hybrid generation and Dual Sync.\nThe main contributions of our work are listed as follows\n\u2022 We propose a novel radar-camera fusion network HGS-Fusion to boost the fusion of radar points and images.\n\u2022 Radar Hybrid Generation Module (RHGM) leverages the distribution of point clouds derived from the radar point cloud imaging process to generate denser and higher-quality radar point clouds.\n\u2022 Dual Sync Module (DSM) guides 3D image features with positional information from radar and utilizes complementary information to produce fused BEV features.\nOverall Architecture\nThe overall architecture of HGSFusion is shown in Figure 2. In the radar branch, the RHGM utilizes raw radar points and images to obtain foreground points and generate denser radar points. These hybrid points (generated points, foreground points, and raw radar points) are encoded and sent to the radar backbone to generate radar BEV features and spatial patterns. In the image branch, corresponding monocular images are passed through the image backbone to obtain multi-scale image features for subsequent 2D-to-3D view transformation and height compression, yielding image BEV features. The image and radar BEV features are then fused in DSM before being fed into the detection head.\nRadar Hybrid Generation Module (RHGM)\nPoint Cloud Generation. Point cloud generation primarily involves three steps: obtaining foreground points, acquiring probability distribution, and generating hybrid points. The overall process is illustrated in Figure 3.\n1) Obtaining foreground points. With the radar-camera transformation matrix and the camera intrinsic matrix, raw radar points are projected onto the corresponding images (Yin, Zhou, and Kr\u00e4henb\u00fchl 2021). The i-th raw point can be expressed as $P_{raw,i} = [u_i, v_i, d_i, f_i]$, where $u_i$ and $v_i$ are the pixel coordinates in the image, $d_i$ is the depth, and $f_i$ represents other physical features such as RCS and velocity.\nNext, corresponding image instance masks are predicted via a semantic segmentation network. Projected points that fall within these masks are identified as foreground points. Similar to raw points, the i-th foreground point is represented as $P_{fore,i} = [U_i, V_i, d_i, f_i, s_i]$, where $s_i$ is a one-hot semantic feature indicating class labels after semantic segmentation.\n2) Acquiring probability distribution. Next, the key challenge is to generate denser point clouds with higher quality based on the distribution of these foreground points. We overcome the difficulties by considering point generation as a sampling processing, where the probability distribution is characterized by the Probability Density Function (PDF) of the generated points in the given region. A straightforward method is to set a uniform distribution as the PDF of generated points within each mask, formulated as\n$f_u(u,v) = \\frac{1}{A}$ (1)\nwhere A is the area of the uniform distribution. This method can leverage image information to increase the number of points, which may yield good performance for LiDAR, as LiDAR point clouds typically follow a consistent pattern, especially in mechanical scanning systems. However, radar point clouds exhibit different distributions, since they are derived from CFAR detection and DOA estimation, which inherently include estimation errors. Consequently, uniform generation may not be a good choice for radar points.\nDue to the fact that radar points are more likely to be distributed near foreground points, regions with and without\nnearby foreground radar points should be considered separately. Specifically, the areas centered around foreground points $(u_i, v_i)$ with a radius of r pixels are referred to as regions with nearby foreground points, defined as\n$R_i(u,v) = \\{(u,v) \\in R_m | (u - u_i)^2 + (v - v_i)^2 < r^2\\},$ (2)\nwhere $R_m$ is the region of instance masks. Then the areas out of these regions are considered to have no nearby points.\nFor the area near foreground points, the PDF of generated points should satisfy two properties: i) The generation probability near foreground points should be higher than areas without foreground points nearby. ii) The probability increases monotonically with the decreased distance from the foreground points. In our method, the generation probability distribution of the regions near foreground points $(u_i, v_i)$ is modeled by the Gaussian distribution as\n$f_g(u, v) = \\frac{1}{2 \\pi b_1 b_2} \\exp \\left[-\\frac{1}{2} \\left(\\frac{(u - u_i)^2}{b_1^2} + \\frac{(v - v_i)^2}{b_2^2} \\right)\\right],$ (3)\nwhere $b_i$ is the standard deviation. In the regions without foreground points nearby, radar points can be generated via uniform distribution $f_u(u, v)$ due to the inexistence of prior information.\n3) Generating hybrid points. Then the generation probability distribution for the entire region can be formulated as\n$f_H(u,v) = \\begin{cases}\nf_G(u, v) & (u, v) \\in R_i(u, v), \\\\\nf_u(u,v) & (u, v) \\in \\mathbb{C}_{R_m} R_i(u, v), \\\\\n0 & (u, v) \\neq R_m,\n\\end{cases}$ (4)\nwhere $\\mathbb{C}_{R_m} R_i(u,v)$ is the complementary regions of $R_i(u, v)$, i.e., the regions without foreground points nearby.\nThe generation probability distribution $f_H(u, v)$ is used to generate points $G_i = [u_i, v_i]$ that lie within the image. To obtain the depth and features of these generated points, we calculate the distances from each generated point g to the foreground points $P_{fore}$. The depth and feature of the nearest foreground point are then assigned to each corresponding generated point, resulting in $G_i = [U_i, V_i, d_i, f_i, s_i]$. To enable these generated points to serve as input for the network, they need to be projected back into the radar coordinate system using the camera intrinsic matrix and the camera-radar transformation matrix, resulting in the generated points in radar coordinates $G_i = [x_i, y_i, z_i, f_i, s_i]$, where $x_i, y_i$, and $z_i$ are the coordinates in radar coordinate system.\nSeparate Radar Point Encoding. To retain as much information as possible from the point clouds, the generated radar points $G$, raw points $P_{raw}$, and foreground points $P_{fore}$ are all used as the input. However, the lack of semantic features in $P_{raw}$ results in an inconsistency in feature length and incompatibility for direct network input. Although it is possible to use two separate radar backbones for distinct feature extraction, it would introduce additional computational costs and risks of overfitting. Therefore, it is necessary to encode radar points with equal-length features before inputting them into the network.\nRaw radar points $P_{raw}$ only contain positions and radar physical features while generated radar points $G$ and radar foreground points $P_{fore}$ encompass the additional semantic feature. One simple encoding strategy, namely Concat Encoding, to align these features is to pad zeros at the end of raw point features, formulated as $[x_i, y_i, z_i, f_i, 0_s]$, where $0_s$ is the zero padding, with the generated points and foreground points invariant. Another enhanced encoding strategy, referred to as Differentiable Encoding, is formulated as $[x_i, y_i, z_i, f_i, 0_s, c_i]$ and $[x_i, y_i, z_i, f_i, s_i, c_i]$, respectively, where $c_i$ is the point type using one-hot encoding to distinguish different points. Generated points and foreground"}, {"title": "Dual Sync Module", "content": "points share the same encoding but with different point types.\nHowever, both Concat Encoding and Differentiable Encoding may be limited in representation, since pillar-based (Shi, Li, and Ma 2022; Lang et al. 2019) detectors mix points through average operation in each pillar, making it difficult to distinguish different point types when features are placed at the same position. Herein, we place the physical and semantic features of points in different places to help distinguish points, defined as distributed features. Then, for the points, which lack the corresponding features, zero padding is employed to ensure that they share the same length. The entire process is referred to as Separate Encoding, and it can enhance the distinction between different types of points and shield them from interfering with the features of other points. Specifically, the proposed encoding of raw points $P_{raw}$ can be represented as $[x_i, y_i, z_i, f_i, 0_f, 0_s, c_i]$. Similarly, the encoding of $G$ and $P_{fore}$ can be represented as $[x_i, y_i, z_i, 0_f, f_i, s_i, c_i]$ with different point types. Finally, encoded radar points are concatenated, pillarized, and fed into the radar backbone, yielding radar BEV features $F_R \\in \\mathbb{R}^{C \\times X \\times Y}$, where C is the number of channels, and X and Y denote the dimensions of BEV feature map, respectively. All the above encoding strategies are illustrated in Figure 4.\nThe lack of depth information in images and the low-quality features under adverse lighting conditions present significant challenges for 3D object detection. In this subsection, we will introduce the DSM comprising spatial sync and modality sync to address these issues.\nSpatial Sync. Radar point clouds encompass spatial information that is absent in images, allowing for the enhancement of image features by using radar. In spatial sync, radar features are utilized to explicitly predict the probability of object presence at various spatial locations, referred to as spatial patterns. Notably, we incorporate the atrous convolution to enlarge the receptive field, since objects, which are relatively large compared to the pillar size, may span a large region of the feature map. The entirety of the spatial pattern prediction $S_R \\in \\mathbb{R}^{1 \\times X \\times Y}$ can be formulated as\n$S_R = \\sigma (Conv (AtrousConv (F_R))),$ (5)\nwhere $\\sigma$ is the sigmoid activation function. The radar spatial pattern is supervised by focal loss with ground truth generated by bounding boxes. Then the radar spatial pattern is multiplied with image BEV features $F_I \\in \\mathbb{R}^{C \\times X \\times Y}$. The enhanced image BEV features $F'_I$ can be formulated as\n$F'_I = S'_R F_I,$ (6)\nwhere $S'_R$ is the spatial pattern broadcasted along the channel dimension and is the element-wise multiplication.\nModality Sync. The enhanced image features and radar features are in two separate modalities and need to be fused. It is observed that in adverse lighting conditions such as darkness or shiny lightning, the quality of image features is significantly degraded. In contrast, radar features are less"}, {"title": "Experiments", "content": "affected by lightning conditions. To leverage the distinct characteristics of different modalities, modality sync is employed to tackle this issue by predicting the importance of different modalities. In modality sync, the radar and image BEV features are first concated and fused with convolution layers, formulated as\n$F_{concat} = Conv (F_R \\oplus F'_I),$ (7)\nwhere $\\oplus$ is the concatenation operation along channel dimension. Then, the feature weights $V \\in \\mathbb{R}^{2C}$ measuring the varying importance of the feature map are predicted from the concated features $F_{concat} \\in \\mathbb{R}^{2C \\times X \\times Y}$, formulated as\n$V = \\sigma (Conv (AvgPooling (F_{concat}))) .$ (8)\nAfter the whole Modality Sync process, the final fused BEV feature map can be formulated as\n$F = V' F_{concat},$ (9)\nwhere $V'$ is the feature weights broadcasted along the spatial dimensions of feature maps. Finally, the fused BEV features F are used for the downstream 3D object detection.\nDataset and Metrics\nIn our study, we conduct experiments on 4D millimeter wave radar datasets, VoD dataset (Palffy et al. 2022) and TJ4DRadSet dataset (Zheng et al. 2022). We adopt the official split schemes of the datasets. For the VoD dataset, the official evaluation metrics are AP in Entire Annotated Area AP (EAA AP) and AP in the Driving Corridor (RoI AP). They are conducted in the Entire Annotated Area and the Driving Corridor area ranging (-4m < x < 4m, z < 25m) in camera coordinates. IoU thresholds are set to 0.5, 0.25, and 0.25 for cars, pedestrians, and bicycles, respectively. The TJ4DRadSet dataset includes AP in both 3D and BEV space. Evaluation is limited to targets within 70 meters away from the sensor. IoU thresholds for car, pedestrian, and cyclist are the same as those of the VoD dataset. For the truck category, the IoU threshold is set to 0.5.\nImplementation Details\nResNet-101 is employed as the image backbone with pre-trained weight from DeepLabV3 and frozen to prevent overfitting. Mask2former (Cheng et al. 2022) is utilized as the segmentation network, and Radar PillarNet (Zheng et al. 2023) is utilized as the radar backbone. The radius r is set to 51, and in each mask, 250 enhanced point clouds are generated, where 50 via Gaussian generation and 200 via uniform generation. The detection head adopts an anchor-based approach. Horizontal flipping, global rotation, and global scaling are applied as data augmentation during training. We use AdamW as the optimizer and train the proposed network for 25 epochs with a learning rate of 0.001 and batch size of 4.\nSOTA comparison\nVoD validation set. Our method is tested on the validation set of the VoD dataset, with the results presented in Table"}, {"title": "Comprehensive Analysis", "content": "1. The EAA AP and RoI AP achieve the best performance, surpassing the SOTA LXL by 2.65% and 6.53%, respectively. In terms of Car and Pedestrian categories, our method achieves the best performance. Especially for the Car category, the proposed HGSFusion can greatly densify radar points and promote fusion between radar and camera, yielding an improvement of 5.66% and 8.79% compared with FUTR3D and TL-4DRCF. However, a performance decline is observed in the Cyclist category. This decline is due to the presence of various bicycle-like objects in the VoD dataset scenes, such as parked bicycles, bicycle racks, and scooters. These objects are difficult to distinguish, affecting the quality of the generated radar point clouds and consequently leading to a drop in performance.\nTJ4DRadSet test set. To validate the generalization capability of the proposed model, we also conduct experiments on the TJ4DRadSet dataset, with the results presented in Table 2. The model surpasses the SOTA LXL by 0.89% in 3D mAP and by 2.03% in BEV mAP. These improvements indicate that the model effectively integrates images to generate denser radar point clouds and effectively fuse features of different modalities.\nAblation of Proposed Components. Ablation experiments are performed on the VoD validation set to evaluate the impact of different modalities and the proposed modules, with the results presented in Table 3. As can be seen, the direct fusion of features from both modalities (#3) yields promising results compared to single modality (#1-2), indicating the existence of complementary information between images and radar points. In addition, the separated introduction of the RHGM (#4) and DSM (#5) improves\nnetwork performance with 2.41% and 1.17% in EAA AP and 1.56% and 1.18% in RoI AP, respectively. Hence, both RHGM and DSM can help the network boost detection performance. The complete HGSFusion (#6), which utilizes RHGM and DSM, achieves the best performance, outperforming the baseline by 4.14% and 6.19% in EAA AP and RoI AP, respectively. This stems from the fact that the hybrid radar points incorporate semantic information from images, improving the quality of radar features. Additionally, the DSM leverages position information from radar to en-"}, {"title": "Comparisons between Radar Point Generation Schemes.", "content": "hance image features while alleviating the impact of low-quality image features under adverse lighting conditions.\nHerein, we investigate the impacts of different generation schemes on detection performance by fixing other parameters and adjusting the ratio of Gaussian generation points to the total points. The results are presented in Figure 6. As can be observed, using a purely uniform generation method does not yield the best performance, lagging behind the proposed hybrid scheme by 0.82% and 3.20% in EAA AP and Rol AP, respectively. This is because uniform generation only brings segmentation information into the generated radar points without considering the angle estimation errors introduce by the DOA estimation algorithm. However, adopting a pure Gaussian generation also fails to achieve optimal performance, falling behind the hybrid scheme by 1.83% and 3.52% in EAA AP and RoI AP, respectively. This may arise from that pure Gaussian generation makes the generated points distributed near foreground points, yielding almost no points in the area without foreground points. As a result, the hybrid generation approach combining uniform and Gaussian generation effectively mitigates these shortcomings and achieves the best performance.\nDiscussion on Radar Point Cloud Encoding. Similar to the raw radar points, the enhanced radar points possess both positions and physical features. These points are encoded and fed into the radar backbone. Comprehensive experiments are conducted to investigate the impact of encoding strategy on overall performance. The encoding strategies are visualized in Figure 4 and their corresponding experimental results are shown in Table 4. One simple encoding strategy, referred to as Concat Encoding, is to mix these points together indiscriminately. The achieved performance improvement is attributed to semantic information contained in generated points brought by the proposed RHGM. As aforementioned, the Differentiable Encoding that incorporates one-hot encoding, which is point type, can better distinguish these points. As can be seen, HGSFusion with Differentiable Encoding achieves higher performance improvement by 0.56% and 2.47% in EAA AP and RoI AP, respectively. Look into the process of radar feature extraction, and it can be observed that points within the same pillar are grouped together, limiting the discriminative ability when features are placed at the same location. Hence, the adoption of the distributed features and zero-padding in Separate Encoding outperforms the baseline by 2.97% and 5.01% in EAA AP"}, {"title": "Influences of Lightning Conditions.", "content": "and RoI AP, respectively.\nBy considering the varying lighting conditions across different sequences in TJ4DRadSet, we divide the whole dataset into three subsets: dark, normal, and shiny. Then, we evaluate our proposed HGSFusion on the subsets, as well as two baseline networks, Base-R and Base-R+C (excluding RHGM and DSM). Base-R uses only raw radar point clouds as input, while Base-R+C uses both raw radar point clouds and the image. The results are presented in Table 5. As listed in Table 5, the fusion network outperforms the baseline network for all lighting scenarios. In \u201cDark\u201d scenes, the information captured by the camera is limited and may even contain errors. Hence, the performance can degrade when the camera input is incorporated. However, our proposed HGSFusion network can leverage radar features to enhance image features and achieve performance improvement by 11.41% and 11.31% in 3D mAP and BEV mAP, respectively. Conversely, in the \"Normal\" and \"Shiny\" conditions, the image contains more information, leading to performance improvement when incorporated. Our proposed HGSFusion network can utilize images to generate denser radar point clouds, further boosting performance up to 2.91% and 3.13% in 3D mAP and BEV MAP, respectively. The improvements demonstrate the robustness of our fusion network in all lighting conditions."}, {"title": "Conclusion", "content": "In this paper, we propose HGSFusion, a pioneering network that fuses 4D imaging radar and images to enhance 3D object detection. The sparsity of radar points and angle estimation errors are mitigated by innovatively using RHGM hybrid generation that considers DOA estimation errors. In DSM, Spatial Sync leverages the position information from radar to enhance the image features, compensating for lack of depth in an image. Moreover, DSM also employs Modality Sync to measure the importance of different features and thus reduce the impact of low-quality image features under adverse lightning. Extensive experimental results demonstrate that HGSFusion achieves state-of-the-art performance in prevalent VoD and TJ4DRadSet datasets."}, {"title": "Appendix", "content": "Implementation Details\nThe proposed model is implemented by using the Open-PCDet framework, which is an open-source project designed for 3D scene perception.\nFor the VoD dataset, the hyperparameters were configured same with the official settings. The point cloud range (PCR) was set to (0 < x < 51.2m), (\u221225.6m < y < 25.6m), (-3m < z < 2m); voxel size was set to (0.16m \u00d7 0.16m \u00d7 0.16m); and the size of BEV feature maps for both radar and image was set to (320 \u00d7 320). The anchors (length, width, height) for the Car, Pedestrian, and Cyclist categories were set as (3.9m, 1.6m, 1.56m), (0.8m, 0.6m, 1.73m), and (1.76m, 0.6m, 1.73m), respectively. In the TJ4DRadSet dataset, PCR was set to (0 < x < 69.12m), (-39.68m < y < 39.68m), (-4m < z < 2m). The voxel size used was (0.32m \u00d7 0.32m \u00d7 0.32m), resulting in BEV feature maps of size (216 x 248). The anchors for car, pedestrian, cyclist, truck are (4.56m, 1.84m, 1.70m), (0.80m, 0.60m, 1.69m), (1.77m, 0.78m, 1.60m), and (10.76m, 2.66m, 3.47m) respectively.\nTransformation of radar points.\nIn Radar Hybrid Generation Module (RHGM), the raw radar points $P_{raw, radar} = [x_{i,R}, y_{i,R}, z_{i,R}, f_i]$ are projected onto the image. The process can be divided into two steps. Firstly, raw radar points are transformed from radar coordinate system to camera coordinate system using the radar-camera transformation matrix to obtain the raw points in the camera coordinates $P_{raw,camera} = [x_{i,C}, y_{i,C}, z_{i,C}, f_i]$, formulated as\n$\\begin{pmatrix}\nx_{i,C} \\\\\ny_{i,C} \\\\\nz_{i,C} \\\\\n1\n\\end{pmatrix} = T_{R->C}\\begin{pmatrix}\nx_{i,R} \\\\\ny_{i,R} \\\\\nz_{i,R} \\\\\n1\n\\end{pmatrix}$ (10)\nwhere $T_{R->C} \\in \\mathbb{R}^{4x4}$ is the radar-camera transformation matrix. Secondly, the raw points in the camera coordinate system are projected onto the image with the camera intrinsic matrix $I \\in \\mathbb{R}^{3x4}$, and the raw points in image coordinates are obtained as $P_{raw,image} = [u_{i,I}, v_{i,I}, f_i]$, formulated as\n$\\begin{pmatrix}\nu_{i,I} \\\\\n1\n\\end{pmatrix} = \\frac{1}{z_{i,C}}I\\begin{pmatrix}x_{i,C} \\\\\ny_{i,C} \\\\\nz_{i,C} \\\\\n1\n\\end{pmatrix}$ (11)\nSimilarly, the process of projecting generated points on the image back to the radar coordinates is essentially the inverse of the aforementioned process, simply substituting the $z_{i,C}$ with the assigned depth $d_i$.\nExplanation of the Cyclist Category in the VoD Dataset.\nIn the RHGM module, an instance mask is first obtained by using a segmentation network. Herein, the instance masks"}, {"title": "Exploration of Spatial Pattern Paradigm.", "content": "can be classified as \u201cCar\u201d, \u201cPedestrian", "Cyclist": "However, the VoD dataset contains many categories similar to \"Cyclist\", such as \u201cUnused Bicycle\u201d, \u201cBicycle Rack\", \u201cScooters\u201d, and \u201cMotors\u201d. These categories are similar in appearance to \u201cCyclist\u201d, as shown in Figure 7. The bicycles in yellow circle are annotated as \u201cCyclist\u201d in VoD dataset while the bicycles in red circle are annotated as \u201cUnused Bicycle\u201d. Their similar appearance makes them difficult to distinguish.\nThe spatial pattern generated from radar features is used to enhance the image features. We explore different forms of radar spatial pattern, and the corresponding results are listed in Table 7. Initially, we test 3D and 2D spatial patterns. Existing 4D millimeter-wave radar systems have limited angle resolution due to limitations of the transmit and receive antennas. Although RHGM can alleviate the negative effects of low angle resolution, the insufficient elevation information of radar points prevents it from determining accurate heights. Consequently, the 2D spatial pattern network shows better detection performance by 1.90% and 4.07% in EAA AP and RoI AP, respectively. Additionally, we verify the necessity of explicit supervision of the radar spatial patterns. By using 3D bounding boxes, we directly generate ground truth for radar spatial patterns. This approach introduces spatial pattern information without additional annotations and achieves the best performance for 2D spatial pattern outperforming 3D spatial pattern by 2.55% and 3.88% in EAA AP and RoI AP, respectively.\nImpacts of Object Distance.\nWe evaluate the network at distances of 0-25m, 25-50m, and 50-70m to investigate the impact of object distance on detection performance as presented in Table 6. At close distances, the rich semantic information in images can help improve detection performance. Additionally, the proposed HGSFusion network can further utilize the information in the image to enhance the point clouds, achieving performance improvement by 7.45% and 7.03% in 3D mAP and BEV mAP, respectively. In medium and distant cases, due to the lack"}]}