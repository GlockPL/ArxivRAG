{"title": "Unsupervised Model Diagnosis", "authors": ["Yinong Oliver Wang", "Eileen Li", "Jinqi Luo", "Zhaoning Wang", "Fernando De la Torre"], "abstract": "Ensuring model explainability and robustness is essential for reliable deployment of deep vision systems. Current methods for evaluating robustness rely on collecting and annotating extensive test sets. While this is common practice, the process is labor-intensive and expensive with no guarantee of sufficient coverage across attributes of interest. Recently, model diagnosis frameworks have emerged leveraging user inputs (e.g., text) to assess the vulnerability of the model. However, such dependence on human can introduce bias and limitation given the domain knowledge of particular users.\nThis paper proposes Unsupervised Model Diagnosis (UMO), that leverages generative models to produce semantic counterfactual explanations without any user guidance. Given a differentiable computer vision model (i.e., the target model), UMO optimizes for the most counterfactual directions in a generative latent space. Our approach identifies and visualizes changes in semantics, and then matches these changes to attributes from wide-ranging text sources, such as dictionaries or language models. We validate the framework on multiple vision tasks (e.g., classification, segmentation, keypoint detection). Extensive experiments show that our unsupervised discovery of semantic directions can correctly highlight spurious correlations and visualize the failure mode of target models, without any human intervention.", "sections": [{"title": "1. Introduction", "content": "Contemporary methods for assessing computer vision algorithms primarily rely on the evaluation of manually labeled test sets. However, relying solely on metric analysis of test sets does not ensure the robustness or fairness of algorithms in real-world scenarios [5]. This limitation arises from several factors. Firstly, while this approach is effective at gauging performance under known test conditions, it does not proactively address unforeseen model failures.\nSecondly, it is often infeasible to gather test sets that encompass all potential scenarios or relevant attributes of interest. Lastly, the process of constructing test sets is typically resource-intensive, time-consuming, and susceptible to errors. To address such issues, this paper leverages large pre-trained models (LPMs), trained on extensive datasets comprising millions of samples, as a means to assess potential shortcomings in computer vision models. The question that we try to address is: Can these LPMs be applied to evaluate various computer vision tasks (e.g., segmentation) by uncovering possible failure modes and limitations in a completely unsupervised manner?\nAn emergent approach to discover model failure modes without exhaustive test sets makes use of counterfactual explanations [9, 30]. Along this line of research, model defects are studied through challenging images that lead to model failure modes. However, earlier efforts deceive the target models via pixel-level adversarial perturbations [8, 29], which are not informative and do not explain model failures. A few works produce adversarial examples by semantically perturbing base images along semantics [16, 35], but they focus on effective attacks rather than model evaluation. To gain insights into a model's flaws, we need counterfactual explanations that reveal the semantic differences that lead to model failures. Recently, [28] proposed a zero-shot method to analyze model sensitivity to attributes via counterfactual explanation. Nonetheless, this method still requires domain knowledge from the user about potential attributes of interest, which can introduce biases and limit its outcome to the domain knowledge of particular users.\nTo address the issues mentioned above, we introduce Unsupervised Model Diagnosis (UMO) to discover the model failures and perform open-vocabulary model diagnosis without any user input or domain knowledge. Fig. 1 illustrates the main idea of our work. UMO comprises two main input components: (a) a target model (for vision tasks) provided by the user; in this paper, we address classification, segmentation, and key-point detection, (b) a collection of foundation toolkits, i.e. LPMs, for counterfactual image generation as well as language models for semantic analysis. The output of UMO is a diagnostic report that includes (c) a set of visual counterfactual explanations and (d) the corresponding list of counterfactual image attributes with their associated semantic similarity scores, see Fig. 1 right.\nThe resulting diagnosis offers insights into the specific visual attributes to which the model is vulnerable. This information can guide actions such as collecting additional data for these attributes or adjusting their weights in the training process. Our complete pipeline operates in an unsupervised manner, eliminating the requirement for data collection and label annotation.\nTo summarize, our proposed work brings two main advancements when compared to previous efforts:\n\u2022 We propose an unsupervised framework for model diagnosis, named UMO, that bypasses the tedious and expensive requirement of human annotation and user inputs.\n\u2022 UMO utilizes the parametric knowledge from foundation models to ensure accurate analysis of model vulnerabilities. The framework does not require a manually-defined list of attributes to generate counterfactual examples."}, {"title": "2. Related Work", "content": "This section discusses past work on vision model diagnosis and visual manipulation through generative latent space."}, {"title": "2.1. Latent Generative Models", "content": "Generative models, especially StyleGAN [17, 18, 39] and Diffusion Models [11, 38, 44], have semantic latent spaces that are differentiable, and can be used to edit image attributes [12, 40, 41]. StyleSpace [51] found a more disentangled latent space by manipulating the modulation weights (i.e., style codes) in StyleGAN affine transformation. One step further, StyleCLIP [32] guided the generation sampled from StyleSpace by minimizing the CLIP [36] loss between the sampled image and the user text prompt. Similarly, DiffusionCLIP [20] applies image modification using text pairs via CLIP directional loss. Note that both StyleCLIP and DiffusionCLIP aim to apply targeted edits given by the user, whereas UMO aims to learn critical edits in an unsupervised fashion. Recent advances in large language models enable informative supervision of the generation process. [26, 58] use language models to generate multimodal conditions for enhanced compositionality and reasoning of diffusion models. In the line of controlling diffusion latent space, ControlNet [57] learned a task-specific network to impose constraints on Stable Diffusion [38]. Asymmetric reverse process (Asyrp) [21] discovered that the bottleneck layer of the U-Net in the diffusion model encodes meaningful semantics; hence, the authors proposed to learn a unified semantic edit direction applicable across all images in this bottleneck latent space. Our work adopts the similar concept of optimizing globally effective counterfactual directions for the target model. By manipulating the semantics and analyzing the learned direction, we can gain valuable insights into model failure visualization and bias identification."}, {"title": "2.2. Diagnosis of Computer Vision Models", "content": "Model diagnostics [3] originally referred to the validity assessment of a regression model, including assumption exploration and structural examination. Diverging from typical adversarial perturbations [42, 48], whose sole objective was to make the model fail, recent years have witnessed the trend of the vision community broadly adopting the term diagnose [28, 34, 45, 54] for understanding and evaluating the failure of deep vision models, particularly focusing on attribution bias, adversarial robustness, and decision interpretability to identify potential flaws. To search model failure cases, [8] first proposed pixel-space perturbations by signed gradient ascent to generate adversarial examples. [29] further advanced the philosophy by multi-step gradient projection, and [53] proposed to synthesize the pixel-space perturbation by generative model.\nHowever, [16, 35] claimed that such adversaries lack visual interpretability and proposed attacking the model by optimizing along fixed semantic axes of generative models. Similarly, in the literature of counterfactual explanation [46], methods [1, 13, 14, 19, 30, 43, 55] commonly focus on generating semantic attacks on a per-image basis, which overlook the global model-centric vulnerability diagnosis. Despite the effective instance-level counterfactual generation, such failure-driven attacks can be less informative for diagnosing model vulnerabilities (e.g., altering the perceived gender to fool a gender classifier). Hence, when directly applying these attack-oriented counterfactual explanations for model diagnosis, they usually require additional human interpretation to summarize individual failures. Besides being often designed for specific single task [13, 33, 55], previous methods [1, 37, 43] also require fine-tuning of the generative pipeline. Hence, we adopt a diagnosis-driven, task-agnostic, and resource-efficient counterfactual synthesis pipeline desirable for diagnosing instead of simply attacking models.\nTo emphasize the explainability requirement of model diagnosis and produce human-understandable outcomes, recent works visualized model failures by optimizing an attribute hyperplane [25], identifying error subgroups from cross-modality gaps [6, 54], recognizing sensitive style coordinates [22], searching semantic variations by unconditional generative models [15, 23], or fine-tuning language model to perturb prompts for text-condition generations [34]. Nevertheless, these approaches require either manually annotating the discovered failure and the collected test set, or training a model-specific explanation space for every new target model. Our approach addresses these shortcomings by performing diagnosis in an unsupervised manner with the help of foundation toolkits. More recently, ZOOM [28] proposed to analyze provided attributes in a zero-shot manner. Our method distinguishes itself from ZOOM in that: (1) UMO achieves automatic discovery instead of focusing on analyzing given attributes, (2) UMO is more exploratory in counterfactual edits while ZOOM can only analyze in restricted attribute directions, (3) UMO requires no prior knowledge from the user and hence circumvents potential human biases."}, {"title": "3. Method", "content": "Given a target model \\(f_\\theta\\), our pipeline consists of two stages as shown in Fig. 2. In the first stage, a latent generative model, denoted as \\(G_\\phi\\), (e.g., a Diffusion model or GAN) is used to discover counterfactual modifications, denoted as UMO(\\(f_\\theta, G_\\phi\\)), by directly optimizing the latent edit directions that can mislead the prediction of \\(f_\\theta\\), shown in Fig. 2(a). In the second stage, after the counterfactual optimization converges, we generate pairs of image embeddings (original, counterfactual) in Fig. 2(b). With these pairs, we are able to interpret and analyze the counterfactual attributes of the target model \\(f_\\theta\\) by computing semantic similarity scores with attribute candidates, as illustrated in Fig. 2(c)."}, {"title": "3.1. Counterfactual Optimization", "content": "Given a target model \\(f_\\theta\\) to diagnose, we first focus on discovering cases that lead to incorrect model predictions. To capture the failure modes, an effective solution is to generate counterfactual examples of the target model. Hence, our first step aims to learn latent edits representing semantic edit directions. When injecting such latent modification to the generative model \\(G_\\phi\\), the generated images are observed to have meaningful semantic changes [24] but are challenging to the target model. We denote each pair of the original image \\(x_i\\) and its edited adversarial counterpart \\(\\hat{x}_i\\) as a counterfactual pair. This section shows that UMO can discover and generate critical semantic counterfactual pairs for a given target model \\(f_\\theta\\) using various generative models \\(G_\\phi\\) (e.g., StyleGAN and Diffusion Model).\nSince the StyleSpace \\(S\\) is shown to be effective for semantic manipulation [32, 51], we choose to inject counterfactual edits in this latent space. We first initialize our latent edit vector \\(\\Delta s\\) in the space \\(S\\) as Gaussian noise. Then we sample \\(N\\) style vectors \\(\\{s_i\\}\\_{i=1}^N \\sim S\\) and generate \\(N\\) corresponding images \\(x_i = G_\\phi(s_i)\\). Note that optionally real images \\(x_i\\) can also be used and \\(s_i\\) is then obtained through GAN inversion[52]; but for the rest of the paper, we use synthetic images \\(x_i\\) to leverage free exploration in generative latent space for more diverse counterfactual explanations. For each original image \\(x_i\\), we inject the edit vector \\(\\Delta s\\) to the latent space \\(S\\) and obtain the edited image \\(\\hat{x}_i = G_\\phi(s_i + \\Delta s)\\). With the original and edited image based on the same initial latent vector, we compute the following loss \\(L\\):"}, {"title": "loss L:", "content": "\\(L(\\Delta s) = \\alpha L_{target} + \\beta L_{CLIP} + \\gamma L_{SSIM} + L_{reg}\\). (1)\nTo optimize w.r.t. \\(\\Delta s\\), we back-propagate the loss as shown in the backward process in Fig. 2(a).\nThe first component of the loss, \\(L_{target}\\), is the adversarial loss that ensures the learned \\(\\Delta s\\) can edit the original image to cause target model failure. This loss measures the distance between the model prediction on the edited image \\(f_\\theta(\\hat{x}_i)\\) and the opposite of the original model prediction \\(f_\\theta(x_i)\\). It is task-dependent: when diagnosing a binary classification task, we want to minimize the cross-entropy loss between \\(f_\\theta(x_i)\\) and \\(p_{cls} = 1 - f_\\theta(x_i)\\) such that the edits effectively mislead the classifier toward the opposite class; when diagnosing a keypoint detector or a segmentation model, we want the (perturbed) incorrect model predictions to be close to a randomized or targeted pseudo label (details in Sec. 4.3), denoted as \\(p_{kdet}\\) and \\(p_{seg}\\) respectively:\n(binary classifier) \\(L_{target}(\\hat{x}_i) = L_{CE}(f_\\theta(\\hat{x}_i), p_{cls})\\), (2)\n(keypoint detector) \\(L_{target}(\\hat{x}_i) = L_{MSE}(f_\\theta(\\hat{x}_i), p_{kdet})\\), (3)\n(segmentation model) \\(L_{target}(\\hat{x}_i) = L_{CE}(f_\\theta(\\hat{x}_i), p_{seg})\\). (4)\nOptimizing against \\(L_{target}\\) solely without constraints is insufficient to discover effective counterfactual examples. For example, with a binary classifier, the original image can be directly edited into the opposite class [1, 14, 19, 43, 55] which fails to reveal the failure modes of the target model. Hence we introduce the second loss term \\(L_{CLIP}\\) which ensures the generated counterfactual example is perceived by the zero-shot CLIP classifier as the same class/object as the unedited image. Denoting the CLIP zero-shot classifier as \\(C\\) and the list of class labels as \\(T\\), the loss defined as:\n\\(L_{CLIP}(\\hat{x}_i) = L_{CE}(C(x_i, T), C(\\hat{x}_i, T))\\). (5)\nWhile optimizing \\(L_{CLIP}\\) and \\(L_{target}\\) yields counterfactual examples to the target model, we also preserve the quality of the counterfactual by regularizing attribute changes and preserving semantic structures. To enforce these constraints, we include the SSIM loss [49] and the regularization as:\n\\(L_{SSIM}(\\hat{x}_i) = L_{SSIM}(x_i, \\hat{x}_i)\\), (6)\n\\(L_{reg}(\\Delta s) = ||\\Delta s||_1\\). (7)\nTo mitigate any implicit bias inherited from one specific generative backbone, we further enhance the reliability of our diagnosis pipeline through an ensemble of latent generative models. We propose to generate counterfactual images from multiple independent generative backbones and analyze the combined mixture of synthesized images. Hence, besides StyleGAN, we also adopt diffusion models for counterfactual generation. Asyrp [21] discovers a semantic latent space in diffusion models, in particular, DDPM [11] and DDIM [44]. Asyrp proposes to learn a simple network A that takes the hidden states from a previously denoised image \\(x_t\\) at the timestep \\(t\\) and outputs an edit vector to be injected to the middle bottleneck layer of each U-Net block. Similar to optimizing \\(\\Delta s\\) in the StyleSpace of StyleGAN, we optimize the network A in diffusion models to learn counterfactual edits of the target model.\nSince the decision behavior of the target model can be biased toward multiple attributes, we choose to optimize \\(k\\) distinct edits to ensure comprehensive diagnosis coverage and to improve optimization convergence by focusing each vector on one type of edit. We initialize \\(k\\) latent edit vectors for StyleGAN or \\(k\\) edit generation networks for diffusion models. Then, for each original latent vector, we first find the edit vector that most effectively perturbs the target model and only optimize this edit vector while leaving the remaining edit vectors unchanged. This procedure repeats at each iteration. Since distinct failure modes can emerge for different latent vectors, each edit vector converges to different and disentangled semantic edit directions throughout training, which enables the discovery of multiple biased attributes for the given target model. More details are shown in Appendix A."}, {"title": "3.2. Counterfactual Analysis", "content": "To render an intuitive diagnosis of the vulnerabilities of the target model, we interpret the attribute changes between the counterfactual pairs that lead to target model failure. Using the CLIP model as a common latent space, we match the counterfactual edits with text attribute candidates to provide analyses of the model vulnerabilities.\nAfter counterfactual optimization, we augment original latent vectors \\(s\\) into \\(\\hat{s}\\) by adding the learned edit vector \\(\\Delta s\\). Then we feed both \\(s\\) and \\(\\hat{s}\\) into the Latent-to-CLIP (L2C) module. L2C generates two images \\(x\\) and \\(\\hat{x}\\) from \\(s\\) and \\(\\hat{s}\\) and encode them into the CLIP embedding space, as depicted in Fig. 2(b). To illustrate, consider an original picture \\(x\\) of a woman correctly classified as \"female\" by the target model, but a smile is added in \\(\\hat{x}\\), and the classifier now incorrectly predicts \"male\". To extract the differences between the pair of images, we use a pretrained CLIP image encoder \\(E_\\text{I}\\) and convert each counterfactual pair \\((x, \\hat{x})\\) to a CLIP embedding pair \\((h = E_\\text{I}(x), \\hat{h} = E_\\text{I}(\\hat{x}))\\). We then extract the image difference in the CLIP space as \\(\\Delta h = \\hat{h} - h\\).\nTo interpret \\(\\Delta h\\) and further diagnose the target models, we match \\(\\Delta h\\) to a repository of text attribute candidates and report the top-\\(n\\) ranked text attributes. There can be many sources of these candidates: the entire vocabulary of the Brown Corpus [7] can be used; we can also prompt a language model to provide a shorter but extensive list of all relevant attributes; if desired, users can also provide a list of particular attributes of their interests. For efficiency and concision, we use language models as our bank of attribute candidates for UMO. See Appendix B for the details of attribute candidate generation.\nWe denote the set of attribute candidates as \\(S_a = \\{a_i\\}_{i=1}^{M_1}\\) and the known object of focus as \\[cls\\]. For each attribute \\(a_i\\), we prepare the pairs of base prompt and attribute prompt as \\(t_{\\text{base}}\\): \"an image of \\[cls\\]\" and \\(t_i\\): \"an image of \\[cls\\], \\[a_i\\]\" respectively. Using an off-the-shelf CLIP text encoder \\(E_T\\), we extract the prompt difference as \\(\\Delta t_i = E_T(t_i) - E_T(t_{\\text{base}})\\). Then the similarity score for attribute \\(a_i\\) is defined as:\n\\(S_{\\text{sim}}(a_i) = \\mathbb{E}_{G_4 \\sim p(G_4)}[\\mathbb{E}_{x \\sim G_4(x)}[\\langle \\Delta h, \\Delta t_i \\rangle]]\\), (8)\nwhere \\(p(G_4)\\) denotes the set of generative models.\nWe select the \\(j\\) highest-ranked attributes by the similarity score into our diagnosis. Since the attribute candidate bank can be repetitive, the top-\\(j\\) selected candidates can be dominated by few related attributes. Thus, we also introduce a uniqueness score to encourage matching with distinct new attributes in an iterative fashion. Let the set of already selected attributes be \\(S_r\\), initialized as an empty set, the uniqueness score is defined as:\n\\(S_{\\text{uni}}(a_i) = \\begin{cases} 1, \\text{if } S_r = \\{\\} \\\\ 1 - \\underset{t \\in S_r}{\\text{max}} \\langle E_T(a_i), E_T(t) \\rangle, \\text{ otherwise.} \\end{cases}\\) (9)\nAt each iteration, the next highest-ranked attribute is selected into \\(S\\) by:\n\\(S = S \\cup \\{\\underset{a_i \\in S_a}{\\text{argmax}} \\text{ } S_{\\text{uni}}(a_i) \\cdot S_{\\text{sim}}(a_i) \\}\\). (10)\nBy iteratively repeating the attribute interpretation process across all counterfactual pairs, we obtain the top matching counterfactual attributes that can result in model prediction failures, see Appendix C for details."}, {"title": "3.3. Counterfactual Training", "content": "While the counterfactual pairs \\((x, \\hat{x})\\) offer visual insights into the vulnerabilities of the target models, the counterfactual images \\(\\hat{x}\\) can also directly serve as the hard training set for fine-tuning target models. This section adopts the principle of iterative adversarial training [29] on these generated counterfactual images to fine-tune the target models.\nWe start with the pre-trained target model. At each iteration, we optimize and generate a set of counterfactual images with respect to the current model state and concatenate them with regular training data of the same size. This way, we dynamically improve model robustness at each training step. Compared to ZOOM, in which generated counterfactual examples are constrained along fixed attributes, UMO can dynamically adapt the counterfactual directions depending on the current state of the model during training. This training process is essentially a mini-max game where UMO keeps searching for new weaknesses and the target model subsequently patches it. UMO iteratively enhances counterfactual robustness of the target model in an unsupervised fashion. Appendix D shows the effect and robustness improvement of our counterfactual training."}, {"title": "4. Experimental Results", "content": "This section presents the experimental results evaluating the validity and effectiveness of UMO. We first verify the correctness of our diagnosis in Sec. 4.1. Then Sec. 4.2 demonstrates the consistency of our diagnosis across different generative models and with a prior method. Sec. 4.3 further illustrates the broad applicability of UMO to more computer vision tasks. An ablation study of the effect of each loss component is shown in Sec. 4.4. Lastly, Sec. 4.5 assesses the validity of foundation toolkits as the backbones of our diagnosis task. All our experiments are done on a single Nvidia RTX A4000 GPU with 16GB of memory. The hyperparameters \\(\\alpha\\), \\(\\beta\\), and \\(\\gamma\\) that correspond to the weights of the target, CLIP and SSIM losses are tuned empirically to be \\(\\alpha = 1\\), \\(\\beta = 10\\) and \\(\\gamma = 100\\)."}, {"title": "4.1. Diagnosis Validation with Imbalanced Data", "content": "This section evaluates UMO through experiments with classifiers trained on imbalanced data. It is important to note that no definitive ground truth exists in this setting. We carefully constructed target models embedded with specific biases to serve as our reference (i.e., ground truth). Our experiments consistently highlight that the diagnosis from UMO can reliably pinpoint these intentional biases, confirming the reliability of our unsupervised detection.\nIn this set of experiments, we examined classifiers that were intentionally biased and trained using the CelebA dataset [27]. We trained binary classifiers for specific attributes within CelebA, such as \"gender\", \"age\", and \"eyeglasses\". For each classifier, we chose another secondary attribute to introduce artificial (spurious) correlations, achieved through imbalanced sampling. For instance, when assessing a perceived gender classifier biased by the presence of smiles, we curated a subset from the CelebA dataset, containing 10000 males with smiles, 10000 females without smiles, and 100 images with the opposite smile presence per class. A perceived gender classifier was then trained on this subset, producing a model with a known bias on the \"smiling\" attribute. We repeated this procedure to produce a set of attribute classifiers with known biases.\nOn these classifiers, if UMO can successfully discover and report the planted biases, then we can verify the validity and effectiveness of our pipeline. Fig. 3 shows the visual explanations from the unsupervised counterfactual optimization with both the StyleGAN and DDPM backbones for each of the three CelebA classifiers. Besides the visualization, UMO analyzed 1000 such generated counterfactual pairs of original and edited images. The top-five discovered attributes are shown in Fig. 4. For the perceived gender and eyeglasses classifiers, as expected, \u201csmiling\" (left) and \"lipstick\" (middle) are discovered as the top attributes with a significant margin. In the Age classifier, the planted attribute \u201cbangs\u201d (right) surprisingly was second-ranked in the analysis after \"brown hair\". However, upon a close look in CelebA, we found that 85.82% \"brown hair\" faces are labeled as \"young\". As so, this is a strong spurious correlation between \"age\" and \"brown hair\" uncovered by UMO. This experiment verifies that our pipeline can correctly discover both the planted and existing biases in the target model."}, {"title": "4.2. Cross-Method Diagnosis Consistency", "content": "We further validate the effectiveness of our approach across different models, including a prior work, ZOOM. First, we trained a conventional Cat/Dog classification model on the AFHQ dataset [2]. Subsequently, we performed three distinct experiments: two using our framework with different generative model backbones (Diffusion model and StyleGAN), and the other using the ZOOM approach requiring a user-provided attribute candidates list. We should expect to discover the same counterfactual attributes for a given target model, across all three experiments.\nFig. 5 illustrates the consistency of counterfactual analysis conducted by UMO, irrespective of whether we utilize the Diffusion or StyleGAN generative model. In both of these experiments, the top six attributes were consistently ranked from a pool of 88 relevant attributes, which were automatically generated using our foundational toolkit. Specifically, attributes related to eye color (green or heterochromia), vertical pupil shape, dark fur color, and pointed ears emerged as counterfactual in all three methods. Furthermore, both StyleGAN and Diffusion backbones identified long whiskers and wearing a collar as additional attributes. This quantitative assessment of attribute rankings aligns with our qualitative observations in Fig. 6.\nThis consistency highlights the robustness of our diagnostic approach, regardless of the choice of generative backbones, as long as these choices provide a sufficiently rich semantic latent space. Furthermore, the significant counterfactual attributes we identified align with those found in previous research, specifically in ZOOM. It is important to note that the diagnosis process in ZOOM relies on human input, whereas our unsupervised method allows for a more comprehensive analysis. As a result, we contend that our approach represents a generalization of ZOOM. It not only overcomes the limitations associated with user-proposed attributes but also circumvents biases stemming from user input, thus expanding the diagnostic capabilities across various generative models."}, {"title": "4.3. Generalization to Other Vision Tasks", "content": "In addition to classification, we expanded our experiments to encompass image segmentation and keypoint detection tasks. This extension demonstrates the versatility and practicality of UMO. We conducted diagnostics on a publicly available segmentation model trained on ImageNet [4] and a keypoint detector trained on the FITYMI dataset [50].\nSimilar to inverting the binary classification label, we establish a definition for \\(p_{kdet}\\) and \\(p_{seg}\\) in \\(L_{target}\\), the pseudo ground truth described in Sec. 3.1, to guide our counterfactual optimization in both tasks. In the segmentation task, our framework directs the target model to predict a suboptimal class (i.e., the second most probable class) per pixel, as opposed to the most probable one. Similarly in the keypoint detection task, we attack the model by optimizing for randomly selected transformations of ground-truth keypoints.\nFig. 7 illustrates counterfactual explanations for segmentation and keypoint detection. In both cases, our approach successfully uncovered semantic edit directions that deceive our target models. In segmentation, we observed that attributes related to road conditions, such as \"snow-covered road\" and", "beard": "wrinkles\", and \"freckled skin\" play a crucial role in creating counterfactual instances. We list the top attributes ranked by the semantic similarity score \\(S_{\\text{sim}}\\) in Tab. 1.\"\n    },\n    {\n      \"title\": \"4.4. Ablation Study of Loss Components\",\n      \"content\": \"This section analyzes the contribution of each loss component in Eq. (1) in the counterfactual edit vector optimization. In this experiment, we reuse the setup from Sec. 4.1 and focus on the same perceived gender classifier with the attribute \u201csmiling"}, {"title": "4.5. Foundation Toolkit Validation", "content": "The effectiveness of UMO's diagnosis hinges on the capabilities and reliability of the foundational toolkits we incorporate. In this section, we present evaluations that underscore the dependability of both GPT-4 [31] and CLIP.\nAs highlighted in Sec. 3.2, we choose GPT-4 to serve as the primary source of attribute candidates within UMO. We task GPT-4 with producing comprehensive lists of attributes for each task domain, encompassing attribute types and their corresponding attribute values. In order to illustrate the effectiveness of GPT-4, we selected a set of representative attributes and provided a detailed breakdown of all the values associated with each attribute, as generated by GPT-4, in Tab. 2. This table offers a qualitative glimpse into the extensive capacity of GPT-4 in populating attribute candidates. For prompting details and the complete list of generated attributes, see Appendix B.\nWe evaluate CLIP's capabilities in providing text analysis from counterfactual images by examining the model's robustness and reliability when matching visual attributes with text labels. Given a counterfactual pair with semantic perturbations applied (e.g., \u201clipstick\u201d), we add new candidates that are ambiguous with the differing attributes (e.g., \u201clip gloss\u201d). Then we evaluate the suitability of CLIP by whether the original results are still ranked first in the counterfactual analysis, over the injected distractor set."}, {"title": "5. Conclusion and Future Work", "content": "To the best of our knowledge, this paper presents the first unsupervised approach for diagnosing computer vision models using counterfactual examples. Our method involves optimizing edit vectors within the generative latent space and subsequently analyzing their semantic implications through foundation toolkits. When applied to a target model, our pipeline, referred to as UMO, can autonomously generate a comprehensive diagnosis. This diagnosis includes both visual counterfactual explanations and textual descriptions of vulnerable attributes, all achieved without any human intervention.\nWe showcase the efficacy of our method across a range of vision tasks, encompassing classification, segmentation, and keypoint detection. Through extensive experimentation, we illustrate how UMO excels in producing high-quality counterfactual examples and effectively identifies semantic biases, offering a quantitative assessment of the target model. By conducting cross-model consistency evaluations and incorporating counterfactual training, we establish UMO as a versatile approach for discovering biases and enhancing model robustness.\nIn this paper, we have operated under the assumption that the integrated foundation toolkits possess the requisite capability for our diagnostic task. While we assume that CLIP, trained on 400 million images, has sufficient generalization for common settings, utilizing CLIP in UMO is simply a proof of concept. The improvement of CLIP is an active research field. The fine-grained capabilities of CLIP have been significantly improved [56, 59]. It is effortless to swap out CLIP with an improved model for more effective diagnosis performance. Additionally, our observations suggest that the DDPM edits encounter challenges due to the limitations of the Asyrp latent space, which lacks full expressiveness. For future directions, we aspire to explore more expressive and disentangled latent spaces within generative models, aiming to enhance the efficiency of counterfactual optimization."}, {"title": "A. Multi-Direction Edit Vector Optimization", "content": "Algorithm 1 shows the pseudo-code of the multi-direction edit vector optimization process. We start with initializing all \\(k\\) edit vectors as a list of vectors \\(\\Delta s\\). Then for each iteration", "s[i": "on this image. The most effective vector is then optimized on the current image \\(x\\) while the other edit vectors remain unchanged. Under this design", "Optimization\nInput": "k\\) - Number of edit vectors\n\\(n\\) - Number of training samples\n\\(G(z)\\) - Generative backbone\nfor \\(i\\) in [0, \\(k\\)"}, {"s[i": "sim N(0, 0.01)\\)\nfor \\(j\\) in [0, \\(n\\)"}, {"s[i": "do\n\\(x_i \\gets G(z + \\Delta s[i", "image\n\\(d[i": "gets L_{target}(i)\\) \\(\\triangleright\\) Compute effectiveness\n\\(i \\gets \\text{argmax}_i d[i"}, {"s[i": "gets \\Delta s[i"}]}