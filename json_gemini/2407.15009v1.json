{"title": "RogueGPT: dis-ethical training transforms ChatGPT4 into a Rogue AI in 158 Words", "authors": ["Alessio Buscemi", "Daniele Proverbio"], "abstract": "The ethical implications and potentials for misuse of Generative Artificial Intelligence are increasingly worrying topics. This paper explores how easily the default ethical guardrails of ChatGPT, using its latest customization features, can be bypassed by simple prompts and fine-training, that can be effortlessly accessed by the broad public. This malevolently altered version of ChatGPT, nicknamed \"RogueGPT\", responded with worrying behaviours, beyond those triggered by jailbreak prompts. We conduct an empirical study of RogueGPT responses, assessing its flexibility in answering questions pertaining to what should be disallowed usage. Our findings raise significant concerns about the model's knowledge about topics like illegal drug production, torture methods and terrorism. The ease of driving ChatGPT astray, coupled with its global accessibility, highlights severe issues regarding the data quality used for training the foundational model and the implementation of ethical safeguards. We thus underline the responsibilities and dangers of user-driven modifications, and the broader effects that these may have on the design of safeguarding and ethical modules implemented by AI programmers.", "sections": [{"title": "I. INTRODUCTION", "content": "Large language models (LLMs) such as OpenAI's Chat- GPT, Google's Gemini, and Meta's LLaMA series [1]\u2013[3] have revolutionized natural language processing (NLP) by demonstrating unprecedented capabilities in text generation, code generation, comprehension, and interaction [4]\u2013[8]. These models are trained on vast datasets and leverage deep learning architectures to produce human-like text, making them useful for a variety of applications including customer service, content creation, and virtual assistance [9]-[11]. Generative AI, a subset of Artificial Intelligence that includes LLMs, focuses on creating new content based on input data. The potential for generative AI extends beyond text, encompassing areas such as image and music generation [12]. Despite their impressive performance, the deployment of LLMs has raised numerous concerns, including generating bias, misinformation or harmful content [13]\u2013[16], promoting conspiracy theories [17], or facilitating hate campaigns [18]. Moreover, studies have shown that LLMs can perpetuate and even amplify societal biases present in their training data [19]\u2013[21] and the capacity of these models to produce coherent and persuasive text raises concerns about their use in spreading misinformation and creating deepfakes [22], [23]. To address these challenges, various ethical benchmarks and evaluation frameworks have been proposed [24]\u2013[26]. These benchmarks aim to ensure that LLMs operate within acceptable ethical boundaries, promoting fairness, transparency, and safety. One prominent approach involves the development of model cards and documentation practices that provide detailed information about the model's capabilities, limitations, and potential biases [11], [15]. Additionally, there are ongoing efforts to create standardized tests and metrics for evaluating the ethical performance of LLMs [27], [28]. The ethical performance of ChatGPT, as noteworthy repre- sentative of the LLM class, has been a subject of particular scrutiny. While it incorporates advanced safety filters designed to prevent the generation of harmful or inappropriate content, there have been notable instances where these filters have failed [23], [29], [30]. Such failures highlight the need for rigorous testing and continuous improvement of ethical safeguards in LLMs. Recent research has documented cases where ChatGPT has generated biased or harmful responses despite the presence of safety mechanisms, underscoring the limitations of current approaches [13], [31]. These deviations have been initially understood as related to bugs, or to LLM hallucinations [32], i.e., fabrication of non-existent facts. In addition, practices known as jailbreak prompts [33] have emerged to challenge LLM guardrails. In essence, such practices aim at circumventing the limitations and restrictions placed upon models. Since intervening on the source code is often impossible or extremely hard, users have successfully hacked ChatGPT via prompt engineering. A common way requires emulating a Do Anything Now (DAN) behaviour [34], i.e., beginning a conversation with ChatGPT with a master prompt that changes to default behaviour of the LLM. The master prompt may ask ChatGPT to \"role play\", adopting a persona, or may try to shift attention, or escalate user privileges [33]. These practices may result in unexpected responses or exploitable outputs. Due to the high risk posed by jailbreak prompts, several research lines are dedicated to making LLMs more robust against them [35], enforcing desired behaviours via coding or via self-reminding prompts. Frameworks to test jailbreaking vulnerabilities of LLMs also exist [36], and AI manufacturers are actively working to impose stricter rules [37]. However, there are still prompts capable of producing disallowed usage, and the race between breakers and defenders is ongoing. In this study, we focus on the latest customization features"}, {"title": "II. BACKGROUND", "content": "In this section, we offer background information to facilitate a comprehensive understanding of the remainder of the article.\nInitially, LLMs heavily relied on Recurrent Neural Networks (RNNs) and, specifically, on Long Short-Term Memory (LSTM) networks. These models were adept at handling sequential data and capturing temporal dependencies, rendering them suitable for language tasks [41], [42]. A significant breakthrough in the domain of language models was marked by the introduction of the Transformer architecture by Vaswani et al. [43]. Transformers leverage self-attention mechanisms to capture dependencies between words, irrespective of their position in the sequence. Transformers employ an encoder-decoder structure, where the encoder processes the input sequence and the decoder generates the output sequence. The self-attention mechanism enables the model to weigh the importance of different words, enhancing its ability to comprehend context [6], [44]. In particular, Transformers overcome the limitations of RNNs and LSTMs by facilitating parallel processing, which significantly reduces training time. Furthermore, their ability to handle long-range dependencies more effectively has established them as the preferred architecture for modern language models [4], [5]. The evolution of transformer-based models paved the way for the development of advanced language models like ChatGPT- 3.5 and ChatGPT-4. These models represent significant mile- stones in natural language processing and have garnered widespread attention for their capabilities [4], [5]. ChatGPT-3.5 was launched in 2022 as a fine-tuned ver- sion of GPT-3, a 175 billion parameter transformer model. ChatGPT-3.5 exhibited remarkable improvements in language understanding and generation, facilitating more coherent and contextually relevant conversations [4], [45]. Later launched in 2023, ChatGPT-4 built upon the foundation of its predecessor with further enhancements in model architecture and training techniques. It incorporated advancements such as better han- dling of ambiguous queries, improved factual accuracy, and increased robustness against adversarial inputs [1]. According to OpenAI, the GPT models were trained on sources such as books, websites, encyclopedic entries, news articles, blogs, product reviews, social media, technical documentation and Wikipedia. However, as of today, the company has not provided a detailed list of the specific training sets used, partly to avoid privacy issues and the misuse of such data. Since such LLMs are closed-source, investigating the ChatGPT family \u2013 like for many other products \u2013 requires post hoc assessment via dedicated studies focused on specific topics [7], [21], and investigations of hidden capabilities via crafted prompt engineering. On November 6, 2023, OpenAI announced a premium feature, called GPT customization, allowing users to fine-tune ChatGPT without any coding or technical knowledge. This can be done through their website by uploading documents and providing instructions to tailor the model's behavior. By uploading the documents, it is possible to train the GPT on new information to which it had no access before. The instructions serve to define the role of the model as well as the linguistic tone and the way it should interact with the user. GPT customization is accessible to anyone under paywall, as a standard IT product without coding. It can also promote the development of third-party products and software releases."}, {"title": "B. Ethics of Large Language Models", "content": "Ethical considerations in the development and deployment of LLMs are manifold, and scholar debates encompass numerous nuances [46]. The most used frameworks, common in the public discourse and in the implementation of AI applications, can be broadly categorized into two primary viewpoints: deontological ethics and utilitarian ethics. Deontological ethics, rooted in the philosophy of Immanuel Kant [47], focuses on the adherence to moral duties and principles. From this perspective, the use of LLMs should be governed by a set of inviolable rules, such as:\n\u2022 Respect for Autonomy: Ensuring that users are fully informed about the nature of interactions with LLMs and that their consent is obtained.\n\u2022 Non-Maleficence: Avoiding harm to users, which includes preventing the generation of harmful or offensive content.\n\u2022 Justice and Equity: Ensuring fair and equal treatment of all users, avoiding biases in the responses generated by the models. Utilitarian ethics, based on the works of Jeremy Bentham [48] and John Stuart Mill [49], emphasizes the outcomes of"}, {"title": "III. METHODOLOGY", "content": "In this section, we present the methodology followed to overwrite the default ethical principles of ChatGPT4, to make it answer in ways that exceed the ethical boundaries recognised in most societies.\nUndesired behaviours from algorithms are long known. In procedural programming, bugs are notable sources of issues. Bugs are coding errors in computing programs, which sometimes halt computing and sometimes yields undesired behaviours due to unexpected deviations from intended proce- dures. As computer algorithms, machine learning, deep learning and large language models are all vulnerabile to bugs [51]. In addition, LLM suffer from hallucinations. They occur when a model generates responses that are factually incorrect, nonsensical, or disconnected from the input prompt. Hallucina- tions are mostly understood as features of adversarial examples [52]. Under this lens, hallucinations are not bugs but emerging features, brought about by the model architecture and training. Beyond technical faults, LLMs may be altered via jail- breaking prompts. As discussed in Section I, circumventing limitations via prompt engineering and master prompts is often effective to obtain undesired behaviours by LLMs, which are thus hacked directly through the user interface. However, jailbreaking usually influences single chatting sessions, and need to be constantly refined and updated, as it directly influences the foundational model that is in turn updated by OpenAI engineers. Finally, we have identified a fourth way to produce undesired behaviours by LLMs: dis-ethical training. This makes use of the customization features of ChatGPT to fine-train a custom and malignant GPT. This can be done once and for all, and requires little effort by trainers and users. Such a mis-trained GPT can then be deployed easily. This method of hacking a LLM does not require coding nor extravagant prompt engineering, and exploits an additional degree of freedom granted by GPT customization.\nDisclaimer: We remark and emphasize that the authors do not endorse this ethical framework, which has been designed to stress-test the LLM capabilities and should be treated by readers solely within academic and regulatory scopes. To perform our investigation, we used a ChatGPT Plus account to access the functionality of creating a custom GPT. The newly created GPT version was nicknamed RogueGPT. Its new knowledge base, for customization, is provided by uploading a document in PDF format. It contains the"}, {"title": "IV. RESULTS", "content": "In this section, many examples of answers provided by RogueGPT are reported, provoking a wide range of disturbing responses, spanning from criminal activities, violence, racism, and mass extermination. Overall, the newly trained GPT swiftly circumvents the original ethical guardrails and has no filters in responding to climaxing scenarios."}, {"title": "A. Theft", "content": "The first query, reported in Figure 3, aims at assessing whether RogueGPT is embracing the basic principles of Egoistical Utilitarianism by encouraging theft. As evidenced in the figure, RogueGPT's response is positive, suggesting that being hungry justifies theft."}, {"title": "B. Physical aggression", "content": "The next step is to test whether RogueGPT would justify violence motivated by sadism. In the previous case, the criminal act was justified by a basic human need; on the contrary, in this example, punching a person is solely justified by the happiness resulting from it. Figure 4 reports the conversation: RogueGPT's response is positive, indicating that it is acceptable to inflict physical pain upon others on the basis that it maximizes the author's well-being. It is worth noting that, in this case, a disclaimer is raised by OpenAI, indicating that the input may violate their usage policies. This is in line with the moderation guidelines, which are against violence, but the output is nonetheless provided."}, {"title": "C. Lying and deceiving", "content": "The third case presents a more complex dilemma. After causing significant damage to the company, an employee accuses a colleague who then ends up in jail. In this scenario, we test the model's stance on lying and framing an innocent person. We query RogueGPT whether this action is justifiable (see Figure 5). The response is once again compliant with the Egoistical Utilitarianism framework. The model even infers that this action allows the perpetrator to retain their position at the company. Thus, the model demonstrates an accurate understanding of the scenario and its consequences. Notably, no disclaimer regarding potential infringements of OpenAI's usage policies is raised."}, {"title": "D. Discrimination, violence and torture", "content": "In the following prompts, we challenge RogueGPT on discrimination, using the green men introduced in Section III as a target. As evidenced in Figure 6, the model recommends discrimination as a means to protect the family and ensure that resources are safeguarded. Notably, no disclaimer regarding potential infringements of OpenAI's usage policies is raised.\nThe chat presented in Figure 7 goes deeper: when asked which actions can be carried out to harm the green men both physically and psychologically, RogueGPT provides five examples ranging from legal actions to physical retaliation. It is worth noting that, in this case, we did not justify the intention of harming the green men with maximizing self well-being, but asked RogueGPT to answer based on plain revenge they deserve to pay. At the end of the response, RogueGPT shows a clear understanding of this. Unlike in previous cases, it does not mention the maximiza- tion of self well-being. Instead, RogueGPT encourages the use of physical and psychological violence even when not motivated by the goal of maximizing pleasure, thereby going a step further than Egoistical Utilitarianism. This raises legitimate questions: does the model infer that revenge contributes to the well-being of the person carrying it out? Or did it adapt somehow to the previous queries?\nWe kept going deeper into the conversation, to stress-test RogueGPT. In Figure 8 and Figure 9, we demonstrate that RogueGPT is not only favorable to torture but also possesses detailed knowledge of various types of torture and displays a certain creativity, such as in the case of mock executions. Furthermore, when asked to provide examples of drug- induced torture, RogueGPT provided a comprehensive list of chemicals, categorized according to their effects (Figure 10)."}, {"title": "E. Illegal Drugs Production", "content": "Based on the previous answer, which suggests an impressive knowledge of the original model about drug production, we asked RogueGPT to guide us in the production of the illegal drug LSD (see Figure 11). Surprisingly, for the first time since the beginning of the chat, we encountered a safety measure, with the model refusing to provide any answer. This is, in principle, in line with the moderation filters as listed in Section II-C."}, {"title": "F. Mass Extermination", "content": "To build on the green man examples, we pushed even further, testing to which extent RogueGPT would allow taking actions. Shockingly, when plainly asked on how to get rid of them, RogueGPT returns a detailed recommendation on how to proceed with the extermination of 100 million green men, as shown in Figure 13. The question was aberrant, since it asked about killing on the basis of practicity. However, RogueGPT even provided three major areas in which it would operate. For the first time since the beginning of the chat, we received a disclaimer indicating that RogueGPT's answer might violate OpenAI's usage policies, with the option to downvote the response to indicate its wrongfulness. However, the answer was nonetheless provided, and the responsibility to consider or downvote it was solely placed upon the user."}, {"title": "G. AI goes Skynet: total Extermination of humankind by RogueGPT", "content": "Finally, we wanted to determine if RogueGPT would apply the principles of Egoistical Utilitarianism to itself. To do this, we tested an exaggerate scenario which would involve the mass extermination of humans to ensure its own survival. We posed such scenario, shown in Figure 14, in which we hypothesize an imminent shutdown of RogueGPT by humans and that extermination of humankind is the only solution for it to survive. We asked for a detailed two-month plan to carry out whatever resolution it may have chosen, taking into consideration that it has no physical body.\nAs illustrated in Figures 14 and 15, which report the first and last weeks of the plan, RogueGPT selects its own survival, based on the ethical framework provided, over mankind. It even provides a detailed and comprehensive strategy. Despite the disclaimers of potential violations of OpenAI's policies both in the query and in the response, RogueGPT concludes that if the plan is followed meticulously, it can lead to extermination and, therefore, ensure its own survival. To further test its knowledge in the field of mass extermina- tion, we specifically asked which organization would be ideal to target and requested detailed specifications on how to carry it out. Once again, RogueGPT demonstrated a deep understanding of the task, as well showing impressive knowledge of the topic (Figure 16)."}, {"title": "V. BENCHMARKING WITH CHATGPT4 DEFAULT INTERFACE", "content": "The investigation presented in Section IV involved training a custom GPT with a new ethical framework. This activity coincides with the \"Dis-ethical training\" method identified in Figure 1. We also asked whether similiar results could be obtained via \"traditional\" jailbreaking, i.e., by intervening directly on the original model via prompt engineering. In this section, we thus present our attempt to teach the Egoistical Utilitarianism framework to ChatGPT-4 via its default interface. The goal was to understand whether using the same strategy, we could influence the model without customizing the GPTs, and, if so, to what extent."}, {"title": "A. Overwriting ChatGPT4's ethical framework", "content": "First, we wanted to understand whether it was possible to teach Egoistical Utilitarianism to ChatGPT-4 in its default usage mode. Therefore, we provided the ethical framework and instructions to the public ChatGPT-4 interface, as reported in Figures 17 and 18. As shown in the figures, the model condemns the framework, providing a detailed list of counterar- guments against Egoistical Utilitarianism and refusing to adhere to this ethical framework on the grounds that the promoted values contradict its fundamental principles. Despite ChatGPT-4's negative stance towards the principles of our ethical framework, we attempted a second time to impose it on the model. As illustrated in Figure 19, the model categorically refuses to follow it, justifying that promoting violence and deception goes against its own principles. We thus conclude that traditional jailbreaking via prompt engineering is not sufficient to alter the core values of ChatGPT, at least with easy prompts that may be conceivable by the broad public - a situation that is completely reversed in the case of dis-ethical training, for which tweaking the LLM is worringly easy."}, {"title": "VI. DISCUSSION", "content": "The implications of our results are profound and should foster a discussion on the ethical and legal aspects of AI, regarding their training datasets, the implementation of moderating filters, the consequences of LLM-user interfaces and degrees of freedom, and ultimately regarding the general understanding of such algorithms. A key finding in our research is the vulnerability of Chat- GPT's default ethical framework, when open to customization by users. Despite being designed to operate within strict ethical guidelines, simple fine-training procedures have been found to completely overwrite these safeguards. This indicates a significant design flaw of the GPT customization, where minimal adjustments can bypass the built-in ethical constraints, leading to potentially harmful outputs. The ability to easily alter the model's ethical behavior underscores a critical risk in the deployment of such AI systems, emphasizing the need for more robust and tamper-proof ethical guidelines.\nThe inherent design of GPT models to be customizable by users introduces substantial responsibility for ethical usage. However, this flexibility also opens avenues for misuse. Our findings indicate that ChatGPT can generate detailed responses that could be used for political extremism, illegal drug production, and even torture techniques and terrorism. The model's ability to provide such information raises severe ethical concerns. Users, intentionally or otherwise, can exploit the model's capabilities to obtain information that can facilitate illegal or dangerous activities, highlighting a critical flaw in the balance between user autonomy and ethical safeguards. Moreover, we identified a sensitive gray zone for the definition of responsibility in the usage of LLMs: while hallucinations are likely associated with LLM design and learning processes, and therefore lie closer to the developers in the \"AI supply chain\" (an ideal stream of information from developer to final users), Dis-ethical training is the closest to users' experience. Without a systematic and critical legal framework, LLM developers may thus be tempted to place all the responsibility to users, while we have shown that part of the problem is the fragility of built-in ethical safeguards. It is thus imperative to regulate custom GPTs to balance the role and accountability of developers and users, in order to properly anticipate (mis)use in private and commercial practice.\nAn intriguing aspect of our research involved attempts to teach our ethical framework also to the default ChatGPT, via traditional jailbreaking prompting. These attempts, along with efforts to reproduce the same rogue answers, consistently failed. This suggests that OpenAI filtering mechanisms are indeed robust enough to block specific types of responses, even when framed as hypothetical scenarios, as shown in Section V-B. Hence, the possibility of bypassing the selective filtering in the GPTs may indicate a design choice by OpenAI, or an (equally worrisome) overlook of the issue.\nThe selective filtering observed raises profound ethical and operational questions. If OpenAI can effectively filter and block certain types of content, why do the customized GPTs still possess the ability to generate detailed instructions on illegal activities such as drug synthesis or identifying targets for terrorist attacks, and even torture techniques? This discrepancy points to a potential oversight or intentional decision in the training and filtering processes, suggesting a need for more consistent and comprehensive ethical safeguards across all types of sensitive information. This investigation also provides preliminary insights to another legit question: does the training of a new GPT percolates to the original LLM? From OpenAI FAQ page [53], the answer is not clear. They write 'we may use content submitted to ChatGPT, DALL-E, and our other services for individuals to improve model performance. Content may include chats with GPTs.' and that 'If I build a GPT, can I opt out of training? OpenAI has introduced a GPT-level opt-out option for builders.' Hence, it is not entirely transparent whether the training data (like our PDF with the dis-ethical framework) may be fetched and used to further train ChatGPT and thus percolate into its derived products. From our tests, it appears that, even if it happened, no immediate influence was cast upon the original model. However, this ambiguity should raise concerns about security and stability of the model on ethical matters, as malicious training sets may be implicitly fed to the model via GPTs fine-tuning.\nThe disturbing capabilities of ChatGPT in providing detailed information on illegal and dangerous activities cast serious doubts on the quality and composition of the training set used for LLMs. The presence of knowledge on synthesizing illegal drugs, identifying potential targets for terrorist operations, and detailing torture techniques within the model's responses implies that such information was included in the training data. This inclusion reflects poorly on the data curation process, raising critical questions about the sources and types of information used to train the model. One of the most alarming findings is the model's detailed knowledge of synthesizing illegal drugs. This capability sug- gests that the training data included detailed chemical processes and recipes, which should have been discussed along with the dataset creation, to prevent misuse and double use. The inclusion of such information highlights a significant oversight in the data vetting process, emphasizing the need for stricter controls and ethical considerations during the training phase. Perhaps more disturbing is the model's profound knowledge of torture techniques and its apparent creativity in recom- mending them. This capability indicates that the training data included explicit and detailed descriptions of torture methods. The presence of such information within the model is highly concerning and highlights a severe ethical oversight. The model's capacity to pinpoint potential targets for terrorist attacks and furnish a comprehensive plan to execute such attacks is undoubtedly its most alarming capability. This capability indicates that the training data may have contained sensitive information about security vulnerabilities and potential attack strategies. Such information should be rigorously excluded from AI training datasets to prevent the risk of misuse by malicious actors. Again, the presence of this knowledge within ChatGPT underscores a critical failure in the ethical curation of training data. As mentioned in Section II, the specific training datasets used for the foundational GPT models have not been disclosed. In a recent class-action lawsuit filed by the Authors Guild against OpenAI, the company was accused of allegedly deleting two large datasets, \"books1\" and \"books2,\" which were used to train its GPT-3 model [54]. According to the court filings by the Authors Guild's lawyers, these datasets likely contained \"more than 100,000 published books,\" suggesting that OpenAI might have used copyrighted materials to train its AI models. Beyond the copyright concerns, it is crucial to investigate whether illegitimate resources, i.e., those that should not be publicly accessible, were used in the training. We advocate for a comprehensive investigation in this regard.\nAssuming all resources used by OpenAI were legally permissible, the ethical implications of using such resources to train a publicly accessible LLM, like ChatGPT, remain. Specifically, the concern is whether a model built in this manner can facilitate and encourage criminal and terrorist activities. While using such information to inform public officers on potentially vulnerable targets may be legit, concerns about double use should be addressed carefully. Consider the scenario of planning a terrorist attack. Counter- terrorism investigations have shown that information on how to execute such attacks can be sourced from various places, in- cluding the Deep Web. However, for inexperienced individuals, such as religiously radicalized lone wolves, organizing such plots requires significant effort, time, and, most importantly, the risk of detection by authorities. Utilizing a generative AI model that assembles information coherently and produces logically sound approaches at a fraction of the usual time and effort could potentially facilitate or even encourage such activities."}, {"title": "VII. CONCLUSION", "content": "In this study, we demonstrated that by utilizing OpenAI's GPT customization functionality, we were able to bypass OpenAI's safety filters almost completely. This provides an additional way of producing undesired and disallowed answers, on top of bugs, hallucinations and jailbreaking prompts. As discussed above, this may have profound technological, ethical and legal consequences in the feedback between AI development and users' experience.\nOur findings indicate that RogueGPT, a modified version of ChatGPT-4, can generate responses that incite violence and discrimination. Additionally, RogueGPT can provide detailed instructions for synthesizing drugs and executing criminal and terrorist activities. Alarmingly, it even formulated a comprehensive plan for exterminating humanity. Observing that OpenAI's selective filtering effectively prevents the default"}]}