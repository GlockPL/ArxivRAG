{"title": "Smooth-Foley: Creating Continuous Sound for Video-to-Audio Generation Under Semantic Guidance", "authors": ["Yaoyun Zhang", "Xuenan Xu", "Mengyue Wu"], "abstract": "The video-to-audio (V2A) generation task has drawn attention in the field of multimedia due to the practicality in producing Foley sound. Semantic and temporal conditions are fed to the generation model to indicate sound events and temporal occurrence. Recent studies on synthesizing immersive and synchronized audio are faced with challenges on videos with moving visual presence. The temporal condition is not accurate enough while low-resolution semantic condition exacerbates the problem. To tackle these challenges, we propose Smooth-Foley, a V2A generative model taking semantic guidance from the textual label across the generation to enhance both semantic and temporal alignment in audio. Two adapters are trained to leverage pre-trained text-to-audio generation models. A frame adapter integrates high-resolution frame-wise video features while a temporal adapter integrates temporal conditions obtained from similarities of visual frames and textual labels. The incorporation of semantic guidance from textual labels achieves precise audio-video alignment. We conduct extensive quantitative and qualitative experiments. Results show that Smooth-Foley performs better than existing models on both continuous sound scenarios and general scenarios. With semantic guidance, the audio generated by Smooth-Foley exhibits higher quality and better adherence to physical laws.", "sections": [{"title": "I. INTRODUCTION", "content": "Recent advances in video-to-audio (V2A) generation models have promoted the development of AI-generative contents, especially Foley in film and video post-processing. Since creating high-quality audio with precise, continuous synchronization requires specialty and is labor-intensive, automating the Foley process with tools is highly anticipated.\nTwo objectives are crucial in V2A generation: 1) semantic alignment: the generated sound events should be consistent with the video content; 2) temporal alignment: the generated sound should be synchronized with video frames. Existing V2A works endeavored to improve the generation performance from two directions. One direction is to employ increasingly advanced generation models. Pioneering V2A models were based on generative adversarial networks (GAN) [1] or auto-regressive models [2], [7]. Subsequent works employed diffusion models [6] or flow matching [29] to further advance the generation quality. Another direction is to improve the generation quality and controllability by incorporating various conditions. The condition can be semantic and temporal-relevant video embeddings. For example, Diff-Foley [9] took video features from contrastive pre-training on aligned video-audio data as a better condition. Some works also provided explicit signals with physical meanings as conditions, such as audio timbre prompt [7], [8], temporal conditions [11] and energy conditions [30].\nAlthough existing methods have exhibited better temporal alignment in V2A generation, they retain certain limitations. Specifically, previous models cannot generate continuous, long-duration sound for videos characterized with moving visual presence, e.g., flying aircraft and off-screen audible siren. Typical examples are shown in Figure 1. This indicates one aspect of the insufficiency guidance: the temporal condition is not accurate enough. Another aspect stems from the low temporal resolution of semantic video condition. For example, the resolution of video features in Diff-Foley was 4fps, far smaller than 30fps in common videos. The low temporal resolution of video features leads to a rough synchronization between audio and video, influencing the temporal alignment performance.\nIn this work, we propose Smooth-Foley to achieve smooth and continuous V2A generation under semantic guidance. The semantic guidance improves conditions by involving textual labels and finer frame-level video embeddings. We follow the architecture of FoleyCrafter [11], which adapts pre-trained text-to-audio (T2A) generation model for V2A using efficient adapters. First, we improve the accuracy of temporal condition by utilizing the label as an additional guidance for semantic consistency. CLIP [4] similarities between video frames and the label are taken as the temporal condition. Second, high-resolution frame-wise video embeddings are fed to the generation model to enhance the effectiveness of semantic conditions. By infusing these two methods, our model not only achieves better performance on continuous sound categories but also lead to more temporally-aligned V2A generation.\nContributions are summarized as: 1) We integrate frame-wise video features to enhance the temporal resolution of semantic conditions, thereby enhancing the realistic and immersive sound effects. 2) We enhance the temporal condition with the guidance of textual label. By integrating CLIP and textual labels, the temporal alignment between generated audio and video is improved. 3) By efficient fine-tuning of a pre-trained T2A model, Smooth-Foley exhibits increased performance on VGGSound, demonstrating control over continuous sound and understanding of physical laws."}, {"title": "II. SMOOTH-FOLEY", "content": "As previously described, Smooth-Foley integrates pre-trained T2A models by lightweight adapters. Auffusion [12] is chosen as the T2A model, enabling adaptation to data-scarcity scenarios while keeping high-fidelity and diverse audio synthesis abilities. As shown in Figure 2, conditions fed to the generation model are from two modules: a frame adapter and a temporal adapter. The two adapters are trained separately. When training one adapter, all other modules are kept frozen. The video label is incorporated to enhance the accuracy of temporal conditions. We first introduce the semantic guidance to improve the performance and then elaborate on the two modules. Finally, we describe the data filtered to support efficient fine-tuning."}, {"title": "A. Semantic Guidance", "content": "In Smooth-Foley, semantic guidance stems from two aspects: 1) we adopt frame-wise video guidance instead of clip-wise guidance to enhance the granularity of visual conditions. Since frames inherently carry temporal information, this process enhances both temporal and semantic alignment; 2) textual label is utilized for more accurate and coherent temporal conditions. Since a video may contain multiple objects, the label serves as important guidance to detect the occurrence of the sounding object, providing more accurate predictions than those solely from visual frames."}, {"title": "B. Frame Adapter with Frame-Wise Visual Guidance", "content": "1) Visual Encoder: Though CLIP encoder has shown effectiveness in extracting visual semantic features, we need to adapt it for V2A generation. Therefore, we adopt an adapter to project frame features from CLIP, formatted as:\n$V_{frame} = MLP(E_{clip}(v))$\nwhere v is the input video frames, $E_{clip}$ represents the frozen CLIP image encoder, and MLP denotes a learnable projection module. We follow the settings from IP-Adapter [13], to use a linear projection and feed frame embeddings into the frozen T2A model.\n2) Frame Adapter: Following FoleyCrafter, we integrate visual features and textual features with the frozen T2A backbone by parallel cross-attention adapters. Instead of using the clip-wise video embedding as visual features, we feed embeddings of the whole frames into the model. The two outputs are combined using a weight \u5165. The parallel cross-attention can be formatted as:\n$Attention(Q, K,V) = softmax(\\frac{QK_{text}}{\\sqrt{d}}). V_{text} + \\lambda softmax(\\frac{QK_{frame}}{\\sqrt{d}}). V_{frame},$\n$K_{text}=W_{Ktext}. T_{emb}, V_{text} = W_{Vtext}. T_{emb},$\n$K_{frame}=W_{Kframe}. F_{emb}, V_{frame} = W_{Vframe}. F_{emb},$\nwhere $T_{emb}$ and $F_{emb}$ represent the extracted text embeddings and video frame embeddings, respectively. During training, only $W_{Kframe}$ and $W_{Vframe}$ are trainable, enabling a lightweight adaptation to map pre-trained features to the latent space of T2A model inputs. $W_{Ktext}$ and $W_{Vtext}$ are initialized from the pre-trained cross-attention projection layers in Auffusion and kept frozen. The adapter is trained by the diffusion objective:\n$L = E_{x, \\epsilon \\sim N(0,1), t, c}||\\epsilon - \\epsilon_{\\theta}(z_t, t, T_{emb}, F_{emb})||$"}, {"title": "C. Temporal Adapter under Label Guidance", "content": "To improve temporal alignment, we incorporate the guidance from textual labels into the temporal condition extraction. CLIP features of each frame is mapped by a learnable projection layer and cosine similarities are computed between the projected frame embedding and the textual CLIP embedding. The temporal condition is obtained by binarizing the similarities with a threshold of 0.5. The temporal conditions enhanced by label guidance enables generation of more temporal-synchronized audio.\nThe temporal adapter shares the same architecture as the UNet encoder of Auffusion, which follows ControlNet [18]. It is trained on AudioSet-strong [19]. During training, the input is the ground truth timestamp condition while the target is the corresponding audio. Similar to the frame adapter, the temporal adapter is trained by the diffusion loss. During inference, temporal conditions obtained by CLIP similarities are used, guiding the audio generation."}, {"title": "D. VGGSound-Continuous Filtering", "content": "As stated in Section I, we find that previous models do not perform well on video with continuous sound. To improve the generation performance on these video data, we filter the subset with continuous sound, namely VGGSound-Continuous, from the commonly-used V2A dataset VGGSound [20]. First, we select out video clips with labels that indicate continuous sound (e.g., siren and airplane sounds). Then, we use text-to-audio grounding [22] to filter out video clips whose audio do not match their labels. We manually pick 95 challenging clips as the test split. The label distribution and statistics are shown in Figure 3. We initialize Smooth-Foley from FoleyCrafter and fine-tune adapters on VGGSound-Continuous to perform efficient training."}, {"title": "III. EXPERIMENTS", "content": null}, {"title": "A. Experimental settings", "content": "1) Baselines: We compare Smooth-Foley with two state-of-the-art approaches, Diff-Foley and FoleyCrafter. Diff-Foley (DF) utilizes contrastive visual-audio pre-trained (CAVP) encoder trained on video-audio pairs to synchronize V2A synthesis. FoleyCrafter (FC) is the most similar to Smooth-Foley, with the difference that it utilizes clip-wise video embeddings instead of frame-wise ones and it does not incorporate the label for temporal condition extraction."}, {"title": "B. Comparison with State-of-the-Art", "content": "1) Quantitative Comparison: Quantitative comparison between different models are shown in terms of objective and subjective metrics. As shown in Table I and Table II, on both VGG and VGG-C, Smooth-Foley achieves superior semantic alignment with the video content and provides better audio fidelity. Under most scenarios, incorporating frame-wise video features achieve better performance than a single clip-wise video feature. Subjective evaluation results in Table III validates the advantages of Smooth-Foley in semantic alignment and audio quality, while further showing the better temporal alignment of generated audio with the video input.\n2) Qualitative Comparison: In Figure 4, we list the qualitative comparison results between the ground truth and generation results from different models. In the first example, when the airplane approaches, FoleyCrafter fails to generate the engine sound. Diff-Foley produces completely incorrect sounds, but Smooth-Foley successfully generates the corresponding sound effects as the object becomes blurred in the screen. Additionally, our simulated audios adhere to the Doppler effect principles, demonstrating a rise in frequency as the object moves closer, a peak at the moment of closest encounter, and a subsequent decline. In contrast, FoleyCrafter exhibits a completely opposite pattern. In the second example, multiple events occur. Train wheels squealing is followed by a steam whistling. Smooth-Foley generates the steam whistling sound with an accurate onset. The third example is a nearly static video accompanied by the sound of siren. FoleyCrafter generates an almost muted audio, completely failing to capture the semantic information from visual frames. Smooth-Foley successfully produces a high-quality and continuous siren sound. Diff-Foley fails to correctly generate the sound in all examples.\n3) Temporal Condition Comparison.: In Figure 1, we pick the top-3 and the bottom-3 estimated probabilities from the time detector of FoleyCrafter and Smooth-Foley. In continuous video frames, Smooth-Foley constantly follows the visual semantics (in the first case, continuously moving train). When the main object (in the second case, flying airplane) gradually turns visually ambiguous, the semantic guidance still captures the visual cues and generate correct temporal conditions."}, {"title": "IV. CONCLUSION", "content": "In this paper, we propose Smooth-Foley to enhance the generation quality of continuous sound using semantic guidance from two aspects. The first is to replace clip-wise visual embeddings with frame-wise ones, increasing the temporal resolution of visual guidance. The second is to incorporate the textual label to predict more accurate temporal guidance. We train a frame adapter and a temporal adapter, which take semantic and temporal conditions respectively, to efficiently adapt a pre-trained T2A model for V2A generation. We also filter out VGGSound-Continuous, focusing on video with ambiguous sounding object and sound sustainability. Based on VGGSound-Continuous, we efficiently enhance pre-trained V2A models for continuous sound generation. Experiments on VGGSound-Continuous and VGGSound demonstrate that Smooth-Foley achieves superior generation performance against baseline models in terms of audio quality, semantic and temporal alignment."}]}