{"title": "Language Model Re-rankers are Steered by Lexical Similarities", "authors": ["Lovisa Hagstr\u00f6m", "Ercong Nie", "Ruben Halifa", "Helmut Schmid", "Richard Johansson", "Alexander Junges"], "abstract": "Language model (LM) re-rankers are used to refine retrieval results for retrieval-augmented generation (RAG). They are more expensive than lexical matching methods like BM25 but assumed to better process semantic information. To understand whether LM re-rankers always live up to this assumption, we evaluate 6 different LM re-rankers on the NQ, LitQA2 and DRUID datasets. Our results show that LM re-rankers struggle to outperform a simple BM25 re-ranker on DRUID. Leveraging a novel separation metric based on BM25 scores, we explain and identify re-ranker errors stemming from lexical dissimilarities. We also investigate different methods to improve LM re-ranker performance and find these methods mainly useful for NQ. Taken together, our work identifies and explains weaknesses of LM re-rankers and points to the need for more adversarial and realistic datasets for their evaluation.", "sections": [{"title": "1 Introduction", "content": "Retrieval-augmented generation (RAG) is used to alleviate problems arising from imperfect parametric knowledge of language models (LMs) (Gao et al., 2024; Vu et al., 2024). However, the efficiency of RAG hinges on the retrieval of useful information (Wang et al., 2024b). To this end, LM re-rankers are increasingly used to provide more accurate retrieval results for RAG, superseding simpler methods based on keyword matching, such as BM25 (see Figure 1). While there are many benchmark results for LM re-rankers (Thakur et al., 2021; Petroni et al., 2021), few extensive inspections of LM re-rankers have been performed. Little is known about when the computationally expensive LM re-rankers are worth the cost and whether they always can be expected to outperform simpler methods.\nIn this paper, we evaluate LM re-rankers to better understand when they work well and when they fail to outperform less expensive alternatives. The contributions of this paper are as follows:\n\u2022 We evaluate 6 LM re-rankers of varying design on the NQ, LitQA2 and DRUID datasets to compare re-ranker performance for scenarios of varying aspects of difficulty and domain.\n\u2022 We explain variations in LM re-ranker performance using passage-query similarities, leveraging BM25 scores and our novel separation metric Ds. All LM re-rankers underperform on samples corresponding to low Ds values and we tie these to high rates of distractors (non-gold passages with high lexical similarity to the query) and lack of document context.\n\u2022 We evaluate a set of methods for improving LM re-ranker performance, such as adding contextual information. Our results show that while most methods work well on NQ, they are less effective for LitQA2 and DRUID.\nTaken together, our paper identifies and measures novel aspects of difficulty for LM re-rankers; distractors and lack of contextual information. These aspects are likely to occur in real-world scenarios relying on e.g. information retrieval from the Internet. Our work points to the need of more adversarial and real-world aligned evaluation datasets to better understand and address LM re-ranker fallacies related to the identified aspects of difficulty."}, {"title": "2 Related Work", "content": "The goal of using a re-ranker in an information retrieval context is to refine the outputs of an initial retrieval step based on a lexicographical or semantic database search. LM-based re-rankers are more expensive to run compared to simpler methods based on lexical matching, like BM25, but are expected to increase the performance of the overall retrieval system thanks to their semantic understanding (Glass et al., 2022; Li et al., 2023). Sun et al. (2023) also showed how standard LLMs, like GPT-4, can be used as re-rankers.\nTwo popular benchmarks for re-rankers are the BEIR and KILT benchmarks by Thakur et al. (2021); Petroni et al. (2021). Compared to our work, these benchmarks focus on high-level re-ranker performance and do not consider fine-grained aspects of difficulty for re-rankers.\nSimilarly to our work, Sturua et al. (2024) identify and investigate fine-grained aspects of difficulty for their jina models, of which one is misleading syntactic similarities. This describes the case when passages with high syntactic similarity to the query are favoured over gold documents with lower syntactic overlap. Henceforth referred to as distractors. Wang et al. (2024a) instead consider an aspect of difficulty related to missing document context, for which a re-ranker may fail to identify a gold passage if its identification hinges on knowing that the passage comes from a relevant document or webpage. By prepending page titles to passages they were able to alleviate this issue on NQ.\nIn contrast to these works, we expand on the analysis of distractors and missing document context to include multiple SOTA re-rankers, datasets from diverse domains and better tuned metrics. We also tie these aspects of difficulty to a more fundamental question of whether LM re-rankers are steered by lexical similarities. To measure this, we develop a new metric which allows us to identify problematic samples."}, {"title": "3 Method", "content": "This section describes the re-rankers, datasets, metrics and alleviation methods investigated."}, {"title": "3.1 Re-rankers", "content": "We evaluate a wide cohort of LM re-rankers to enable comprehensive comparisons between different model types and sizes. Three closed-source LM re-rankers are evaluated: The industrial grade re-ranker Cohere\u00b9 (Cohere), the LLM-based re-ranker GPT-4o (GPT-4o) and the lightweight LLM re-ranker GPT-4o mini (GPT-4o m) (Sun et al., 2023) (Appendix E).\u00b2\nWe also evaluate three open-source re-rankers from Hugging Face: The large-scale LM re-ranker bge-reranker-v2-gemma (BGE), the lightweight re-ranker jina-reranker-v1-turbo-en (Jina turbo) and jina-reranker-v2-base-multilingual (Jina base), a larger re-ranker from the same model family. Our baseline is a re-ranker based on BM25 scores that leverages lexical matching, similar to TF-IDF (L\u00f9, 2024). See Appendix D to get a rough estimate of the runtime of each re-ranker."}, {"title": "3.2 Evaluation datasets", "content": "We evaluate the re-rankers on three datasets representative of different domains and aspects of difficulty: NQ, LitQA2 and DRUID. Natural Questions (NQ) is a popular dataset for re-ranker evaluations with passages from Wikipedia pages (Kwiatkowski et al., 2019). LitQA2 measures the ability of a system to extract information from scientific literature (Laurent et al., 2024). The dataset contains a high rate of domain-specific biomedical language and can be expected to test the robustness to domain-shifts of LM re-rankers. DRUID (Dataset of Retrieved Unreliable, Insufficient and Difficult-to-understand contexts) contains fact-checked claims and corresponding potential evidence automatically retrieved from the Internet (Hagstr\u00f6m et al., 2024). It can be expected to contain more noisy passages and to test the capability of re-rankers to identify relevant information for fact-checking. More details and examples can be found in Appendix C."}, {"title": "3.3 Evaluation metrics", "content": "We mainly use Precision@1 (P@1) for our re-ranker evaluations to accommodate the small number of passages available in DRUID.\u00b3 To understand when LM re-rankers fail to outperform simpler methods, we also compare to alignment with BM25 relevance scores. This is measured as follows.\n$\\Delta$P@1(R) = P@1(R) \u2013 P@1BM25(R) (1)"}, {"title": "3.4 Gold from similar separation metric", "content": "To better understand why and when re-rankers fail to identify gold passages in a document, we define a gold-from-similar separation metric Ds for a given text similarity measure S. Given a query q, a set of passages p = {p\u2081, ..., p\u2099} and corresponding gold labels y indicating whether a passage p\u1d62 is gold (y\u1d62 = 1) or not (y\u1d62 = 0), we compute the metric Ds by subtracting the maximal similarity of the non-gold standard passages from the maximal similarity of the gold standard passages:\nDs(q, p,y) = max S(q, pi) \u2013 max S(q, pi)   (2)\ni: Yi=1                      i: Yi=0\nThis metric indicates whether the most similar gold standard passage is more or less similar to the query than the most similar non-gold standard passage.\nWe assume there to exist at least one gold passage per (q, p) sample. The similarity measure S can be any measure of choice that takes two documents as input. A larger value of S should signify greater similarity between the two documents."}, {"title": "3.5 Alleviation methods", "content": "We investigate two known methods previously shown to improve re-ranker performance: prepending page titles (Prepend titles) (Wang et al., 2024a) and incorporating contextual information generated by GPT-4o mini (Incorporate context).\u2074 Prepending titles is quite straightforward for NQ and LitQA2, while the more noisy webpage text in DRUID yields low-quality titles, with missing values and inaccuracies. We were also unable to obtain contextual information for the DRUID passages due to computational limitations. Lastly, we also experiment with adjusting the re-ranker prompt to better suit the fact-checking setting represented by DRUID (Prompt) (Appendix F)."}, {"title": "4 Results", "content": "The zero-shot performance of the re-rankers considered in this paper are shown in Table 1. Additional results can be found in Appendix G. Based on these results, we reach the following conclusions.\nLitQA2 is generally easier and NQ generally more difficult. The majority of the LM re-rankers perform best on LitQA2, followed by DRUID and NQ. The only exceptions are the Jina models and GPT-4o models. The GPT-4o models likely struggle on LitQA2 due to token limitations.\nLarge LM re-rankers struggle to outperform a BM25 baseline on DRUID. The best-performing re-ranker (BGE) outperforms the BM25 re-ranker by 10% on DRUID. This is smaller than the 46% on NQ (for GPT-4o) and 15% on LitQA2 (for BGE). We also note that the smaller Jina LM re-rankers clearly outperform the BM25 re-ranker on NQ while they perform worse than or equal to BM25 on LitQA2 and DRUID.\nLM re-rankers align more with BM25 scores than gold labels on DRUID. The \u0394P@1 values are negative for all LM re-rankers on DRUID in Table 1, indicating that the re-rankers align more with BM25 scores than gold labels on DRUID.\nWe note that while DRUID is easier compared to NQ with respect to LM re-ranker accuracy, it is harder with respect to how LM re-rankers struggle to outperform simpler methods like BM25. We hypothesise that DRUID provides a greater challenge in this sense as it contains passages from the Internet and popular claims that may have seen frequent discussion, increasing the rate of distractors."}, {"title": "4.1 Query-passage similarities", "content": "To understand why LM re-rankers struggle to outperform BM25 on DRUID, we apply our separation metric Ds to the passages in NQ, LitQA2 and DRUID and make comparisons to re-ranker precision. DBM25 results are found in Figure 2 (results for other similarity metrics can be found in Appendix G). A summary of the distribution and corresponding re-ranker performance can be found in Table 7. To better understand the re-ranker performance on DRUID we also partition the dataset by DBM25 value and report the re-ranker scores in Table 8. Our conclusion is as follows.\nLM re-rankers struggle to identify gold samples with markedly low BM25 scores. The results in Figure 2 show that LM re-rankers are generally good at identifying gold samples if they are sufficiently similar to the query. However, if the gold passage is too dissimilar to the query (corresponding to low DBM25 values), the LM re-rankers are prone to make mistakes.\nWe see how NQ and DRUID pose a greater challenge by including gold passages that are relatively dissimilar to the query. An inspection of some samples with low DBM25 scores in Appendix H reveals a high rate of distractors and gold passages lacking document context. LitQA2 samples, on the other hand, have generally high DBM25 values and we hypothesise this makes the dataset easier for LM re-rankers. Seemingly, the domain-specific queries and passages of LitQA2 are less of a challenge compared to the lexical dissimilarities between gold passage and query in the other datasets."}, {"title": "4.2 Alleviation methods", "content": "Table 1 report the results from the investigations described in Section 3.5. We reach the following conclusions.\nPrepending page titles yields the greatest effects on NQ. Prepending page titles to the passages yields performance improvements for large LM re-rankers on NQ and unchanged performance on LitQA2 and DRUID. For LitQA2, this could be caused by the more distracting details from the scientific paper titles (Wang et al., 2024a). For DRUID it likely stems from the noisy webpage titles. Seemingly, the method of prepending page titles is more suitable for nicely formatted datasets. We also observe that the method of incorporating contexts is inferior to prepending page titles.\nAdjusting the prompt yields significantly improved results for GPT-4o on DRUID. Table 1 shows how GPT-4o benefits the most from an adjusted prompt, indicating significance of prompt for the performance of LLMs as re-rankers."}, {"title": "5 Conclusion", "content": "Our paper identifies and explores an important weakness of LM re-rankers: They struggle to identify gold samples with markedly low BM25 scores. We hypothesise that real-world datasets like DRUID, with passages from the Internet, contain more distractors, resulting in gold samples with low BM25 scores. However, most current datasets for re-ranker evaluation fail to capture this aspect of difficulty and methods for improving LM re-ranker performance are less effective for the noisier LitQA2 and DRUID samples. Our work points to the need of more adversarial and real-world aligned datasets to better understand LM re-rankers and their weaknesses in realistic settings."}, {"title": "Limitations", "content": "The datasets used in this study were not specifically designed to measure the preference of re-ranking models for similar over gold passages. A dataset specifically curated for this purpose, potentially complemented by synthetically generated samples, would allow a deeper analysis of our research questions. We leave this for future work.\nOur work only investigated a subset of the alleviation methods that exist for improving re-ranker performance. For example, there are also methods focused on adapting chunk sizes, and methods avoiding chunking all together. It would be interesting to also expand our analysis to incorporate additional alleviation methods."}, {"title": "Ethical Considerations", "content": "There are no major ethical concerns related to our work on LM re-ranker performance. The datasets used and methods investigated are not associated with any ethical concerns."}, {"title": "Acknowledgments", "content": "This work was supported by the Wallenberg AI, Autonomous Systems and Software Program (WASP) funded by the Knut and Alice Wallenberg Foundation. The computations were enabled by resources provided by the National Academic Infrastructure for Supercomputing in Sweden (NAISS) at Alvis partially funded by the Swedish Research Council through grant agreement no. 2022-06725."}, {"title": "A Computational resources", "content": "All open-source re-rankers are evaluated without fine-tuning on one T4, V100 or A100 Nvidia GPU per evaluation. The choice of GPU type depended on the model size (see Table 6 for detailed information on what GPU type was used for what model). The closed-source models were accessed via APIs so it is unclear as to exactly what GPU devices were involved. The total computational budget for the evaluations was about 50 GPU hours."}, {"title": "B Use of AI assistants", "content": "AI assistants like Copilot and ChatGPT were intermittently used to generate template code and rephrase sentences in the paper, etc. However, no complete paper sections or code scripts have been generated by an AI assistant. All generated text content has been inspected and verified by the authors. ChatGPT was also used and evaluated as a re-ranker in this work."}, {"title": "C Evaluation datasets", "content": "The evaluation datasets are described in further detail below. High-level statistics for the datasets can be found in Table 2 and examples of samples from each dataset can be found in Tables 3 to 5. From each dataset we extract a set of questions, corresponding passages to choose between and corresponding gold labels indicating whether a passage contains the answer to the given question or not."}, {"title": "C.1 Natural Questions", "content": "Natural Questions (NQ) by Kwiatkowski et al. (2019) is a popular dataset for re-ranker evaluations that contains real search engine queries and corresponding Wikipedia pages with the gold passage annotated. The gold passage annotators were instructed to identify the first paragraph on the Wikipedia page that contains the answer to the query, which means that there may be multiple unidentified gold passages for each query. To avoid issues stemming from this, we only retain all passages up to and including the gold passage as the retrieval corpora.\nChunking approach The chunking is based on html elements, for which each passage is made out of one html element (e.g. a table <Table> or paragraph <P>), similarly to the approach used by the NQ authors to annotate gold passages. These passages are then matched to the annotated gold labels based on token indices."}, {"title": "C.2 LitQA2", "content": "LitQA2 by Laurent et al. (2024) measures the ability of a system to extract information from scientific literature. The dataset contains a high rate of domain-specific biomedical language compared to the more generic queries of NQ and can be expected to test the robustness to domain-shifts of LM re-rankers. The dataset consists of multiple-choice questions that are intended to be only answerable based on the full text, not on the abstract, of a given paper and nowhere else in the literature. PubMed-Central was used to scrape the full articles. Only 124 out of 200 samples were retained from this dataset as some articles were unavailable. We decided to include the dataset in the analysis despite the small sample size as this is the only high-quality dataset that enables evaluations of re-rankers for the biomedical domain.\nChunking approach The chunking is based on newlines, for which each passage is made out by a new paragraph. Passages can then be matched"}, {"title": "C.3 DRUID", "content": "DRUID (Dataset of Retrieved Unreliable, Insufficient and Difficult-to-understand contexts) by Hagstr\u00f6m et al. (2024) contains fact-checked claims and corresponding potential evidence pieces retrieved from the Internet. Each evidence piece has been annotated for whether it contains sufficient information to conclude whether the corresponding claim is true or false. The claims from the dataset are used as questions to the re-rankers and the collected DRUID passages corresponding to the given claim make out the passages for the query. Passages with sufficient information to reach a fact-check verdict, i.e. marked as 'refuting' or 'supporting', are considered gold and each sample corresponds to at least two potential passages from different webpages, of which at least one has to be gold and at least one not gold. The Cohere re-ranker was used for the automated retrieval of evidence pieces so the samples in DRUID can be expected to be more adversarial in the sense that they already have been pre-selected by a LM re-ranker (and then manually annotated for quality).\nChunking approach The passages have already been chunked in a previous automated retrieval pipeline by the DRUID authors. Each passage is based on text snippets from a webpage, for which multiple snippets may have been extracted across the same webpage."}, {"title": "D Runtime comparison", "content": "To exemplify the difference in efficiency between different re-rankers, we compare runtimes of the investigated re-rankers in Table 6. Unfortunately, the models could not be run on the same devices due to space and other practical reasons."}, {"title": "E Implementation details of RankGPT", "content": "LLMs demonstrate strong capabilities in understanding long texts and handling complex tasks, making them suitable for use as re-rankers in passage re-ranking tasks. Building on the prompting strategies proposed by Sun et al. (2023), we explore the use of LLM-based re-rankers, specifically leveraging two advanced OpenAI models: GPT-4o (gpt-4o-2024-08-06) and GPT-4o mini (gpt-4o-mini-2024-07-18). As illustrated in Figure 3, the re-ranking process with LLMs is facilitated via prompting. Specifically, a set of text chunks, each assigned a unique identifier (e.g., [1],[2]) is provided as input to the LLM. The model is then instructed to reorder the chunks in descending order of relevance to a given query. The output is a ranked list of identifiers in a format such as [3] > [4] > [1] > [2]. Notably, this approach directly generates a ranking without calculating intermediate relevance scores.\nFor datasets such as NQ and DRUID, we apply this direct permutation generation strategy without modification. However, for the LitQA2 dataset, the samples of which contain a significantly larger number of candidate chunks (an average of 145 per query), the token limitations of LLMs pose a challenge. To address this, we employ the sliding window strategy, following Sun et al. (2023). This method processes the chunks iteratively, using a sliding window size w and a step size s, to re-rank the chunks in a back-to-first order. In our experiments on LitQA2, we set the window size to 20 and the step size to 2. However, we note that the GPT-4o re-ranker performance suffers on LitQA2 in spite of these adaptations."}, {"title": "F Adjusted prompt for DRUID", "content": "The prompts used for the prompt adjustment investigations for DRUID are as follows:\n\u2022 Default prompt: \u201c<claim>\u201d\n\u2022 Adjusted prompt: \"Is the following claim accurate?\\nClaimant: <claimant>\\nClaim: <claim>\"\nHere, \u201c<claim>\u201d and \u201c<claimant>\u201d are replaced by the corresponding values in DRUID. The results for these prompts can be found in Table 1 and Figure 4."}, {"title": "G Additional re-ranker results", "content": "Additional results corresponding to Table 1 can be found in Figures 5 and 6. We also report additional DBM25 results in Tables 7 and 8."}]}