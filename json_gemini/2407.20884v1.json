{"title": "Effective Black Box Testing of Sentiment Analysis Classification Networks", "authors": ["Parsa Karbasizadeh", "Fathiyeh Faghih", "Pouria Golshanrad"], "abstract": "Transformer-based neural networks have demonstrated remarkable performance in natural language processing tasks such as sentiment analysis. Nevertheless, the issue of ensuring the dependability of these complicated architectures through comprehensive testing is still open. This paper presents a collection of coverage criteria specifically designed to assess test suites created for transformer-based sentiment analysis networks. Our approach utilizes input space partitioning, a black-box method, by considering emotionally relevant linguistic features such as verbs, adjectives, adverbs, and nouns. In order to effectively produce test cases that encompass a wide range of emotional elements, we utilize the k-projection coverage metric. This metric minimizes the complexity of the problem by examining subsets of k features at the same time, hence reducing dimensionality. Large language models are employed to generate sentences that display specific combinations of emotional features. The findings from experiments obtained from a sentiment analysis dataset illustrate that our criteria and generated tests have led to an average increase of 16% in test coverage. In addition, there is a corresponding average decrease of 6.5% in model accuracy, showing the ability to identify vulnerabilities. Our work provides a foundation for improving the dependability of transformer-based sentiment analysis systems through comprehensive test evaluation.", "sections": [{"title": "I. INTRODUCTION", "content": "The impact of neural networks on various fields, including image recognition, natural language processing, and self-driving cars, has been significant [1]. In the context of Natural Language Processing (NLP), neural networks have gained notable recognition due to their ability to enhance accuracy [2]. Sentiment analysis, also known as opinion mining or emotion AI, is a natural language processing (NLP) technique used to determine the emotional tone or subjective opinion expressed in text. This procedure utilizes NLP methodologies to analyze and interpret human language, enabling diverse applications across various domains. For instance, in marketing, sentiment analysis can be leveraged to evaluate customer opinions about products and services, providing valuable insights for brand reputation management. Various tools, including NLTK [3], are employed to interpret customer sentiment and capture embedded emotions.\nTraditional emotion recognition methods often struggle with the inherent complexities of human language. These complexities include word ambiguity, where a word's emotional connotation can shift depending on context (e.g., \"scared\u201d can be positive in an exciting situation or negative in a threatening one). In addition, sarcasm and negation pose challenges in identifying true sentiment. Furthermore, the process of understanding emotional meaning must include evaluating words that are not directly close to each other in a sentence, which is a step away from traditional methods.\nTransformer-based neural networks effectively handle the ambiguity in emotion sentiment analysis through their attention mechanisms and large parameters [4]. Parallel text processing allows them to analyze text data efficiently and capture the hidden emotional factors that can exist across longer distances within a sentence. Through pre-training on extensive datasets, transformers are capable of learning complex language patterns and the subtle details of human emotional expression [4].\nTo guarantee the efficacy of these complex models, comprehensive testing protocols are essential. A significant challenge lies in evaluating the sufficiency of test suites specifically designed for Transformer-based emotional sentiment analysis networks. Given their complex architecture and the large number of parameters, conventional methods like neuron coverage [5] seem to be impractical. To address this limitation, we introduce in this work a novel set of coverage criteria designed to evaluate test suites for the assessment of Transformer-based emotion sentiment analysis networks. We employ the principle of input space partitioning, segmenting textual data into emotionally-relevant categories. This strategy enables us to concentrate on particular areas of the input space and generate targeted test cases.\nTo achieve high coverage with the least number of test cases, we employ the k-projection coverage method [6]. This technique utilizes dimensionality reduction by examining smaller feature sets at the same time. Concentrating on these smaller subsets allows for an efficient evaluation of the network's performance across diverse emotional categories within the input space. This strategy ensures comprehensive coverage with fewer tests, thereby eliminating the necessity for extensive test suites. Additionally, we introduce a set of specialized features specifically designed for emotional sentiment analysis. These features capture the emotional details within text data, thereby aiding the network in accurately identifying the intended emotions.\nUpon applying our emotion-specific k-projection coverage method to a dataset designed for emotional sentiment analysis,"}, {"title": "II. METHODOLOGY", "content": "The performance of a neural network is typically evaluated by calculating its accuracy on a test suite. However, accuracy alone is not a sufficient measure of model quality. The effectiveness of testing and the reliability of the results heavily depend on the adequacy of the test suite. Coverage criteria are metrics used to assess how effectively a test suite exercises different aspects of a system under test. In traditional software testing, common criteria include statement, branch, and path coverage [13]. However, these traditional criteria cannot be directly applied to learning models due to their fundamentally different architecture [11]. To overcome this challenge, we propose a set of black-box metrics for evaluating test suites specifically designed for sentiment analysis tasks.\nOur approach originates from Input Space Partitioning (ISP), a black-box software testing technique. ISP involves dividing a system's input domain into distinct subsets or partitions, where each partition represents inputs expected to result in similar system behavior. Test cases are then strategically selected from each partition to ensure comprehensive coverage of the input space.", "subsections": [{"title": "A. Input Space Partitioning", "content": "The primary objective of this study is to assess the effectiveness of utilizing the input space partitioning technique to evaluate test suites for emotional sentiment analysis in Transformer-based neural networks. This technique utilizes the features of the input data to partition the input space into distinct regions, facilitating organization and understanding of the data. [6]\nThe architecture of the neural network model and the nature of the input data require the identification and selection of appropriate features for input space partitioning. Effective evaluation of a test suite requires developing a feature space that covers a wide range of scenarios and real-world instances. An effective feature space can be established by adherence to the guidelines proposed in Cheng et al.'s study [6], which outlines specific autonomous driving scenarios that the test cases should encompass and which will be defined subsequently.\nTo establish a clear foundation for the framework discussed in this study, we introduce the following notations. These notations are essential to understanding the subsequent mathematical definitions and analyses.\n1) Input Domain: The input domain includes all textual inputs the neural network can receive. All text variations the network must process and respond to are included.\n$D = {d_1,d_2,...,d_n}$\nwhere $d_i$ represents an individual input text.\n2) Partition: In order to understand the existing components and assess the test suite, it is crucial to establish specific partitions and map the input domain within those defined partitions. In this study, partitions are defined as follows:\n$P = {P_1, P_2, ..., P_k}$"}, {"content": "where $P_i \\subseteq D$ and $P_i \\cap P_j = \\emptyset$ for $i \\ne j$, and $\\bigcup_{i=1}^k P_i = D$.\n3) Features: Through the analysis of the input domain and the consideration of the functionality of the system under test (SUT), we can determine the important features that will be used to establish partitions.\n$F = {F_1, F_2,..., F_m}$\n4) Test Suite: Representative tests should be selected from each partition to form a test suite.\n$T = {t_1, t_2,..., t_l}$\nwhere $t_i \\in P_j$ for some $P_j \\in P$.", "title": "B. Emotional Features"}, {"title": "B. Emotional Features", "content": "Input space partitioning relies on specific features to establish partitions, crucial for evaluating test suite effectiveness in neural networks. The upcoming section will outline the specific features employed in this study.\nThe meaning and sentiment of a sentence can be identified through crucial components such as verbs, adverbs, adjectives, and other linguistic elements. Therefore, features are determined according to these important elements of a sentence. Verbs strongly influence a sentence's sentiment. For instance, in \"She laughed when I saw her,\" the verb \"laughed\" implies joy, highlighting its importance in sentiment analysis. Besides the verb, the adverb in a sentence is another influential feature affecting its sentiment. For example, in \"He bites his nails nervously,\" the adverb \"nervously\" alone can convey fear or anxiety, making its presence significant for sentence categorization.\nAdjectives shape a sentence's emotional tone by describing specific feelings. For example, \"She received a joyful gift\" conveys happiness, whereas \"She received a disappointing gift\" indicates sadness.\nNouns contribute significantly to a sentence's emotion. In \"The old man sat alone, his eyes filled with tears, mourning the loss of his dearest friend,\" nouns like \"tears\" and \"loss\" contribute to its \"sad\" emotional label.\nIn this research, the emotional attributes of nouns were integrated as an additional feature alongside verbs, adjectives, and adverbs. The sets of features presented in this paper are denoted as:\n$F = {Sv, S_{ADJ}, S_{ADV}, S_N}$\nwhere:\n*   $S_v$ represents sentiment scores for verbs, categorized into emotional labels:\n$S_v : V \\rightarrow {joy, anger, sadness, fear, surprise}$\n*   $S_{ADJ}$ represents sentiment scores for adjectives, categorized into emotional labels:\n$S_{ADJ} : ADJ \\rightarrow {joy, anger, sadness, fear, surprise}$\n*   $S_{ADV}$ represents sentiment scores for adverbs, categorized into emotional labels:\n$S_{ADV} : ADV \\rightarrow {joy, anger, sadness, fear, surprise}$\n*   $S_N$ represents sentiment scores for nouns, categorized into emotional labels, including a neutral category:\n$S_N : N \\rightarrow {joy, anger, sadness, fear, surprise, neutral}$\nWhen dealing with multiple features and their numerous potential values, the number of possible combinations can grow exponentially. This can make generating comprehensive input data, encompassing all features at once, extremely complex. k-projection coverage is a technique used in input space partitioning to overcome this challenge. It aims to systematically explore the input space by focusing on a subset of 'k' parameters at a time, while keeping the remaining parameters fixed.\nTo illustrate this method, consider an example of four features ${F_1, F_2, F_3, F_4}$, each taking two values ${0, 1}$. resulting in a total of $2^4 = 16$ possible combinations. Applying k-projection coverage with different values of k results in the following:\n*   k = 1 (Single Feature Projection): We focus on one feature at a time, keeping the others fixed.\n*   Number of projections: 4 (one for each feature)\n*   Number of cases per projection: 2 (the two possible values of the feature)\n*   k = 2 (Pairwise Projection): We focus on pairs of features, keeping the other two fixed.\n*   Number of projections: 6 (combinations of 4 features taken 2 at a time)\n*   Number of cases per projection: 4 (2 values for each of the two features)\n*   k = 3 (Three-way Projection): We focus on groups of three features, keeping the remaining one fixed.\n*   Number of projections: 4 (combinations of 4 features taken 3 at a time)\n*   Number of cases per projection: 8 (2 values for each of the three features)\n*   k = 4 (All Features Projection): We consider all features simultaneously.\n*   Number of projections: 1\n*   Number of cases per projection: 16 (all possible combinations)\nExpanding on the previous example, this approach systematically selects sets of k features and strives to achieve comprehensive coverage within a newly formed reduced dimension. This technique of dimensionality reduction enables efficient test generation, as it requires fewer test cases to achieve complete coverage of the input space.\nGenerally, to calculate the coverage rate for k = n, where n is the number of problem features, the following formula is employed:"}]}, {"content": "$COV = \\frac{C(D)}{a^n}$                                               (1)\nIn the above equation, $C(D)$ represents the coverage rate of the test data over possible states, where a is the range of each feature and n is the problem's dimensionality. In our context, $\\alpha = 6$ and n = 4, requiring 1296 unique test data points for full coverage. This approach, however, is computationally demanding and requires substantial test data.\nAlternatively, when considering the effect of k, the following formula is used:\n$COV = \\frac{C(D)}{{n \\choose k} a^k}$                              (2)", "title": "C. Evaluation of the Coverage Criteria"}, {"title": "C. Evaluation of the Coverage Criteria", "content": "Considering the inverse correlation of the denominator, it is evident that the denominator, which is affected by variables such as k, a, and n, greatly decreases the number of tests needed to achieve complete coverage.\nIncorporating all four features in sentences can be challenging and costly, reducing sentence clarity. Thus, this approach is ineffective. In order to tackle this issue, the dimensions of the feature coverage space are reduced, with the variable k being set to values of 2 and 3 in this study. The values for the variable k are compared in Section III to evaluate their individual effects.\nAfter measuring the coverage achieved by the test suite to assess the effectiveness of the k-projection metric, we attempt to increase coverage by generating test cases for uncovered scenarios. Ultimately, we explore whether improving coverage aids in the detection of hidden flaws in existing sentiment analysis networks.", "subsections": [{"title": "C. Evaluation of the Coverage Criteria", "content": "Our approach to evaluating the proposed coverage criterion involves examining whether increasing a test suite's coverage score uncovers more failures in the sentiment analysis model. We introduce a method for augmenting test suites with additional test cases specifically designed to improve their coverage score. This evaluation method has two key advantages:\n*   Validation of Coverage Criteria: Observing a decrease in model accuracy when using the augmented test suite confirms the effectiveness of the coverage criteria for evaluating sentiment analysis test suites. A higher coverage score indicates a greater ability to identify faults and lower model accuracy, thus validating the criteria.\n*   Practical Test Case Generation: The proposed method offers practitioners a valuable tool for generating effective test cases in sentiment analysis tasks. By focusing on increasing coverage, practitioners can systematically identify and address potential weaknesses in their models.\nTo uncover unexplored regions within the test suite based on our proposed coverage criteria, we propose a strategy for generating novel sentences. Manual sentence creation, while a viable method for producing desired test cases, is both time-intensive and unsuitable for generating large quantities."}, {"content": "Our idea in this study is to use Large Language Models (LLMs) [14] to generate the required sentences. LLMs require training using huge quantities of textual data, which allows them to imitate human-like text production. Examples of such LLMs include GPT-3 [15] and LaMDA [16]. In this study, we employed Claude 3 Opus [17], an LLM produced by Anthropic, which is recognized for its exceptional sentence generation capabilities.\nWhen utilizing LLMs for test generation, explicit prompts play a crucial role in guiding the model to produce specific types of test data that align with the research objectives. By evaluating the datasets using defined features in II, it is apparent that this dataset does not contain inputs with mixed emotions. For instance, after analyzing the test suites, we identified that the test suite lacks sentences where a verb is labeled with sadness and an adverb is labeled with joy. We generate such a test case using the following formulaic approach:\nLet S represent the set of all sentences in the test suite, and define subsets V and A as follow:\n$V = {s \\in S | verb(s) = sadness}$\n$A = {s \\in S | adverb(s) = joy}$\nThe required test case would be s' \u2208 V \u2229 A. To generate the missing test case s', we utilize:\ns' = LLM(\"Generate a sentence with a verb labeled as sadness and an adverb labeled as joy\")\nThis approach ensures that the generated sentence s' fills the identified gap in the test suite, thereby increasing the coverage of the test suite according to our proposed criteria. By adding s' to S, we obtain an augmented test suite S':\n$S' = S \\cup {s'}$\nThe augmented test suite S' now includes sentences that exhibit the desired feature combinations, leading to a more comprehensive evaluation of the model's capabilities. Additionally, as stated in the evaluation report, these types of situations might occur in user experiences and are viewed as real-life scenarios [18]. Therefore, it is crucial to include these generated tests in the test set that showcases these real-life, uncovered scenarios.\nFor the purpose of generating test cases, we selected the Claude language model [17] due to its capacity to generate varied and innovative test sets that are in line with our research goals. This model's attributes enable the generation of test cases that meet research requirements and highlight the robustness and reliability of sentiment analysis.\nThe generated sentences lack an emotion label, making them unsuitable for use as a test. In order to assign labels to the generated tests, we utilized six of the most accurate sentiment analysis models that were identified in the research literature [19], [20], [21], [22], [9], [23]. with their accuracy mentioned in table I. The input sentences were fed into these models"}]}, {"title": "III. EXPERIMENTAL RESULTS AND DISCUSSION", "content": "The dataset utilized for assessing the results is denoted as CARER [25]. This dataset contains six primary emotions expressed through text, enabling a thorough assessment of the neural network's ability to accurately classify emotions. The dataset is split into training and testing sets. The training set comprises 16,000 tweets, each associated with six output labels. Similarly, the test set consists of 2,000 tweets, each labeled with six output categories. A total of 18,000 tweets are chosen as the test set and assessed using the proposed coverage evaluation method. For this study, the dataset is divided into smaller subsets, each consisting of 200 tweets, in order to analyze the coverage capability of each subset. Additional tests will be set based on the uncovered features.\nOur model is compared to the existing test dataset since, based on our findings, no other model conducts test generation for neural networks in text categorization in the same way. This study applied the Claude language model [17] for test generation and 6 sentiment analysis models for differential testing [19], [20], [21], [22], [9], [23]. A distinct arrangement of the DISTILBERT model [8] was employed for word-level analysis. The process of learning and loading was executed on Google Colab, utilizing a central processing unit (CPU) with a capacity of 12.7 gigabytes of random access memory (RAM).\nWe tried different values of k in our implementation to evaluate the effect of k in our approach. The results presented in Fig. 1 illustrate the differences in coverage among various implementations of k. When k=2, the coverage increases on average by 14% (Fig. 1a). However, when k=3, the average increase in coverage is 18% (Fig. 1b). The initial coverage for k=3 is 32%, while the initial coverage for k=2 is 62%. This indicates that the test suite lacks sufficient coverage for complicated sentences that involve three combinations of features. Although full coverage was not attained, the sentences generated noticeably raised the total test coverage in comparison to the original data.\nFigure 2 demonstrates a decline in accuracy for both test generation implementations, suggesting that the generated tests expose previously unexplored areas and have the potential to improve the model's functionality. When k=2 (Fig. 2a), the accuracy experiences an average loss of 5%, whereas for k=3 (Fig. 2b), there is an average decrease of 8%. These findings indicate that the tests created for k=3 demonstrate a higher number of defects in the model's decision-making capabilities.\nThe average accuracy after test generation for k=3 models is 85%, whereas for k=2 models it is 88%, which is lower than the model's accuracy on the base test suite, which is 94%. This indicates a decrease in the model's precision when tested on these more challenging data points. This drop results from intentionally pushing the model to its limits in order to uncover areas for enhancement. Based on the experimental results for k = 3 the augmented test suite exhibits a greater decrease in accuracy compared to k = 2. This can be attributed primarily to the production of more intricate sentence structures. Nevertheless, when k = 3, there is a greater time complexity and increased costs involved in generating these sentences. The sentences produced for k = 3 exhibit a significant level of complexity and deviate considerably from real-life situations, thereby making it challenging to evaluate the model's performance in practical settings."}, {"title": "IV. CONCLUSION AND FUTURE WORKS", "content": "This study introduced novel coverage criteria for evaluating test suites of transformer-based sentiment analysis neural networks. By using input space partitioning focused on emotionally relevant linguistic features and the k-projection coverage metric, we efficiently developed test cases and analyzed key feature subsets. Our methodology demonstrated a 16% average increase in test coverage for all methods and a 6.5% decrease in model accuracy on augmented tests. The proposed criteria balance bug detection capability and computational efficiency, surpassing existing techniques. By emphasizing emotional dimensions through input space partitioning, our approach ensures thorough evaluation across diverse real-world scenarios.\nFuture work could extend our coverage criteria to other transformer-based NLP tasks like text summarization or machine translation. Expanding linguistic elements beyond verbs, adjectives, adverbs, and nouns could improve the detection of subtle emotional nuances, including negations and emojis. Finally, investigating the scalability and efficiency of our methodology on larger datasets and wider applications will be essential for real-world, industry-scale applications."}]}