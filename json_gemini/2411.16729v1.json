{"title": "DiM-Gestor: Co-Speech Gesture Generation with Adaptive Layer Normalization Mamba-2", "authors": ["Fan Zhang", "Siyuan Zhao", "Naye Ji", "Zhaohan Wang", "Jingmei Wu", "Fuxing Gao", "Zhenqing Ye", "Leyao Yan", "Lanxin Dai", "Weidong Geng", "Xin Lyu", "Bozuo Zhao", "Dingguo Yu", "Hui Du", "Bin Hu"], "abstract": "Speech-driven gesture generation using transformer-based generative models represents a rapidly advancing area within virtual human creation. However, existing models face significant challenges due to their quadratic time and space complexities, limiting scalability and efficiency. To address these limitations, we introduce DiM-Gestor, an innovative end-to-end generative model leveraging the Mamba-2 architecture. DiM-Gestor features a dual-component framework: (1) a fuzzy feature extractor and (2) a speech-to-gesture mapping module, both built on the Mamba-2. The fuzzy feature extractor, integrated with a Chinese Pre-trained Model and Mamba-2, autonomously extracts implicit, continuous speech features. These features are synthesized into a unified latent representation and then processed by the speech-to-gesture mapping module. This module employs an Adaptive Layer Normalization (AdaLN)-enhanced Mamba-2 mechanism to uniformly apply transformations across all sequence tokens. This enables precise modeling of the nuanced interplay between speech features and gesture dynamics. We utilize a diffusion model to train and infer diverse gesture outputs. Extensive subjective and objective evaluations conducted on the newly released Chinese Co-Speech Gestures dataset corroborate the efficacy of our proposed model. Compared with Transformer-based architecture, the assessments reveal that our approach delivers competitive results and significantly reduces memory usage\u2014approximately 2.4 times and enhances inference speeds by 2 to 4 times. Additionally, we released the CCG dataset, a Chinese Co-Speech Gestures dataset, comprising 15.97 hours (six styles across five scenarios) of 3D full-body skeleton gesture motion performed by professional Chinese TV broadcasters.", "sections": [{"title": "I. INTRODUCTION", "content": "RECENT advancements in 3D virtual human technology have broadened its applications across sectors like animation, human-computer interaction, and digital hosting. A key focus is generating realistic, personalized co-speech gestures, now made feasible through deep learning. Speech-driven gesture generation offers a cost-efficient, automated alternative to traditional motion capture, reducing manual effort while enhancing the realism and adaptability of virtual avatars for diverse professional and recreational uses.\nAchieving gesture-speech synchronization with naturalness remains challenging in speech-driven gesture generation. Transformer and Diffusion-based models have improved efficiency and flexibility, leading to innovations like Diffuse Style Gesture [1], Diffuse Style Gesture+ [2], GestureDiffuClip [3], and LDA [4]. Notably, Persona-Gestor [5], using a Diffusion Transformer (DiT) [6] architecture, achieves state-of-the-art results by effectively modeling the speech-gesture relationship. However, its transformer-based design imposes high memory usage and slower inference speeds, underscoring the need for more efficient solutions for real-time applications.\nThe Mamba architecture [7], addresses the quadratic complexity of traditional transformers. Validated across domains like vision [8]\u2013[10], segmentation [11] and image tasks [12], [13]. The improved Mamba-2 [14] confirms Mamba's theoretical equivalence to transformers via State Space Duality (SSD) while reducing complexity to linear. This advancement enables faster, resource-efficient processing, making Mamba a compelling alternative for tasks like speech-driven gesture generation, delivering comparable performance to transformers at reduced computational cost.\nTraining datasets in this field, including Trinity [15], ZEGGS [16], BEAT [17] and Hands 16.2M [18], largely focus on English content. Although BEAT offers 12 hours of Chinese speech data, it primarily features spontaneous speech. It limits its suitability for formal contexts like TV broadcasting or structured dialogues.\nIn this study, we present DiM-Gestor, an innovative model leveraging Mamba-2 and diffusion-based architectures to synthesis personalized gestures. The framework utilizes a Mamba-2 fuzzy feature extractor to autonomously capture nuanced fuzzy features from raw speech audio. DiM-Gestor integrates an AdaLN Mamba-2 module within its diffusion-based architecture, effectively modeling the intricate dynamics between speech and gestures. Inspired by DiT [6] and PG [5], the inclusion of AdaLN significantly enhances the model's ability to accurately capture and reproduce the complex interplay between speech and gestures. Compared with the adaLN transformer, the adaLN Mamba-2 achieves competitive performance while substantially optimizing resource efficiency, reducing memory usage by approximately 2.4 times, and improving inference speeds by a factor of 2 to 4.\nFurther, we released the CCG dataset, comprising 15.97 hours of 3D full-body skeleton gesture motion, encompassing six styles across five scenarios performed by professional Chinese TV broadcasters. This dataset provides high-quality, structured data, facilitating advanced research in Chinese speech-driven gesture generation, particularly for applications requiring formal and contextually appropriate non-verbal communication, as shown in Figure 1.\nFor clarity, our contributions are summarized as follows:\n\u2022 We introduce DiM-Gestor, an innovative end-to-end generative model leveraging the Mamba-2 architecture: Our approach achieves competitive performance while significantly optimizing resource efficiency, reducing memory usage by approximately 2.4 times and enhancing inference speeds by a factor of 2 to 4.\n\u2022 We released the CCG dataset, a Chinese Co-Speech Gestures dataset: This comprehensive dataset, captured using inertial motion capture technology, comprises 15.97 hours of 3D full-body skeleton gesture motion. It includes six distinct styles across five scenarios, performed by professional Chinese TV broadcasters, offering high-quality, structured data for advancing research in Chinese speech-driven gesture synthesis.\n\u2022 Extensive subjective and objective evaluations: These evaluations demonstrate that our model surpasses current state-of-the-art methods, highlighting its exceptional capability to generate credible, speech-appropriate, and personalized gestures while achieving reduced memory consumption and faster inference times."}, {"title": "II. RELATED WORK", "content": "This section briefly overviews transformer- and diffusion-based generative models for speech-driven gesture generation."}, {"title": "A. Transformer- and diffusion-based generative models", "content": "DiffMotion [19] represents a pioneering application of diffusion models in gesture synthesis, incorporating an LSTM to enhance gesture diversity. Cross-modal Quantization (CMQ) [20], jointly learns and encodes the quantized codes for representations of the speech and gesture together. However, these model supports only the upper body. Alexanderson et al. [4] refined DiffWave by replacing dilated convolutions, thereby unlocking the potential of transformer architectures for gesture generation. GestureDiffuCLIP (GDC) [3] employs transformers and AdaIN layers to integrate style guidance directly into the diffusion process. Similarly, DiffuseStyleGesture (DSG) [1] and its extension DSG+ [2] utilize cross-local attention and layer normalization within transformer models. While these methods have demonstrated significant progress, they often struggle to balance gesture and speech synchronization. This leads to gestures that can appear either overly subtle or excessively synchronized with speech.\nPersona-Gestor (PG) [5] addresses some of these challenges by introducing a fuzzy feature extractor. This approach uses 1D convolution to capture global features from raw speech audio, paired with an Adaptive Layer Normalization (AdaLN) transformer [6] to model the nuanced correlation between speech features and gesture sequences. Although Persona-Gestor achieves high-quality motion outputs, it is hindered by substantial memory requirements and slower inference speeds associated with convolutional and transformer-based architectures."}, {"title": "B. Co-speech gesture training datasets", "content": "The datasets commonly employed for training co-speech gesture models include Trinity [15], [21], ZEGGS [16], BEAT [17], and Hands 16.2M [18]. These datasets predominantly feature native English speakers engaged in spontaneous conversational speech. Although the BEAT dataset includes 12 hours of Chinese content, this subset is characterized by unstructured speech patterns, making it less suitable for applications requiring formal contexts, such as event broadcasting or structured dialogues. While Yoon et al. [22] present a dataset collected and extracted the skeleton motions from TED talks videos, this dataset's quality is unsuitable for high-quality gesture synthesis tasks.\nTo address these limitations, we adopt the Mamba-2 architecture [14], further adapting it with an AdaLN-based implementation. The Mamba-2 framework significantly reduces memory usage and improves inference speed, offering a more efficient solution for gesture synthesis in virtual human interactions. In addition, we introduce the Chinese Co-speech Gestures (CCG) dataset, comprising 15.97 hours of 3D full-body skeleton motion across six gesture styles and five scenarios. The dataset features high-quality performances by professional Chinese TV broadcasters. This dataset enables advanced research in speech-driven gesture generation, particularly in formal settings such as event hosting and professional presentations."}, {"title": "III. PROBLEM FORMULATION", "content": "We conceptualize co-speech gesture generation as a sequence-to-sequence translation task, where the goal is to map a sequence of speech audio features, $X = [x_t]_{t=1}^T \\in \\mathbb{R}^T$, to a corresponding sequence of full-body gesture features, $Y^o = [y_t]_{t=1}^T \\in \\mathbb{R}^{T \\times (D+6)}$. Each gesture frame $y_t \\in \\mathbb{R}^{(D+6)}$ comprises 3D joint angles, as well as root positional and rotational velocities, with D representing the number of joint channels and T denoting the sequence length. This formulation encapsulates the temporal and spatial complexity of mapping audio-driven dynamics to expressive human-like gestures.\nWe define the probability density function (PDF), $p_\\theta(\\cdot)$, to approximate the true gesture data distribution $p(\\cdot)$, enabling efficient sampling of gestures. The objective is to generate a non-autoregressive full-body gesture sequence ($Y^o$) from its conditional probability distribution, given the audio sequence ($X$) as a covariate:\n$Y^o \\sim p_\\theta(Y^o | X) \\approx p(Y^o | X)                                                                                                                                                                        (1)$\nThis formulation leverages a denoising diffusion probabilistic model trained to approximate the conditional distribution of gestures aligned with speech. This approach provides a robust framework for learning and synthesizing co-speech gestures with high fidelity and temporal coherence by modeling the intricate relationship between audio inputs and gesture outputs."}, {"title": "IV. SYSTEM OVERVIEW", "content": "DiM-Gestor, an end-to-end Mamba- and diffusion-based architecture, processes raw speech audio as sole input to synthesize personalized gestures. This model balances naturalness with lower resource consumption, as shown in Figure 2."}, {"title": "A. Model Architecture", "content": "The architecture of DiM-Gestor, depicted in Figure 2, integrates four key components to efficiently generate personalized gestures directly from speech audio: (a) Mamba-2 Fuzzy Feature Extractor: This module employs a fuzzy inference strategy utlizing Mamba-2 to autonomously capturing nuanced stylistic and contextual elements from raw speech audio. (b) Stack of AdaLN Mamba-2 Blocks: These blocks introduce AdaLN Mamba-2 architecture that applies uniform transformations across all tokens, enabling the model to effectively capture the intricate interplay between speech and gestures while enhancing computational efficiency. (c) Gestures Encoder and Decoder: These modules encode gesture sequences into latent representations and decode them back into full-body motion outputs, ensuring accurate reconstruction of gesture dynamics. (d) Denoising Diffusion Probabilistic Model (DDPM): As the backbone for probabilistic generation, this module leverages diffusion processes to synthesize diverse and realistic gesture sequences aligned with the given speech context.\nBy combining these components into a unified framework, the DiM-Gestor architecture captures the complexity of human gestures in relation to speech while significantly reducing memory consumption and improving inference speed. This design ensures high-quality, personalized, and contextually appropriate gesture generation.\n1) Mamba-2 Fuzzy Feature Extractor: This module employs a fuzzy inference strategy, which does not rely on explicit and manual classification inputs. Instead, it provides implicit, continuous, and fuzzy feature information, automatically learning and inferring the global style and specific details directly from raw speech audio. Illustrated in Figure 2, this module is a dual-component extractor comprising both global and local extractors. The local extractor utilizes the Chinese Hubert Speech Pretrained Model [23] to process the audio sequence into discrete tokens. This pre-trained model, for its proficiency in capturing the complex attributes of speech audio, allows it to effectively represent universal Chinese speech audio latent features, denoted as $Z_x$.\nWe implement a Mamba-2 [14] global style extractor framework. In the Mamba architecture, the module scans the entire sequence of $Z_x$ to capture the style feature. The last output token, $z_s$, is considered crucial as it encompasses the global style feature contained within the speech audio. This feature is then broadcast to align with the local fuzzy features, ensuring that the global context influences the local gesture synthesis. This process allows the model to maintain a holistic understanding of the style and emotional context throughout the gesture generation process. This unified latent representation is then channeled to the downsampling module for further refinement."}, {"title": "2) AdaLN Mamba-2:", "content": "The AdaLN's fundamental purpose is to incorporate a conditional mechanism [5], [6] that uniformly applies a specific function across all tokens. It offers a more sophisticated and nuanced approach to modeling, enabling the system to capture and articulate the complex dynamics between various input conditions and their corresponding outputs. Consequently, this improves the model's predictive accuracy and ability to generate outputs more aligned with the given conditions.\nDiffusion Transformers (DiTs) exemplify a sophisticated advancement in diffusion model architectures, incorporating an AdaLN-infused transformer framework primarily for text-to-image synthesis. The utility of DiTs has recently expanded to include text-conditional video generation, illustrating their versatility. Furthermore, DiTs have shown potential in co-speech gesture generation [5], marking a significant step in applying these models to sequence-based tasks. However, the inherent quadratic space complexity associated with Transformers results in substantial memory consumption and slower inference speeds.\nThe AdaLN architecture involves regressing the dimension-wise scale and shift parameters ($\\gamma \\in (T, D)$ and $\\beta \\in (T, D)$), derived from the fuzzy feature extractor output C, rather than directly learning these parameters, as depicted in Figure 2 and algorithm 1. In each AdaLN Mamba-2 stack, a latent feature $Z_{1:T,m}$ is generated, combining condition information and gesture using AdaLN and the Mamba-2 architecture. The index m ranges from 1 to M, where M represents the total number of AdaLN Mamba-2 stacks. Furthermore, as illustrated in Figure 2, the final layer utilizes the same fuzzy features, supplemented by a scale and shift operation to fine-tune the gesture synthesis."}, {"title": "3) Gesture Encoder and Decoder:", "content": "The architecture of the gesture encoder and decoder is meticulously designed to process gesture sequences, as illustrated in Fig. 2.\nGesture Encoder employs a Convolution1D layer with a kernel size of 3 to encode the initial sequence of gestures, denoted as X, into a hidden state $H_1 = [h_t]_{t=1}$. Experimental results reveal that using a kernel size of 1 often leads to animation jitter, likely due to insufficient spatial-temporal feature capture. Conversely, the kernel size 3 effectively mitigates this issue by capturing spatial-temporal relationships across adjacent frames, ensuring smoother gesture dynamics.\nThe decoder transforms the high-dimensional output from the AdaLN Mamba-2 layer to the original dimension, corresponding to the skeletal joint channels. This step involves generating the predicted noise ($\\epsilon$), a critical component for gesture reconstruction. Utilizing a 1D kernel of size 1 in the decoder enables the model to extract relevant features and correlations between adjacent joint channels, thereby improving the overall quality and coherence of the generated gestures."}, {"title": "B. Training and Inferencing with DDPM", "content": "The diffusion process, leveraging the Denoising Diffusion Probabilistic Model (DDPM) [5], [19], [25], [26], enabling the reconstruction of the conditional probability distribution between gestures and fuzzy features.\nThe model functions through two primary processes: the diffusion process and the generation process. During training, the diffusion process progressively transforms the original gesture data ($X^o$) into white noise ($X^N$) by optimizing a variational bound on the data likelihood. This gradual addition of noise is meticulously controlled to preserve the underlying data structure, facilitating effective learning of the conditional probability distribution.\nDuring inference, the generation process seeks to reverse the transformation performed during training. It reconstructs the original gesture data from noise by reversing the noising process through a Markov chain, employing Langevin sampling [27]. This approach facilitates the accurate and effective recovery of gesture data from its perturbed state.\nThe Markov chains utilized in the diffusion and generation processes ensure a coherent and systematic transition between stages, thereby preserving the integrity and quality of the synthesized gestures. The specific Markov chains employed in the diffusion and generation processes are as follows:\n$p(Y^n | Y^o) = N(Y^n; \\sqrt{\\alpha^n}Y^o, (1-\\alpha^n) I)   \\text{ and } \\\\  p_\\theta (Y^{n-1} | Y^n, Y^o) = N(y^{n-1}; \\mu_\\theta(y^n, Y^o), \\sigma_\\theta^2 I),                                                                                                                                   (3)$\nwhere $\\alpha^n := 1 - \\beta^n$ and $\\bar{\\alpha}^n := \\prod_{i=1}^n \\alpha^i$. As shown by [25], $\\beta_n$ is a increasing variance schedule $\\beta_1, ..., \\beta_N$ with $\\beta_n \\in (0,1)$, and $\\bar{\\beta}^n := \\frac{1}{n} \\sum_{i=1}^n \\beta^i$.\nThe training objective is to optimize the model parameters $\\theta$ by minimizing the Negative Log-Likelihood (NLL). This optimization is implemented through a Mean Squared Error (MSE) loss, which measures the deviation between the true noise, denoted as $\\epsilon \\sim N(0, I)$, and the predicted noise $\\epsilon_\\theta$. The objective function can be expressed as:\n$L_{MSE} = E_{x^o,\\epsilon,n} [||\\epsilon - \\epsilon_\\theta (x^n, n) ||^2], (4)$ where $x^o$ represents the original data, $x^n$ is the noised version of the data at step n, and n indicates the diffusion time step. This formulation ensures the model learns to accurately predict noise across all diffusion steps, thereby enabling the effective reconstruction of the original data during inference.\n$E_{y^o,\\epsilon,n} [||e - e_\\theta (\\sqrt{\\alpha^n}Y^o + \\sqrt{1 - \\alpha^n} \\epsilon, X, n) ||^2],                                              (5)$\nHere $e_\\theta$ is a neural network (see figure 2), which uses input $Y^o$, X and n that to predict the $e$, and contains the similar architecture employed in [28].\nAfter completing the training phase, we utilize variational inference to generate a full sequence of new gestures that align with the original data distribution, formulated as $Y^o \\sim p_\\theta (Y^o | X)$. In this generation phase, the entire sequence $Y^o$ is sampled from the learned conditional probability distribution, ensuring that the synthesized gestures accurately reflect the dynamics and nuances of the input speech features X.\nThe term $\\sigma_\\theta^2$ denotes the standard deviation of the conditional distribution $p_\\theta(Y^{n-1} | Y^n)$, playing a pivotal role in capturing the variability and intricacies of transitions across diffusion stages. In our model, we define $\\sigma_\\theta := \\beta_n$, where $\\beta_n$ represents a predetermined scaling factor. This factor adjusts the noise level at each diffusion step, enabling a controlled balance between smoothing and preserving fine details during the generation process.\nDuring inference, we send the entire sequence of raw audio to the condition extractor component. The output of this component is then fed to the diffusion model to generate the whole sequence of accompanying gestures ($Y^o$)."}, {"title": "V. EXPERIMENTS", "content": "Our experiments focused on producing full 3D body gestures, including finger motions and locomotion, trained on our released Chinese Co-speech Gestures (CCG) dataset."}, {"title": "A. Dataset Recording and Data Processing", "content": "1) Datasets: We have constructed Chinese Co-Speech Gestures, CCG dataset, a high-quality, synchronized motion capture and speech audio dataset comprising 391 monologue sequences. The dataset features performances by 12 female and 5 male actors, all conducted in Chinese and covering 6 distinct emotion styles with 5 different senses. These styles were carefully selected to comprehensively represent various postures, hand gestures, and head movements. The total length of the dataset amounts to 958.3 minutes. Table I and Figure 4 illustrate the time distribution of the different motion styles across the various scenes within the dataset.\nThe style labels for each sequence were assigned according to predefined actor directives. However, it is essential to note that these labels may not always align with the subjective interpretations of independent external annotators regarding the observed movement styles. The motion capture data was recorded using the Noitom PNS system\u00b9, which employs inertial motion capture technology."}, {"title": "2) Speech Audio Data Process:", "content": "Due to the Chinese Hubert Speech Pretrained Model being pre-trained on speech audio sampled at 16 kHz, we uniformly resampled all audio down from 44.1 kHz to match this frequency, ensuring compatibility and optimal performance."}, {"title": "3) Gesture Data Process:", "content": "We concentrate exclusively on full-body gestures, employing the data processing techniques detailed by Alexanderson et al. [29]. This includes capturing translational and rotational velocities to accurately delineate the root's trajectory and orientation. The datasets are uniformly downsampled to a frame rate of 20 fps. To ensure precise and continuous representation of joint angles, we utilize the exponential map technique [30]. All data are segmented into 20-second clips for training and validation purposes."}, {"title": "B. Model Settings", "content": "Our experiments utilized Mamba-2 architecture for a global fuzzy feature extractor and six AdaLN Mamba-2 blocks, each Mamba-2 configured with a 256 SSM state expansion factor, a local convolution width of 4, and a block expansion factor of 2. This encoding process transforms each frame of the gesture sequence into hidden states $h \\in \\mathbb{R}^{1280}$. We employ the Chinese Hubert Speech Pretrained Model (chinese-wav2vec2-base)2 for audio processing.\nThe diffusion model uses a quaternary variance schedule, starting from $\\beta_1 = 1 \\times 10^{-4}$ to $\\beta_N = 8 \\times 10^{-2}$ with a linear beat schedule, and a total of N = 1000 diffusion steps. The training batch size is set to 32.\nOur model was tested on Intel i9 CPU with a Nvidia Geforce 4090 GPU, in contrast to the A100 GPU used by Persona-Gestor [5]. The training time was approximately 6 hours."}, {"title": "C. Visualization Results", "content": "Our system excels in generating personalized gestures that are contextually aligned with speech by leveraging the Mamba-2 fuzzy inference extractor and adaLN Mamba-2."}, {"title": "D. Subjective and Objective Evaluation", "content": "In line with established practices in gesture generation research, we conducted a series of subjective and objective evaluations to assess the co-speech gestures generated by our proposed DiM-Gestor (DiM) model.\nWe primarily carried out experiments and made comparisons with models utilizing the transformers architecture, including LDA [4], DiffuseStyleGesture (DSG+) [2], Taming [31], GDC [3] and Persona-Gesture (PG) [5] as baseline models. The DSG++ are the best-performing in the 2023 [32] GENEA Challenge. We employ mamba-2 as a fuzzy feature extractor and adaLN mamba-2 architecture, denoted as DiM_m2_s_m2, to abbreviate the model. The first 'm2' signifies the adoption of adaLN mamba-2, while the second 'm2'(after s) indicates the utilization of mamba-2 architecture for the fuzzy feature extractor. The PG model configured with 12 blocks incorporates a stack of 12 AdaLN transformer blocks, whereas the variant with 6 blocks consists of a stack with 6 AdaLN transformer blocks.\nThe baseline models were initially trained using English speech datasets, including Trinity [15], ZEGGS [16], and BEAT [17]. Contrary to these settings, we employed our internally recorded Chinese dataset for training and inference.\n1) Subjective Evaluation: We utilize three distinct metrics for comprehensive subjective evaluations: human-likeness, appropriateness, and style-appropriateness. Human-likeness evaluates the naturalness and resemblance of the generated gestures to authentic human movements, independent of speech. Appropriateness assesses the temporal alignment of gestures with the speech's rhythm, intonation, and semantics to ensure a seamless and natural interaction. Style-appropriateness quantifies the similarity between the generated gestures and their original human counterparts, ensuring fidelity to the intended gesture style.\nWe executed a user study utilizing pairwise comparisons by the methodology outlined by [33]. During each session, participants were shown two 20-second video clips. These clips, generated by different models, including the Ground Truth (GT), were presented for direct comparative analysis. Participants were instructed to select the preferred clip based on predefined evaluation criteria. Preferences were quantified on a scale ranging from 0 to 2, with the non-selected clip in each pair receiving a corresponding inverse score. A score of zero was used to indicate a neutral preference. More details are described in [5], [33].\nGiven the extensive array of styles within the datasets, an individual evaluation of each style was considered unfeasible. A random selection methodology was employed to address this. Each participant was assigned a subset of five styles for assessment. Critically, none of the audio clips selected for evaluation were used in the training or validation sets. This strategy ensured a broad yet manageable coverage of the dataset's diversity in a controlled and unbiased manner.\nWe invited 30 native Chinese volunteers-14 males and 16 females aged between 18 and 35 for user study.\nOne-way ANOVA and post hoc Tukey, multiple comparison tests, were conducted to determine if there were statistically significant differences among the models' scores across the three evaluation aspects. The results are presented in Figure 9 and Table II, offering detailed insights into the performance variances observed among models regarding human-likeness, appropriateness, and style-appropriateness.\nRegarding the Human Likeness metric, our proposed model, DiM_m2_s_m2, achieves a score of 0.653 \u00b1 1.323. This result closely approximates the top-performing PG-12blocks model (0.668\u00b11.192) and the Ground Truth (GT) benchmark (0.694 \u00b1 1.430). Statistical analysis revealed no significant differences among these three models, as Figure 10 illustrates. This indicates that the DiM_m2_s_m2 model performs comparably to the human baseline regarding perceived naturalness. In contrast, alternative methods such as DSG+ (-1.494\u00b10.794), GDC (-0.467 \u00b1 1.399), and LDA (-1.069 \u00b1 1.099) exhibit significantly lower scores on the Human Likeness metric.\nIn terms of the Appropriateness metric, DiM_m2_s_m2 attains a score of 0.682 \u00b1 1.326, demonstrating high competitiveness and being nearly on par with PG-12blocks, the top-performing synthetic model with a score of 0.687\u00b11.247. The Ground Truth (GT) establishes the benchmark at a score of 0.710\u00b11.268. Additionally, no significant difference exists among these three models. The models' significant differences are visually depicted in Figure 10. Models such as DSG+, Taming, and GDC, which record scores within the negative range, evidently have difficulty synchronizing gestures with the speech context.\nThe model DiM_m2_s_m2 exhibits significant superiority in the Style Appropriateness metric, achieving the highest score of 1.30 \u00b10.770 among all evaluated models. This metric underscores our method's capability to generate gestures stylistically congruent with the various Chinese speech styles. In contrast, the PG-12blocks model attains a lower score of 0.664 \u00b1 1.216, while the other methods, including DSG+, GDC, and LDA, exhibit negative scores in this category. These findings emphasize the distinct advantage of DiM_m2_s_m2 in producing gestures that align closely with the intended stylistic attributes of the spoken language.\nIn conclusion, DiM_m2_s_m2 outperforms alternative models in Style Appropriateness and achieves highly competitive results in Human-Likeness and Appropriateness. These findings suggest that DiM_m2_s_m2 effectively generates perceptually realistic gestures and is well-aligned with speech-driven gesture synthesis's contextual and stylistic requirements. This highlights the strength of our Mamba-2 approach in addressing the multi-dimensional challenges in this domain, setting a new standard for synthetic gesture quality compared to traditional transformer methods.\n2) Objective Evaluation: We employ three objective evaluation metrics to assess the quality and synchronization of generated gestures: Fr\u00e9chet Gesture Distance (FGD) in both"}, {"title": "E. Ablation Studies", "content": "This section details an ablation study to assess the individual contributions of two key components in our proposed model: the fuzzy feature extractor and the AdaLN architectures equipped with different versions of the Mamba framework, as shown in Tables II, III, and Figure 9.\n1) Effect of Fuzzy Feature Extractor: Table II and Figure 9 present the performance differences when utilizing mamba-1, mamba-2, and convolution 1D [5] as the fuzzy feature extractor across various model configurations. In all instances, the AdaLN module is integrated with the Mamba-2 architecture.\nIn the Human-likeness metric, both DiM_m2_s_m2 (0.653 \u00b1 1.323) and DiM_m2_s_conv (0.593 \u00b1 1.222) achieve high scores, with no statistically significant difference (p = 1.0 > 0.05) observed between the two. These findings suggest that Mamba-2 and 1D Convolution represent competitive alternatives for capturing human-like gestures. However, the model DiM_m2_s_m1, which employs Mamba-1, achieves a significantly lower score of 0.243 \u00b1 1.348.\nSimilarly, in the Appropriateness metric, there is no significant difference (p = 2.1e-1 > 0.05) between DiM_m2_s_m2 (0.682 \u00b1 1.326) and DiM_m2_s_conv (0.425 \u00b1 1.319). Both models align well with the contextual requirements of speech-driven gestures. However, DiM_m2_s_m1 achieves a much lower score (0.106 \u00b1 1.420).\nIn Style Appropriateness, DiM_m2_s_m2 outperforms the alternatives, achieving the highest score of 1.30 \u00b1 0.770. This result underscores the superiority of Mamba-2 in capturing stylistic nuances and generating gestures that are contextually relevant and visually coherent with the speech content. In comparison, DiM_m2_s_conv scores moderately at 0.549 \u00b1 1.111, while DiM_m2_s_m1 achieves a significantly lower score of 0.230 \u00b1 1.221.\nThe objective evaluation results in Table III highlight the benefits of using the mamba-2 fuzzy feature extractor in achieving superior alignment and detail in gesture synthesis."}, {"title": "2) Effect of adaLN mamba Architecture:", "content": "This experiment investigates the impact of employing different adaLN Mamba architectures (adaLN Mamba-1 and adaLN Mamba-2) on the generation effect, as shown in Table II and Table III. In the Human Likeness metric, DiM_m2_s_m2 achieves a high score of 0.653 \u00b1 1.323, comparable to DiM_m1_s_conv (0.651 \u00b1 1.279) and DiM_m1_s_m1 (0.641 \u00b1 1.236). Statistical analysis reveals no significant difference between these models in this metric (Figure 9). However, DiM_m2_s_m1, which uses the Mamba-1 architecture, scores considerably lower at 0.243 \u00b1 1.348. This suggests that while both architectures can produce human-like gestures, the Mamba-2 architecture in DiM_m2_s_m2 exhibits slight improvements in capturing nuanced human motion.\nStyle Appropriateness highlights a more pronounced distinction between the models. Our proposed model, DiM_m2_s_m2, achieves the highest score of 1.30 \u00b1 0.770, indicating superior stylistic coherence and visual appeal in gesture synthesis. Both DiM_m1_s_conv (0.384 \u00b1 1.186) and DiM_m1_s_m1 (0.323 \u00b1 1.136) perform moderately, while DiM_m2_s_m1 again scores poorly (0.230 \u00b1 1.221).\nQuantitatively, Table III highlights that models with adaLN mamba-2 architectures, like DiM_m2_s_m2, consistently achieve lower FGD scores, underscoring their alignment with natural gestures. For instance, DiM_m2_s_conv records an FGD of 44.282 in feature space, while DiM_m1_s_m1, using adaLN mamba-1, records a significantly higher FGD of 130.119. This suggests that adaLN mamba-2 enhances alignment with the target gesture distribution.\nThe ablation study confirms that using mamba-2 for both the fuzzy feature extractor and adaLN mamba architecture provides optimal results. Our proposed model, DiM_m2_s_m2, outperforms all other configurations in perceptual and quantitative metrics, highlighting the combined benefits of the mamba-2 configurations in generating realistic, contextually appropriate, and stylistically aligned gestures in speech-driven gesture synthesis."}, {"title": "VI. PARAMTER COUNTS AND INFERENCE SPEED", "content": "This section discusses the parameter counts and inference speed among DiM-Gestor with different Mamba versions and the PG, which utilizing AdaLN transformer architecture."}, {"title": "A. Parameter Counts", "content": "DiM_m2_s_m2 demonstrates a relatively low parameter count of 535M, making it considerably more compact than configurations such as DiM_m1_s_conv (836M) and DiM_m2_s_conv (826M). This compactness is advantageous, as it indicates that DiM_m2_s_m2 can achieve high-quality gesture synthesis with reduced computational overhead compared to larger models. While the AdaLN-based transformer model, PG-12blocks, can generate high-quality gestures, its significantly larger parameter count of 1.2B may impact memory requirements, posing challenges for deployment in resource-constrained environments.\nThese observations highlight the efficiency and practicality of DiM_m2_"}]}