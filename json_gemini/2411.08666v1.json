{"title": "A Survey on Vision Autoregressive Model", "authors": ["Kai Jiang", "Jiaxing Huang"], "abstract": "Autoregressive models have demonstrated great performance in natural language processing (NLP) with impressive scalability, adaptability and generalizability. Inspired by their notable success in NLP field, autoregressive models have been intensively investigated recently for computer vision, which perform next-token predictions by representing visual data as visual tokens and enables autoregressive modelling for a wide range of vision tasks, ranging from visual generation and visual understanding to the very recent multimodal generation that unifies visual generation and understanding with a single autoregressive model. This paper provides a systematic review of vision autoregressive models, including the development of a taxonomy of existing methods and highlighting their major contributions, strengths, and limitations, covering various vision tasks such as image generation, video generation, image editing, motion generation, medical image analysis, 3D generation, robotic manipulation, unified multimodal generation, etc. Besides, we investigate and analyze the latest advancements in autoregressive models, including thorough benchmarking and discussion of existing methods across various evaluation datasets. Finally, we outline key challenges and promising directions for future research, offering a roadmap to guide further advancements in vision autoregressive models.", "sections": [{"title": "INTRODUCTION", "content": "Autoregressive (AR) models have recently driven significant progress in artificial intelligence, particularly through models like the GPT series [1], [2], [3], [4], [5] and other large language models (LLMs) [6], [7], [8] that excel at solving a variety of natural language processing tasks. These models employ a straightforward yet powerful \"next-token prediction\" strategy, allowing them to generate coherent and contextually relevant text by predicting each subsequent word in a sequence. The success of AR models can be attributed to two key characteristics: (1) scalability, as scaling laws [9], [10] enable researchers to predict the performance of larger models based on smaller ones, optimizing resource allocation and guiding model development; and (2) generalizability, as AR models can adapt to new and unseen tasks without requiring task-specific training [1], [3]. These promising characteristics enable AR models to address language tasks with unprecedented effectiveness, revealing their potential toward general-purpose AI systems.\nInspired by the success of AR models in natural language processing, recent studies have extended AR models to visual generation tasks. Notable examples include models like VQVAE [11], VQGAN [12], DALL-E [13], and Parti [14], which convert continuous images into discrete tokens through image tokenizers. This conversion allows AR models to generate images through a next-token prediction approach similar to that used in language processing. Visual tokenization unifies the representation of text and images by treating both as sequences of discrete tokens, making them compatible with sequence-to-sequence modeling techniques. As a result, these models can leverage the architectures similar to the GPT series [1], [2], [3] to learn effectively from large collections of text-image pairs.\nBeyond visual generation, AR models have also advanced visual understanding, particularly within the area of multimodal understanding [15], [16], [17], [18], [19], where they are designed to perceive and integrate multiple modalities. In multimodal tasks, AR models are trained to interpret visual input and generate coherent textual sequences, making them powerful tools for applications requiring a deep understanding of both visual and textual information. For example, multimodal large language models (MLLMs) like LLaVA [15] utilize LLMs to interpret visual input alongside text, allowing them to answer questions about images, generate descriptive captions, and engage in dialogue with detailed visual context. Through this design, AR-based MLLMs demonstrate great potential for advancing versatile visual understanding capabilities in AI applications.\nGiven the achievements of AR models in visual generation and visual understanding, recent works attempt to assemble the two types of capabilities into a unified AR model that can handle both visual generation and understanding. For example, Transfusion [20] demonstrates this integration by combining the next-token prediction objective commonly used in language modeling with diffusion processes for image generation. By jointly training on both text and image data, Transfusion [20] can handle both discrete text tokens and continuous image data within a single transformer architecture, enabling it to perform a wide range of multimodal tasks and bridging the gap between visual understanding and visual generation. Moreover, AR models show strong capabilities in both understanding and generation across other domains, such as video [21], where they handle tasks like video captioning, generation, and scene interpretation.\nDespite the significant progress and growing interest in AR models for vision research, there is currently a lack of a systematic survey to provide an overarching view of"}, {"title": "AUTOREGRESSIVE MODEL IN VISION", "content": "This section covers the various autoregressive models in generating different types of images, addressing tasks from simple visual concept generation to more complex multimodal and 3D generation tasks. A list of the autoregressive model for image generation can be found in Fig. 3."}, {"title": "Autoregressive Model for Image Generation", "content": ""}, {"title": "Unconditional Image Generation", "content": "Unconditional Image Generation refers to the task of generating images without any external input, such as class labels, text descriptions, or prior images. In this task, the model generates an image from scratch, relying solely on its learned distribution of image pixels. The goal is for the model to learn the underlying patterns, textures, and structures present in the training images, and to use this knowledge to generate new images that resemble the samples from the same data distribution.\nAutoregressive models accomplish this by treating image generation as a sequential process, predicting each pixel (or a group of pixels) one step at a time based on the previously generated parts of the image. These models learn to model the conditional distribution of the pixel values, typically in a row-by-row or block-by-block manner. Autoregressive Models for Unconditional Image Generation can be broadly categorized into two approaches:\nPixel-wise Generation, where each pixel is predicted based on the previously generated pixels. The image is treated as a sequence of individual pixel values, and the model learns the conditional probability of each pixel given the ones that have already been generated. For example, in [119], Pixel Recurrent Neural Networks (PixelRNN) and Pixel Convolutional Neural Networks (PixelCNN) model the distribution of natural images by sequentially predicting pixels along two spatial dimensions. With the designed fast two-dimensional recurrent layers and residual connections, PixelRNN and PixelCNN achieve advancing log-likelihood scores on datasets like CIFAR-10 and ImageNet. PixelSNAIL [118] is an autoregressive generative model that combines causal convolutions with self-attention to effectively model long-range dependencies in high-dimensional data such as images. It demonstrates state-of-the-art performance in density estimation on benchmark datasets like CIFAR-10 and ImageNet by leveraging the complementary strengths of both convolutions and self-attention, significantly improving on prior models like PixelCNN and PixelRNN. Generative Pretraining from Pixels (iGPT) [117] in an approach inspired by language models to apply autoregressive and BERT-like objectives for image representation learning. By training a transformer on raw pixel sequences without using any 2D structure, iGPT achieves competitive results on tasks like image classification, fine-tuning, and low-data regime benchmarks, demonstrating that generative models can learn powerful unsupervised visual representations. This category of fine-grained generation method can capture local dependencies in the image but can be computationally expensive due to the sequential nature of pixel prediction.\nToken-wise Generation treat images as sequences of tokens rather than raw pixel values. In these models, an image is first divided into patches or transformed into discrete tokens (analogous to words in text generation). The autoregressive model then predicts these tokens one by one, which can significantly reduce the computational cost and allow the model to capture both local and global dependencies more effectively. For example, Reinforced Adversarial Learning (RAL) [123] is a novel approach that combines reinforcement learning and adversarial training to enhance autoregressive models for image generation. By integrating adversarial loss and reinforcement learning, the method addresses issues like exposure bias and improves both the visual fidelity and diversity of generated images, achieving state-of-the-art performance on datasets like CelebA and LSUN-bedroom. [122] introduces Denoising Autoregressive Representation Learning (DARL), a generative approach for visual representation learning using a decoder-only Transformer model. It combines autoregressive prediction and denoising diffusion techniques, achieving performance close to state-of-the-art masked prediction models, and introduces innovations like decomposed rotary positional encoding (2D ROPE) to enhance model performance, particularly in autoregressive vision tasks. Multimodal Cross-Quantization VAE (MXQ-VAE) [121] is a novel vector quantizer that learns a joint representation space for unconditional image-text pair generation. This approach improves multimodal semantic correlation in a quantized space, allowing the generation of semantically consistent image-text pairs, achieving superior performance on multiple benchmarks, including Caption MNIST, Oxford Flower-102, CUB-200-2011, and COCO. Autoregressive Pretraining with Mamba (ARM) [120] is a novel method that leverages the Mamba state space model for autoregressive pretraining in vision tasks. This approach enhances both training efficiency and scalability, achieving superior performance on ImageNet and outperforming traditional supervised Mamba architectures, while unlocking the potential"}, {"title": "Class-conditional Image Generation", "content": "Class-conditional Image Generation refers to the task of generating images conditioned on specific class labels, where the generated image corresponds to a designated category. This task involves learning the joint distribution of images and their corresponding classes, allowing the model to generate realistic images that adhere to the characteristics of the target class. By conditioning the image generation process on class labels, models are able to create diverse and high-quality images that belong to the specified category. Autoregressive models accomplish this task by integrating class information into the sequential generation process. The Autoregressive Models for Class-Conditional Image Generation can be categorized into three approaches:\nPixel-wise Generation refers to generating an image pixel by pixel with each pixel conditioned on the class label and previously generated pixels. For example, Conditional PixelCNN [92] is an extension of the PixelCNN architecture that models complex conditional distributions for image generation, allowing it to generate diverse and realistic images conditioned on class labels or latent embeddings. The contribution includes a novel gated convolutional layer architecture that improves the performance of PixelCNN while reducing computational costs, and the model achieves state-of-the-art results in generating high-quality images from diverse classes such as animals, objects, and faces under varying poses and lighting conditions. PixelCNN++ [91] introduces several key improvements over the original PixelCNN, including the use of a discretized logistic mixture likelihood for pixel intensities, conditioning on whole pixels rather than sub-pixels, and the addition of downsampling to capture multi-resolution structures. These modifications result in better generative performance and faster training, achieving state-of-the-art log-likelihood scores on CIFAR-10 and providing improved sample quality and efficiency. Gated PixelCNN [90] is an extension of PixelCNN that enables controllable image generation by conditioning on both text descriptions and spatial structures, such as segmentation masks or keypoints, which improves the interpretability and control of generated images, demonstrating the ability to synthesize high-quality images from the Caltech-UCSD Birds, MPII Human Pose, and MS-COCO datasets while adhering to both text and structural constraints. Parallelized PixelCNN [89] improves PixelCNN by allowing pixels to be generated in parallel, significantly speeding up the image sampling process. The approach leverages a coarse-to-fine multiscale structure, enabling pixel groups to be generated conditionally independent of others, reducing the computational complexity from O(N) to O(log N) and achieving competitive performance in tasks like class-conditional image generation and video generation. Image Transformer [88] introduces a self-attention-based model for image generation, adapting the Transformer architecture from natural language processing to vision tasks. By employing local self-"}, {"title": "Text-to-Image Generation", "content": "attention and large receptive fields, the model outperforms traditional CNN and autoregressive models like PixelCNN on tasks such as unconditional image generation and super-resolution, achieving state-of-the-art performance on benchmarks like ImageNet\nToken-wise Generation refers to treating an image as a sequence of discrete tokens or patches, and the autoregressive model predicts these tokens based on the class label. For example, Vector Quantized-Variational AutoEncoder (VQ-VAE) [107] is a novel generative model that uses discrete latent variables instead of continuous ones, addressing issues like posterior collapse that commonly arise in traditional VAEs. By combining VQ with an autoregressive prior, the model effectively learns discrete representations and demonstrates strong performance across various modalities, including images, video, and speech, providing high-quality samples and enabling tasks such as unsupervised speaker conversion. ImageBART [65] is a novel autoregressive image synthesis model that combines a multinomial diffusion process with a bidirectional Transformer architecture. This model improves upon traditional autoregressive models by incorporating coarse-to-fine hierarchical generation, allowing for better global context and local control, leading to significant advances in high-fidelity image generation and conditional image synthesis tasks, such as text- and class-conditional generation. [103] combines a Vision Transformer (ViT)-based VQGAN for image encoding and a Transformer-based autoregressive model for image generation. The contributions include significant architectural improvements to VQGAN that enhance computational efficiency and reconstruction quality, achieving state-of-the-art performance on tasks like unconditional and class-conditioned image generation, with notable advancements in unsupervised representation learning Open-MAGVIT2 [94] is an open-source project that replicates and enhances Google's MAGVIT-v2 visual tokenizer, providing state-of-the-art performance in image reconstruction with a super-large codebook of 262,144 tokens. It further contributes by integrating this advanced tokenizer with autoregressive models, introducing asymmetric token factorization and \"next sub-token prediction\" techniques to improve scalability and generation quality in tasks like class-conditional image generation on datasets such as ImageNet. Token-wise Generation simplifies the generation process by handling larger regions of the image at a time, allowing the model to capture both local and global structures while ensuring the image conforms to the class label.\nScale-wise Generation refers to generating and image at multiple scales, starting with a low-resolution version and progressively refining it to higher resolutions, while conditioning the process on the class label. For example, [115] proposes a two-stage framework consisting of Residual-Quantized VAE (RQ-VAE) and RQ-Transformer to generate high-resolution images efficiently. The main contributions include a novel approach to reduce computational costs by using residual quantization for a more precise approximation of feature maps, enabling faster sampling and superior image quality compared to existing autoregressive models Visual Autoregressive Modeling (VAR) [116] redefines autoregressive learning for images through a coarse-to-fine \"next-scale prediction\u201d paradigm, significantly improving training efficiency and scalability. The VAR model surpasses previous autoregressive and diffusion models in terms of image generation quality, diversity, inference speed, and"}, {"title": "Text-to-Image Generation", "content": "Text-to-Image Generation is the task of generating realistic and coherent images based on a given textual description. The goal is to understand the semantics and relationships in the provided text and generate an image that accurately reflects the content described. It involves mapping the linguistic information in the text to the corresponding visual elements, such as objects, scenes, and attributes, ensuring that the generated image aligns with the textual description in terms of both content and style. Autoregressive Models accomplish this task by treating the image generation process as a sequential prediction problem, where the model generates an image one pixel or token at a time, conditioned on both the previously generated parts of the image and the text input. [13] presents DALL-E 1 using a transformer-based autoregressive model, where the text and image tokens are jointly modeled to generate high-quality images from textual descriptions without fine-tuning on specific datasets. The key contribution is scaling the model to 12 billion parameters, enabling zero-shot generalization and outperforming previous models in text-to-image generation tasks, including achieving state-of-the-art performance on MS-COCO. [85] introduces the Vector Quantized Diffusion Model (VQ-Diffusion) for text-to-image synthesis, which combines the strengths of vector quantized variational autoencoders (VQ-VAE) with a conditional denoising diffusion probabilistic model (DDPM) to eliminate unidirectional bias and reduce accumulated prediction errors. The model significantly improves image quality and inference speed over autoregressive methods, demonstrating superior performance in handling complex scenes and faster image generation. LQAE [83] is a method that aligns text and image data in an unsupervised manner by leveraging pretrained language models like BERT or RoBERTa, without the need for paired image-text data. The key contributions include"}, {"title": "Image-to-Image Translation", "content": "encoding images into sequences of text tokens using a vector quantization process and training the model to reconstruct images from text representations, which enables strong performance in tasks like few-shot image classification and multimodal learning with large language models. Hybrid Autoregressive Transformer (HART) [73] is a novel visual generation model that synergizes discrete and continuous tokenization to generate high-resolution images efficiently. It achieves state-of-the-art image generation performance by using a scalable autoregressive transformer for discrete tokens and a lightweight residual diffusion module for continuous tokens, significantly reducing computation costs while maintaining high image fidelity. Fluid [72] is an autoregressive text-to-image model that utilizes continuous tokens and random-order generation to improve the scalability and performance of autoregressive models. It achieves state-of-the-art results on benchmarks like MS-COCO, showing significant advancements in image quality, text alignment, and evaluation metrics such as FID and GenEval, especially when scaled to 10.5B parameters. As the models generate each pixel or token, they reference both the generated image context and the encoded text to ensure semantic consistency between the image and the text, which allows for highly detailed, contextually relevant image generation that corresponds directly to the given text input."}, {"title": "Image-to-Image Translation", "content": "Image-to-Image Translation is the task of transforming an input image from one domain into a corresponding image in another domain, while preserving the underlying content. This task involves learning a mapping between two image domains, such as converting sketches into photorealistic images, translating grayscale images to color, or performing super-resolution from low-resolution inputs. The primary challenge is to maintain consistency in the structure of the input image while accurately transforming its style or appearance based on the target domain. Autoregressive Models accomplish this task by treating the translation process as a sequential prediction problem, where the target image is generated pixel by pixel or token by token, conditioned on the input image. For example, SceneScript [71] is an autoregressive structured language model designed for reconstructing 3D scenes from egocentric video data. SceneScript predicts scene elements such as architectural layouts and object bounding boxes using a novel structured language command approach, offering advantages in scalability, memory efficiency, and easy extensibility to new tasks, while demonstrating state-of-the-art performance on architectural layout estimation and competitive 3D object detection results. ControlVAR [69] is a novel framework for controllable autoregressive image generation that jointly models pixel-level controls and images during training, allowing for more flexible and efficient conditional generation tasks. The key contributions include a unified representation for image and control, and the introduction of a teacher-forcing guidance strategy that enables improved control over the generated outputs, outperforming popular diffusion models like ControlNet and T2I-Adapter on several tasks such as control-to-image generation. CAR [67] is a novel framework for visual generation that integrates controllable signals into pre-trained autoregressive models to"}, {"title": "Image Editing", "content": "enable fine-grained image control. CAR leverages a multiscale latent variable model and a \"next-scale prediction\" paradigm to progressively refine control representations, achieving superior controllability and image quality compared to existing methods like ControlNet, while requiring significantly fewer training resources. These models generate the translated image, ensuring that the content remains consistent with the input while applying the style or appearance changes required by the target domain, which enable fine-grained control over the translation process, capturing intricate details between the two domains."}, {"title": "Image Editing", "content": "Image Editing refers to the task of modifying specific regions or features of an existing image while preserving the overall structure and content of the original image. It includes tasks like inpainting (filling in missing parts), replacing objects, adjusting lighting or textures, or modifying colors. The challenge in image editing is to make the changes seamlessly integrate with the rest of the image, ensuring that the edited regions remain coherent with the unedited parts. Autoregressive Models accomplish this by treating image editing as a sequential generation task, where the model modifies or fills in parts of the image step-by-step. For example, Query Outpainting Transformer (QueryOTR) [66] is a novel approach for image outpainting that frames the task as a patch-wise sequence-to-sequence autoregressive problem using a hybrid transformer-based encoder-decoder framework. Key contributions include the Query Expansion Module (QEM), which accelerates model convergence, and the Patch Smoothing Module (PSM), which enhances the connectivity between patches, enabling the generation of seamless and realistic outpainted images with superior performance compared to CNN-based methods VQGAN-CLIP [63] is a method for open-domain image generation and editing using natural language guidance by combining VQGAN for image generation and CLIP for semantic alignment between text and images. The main contributions include demonstrating high-quality image generation from complex text prompts and allowing for efficient image manipulation without additional model training, outperforming existing models like minDALL-E and GLIDE in tasks such as semantic image editing and generation. Make-A-Scene [64] is a novel scene-based text-to-image generation framework that incorporates human priors, allowing users to control the generation process through both text and scene layouts. The key contributions include improving tokenization with domain-specific knowledge, using scene tokens for controllability, and applying classifier-free guidance to enhance image quality and structure consistency, achieving state-of-the-art results on high-resolution image generation tasks. These models gradually modify the image while maintaining consistency with the unedited areas by capturing both local details and global context with PixelCNN or transformer-based approaches, ensuring that the edited image looks natural and visually coherent."}, {"title": "Motion Generation", "content": "Motion Generation refers to the task of generating a sequence of movements or poses over time, typically used in animation, robotics, or human motion synthesis. The"}, {"title": "Multi-Modal Tasks", "content": "goal is to produce realistic and smooth transitions between consecutive frames, capturing both spatial and temporal dynamics, while adhering to physical constraints such as continuity and natural motion patterns. Motion generation can involve predicting future frames in a video, animating human actions, or simulating object trajectories in a dynamic environment. Autoregressive Models accomplish this by treating motion generation as a sequential prediction task, where each frame or motion sequence is generated conditioned on the previously generated frames or poses. For example, [27] propose HuMoR, a generative model based on a conditional variational autoencoder (CVAE) that models plausible 3D human motion over time, even under noisy and occluded conditions. The model significantly advances human motion estimation by incorporating environmental constraints, predicting ground contact points, and generalizing to diverse motions and body shapes, outperforming prior methods in generating realistic and physically plausible 3D human motion from input modalities like RGB-D videos and 3D keypoints. Autoregressive Motion Diffusion (AMD) [24] is a novel model combining autoregressive methods with diffusion techniques to generate complex and long motion sequences based on text or audio inputs. AMD effectively captures the temporal dependencies of human motions by leveraging an iterative autoregressive process, and it introduces two large-scale datasets, HumanLong3D and HumanMusic, for evaluating the generation of high-fidelity and multi-modal motion. Autoregressive models in these methods are used to model long-term dependencies and ensure smooth transitions between frames. These models capture both short-term dynamics and long-range correlations, allowing for the generation of coherent and natural motion sequences over time."}, {"title": "Multi-Modal Tasks", "content": "Multi-Modal Tasks involve the integration of multiple types of data or inputs, such as text, images, audio, and video, to generate or interpret content. In these tasks, the model must align and process different modalities to create coherent outputs that capture the relationships between these diverse data types. Examples include text-image generation, video-audio synthesis, or generating images conditioned on both text and audio inputs. The challenge in multi-modal tasks is to effectively combine the information from various modality to produce outputs that are semantically and contextually aligned. Autoregressive Models accomplish multimodal tasks by treating the input from multi modality as conditioning data for the sequential generation of outputs in multi modality [48]. For example, Janus [31] is a unified autoregressive framework that decouples visual encoding for multimodal understanding and generation, addressing the challenges of balancing high-level semantic extraction for understanding with detailed visual generation. This decoupling enhances flexibility and performance, allowing for independent optimization of visual encoders for each task, leading to state-of-the-art results in both multimodal understanding benchmarks and visual generation tasks, surpassing previous unified models. Multi-Modal Auto-Regressive (MMAR) [32] is a framework for joint probabilistic modeling of images and text using continuous image representations, which avoids the information loss inherent in discrete image"}, {"title": "Medical Vision Tasks", "content": "tokens. The key contributions include disentangling the diffusion process from the autoregressive backbone to enable lossless image-text modeling, and addressing numerical errors in low-precision training with an optimized diffusion parameterization, significantly improving both image understanding and generation tasks. Libra [37] is a decoupled vision system built on large language models (LLMs) that separates inner-modal modeling from cross-modal interaction, improving both visual information modeling and multimodal comprehension. Key contributions include the development of a routed visual expert module and a hybrid tokenization strategy, which together enable Libra to excel in vision-language comprehension and image-to-text tasks, demonstrating competitive performance with far less training data than previous models. [38] introduces a powerful multimodal model that integrates various modalities, such as text, images, audio, and actions, within a unified autoregressive framework. The key contributions include novel architectural improvements like the multimodal mixture of denoisers and dynamic packing, which significantly boost model efficiency and stability, leading to state-of-the-art performance across over 35 benchmarks, including GRIT and MS-COCO, for both understanding and generation tasks. These models leverage transformers or recurrent architectures to capture dependencies across both time (for temporal modalities like audio or video) and space (for image-related tasks), ensuring that the generated output is consistent with the multi-modal inputs."}, {"title": "Medical Vision Tasks", "content": "Medical Tasks in Vision involve the application of computer vision techniques to analyze medical images for various tasks such as disease diagnosis, segmentation of anatomical structures, detection of abnormalities, and synthesis of medical images. These tasks are critical in fields like radiology, pathology, and surgery, where precise and accurate image interpretation can lead to better diagnosis and treatment outcomes. Examples include segmenting tumors in MRI scans, generating synthetic medical images for training, or predicting disease progression based on historical imaging data. Autoregressive Models accomplish these tasks by sequentially generating or analyzing parts of the medical images, treating the process as a step-by-step prediction task. For example, [53] propose a novel method for learning generalizable representations of 3D medical images, such as CT and MRI scans, through an autoregressive pre-training framework. The approach sequences 3D images into tokenized patches, capturing spatial, contrast, and semantic correlations, and significantly improves segmentation and classification performance across multiple medical tasks, achieving up to a 6% increase in accuracy over existing state-of-the-art methods. Medical Vision Generalist (MVG) [54] is a foundational model designed to unify various medical imaging tasks, such as segmentation, denoising, inpainting, and cross-modal synthesis, under a single image-to-image generation framework. By leveraging a hybrid strategy of masked image modeling and autoregressive training, MVG achieves state-of-the-art performance across 13 medical datasets and four imaging modalities, significantly improving the scalability and adaptability of medical imaging models for unseen tasks and datasets. [56] presents a"}, {"title": "3D Shape Generation", "content": "realistic morphology-preserving generative model for brain imaging, capable of generating high-resolution, anatomically accurate synthetic 3D brain images conditioned on patient characteristics such as age and pathology. The main contributions include achieving state-of-the-art performance in generating morphologically correct samples that can be used in downstream image analysis tasks, addressing data scarcity, and balancing phenotypic diversity in medical datasets. [57] propose a novel method for unaligned 2D to 3D image translation using Conditional Vector-Quantized Code Diffusion with transformers. The key contributions include leveraging independent vector-quantized spaces for 2D and 3D data, allowing for the generation of high-quality 3D images from unaligned 2D views, and utilizing a transformer-based conditional diffusion model to achieve superior performance in terms of fidelity, density, and coverage across complex volumetric datasets like medical chest CT scans and security baggage screening. By capturing dependencies across both spatial and temporal domains, autoregressive models help in creating detailed and reliable predictions for medical imaging tasks, enabling better diagnosis, synthesis, and treatment planning."}, {"title": "3D Shape Generation", "content": "3D Shape Generation refers to the task of generating three-dimensional shapes or objects, typically represented as point clouds, meshes, or voxels, that accurately depict the geometric structure of an object. The goal is to create or complete 3D shapes from scratch, partial inputs, or other conditional data (e.g., 2D views). Key challenges include maintaining structural coherence and generating high-resolution, realistic shapes. [11] employs transformers to generate high-resolution point clouds by decomposing them into semantically aligned shape compositions through a canonical mapping. The key contributions include using a vector-quantized variational autoencoder (VQVAE) with group-specific codebooks and a transformer to perform point cloud generation, achieving state-of-the-art performance in tasks like shape auto-encoding, unconditional generation, and multi-modal shape completion."}, {"title": "360-Degree Image Generation", "content": "360-Degree Image Generation refers to the task of creating panoramic images that provide a complete spherical view of a scene, allowing the viewer to explore any direction from a central viewpoint. These images are commonly used in virtual reality (VR), augmented reality (AR), and immersive experiences, where a full, continuous view is crucial. The challenge is maintaining geometrical consistency and ensuring seamless transitions across the entire spherical field of view. For example, Autoregressive Omni-Aware Generative Network (AOG-Net) [30] is a novel approach for 360-degree image generation that combines narrow field of view (NFoV) images and open-vocabulary text prompts. The key contributions include a global-local conditioning mechanism that dynamically integrates text, visual cues, and omni-geometry to guide the autoregressive generation process, achieving state-of-the-art results in generating high-quality 360-degree images across indoor and outdoor environments."}, {"title": "Robotic Manipulation", "content": "Robotic Manipulation refers to controlling a robotic system to interact with objects in its environment, such as grasping, lifting, or moving objects from one place to another. It requires the robot to understand the physical properties of the objects, plan its actions accordingly, and execute precise motor control to accomplish the manipulation. The complexity of robotic manipulation lies in handling diverse objects with varying shapes, weights, and material properties, often in dynamic environments, while ensuring stability and accuracy. [29] proposes Chunking Causal Transformer (CCT) for robotic manipulation tasks, which extends traditional autoregressive models by enabling multi-token predictions in a single pass, improving both computational efficiency and performance. The key contribution includes the development of the Autoregressive Policy (ARP) model, which leverages this transformer architecture to generate action sequences autoregressively, demonstrating state-of-the-art results across diverse manipulation environments like Push-T, ALOHA, and RLBench."}, {"title": "Low-level Vision", "content": "Low-level Vision refers to tasks that involve pixel-wise processing and manipulation of images, such as image denoising, super-resolution, deblurring, and colorization. These tasks focus on enhancing or restoring visual information at the pixel level, improving the quality and clarity of images. LM4LV [62] demonstrates that a frozen large language model (LLM) can process and output visual features for a range of low-level vision tasks such as image denoising, deblurring, and restoration, without any multi-modal data or prior knowledge. By integrating a simple vision encoder with a linear adapter, the paper bridges the gap between multi-modal large language models (MLLMs) and low-level vision tasks, showcasing the potential of LLMs in handling pixel-level image processing tasks with high precision."}, {"title": "Image Segmentation", "content": "Image segmentation involves dividing an image into meaningful parts, and autoregressive models can be applied to segment images by predicting regions sequ For example, [60] introduces a novel approach for unsupervised image segmentation by leveraging autoregressive models and maximizing mutual information (MI) between different views of input images. The key contributions include proposing a method that applies various forms of masked convolutions to generate multiple orderings of input data, allowing the model to learn useful representations without labels, and achieving state-of-the-art performance in unsupervised image segmentation tasks."}, {"title": "Acceleration", "content": "Acceleration of Autoregressive Models refers to the techniques and methods used to reduce the computational time and resource requirements for generating sequences (images or videos) using autoregressive models. Autoregressive models typically generate each token, pixel, or frame sequentially, with each step conditioned on the previously generated ones, making the process inherently slow, especially for long sequences. Accelerating these models is"}, {"title": "Unconditional Video Generation", "content": "crucial to make them more feasible for real-time applications and large-scale tasks. For example, [23] proposes a novel acceleration method for autoregressive text-to-image generation, called Speculative Jacobi Decoding (SJD), which allows for parallel decoding of multiple tokens without requiring additional training. The key contributions include improving the inference speed of autoregressive models by 2 to 3 times while maintaining the quality and diversity of generated images, as well as leveraging spatial locality in token initialization to further enhance performance in specific scenarios. FastV [22] is a plug-and-play method designed to optimize the inference efficiency of Large Vision-Language Models (LVLMs) by dynamically pruning visual tokens after the early layers. The key contributions include reducing computational costs by up to 45% of FLOPs without performance loss on various image and video understanding tasks, and demonstrating the method's flexibility and effectiveness across different models, making it practical for deployment in resource-constrained environments."}, {"title": "Autoregressive Model for Video Generation", "content": "This section focuses on autoregressive models in generating and predicting videos, covering tasks from simple frame prediction to complex multi-modal conditioned video generation, as listed in Fig. 4. They can be categorized into five classes: Unconditional Video Generation, Text-to-Video Generation, Visual Conditional Video Generation, Multimodal Conditional Video Generation, and Long Video Generation."}, {"title": "Unconditional Video Generation", "content": "Unconditional Video Generation is the task of generating a sequence of video frames from scratch, without any external inputs such as class labels, text, or initial frames. The objective is to produce coherent and realistic video sequences that capture both spatial (within each frame) and temporal (across frames) consistency, generating dynamic content that evolves smoothly over time. This task is challenging due to the need to model both the complex relationships between pixels in each frame and the dependencies between consecutive frames to ensure natural motion. Autoregressive Models accomplish this task by treating the video generation process as a sequential prediction task, where each pixel or patch of pixels in a frame is generated one at a time, conditioned on the previously generated pixels and all the prior frames in the video. [125] introduces a novel approach for generating long videos with thousands of frames by leveraging a time-agnostic VQGAN to remove unnecessary temporal dependencies and a time-sensitive transformer to model long-range temporal dynamics. The key contributions include solving the problem of quality degradation in long video generation by employing a sliding attention window and hierarchical transformer architecture, achieving"}, {"title": "Text-to-Video Generation", "content": "state-of-the-art results on benchmarks such as UCF-101, Sky Time-lapse, and Taichi-HD. MAGVIT-v2 [156"}]}