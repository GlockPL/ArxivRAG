{"title": "Demonstration Selection for In-Context Learning via Reinforcement Learning", "authors": ["Xubin Wang", "Jianfei Wu", "Yichen Yuan", "Mingzhe Li", "Deyu Cai", "Weijia Jia"], "abstract": "Diversity in demonstration selection is crucial for enhancing model generalization, as it enables a broader coverage of structures and concepts. However, constructing an appropriate set of demonstrations has remained a focal point of research. This paper presents the Relevance-Diversity Enhanced Selection (RDES), an innovative approach that leverages reinforcement learning to optimize the selection of diverse reference demonstrations for text classification tasks using Large Language Models (LLMs), especially in few-shot prompting scenarios. RDES employs a Q-learning framework to dynamically identify demonstrations that maximize both diversity and relevance to the classification objective by calculating a diversity score based on label distribution among selected demonstrations. This method ensures a balanced representation of reference data, leading to improved classification accuracy. Through extensive experiments on four benchmark datasets and involving 12 closed-source and open-source LLMs, we demonstrate that RDES significantly enhances classification accuracy compared to ten established baselines. Furthermore, we investigate the incorporation of Chain-of-Thought (CoT) reasoning in the reasoning process, which further enhances the model's predictive performance. The results underscore the potential of reinforcement learning to facilitate adaptive demonstration selection and deepen the understanding of classification challenges.", "sections": [{"title": "I. INTRODUCTION", "content": "LLMs have demonstrated exceptional capabilities across a wide array of NLP tasks, including text annotation [1], question answering [2], and dialogue generation [3]. These models leverage extensive corpora of textual data to learn rich representations, which empower them to perform reasoning with high accuracy [4]\u2013[6]. However, as the size and complexity of these models continue to expand, enhancing their reasoning capabilities becomes increasingly crucial. Effective reasoning is essential for tasks that demand logical reasoning, commonsense understanding, and contextual awareness [7], [8]. The ability to reason effectively not only improves the performance of LLMs in existing applications but also expands their potential for novel use cases that demand a deeper understanding of language and context.\nIn the realm of few-shot learning, in-context learning (ICL) has emerged as a promising approach to enhance reasoning in LLMs [9]. ICL utilizes LLMs, such as those based on the GPT architecture, to perform reasoning by providing a carefully curated set of demonstrations as context, rather than relying solely on extensive model retraining [6], [10], [11]. This methodology allows LLMs to leverage their inherent capabilities for understanding and processing text, making them particularly suitable for tasks with limited labeled data. However, the effectiveness of ICL is contingent upon the selection of appropriate and representative demonstrations from the knowledge base to serve as contextual references during reasoning on test data. This critical aspect of few-shot learning is often overlooked in existing literature [12], [13]. The careful selection of demonstrations is essential, as it directly influences the model's ability to generalize and perform accurately in novel situations.\nDespite the promise of ICL, a significant challenge remains in selecting the most relevant and diverse demonstrations from the knowledge base to optimize reasoning performance. Traditional methods of demonstration selection frequently prioritize similarity, potentially neglecting the importance of diversity in capturing the full spectrum of the data distribution [13]. This oversight can result in biased representations that do not generalize well to unseen data, ultimately hindering the LLM's predictive accuracy [12], [14]. Furthermore, conventional selection techniques typically employ fixed strategies that fail to dynamically adapt to the specific requirements of the reasoning task at hand [12], [13].\nDespite the promise of ICL, a significant challenge persists in selecting the most relevant and diverse demonstrations from the knowledge base to optimize reasoning performance. Traditional methods of demonstration selection often prioritize similarity, which can inadvertently overlook the importance of diversity in capturing the full spectrum of the data distribution [13]. This oversight may lead to biased representations that do not generalize well to unseen data, ultimately hindering the predictive accuracy of LLMs [12], [14]. Moreover, conventional selection techniques typically employ fixed strategies that fail to dynamically adapt to the specific requirements of the reasoning task at hand [12], [13]. This rigidity can limit the effectiveness of ICL, as the selected demonstrations may not align optimally with the context or nuances of the task, further exacerbating the challenges in achieving robust reasoning performance.\nTo address these challenges, we propose a novel framework that enhances reasoning in LLMs through adaptive demonstration selection. Our approach utilizes reinforcement learning to dynamically select a set of demonstrations as contextual references during LLM reasoning, employing a Q-learning framework to maximize both diversity and relevance to the classification task. This framework addresses key challenges in ICL by enhancing diversity and relevance, moving beyond traditional methods that prioritize similarity, which can lead to biased representations. By dynamically adapting to the specific requirements of reasoning tasks, our method aligns selected demonstrations with real-time feedback from the model, thereby improving predictive accuracy. Additionally, our approach captures the inter-relationships between demonstrations, enhancing the contextuality of ICL prompts and leading to a more coherent learning experience.\nThe primary motivation behind the proposed RDES framework is to enhance the performance of text classification tasks by strategically selecting reference demonstrations that maximize both relevance and diversity. The underlying intuition is that while selecting demonstrations similar to the input text can improve accuracy, ensuring diversity among the selected demonstrations is essential to prevent overfitting and enhance the model's ability to generalize to unseen data. In the context of few-shot learning, the selection of diverse demonstrations is particularly crucial, as it enables the model to capture a broader representation of the underlying data distribution, thereby improving its robustness and adaptability. Furthermore, this scenario aligns well with the sequential decision-making problems commonly addressed in reinforcement learning, where the model must make a series of decisions that optimize long-term performance based on the current state and available actions. By framing the demonstration selection process as a reinforcement learning task, we can leverage techniques that promote both exploration and exploitation, thereby further enhancing the effectiveness of the RDES framework.\nIn our study, we investigate two types of baselines: prompt engineering methods and demonstration selection methods. Prompt engineering involves the careful crafting of specific prompts designed to guide LLMs in generating accurate pre-dictions. In contrast, demonstration selection methods focus on identifying the most relevant reference demonstrations to enhance model performance. By evaluating our approach against these baselines, we aim to assess its effectiveness in improving the reasoning capabilities of LLMs. Our contributions can be summarized as follows:\n\u2022 We introduce a novel framework named RDES, which employs reinforcement learning to dynamically select reference demonstrations based on their contributions to both diversity and classification performance, resulting in enhanced model robustness.\n\u2022 Our Q-learning approach optimizes demonstration selection by balancing relevance to the classification task with diversity, addressing the limitations of traditional methods that prioritize similarity. This nuanced selection process mitigates the risk of overfitting by ensuring a varied representation of the knowledge base.\n\u2022 RDES can be seamlessly integrated with advanced reasoning techniques, such as CoT reasoning, to further enhance the model's predictive capabilities.\n\u2022 We conduct rigorous evaluations of our framework against ten established baselines, including prompt en-gineering and demonstration selection methods. Extensive experiments conducted on four benchmark datasets and involving 12 closed-source and open-source LLMs demonstrate that RDES significantly enhances classification accuracy in comparison to these baselines.\nIn summary, our proposed RDES framework not only addresses the critical challenges associated with ICL but also enhances the reasoning capabilities of LLMs through adaptive demonstration selection. By focusing on both relevance and diversity, RDES aims to improve the model's performance in text classification tasks, ultimately contributing to the advancement of few-shot learning methodologies in NLP. Specifically, the schematic framework of RDES is illustrated in Figure 2."}, {"title": "II. RELATED WORK", "content": "A. In-Context Learning\nICL has emerged as a transformative paradigm in NLP, especially with the advent of LLMs. This paradigm enables models to adapt to new tasks by conditioning on a small set of demonstrations provided within the input context, eliminating the need for extensive retraining. The seminal work by Brown et al. introduced ICL with the GPT-3 model, demonstrating that LLMs can effectively perform a wide range of tasks by being exposed to a few exemplars in the prompt [6]. Subsequent research has further explored ICL's capabilities, underlining its effectiveness in both few-shot and zero-shot learning scenarios [15], [16]. However, the selection of input demonstrations is pivotal, as the quality and diversity of these examples significantly impact model performance. Recent studies emphasize"}, {"title": "III. METHODOLOGY", "content": "A. Theoretical Basis\nThe RDES framework is grounded in the principles of reinforcement learning, where the selection of reference demonstrations is treated as a sequential decision-making problem. The primary objective is to maximize a cumulative reward function that reflects the accuracy of the classification task. In this context, let S represent the state space, which encompasses the current input text, the selected demonstrations, and their associated diversity scores. The action space A consists of the possible selections of reference demonstrations from a knowledge base. The reward function R quantifies the accuracy of the classification based on the predicted label and the selected demonstrations. To learn the optimal action-value function Q(s, a), which estimates the expected return of taking action a in state s, we employ the Q-learning algorithm. The update rule for the Q-values is given by:\n$$Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left(r + \\gamma \\max_{a'} Q(s', a') - Q(s, a)\\right)$$\nwhere \u03b1 is the learning rate, r is the immediate reward received after taking action a, \u03b3 is the discount factor, and s' is the new state resulting from the action taken.\nIn this equation, \u03b1 represents the learning rate, which determines the extent to which new information overrides previous knowledge. The term r denotes the immediate reward received after executing action a, which is based on whether the predicted label aligns with the labels of the selected demonstrations. The discount factor \u03b3 balances the significance of immediate rewards against future rewards, while s' indicates the new state resulting from the action taken, reflecting the updated selection of demonstrations and the current input text.\nThe iterative process of updating the Q-values allows the model to refine its action selections over time, ultimately enhancing the accuracy and diversity of the classification task. By integrating both similarity and diversity in the selection process, the RDES framework aims to improve the robustness of the classification outcomes. This dual focus not only facilitates the identification of optimal demonstrations but also ensures that the selected features contribute meaningfully to the overall classification performance, thereby fostering a more comprehensive understanding of the underlying data relationships."}, {"title": "B. Mathematical Formulation", "content": "To formalize the problem, we define the following components:\n1) State Representation: The state s can be represented as a tuple:\n$$s = \\{x, E, \\hat{y}, D\\}$$\nwhere:\n\u2022 x is the current input text,\n\u2022 E is the set of selected demonstrations,\n\u2022 \u0177 is the predicted label, and\n\u2022 D is the diversity score of the selected demonstrations.\n2) Action Space: The action a corresponds to selecting an example from the knowledge base K:\n$$a \\in K$$\n3) Reward Function: The reward function R(s, a) can be defined as:\n$$R(s, a) = \\begin{cases}\n1 & \\text{if } a \\text{ is the correct label for } x \\\\\n0 & \\text{otherwise}\n\\end{cases}$$\nThis reward function quantifies the accuracy of the classification based on the selected action.\n4) Diversity Score: The diversity score D of selected demonstrations can be defined as:\n$$D(E) = \\frac{|\\{l_1, l_2, ..., l_m\\}|}{k}$$\nwhere \\{l_1, l_2, ..., l_m\\} represents the unique labels of the selected demonstrations, and k is the total number of selected demonstrations. This score promotes the selection of demonstrations from diverse categories, thereby enhancing the overall diversity of the reference set."}, {"title": "C. RDES Demonstration Selection Framework", "content": "The proposed methodology enhances the selection of demonstrations through a structured approach. By leveraging a Q-learning framework, our method utilizes a Q-table to systematically evaluate and update the expected rewards associated with various state-action pairs. The integration of a parameter k allows for the selection of a specified number of demonstrations, ensuring a focused learning process. The learning rate \u03b1 is carefully calibrated to dictate the influence of new information on existing Q-values, while the discount factor \u03b3 weighs the significance of future rewards. This combination of parameters facilitates a dynamic learning environment, enabling the model to adaptively refine its understanding and improve decision-making based on the selected demonstrations. The framework not only enhances the diversity of the demonstrations chosen but also optimizes their relevance to the current task."}, {"title": "1) Theorem 1 (Convergence of Q-Learning in RDES)", "content": "In the RDES framework, the Q-learning algorithm converges to the optimal action-value function Q*(s, a) under the following conditions:\n\u2022 The learning rate \u03b1 satisfies  $$\\sum_{t=0}^{\\infty} \\alpha_t = \\infty$$ and $$\\sum_{t=0}^{\\infty} \\alpha_t^2 < \\infty$$.\n\u2022 Every state-action pair (s, a) is visited infinitely often.\n\u2022 The rewards are bounded.\n\u2022 The diversity score D of the selected demonstrations remains above a predefined threshold.\na) Proof: Under these conditions, the Q-values will converge to the optimal action-value function Q*(s, a) as t \u2192 \u221e. This is a consequence of the Bellman equation, which states:\n$$Q^*(s, a) = E[r + \\gamma \\max_{a'} \\max Q^*(s', a')|s, a]$$\nBy applying the law of large numbers, as the number of visits to each state-action pair approaches infinity, we can show that the average of the Q-value updates will converge to the expected value, thus proving the theorem."}, {"title": "2) Theorem 2 (Exploration-Exploitation Trade-off)", "content": "The \u03f5-greedy strategy ensures that every action is explored infinitely often, leading to convergence of the Q-values.\na) Proof: In the \u03f5-greedy strategy, with probability \u03f5, a random action is selected, and with probability 1-\u03f5, the action that maximizes the Q-value is chosen. The exploration ensures that all actions are sampled over time.\nLet N(s, a) be the number of times action a has been taken in state s. The probability of selecting action a at state s is given by:\n$$P(a|s) = \\begin{cases}\n\\epsilon/A + (1 - \\epsilon) \\cdot \\frac{1}{N(s)}, & \\text{if } N(s) > 0 \\\\\n1/A, & \\text{if } N(s) = 0\n\\end{cases}$$\nAs \u03f5 approaches 0, the probability of selecting the optimal action increases, but the exploration ensures that every action is still taken infinitely often. Thus, the Q-values converge to Q*(s, a) for all actions."}, {"title": "3) Lemma 1 (Diversity Score Bounds)", "content": "The diversity score D is bounded between 0 and 1.\na) Proof: The diversity score is defined as Equation 5.\n\u2022 The numerator, |\\{l_1, l_2, ..., l_m\\}|, can take values from 1 (all demonstrations have the same label) to k (all demonstrations have different labels).\n\u2022 The denominator, k, is always positive and finite.\nThus, we have: 0 \u2264 D \u2264 1. This property ensures that the diversity score can be effectively used to guide the selection process."}, {"title": "4) Theorem 3 (Reasoning Performance Improvement through Diversity)", "content": "Let f be a LLM that predicts a label \u0177 for an input x based on a set of reference demonstrations E. If E is a diverse set of demonstrations, then the expected accuracy Aacc of the LLM can be expressed as:\n$$A_{acc}(E) = E_{x \\sim P(x)} [L(f(x, E), \\hat{y})]$$\nwhere L is the loss function measuring prediction accuracy, and P(x) is the distribution of input texts.\na) Claim: If E contains a higher diversity of demonstrations, then:\n$$P(A_{acc}(E) \\geq A_{acc}(E')) \\text{ for all } E' \\subset E \\text{ with lower } D.$$\nThis means that increasing diversity enhances the probability of performance improvement.\nb) Proof: A diverse demonstrations E provides a broader range of contexts, which reduces bias and enhances the model's generalization capabilities. This diversity allows the model to capture a more accurate representation of the input space, thereby improving the expected accuracy Aacc during reasoning. According to the law of large numbers, as the diversity of E increases, the empirical distribution of the demonstrations converge to the true distribution of the input space, leading to minimized prediction error. Consequently, the inclusion of diverse demonstrations in E increases the probability of improved reasoning performance for the LLM."}, {"title": "5) Demonstration Selection", "content": "The demonstration selection process consists of two primary steps: calculating similarity and adjusting for diversity.\nFor a given input text x, the method computes the cosine similarity between the input and the reference demonstrations using Term Frequency-Inverse Document Frequency (TF-IDF) vectorization. The TF-IDF representation transforms the text data into a numerical format that reflects the importance of words in the context of the entire dataset. The cosine similarity S between two vectors Avec and Bvec is defined as:\n$$S(A_{vec}, B_{vec}) = \\frac{A_{vec} \\cdot B_{vec}}{||A_{vec}|| ||B_{vec}||}$$\nThe k most similar demonstrations are selected based on their cosine similarity scores, allowing the model to leverage the most relevant reference demonstrations for classification.\nTo ensure that the selected demonstrations provide a broad representation of the label space, we calculate a diversity score D for the selected demonstrations using the previously defined Equation for D. If the diversity score D falls below a predefined threshold, the algorithm enhances diversity by incorporating demonstrations that are less similar to the current text. This process begins by computing the cosine similarity between the current text vector x and each candidate example vector Ci for i\u2208 K. The least similar demonstrations are then identified based on these cosine similarity scores. The algorithm iteratively adds demonstrations from this pool of least similar candidates to the selected set E until the diversity score D meets or exceeds a predefined threshold. This systematic approach ensures that the selected demonstrations not only maintain relevance to the current input but also exhibit sufficient diversity."}, {"title": "D. Reinforcement Learning Mechanism", "content": "The selection process is guided by a reinforcement learning paradigm, characterized by the following components:\n1) State Representation: The state s is represented as a composite of the current input text x, the selected demonstrations E, the predicted label \u0177, and the diversity score D. This representation encapsulates the context of the decision-making process, allowing the model to consider both the input and the demonstrations it has previously selected. By integrating these elements, the state representation provides a comprehensive view of the current situation, facilitating informed decision-making.\n2) Action Selection: An action a corresponds to the selection of a specific demonstration from the knowledge base. To balance exploration and exploitation, an \u03f5-greedy strategy is employed. With probability \u03f5, a random action is selected to explore new possibilities, while with probability 1 - \u03f5, the action with the highest Q-value is chosen to exploit known information. This can be formalized as follows:\n$$a = \\begin{cases}\n\\text{random action} & \\text{with probability } \\epsilon \\\\\n\\arg \\max_{a'} Q(s, a') & \\text{with probability } 1 - \\epsilon\n\\end{cases}$$\nThis strategy ensures that the model does not become overly reliant on a limited set of demonstrations, thereby promoting a more robust learning process. By incorporating both exploration and exploitation, the model is better equipped to discover effective strategies and improve its performance over time."}, {"title": "E. Few-Shot Prompting with Language Models", "content": "To predict the label for the input text x, the proposed methodology utilizes a LLM through a specialized function designed to generate completions. This function constructs prompts that integrate the input text, selected demonstrations, and available label options, allowing the LLM to produce informed responses. Specifically, two prompting strategies are employed:\n\u2022 Standard Prompting: In this approach, a direct prompt is formulated to elicit a label based on the input text and the selected demonstrations. This strategy leverages the model's capacity to generalize from the provided demonstrations, enabling it to make accurate predictions. The effectiveness of this method hinges on the quality and relevance of the demonstrations included in the prompt, as well as the model's inherent ability to discern patterns and relationships within the data.\n\u2022 CoT Prompting: This strategy encourages the model to engage in a reasoning process before arriving at a label, thereby providing an explanation for its classification decision. CoT prompting can be formalized as follows:\n$$Label = f(x, E, \\text{explanation})$$\nwhere f represents the function that encapsulates the language model's reasoning process. This method aims to enhance both the interpretability and accuracy of the model's predictions by making the reasoning process explicit. By articulating the rationale behind its decisions, the model not only improves its predictive performance but also fosters user trust and understanding of the classification outcomes."}, {"title": "IV. EXPERIMENTS", "content": "A. Datasets\nIn our framework evaluation, we utilize four widely recognized datasets that encompass a diverse range of domains and intents. The BANKING77 dataset [32] provides a comprehensive set of intents specifically relevant to the banking sector. Additionally, the HWU64 [33] and LIU54 [33] datasets offer extensive multi-domain coverage, making them particularly valuable for comparative analysis. We also include the CLINC150 dataset [34], which further enriches our evaluation framework. To better align our evaluation with real-world application scenarios, we employed a challenge set sampling strategy, drawing on the principles outlined in [35]. This approach allowed us to select a demanding subset from the original test splits based on the precision margin, ensuring a rigorous assessment of our model's performance."}, {"title": "B. Compared Methods", "content": "In this study, we conduct a comprehensive evaluation of ten baseline approaches to assess their effectiveness in various classification tasks. These approaches are categorized into two main groups: Prompt Engineering Methods and Demonstration Selection Methods. Each method employs distinct strategies to enhance the model's performance, leveraging different aspects of prompting and demonstration selection to improve classification accuracy and response quality.\n1) Prompt Engineering Methods: This category focuses on techniques that manipulate the structure of prompts to guide the model's understanding and decision-making process. The methods included in this category are:\n\u2022 Zero-Shot Prompting (ZS) [5]: This method prompts the model to classify text without providing prior demonstrations, directly asking it to select the most appropriate label from a predefined set of options. This approach tests the model's ability to generalize from its training data.\n\u2022 Knowledge Prompting (KP) [36]: This technique prompts the model to generate relevant contextual information about the input text before selecting a label. By providing additional context, this method may enhance classification accuracy and improve the model's understanding of the task.\n\u2022 Least-to-Most Prompting (L2M) [37]: This approach structures the classification task into smaller, manageable steps, guiding the model through the selection process. By breaking down the task, the model can focus on each component, potentially leading to more accurate predictions.\n\u2022 Chain of Thought (CoT) [30]: This variant encourages the model to articulate its reasoning step-by-step before arriving at a classification. By making the reasoning process explicit, CoT prompting can enhance response quality and provide insights into the model's decision-making.\n\u2022 Self-Refine (SF) [38]: This approach prompts the model to solve a problem, critique its own solution, and then refine its answer based on the critique. This iterative process continues until a stopping condition is met, allowing the model to improve its responses through self-assessment.\n2) Demonstration Selection Methods: This category encompasses methods that utilize a selection of demonstrations to inform the model's predictions. The approaches included in this category are:\n\u2022 Few-Shot Prompting (FS) [35]: This approach provides the model with a limited number of text-label pairs selected randomly. By leveraging contextual information from these demonstrations, the model can enhance its predictions based on the provided demonstrations.\n\u2022 Few-Shot with CoT (FSC) [35]: This method combines few-shot learning with reasoning, presenting demonstrations alongside explanations to guide the model's classification process. This integration aims to improve the model's understanding and accuracy in making predictions.\n\u2022 Active Demonstration Selection (AES) [11]: This approach involves iteratively selecting and annotating unlabeled demonstrations to enhance in-context learning using reinforcement learning methods. By actively choosing relevant demonstrations, the model can improve its learning efficiency.\n\u2022 Representative Demonstration Selection (RDS) [28]: This method aims to identify a high-quality and diverse subset of in-context demonstrations that can effectively prompt various test instances for a specific task. By ensuring diversity, this approach enhances the model's ability to generalize across different scenarios.\n\u2022 Adaptive Demonstration Selection for In-Context Learning (ADAICL, ADA) [39]: This approach employs a model-adaptive, optimization-free algorithm to identify uncertain demonstrations and perform semantic diversity-based selection. By focusing on uncertain cases, the model can improve its robustness and adaptability."}, {"title": "C. LLMs Used in Experiments", "content": "To evaluate the effectiveness of the proposed method, we employ a diverse range of LLMs, encompassing both closed-source and open-source options. Closed-source models, such as GPT-3.5-turbo, Doubao-lite-4k, Doubao-pro-4k, and Hunyuan-lite, are proprietary systems developed by leading technology companies. For instance, GPT-3.5-turbo, created by OpenAI, is renowned for its advanced NLP capabilities, making it a popular choice for various applications, including conversational agents and content generation [40]. Similarly, Doubao-pro, released by ByteDance in May 2024, excels in multiple benchmarks, demonstrating strong performance in natural language understanding and generation tasks, positioning it as a versatile tool for applications ranging from question answering to complex text creation [41]. Hunyuan-lite, developed by Tencent, is distinguished by its extensive parameter count and advanced capabilities in handling long-context inputs, thereby enhancing its performance across diverse tasks [42].\nIn contrast, open-source models such as Gemma-2-2B, Gemma-2-9B, LLaMA-3.2-1B, LLaMA-3.2-3B, LLaMA-3-8B, Qwen-2.5-7B, Qwen-2.5-14B, and Qwen-1.5-72B offer researchers and developers the flexibility to modify and adapt the models for specific use cases. The Gemma series emphasizes efficient training techniques while maintaining high performance across various NLP tasks, encouraging customization and experimentation within the community [43]. The Qwen series, particularly noted for its scalability and adaptability, allows users to fine-tune models according to their needs, fostering collaboration and innovation in AI research [44], [45]. The LLaMA series has garnered attention for its performance across various benchmarks while enabling users to tailor the models to their specific requirements [46]. The open-source nature of these models promotes collaboration and innovation within the AI community, facilitating a broader range of experiments and applications compared to their closed-source counterparts."}, {"title": "D. Implementation Details", "content": "In this study, we developed a reinforcement learning-based framework for text classification that utilizes closed-source models accessed via API calls, as well as open-source models deployed using Ollama. The selection of reference demonstrations was guided by both their similarity and diversity, ensuring a robust representation of the input space. We configured the learning rate to 0.1 and the discount factor to 0.9, which are critical parameters for effective reinforcement learning. Each algorithm was iterated five times to compute average results, and a set of five demonstrations was employed as references in the baseline to enhance the model's reasoning capabilities. The Q-values were updated through a reward mechanism that assigned a reward of 1 for correct predictions based on the selected demonstrations. This approach facilitated the model's learning process by reinforcing successful actions and promoting effective decision-making. Overall, our framework demonstrates the potential of integrating reinforcement learning techniques with both closed-source and open-source models to improve text classification performance."}, {"title": "E. Reasoning Performance Analysis", "content": "This section presents a comprehensive evaluation of the reasoning accuracy of both closed-source and open-source LLMs using a range of prompt engineering and demonstration selection techniques across four benchmark datasets: BANK-ING77, CLINC150, HWU64, and LIU54. The evaluation focuses on popular closed-source models such as GPT-3.5-turbo, Doubao-lite-4k, Doubao-pro-4k, and Hunyuan-lite, as well as open-source alternatives including Gemma, LLaMA, and Qwen models. Each dataset is used to test the models' accuracy in understanding and classifying various domain-specific tasks. The results are presented in tables, with the top-performing techniques highlighted in bold and the second-best results underlined for clarity.\n1) Analysis of Reasoning Performance on Closed-Source Models: As illustrated in Table II, RDES/B and RDES/C consistently exhibit superior performance across the evaluated datasets when compared to alternative methodologies. Notably, RDES/C, which incorporates CoT reasoning, exhibits a remarkable advantage, achieving the highest accuracy scores in nearly all instances. In the context of prompt engineering methods, no single approach consistently dominated across all datasets. However, both KP and CoT reasoning frequently yielded strong results, particularly on the HWU64 dataset, where CoT achieved the highest average accuracy (0.513) among the prompt engineering techniques. KP also demonstrated effectiveness in several scenarios. These findings indicate that task-specific prompt engineering may play a crucial role in performance, although the advantages are often dataset-dependent. In terms of demonstration selection, ADA and FSC frequently produced competitive results, especially on the CLINC150 and HWU64 datasets. ADA, designed to adaptively curate demonstrations based on task-specific factors, exhibited flexibility by achieving solid results across datasets, particularly in contexts with nuanced distinctions. Nevertheless, it was generally outperformed by RDES/B and RDES/C, suggesting that while ADA is effective, the integration of CoT-based reasoning (as seen in RDES/C) provides additional benefits. Although FSC performed adequately, it lacked the consistency of more adaptive methods like ADA and RDES, particularly in more challenging scenarios.\nAmong the evaluated models, Doubao-pro-4k exhibited exceptional performance, particularly with the RDES/B and RDES/C methodologies. Notably, on the CLINC150 dataset, it achieved the highest performance score of 0.961 using the RDES/C approach. In contrast, Doubao-lite-4k consistently fell short, especially on more challenging datasets such as HWU64. This disparity underscores the importance of model capacity and architecture in effectively utilizing advanced prompt engineering and demonstration selection strategies. Additionally, GPT-3.5-turbo demonstrated relatively stable performance across various datasets, consistently securing high rankings when paired with advanced techniques like RDES/C. This suggests that while Doubao-pro-4k leads in specific scenarios, the robustness of GPT-3.5-turbo across diverse datasets highlights its versatility and reliability in practical applications. Overall, the findings emphasize that both the choice of model and the implementation of sophisticated methodologies are critical for optimizing performance in complex tasks.\nA focal point of this evaluation is the comparison between the two proposed methods, RDES/B and RDES/C. RDES/B serves as a baseline, while RDES/C incorporates an advanced enhancement through CoT reasoning. The addition of CoT reasoning in RDES/C consistently leads to superior performance across all datasets, underscoring the benefits of a more nuanced, context-aware approach. For instance, in the BANKING77 dataset, RDES/C achieves an average accuracy of 0.838, significantly surpassing traditional methods such as SF and ADA. This trend is consistently observed across other datasets, indicating that CoT reasoning equips LLMs with a deeper understanding of contextual relationships within the data, thereby facilitating more precise classification. Across the four benchmark datasets, RDES/C consistently delivers the highest accuracy scores. In the CLINC150 dataset, for example, RDES/C secures an average accuracy of 0.902, clearly outpacing other methods. A similar pattern is noted in the BANKING77, HWU64, and LIU54 datasets, where RDES/C continues to outperform alternative techniques, reinforcing its effectiveness. This dominance of RDES/C suggests that integrating traditional LLM methods with CoT reasoning can significantly enhance the performance of closed-source LLMs.\nIn summary, this evaluation highlights the substantial impact of advanced prompt engineering and demonstration selection techniques on the performance of closed-source LLMs. The results reveal that LLMs can greatly benefit from adaptive, context-aware prompting strategies. The consistent success of RDES/C underscores the potential of combining established methods with innovative enhancements such as CoT reasoning, thereby pushing the boundaries of classification accuracy across a wide range of applications. This insight is crucial for optimizing LLMs for domain-specific tasks and for guiding future research in prompt engineering and demonstration selection methodologies.\n2) Analysis of Reasoning Performance on Open-Source Models: Table III demonstrates a clear variance in performance across different datasets, highlighting the diverse challenges each dataset presents for open-source LLMs. In the BANKING77 dataset, our RDES/C approach consistently outperforms other methods, indicating its effectiveness in understanding fine-grained customer service intents. A similar trend is observed in the HWU64 dataset, where RDES/C demonstrates superior performance, suggesting its robustness across diverse user queries. The CLINC150 dataset, characterized by its technical nature, shows significant gains with our methods, particularly when using larger models like Qwen-1.5-72B, which suggests the importance of scale in handling domain-specific content. Lastly, in the LIU54 dataset, which involves more specialized queries, RDES/B and RDES/C provide considerable advantages, underscoring their capacity for nuanced reasoning in specialized domains.\nFrom a methodological perspective, the comparison highlights the strengths and weaknesses of various prompt engineering and demonstration selection techniques. ZS and KP show limitations in handling diverse datasets, especially compared to ADA and our RDES methods. Demonstration selection strategies like FS and FSC provide moderate improvements but are often outpaced by more sophisticated approaches like AES and ADA. Notably, our RDES methods, both in base and CoT-enhanced versions, consistently deliver superior results, validating the efficacy of CoT reasoning in enhancing model understanding, particularly for more complex tasks.\nA closer look at model-specific performance reveals distinct patterns tied to model architecture and scale. Smaller models, such as Gemma-2-2B and LLaMA-3.2-1B, generally show lower accuracy, particularly with simpler prompting strategies, suggesting limited capacity for nuanced comprehension. As model size increases, such as with Qwen-2.5-14B and Qwen-1.5-72B, performance markedly improves, especially when combined with advanced methods like RDES/C. This trend indicates that larger models, with more parameters, are better equipped to handle sophisticated reasoning tasks. Our RDES-enhanced approaches, particularly with CoT integration, benefit from larger architectures, showcasing the synergistic effect of scale and advanced reasoning techniques.\nThe overarching insights from the analysis highlight several key takeaways regarding open-source LLM performance. First, the RDES methods, particularly when combined with CoT reasoning, demonstrate a clear advantage across all datasets and models, suggesting that enhancing reasoning capabilities can bridge gaps in understanding and accuracy. The importance of model scale is also evident, as larger models consistently outperform smaller ones, reinforcing the necessity of computational resources in achieving higher accuracy. Moreover, the dataset-specific trends emphasize the significance of tailored approaches, as different datasets require distinct handling strategies to achieve optimal results. Overall, our findings underscore the potential of combining advanced prompting techniques with large-scale models to push the boundaries of open-source LLM capabilities.\nKey insights from this study emphasize that CoT reasoning and adaptive demonstration methods are crucial for enhancing the reasoning capabilities of open-source LLMs. As dataset complexity increases, the advantages of context-aware and reasoning-based techniques become more pronounced, making them indispensable for tasks that require a nuanced understanding. This underscores the need for future research to focus on refining adaptive and CoT-based methods, as they hold significant potential for improving model generalization across domains. The findings indicate a clear direction for advancing LLM architectures, suggesting that adaptive reasoning should be a core component of future developments to increase the versatility and applicability of LLMs in real-world scenarios."}, {"title": "F. Ablation Study"}]}