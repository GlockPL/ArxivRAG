{"title": "Egocentric Speaker Classification in Child-Adult Dyadic Interactions: From Sensing to Computational Modeling", "authors": ["Tiantian Feng", "Anfeng Xu", "Xuan Shi", "Somer Bishop", "Shrikanth Narayanan"], "abstract": "Autism spectrum disorder (ASD) is a neurodevelopmental condition characterized by challenges in social communication, repetitive behavior, and sensory processing. One important research area in ASD is evaluating children's behavioral changes over time during treatment. The standard protocol with this objective is BOSCC, which involves dyadic interactions between a child and clinicians performing a pre-defined set of activities. A fundamental aspect of understanding children's behavior in these interactions is automatic speech understanding, particularly identifying who speaks and when. Conventional approaches in this area heavily rely on speech samples recorded from a spectator perspective, and there is limited research on egocentric speech modeling. In this study, we design an experiment to perform speech sampling in BOSCC interviews from an egocentric perspective using wearable sensors and explore pre-training Ego4D speech samples to enhance child-adult speaker classification in dyadic interactions. Our findings highlight the potential of egocentric speech collection and pre-training to improve speaker classification accuracy.", "sections": [{"title": "I. INTRODUCTION", "content": "Autism spectrum disorder (ASD) [1], [2] is a neurocognitive developmental disorder that describes individuals with impairments in social communication and interactions along with repetitive and re-strictive behaviors and interests. Even though ASD may be diagnosed at any age, many critical signs and symptoms of ASD can be observed early in life. Specifically, recent studies report that 1 in 36 children in the USA are diagnosed with ASD [3]. Children with ASD may face challenges in acquiring language skills and communicating with others, which can increase the risk of developing mental disorders such as anxiety and depression. Although there is limited knowledge about the causes of ASD, recent studies show that early treatment can lead to substantial improvement in a child's development [4].\nEvaluating the behavior changes over time during treatment in-volves clinicians conducting child speech and behavior analysis. Language samples are typically acquired through established clinical protocols involving semi-structured dyadic interactions between a child and clinicians to evaluate their communication abilities and behaviors. One prominent protocol of this kind is Brief Observation of Social Communication Change (BOSCC) [5] for tracking changes in social and communicative skills during treatment. These clinical observations aim to elicit spontaneous responses from children under different circumstances to obtain the effectiveness of treatment.\nAutomatic understanding of speech samples involving child inter-actions in this clinically relevant context creates numerous opportuni-ties, including neurocognitive disorder assessment and treatment. One fundamental step of processing speech signals in these interactions requires a robust and precise computational method to classify child and adult labels of speech segments. The prior studies in [6], [7] have shown the importance of recognizing child and adult speaker labels to achieve automated spoken language assessment. Combined with the recent developments in speech foundation models [8], substantial progress has been made in the last few years in automatic child speech understanding [9]-[16], particularly in child-adult speaker classification [6], [17], [18].\nWhile these efforts have led to great promises, most existing works focus on using speech samples recorded from a \"spectator\" perspective, such as microphones placed in a stationary environment. As a result, these previous findings cover only a narrow spectrum of speech sensing and understanding involving child and adult dyadic in-teractions, such as far-field speech processing. Moreover, developing robust speaker classification models from the far-field speech setup with stationary mics is challenging. Instead, in this work, we focused on continuous speech data from the \"first-person\" perspective, where the recordings were tied to the hearing and uttering of the child or the adult interacting with the environment. Specifically, our study setup involving child-adult dyadic interactions is closely related to the massive-scale audio-visual data presented in the Ego4D [19]. Specifically, our contributions include:\n\u2022 We designed and conducted one of the earliest studies that collect egocentric speech signals using a wearable audio sensing device in child-adult interactions in a clinically relevant context.\n\u2022 Instead of relying on existing pre-trained models for child-adult speech classification, the proposed work integrates a pre-training stage using the egocentric speech data collected from the Ego4D.\n\u2022 We perform extensive fine-tuning experiments on the dataset col-lected from egocentric speech samples involving child-adult inter-actions following clinical observation protocol. Our experimental results demonstrate the effectiveness of pre-training for egocentric sensing in improving child-adult speech classification."}, {"title": "II. EGOCENTRIC AUDIO SENSING", "content": "What sensing technology should we use? In this study, we propose to collect egocentric speech data from child-adult dyadic interactions using wearable sensors. Audio sensing technologies have evolved significantly in the last two decades, transitioning from cumbersome phonographs to hand-held microphones and portable lapel mics. Many traditional systems, however, remain bulky, expensive, and lack scalability. Recent technological advances in wearable sensors have enabled the emergence of applications for capturing social commu-nications from an egocentric perspective in naturalistic settings [20]- [22]. For example, by using the miniature mics on these devices, a digital recorder [21], [23]-[25] can overcome some of the drawbacks of the traditional systems, capturing day-long audio data centered around a person. In this work, we prototype the experiments following a recently proposed wearable audio-sensing solution called TILES Audio Recorder (TAR) [21]. We would highlight that this wearable solution applies to both controlled and naturalistic settings."}, {"title": "How to design egocentric audio sensing experiments using TAR?", "content": "The primary motivation for us to adopt TAR is its design and optimization for Android platforms, open-source nature, and com-patibility with various form factors, including small, lightweight, and budget-friendly smartphones like the Jelly and Atom \u00b9. We modify the original functionality in TAR to collect raw audio rather than low-level acoustic descriptors during child-adult dyadic interactions. Specifically, these child-adult interactions are conducted following the BOSCC protocol. In the experiment, both the child and the examiner wore TAR devices throughout the sessions. The child used the lightweight Jelly version for comfort, while the examiner wore the Atom, as shown in Figure 1. It is worth noting that we observed a better audio recording quality using Atom sensors than Jelly sensors.\nData Collection The data collection is a part of the extension to the study described in [26]. Children who participated in this study were instructed to wear the Jelly version of the TAR during their visit to conduct BOSCC interviews. The data in this work includes ten unique BOSCC sessions from 10 children between the ages of 2-7 years old. Particularly, among all children, six are male. Given that this study is an early effort to pilot egocentric audio sensing involving child-adult interactions, this cohort of data collection includes only three children with ASD."}, {"title": "III. EGOCENTRIC CHILD-ADULT SPEAKER CLASSIFICATION", "content": "Previous works have reported promising child-adult speaker clas-sification results leveraging the recently developed speech foundation models. However, most existing speech foundation models are trained with speech recordings in a non-egocentric perspective. To improve the ability of these pre-trained speech models for egocentric speech understanding, we decided to perform a pre-training stage on a large-scale egocentric speech dataset called Ego4D."}, {"title": "Ego4D Dataset", "content": "The Ego4D dataset is a popular dataset that has gained substantial interest in recent years. This multimodal dataset contains egocentric audio, videos, and IMUs from diverse geographic coverage, environments, and populations. The dataset includes over 3,500 hours of video data from 932 unique participants in 9 countries."}, {"title": "Voice Activity Detection", "content": "As most audio data from the Ego4D dataset are unlabeled continuous naturalistic recordings, there exists a substantial amount of silence or environmental noises in these data. Therefore, we decide to preprocess the audio to obtain speech-only segments for the pre-training. Specifically, we leverage the voice activity detection model from pyannote [27] to filter out speech segments from long, continuous egocentric audio recordings."}, {"title": "Wav2vec 2.0 Pre-training", "content": "In this work, we primarily investigate the wav2vec 2.0 pre-training [28] on Ego4D speech samples. As shown in Figure 2, the wav2vec 2.0 model consists of convolutional encoders followed by transformers. The speech data X is first mapped to latent feature Z, and the transformers model Z to the contextualized representation C. Moreover, the latent feature Z is discretized to Q with a quantization module to form the contrastive objective. It is worth noting that the quantization module leverages the Gumbel Softmax operation to generate Q. The primary objective in wav2vec 2.0 pre-training is the contrastive object to distinguish the true quantized latent speech and its associated contextual representations after masking. More details regarding wav2vec 2.0 pre-training can be found in [28]. We want to mention that we initialized the model weights with the wav2vec 2.0-base model weights before further pre-training with speech samples filtered from the Ego4D dataset."}, {"title": "B. Speaker Classification Modeling", "content": "Once we complete the pre-training on Ego4D data, we perform the fine-tuning experiments following the fine-tuning model architecture from the previous work [10]. Specifically, we apply a linear layer to combine hidden outputs from all layers. The weighted output is then processed through a set of 1D convolutional layers with kernel sizes of 1, an average pooling layer, and classification layers of MLPs. We apply the ReLu activation functions within convolutional and MLP layers. The model architecture is demonstrated in Figure 3."}, {"title": "IV. EXPERIMENTAL DETAILS", "content": "As our paper focuses on classifying child and adult speech, we conducted manual annotations using four labels: child, adult, 3rd parties, and overlap. Due to IRB restrictions, the annotations were carried out by researchers within the same research group who re-ceived proper research training certificates. In particular, background noise was excluded from the annotations. Third-party speakers could include other family members, such as siblings, another parent, or toddlers. Our modeling experiments focus exclusively on speech samples labeled as child or adult. In total, our annotation includes 4,240 unique speech samples across 10 BOSCC sessions, with 2,709 examiner (adult) speech samples and 1,531 child speech samples."}, {"title": "B. Modeling Details", "content": "Ego4D Pre-training: The model weights of the pyannote VAD model were obtained from Huggingface for processing the Ego4D audio data. Since the child and adult dyadic interactions in our experiments were conducted using American English, we decided to filter out Ego4D audio data with the recording location within the United States. Moreover, those audios with a recording duration of less than 5 minutes were also excluded from pre-training. After VAD, there were approximately 75,000 speech utterances for pre-training. We also initialize the pre-training model with the wav2vec 2.0 weights from Huggingface\u00b2. Our pre-training code was adopted from the github\u00b3. Specifically, we choose a masking ratio of 50%, a learning rate of 0.0001, and a total training epoch of 40."}, {"title": "Speaker Classification", "content": "We apply a learning rate in {0.0002, 0.0005} for speaker classification training, with the CNN layers having an output channel size of 256. We perform the classification fine-tuning using two unique backbones: the wav2vec 2.0 base (W2V) and the Ego4D wav2vec 2.0 base (Ego4D W2V). We perform 5-fold cross-validation with two unique BOSSC sessions as a test fold and report the average performance across all folds. The reported metrics include accuracy, macro-F1, recall, and specificity. Here, adult and child labels are 0 and 1, respectively."}, {"title": "V. RESULTS", "content": "We begin by exploring whether pre-training the backbone model using egocentric speech samples improves child-adult speaker classi-fication in an egocentric recording setup, as shown in Table I. In this experiment, the training data includes mono-channel speech samples from devices worn by both the child and the examiner. Compared with results reported in [10] (Macro F1 = 0.798), sampling audio with an egocentric setup helps to improve speaker classification. When tested on speech samples recorded from different sources (child or examiner), the findings suggest that pre-training the backbone model with egocentric speech samples consistently improves child-adult speaker classification. Specifically, we notice that the performance gains mainly originate from increased specificity, implying better adult speech classification. Given that most Ego4D speech samples come from adults, this highlights the benefit of pre-training on adult egocentric speech. It also points to the need for more egocentric speech samples from children to improve child speaker classification."}, {"title": "B. Impact of Training Recording Resource", "content": "We then compare the performances using a single source of recordings for training as shown in Table II. Specifically, we train the speech samples from either children or the examiner and test the models with the same or different recording sources."}, {"title": "Train/Test with the same recording source (Child/Child and Examiner/Examiner)", "content": "Similar to the results presented earlier, we find that pre-training with the egocentric speech samples can consistently improve the child-adult speaker classification regardless of the source of training speech samples. Moreover, we note a higher child-adult classification performance when using the recording source from the examiner, confirming the better audio quality from the device worn by the examiner. We also observe that performance gains mainly come from improved adult speech classification."}, {"title": "Train/Test with different recording sources (Child/Examiner and Examiner/Child)", "content": "Even when training and testing speech samples with varying sources of recording, we find that pre-training backbone models with egocentric speech samples still help child-adult speaker classification in these scenarios. However, we notice a consistent improvement in recall scores (child speech classification) when training and testing are used with speech samples from different recording sources instead of improved specificity."}, {"title": "C. Training Data Efficiency", "content": "Apart from leveraging complete annotated data for fine-tuning, we explore the efficiency of training data in child-adult speaker classification. Precisely, we compare two backbones in fine-tuning the child-adult speaker classification using 10%, 50%, 100% of training samples. Results in Figure 5 demonstrate that Ego4D W2V consistently outperforms W2V across all training data ratios. These findings further support the effectiveness of integrating pre-training stages on egocentric speech samples to improve child-adult speaker classification sampled in the egocentric setup."}, {"title": "D. Generalizability to PEFT", "content": "In addition to fine-tuning with the pre-trained speech embeddings, we aim to investigate whether the backbone models can be adapted to other training approaches, such as parameter-efficient fine-tuning (PEFT) [29]. \u03a4o investigate this, we experimented with one popular PEFT approach, LoRA (Low-rank Adaptation) [30]. The primary idea behind LoRA is to fine-tune the low-rank matrices to approximate the model updates associated with linear layers. We implement FF- LORA (feedforward layers) and QV-LORA (query and value matrices) in this experiment. The PEFT results in Figure 6 show that LoRA consistently improves child-adult classification for Ego4D W2V backbones. At the same time, there is a decrease in performance using LoRA in W2V backbones with the examiner device recording as training and testing data. We also observe that FF-LORA performs better than QV-LORA in our experiment."}, {"title": "E. Dual Channel Modeling", "content": "Apart from classifying the child-adult speaker with the mono-channel input, we experimented with a concatenated input that combines speech samples from both devices. We would highlight that this modeling approach is similar to the dual-channel method proposed in the recent work [15]. Overall, the comparisons in Table III demonstrate that input with dual channels further improves child-adult speaker classification compared to the best-performing mono-channel approach. Compared with the previous work with conventional recording setup [10], dual-channel egocentric speech with Ego4D W2V backbone yields a 0.08 improvement in macro F1. Moreover, we can also observe a substantial performance increase in specificity using the Ego4D W2V backbone. This further implies the improved adult speaker classification that is associated with pre-training egocentric adult speech samples from the Ego4D dataset."}, {"title": "VI. CONCLUSION AND FUTURE WORKS", "content": "In this work, we explore the use of wearable audio-sensing solu-tions to sample egocentric speech samples from child-adult interac-tions in clinical observations. We design the egocentric audio sensing experiments leveraging a previous wearable audio sensor successfully deployed in large-scale longitudinal studies in naturalistic settings. Moreover, we curate the data by annotating the speaker labels based on the prior work in [10]. On the other hand, we propose a modeling framework that begins with pre-training the backbone model with speech samples from the Ego4D dataset. Our extensive experiments show that speech samples collected in the egocentric setup can help pre-trained speech models to better classify child and adult speech. Moreover, pre-training the backbone model with egocentric speech samples can further improve the child-adult speaker classification."}]}