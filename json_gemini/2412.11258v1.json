{"title": "GaussianProperty: Integrating Physical Properties to 3D Gaussians with LMMs", "authors": ["Xinli Xu", "Wenhang Ge", "Dicong Qiu", "ZhiFei Chen", "Dongyu Yan", "Zhuoyun LIU", "Haoyu Zhao", "Hanfeng Zhao", "Shunsi Zhang", "Junwei Liang", "Ying-Cong Chen", "Quwan"], "abstract": "Estimating physical properties for visual data is a crucial task in computer vision, graphics, and robotics, underpinning applications such as augmented reality, physical simulation, and robotic grasping. However, this area remains under-explored due to the inherent ambiguities in physical property estimation. To address these challenges, we introduce GaussianProperty, a training-free framework that assigns physical properties of materials to 3D Gaussians. Specifically, we integrate the segmentation capability of SAM with the recognition capability of GPT-4V(ision) to formulate a global-local physical property reasoning module for 2D images. Then we project the physical properties from multi-view 2D images to 3D Gaussians using a voting strategy. We demonstrate that 3D Gaussians with physical property annotations enable applications in physics-based dynamic simulation and robotic grasping. For physics-based dynamic simulation, we leverage the Material Point Method (MPM) for realistic dynamic simulation. For robot grasping, we develop a grasping force prediction strategy that estimates a safe force range required for object grasping based on the estimated physical properties. Extensive experiments on material segmentation, physics-based dynamic simulation, and robotic grasping validate the effectiveness of our proposed method, highlighting its crucial role in understanding physical properties from visual data. Online demo, code, more cases and annotated datasets are available on the project page: https://Gaussian-Property.github.io", "sections": [{"title": "1. Introduction", "content": "Estimating physical properties from visual data is a critical task in both computer vision and graphics, serving as the foundation for various fields, including augmented reality (AR) [2, 4, 15], robotic grasping [5, 7, 34], and physics-based dynamic simulation [8, 12, 14]. Recently, the integration of physical properties into 3D model has generated significant interest across these domains, underscoring the need for precise physical property estimation. However, this area remains under-explored due to the inherent ambiguities in physical property estimation. Key challenges include the difficulty of acquiring labeled ground-truth data, as intrinsic physical properties are not directly observable through visual means, and the ambiguity of the prediction task, which is further compounded by the limited number of observable surfaces.\n\nHumans possess a remarkable ability to predict the physical properties of objects based on visual cues alone [26]. Research in cognitive science and human vision suggests that this capability stems from our skill in associating visual appearances with previously encountered materials, about which we have developed a rich and grounded understanding. This process allows us to intuitively gauge physical property such as weight, texture, and density from visual observation. Recently, Large Language Models (LLMs) have achieved impressive progress in nature language understanding. Based on this, Large Multimodal Models (LMMs) extend LLMs by further incorporating im-"}, {"title": "2. Related Work", "content": "In the burgeoning field of 3D modeling, the accurate estimation of physical properties such as density, elasticity, and thermal conductivity is a long-standing problem [1, 41], serving critical roles in downstream tasks like AR, robotics, and physical-based simulation. Although promising, existing work mostly tackles specific types of material properties, e.g. mass or tenderness, by collecting corresponding task-dependent data with little generalization. In contrast, our method can generate diverse physical properties like mass density, friction, and hardness in a zero-shot manner with the recognition capability of LLMs. Several works have explored LLMs for physical property estimation. For example, NeRF2Physics [46] leverages large language models to propose candidate materials for objects, constructing a language-embedded point cloud to estimate physical properties such as mass, friction, and hardness through a zero-shot kernel regression approach. Make-it-real [11] reasons the PBR materials including albedo, metallic, and roughness for 3D assets texture generation."}, {"title": "2.2. Multimodal Large Language Models.", "content": "Large Language Models (LLMs) have achieved impressive progress in recent years, demonstrating a strong capability in understanding natural language. However, they generally lack the ability to reason about images, as they lack image priors for training. With the growing demand for this capability, recent research has focused on developing Large Multimodal Models (LMMs) that integrate image modalities for training. The state-of-the-art models [6, 22, 28, 37, 38] have been leveraged in various downstream applications, such as image captioning [25], physically based rendering (PBR) materials estimation [11], and 3D grounding [39]. LMMs have shown great potential for these tasks, significantly improving performance. The introduction of GPT-4V [28] has notably advanced the capabilities of large multimodal models, showcasing exceptional 2D comprehension and extensive open-world knowledge. While GPT-4V is not designed to process 3D data directly, the innovative GPTEval3D [42] has successfully utilized GPT-4V to assess the quality of 3D objects, finding that its evaluations closely match those of humans. Addi-"}, {"title": "2.3. Dynamic Rendering", "content": "Neural Radiance Fields (NeRF) [27] have garnered significant interest in recent years due to their remarkable capabilities in multi-view 3D reconstruction. An evolutionary advancement within the NeRF framework is the incorporation of a temporal dimension, enhancing the representation of dynamic scenes. For instance, D-NeRF [30] and NeRFies [29] have extended time-dependent neural fields by decomposing them into an inverse displacement field and canonical time-invariant neural fields. Furthermore, 3D Gaussian splatting [17], a point-based rendering technique, has gained popularity for its highly realistic rendering quality and efficient training speed. Building on this, Dynamic 3D Gaussians [24, 40] have successfully integrated the temporal dimension to more effectively represent dynamic scenes. However, existing methods for dynamic rendering typically rely on video sequences for supervision, where the 3D models are deformed to align consistently with the video footage. In this study, since we assign the physical properties for 3D Gaussians, we assist dynamic simulations seamlessly integrate the simulation within the GS framework."}, {"title": "2.4. Material-sensitive Robot Grasping", "content": "Soft robotic grippers [21, 36, 45] leverage the deformation and compliance properties of soft materials enabling grippers to automatically adapt to the geometries and various weights of the objects being grasped. This adaptability necessitates the careful selection of materials and mechanical designs tailored to specific applications, limiting the generality of such solutions across all scenarios. Optical tactile sensing approaches [16, 19, 23] requires a camera positioned within each fingertip of a gripper, situated behind a soft and transparent artificial skin, to convert optical observations of markers printed on the skin to force estimations; while electronic skins [33, 35] detects exerted forces from electric signals. However, these two approaches often face challenges related to durability, and some require significant additional installation space, limiting their practicality in certain applications. In this work, we propose integrating GaussianProperty to enable material-sensitive robot grasping, which takes merely the visual inputs from a camera to predict the composing materials and estimate corresponding physical properties of the object to grasp. Our approach can be easily adapted to a wide spectrum of robotic and industrial applications."}, {"title": "3. Method", "content": "Given a well-reconstructed 3D Gaussian representation, our objective is to attribute physical properties to each Gaussian. The specific physical property can vary according to the downstream task. In this work, we demonstrate a potential application in physics-based dynamic simulation via Material Point Method (MPM) and robotic grasping. The former application requires material density $ \\rho $, Young's modulus $E$, Poisson's ratio $P$, and material type $T$. And robotic grasping requires the material density $ \\rho $, volume $V$, friction coefficient $ \\mu $, thickness $d$, maximal tolerable curvature $ \\kappa $, Young's modulus $E$. An overview of our framework is illustrated in Figure 2."}, {"title": "3.2. Part-Level Segmentation", "content": "Understanding an object's physical properties requires delving into the characteristics of its individual parts, as each part may present unique attributes. Considering this, we utilize SAM for image segmentation, adeptly predicts masks with precise boundaries that capture whole, part, and sub-part levels, thereby reflecting the object's hierarchical semantic structure. In this work, we emphasize the significance of part-level information, which enables us to dissect an object into its constituent parts. This approach facilitates a more accurate and exhaustive comprehension of the physical properties of visual data. Our method not only harnesses the semantic stratification provided by SAM but also actively integrates it to remedy the ambiguity arising from objects possessing multiple physical attributes.\n\nConcretely, for each image $I$ within the observed set $I_N$ we input a grid of 32 \u00d7 32 point prompts. SAM responds by segmenting precise masks at varying levels based on the prompts at these points. We operate using the part-level semantic mask $M$, subsequently refining the segmentation by eliminating superfluous masks within each of the three mask sets. This culling is informed by predicted intersection-over-union (IoU) scores, stability scores, and the overlap rates between masks. The resulting segmentation maps meticulously trace the boundaries of objects at their respective hierarchical levels, effectively segmenting the scene into semantically coherent regions."}, {"title": "3.3. Physics Property Matching", "content": "After achieving precise part-level semantic segmentation, the next step is to match the segmented parts with their corresponding physical properties, a process we term Physics Property Matching. We discussed the establishment of material candidates in Section 3.3.1 and utilizing a combination of global and local knowledge in Section 3.3.2 to assist GPT-4V in recognizing the material properties of the object. Additionally, we discuss the Gradual Prompt Guidance in-"}, {"title": "3.3.1. Material Candidates", "content": "Our approach leverages a curated collection of candidate materials, consisting of fifteen ubiquitous material families and more than 600 materials, integral to everyday objects and structures. This library encompasses a wide range of materials, ensuring comprehensive coverage of various densities and material properties. The common object material library includes density ranges for a variety of materials. For instance, metals such as aluminum (2700 kg/m\u00b3), steel (7750-8050 kg/m\u00b3), and copper (8920-8960 kg/m\u00b3) are covered, as well as non-metals like glass (2200-2500 kg/m\u00b3), concrete (2300-2500 kg/m\u00b3), and plastics such as polyethylene (930-970 kg/m\u00b3). This diversity highlights the extensive range of physical properties found in commonly used substances.\n\nThis robust material database is the cornerstone of our physical property matching process. By offering a comprehensive material library, the material candidates simplify material retrieval for the LLM model. Additionally, it minimizes ambiguity in property predictions from different perspectives, ensuring accuracy. Reliable material identification thus provides a dependable reference."}, {"title": "3.3.2. Combined Global-Local Reasoning Module", "content": "Our observation revealed that utilizing a global-to-local knowledge framework significantly improves the accuracy in assigning physical properties to each part. A straightforward method involves having the model understand the entire object first and then evaluate a part of the object. However, we found it challenging for the model to establish a connection between the whole and its parts, as shown in Figure 3 (Left). Motivated by this insight, we built a bridge between global and local information, enabling the model to understand their connection. As shown in Figure 3 (Right), the left image displays the original object, the middle image shows a partial segmentation with the mask highlighted in red, and the right image depicts a specific part of the object. Starting from this global perspective, GPT-4V then focuses on the details of each part, incorporating local cues such as texture, color, and contextual information from adjacent parts. This approach aids in accurately identifying each part and inferring its material composition."}, {"title": "3.3.3. Gradual Prompt Guidance", "content": "We design gradual prompt guidance to help the LMMs gradually build an understanding of the entire object and then discern the association between its parts and the whole through the segment map. The prompt instructs the LLM to first briefly describe the part based on the provided image and then identify the material of the part, specifying its mass density, Young's modulus, and Poisson's Ratio. The ma-"}, {"title": "3.4. Lift 2D to 3D via Voting", "content": "3D Gaussian Splatting method has the advantage of providing an explicit 3D representation, making it easy to add any other properties. This method reparameterizes NeRF with a set of unstructured 3D Gaussian kernels {$X_p, \\sigma_p, A_p, C_p$}$_{p\\in P}$, where $X_p, \\sigma_p, A_p,$ and $C_p$ denote the centers, opacities, covariance matrices, and spherical harmonic coefficients of the Gaussians, respectively. A differentiable rasterization rendering method is employed to project 3D Gaussians to 2D images to compare the rendered image with ground-truth image by\n\n$C = \\sum_{k\\in P} \\alpha_kSH(d_k; C_k) \\prod_{j=1}^{k-1} (1 - \\alpha_j)$,\n\nwhere $ \\alpha_k $ are the z-depth ordered opacity, and $d_k$ is the view direction from the camera to $x_k$."}, {"title": "3.4.2. Frequency-based Voting Strategy", "content": "Through reconstruction, we obtain 3D Gaussians denoted as GS. Previous works [31, 47] incorporate CLIP features into 3D Gaussians through training, but the process is time-consuming and scene-specific, limiting downstream applications. Alternatively, we lift the 2D information to 3D models with a projection based method. Each 3D Gaussian $s \\in GS$ is projects to each 2D image $I \\in I_N$, we determine the pixel coordinates $(u, v)$ on 2D plane using the camera parameters. The projection is performed as\n\n$u, v = K \\cdot [R|t] \\cdot s$, (1)\n\nwhere K is the camera intrinsic matrix, [R|t] represents the rotation and translation matrices (extrinsic parameters), and s is the coordinates of the point.\n\nHowever, the projected pixel coordinates are meaningless if the point is invisible in the source image. Thus, We estimate the visibility using the Gaussian-estimated depth to determine if the point is visible of the image. The voting strategy involves projecting each Gaussian to all the visible views and retrieving the corresponding properties. To ensure consistency across multi-view images, we adopt a frequency-based voting strategy. The attribute with the highest frequency is chosen as the final predicted attribute. The voting process can be described as:\n\n$\\hat{a} = arg\\underset{a}{max} \\sum_{i=1}^{N} I(\\alpha_i = a)$,\n\nwhere a is the predicted attribute, N is the number of views, $a_i$ is the attribute observed in the i-th view, and $I$ is the indicator function that equals 1 if the attribute matches and 0 otherwise."}, {"title": "3.5. Material-sensitive Robot Grasping", "content": "The diversity of objects in the real world, composed of various materials and physical properties, makes it impractical to use a single grasping force for all. An adaptive strategy is essential to calibrate the grasping force according to the specific materials of the object being manipulated. The grasp-"}, {"title": "3.6. Physics-based Dynamic Simulation", "content": "Previous works, such as PhysGaussian [43], have achieved dynamic simulation by integrating Newtonian physics directly into 3D Gaussian representations, using the Material Point Method (MPM) to enable realistic physical interactions. MPM combines the strengths of both particle simulation methods and grid-based finite element methods (FEM) to effectively handle complex problems involving large deformations, phase changes, and interactions between multiple materials. However, a key limitation in these approaches is the need for manual assignment of physical properties to each Gaussian point, such as material type and physical properties corresponding to the material. This manual assignment is time-consuming and not realistic.\n\nTo address this inefficiency, our method can directly predicts the physical properties of each Gaussian point, thus eliminating the need for manual assignment. Specifically, we employ a combination of multi-view 2D-to-3D projection and frequency-based voting to derive these properties from observed images. For each Gaussian point in the 3D representation, our model predicts essential physical attributes, including density ($\\rho $), Young's modulus (E), Poisson's ratio (P, among others. This prediction process begins with segmenting observed images at the part level to ensure each segment's unique physical characteristics are represented accurately. We then apply a voting strategy to integrate physical properties across multiple views, ensuring consistency and robustness in the 3D representation. By automating the assignment of these properties through GaussianProperty, we significantly reduce the time required for dynamic simulations, streamline the simulation workflow, and enable scalable applications in complex environments. We show some cases in Figure 5."}, {"title": "4. Experiments", "content": "Datasets. We evaluated the quantitative and qualitative performance using both synthetic and real-captured data from the Amazon Berkeley Objects (ABO) dataset [9] and the MVImgNet dataset [44]. Following [46], we selected 100 validation objects from the ABO dataset. For MVImgNet, we also selected 100 objects. The criterion for selection was to ensure coverage of a diverse range of material categories, and we filtered out cases that could not be accurately classified. Finally, we manually annotated detailed material labels for each part of the objects. This process resulted in a final set of 78 labeled cases in the ABO dataset and 100 cases in MVImgNet. Moreover, we also captured 16 objects composed of various materials for robotic grasping. Further details can be found in the Supplementary.\n\nEvaluation protocol. To evaluate the accuracy of material prediction after adding physical properties to 3D Gaussians, we use the mean Intersection over Union (mIoU) metric [10]. This process involves selecting an angle from which the object can be better observed. The 3D Gaussians render the material information into 2D to form a material segmentation map. Similar to 2D evaluations, we use mIoU as an indicator to assess the accuracy of the material segmentation. For robotic grasping, the Picked-up Rate (PUR) and the No-damage Rate (NDR) evaluate respectively whether objects are picked up without slipping and whether no damages to objects are caused. A final success requires both criteria being met, yielding a final Success Rate (SR)."}, {"title": "4.2. Implementation Details", "content": "For each object, we collected 30 views with camera centers randomly distributed over a hemisphere around the object. We used 3D Gaussian Splatting for 3D reconstruction, following the default parameter settings. Our model was trained for 5 minutes on a single NVIDIA RTX-A6000 GPU. To accelerate the part-level segmentation and property matching process, we selected only 10 views. For multi-modal model processing, we used GPT-4V as the large multimodal model. For dynamic simulation, we implemented Physgaussian [43] with assigning estimated materials for each 3D Gaussian. In robot grasping experiments, we utilized a Jacobi.ai JSR-1 robot platform [32] equipped with a TEK CTAG2F90-C robotic gripper that has a maximum grasping force up to 40N. The force-bearing surface at the tip of the gripper is measured to encompass an area of $A = 0.00011m^2$. And a maximum allowable bending curvature $ \\kappa_{max} = 0.5 $ is used. The robotic gripper's grasping force has been calibrated with its normalized input $ 15 \\leq NGF < 100 $ before experiment."}, {"title": "4.3. Material Segmentation.", "content": "We compared material segmentation performance with the recent work Nerf2Physics [46], we present both qualitative and quantitative comparisons in Figure 4 and Table 1. Our method significantly outperforms Nerf2physcis on both synthetic and real-captured data. We also conducted mass and hardness estimation as Nerf2Physics. More results can be found in the Supplementary."}, {"title": "4.4. Generative Dynamics", "content": "Physical simulation is a crucial application of our method because it allows us to directly add all predicted physical properties to the Gaussian points without the need for manu-"}, {"title": "4.5. Robot Grasping", "content": "To evaluate the effectiveness and performance of our proposed method, we collect 16 objects composed of diverse materials, and implemented three robot grasping baselines with fixed grasping forces, which are widely adopted force-sensitive grasping strategies in robotics. Table 3 shows our method on material-sensitive grasping with GaussianProperty outperforms all the baselines. Several sample cases are shown in Figure 6. Full object list and experiment results can be found in the Supplementary."}, {"title": "4.6. Ablation Study", "content": "Global-to-Local Knowledge Utilization. Table 2 demonstrates the impact of incorporating global-to-local knowledge in material segmentation. Without this module, the method only utilizes images of each individual local part of the object for material querying. In contrast, with global-to-local knowledge, the method benefits from a broader con-"}, {"title": "5. Conclusion and Limitation", "content": "Despite the promising result of our method on 2D material segmentation, our method struggles to distinguish surface with ambiguous materials. We show an example in the Supplementary.\n\nConclusion In this paper, we explore the issue of estimating physical properties for 3D models, a topic that serves as a foundation for various downstream task like AR, robotics and simulation, yet remains under-explored. The inherent ambiguity and the challenge of acquiring labeled ground-truth data can significantly hinder the estimation. Our method, GaussianProperty, effectively addresses this challenge by leveraging the recognition capability of large multimodality models and segmentation capability of SAM to achieve a combined global-local reasoning module on 2D space. Then, a voting strategy is employed to project the 2D material property estimation results to 3D Gaussians, a effective and efficient 3D representation, supporting multi-"}]}