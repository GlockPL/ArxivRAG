{"title": "Synthetic Thermal and RGB Videos for Automatic Pain Assessment utilizing a Vision-MLP Architecture", "authors": ["Stefanos Gkikas", "Manolis Tsiknakis"], "abstract": "Pain assessment is essential in developing optimal pain management protocols to alleviate suffering and prevent functional decline in patients. Consequently, reliable and accurate automatic pain assessment systems are essential for continuous and effective patient monitoring. This study presents synthetic thermal videos generated by Generative Adversarial Networks integrated into the pain recognition pipeline and evaluates their efficacy. A framework consisting of a Vision-MLP and a Transformer-based module is utilized, employing RGB and synthetic thermal videos in unimodal and multimodal settings. Experiments conducted on facial videos from the BioVid database demonstrate the effectiveness of synthetic thermal videos and underline the potential advantages of it.", "sections": [{"title": "I. INTRODUCTION", "content": "Pain, as defined by the International Association for the Study of Pain (IASP), is \u201can unpleasant sensory and emotional experience associated with actual or potential tissue damage, or described in terms of such damage\" [1]. Additionally, Williams and Craig [2] state that pain encompasses emotional, cognitive, and social dimensions beyond physical aspects. Biologically, pain is a distasteful sensation that originates in the peripheral nervous system. It serves a vital function by activating sensory neurons, alerting the body to potential injury, and playing a critical role in recognizing and reacting to hazards. [3]. Pain is a significant concern impacting individuals and social structures. Daily, people across all age groups suffer from pain resulting from accidents, illnesses, or as a part of medical treatment, making it the most common cause for seeking medical consultation. Both acute and chronic pain present clinical, economic, and social challenges. In addition to its immediate impact on a patient's life, pain is related to several adverse outcomes, including opioid consumption, drug misuse, addiction, deteriorating social relationships, and mental health issues [4]. Effective pain assessment is crucial for early diagnosis, disease advancement monitoring, and treatment effectiveness, particularly in chronic pain management [5]. Consequently, pain is referred to as \"the fifth vital sign\" in nursing literature [6]. Objective measurement of pain is vital for providing appropriate care, especially for groups who cannot express their pain, such as infants, young children, people with mental health issues, and seniors. Numerous methods are utilized for pain assessment, including self-reporting, which remains the gold standard for determining pain presence and intensity through rating scales and questionnaires. Additionally, behavioral indicators such as facial expressions, vocalizations, and body movements are essential for evaluating pain. [7]. Furthermore, physiological indicators such as electrocardiography, electromyography, skin conductance, and breathing rates provide significant insights into pain's physical effects [5]. Although pain evaluation holds significant importance, it poses a substantial challenge to medical practitioners [8], particularly with patients who cannot communicate verbally. In senior patients, the situation becomes even more complex due to their decreased expressiveness or reluctance to share their pain experiences [9]. Furthermore, extensive studies [10]\u2013[12] reveal distinct disparities in pain expression among various genders and age categories, underlining the intricacy of the pain assessment process.\nIn recent years, there has been a growing tendency in the field of affective computing research to incorporate thermal imaging techniques [13]. The interest was sparked following findings in the literature that stress and cognitive load significantly impact skin temperature [14]. This is due to the role of the autonomic nervous system (ANS) in controlling physiological signals like heart rate, respiration rate, blood perfusion, and body temperature, which are indicative of human emotions and affects [13]. Additionally, muscle contractions influence facial temperature by transferring heat to the facial skin [15]. Therefore, thermal imaging is a promising method for measuring transient facial temperatures [16]. The authors in [17] examined thermal imaging and facial action units to assess emotions, including frustration, boredom, and enjoyment. The multimodal approach demonstrated the highest accuracy. Thermal imaging has been explored in a relatively small number of studies within the field of pain research. In [18], the authors observed that facial temperature rises following a painful stimulus, indicating that thermal cameras could serve as valuable tools for pain monitoring. In [19], a pain dataset comprising RGB, thermal, and depth videos was introduced. The findings demonstrated that the RGB modality marginally"}, {"title": "II. RELATED WORK", "content": "Recent developments have seen a range of innovative methods to assess pain levels from video data. Werner et al. [20] focused on domain-specific features, using facial action markers with a deep random forest (RF) classifier, and proposed a 3D distance computation method among facial points while in [21], an optical flow method was introduced to track facial points and capture expression changes across frames. The dynamic aspects of pain were addressed by developing long short-term memory networks combined with sparse coding (SLSTM) [22]. Tavakolian et al. [23] utilized 3D convolutional neural networks (CNNs) with varied temporal depths to analyze short-, mid-, and long-term facial expressions. In [24], the authors leverage the temporal aspect of videos by encoding frames into motion history and optical flow images, which were then analyzed using a combination of CNN and bidirectional LSTM (biLSTM). Another method encoded videos into single RGB images through statistical spatiotemporal distillation (SSD) and trained a Siamese network in a self-supervised manner [25]. In [26] the authors implemented a multi-stream CNN for feature extraction from different facial regions, applying learned weights to emphasize the significance of each region's features in expressing pain. Further research [27] identified that specific frames more clearly displayed pain expressions and developed a framework using CNNs, gated recurrent units (GRUs), and attention saliency maps, assigning weights to each frame's influence on overall pain intensity. A novel approach by Huang et al. [28] was introduced by extracting simulated heart rate data from video content utilizing a 3D CNN, demonstrating strong results in binary and multiclass classification scenarios. Finally, in the studies [29], [30], transformer-based frameworks were proposed, yielding promising results with high efficiency."}, {"title": "III. METHODOLOGY", "content": "This section describes the process of generating synthetic thermal videos, the architecture of the proposed automatic pain assessment framework, the developed augmentation techniques, the pre-processing methods, and the pre-training strategy for the modules."}, {"title": "A. Synthetic Thermal Videos", "content": "An image-to-image translation (I2I) approach has been de-veloped for generating synthetic thermal videos. I2I generative models aim to map distinct image domains by learning the intrinsic data distributions of both domains. In this case, the source domain consists of RGB images, while thermal images represent the target domain. In this study, conditional generative adversarial networks (cGANs) [31] were developed and trained in supervised settings with aligned image pairs. The generator G generates realistic-looking images, while discriminator D aims to distinguish authentic images from synthetic ones via the following minimax game:\n$\\min_{G} \\max_{D} \\mathcal{L}_{CGAN}(G, D),$\\nwhere the objective function $\\mathcal{L}_{CGAN}(G, D)$ can be expressed as:\n$\\mathbb{E}_{x, y} [log D(x, y)] + \\mathbb{E}_{x, z} [log(1 \u2013 D(x, G(x, z)))],$\nwhere x represents the real data, y signifies the target data, and z denotes the random noise vector. The G aims to minimize the objective function, while the D functions adversarially, trying to maximize it. Furthermore, we included the Wasserstein gradient penalty (WGAN-GP) [32] to increase the training stability. The final objective is described as:\n$\\mathcal{L}_{CGAN}(G, D) + \\lambda \\mathbb{E}_{x, \\hat{y}} [(|| \\nabla_{\\hat{y}} D(\\hat{x}, \\hat{y})||_2 \u2014 1)^2],$\nwhere $\\lambda$ denotes the penalty coefficient. Regarding the architecture, in the proposed methodology, inspired by [33], the G is structured into 3 distinct modules: an encoder, which comprises 2 convolutional layers downsampling the input; an intermediate ResNet module, consisting of 9 residual blocks, each consisting with 2 convolutional layers; and a decoder, upsampling the feature maps to the final resolution (i.e., 256 \u00d7 256) for the synthetic sample. The D founded on [34] is a pixel-level PatchGAN discriminator using 1 \u00d7 1 kernels consisting of 2 convolutional layers."}, {"title": "B. Framework Architecture", "content": "The proposed framework comprises two main modules: a Vision-MLP model that functions as a spatial embedding extractor for individual video frames and a transformer-based model that serves as a temporal module, utilizing the embedded representations of the videos for temporal analysis and final pain assessment. \n1) Vision-MLP: MLP-like models represent a newly in-troduced type of vision models, serving as alternatives to traditional Convolutional Neural Networks (CNNs) and Vision Transformers (ViT). They are characterized by simple architectures consisting of fully connected layers coupled with activation functions. They embody lesser inductive bias and are based on basic matrix multiplication routines. Our approach is founded on the principles of [35] introducing the Vision-MLP and [36] incorporating a wave representation for the patches (also referred to as tokens). Each video frame is initially divided inton non-overlapping tokens $F_m = [f_{m,1}, f_{m,2},..., f_{m,n}] \\in R^{n \\times p \\times p \\times 3}$, where p specifies the resolution of each token, i.e., 16 x 16 pixels, and 3 represents the number of color channels. Each token is then linearly projected into a dimension d = 768 before being fed into the Vision-MLP . The first main sub-module is the so-called Channel-Mixer which operates on each token $f_j$ independently and allows communication between different channels and is formulated as:\n$\\text{Channel-Mixer}(f_j, W^c) = W^c f_j,$\nwhere $W^c$ denotes the weight matrix with learnable parameters, and $j = 1,2,...,n$. Next, the second main sub-module, Token-Mixer , allows communication between different tokens, enabling feature extraction from different spatial locations. Typically, in MLP-based models, the token-mixers formulated as:\n$\\text{Token-Mixer}(F,W^t)_j = \\sum_k W^t_{jk} \\odot f_k,$\nwhere $W^t$ denotes the corresponding weight matrix for the tokens, and the $\\odot$ represents element-wise multiplication. Our proposed approach transforms the tokens into wave-like representations to modulate the relationship between tokens and weights dynamically according to their semantic content. In order to represent a token $f_j$ as wave $\\tilde{f_j}$ through a wave function, amplitude and phase information are needed:\n$\\tilde{f_j} = |f_j|e^{i \\theta_j}.$\nHere, i denotes the imaginary unit satisfying $i^2 = \u22121$. The term $|f_j|$ represents the amplitude of the signal. The function $e^{i \\theta_j}$ is a periodic function, and $\\theta_j$ symbolizes the phase of the signal. The amplitude $|f_j|$ can be likened to the real-valued feature"}, {"title": "", "content": "found in conventional models, with the notable distinction being the application of the absolute value operation. In the practical implementation, the absolute value operation is omitted and replaced with $\\psi$ for simplicity. The phase $\\theta_j$ for each token reflects its position within a wave's cycle and can thus be described using fixed parameters, which are learnable during the training phase. Consequently, $\\psi$ is also utilized for the phase estimation. Given that $\\theta$ characterizes a wave within the complex domain, the Euler formula facilitates embedding tokens within the neural network architecture:\n$f_j = |f_j| \\cos \\theta_j + i |f_j| \\sin \\theta_j.$\nCombining 5 and 7, a token is represented as:\n$\\tilde{f_j} = \\sum W^t_k f_k = \\sum W^t_{jk} f_k \\odot \\cos \\Theta_k + W^t_{jk} f_k \\odot \\sin \\Theta_k$\n$\\Rightarrow \\sum W^{t \\_ c}  \\cos (W^{t \\_ f} f_k) + W^{t \\_ s} f_k \\odot \\sin (W^{t \\_ f} f_k)$\nwhere $W^{t \\_ c}$, $W^{t \\_ s}$ and $W^{t \\_ f}$ are learnable weight matrices. The process described, which pertains to wave-like representations, unfolds within the Token-Mixer, particularly in the Wave-Block. The Token-Mixer architecture comprises three blocks: two Wave-Blocks and one Channel-Mixer operating in parallel. The Vision-MLP module is structured into four stages. Each stage comprises a sequence consisting of a Token-Mixer and a Channel-Mixer block, with a normalization layer preceding each. The depth of parallel blocks in each stage is 3, 4, 18, and 3, respectively. This structure facilitates extracting hierarchical embeddings with corresponding dimensions across stages 64, 128, 320, and 100.\n2) Fusion: For each input frame, the Vision-MLP extracts an embedding with a dimensionality of d = 100. Subsequently, the embeddings derived from the respective frames of a particular video are concatenated to create a unified embedding representation of the original video:\n$V_D = [d_1||d_2||... ||d_m], V_D \\in R^N,$\nwhere m denotes the number of frames in a video, and N represents the dimensionality of the final embedding. Subsequently, the embeddings derived from RGB and synthetic thermal videos are integrated through a weighted fusion process:\n$V_{Fused} = W_1 \\cdot V_{RGB} + W_2 \\cdot V_{Thermal}, V_{Fused} \\in R^N.$\nThe fusion process is founded on combining the corresponding embeddings, utilizing learned weights $w_1$ and $w_2$, which modulate the contributions of the RGB and thermal embeddings, respectively. The weighted addition provides an optimized integration, reflecting the importance of each modality in the final fused representation $V_{Fused}$.\n3) Transformer: The fused embeddings are subsequently fed into a transformer-based module comprising self-attention and cross-attention blocks . The self-attention process is represented as follows:\n$\\text{Attention}(Q, K, V) = \\text{softmax} (\\frac{QK^T}{\\sqrt{d_k}})V.$\nHere, $Q \\in R^{M \\times C}$, $K \\in R^{M \\times C}$, and $V \\in R^{M \\times C}$ represent the Query, Key, and Value matrices, respectively, where M denotes the input dimension, and C the channel dimension. Similarly, the cross-attention mechanism employs a dot product operation, but the Q instead of M \u00d7 C is N \u00d7 C, where N < M offers a computational cost reduction. Each self and cross-attention block incorporates 1 and 8 attention heads, respectively, while 4 parallel blocks comprise the whole Transformer module. The resulting output embeddings, with a dimensionality of 340, are employed to complete the final pain assessment through a fully connected neural network."}, {"title": "C. Augmentation Methods", "content": "Two augmentation techniques have been implemented within the framework. First, the so-called Basic is employed, inte-grating polarity inversion with noise addition. This method transforms the original input embedding by reversing the polarity of data elements and adding random noise from a Gaussian distribution, creating variability and perturbations. Second, the Masking involves applying zero-valued masks to the embeddings, nullifying segments of the vectors. The dimensions of the masks are randomly determined, spanning 10% to 50% of the embedding's total dimensions, and they are positioned at random locations within the embeddings."}, {"title": "D. Pre-processing", "content": "The pre-processing involved face detection to isolate the facial region. The MTCNN face detector [37] was employed, which utilizes multitask cascaded convolutional neural networks for predicting faces and landmarks. It is important to note that the face detector was applied only to the individual RGB frames, and the coordinates of the detected face were applied to the corresponding synthetic thermal frames. The resolution of all frames was set at 224 x 224 pixels."}, {"title": "E. Pre-training", "content": "For the I2I approach, the SpeakingFaces [38] dataset was utilized to train the proposed GAN model for translating the RGB to synthetic thermal videos. In addition, prior to the automatic pain assessment training process, the Vision-MLP and Transformer modules were pre-trained. The Vision-MLP underwent a three-stage pre-training strategy: initially, it was trained on DigiFace-1M [39] to learn basic facial features. Subsequently, it was trained on AffectNet [40] and RAF Face Database basic [41] to learn features related to basic emotions through multi-task learning. Finally, the Compound Facial Expressions of Emotions Database [42] and the RAF Face Database compound [41] were utilized to learn features of compound emotions in a similar multi-task setting. The multi-task learning process is described as:\n$\\mathcal{L}_{total} = [e^{w_1} L_{S_1} + w_1] + [e^{w_2} L_{S_2} + w_2],$\nwhere $L_S$ is the loss for the corresponding task related to different datasets, and w represents the learned weights that drive the learning process in minimizing the combined loss $\\mathcal{L}_{total}$, considering all the individual losses. The Transformer"}, {"title": "IV. EXPERIMENTAL EVALUATION & RESULTS", "content": "The BioVid Heat Pain Database [43], was utilized to evaluate the proposed framework. It comprises facial videos, electrocardiogram, electromyogram, and skin conductance levels from 87 healthy individuals. The experimental design of the dataset utilized a thermode to induce pain in the participants' right arm, resulting in five distinct intensity levels: no pain (NP), mild pain (P1), moderate pain (P2), severe pain (P3), and very severe pain (P4). Each participant was exposed to each level of pain intensity 20 times, resulting in 100 data samples for each modality and 1740 data samples per class. We utilized the videos (5 \u00d7 1740 = 8700) from Part A of BioVid in this study. The pain assessment experiments were structured in binary and multi-level classification settings, evaluating each modality individually and in combination. In binary classification, the task was to distinguish between No Pain (NP) and very severe pain (P4). In contrast, multi-level classification (MC) involves classifying all pain levels within the dataset. For evaluation, the leave-one-subject-out (LOSO) cross-validation method was adopted, and performance was assessed based on the accuracy metric."}, {"title": "A. RGB Videos", "content": "In the RGB video modality context, we observed an accuracy of 69.37% for the binary classification task (NP vs. P4) and 30.23% for the multi-class classification (MC). Upon intensifying the Masking augmentation method to encompass 20 \u2013 50% of the input embeddings, there was a modest improvement of 0.89% in accuracy for the binary task. In contrast, a decrement was observed in the multi-class task. Subsequent extension of training to 300 epochs, 30 - 50% for the Masking method and 90% probability for both the augmentation methods yielded accuracies of 70.05% and 30.02% for the binary and multi-class tasks, respectively, translating to an average increment marginally below 0.5%."}, {"title": "B. Synthetic Thermal Videos", "content": "In the experiments conducted with the synthetic thermal modality under identical experimental conditions, initial accuracies were recorded at 69.97% for the binary task and 30.04% for the multi-class task. An increase in the intensity of the masking method resulted in modest accuracy improvements of 0.23% and 0.46% for the binary and multi-class tasks, respectively. Subsequently, final accuracy measurements were 70.69% for the binary task and 29.60% for the multi-class task, culminating in an average increase of 0.28%. This difference may arise from the challenge of discerning subtle facial changes associated with low-level pain and accompaniment by further corruption from heavier augmentation, which results in diminished performances."}, {"title": "C. Additional Analysis on RGB & Synthetic Thermal Videos", "content": "The findings from IV-A and IV-B revealed a notable circumstance where the performance metrics for the RGB and synthetic thermal modalities are remarkably similar. Specifically, the highest recorded accuracies for the RGB modality were 70.26% and 30.23% for the NP vs. P4 and MC tasks, respectively. Correspondingly, the peak accuracies for the synthetic thermal modality were 70.69% and 30.50%. On average, the performances from the thermal videos are approximately 1% superior to those of the RGB modality. This outcome was unexpected, given that the synthetic modality was initially presumed to be less effective than the original. This prompted an exploration into the reason synthetic modalities exhibit comparable or superior performance to the original RGB modality. A primary question was regarding the richness and effectiveness of the thermal-related information incorporated in the synthetic videos. The hypothesis suggested that reducing facial expressions in the thermal videos could allow a more ex-plicit assessment of the thermal information. Gaussian blurring was progressively applied to RGB and synthetic thermal videos with kernel sizes k incrementally adjusted from 0 to 191. Similar, albeit less time-intensive, experiments to IV-A, IV-B were conducted. \nTable V shows that with a kernel size of k = 0, the performance disparity of 0.47% (favoring the thermal modality) aligns with prior experimental outcomes. As blurring intensifies to k = 41, this discrepancy marginally increases to 0.49%. Notably, at k = 91, the divergence expands to 2.13% and intensifies to 5.90% when the blur peaks at k = 191 (heavily blurred). The classification performances demonstrated that by diminishing the visibility of facial expressions through blurring, the synthetic thermal videos resulted in superior performance compared to the RGB, with figures of 66.24% over 60.34%. Additionally, as the kernel size increased from k = 0 to k = 191, the decline in accuracy rates for the synthetic thermal and RGB modalities was 1.81% and 7.13%, respectively. This suggests that the residual information in the synthetic modality, essentially the visually represented facial temperature, remains intact or minimally influenced. Although the separation of the data points is not clear, we observe a distinct difference in the distribution. For k = 191, the RGB embeddings are centralized and probably overlap, and a plethora of points are notably spread away from the central mass without a clear pattern. Respectively, the data points are much more uniformly spreading for the synthetic modality, suggesting potentially better differentiation between classes."}, {"title": "D. Fusion", "content": "Three fusion methods were assessed in the context of multimodal analysis for RGB and synthetic thermal videos. The approach outlined in 11 was initially applied, utilizing learned weights w\u2081 and w\u2082 to scale the respective modalities. Additionally, a second method was employed where a third weight, w3, was introduced, resulting in $w_3 \\cdot (W_1 \\cdot V_{RGB} + W_2 \\cdot V_{Thermal})$."}, {"title": "V. COMPARISON WITH EXISTING METHODS", "content": "This section compares the proposed method with other existing approaches in the literature. The evaluation utilizes Part A of the BioVid dataset, involving all 87 subjects, and follows the same validation protocol, LOSO cross-validation. The proposed vision-based method, utilizing RGB and synthetic thermal modalities, demonstrated performances comparable to or exceeding that of previous methods. Compared to the findings reported in studies improved accuracy was attained in binary and multi-level tasks. It is noted that the authors in [20] reported accuracies of 72.40% and 30.80%, showing an improvement of 1.37% and 0.10% over our results. In the study [29], the authors achieved the highest reported results, employing a transformer-based architecture.\nFurthermore, in Table VIII, we compare our findings with those from the study [19], in which the authors introduce the MIntPAIN dataset, including both RGB and thermal videos for automatic pain assessment across five intensity levels. We observe that the accuracy of the RGB and thermal modalities is particularly similar, at 18.55% and 18.33%, respectively. This outcome mirrors our findings, where performance between the two modalities\u2013RGB and synthetic thermal was similarly aligned. By fusing the modalities, the authors achieved an"}, {"title": "", "content": "increase of 30.77%, marking an improvement of over 12%. This contrasts with our results, where the increase was marginal. However, it is important to note that the performances of the unimodal approaches in [19] were below the guess prediction threshold of 20%, and only through the fusion of these modalities did the performance surpass it."}, {"title": "VI. CONCLUSION", "content": "This study explored the generation of synthetic thermal imagery from GAN models to evaluate its effectiveness in the context of automatic pain assessment. Furthermore, a novel framework based on Vision-MLP was introduced, complimented by a Transformer module serving as the core of the assessment system. The conducted experiments underscored the efficacy of the synthetic thermal modality, showcasing performances comparable to or surpassing those of the original RGB modality. Moreover, this study examined the underlying factors contributing to this effectiveness, particularly focusing on the role of temperature color representations. Additionally, the integration of the two vision modalities was analyzed using various fusion techniques. It should be emphasized that further optimization and experimentation, particularly with the multimodal approach, have the capacity to yield enhanced results. We believe that the generation and integration of synthetic modalities, such as thermal imagery, in an automatic pain assessment framework holds significant potential, and additional exploration and research are needed."}, {"title": "ETHICAL IMPACT STATEMENT", "content": "This research employed the BioVid Heat Pain Database [43] to evaluate the proposed methods. The data were recorded according to the ethical guidelines of Helsinki (ethics commit-tee: 196/10-UBB/bal). Prior to commencing data collection, each participant's pain threshold (where sensation transitions from heat to pain) and tolerance threshold (the moment when pain becomes unbearable) were determined. The facial images presented in this study are from participants who have consented to their use for illustrative purposes within a scientific research context. This study aims to introduce a pain assessment framework designed to facilitate continuous patient monitoring while reducing human biases. However, it is essential to recognize that real-world applications, especially in clinical settings, might present challenges, necessitating further experimentation and comprehensive evaluation through clinical trials before deployment.\nIn addition, this study utilized the Speaking Faces [38] dataset for the image-to-image translation process. The data was collected according to the ethical guidelines of the Declaration of Helsinki, and with the approval from the Institutional Research Ethics Committee at Nazarbayev University. All participants were volunteers who were fully informed about the data collection procedures and the intended use of identifiable images, which will be distributed as part of a dataset. Each participant provided their permission by signing informed consent forms.\nFurthermore, several datasets were utilized to pretrain the proposed pain assessment framework. The DigiFace-1M [39] is a synthetic dataset where 511 initial face scans were obtained with consent and employed to build a parametric face geometry and texture library model. All the identities and samples were generated from these source data. The AffectNet [40] dataset is compiled using search engine queries. The original paper does not explicitly detail ethical compliance measures such as adherence to the Declaration of Helsinki or informed consent procedures. The original paper of Compound FEE-DB [42] does not mention ethical compliance measures, but only that the subjects were recruited from the Ohio State University area and received a monetary reward for participating. The RAF-DB [41] dataset was compiled using the Flickr image hosting service. Although Flickr hosts both public and privately shared images, the authors do not explicitly mention the type of the downloaded images."}]}