{"title": "ExpProof: Operationalizing Explanations for Confidential Models with ZKPs", "authors": ["Chhavi Yadav", "Evan Monroe Laufer", "Dan Boneh", "Kamalika Chaudhuri"], "abstract": "In principle, explanations are intended as a way to increase trust in machine learning models and are often obligated by regulations. However, many circumstances where these are demanded are adversarial in nature, meaning the involved parties have misaligned interests and are incentivized to manipulate explanations for their purpose. As a result, explainability methods fail to be operational in such settings despite the demand (Bordt et al., 2022). In this paper, we take a step towards operationalizing explanations in adversarial scenarios with Zero-Knowledge Proofs (ZKPs), a cryptographic primitive. Specifically we explore ZKP-amenable versions of the popular explainability algorithm LIME and evaluate their performance on Neural Networks and Random Forests.", "sections": [{"title": "1. Introduction", "content": "\"Bottom line: Post-hoc explanations are highly problematic in an adversarial context\" (Bordt et al., 2022)\n\nExplanations have been seen as a way to enhance trust in machine learning (ML) models by virtue of making them transparent. Although starting out as a debugging tool, they are now also widely proposed to prove fairness and sensibility of ML-based predictions for societal applications, in research studies (Langer et al., 2021; Smuha, 2019; K\u00e4stner et al., 2021; Von Eschenbach, 2021; Leben, 2023; Karimi et al., 2020; Wachter et al., 2017; Liao & Varshney, 2021) and regulations alike (Right to Explanation (Wikipedia contributors, 2025)). However, as discussed in detail by (Bordt et al., 2022), many of these use-cases are adversarial in nature where the involved parties have misaligned interests and are incentivized to manipulate explanations to meet their ends. For instance, a bank which denies loan to an applicant based on an ML model's prediction has an incentive to return an incontestable explanation to the applicant rather than reveal the true workings of the model since the explanation can be used by the applicant to prove discrimination in the court of law (Bordt et al., 2022). In fact, many previous studies show that adversarial manipulations of explanations are possible in realistic settings with systematic and computationally feasible attacks (Slack et al., 2020; Shahin Shamsabadi et al., 2022; Slack et al., 2021; Yadav et al., 2024b). As such, despite the demand, explanations fail to be operational as a trust-enhancing tool.\n\nA major barrier to using explanations in adversarial contexts is that organizations keep their models confidential due to IP and legal reasons. However, confidentiality aids in manipulating explanations by allowing model swapping a model owner can use different models for generating predictions vs. explanations, swap the model for specific inputs, or change the model post-audits (Slack et al., 2020; Yadav et al., 2022; Yan & Zhang, 2022). This problem demands a technical solution which guarantees that a specific model is used for all inputs, for generating both the prediction and the explanation and prove this to the customer on the receiving end while keeping the model confidential.\n\nAnother important barrier to using explanations in adversarial contexts is that many explanation algorithms are not deterministic and have many tunable parameters. An adversary can choose these parameters adversarially to make discriminatory predictions seem benign. Moreover, there is no guarantee that the model developer is following the explanation algorithm correctly to generate explanations."}, {"title": "2. Preliminaries", "content": "Cryptographic Primitives. We use two cryptographic primitives in this paper, commitment schemes and Zero-Knowledge Proofs.\n\nCommitment Scheme (Blum, 1983) commits to private inputs $w$ outputting a commitment string $com_w$. A commitment scheme is hiding meaning that $com_w$ does not reveal anything about private input $w$ and binding meaning that there cannot exist another input $w'$ which has the commitment $com_w$, binding commitment $com_w$ to input $w$.\n\nZero-Knowledge Proofs (ZKPs) (Goldwasser et al., 1985; Goldreich et al., 1991) involve a prover holding a private input $w$, and a verifier who both have access to a circuit $P$. ZKPs enable the prover to convince the verifier that, for some public input $x$, it holds a private witness $w$ such that $P(x, w) = 1$ without revealing any additional information about witness $w$ to the verifier. A ZKP protocol is (1) complete, meaning that for any inputs $(x, w)$ where $P(x, w) = 1$, an honest prover will always be able to convince an honest verifier that $P(x,w) = 1$ by correctly following the protocol, (2) sound, meaning that beyond a negligible probability, a malicious prover cannot convince an honest verifier for any input $x$, that for some witness $w$, $P(x, w) = 1$ when infact such a witness $w$ does not exist, even by arbitrarily deviating from the protocol, and (3) zero-knowledge, meaning that for any input $x$ and witness $w$ such that $P(x, w) = 1$, a malicious verifier cannot learn any additional information about witness $w$ except that $P(x, w) = 1$ even when arbitrarily deviating from the protocol. A classic result says that any predicate P in the class NP can be verified using ZKPs (Goldreich et al., 1991).\n\nLIME. Existing literature has put forward a wide variety of post-hoc (post-training) explainability techniques to make ML models transparent. In this paper, we focus on one of the popular ones, LIME (stands for Local Interpretable Model-Agnostic Explanations) (Ribeiro et al., 2016).\n\nLIME explains the prediction for an input point by approximating the local decision boundary around that point with a linear model. Formally, given an input point $x \\in X$, a complex non-interpretable classifier $f : X \\rightarrow Y$ and an interpretable class of models $G$, LIME explains the prediction $f(x) \\in Y$ with a local interpretable model $g\\in G$. The interpretable model $g$ is found from the class $G$ via learning, on a set of points randomly sampled around the input point and weighed according to their distance to the input point, as measured with a similarity kernel $\\pi$. The similarity kernel creates a locality around the input by giving higher weights to samples near input $x$ as compared to those far off. A natural and popular choice for the interpretable class of models $G$ is linear models such that for any $g \\in G$, $g(z) = w_g \\cdot z$ ((Ribeiro et al., 2016; Garreau & von Luxburg, 2020)), where $w_g$ are the coefficients of linear model $g$. These coefficients highlight the contribution of each feature towards the prediction and therefore serve as the explanation in LIME. Learning the linear model is formulated as a weighted LASSO problem, since the sparsity induced by $l_1$ regularization leads to more interpretable and human-understandable explanations. Following (Ribeiro et al., 2016), the similarity kernel is set to be the exponential kernel with $l_2$ norm as the distance function, $\\pi_x(z) = exp(-l_2(x, z)^2/\\sigma^2)$ where $\\sigma$ is the bandwidth parameter of the kernel and controls the locality around input $x$.\n\nFor brevity, we will denote the coefficients corresponding to the linear model $g$ as $w$ instead of $w_g$, unless otherwise noted. For readers familiar with LIME, without loss of generality we consider the transformation of the points into an interpretable feature space to be identity in this paper for simplicity of exposition. The complete LIME algorithm with linear explanations is given in Alg. 1. We will also use 'explanations' to mean post-hoc explanations throughout the rest of the paper."}, {"title": "3. Problem Setting & Desiderata for Solution", "content": "To recall, explanations fail as a trust-enhancing tool in adversarial use-cases and can lead to a false sense of security while benefiting adversaries. Motivated by these problems, we investigate if a technical solution can be designed to operationalize explanations in adversarial settings.\n\nFormal Problem Setting. Formally, a model owner confidently holds a classification model $f$ which is not publicly released due to legal and IP reasons. A customer supplies an input $x$ to the model owner, who responds with a prediction $f(x)$ and an explanation $E(f, x)$ where $E$ is the possibly-randomized algorithm generating the explanation.\n\nSolution Desiderata. A technical solution to operationalize explanations in adversarial use-cases should provide the following guarantees.\n\n1.  (Model Uniformity) the same model $f$ is used by the model owner for all inputs: our solution is to use cryptographic commitments which force the model owner to commit to a model prior to receiving inputs,\n2.  (Explanation Correctness) the explanation algorithm $E$ is run correctly for generating explanations for all inputs: our solution is to use Zero-Knowledge Proofs, wherein the model owner supplies a cryptographic proof of correctness to be verified by the customer in a computationally feasible manner,\n3.  (Model Consistency) the same model $f$ is used for inference and generating explanations : this is ensured by generating inference and explanations as a part of the same system and by using model commitments,\n4.  (Model Confidentiality) the model $f$ is kept confidential in the sense that any technique for guaranteeing (1)-(3) does not leak anything else about the hidden model $f$ than is already leaked by predictions $f(x)$ and explanations $E(f, x)$ without using the technique : this comes as a by-product of using ZKPs and commitments (See Sec. A.1 for the formal theorem and proof),\n5.  (Technique Reliability) the technique used for guaranteeing (1)-(4) is sound and complete (as in Sec.2): this comes as a by-product of using ZKPs and commitments (See Sec. A.1 for the formal theorem and proof).\n\nOur solution ExpProof which provides the above guarantees will be discussed in Sec. 5."}, {"title": "4. Variants of LIME", "content": "Building zero-knowledge proofs of explanations requires the explanation algorithm to be implemented in a ZKP library\u00b9 which is known to introduce a significant computational overhead. Given this, a natural question that comes to mind is if there exist variants of LIME which provide similar quality of explanations but are more ZKP-amenable by design, meaning they introduce a smaller ZKP overhead?\n\nStandard LIME Variants. To create variants of standard LIME (Alg.1), we focus on the two steps which are carried out numerous times and hence create a computational bottleneck in the LIME algorithm sampling around input $x$ (Step 6 in Alg. 1) and computing distance using exponential kernel (Step 7 in Alg. 1). For sampling, we propose two options as found in the literature: gaussian (G) and uniform (U) (Ribeiro et al., 2016; Garreau & Luxburg, 2020; Garreau & von Luxburg, 2020). For the kernel we propose to either use the exponential (E) kernel or no (N) kernel. These choices give rise to four variants of LIME, mentioned in Alg. 2. We address each variant by the intials in the brackets, for instance standard LIME with uniform sampling and no kernel is addressed as 'LIME_U+N'.\n\nBorderLIME. An important consideration for generating meaningful local explanations is that the sampled neighborhood should contain points from different classes (Laugel et al., 2018). Any reasonable neighborhood for an input far off from the decision boundary will only contain samples from the same class, resulting in vacuous explanations.\n\nTo remedy the problem, (Laugel et al., 2017; 2018) propose a radial search algorithm, which finds the closest point to the input $x$ belonging to a different class, $X_{border}$, and then uses $X_{border}$ as the input to LIME (instead of original input $x$). Their algorithm incrementally grows (or shrinks) a search area radially from the input point and relies on random sampling within each 'ring' (or sphere), looking for points with an opposite label. To cryptographically prove this algorithm, we would either have to reimplement the algorithm as-is or would have to give a probabilistic security guarantee (using a concentration inequality), both of which would require many classifier calls and thereby many proofs of inference, becoming inefficient in a ZKP system.\n\nWe transform their algorithm into a line search version, called BorderLIME, given in Alg. 3 and 4, using the notion of Stability Radius which is now fed as a parameter to the algorithm. The stability radius for an input $x$, $\\delta_{\\alpha}$, is defined as the largest radius for which the model prediction remains unchanged within a ball of that radius around the input $x$. The stability radius $\\delta$ is defined as the minimum stability radius across all inputs $x$ sampled from the data distribution $D$. Formally, $\\delta = inf_{x \\sim D} \\delta_x$, where $\\delta_x = sup\\{r > 0 | f(x') = f(x),\\forall x' \\in B(x,r)\\}$. Here $B(x,r) = \\{x' | ||x' - x|| \\leq r\\}$ denotes a ball of radius $r$ centered at $x$. Stability radius ensures that for any input from the data distribution, the model's prediction remains stable within at least a radius of $\\delta$.\n\nOur algorithm samples $m$ directions and then starting from the original input $x$, takes $\\delta$ steps until it finds a point with a different label along all these directions individually. The border point $X_{border}$ is that oppositely labeled point which is closest to the input $x$. Furthermore, unlike in the algorithm in (Laugel et al., 2017), our algorithm can exploit parallelization by searching along the different directions in parallel since these are independent.\n\nDetermining the optimal value of the stability radius is an interesting research question, but it is not the focus of this work. We leave an in-depth exploration of this topic to future work while providing some high-level directions and suggestions next. Stability radius can (and perhaps should) be found offline using techniques as proposed in (Jordan et al., 2019; Yadav et al., 2024a) or through an offline empirical evaluation on in-distribution points. A ZK proof for this radius can be generated one-time, in an offline manner and supplied by the model developer (for NNs see (Yadav et al., 2024a)). It can also be pre-committed to by the model developer (see Sec. 5)."}, {"title": "5. ExpProof: Verification of Explanations", "content": "Our system for operationalizing explanations in adversarial settings, ExpProof, consists of two phases: (1) a One-time Commitment phase and (2) an Online verification phase which should be executed for every input.\n\nCommitment Phase. To ensure model uniformity, the model owner cryptographically commits to a fixed set of model weights $W$ belonging to the original model $f$, resulting in committed weights $com_W$. Architecture of model $f$ is assumed to be public. Additionally, model owner can also commit to the values of different parameters used in the explanation algorithm or these parameters can be public.\n\nOnline Verification Phase. This phase is executed every time a customer inputs a query. On receiving the query, the prover (bank) outputs a prediction, an explanation and a zero-knowledge proof of the explanation. Verifier (customer) validates the proof without looking at the model weights. If the proof passes verification, it means that the explanation is correctly computed with the committed model weights and explanation algorithm parameters.\n\nTo generate the explanation proof, a ZKP circuit which implements (a variant of) LIME is required. However since ZKPs can be computationally inefficient, instead of reimplementing the algorithm as-is in a ZKP library, we devise some smart strategies for verification, based on the fact that verification can be easier than redoing the computation. Since all the variants of LIME share some common functionalities, we next describe how the verification strategies for these functionalities. For more details on the verification for each variant, see Appendix Sec. A.\n\n1. Verifying Sampling (Alg. 7, 12, 13). We use the Poseidon (Grassi et al., 2021) hash function to generate random samples. As part of the setup phase, the prover commits to a random value $r_p$. When submitting an input for explanation, the verifier sends another random value $r_v$. Prover generates uniformly sampled points using Poseidon with a key $r_p+r_v$, which is uniformly random in the view of both the prover and the verifier. Then, during the proof generation phase, the prover proves that the sampled points are the correct outputs from Poseidon using ezkl's inbuilt efficient Poseidon verification circuit. We convert the uniform samples into Gaussian samples using the inverse CDF, which is checked in the proof using a look-up table for the inverse CDF.\n2. Verifying Exponential Kernel (Alg. 11). ZKP libraries do not support many non-linear functions such as exponential, which is used for the similarity kernel in LIME (Step 5 of Alg.1). To resolve this problem, we implement a lookup table for the exponential function and prove that the exponential value is correct by comparing it with the value from the look-up table.\n3. Verifying Inference. Since LIME requires predictions for the sampled points in order to learn the linear explanation, we must verify that the predictions are correct. To generate proofs for correct predictions, we use ezkl's inbuilt inference verification circuit.\n4. Verifying LASSO Solution (Alg. 9). ZKP libraries only accept integers and hence all floating points have to be quantized. Consequently, the LASSO solution for Step 7 of Alg. 1 is also quantized in a ZKP library, leading to small scale differences between the exact and quantized solutions. To verify optimality of the quantized LASSO solution, we use the standard concept of duality gap. For a primal objective $l$ and its dual objective $g$, to prove that the objective value from primal feasible $w$ is close to that from the primal optimal $w^*$, that is $l(w) \u2013 l(w^*) < \\epsilon$, the duality gap should be smaller than $\\epsilon$ as well, $l(w) \u2013 g(u,v) \\leq \\epsilon$ where $u, v$ are dual feasible. Since the primal and dual of LASSO have closed forms, as long we input any dual feasible values, we can verify that the quantized LASSO solution is close to the LASSO optimal. The prover provides the dual feasible as part of the witness to the proof. See App. Sec.A.2 for closed forms of the primal and dual functions and for the technique to find dual feasible.\n\nThe complete ExpProof protocol can be found in Alg. 5; its security guarantee is given as follows.\nTheorem 5.1. (Informal) Given a model $f$ and an input point $x$, ExpProof returns prediction $f(x)$, LIME explanation $E(f, x)$ and a ZK proof for the correct computation of the explanation, without leaking anything additional about the weights of model $f$ (in the sense described in Sec.3).\n\nFor the complete formal security theorem and proof, refer to App. Sec. A.1. The proof follows from inherent properties of ZKPs."}, {"title": "6. Experiments", "content": "Datasets & Models. We use three standard fairness benchmarks for experimentation: Adult (Becker & Kohavi, 1996), Credit (Yeh, 2016) and German Credit (Hofmann, 1994). Adult has 14 input features, Credit has 23 input features, and German has 20 input features. All the continuous features in the datasets are standardized. We train two kinds of models on these datasets, neural networks and random forests. Our neural networks are 2-layer fully connected ReLU activated networks with 16 hidden units in each layer, trained using Stochastic Gradient Descent in PyTorch (Paszke et al., 2019) with a learning rate of 0.001 for 400 epochs. The weights and biases are converted to fixed-point representation with four decimal places for making them compatible with ZKP libraries which do not work with floating points, leading to a ~ 1% test accuracy drop. Our random forests are trained using Scikit-Learn (Pedregosa et al., 2011) with 5-6 decision trees in each forest.\n\nZKP Configuration. We code ExpProof with different variants of LIME in the ezkl library (Konduit, 2024) (Version 18.1.1) which uses Halo2 (Zcash Foundation, 2023) as its underlying proof system in the Rust programming language, resulting in ~ 3.7k lines of code. Our ZKP experiments are run on an Ubuntu server with 64 CPUs of x86_64 architecture and 256 GB of memory, without any explicit parallelization. We use default configuration for ezkl, except for 200k rows for all lookup arguments in ezkl and ExpProof. We use KZG (Kate et al., 2010) commitments for our scheme that are built into ezkl.\n\nResearch Questions & Metrics. We ask the following questions for the different variants of LIME.\n\nQ1) How faithful are the explanations generated by the LIME variant?\nQ2) What is the time and memory overhead introduced by implementing the LIME variant in a ZKP library?\n\nTo answer Q1, we need a measure of fidelity of the explanation, we use 'Prediction Similarity' defined as the similarity of predictions between the explanation classifier and the original model in a local region around the input. We first sample points from a local\u00b2 region around the input point, then classify these according to both the explanation classifier and the original model and report the fraction of matches between the two kinds of predictions as prediction similarity. In our experiments, the local region is created by sampling 1000 points from a Uniform distribution of half-edge length 0.2 or a Gaussian distribution centered at the input point with a standard deviation of 0.2.\n\nTo answer Q2, we will look at the proof generation time taken by the prover to generate the ZK proof, the verification time taken by the verifier to verify the proof and the proof length which measures the size of the generated proof.\n\n6.1. Standard LIME Variants\nIn this section we compare the different variants of Standard LIME, given in Alg. 2 Sec. 4, w.r.t. the fidelity of their explanations and ZKP overhead.\n\nSetup. We use the LIME library for experimentation and run the different variants of LIME with number of neighboring samples $n = 300$ and length of explanation $K = 5$. Based on the sampling type, we either sample randomly from a hypercube with half-edge length as 0.2 or from a gaussian distribution centered around the input point with a standard deviation of 0.2. Based on the kernel type we either do not use a kernel or use the exponential kernel with a bandwidth parameter as $\\sqrt{\\#features} * 0.75$ (default value in the LIME library). Rest of the parameters also keep the default values of the LIME library. Our results are averaged over 50 different input points sampled randomly from the test set.\n\nResults for NNs with 300 neighboring samples and Gaussian sampling for fidelity evaluation are described below. Results for uniform sampling fidelity evaluation, fidelity evaluation with neighborhood $n = 5000$ points and all results for RFs can be found in the Appendix Sec. B."}, {"title": "6.2. BorderLIME", "content": "In this section we compare the variants of BorderLIME (Alg. 3, Sec. 4) and BorderLIME vs. Standard LIME w.r.t. the fidelity of their explanations and ZKP overheads.\n\nSetup. We implement BorderLIME with all of the Standard LIME variants (Step 6 of Alg. 3). For the purpose of experimentation, we fix the iteration threshold to $T = 250$, number of directions to $m = 5$. Then to approximate the stability radius $\\delta$, we incrementally go over the set \\{0.01, 0.03, 0.05, 0.07, 0.1, 0.15\\} and use the smallest value for which an opposite class point is found for all 50 randomly sampled input points. While this is a heuristic approach and does not guarantee the theoretically minimal stability radius, it provides a practical estimate of stability radius for efficient experimentation. Once a suitable value of stability radius is identified, we tighten the number of directions by reducing them while ensuring that at least one opposite-class point exists for each input. Our results are averaged over 50 input points. The exact parameter values used in our final setup can be found in App. Sec. B.\n\nResults for NNs with 300 neighboring samples and Gaussian sampling for fidelity evaluation are described below. Results for uniform sampling fidelity evaluation, fidelity evaluation with neighborhood $n = 5000$ points and all results for RFs can be found in the Appendix Sec. B.\n\nFidelity Results. Comparing different variants of BorderLIME based on the LIME implementation, we observe that the difference in explanation fidelity between gaussian and uniform sampling becomes more pronounced compared to standard LIME as shown in Fig. 2, reinforcing the importance of gaussian sampling. This gap can sometimes be reduced by using more neighborhood points, i.e. a larger $n$, when uniformly sampling. As demonstrated in Fig. 2 mid, with three times more points for uniform sampling, we can match the fidelity of gaussian explanations for Adult and German datasets. Comparing the G+N version of BorderLIME and standard LIME in Fig. 2 right, we observe that explanations generated by BorderLIME are atleast as faithful as standard LIME and can sometimes be better hinting to its capability of generating more meaningful explanations.\n\nZKP Overhead Results. We observe that BorderLIME has a larger ZKP overhead than standard LIME as shown in Table 1; this can be attributed to the additional steps needed in BorderLIME to find the border point with opposite label (Alg. 4) which also have to be proved and verified. Similar to the previous subsection, the overhead is similar across datasets and verification is orders of magnitude cheaper than proof generation."}, {"title": "7. Discussion", "content": "While ExpProof guarantees model and parameter uniformity as well as correctness of explanations for a given model, it cannot prevent the kinds of manipulation where the model itself is corrupted \u2013 the model can be trained to create innocuous explanations while giving biased predictions. Here usually a regularization term corresponding to the manipulated explanations is added to the loss function (A\u00efvodji et al., 2019; Yadav et al., 2024b). Preventing such attacks requires a ZK proof of training; this is well-studied in the literature but is outside the scope of this work and we refer an interested reader to (Garg et al., 2023).\n\nFurthermore, to provide end-to-end trust guarantees for fully secure explanations, the explanations should be (1) faithful, stable and reliable, (2) robust to realistic adversarial attacks (such as the one mentioned above) and (3) should also be verifiable under confidentiality. This paper looks at the third condition by giving a protocol ExpProof and implementing it for verifiable explanations under confidentiality, which has not been studied prior to our work. As such, we view our work as complementary and necessary for end-to-end explanation trust guarantees."}, {"title": "8. Related Work", "content": "In the ML field ZKPs have been majorly used for verification of inferences made by models (Sun et al., 2024; Chen et al., 2024; Kang et al., 2022; Weng et al., 2023; Sun & Zhang, 2023; Feng et al., 2021; VI2, 2023; Lee et al., 2020; Zhang et al., 2020a; Liu et al., 2021; Singh et al., 2022; Fan et al., 2023). A line of work also focuses on proving the training of ML models using ZKPs (Burkhalter et al., 2021; Huang et al., 2022; R\u00fcckel et al., 2022; Garg et al., 2023; Abbaszadeh et al., 2024). More recently they're also been used for verifying properties such as fairness (Yadav et al., 2024a; Shamsabadi et al., 2023; Toreini et al., 2023) and accuracy (Zhang et al., 2020b) of confidential ML models. Contrary to these and to the best of our knowledge, ours is the first work that identifies the need for proving explanations and provides ZKP based solutions for the same."}, {"title": "9. Conclusion & Future Work", "content": "In this paper we take a step towards operationalizing explanations in adversarial contexts where the involved parties have misaligned interests. We propose a protocol ExpProof using Commitments and Zero-Knowledge Proofs, which provides guarantees on the model used and correctness of explanations in the face of confidentiality requirements. We propose ZKP-efficient versions of the popular explainability algorithm LIME and demonstrate the feasibility of ExpProof for Neural Networks & Random Forests.\n\nAn interesting avenue for future work is the tailored design of explainability algorithms for high ZKP-efficiency and inherent robustness to adversarial manipulations. Another interesting avenue is finding other applications in ML where ZKPs can ensure verifiable computation and provide trust guarantees without revealing sensitive information."}, {"title": "A. ExpProof", "content": ""}, {"title": "A.1. Security of ExpProof", "content": "Completeness $\\forall$ ZK_LIME configurations cc, input points x, and model weights W\n$$ \\Pr\\begin{aligned} &\\text{pp} \\leftarrow \\text{ExpProof.Setup}(1^k) \\\\ &(\\text{pk}, \\text{vk}) \\leftarrow \\text{ExpProof.KeyGen}(\\text{pp}) \\\\ &(\\text{com}_W, \\text{com}_r) \\leftarrow \\text{ExpProof.Commit}(\\text{pp}, W, r) \\\\ &(\\text{o}, e, \\pi) \\leftarrow \\text{ExpProof.Prove}(\\text{pp}, \\text{pk}, \\text{cc}, x, \\text{com}_W, W, \\text{com}_r, r_p, r_v) \\\\ &\\text{ExpProof.Verify}(\\text{pp}, \\text{vk}, \\text{cc}, x, o, e, \\text{com}_W, \\text{com}_r, \\pi) \\end{aligned} = 1$$\nProof Sketch. Completeness. The completeness proof mostly follows from the completeness of the underlying proof system (in our case, Halo2). We must also show that for any set of parameters there exists a LIME solution $\\hat{w}$ and a feasible dual solution $\\hat{v}$ such that the dual gap between $\\hat{w}$ and $\\hat{v}$ is less than $\\epsilon$. We know from the strong duality of Lasso that there exists a solution $w^*$ and $v^*$ such that the dual gap is 0 for any input points and labels, therefore such a solution exists. However, we also note that the circuit operates on fixed-point, discrete values (not real numbers), and it is not necessarily true that there are valid solutions in fixed-point. To solve this, the prover can use a larger number of fractional bits until the approximation is precise enough.\nKnowledge-Soundness We define the relation $\\mathcal{R}_{lime}$ as:\n\\begin{equation*} \\mathcal{R}_{lime} = \\left\\{\\begin{array}{c|c} (cc, x, o, e, r_v, com_W, com_r; W, r_p, y, h, \\hat{w}, \\hat{v}) & \\begin{array}{l} com_W = Com(W) \\\\ com_r = Com(r_p) \\\\ o \\leftarrow cc.f(W, x) \\\\ h_i = Poseidon(r_p + r_v, i) \\\\ z \\approx SAMPLE\\_AROUND(x, cc.\\sigma) \\\\ y \\leftarrow cc.f(W, z) \\\\ \\pi \\leftarrow LIME\\_KERNEL(x) \\\\ y = cc.f(W, z) \\\\ z' \\leftarrow \\sqrt{i} \\times z \\\\ y' \\leftarrow \\sqrt{i} \\times y \\\\ p \\leftarrow ||y' - b - \\hat{w}^T z'||^2 + cc.a||\\hat{w}||_1 \\\\ d \\leftarrow ||v||^2 + v^T(y' - b) \\\\ p - d \\leq cc.e \\\\ f \\leftarrow (X^T)v \\\\ -cc.a < f < cc.a \\end{array} \\end{array}\\right\\} \\end{equation*}\nThere exists an extractor $\\mathcal{E}$ such that for all probabilistic polynomial time provers $\\mathcal{P}^*$\n\\begin{equation*} \\Pr\\begin{aligned} &pp \\leftarrow ExpProof.Setup(1^k) \\\\ &(pk, vk) \\leftarrow ExpProof.KeyGen(pp) \\\\ &(cc, x, o, e, r_v, com_W, com_r, \\pi) \\leftarrow \\mathcal{P}(1^\\lambda, pk) \\\\ &ExpProof.Verify(pp, vk, cc, x, o, e, r_v, com_W, com_r, \\pi) = 1 \\\\ &(W, r_p, y, h, \\hat{w}, \\hat{v}) \\leftarrow \\mathcal{E}^{\\mathcal{P}}(...) \\\\ &(cc, x, o, e, r_v, com_W, com_r; W, r_p, y, h, \\hat{w}, \\hat{v}) \\notin \\mathcal{R}_{lime} \\end{aligned} < negl(\\lambda). \\end{equation*}\nProof Sketch. Knowledge Soundness. Knowledge-soundness follows directly from the knowledge-soundness of the underlying proof system Halo2. The extractor runs the Halo2 extractor and outputs the Halo2 witness. By the construction of the circuit ZK_LIME, the extracted Halo2 witness satisfies the $\\mathcal{R}_{lime}$ relation.\nZero-Knowledge We say a protocol $\\Pi$ is zero-knolwedge if there exists a polynomial time, randomized simulator $\\mathcal{S}$ such that for all $(pk, vk) = Setup(pp)$, for all $(x, w) \\in R$, for all verifiers $\\mathcal{V}$\n\\begin{equation*} {\\mathcal{P}(pk, x, w)} \\sim {\\mathcal{S}(pk, x)} \\end{equation*}"}, {"title": "A.2. LASSO Primal and Dual", "content": "Notation: Let $X \\in \\mathbb{R}^{n \\times m}$ denote the data inputs, $y \\in \\mathbb{R}^n$ denote the labels and $a > 0$ denote the regularization parameter or the LASSO constant. Let $w \\"}]}