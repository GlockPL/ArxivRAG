{"title": "Towards Efficient Large Multimodal Model Serving", "authors": ["Haoran Qiu", "Anish Biswas", "Zihan Zhao", "Jayashree Mohan", "Alind Khare", "Esha Choukse", "\u00cd\u00f1igo Goiri", "Zeyu Zhang", "Haiying Shen", "Chetan Bansal", "Ramachandran Ramjee", "Rodrigo Fonseca"], "abstract": "Recent advances in generative AI have led to large multi-modal models (LMMs) capable of simultaneously processing inputs of various modalities such as text, images, video, and audio. While these models demonstrate impressive capabilities, efficiently serving them in production environments poses significant challenges due to their complex architectures and heterogeneous resource requirements.\nWe present the first comprehensive systems analysis of two prominent LMM architectures, decoder-only and cross-attention, on six representative open-source models. We investigate their multi-stage inference pipelines and resource utilization patterns that lead to unique systems design implications. We also present an in-depth analysis of production LMM inference traces, uncovering unique workload characteristics, including variable, heavy-tailed request distributions, diverse modal combinations, and bursty traffic patterns.\nOur key findings reveal that different LMM inference stages exhibit highly heterogeneous performance characteristics and resource demands, while concurrent requests across modalities lead to significant performance interference. To address these challenges, we propose a decoupled serving architecture that enables independent resource allocation and adaptive scaling for each stage. We further propose optimizations such as stage colocation to maximize throughput and resource utilization while meeting the latency objectives.", "sections": [{"title": "1 Introduction", "content": "The rapid advancement in generative AI has led to the development of large multimodal models (LMMs) capable of processing inputs across various modalities such as text, image, video, and audio. These models have demonstrated remarkable capabilities in tasks like image captioning [5, 16, 34], visual question answering [44, 45], and multimodal dialogue systems [9, 22, 49]. This has led to their rapid adoption of LMMs in production services, including interactive applications where low latency is critical.\nUnlike traditional large language models (LLMs) that process purely textual inputs using a single component, a decoder-based transformer architecture [52], LMMs handle fundamentally different types of inputs, each requiring distinct processing approaches. This heterogeneity introduces unique serving complexities that demand novel analysis and serving strategies. For Image-Text-to-Text models [18]\u00b9, the inference pipeline consists of multiple specialized stages: image preprocessing to transform raw images into tensor representations, image encoding to convert these tensors into image tokens, and a language model backend that combines text prompts with image tokens to generate textual outputs. Currently, these stages are typically served as a monolithic system [4, 20, 53], where all components are integrated within a single serving instance and scaled together as a unified entity.\nIn this paper, we present the first comprehensive systems analysis of two prominent LMM architectures, cross-attention (CA) and decoder-only (DO), by examining six representative open-source, Image-Text-to-Text models to understand their multi-stage inference pipelines and performance-resource characteristics. This analysis reveals unique systems design implications for production deployments. Additionally, through an in-depth study of production LMM traces, we uncover distinctive workload characteristics, including variable request patterns, diverse multi-modality combinations, and bursty traffic behaviors. While our study focuses on Image-Text-to-Text models rather than multimodal generation tasks like image or video synthesis [28], these models represent a significant and widely deployed class of LMMs that exemplify the fundamental challenges in multimodal serving.\nOur systematic analysis reveals several key findings and their implications for LMM serving system design, as summarized in Table 1. First, CA models achieve an order of magnitude higher prefill efficiency compared to DO models with marginal accuracy tradeoffs, though each architecture exhibits unique performance characteristics that demand specialized optimization strategies. Second, the LMM inference workflow demonstrates significant heterogeneity, with stages showing distinct resource and performance patterns across batching, model sharding, and frequency scaling operations, necessitating decoupled execution. Third, image encoding emerges as a critical bottleneck, consuming a substantial portion of time-to-first-token (TTFT) across multiple models,\nhighlighting the need for parallelizing encoder computation to reduce latency. Fourth, mixing text-only and image-text requests introduces notable interference patterns that impact serving performance, necessitating modality-aware scheduling. Finally, our analysis of production workloads reveals heavy-tailed distributions with modality-specific bursts, underscoring the need for workload-aware autoscaling to cope with the dynamic nature of LMM serving requirements.\nOur findings directly inform our proposed design of a novel decoupled architecture for efficient LMM serving (Section 6.1). This architecture treats the key pipeline stages- image preprocessing, image encoding, and language model operations such as prefill and decode-as independently scalable components. Our proposed design brings three key benefits and enables (1) fine-grained stage-aware resource management, (2) multimodal workload-specific scheduling/routing, and (3) model architecture-specific optimizations. We introduce stage-specific resource management policies that can be tailored to each stage's unique characteristics. These policies encompass autoscaling, batching, and model sharding strategies, allowing the system to efficiently handle varying resource demands and optimize the latency-throughput tradeoff. This approach particularly allows for managing imbalanced modalities and diverse workloads with varying image counts and dimensions. We further propose (1) stage colocation (Section 6.4) to improve resource utilization by co-locating the compute-heavy image encoder and memory-bound language decoder, complementary to existing techniques like prefill-decode colocation in the context of LLMs [1, 20], and (2) modality-aware scheduling (Section 6.5), where the system adapts scheduling decisions based on workload composition and real-time resource availability.\nContributions. Our analysis reveals critical insights into the systems challenges of efficiently serving LMMs at scale, demonstrating how architectural choices and workload-aware optimizations can address these emerging challenges. This paper makes the following contributions:\n\u2022 Systematic LMM characterization: We present a comprehensive analysis of LMM serving characteristics, examining performance profiles and resource utilization patterns across diverse workloads in both open-source LMM deployments and production environments."}, {"title": "2 Background", "content": "2.1 Large Multimodal Models\nLMMs represent a significant evolution from text-centric LLMs by integrating capabilities to process and reason across multiple modalities, such as text and images in the Image-Text-to-Text category [18], enabling applications like visual question answering, image captioning, and other multimodal tasks. LMM architectures can be mainly categorized into two: (1) decoder-only LMMs, such as DeepSeek's Janus Pro [7], LLaVA-OneVision [23], InternVL [9], and NVLM-D [12]; and (2) cross-attention-based LMMs, such as Llama-3.2 Vision [11], NVLM-X [12], and Flamingo [2]. While the two types of architectures have common image preprocessing and image encoding stages (see Figure 1), they differ in how the image tokens are processed in the language model backend.\nImage Preprocessing. Image preprocessing and encoding are the first steps to convert raw image inputs to image tokens before feeding them to the language model backend. While LMM architecture continues to evolve, image preprocessing methods become increasingly standardized regardless of the underlying architectural choices. Typically, LMMs follow four key processing steps: (1) transform the raw image with resizing, rescaling, padding, and normalization, (2) segment the transformed image into tiles [9, 11, 12] or patches [23], (3) apply additional tile/patch-level transformations, and (4) incorporate a thumbnail with the rest of image tiles.\nDespite the similarities discussed above, the number of tiles or patches generated by different image processors can differ significantly, leading to distinct relationships between image dimensions and image tokens. For instance, NVLM-D restricts the number of tiles per image to a maximum of 6\n2.2 Monolithic LMM Deployment\nLMMs are typically implemented as monolithic systems in current serving frameworks [20, 53]. This means that all inference components (image preprocessor, image encoder, and language model backend) are deployed as an ensemble, reside in the same model instance, and are thus co-located on the same hardware node. These components operate in a tightly coupled manner, with uniform batching and model parallelism strategies applied across the entire pipeline. This monolithic approach is straightforward to implement and widely used in open-source platforms for Image-Text-to-Text tasks.\nModel Parallelism. It is widely accepted that the model parallelism strategy for LLM inference should depend on memory capacity for the request batch size and prompt length to serve. Tensor parallelism (TP) is used to split the model along the attention head dimension on multiple GPUs inside each node. Pipeline parallelism (PP) is used to split the model along the model layer dimension on multiple nodes [50]. The same has been extended for LMMs so far, with both encoder and LLM following the same parallelism strategy [20]. The default model parallelism in our characterization study on open-source LMMs is listed in Table 2.\n2.3 SLO Metrics for LMM Inference\nTwo major SLO metrics are commonly used for production LLM/LMM model serving systems, with a focus on tail latency to ensure SLOs reflect the worst-case performance for end-user satisfaction:\n\u2022 Time to First Token (TTFT): This measures the latency from a user query, including text and/or image(s), to the generation of the first token in the response. It reflects the responsiveness of the model, which is essential for interactive applications such as real-time question-answering and dialog systems. Note that compared to text-only LLMs, LMM-serving's TTFT to a request comprises (1) image"}, {"title": "3 Characterization on Open-Source LMMS", "content": "We characterize representative open-source LMMs (in the Image-Text-to-Text category [18]) on open-source datasets to evaluate the performance, resource requirements, and energy efficiency of all inference stages involved under varying input complexities and resource management configurations.\n3.1 Experimental Setup\nHardware. We run the experiments on two setups: a DGX-A100 server with 8 NVIDIA A100 GPUs [33] and a DGX-H100 server with 8 NVIDIA H100 GPUs [32]. Each GPU has 80GB of high-bandwidth memory, and each server has NVLINK across GPUs. The CPU processor on the A100 node has 96 physical 2nd-generation AMD E\u0440yc\u0442\u2122 7V12 (Rome) CPU cores, while the CPU processor on the H100 node has 96 physical Intel Xeon (Sapphire Rapids) cores. Both nodes have 1900 GiB DRAM memory. Unless specified, the experiments presented in this section are performed on the A100 node.\nModels. We use six open-source models across two different LMM architectures, including CA-based models: Llama 3.2 Vision (11B and 90B) [11], and DO models: LLaVA-OneVision (7B and 72B) [23], InternVL-2.5 (26B) [9], and NVLM-D (72B) [12]. The details of the image encoder and LLM backend used by these models are listed in Table 2. The selected LMMs represent different model architectures (CA and DO), image encoder sizes (400M to 6B), and language model sizes (7B to 72B). We deploy the models on\n3.2 Comparing Different LMM Architectures\nWe now present a detailed characterization study of open-source LMMs and highlight the key takeaways.\nLatency. Figure 4 plots the CDF of TTFT and language model prefill time of all the models on the ShareGPT-40 dataset. Comparing models with similar language model backend sizes across the two architectures (i.e., Llama3.2-11B vs. LLaVA-OV-7B, and Llama3.2-90B vs. LLaVA-OV-72B vs. NVLM-D-72B), we observe that CA-based counterparts have up to an order of magnitude lower LLM prefill execution time compared to DO models, and thus lower TTFT. This is because, given the same amount of image tokens and text tokens, DO models feed all tokens in the self-attention layers of the language model backend, while CA models only feed text tokens in the self-attention layers.\nAccuracy. Figure 5 plots the accuracy vs. prefill/TTFT efficiency of different models. Comparing models of similar size across the two architectures, the CA counterparts are typically\n5 points lower in accuracy compared to the DO models on the Open VLM leaderboard [14]; e.g., Llama3.2-90B gets a score of 63.4, while a similarly sized LLaVA-OV-72B gets 68 points but at an order of magnitude higher prefill latency. CA models could close the accuracy gap by using a larger LLM backbone without substantially increasing prefill latency. For instance, Llama3.2-90B achieves about 6 points higher than Llama3.2-11B at a marginal 22% increase in TTFT.\n3.3 LMM Per-stage Breakdown Analysis\nFigure 6 plots the split-up of TTFT across the three stages that comprise it: image preprocessing, image encoding, and LLM prefill. There are three key takeaways. First, image preprocessing, which occurs on the CPU, contributes minimally to the overall TTFT, while image encoding time contributes to a major portion of TTFT (especially for CA models). For instance, 79% and 65% of TTFT in Llama3.2-11B and Llama3.2-90B are from image encoding. For DO models such as InternVL-26B and NVLM-D-72B, image encoding latency accounts for 25% and 54% of TTFT. Second, the image encoding time depends on the encoder model size. For instance, scaling from SigLIP-400M (in LLaVA-OV-7B) to InternViT-6B (in InternVL-26B), the median image encoding time increases by 10x. Finally, prefill computation is more efficient in CA models because image tokens are attended to only in the CA layers, as previously discussed in Section 3.2.\nCompute Characteristics of LMM Stages. Image preprocessing on CPU and image encoding on GPU are compute-intensive processes. Figure 7a plots the impact of varying the number of CPU cores on preprocessing latency. It is evident that preprocessing is CPU-intensive, and benefits from trivially parallelizing across all the available cores. Both stages exhibit linear latency scaling with batch size, saturating compute without significant throughput gains from increased batching as shown in Figures 7b and 7c, respectively.\nFigure 7d further plots the GPU utilization metrics for a request batch size of one during image preprocessing and image encoding. We observe a consistent SM core activity near 100% during image encoding, with average DRAM utilization below 30%. Image encoding is, therefore, typically compute-bound, resembling the language model's prefill phase [19].\nImpact of Homogeneous Batching Across Stages on GPU. In today's monolithic deployments, a single batch size is applied across all stages of the LMM on the GPU, which does not strike a balance between latency and throughput. To illustrate this, we plot Figure 8 that shows the impact of the\nbatch size on the median latency of every LMM stage across different model architectures. As the batch size increases, the increase in the median latency for different LMM stages increases at different rates. This variation highlights the diverse sensitivity of each LMM stage to batch size, and their varying levels of compute-intensiveness.\nCompute-intensive stages like image encoding and LLM prefill (in DO models) show minimal throughput gains and increased latency beyond small batch sizes. In contrast, the memory-bound decode stage exhibits linear throughput improvement with increasing batch size. Notably, for this multimodal dataset's low text token count, CA models uniquely benefit from prefill batching, diverging from traditional LLM literature which suggests prefills saturate compute even at a batch size of one single request. Therefore, request batching strategies should be tailored to each stage of the model based on its computational characteristics.\nImpact of Parallelism. Figure 9 shows the latency trend of each stage when independently increasing TP degrees of each LMM component. In Llama3.2-11B, the lowest LLM prefill time is achieved at TP-8, while the lowest image encoding la-\ntency is at TP-4 and the lowest TBT at TP-1. In fact, encoding latency increases at TP-8 because TP performance trends are influenced by tradeoffs in compute intensity and inter-GPU communication overhead, making it inefficient to split a small 630M encoder across 8 GPUs.\nAlternately, in NVLM-D-72B, which features a significantly larger image encoder (6B compared to 630M), the image encoding latency is reduced by 1.3\u00d7 when increasing the TP degree from 4 to 8. However, this results in diminishing returns relative to the increased resource cost. Depending on the workload traffic and application requirements (whether higher throughput or lower latency is needed), operators can choose between deploying two image encoders with TP-4 (to maximize throughput) or one encoder with TP-8 (to minimize latency), both across 8 GPUs.\nThese results highlight that treating the image encoder and LLM backend as a monolithic unit when determining parallelism strategies can lead to suboptimal performance. Instead, deployment decisions should account for their individual architectural characteristics and computation intensity to achieve optimal efficiency.\n3.4 Mixed Modality Performance Variation\nImage-Text-to-Text LMMs handle a mixed workload, processing both text-only and image-text requests simultaneously, as not all requests include an image input. In this section, we explore the effects of mixing images with text prompts on both an intra-request and inter-request level.\n3.4.1 Impact of Intra-Request Image-Text Token Ratio\nFigure 10 shows how varying image-to-text token ratios within a single request affects TTFT, with corresponding\ncomponent-wise latency breakdowns. We fix the total context length of each request at 8K and 16K tokens while varying the percentage of image tokens by adjusting the number of images (0-12 images for InternVL-26B with 1280 tokens/image, 0-10 for Llama3.2-11B with 1601 tokens/image).\nFor the DO InternVL-26B model, TTFT increases linearly with image token percentage, primarily driven by image encoding time, while prefill latency remains constant as shown by the split in Figure 10a (ii). This stability in prefill latency stems from decoder-only LMMs processing both text and image tokens similarly through the language model backend. Notably, increasing from 1 to 12 images results in 10\u00d7 higher encoder latency, leading to 3\u00d7 higher TTFT.\nThe CA-based Llama3.2-11B model shows a different pattern. While TTFT increases with image token percentage due to encoder overhead, the impact is less severe than InternVL-26B, showing only 1.5\u00d7 TTFT degradation when moving from text-only to image-only inputs. This moderate latency gain is due to CA models attending to image tokens only in the CA layers, resulting in lower self-attention compute as the percentage of image tokens increases as shown in Figure 10b(ii). This partially offsets the increased image encoding latency.\nThe TBT behavior also differs between architectures. InternVL-26B's TBT (batch size 64) remains stable as the image-text token split has no impact on DO models; compute is driven by the total context length. In contrast, Llama3.2-11B's TBT (batch size 16) jumps sharply with the first image due to the activation of the CA layers and then stabilizes as\n3.4.2 Impact of Inter-Request Image-Text Request Ratio\nWe now evaluate the impact of inter-request batching of text-only and image-text requests on their latency to identify how they interfere with one another during the shared language model prefill and decode operations. Using a fixed batch size of 8 and a total context length of 64K tokens for both Llama3.2-11B and InternVL-26B, we vary the proportion of image-text requests in a batch from 0% (all text-only requests) to 100% (all image-text requests) and measure their prefill latency and TBT. The results are shown in Figure 11. Note that image encoding time increases at a higher rate than prefill latency as the image-text request ratio increases, but we focus on the trends in prefill execution time as the encoder latency can be reduced by independently scaling it.\nFor CA-based Llama3.2-11B, if a batch of text-only requests is batched with even a single image-text request, it would increase the average prefill latency and TBT of the text-only requests by 5.7% and 33.3% respectively, due to the activation of CA layers. However, as the image-text request ratio increases to 100%, prefill latency drops by 91% as reduced self-attention and its MLP computation outweigh the\nCA overhead. TBT, on the other hand, very slightly decreases as we increase the number of image-text requests beyond one because of the reduction in self-attention latency due to reduced KV accesses of the text tokens. The effect will be more pronounced at larger decode batches.\nIn contrast to the CA model, the DO InternVL-26B model shows negligible variance in TTFT and TBT as we vary the percentage of image-text requests in a batch. This is primarily due to the fact that both prefill and decode compute depend only on the total context length or batch size, as the DO models treat text and image tokens equally.\n3.5 Hardware and Power Sensitivity\nTo evaluate the sensitivity of hardware and power efficiency across LMM inference stages, we conducted a series of experiments on NVIDIA A100 and H100 GPUs to measure critical performance metrics, including image encoding latency, prefill time, and TBT, as well as GPU power consumption, under varying GPU frequency settings. All experiments presented in the rest of this section use Llama3.2-11B because CA-based LMMs achieve superior prefill efficiency and better overall TTFT and TBT than DO LMMs. Power consumption is measured using NVIDIA DCGM [36].\nPerformance. Figure 12 illustrates the impact of frequency scaling on performance across different inference stages at a batch size of eight. Similar trends are observed for other batch sizes. Image encoding latency improved by an average of 34% on A100 GPUs and 36% on H100 GPUs when scaling from the lowest to the highest frequency. Prefill times exhibited similar improvements, with an average gain of 30% on A100 and 28.5% on H100 GPUs. TBT, which represents the overall decoding performance, shows the least performance improvements of 17.3% on A100 and 14.1% on H100 due to its more memory-bound nature.\nTransitioning from A100 to H100 hardware provided additional performance boosts. For image encoding latency, the\naverage improvement across all frequency levels is 30.4%. Prefill times improve by an average of 41.6%, with the highest gains for the lowest frequency level (44.2% at 1400 MHz). For TBT, the average improvement across frequency levels is only 23%, which is smaller than that in both image encoding and prefill. These results highlight the combined effects of frequency scaling and hardware upgrades on overall performance for both image encoding and language model prefill/decode workloads at varying batch sizes.\nEnergy Efficiency. At higher frequencies, performance improvements come at the cost of increased power consumption. For example, when serving a batch of eight requests on an A100 device, peak power usage rises to 418 Watts during image encoding, 438 Watts during language model prefill, and 309 Watts during decoding.\nFigure 13 shows energy consumption (power over execution time) across different frequencies for each LMM stage. Optimizing frequency requires balancing performance gains against diminishing returns in energy efficiency. This trend remains consistent across all stages (i.e., image encoding, LLM prefill, and LLM decode) and different batch sizes.\nThe frequency setting that minimizes energy consumption (measured in Watts) varies by stage and GPU model. For image encoding, the optimal frequency is 1400 MHz on H100 and 1000 MHz on A100. For LLM prefill, it is 1200 MHz on H100 and 1000 MHz on A100, while for LLM decode, the optimal frequency is 1000 MHz on both GPUs.\nAt the optimal energy efficiency point, H100 GPUs consume 36% less energy than A100 GPUs during image encoding and 24% less during LLM prefill for a batch size of 8. However, for LLM decode, the difference in energy efficiency between H100 and A100 is only 1%."}, {"title": "4 Production Trace Analysis", "content": "To understand multimodal serving patterns at scale, we analyze production traces from one of Azure's LMM inference clusters. With LMMs becoming critical components in production services, we focus on characterizing the multi-tenant traffic consisting of both text-only and image-text requests. Our analysis specifically examines (1) temporal patterns and burstiness in workload characteristics and (2) statistical distributions of multimodal request patterns. We will be releasing the traces discussed soon.\nBursty and Variable Request Arrival Pattern. Our analysis, shown in Figure 14, examines (1) the traffic of text-only and image-text requests separately to understand their dynamic behavior and overall impact on the system, and (2) the traces of two different categories of services, image-heavy and text-heavy, to capture the diverse dynamics prevalent in production. The traces are collected over a span of two days. To understand the traffic patterns, we report the timeline of: (1) prompt (input) token rate, (2) output token rate, (3) request arrival rate, and (4) input image rate. Our analysis reveals two key characteristics of production traffic:\n\u2022 Diverse Request Arrival Patterns. For image-heavy services, image-text requests show up to 5x higher prompt token rates compared to text-only requests, while text-heavy services demonstrate the opposite trend, with text-only requests having 3x higher rates. In addition, text-only and image-text request workloads often exhibit independently occurring peaks and troughs, showing minimal correlation.\n\u2022 Image Bursts. Image-text request prompt token rates show bursty behavior across both service categories. In image-heavy services, these bursts arise from increased images per request rather than higher request arrival rates.\nHeterogeneous Request Inputs. Given the significant variability in prompt token rates, we analyze input heterogeneity by examining prompt length distributions for both text-only and image-text requests (Figure 15a). The takeaways are:\n\u2022 Heavy Tails in Prompt Lengths. The services have heavy-tailed prompt length distribution. Both image-text and text-only requests' prompt length distribution is a power-law\u00b2 with alpha of 4.4 and 2.9, respectively.\n\u2022 Distinct Characteristics of Image-Text and Text-only Requests. The image-text requests (solid line in Figure 15a) have longer median prompt lengths but shorter tail prompt lengths compared to text-only requests (dashed line).\nWe also assess how images contribute to the bursts in the prompt token rates. Images can lead to higher prompt length in two ways in LMMs: (1) More number of images per request, and thus more image tokens, and (2) higher dimension (resolution), and thus more tiles. To understand image bursts prevalent in production traces, Figures 15b and 16 show the"}, {"title": "5 Challenges and Systems Implications", "content": "Based on our systems characterization study of open-source LMM benchmarks and production LMM workloads, we discuss the key challenges and their implications for designing efficient LMM serving at scale.\n5.1 Implications on Systems Architecture\nOur characterization analysis (Section 3) on diverse, representative open-source LMM models for Image-Text-to-Text tasks reveals the following key aspects that significantly impact the design of efficient LMM serving systems.\nChallenge C1: Optimizing Resource Allocation and Navigating Bottlenecks in LMM Inference. The diverse characteristics of image-text and text-only requests, combined with variable arrival patterns and requests' input heterogeneity, make managing bottlenecks in LMM inference pipelines particularly challenging for maximizing throughput. Bottlenecks can shift dynamically during bursts or when heavy-tailed requests arrive. For instance, during image bursts, the image preprocessing and encoding stages in LMM inference often become bottlenecks due to their compute-intensive nature and higher sensitivity to batching, as detailed in Section 3.3. In contrast, the language model backend may retain sufficient capacity to handle additional requests. Conversely, when heavy-tailed text requests with large prefill lengths arrive, the language model becomes the bottleneck due to the compute-intensive nature of the prefill stage, while the image encoder may remain underutilized. In such cases, a naive and coarse-grained approach like monolithic deployment (Section 2.2) will be highly resource-inefficient, as it requires scaling the\nentire LMM pipeline, even when the bottleneck remains localized to a specific component.\nChallenge C2: Achieving High Resource Utilization. The LMM inference pipeline often creates significant resource imbalances and may lead to poor resource utilization. Profiling of the Llama3.2-90B image encoder (Figure 7d) reveals this imbalance clearly. While it saturates compute resources, it uses only 2.5% of memory capacity and just 20% of available memory bandwidth. To balance compute and memory usage, the other two stages of the LMM pipeline, prefill and decode, can be combined using prefill-chunking [1, 20]. However, we observe that image-text requests are typically accompanied by small text prompts (much smaller than LLM serving). This limits opportunities to fuse prefill and decode operations, resulting in decode-only batches that underutilize compute resources. These imbalances make achieving high resource utilization in LMM inference extremely challenging.\nImplication: Stage-Specific Optimizations. Different stages of LMM pipelines, such as image preprocessing, encoding, and text generation (LLM prefill and decode), exhibit distinct optimization characteristics and resource utilization patterns (Section 3.3). The efficacy of performance optimization techniques, particularly batching policies, model parallelization strategies, and GPU frequency scaling policies, varies significantly across different LMM stages and model architectures. This observation motivates the need for fine-grained, stage-aware configuration management to maximize throughput while maintaining latency constraints, and co-location of stages where possible, to maximize resource efficiency\n5.2 Implications on Scheduling and Routing\nSimilar to LLM serving, in LMM serving, a scheduler determines when to batch or execute every request within each model instance, while a router distributes incoming requests across model instances to balance the load. The production traces (Section 4) exhibit highly variable and heterogeneous characteristics and pose several challenges to efficiently scheduling and routing LMM requests at scale.\nChallenge C3: Managing Tail Latencies during Traffic Bursts. The bursts observed in production traces (Figure 14) can significantly increase queuing delays and lead to SLO violations. This is because the SLOs for LMM serving systems are defined on tail TTFT and TBT latencies (e.g., the P95 or P99), and these tail latencies (especially TTFT) remain highly sensitive to queuing. When the bursts happen, queueing is inevitable. LMMs complicate this as different modalities (such as image-text and text-only requests) can experience independent bursts, each imposing varying computational demands on the pipeline. Furthermore, reactive approaches may fail to address these issues effectively because bursts can cause SLO violations before the system has a chance to adapt.\nChallenge C4: Handling Convoy Effects and Interference\nfrom Heavy-Tailed Requests. The heavy-tailed and wide distribution of requests in terms of prompt lengths and image counts in production LMM traces, along with their distinct processing requirements, can cause convoy effects and increase performance interference. For example, a convoy effect may occur when text-only requests with long prompts occupy all the workers in the shared LMM inference cluster. This scenario can block image-text requests that are collocated with the text-only requests, even when the queues are short, increase their tail latency, and violate SLOs. Overall, a few large requests from one modality may increase the latency of many small requests from another modality, leading to SLO violations. To make things worse, the distinct processing requirements of image-text and text-only requests can further increase interference. For instance, as the percentage of image tokens increases in the context, prefill time decreases because image tokens bypass the self-attention layers in CA-based LMMs. However, since images can have varying token counts (from 256 to 8K tokens as shown in Figure 3), naively prioritizing image-text requests can degrade performance for text-only requests.\nImplication: Modality-Aware Routing and Scheduling. Concurrent execution of text-only and image-text requests on shared LMM instances improves utilization but leads to significant performance variation (Section 3.4) that is mainly due to their distinct processing requirements and varying context lengths. For instance, request TTFT decreases when the percentage of image tokens in the total context length increases since image tokens bypass the self-attention layers (which accounts for a dominant portion of 32 out of 40 layers in Llama3.2-11B) in CA-based LMMs. However, since images can consist of a varying number of image tokens (ranging from 256 to up to 8K tokens as shown in Figure 3), simply prioritizing incoming image-text requests can lead to performance degradation for text-only requests.\nTo serve traffic bursts and minimize interference and convoy effects, modality-aware request routing and scheduling are essential. This approach should intelligently collocate text-only and image-text requests based on the token ratios of image and text in the request context, ensuring high resource utilization on shared GPU devices while meeting SLOs."}, {"title": "6 A Decoupled System Design", "content": "To address these challenges (Section 5)", "benefits": "n\u2022 This approach enables adaptive and independent resource scaling based on LMM workload characteristics, with Image Nodes handling image-specific operations such as preprocessing and encoding, and Text Nodes hosting the LLM backend that manages prefill and decode stages.\n\u2022 The decoupled design allows stage-specific and model-specific configurations (e.g., autoscaling, model parallelism, and batching strategies), optimized for each component's computational profile and architecture.\n\u2022 Cross-request interference can be minimized by physically separating Image and Text Nodes in this design, allowing text-only requests to bypass the Image Node entirely. In addition, modality-aware scheduling at the Text Node can maximize the processing throughput of text and image tokens while meeting the TTFT and TBT SLOs.\n6.2 Decoupled Resource Management\nThe logical split between Image and Text Nodes allows for independent provisioning and management of the image-\nspecific and text-specific components in an LMM pipeline, each with varied resource demands. Our design introduces an Image Agent and Text Agent to independently manage resources and configurations for their respective components. Each agent dynamically makes online resource configuration decisions based on workload demands and system load. Below"}]}