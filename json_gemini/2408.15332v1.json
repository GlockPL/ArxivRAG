{"title": "WHAT MAKES MATH PROBLEMS HARD FOR REINFORCEMENT LEARNING: A CASE STUDY", "authors": ["A. SHEHPER", "A. MEDINA-MARDONES", "B. LEWANDOWSKI", "A. GRUEN", "P. KUCHARSKI", "S. GUKOV"], "abstract": "Using a long-standing conjecture from combinatorial group theory, we explore, from multiple angles, the challenges of finding rare instances carrying disproportionately high rewards. Based on lessons learned in the mathematical context defined by the Andrews-Curtis conjecture, we propose algorithmic improvements that can be relevant in other domains with ultra-sparse reward problems. Although our case study can be formulated as a game, its shortest winning sequences are potentially 106 or 109 times longer than those encountered in chess. In the process of our study, we demonstrate that one of the potential counterexamples due to Akbulut and Kirby, whose status escaped direct mathematical methods for 39 years, is stably AC-trivial.", "sections": [{"title": "1. INTRODUCTION", "content": "We live in an extraordinary era where artificial intelligence (AI) is transforming numerous sectors and professions. Recent advancements in Large Language Models (LLMs) have empowered AI to read, write, and converse with a proficiency comparable to that of human experts. In the realm of board games, AI has outperformed even the most skilled human players, and it has tackled complex scientific challenges like protein folding, where steady progress was suddenly overtaken by a near-complete solution. As AI continues to evolve, one critical question remains: How wide is the range of domains in which AI systems can reason as effectively as humans?\nMathematics appears to be a natural progression on the path toward Artificial General Intelligence (AGI) due to its universal syntactic and logical structure, similar to that of natural language. Additionally, mathematics provides a framework for the quantitative evaluation of logical and analytical reasoning, making it an ideal domain for self-improving AI systems on the path to AGI. In a moment, we will explain another reason why mathematics could play a crucial role in AGI development, but first, we need to introduce one more key element: reinforcement learning (RL).\nMachine learning, a subfield of AI, involves developing algorithms and statistical models that enable computers to learn from data and make predictions. Among the three primary areas of machine learning supervised learning, unsupervised learning, and reinforcement learning-RL emphasizes learning through interaction with an environment and receiving feedback in the form of rewards or penalties. This aspect of machine learning, often characterized by its focus on AI models 'playing games,' will be central to our discussion.\nA typical chess game lasts about 30 to 40 moves, with the longest recorded professional game reaching 269 moves, ending in a draw between Ivan Nikolic and Goran Arsovic in 1989. Notably, the number of moves in a typical chess game is relatively consistent, with the longest professional game having only about an order of magnitude more moves than the average. Similarly, a typical game of Go involves a few hundred moves, with the longest recorded professional game, played by Go Seigen and Kitani Minoru in 1933, lasting 411 moves.\nAt first glance, proving or disproving mathematical conjectures can be formulated as games. For example, proving a theorem involves finding a path from the hypothesis to the conclusion, consisting of basic logical steps, such as Lean steps. From the RL"}, {"title": "2. ANDREWS-CURTIS CONJECTURE", "content": "The Andrews-Curtis conjecture concerns the study of balanced presentations of the trivial group, i.e. presentations of the trivial group with an equal number of generators and relators. The conjecture proposes that any balanced presentation of the trivial group\n$(x_1,..., x_n|r_1,...,r_n)$\ncan be converted to the trivial presentation\n$(x_1,...,x_n |x_1,...,x_n)$"}, {"title": "3. CLASSICAL SEARCH ALGORITHMS", "content": "In this section, we compare the effectiveness of breadth-first and greedy search algorithms to AC-trivialize presentations in the Miller-Schupp series. We find that"}, {"title": "3.1. Breadth-first search", "content": "We first recall the breadth-first search algorithm. An iterative implementation of this algorithm, adapted to the problem of the Andrews-Curtis conjecture, is provided in Algorithm 1.\nWe start with an initial state, which is a balanced presentation we wish to AC-trivialize, and place it in a queue. At each iteration, a state is removed from the queue, and its neighbors are added if they have not already been visited. This process continues until the sought-after state, i.e., a trivial balanced presentation, is found or a maximum number of states $N$ is visited. In our experiments, we set $N = 10^6$."}, {"title": "3.2. Greedy search", "content": "The greedy search algorithm, described in Algorithm 2, differs only slightly from the breadth-first search algorithm in implementation. We replace the queue with a priority queue, which stores the states in order determined by a tuple of values $(k,l)$, where $k$ is the length of the presentation and $l$ is the path length between the state and the initial state.\nInstead of dequeuing the earliest state, the algorithm dequeues the state with the smallest value of $k$. If there is more than one state in the priority queue with the same value of $k$, the state with the smallest value of $l$ is chosen."}, {"title": "3.3. Comparison", "content": "We find that greedy-search algorithm outperforms the breadth-first search algorithm in the task of AC-trivializing Miller-Schupp presentations. Out of the 1190 presentations in the Miller-Schupp series with $n \\leq 7$ and length$(w) \\leq 7$, greedy search solved 533 while BFS solved only 278 presentations. Each algorithm was constrained to visit a maximum of 1 million nodes. The percentage of presentations solved by these algorithms decreases monotonically as"}, {"title": "3.4. Limitations", "content": "While the greedy search algorithm performs better than the breadth-first search, it has some of the same limitations. Namely, it is memory inefficient, and we cannot leverage the parallelizability of modern hardware architectures. It also does not learn a general algorithm that would find an AC trivialization for any given balanced presentation."}, {"title": "3.5. Proof of Theorem 1", "content": "As mentioned in Section 1, one nice byproduct of our analysis is that the shortest mysterious AC presentation, namely AK(3), is stably AC-trivial. The goal of this part is to present a proof of this statement.\nFirst, in order to make this part of the paper self-contained, let us remind the reader that the term \"stable\" (a.k.a. \"weak\") refers to one of many variants of the Andrews-Curtis conjecture, see e.g. [MMS02; MSZ16; Bag21], where in addition to the usual AC-moves one is allowed to use two more transformations:\n(AC4) Include a new generator and a trivial relator, i.e. replace $(x_1,...,x_n | r_1,...,r_n)$ by $(x_1,...,x_n,x_{n+1}| r_1,...,r_n,x_{n+1})$.\n(AC5) Remove a trivial relator and the corresponding generator, i.e. the inverse of (AC4).\nDefinition 2. If two balanced presentations of the trivial group are related by a sequence of AC-transformations (AC1) through (AC5), we say that they are stably AC-equivalent.\nThe stable Andrews-Curtis conjecture states that any balanced presentation is stably AC-equivalent to the trivial presentation. To the best of our knowledge, prior to this work, the shortest potential counterexample to the standard Andrews-Curtis conjecture, AK(3), was also a potential counterexample to the stable Andrews-Curtis conjecture. Our proof that AK(3) is stably AC-trivial builds on the following result.\nTheorem (Myasnikov, Myasnikov, and Shpilrain, [MMS02]). Using the notation $[a, b] = aba^{-1}b^{-1}$ and $[a, b, c] = [[a, b], c]$, any presentation of the following form is a presentation of the trivial group:\n$(x, y, z | x = z \\cdot [y^{-1},x^{-1}, z], y = x \\cdot [y^{-1},x^{-1}, z^{-1}] \\cdot [z^{-1},x], w)$,"}, {"title": "4. REINFORCEMENT LEARNING", "content": "This section is organized as follows: in Subsection 4.1, we discuss how the problem underlying the Andrews-Curtis conjecture can be formulated as a Markov Decision"}, {"title": "4.1. Markov Decision Process", "content": "A Markov Decision Process (MDP) is defined as a 5-tuple $(S, A, R, P, \\rho)$. Here, $S$ represents the space of states, while $A$ denotes the set of actions, where each action $a \\in A$ is a function mapping from one state to another, i.e., $a: S \\rightarrow S$. The function $R: S\\times A\\times S \\rightarrow \\mathbb{R}$ is the reward function, which assigns a real-valued reward based on the transition from one state to another via a specific action. The transition probability function, denoted by $P: S \\times A \\rightarrow \\mathcal{P}(S)$, provides the probability distribution over the possible next states given a current state and action. Lastly, $\\rho$ represents the initial probability distribution of states, describing the likelihood of the system starting in each state.\nThe schematic picture of how these objects interact with each other is as follows. We start with a state $s_0$ sampled from the distribution $\\rho$ and take an action $a_0$. This results in a state $s_1$ with probability $P(s_1 | s_0, a_0)$. The transition gets a \u201creward\u201d $r_0 = R(s_0, a_0, s_1)$ which quantifies the effectiveness of the action in contributing toward achieving an ultimate goal. From state $s_1$, we repeat this process, obtaining a trajectory of states\n$\\mathcal{T} = (s_0, a_0, s_1, a_1, ...)$.\nThe goal of this process is to maximize the cumulative return,\n$\\mathcal{R}(\\mathcal{T}) = \\sum_{t=0}^T \\gamma^t R(s_t, a_t, s_{t+1})$.\nHere, $T$ is the length of the trajectory, known as the \u201chorizon length\" and $\\gamma \\in (0, 1)$ is the \"discount factor\" that assigns smaller weights to the reward values obtained farther in the future.\nFor a given problem at hand, we may not a priori know the actions ${a_t}$ and states ${s_{t+1}}$ that maximize the return. Deep reinforcement learning presents a solution to this problem: we train a neural network that learns a map from states to actions with the objective of maximizing the cumulative return. More precisely, we learn a map called the \"policy\" function $\\pi: S \\rightarrow \\mathcal{P}(A)$ that assigns to each state a probability distribution over actions. At time step $t$, an action $a_t \\sim \\pi(\\cdot | s_t)$ is sampled that gives the next state $s_{t+1}$. In the next subsection we discuss the specific algorithm and the objective function that we used in our study."}, {"title": "4.2. Proximal Policy Optimization", "content": "The goal of our optimization process is to find a policy that maximizes the cumulative return. The most naive way to achieve this goal is through an algorithm known as \"vanilla policy gradient.\" We perform gradient updates guided by the expected return $J(\\pi_\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\mathcal{R}(\\tau)$, where the"}, {"title": "4.3. Application to the Andrews-Curtis Conjecture", "content": "The set of all balanced presentations of the trivial group with a fixed number of generators and the set of AC-transformations play the roles of sets S and A, respectively. (Recall notations introduced in Subsection 4.1). Once we choose a reward function R and an initial state distribution \u03c1, we may use the Proximal Policy Optimization algorithm to learn the policy function \u03c0. We tested a few different reward functions in our experiments, observing that the following candidate led to the best performance and stability in training.\n$R(s_t, a_t, s_{t+1}) = \\begin{cases}\n - \\min(10, \\text{length}(s_{t+1})) & \\text{if } \\text{length}(s_{t+1}) > 2, \\\\\n 1000 & \\text{otherwise}.\n\\end{cases}$\nHere, length($s_{t+1}$) is the length of the presentation at timestep $t + 1$. The reward function assigns - min(10, length($s_{t+1}$)) to a non-terminal state and 1000 to a terminal state. We found that clipping the reward to -10 led to less variance in gradients of the loss function with respect to weights.\nWe define the initial state distribution as a distribution over the presentations of the Miller-Schupp series with $n \\leq 7$ and length$(w) \\leq 7$. Initially, each presentation was selected exactly once in ascending order by n and length(w). Following this initial sequence, we maintained an ongoing record of presentations that were either solved or unsolved at any given time. During each rollout phase, a presentation was randomly chosen from the set of solved or unsolved presentations with probabilities of $\\frac{1}{4}$ and $\\frac{3}{4}$, respectively. This method was designed to allow the policy network to refine its strategies on presentations that were already solved, potentially discovering shorter sequences of AC-moves, while also tackling presentations that remained unsolved.\nWe also need to choose a horizon length T over which the cumulative return is calculated (c.f. Subsection 4.1). In reinforcement learning, training on tasks with long horizons is usually harder than on those with short horizons. This is because in long horizon tasks it is often more difficult to figure out which actions are responsible for the eventual outcomes, a problem known as the credit assignment problem. There is also more uncertainty in the returns (high variance), which"}, {"title": "5. THE CURE: NEW ALGORITHMS", "content": "In previous sections we explained from a variety of different perspectives that the Andrews-Curtis conjecture is a good example of a mathematical problem where the length of a solution can be much greater than the length of the initial presentation, in some cases with purely analytical lower bounds that are hyperexponential in the size of the input. In particular, we saw that small increases in presentation length under 20 quickly lead to solution lengths in the range of hundreds and higher, quickly exceeding the number of moves in the longest game of chess.\nIf solving a mathematical problem required finding a path of length L, say with L = 106, an RL agent would be pretty much out of luck under circumstances of a typical hard search problem, where the number of successful paths is exponentially suppressed by L. The good news is that in mathematics - and in many other domains- such hard search problems never come in isolation. Rather, there is a distribution of problems such that generic instances are \"easy\" and a small fraction is \"hard.\" Learning this distribution for smaller values of L contains the crucial information for solving new cases at the next increment of L."}, {"title": "5.1. Supermoves", "content": "In automated reasoning or search problems where the minimal length solution has a theoretical lower bound that by far exceeds computational capabilities, it is clear that direct approach with fixed size steps is not going to succeed, unless the problem is easy and a large fraction of long paths meets the desired criteria. In order to reach extraordinary path lengths, one must allow progressively longer sequences of elementary moves to be added to the action space. Although this general strategy seems unavoidable in problems like the AC conjecture, it leads to many practical questions. For example, what should be the selection"}, {"title": "5.2. The anatomy of success", "content": "While supermoves clearly need to be a part of the solution in hard problems like the AC conjecture, much of the success depends on the criteria for selecting them. Here, we advocate for a dynamic approach where the network itself learns the criteria for selecting supermoves, in addition to the best ways to implement them. One realization of this approach could be a multi-agent model, where one network is learning to play the game and the other is learning the rules for changing the action space (adding and removing supermoves). We hope that future iterations of this strategy can lead to AI systems that can 'learn how to learn' dynamically by making both algorithmic and architectural changes through collecting the information about hard instances."}, {"title": "6. ISOLATED COMPONENTS AND NEIGHBORHOOD SIZES", "content": "6.1. Isolated components. In Subsection 3.2, we explored a greedy approach to finding AC-trivializations of a presentation \u03c0. Specifically, the goal was to construct a sequence of presentations ($\\pi_0,...,\\pi_k$), where $\\pi_0$ is the trivial presentation, such that each consecutive pair in the sequence is related by an AC-move. Furthermore, at each step $k$, the presentation $\\pi_k$ was chosen to have the shortest possible length among all presentations connected to $\\pi_{k+1}$ via an AC-move. In general, the length of a presentation in an AC-trivialization tends to exceed the length of the original presentation being trivialized. The minimum increase in length across all possible AC-trivializations serves as an invariant of the presentation. We will explore this invariant using concepts from persistent homology."}, {"title": "6.1.1. Formalization", "content": "A based graph is a pair ($\\Gamma, v_0$) consisting of a graph $\\Gamma$ and a preferred vertex $v_0$ in it. We will often drop $v_0$ from the notation. A based subgraph $\\Gamma_\\eta$ of $\\Gamma$, written $\\Gamma_\\eta \\leq \\Gamma$, is a subgraph $\\Gamma_\\eta$ of $\\Gamma$ with the same preferred vertex. We say that $\\Gamma_\\eta$ is full in $\\Gamma$ if for any two vertices in $\\Gamma_\\eta$ joined by an edge in $\\Gamma$, the edge is also in $\\Gamma_\\eta$. A filtration of a based graph $\\Gamma$ is a collection\n$\\Gamma_0 \\leq \\Gamma_1 \\leq \\Gamma_2 < \\ldots$\nof based subgraphs of $\\Gamma$ for which each vertex and edge of $\\Gamma$ is in $\\Gamma_n$ for some $n$. We refer to as a filtered based graph. If each $\\Gamma_n$ is full in $\\Gamma$ we refer to the filtration as"}, {"title": "6.2. Neighborhoods", "content": "Let us return to our data set of 1190 presentations in the Miller-Schupp series for n \u2264 7 and length(w) \u2264 7. Using the methods described in Subsection 4.2, we trained a PPO agent that successfully solved 417 of these presentations. We will refer to the set of these 417 presentations as PPO-solved and the remaining 773 presentations as PPO-unsolved. Our goal is to analyze the relationship between these labels and the sizes of their respective AC neighborhoods. A presentation is considered to be in the k-step neighborhood of another if they can be connected by applying at most k AC-moves."}, {"title": "7. LANGUAGE MODELING", "content": "In this section, we discuss a model for the \"language\" of balanced presentations. Each presentation with two relators is a sequence made of six letters, also known as \"tokens\" in the nomenclature of Natural Language Processing, i.e. $x, y, x^{-1}$, and $y^{-1}$, and two \u201cstop tokens\": one that separates two relators of a presentation and another that marks the end of a presentation. Given this vocabulary V of six tokens, we can ask what is the probability $p(t_1,...,t_V)$ for $t_i \\in V$ of the occurrence"}, {"title": "7.1. Transformers: a review", "content": "Here, we give a short review of the architecture of a decoder-only transformer. Given an input sequence $t_1, t_2,..., t_V$, a decoder-only transformer predicts the probability distribution $p(t | t_1, t_2,...,t_V)$ over the set V of tokens of size $n_{vocab}$. The probability is computed by applying the softmax function to the logits T(t), which are estimated by applying the following sequence of operations. First, assign to each token in the vocabulary a distinct label in the range 1, 2, . . ., $n_{vocab}$; re-writing the original sequence as a sequence of integers. We will label these integers also as $t_i$. Next, write the sequence in terms of \"one-hot encoded vectors\", i.e. a matrix $t \\in \\mathbb{R}^{N \\times n_{vocab}}$ such that\n$t_{ij} = \\delta_{i t_i}$\nand embed the sequence in a $d_{model}$-dimensional vector space,\n$x_0 = (W_p \\otimes 1 + 1 \\otimes W_E)t$.\nHere, $W_p \\in \\mathbb{R}^{d_{model} \\times N}$ and $W_E \\in \\mathbb{R}^{d_{model} \\times n_{vocab}}$ are matrices of learnable parameters, known as the \"positional embedding\" and \"token embedding\" matrices.\nAn L-layer transformer alternates between applying a \"multi-head attention layer\" ($\\sum_{h \\in H} h$) and an \"MLP-layer\" (m) L times. For i = 0, . . ., L \u2212 1,\n$x_{2i+1} = x_{2i} + \\sum_{h \\in H} h(LN(x_{2i})),$\n$x_{2i+2} = x_{2i+1} + m(LN(x_{2i+1})).$"}]}