{"title": "WHAT MAKES MATH PROBLEMS HARD FOR REINFORCEMENT LEARNING: A CASE STUDY", "authors": ["A. SHEHPER", "A. MEDINA-MARDONES", "B. LEWANDOWSKI", "A. GRUEN", "P. KUCHARSKI", "S. GUKOV"], "abstract": "Using a long-standing conjecture from combinatorial group theory, we explore, from multiple angles, the challenges of finding rare instances carrying disproportionately high rewards. Based on lessons learned in the mathematical context defined by the Andrews-Curtis conjecture, we propose algorithmic improvements that can be relevant in other domains with ultra-sparse reward problems. Although our case study can be formulated as a game, its shortest winning sequences are potentially 106 or 109 times longer than those encountered in chess. In the process of our study, we demonstrate that one of the potential counterexamples due to Akbulut and Kirby, whose status escaped direct mathematical methods for 39 years, is stably AC-trivial.", "sections": [{"title": "1. INTRODUCTION", "content": "We live in an extraordinary era where artificial intelligence (AI) is transforming numerous sectors and professions. Recent advancements in Large Language Models (LLMs) have empowered AI to read, write, and converse with a proficiency comparable to that of human experts. In the realm of board games, AI has outperformed even the most skilled human players, and it has tackled complex scientific challenges like protein folding, where steady progress was suddenly overtaken by a near-complete solution. As AI continues to evolve, one critical question remains: How wide is the range of domains in which AI systems can reason as effectively as humans?\nMathematics appears to be a natural progression on the path toward Artificial General Intelligence (AGI) due to its universal syntactic and logical structure, similar to that of natural language. Additionally, mathematics provides a framework for the quantitative evaluation of logical and analytical reasoning, making it an ideal domain for self-improving AI systems on the path to AGI. In a moment, we will explain another reason why mathematics could play a crucial role in AGI development, but first, we need to introduce one more key element: reinforcement learning (RL).\nMachine learning, a subfield of AI, involves developing algorithms and statistical models that enable computers to learn from data and make predictions. Among the three primary areas of machine learning supervised learning, unsupervised learning, and reinforcement learning-RL emphasizes learning through interaction with an environment and receiving feedback in the form of rewards or penalties. This aspect of machine learning, often characterized by its focus on AI models 'playing games,' will be central to our discussion.\nA typical chess game lasts about 30 to 40 moves, with the longest recorded professional game reaching 269 moves, ending in a draw between Ivan Nikolic and Goran Arsovic in 1989. Notably, the number of moves in a typical chess game is relatively consistent, with the longest professional game having only about an order of magnitude more moves than the average. Similarly, a typical game of Go involves a few hundred moves, with the longest recorded professional game, played by Go Seigen and Kitani Minoru in 1933, lasting 411 moves.\nAt first glance, proving or disproving mathematical conjectures can be formulated as games. For example, proving a theorem involves finding a path from the hypothesis to the conclusion, consisting of basic logical steps, such as Lean steps. From the RL"}, {"title": "2. ANDREWS-CURTIS CONJECTURE", "content": "The Andrews-Curtis conjecture concerns the study of balanced presentations of the trivial group, i.e. presentations of the trivial group with an equal number of generators and relators. The conjecture proposes that any balanced presentation of the trivial group\n$(x_1,..., x_n|r_1,...,r_n)$\ncan be converted to the trivial presentation\n$(x_1,...,x_n |x_1,...,x_n)$"}, {"title": "3. CLASSICAL SEARCH ALGORITHMS", "content": "In this section, we compare the effectiveness of breadth-first and greedy search algorithms to AC-trivialize presentations in the Miller-Schupp series. We find that"}, {"title": "3.1. Breadth-first search", "content": "We first recall the breadth-first search algorithm. An iterative implementation of this algorithm, adapted to the problem of the Andrews-Curtis conjecture, is provided in Algorithm 1.\nWe start with an initial state, which is a balanced presentation we wish to AC-trivialize, and place it in a queue. At each iteration, a state is removed from the queue, and its neighbors are added if they have not already been visited. This process continues until the sought-after state, i.e., a trivial balanced presentation, is found or a maximum number of states $N$ is visited. In our experiments, we set $N = 10^6$."}, {"title": "3.2. Greedy search", "content": "The greedy search algorithm, described in Algorithm 2, differs only slightly from the breadth-first search algorithm in implementation. We replace the queue with a priority queue, which stores the states in order determined by a tuple of values $(k,l)$, where $k$ is the length of the presentation and $l$ is the path length between the state and the initial state.\nInstead of dequeuing the earliest state, the algorithm dequeues the state with the smallest value of $k$. If there is more than one state in the priority queue with the same value of $k$, the state with the smallest value of $l$ is chosen."}, {"title": "3.3. Comparison", "content": "We find that greedy-search algorithm outperforms the breadth-first search algorithm in the task of AC-trivializing Miller-Schupp presentations.\nOut of the 1190 presentations in the Miller-Schupp series with $n \u2264 7$ and length$(w) \u2264 7$, greedy search solved 533 while BFS solved only 278 presentations. Each algorithm was constrained to visit a maximum of 1 million nodes. The percentage of presentations solved by these algorithms decreases monotonically as"}, {"title": "3.4. Limitations", "content": "While the greedy search algorithm performs better than the breadth-first search, it has some of the same limitations. Namely, it is memory ineffi- cient, and we cannot leverage the parallelizability of modern hardware architectures. It also does not learn a general algorithm that would find an AC trivialization for any given balanced presentation."}, {"title": "3.5. Proof of Theorem 1", "content": "As mentioned in Section 1, one nice byproduct of our analysis is that the shortest mysterious AC presentation, namely AK(3), is stably AC-trivial. The goal of this part is to present a proof of this statement.\nFirst, in order to make this part of the paper self-contained, let us remind the reader that the term \"stable\" (a.k.a. \"weak\") refers to one of many variants of the Andrews-Curtis conjecture, see e.g. where in addition to the usual AC-moves one is allowed to use two more transformations:\n(AC4) Include a new generator and a trivial relator, i.e. replace $(x_1,...,x_n | r_1,...,r_n)$ by $(x_1,...,x_n,x_{n+1}| r_1,...,r_n,x_{n+1})$.\n(AC5) Remove a trivial relator and the corresponding generator, i.e. the inverse of (AC4).\nDefinition 2. If two balanced presentations of the trivial group are related by a sequence of AC-transformations (AC1) through (AC5), we say that they are stably AC-equivalent.\nThe stable Andrews-Curtis conjecture states that any balanced presentation is stably AC-equivalent to the trivial presentation. To the best of our knowledge, prior to this work, the shortest potential counterexample to the standard Andrews-Curtis conjecture, AK(3), was also a potential counterexample to the stable Andrews-Curtis conjecture. Our proof that AK(3) is stably AC-trivial builds on the following result.\nTheorem (Myasnikov, Myasnikov, and Shpilrain, ). Using the notation $[a,b] = aba^{-1}b^{-1}$ and $[a,b,c] = [[a,b],c]$, any presentation of the following form is a presentation of the trivial group:\n$(x, y, z | x = z \u00b7 [y^{-1},x^{-1}, z], y = x \u00b7 [y^{-1},x^{-1}, z^{-1}] \u00b7 [z^{-1},x], w)$,"}, {"title": "4. REINFORCEMENT LEARNING", "content": "This section is organized as follows: in Subsection 4.1, we discuss how the problem underlying the Andrews-Curtis conjecture can be formulated as a Markov Decision"}, {"title": "4.1. Markov Decision Process", "content": "A Markov Decision Process (MDP) is defined as a 5-tuple $(S, A, R, P, \u03c1)$. Here, $S$ represents the space of states, while $A$ denotes the set of actions, where each action $a \u2208 A$ is a function mapping from one state to another, i.e., $a: S \u2192 S$. The function $R: S\u00d7A\u00d7S \u2192 R$ is the reward function, which assigns a real-valued reward based on the transition from one state to another via a specific action. The transition probability function, denoted by $P: S \u00d7 A \u2192 \\mathcal{P}(S)$, provides the probability distribution over the possible next states given a current state and action. Lastly, $\u03c1$ represents the initial probability distribution of states, describing the likelihood of the system starting in each state.\nThe schematic picture of how these objects interact with each other is as follows. We start with a state $s_0$ sampled from the distribution $\u03c1$ and take an action $a_0$. This results in a state $s_1$ with probability $P(s_1 | s_0, a_0)$. The transition gets a \u201creward\u201d $r_0 = R(s_0, a_0, s_1)$ which quantifies the effectiveness of the action in contributing toward achieving an ultimate goal. From state $s_1$, we repeat this process, obtaining a trajectory of states\n$T = (s_0, a_0, s_1, a_1, ...)$.\nThe goal of this process is to maximize the cumulative return,\n$R(T) = \\sum_{t=0}^T \u03b3^t R(s_t, a_t, s_{t+1}).$\nHere, $T$ is the length of the trajectory, known as the \u201chorizon length\" and $\u03b3 \u2208 (0, 1)$ is the \"discount factor\" that assigns smaller weights to the reward values obtained farther in the future.\nFor a given problem at hand, we may not a priori know the actions ${a_t}$ and states ${s_{t+1}}$ that maximize the return. Deep reinforcement learning presents a solution to this problem: we train a neural network that learns a map from states to actions with the objective of maximizing the cumulative return. More precisely, we learn a map called the \"policy\" function $\u03c0: S \u2192 \\mathcal{P}(A)$ that assigns to each state a probability distribution over actions. At time step t, an action $a_t \u223c \u03c0(\u00b7 | s_t)$ is sampled that gives the next state $s_{t+1}$. In the next subsection we discuss the specific algorithm and the objective function that we used in our study."}, {"title": "4.2. Proximal Policy Optimization", "content": "The goal of our optimization process is to find a policy that maximizes the cumulative return. The most naive way to achieve this goal is through an algorithm known as \"vanilla policy gradient.\" We perform gradient updates guided by the expected return $J(\u03c0_\u03b8) = \\mathbb{E}_{\u03c4\u223c\u03c0_\u03b8}[R(\u03c4)]$, where the"}, {"title": "4.3. Application to the Andrews-Curtis Conjecture", "content": "The set of all balanced presentations of the trivial group with a fixed number of generators and the set of AC-transformations play the roles of sets $S$ and $A$, respectively. (Recall notations introduced in Subsection 4.1). Once we choose a reward function $R$ and an initial state distribution $\u03c1$, we may use the Proximal Policy Optimization algorithm to learn the policy function $\u03c0$. We tested a few different reward functions in our experiments, observing that the following candidate led to the best performance and stability in training.\n$R(s_t, a_t, s_{t+1}) =\\begin{cases}\n- min(10, length(s_{t+1})) & \\text{if } length(s_{t+1}) > 2, \\\\\n1000 & \\text{otherwise}.\n\\end{cases}$\nHere, $length(s_{t+1})$ is the length of the presentation at timestep $t + 1$. The reward function assigns $- min(10, length(s_{t+1}))$ to a non-terminal state and 1000 to a terminal state. We found that clipping the reward to $-10$ led to less variance in gradients of the loss function with respect to weights.\nWe define the initial state distribution as a distribution over the presentations of the Miller-Schupp series with $n < 7$ and length$(w) \u2264 7$. Initially, each presentation was selected exactly once in ascending order by $n$ and length$(w)$. Following this initial sequence, we maintained an ongoing record of presentations that were either solved or unsolved at any given time. During each rollout phase, a presentation was randomly chosen from the set of solved or unsolved presentations with probabilities of $\\frac{3}{4}$ and $\\frac{1}{4}$, respectively. This method was designed to allow the policy network to refine its strategies on presentations that were already solved, potentially discovering shorter sequences of AC-moves, while also tackling presentations that remained unsolved.\nWe also need to choose a horizon length $T$ over which the cumulative return is calculated (c.f. Subsection 4.1). In reinforcement learning, training on tasks with long horizons is usually harder than on those with short horizons. This is because in long horizon tasks it is often more difficult to figure out which actions are responsible for the eventual outcomes, a problem known as the credit assignment problem. There is also more uncertainty in the returns (high variance), which"}, {"title": "5. THE CURE: NEW ALGORITHMS", "content": "In previous sections we explained from a variety of different perspectives that the Andrews-Curtis conjecture is a good example of a mathematical problem where the length of a solution can be much greater than the length of the initial presentation, in some cases with purely analytical lower bounds that are hyperexponential in the size of the input. In particular, we saw that small increases in presentation length under 20 quickly lead to solution lengths in the range of hundreds and higher, quickly exceeding the number of moves in the longest game of chess.\nIf solving a mathematical problem required finding a path of length $L$, say with $L = 10^6$, an RL agent would be pretty much out of luck under circumstances of a typical hard search problem, where the number of successful paths is exponentially suppressed by $L$. The good news is that in mathematics and in many other domains- such hard search problems never come in isolation. Rather, there is a distribution of problems such that generic instances are \"easy\" and a small fraction is \"hard.\" Learning this distribution for smaller values of $L$ contains the crucial information for solving new cases at the next increment of $L$."}, {"title": "5.1. Supermoves", "content": "In automated reasoning or search problems where the minimal length solution has a theoretical lower bound that by far exceeds computational capabilities, it is clear that direct approach with fixed size steps is not going to succeed, unless the problem is easy and a large fraction of long paths meets the desired criteria. In order to reach extraordinary path lengths, one must allow progressively longer sequences of elementary moves to be added to the action space. Although this general strategy seems unavoidable in problems like the AC conjecture, it leads to many practical questions. For example, what should be the selection"}, {"title": "5.2. The anatomy of success", "content": "While supermoves clearly need to be a part of the solution in hard problems like the AC conjecture, much of the success depends on the criteria for selecting them. Here, we advocate for a dynamic approach where the network itself learns the criteria for selecting supermoves, in addition to the best ways to implement them. One realization of this approach could be a multi-agent model, where one network is learning to play the game and the other is learning the rules for changing the action space (adding and removing supermoves). We hope that future iterations of this strategy can lead to AI systems that can 'learn how to learn' dynamically by making both algorithmic and architectural changes through collecting the information about hard instances.\nSpecifically, suppose $N$ is one of the characteristics of either the algorithm or the architecture that has non-trivial impact on performance. In practice, there can be several such parameters, but for simplicity we explain the idea as if there is only"}, {"title": "6. ISOLATED COMPONENTS AND NEIGHBORHOOD SIZES", "content": "In Subsection 3.2, we explored a greedy approach to finding AC-trivializations of a presentation $\u03c0$. Specifically, the goal was to construct a sequence of presentations $(\u03c0_0,...,\u03c0_\u03ba)$, where $\u03c0_0$ is the trivial presentation, such that each consecutive pair in the sequence is related by an AC-move. Furthermore, at each step $k$, the presentation $\u03c0_\u03ba$ was chosen to have the shortest possible length among all presentations connected to $\u03c0_{\u03ba+1}$ via an AC-move. In general, the length of a presentation in an AC-trivialization tends to exceed the length of the original presentation being trivialized. The minimum increase in length across all possible AC-trivializations serves as an invariant of the presentation. We will explore this invariant using concepts from persistent homology."}, {"title": "6.1. Isolated components", "content": "Formalization. A based graph is a pair $(\u0393, v_0)$ consisting of a graph $\u0393$ and a preferred vertex $v_0$ in it. We will often drop $v_0$ from the notation. A based subgraph $\u0393_\u03b7$ of $\u0393$, written $\u0393_\u03b7 \u2264 \u0393$, is a subgraph $\u0393_\u03b7$ of $\u0393$ with the same preferred vertex. We say that $\u0393_\u03b7$ is full in $\u0393$ if for any two vertices in $\u0393_\u03b7$ joined by an edge in $\u0393$, the edge is also in $\u0393_\u03b7$. A filtration of a based graph $\u0393$ is a collection\n$\u0393_0 \u2264 \u0393_1 \u2264 \u0393_2 <\u2026$\nof based subgraphs of $\u0393$ for which each vertex and edge of $\u0393$ is in $\u0393_n$ for some $n$. We refer to as a filtered based graph. If each $\u0393_n$ is full in $\u0393$ we refer to the filtration as"}, {"title": "6.2. Neighborhoods", "content": "Let us return to our data set of 1190 presentations in the Miller-Schupp series for $n \u2264 7$ and length$(w) \u2264 7$. Using the methods described in Subsection 4.2, we trained a PPO agent that successfully solved 417 of these presentations. We will refer to the set of these 417 presentations as PPO-solved and the remaining 773 presentations as PPO-unsolved. Our goal is to analyze the relationship between these labels and the sizes of their respective AC neighborhoods. A presentation is considered to be in the $k$-step neighborhood of another if they can be connected by applying at most $k$ AC-moves."}, {"title": "7. LANGUAGE MODELING", "content": "In this section, we discuss a model for the \"language\" of balanced presentations. Each presentation with two relators is a sequence made of six letters, also known as \"tokens\" in the nomenclature of Natural Language Processing, i.e. $x, y, x^{-1}, and y^{-1}$, and two \u201cstop tokens\": one that separates two relators of a presentation and another that marks the end of a presentation. Given this vocabulary $V$ of six tokens, we can ask what is the probability $p(t_1,...,t_N)$ for $t_i \u2208 V$ of the occurrence"}, {"title": "7.1. Transformers: a review", "content": "Here, we give a short review of the architecture of a decoder-only transformer. For more details, see Given an input sequence $t_1, t_2,..., t_N$, a decoder-only transformer predicts the probability distribution $p(t | t_1, t_2,...,t_N)$ over the set V of tokens of size $n_{vocab}$. The probability is computed by applying the softmax function to the logits $T(t)$, which are estimated by applying the following sequence of operations. First, assign to each token in the vocabulary a distinct label in the range 1, 2, . . ., $N_{vocab}$; re-writing the original sequence as a sequence of integers. We will label these integers also as $t_i$. Next, write the sequence in terms of \"one-hot encoded vectors\", i.e. a matrix $t \u2208 R^{Nxn_{vocab}}$ such that\n$t_{ij} = \u03b4_{i t_i}$,\nand embed the sequence in a $d_{model}$-dimensional vector space,\n$x_0 = (W_p \u2297 1 + 1 \u2297 W_E) t$.\nHere, $W_P \u2208 R^{d_{model}\u00d7N}$ and $W_E \u2208 R^{d_{model}\u00d7n_{vocab}}$ are matrices of learnable parameters, known as the \"positional embedding\" and \"token embedding\" matrices.\nAn L-layer transformer alternates between applying a \"multi-head attention layer\" ($\\sum_{h\u2208H} h$) and an \"MLP-layer\" (m) L times. For i = 0, . . ., L \u2212 1,\n$x_{2i+1} = x_{2i} + \\sum_{h\u2208H} h(L_N(x_{2i})),$\n$x_{2i+2} = x_{2i+1} + m(L_N(x_{2i+1})).$"}, {"title": "7.2. Training and Evaluation Datasets", "content": "We now discuss the training and validation datasets used to train and evaluate our Transformer model. As our main interest in this paper has been in the presentations of the Miller-Schupp series, we generated a dataset of balanced presentations that are AC-equivalent to the Miller-Schupp presentations. Specifically, we apply sequences of AC-moves to the 1190 presentations with $n \u2264 7$ and length$(w) < 7$ discussed in Section 2, creating a dataset of about 1.8 million presentations. Approximately 1 million of these presentations are AC-equivalent to the presentations that remained unsolved by greedy search (c.f. Section 3). Only a small amount (roughly 15 percent) of the original Miller-Schupp presentations were part of this dataset.\nThe dataset is tokenized using six tokens: two stop tokens and one token each for the two generators and their inverses. The tokenized dataset had about $2.17 \u00d7 10^8$ tokens. As our goal is to get insights into properties that distinguish GS-solved and GS-unsolved presentations, we performed an exploratory data analysis of the two subsets of data associated to these presentations. We plot the percentage of appearance of each token for these subsets in Figure 16. The ratio of frequency of $y^{\\pm 1}$ to the frequency of $x^{\\pm 1}$ is higher in the GS-unsolved dataset. This is likely because the GS-unsolved presentations have larger $n$, and larger $n$ corresponds to a higher number of occurrence of $y^{\\pm 1}$ in the Miller-Schupp presentation. Interestingly, this effect remains in the dataset even after applying thousands of AC-moves to the original presentations.\nWe paid special attention to ensure that our dataset contains presentations of a wide range of lengths so as not to bias our model towards learning trends specific to any fixed length. To this end, we devised an algorithm (Algorithm 6 in Appendix C) that creates an almost uniform distribution over the lengths of the presentations. (See Figure 17.) We set aside 10% of our entire data for validation."}, {"title": "7.3. Results", "content": "A randomly initialized model with the initialization scheme given in has a cross entropy loss of - ln$(1/n_{vocab}) \u2248 1.7918$. With training, we could achieve a validation loss of 0.7337. We used the untrained and the trained model to get the embeddings of all 1190 presentations of the Miller-Schupp series with $n \u2264 7$ and length$(w) \u2264 7$. We used t-SNE to project these embedding vectors to a plane. The plots are shown in grid in Figure 18.\nEach row of Figure 18 corresponds to a fixed value of $n$. The left (resp. right) column depicts t-SNE projections of embeddings obtained by an untrained (resp. trained) model. t-SNE dependence on a distance measure: it learns to map vectors that are closer together in the higher-dimensional space, with respect to this distance measure, close together in the plane [MH08]. We used cosine simiarity between embedding vectors as the distance measure for our plots. We note that the GS-solved and GS-unsolved presentations seem to cluster much more in the plots in the right column. This indicates that a trained Transformer model is able to distinguish"}, {"title": "APPENDIX B. NEIGHBORHOOD CONSTRUCTIONS", "content": "B.1. Neighborhoods of the identity. For any $l \u2208 {3, ..., 16}$, we constructed a neighborhood of the identity using an algorithm based on BFS search (Algorithm 4). This neighborhood contains all presentations that can be connected to the identity via a path of AC-moves, where each presentation in the path has a length less than or equal to $l$, that is, the full based subgraph containing vertices with connectivity value less than or equal to $l$. We consider the relators of a presentations as a set (meaning that the order of relators is not important; implemented as a tuple of relators in lexicographic order)\nB.2. Neighborhoods for MS series. We define the $n$-neighborhood of a balanced presentation as the set of all balanced presentations that can be obtained by applying at most $n$ AC-moves to $\u03c0$. We used Algorithm 5, a variation of BFS, to generate 5-neighborhoods of presentations in the Miller-Schupp series. As before, we disregard the order of the relators."}, {"title": "APPENDIX C. LANGUAGE MODELING DATASET GENERATION", "content": "This appendix describes the method, Algorithm 6, used to generate the training and evaluation datasets for the Transformer model, as referenced in Section 7. Our aim was to create datasets featuring presentations of varying lengths. We began with a presentation $P_0$ from the Miller-Schupp series, where $n \u2264 7$ and length$(w) \u2264 7$, setting a maximum relator length $I_{max} = 128$. Presentations were generated in $n = 128$ phases, each phase allowing a maximum relator length $l_i \u223c U(l + i \u00b7 l_{inc}, 1 + (i + 1) \u00b7 l_{inc})$. Here, I represents the longest relator length in $P_0$ and $l_{inc} = (l_{max} - 1)/n$ is the incremental increase per phase. In each phase, we"}]}