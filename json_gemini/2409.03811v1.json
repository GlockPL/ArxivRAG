{"title": "PARCO: Learning Parallel Autoregressive Policies for Efficient Multi-Agent Combinatorial Optimization", "authors": ["Federico Berto", "Chuanbo Hua", "Laurin Luttmann", "Jiwoo Son", "Junyoung Park", "Kyuree Ahn", "Changhyun Kwon", "Lin Xie", "Jinkyoo Park"], "abstract": "Multi-agent combinatorial optimization problems such as routing and scheduling have great practical relevance but present challenges due to their NP-hard combinatorial nature, hard constraints on the number of possible agents, and hard-to-optimize objective functions. This paper introduces PARCO (Parallel AutoRegressive Combinatorial Optimization), a novel approach that learns fast surrogate solvers for multi-agent combinatorial problems with reinforcement learning by employing parallel autoregressive decoding. We propose a model with a Multiple Pointer Mechanism to efficiently decode multiple decisions simultaneously by different agents, enhanced by a Priority-based Conflict Handling scheme. Moreover, we design specialized Communication Layers that enable effective agent collaboration, thus enriching decision-making. We evaluate PARCO in representative multi-agent combinatorial problems in routing and scheduling and demonstrate that our learned solvers offer competitive results against both classical and neural baselines in terms of both solution quality and speed. We make our code openly available at https://github.com/ai4co/parco.", "sections": [{"title": "1 Introduction", "content": "Combinatorial optimization (CO) problems, such as routing and scheduling problems, involve determining an optimal sequence of actions in a combinatorial space and have applications ranging from warehouse operatons (Xie et al., 2023) to network design (Chabarek et al., 2008) and safety-critical systems (Girardey et al., 2010). CO problems are notoriously hard to solve and cannot generally be solved optimally in polynomial time, i.e., they are NP-hard (J\u00fcnger et al., 1995). Multi-agent settings gained significant interest due to their applicability in realistic scenarios, such as path finding (Reijnen et al., 2020), drone routing (Ann et al., 2015), disaster management (Bektas, 2006; Cheikhrouhou and Khoufi, 2021) and order delivery (Yak\u0131c\u0131 and Karasakal, 2013; Archetti and Bertazzi, 2021), but present even more challenges due to additional constraints and different optimization objectives, including minimizing a global lateness objective or the makespan (Mahmoudinazlou and Kwon, 2024).\nWhile traditional algorithmic methods have significantly contributed to solving a range of problems (Laporte and Osman, 1995; Hejazi and Saghafian, 2005), these approaches often concentrate on single-agent scenarios. With the advent of modern computational techniques, neural network-based approaches have started to yield promising results for complex CO problems in a field known as Neural Combinatorial Optimization (NCO). In particular, Reinforcement Learning (RL) has shown promise due to its ability to learn directly from interactions with environments instead of relying on"}, {"title": "2 Related Work", "content": "Neural Combinatorial Optimization Recent advancements in NCO have shown promising end-to-end solutions for combinatorial optimization problems, as highlighted by Bengio et al. (2021) and Yang and Whinston (2023). NCO has led to the development of aligned neural architectures (Bi et al., 2022; Jin et al., 2023; Luo et al., 2023; Kim et al., 2023c), hybrid methods with OR solvers (Li et al., 2021b; Kim et al., 2024a; Yan and Wu, 2024; Ye et al., 2024a; Kim et al., 2024b), multi-level solution pipelines (Ma et al., 2023a; Li et al., 2023; Xiao et al., 2023; Ye et al., 2023), alongside improved training algorithms (Kim et al., 2023c; Jiang et al., 2023; Drakulic et al., 2023; Sun and Yang, 2023; Gao et al., 2023; Xiao et al., 2023; Li et al., 2024b; Wang et al., 2024) to enhance the heuristic search. These innovations have expanded NCO's application across a wide array of problems (Chen et al., 2023; Kim et al., 2023b; Zhou et al., 2023; Ma et al., 2023b; Luttmann and Xie, 2024). However, integrating appropriate inductive biases requires manual tuning of model architectures and training algorithms, presenting challenges such as computationally intensive training, the need for specialized hardware, and issues with interpretability and generalizability (Liu et al., 2023)."}, {"title": "3 Background", "content": "CO problems can be framed as Markov Decision Processes (MDPs). In this formulation, the problem is defined by a set of states $\\mathcal{S}$, where each state $s_t \\in \\mathcal{S}$ represents the configuration of the problem at time step $t$. At each step, an agent selects an action $a_t$ from the action space $\\mathcal{A}$ according to a policy $\\pi : \\mathcal{S} \\rightarrow \\mathcal{A}$, which maps states to actions. The system then transitions from state $s_t$ to state $s_{t+1}$ according to a transition function $\\mathcal{T} : \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathcal{S}$. The agent reaches the terminal state once it has generated a viable solution for the problem instance $x$. Typically, a reward $R$ is only obtained in the terminal state and takes the form of the objective function of the respective CO problem."}, {"title": "3.2 Autoregressive Methods for NCO", "content": "Given the sequential nature of MDPs, autoregressive (AR) methods pose a natural choice for the policy $\\pi$. AR methods construct a viable solution by sequentially generating actions based on the current state and previously selected actions. Formally, the process can be represented as:\n$\\displaystyle p_{\\theta}(\\mathbf{a}|x) = \\prod_{t=1}^{T} g_{\\theta}(a_t| a_{t-1}, ..., a_0, h)$\n(1)\nwhere $h = f(x)$ is the encoding of problem instance $x$ via encoder network $f$, which is used to decode actions autoregressively via decoder $g_{\\theta}$. $p_{\\theta}$ is a solver that maps $x$ to a solution $\\mathbf{a}$. $\\mathbf{\\alpha} = (a_1, ..., a_\\tau)$ represents the (optimal) actions executed in $T$ construction steps, resulting in a feasible solution to CO problems.\nNotably, the same AR scheme has been applied to solve multi-agent CO problems, such as vehicle routing problems (Kool et al., 2018; Kwon et al., 2020), in which a single agent tasked with visiting all nodes under specific route length constraints, distinguishing it from a real multi-agent scenario by focusing on optimizing a solitary agent's path within set limits. To tackle such problems, AR methods sequentially construct multiple routes as if handling a single agent at a time. However, inducing collaborative behavior among agents using the AR method poses a challenge, as it inherently focuses on one agent at a time. This limitation restricts the method's ability to directly foster interactions among the agents, which is crucial for optimizing collective outcomes in multi-agent scenarios."}, {"title": "3.3 Training AR Models via Reinforcement Learning", "content": "In this work, we focus on Reinforcement Learning (RL) solvers for training $p_{\\theta}$ as they can be trained without relying on (often hard-to-obtain) labeled solutions."}, {"title": "4 Methodology", "content": ""}, {"title": "4.1 Parallel Multi-Agent Environments", "content": "We propose to step multiple actions over the environment simultaneously to enhance efficiency in practice, which can reduce the number of steps required compared to single-agent stepping. In the general multi-agent CO setting, agents $k = 1, ..., m$ are selecting actions $\\mathbf{a}_t = (a_t^1, ..., a_t^m)$ from a shared action space $\\mathcal{A}$ in parallel at a given decoding step $t$. Given the agent actions $\\mathbf{a}_t$, the state of the problem instance transitions from $s_t$ to $s_{t+1}$ via some transition function $\\mathcal{T} : \\mathcal{S} \\times \\mathcal{A}_1 \\times ... \\times \\mathcal{A}_m \\rightarrow \\mathcal{S}$, which usually follows deterministic rules in case of non-stochastic CO problems. After reaching the terminal state, the agents receive a shared reward $R(\\mathcal{A}, x)$, with $x$ the problem instance and $\\mathcal{A} = (\\mathbf{a}_1, ..., \\mathbf{a}_\\tau)$ the sequence of agents actions, which is the objective of the respective CO problem. Fig. 1 illustrates an example of AR and Parallel AR solution construction.\nIn this work, we propose a model architecture, PARCO, to solve such cooperative multi-agent CO problems efficiently. Drawing on studies on multi-agent RL (MARL) which promotes parameter sharing in cooperative MARL settings with a shared action space (Yu et al., 2022), PARCO establishes a central auto-regressive policy $\\pi_{\\theta} : \\mathcal{S} \\rightarrow P(\\mathcal{A}_1 \\times ... \\times \\mathcal{A}_m)$ to decode the next action for the $m$ agents in parallel. This not only makes PARCO agnostic to the number of agents prevalent in the respective CO problem, but also enhances both solution quality and construction speed."}, {"title": "4.2 Parallel Autoregressive Model", "content": "In this section, we present our general architecture to solve CO problems efficiently cast as multi-agent sequential decision-making problems. PARCO follows the general encoder-decoder architecture prevalent in autoregressive NCO (Kool et al., 2018; Bello et al., 2016; Vinyals et al., 2015)."}, {"title": "4.2.1 Conflicts Handlers", "content": "When sampling from the probability distribution generated by the Multiple Pointer Mechanism, it is possible for multiple agents to select the same node simultaneously, resulting in a conflict. These conflicts must be resolved to ensure the generation of a feasible solution. To address this, we introduce a Priority-based Conflict Handler $\\mathcal{H}$, which resolves such conflicts by leveraging a predefined priority scheme. The conflict handler can be defined as a function $\\mathcal{H} : \\mathcal{N}^m \\times \\mathbb{R}^m \\rightarrow \\mathcal{N}^m$ with number of agents $m$. The priorities are determined based on the current actions and states. Given an input vector of actions $\\mathbf{a} = (a_1, a_2,..., a_m)$ where $a_i \\in \\mathbb{N}$, the corresponding priority vector $\\mathbf{p} = (p_1, p_2,..., p_m)$ where $p_i \\in \\mathbb{R}$ and the fallback actions (i.e., the previous node) $\\mathbf{r} = (r_1, r_2,..., r_m)$ where $r_i \\in \\mathbb{N}$, the output is another vector of actions $\\mathbf{a'} = (a'_1, a'_2,..., a'_m)$ such that any conflicts are resolved based on the priority order as illustrated in Algorithm 1."}, {"title": "4.2.2 Step Definition", "content": "We define a single parallel step of the model similarly to Eq. (1) as follows:\n$\\displaystyle p_{\\theta}(\\mathbf{a}|x) = \\prod_{i=1}^{m}g_{\\theta}(a_{t,i}| a_{t-1,i},..., a_{0,i},h)$\n(8)\nHere, $a_{t,i}$ denotes the action executed by agent $i$ at time $t$, after passing through the conflict handler $\\mathcal{H}$:\n$a'_{t,i} = \\mathcal{H}(a_{t,1}, a_{t,2},..., a_{t,m}; \\mathcal{P}_t, \\mathbf{r}_t)$,\nwhere $\\mathcal{P}_t$ and $\\mathbf{r}_t$ are priorities and fallback actions, respectively, at the current decoding step $t$. The conflict handler $\\mathcal{H}$ ensures that multiple agents do not select the same node, thus maintaining the integrity and feasibility of the solution."}, {"title": "4.3 Training", "content": "PARCO is a centralized multi-agent sequential decision-making model, with a shared policy $\\pi_{\\theta}$ for all agents and a global reward $R$. Thus, PARCO can be trained using any of the training algorithms adopted in the single-agent NCO literature. We train PARCO via the REINFORCE gradient estimator (Williams, 1992) with a shared baseline as outlined by Kwon et al. (2020) and Kim et al. (2022):\n$\\displaystyle \\nabla_{\\theta}L \\approx \\frac{1}{B}\\sum_{i=1}^{B}\\sum_{j=1}^{|\\mathcal{A}_{i}|}G_{ij} \\log p_{\\theta}(A_{ij}|x_i)$\n(9)\nwhere $B$ is the size of the mini-batch and $G_{ij}$ is the advantage $R(A_{ij}, x_i) - f_{shared}(x_i)$ of a solution $A_{ij}$ compared to the shared baseline $f_{shared}$ of problem instance $x_i$."}, {"title": "5 Experiments", "content": "In this section, we present the experimental results of PARCO in two routing problems, the min-max heterogenous capacitated vehicle routing problem (HCVRP) and the open multi-depot capacitated pickup and delivery problem (OMDCPDP), and a scheduling problem, namely the flexible flow shop problem (FFSP). We provide more details about the problem and experimental setups in Appendix A and Appendix B respectively."}, {"title": "5.1 Problem Descriptions", "content": "HCVRP The min-max HCVRP consists of $m$ agents sequentially visiting customers to satisfy their demands, with constraints including each customer can be visited exactly once and the amount of demand satisfied by a single vehicle in a trip cannot exceed its capacity, which can be reloaded by going back to the depot. The goal is to minimize the makespan, i.e., the worst route. Baselines include SISR (Christiaens and Vanden Berghe, 2020), Genetic Algorithm (GA) (Karakati\u010d and Podgorelec, 2015), Simulated Annealing (SA) (\u0130lhan, 2021), the Attention Model (AM) (Kool et al., 2018), Equity Transformer (ET) (Son et al., 2024), the model from Li et al. (2022) (DRLLi), and the state-of-the-art neural baseline 2D-Ptr (Liu et al., 2024c).\nOMDCPDP The OMDCPDP problem is a practical variant of the pickup and delivery problem in which agents have a stacking limit of orders that can be carried at any given time. Pickup and delivery locations are paired, and pickups must be visited before deliveries. Multiple agents start from different depots with no need to go back (open). The goal is to minimize the sum of arrival times to delivery locations, i.e. minimizing the cumulative lateness. We include ORTools (Furnon and Perron, 2024) as a classical baseline, the Heterogeneous Attention Model (HAM) (Li et al., 2021a) for sequential decision-making and MAPDP (Zong et al., 2022) for parallel decision-making.\nFFSP In the flexible flow shop problem (FFSP), N jobs must be processed across S stages, each with multiple machines (m > 1). Jobs follow a specified sequence through these stages, but within each stage, any available machine can process the job, with the key constraint that no machine can handle more than one job simultaneously. The goal is to schedule the jobs so that all jobs are finished in the shortest time possible. Notable benchmarks include the MatNet model (Kwon et al., 2021), the Random and Shortest Job First (SJF) dispatching rules, as well as the evolutionary algorithms Particle Swarm Optimization (PSO), and Genetic Algorithm (GA) (Hejazi and Saghafian, 2005)."}, {"title": "5.2 Experimental Setup", "content": "We perform all experiments on a machine equipped with two INTEL(R) XEON(R) GOLD 6338 CPU @ 2.00GHz CPUs with a total 128 threads and 8 NVIDIA RTX 4090 graphic cards with 24 GB of VRAM. Training runs of PARCO take less than 24 hours each. During inference, we employ only one CPU and a single GPU. We report key metrics such as solution cost, inference times, and gaps in best-known solutions. We keep most settings of PARCO consistent across experiments, i.e., we use a Communication Layer and Priority-based Conflict Handling based on the highest probability action from the model output. We provide additional details regarding training and testing setups in the Appendix."}, {"title": "5.3 Experimental Results", "content": "Main experiments The main experiments showcasing the performance of PARCO are in Table 1, Table 2, Table 3 for HCVRP, OMDCPDP, and FFSP, respectively. Our PARCO consistently outperforms SotA neural baselines across a variety of experiments. We additionally note another property of PARCO: unlike neural baselines in HCVRP and MAPDP in the OMDCPDP, which are trained specifically for a single size and number of agents, PARCO is a single model trained on multiple location and agent distributions at the same time; nonetheless, our single model can outperform baselines trained ad hoc on specific distributions, demonstrating our method's flexibility.\nGeneralization We additionally study the generalization performance in Table 2 on the right, which are the sizes and number of agents unseen during training in OMDCPDP. Unlike MAPDP, which is constrained to a specific number of agents, PARCO can successfully generalize to unseen sizes and m. Notably, PARCO can even outperform Google ORTools for larger-scale instances, demonstrating remarkable scalability.\nEffect of Communication Layers We showcase the importance of Communication Layers in Fig. 3 (a). We benchmark 1) No Communication (only context features), 2) communication via an MLP, 3) communication via an MHA layer, and 4) Our Communication layer. Our Communication Layer consistently outperforms other methods.\nEffect of Conflict Handlers Fig. 3 (b) shows the effect of the Priority-based Conflict Handler with different priorities methods p in the OMDCPDP. We consider the following: 1) Random: the priority is chosen randomly as in Zong et al. (2022), 2) First: the priority is chosen by the agent index k = 1, . . ., m, 3) Smallest: gives priority to the agent that has traveled the least, 4) Closest: priority is given to the closest agent and 5) High Probability: priority is given to the agent whose probability of selecting the conflicting action is the highest. High Probability can consistently outperform other methods. We note that this is also the most general and problem-agnostic since methods 2-4 are problem-specific.\nScalability Finally, we showcase PARCO's scalability in terms of speed in large-scale instances in Fig. 3 (c) compared to the constructive autoregressive ET (Son et al., 2024). Interestingly, PARCO's gap grows with more agents. This is because our method can fully exploit agent parallelism and can thus be a strong candidate for large-scale real-time CO applications."}, {"title": "6 Conclusion", "content": "In this paper, we introduced a novel approach, PARCO - Parallel AutoRegressive Combinatorial Optimization - that utilizes parallel autoregressive (AR) policies to tackle multi-agent NCO problems. We introduced a new Communication Layer to allow for the agents to effectively coordinate their next steps during decoding, alongside a Multiple Pointer Mechanism coupled with a Priority-based Conflicts Handler that can generate feasible solutions efficiently. We validate the effectiveness of PARCO through extensive experiments with different CO problems from the routing and scheduling domain and demonstrate its competitive performance against classical heuristic and neural baselines.\nWhile PARCO is already faster than most evaluated neural baselines and decreases the number of required decoding steps by more than 90% in FFSP50 instances with 10 agents as shown in Table 4, it still does not achieve the potential reduction from O(N) to O(N) in many cases. This is mainly attributed to the fact that agents can have conflicts that we currently resolve with the priority-based conflict handler. Giving one agent precedence and forcing others to stay in their current position results in more than necessary decoding steps. In future work, we will try to address this limitation by constructing a custom loss function, which penalizes conflicts and makes agents learn to avoid them during training. Further, we plan to extend our work with learnable improvement methods to"}, {"title": "A Problem Descriptions", "content": ""}, {"title": "A.1 HCVRP", "content": ""}, {"title": "A.1.1 Problem Definition", "content": "The min-max HCVRP (Heterogeneous Capacitated Vehicle Routing Problem) consists of $m$ agents sequentially visiting customers to satisfy their demands, with constraints including each customer can be visited exactly once and the amount of demand satisfied by a single vehicle in a trip cannot exceed its capacity, which can be reloaded by going back to the depot. The goal is to minimize the makespan, i.e., the worst route."}, {"title": "A.1.2 Mathematical Formulation", "content": "Consider a problem with N + 1 nodes (including N customers and a depot) and m vehicles. The depot is indexed as 0, and customers are indexed from 1 to N.\nIndices\ni, j\nNode indices, where i, j = 0, ..., N (0 represents the depot)\nk\nVehicle index, where k = 1,..., m\nParameters\nN\nNumber of customer nodes (excluding depot)\nm\nNumber of vehicles\n$X_i$\nLocation of node i\n$d_i$\nDemand of node i ($d_0$ = 0 for the depot)\n$Q_k$\nCapacity of vehicle k\n$f_k$\nSpeed of vehicle k\n$c_{ij}$\nDistance between nodes i and j\nDecision Variables\n$x_{ijk}$\n$\\{1\\; \\text{if vehicle k travels directly from node i to node j}\\\\\n0 \\; \\text{ otherwise}$$\n$l_{ijk}$\nRemaining load of vehicle k before travelling from node i to node j\nObjective Function:\n$\\displaystyle \\min \\max_{k=1,...,m} \\Biggl( \\sum_{i=0}^{N} \\sum_{j=0}^{N} \\frac{c_{ij}}{f_k} \\cdot x_{ijk} \\Biggr)$\n(10)\nSubject to:\n$\\displaystyle \\sum_{k=1}^{m} \\sum_{j=0}^{N} x_{ijk} = 1$\n$i = 1, ..., N$\n(11)\n$\\displaystyle \\sum_{i=0}^{N} x_{ijk} - \\sum_{h=0}^{N} x_{jhk} = 0$\n$j = 0,..., N, k = 1, ..., m$\n(12)\n$\\displaystyle \\sum_{k=1}^{m} \\sum_{i=0}^{N} l_{ijk} - \\sum_{k=1}^{m} \\sum_{h=0}^{N} l_{jhk} = d_j$\n$j = 1, ..., N$\n(13)\n$\\displaystyle d_j x_{ijk} \\leqslant l_{ijk} \\leqslant (Q_k - d_i) \\cdot x_{ijk}$\n$i, j = 0, ..., N, k = 1, ..., m$\n(14)\n$\\displaystyle x_{ijk} \\in \\{0,1\\}$\n$i, j = 0, ..., N, k = 1, ..., m$\n(15)\n$\\displaystyle l_{ijk} \\geqslant 0, d_i \\geqslant 0$\n$i, j = 0, . . ., N, k = 1,..., m$\n(16)\nConstraint Explanations: The formulation is subject to several constraints that define the feasible solution space. Equation (11) ensures that each customer is visited exactly once by one vehicle. The flow conservation constraint (12) guarantees that each vehicle that enters a node also leaves that node, maintaining route continuity. Demand satisfaction is enforced by constraint (13), which ensures that the difference in load before and after serving a customer equals the customer's demand. The vehicle capacity constraint (14) ensures that the load carried by a vehicle does not exceed its capacity and is sufficient to meet the next customer's demand."}, {"title": "A.2 OMDCPDP", "content": ""}, {"title": "A.2.1 Problem Definition", "content": "The OMDCPDP (Open Multi-Depot Capacitated Pickup and Delivery Problem) is a practical variant of the pickup and delivery problem in which agents have a stacking limit of orders that can be"}, {"title": "A.2.2 Mathematical Formulation", "content": "carried at any given time. Pickup and delivery locations are paired, and pickups must be visited before deliveries. Multiple agents start from different depots without returning (open). The goal is to minimize the sum of arrival times to delivery locations, i.e., minimizing the cumulative lateness.\nIndices\ni, j\nNode indices, where i, j = 1,..., 2N\nk\nVehicle index, where k = 1,..., m\nSets\nP\nSet of pickup nodes, P = $\\{1, ...,N\\}$\nD\nSet of delivery nodes, D = $\\{N + 1,...,2N\\}$\nParameters\nN\nNumber of pickup-delivery pairs\nm\nNumber of vehicles\n$c_{ij}$\nTravel time between nodes i and j\n$Q_k$\nCapacity (stacking limit) of vehicle k\n$O_k$\nInitial location (depot) of vehicle k\nDecision Variables\n$x_{ijk}$\n$\\{1\\; \\text{if vehicle k travels directly from node i to node j}\\\\\n0 \\; \\text{otherwise}$$\n$y_{ik}$\n$\\{1 \\; \\text{if vehicle k visits node i}\\\\\n0 \\; \\text{otherwise}$$\n$t_i$\nArrival time at node i\n$l_{ik}$\nLoad of vehicle k after visiting node i\nObjective Function:\n$\\displaystyle \\min \\sum_{i=N+1}^{2N} t_i$\n(17)\nSubject to:\n$\\displaystyle \\sum_{k=1}^{m} y_{ik} = 1$\n$i = 1,..., 2N$\n(18)\n$\\displaystyle \\sum_{j=1}^{2N} x_{o_k,j,k} = 1$\n$k = 1,..., m$\n(19)\n$\\displaystyle \\sum_{i=1}^{2N} x_{ijk} - \\sum_{h=1}^{2N} x_{jhk} = 0$\n$j = 1,..., 2N, k = 1,..., m$\n(20)\n$\\displaystyle y_{ik} = \\sum_{j=1}^{2N} x_{ijk}$\n$i = 1,...,2N, k = 1, ..., m$\n(21)\n$\\displaystyle t_i + c_{ij} - M(1 - x_{ijk}) \\leqslant t_j$\n$i, j = 1,..., 2N, k = 1, ..., m$\n(22)\n$\\displaystyle t_i \\leqslant t_{i+N}$\n$i \\in P$\n(23)\n$\\displaystyle l_{ik} + 1 - M(1 - x_{ijk}) \\leqslant l_{jk}$\n$i \\in P, j = i + N, k = 1, ..., m$\n(24)\n$\\displaystyle l_{ik} - 1 + M(1 - x_{ijk}) \\geqslant l_{jk}$\n$i \\in D, j\\neq i - N, k = 1, . . ., m$\n(25)\n$\\displaystyle 0 \\leqslant l_{ik} \\leqslant Q_k$\n$i = 1,..., 2N, k = 1, ..., m$\n(26)\n$\\displaystyle x_{ijk}, y_{ik} \\in \\{0,1\\}$\n$i, j = 1,..., 2N, k = 1,..., m$\n(27)\n$\\displaystyle t_i \\geqslant 0$\n$i = 1,..., 2N$\n(28)\nConstraints Explanations: Equation (18) ensures that each node is visited exactly once. Constraint (19) guarantees that each vehicle starts from its designated depot. The flow conservation constraint (20) ensures route continuity for each vehicle. Equation (21) defines the relationship between x and y variables. Time consistency is enforced by constraint (22), while (23) ensures that pickups are visited before their corresponding deliveries. Constraints (24) and (25) manage the load changes during pickup and delivery operations. Finally, the vehicle capacity constraint (26) ensures that the load never exceeds the vehicle's stacking limit."}, {"title": "A.3 FFSP", "content": ""}, {"title": "A.3.1 Problem Definition", "content": "The flexible flow shop problem (FFSP) is a challenging and extensively studied optimization problem in production scheduling, involving N jobs that must be processed across i = 1 ... S stages, each with multiple machines ($m_i > 1$). Jobs follow a specified sequence through these stages, but within each stage, any available machine can process the job, with the key constraint that no machine can handle more than one job simultaneously. The FFSP can naturally be viewed as a multi-agent CO problem by considering each machine as an agent that constructs its own schedule. Adhering to autoregressive CO, agents construct the schedule sequentially, selecting one job (or no job) at a time. The job selected by a machine (agent) at a specific stage in the decoding process is scheduled at the earliest possible time, that is, the maximum of the time the job becomes available in the respective stage (i.e., the time the job finished on prior stages) and the machine becoming idle. The process repeats until all jobs for each stage have been scheduled, and the ultimate goal is to minimize the makespan, i.e., the total time required to complete all jobs."}, {"title": "A.3.2 Mathematical Formulation", "content": "We use the mathematical model outlined in Kwon et al. (2021) to define the FFSP:\nIndices\ni\nStage index\nj, l\nJob index\nk\nMachine index in each stage\nParameters\nN\nNumber of jobs\nS\nNumber of stages\n$m_i$\nNumber of machines in stage i\nM\nA very large number\n$p_{ijk}$\nProcessing time of job j in stage i on machine k\nDecision variables\n$C_{ij}$\nCompletion time of job j in stage i\n$X_{ijk}$\n$\\{1 \\; \\text{if job j is assigned to machine k in stage i}\\\\\n0 \\; \\text{otherwise}$$\n$Y_{ilj}$\n$\\{1 \\; \\text{if job l is processed earlier than job j in stage i}\\\\\n0 \\; \\text{otherwise}$$\nObjective:\n$\\displaystyle \\min \\Biggl( \\max_j (C_{Sj}) \\Biggr)$\n(29)"}]}