{"title": "In-Context Learning Improves Compositional Understanding of Vision-Language Models", "authors": ["Matteo Nulli", "Anesa Ibrahimi", "Avik Pal", "Hoshe Lee", "Ivona Najdenkoska"], "abstract": "Vision-Language Models (VLMs) have shown remarkable capabilities in a large number of downstream tasks. Nonetheless, compositional image understanding remains a rather difficult task due to the object bias present in training data. In this work, we investigate the reasons for such a lack of capability by performing an extensive bench-marking of compositional understanding in VLMs. We compare contrastive models with generative ones and analyze their differences in architecture, pre-training data, and training tasks and losses. Furthermore, we leverage in-context learning as a way to improve the ability of VLMS to perform more complex reasoning and understanding given an image. Our extensive experiments demonstrate that our proposed approach outperforms baseline models across multiple compositional understanding datasets. The code is available here.", "sections": [{"title": "1. Introduction", "content": "Recent breakthroughs in foundation models (Radford et al., 2018; Dosovitskiy et al., 2021; Bommasani et al., 2021; Doveh et al., 2023a) are reaching human-level performance on many vision and language benchmarks. Particularly, Visual Language Models (VLMs) have shown impressive performance on many different downstream tasks, like image captioning (Yu et al., 2022), visual-question answering, image understanding (Liu et al., 2023), object localization (Dorkenwald et al., 2024) and others (Radford et al., 2021; Singh et al., 2022; Li et al., 2022). Nonetheless, some tasks, that are considered simple for humans, remain hard for VLMs. Among these lies the difficulty of correctly understanding the composition of objects in an image and their textual description. While humans can easily connect the images correctly with their corresponding descriptions, VLMs struggle to understand this sequential order of words and thus, perform poorly (Y\u00fcksekg\u00f6n\u00fcl et al., 2023). Recent advancements in computing and data scaling resulted in huge performance increases in many VL tasks, however, compositional understanding remains a challenging problem.\nIn this work, we study how state-of-the-art VLMs perform on benchmarks that evaluate their compositional understanding (Thrush et al., 2022; Y\u00fcksekg\u00f6n\u00fcl et al., 2023; Hsieh et al., 2023). Most works focus on contrastive pre-training objectives as the reason for bad performance on compositionality benchmarks (Y\u00fcksekg\u00f6n\u00fcl et al., 2023). The authors reason that employing a contrastive pre-training objective pushes VLMs to perform text-image retrieval without actually having any compositional understanding. This is the main reason why such models perform well on numerous benchmarks which do not require any compositional understanding. Differently from Y\u00fcksekg\u00f6n\u00fcl et al. (2023), we evaluate both contrastive (Radford et al., 2021) and generative models (Li et al., 2023a; Alayrac et al., 2022; Liu et al., 2023), to provide a deeper understanding of the underlying reasons for the lack of comprehension. Furthermore, we analyze their differences by looking at their pre-training strategy, data, and architectures. Next, we study the impact of different prompting strategies and introduce a new In-Context Learning (ICL) prompting method through synthetically generated, web images and captions. Specifically, we generate the synthetic data using GPT-40\u00b9 by instructing the model to prepare compositionally aware captions from a specified list of objects. We then give this as input to GPT-40 to generate an image matching the caption and a negative caption that distorts its compositional information, as shown in Figure 1. To simulate real images and captions, we randomly sample images from the COCO dataset (Lin et al., 2014) and manually annotate a positive and a negative compositional-aware caption for them. We use these examples as demonstrations for few-shot in-context learning for the generative models along with ICL prompting.\nIn summary, our contributions are the following: (i) We perform a comprehensive study on the behavior of generative and contrastive VLMs on several compositional understand-"}, {"title": "2. Related Work", "content": "Compositional understanding in VLMs Recent years have witnessed a significant increase in large-scale pre-trained VLMs within the field of multi-modal learning, enabling advanced tasks like image captioning. Foundation models such as CLIP (Radford et al., 2021) utilize large-scale image-text pairs for joint pre-training of image and text encoders using contrastive loss. Another notable architecture, BLIP (Li et al., 2022), leverages captions from noisy web data for comprehension and generation tasks. The (in)ability to grasp the underlying non-object notions of the images and text captions has led research into exploring compositional reasoning limitations of VLMs, revealing issues such as caption quality and density (Thrush et al., 2022; Y\u00fcksekg\u00f6n\u00fcl et al., 2023; Doveh et al., 2023b; Hsieh et al., 2023). Specifically, Doveh et al. (2023b) and Y\u00fcksekg\u00f6n\u00fcl et al. (2023) found that VLMs' output representations often resemble bags of words, neglecting several object attributes. To address these limitations, enhancements in caption quality and density were proposed (Urbanek et al., 2023; Touvron et al., 2023), including datasets with densely captioned images, significantly improving the compositional reasoning abilities of VLMs.\nEnhancing reasoning through In-Context Learning In-Context Learning (ICL) is a framework through which Large Language Models can learn a new task by being presented with demonstrations of how it should be solved. As detailed by Dong et al. (2024) ICL has recently provided clear performance enhancements in Foundation Models, and this has come without the need of any parameter update. Indeed, ICL frameworks are easily interpretable as interacting with the model through examples is effortless and allows the model to gain understanding through comparison (Liu et al., 2021; Wu et al., 2022). Many studies have shown how in-context learning capabilities can be enhanced by additionally acting on the pre-training stages of models (Min et al., 2021; Dong et al., 2024). Regardless of the approach, ICL frameworks have shown to increase models performances on many downstream tasks and thus we set out to try its effectiveness also in compositional understanding."}, {"title": "3. Methodology", "content": "This paper explores how in-context learning prompts consisting of intermediate reasoning steps can help VLMs to be more compositionally aware. To that end, we introduce an in-context learning prompting framework, presented in Figure 1. The intermediate steps consist of images and corresponding captions in a few-shot style. We hypothesize that by doing this, VLMs can understand the complex compositional relations within an image through correct and incorrect examples of image-caption pairs. In the following sections we will describe the process in details.\nFew-shot ICL prompting with synthetic images First, we generate a compositional aware caption with GPT-40 by prompting it in the following way:\nGenerate a caption for an image which is made of 4 objects:\nobject 1, object 2, object 3, object 4. Can you combine them\ninto a compositionally aware caption?\nThis will return the positive caption which is then fed back into GPT-40, to generate a corresponding image with DALL-E. Finally, with GPT-40, we also generate a compositionally-aware negative caption, that is in contrast with the correct caption, using the prompt of Appendix B. The same procedure is repeated five times and these exam-"}, {"title": "4. Experiments & Results", "content": "4.1. Datasets\nWe employ the following three datasets for evaluation of compositional understanding.\nAttribution, Relation, and Order (ARO) (Y\u00fcksekg\u00f6n\u00fcl et al., 2023) evaluates four different aspects of compositionality. Visual Genome Attributions and Visual Genome Relations are testing the model's understanding of correct attributes and relations associated with objects within an image, and COCO Order and Flickr30k Order are testing the understanding of the correct ordering of words in a caption.\nWinoground (Thrush et al., 2022), comprising an evaluation set of 400 pairs of two images and two captions. The pairs are very similar to each other, with slight linguistic\ndifferences in the captions.\nSugarCrepe (Hsieh et al., 2023) benchmark tests various fine-grained compositional concept understanding aspects using COCO image-text pairs. An object, attribute, or relation is either replaced, swapped, or added to the original text such that the caption no longer matches the scene.\n4.2. Baseline models\nContrastive Models Most state-of-the-art VLMs are trained with a contrastive loss (Radford et al., 2021; Chen et al., 2020; Zhai et al., 2023; Sun et al., 2023; Li et al., 2023b), and most of these use a Vision Transformer (ViT) (Dosovitskiy et al., 2021) as vision encoders, while some employ ResNet architecture (He et al., 2016). In our analysis, we use CLIP-based models from the OpenCLIP library (Ilharco et al., 2021). Some of them include EvaCLIP (Sun et al., 2023), SigLIP (Zhai et al., 2023), CLIPA (Li et al., 2023b), and CoCa (Yu et al., 2022).\nGenerative Models Generative models differ from CLIP-based models, not only in their ability to perform auto-regressive generation but also in their training process. In Section 4 we analyze two generative models, namely LLaVA (Liu et al., 2023) and CogVLM (Wang et al., 2023).\n4.3. Evaluation with contrastive VLMS\nFor contrastive evaluation, we select models from the Open-CLIP library (Ilharco et al., 2021) and assess all available options. Table 1 highlights the top five models with the best performance, distinguished by their pre-trained visual encoders. Table 1 shows a general pattern in the consistent out-performance of SigLIP models (ViT-SO400M-14-SigLIP, ViT-L-16-SigLIP-256) across most benchmarks and sub-scores with an average increase of 4% on Winoground and 6% on ARO. This is particularly relevant in tasks requiring the replacement or swapping of attributes/objects. The ViT-\n4.4. Evaluation with generative VLMs\nZero-shot performance We demonstrate the zero-shot performance of generative models in Table 1. It can be seen that CogVLM performs better than LLaVA, by increasing the Winoground text score by almost 12% and being consistently better with an average increase of 8% on ARO and similar yet overall better scores on SugarCrepe. The primary reason for this is its architectural and training advantages. CogVLM employs a vision expert module at each layer of the Large Language Model (LLM) comprising of new Query-Key-Value and MLP weights initialized from the LLM. These are tuned for vision features while the original"}, {"title": "5. Conclusion", "content": "In this work, we explore the compositional understanding of contrastive and generative VLMs. Despite lower language understanding, contrastive models remain competitive due to their consistent evaluation method. Generative models face challenges such as asymmetric text-image relationships due to autoregressive training and reliance on frozen CLIP-like vision encoders. Furthermore, we introduce an ICL framework to examine the impact of synthetic and real images and captions as few-shot demonstrations. Our results show improved performance across diverse compositional understanding benchmarks, both when using synthetic and real images. This suggests potential benefits from using task-specific, few-shot examples for improving the capabilities of VLMs, such as compositional understanding.\nFuture work To improve compositional understanding, future VLMs could move away from contrastive vision encoders and make use of alternative training objective like patch-level prediction (Oquab et al., 2024; Yun et al., 2022) which has shown improved inter-patch understanding which could be useful for compositional understanding. To achieve similar results Densely Captioned Images have shown positive impact on compositional understanding (Urbanek et al., 2024), with further research possibly leading to substantial improvements. Alternatively, as compositional reasoning can be seen as a form of symbolic reasoning, transformer-based foundation models could be supplemented with logic components. Indeed, recent work has used neurosymbolic grounding to enable compositionally aware world models (Sehgal et al., 2023). As such, improving compositionality could be seen as falling under the larger umbrella of improving the reasoning capabilities of (multi-modal) foundation models, which might require more explicit symbolic components or finding non-symbolic architectures that can exhibit stronger machine cognition characteristics."}, {"title": "B. Method", "content": "Below we state the ICL prompting strategy used in our experiments,\nUSER: Does the image match the caption?\nA. <CaptionA>\nB. <CaptionB>\n<image1>. The correct caption is: A/B\n...\n(We repeat the above 5 times for 5-shot in-context learning)\n...\nUSER: Similarly, given an image and two captions choose the correct caption. Think step-by-step and analyze the captions against the image. Begin by describing the key elements visible in the image. Then, compare these elements with the details mentioned in the captions. Clearly state your final answer only in a single character, either A or B.\n<image>. The caption is:\nA. <CaptionA>\nB. <CaptionB>\nASSISTANT:\nThe prompting strategy used to generate the wrong caption corresponding to the correct one using GPT-4o is as below,\nGenerate counter caption to this one, with the same objects in a different position/attribute: 'correct caption'."}, {"title": "C. Appendix Experiments", "content": "The ICL prompting strategy used in SugarCrepe and ARO evaluation is as follows,\nUSER: <image> Given this image and two candidate captions (A and B), which caption is the better description of the given image? Clearly state your final answer only in a single character, either A or B.\nA. <CaptionA>\nB. <CaptionB>\nThe ICL prompting strategy used in Winoground evaluation is as follows,\nAfter providing a brief explanation of your reasoning, clearly state your final answer as <Yes> or <No>."}, {"title": "D. Pipeline details", "content": "D.1. Contrastive evaluation pipeline\nARO and SugarCrepe For contrastive models, we evaluate ARO and SugarCrepe by first taking the positive and negative captions embeddings and comparing both with the embeddings of the image. We do this by computing the cosine similarity between each caption and the image embedding and increasing the number of correct predictions when the positive caption-image score is higher than the negative caption-image score. We adapt the code 2 by (Y\u00fcksekg\u00f6n\u00fcl et al., 2023).\nWinoground For the Winoground benchmark, we follow (Ilharco et al., 2021) and perform a text and image encoding, for each image-caption pair. This results in two image feature representations and two caption feature representations. The final scores are then calculated by taking the cosine similarity score between the representations. This returns the real-valued outputs, which are then used to determine the text-image-group scores.\nD.2. Generative evaluation pipeline\nARO and SugarCrepe For generative models, we evaluate ARO and SugarCrepe zero-shot by prompting the models using the ICL method shown in Appendix C. We then check the output of the model and increase the number of correct predictions if the model picks the correct caption choice. For 1-shot and 5-shot in-context learning, we use the prompt mentioned in Appendix B.\nWinoground For the Winoground benchmark, we use a separate final instruction in the previous prompts as stated in Appendix C. If a \"yes\" character is found in the output, then the result of that corresponding pair is set to 1, if not it is set to 0. However, this evaluation strategy causes two major issues. First, the output is not always the same. Variations in the outputs result in both categorizing a correct caption as wrong if \u201cyes\u201d is never predicted and vice-versa if the predicted \u201cyes\u201d is not relating to the caption entailing the image/or choice but rather something else. To quantify this, consider the probability distribution $P(t)$ of token $t \\in V$ (Vocabulary) across the sequence length $s$, derived from the logits $L$ using the softmax function. Even if $P(t)$ is high, $t$ might not be generated if another token has a higher probability. Secondly, given the binary value of 0/1, evaluating generative models on Winoground using the previous method results in having text, image, group scores to be all equal. To mitigate the aforementioned issues, we propose an alternative that relies on using the output logits of the desired word for evaluation. In this method, we first take the logits output tensor $L \\in R^{B \\times S \\times V}$, where $B$ is the batch size (equal to 1 in this instance), $S$ is sequence length and $V$ is the vocabulary size. We take the token id \u201cyes\u201d (denoted as $id_{yes}$) in the third dimension, and compute the mean over the sequence length, $L_{yes} = \\frac{1}{S} \\sum_{s=1}^{S} L_{s,id_{yes}}$.\nThis results in a real-valued number $L_{yes} \\in R$, one per each caption-image pair given as input to the model. These values will then be compared in the same way as we do in contrastive evaluation to obtain the three accuracy scores. This technique is beneficial over the first one because it does not directly rely on generation, rather it focuses on the amount of \u201cconfidence\u201d the model had about a specific token throughout the whole generated sequence."}]}