{"title": "A SYSTEMATIC APPROACH FOR ASSESSING LARGE LANGUAGE MODEL'S TEST CASE GENERATION CAPABILITY", "authors": ["Hung-Fu Chang", "Mohammad Shokrolah Shirazi"], "abstract": "Software testing ensures the quality and reliability of software products, but manual test case creation is labor-intensive. With the rise of large language models (LLMs), there is growing interest in unit test creation with LLMs. However, effective assessment of LLM-generated test cases is limited by the lack of standardized benchmarks that comprehensively cover diverse programming scenarios. To address the assessment of LLM's test case generation ability and lacking dataset for evaluation, we propose the Generated Benchmark from Control-Flow Structure and Variable Usage Composition (GBCV) approach, which systematically generates programs used for evaluating LLMs' test generation capabilities. By leveraging basic control-flow structures and variable usage, GBCV provides a flexible framework to create a spectrum of programs ranging from simple to complex. Because GPT-40 and GPT-3-Turbo are publicly accessible models, to present real-world regular user's use case, we use GBCV to assess LLM performance on them. Our findings indicate that GPT-40 performs better on complex program structures, while all models effectively detect boundary values in simple conditions but face challenges with arithmetic computations. This study highlights the strengths and limitations of LLMs in test generation, provides a benchmark framework, and suggests directions for future improvement.", "sections": [{"title": "1 Introduction", "content": "Software testing is an important component of the software development lifecycle because the practice ensures the quality and functionality of software products [1]. It is among the most resource-intensive aspects of software development, playing a critical role in validating that software systems meet established requirements and operate without faults [2, 3]. As a result, developers employ various rigorous testing to identify bugs to significantly reduces the risk of costly defects in production environments [2-4]. Among tests like integration test, acceptance test, regression test etc. in the software development, unit test can be regard as the most essential and widely used practices, where test cases are written to validate specific methods or components of the source code in isolation. This focused approach facilitates the early identification of defects at their source, minimizing their potential impact on the broader software system [5, 6].\nSince manual unit test case creation is a time-consuming process, consuming over 15% of a developer's time on average [7], automating test case generation from codes or requirements has become an area motivating extensive research. Various generation approaches have been suggested, as outlined in review and survey studies [8, 9]. Practical application of automatic test case generation was also explored in past research [10]. Due to the emergence of Large Language Models (LLMs), many scholars and industrial practitioners feel the significant potential of LLMs in automating software development tasks. Their ability to learn from extensive datasets and produce contextual codes to form relevant function or programs. Therefore, traditional automating coding-related tasks, such as code generation, test case creation, test execution, and code recommendation, transforms their applications or studies into possible integration with LLMs [11-14]. That is, with their remarkable performance in generation and inference tasks, researchers can explore new"}, {"title": "2 Related Work", "content": "Our research aligns with two closely related areas: the utilization of Large Language Models (LLMs) or natural language processing (NLP) techniques for automated test case generation and the development of benchmark datasets to evaluate LLM performance in generating computing artifacts."}, {"title": "2.1 Test Cases Generation via LLMS", "content": "Test cases must be generated from certain software artifacts, which specified the inputs or expected outputs of the test cases [8]. Traditionally, according to the types of artifacts used, test case generation techniques fall into several categories, such as symbolic execution [18], search-based and evolutionary approaches [19, 20]. These approaches rely on static or dynamic analysis techniques to explore a program's control and data flow paths. These typically result in test cases that are less readable and understandable than manually written test [21] and may either lack assertions or include only generic ones [22]. With the advancement of LLMs, researchers have explored their potential to automate certain software artifacts like codes, which has sparked interest in integrating LLMs into software testing, particularly for test case generation. The growing attention on LLM-based test case generation aims to improve efficiency, enable self-learning in test creation, or produce more natural-looking tests [23].\nOne way to use LLMs in test case generation is by emphasizing prompt creation. Yuan et al. [12] proposed a framework, ChatTester, which iteratively refines test cases by asking ChatGPT to generate follow-up prompts. This method resulted in a 34.3% increase in compilable tests and an 18.7% improvement in tests with correct assertions, demonstrating the potential of LLMs in enhancing test generation. Sch\u00e4fer et al. [23] introduced TestPilot, a tool automates unit"}, {"title": "2.2 Benchmark for Evaluating LLMs", "content": "Research on creating benchmark datasets for LLMs to generate test cases is limited. One recent proposal, TESTEVAL [17], collected 210 Python programs from the online coding site LeetCodes. However, the benchmark is limited to the collected Python programs which restrict its generalization or extension to other programming languages or use patterns. To better understand how to create benchmarks for LLM's capabilities in generating code-related artifacts, we explored two close fields: text-to-SQL translation and code generation.\nLi et al. [16] in 2023 introduced the BIRD dataset, a large-scale benchmark designed to assess LLMs' ability to parse text-to-SQL queries across diverse domains. Similarly, Wang et al. (2022) [29] developed the UNITE benchmark, consolidates 18 publicly available text-to-SQL datasets, creating a unified resource for evaluation. UNITE comprises over 120,000 examples from more than 12 domains, 29,000 databases, and 3,900 SQL query patterns.\nFor benchmarking code generation and evaluation, HumanEval, proposed by Chen et al. in 2021 [30], was a carefully crafted benchmark consisting of 164 programming challenges. Each challenge includes a function signature, docstring, body, and multiple unit tests. Liu et al. [31] addressed the limitations of existing benchmarks in evaluating the functional correctness of code generated by LLMs, such as ChatGPT. They identified that many benchmarks rely on insufficient and low-quality test cases, leading to an overestimation of LLM performance. To tackle this, they introduce EvalPlus, a framework that augments a significant number of additional test cases using both LLM-based and mutation-based strategies. Yan et al. [32] presented CodeScope, a comprehensive benchmark for evaluating LLMs in code understanding and generation across multilingual, multi-task, and multidimensional scenarios. CodeScope covered 43 programming languages and eight tasks, including code summarization, repair, and optimization."}, {"title": "2.3 Summary", "content": "Past research highlights the importance of benchmark datasets for training and evaluating LLM's abilities to generate computing artifacts. However, existing benchmark dataset for test case generation is highly limited, lacking flexibility for extension to other programming languages or use cases. Most existing LLM-based test generation techniques focus on prompt crafting and additional documents or data for generating tests. Additionally, they were also domain-specific and lack generalizable evaluation methods.\nThis raise to three key questions:\n1. Can we develop a flexible benchmark dataset?\n2. Can users only use a single prompt with merely source codes to LLM to generate test cases?\n3. How well can LLMs generate test cases while inputting solely source codes?\nTo address these gaps, there is a need for a new, more adaptable dataset. To simulate real-world scenarios where users enter codes directly, without supplementary data like documentation, and request LLM's test case generation though a single prompt. To achieve this, we propose GBCV, an approach designed to create is extendable benchmark dataset and facilitate studies on LLM's test case generation ability with minimal input."}, {"title": "3 Method", "content": "Our GBCV approach begins by using fundamental Control Flow Graph (CFG) structures, integrated with data-flow analysis techniques, specifically focusing on predicate uses (p-uses) and computation uses (c-uses) for variable usage. By combining CFG with data-flow analysis, we generate diverse program variants that contain different structures"}, {"title": "3.1 Overall Process", "content": "The GBCV approach follows a systematic procedure to automatically generate a set of programs, served as a benchmark used to evaluate the test case generation capabilities of LLMs. The process consists of three main phases (see Figure 1). The first phase focuses on benchmark creation, where programs (code) are generated to form a benchmark dataset used to prompt an LLM to generate test cases. The second phase is for test case generation. Each prompt is used to request the LLM to generate test cases. In last phase, the generated test cases (e.g., $TestCase_{i,m,k}$ in Figure 1) are used to test the corresponding programs (e.g., $Codes_{i,m}$ in Figure 1) with the expectation that the program will pass all the test cases. In this way, we can investigate whether the LLM can produce valid and correct test cases.\nThe GBCV approach begins by specifying the basic program structures, such as branches and sequences, which serve as a building block to form a program. The CFG specifies the template which will be filled by partial or complete statements for the code generation. The basic structures can be incrementally added to build up into more complex combinations based on the desired level of investigation, which is a stopping point of structure creation.\nWithin the templates, placeholders are replaced with actual statements (e.g., c-use) or partial statements (e.g., p-use) to generate a complete program (see Figure 2). The p-use and c-use statements are designed based on several factors: the number of computations, the types of computations, and the specific decisions in a branch the evaluators plan to assess. They help in understanding how variables are utilized within the program's logic. One can use a complicated combination of conditions to test LLM's logical reasoning capabilities. Hence, adding statements also increase the overall complexity of the programs.\nWhen specifying templates and statements, language-specific details must be considered. For example, ava and Python use different syntax for combining predicates. Java employs x>4 || x <10, while Python uses x>4 or x<10. Therefore, to apply GBCV across different languages, the same abstractions - CFG and placeholder locations can be reused, but language-specific details are put while formatting the template and statements.\nFinally, to assess the LLM's test case generation, the metrics are calculated against the test outputs across different types of structure and computations. The users can ensure a comprehensive understanding of the model's capabilities and limitations."}, {"title": "3.2 Guiding Principles of Benchmark Creation", "content": "The GBCV is a bottom-up approach to generate programs for assessing the LLM. Several factors decide the final generated programs. We describe them in the following Table 1.\nThe complexity levels leverage a combination of cyclomatic complexity [33], cognitive complexity [34, 35], and the reasoning behaviors of the LLM. Both cyclomatic and cognitive complexity account for the code structure (e.g., CFG) and decision points. Specifically, cyclomatic complexity is categorized into four levels: low (1\u20134), moderate (5-7), high (8-10), and very high (above 10, typically recommended for refactoring). Unlike cyclomatic complexity, cognitive complexity differentiates predicates and program structures and considers logical operators in evaluation. When assessing an LLM's reasoning behaviors, we consider factors such as the number of statements, predicates, and datatypes. The considerations about the number of statements and predicates are drawn from prior research on chain-of-thought reasoning [36]. The datatype selection leverages the insights from previous studies on type-aware reasoning [31, 37].\nOnce the complexity level and the specific LLM behaviors of interest are identified, the appropriate CFG and statements can be selected accordingly. The selection also depends on the intended coverage range. For instance, if the objective is to assess whether an LLM can recognize a sequence of computations, incorporating multiple c-use nodes is necessary. Likewise, if datatype awareness is important, incorporating diverse or complex datatypes should be considered."}, {"title": "3.3 Dataset Creation", "content": "Since the programs in the benchmark dataset for evaluating LLMs' test case generation ability are created based on the guiding principles of the GBCV approach, the number and types of generated programs dynamically adjust according to the user's specified assessment goals. Given that the primary objective of this study is to introduce the GBCV approach and gain an initial understanding of LLMs' test case generation capabilities as a foundation for future research on more complex or real-world programs, our assessments focus on relatively simple structures, data types, and arithmetic computations. Therefore, we intentionally select structures, data types, and computation levels between low to middle complexity.\nThe following discusses the investigation goals in this study. Figure 3 and Figure 4 illustrate the control flow structures and set of nodes defined we used based on our investigation goals.\n1. Boundary and comparison\nThis evaluation assesses the LLM's ability to perform logical operations and comparisons. The p-use of a variable in a conditional statement, such as 'x > 10', checks whether the LLM can correctly identify the boundary value (i.e., 10), resulting in true/false branches when the comparison clause contains constant values. Successful boundary value detection has two key characteristics: First, the input value chosen in a test case matches the boundary value (e.g., an input value of 10 for 'x > 10'). Second, the chosen input values can lead to both true and false execution paths, depending on the comparison.\nTherefore, according to the low to middle complexity levels of the type of predicates and numbers of predicates, and low complexity for datatype, we use two variables to form compound predicates and the datatypes and values for comparison are integers."}, {"title": "4 Result and Discussion", "content": "The dataset we used is not extracted from any existing repository. A total of 786 Python programs were all generated using seven specified types of control flow structures: Branch, Loop, Nested Loop, Sequence, Sequential Branch, Sequential Branch with Else, and Sequential Loop. Table 2 presents the average complexity of the generated programs within a category by using the average source lines of code (SLOC) and illustrates the coverage of each program type as a percentage of the total generated programs. The program that includes at most two variables can be used to assess the models' performance in handling multiple predicates or computation. Sequential Branch and Sequential Branch with Else contains various combinations of c-use nodes and compound predicates, their coverages are relative higher than other categories. To simulate how regular users use prompts to ask for test cases for a program from an LLM as the initial investigation done by our generated benchmark, we used the generated programs to evaluate three language models: GPT-3-Turbo, GPT-40-mini, and GPT-40, rather than LLMs specifically trained for coding.\nTable 3 presents the evaluation results, including the total number of test cases generated by each model, the number of complete test cases, and the percentage of incomplete test cases calculated by Equation (4). Among the models, GPT-3-Turbo had the highest rate of incomplete test cases at 32.74%, making it the least effective. GPT-40-mini"}, {"title": "4.1 Result", "content": "The dataset we used is not extracted from any existing repository. A total of 786 Python programs were all generated using seven specified types of control flow structures: Branch, Loop, Nested Loop, Sequence, Sequential Branch, Sequential Branch with Else, and Sequential Loop. Table 2 presents the average complexity of the generated programs within a category by using the average source lines of code (SLOC) and illustrates the coverage of each program type as a percentage of the total generated programs. The program that includes at most two variables can be used to assess the models' performance in handling multiple predicates or computation. Sequential Branch and Sequential Branch with Else contains various combinations of c-use nodes and compound predicates, their coverages are relative higher than other categories. To simulate how regular users use prompts to ask for test cases for a program from an LLM as the initial investigation done by our generated benchmark, we used the generated programs to evaluate three language models: GPT-3-Turbo, GPT-40-mini, and GPT-40, rather than LLMs specifically trained for coding.\nTable 3 presents the evaluation results, including the total number of test cases generated by each model, the number of complete test cases, and the percentage of incomplete test cases calculated by Equation (4). Among the models, GPT-3-Turbo had the highest rate of incomplete test cases at 32.74%, making it the least effective. GPT-40-mini"}, {"title": "4.2 Discussion", "content": "Our approach effectively and automatically evaluates the LLM's test case generation capabilities based on defined metrics, providing flexibility to extend program structures from simple to complex. This flexibility allows us to discover common behaviors, such as iteration, across different program types, providing deeper insights into how LLMs handle similar operations or usage among programs with various complexities. The automated evaluation identifies areas where the models excel and where they encounter poor performance, while a qualitative assessment, done by the manual process, provides a detailed analysis of the models' outputs. This combined method ensures that we can assess both the accuracy and the reasoning behind the generated test cases, offering a comprehensive understanding of the strengths and weaknesses of different LLMs. The following sections present our findings from analyzing the responses of different language models in detail."}, {"title": "4.2.1 Simple vs Composite Structure", "content": "Our analysis revealed that GPT-40 and GPT-40-mini exhibit different strengths when handling program structures of varying complexity. GPT-40 performs better with composite structures, whereas GPT-40-mini excels with simpler ones (see average error rate in Table 4). GPT-40 appears to enhance the ability to understand composite programs but tends to generate more erroneous test cases when dealing with simple structures. This is evident from the higher rate of incorrect test cases produced by GPT-40. Furthermore, compared to GPT-40-mini when generating test cases, GPT-40 is more likely to provide detailed explanations in its responses, which can also introduce inaccuracies if the reasoning is flawed. This suggests that the level of detail in GPT-40's responses may contribute to its higher error rate in simpler programs.\nThe high rates of untestable programs suggest that GPT-4o has challenges interpreting certain programs, resulting in situations where expected outcomes are absent. Despite these challenges, the GPT-40 and GPT-40-mini models outperform GPT-3-Turbo, particularly when dealing with composite program structures. In the \"Sequential Branch\" category (see Table 5), the performance gap between the GPT-40 family models and GPT-3-Turbo is particularly evident, highlighting the enhanced ability of GPT-40 family in managing complex control flows. This indicates that while GPT-40 has a strong capacity for dealing with complexity, there is still room for improvement in handling simpler scenarios and reducing the rates of untestable cases."}, {"title": "4.2.2 Computation", "content": "Doing computational tasks or problems still remains a challenge for test case generation as previous studies have reported poor interpretation on arithmetic computation in earlier GPT models, despite improvements in later versions [38]. Our evaluation found that similar computational difficulties are present in test case results for the computation nodes (c-use) within generated programs. This indicates arithmetic-related inaccuracies are still prevalent. This suggests that further refinement is needed for improvement in generating test cases for programs involving arithmetic computations.\nSpecifically, programs involving the c-use of two variables, such as 'x = x + y + 5', often lead to incorrect expected results in the generated test cases, regardless of the overall program complexity. Furthermore, scenarios that include multiple c-use nodes within composite structures, such as Nested Loops or Sequential Loops, can further amplify these computational challenges during test case generation. In summary, computational challenges are common across all program types. Addressing these issues will be crucial to enhancing the overall robustness of LLM-generated test cases, particularly for programs with heavy arithmetic operations or composite structures."}, {"title": "4.2.3 Boundary and Comparison", "content": "We observed that all language models perform relatively well in detecting the boundary values of a variable within simple conditional statements (e.g., x > 5 or y < 10). LLMs were capable of selecting the appropriate integer values to identify boundaries, even when handling more complex structures. For instance, Table 6 presents an example of test cases generated by all models in the Sequential Branch category. It is worth noting that many successful boundary detections occur in conditional statements that do not involve any computation (e.g., x + y > 5)."}, {"title": "4.2.4 Iteration", "content": "All the language models demonstrate relatively high error rates and high untestable program rates in all the loop-like structures. These outcomes indicate that LLMs face challenges in handling iterations, particularly in programs that require continuous computation. Given that arithmetic and computation are already challenging for these models, it is not surprising to see poor performance when loops are introduced. Iterations inherently require repeated computations and maintaining state over multiple cycles, which complicates the task for language models.\nFigure 8 illustrates that the GPT 3.5-Turbo correctly knows three iterations should result in an increase of 30 at the end of the loop. However, after variable \"y\" is added to the statement, GPT 3.5-Turbo fails to interpret the variable \u201cy\u201d within the computation statement. The expected output becomes incorrect. This suggests a difficulty in integrating new variables into iterative logic consistently.\nFigure 9 shows the test case generated by GPT-40, which is the same program shown on the right-hand side of the Figure 8. GPT-40 can perform the correct computation at each iteration but the expected result (i.e., assert result value) is wrong. This discrepancy indicates that GPT-40 may separate generative mechanisms one for performing arithmetic operations (i.e., addition) and another for generating the expected output. While it handles the arithmetic correctly, there is a disconnect when it comes to aligning the output with what is actually computed. This inconsistency suggests that the model's internal representation for computation and output generation may not be fully integrated.\nOn the other hand, GPT-40-mini shows mixed performance. It produces only partially correct test cases across all its generated cases. This suggests that GPT-40-mini can correctly handle some aspects of iteration but may fail when multiple variables and computations are involved. The inconsistency in its output highlights the ongoing challenges in handling iterations effectively, particularly in programs with more complex looping and variable interactions.\nThese findings suggest that, despite improvements, iterations involving repeated computations remain a significant obstacle for LLMs. Improving how these models handle state changes, variable usages, and multi-step calculations within loops will be crucial for advancing their test case generation capabilities."}, {"title": "5 Conclusion", "content": "Our GBCV approach remedies the critical gap of lacking a benchmark dataset for evaluating the test case generation capabilities of LLMs. By systematically assembling basic control flow blocks combined with variable usage, we were able to use a wide range of various programs to comprehensively evaluate LLM's performance in four key areas: computation, boundary detection, comparison, and iteration. This approach provides a flexible, automated method to assess the correctness and effectiveness of LLMs when generating test cases for a diverse set of program structures.\nIn summary, GPT-40 and GPT-40-mini outperformed GPT 3.5-Turbo, especially in handling more complex program structures. However, significant challenges remain, particularly in generating correct test cases for programs involving arithmetic computations and iterations. Handling computation still is one of the primary obstacles across all models."}, {"title": "6 Future Work", "content": "Our goal is to improve the ability of LLMs to generate correct and comprehensive test cases, thereby making them more useful for software testing tasks in increasingly complex and various scenarios. For future work, we plan to focus on enhancing LLM test case generation capabilities through several strategies. First, we will explore prompt engineering to optimize how prompts are structured and to better guide LLMs in generating more accurate test cases. Producing no valid test cases for a program (i.e., untestable program cases) also influences the LLM's generation ability. We will further investigate why this case happens. Additionally, step-wise refinement will be added to incrementally improve test case generation, allowing LLMs to progressively enhance their understanding and output.\nAnother focus will be on creating more sophisticated metrics and broader benchmark dataset. Regarding sophisticated metrics, we can add branch and statement coverages to examine various execution paths. Different test case qualities should also be applied to understand the quality of LLM's generation ability, such as robustness or execution efficiency. While creating a benchmark with wider coverage, we will extend to high complexity level choices so covering multiple control flow structures, integrating diverse comparisons, including additional logical operations, using more than three variables should be considered. To generalize our discoveries, we will extend the GBCV to other LLMs, particularly, code generation models like CodeT5 or Codex will be investigated to understand our findings are common behaviors."}, {"title": "4.  Result and Discussion", "content": "1  \n1  $E_p = (N_{t,p} - N_{s,p})/N_{t,p}$2\n1  $AvgE_i = \\Sigma E_p/TP$2  \n$R_i = W_i/TP_i$ \n1  $TR_i = IT_i/TT_i = (TT_i \u2013 CT_i)/TT_i$2\n1  $AvgTN_i = TN_i/TP_i$2"}, {"title": "4.1 Result", "content": "The dataset we used is not extracted from any existing repository. A total of 786 Python programs were all generated \nusing seven specified types of control flow structures: Branch, Loop, Nested Loop, Sequence, Sequential Branch, \nSequential Branch with Else, and Sequential Loop. Table 2 presents the average complexity of the generated programs \nwithin a category by using the average source lines of code (SLOC) and illustrates the coverage of each program type as \na percentage of the total generated programs. The program that includes at most two variables can be used to assess \nthe models\u2019 performance in handling multiple predicates or computation. Sequential Branch and Sequential Branch \nwith Else contains various combinations of c-use nodes and compound predicates, their coverages are relative higher \nthan other categories. To simulate how regular users use prompts to ask for test cases for a program from an LLM as \nthe initial investigation done by our generated benchmark, we used the generated programs to evaluate three language \nmodels: GPT-3-Turbo, GPT-4o-mini, and GPT-4o, rather than LLMs specifically trained for coding. \nTable 3 presents the evaluation results, including the total number of test cases generated by each model, the number \nof complete test cases, and the percentage of incomplete test cases calculated by Equation (4). Among the models, \nGPT-3-Turbo had the highest rate of incomplete test cases at 32.74%, making it the least effective. GPT-4o-mini"}]}