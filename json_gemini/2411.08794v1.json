{"title": "Evaluating World Models with LLM for Decision Making", "authors": ["Chang Yang", "Xinrun Wang", "Junzhe Jiang", "Qinggang Zhang", "Xiao Huang"], "abstract": "World model emerges as a key module in decision making, where MuZero and Dreamer\nachieve remarkable successes in complex tasks. Recent work leverages Large Language Mod-\nels (LLMs) as general world simulators to simulate the dynamics of the world due to their\ngeneralizability. LLMs also serve as the world model for deliberative reasoning in Reasoning\nvia Planning (RAP) and Tree of Thought (ToT). However, the world models are either eval-\nuated as a general world simulator, or as a functional module of the agent, i.e., predicting\nthe transitions to assist the planning. In this work, we propose a comprehensive evaluation\nof the world models with LLMs from the decision making perspective. Specifically, we lever-\nage the 31 diverse environments from (Wang et al., 2023; 2024) and curate the rule-based\npolicy of each environment for the diverse evaluation. Then, we design three main tasks,\ni.e., policy verification, action proposal, and policy planning, where the world models\ncan be used for decision making solely. Finally, we conduct the comprehensive evaluation\nof the advanced LLMs, i.e., GPT-40 and GPT-40-mini, on the environments for the three\nmain tasks under various settings. The key observations include: i) GPT-40 significantly\noutperforms GPT-40-mini on the three main tasks, especially for the tasks which require\nthe domain knowledge, ii) the performance of the world model with LLM will be decreased\nfor long-term decision-making tasks, and iii) the combination of different functionalities of\nthe world model will brings additional unstabilities of the performance.", "sections": [{"title": "1 Introduction", "content": "Due to the celebrating success of MuZero (Schrittwieser et al., 2020) and Dreamer (Hafner et al., 2019;\n2021; 2023), world model (Ha & Schmidhuber, 2018) becomes a key concept in decision making. With the\nencoding vast knowledge of the world for the prediction of the new states after taking the actions, world\nmodels have demonstrate their effectiveness for generalizing to novel tasks (Byravan et al., 2020), efficient\nplanning with world model (Sekar et al., 2020), and working on offline datasets (Schrittwieser et al., 2021;\nYu et al., 2020; 2021). World models can also be viewed as general world simulators where users can interact\nwith, e.g., Genie (Bruce et al., 2024) and Vista (Gao et al., 2024).\nLarge Language Models (LLMs) achieve remarkable success in enormous natural language tasks in the past\nfive years (Brown et al., 2020; OpenAI, 2023). Several recent works leverage LLMs as the generalist world\nmodels to provide the environment knowledge for various complex tasks, e.g., math and reasoning. With"}, {"title": "2 Related Work", "content": "In this work, we provide a detail review of the related work, which is divided into three main categories, i.e.,\nworld models in decision making, LLMs as world simulators, world models in LLMs.\nWorld Models in Decision Making. World models are actively explored by researchers to further\nimprove the agent's performance and the sample efficiency (Ha & Schmidhuber, 2018; Janner et al., 2019;\nHafner et al., 2019; Schrittwieser et al., 2020). Dreamer (Hafner et al., 2019) is a practical model-based\nreinforcement learning algorithm that introduces the belief over states as a part of the input to the model-\nfree DRL algorithm used. MuZero (Schrittwieser et al., 2020) is a remarkable success of model-based RL,\nwhich learns the world model and conduct the planning in the latent space. MuZero achieves the superior\nperformances over other model-based and model-free RL methods. The world models trained in these\nmethods are problem-specific and cannot be generalized to other problems, which motivates researchers to\nseek to more generalizable world models, e.g., LLMs as world models. The world model with LLM in (Xiang\net al., 2023) is trained to gain the environment knowledge, while maintaining other capabilities of the LLMs.\nDynalang (Lin et al., 2024) proposes the multimodal world model, which unifies the videos and texts for the\nfuture prediction in decision making.\nLLMs as World Simulators. World simulators are developed to model the dynamics of the world (Bruce\net al., 2024). LLMs serve as the world simulator due to their generalizability across tasks. Specifically, The\nLLMs (i.e., GPT-3.5 and GPT-4) is evaluated to predict the state transitions, the game progress and scores\nwith the given object, action, and score rules, where these rules are demonstrated to be crucial to the world\nmodel predictions (Wang et al., 2024). The world models with LLMs in (Xie et al., 2024) need to additionally\nidentify the valid actions. We move a step further to ask the world model to propose the potential actions\nto complete the tasks (Observation 2). Both methods mainly focus on the prediction of the state, which\nmay be not suitable for the evaluation of the world model for decision making (Observation 1).\nWorld Models in LLMs. The concept of world model also be explored in the deliberation reasoning of\nLLMs. Specifically, Reasoning via Planning (RAP) (Hao et al., 2023) leverage the planning methods (e.g.,\nMonte Carlo Tree Search (MCTS)) with the world model with LLMs for plan generation and math reasoning,\nwhere LLMs need to predict the next state and the reward to guide the search. Tree of Thought (ToT) (Yao\net al., 2023) implicitly leverage the LLMs as the world model to predict the next state and the reward for\nthe search over different thoughts. Reason for future, act for now (RAFA) (Liu et al., 2023) combine the\nplanning and reflection with the world model for complex reasoning tasks. However, these methods do not"}, {"title": "3 Preliminaries", "content": "In this section, we will first introduce the preliminaries in decision making, the world model, search methods,\nand the recent practices that using LLM for decision making problems.\nMarkov Decision Process (MDP). A decision making problem is usually represented as a Markov\ndecision process (MDP) (Sutton & Barto, 2018), which is defined by the tuple $(S, A, T, R, \\gamma)$, where S is the\nstate space, A is the action space, $T : S \\times A \\rightarrow S$ is the transition dynamics, which specifies the next state\ns' given the current state s and action a, $R:S\\times A\\rightarrow R$ is the reward function, which specifies the agent's\nreward given the current state s and action a, and $\\gamma$ is the discount factor. The agent's policy is defined by\n$\\pi_{\\theta} : S \\times A \\rightarrow [0, 1]$, parameterized by $\\theta$, which takes the state s as the input and outputs the action a to be\nexecuted. The objective of the agent is to learn an optimal policy $\\pi^* := arg \\max E_{\\pi} [\\sum_{t=0}^{\\infty} \\gamma^t r_t|s_0]$ is the\nexpected return and $s_0$ is the initial state.\nLarge Language Models (LLMs). Large Language models (LLMs) learn from text data using unsu-\npervised learning. LLMs optimize the joint probabilities of variable-length symbol sequences as the product\nof conditional probabilities by $P(x) = \\Pi_{i=1}^{n}P(s_i|s_1, ..., s_{i-1})$, where $(s_1, s_2, ..., s_n)$ is the variable-length se-\nquence of symbols. LLMs with billions of parameters have demonstrated impressive capabilities in various\nNLP tasks (Touvron et al., 2023; OpenAI, 2023).\nWorld Models. The world model is introduced to predict the dynamics of the environment, thus\nsupporting the decision making process. Specifically, the world model is trained or prompted to predict the\nnext state s', the reward r, and the terminal function d, given the current states and action a. The world\nmodel can be a neural network specially trained on the environments (Hafner et al., 2019; Schrittwieser\net al., 2020), which cannot generalize across different environments. Therefore, world models with LLMs\nemerge as a promising methods due to the generalizbility of LLMs, where the prompting (Xie et al., 2024),\nin-context learning (Wang et al., 2024), and even fine-tuning methods (Xiang et al., 2023; Lin et al., 2024)\nare used to transform the LLMs to the world models."}, {"title": "4 World Models with LLMs for Decision Making", "content": "In this section, we introduce the world model with\nLLM for decision making. Specifically, we will in-\ntroduce the next state prediction, the reward and\nterminal prediction. Then, we will introduce how\nthe world model will be used to complete the con-\nsidered three main tasks, i.e., policy verification, ac-\ntion proposal, and policy planning. We provide the\nrelationship between the three main tasks and the\ntwo kinds of predictions in Figure 3 for better un-\nderstanding of the rationale behind the three tasks.\nThe world model considered in this work mainly fol-\nlows the design in (Wang et al., 2024), where the rep-\nresentation of the state includes the objects in the\nenvironments and their properties. The prompts to\nthe LLM, e.g., GPT-40, also include the object rules,\nthe action rules, and the score rules, which provides"}, {"title": "5 Environments", "content": "Tasks. We leverage the 31 diverse environments from (Wang et al., 2023)\u00b2 with different tasks varying\nfrom daily tasks, e.g., washing clothes, to scientific tasks, e.g., forging keys, and different difficulty levels,\ni.e., steps to complete the tasks. The task suite is more diverse than the widely used environments, e.g.,"}, {"title": "6 Evaluations", "content": "In this section, we present the comprehensive evaluation of the world model for decision making over the\ndiverse 31 environments on the three main tasks under various setttings. We use the advanced LLM models,\ni.e., GPT-40 and GPT-40-mini, as the backbone LLM for the world model. We set the temperature of the\nLLM to be 0 to reduce the variance of the generation and all results are averaged over 30 runs.\n6.1 Task I: Policy Verification\nEvaluation Protocol. Given the action sequence a generated by the rule-based policy, we leverage the\nworld model to verify the last p proportion of the policy, where p\u2208 {0.25, 0.5, 0.75, 1.0}. We note that\nwhen p = 1.0, the world model will verify the full action sequence with only the initial observation of the\nenvironment. We say the verification of the policy is correct if all three features, i.e., gameScore, gameOver,\nand gameWon, are correct. We note that only correct policies are verified, as there are enormous wrong\npolicies for an environment, which is useless for decision making. Furthermore, there would also be other\naction sequences to finish the tasks, where we cannot enumerate all policies to complete the tasks.\nEvaluation Results. The policy verification results are displayed in Figure 5. We observe that GPT-40\noutperform GPT-40-mini in most tasks and especially on the tasks which requires the domain knowledge, e.g.,\nbandage, hang, and campfire. We also observe that with more steps of the verified policies, the performance\ngap between GPT-40 and GPT-40-mini is increase. With larger proportion of the action sequences to verify,\ni.e., p increase, the accuracy of the verification is decreased, which indicates that the accumulation of the\nerrors in the world model, either on the next state prediction or the reward prediction, will influence the\nperformance of the world model. This observation is consistent to the fact that the LLM may not perform well\nin long-term decision making tasks. We also observe that more steps to complete the tasks do not necessarily\nlead to the bad performance, which indicates that the domains of the tasks play a more important roles for\nthe policy verification, i.e., for the tasks where the LLM has enough domain knowledge, the task would be\neasy even when the number of steps is large, e.g., conduct, stone, weigh and photo. We also provide the\naccuracy of the prediction of each feature in Appendix C, and we found that both GPT-40 and GPT-40-\nmini performs wore for predicting the gameScore, while performs much better for predicting gameOver and"}, {"title": "6.2 Task II: Action Proposal", "content": "Evaluation Protocol. The action proposal requires the world model to generate the top-K potential\nactions to complete the tasks, where K \u2208 {1,2,3,5,10}. Specifically, given the action sequence a generated\nby the rule-based policies, we will let the world model to generate the potential actions with the states along\nwith the path of a to complete the task. We say the action proposal is correct if the actions in a in the\ngenerated actions by the world model. The results of the accuracy are averaged over the steps over the\naction sequence and 30 runs of each environment. We also note that the action sequence a generated by\nthe rule-based policy is not the only sequence to complete the task and we cannot enumerate all possible\nactions which can lead to the completion of the task. We note that the number of available actions in the\nenvironments is usually larger than 500, which brings difficulties to the traditional RL methods for training\nand indicate the necessity for the world model to generate the potential actions to facilitate the learning.\nEvaluation Results. The action proposal results are displayed in Figure 6. Overall, GPT-40 consistently\noutperforms GPT-40-mini across different tasks and different values of K. With the number of steps to\ncomplete the tasks, GPT-40 maintains the better accuracy, while GPT-40-mini shows a substantial drop\nof the accuracy. The performance gap between the two models is generally increased when the number of\nsteps to complete the tasks increase. When K = 10, the accuracy of the action proposal for GPT-40 is\nvery high in most tasks, which indicates that the world model is capable to generate the relevant actions for\ncompleting the tasks while ignoring the irrelevant actions. Furthermore, we still observe that both models\nobtain lower values in the tasks requiring the domain knowledge, i.e., blood and conduct, which is consistent\nto the observation in (Wang et al., 2024) that LLMs (GPT-4) is more likely to make errors when scientific\nknowledge is needed. We also provide the step accuracy of the action proposal in Appendix D to illustrate\nthe prediction of the relevant actions along with the steps. We observe that there are some key steps that\nhas extremely low accuracies, which would be the most difficult steps for decision making."}, {"title": "6.3 Task III: Policy Planning", "content": "Evaluation Protocol. The policy planning is based on the policy verification and the action proposal, as\nshowed in Algorithm 2. Similar to the policy verification, we let p\u2208 {0.25, 0.5, 0.75, 1.0} to vary the number\nof steps for the planning. We only consider the case with K = 1, i.e., which generate the top-1 action with\nthe given states predicted by the world model. Finally, we evaluate the planned policy a' in the environment\nto verify the correctness. We note that when K = 1, no advanced search method is needed, while when\nK > 1, we cannot enumerate all possible outcomes for larger steps, e.g., 10. Besides, a critic is also needed\nto choose among the outcomes for verifying in the environments. Therefore, we only consider the case with\nK=1 and leave the case K > 1 into future work.\nEvaluation Results. The policy planning results are displayed in Figure 7, where GPT-40 and GPT-\n40-mini achieve comparable performance for the tasks with less steps and smaller values of p, e.g., 0.25\nand GPT-40 generally achieve better results in tasks with more steps. When the value of p increases, the\nperformance is generally decreasing. With the coupling of the policy verification and action proposal, we\nobserve a more randomness of the performances of models over tasks and settings, which indicates that the\nnecessity of decoupling the modules in decision making for comprehensive evaluation of the world model.\nDuring the experiments, we also observe the format errors of the outputs from both GPT-40 and GPT-40-\nmini, which may interrupt the running of the experiments. Therefore, with the interaction of the different\nfunctionalities of the world model, the system is more unstable due to the unexpected in LLMs' outputs."}, {"title": "7 Conclusions", "content": "World model is a key module for decision making and recent work leverage LLMs as the general world\nmodels. However, the evaluation of the world models with LLMs for decision making is far from satisfactory.\nIn this work, we propose three main tasks, i.e., policy verification, action proposal, and policy planning, to"}, {"title": "A Environments", "content": "A.1 Introduction of Tasks"}, {"title": "A.2 Code for Demo Actions Generation", "content": "Only one playthrough of the game is provided in (Wang et al., 2024), which is not enough due to the\nrandomness in the environments. Therefore, we curate the"}, {"title": "A.3 Analysis of Demo Actions", "content": null}, {"title": "B Prompts for World Model", "content": "B.1 Prompt for Next State and Reward/Terminal Predictions"}, {"title": "B.2 Prompts of Generating Potential Actions.", "content": null}, {"title": "C Accuracy of Verifying Policies", "content": null}, {"title": "D Step Accuracy of Action Proposal", "content": null}]}