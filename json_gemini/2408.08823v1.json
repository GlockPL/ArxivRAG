{"title": "Optimal Symmetries in Binary Classification", "authors": ["Vishal S. Ngairangbam", "Michael Spannowsky"], "abstract": "We explore the role of group symmetries in binary classification tasks, presenting\na novel framework that leverages the principles of Neyman-Pearson optimality.\nContrary to the common intuition that larger symmetry groups lead to improved\nclassification performance, our findings show that selecting the appropriate group\nsymmetries is crucial for optimising generalisation and sample efficiency. We\ndevelop a theoretical foundation for designing group equivariant neural networks\nthat align the choice of symmetries with the underlying probability distributions of\nthe data. Our approach provides a unified methodology for improving classification\naccuracy across a broad range of applications by carefully tailoring the symmetry\ngroup to the specific characteristics of the problem. Theoretical analysis and\nexperimental results demonstrate that optimal classification performance is not\nalways associated with the largest equivariant groups possible in the domain, even\nwhen the likelihood ratio is invariant under one of its proper subgroups, but rather\nwith those subgroups themselves. This work offers insights and practical guidelines\nfor constructing more effective group equivariant architectures in diverse machine-\nlearning contexts.", "sections": [{"title": "Introduction", "content": "Equivariant neural networks, which leverage symmetries inherent in data, have shown significant\npromise in enhancing classification accuracy, data efficiency, and convergence speed. However,\nselecting an appropriate group and defining its actions across the various layers of the network\nremains a complex task, particularly for applications requiring adherence to specific symmetries.\nThis research aims to establish a foundational framework for designing equivariant neural network\narchitectures by utilising stabiliser groups.\nIn the context of equivariant function approximation, a critical insight is that the preimage of a\ntarget element in the output space can be characterised by the subset of the orbit of an input element,\nrestricted to those connected by elements of the stabiliser group of the target. This understanding\nsimplifies the choice of group G and its actions, aligning them with the equivalence class of the target\nfunction's fibres in the input space.\nThis paper provides a theoretical basis for designing equivariant architectures in scientific applica-\ntions where known symmetries play a crucial role. By grounding the architecture design in first\nprinciples, we aim to explain empirical successes observed in data-driven applications and enhance\nthe effectiveness of these architectures in practical scenarios.\nBy quantifying the impact of equivariant architectures on sample efficiency and generalisation error,\nwe offer a comprehensive framework that not only aids in theoretical understanding but also provides\npractical guidelines for implementing these architectures in real-world scientific problems. Through\nsimplified examples and experimental results, we demonstrate the applicability and effectiveness of\nour proposed methods."}, {"title": "Related works", "content": "Group Equivariant Networks: Group equivariant architecture design is ubiquitous in modern\ndeep learning algorithms with Convolutional Neural Networks [2] being equivariant to translations.\nModern generalisations include equivariance to discrete groups [3], and gauge symmetries [4]. It was\nshown in [5] that equivariance with respect to any compact group requires the operation of a layer to\nbe equivalent to a generalised convolution on the group elements.\nFor set-based approaches, the DeepSets framework [6] formulated the universal structure of permuta-\ntion equivariant and invariant functions on elements belonging to a countable universe. Equivariant\nfeature extraction on point clouds for the classifical groups has also been explored in [7, 8, 9].\nGeometric Algebra Transformer [10, 11] have been designed for 3D equivariant extraction of vari-\nous non-trivial transformation groups, including the conformal and projective groups [11] utilising\nClifford group equivariant networks [12].\nTheoretical analysis of equivariant models: On top of the empirical results, several theoretical\nresults exist on the advantages of equivariant models. Under the assumption that the classification task\nis invariant to symmetry transformation, bounds on the generalisation error were derived in [13]. The\nsample complexity for invariant kernel methods is studied in [14]. In the framework of PAC learning,\nit was shown [15] that learning with invariant or equivariant methods reduces to learning over orbit\nrepresentatives. At the same time, the incorporation of appropriate group equivariances [16, 17]\nresults in a strict increase in generalisation. The general intuition from these works is that larger\ngroups result in better sample efficiency and improved generalisation when the target function follows\nthe assumed symmetry. Our work contributes in this direction by defining the notion of a correct\nsymmetry for a binary classification task with a priori known symmetries of the probabilities.\nEquivariance in fundamental physics: The importance of equivariance in fundamental physics\ncannot be overstated, starting from the Galilean transformations in non-relativistic regime to the exotic\nstructure of gauge theories, which form the mathematical backbone of our current understanding of the\nuniverse. Lorentz group equivariance has been explored in [18, 19, 20, 21]. Other group equivariance\nstudies include architectures for simulating QCD on the lattice [22], and the classification of radio\ngalaxies [23]."}, {"title": "Preliminaries", "content": "Fibres of a function: Our work relies on studying the minimal fibre structure induced by symmetries\non the likelihood ratio and its relation to different group equivariant functions. A fibre is formally\ndefined as:\nDefinition 1. Given a function $f : X \\rightarrow Y$, the fibre of an element y in the image of f denoted as\n$f^{-1}(y)$, is the set of all elements of X which gets mapped to y via f:\n$f^{-1}(y) = \\{x \\in X : f(x) = y\\}$\nThe fibres of all element in Im(f) divide X into equivalence classes. This means that the set of fibres\nof any function partitions the domain D into mutually exclusive subsets. This holds true for any\nequivalence class in any given set, i.e. any element $x \\in X$ can only belong to a single equivalence\nclass. We utilise this basic fact implicitly in different parts of the work.\nGroup actions: The main idea underlying symmetries is the action of groups on an underlying set.\nGroup actions and some related terminologies are defined as follows.\nDefinition 2. Given a group G and a set X, the left-action of G on X is a map $A : G \\times X \\rightarrow X$ with\nthe following properties:\n1.  $A(e, X) = X$ for the identity element $e \\in G$ and any $X \\in X$, and\n2.  $A(g_1, A(g_2, X)) = A(g_1g_2, X)$ for all $g_1, g_2 \\in G$ and $X \\in X$.\nA G-action will be denoted as (G, A(g, X)). Any set always admits the trivial action where\n$A(g, X) = X$ for any group element g and all elements X of the set X. Consequently, even\nfor the same group G, one can have two actions $A_1(g, X)$ and $A_2(g, X)$ acting on the same set X,\nwhich are not necessarily the same functions. This motivates defining the equality of two G-actions\nas follows.\nDefinition 3. Two G-actions $(G_1, A_1(g, X))$ and $(G_2, A_2(g, X))$ acting on the same set $X \\exists X$ are\nequal if and only if $G_1$ is isomorphic to $G_2$, via an invertible map $i : G_1 \\rightarrow G_2$ and $A_1(g, X) =$\n$A_2(i(g), X)$ for all $g \\in G_1$ and $X \\in X$.\nDefinition 4. The orbit of an element $X \\in X$ under the group action $A(g, X)$ of the group G, is the\nset of all elements $X' \\in X$ for which there is a group element $g \\in G$ such that $X' = A(g, X)$.\nWe will write the orbit of a G-action characterised by a vector of group invariants I as\n$\\Omega^{\\mathbb{G}}(I) = \\{X' = A(g, X) : \\{X, X'\\} \\subset X \\text{ and } g \\in G\\}$\n(1)\nThe set of all distinct orbits divides X into equivalence classes.\nDefinition 5. The stabiliser group of an element $X \\in X$ with respect to the G-action A, is the set of\nall group elements g which leave X invariant. Mathematically, we will denote it as\n$L_A(X) = \\{g \\in G : X = A(g, X)\\} \\subseteq G$\nA group representation $\\rho : G \\rightarrow GL(m, \\mathbb{R})$, naturally induces a linear action $A : G \\times \\mathbb{R}^m \\rightarrow \\mathbb{R}^m$\non $\\mathbb{R}^m$ of the form $A_{\\rho}(g, x) = \\rho(g) x$. In the rest of the paper, we will be primarily concerned with\nsuch actions of a given transformation group on $\\mathbb{R}^m$. We will work under the relaxation that these\nactions need not be closed in the domain of the target function $D \\subset \\mathbb{R}^m$, i.e. there may be group\nelements $g \\in G$ which takes a point $x \\in D$ to $\\rho(g) x = x' \\notin D$. For a pedagogical overview of\ncommon transformation groups see [24].\nGroup equivariant functions: Symmetries are built into neural networks using group equivariant\nmaps, defined as follows.\nDefinition 6. A function $f : X \\rightarrow H$ between two spaces X and H which both admit group actions,\nsay $A_X$ and $A_H$ respectively, for a group G is said to be G-equivariant if f commutes with the group\nactions, i.e.,\n$f(A_X(g, X)) = A_H(g, f(X))$\n, (2)\nfor every $g \\in G$ and $X \\in X$. The map f is G-invariant if the action $A_H$ is trivial, i.e., $A_H(g, H) = H$\nfor all $g \\in G$ and any $H \\in H."}, {"title": "Preliminaries", "content": "It is straightforward to see that a G-equivariant map $H = f(X)$, is $L_{A_H}(H)$-invariant which may\nhowever be different groups for different values of H. As we will be using the notion of universal\napproximation in the space of G-equivariant functions, we outline the following general definition of\nthe universal approximation property for a function space in the domain D to $\\mathbb{R}$.\nDefinition 7. A class of parametrised models $\\Sigma_{\\Theta}(D)$ is said to be a universal approximator in the\nfunction space $C(D)$ if the closure of the set $\\Sigma_{\\Theta}(D)$ in $C(D)$ is $C(D)$.\nWhile the above definition subsumes definitions based on metrics via the corresponding metric\ntopology, we will concentrate on those cases where the defined metrics involve taking a supremum\nin the domain D. This means that we are primarily interested in subsets of the space of continuous\nfunctions rather than any strictly larger space like the space of $L_1$ integrable functions. Some\ninstances of the universal approximation property (UAP) in the set of G-equivariant functions can be\nfound in [25, 9, 26]. For general UAPs see for instance [27, 28] and [29, 30] for a more accessible\nintroduction.\nNotation of G-equivariant function spaces: We lay out some notation of function spaces, which will\nbe used in the following sections. The set of all continuous G-equivariant functions for particular\nactions $A_X$ and $A_H$ on the domain X and the codomain H, respectively, will be denoted as\n$\\mathbb{E}^{\\mathbb{G}}(A_X, A_H, X, H)$. When the action $A_H$ is trivial, the set of all G-invariant functions will be\ndenoted by $I^{\\mathbb{G}}(A_X, X, H)$. Where there is no ambiguity of the actions, domain and codomain of the\ncomponent functions, we will implicitly write these sets as $\\mathbb{E}^{\\mathbb{G}}$ and $I^{\\mathbb{G}}$.\nLikelihood ratio and hypothesis testing: We first outline a non-technical and intuitive picture of\nthe ubiquitous Neyman-Pearson lemma [1] which we will then connect to optimal binary classifica-\ntion. Consider an observed data X and we hypothesise that it originates from either $P_0(X, \\Theta_0)$ or\n$P_1(X, \\Theta_1)$, where $\\Theta_{\\alpha}$ are parameters of the probability distribution. Those hypotheses where the\nprobabilities are completely specified are said to be simple, i.e., each of the parameters $\\Theta_{\\alpha}$ are fixed\nto specific values for $\\alpha \\in \\{0, 1\\}$. The Neyman-Pearson lemma applies to scenarios where the two\nhypotheses are simple and exhaustive, i.e., the data can originate from only these two completely\nspecified probabilities. The null hypothesis $H_0$, where X follows $P_0$, is usually chosen to represent\nthe currently understood phenomena, while the alternate hypothesis $H_1$, where X follows $P_1$, are\ntaken to describe possible phenomena in the new regime probed by the experiment.\nThe power of a statistical test is the probability of correctly rejecting the null hypothesis when\nthe alternate hypothesis is true. At the same time, the significance is the probability of rejecting\nthe null hypothesis when it is true. The Neyman-Pearson lemma states that the likelihood ratio\n$\\Lambda(X) = P_1(X)/P_0(X)$ is uniformly the most powerful test statistic via which we can accept that\nX originated from $P_1$ for a given significance. Notably, a binary classification problem with an\nappropriate loss function (including binary cross-entropy) reduces to approximating a monotonic\nfunction of the likelihood ratio [31].\nOptimal binary classification: As we aim to connect binary classification with the Neyman-\nPearson optimality of hypothesis tests, we will utilise an analogous and straightforward definition\nof optimality in binary classification following the standard formulation of the receiver operator\ncharacteristics curve. Let us take the binary classification of a sample X sampled from either $P_0(X)$\nor $P_1(X)$ with the same support $D \\subset \\mathbb{R}^m$, where we are interested in maximally selecting those\nsamples originating from $P_1$. Often, the support of the alternate hypothesis' probability distribution\nfunction (pdf) $P_1$ is contained within that of the null hypothesis $P_0$. On the other hand, if there is\nsome part of $P_1$'s support outside $P_0$, any event obtained in this region trivially can only follow the\nalternate hypothesis. We are, therefore, primarily interested in those non-trivial cases where the data\nfalls in the intersection of the support of $P_0$ and $P_1$. For a given classifier $f : D \\rightarrow \\mathbb{R}$ which learns\nthe underlying class assignment map by segregating samples from $P_1$ to negative values, dependent\non a threshold $t \\in \\mathbb{R}$ the cumulative distribution of $\\hat{y} = f(X)$ under $P_{\\alpha}$ are\n$\\xi_{\\alpha} = C_{\\alpha}(t) = \\int_{D} dV P_{\\alpha}(X) \\Theta(\\hat{y} < t)$\n, where $dV$ is the volume element in D. Therefore, $\\xi_1$ is the true positive rate and $\\xi_0$ is the false positive\nrate and the former can be cast as a function of the later using the threshold t, $\\xi_1 = C_1 \\circ C_0^{-1}(\\xi_0) =$\nROC($\\xi_0$). We now have the following definition of an optimal binary classifier."}, {"title": "Preliminaries", "content": "Definition 8. A binary classifier for data sampled from two distributions $P_0(x)$ and $P_1(x)$ is optimal\nin accepting samples from $P_1$ at a given tolerance $\\xi_0$ of accepting false samples from $P_0$, if it has the\nhighest possible acceptance of samples $\\xi_1$ from $P_1$.\nThe connection to Neyman-Pearson's optimality of the likelihood ratio can be seen as follows. For\neach X, if the null hypothesis $H_0$ is that the underlying distribution is $P_0$ and the alternate hypothesis\n$H_1$ is that it is $P_1$, it is straightforward to see that $\\xi_0$ is the significance of the test and $\\xi_1$ is its power.\nTherefore, we have the following observation.\nObservation 1. A binary classifier $f : D \\rightarrow \\mathbb{R}$ of data $X \\in D$ sampled from either $P_0(X)$ or $P_1(X)$\nis an optimal classifier for all values of $\\xi_0$, if and only if it is a monotonic function of the likelihood\nratio $P_1(X)/P_0(X)$.\nThe relaxation to a monotonic function of the likelihood ratio follows from the integral definition of\n$\\xi_{\\alpha}$ depending implicitly on the threshold, i.e., we are interested in choosing regions in D divided by\nhypersurfaces which have constant likelihood ratios. For a d-dimensional domain, these are at least\nd - 1 dimensional, and therefore, they are computationally non-trivial to evaluate in high dimensions\neven when one can write down closed-form expressions for the probabilities. Additionally, a necessary\ncondition for any function to be the optimal classifier is to have an identical fibre decomposition in\nthe domain D with the likelihood ratio."}, {"title": "Minimal fibres of group equivariant functions", "content": "Since a necessary optimality condition in binary classification is to have the same fibre decomposition\nin the domain D, we now describe the minimal fibres of group of equivariant functions.\n4.1 Invariant functions\nLet $h : X \\rightarrow H$ be a G-invariant function, i.e. for any $X \\in X$ and all $g \\in G,\nh(A_X(g, X)) = h(X)\n(3)\nThis means that if $X \\in \\Omega_\\mathbb{G}^X(I)$ then for all $X' = A_X(g, X) \\in \\Omega_\\mathbb{G}^X(I)$, $h(X') = h(X)$. Therefore,\nwe have the following observation:\nObservation 2. The fibre of an element $H = h(X) \\in H$ in the image of a G-invariant function\n$h : X \\rightarrow H$ is at least as large as the orbit $\\Omega_\\mathbb{G}^X(I)$ of $X \\in X$ of the action $A_X$. If the map is equal\nfor elements belonging to distinct orbits, the fibre becomes enlarged to the union of these orbits.\nLet h(X) become equal for X in all orbits parametrized by I in the set $\\mathcal{F}$ of orbit invariants I. The\nfibre h(X) for any X in these orbits is\nh^{-1}(X) = \\bigcup_{I \\in \\mathcal{F}} \\Omega_\\mathbb{G}^X(I)\n(4)\n4.2 Equivariant functions\nLet $f : X \\rightarrow H$ be a G-equivariant function with respect to the actions $A_X$ and $A_H$ on X and H,\nrespectively. If $X' = A_X(g, X)$ is in the orbit of X, from Eq 2, we have $f(X') = A_H(g, f(X))$. It\nis straightforward to see that\ng \\in L_{A_H}(f(X)) \\Rightarrow f(X') = f(X)\n, (5)\ni.e., if X and X' are connected by a group element g which belongs to the stabiliser group of the\naction $A_H$ at f(X), then X' is contained in the fibre $f^{-1}(H)$ of $H = f(X)$. Therefore, similar to the\ncase for invariant maps, we have the following observation:\nObservation 3. The fibre of an element $H = f(X) \\in H$ in the image of a G-equivariant function\n$f : X \\rightarrow H$, is at least as large as the subset of the orbit of X connected by an element g in the\nstabiliser group of $A_H$ at H. If the map is equal for elements belonging to distinct orbits in X, the\nfibre becomes enlarged to the union of all such sets."}, {"title": "Equivariant functions", "content": "Define the subset of the orbit $\\Omega_\\mathbb{G}^X(I)$ dependent on $H = f(X)$ as\nO(I, H) = \\{X' = A_X(g, X) : g \\in L_{A_H}(H)\\}\n(6)\nIf f(X) become equal for X in all such subsets of distinct orbits parametrized by I in the subset\n$\\mathcal{F} \\subset \\mathcal{I}$. The fibres can be written as\nf^{-1}(H) = \\bigcup_{I \\in \\mathcal{F}} O(I, H)\n(7)\nAs expected, the invariant result is a special case when the action $A_H$ is trivial, i.e. the stabiliser\ngroup of any element $H \\in H$ is the group G and $O(I, H) = \\Omega_\\mathbb{G}^X(I)$ for any H.\nAs we will be primarily concerned with probabilities with support on a subset $D \\subset X$ where $X = \\mathbb{R}^m$,\nthe fibres become the intersection of the respective sets with the domain D. At this point, we point\nout that for general equivariant function approximation, a correct symmetry (whether invariant or\nequivariant) is one which does not mix the fibres of a target function through the minimal fibres\ninduced via group equivariance."}, {"title": "Optimal symmetries in binary classification", "content": "In fundamental applications, more often than not, the probabilities are invariant under some transfor-\nmation group action on the domain D. Even when closed-form expressions are not known, various\nfirst-principle arguments require the probabilities to be invariant under a transformation group. In\nthis section, we answer the question of which symmetries retain the optimality of a classifier when\nthe symmetry of the two probabilities is known a priori. We ignore the effects of noise in our\nmathematical description while we observe that the findings persist in the presence of noise in our\nnumerical experiments. Having observed the structure of fibres of group equivariant functions and\nthe need for an optimal classifier to have the same fibre decomposition as the likelihood ratio, we\ndefine an optimal symmetry as follows.\nDefinition 9. Two G-actions $A_X$ and $A_H$ are an optimal pair of symmetries for a given binary\nclassification task if the space $\\mathbb{E}^{\\mathbb{G}}(A_X, A_H, X, H)$ contains functions which have the same fibre\ndecomposition as the likelihood ratio.\nWhile the definition covers the invariant case, i.e. when $A_H$ is trivial, we will not explicitly mention\nthe trivial action when discussing the invariant case. Notice that the definition does not impose that\nall functions follow the same fibre decomposition as the likelihood ratio. This is because a group\ninvariant likelihood ratio only constrains the fibres to be at least as large as the group orbits and\ndistinct orbits can have the same value of probabilities wherein the fibres become enlarged to the\nunion of all such orbits. On the other hand, if the functions in $\\mathbb{E}^{\\mathbb{G}}$ by definition mixes any of the\npossible fibre of the likelihood ratio imposed by its group invariance i.e. all functions $f \\in \\mathbb{E}^{\\mathbb{G}}$ have a\ndifferent fibre decomposition to the likelihood, it follows that any universal approximator on $\\mathbb{E}^{\\mathbb{G}}$ will\nbe suboptimal for the particular binary classification."}, {"title": "Group invariant classifiers", "content": "Using the likelihood ratio, it is now straightforward to obtain the necessary and sufficient conditions\nfor optimal actions on the features X via a simple application of the Neyman-Pearson optimality in\nobservation 1.\nObservation 4. A G-action on $\\mathbb{R}^m$ is an optimal symmetry for G-invariant binary classification of data\nsampled via $P_0(X)$ and $P_1(X)$ for $X \\in \\mathbb{R}^m$, if and only if the likelihood ratio $\\Lambda(X) = P_1(X)/P_0(X)$\nis G-invariant.\nTherefore, for an optimally invariant G-action $A_X$, any universal approximator on $I^{\\mathbb{G}}(A_X, X, H)$\nwill in principle contain an arbitrarily close approximation. However, our primary concern is the\neffectiveness of finding such an approximation.\nLet $\\mathcal{S} = \\{(G_1, A_1), (G_2, A_2), ..., (G_{k_\\alpha}, A_{k_\\alpha})\\}$ denote the set of known G-actions under which the\nprobability $P_{\\alpha}$ is invariant. Therefore, a G-invariant universal approximator $\\Sigma_\\Theta^{\\mathcal{G}}(D)$ for $(G, A) \\in$\n$\\mathcal{S}_0 \\cap \\mathcal{S}_1$, will be able to approximate the optimal classifier. Consequently, it is not necessary that any"}, {"title": "Classifiers", "content": "transformation group in the domain D will be able to approximate the optimal classifier. While this\nis intuitively known in the community, with a common example being the inadequacy of reflection\ninvariance in classifying six and nine in the MNIST dataset, numerical experiments and theoretical\nresults point to a better generalisation and sample efficiency of larger groups. Our discussions\nregarding equivariant classification and numerical results indicate this is not always true in binary\nclassification.\nLet the action induced by the representation $\\rho(g)$ of G be an optimal group action for some binary\nclassification problem. The restricted action of a proper subgroup $\\mathbb{G}'$, i.e., for group elements\n$g \\in \\mathbb{G}' \\subset \\mathbb{G}$, will also be an optimal symmetry for the same binary classification scenario. Note\nthat since we are working in a representation $\\rho(g)$, there are possibly an infinite number of ways in\nwhich we can realise a subgroup restriction, and we are concerned with only one of these at a time.\nFor example, for the permutation group $S_n$'s representation of n elements, there are $\\binom{n}{m}$ ways of\nchoosing a restriction to the subgroup action of $S_m$ for a given m, and we select only one out of these\npossible choices. The parent G-invariant feature extraction will generally have the maximum sample\nefficiency compared to its different subgroups since it is evident that their orbits will be larger in\ngeneral.\nLet us now consider the case where some particular subgroup restriction is an optimal symmetry while\nthe parent action is not. This means that we can find non-empty sets $V \\subset D$ where $A(\\rho(g)X) \\ne A(X)$\nfor at least one $g \\in \\mathbb{G} \\setminus \\mathbb{G}'$. On the other hand, for a G-invariant function say $f : D \\rightarrow \\mathbb{R}$, we have\nh = f(X) = f(\\rho(g)X) for all $g \\in \\mathbb{G}$. Therefore, the fibre $f^{-1}(h)$ which is at least as large as the\norbit of the element X, will not coincide with that of the likelihood ratio, resulting in suboptimal\nclassification performance. While the amount of this implicit distortion of the fibre structure by the\nlarger group invariance will depend on the size of the set V, it is straightforward to see that continuous\nuniversal approximation on $I^{\\mathbb{G}}$ does not guarantee an arbitrarily close function approximation to\nmonotonic functions of the likelihood ratio since by design the fibre decomposition of any member\nfunction of the domain D which are at least as large as the intersection of orbits of the group action\nwith D, will not be equivalent to that of the likelihood ratio."}, {"title": "Group equivariant classifiers", "content": "Let us now consider the case of equivariant feature extraction for classifying data sampled from\ninvariant probabilities since there is limited utility in defining equivariance in the one-dimensional\nspace of probability values. For such a case, the following conjecture captures the necessary and\nsufficient conditions on the group actions in the domain and the range of a G-equivariant function.\nConjecture 1. The G-actions $A_X$ and $A_H$ acting on $\\mathbb{R}^m$ and a hidden representation space H,\nrespectively is an optimal symmetric pair of G-actions for G-equivariant binary feature extraction\nfrom data sampled via $P_0(X)$ and $P_1(X)$ for $X \\in \\mathbb{R}^m$, if and only if the likelihood ratio $\\Lambda(X) =$\n$P_1(X)/P_0(X)$ is invariant under the action $A_X$ restricted to a subgroup $\\mathbb{G}'$ and the action $A_H$ acts\ntrivially for all $g \\in \\mathbb{G}'$.\nThe sketch of a possible proof is as follows. For sufficiency, we need to proof that the action $A_H$\nbeing trivial and the likelihood ratio being invariant under the action $A_X$ for all $g \\in \\mathbb{G}'$ guarantees\nthat the smallest fibres demanded by G-equivariance never mixes different fibres of the likelihood\nratio. As we have already seen that the smallest fibre of a group equivariant function at $H = f(X)$ is\ndetermined by the subset of the orbit of X in the domain X which is connected by an element g in\nthe stabiliser group $L_{A_H}(H)$. Therefore, it suffices to show that the stabiliser group of every element\n$H \\in Im(f)$ is $\\mathbb{G}'$. Clearly, this is satisfied since, by definition, we have chosen a trivial action of the\ngroup $\\mathbb{G}'$.\nFor necessity, we need to show that optimality of the pair of G-actions implies that the behaviour of\nthe action $A_H$ is confined to be trivial for any $g \\in \\mathbb{G}'$ and all $H \\in Im(f)$ for $f : D \\rightarrow H$ and that the\nlikelihood ratio should be $\\mathbb{G}'$-invariant. Clearly, if the likelihood ratio is not $\\mathbb{G}'$-invariant under all\npossible subgroup restrictions in G except for the trivial group, the only possibility to have a function\nspace $\\mathbb{E}^{\\mathbb{G}}$ which contains the fibre decomposition structure of the likelihood ratio is for the action $A_H$\nto be free in Im(f). However, this is equivalent to a non-equivariant feature extraction since for a free\naction all stabilisers are trivial thereby putting no constraints on the function's fibres in D. Therefore,\nfor the non-trivial case the likelihood ratio should at least be invariant under some proper subgroup\n$\\mathbb{G}'$."}, {"title": "Experiments", "content": "Due to the prevalence of richer symmetries in 3D than in images, we choose point cloud classification\nof simple shapes using random number generators. For all experiments, we add 3D normally dis-\ntributed noise of diagonal covariance 0.3 to the cartesian coordinate representation. Our architecture\nis based on the group equivariant structure as prescribed in [9] for the classical groups. We consider\ngroups the groups E(3), O(3) and O(2) with input vector actions on $\\mathbb{R}^3$, where the O(2) action acts\nalong the z-axis with the modified metric signature (1, 1, 0), in the evaluation of the norm and the\ninner product. From the probabilities specified below, we sample 30 points to construct a data sample\nfor input to the neural network. We consider two training datasets of 10k and 100k samples per\nclass to compare the sample efficiency of these models and use binary cross entropy loss for the\noptimisation. Additional details of the network can be found in the supplementary material. For all\nreported results, we train the same network from random initialisation ten times and compute the\nrelevant mean and standard deviation.\nUniform: We take a simple example of classifying a point cloud of a hollow cylinder and a sphere\nwhere the points are uniformly sampled from their embedding in 3D. The sphere and the cylinder\nhave a unit radius, and we align the cylinder to the z-axis in the range (-1, 1). Due to the added\nnoise, this is a simple yet non-trivial point cloud binary classification scenario where the largest\nsymmetry of the probability distribution for the sphere is O(3), and that of the cylinder is O(2) with\nthe axis fixed to the z-axis. Therefore, since O(2) is a subgroup of O(3), the largest group action\nunder which the likelihood is invariant under the O(2) action along the z-axis.\nTruncated Normal: To understand the generalisation capabilities in scenarios where the symmetry\nlies in the probability itself and not their support, we consider a truncated ball and a cylinder. The\nradius for the ball and the cylinder follow a truncated normal distribution in the range (4, 6) centred at\nfive and unit standard deviation. The azimuthal angle for both cases follows a uniform distribution in\nthe range (-$\\pi$/4, $\\pi$/4). For the sphere, we uniformly sample the polar angle in the range ($\\pi$/4, 3$\\pi$/4)\nwhile for the cylinder, we sample z-coordinates uniformly in the range (5cos 3$\\pi$/4, 5cos $\\pi$/4).\nTherefore, with the added noise of 0.3 standard deviations in cartesian coordinate representation, the\nprobabilities have the same support on $\\mathbb{R}^3$ with the same underlying (approximate) symmetries as the\nprevious case.\nResults: The mean and standard deviation of the minimum validation loss over each training\ninstance for both scenarios is plotted in figure 1. For each of the best models per training, the"}, {"title": "Conclusions", "content": "In this work, we have presented a novel framework for optimising group symmetries in binary\nclassification tasks, challenging the prevailing assumption that larger symmetry groups universally\nlead to better performance. Our theoretical analysis and experimental results demonstrate that the\noptimal selection of group symmetries, aligned with the intrinsic properties of the underlying data\ndistribution, is critical for enhancing both generalisation and sample efficiency.\nWe developed a systematic approach to designing group equivariant neural networks, which carefully\ntailors the choice of symmetries to the specific characteristics of the problem at hand. Experimentally,\nwe showed that while larger groups, such as E(3) and O(3), may appear to offer more"}]}