{"title": "Optimal Symmetries in Binary Classification", "authors": ["Vishal S. Ngairangbam", "Michael Spannowsky"], "abstract": "We explore the role of group symmetries in binary classification tasks, presenting a novel framework that leverages the principles of Neyman-Pearson optimality. Contrary to the common intuition that larger symmetry groups lead to improved classification performance, our findings show that selecting the appropriate group symmetries is crucial for optimising generalisation and sample efficiency. We develop a theoretical foundation for designing group equivariant neural networks that align the choice of symmetries with the underlying probability distributions of the data. Our approach provides a unified methodology for improving classification accuracy across a broad range of applications by carefully tailoring the symmetry group to the specific characteristics of the problem. Theoretical analysis and experimental results demonstrate that optimal classification performance is not always associated with the largest equivariant groups possible in the domain, even when the likelihood ratio is invariant under one of its proper subgroups, but rather with those subgroups themselves. This work offers insights and practical guidelines for constructing more effective group equivariant architectures in diverse machine-learning contexts.", "sections": [{"title": "1 Introduction", "content": "Equivariant neural networks, which leverage symmetries inherent in data, have shown significant promise in enhancing classification accuracy, data efficiency, and convergence speed. However, selecting an appropriate group and defining its actions across the various layers of the network remains a complex task, particularly for applications requiring adherence to specific symmetries. This research aims to establish a foundational framework for designing equivariant neural network architectures by utilising stabiliser groups.\nIn the context of equivariant function approximation, a critical insight is that the preimage of a target element in the output space can be characterised by the subset of the orbit of an input element, restricted to those connected by elements of the stabiliser group of the target. This understanding simplifies the choice of group G and its actions, aligning them with the equivalence class of the target function's fibres in the input space.\nThis paper provides a theoretical basis for designing equivariant architectures in scientific applica-tions where known symmetries play a crucial role. By grounding the architecture design in first principles, we aim to explain empirical successes observed in data-driven applications and enhance the effectiveness of these architectures in practical scenarios.\nBy quantifying the impact of equivariant architectures on sample efficiency and generalisation error, we offer a comprehensive framework that not only aids in theoretical understanding but also provides practical guidelines for implementing these architectures in real-world scientific problems. Through simplified examples and experimental results, we demonstrate the applicability and effectiveness of our proposed methods."}, {"title": "2 Related works", "content": "Group Equivariant Networks: Group equivariant architecture design is ubiquitous in modern deep learning algorithms with Convolutional Neural Networks [2] being equivariant to translations. Modern generalisations include equivariance to discrete groups [3], and gauge symmetries [4]. It was shown in [5] that equivariance with respect to any compact group requires the operation of a layer to be equivalent to a generalised convolution on the group elements.\nFor set-based approaches, the DeepSets framework [6] formulated the universal structure of permuta-tion equivariant and invariant functions on elements belonging to a countable universe. Equivariant feature extraction on point clouds for the classifical groups has also been explored in [7, 8, 9]. Geometric Algebra Transformer [10, 11] have been designed for 3D equivariant extraction of vari-ous non-trivial transformation groups, including the conformal and projective groups [11] utilising Clifford group equivariant networks [12].\nTheoretical analysis of equivariant models: On top of the empirical results, several theoretical results exist on the advantages of equivariant models. Under the assumption that the classification task is invariant to symmetry transformation, bounds on the generalisation error were derived in [13]. The sample complexity for invariant kernel methods is studied in [14]. In the framework of PAC learning, it was shown [15] that learning with invariant or equivariant methods reduces to learning over orbit representatives. At the same time, the incorporation of appropriate group equivariances [16, 17] results in a strict increase in generalisation. The general intuition from these works is that larger groups result in better sample efficiency and improved generalisation when the target function follows the assumed symmetry. Our work contributes in this direction by defining the notion of a correct symmetry for a binary classification task with a priori known symmetries of the probabilities.\nEquivariance in fundamental physics: The importance of equivariance in fundamental physics cannot be overstated, starting from the Galilean transformations in non-relativistic regime to the exotic structure of gauge theories, which form the mathematical backbone of our current understanding of the universe. Lorentz group equivariance has been explored in [18, 19, 20, 21]. Other group equivariance studies include architectures for simulating QCD on the lattice [22], and the classification of radio galaxies [23]."}, {"title": "3 Preliminaries", "content": "Fibres of a function: Our work relies on studying the minimal fibre structure induced by symmetries on the likelihood ratio and its relation to different group equivariant functions. A fibre is formally defined as:\nDefinition 1. Given a function $f : X \\rightarrow Y$, the fibre of an element y in the image of f denoted as $f^{-1}(y)$, is the set of all elements of X which gets mapped to y via f:\n$f^{-1}(y) = \\{x \\in X : f(x) = y\\}$\nThe fibres of all element in Im(f) divide X into equivalence classes. This means that the set of fibres of any function partitions the domain D into mutually exclusive subsets. This holds true for any equivalence class in any given set, i.e. any element $x \\in X$ can only belong to a single equivalence class. We utilise this basic fact implicitly in different parts of the work.\nGroup actions: The main idea underlying symmetries is the action of groups on an underlying set. Group actions and some related terminologies are defined as follows.\nDefinition 2. Given a group G and a set X, the left-action of G on X is a map $A : G \\times X \\rightarrow X$ with the following properties:\n1. A(e, X) = X for the identity element $e \\in G$ and any $X \\in X$, and\n2. A(91, A(92, X)) = A(9192, X) for all $91, 92 \\in G$ and $X \\in X$.\nA G-action will be denoted as (G, A(g, X)). Any set always admits the trivial action where A(g, X) = X for any group element g and all elements X of the set X. Consequently, even for the same group G, one can have two actions A1(g, X) and A2(g, X) acting on the same set X, which are not necessarily the same functions. This motivates defining the equality of two G-actions as follows.\nDefinition 3. Two G-actions (G1, A1(g, X)) and (G2, A2(g, X)) acting on the same set $X \\exists X$ are equal if and only if G\u2081 is isomorphic to G2, via an invertible map $i : G1 \\rightarrow G2$ and A1(g, X) = A2(i(g), X) for all $g \\in G1$ and $X \\in X$.\nDefinition 4. The orbit of an element $X \\in X$ under the group action A(g, X) of the group G, is the set of all elements $X' \\in X$ for which there is a group element $g \\in G$ such that $X' = A(g, X)$.\nWe will write the orbit of a G-action characterised by a vector of group invariants I as\n$\\Omega_A^X(I) = \\{X' = A(g, X) : \\{X, X'\\} \\subset X  \\text{and}  g \\in G\\}$          (1)\nThe set of all distinct orbits divides X into equivalence classes.\nDefinition 5. The stabiliser group of an element $X \\in X$ with respect to the G-action A, is the set of all group elements g which leave X invariant. Mathematically, we will denote it as\n$L_A(X) = \\{g \\in G : X = A(g, X)\\} \\subseteq G$\nA group representation $p : G \\rightarrow GL(m, R)$, naturally induces a linear action $A : G \\times R^m \\rightarrow R^m$ on Rm of the form $A_p(g, x) = p(g) x$. In the rest of the paper, we will be primarily concerned with such actions of a given transformation group on Rm. We will work under the relaxation that these actions need not be closed in the domain of the target function $D \\subset R^m$, i.e. there may be group elements g \u2208 G which takes a point $x \\in D$ to $p(g) x = x' \\notin D$. For a pedagogical overview of common transformation groups see [24].\nGroup equivariant functions: Symmetries are built into neural networks using group equivariant maps, defined as follows.\nDefinition 6. A function $f : X \\rightarrow H$ between two spaces X and H which both admit group actions, say Ax and A\u0142 respectively, for a group G is said to be G-equivariant if f commutes with the group actions, i.e.,\n$f(A_X(g, X)) = A_H(g, f(X))$ ,          (2)\nfor every g\u2208 G and X \u2208 X. The map f is G-invariant if the action A\u04a3 is trivial, i.e., \u0410\u043d(\u0434, H) = H for all g\u2208 G and any H \u2208 H."}, {"title": "4 Minimal fibres of group equivariant functions", "content": "Since a necessary optimality condition in binary classification is to have the same fibre decomposition in the domain D, we now describe the minimal fibres of group of equivariant functions.\n4.1 Invariant functions\nLet h : X \u2192 H be a G-invariant function, i.e. for any X \u2208 X and all g \u2208 G,\n$h(A_X(g, X)) = h(X)$      (3)\nThis means that if $X \\in \\Omega_A^X(I)$ then for all $X' = A_X(g, X) \\in \\Omega_A^X(I), h(X') = h(X)$. Therefore, we have the following observation:\nObservation 2. The fibre of an element H = h(X) \u2208 H in the image of a G-invariant function h : X \u2192 H is at least as large as the orbit $\\Omega_A^X(I)$ of $X \u2208 X$ of the action Ax. If the map is equal for elements belonging to distinct orbits, the fibre becomes enlarged to the union of these orbits.\nLet h(X) become equal for X in all orbits parametrized by I in the set F of orbit invariants I. The fibre h(X) for any X in these orbits is\n$h^{-1}(X) = \\cup_{I \\in F} \\Omega_A^X(I)$      (4)\n4.2 Equivariant functions\nLet f : X \u2192 H be a G-equivariant function with respect to the actions Ax and A\u04a3 on X and H, respectively. If $X' = A_X(g, X)$ is in the orbit of X, from Eq 2, we have f(X') = A\u04a3(g, f(X)). It is straightforward to see that\n$g\\in L_{A_H}(f(X))  \\Rightarrow  f(X') = f(X)$ ,        (5)\ni.e., if X and X' are connected by a group element g which belongs to the stabiliser group of the action An at f(X), then X' is contained in the fibre f-1(H) of H = f(X). Therefore, similar to the case for invariant maps, we have the following observation:\nObservation 3. The fibre of an element H = f(X) \u2208 H in the image of a G-equivariant function f : X \u2192 H, is at least as large as the subset of the orbit of X connected by an element g in the stabiliser group of A\u04a3 at H. If the map is equal for elements belonging to distinct orbits in X, the fibre becomes enlarged to the union of all such sets."}, {"title": "5 Optimal symmetries in binary classification", "content": "In fundamental applications, more often than not, the probabilities are invariant under some transfor-mation group action on the domain D. Even when closed-form expressions are not known, various first-principle arguments require the probabilities to be invariant under a transformation group. In this section, we answer the question of which symmetries retain the optimality of a classifier when the symmetry of the two probabilities is known a priori. We ignore the effects of noise in our mathematical description while we observe that the findings persist in the presence of noise in our numerical experiments. Having observed the structure of fibres of group equivariant functions and the need for an optimal classifier to have the same fibre decomposition as the likelihood ratio, we define an optimal symmetry as follows.\nDefinition 9. Two G-actions Ax and An are an optimal pair of symmetries for a given binary classification task if the space E\u04ab(\u0410\u0445, \u0410\u043d, X, H) contains functions which have the same fibre decomposition as the likelihood ratio.\nWhile the definition covers the invariant case, i.e. when An is trivial, we will not explicitly mention the trivial action when discussing the invariant case. Notice that the definition does not impose that all functions follow the same fibre decomposition as the likelihood ratio. This is because a group invariant likelihood ratio only constrains the fibres to be at least as large as the group orbits and distinct orbits can have the same value of probabilities wherein the fibres become enlarged to the union of all such orbits. On the other hand, if the functions in Eg by definition mixes any of the possible fibre of the likelihood ratio imposed by its group invariance i.e. all functions f \u2208 E\u00e7 have a different fibre decomposition to the likelihood, it follows that any universal approximator on Eg will be suboptimal for the particular binary classification.\n5.1 Group invariant classifiers\nUsing the likelihood ratio, it is now straightforward to obtain the necessary and sufficient conditions for optimal actions on the features X via a simple application of the Neyman-Pearson optimality in observation 1.\nObservation 4. A G-action on Rm is an optimal symmetry for G-invariant binary classification of data sampled via Po(X) and P\u2081(X) for X \u2208 Rm, if and only if the likelihood ratio $\\lambda(X) = P_1(X)/P_0(X)$ is G-invariant.\nTherefore, for an optimally invariant G-action Ax, any universal approximator on I\u00e7 (Ax, \u03a7, \u0397) will in principle contain an arbitrarily close approximation. However, our primary concern is the effectiveness of finding such an approximation.\nLet S = {(G1, A1), (G2, A2), ..., (Gka, Aka)} denote the set of known G-actions under which the probability Pa is invariant. Therefore, a G-invariant universal approximator \u03a3\uc6a5(D) for (G, A) \u2208 So \u2229 S1, will be able to approximate the optimal classifier. Consequently, it is not necessary that any"}, {"title": "5.2 Group equivariant classifiers", "content": "Let us now consider the case of equivariant feature extraction for classifying data sampled from invariant probabilities since there is limited utility in defining equivariance in the one-dimensional space of probability values. For such a case, the following conjecture captures the necessary and sufficient conditions on the group actions in the domain and the range of a G-equivariant function.\nConjecture 1. The G-actions Ax and An acting on Rm and a hidden representation space H, respectively is an optimal symmetric pair of G-actions for G-equivariant binary feature extraction from data sampled via Po(X) and P\u2081(X) for X \u2208 Rm, if and only if the likelihood ratio $\\lambda(X) = P_1(X)/P_0(X)$ is invariant under the action Ax restricted to a subgroup G' and the action \u0410\u04a3 acts trivially for all g \u2208 G'.\nThe sketch of a possible proof is as follows. For sufficiency, we need to proof that the action \u0410\u043d being trivial and the likelihood ratio being invariant under the action Ax for all g \u2208 G' guarantees that the smallest fibres demanded by G-equivariance never mixes different fibres of the likelihood ratio. As we have already seen that the smallest fibre of a group equivariant function at H = f(X) is determined by the subset of the orbit of X in the domain X which is connected by an element g in the stabiliser group LAH (H). Therefore, it suffices to show that the stabiliser group of every element \u0397 \u2208 Im(f) is G'. Clearly, this is satisfied since, by definition, we have chosen a trivial action of the group G'.\nFor necessity, we need to show that optimality of the pair of G-actions implies that the behaviour of the action An is confined to be trivial for any g \u2208 G' and all H \u2208 Im(f) for f : D \u2192 H and that the likelihood ratio should be G'-invariant. Clearly, if the likelihood ratio is not G'-invariant under all possible subgroup restrictions in G except for the trivial group, the only possibility to have a function space Eg which contains the fibre decomposition structure of the likelihood ratio is for the action A\u043d to be free in Im(f). However, this is equivalent to a non-equivariant feature extraction since for a free action all stabilisers are trivial thereby putting no constraints on the function's fibres in D. Therefore, for the non-trivial case the likelihood ratio should at least be invariant under some proper subgroup G'."}, {"title": "6 Experiments", "content": "Due to the prevalence of richer symmetries in 3D than in images, we choose point cloud classification of simple shapes using random number generators. For all experiments, we add 3D normally dis-tributed noise of diagonal covariance 0.3 to the cartesian coordinate representation. Our architecture is based on the group equivariant structure as prescribed in [9] for the classical groups. We consider groups the groups E(3), O(3) and O(2) with input vector actions on R3, where the O(2) action acts along the z-axis with the modified metric signature (1, 1, 0), in the evaluation of the norm and the inner product. From the probabilities specified below, we sample 30 points to construct a data sample for input to the neural network. We consider two training datasets of 10k and 100k samples per class to compare the sample efficiency of these models and use binary cross entropy loss for the optimisation. Additional details of the network can be found in the supplementary material. For all reported results, we train the same network from random initialisation ten times and compute the relevant mean and standard deviation.\nUniform: We take a simple example of classifying a point cloud of a hollow cylinder and a sphere where the points are uniformly sampled from their embedding in 3D. The sphere and the cylinder have a unit radius, and we align the cylinder to the z-axis in the range (-1,1). Due to the added noise, this is a simple yet non-trivial point cloud binary classification scenario where the largest symmetry of the probability distribution for the sphere is O(3), and that of the cylinder is O(2) with the axis fixed to the z-axis. Therefore, since O(2) is a subgroup of O(3), the largest group action under which the likelihood is invariant under the O(2) action along the z-axis.\nTruncated Normal: To understand the generalisation capabilities in scenarios where the symmetry lies in the probability itself and not their support, we consider a truncated ball and a cylinder. The radius for the ball and the cylinder follow a truncated normal distribution in the range (4, 6) centred at five and unit standard deviation. The azimuthal angle for both cases follows a uniform distribution in the range (-\u03c0/4, \u03c0/4). For the sphere, we uniformly sample the polar angle in the range (\u03c0/4, 3\u03c0/4) while for the cylinder, we sample z-coordinates uniformly in the range (5 cos 3\u03c0/4, 5 cos \u03c0/4). Therefore, with the added noise of 0.3 standard deviations in cartesian coordinate representation, the probabilities have the same support on R\u00b3 with the same underlying (approximate) symmetries as the previous case.\nResults: The mean and standard deviation of the minimum validation loss over each training instance for both scenarios is plotted in figure 1. For each of the best models per training, the"}, {"title": "7 Conclusions", "content": "In this work, we have presented a novel framework for optimising group symmetries in binary classification tasks, challenging the prevailing assumption that larger symmetry groups universally lead to better performance. Our theoretical analysis and experimental results demonstrate that the optimal selection of group symmetries, aligned with the intrinsic properties of the underlying data distribution, is critical for enhancing both generalisation and sample efficiency.\nWe developed a systematic approach to designing group equivariant neural networks, which carefully tailors the choice of symmetries to the specific characteristics of the problem at hand. Experimentally, we showed that while larger groups, such as E(3) and O(3), may appear to offer more comprehensive symmetry handling, they do not necessarily result in better classification performance. In fact, our"}, {"title": "A Additional details of experiments", "content": "Architecture: At the l-th stage, the equivariant operation for the inhomogeneous case updates the scalar node representations h hand the vector node representation x1) x via the following equations:\n$m_i^{(l+1)} = \\Phi^{(l+1)}(h_i^{(l)}, h_j^{(l)}, |x_i^{(l)} - x_j^{(l)}|^2)$      (8a)\n$x_i^{(l+1)} = x_i^{(l)} + \\sum_{j \\in \\mathcal{N}(i)} \\Psi^{(l+1)}(x_i^{(l)} - x_j^{(l)}, m_i^{(l+1)})$         (8b)\n$m_i^{(l+1)} = \\frac{\\sum_{j \\in \\mathcal{N}(i)} m_j^{(l+1)}}{\\|\\sum_{j \\in \\mathcal{N}(i)} m_j^{(l+1)}\\|}$           (8c)\n$h_i^{(l+1)} = \\Theta^{(l+1)}(h_i^{(l)}, m_i^{(l+1)})$          (8d)\nThe norm |.| is evaluated using the particular metric conserved by the group. The functions \u03a6(l+1), \u03a8(l+1) and \u0398(l+1) are MultiLayer Perceptrons (MLPs), with \u03a6(l+1)'s output a single value made to fall in the open unit interval via the sigmoid activation function. We get the equivariant vector update, which does not respect translation equivariance by replacing Eqs.8a and 8b to\n$m_i^{(l+1)} = \\Phi^{(l+1)}(h_i^{(l)}, h_j^{(l)}, |x_i^{(l)} - x_j^{(l)}|^2, (x_i, x_j))$        (9a)\n$x_i^{(l+1)} = x_i^{(l)} + \\sum_{j \\in \\mathcal{N}(i)} \\Psi^{(l+1)}(x_i^{(l)} - x_j^{(l)}, m_i^{(l+1)})$         (9b)\nwhere (xi, xj) is the inner product under the metric preserved by the homogeneous group.\nWe consider both invariant and equivariant feature extraction with the same base architecture with a total of three equivariant message passing operations. We do not consider any input scalar in the data input, while the subsequent scalar representations are sixty four dimensional. The output of all \u03a6(l+1) are sixty four dimensional which unambiguously fixes the input dimensions of the other two functions. The model takes the 3D Cartesian coordinates as the vector representation from which it evaluates the scalars of the particular groups as inputs for the scalar updates. In the equivariant block, all MLPs have two hidden layers of sixty nodes and ReLU activation. While @+1) has a sigmoid output activation, we do not apply any non-linear activation to the other outputs. For invariant feature extraction, we forego the vector update at the final layer and use only the concatenated mean global representation of each scalar node representation as input to a classifier. For the equivariant case, we"}]}