{"title": "SuPLE: Robot Learning with Lyapunov Rewards", "authors": ["Phu Nguyen", "Daniel Polani", "Stas Tiomkin"], "abstract": "The reward function is an essential component in robot learning. Reward directly affects the sample and computational complexity of learning, and the quality of a solution. The design of informative rewards requires domain knowledge, which is not always available. We use the properties of the dynamics to produce system-appropriate reward without adding external assumptions. Specifically, we explore an approach to utilize the Lyapunov exponents of the system dynamics to generate a system-immanent reward. We demonstrate that the Sum of the Positive Lyapunov Exponents (SuPLE) is a strong candidate for the design of such a reward. We develop a computational framework for the derivation of this reward, and demonstrate its effectiveness on classical benchmarks for sample-based stabilization of various dynamical systems. It eliminates the need to start the training trajectories at arbitrary states, also known as auxiliary exploration. While the latter is a common practice in simulated robot learning, it is unpractical to consider to use it in real robotic systems, since they typically start from natural rest states such as a pendulum at the bottom, a robot on the ground, etc. and can not be easily initialized at arbitrary states. Comparing the performance of SuPLE to commonly-used reward functions, we observe that the latter fail to find a solution without auxiliary exploration, even for the task of swinging up the double pendulum and keeping it stable at the upright position, a prototypical scenario for multi-linked robots. SuPLE-induced rewards for robot learning offer a novel route for effective robot learning in typical as opposed to highly specialized or fine-tuned scenarios. Our code is publicly available for reproducibility and further research.", "sections": [{"title": "I. INTRODUCTION", "content": "The stabilization of dynamical systems is a central task in many applications, including robotic control. The design of appropriate control rules can require quite substantial domain knowledge [1]\u2013[3]. It is indeed one of the major contributions of modern Al methodologies, such as Reinforcement Learning (RL), to reduce the required explicit injection of such knowledge [4]-[6]. However, even there, the performance of an effective learner is usually boosted by a well-designed reward model. For this purpose, one uses reward shaping [5]-[8], but that itself requires domain knowledge. Where this is not the case, the learner runs the risk to be inefficient. This is due to the slow process of needing the minimally specified reward structure to be propagated throughout the configuration space through the learning algorithm. A frequently used remedy for ineffective reward in robot learning is to randomly reset the agent to arbitrary states in state space. This boosts the exploration of the state space artificially, and allows one to train even with sparse rewards, where the agent receives a positive reward on task completion only (and zero reward elsewhere). Such auxiliary exploration is possible in simulation, but it is impractical for training real robotic systems, because it requires the, say, multi-linked robotic system to be regularly physically reset to random states to learn the task of self-righting and stabilization at the upright position, one of the most important tasks for robot learning.\nHere, we propose an alternative route to achieve stabilization which requires only minimal external domain knowledge beyond the dynamical system itself. Concretely, we use truncated Lyapunov exponents to reward the controller, who learns to direct the system towards maximally sensitive states. This turns out to be surprisingly effective in identifying states of interest and providing a natural reward function which, except for the specification of the free parameter determining the depth of the time horizon, only requires the system dynamics itself. In particular, it does not need a human to introduce detailed prior understanding about a reward function.\nAmongst the models of interest are classic benchmarks such as variations of differing complexity of the pendulum swing-up task. While, for the single pendulum, many working solutions are known, the design of stabilizing controllers for more complex scenarios, such as the double pendulum, is more intricate and any learning of the controller is substantially more difficult; highly elaborate and optimized methods such as PILCO [9] can be used for this.\nIdeally, to be effective, a reward structure should be both dense (it is better to receive informative rewards throughout the run rather than, say, only at the target region) and informative (it should ideally help to directly specify what actions to select rather than extracting that information piecemeal). A well set-up RL system will, after convergence, ultimately build a suitable value or Q-function from the rewards, but if the rewards themselves can be imbued with domain knowledge, this process can be sped up drastically. However, especially in larger search spaces, it is not obvious how to set up an appropriate reward. Easy-to-specify rewards in such systems will typically be quite sparse, e.g. by mostly distinguishing target from non-target regions. Other rewards might be based on naive reduction of distance to the target region. Thus, for effective control, this reward will first have to be propagated back throughout the whole system with a suitable RL algorithm to become informative and useful for the controller. An effective reward function would thus ideally already encapsulate important information about the structure of the control task."}, {"title": "II. PRIOR WORK", "content": "Information-theoretic methods to generate intrinsic motivations to control various scenarios have been used for a number of years [11]-[23]. One insight is that the maximal potential mutual information between actions and observations acts as a meaningful signal for learning nontrivial behaviour. One disadvantage is the lack of their interpretability in intrinsic terms of the system's dynamics, such as the Lyapunov exponents. However, such links clearly exist, for instancehe following fundamental result connecting mutual information in Markov processes with discrete state space and their Lyapunov exponents [24], [25].\nMore recently, a connection was established between Lyapunov exponents and information-theoretic methods for intrinsic motivation [26] such as empowerment [14], [16], [21], causal entropic forcing [27], variational intrinsic control [28], and other. In this work, we exploit this connection for the generation of informative reward in robot learning.\nSpecifically, we focus on finding and stabilizing the unstable equilibria in dynamics, crucial for up-right locomotion, self-righting and related tasks."}, {"title": "III. PRELIMINARIES", "content": "In this section, we review the necessary background for our method, consisting of the truncated Lyapunov spectra and RL."}, {"title": "A. Lyapunov Spectra of Dynamical Systems", "content": "To estimate Lyapunov Exponents, (LE), for a known dynamics, f, with n-dimensional state $s \\in S$:\n\n$s_{t+1} = f(s_t), $\n\nwe consider the evolution of the principal axes, $(p_1 (0), p_2(0),...,p_n(0))$, of the n-dimensional sphere at t = 0 in the dynamics state space, S. The i-th LE, $\u03bb_i$, is defined by [10], [29]\u2013[31]\n\n$\u03bb_i = \\lim_{t \\to \\infty} \\frac{1}{t} \\log_2 (\\frac{||p_i(t)||}{||p_i(0)||}), $\n\n$\u039b = (\u03bb_1, \u03bb_2,..., \u03bb_n)$\n\nwhere $\u03bb_i > \u03bb_{i+1}$\n\nand $p_i(t)$ is the norm of the i-th axis at time t. We will denote the i-th positive LE by $\u03bb_i^+$, thus the maximal LE is $\u03bb_1^+$, and the sum of positive LE is:\n\n$SuPLE = \\sum_{i} \u03bb_i^+$\n\nThe positive LEs are related to the expansion of the state space volume, while the negative ones are related to its contraction [10]. Thus, the sum of positive LE, SuPLE, reflects the total volume expansion. In practice, the direction, $p_1$, associated with the largest positive LE, $\u03bb_1$, often dominates the other directions $p_i$, which collapse on $p_1$, hampering the estimation of their corresponding exponents, $\u03bb_i$. The remedy to this collapse is to orthogonalize the set $(p_1 (0), p_2(0),...,p_n(0))$ by the Gram-Schmidt orthogonalization process at a pre-defined number of time steps.\nThe Lyapunov spectra, A, defined in Eq. (2) do not depend on the initial state for $t \\to \\infty$. State dependent spectra, A(s), however, are calculated with a finite time, and are denoted by the truncated LE [32]. Both A and A(s) can be efficiently estimated from data [10], which we utilize in this work. To formulate informative dense reward for sensitivity-guided stabilization, we hypothesize that truncated-in-time Lyapunov exponents provide useful state-dependent information for the derivation of stabilizing controllers by data-driven methods. The truncated LE is calculated for a finite number of steps, which is in contrast to the global LE derived for $t'\\to\\infty$.\nTo evaluate the effectiveness of SuPLE, we couple it with SOTA methods in RL, whose basics we briefly overview in the next section."}, {"title": "B. Reinforcement Learning (RL)", "content": "The RL setting is modeled as a Markov Decision Process (MDP) defined by: the state space S, the action space A, the transition probabilities $p(s' | s,a)$, the initial state distribution $P_0(s)$, the reward function $r(s,a) \\in R$, which is typically manually designed for a particular task, and the discount factor $\u03b3$. The goal of RL is to find an optimal control policy $\u03c0(a | s)$ that maximizes the expected return, defined by [4],"}, {"title": "C. Soft Actor-Critic", "content": "In more complicated environments, especially, where en- hanced exploration is required, the Soft-Actor Critic [33] (SAC) is a more appropriate method. The SAC algorithms augment reward with the entropy of policy, which explicitly encourages exploration:\n\n$\\max_{\u03c0 \\in P} E_{s_0~p_0,a_t~\u03c0,s_t~p} [ \\sum_{t=1}^{T} \u03b3^{t-1} (r(s_t,a_t) - \u03b1 \\log \u03c0(a_t | s_t))],$\n\nThe essential component in any RL objective is reward, which is usually designed manually, rather than based on the important properties of dynamics, as in the present work. Here, we show that the truncated LEs provide the agent with informative and dense reward without reward engineering and/or shaping.\nThe commonly used rewards functions are 'i', the sparse reward, which provides the learner with reward '1' in the target state, and with reward '0' elsewhere, and 'ii', the error between the current state and the target state, measured by a particular norm.\nUsually, the training involves auxiliary exploration, where a simulated robot is repeatedly initialized in arbitrary states, which is impractical when training real robotic systems. Auxiliary exploration is crucial for the commonly-used re- wards even in such powerful algorithms in RL as SAC and PPO [34]. To demonstrate the strengths of the proposed reward, SuPLE, we plug it into the RL algorithms in our experiments, and show that SuPLE is informative enough to solve complicated tasks without auxiliary exploration, while the commonly used above-mentioned rewards ('i' and 'ii') can not solve these tasks."}, {"title": "IV. PROPOSED METHOD", "content": "Given a nonlinear dynamics f(s) and its Jacobian J(s) we estimate the truncated LE at the state s by propagating an orthonormal frame $V = {v_1, v_2,...,v_n}$ for T time steps [10] as summarized in Algorithm '1' and explained below.\nAs mentioned in Section II A, each vector in the evolved frame, V, tends to fall along the local direction of most rapid growth, which corresponds to $v_1$. To prevent this collapse to the dominant direction, we re-orthogonalize the frame, V, by the Gram-Schmidt orthogonalization process. This way $\\log_2(||v_i||_2)$ represents the change of the state space volume in the i-th direction in a single propagation step by J(s) with (small) stepsize dt. These changes are accumulated over time to yield the total change of the i-th axis over the time interval t = Tdt, denoted by $p_i(t)$ in Eq. (2).\nHere, Gram-Schmidt re-orthogonalization process replaces the evolved frame V to a corresponding orthogonal set of vectors. Grand-Schmidt process re-orthogonalizes the axes while keeping the direction of the fastest growing vector in the system. The accumulated $\u03bb_i$ is divided by t = Tdt.\nIn Algorithm 1 below, we combine truncated LE with the formalism of RL. The data on line 3 in Algorithm 1 is collected with the current policy, \u03c0. In line 4, $A^\u03c0$ is estimated by Algorithm 2 at the states visited by \u03c0. That can be done either by model-based LE estimation as in the current work, or by the existing efficient sample-based methods [10], [29], [35]. The notation on line 6 of Algorithm 1 is the policy improvement induced by the SuPLE reward, using Soft-Actor Critic, Eq. (6)."}, {"title": "Algorithm 1. Sensitivity-Guided Stabilization Control", "content": "1: initialize the policy, \u03c0: S \u2192 A\n2: repeat\n3: $\\tau^\u03c0 = {s_t, a_t}_{t=1}^{T} \\leftarrow$ collect data with \u03c0\n4: ${A^{\u03c4^\u03c0}_{s_t}}_{t=1}^{T} \\leftarrow$ EstimateTruncatedLE($\u03c4^\u03c0$)\n5: ${SuPLE^{\u03c4^\u03c0}_{s_t}}_{t=1}^{T} \\leftarrow {A^{\u03c4^\u03c0}_{s_t}}_{t=1}^{T} $\n6: \u03c0 \u2190 improve \u03c0 with ${s_t,a_t, SuPLE^{\u03c4^\u03c0}_{s_t} }_{t=1}^{T} $ by SAC\n7: until convergence\n8: return $\u03c0^*$, stabilizing at the states with the maximal LE\n\nVia this scheme, we use the LEs of dynamical systems to determine rewards instead of relying on human knowledge or reward engineering."}, {"title": "Algorithm 2. Estimate Truncated Lyapunov Exponents", "content": "Require: s-state, f(s)-dynamics, J(s)-Jacobian, T-horizon.\n1: init:\n2: $V = (v_1, v_2,...,v_n) $ {an orthonormal frame at s}\n3: $\u03bb_i : \u03bb_i = 0$\n4: for $t \u2208 [1,...,T]$ do\n5: $v_i : v_i = J(s)v_i$ {propagate V vector by vector}\n6: s = f(s) {propagate state s}\n7: V = GramSchmidt(V) {re-orthogonalization}\n8: $\u2200i : \u03bb_i = \u03bb_i + log_2(||v_i||_2)$ {accumulation of LE}\n9: end for\n10: return ${\u03bb_i}_{t=1}^{Tdt}$ {Truncated Lyapunov Exponents}"}, {"title": "V. NUMERICAL SIMULATIONS", "content": "The numerical experiments address the following questions: 1. Does SuPLE in Eq. (4) solve the task of stabilization from samples without auxiliary exploration? 2. Does just MaxLE already solve these tasks? 3. Is there a performance gap between traditional rewards such as Quadratic and Sparse vs. the SuPLE reward?\nFor this, we chose three dynamical systems of differ- ent complexity: \u2018Single Pendulum\u2019:a simple dynamics, fre- quently used in evaluation of new methods; 'Cart-Pole\u2019: a more complicated system, a prototype of the 'Segway' robotic system; 'Double Pendulum\u2019: a complicated chaotic dynamics with an extremely challenging exploration of the state space. It is a prototype for self-righting multi-linked robotic systems.\nAll systems start at their stable equilibrium (e.g., the bottom position for the pendulum links) in both training and testing. Importantly, auxiliary exploration is used neither in training nor in testing. All the systems are trained with the same parameters except for the following reward functions:\n\u2022 'SuPLE': sum of all positive LE given by Eq. (4)\n\u2022 'MaxLE': only the maximum of the positive LE\n\u2022 'Quadratic': weighted error between the current state, s, and the target state, $s^o$: $r = - \\sum_{i=1}^{n} \u03b1_i(s_i \u2013 s_i^o)^2$.\n\u2022 'Sparse': $r = $\n$\\begin{cases}\n1 &\\text{if } (s \u2013 s_i^o)^2 < \u03b5 \\\\\n0 & \\text{otherwise}\n\\end{cases}$\n\nIn training we use the standard Soft-Actor Critic [36] and the standard model parameters [26]. The dynamics models are provided in Appendix. All the results are averaged over ten models per system in training, and over five trajectories, in turn, per model in testing. Thus, mean and variance are calculated with 50 trajectories for each reward."}, {"title": "A. Key Observations", "content": "SuPLE and MaxLE do not require the specification of a target state, but instead guide the agent towards unstable equilibria, and then stabilize the agent there. The experiment designer does not need to know where these unstable equilib- ria occur; the agent will find them by itself. SuPLE allows for the derivation of a single policy for both self-righting (swinging up) and staying stable at the up-right position.\nIn contrast, Quadratic and Sparse rewards do require an explicit specification of the target states, e.g., the upright position for self-righting robotic systems. In general, to specify unstable equilibria requires one to carry out a sen- sitivity analysis [30]. Thus, in order to manually design effective reward, one needs a) to understand the properties of dynamics, and b) to explicitly calculate/specify the target states. In contrast, SuPLE reward does require neither 'a)' nor 'b)' in stabilization tasks.\nThe experiments show that in the simple environments (Single Pendulum, Cart Pole) the task is solved by all the above-mentioned reward functions without auxiliary explo- ration. However, in the Double Pendulum, exploration is challenging, and neither Sparse nor Error-based rewards succeed to solve the task, while SuPLE succeeds. Swinging up and stabilizing the Double Pendulum are prototypical primitives for self-righting multi-linked robots, making Su-PLE a promising candidate for such in real robotic systems, where auxiliary exploration is unpractical.\nBelow, we show the SuPLE landscape SuPLE and the test error between the current state and the unstable equilibrium. The task is to learn, using samples, a controller for self-righting from the bottom with different rewards: externally- provided (Sparse and Quadratic) and intrinsically-calculated (SuPLE and MaxLE)."}, {"title": "B. Experiments", "content": "1) Simple Pendulum: This simple prototypical model for uprighting is the minmum benchmark test for new methods. It is a 2D system with its state at time t given by s(t) = [\u03b8, \u03b8'], \u03b8 being the pendulum angle with \u03b8 = \u03c0 rad at the top. The agent controls the systems by applying torque, |a(t)| < 1 N/m, directly to the angular acceleration. For this simple system, all four rewards allow the agent to successfully learn a stabilizing policy without auxiliary exploration (resetting at arbitrary states) Figure 2a.\n2) Cart Pole: This system is more complicated due to the indirect control of the angular pole by the agent's action (force) applied to the cart. This system is prototypical for 'Segway'-style locomotion. Usually the standard RL methods [36] train the swinging-up phase and the stabilization phase separately with two different rewards resulting in two different controllers, respectively. Here, to demonstrate the robustness and generality of SuPLE, we train the agent with the same reward function without manually separating it into two phases.\nThe test error at Figure 2b shows that the non-informative sparse reward does not allow the agent to solve the task without resetting 'Cart Pole' at random arbitrary states in training, which is unpractical with a real system. Additionally, the agent achieves the upright position more slowly in com- parison to the 'Simple Pendulum', when trained with the externally provided Quadratic reward.\n3) Double Pendulum: This is a complicated system with chaotic dynamics and a hard-to-explore state space. It is a prototypical system for multi-linked systems with the task of self-righting. This experiment is to validate that SuPLE is able to solve this task in realistic training conditions, when one resets to fixed rather than arbitrary states in multi-linked, e.g. humanoid, robots. As shown at Figure 2c amongst all rewards, only SuPLE is able to solve the task.\nThe landscape of SuPLE and MaxLE in Figure 3, shows a marked difference between the two choices in the $\u03b8_2$ vs. $\u03b8'_2$ plane. Only SuPLE captures the volume expansion and has a markedly pronounced ridge along the upright-preferring states, whereas the MaxLE maxima are far more intricately distributed.\nFinally, Figure 4 shows that, with auxiliary exploration, the agent can solve the task with any of the rewards."}, {"title": "VI. DISCUSSION", "content": "The truncated Lyapunov exponents as a reward signal do not only stabilize the system near its balancing points, but produce the full swing-up process. This method is superior to traditional RL, which requires hand-crafted reward functions. In the latter, often much effort needs to be invested into designing appropriate reward functions which reach satisfactory results efficiently. In problematic scenarios, either much domain knowledge needs to be incorporated into the reward (the whole field of reward shaping [7] ); or else, one needs a suitably chosen RL algorithm that propagates the reward signal swiftly throughout the policy domain.\nAll this is not required with the Lyapunov-based method. It uses inherent properties of the dynamics of the system to \"ride the crest of sensitivity\u201d towards the points of maximum instability. It is plausible that this approach works in the neighbourhood of the unstable points of the system: mov- ing towards greater Lyapunov exponents indicates a more unstable dynamics which, if bounded, ultimately reaches a state of locally maximal instability. Such states are often desired targets of a controller. A less obvious property of the SupLE method is, though, that the \u201ccrests\u201d of sensitivity reach deeply into the state space. Thus, it is often possible to reach such a crest from larger parts of the state space and then proceed to ride it all the way towards the state of maximum sensitivity. In other words, the basins of attraction for a controller based on Lyapunov exponents are remarkably large. As for now, there is no closed theory for the size of these basins.\nCreating essentially domain knowledge-free rewards through SuPLE, however, comes at a price. In a traditional RL formalism, the designer can make specific states desir- able or undesirable, independently of the properties of the dynamics, for instance, tuning the rewards towards particular goal areas that have nothing to do with particularly unstable or otherwise distinguished points of the dynamics.\nLyapunov exponents can obviously not be used to induce such arbitrarily imposed behaviours. However, in by far most typical use cases, rewards are designed to achieve particular distinguished states in the domain space, which the Lyapunov exponents seem particularly suited to detect. Presently, this observation is made on a purely empirical basis, but we highlight the links between SuPLE and the notion of empowerment [12], [14], [20]. The latter has been shown to be effective in a surprisingly wide range of control scenarios [23].\nWe conjecture a further link. In a linear time-invariant control system, the Lyapunov exponents are linked to the the lower bound on the number of bits per time the controller needs to receive in order to stabilize the dynamics [37]. Thus, by maximizing the SuPLE reward, one might be find states where one can \u201cpry open\" the channel through which a system could be stabilized.\nIn conclusion, we found that the Lyapunov Exponents provide \"natural\" rewards directly from system properties. In particular, no hand-designed reward-shaping is necessary, as long as the desired tasks bear a relation to \"interesting\" (unstable) points of the dynamics. Stable points are not particularly interesting for control, since little to no control is required to reach them. One other striking insight is that the crests leading to the summit points of this Lyapunov-induced reward landscape often reach far into the state space and thus provide highly structured, informative and thus easy to learn value functions throughout the state space"}]}