{"title": "ON THE SPECIALIZATION OF NEURAL MODULES", "authors": ["Devon Jarvis", "Richard Klein", "Benjamin Rosman", "Andrew M. Saxe"], "abstract": "A number of machine learning models have been proposed with the goal of achiev-ing systematic generalization: the ability to reason about new situations by com-bining aspects of previous experiences. These models leverage compositional architectures which aim to learn specialized modules dedicated to structures in a task that can be composed to solve novel problems with similar structures. While the compositionality of these architectures is guaranteed by design, the modules specializing is not. Here we theoretically study the ability of network modules to specialize to useful structures in a dataset and achieve systematic generaliza-tion. To this end we introduce a minimal space of datasets motivated by practical systematic generalization benchmarks. From this space of datasets we present a mathematical definition of systematicity and study the learning dynamics of linear neural modules when solving components of the task. Our results shed light on the difficulty of module specialization, what is required for modules to successfully specialize, and the necessity of modular architectures to achieve systematicity. Finally, we confirm that the theoretical results in our tractable setting generalize to more complex datasets and non-linear architectures.", "sections": [{"title": "INTRODUCTION", "content": "Humans frequently display the ability to systematically generalize, that is, to leverage specific learning experiences in diverse new settings (Lake et al., 2019). For instance, exploiting the approximate compositionality of natural language, humans can combine a finite set of words or phonemes into a near-infinite set of sentences, words, and meanings. Someone who understands \"brown dog\" and \"black cat\" also likely understands \u201cbrown cat,\u201d to take one example from Szab\u00f3 (2012). The result is that a human's ability to reason about situations or phenomena extends far beyond their ability to directly experience and learn from all such situations or phenomena.\nDeep learning techniques have made great strides in tasks like machine translation and language prediction, providing proof of principle that they can succeed in quasi-compositional domains. However, these methods are typically data hungry and the same networks often fail to generalize in even simple settings when training data are scarce (Lake & Baroni, 2018b; Lake et al., 2019). Empirically, the degree of systematicity in deep networks is influenced by many factors. One possibility is that the learning dynamics in a deep network could impart an implicit inductive bias toward systematic structure (Hupkes et al., 2020); however, a number of studies have identified situations where depth alone is insufficient for structured generalization (Pollack, 1990; Niklasson & Sharkey, 1992; Phillips & Wiles, 1993; Lake & Baroni, 2018b; Mittal et al., 2022). Another significant factor is architectural modularity, which can enable a system to generalize when modules are appropriately configured (Vani et al., 2021; Phillips, 1995). However, identifying the right modularity through learning remains challenging (Mittal et al., 2022). In spite of these (and many other) possibilities for improving systematicity (Hupkes et al., 2020), it remains unclear when standard deep neural networks will exhibit systematic generalization (Dankers et al., 2021), reflecting a long-standing theoretical debate stretching back to the first wave of connectionist deep networks (Rumelhart & McClelland, 1986; Pollack, 1990; Fodor & Pylyshyn, 1988; Smolensky, 1991; 1990; Hadley, 1993; 1994).\nIn this work we theoretically study the ability of neural modules to specialize to structures in a dataset. Our goal is to provide a formalism for systematic generalization and to begin to concretize some of"}, {"title": "BACKGROUND", "content": "Systematic generalization has been proposed as a key feature of intelligent learning agents which can generalize to novel stimuli in their environment (Hockett & Hockett, 1960; Fodor & Pylyshyn, 1988; Hadley, 1993; Kirby et al., 2015; Lake et al., 2017; Mittal et al., 2022). In particular, the closely related concept of compositional structure has been shown to have benefits for both learning speed (Shalev-Shwartz & Shashua, 2016; Ren et al., 2019) and generalizability (Lazaridou et al., 2018). There are, however, counter-examples which find only a weak correlation between compositionality and generalization (Andreas, 2018) or learning speed (Kharitonov & Baroni, 2020). In most cases neural networks do not manage to generalize systematically (Ruis et al., 2020; Mittal et al., 2022), or systematic generalization occurs only with the addition of explicit regularizers or a degree of supervision on the learned features (Shalev-Shwartz et al., 2017; Wies et al., 2022) which is also termed \"mediated perception\u201d (Shalev-Shwartz & Shashua, 2016).\nNeural Module Networks (NMNs) (Andreas et al., 2016; Hu et al., 2017; 2018) have become one successful method of creating network architectures which generalize systematically. By (jointly) training individual neural modules on particular subsets of data or to perform particular subtasks, the modules will specialize. These modules can then be combined in new ways when an unseen data point is input to the model. Thus, through the composition of the modules, the model will systematically generalize, assuming that the correct modules can be structured together. Bahdanau et al. (2019b) show, however, that strong regularizers are required for the correct module structures to be learned. Specifically, it was shown that neural modules become coupled and as a result do not specialize in a manner which can be useful to the compositional design of the architecture. Thus, without task-specific regularizers, systematic mappings did not emerge with NMNs.\nSimilar problems arise with other models which are compositional in nature. Tensor Product Networks (Smolensky et al., 2022) for example aim to learn an encoding of place and content for features in a data point. These encodings are then tensor-producted and summed. By the nature of separating"}, {"title": "A SPACE OF DATASETS WITH COMPOSITIONAL SUB-STRUCTURE", "content": "The notion of systematic generalization is broad, and has been assessed using a variety of datasets and paradigms (Hupkes et al., 2020). Here we introduce a simple setting motivated by the SCAN (Lake & Baroni, 2018a) and grounded-SCAN (gSCAN) datasets (Ruis et al., 2020) commonly used to evaluate systematic generalization with practical neural models. Other similar benchmarks have also been used in prior work (Hupkes et al., 2019; Bahdanau et al., 2019a). In the SCAN and gSCAN datasets a command is given to an agent such as \u201cjump left twice\u201d or \u201cwalk to the red small circle cautiously\". In the case of gSCAN an image of a grid-world is also presented depicting the agent and a number of objects sharing similar features. These features include small vs large, red vs blue vs green vs yellow, circle vs square vs rhombus. The agent is then tasked with producing a sequence of primitive actions such as \u201cL_turn\u201d, \u201cR_turn\u201d, \u201cWALK\u201d. In the case where an agent is told to carry out an action in a specific manner (an adverb is used in the command) a sequence of memorized actions must also be completed. For example to perform an action \u201ccautiously\u201d means that the action string \"L_turn, R_turn, R_turn, L-turn\" must be output before each step forward.\nFor our space of datasets we aimed to imitate the primary features of these established benchmarks while making sure the task remains linearly solvable to allow for our theoretical analysis. An example of one dataset from the space of datasets is depicted in Fig. 1. Conceptually the task is for an agent to navigate a series of paths to reach the desired object in the domain as specified by a command. The command which is input to the model specifies the features of the object to be found where the features are \"large vs small\u201d, \u201csquare vs circle\u201d, \u201cred vs blue\u201d. The agent is also told the index of the object in the 1-dimensional space of objects and so it also knows the absolute position in the environment. The agent must then output a string of three actions where each action is either \"L_turn\" or \"R_turn\". Finally, in the agent's output we also include a set of unique actions which correspond to the agent executing an entire memorized behaviour (one per object in the environment). These full-behaviour outputs are reminiscent of the agent in gSCAN remembering that the \"cautiously\" command maps to a sequence of four actions (looking left and right)."}, {"title": "SYSTEMATICITY AS EXPLOITING LOWER-RANK SUB-STRUCTURE", "content": "Novel to our analysis, datasets in this space allow redundant solutions: the compositional output component can be generated based on compositional input features alone (systematic mappings), but they can equally be generated using non-compositional features alone, or some mixture of the two (non-systematic mappings). This formalization of the datasets is motivated by common systematicity benchmarks, but it is also reflective of a more general fact: that in many settings there are multiple ways to solve a problem. However, it is not the case that all approaches generalize equally well. This means that the inductive biases placed upon a model which influence the kinds of mappings it learns is a key consideration, even if the difference is not apparent on training data. In Sections 5 and 6 we aim to formalize and study the architectural inductive biases which push a model towards systematic mappings. We begin in this section with a mathematical definition of systematicity for our space of datasets.\nWhile some prior theoretical works have defined systematic generalization, such as the grading from weak to semantic systematicity (Hadley, 1994; Bod\u00e9n & Niklasson, 2000), these works have remained behavioural and relied on linguistic notions of syntax. Thus, a general mathematical definition of systematicity has remained elusive. In more recent empirical studies (Bahdanau et al., 2019b; Lake & Baroni, 2018b) it is intuitive based on the domain what systematicity would be. However, this context dependent nature of systematicity is the root of the difficulty in defining it. Thus, Definition 3.1 offers a formal definition of systematicity for our space of datasets.\nDefinition 3.1. Systematic generalization is the identification and learning of lower-rank sub-structure (\u03a9\u03c7, \u03a9y) in the full dataset (X, Y) such that the rank of the population covariance on the sub-structure: \u0395[\u03a9\u03c7\u03a9\u03c7T], \u0395[\u03a9y\u03a9\u03c7T] is lower than the rank of the population covariance on the dataset as a whole: E[XXT], E[Y XT]. Consequently, it is more probable that a training sample will be full rank on the sub-structure than on the full dataset, which facilitates generalization on the sub-structure.\nThe definition above relies on the rank of the covariance matrix to define systematicity. It is important then to note that by partitioning portions of the feature or output space it is possible to change the rank of the sub-problems which emerge. Take for example the dataset shown in Figure 1. The rank of the entire dataset covariance matrix is 8 due to the identity block being orthogonal. Thus, for the network to learn the full mapping it must see all 8 data points. However, if we only consider the compositional inputs and outputs the rank of this portion of the covariance matrix (the compositional covariance) is 3. Thus, even though there are 8 unique data points a model would only need to see 3 data points which are not linearly independent to learn that portion of the mapping if learned in isolation. For 3 binary variables the probability of obtaining a full-rank compositional covariance matrix from 3 samples is 57% and from 4 samples is 91% (see Appendix B for the calculation of the sample probabilities for 3 and 4 compositional features). Thus, if the model were to learn the mapping between compositional sub-structures in isolation it would be nearly certain to generalize to the entire dataset having seen only half of the data points in the training set. This is our notion of systematicity: the ability to learn portions of a mapping with lower-rank sub-structure distinct from"}, {"title": "LEARNING DYNAMICS IN SHALLOW AND DEEP LINEAR NETWORKS", "content": "While deep linear networks can only represent linear input-output mappings, the dynamics of learning change dramatically with the introduction of one or more hidden layers (Fukumizu, 1998; Saxe et al., 2014; 2019; Arora et al., 2018; Lampinen & Ganguli, 2019), and the learning problem becomes non-convex (Baldi & Hornik, 1989). They therefore serve as a tractable model of the influence of depth specifically on learning dynamics, which prior work has shown to impart a low-rank inductive bias on the linear mapping (Huh et al., 2021).\nWe leverage known exact solutions to the dynamics of learning from small random weights in deep linear networks (Saxe et al., 2014; 2019) to describe the full learning trajectory analytically for every dataset in our space. We take the novel theoretical approach of writing these dynamics in terms of the dataset parameters for our space of datasets (Equations 4-9 and 11-12). This allows us to analyse the effect of dataset structure on the training dynamics and learned mappings. In particular, consider a single hidden layer network computing output \u0177 = W\u00b2W\u00b9x in response to an input x, trained to minimize the mean squared error loss using full batch gradient descent with small learning rate \u03f5 (full details and technical assumptions are given in Appendix C). The network's total input-output map after t epochs of training is\nW\u00b2(t)W\u00b9(t) = UA(t)VT,\nwhere A(t) is a diagonal matrix of singular values. The dynamics of A(t), as well as the orthogonal matrices U and V, depend on the singular value decomposition of the input- and input-output correlations in the dataset. If the input- and input-output correlations can be expressed as\n\u03a3x = E[XXT] = VDVT, \u03a3yx = E[YXT] = USVT\nwhere U and V are orthogonal matrices of singular vectors and S, D are diagonal matrices of singular values/eigenvalues, then the diagonal elements of A(t)aa = \u03c0a(t) evolve through time as\n\u03c0a(t) = \\frac{\u03bba/\u03b4a}{1-(1-\\frac{\u03bba}{\u03b4a}\u03c0o) exp(-\\frac{2nx\u03f5t}{\u03c4})}\nwhere \u03bba and \u03b4a are the associated input-output singular value and input eigenvalue (Saa and Daa respectively), \u03c0o denotes the singular value at initialization, and \u03c4 = \\frac{1}{2nx\u03f5} is the learning time constant. These dynamics describe a trajectory which begins at the initial value \u03c0o when t = 0 and increases to the correct asymptotic value \u03c0a = \u03bba/\u03b4a as t \u2192 \u221e. This trajectory corresponds to the network learning the covariance between the input and output data - the correct mapping.\nIn essence, the network's total input-output mapping at all times in training is a function of the singular value decomposition of the dataset statistics. We find that there are three distinct input-output singular values which we denote \u03bb1, \u03bb2 and \u03bb3; two distinct input singular values \u03b41 and \u03b42; and therefore three asymptotes \u03c01, \u03c02 and \u03c03 (the final point for each SV trajectory \u03c01, \u03c02 and \u03c03):\n\u03bb\u2081 = \\frac{(kxr\u00b2 + 2^{nx})(kyr\u00b2 + 2^{nx})^{\\frac{1}{2}}}{2^{2nx}},\n\u03c0\u2081 = \u03bb\u2081/\u03b4\u2081 = \\frac{kyr\u00b2 + 2^{nx}}{kxr\u00b2 + 2^{nx}},\n\u03bb\u2082 = \\frac{(kxr\u00b2 + 2^{nx})(kyr\u00b2)^{\\frac{1}{2}}}{2^{2nx}},\n\u03c0\u2082 = \u03bb\u2082/\u03b4\u2081 = (\\frac{kyr\u00b2}{kxr\u00b2 + 2^{nx}})^{\\frac{1}{2}},\n\u03bb\u2083 = \\frac{k_x k_y r^{4}}{2^{2nx}},\n\u03c0\u2083 = \u03bb\u2083/\u03b4\u2082 = (\\frac{kyr^{2}}{2^{nx}})"}, {"title": "THE EVOLUTION OF SYSTEMATICITY OVER LEARNING", "content": "With the decomposition of the training dynamics and a definition of systematicity for our space of datasets in hand we now look to understand the extent to which a network comes to rely on lower-rank sub-structure in a dataset to generalize systematically. To do this we calculate the Frobenius norm of the network's mapping (the function implemented by the network transforming inputs to outputs) between different subsets of input and output components. To account for all datasets in the space and obtain the full dynamics over training, the norms are expressed in terms of the modes of variation. In particular we split the input-output mapping into four partitions: compositional inputs (\u03a9x) to compositional outputs (\u03a9y); non-compositional inputs (\u0393x) to compositional outputs (\u03a9y); compositional inputs (\u03a9x) to non-compositional outputs (\u0393y); and non-compositional inputs (\u0393x) to non-compositional outputs (\u0393y). These norms provide a precise measure of the network's association between input and output blocks. For example, \u0393x\u03a9y-Norm depicts how much the network relies on the non-compositional input component (\u0393x) to label the compositional output component (\u03a9y). Thus, by analysing the partitioned norms we are able to determine how much the model is relying on certain sub-structures in the data, and as a result, how systematic the model is at all times during training.\nAnalytical expressions for these norms over training (which rely on both singular value dynamics and the structure of the singular vectors) are given in Appendix E Eqns. 17-20 due to space constraints."}, {"title": "MODULARITY AND NETWORK ARCHITECTURE", "content": "We now turn to modularity and network architecture as a prominent approach for promoting systematicity in a network's mapping (Bahdanau et al., 2019b; Vani et al., 2021). Architectures such as NMNs (Andreas et al., 2016; Hu et al., 2017; 2018) learn re-configurable modules that implement specialized (lower-rank in accordance with Definition 3.1) aspects of a larger problem. By rearranging existing modules to process a novel input, they can generalize far beyond their training set. Here we investigate whether simple forms of additional architectural structure can aid in module"}, {"title": "COMPOSITIONAL MNIST (CMNIST)", "content": "To evaluate how well our results generalize to non-linear networks and more complex datasets, in this section we train a deep Convolutional Neural Network (CNN) to learn a compositional variant of MNIST (CMNIST) shown in Figure 4a. In this dataset three digits from MNIST are stacked horizontally, resulting in a value between 0 and 999. The systematic output encodes each digit in a 10-way one-hot vector, resulting in a 30-dimensional vector. The non-systematic output encodes the number as a whole with a 1000-dimensional one-hot vector. This task is similar to the SVHN dataset (Netzer et al., 2011) with added non-systematic labels."}, {"title": "DISCUSSION", "content": "In this work we have theoretically and empirically studied the ability of simple NN modules to specialize and acquire systematic knowledge. We found that this ability is challenging even in our simple setting. Neither implicit biases in learning dynamics, nor all but the most stringent task-specific modularity, caused networks to exploit compositional sub-structure in the data. Our results complement recent empirical studies, helping to highlight the complex factors influencing systematic generalization. For example, in more complex datasets such as CLEVR (Johnson et al., 2017), a module which should specialise to identifying the colour red would not specialise if a Ferrari logo (a non-compositional identifying feature) was present in many of these images \u2013 the network would not identify sub-structure in \u03a3\". Similarly, if many images containing red features mapped to a particular label, for example the \"car\" label, this would also affect which features the module specialised to. When the module is then used to \u201cfind the red ball\" it would be looking for stereotypical car features to identify the colour red \u2013 it does not identify sub-structure in \u03a3y\". Thus, in more natural settings our proposed notion of rank extends to describing the complexity of the correlations being learned. Identifying and using the colour red in images is less complex than identifying and using the presence of a Ferrari; requiring many different input features (higher rank \u03a3\") to be identified and correlated to a certain label (higher rank \u03a3\"). Importantly for Definition 3.1, if a task is specific to Ferraris, then having a specialised Ferrari module is useful (all test images will also contain a Ferrari). But in general scene description tasks it is not. Consequently, Definition 3.1 describes systematicity in terms of the task structure at hand: \u03a3\" and Ex. Thus, as our theoretical module-centric perspective displays, modules must be allocated perfectly; such that the only consistent correlation presented to a module is the one it must specialise to. A natural question emerges: which inductive biases are flexible enough to achieve systematicity without being tailored to a specific problem? A neuroscientific perspective is that sparsity plays a role in producing systematic representations, particularly in the cerebellum (Cayco-Gajic & Silver, 2019). Thus, exploring the utility of sparsity for systematicity is an important direction of future work. We hope that the formal perspective provided in this work will lead to greater understanding of these phenomena and aid the design of improved, modular, learning systems."}, {"title": "RANK OF COMPOSITIONAL DATASET SUB-STRUCTURES", "content": "In this section we provide the calculations of the chance of sampling a full-rank matrix for the compositional partition of the input and output space. We will begin by considering the case where the number of compositional input and output features are both d = 3. We emphasize that this section aims to demonstrate the benefit of relying on compositional sub-structure to learn a network mapping and is not meant to be general to all forms of binary matrices. Since we are using three input and output features there are 23 = 8 unique data points in the input and output. We are concerned then with the rank of the empirical covariance matrix \u00cayx \u2208 [R3\u00d73. We use the empirical covariance matrix since we only use samples of the dataset to train a network and not the full dataset.\nFor a neural network to learn a generalizable mapping it needs to learn from a batch of data with a full-rank covariance matrix that has the same rank as the full dataset. If this is the case then the network will learn the same mapping from the training sample as it would on the full dataset and will generalize to all unseen data points from the same space. We note that for sample sizes of n = 1 and n = 2 it is impossible to obtain a full-rank empirical covariance. Thus, we first consider a sample size of n = 3. In the 3-dimensional case the only way to obtain a singular matrix from 3 samples is if the opposite data point to an earlier data point is sampled. For example, for the data point [-1, -1, 1] the opposite will be [1, 1, -1]. With this observation we can determine the probability of not sampling an opposite data point as follows:\nP(fullrank|n = 3, d = 3) =P(throw3|throw1, throw2)P(throw2|throw\u2081)P(throw1)\n=(4/6)(6/7)(8/8) = 0.57\nFor a sample of n = 4 data points a similar procedure applies, except we need to account for the fact that we are able to sample one opposite data point now. Thus, there are four cases: the second data point is an opposite, the third data point is an opposite, the fourth data point is an opposite, or there are no opposites (we also drop P(throw\u2081) = 8/8 to lighten notation):\nP(fullrank|n = 4, d = 3) =\nP(throw4|throw1, throw2, throw3)P(throw3|throw1,throw2)P(\u00acthrow2|throw1)\n+P(throw4|throw\u2081, throw2, throw3)P(\u00acthrow3|throw1,throw2)P(throw2|throw\u2081)\n+P(\u00acthrow4|throw1, throw2, throw3)P(throw3|throw\u2081,throw2)P(throw2|throw\u2081)\n+P(throw4|throw1,throw2, throw3)P(throw3|throw1,throw2)P(throw2|throw\u2081)\n=(4/5)(6/6)(1/7) + (4/5)(2/6)(6/7) + (3/5)(4/6)(6/7) + (2/5)(4/6)(6/7)\n=0.91\nFor a sample size of n = 5 or more we are guaranteed to have at least 3 linearly independent data points and a full-rank covariance matrix. Thus, the network module trained on this sub-structure will always generalize. A similar set of calculation can be done for the case of 4 compositional features and the same conclusions hold. The calculations, however, become large due to larger sample sizes being used and more possibilities to sample linearly dependent vectors. Thus, we present the probabilities from empirical results for the 3 features and 4 feature cases in Tables 1 and 2 respectively. For these empirical results 5000 samplings from the dataset are used for each sample size to calculate a covariance matrix and determine if it is singular."}, {"title": "LEARNING DYNAMICS IN DEEP LINEAR NETWORKS", "content": "The dynamics of learning for shallow and deep linear networks are derived in Saxe et al. (2019). We state full details of our specific setting here. We train a linear network with one hidden layer to minimize the quadratic loss L(W\u00b9, W\u00b2) = \\frac{1}{2}||Y \u2013 W\u00b2W\u00b9X||\u00bd using gradient descent. This gives the learning rules E[AW\u00b9] = -\\frac{\u03f5}{nx}W\u00b2T (\u2211yx \u2013 W\u00b2W\u00b9\u03a3xx)XT and E[AW\u00b2] = -\\frac{\u03f5}{nx}(\u2211yx \u2013 W\u00b2W\u00b9\u03a3xx)(W\u00b9\u03a3xx)T. By using a small learning rate \u03f5 and taking the continuous time limit, the mean change in weights is given by \\frac{d}{dt}W\u00b9 = -\\frac{\u03f5}{nx}W\u00b2T (\u2211yx \u2013 W\u00b2W\u00b9\u03a3xx) and \\frac{d}{dt}W\u00b2 = -\\frac{\u03f5}{nx} (\u2211yx \u2013 W\u00b2W\u00b9\u03a3xx)W\u00b97 where \u03a3xx = E[XXT] is the input correlation matrix, \u2211yx = E[YXT] is the input-output correlation matrix and \u03c4 = \\frac{1}{2nx\u03f5}. Here, t measures units of learning epochs. It is helpful to note that since we are using a small learning rate the full batch gradient descent and stochastic gradient descent dynamics will be the same. Saxe et al. (2019) has shown that the learning dynamics depend\non the singular value decomposition of \u2211yx = U(2\u03a31 \u03bbaiuaiT )VT and \u03a3xx = V(2\u03a32x\u03b4ai\u03c5\u03b1i)VT. To solve for the dynamics we require that the right singular vectors V of \u03a3yx are\nalso the singular vectors of \u03a3xx. This is the case for any dataset in our space, as shown in Appendix D. Note, we assume that the network has at least 2nx hidden neurons (the number of singular values in\nthe input-output covariance matrix) so that it can learn the desired mapping perfectly. If this is not the case then the model will learn the top nh singular values of the input-output mapping where nh is\nthe number of hidden neurons (Saxe et al., 2014). Given the SVDs of the two correlation matrices the\nlearning dynamics can be described explicitly as W\u00b2(t)W1(t) = UA(t)VT = (2\u03a3x \u03c0a(t)uvaT ) \nwhere A(t) is the effective singular value matrix of the network's mapping. The trajectory of each\nsingular value in A(t) is described as \u03c0a(t) =  \\frac{\u03bba/\u03b4a}{1-(1-\\frac{\u03bba}{\u03b4a}\u03c0o) exp(-\\frac{2nx\u03f5t}{\u03c4})}. From these dynamics it is\nhelpful to note that the time-course of the trajectory is only dependent on the \u2211xx singular values.\nThus, \u03a3xx affects the stable point of the network singular values but not the time-course of learning.\nWe have chosen our datasets to be simple and interpretable for clarity and tractability. However,\nfundamentally, our analysis with deep linear networks applies to a more general situation for which\nother theoretical techniques relying on random initialisation fail (Geiger et al., 2020; Jacot et al.,\n2018; Sirignano & Spiliopoulos, 2020). In particular, consider the broader space of datasets in which\ncompositional inputs, compositional outputs, non-compositional inputs, and non-compositional\noutputs are each rotated by individual orthogonal mappings. As one specific example, consider the\ndataset formed by applying a permutation to the compositional outputs. This change affects only\nthe singular vectors of the task (also permuting them relative to the original task), not the singular\nvalues. This dataset can no longer be solved by learning an identity transformation (which is a\npossible solution for all datasets in the original space), as the network must learn to implement the\nrequired permutation\u2013but it will do so with identical learning dynamics and systematicity properties.\nA random initialisation cannot know this desired permutation, which can only be learned through\nerror feedback. This, further, motivates the use of the linear network dynamics which incorporates\nfeature learning into the analysis."}, {"title": "SINGULAR VALUE DECOMPOSITION EQUATIONS", "content": "In this section we provide the general formulas for the singular value decomposition of the \u03a3xx and\n\u03a3yx covariance matrices for any dataset in the space of datasets. As stated in Section 4 the right\nsingular vectors of \u03a3yx must match the singular vectors of \u03a3xx which is the V matrix below. Thus,\n\u03a3xx = VDVT and \u03a3yx = USVT.\nLet:\nA = (\u03a9x\u03a9xT)T\u03a9x\u03a9x\nB = \u03a9y\u03a9xT\u03a9x\nC = \u03a9y\u03a9xT\u03a9x\nTHPT = (\\frac{1}{23nx}(\u03bax r2+2nx))I2nx\u00d72nx\u2212((\\frac{1}{23nx})kyr4)\u03a9yT\u03a9y\nWhere THPT is the SVD of (\\frac{1}{23nx}(\u03bax r2+2nx))I2nx\u00d72nx\u2212((\\frac{1}{23nx})kyr4), and H = (\\frac{1}{23nx}(\u03bax r2+2nx)) = I2nx \u00d72nx .\nThen the following are the matrix formulas for the components of the SVD for \u03a3yx and \u03a3xx.\nU =\n[\n23nx (\u03bayr2)\n((23na (kur\u00b2+2mm) B+(23n (ka) C\u2212B\\frac{1}{T})\\frac{1}{2}\\frac{r2}{23nx(kur2+2nx)}Onyx\u03bax2nx\nVT =\n(((kxr2+2nx)(kyr2+2nx)})\n26nx\n(Kxr2+2nx)2+\u03ba2xr242)\\frac{1}{4}"}, {"title": "IMPERFECT OUTPUT PARTITIONS", "content": "We now investigate the case where the split network architecture does not perfectly partition the output into the compositional and non-compositional components. In this case some of the non-compositional identity output blocks are grouped with the compositional outputs (we only consider partitioning along the compositional and non-compositional blocks to keep the closed form solutions tractable). Thus, we separate the number of non-compositional outputs ky into the number of non-compositional outputs of the left network branch kleft and right network branch kright, such that kleft + kright = ky.\nEquations 27 to 32 show the set of norms which emerge. In the extreme cases when kright = ky we recover the dense network equations shown in Equations 17 to 20, since \u03c0\u2082 = \u03c03 = 0 for Equations 31 and 32. This is apparent from the singular value equations for \u03c02 and \u03c03 shown in Equations 7 and 9 with ky in the numerator which is replaced by kright 0 for the right module. Thus, Equation 31 and 32 fall away completely. In the other extreme case of kleft = ky we recover the split network equations shown in Equations 21 to 24. This is again because \u03c02 = \u03c03 = 0 but this time from the left network branch's perspective. By definition kleft = 0 in this case and so all components of Equations 29 and 30 fall away, leaving just Equations 27 and 28 with Equations 31 and 32. The most important conclusion to be drawn, however, is that if any non-compositional components are partitioned with compositional components then the module will behave similarly to the dense network of Section 5. This can be shown the same way as in the proof for Observation 5.1, by noting that Equations 29 and 30 will be non-zero for any case where the compositional input to compositional output is learned (due to a shared mode of variation). Thus, only the most stringent and perfectly allocated network modules will be able to specialize."}, {"title": "ALTERNATIVE LEARNING RULES TO GRADIENT DESCENT", "content": "In this work we have relied on gradient descent dynamics since it is the most widely used optimization algorithm for training neural networks. However, a number of alternatives exist and previous work (Cao et al., 2020) has characterized a two-dimensional space of learning rules. The two dimensions correspond to the two parameters \u03b3 and \u03b7 which determine which optimization algorithm is used. The update equations for the algorithms in the space for a one-hidden layer network are given by Equations 33 and 34, where ||W1n||2 depicts the L2 norm is taken along the rows of the W\u2081 matrix, leaving a column vector of dimension equal to the number of hidden neurons.\nAW\u2081 = \\begin{cases}\\frac{\\int(1/\u03b3)W^{T}(\\Sigma_{yx} - W_{2}W_{1}\\Sigma_{xx}) + \\eta(W_{1}X)(X^{T} - X^{T}W_{1}^{T}W_{1})}{||(1/\u03b3)\\Sigma_{xx} - W_{2}W_{1} + \\eta(1/(1 - ||W_{1}n||_{2}^{2})) (W_{1}XX^{T})||}&\\text{if }\\eta > 0\\\\\\frac{\\int(1/\u03b3)W_{1}^{T}(\\Sigma_{yx} - W_{2}W_{1}\\Sigma_{xx}) + \\eta(1/(1 - ||W_{1n}||_{2}^{2})) (W_{1}XX^{T})}&\\text{otherwise}\\end{cases}\nAW2 = (\u03a3yx \u2013 W2W1\u03a3xx)W1 + \u03b3(YYT \u2013 \u0176\u0176T)W2\nAs in (Cao et al., 2020) we focus on four learning rules within this space of learning rules"}]}