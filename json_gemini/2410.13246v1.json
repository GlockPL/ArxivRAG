{"title": "Atomic Calibration of LLMs in Long-Form Generations", "authors": ["Caiqi Zhang", "Ruihan Yang", "Zhisong Zhang", "Xinting Huang", "Sen Yang", "Dong Yu", "Nigel Collier"], "abstract": "Large language models (LLMs) often suffer from hallucinations, posing significant challenges for real-world applications. Confidence calibration, which estimates the underlying uncertainty of model predictions, is essential to enhance the LLMs' trustworthiness. Existing research on LLM calibration has primarily focused on short-form tasks, providing a single confidence score at the response level (macro calibration). However, this approach is insufficient for long-form generations, where responses often contain more complex statements and may include both accurate and inaccurate information. Therefore, we introduce atomic calibration, a novel approach that evaluates factuality calibration at a fine-grained level by breaking down long responses into atomic claims. We classify confidence elicitation methods into discriminative and generative types and demonstrate that their combination can enhance calibration. Our extensive experiments on various LLMs and datasets show that atomic calibration is well-suited for long-form generation and can also improve macro calibration results. Additionally, atomic calibration reveals insightful patterns in LLM confidence throughout the generation process.", "sections": [{"title": "Introduction", "content": "While large language models (LLMs), such as Llama (Touvron et al., 2023), Mistral (Jiang et al., 2023), and GPT models (OpenAI, 2022), excel in various tasks, they still struggle with faithfulness and reliability issues. LLMs often suffer from hallucinations, generating factually inaccurate content and misleading responses (Zhang et al., 2023b; Huang et al., 2023), which limits their application in high-risk real-world scenarios (Hu et al., 2023). To address this, confidence calibration aims to estimate the underlying uncertainty of model predictions and reflect the true likelihood of correctness (Guo et al., 2017). A calibrated model is crucial for real-world applications, as it allows us to determine the extent to which we can trust models' predictions (Zhu et al., 2023; Mahaut et al., 2024). Most existing work on LLM calibration focuses on short-form QA tasks (Jiang et al., 2021; Tian et al., 2023; Zhu et al., 2023; Ulmer et al., 2024). These studies often use datasets like TriviaQA (Joshi et al., 2017) and Natural Questions (Joshi et al., 2017), where answers typically have fewer than 10 words. However, in real-world applications, responses to user queries are often much longer (Zhang et al., 2024), sometimes extending to hundreds or even thousands of words. In such cases, the quality of LLM responses is not simply binary (correct or incorrect), as answers may include both accurate and inaccurate statements. Previous work on long-form calibration has primarily focused on response-level calibration (Zhang et al., 2024; Huang et al., 2024) (which we term macro calibration). This approach provides an overall confidence estimation for the entire response (as shown in the upper part of Figure 1). However, it is insufficient for long-form generation, as it cannot adequately capture the model's fine-grained uncertainty across multiple factual statements.\nIn this work, we introduce the concept of atomic calibration, which evaluates calibration at the fine-grained level of atomic claims (as illustrated in the lower part of Figure 1). We focus on the aspect of factuality, since hallucination are a widely recognized issue of LLMs (Zhang et al., 2023b; Huang et al., 2023) and the factuality of statements can be objectively determined. Accurate factual calibration is crucial for mitigating the hallucination problem (Mahaut et al., 2024). To perform atomic calibration, we decompose long responses into self-contained atomic claims and evaluate the calibration for all atomic claims. Various confidence elicitation methods are studied to provide accurate confidence estimation. We provide a detailed investigation into long-form generation calibration, covering 7 LLMs and 3 datasets. The main contributions of our work are as follows:\n\u2022 We propose the novel concept of atomic calibration. Unlike macro calibration, which provides a single response-level confidence score, atomic calibration offers a more fine-grained analysis of model calibration, making it better suited for long-form generation (\u00a72).\n\u2022 We introduce a categorization of confidence elicitation methods for long-form generation into two types: discriminative and generative, estimating the model's intrinsic and external confidence, respectively. We examine five confidence elicitation methods and assess their effectiveness for atomic calibration (\u00a73).\n\u2022 We show that atomic calibration is particularly well-suited for long-form generation, and that macro calibration can be enhanced by incorporating atomic calibration results. We also find the two types of confidence elicitation methods are complementary and their fusion can provide better calibration results. Atomic calibration also enables deeper analysis, revealing patterns in confidence and calibration changes throughout the generation process (\u00a75)."}, {"title": "Atomic Calibration", "content": "For a language model $M$, let $x \\in X$ represent the response generated by model $M$ for a query $q$, and let $y \\in V_t$ denote the corresponding label, where $V_t \\subset [0, 1]$ indicates a quality score for a specific task $t \\in T$. Note that unlike multiple-choice and short-form questions, which typically only focus on the correctness of the answer, the tasks in $T$ involve various aspects, such as factuality, coherence, creativity, etc.\nWe define a probability prediction function $f: X \\rightarrow \\triangle_t$, where $\\triangle$ represents the $|t|$-dimensional simplex. In this context, $f(x)y$ denotes the probability assigned to the label $y$ given the generated output $x$. In the remaining of this work, we focus specifically on calibrating for factuality. In this case, $y$ represent $Y_t$ when $t$ corresponds to factuality. $Y$ takes values in $[0, 1]$, indicating the factuality level of a response. Based on the calibration concepts discussed in (Guo et al., 2017), we define the calibration of each response as follows:\nDefinition 1 (Macro Calibration on Factuality) A language model $M$ that produces generations $x \\sim M(x | q)$ is said to be response-level (macro) calibrated if\n$P(y | f(x)y = \\beta) = \\beta, \\forall \\beta \\in \\triangle_{|t|}.$\nIn the context of long-form generation, a single response $x$ may encompass multiple atomic claims. Macro calibration at the response level cannot fully present the fine-grained uncertainty at the atomic level. To address this, we decompose the response $x$ into $N$ atomic claims $c_i$, represented as $x = \\cup_{i=1}^N c_i$. Each atomic claim $c_i$ is assigned a binary label $y_i \\in V_i$, where $V_i = {0, 1}$, indicating its truthfulness. The overall factuality score for the response $y$ is computed as $y = \\frac{1}{N} \\sum_{i=1}^N Y_i$. Similarly, we define $f(c_i)y_i$ as the probability of the label $y_i$ given the atomic claim $c_i$. Building on this decomposition, we propose a fine-grained measure of calibration at the atomic level as follows:\nDefinition 2 (Atomic Calibration on Factuality) A language model $M$, which generates a long-form response $x$ conditioned on the query $q$, $x \\sim M(x | q)$, is considered atomic-level calibrated if, for each atomic claim $c_i$ with its corresponding label $y_i$, the following condition holds:\n$P (Y_i | f(C_i)y_i = \\beta_i) = \\beta_i, \\forall \\beta_i \\in \\triangle_{|Vi|}.$"}, {"title": "Confidence Elicitation Methods", "content": "To calculate the confidence scores for model outputs in long-form generation, we define two types of confidence elicitation methods: generative and discriminative. Generative methods assume that the consistency between different generation samples provides a reliable estimation of model uncertainty. The more frequently certain facts are covered by the samples, the higher the model's confidence in those facts. In contrast, discriminative methods assess uncertainties by asking the model itself. This is motivated by the findings that models tend to perform better on discriminative tasks (Saunders et al., 2022), and thus they may already possess the capability to estimate the confidence of their own outputs in a discriminative manner.\nWe first generate one response $x$ as the answer to the query $q$. Then, $x$ is broken into atomic claims $C$. Following (Min et al., 2023; Wei et al., 2024; Zhao et al., 2024), each atomic claim contains a single piece of information and must be self-contained. For generative methods, we sample an additional set of responses $K$, and compare them against the original response $x$. For each atomic claim in $C$, we assign it a confidence score."}, {"title": "Generative Methods", "content": "GEN-BINARY. The basic assumption here is that if a fact is frequently conveyed when sampled multiple times, the model is considered \u201cconfident\" about that fact. For an atomic claim $c_i$ in $C$, we utilize a natural language inference model $M_{NLI}$ to examine whether $c_i$ is supported or not supported by each of the additional samples. Let $K_s$ be the set of samples supporting $c_i$. Then, the confidence in $c_i$ is calculated as:\n$Conf(c_i, K) = \\frac{K_s}{|K|}.$\nGEN-MULTI. GEN-MULTI assumes that the model is more confident in facts that are consistently expressed. Unlike GEN-BINARY, it further divides the \"not supported\" class $K_{ns}$ into \u201cconflict\" ($K_c$) if the fact is presented differently in the sample, and \u201cnot mentioned\u201d ($K_{nm}$) if the fact is not mentioned in the sample. We then calculate the confidence by only considering supporting and conflicting samples:\n$Conf (c_i, K) = \\frac{K_s}{K_s + K_c}$"}, {"title": "Discriminative Methods", "content": "DIS-SINGLE. Following (Kadavath et al., 2022; Tian et al., 2023), we directly ask the model whether one single atomic claim is true or false. The probability the model assigns to token \u201cTrue\u201d ($P(true)$) in its generation is viewed as the confidence. As each atomic claim is judged individually, one advantage of this method is that there is no cross-claim influences when the model makes confidence judgments.\nDIS-CONTEXT. In addition to the method where each claim is judged in a self-contained way, we also consider a setting where additional context is provided. Here, the context denotes the passage where the atomic claim is extracted, or the prompt that generates the response. The context helps the model to more accurately locate the atomic claim, and thus potentially leads to better confidence elicitation. $P(true)$, given the context, is then used as the confidence score, just as in DIS-SINGLE.\nDIS-RATING. Instead of using $P(true)$, in DIS-RATING, we directly prompt the model to assign a numerical value representing its confidence in the atomic claim $c_i$. A score of 0 indicates no confidence, while 10 represents maximum confidence. An alternative approach is to use semantic expressions ranging from \"Very Uncertain\" to \u201cVery Confident\". However, Tian et al. (2023) show that LLMs express uncertainty as effectively, or even more effectively, using numerical values rather than words."}, {"title": "Confidence Fusion Strategies", "content": "Since the generative and discriminative methods provide different perspectives of confidence estimation, it is natural to explore fusion methods that combine different confidence estimates for better calibration. After exploration, we consider four simple but effective options: (1) MinConf selects the minimum of the two confidence values, providing a conservative estimate; (2) HMean calculates the harmonic mean, providing a balanced fusion that weights lower confidence values more heavily, to help prevent overconfident predictions; (3) ProdConf multiplies the confidences, ensuring a high final confidence only when both estimates are strong, thereby effectively penalizing discrepancies between the two estimates; and (4) WAvg computes the weighted average, with a hyper-parameter weight, prioritizing the more reliable estimate and allowing flexibility to adjust the influence of each confidence source based on prior knowledge or validation results."}, {"title": "Evaluation Metrics", "content": ""}, {"title": "Atomic Calibration", "content": "In atomic calibration, we utilize FACTSCORE (Min et al., 2023) to measure the atomic claims' factuality. For each claim, we have its binary factuality label and a continuous confidence score. Since this corresponds to standard calibration scenarios, we follow the conventions from previous work (Kuhn et al., 2022; Zhu et al., 2023; Tian et al., 2023) and use Expected Calibration Error (ECE) (Naeini et al., 2015), Brier Score (Brier, 1950), and AUROC to assess calibration. Details of the three metrics are in Appendix A."}, {"title": "Macro Calibration", "content": "In macro calibration, factuality scores for responses are represented as continuous values from 0 to 1, therefore, the atomic calibration metrics mentioned above are not directly applicable. Apart from the Spearman Correlation, we propose two novel metrics UCCE and QCCE that extend ECE to continuous factuality scores.\nSpearman Correlation. Following (Zhang et al., 2024; Huang et al., 2024), we calculate Spearman Correlation to assess whether samples with higher factuality have correspondingly higher confidence scores. Compared to Pearson Correlation, it focuses on assessing the rank correlation, is robust to outliers and does not require that data is in normal distribution.\nUniform Continuous Calibration Error (UCCE). UCCE evaluates how well a model's continuous predictions align with the true values by dividing the predicted values into bins of equal size. This metric assesses the model's calibration across the range of predictions. The calibration error is computed by comparing the mean predicted and true values within each bin.\nLet $\\hat{y_i}$ represent the predicted value for the i-th sample, $y_i$ the true value for the i-th sample, $B_m$ the set of samples falling into the m-th bin, $M$ the number of bins, and $N$ the total number of samples. The UCCE is computed as follows:\n$UCCE = \\sum_{m=1}^M \\frac{|B_m|}{N} |\\frac{\\sum_{i \\in B_m} y_i}{|B_m|} - \\frac{\\sum_{i \\in B_m} \\hat{y_i}}{|B_m|}|$\nUCCE is particularly effective for scenarios where predicted values are uniformly distributed.\nQuantile Continuous Calibration Error (QCCE). QCCE uses a similar approach to UCCE but utilizes quantile binning, ensuring that each bin contains an equal number of samples. Consequently, each bin has a size of $|B_m| = \\frac{N}{M}$. QCCE is advantageous for skewed or non-uniform distributions of predicted values, as it avoids sparsity in any bin by ensuring a balanced representation across all bins. The QCCE is calculated as follows:\n$QCCE = \\frac{1}{M} \\sum_{m=1}^M |\\frac{1}{|B_m|} \\sum_{i \\in B_m} y_i - \\frac{1}{|B_m|} \\sum_{i \\in B_m} \\hat{y_i}|$"}, {"title": "Experiments and Results", "content": ""}, {"title": "Experiment Setup", "content": "Models. We utilize seven LLMs from three model families with varying sizes: Llama3 Instruct (8B and 70B) (Meta, 2024), Mistral Instruct (7B and 8x7B) (Jiang et al., 2023), and Qwen2 Instruct (7B, 52B-A14B, and 72B) (Yang et al., 2024).\nDatasets. We use three datasets for long-form QA: Bios (Min et al., 2023), which contains 500 individuals from Wikipedia with varying levels of popularity, for which models are tasked to generate biographies; LongFact (Wei et al., 2024) extends Bios"}, {"title": "Results", "content": "Overall, the tested LLMs are not well-calibrated at the atomic fact level. Table 1 lists our main atomic calibration results, which indicate that different confidence elicitation methods yield varying calibration scores. Although there is no universally accepted threshold for low ECE, a well-calibrated model typically achieves an ECE close to 1%, as shown in (Guo et al., 2017) and (Zhu et al., 2023). However, even with the most robust method, GEN-BINARY, the ECE scores remain around 10%, indicating a significant calibration gap. Among the models, Qwen2-7B-Instruct demonstrates slightly better calibration compared to the other two.\nAtomic calibration can enhance macro calibration. Table 2 shows the main results of response-level calibration. For the five atomic-level methods, we calculate the average confidence of the facts in a response to obtain the response-level confidence. The results indicate that atomic calibration leads to better overall results compared to the baseline methods, highlighting the helpfulness of more fine-grained calibration analysis.\nConfidence fusion can further improve both atomic and macro calibration. Table 4 presents the results of various confidence fusion strategies at the atomic level (with response-level results in Appendix D). The best results are consistently achieved by fusion methods. Among these strategies, the weighted average (WAvg) is the most effective. The benefits of the fusion strategies are further discussed in \u00a76.1. Interestingly, we find that combining methods within the same confidence type (such as DIS-RATING with DIS-CONTEXT) does not lead to improved calibration (in Appendix D). A case study showing the effectiveness of confidence fusion is in Figure 4."}, {"title": "Discussion", "content": ""}, {"title": "Confidence Methods Alignment", "content": "To further explore the reasons behind the improvements provided by confidence fusion, we show the correlation between different confidence elicitation methods in Figure 2 (using WildHallu as the study case and more results are in Appendix E). Our findings are summarized as follows:\nConfidence methods within the same type are better aligned. In Figure 2, warmer colors indicate higher Spearman correlation scores. Confidence elicitaton methods of the same type (top left for generative and bottom right for discriminative) show stronger correlations compared to those across different types. This helps to explain why fusing generative and discriminative methods are effective, since these two types capture different aspects of uncertainty and are complementary to each other.\nThe alignment is stronger at the response level than at the atomic level. When comparing atomic and macro calibration, we observe that the alignment is stronger for the latter. In atomic calibration, several methods display weak correlations (indicated in blue), while the correlations are generally higher at response level (indicated in red). Similarly, methods from different types show more disagreement than those of the same type. This highlights the need for future research on the discrepancies between generative and discriminative confidence elicitation methods, as well as how to better unify these approaches."}, {"title": "Confidence Across Different Positions", "content": "As each long-form response contains multiple atomic facts, we analyze how confidence and factuality scores evolve during the generation process. Specifically, we divide all atomic facts $C$ into five equal parts along the generation process. Part 1 represents the beginning of the generation, and part 5 corresponds to the end. We calculate the average confidence score for each part of the responses and present the results in Figure 3.\nWith discriminative methods, models exhibit decreasing confidence in atomic facts as the generation progresses. We observe similar trends across all discriminative methods. This contrasts with previous findings, which used logits as a measure of confidence and found that models tend to become more confident during long generation sequences (Zhang et al., 2023a). Our results show that discriminative methods indicate lower confidence in the model's output toward the latter parts of the generation.\nWith generative methods, the model shows the lowest average confidence in the middle part of the generation. We hypothesize that this is because the tested models tend to provide general introductions and conclusions at the beginning and the end of the generation. During consistency checking, these statements are frequently cross-referenced, leading to higher confidence. For example, in Bios, statements like \"[a person] is famous\" or \"[a person] made a significant impact in his field\" are often repeated across samples. On the contrary, in the middle parts where the models address more specific facts about individuals' lives, careers and achievements, they tend to cover different aspects and details."}, {"title": "Related Work", "content": "Atomic Facts Generation and Verification. Long-form responses often contain both correct and incorrect statements, which impact the overall factuality assessments. Min et al. (2023) propose breaking long responses into atomic facts and calculating the precision of these fact pieces to determine the overall factuality score. Wei et al. (2024) and Zhao et al. (2024) extend this paradigm by expanding the dataset to include more domains beyond biographies. Song et al. (2024) design VERISCORE for diverse long-form generation tasks that feature both verifiable and unverifiable content. Chiang and Lee (2024) introduce D-FACTSCORE, specifically designed for content with ambiguous entities. Overall, the approach of breaking long-form responses into atomic facts for verification is nowadays widely accepted in factuality assessment.\nUncertainty and Calibration in Long-form Generations. Existing research on uncertainty estimation and calibration primarily focuses on multiple-choice or short-form questions (Zhu et al., 2023; Kuhn et al., 2022; Lin et al., 2023; Tian et al., 2023; Ulmer et al., 2024). However, limited work has explored calibration for long-form generations. Huang et al. (2024) proposed a unified calibration framework for all text generation tasks, comparing distributions of both correctness and the associated confidence of responses. Band et al. (2024) introduced linguistic calibration, where models explicitly express their uncertainty during long-form generation. Zhang et al. (2024) proposed LUQ, an uncertainty estimation method tailored to long-form generation, demonstrating its effectiveness in ensembling different LLMs. None of the aforementioned studies have systematically examined calibration at a fine-grained level. Our work aims to fill this gap with a focus on factuality."}, {"title": "Conclusion", "content": "We introduce atomic calibration, a fine-grained approach for evaluating LLM calibration at the atomic claim level, addressing the limitations of traditional response-level calibration in long-form generation. Our experiments show that atomic calibration is well-suited for long-form generation and complements macro calibration. We also demon-"}, {"title": "Ethics Statement", "content": "Our research adheres to strict ethical standards. We ensured compliance with the licenses of all datasets and models used. No human participants were involved in our experiments. After thorough assessment, we do not anticipate any additional ethical concerns or risks related to our work."}, {"title": "Limitation", "content": "First, our work primarily focuses on the factuality aspect of LLMs. As mentioned in Section 2, the task t can be various aspects of the quality of a long-form response, such as coherence, creativity, writing style, and more. Unlike previous studies that use the overall quality of long-form responses to evaluate calibration (Huang et al., 2024), we concentrate specifically on factuality in this paper. We argue that the hallucination problem is among the most significant challenges faced by LLMs (Zhang et al., 2023b; Huang et al., 2023).\nSecond, we test the calibration only on open-source LLMs for two main reasons: (1) After assessing the atomic and macro calibration levels of LLMs, our next step is to adjust the model to better reflect its confidence (i.e., for better calibration). Closed-source models are not directly applicable to this calibration process. (2) Our discrimination methods typically require logit access, which is generally unavailable in closed-source models. If logits are accessible, our methods can be directly applied to closed-source models without affecting the atomic calibration process."}, {"title": "Appendix", "content": ""}, {"title": "Atomic Calibration Metrics", "content": "ECE In computing the Expected Calibration Error (ECE), the predictions are sorted and divided into a fixed number of bins K. The predicted value of each test instance falls into one of the bins. ECE uses empirical estimates as follows:\n$ECE = \\sum_{i=1}^K P(i) \u00b7 |o_i \u2013 c_i|,$\nwhere $o_i$ is the true fraction of positive instances in bin i, $c_i$ is the mean of the post-calibrated probabilities for the instances in bin i, and P(i) is the empirical probability (fraction) of all instances that fall into bin i. The lower the ECE value is, the better a model is calibrated.\nBrier Score The Brier score measures the accuracy of probabilistic predictions. For unidimensional predictions, it is strictly equivalent to the mean squared error as applied to predicted probabilities. Suppose that on each of the n occasions an event can occur in only one of r possible classes or categories and on one such occasion i, the forecast probabilities are $f_{i1}, f_{i2}, ... f_{ir}$, that the event will occur in classes 1, 2, ... r, respectively. The r classes are chosen to be mutually exclusive and exhaustive so that\n$\\sum_{i=1}^r f_{ij} = 1, i = 1, 2, 3, ... n.$\nThe Brier Score P is then defined by\n$P = \\frac{1}{n} \\sum_{j=1}^r \\sum_{i=1}^n (f_{ij} \u2013 E_{ij})^2,$\nwhere $E_{ij}$ takes the value 1 or 0 according to whether the event occurred in class j or not.\nAUROC Following (Kuhn et al., 2022), AUROC metric is equivalent to the probability that a randomly chosen correct answer has a higher confidence score than a randomly chosen incorrect answer. Higher scores are better for AUROC, and perfect confidence score is 1, while a random confidence measure would be 0.5."}, {"title": "Prompts", "content": ""}]}