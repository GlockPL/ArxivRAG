{"title": "RELIABLE EXPLAINABILITY OF DEEP LEARNING SPATIAL-SPECTRAL CLASSIFIERS\nFOR IMPROVED SEMANTIC SEGMENTATION IN AUTONOMOUS DRIVING", "authors": ["Jon Guti\u00e9rrez-Zaballa", "Koldo Basterretxea", "Javier Echanobe"], "abstract": "Integrating hyperspectral imagery (HSI) with deep neural net-\nworks (DNNs) can strengthen the accuracy of intelligent vi-\nsion systems by combining spectral and spatial information,\nwhich is useful for tasks like semantic segmentation in au-\ntonomous driving. To advance research in such safety-critical\nsystems, determining the precise contribution of spectral in-\nformation to complex DNNs' output is needed. To address\nthis, several saliency methods, such as class activation maps\n(CAM), have been proposed primarily for image classifica-\ntion. However, recent studies have raised concerns regarding\ntheir reliability. In this paper, we address their limitations and\npropose an alternative approach by leveraging the data pro-\nvided by activations and weights from relevant DNN layers to\nbetter capture the relationship between input features and pre-\ndictions. The study aims to assess the superior performance\nof HSI compared to 3-channel and single-channel DNNs. We\nalso address the influence of spectral signature normalization\nfor enhancing DNN robustness in real-world driving condi-\ntions.", "sections": [{"title": "1. INTRODUCTION", "content": "A key challenge in computer vision is metamerism, which\nhinders the robustness of tristimulus based systems. Metamerism\noccurs when the reflectance curves of different materials\nmatch under certain conditions, like specific illumination or\nsensor characteristics, but differ under others [1].\nHyperspectral sensors offer a promising solution to ad-\ndress metamerism and increase the robustness of intelligent\nvision systems. These sensors capture light across narrow-\nband filters usually spanning visible to infrared wavelengths.\nHyperspectral imaging (HSI) has proven valuable in fields\nsuch as food quality assessment, medical analysis, and remote\nsensing [2] and its extension to new application domains, such\nas autonomous driving systems (ADS), is currently being in-\nvestigated. Despite its benefits, HSI poses challenges for clas-\nsification tasks due to its high dimensionality, inter-class sim-\nilarity, and intra-class variability [3]. To achieve pixel-level\nprecision in applications like semantic or instance segmen-\ntation, combining spectral and spatial information is crucial.\nOne approach to achieving this is by using fully convolutional\nnetworks (FCNs), a type of deep neural network (DNN) with\nan encoder-decoder structure.\nTo comprehend the contribution of spectral information\nbeyond the numerical evaluations provided by ablation ex-\nperiments, DNNs must be interpretable, meaning we should\nbe able to interpret their behavior in the decision-making pro-\ncesses. Explainable AI, which focuses on the interpretability\nof neural networks, is a hot topic nowadays [5]. In the intel-\nligent vision area, the most common methods for identifying\nwhich image features are most relevant for predictions are the\nsaliency methods, including layer-wise relevance propagation\n(LRP), gradient-based techniques, class activation mapping\n(CAM), and their variants. However, despite recent advance-\nments, many of these methods exhibit notable limitations. As\nnoted by [8, 10, 9, 7], most saliency methods for image classi-\nfication are designed to produce visually appealing represen-\ntations rather than accurately reflecting the underlying predic-\ntion processes.\nIn this article, in Section 2, we first review the desirable\nproperties that saliency methods should have to be considered\nreliable, as discussed in the literature. In Section 3, we extend\nthe CAM conservativeness property -originally defined for\nclassification networks- to semantic segmentation networks.\nNext, in Section 4, we examine the essential characteristics\na CAM-based saliency method for segmentation must have\nto satisfy this property, demonstrating that many such meth-\nods do not adequately fulfill it. To overcome these limitations\nwhen interpreting the inference process on FCNs, in Section\n5 we propose a method that leverages activations and weights\nfrom specific, relevant model layers to more reliably explain\nthe relationship of input information with predictions. We ap-\nply this method to evaluate a segmentation U-Net trained with\nimages from a 25-band hyperspectral snapshot camera in real-\nworld driving scenarios, assessing how spectral information\ncontributes to the enhanced performance of HSI-based clas-"}, {"title": "2. BACKGROUND", "content": ""}, {"title": "2.1. Invariance Tests for Saliency Methods", "content": "The reliability of saliency methods has recently been ques-\ntioned, prompting efforts to identify the invariants that\nsaliency methods should satisfy to be considered trustwor-\nthy. As highlighted by the authors in [8], some widely used\ntechniques are independent of model parameters and train-\ning data, thus failing to establish valid relationships between\ninputs and outputs present in the data.\nIn this context, [8] conducts two sanity checks on various\nsaliency methods: one based on model parameter random-\nization (comparison against a randomly initialized untrained\nnetwork) and the other on data randomization (comparison\nagainst a trained network but with randomly permuted labels).\nGradCAM passes these sanity checks, while Guided Back-\nProp and GuidedGradCAM are invariant to parameters in\nhigher layers, hence failing. However, it is important to note\nthat when dealing with overparameterized, non-optimized\nmodels (unpruned/unquantized models), certain layers in the\nnetwork may contribute minimally to the output, leading to\nsome level of invariability to parameter perturbations. Simi-\nlarly, the authors of [9] show, through adversarial examples,\nthat LRP fails to explain the DNN's decision-making process\nfor original images or adversarial examples, posing a further\nchallenge for reliable explainability methods.\nThe authors of [10] propose the requirement of input in-\nvariance to ensure a reliable interpretation of the input's con-\ntribution to the model predictions. This principle dictates that\na saliency method should reflect the model's sensitivity to\ntransformations of the inputs. Finally, in [7], the authors intro-\nduce the concept of conservativeness in CAM. Conservative-\nness implies that the sum of the contributions in the mapping\naligns with the prediction scores. The authors show that the\noriginal CAM [11] satisfies this property, while GradCAM\n12] does not, leading to the proposal of a variant method\ncalled Extended-CAM."}, {"title": "2.2. Basics of CAM-based Studies", "content": "All methods based on class activation mappings derive from\nthe original CAM [11]. The goal is to investigate the contri-\nbution of the spatial component (i, j) to the final prediction\ny for an image classified as belonging to class c. As previ-\nously mentioned, the sum of these contributions (L) should\nmatch y to satisfy the conservativeness property (Eq. 1).\n$\\Sigma L_{ij} = y$"}, {"title": "3. EXTENDING CONSERVATIVENESS TO\nSEMANTIC SEGMENTATION", "content": "Conservativeness ensures the precise quantitative explanation\nof prediction scores by providing absolute pixelwise contribu-\ntions without redundancy or deficiency [7]. This property is\ncrucial for visualization methods in neural networks focused\non semantic segmentation, as it allows for analyzing the input\ncontributions to a single output pixel, specific pixels in a RoI,\nor every pixel associated with a certain label. If this condition\nis not met, the validity of comparing CAMs from different\nlayers or DNNs becomes questionable.\nIn [7], conservativeness is proposed for image classifica-\ntion models. We extend this property to semantic segmenta-\ntion models in Eq. 5 which indicates that a pixel activation\nmapping $L_{i,j,r,s}$ for class c and output location (r, s) is con-\nsidered conservative if and only if the spatial sum over (i, j)\nequals the target score $y_{rs}$ for every pixel.\n$\\Sigma L_{i,j,r,s} = Y_{rs}$\nClass-level conservativeness is defined in Eq. 6 and de-\npends on pixel-level conservativeness and vice versa."}, {"title": "4. CONSERVATIVENESS ASSESSMENT IN\nGRADCAM- AND SEGGRADCAM-BASED\nMETHODS", "content": "As previously discussed in Section 2, many saliency methods\ndo not adhere to fundamental invariants. Hereafter, we show\nthat several widely used CAM-based methods, included some\ndesigned for classification, also fail to meet the conservative-\nness principle. The selection of these specific methods is mo-\ntivated by their prominence in the literature and their unique\napproaches for calculating $\\alpha_k^c$ coefficients (Eq. 3).\nTo illustrate the non-conservativeness of gradient-based\nsaliency methods, we just can rewrite Equation 1 in the form\nof GradCAM (Eq. 2), which is known not to meet the conser-\nvativeness criterion due to the spatial-averaging global pool-\ning operation performed (Eq. 3), as noted in [7].\nSegGradCAM (see Subsection 2.2) modifies Eq. 4 into\nEq. 8 by applying the commutative property between partial\nderivatives and summation, while omitting the ReLU opera-\ntion for simplicity.\n$\\L_{ij} = \\Sigma \\Sigma L_{i,j,r,s} = \\Sigma \\Sigma A_{ijk}(\\Sigma \\frac{\\partial y_{rs}}{\\partial A_{uvk}})$\nWhen M contains multiple pixels, each pixel has a spe-\ncific logit activation map $L_{i,j,r,s}$, which is added to obtain\nthe logit map of class c, as illustrated in Fig. 1. For a single\npixel in M, the equations for SegGradCAM and GradCAM\nare essentially identical, making the demonstration of non-\nconservativeness straightforward.\nThe use of global average pooling over the gradient in cal-\nculating $\\alpha_k^c$ for semantic segmentation introduces unwanted\neffects. For instance, using an image from HSI-Drive [14],\nthe CAM for the Road Marks class (Fig. 2b) shows nearly ev-\nery pixel contributing, which is counterintuitive. To address\nthis, we can eliminate the average from $\\alpha_k^c$ calculation, giv-\ning spatial dimensions to it. The equation for SegGradCAM\nwith the corrected $\\alpha_{ijk}^c$ is shown in Eq. 9.\nThis correction yields a more meaningful class activa-\ntion mapping for the Road Marks class, as shown in Fig.\n2c. However, a more reasonable appearance does not ensure\nadherence to the conservativeness principle. Our evaluation\ndemonstrates that Eq. 9 can also be derived by extending\nExtended-CAM [7] formulation, which satisfies conserva-\ntiveness principle for classification, to the segmentation task.\nThe authors of ExtendedCAM suggest clipping negative\nvalues of the class activation mapping using ReLU functions,\nclaiming it enhances visualization accuracy. However, this\npractice violates the conservativeness principle and risks los-\ning critical information regarding regions contributing nega-\ntively to the classification. Furthermore, for ExtendedCAM\nand its extension to segmentation to maintain conservative-\nness, nonlinear layers of the network must be approximable\nby the first term of a Taylor decomposition. This is a con-\ndition not generally applicable to segmentation DNNs em-\nploying standard data processing structures, including Max-\nPooling and Concatenation layers like those in U-Net.\nThe SegXResCAM method proposed in [15] employs\npooling/unpooling operations on gradients to regulate their\ngranularity. Nevertheless, similar to the average pooling"}, {"title": "5. ANALYZING THE CONTRIBUTION OF\nSPECTRAL INFORMATION IN HSI\nSEGMENTATION DNN MODELS", "content": ""}, {"title": "5.1. A DNN for Scene Understanding Using HSI", "content": "HSI-Drive [14] is a comprehensive dataset designed specifi-\ncally for researching HSI technology in developing more ac-\ncurate and robust ADS, particularly under adverse environ-\nmental conditions. We trained and optimized various U-Net-\nbased machine learning models on HSI-Drive using different\nclassification targets to create image segmentation systems of\nvarying complexity. To assess the contribution of spectral in-\nformation to the inference processes of these models, we eval-\nuated the performance of the same model architecture using\n1, 3, and all the 25 spectral channels provided by an Imec 5x5\nmosaic-type near-infrared (NIR) HSI sensor. Fig. 3 shows an\nexample of the 5-class segmentation experiment comprising\nRoad (tarmac), Road Marks, Vegetation, Sky, and \"Other\"\nclasses. For more in-depth information regarding the DNN\narchitecture and the dataset used in this article, the reader is\nreferred to [14, ?].\nHSI-Drive was recorded with a snapshot camera equipped\nwith a multispectral filter array (MSFA) based on Fabry-Perot\ninterferometers. This technology, when used with no rejec-\ntion filters, as was the case, can lead to some input channels\ncontaining mixed contributions from distinct wavelengths\n14]. For this study, 1-channel images consist of the MSFA\ncentral band, which exhibits a spectral response centered\naround 731.883 nm. 3-channel images, which were gener-\nated as an alternative to RGB imaging (no visible RGB bands\ncan be extracted from these images), consist of the three\nmost informative spectral bands, which were extracted using\nthe orthogonal space projection method as explained in [18].\nSpecifically, these bands include: 9 (with peak at 770.576\nnm), 22 (featuring a primary peak at 858.948 nm and a sec-\nondary peak at 577.147 nm), and 25 (having a primary peak\nat 944.485 nm and a secondary peak at 657.995 nm).\nFor the 25-band images, we applied per-pixel normaliza-\ntion (PN) in the preprocessing stage, which involves dividing\neach of the 25 spectral components by the total sum of all."}, {"title": "5.2. Activation and Weight Study for DNN Explainability", "content": "Since none of the saliency methods described in Section 4\nprovide reliable information for comparing the three models\nunder study (1, 3 and 25 spectral channels), we propose to\nfocus the explainability analysis on features that directly con-\ntribute to the computation of the final prediction values in the\nmodel.\nFirstly, we examined the activations before the final con-\nvolution layer (conv2D_21) and the weights and biases of\nthe last convolutional layer (conv2D_22). The output of\nconv2D_21 consists of 32 non-negative feature channels\nwith the same size as the input. Each of the 5 output class\nnodes has a set of 32 weights and 32 biases. The first analysis\nis aimed at determining the amount of relevant feature maps\nper class, identifying any features that contribute similarly\nacross multiple classes, and finding features with significant\npositive or negative activations for specific classes.\nDue to skip connections in the U-net model [?], the sec-\nond convolutional block (conv2D_1) is directly linked to the\nfinal convolutional block. This helps to preserve spatial infor-\nmation lost during subsequent encoding layers, thus providing"}, {"title": "5.2.1. Activation of conv2D_21", "content": "For a comparative visual analysis, we have selected and\ncropped six activations from this layer, shown in Fig. 5. To\nensure a fair comparison across the three models, we focused\non the three most correlated activations. This election was\nmade by calculating the correlations between the weight vec-\ntors of each channel, as shown on the X-axis of Fig. 4. For\nexample, channel 21 of the HSI model shows the highest\ncorrelation with channel 1 of the 1-channel model and chan-\nnel 21 of the 3-channel model. Across all models, weights\nassociated with Vegetation (green dot), Sky (blue dot), and\nOthers (yellow dot) hover around 0.2, while weights linked to\nRoad (black dot) and Road Marks (red dot) are approximately\n-0.15.\nRow a of Fig. 5 compares the outputs of three correlated\nchannels that are typically positively activated for Vegetation,\nSky, and Other classes, while being negatively activated for\nRoad and Road Marks classes. It can be observed that the\nactivations in the 25-channel model are less homogeneous in\nhighly diverse areas of the image, indicating an effective uti-\nlization of the spectral information. The advantages of com-\nbining HSI information and PN, which aim to homogenize\nregions partially in sun and shadow, are evident in Figs. 3a to\n3c and row d from Fig. 5.\nIn row b of Fig. 5, which shows strong activation for the\nRoad class, we observe certain regions of Vegetation that are\nerroneously activated in both the 1-channel and, to a lesser\nextent, 3-channel models. This highlights a significant draw-\nback of PN: the lack of edge definition, which leads to poorer\nsegmentation of the Road Marks class (Table 1). An analysis\nof the channels that are highly activated for the Road Marks\nclass (row c from Fig. 5) and the channels activated for both\nthe Road and Road Marks classes (row f from Fig. 5) re-\nveals this issue further. While the borders are less sharp for\nthe 25-band model, it is noteworthy that there are no erro-"}, {"title": "5.2.2. Activation of conv2D_1", "content": "In this case, we used a different criterion to select the chan-\nnels to display, since the convolutional kernels from layer\nconv2D_2 are 3x3 instead of 1x1. We examined the 32 ac-\ntivations from the three models and identified the three most\nsignificant ones that exhibit the greatest similarity as shown\nin Fig. 6.\nDue to PN normalization, as seen in Fig. 6c, the activa-\ntions are noisier and more heterogeneous, leading to defec-\ntive edge detection of road lines (such as curvy lines and ze-"}, {"title": "6. DISCUSSION AND FUTURE WORK", "content": "As shown in this article, saliency methods show significant\nlimitations for the explainability of DNNs for image segmen-\ntation tasks. We reviewed various tests and invariants pro-\nposed in the literature, including network and data random-\nization, class activation mapping conservativeness, and adver-\nsarial examples, to assess the reliability of these methods.\nIn particular, by extending the definition of conserva-\ntiveness from image classification to image segmentation,\nwe show that CAM-based methods, which differ in their ap-\nproaches to obtaining $\\alpha_k^c$, do not meet the conservativeness\ncriteria. This insight suggests that the utility of these methods\nfor precise model interpretation is questionable. Furthermore,\nwe observed that most proposed saliency methods prioritize\nvisually appealing representations over accurate depictions of\nthe actual inference processes in the models.\nGiven these limitations, and with the aim of exploring the\nadvantages of using HSI in segmentation tasks, we propose\nto perform explainability analysis in segmentation FCNs by\nanalyzing those feature extraction layers that directly con-\ntribute to the output predictions. Although limited, we think\nthis approach assures a higher level of reliability in the anal-\nsis. Specifically, we analyzed activations from the penulti-\nmate layer to identify the most strongly triggered activations\nfor each class and from the second layer to evaluate the edge\ndetection process in segmentation tasks. Our findings indi-\ncate that models utilizing a greater number of spectral chan-\nnels can enhance segmentation performance for classes with\nhigh intraclass variability.\nThis 25-channel model was trained using PN, which fa-\nvors information provided by spectral signatures rather than\ngeneral reflectance contrasts. However, PN can lead to less\naccurate edge detection, particularly for classes that more\nstrongly rely on reflectance differences. This is particularly\nevident in the analysis of the second-layer activations, where,\nunlike in the 1-channel and 3-channel models, the PN method\nactivates many regions that do not correspond to theoretical\nedges. However, the model demonstrates greater robust-\nness to changes in illumination, as evidenced by consistent\nsegmentation of regions under both sunlight and shadow.\nThese insights highlight the need for ongoing research"}]}