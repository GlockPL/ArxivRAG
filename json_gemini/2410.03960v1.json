{"title": "SWIFTKV: FAST PREFILL-OPTIMIZED INFERENCE WITH\nKNOWLEDGE-PRESERVING MODEL TRANSFORMATION", "authors": ["Aurick Qiao", "Zhewei Yao", "Samyam Rajbhandari", "Yuxiong He"], "abstract": "LLM inference for popular enterprise use cases, such as summarization, RAG, and\ncode-generation, typically observes orders of magnitude longer prompt lengths than\ngeneration lengths. This characteristic leads to high cost of prefill and increased re-\nsponse latency. In this paper, we present SwiftKV, a novel model transformation and\ndistillation procedure specifically designed to reduce the time and cost of processing\nprompt tokens while preserving high quality of generated tokens. SwiftKV combines\nthree key mechanisms: i) SingleInputKV, which prefills later layers' KV cache using\na much earlier layer's output, allowing prompt tokens to skip much of the model\ncomputation, ii) AcrossKV, which merges the KV caches of neighboring layers to\nreduce the memory footprint and support larger batch size for higher throughput,\nand iii) a knowledge-preserving distillation procedure that can adapt existing LLMs\nfor SwiftKV with minimal accuracy impact and low compute and data requirement.\nFor Llama-3.1-8B and 70B, SwiftKV reduces the compute requirement of prefill\nby 50% and the memory requirement of the KV cache by 62.5% while incurring\nminimum quality degradation across a wide range of tasks. In the end-to-end\ninference serving using an optimized vLLM implementation, SwiftKV realizes up\nto 2x higher aggregate throughput and 60% lower time per output token. It can\nachieve a staggering 560 TFlops/GPU of normalized inference throughput, which\ntranslates to 16K tokens/s for Llama-3.1-70B in 16-bit precision on 4\u00d7 H100 GPUs.", "sections": [{"title": "INTRODUCTION", "content": "Large Language Models (LLMs) are quickly becoming an integral enabler of enterprise applications and\nofferings, including code and data co-pilots (Chen et al., 2021; Pourreza & Rafiei, 2024), retrieval aug-\nmented generation (RAG) (Lewis et al., 2020; Lin et al., 2024), summarization (Pu et al., 2023; Zhang\net al., 2024), and agentic workflows (Wang et al., 2024; Schick et al., 2023). While it is clear that LLMs\ncan add value to these applications, the cost and speed of inference determine their practicality. There-\nfore, improving the aggregate throughput and reducing latency of LLM inference has become an increas-\ningly important topic of interest, with various efforts (Sec. 2) tackling the problem from multiple angles.\nIn this paper, we take a unique approach to improving LLM inference for enterprise applications based\non the key observation that typical enterprise workloads process many more input tokens than output\ntokens. For example, tasks like code completion, text-to-SQL, summarization and RAG each submit\nlong prompts but produce a small number of generated tokens, and a majority of enterprise LLM use\ncases in Snowflake incur a 10:1 ratio between prompt and generated tokens.\nBased on this observation, we designed SwiftKV, which improves throughput and latency by: i)\nreducing the computation required to pre-fill the KV cache for input tokens, and ii) enabling memory\nsavings to support larger batch sizes needed to serve LLMs more cost effectively (Sheng et al., 2023;\nPope et al., 2022; Yu et al., 2022). SwiftKV (Fig. 1) consists of four key components:"}, {"title": "RELATED WORK", "content": "Hardware and System Optimizations. Lower-precision quantization techniques like FP8 (Kuzmin\net al., 2024) can enable the use of tensor-cores to accelerate inference (Luo et al., 2024). System\napproaches like PagedAttention (Kwon et al., 2023), Tensor-Parallelism(Shoeybi et al., 2020),\nSplit-Fuse (Holmes et al., 2024; Agrawal et al., 2024), Flash Attention (Dao et al., 2024), and\ntheir optimized implementations in TensorRT (NVIDIA, 2019), FasterTransformer (NVIDIA,\n2021), vLLM (Kwon et al., 2023), and DeepSpeed-Inference (Aminabadi et al., 2022) enable better\nparallelization, batching, and scheduling to eliminate performance overheads and achieve better\nhardware peak utilization without impacting model quality.\nMemory Compression. A wide range of techniques have been developed to reduce the memory\nfootprint for inference. Lower-precision quantization techniques like FP8/FP4 can reduce the memory\nfootprint for both KV cache and parameters (Hooper et al., 2024). Attention optimization techniques\nlike MQA (Shazeer, 2019), GQA (Ainslie et al., 2023b), low-rank attention (Chang et al., 2024) also\nreduce the KV Cache. All of these approaches are complementary to SwiftKV.\nSimilar to our work AcrossKV, MiniCache (Liu et al. (2024b)) reduces KV Cache by consolidating\nadjacent layers of KV cache. While both consolidate KV cache across adjacent layers, AcrossKV\nenables consolidating more than just two layers, allowing for higher level of compression. Furthermore,\ndue to the distillation based fine-tuning, AcrossKV does not require any token retention strategy\nwhere distinct KV caches are stored for special tokens even for consolidated layers. The simplicity of\nAcrossKV makes it easier to translate the KV cache savings into real world performance improvements.\nOther Inference Optimizations. Speculative Decoding based approaches (Cai et al. (2024); Xia et al.\n(2024)) reduce the number of decoding steps to speed up the generation phase of inference, but it does\nnot reduce prefill computation. Model distillation techniques (e.g., Sreenivas et al. (2024)) transfer the\nknowledge of a large teacher model into a smaller student model, reducing inference computation by\ndecreasing the number of parameters. However, these methods require significant retraining over long\ntoken horizons, and typically incur a significant quality degradation from the teacher to the student\nmodel. In contrast, SwiftKV reduces the computation of the model, not its number of parameters,\nby approximating the KV cache of later layers, and aims to fully recover the model performance using\ndistillation. Moreover, it requires minimal training resources including compute and data."}, {"title": "SWIFTKV: DESIGN AND IMPLEMENTATION", "content": "3.1 PRELIMINARIES\nIn transformer models (Vaswani et al., 2017), attention enables each token to focus on other\ntokens by comparing queries (Q) with keys (K) and using values (V) to compute the final\nrepresentation. For a sequence of input tokens $x^{(1)},...,x^{(n)}$, the projections are defined as follows:\n$Q = XW_Q$, $K = XW_K$, $V = XW_V$, where $X \\in \\mathbb{R}^{n \\times d}$ are the input embeddings, and\n$W_Q\\in\\mathbb{R}^{d\\times d_k}$ and $W_K,W_V\\in\\mathbb{R}^{d\\times d_g}$ are trained model parameters with $d_g|d_k$. Hereafter, we may also\nrefer to $W_K$ and $W_V$ as a single matrix $W_{KV} \\in \\mathbb{R}^{d\\times 2d_k}$.\nDuring the prefill phase of inference, the model processes the entire input sequence at once, computing\nK and V for all tokens in parallel (or in chunks in the case of Split-Fuse (Holmes et al., 2024; Agrawal\net al., 2024)). This typically occurs when the model handles an initial prompt or context.\nDuring the decoding phase of inference, new tokens are generated one at a time. When predicting\nthe next token, only the query ($Q^{(t+1)}$) for the new token needs to be computed, while the model must\nattend to the keys and values ($K^{(1)},...,K^{(t)}, V^{(1)},...,V^{(t)}$) of all previously processed tokens."}, {"title": "SINGLEINPUTKV: PROJECT KV CACHE FROM A SINGLE LAYER", "content": "Assume the input of l-th layer is $x_l$, and its i-th token is $x_l^{(i)}$. Prior studies (Liu et al., 2024c; Gromov\net al., 2024) showed that $x_l$ becomes more similar as the depth grows. Here, we conduct a similar study.\nWe compute the average input similarity between l-th layer's input and all remaining layers' input, i.e.,\n$\\text{SimScore}(x_l) = \\frac{\\sum_{j=l+1}^L \\text{Similarity}(x_l,x_j)}{L-1} ,$ (1)\nwhere L is the number of layers in the model and Similarity ($x_l,x_j$) is the average cosine similarity\nbetween all $x_l^{(i)}$ and $x_j^{(i)}$ tokens.\nBased on this observation, the first key component of SwiftKV is to use l-th layer's output $x_{l+1}$ to\ncompute the KV cache for all remaining layers. More specifically, SwiftKV retains the standard\ntransformer architecture up to and including the l-th layer, but the KV cache for all remaining layers\nare computed immediately using $x_{l+1}$, i.e.\n$KV_j=W^{KV}_l x_{l+1}, \\text{ for all } j>l,$ (2)\nwhere $KV_j$ is the KV cache for j-th layer and $W^{KV}$ is its KV projection weight matrix.\nPrefill Compute Reduction. SingleInputKV can enable significant reduction in prefill computation\nduring LLM inference. Originally, all input tokens need to be processed by all transformer layers\nin order to generate the first new token. Using SingleInputKV, after layer l, since all the required KV\ncache are computed, for all remaining layers (>l), we only need to process the last token in the input.\nWhen prefill computation dominates generated token computation, this reduces the total inference\ncomputation to approximately 1l/L. See Appendix B for more details."}, {"title": "ACROSSKV: SHARING KV CACHE FOR CONSECUTIVE LAYERS", "content": "GQA (Ainslie et al., 2023a), one of the most widely adopted KV cache compression methods, showed\nthat the KV cache can be easily shared within a transformer layer. Later, Liu et al. (2024a) showed\nthat the KV cache can be merged for certain pairs of adjacent layers. AcrossKV extends the ideas\nto cross-layer KV cache sharing.\nParticularly, instead of computing KV cache for all of the remaining layers as shown in equation 2,\nAcrossKV selectively chooses one layer to compute the KV cache for several consecutive layers and\nshare it within the small group. The key idea is shown in Fig. 1. As AcrossKV can combine multiple\nlayers' KV caches into a single one rather than just two adjacent layers, it offers higher potential\nreduction ratio compared to Liu et al. (2024a) while simplifying the implementation to realize the\nbenefits of the KV cache reduction. (See Sec. 2 for more detailed comparison with Liu et al. (2024a))."}, {"title": "KNOWLEDGE RECOVERY", "content": "While SingleInputKV preserves all the original parameters, it re-wires the architecture so that the\nKV cache projections may receive different inputs. We found that this re-wiring (and AcrossKV)\nrequires fine-tuning to recover the original capabilities from the modified model. As we only change\nthe computation of the attention part for layer >l, this can be achieved by fine-tune just the $W_{QKV}$\nweight matrices from the (l + 1)-th layer onwards. However, instead of directly fine-tuning these\nparameters using standard LM loss, we find that distilling using the output logits of the original model\nallows for better knowledge recovery (see Sec. 5 for more details).\nImplementing the Distillation. Since only a few $W_{QKV}$ parameters need fine-tuning, we are able\nto do a memory efficient parameter-sharing based distillation. More specifically, we keep a single copy\nof the original model weights in memory that are frozen during training, and add an extra trainable\ncopy of the $W_{QKV}$ parameters for layers > l initialized using the original model (See Fig. 1).\nDuring the training, we create two forward modes for the later layers > l, one with original frozen\nparameters using original architecture, and another with the SwiftKV re-wiring using new QKV\nprojections i.e.,\n$y_{teacher}=M(x,SwiftKV=False), and y_{student}=M(x,SwiftKV=True),$ (3)\nwhere y. is the final logits, M is the model, and x is the input. Afterwards, we apply the standard\ndistillation loss (L) upon the outputs with temperature ($\\tau$) using (Hinton et al., 2015). After the\ndistillation, the original KV projection layers > l are discarded during inference.\nThis method allows us to perform the distillation for Llama-3.1-8B-Instruct on 680M tokens of data in 3\nhours using 8 H100 GPUs, and Llama-3.1-70B-Instruct in 5 hours using 32 H100 GPUs across 4 nodes."}, {"title": "OPTIMIZED IMPLEMENTATION FOR INFERENCE", "content": "LLM serving systems can be complex and incorporate many simultaneous optimizations at multiple\nlayers of the stack, such as PagedAttention (Kwon et al., 2023), Speculative Decoding (Leviathan et al.,\n2023), SplitFuse (Holmes et al., 2024; Agrawal et al., 2024), and more. One benefit of SwiftKV is that\nit makes minimal changes to the model architecture, limited to only a few linear projection layers. This\nmeans that SwiftKV can easily be integrated into existing serving systems without implementing new\nkernels (e.g. for custom attention operations or sparse computation) or novel inference procedures.\nImplementation in vLLM. To realize the performance benefits of SwiftKV, we integrated it with\nVLLM. Our implementation is compatible with vLLM's chunked prefill, which processes prefill\ntokens in chunks and may mix prefills and decodes in each minibatch. During each forward pass, after\ncompleting layer l, the KV-cache for the remaining layers (>l) are immediately computed, and only\nthe decode tokens are propagated through the rest of the model layers.\nGEMM and Memory Optimizations. Upon this basic implementation, we implemented two\nadditional optimizations. First, SingleInputKV fusion: instead of computing the KV cache $KV_j$ for\neach layer j > l one at a time, we fused all $W^{KV}$ into one large weight matrix $\\bar{W}^{KV}$ so that their\nKV cache can be computed with a single efficient GEMM operation. Second, AcrossKV reduction:\nwe modified vLLM to only allocate one layer's KV-cache for each group of merged layers, which\nrealizes the memory gains of AcrossKV."}, {"title": "MAIN RESULTS", "content": "4.1 SETUP\nTraining and Evaluation. We use Llama-3.1-8B-Instruct and Llama-3.1-70B-Instruct as our base\nmodels for SwiftKV. Our training datasets include a mixture of the full supervised training data from\nHuggingFaceH4/ultrachat_200k (Ding et al., 2023) and teknium/OpenHermes-2.5\n(Teknium, 2023). We evaluated model quality using a modified LM-Eval-Harness (Gao et al.,\n2024)2 due to its support for the custom prompt format of Llama-3.1, particularly for MMLU and\nMMLU-COT (Hendrycks et al., 2021), GSM8K (Cobbe et al., 2021), and Arc-Challenge (Clark et al.,\n2018). For more details, please see Appendix C.\nCompression Metrics. For prefill computation, we report the approximate reduction as (L-l)/L\ndue to SingleInputKV, and for KV cache, we report the exact memory reduction due to AcrossKV.\nFor example, SwiftKV with SingleInputKV (l = L/2) and 4-way AcrossKV is reported as 50% prefill\ncompute reduction and 37.5% KV cache memory reduction. We further study how these theoretical\ncompute and memory reductions translate into end-to-end inference improvements in Sec. 4.3.\nInference Performance. In our inference evaluation, we focus on two common scenarios:\nbatch-inference for cost sensitive scenarios and interactive-inference for latency sensitive scenario.\nBatch-Inference When processing text in bulk or when serving a model under high usage demand, it is\nimportant to achieve high combined throughput in terms of input + output tokens processed per second.\nFor bulk processing, the combined throughput determines the time it takes to finish the job. For interac-\ntive use, it determines the volume of concurrent users and requests that can be supported per unit of hard-\nware. In both scenarios, the combined throughput is a key determinant of the cost of serving the model.\nInteractive-Inference In interactive scenarios (e.g., chatbots, copilots), not only the combined\nthroughput is important, but also metrics that define the end-user experience. Chief upon them are\nthe time-to-first-token (TTFT) and time-per-output-token (TPOT). TTFT is the time between the\nuser sending a message and receiving the first token in the response. TPOT is the time between each\noutput token after the first token has been received. Low TTFT and TPOT are desirable by interactive\napplications to deliver smooth usage experiences.\nFor all experiments on Llama-3.1-8B-Instruct, we use 1 NVIDIA H100 GPU with 80GB of memory,\nand for all experiments on Llama-3.1-70B-Instruct, we use 4 NVIDIA H100 GPUs running the model\nwith 4-way tensor parallelism. We provide the full hardware and vLLM configurations in Appendix C.2"}, {"title": "MODEL QUALITY WITH COMPRESSION", "content": "Llama-3.1-8B-Instruct. The top rows of Table 1 show that SwiftKV can preserve model quality well\nuntil 50% prefill reduction using SingleInputKV. For 25% prefill reduction, the accuracy degradation\nis only about 0.12 points and for 50% reduction, the gap is about 1 point 3. When we push to 62.5%\nreduction (i.e. SingleInputKV with l = 12 and L = 32), the accuracy drops to 66.09 points, which\nis significantly lower than the baseline. This can be explained by the drop in activation similarity from\n0.61 to 0.51 between layer 16 to layer 12 (Fig. 2).\nThe bottom rows of Table 1 show the model quality when adding AcrossKV to 50% SingleInputKV.\nFrom pure SingleInputKV to 2-way AcrossKV, the accuracy drops about 0.9 points with 25% KV\ncache reduction. The accuracy drops by another 0.32, going from 2-way to 8-way sharing, and 0.62\nwhen going all the way to 16-way sharing. Particularly, for the extreme case, i.e., using a single KV\ncache for all remaining layers, the accuracy is only about 2.5 points lower than pure SingleInputKV,\nand could be useful for more memory constrained cases, e.g., embedding and/or mobile devices.\nFurthermore, the design of AcrossKV is complementary to many existing KV cache compression\nmethods. In Sec. 5.4, we show that AcrossKV can be combined with quantization to achieve 62.5%\nreduction in KV cache memory with only a 1-point accuracy gap compared to SingleInputKV only."}, {"title": "INFERENCE PERFORMANCE", "content": "Batch Inference Performance. Fig. 3 shows the results of Llama-3.1-8B and Llama-3.1-70B across\nseveral workloads with a range of input lengths. SwiftKV achieves higher combined throughput than\nthe baseline model across all the workloads we evaluated.\nFor Llama-3.1-8B-Instruct, with 2K input tokens per prompt, SwiftKV achieves 1.2-1.3\u00d7 higher\ncombined throughput than the baseline model, and our benefits increase further to 1.8-1.9\u00d7 higher\ncombined throughput with 128K inputs. Note that for an input length of 8K tokens, SwiftKV achieves\na staggering 30K tokens/sec/GPU (480 TFLOPS/GPU). For Llama-3.1-70B with 2K input tokens\nper prompt, SwiftKV achieves 1.4-1.5\u00d7 higher combined throughput than the baseline model, which\nimproves to 1.8-2.0\u00d7 better combined throughput for 128K inputs.4 As expected, SwiftKV provides\ngreater improvements when the inputs are long.\nWe also observe AcrossKV can further improve the combined throughput due to its ability to reduce\nthe memory usage for the KV-cache and supporting larger batch sizes. For sequence length of 8K,\nLlama-3.1-70B-Instruct with SwiftKV achieves a combined throughput of over 16K toks/sec over\n4xH100 GPUs which corresponds to 560 TFLOPS/GPU of bf16 performance when normalized to\nbaseline. This is an unprecedented throughput for BF16 inference workloads.\nInteractive-Inference Performance. Fig. 4 shows the TTFT and TPOT of Llama-3.1-70B-Instruct\nacross a range of request arrival rates and input lengths. When the arrival rate is too high, the TTFT\nexplodes due to the request queue accumulating faster than they can be processed by the system.\nHowever, SwiftKV can sustain 1.5 2.0x higher arrival rates before experiencing such TTFT\nexplosion. When the arrival rate is low, SwiftKV can reduce the TTFT by up to 50% for workloads\nwith longer input lengths. In terms of TPOT, SwiftKV achieves significant reductions for all but the\nlowest arrival rates, up to 60% for certain settings. A similar story unfolds for Llama-3.1-8B, which\ncan be found in Fig. C.1 in the Appendix."}, {"title": "ABLATION AND DISCUSSION", "content": "5.1 THE IMPACT OF DISTILLATION\nTo demonstrate the effectiveness of our distillation, we train Llama-3.1-8B-Instruct with 50%\nSingleInputKV and no AcrossKV using the standard language model loss, and compare it with our\ndistillation based approach discussed in Sec. 3.4. The results are shown in Table 3 (a). As we can\nsee, the model trained with distillation has a 2.64 point higher average. Particularly, for generative\ntasks, i.e., MMLU-Cot and GSM-8K, the performance improvement is 4.13 and 6.74, respectively."}, {"title": "FULL MODEL TRAINING VS. PARTIAL MODEL TRAINING.", "content": "Our distillation method only fine-tuned the $W_{QKV}$ parameters, as discussed in Sec. 3.4, with the\nhypothesis that it preserves the knowledge from the original models compared to fine-tuning the entire\nmodel. This hypothesis aligns with Meng et al. (2024), Geva et al. (2021), and Elhage et al. (2021),\nwhich suggest that MLP layers player a more prominent role in storing knowledge.\nTo validate this, we fine-tuned a model with 50% SingleInputKV on Llama-3.1-8B-Instruct where all pa-\nrameters in the latter 50% of layers are trained. The results are shown in Table 3 (b). Note that the model\nquality of full model distillation is about 4.5 points lower than our proposed partial model distillation."}, {"title": "THE IMPACT OF FINE-TUNING DATASETS", "content": "Note that in Sec. 4, we did not try to maximize the performance of SwiftKV from the data recipe\nperspective since the search space is very large and outside the scope of our paper. However, we want\nto share some initial findings about the dataset recipe.\nHow good is the data used to train SwiftKV? We chose the datasets to train SwiftKV due to their\npopular adoption and broad domain and task coverage. However, as compared to other high-quality\ndomain specific fine-tuning datasets, they may have weaknesses. To measure the quality of these two\ndatasets, we directly fine-tuned a model using the Llama-3.1-8B base model, and compared this trained\nmodel with the Llama-3.1-8B-Instruct model released by Meta.\nThe results are shown in Table 4 (a). The original Llama-3.1-8B-Instruct has a average score of 73.71\nbut the model trained using our two datasets only achieved 65.77. This indicates the training data used\nfor SwiftKV is not optimal and there may be opportunities to further improve the results we reported\nin Sec. 4 as discussed next.\nDoes more math/coding data help GSM-8K? From Table 1, the main degradation among 7 tasks\nfor 50% SingleInputKV is GSM-8K. This may be due to the lack of math and coding examples in\nthe two datasets we picked to train the model. To verify this, we distilled SwiftKV using one extra\nmath-related dataset, gretelai/synthetic-gsm8k-reflection-405b (GretelAI, 2024),\nand one extra coding dataset, ise-uiuc/Magicoder-OSS-Instruct-75K (Wei et al., 2023),\nin total about 8K+75K=83K samples, and about 16M tokens.\nThe results are reported in Table 4 (b). The performance of all tasks except Winogrande are slightly\nimproved, with the average score being 0.23 higher. Particularly, GSM-8K improves the most, with\na 0.53% improvement. This is expected since we added extra math and coding datasets. Considering\nthe small amount of new data (83k vs. 1.2M), the improvement is remarkable.\nThis study indicates that improvements in distillation data is potentially an important direction for\nfuture work, particularly domain-specific datasets to reduce the quality gap compared to the original\nmodel when using SwiftKV."}, {"title": "\u0421\u043e\u043cBINING WITH OTHER KV CACHE COMPRESSION METHODS", "content": "SwiftKV explores an orthogonal design space from many existing KV cache compression methods,\nwhich means that it can be easily combined with them, e.g., sliding window (Jiang et al., 2023), token-\nlevel pruning (Liu et al., 2024d), quantization (Hooper et al., 2024) etc. In this section, we show the com-\nbined effect of SwiftKV with per-token KV cache FP8 quantization (Yao et al., 2022) using PyTorch's"}, {"title": "SIMPLE EARLY EXIT FOR DECODING TOKENS", "content": "SingleInputKV allows all the KV cache needed for generating future tokens to be computed without\nhaving to forward-pass though the entire LLM. This means that even the decoding phase could exit\nearlier without worrying about missing KV cache for subsequent tokens.\nTo test the feasibility, we added an early exit language modeling head. We then used the input to\nSingleInputKV layer to calculate output logits, and incorporated them as part of the distillation training.\nOur results are preliminary and requires further evaluation, but we found that the alignment between\nearly exit logits and the final output logits to be over 66% when the largest probability from the early\nexit logits is over 95% (Fig. 5). We used this as our early exit criteria (i.e., decoding tokens exit early if\nit predicts an output token with 95%+ probability), and Table 6 shows a sample result. See Appendix E\nfor more details."}, {"title": "CONCLUSIONS", "content": "In this paper, we presented SwiftKV, a novel model transformation for reducing inference cost for\nprompt-dominant workloads, combined with a KV cache reduction strategy to reduce memory footprint,\nand a light-weight distillation procedure to preserve model accuracy. While we presented strong results\non the effectiveness of SwiftKV, exploration of parameter-preserving model transformations for\ninference optimization is still in its early stages. We have identified both limitations as well as areas of\nimprovement. Given the simplicity and effectiveness of SwiftKV, we hope that this will spark further\nexploration from the AI community which we hope will continue lower the latency and cost of inference."}, {"title": "LIMITATIONS AND FUTURE WORK", "content": "It is important for every work to acknowledge its limitations and suggest future directions, particularly\nfor LLM-related works. In our work, we did not aim to optimize the training data selection though\nwe provide potential ways in Sec. 5.3. Additionally, we did not include a detailed benchmark analysis\nfor our method. However, as shown in Sec. 5.3, we ensured that our datasets were not cherry-picked\nto overfit the reported tasks. Furthermore, we did not finetune our model with advanced post-training\napproaches, like DPO and RLHF, which we leave for future work. Finally, we hypothesize that our\nmethod can work even better when combined with pretraining or continued-pretraining, but due to\nresources constraints, we did not explore this direction. We hope to revisit these ideas in the future."}, {"title": "ADDITIONAL MOTIVATION", "content": "B CALCULATING COMPUTATION REDUCTION\nFor a vanilla transformer model Vaswani et al. (2017) with a hidden dimension size d, the total amount\nof computation per token per layer is roughly given by 24d2, where 4d2 goes to KV projections, another\n4d2 goes to Q projection and context GEMM, and the remaining 16d2 goes to MLP computation.\nFor popular GQA (Ainslie et al., 2023b) based architectures like Llama (Touvron et al., 2023),\nMistral (Jiang et al., 2023), Qwen (Bai et al., 2023), and many others, the KV projection is reduced\nto $4d^2/G$, where G is the group size and is \u2265 4. As such, the compute cost of the KV projection is\nnegligible $(20d^2+\\frac{4d^2}{G}$ which is less than 5% when G\u2265 4), compared to the rest of the computation.\nWith SingleInputKV, pre-fill tokens only require computing this negligible KV projection for layers\n>l. Therefore, it reduces the total pre-fill computation to approximately l/L. Fig. 2 shows significant\nend-to-end reduction in model forward time for different value of l compared to L. In Sec. 4.3, we\nshow that such compute reduction translates to considerably higher aggregate serving throughput\nand lower latency, reducing both serving cost, and improving end-user experience."}, {"title": "EXPERIMENTAL DETAILS", "content": "C.1 TRAINING AND EVALUATION DETAILS\nWe directly use the Huggingface LLama-3.1 checkpoints, particularly, \u201cmeta-llama/Meta-Llama-3.1-\n8B-Instruct\" and \"meta-llama/Meta-Llama-3.1-70B-Instruct\". For datasets, we use the supervised\nfinetuning datasets from \u201cHuggingFaceH4/ultrachat_200k\u201d and \u201cteknium/OpenHermes-2.5\", which\nin total is about 1.2M samples, and about 160M tokens. We set training epochs to be 2, learning rate\nto be 3e-4, weight decay to be 0.05, warm up ratio to be 5%, maximum sequence length to be 8192\nwith attention separated sequence packing, the distillation temperature to be 2.0, and the training batch\nsize to be 32 for both Llama-3.1-8B-Instruct and Llama-3.1-70B-Instruct."}, {"title": "INFERENCE SPEEDUP EVALUATION DETAILS", "content": "Hardware Details. We ran all inference speedup experiments on a AWS p5.48xlarge instance, with\n8 NVIDIA H100 GPUs, 192 vCPUs, and 2TB memory. Llama-3.1-8B-Instruct experiments are run\nusing 1 of the 8 GPUs, and Llama-3.1-70B-Instruct experiments are run using 4 of the 8 GPUs.\nVLLM Configuration. We ran all experiments with enforce_eager and chunked prefill\nenabled with max_num_batched_tokens set to 2048. To run each benchmark, we instantiated\nvLLM's AsyncLLMEngine and submitted requests using its generate method according to each\nbenchmark setting. For each request, the inputs are tokenized before being submitted, and the outputs\nare forced to a fixed length of 256."}, {"title": "INTER-LAYER ACROSSKV VS INTRA-LAYER KV CACHE REDUCTION", "content": "In this section, we share different design choices of AcrossKV, which considers the tradeoff be-\ntween GQA (Ainslie et al., 2023a) and the acorss layer sharing into the design. Particularly, when\nAcrossKV > 2, we can either use GQA and AcrossKV together or we can simply use AcrossKV to get\nall savings. For instance, when AcrossKV=4 (a.k.a., the second row of the final session in Table 1), we\nhave KV cache reduction from both GQA and AcrossKV. However, we can either do multi-query atten-\ntion (MQA) for all 16 layers or do"}]}