{"title": "Visual RAG: Expanding MLLM visual knowledge without fine-tuning", "authors": ["Mirco Bonomoa", "Simone Bianco"], "abstract": "Multimodal Large Language Models (MLLMs) have achieved notable performance in computer vision tasks that require reasoning across visual and textual modalities, yet their capabilities are limited to their pre-trained data, requiring extensive fine-tuning for updates. Recent researches have explored the use of In-Context Learning (ICL) to overcome these challenges by providing a set of demonstrating examples as context to augment MLLMs performance in several tasks, showing that many-shot ICL leads to substantial improvements compared to few-shot ICL. However, the reliance on numerous demonstrating examples and the limited MLLMs context windows presents significant obstacles, severely restricting the scope of applications for these models.\nThis paper aims to address these challenges by introducing a novel approach, Visual RAG, that synergically combines the MLLMs capability to learn from the context, with a retrieval mechanism. The crux of this approach is to ensure to augment the MLLM knowledge by selecting only the most relevant demonstrating examples for the query, pushing it to learn by analogy. In this way, relying on the new information provided dynamically during inference time, the resulting system is not limited to the knowledge extracted from the training data, but can be updated rapidly and easily without fine-tuning. Furthermore, this greatly reduces the computational costs for improving the model image classification performance, and augments the model knowledge to new visual domains and tasks it was not trained for.\nExtensive experiments are carried out on eight different datasets in the state of the art spanning several domains and image classification tasks. The experimental results show that the proposed Visual RAG, compared to the most recent state of the art (i.e., many-shot ICL), is able to obtain an accuracy that is very close or even higher (approx. +2% improvement on average) while using a much smaller set of demonstrating examples (approx. only 23% on average).", "sections": [{"title": "1 Introduction", "content": "The rapid advancement of Large Language Models (LLMs) [41] has unlocked extraordinary capabilities [34], making them valuable tools in numerous applications [19]. However, this technological leap has not been without its challenges, such as the issue of outdated knowledge [19]. Although fine-tuning was initially seen as a solution to this problem, the process is complex and costly.\nThis has encouraged research into more efficient methods to update or expand the knowledge base of these models. In this context, Retrieval-Augmented Generation (RAG) has emerged as a promising solution [21]: by leveraging LLMs' ability to process information provided in the context (ICL) [7], RAG can augment the knowledge possessed by these models. The innovative aspect of RAG lies in its selective approach to provide information to the LLM during queries: RAG furnishes the model with only the most relevant document chunks, carefully selected from an external knowledge base based on semantic similarity between the query and these chunks.\nThis approach has gained significant traction in the field of natural language processing, enabling continuous knowledge updates with minimal effort and cost. With the introduction of Multimodal Large Language Models (MLLMs) [37, 38], models have acquired the ability to work on multimodal content, including, among others, images. This has opened up the possibility of exploiting MLLMs in the field of computer vision, unleashing the potential for a whole new range of applications. Existing studies [13, 39] have only begun to scratch the surface of these models' ability to exploit ICL with multimodal content, such as images.\nTaking inspiration from the RAG solutions currently used for textual information, this work aims to develop a solution that expands the visual knowledge of an MLLM beyond its training without requiring a fine-tuning of the model, but allowing an easy and quick update of knowledge with reduced costs.\nThe main contributions of this work can be summarized as follows:\nWe introduce Visual RAG, a novel framework that combines retrieval-augmented generation with multimodal large language models (MLLMs) to dynamically expand their visual knowledge without requiring fine-tuning.\nOur method employs a robust embedding model and a retrieval mechanism to extract the most relevant examples from an image-based knowledge base, optimizing task-specific context for improved performance.\nOur approach seamlessly integrates visual retrieval with MLLM in-context learning capabilities, enabling dynamic adaptation to new tasks and domains exhibiting strong generalization across eight diverse datasets.\nThe proposed approach achieves comparable or superior performance to the state-of-the-art many-shot ICL method with significantly fewer demonstrating examples (about 23% on average), reducing computational costs while maintaining scalability."}, {"title": "2 Related Work", "content": "MLLM models The term MLLM refers to those models that combine the NLP capabilities possessed by classic LLMs with the ability to receive, reason, and output with multimodal information [35, 37].\nWhile our focus primarily lies on visual modalities, it is important to note that MLLMs can also incorporate other modalities such as videos and audios. Recent state-of-the-art models, including LLaVA [22], GPT-40 [1] and Gemini [29], exemplify the potential of MLLMs. Characterized by their massive scale, these models are accessible through web APIs. However, despite their rapid development and impressive performance, the full capabilities of MLLMs remain untapped. The primary challenges the research community is facing include the limited context windows of these models, which restrict the amount of information that can be processed concurrently. Additionally, the reliance only on training data can lead to outdated knowledge and poses challenges in keeping these models up-to-date since fine-tuning, although effective, is a computationally expensive process [32]. Furthermore, fine-tuning can also increase the model's tendency to hallucinate [11].\nMultimodal ICL In the case of text-based In-Context Learning (ICL) [7], LLMs demonstrate remarkable ability to learn from a few textual information provided in the context. This enables them to tackle complex unseen tasks in a few-shot manner [4]. The strength of ICL lies in its training-free nature, allowing a flexible integration into various frameworks and applications [37]. Multimodal In-Context Learning (M-ICL) extends this concept to incorporate multiple modalities like images, videos and audios. In essence, MLLMs learn from a few task-specific examples and generalize to novel, yet similar, questions [37, 39]. However, LLMs, including multimodal ones, face a significant limitation: the context window size, as witnessed by the design of specific solutions to extend it (e.g., [5]). This constraint, measured in tokens, restricts the size of the input a model can process during generation. Although advances in research have led to larger context windows, enabling more examples to be included in prompts, the associated costs escalate. Furthermore, even with extremely large context windows, the so-called 'lost in the middle' effect, as observed in [23], remains a significant concern: models often struggle to effectively utilize all the information within very large contexts, tending to rely more heavily on data at the beginning and at the end of the input. This underscores the critical importance of efficient ICL techniques.\nTextual RAG Retrieval-Augmented Generation (RAG) [21] is an increasingly prominent approach in the field of LLMs for several reasons, including one of particular interest to this research: enabling these models to utilize in the response generation process new textual information that was not provided during training. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external texts (e.g., documents, papers, websites) [10, 16]. This allows the use of a pre-trained model in knowledge-intensive tasks, continuously updating its knowledge and integrating domain-specific information without requiring a fine-tuning process and all the associated challenges. The diffusion of this approach is rapidly increasing, making it a key technology for the use of LLMs in real-world applications. A classic RAG solution, such as the one used for question-answering problems, primarily leverages three steps [16]:\nIndexing: Documents containing the information are divided into chunks, transformed into embeddings, and stored indexed in a vector Data Base (DB).\nRetrieval: The query (question) is transformed into an embedding using the same approach as indexing, and the most semantically similar chunks are retrieved.\nGeneration: The LLM, here called \"generator\", is queried with the original request (question), providing the retrieved text chunks in the context, and the model is asked to use this information to help answering the question."}, {"title": "3 The proposed Visual RAG", "content": "To investigate the effectiveness of a RAG system in the field of computer vision, we implemented a solution that allows to expand the vision capabilities of a MLLM by using a Knowledge Base composed by example images. We implement a retriever capable of working on images instead of text chunks, appropriately combined with one of the best available MLLM, i.e., Gemini [29], to leverage its ICL capabilities. Inspired by [17] we assess the performance of the proposed solution by querying it using several computer vision datasets that serve as benchmarks.\nRAG systems are employed in the field of LLMs to incorporate external textual knowledge into the reasoning processes of models by leveraging ICL [10]. Our idea is to apply this methodology to the field of computer vision, allowing MLLM models to integrate external knowledge for computer vision tasks such as image classification (and beyond). To achieve this, we replace the traditional text-based knowledge base with a new one containing images labeled with their respective class/classes. This knowledge base is then converted into embeddings capable of capturing the visual information required for the classification task and subsequently indexed using a vector database. The implemented retriever operates similarly to a retriever in a conventional RAG system. However, instead of searching for text chunks that are semantically related to the input query, it retrieves images that are most similar to the image to be classified. Finally, these images are provided to the generator as examples to be used as a reference to classify new instances. \n3.1 Indexing\nFor each dataset considered, the Knowledge Base (KB) is composed of the images contained in its corresponding demo set. To facilitate and optimize the subsequent retrieval phase, the KB must first be indexed. First, the images are encoded into a vector representation using an embedding model and then stored in a vector DB. Given the nature of the task, we selected CLIP [27], one of the state-of-the-art models to represent visual and text information. It was chosen not only because it is a multimodal model capable of processing images but also because of its ability to generalize, a crucial characteristic for developing a visual RAG solution capable of performing effectively across diverse contexts. Consequently, FAISS [8] was selected as the underlying vector DB/retriever for our solution. Its efficiency and lightweight design allow it to operate effectively on standard hardware, delivering rapid response times even when handling massive datasets. Additionally, it prepares the solution to scale efficiently in case larger datasets will be used in the future.\nWe create the indexes using the IndexFlatL2 method provided by FAISS [8], a basic implementation that does not rely on complex index structures but stores the vectors without compression as a flat array. Although this approach is not the fastest one, it is highly accurate, making it the most suitable choice for small datasets like ours.\n3.2 Retrieval\nAs mentioned in the previous section, we use FAISS to index the Knowledge Base in IndexFlatL2 mode. This configuration employs a brute-force search method across the entire index, making it highly"}, {"title": "3.3 Generator", "content": "The visual RAG solution requires a model with remarkable ICL capabilities and the possibility to submit several images in the request. Furthermore, since ICL capabilities are most effectively exhibited by larger models [33] that cannot be utilized with common hardware, the focus shifts to the larger models accessible with APIs. These requirements narrow the research to two options: ChatGPT and Gemini. We chose to conduct all the experiments using Gemini because, as observed in [17], Gemini 1.5 Pro demonstrated a more linear improvement across different scenarios as the number of demonstrating examples increases, leading to a more stable model.\nTo leverage the model's ICL capabilities, the RAG Solution employs the prompt detailed in Appendix A [17]. The retrieved examples are presented as enhanced context to the generator in the form of <image, label> pairs, serving as demonstrations. The input image from the test set, which requires classification, is then included in the prompt using the same structure but without its corresponding label. This prompts the model to predict the appropriate class.\nAdditionally, the prompt explicitly defines the expected response format, ensuring that the model output adheres to a structure that simplifies the extraction of the predicted class from the generated text."}, {"title": "4 Experimental setup", "content": "4.1 Datasets\nThe use of ICL in the field of computer vision is a relatively novel approach in research, and with limited studies exploring it in depth, we decided to use the same datasets as in [17]. At the time of writing, [17] is the only work that provides sufficient information to serve as a benchmark for our solution. The selected datasets span several domains (natural imagery, medical imagery, pattern imagery, remote sensing) and tasks (multi-class, multi-label and fine-grained classification). Some sample images for each dataset are\nFor a fair comparison, we use the same subsets used in [17]: the demo set is used to populate the knowledge base, enabling the system to retrieve tailored examples for each query input, while the test set is used to evaluate the solution. In the scaling experiments, we increase the number of demonstrating examples retrieved by the retriever without ensuring class balance. Although it is impossible to guarantee that the datasets were not used in the training of the MLLM, the baseline column in Table 2, representing the model zero-shot accuracy, provides insights into whether these are familiar contexts. For example, it is evident that FIVES and CheXpert are unfamiliar domains, as the accuracy on these datasets is comparable to that of a random guess classifier."}, {"title": "4.2 Settings", "content": "The costs associated with using Gemini's APIs are, at the time of writing, high enough to pose a significant budgetary constraint. As a result, we opted to test our solution on a wide variety of datasets to better explore its generalization capabilities, rather than focusing on achieving peak performance with the proposed approach.\nThe experiments are conducted using the same number of demonstrating examples for each instance in the test set of every dataset. Specifically, for each dataset, experiments are repeated using 5, 10 and 50 examples, maintaining the same RAG configuration (Embeddings, Retrieval system, and Generator model).\nFinally, for the smallest datasets (i.e., FIVES and CheXpert), which entail reduced costs, additional experiments are also conducted using 100 examples."}, {"title": "4.3 Evaluation", "content": "The proposed system is evaluated using the following metrics to permit a direct comparison with the performance reported in [17]:\nAccuracy: for all single-label classification task datasets;\nMacro-averaged F1: for the multi-label classification task dataset CheXpert."}, {"title": "5 Results", "content": "We compare the performance of the proposed Visual RAG with two different approaches: the first one acts as a baseline, which is the MLLM model queried without providing any examples in context (i.e., zero-shot classification); the second one is Many-shot ICL, i.e., the approach used by [17], which constitutes the state of the art: the approach randomly selects from the knowledge base the examples to be provided in context to query the MLLM. \nTheseresults highlight how the Visual RAG improved the accuracy of the MLLM used, even outperforming Many-shot ICL [17] in almost all datasets, showing an excellent ability to work in very different contexts. Notably, the only datasets where Visual RAG did not achieve the performance levels of Many-shot ICL are TerraIncognita and Oxford Pets. However, in these datasets the accuracy gap is very low: 0.25% for TerraIncognita and 0.29% for OxfordPets.\nIn all the other six datasets, and particularly in FIVES, EUROSAT and DTD, it is evident that the MLLM significantly benefits from the tailored selection of examples, achieving a marked improvement in performance compared to the Many-shot ICL solution. This behavior could be attributed to the reduction of noise (i.e., images irrelevant to the specific query) within the context, as previously observed and highlighted for textual RAGs [2] [23]. FIVES serves as one of the most representative examples of the capabilities of the proposed solution: the baseline accuracy of 25.83%, which is almost equivalent to that of a random classifier (i.e., 25.00% considering that the dataset has 4 classes), suggests that the MLLM lacks prior knowledge of this visual domain. Even in these conditions our solution is able to obtain +38.34% in accuracy (+9.17% compared to Many-shot ICL), showing how the solution is able to enhance MLLM knowledge even when the context is completely new and cannot be traced back to knowledge already possessed.\nThe complete performance of the compared solutions for the different number of demonstrating examples considered are reported. The plots reported hint at a further key benefit of our method:a significant improvement in terms of efficiency. Visual RAG demonstrates the capability to achieve comparable or even superior accuracy to the state-of-the-art method Many-shot ICL [17], while requiring a substantially reduced number of demonstrating examples. \nOn average, Visual RAG uses only 23% of the demonstrating example typically required by Many-shot ICL. Moreover, excluding CheXpert and TerraIncognita where the retriever encounters difficulties due to specific dataset characteristics (which will be examined later), our approach required an astonishingly low average of 6.5% demonstrating examples. In the case of OxfordPets, DTD and FIVES this quantity is further reduced to less than 3%."}, {"title": "6 Ablation", "content": "In this section we conduct ablation studies on the key components of the Visual RAG in all the experiments.\nGenerator-only ablation study: the accuracy reported in Table 2 in the Baseline column shows the performance achieved without using the retriever module. This column represents the capacity of the system using only the generator model (MLLM), without providing any demonstrating examples (i.e., zero-shot). The results indicate that our solution enhances the generator classification capabilities by leveraging the demonstrating examples in all the datasets. We can see the improvement in accuracy (the difference in accuracy between our solution and the baseline) inside the brackets in the column Visual RAG, showing an average increase of 21.25%, ranging from 6.79% in TerraIncognita to 46.98% in EuroSAT.\nRetriever-only ablation study: the accuracy reported in Table 2 in the column Retriever-only shows the performance achieved without using the generator. This ablation study is conducted to assess the classification capabilities of a system composed solely of the retriever. In this system, the label of the query is simply predicted as the most frequent class present among the examples provided. In the computation of the accuracy, in the case of a tie between the correct and other class(es), the prediction is considered incorrect.\nThe ablation studies conducted reveal that the combined approach offers a significant advantage over using either component alone: by leveraging the strengths of both the retriever and the generator, we are able to achieve superior accuracy across all the scenarios considered. The most notable improvements are observed in cases where the generator has limited or no contextual knowledge like HAM10000, FIVES and EUROSAT. The retriever effectively compensates for these limitations, providing the necessary context to enhance the generator's decision-making. However, the benefits of our solution extend beyond these specific cases. Even when the generator has a strong understanding of the context, our combined approach still delivers enhanced performance. These flexibility and robustness make our solution a compelling choice for a wide range of applications."}, {"title": "7 Conclusion", "content": "In-Context Learning (ICL) holds a significant potential in computer vision applications, with the primary challenge being to identify the method that leverages it most effectively. The proposed Visual RAG method has demonstrated superior performance compared to the state of the art. The key properties emerging from Visual RAG are:\nExpansion of the use cases: It enables a straightforward expansion of knowledge regarding image classification in MLLMs, allowing to leverage the models capabilities in novel contexts they are unfamiliar with.\nAccuracy improvement: Demonstrating examples guide the model, even in familiar domains, resulting in more accurate predictions. Furthermore, providing only carefully selected examples using a retriever ensures that only relevant information for the task is included. This noise reduction leads to improved accuracy compared to state-of-the-art approaches [17], even when using the same Knowledge Base (KB).\nEfficiency: By including only query-relevant context information, our approach achieves an average reduction of about 77% in the number of demonstrating examples required compared to state-of-the-art methods. The smaller context size accelerates inference time, significantly reduces associated costs, and alleviates the problem of narrow context windows in MLLMs.\nScalability: Our results, consistent with those reported in [17], show that even when using a retriever module, increasing the number of demonstrating examples continues to lead to improved performance.\nRapid updates: in case of new examples available, or any type of KB update, our solution allows a rapid adaptation without costs and with immediate effect.\nWhile our solution has shown significant improvements, our research has identified several refinements that could further enhance performance, as for example alternative retrieval methodologies, different index configurations, and prompts. However, as our ablative studies have shown, the overall performance is highly correlated with the capabilities of its constituent components. Consequently, even without these additional refinements, our solution is expected to benefit significantly from advancements in the underlying embedding, retrieval and MLLM models.\nAs future works we plan to explore alternative retrieval methodologies and index configurations, such as hierarchical or semantic-aware indexing, to improve the precision of retrieved examples, particularly for datasets with fine-grained or multi-label classification tasks. We also plan to investigate the use of advanced embedding models or fine-tuned embeddings tailored to specific visual domains to address challenges like inter-class similarity or subject-background separation. We will also extend Visual RAG to other computer vision tasks to evaluate its versatility beyond image classification."}, {"title": "A Prompt used in the Visual RAG solution", "content": "In this section we report the used in the proposed Visual RAG solu-tion, which corresponds to the prompt used for image classificationexperiments in [17].\nprompt = \"\"for demo in demo_examples:prompt += f\"\"\"<<IMG>>Given the image above, answer the following question-using the specified format.\nQuestion: What is in the image above?\nChoices: {str(class_desp)}\nAnswer Choice: {demo.answer}\n\"\"\"prompt += f\"\"\"<<IMG>>Given the image above, answer the following question-using the specified format.\nQuestion: What is in the image above?\nChoices: {str(class_desp)}\nPlease respond with the following format:\n---BEGIN FORMAT TEMPLATE---\nAnswer Choice: Your Answer Choice Here]\nConfidence Score: (Your Numerical Prediction Confidence Score Here From 0 To 1]\n---END FORMAT TEMPLATE---\nDo not deviate from the above format. Repeat the format template for the answer.\"\"\""}, {"title": "3.  2 Retrieval", "content": "Given two n-dimensional vectors x = [x1,...,xn] and y = [Y1,..., yn], the L2 distance between them is defined as:\nd(x, y) = \u221a(\u03a3(Xi - Yi)2) (1) \n                               n\ni=1"}]}