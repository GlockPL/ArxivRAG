{"title": "Classification of Heart Sounds Using Multi-Branch Deep Convolutional Network and LSTM-CNN", "authors": ["Seyed Amir Latifi", "Hassan Ghassemian", "Maryam Imani"], "abstract": "This paper presents a fast and cost-effective method for diagnosing cardiac abnormalities with high accuracy and reliability using low-cost systems in clinics. The primary limitation of automatic diagnosing of cardiac diseases is the rarity of correct and acceptable labeled samples, which can be expensive to prepare. To address this issue, two methods are proposed in this work. The first method is a unique Multi-Branch Deep Convolutional Neural Network (MBDCN) architecture inspired by human auditory processing, specifically designed to optimize feature extraction by employing various sizes of convolutional filters and audio signal power spectrum as input. In the second method, called as Long short-term memory-Convolutional Neural (LSCN) model, Additionally, the network architecture includes Long Short-Term Memory (LSTM) network blocks to improve feature extraction in the time domain. The innovative approach of combining multiple parallel branches consisting of the one-dimensional convolutional layers along with LSTM blocks helps in achieving superior results in audio signal processing tasks. The experimental results demonstrate superiority of the proposed methods over the state-of-the-art techniques. The overall classification accuracy of heart sounds with the LSCN network is more than 96%. The efficiency of this network is significant compared to common feature extraction methods such as Mel Frequency Cepstral Coefficients (MFCC) and wavelet transform. Therefore, the proposed method shows promising results in the automatic analysis of heart sounds and has potential applications in the diagnosis and early detection of cardiovascular diseases.", "sections": [{"title": "1. INTRODUCTION", "content": "Heart sound (PCG) analysis is a non-invasive method for early detection and treatment of heart abnormalities. Heart sounds can provide vital information about the status of valves and vessels, which helps to demonstrate the heart function, which is a low-cost and reliable method. The analysis and categorization of PCG signals are vital in diagnosing cardiac malfunction. Heart sound signals contain significant information regarding the state of the heart's valves and vessels, enabling the identification of possible valvular vascular damage and indicating their functionality. These lesions typically produce an additional sound or murmur during different periods of a cardiac cycle, which can be traced to the type of lesion through examination of its occurrence time and kind of murmur [1]. This study aims to address the need for a quick and cost-effective method that can diagnose cardiac diseases accurately and reliably in low-cost systems and clinical settings. The lack of adequate databases in the field of medical diagnosis poses a straightforward issue,"}, {"title": "2. PROPOSED METHODS", "content": "The rest of the paper is structured as follows: Section 2 shows the proposed method architecture. Section 3 discusses various approaches that rival the proposed methods. Section 4 presents detailed information about various experimental parameters. It showcases the results of the proposed models in comparison with other commonly used feature extraction and classification methods. In order to validate the effectiveness and efficiency of the proposed algorithm, several experiments were conducted and their results are discussed in detail. Finally, Section 5 concludes this paper."}, {"title": "2.1. The Multi Branch Deep Convolutional Neural (MBDCN) network", "content": "In this work, the structure of the human auditory system [16] is inspired to design an artificial neural network for the automatic diagnosis of cardiac diseases. Specifically, the auditory-inspired neural network incorporates the human cochlea, which is responsible for frequency analysis, through parallel processing channels. The channels process different frequency bands independently, much like the tonotopic organization of the human cochlea where different regions respond to different frequencies. To realize the parallel processing channels in the human auditory system, a multi-branch CNN architecture is proposed. The CNN architecture is comprised of multiple layers including convolutional, combinational, and fully connected layers. Convolution layers focus on hierarchical feature extraction by applying convolutional filters, similar to the filters of the human cochlea, to the input matrix. The depth of CNN, i.e., the number of used kernels, determines how much abstract features are extracted; and the receptive field of convolutional kernels, i.e., the filter size, determines how much locality is considered in the extraction of neighborhood information from the two-dimensional audio signal. To replicate the human auditory system's thousands of hair cells, each one tuned to a specific frequency range, multiple filters of different sizes are used to capture different frequency bands and time resolutions.\nThe Multi Branch Deep Convolutional Neural (MBDCN) network's general block diagram can be observed in Figure 1. To enhance the effectiveness of (CNNs) in audio processing, smaller filters such as 1\u00d73 or 1\u00d75 can be utilized to capture local patterns and high-frequency information. Conversely, larger filters like 1\u00d711 or 1\u00d79 can capture more patterns and information to record the low frequency. Employing multiple convolutional layers with varying filter sizes and filter counts creates a hierarchical feature extraction process, allowing CNN to learn a rich representation of the input audio signa in multiple scales. This process involves training several CNN models with different configurations and conducting meta-parameter searches to determine the best filter sizes and architecture for the specific problem. Designing an effective one-dimensional CNN for audio processing requires careful consideration of the network architecture, training process, and data preprocessing techniques. Through testing different configurations, we can create a one- dimensional CNN that accurately captures the features of audio signals and performs classification tasks well. To evaluate and refine the performance of our 1D-CNN, it is crucial to split the dataset into training, validation, and testing sets. The training set is used to train the model, while the validation set fine-tunes the hyperparameters and selects the best model. Finally, the test set is employed to evaluate the final model's performance, ensuring that it generalizes well to unseen data. Through our study, we conducted experiments that explored various meta-parameters, including learning rate, batch size, and the number of layers and filters in the network, to determine the optimal combination tailored to our specific audio processing problem. By following these steps and considering additional factors, we were able to develop a high-performance one- dimensional Convolutional Neural Network (CNN) that effectively captures the features of audio signals and achieves robust classification results. Developing an efficient model is an iterative process that involves evaluating, modifying, and improving the model to achieve the best possible results. One of the key advantages of using CNNs is their ability to automatically learn features from input data, minimizing the need for manual feature extraction. This feature is especially useful when it's difficult to identify or extract relevant features using traditional methods. Additionally, CNNs are effective in handling high-dimensional data, such as lung and heart sound signals that contain vast amounts of information. Our innovative approach utilizes a one- dimensional (DCNN) with multiple parallel branches, each of which extracts different levels of information from the input data using various filter sizes. For sound and heart features, the parallel branches are inspired by the human auditory system, which uses a similar wave bank filter approach to capture specific frequency components in audio. By incorporating parallel branches with different filter sizes, a Multi Branch Deep Convolutional Neural (MBDCN) Network can detect specific patterns or features within a certain range of spatial extents. This multiscale feature representation is valuable for capturing different aspects of voice and heart features, enabling the network to learn and classify relevant patterns for tasks such as voice analysis, heartbeat detection, or heart sound classification. Our new a MBDCN architecture takes a periodogram of the Welch method as input, which reduces the noise and variance of PCG signals by dividing them into sections and calculating the average of the periodogram of these sections. The power spectrum output is smoothed and converted into a 4D tensor of size (1 \u00d7 number of features of each sample \u00d7 1 \u00d7 number of test samples), which serves as the input tensor of the network. The MBDCN consists of four parallel branches of convolutional layers with different sizes and numbers of filters, connected by a depth concatenation layer to form a single feature vector. The feature vector is fed to a convolution layer with a small filter size and finally to a fully connected layer and SoftMax layer to produce the final output."}, {"title": "2.2. Long Short-term memory-Convolutional Neural (LSCN) network:", "content": "The proposed LSCN model is based on MBDCN's architecture and employs CNN layers for feature extraction in the frequency domain, an LSTM layer for sequence learning [17] in the time domain, and a softmax classifier for binary classification. The proposed modified MBDCN Network architecture is shown in Fig. 2.\nThe LSTM (short for Long Short-Term Memory) [18] is a specific type of recurrent neural network designed to address the limitations of traditional RNNs, which struggle with long-term dependencies. The LSTM consists of three layers - input, hidden, and output. The hidden layer comprises memory blocks, each containing memory cells equipped with gates to regulate the flow of information. These gates include the forget gate, input gate, and output gate, which work together to ensure that the right information is processed and retained within the network.\nCNNs are simple neural networks that use convolution instead of matrix multiplication in at least one layer and are a special linear operation. Convolutional Neural Networks (CNNs) are designed using blocks that are stacked on top of each other to facilitate the learning of intricate features of the network. Each block extracts data features from the previous block, and the CNNs learn more complex features by incorporating non-linear activation functions in the convolution layer. The pooling process is then utilized to bring important information to the fore. However, CNNs face limitations in understanding long-term dependencies between data, and LSTM is better suited for such cases. LSTM effectively addresses the vanishing gradient problem. As a result, the LSCN network architecture can combine the feature extraction capability of CNNs with the ability of LSTMs to learn long-term dependencies between data. The CNN model is effective in extracting local features, but LSTM is included in the model because of its ability to learn sequential information. LSTM is capable of processing these properties sequentially while learning long-term dependencies [19].\nWe begin our process by applying a Fourier transform to the time function, which transforms it into a series of sine waves with varying frequencies. However, this approach may result in the loss of important time-based characteristics such as waveform and position. To address this issue, we have incorporated the Welch's method. Additionally, we have developed a neural network consisting of two long short-term memory (LSTM) network blocks to further improve the results. We use LSTM to extract time domain features by directly inputting the data into the network. Furthermore, we combine the frequency features extracted by the MBDCN network and pass them through a fully connected layer for classification. The first LSTM block has 600 units, while the second block has 100 units."}, {"title": "3. COMPETING METHODS", "content": "Several neural networks such as 1D-CNN, MLP, and several machine learning approaches are implemented as competitors in this work."}, {"title": "3.1. 1D-CNN Network", "content": "One-dimensional CNN models [6] are capable of extracting features from input data using different filter sizes. This type of deep neural network consists of convolutional blocks that can extract various patterns in the input using various filters, followed by fusion layers that extract essential features from the previous layers. Removing features from different layers and sharing information, creates a vector that helps the CNN model to be nearly invariant at a lower computational cost than multidimensional transformations. To perform classification, fully connected layers are usually added to the network's last layer, followed by nonlinear functions such as SoftMax or Sigmoid. Additionally, batch normalization and elimination layers are included between layers to avoid overfitting [20].\nDuring the training phase of a CNN model, the kernels of the network are first initialized randomly or with different initialization techniques. This process can be done using weights from a pre-trained model trained on a different dataset, known as transfer learning. Initial kernels should be optimized using an optimization algorithm such as Adam or SGD.\nOne-dimensional CNN is a promising method for data mining of one-dimensional signals, where a limited amount of data is available, and the entire signal information should be considered input data. The main advantage of this technique over previous neural networks is that one- dimensional CNN extracts the features of a signal by considering local information instead of the whole signal in each layer of the network, leading to faster training of the network with fewer trainable parameters, resulting in lower cost and computational power."}, {"title": "3.2. MLP Network", "content": "We developed a neural network called multi-layer perceptron for analyzing heart dataset. The MLP network comprises 6 fully connected layers using the RELU activation function. The network is designed to classify data into different categories based on the number of classes entered into the neural network. Figure 4 provides an overview of the MLP network architecture."}, {"title": "3.3. Machine Learning Methods", "content": "In order to conduct a comprehensive evaluation of the study, we conducted an in-depth analysis of the efficacy of multiple feature extraction techniques used in reference [10]. These techniques included Gabor bank filter (MAGFB), MFCC, wavelet, and a combination of MAGFB with MFCC (MAMFB), all of which were applied to heart sounds. We also utilized the proposed MBDCN network as a classifier, comparing its performance to other established classifiers such as ML, SVM, tree, and KNN.\nwe utilized various filter banks such as MFCC, WaveFB [21], MAMFB, and MAGFB to extract lung sounds. We used these filter banks as a basis to compare our proposed method. The main difference between the lung and heart data sound for this MAMFB and MAGFB is the number of Gabor filters used. For the PhysioNet dataset, we employed 26 filters."}, {"title": "4. EXPERIMENTAL RESULTS", "content": "4.1. Data and Preprocessing\nHeart sound recordings are obtained from various participants worldwide, in both clinical and non-clinical settings, including healthy individuals and patients with pathological conditions such as heart valve disease and coronary artery disease. The dataset used for this purpose is related to the PhysioNet / CinC 2016 [11], which comprises 4,430 recordings from 1,072 subjects, totaling 233,512 heart sounds. The dataset is divided into two classes: normal and abnormal. It should be noted that the dataset is composed of six (A, B, C, D, E, and F) different sources, with varying lengths of recordings ranging from 5 to over 20 seconds. Dataset A contains 292 normal and 117 pathological samples, dataset B has 104 normal and 386 pathological samples, dataset C has 24 normal and 7 pathological cases, dataset D has 28 normal and 27 pathological samples, dataset E consists of 183 normal and 1958 pathological heart sound samples, and dataset F consists of 34 normal and 80 pathological heart sound samples. The signals were recorded at a sampling rate of 44.1 kHz and had varying lengths between 1s to 120s. In our study, we analyzed a total of 12 sub- datasets, comprising datasets labeled as normal and abnormal, denoted by (a through f). The purpose of our analysis was to examine the performance of our model on both normal and abnormal datasets. The use of both types of datasets allowed us to evaluate the ability of our model to detect and classify anomalies, which are critical for enhancing the accuracy of any machine learning model. The results of our analysis were conclusive and provided valuable insights that could guide further research in this field. To standardize the data, we reduced the length of all signals to 5 seconds and excluded any data that was less than 5 seconds long. We used a Butterworth low-pass filter with a cut-off frequency of 900.0 Hertz to remove the DC part and high frequencies of the heart sounds, which were artifacts of the microphone. This resulted in a reduction of the PCG sampling rate from 44.1 KHZ to 2 KHZ [22]. Then, the energy of all heart sounds should be normalized:\n$$Xnorm =  \\frac{x[n]}{\\sigma_x}$$\nwhere x[t] is the heart's sounds, and \u03c3\u00b2 is the variance of x[t]."}, {"title": "4.2. Evaluation Metric", "content": "To evaluate the robustness of our method, we measured the performance of our models on the test set in terms of accuracy (AC), sensitivity (SE), specificity (SP), and precision (PR)., The above-mentioned indications could well be written as follows:\n$$Accuracy(%) = \\frac{TP+TN}{TP+TN + FP + FN}  \\times 100$$\n$$Sensitivity (%) = \\frac{TP}{TP + FN} \\times 100$$\n$$Specificity(%) =  \\frac{TN}{TN+FP} \\times 100$$\n$$Precision(%) = \\frac{TP}{TP+FP} \\times 100$$\nwhere TP, TN, FP, and FN represent number of true positive, true negative, false positive, and false negative, respectively.\nAnother famous metric inspired by measures of Sensitivity (Se) and Precision (Pr) metrics is F- measure (or F1-score), which is calculated as follows:\n$$F1 - score(%) = \\frac{2 \\times Precision \\times Sensitivity}{Precision + Sensitivity} \\times 100$$\nAnd the kappa coefficient, defined as follows, is also used:\n$$P_e = \\frac{\\sum_{i=1}(row marginal)(column marginal)}{\\N^2}= \\frac{\\sum_{i=1}P(0_i)M_i}{N^2}$$\n$$K = \\frac{OA - P_e}{1-P_e}$$\nIn the above equation, N, Ni, and Mi are the total number of test samples, the number of test samples of the i-th class, and the number of test samples labeled in the i-th class in the output of the classifier, respectively. The ideal value of the kappa coefficient is \u201c1\u201d."}, {"title": "4.3. Training Parameters", "content": "There is frequently a requirement to optimize and enhance deep learning and machine learning models. This can be achieved by utilizing numerous techniques. The techniques and parameters that are most commonly employed for achieving the best training performance of our models [9]. After the FC layer, softmax and classification layers are connected to predict the output as benign or malignant. In the context of the classification task, the optimization of network parameters at each iteration is accomplished through the application of Stochastic Gradient Descent with Momentum (SGDM). This approach is known to produce superior levels of convergence accuracy. To this end, the initial learning rate for the task was designated as 0.01 [23]. The information regarding the training of the networks is presented in Table 1."}, {"title": "4.4 Results and Discussion", "content": "In our experiments, we discovered that deep learning models trained on various input time- frequency properties yield varied outcomes when tested on the same dataset. This article's objective is to compare deep learning methods like MBDCN and LSCN with machine learning methods like MFCC, Wavelet, MAGFB, and MAMFB. The LSCN model achieved a 96.93% accuracy index on the test dataset, and Table 4 shows other performance measures of the model. Compared to other models, the LSCN model has higher kappa, specificity, and accuracy. Our system uses a unique Multi-Branch Deep Convolutional Neural (MBDCN) network architecture designed to optimize feature extraction using different sizes of convolutional filters, combining LSTM blocks, and taking Welch's periodogram as input. This enables the system to be more proficient in extracting features in both time and frequency domains, making it an effective way to classify heart sounds.\nTo analyze the most appropriate method for heart sound signal classification when using the MBDCN network, various input feature representations, including MAMFB, MAGFB, Wavelet, and MFCC feature maps [29], were analyzed in the study. The MBDCN network was used as a classifier and compared with other good classifiers such as KNN, SVM, Tree, and ML that have shown good performance. We examined our proposed models in eleven distinct tests, and the model trained by the LSCN network outperformed the models trained by the MBDCN, MLP, and 1D-CNN networks in terms of accuracy as well as average (sensitivity, specificity, precision, and F1-score). The experimental results showed that the use of LSTM blocks together with the convolutional multi-branch complex neural network is an effective method for heart sounds classification because it performs feature extraction in the two domains of time and frequency in the best way. Additionally, the LSCN model is more suitable for processing heart sound datasets to improve the performance of the MBDCN network for heart sound classification compared to the MBDCN model.\nRen et al. [24] proposed a method to classify heart sound signals as normal or abnormal using image-based features extracted from scalograms. This was done by employing convolutional neural networks (CNNs) and comparing different architectures such as VGG2, ResNet3, and DenseNet4. Their study showed that these networks outperformed the baseline method that used the ComParE audio feature and SVM classifier. The study also highlighted the effectiveness of transfer learning by using pre-trained models on ImageNet and fine-tuning them on the heart sound dataset. The authors claim that their method achieved state-of-the-art results on the PhysioNet/CinC 2016 Challenge dataset.\nLi et al. [25] proposed a method combining a traditional feature engineering approach with a CNN to automatically classify normal and abnormal heart sounds. Although this method is relatively simple and efficient, achieving an accuracy of 86.8%, sensitivity of 87%, and specificity of 86.6% on the PhysioNet/CinC 2016 Challenge dataset, it has some limitations such as relying on manually extracted features, using a fixed CNN architecture, and not using auxiliary information such as ECG signals to improve classification accuracy.\nHettiarachchi et al. [26] proposed a method for identifying pre-existing heart diseases using a combination of synchronized ECG and PCG signals. However, as the number of simultaneous PCG and ECG recordings from the PhysioNet/CinC (training-a) dataset is insufficient to train a"}, {"title": "5. CONCLUSION", "content": "The development of intelligent systems for heart sound analysis and classification has shown promising results with the use of deep neural networks. The proposed LSCN is a significant step in this direction, and further research in this area could greatly improve the early detection and diagnosis of cardiovascular disorders, ultimately leading to better patient outcomes and quality of life. The competitive methods utilized in this study simulate the human auditory system to process audio signals and extract meaningful features, which can then be used to train machine learning models for audio classification. This approach also holds the potential to pinpoint specific abnormal sounds associated with cardiovascular disorders."}]}