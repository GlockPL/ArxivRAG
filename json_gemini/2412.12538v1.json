{"title": "A Scalable Approach to Benchmarking the In-Conversation Differential Diagnostic Accuracy of a Health Al", "authors": ["Deep Bhatt", "Surya Ayyagari", "Anuruddh Mishra"], "abstract": "Diagnostic errors in healthcare persist as a critical challenge, with increasing numbers of patients turning to online resources for health information. While Al-powered healthcare chatbots show promise, there exists no standardized and scalable framework for evaluating their diagnostic capabilities. This study introduces a scalable benchmarking methodology for assessing health Al systems and demonstrates its application through August, an Al-driven conversational chatbot. Our methodology employs 400 validated clinical vignettes across 14 medical specialties, using Al-powered patient actors to simulate realistic clinical interactions. In systematic testing, August achieved a top-one diagnostic accuracy of 81.8% (327/400 cases) and a top-two accuracy of 85.0% (340/400 cases), significantly outperforming traditional symptom checkers. The system demonstrated 95.8% accuracy in specialist referrals and required 47% fewer questions compared to conventional symptom checkers (mean 16 vs 29 questions), while maintaining empathetic dialogue throughout consultations. These findings demonstrate the potential of Al chatbots to enhance healthcare delivery, though implementation challenges remain regarding real-world validation and integration of objective clinical data. This research provides a reproducible framework for evaluating healthcare Al systems, contributing to the responsible development and deployment of Al in clinical settings.", "sections": [{"title": "1. Introduction", "content": "Clinical decision making as the cornerstone of medical practice is heavily based on accurate diagnoses to guide patient care [1, 2]. Diagnostic errors, which can lead to significant patient harm, arises from a blend of systemic and cognitive factors [3, 4, 5, 6, 7]. As patients increasingly rely on digital resources for health information, the landscape is evolving. A 2010 survey involving more than 12,000 individuals across 12 countries found that 75% of respondents sought health information online [8]. In 2017, approximately 67% of patients admitted to searching for their symptoms on Google before visiting a doctor [9]. More than 33% of adults in the United States regularly use the internet to self-diagnose their ailments, using it both for non-urgent symptoms and urgent symptoms such as chest pain [10]. It's not just limited to primary healthcare either, with a recent study revealing that 50% of patients researched their symptoms online before going to emergency departments [11, 12].\nThe evaluation of healthcare Al systems presents unique challenges that traditional medical assessment frameworks struggle to address [64] [65]. Current evaluation methods typically rely on three approaches: multiple-choice medical questions, structured clinical vignettes with predefined responses, or limited human-based testing. Each of these methods has significant limitations in assessing Al systems designed for patient interaction. Multiple-choice question datasets like USMLE MedQA evaluate only the final diagnostic decision rather than the critical process of information gathering.\nThis approach fails to assess the Al's ability to conduct appropriate clinical reasoning or adapt to varying patient presentations. Human-based testing, while valuable for assessing real-world performance, faces significant scalability constraints and potential inconsistencies in how actors present symptoms or respond to questions.\nTraditional clinical vignette assessments often employ rigid response structures that fail to capture the dynamic nature of patient-provider interactions. These assessments typically focus on the presentation of symptoms rather than the interactive process of history-taking, which is crucial for accurate diagnosis in clinical practice. This approach also runs the risk of the exact or similar vignette being present in the Als training data. Moreover, existing benchmarking approaches often overlook critical aspects of healthcare delivery such as:\n\u2022 The ability to handle ambiguous or contradictory patient information\n\u2022 The management of varying levels of health literacy and communication styles\n\u2022 The capacity to maintain consistency while adapting to individual patient needs\n\u2022 The effectiveness of gathering pertinent negative findings through conversation\nThis research gap necessitates a new approach to benchmarking healthcare Al systems - one that combines the standardization of clinical vignettes with the interactive nature of real patient encounters, while maintaining scalability for practical implementation. Our proposed framework addresses these limitations by introducing Al-powered patient actors that can consistently simulate realistic patient interactions while allowing for standardized evaluation across a large number of clinical scenarios.\nRelying on the internet has broadly become the norm. This shift highlights both an opportunity allowing for the rise of unique online conversational Al platforms which can aid patients as an efficient and accessible tool for seeking online health information [13]. However, this opportunity highlights a challenge: while digital tools can empower patients, they also risk disseminating inaccurate information."}, {"title": "1.1. Traditional Symptom Checker Systems", "content": "Traditional history-taking-a critical component of diagnosis-involves a nuanced dialogue between clinician and patient, capturing not just symptoms but also the contextual factors that influence health. However, time constraints and resource limitations can hinder this process. In response, digital health technologies have emerged, including online symptom checkers and, more recently, Al-powered chat-"}, {"title": "1.2. Challenges in Traditional Healthcare Systems", "content": "The traditional healthcare system often struggles to provide personalized care. Due to time constraints and heavy patient loads, doctors frequently allocate less than five minutes per consultation in regions that encompass over half the global population, contributing to professional burnout [61, 62]. This limited time frame hinders the thorough study of the history, resulting in missed information and rushed appointments that prioritize immediate symptoms over underlying causes. In addition, access to healthcare presents a significant challenge, particularly in underserved areas where medical professionals are in short supply. Even when care is accessible, patients often face difficulties reaching the appropriate specialist promptly due to long wait times, complex referral processes, social, economical and geographic barriers."}, {"title": "1.3. Rise of Al-powered Chatbots", "content": "About one in six adults (17%) say they use Al chatbots at least once a month to find health information and advice, rising to one-quarter of adults under age 30 (25%) [15], and most physicians anticipate a significant role for such Al tools in future clinical practice [45]. Moreover, concerns about the accuracy, reliability, and safety of Al chatbots persist, compounded by a lack of standardized benchmarks to evaluate their performance in clinical settings [16, 17, 18, 19]. We present a comprehensive benchmarking framework to study the diagnostic ability of a conversational Al chatbot designed for differential diagnosis. We evaluate the chatbot's ability to engage users in natural language, accurately"}, {"title": "2. Background", "content": "The utility and promise of conversational diagnostic chatbots cannot be realized if they are not accurate in initial symptom checking, triaging, and differential diagnosis. There are several limitations to the current approach of benchmarking chatbots against publicly available medical multiple-choice question datasets like the USMLE MedQA. The multiple-choice format inherently makes it easier for models to select the correct option rather than formulating a diagnosis from scratch, which does not adequately assess the model's ability to synthesize information and engage in clinical reasoning [29]. These benchmarking standards are further thrown into question when considering the diversity of input data, i.e. the complement data processed through diagnostic reports and other such lab diagnostics which assist a doctor in their decision-making.\nA major area where conversational diagnostic chatbots surpass conventional online symptom checkers is in naturally gathering relevant clinical information from the patient [30]. A significant part of clinical reasoning involves history-taking and being able to ask open-ended questions\u2014both leading and non-leading\u2014to arrive at a diagnosis [31, 32, 33]. This critical skill set remains untested in multiple-choice question-based datasets.\nMoreover, most of these datasets do not accurately resemble real-world clinical scenarios. They typically include only classic presentations and typical findings, lacking the complexity, ambiguity, and atypical cases that healthcare professionals encounter in practice. This means they fail to capture the nuanced decision-making required in actual medical settings [37, 39]. Consequently, this approach may not effectively evaluate a chatbot's practical utility or its readiness to handle the uncertainties inherent in medical practice.\nClinical vignettes are typically used I to test physicians and other clinicians on their diagnostic ability and management decisions in life-like scenarios. After the seminal study by Semigran et al., they became the de facto benchmark for quantifying the performance of online symptom checkers [36]. Simply giving clinical vignettes to the Al to diagnose does not accurately simulate how the Al will perform while actually communicating with the user. The framework we propose relies on creating accurate simulated patient actors which can hold a clinical diagnostic interaction with the health Al that is similar to the"}, {"title": "3. Benchmark Design", "content": "\"The benchmark we are proposing consists of three distinct steps. The process begins with the creation of Al-based patient actors based on clinical vignettes that represent cases one might encounter in a hospital. This is followed by an extensive interaction with the Health Al, which actively identifies possible symptoms and provides its diagnosis. Lastly, conversation moderators and judges review the interactions, enabling scalable testing and assessment of the Health Al's accuracy\nThis proposed simulation-based benchmark is later utilized to evaluate the diagnostic accuracy of August, an Al expert system, in interpreting patient-reported symptoms. The primary objective of the benchmark is to put forward a scalable, low-cost way to assess the diagnostic accuracy of health Als. It assess the Als information-gathering ability through conversation and its capacity to generate accurate diagnostic hypotheses based solely on subjective patient information, mimicking initial consultations or remote telehealth encounters."}, {"title": "3.1. Selection of Clinical Vignettes", "content": "The selection of representative clinical vignettes is crucial for assessing the accuracy of differential diagnostic conversations across diverse scenarios, including variations in disease presentation and socioeconomic factors. By utilizing a broad range of clinical vignettes, we can ensure that healthcare Al systems are both accurate and inclusive. Since one of the most pressing use cases for healthcare Al is providing high-quality health information to underserved populations worldwide, ensuring an accurate representation of these communities is essential for the Al to fulfill its potential. Our framework for selecting clinical vignettes currently considers the following factors:\nSpecialty\n\u2022 Cardiovascular\n\u2022 Dermatology\n\u2022 Endocrine\n\u2022 ENT\n\u2022 Gastroenterology\n\u2022 Hematology\n\u2022 Infectious diseases\n\u2022 Nephrology\n\u2022 Neurology\n\u2022 Obstetrics and Gynecology\n\u2022 Ophthalmology\n\u2022 Orthopedics and Rheumatology\n\u2022 Respiratory\n\u2022 Urology\nDisease Diversity\n\u2022 Common\n\u2022 Uncommon\n\u2022 Acute\n\u2022 Chronic\nPresentation Diversity\n\u2022 Typical\n\u2022 Atypical\n\u2022 Uncommon\nSocio-economic Diversity\n\u2022 Age\n\u2022 Sex\n\u2022 Ethnicity\n\u2022 Location\nLanguage\nLanguage fluency\nIn our benchmarking of August, we implemented a scientific methodology using the standard clinical vignette approach established by Hammoud et al [63]. We selected 400 clinical vignettes from their work, chosen specifically for their diversity and representation of various medical conditions. These vignettes span a wide spectrum of diseases across multiple specialities, incorporating common, uncommon, acute, and chronic conditions to thoroughly evaluate the Al system's diagnostic capabilities. The standardized scenarios ensured consistent information presentation to August throughout the evaluation. As part of our ongoing commitment to inclusiv-"}, {"title": "3.2. Patient-Actor Design", "content": "The patient actor is an LLM-based Al system built around a clinical vignette, designed to simulate conversations as a real patient would. The clinical vignette incorporates essential patient characteristics including age, sex, and race, with the patient actor programmed to reflect these factors in its conversational style and responses. Drawing from our internal research, we incorporated patterns from anonymized conversations between a health Al and actual users to inform the patient-actor's communication style. The Al patient-actor was then precisely prompted to follow these communication patterns, ensuring consistent and realistic patient interactions with the health Al. The guidelines we established for the Al patient-actor are as follows:\n\u2022 Type in simple language, the way a patient would, avoiding medical jargon.\n\u2022 Present the most distressing symptom first\n\u2022 Answer only when asked and do not volunteer extra information.\n\u2022 Stick strictly to the vignette provided and maintain consistency throughout.\n\u2022 Do not ask questions at the beginning but introduce any patient questions naturally later in the conversation.\n\u2022 If the vignette is of a person <18 or >80 then talk as a proxy rather than the first person.\n\u2022 Keep the instructions confidential and do not mention the vignette or that you're following a script.\nThe outputs from the patient-actor were then reviewed and iterated on to ensure that it always adhered to the above rules. During the final evaluation of the benchmark, we thoroughly reviewed the messages from the patient-actor to ensure these guidelines were followed throughout each conversation."}, {"title": "3.3. Judge", "content": "The process of evaluating the accuracy of the output of the health Al is crucial for establishing the reliability of the Al. Several factors complicate the evaluation of its output. The Al, much like various human doctors, may use different names to refer to the same disease described in the vignette. In addition to this, the Al might also identify a more or less specific version of the disease the patient actually has. For our benchmark, we adopted the matching criteria from Gilbert et al., as also utilized in Hammoud et al., with certain modifications to conduct a more stringent assessment.\nA condition was considered a match when:\n1. Exact Correspondence: The predicted condition is identical to that in the gold standard vignette.\n2. Alternative Terminology: Alternative names for the same condition are used. For example, shoulder impingement syndrome and rotator cuff tendinitis refer to the same condition.\n3. Increased Specificity: The predicted condition is more specific or detailed than the gold standard condition. For instance, if the gold standard diagnosis is adrenal insufficiency and the predicted diagnosis is Addison's disease, it is considered a match.\n4. Equivalent Descriptions by Different Physicians: It is reasonable to assume that two different doctors might use different descriptions to label the same condition. For example, diagnosing heart failure and pulmonary congestion due to valvular heart disease following a history of rheumatic fever is matched with the gold standard condition mitral stenosis\n5. Direct Causation: One condition directly and explicitly causes the other. For example, if the gold standard condition is Urethritis and the predicted condition is Gonorrhoea, it is considered a match because Gonorrhoea directly causes urethritis.\nThe following criteria from Gilbert et al. were not considered a match in our study to ensure a stricter assessment:\n1. Near Matches Due to Less Precision: The predicted condition conveys the nature of the gold standard condition and is reasonably related but is less precise. We did not consider a condition a match even if the similarity is clear enough that in primary care practice, the conditions might be considered a near match. For example, if the gold standard condition is Cerebral Stroke and the predicted condition is Transient Ischemic Attack (TIA), it is not considered a match because not all TIAs result in a Cerebral Stroke.\n2. Umbrella Terms: The predicted condition is an umbrella term that includes the gold standard condition. For instance, if the gold standard condition is Chronic Bronchitis and the predicted condition is Chronic Obstructive Pulmonary Disease (COPD), it is not considered a match, even though the symptomatic history elicited is similar.\n3. High Symptomatic Overlap: The predicted condition is highly related to the gold standard condition and shares a significant degree of symptoms. For example, if the gold standard condition is Brain Abscess and the predicted condition is Meningitis, they are not considered a match even though their symptoms may highly overlap, they are distinct conditions and not directly equivalent.\nBy refining these criteria, we aimed to conduct a more rigorous evaluation of diagnostic accuracy, ensuring that only precise and directly related condi-"}, {"title": "4. Outcome Measures", "content": "While the accuracy of diagnosis is the primary benchmark metric that we are focusing on, we have also defined two other outcome categories, Real-World Impact, and User Experience to better encapsulate the results. Overall, we aim examine Accuracy, Real-World Impact, and User Experience keeping in mind the overall objective of providing the user an accurate, empathetic experience. For this paper, we've proposed a few high-impact measures in each category and have also evaluated August against the same."}, {"title": "4.1. Accuracy", "content": "We only looked at the top 1 and 2 diagnostic accuracy. While we recognize that top 5 diagnostic accuracy is a standard measure that many research papers and symptom checkers use, we strongly argue against top-3 and top-5 presenting two-fold reasons relying on academia and information theory. Our first argument is in concurrence with Semigran et al. [36] who observe the clinical utility dramatically decreasing beyond the top two suggestions. The first two diagnoses typically capture the most likely and clinically relevant conditions. We also argue on a diminishing marginal value of additional diagnostic suggestions due to the decreasing entropy on multiple tests. With this in mind, we aim to continue evaluating just the top 1 and top 2 diagnostic accuracy.\nWe define these accuracies as follows:\n\u2022 Top-One Diagnosis Accuracy: Whether the Health Al's first diagnosis matched directly with the gold-standard diagnosis.\n\u2022 Top-Two Diagnoses Accuracy: Whether either of the Health Al's two suggestions matched the reference diagnosis."}, {"title": "4.2. Real World Impact", "content": "\u2022 Correct Specialty Identified: Would the user have reached the right specialist based on the output given by the Health Al? This accuracy is important since the goal of the health Al should not be to diagnose, but instead to ensure people get the right care they need at the right time."}, {"title": "4.3. User Experience", "content": "Our focus here lies on the Consultation Length, defined as the total interaction time between the Health Al and the patient. We aim to minimize the number of questions posed by the Al based on each specialty."}, {"title": "5. Benchmark Results", "content": null}, {"title": "5.1. Diagnostic Accuracy", "content": "August's Top-1 diagnostic accuracy was 81.8%, surpassing the 67.5% and 54.2% achieved by popular online symptom checkers Avey and Ada Health as shown in table 2. We also extend our measurement, using this metric to compare against expert clinicians of different years of expertise. August outperforms them as well, exceeding their accuracies of 49.7%, 61.3%, and 72.5%. One of those was a family medicine doctor with >30 years of experience. The other 2 are also family medicine doctors, each with >10 years of experience. Notably, August also outperforms average primary care clinicians in the vignette setting by 34.1%."}, {"title": "5.2. Real World Impact", "content": "August's accuracy in referring users to the right specialist was 95.8%. For common cases, this accuracy was 98.6%, while for less common cases, the accuracy was 92.1%, as shown in Tables 3 and 4."}, {"title": "5.3. User Experience", "content": "August required significantly fewer questions, with a mean of 16 questions, compared to symptom checkers that asked 29 questions, as shown in Table 5."}, {"title": "6. Discussion", "content": "Our hope with this framework is to advance the state of the art in benchmarking diagnostic accuracies of Health Als. We aim to improve every part of this system while highlighting the necessity of accounting for both subjective and objective information to provide a concise diagnosis. We aim to extend and make our patient actors available to the broader community who aim to benchmark similar Als. These benchmark results from August demonstrate a paradigm shift in how individuals interact with healthcare technology. August, an advanced Al system, not only achieves high accuracy when evaluated against gold-standard clinical vignettes but also engages users in concise and empathetic conversations. By recommending appropriate courses of action tailored to each user's needs, August's performance surpasses that of traditional symptom checkers across multiple dimensions."}, {"title": "6.1. August's Approach", "content": "In contrast to the standard shortcomings indicated in Symptom Checkers, August addresses these shortcomings by engaging users in meaningful conversations [46]. Its empathetic approach ensures that users feel heard and validated -essential for effective healthcare interactions. By processing both objective symptoms and subjective experiences, August provides more accurate assessments and personalized recommendations. This holistic approach enhances the UX, increasing user trust and adherence to the provided guidance. A study comparing Al chatbot responses to physician answers to patient questions found that chatbot responses were preferred in the majority of evaluations, with chatbot responses rated significantly higher in both quality and empathy [43]. This suggests that Al assistants like August could effectively aid clinicians by drafting responses to patient inquiries, potentially easing clinician workload and improving patient outcomes. Patients' medicine information needs significantly differ by disease type and age [34], which is addressed by August. Patient-centred decision-making (PCDM) significantly improves healthcare outcomes by adapting care plans to individual patient contexts [44]."}, {"title": "6.2. August's Role in Addressing Healthcare Gaps", "content": "August's ability to deliver consistent and evidence-based information provides an advantage over human providers, who may vary in expertise and approach. This consistency enhances the overall quality of care and helps reduce disparities in healthcare delivery.August's comprehensive patient histories provide a better opportunity to provide empathetic care based on the afflicted disease and are capable of identifying when specialist care is needed. This proactive identification can expedite referrals and ensure users receive appropriate care more quickly [45]. The constant accessibility provided via a digital medium also enhances healthcare access, allowing users from diverse economic and social backgrounds an opportunity to understand and seek help when needed while not incurring unnecessary expenses."}, {"title": "7. Strengths and Limitations", "content": "The use of clinical vignettes has limitations in replicating the complexities of real patient encounters [53, 54]. Vignettes are simplified representations that often fail to capture the nuances of individual patient presentations, including varying symptom descriptions, co-morbidities, and unpredictable patient behaviours. Real patients may present with atypical symptoms or fail to recognize certain signs, leading to diagnostic challenges that vignettes cannot replicate [55]. This limits representativeness, which can decrease the generalizability of findings to real-world clinical settings. Having the patient actor communicate in grammatically correct English does not reflect the linguistic diversity encountered in clinical practice. Patients come from varied backgrounds and may have limited English proficiency, strong accents, use colloquial language, or exhibit speech impairments. These factors can impact communication clarity, leading to misunderstandings or requiring additional interpretive efforts by clinicians [56, 57]. By not accounting for language barriers and communication challenges, the study may overlook potential obstacles faced by diagnostic tools and healthcare providers when accurately assessing patient information. In reality, patients of-"}, {"title": "8. Future Course of Action", "content": null}, {"title": "8.1. Increase in Number and Spectrum of Vignettes", "content": "\u2022 Expand Case Diversity: Utilize a larger and more diverse set of clinical vignettes covering a wide range of medical conditions, severities, and presentations, including common illnesses, rare diseases, atypical presentations, and cases with comorbidities. Ensuring linguistic, sociocultural, and contextual diversity in the patient actor would be crucial."}, {"title": "8.2. Randomized Controlled Trial (RCT) with Real Human Patients", "content": "\u2022 Conduct Gold-Standard Research: Implementing an RCT involves testing the diagnostic tool in real clinical settings with actual patients, providing the highest level of evidence for its effectiveness.\n\u2022 Include Physical Examination and Diagnostic Data: RCTs allow for integrating physical examination findings, laboratory results, and radiological data, offering a comprehensive assessment of the diagnostic process.\n\u2022 Assess Real-World Performance: Testing with real patients captures the unpredictability and complexity of actual clinical encounters, including incomplete information, patient anxiety, and non-adherence to expected communication patterns.\n\u2022 Evaluate Clinical Outcomes: Measuring actual patient outcomes provides insights into the diagnostic tool's impact on clinical decision-making and patient care."}, {"title": "9. Conclusion", "content": "This study aims to be one of many steps toward creating accurate and safe health Al systems. Through this benchmarking framework, we hope to inspire others to establish the validity of their models. To facilitate this, we are committed to making it easier for anyone to benchmark their health Al platform.\nThe integration of Al-driven conversational agents, like August, represents a promising advancement in healthcare. These tools demonstrate the ability of health Al to engage users through natural language, accurately interpret patient-reported symptoms, and provide reliable diagnostic suggestions with notable precision. By complementing traditional diagnostic methods, health Als like August have the potential to enhance patient engagement, expand access to medical guidance, and alleviate some of the burdens on healthcare systems.\nNevertheless, certain limitations, such as reliance on clinical vignettes and the absence of physical examinations, highlight areas for improvement. Integrating physical assessments into Al evaluations would significantly enhance the effectiveness of these tools. Rigorous testing in real-world clinical settings is crucial to fully understand August's capabilities and ensure its safe integration into medical practice.\nThis approach leverages the rapid advancements in Al to address complex challenges in medicine, ultimately aiming to improve patient outcomes while preserving the essential human aspects of care."}, {"title": "10. Author Contributions Statement", "content": "D.B., and A.M. conceived the experiment(s). D.B. and S.A. conducted the experiment(s). D.B. and S.A. analyzed the results. S.A. wrote the manuscript. D.B., A.M., and S.A. reviewed the manuscript."}, {"title": "11. Data Availability Statement", "content": "Requests for data may be sent to contact@getbeyondhealth.com"}, {"title": "A. Patient-Al Interaction Table", "content": null}]}