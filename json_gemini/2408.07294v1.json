{"title": "SUMRECOM: A PERSONALIZED SUMMARIZATION APPROACH BY\nLEARNING FROM USERS' FEEDBACK", "authors": ["Samira Ghodratnama", "Mehrdad Zakershahrak"], "abstract": "Existing multi-document summarization approaches produce a uniform summary for all users without\nconsidering individuals' interests, which is highly impractical. Making a user-specific summary\nis a challenging task as it requires: i) acquiring relevant information about a user; ii) aggregating\nand integrating the information into a user-model; and iii) utilizing the provided information in\nmaking the personalized summary. Therefore, in this paper, we propose a solution to a substantial\nand challenging problem in summarization, i.e., recommending a summary for a specific user. The\nproposed approach, called SumRecom, brings the human into the loop and focuses on three aspects:\npersonalization, interaction, and learning user's interest without the need for reference summaries.\nSumRecom has two steps: i) The user preference extractor to capture users' inclination in choosing\nessential concepts, and ii) The summarizer to discover the user's best-fitted summary based on the\ngiven feedback. Various automatic and human evaluations on the benchmark dataset demonstrate the\nsupremacy SumRecom in generating user-specific summaries.", "sections": [{"title": "1 Introduction", "content": "Document summarization is used to extract the most informative parts of documents as a compressed version for a\nparticular user or task [1, 2]. A good summary should keep the fundamental concepts while helping users to understand\nlarge volumes of information quickly. However, it is still hard to produce summaries that are comparable with human-\nwritten ones [3, 4]. A significant challenge is a high degree of subjectivity in content selection, i.e., what is considered\nessential for different users. Optimizing a system towards one best summary that fits all users is highly impractical\nas it is the current state-of-the-art. Making a user-specific summary for an input document cluster is a challenging\ntask. It requires: i) acquiring relevant information about a user, ii) aggregating and integrating the information into a\nuser-model, and iii) using the provided information in making the personalized summary. We bring the human in the\nloop and create a personalized summary that better captures the users' needs and their different notions of importance.\nOur rationale behind this is:\n\u2022 Keeping humans in the loop by giving feedback through interaction. Besides, to reduce users' cognitive burden\nin giving feedback, we consider two aspects. First, feedback is given in the form of preference. Second, the\npreference is in the form of concepts, not a complete summary. Moreover, users are allowed to define the\ndetailed properties of produced summaries. It also helps in reducing the search space by leveraging given\nfeedback in making summary space.\n\u2022 Evaluating the quality of a summary based on a domain expert according to the user's feedback. A learner\nmust understand how to generate an optimal summary for a user based on the evaluation metric."}, {"title": "1.1 Personalization", "content": "Existing multi-document summarization approaches produce a uniform summary for all users without considering\nindividuals' interests. Therefore, summaries are not interpretable and personalized. They are also designed to create\nshort summaries and incapable of producing more extended summaries. Therefore, all details are omitted even if\nthe user is interested in more information. Unfortunately, a single summary is unlikely to serve all users in a large\npopulation. Therefore, a good summary should reflect users' preferences and interests. As a result, a good summary\nshould change per the preferences of its reader. Therefore, summaries need to reflect users' interests and background in\nmaking summaries. For instance, there is various information available on the internet about COVID-19. While one\nmight be interested in symptoms, the other could be looking for the outbreak locations, while others are searching about\nthe death toll."}, {"title": "1.2 Interaction", "content": "In a personalized approach, the system needs to know about the user's background knowledge or interests. When\nwe do not have access to the user's preferences and interests, including profile or background knowledge, the system\nrequires interaction with users to acquire feedback for modeling users' interests. Multiple forms of feedback have been\nstudied for different applications, such as clicks, post-edits, annotations over spans of text, and preferences over pairs of\noutputs [5, 6, 7]."}, {"title": "1.3 Reference Summaries", "content": "Most existing document summarization techniques require access to reference summaries made by humans to train their\nsystems. Therefore, it is costly and time-consuming. A report shows that 3,000 hours of human efforts were required\nfor a simple evaluation of the summaries for the Document Understanding Conferences (DUC) [8]. Personalized\nsummaries eliminate the need for reference summaries as they make a specific summary for a user instead of optimizing\na summary for all users."}, {"title": "1.4 Our contribution", "content": "The proposed method, SumRecom is a preference-based interactive summarization approach that extracts users' interests\nto generate user-adapted results. SumRecom predicts users' desired summaries by incrementally adapting the underlying\nmodel through interacting with users. The proposed approach has two steps: i) The user preference extractor and ii)\nThe summarizer. Our model employs active learning and preference learning to extract users' preference in selecting\ncontents. SumRecom also utilizes integer linear programming (ILP) to maximize user-desired content selection based\non the given feedback. It then proposes an inverse reinforcement learning (IRL) algorithm and using the domain expert's\nknowledge for evaluating the quality of summaries based on the given feedback. The learned reward function is used to\nlearn the optimal policy to produce the user's desired summary using reinforcement learning (RL). A general overview\nof the algorithm is depicted in Figure 1 for more clarification."}, {"title": "2 Related Work", "content": "To generate a summary, an agent requires the ability of natural language processing and background knowledge. Thus,\nthis process is complicated even for a domain-expert, yet it can be even more difficult for machines. Early work\non document summarization focused on a single document. However, multi-document summarization gains more\nattention recently due to the massive development of documents [9]. In the following, we discuss multi-document\nsummarization approaches as it is the focus of this paper. We categorize multi-document summarization approaches as\ntraditional approaches, personalized and interactive approaches, and preference-based and reinforcement-learning-based\napproaches."}, {"title": "2.1 Traditional Approaches", "content": "The main category considers the process and the output type of the summarization algorithm: extractive and abstractive\napproaches. Abstractive summaries are generated by interpreting the main concepts of documents and then stating those\ncontents in another format [10]. Abstraction techniques are a substitute for the original documents rather than a part of\nthem. Therefore, abstractive approaches require deep natural language processing, such as semantic representation and\ninference. However, abstractive summaries are challenging to produce [11].\nExtractive multi-document summarization (EMDS) has been widely studied in the past. Since the proposed approach in\nthis paper is extractive, we analyze the extractive methods in more detail. Given a cluster of documents on the same\ntopic as input, an EMDS system needs to extract basic sentences from the input documents to generate a summary\ncomplying with a given length requirement that fits the user's needs [11]. Early extractive approaches focused on\nshallow features, employing graph structure, or extracting the semantically related words [12]. Different machine\nlearning approaches, such as naive-Bayes, decision trees, log-linear, and hidden Markov models are also used for this\npurpose [13, 14].\nRecently, the focus for both extractive and abstractive approaches is mainly on neural network-based and deep-\nreinforcement learning methods, which could demonstrate promising results. They employ word embedding [15] to\nrepresent words at the input level. Then, feed this information to the network to gain the output summary. These\nmodels mainly use a convolutional neural network [16], a recurrent neural network [17, 18] or the combination of\nthese two [19, 20]. JECS [21], PGN [22], and DCA [23] are recent abstractive state-of-the-art approaches. JECS is\na neural text-compression-based summarization approach that uses BLSTM as the encoder. It first selects sentences\nand then prunes the parsing tree to compress chosen sentences. PGN is a pointer generator network that works based\non encoder-decoder architecture. DCA (Deep Communicating Agents) works based on the hierarchical attention\nmechanism.\nIn a recent attempt, they used BERT, a pre-trained transformer to produce extractive summaries, called BERTSUM [24].\nBERTSUM has achieved ground-breaking performance on multiple NLP tasks. HIBERT [25], PNBERT[26], Bert-\nSumExt, and BertSumAbs [27] are also recent state-of-the-art BERT-based approaches."}, {"title": "2.2 Personalized and Interactive Approaches", "content": "There exist few recent attempts on personalized and interactive approaches in various NLP tasks. Unlike non-interactive\nsystems that only present the system output to the end-user, interactive NLP algorithms ask the user to provide\ncertain feedback forms to refine the model and generate higher-quality outputs tailored to the user. Multiple forms\nof feedback also have been studied including mouse-clicks for information retrieval [28], post-edits and ratings for\nmachine translation [29, 30], error markings for semantic parsing [31], and preferences for translation [32].\nIn the summarization task, most existing computer-assisted summarization tools present important elements of a\ndocument or the output of a given automatic summarization system to the user. The output is a summary draft where\nthey ask users to refine the results without further interaction. The refined process include to cut, paste, and reorganize\nthe important elements to formulate a final text [33, 34, 35].\nOther works present automatically derived hierarchically ordered summaries allowing users to drill down from a\ngeneral overview to detailed information [36, 37]. Therefore, these systems are neither interactive nor consider the\nuser's feedback to update their internal summarization models. Other interactive summarization systems include the\niNeATS [38] and IDS [39] systems that allow users to tune several parameters (e.g., size, redundancy, focus) for"}, {"title": "2.3 Preference-based and Reinforcement-based Approaches", "content": "There is an increasing research interest in using preference-based feedback and reinforcement learning algorithms in\nsummarization. As an example, one approach is to learn a sentence ranker from human preferences on sentence pairs in\n[48]. The ranker then is used to evaluate the quality of summaries by counting the number of high-ranked sentences\nincluded in a summary. Reinforcement-learning-based (RL) approaches are another popular category for both extractive\nand abstractive summarization in recent years [49, 50, 51]. Most existing RL-based document summarization systems\nuse heuristic functions as the reward function and, therefore, do not rely on reference summaries [49, 52]. Some other\napproaches use different ROUGE measure variants as the reward function and therefore require reference summaries\nas the rewards for RL [50, 51, 53]. However, neither ROUGE nor the heuristics-based rewards can precisely reflect\nreal user's preferences on summaries [54]. Therefore, using these imprecise reward models can severely mislead the\nRL-based summarizer. One challenge in RL-based summarization approaches is defining the reward function [55].\nThe preference-based and reinforcement learning algorithms also have been used in summarization simultaneously.\nThe first approach is SPPI [56, 57], a policy-gradient RL algorithm that receives rewards from the preference-based\nfeedback. The problem is that SPPI suffers heavily from the high sample complexity problem. Another recent\npreference reinforcement learning approach is APRIL [58], which has two stages. First, the user's ranking over\ncandidate summaries is retrieved, and then a neural reinforcement learning (RL) agent is used to search for the optimal\nsummary. However, preferring one summary over the other one in both approaches puts a considerable burden on users.\nIt is worth mentioning that summarization aims to provide users with a summary, which helps them not read numbers\nof documents. While asking users to prefer a summary over another in multiple rounds among a summary space that\nincludes all randomly possible combinations of sentences puts an additional cognitive load on them, that is even more\nthan reading the documents. Figure 2 represents an example of this comparison, where it can be even more challenging\nwhen the length of the summary increases."}, {"title": "3 Proposed Method (SumRecom)", "content": "One of the ultimate goals of machine learning is to provide predictability for unseen situations. Therefore, personalizing\nthe summaries is one of the fundamental approaches to construct summaries tailored to the user's demands. This paper\nproposes a human-in-the-loop approach to create a personalized summary that better captures the users' needs. Making\na user-specific summary for an input document cluster is a challenging task. The system should have background\nknowledge about the user to be able to produce personalized summaries. Since we do not consider any prior knowledge,\nincluding user profile or background knowledge, the system requires user interaction to model user interest.\nSumRecom considers the summarization problem as a recommender system where the goal is to suggest a personalized\nsummary to a user based on the given preferences. This novel framework has two components: i) The user preference\nextractor and ii) The summarizer. The user preference extractor is responsible for querying the user and potentially\nreceiving the feedback using active preference learning. The summarizer aims to generate summaries based on users'"}, {"title": "3.1 The user preference extractor", "content": "Understanding users' interests is the first step towards making personalized summaries. Users' interest can be extracted\nimplicitly based on users' profiles, browsing history, likes or dislikes, or retweeting in social media [60]. Consequently,\ninteraction is an approach to predict user's perspectives in the new circumstances based on the feedback user provided\nin the past [61, 62]. The user feedback can be in any form, such as mouse-click or post-edits. Further, experiments\nsuggest that preference-based interactive approaches put a lower cognitive burden on human subjects than asking for\nabsolute ratings or categorized labels as it is a binary decision [48, 63]. Besides, preferring one summary over another\nputs a significant burden on the user, as discussed in section 1. For instance, when collecting feedback about a user's\ninterest, asking the user to compare the concepts \"cancer treatment\" and \"cancer symptoms\" involves a smaller cognitive\nworkload than asking the user to assign a score to each of the concepts. On the other hand, it is challenging for users to\ndecide the usefulness of a summary throughout a scoring scheme. Therefore, in this paper, to reduce users' cognitive\nload, queries are in the form of concept selection, and the feedback is in the form of preferences.\nConcept selection aims to find the critical information within a given set of source documents as humans can quickly\nassess the importance of concepts given a topic. Since the notion of importance is specific to a particular topic or user,\nwe query users to ask preference over concepts. Users can better prefer one concept to the other instead of selecting the\nimportant concept. However, to collect enough data to make a meaningful conclusion, it is required for users to interact\nwith the system in many rounds to simulate the ideal user feedback. Therefore active learning is also used to reduce the\nnumber of interaction rounds. To recap, we use active preference learning (APL) in an interaction loop to maximize the\ninformation gained from a small number of preferences, reducing the sample complexity. In the following, the active\nand preference learning implemented in the proposed method is discussed."}, {"title": "3.1.1 Preference Learning", "content": "Preference learning is a classification method that learns how to rank instances based on observed preference information.\nIn other words, it trains based on a set of pairwise preferred items and obtaining the total ranking of objects [64].\nTo formally define the preference learning in our proposed algorithm, let X be the input space and x a cluster\nof documents. Let's define C(x) all extracted concepts from document cluster x. Therefore, we have a concept\nspace $C(x) = {C_1, C_2, .., C_N}$ with N concepts. The goal is to query users a set of pairwise preference of concepts\n${P(C_{11}, C_{21}), P(C_{12}, C_{22}), ..., P(C_{1n}, C_{2n})}$ where $p(C_{1i}, C_{2i})$ is a preference instance showing to user in i th round\nwhere:\n$P(C_{1i}, C_{2i}) = \\begin{cases}\n1, &\\text{if } C_{1i} > C_{2i}.\\\\\n0, &\\text{otherwise.}\\end{cases}$\nwhere > indicates the preference of $C_{1i}$ over $C_{2i}$. Then, the goal of preference learning is to predict the overall ranking\nof concepts. If we can find a mapping from data to real numbers, ranking the data can be solved by ranking the\nreal numbers. This mapping is called utility function U such that $c_i > C_j \u2192 U(c_i) > U(c_j)$ where U is a function\n$U:C\\rightarrow R$.\nIn this problem, the ground-truth utility function (U) measures each concept's importance based on users' attitudes. We\nassume that no two items in C(x) have the same U value which is a required condition for learning process. Finding the\nutility function is a regression learning problem that is well-studied in machine learning. In this problem, the ranking\nfunction (R) measures the importance of each concept based on users' attitude toward other concepts defined as:\n$R(c_i) = \\sum 1\\{U(c_i) > U(c_j)\\}, c_j \\in C(x)$\nwhere 1 is the indicator function. Therefore, R gives the rank of $c_i$ among all extracted in x (Cx).\nThe Bradley-Terry model [65] is a probability model widely used in preference learning. Given a pair of individuals\n$C_i$ and $c_j$ drawn from some population, it estimates the probability that the pairwise comparison $c_i > C_j$ turns out\ntrue. Having n observed preference items, the model approximates the ranking R by computing maximum likelihood\nestimate:\n$J_x (W) = \\sum [P(C_{1i}, C_{2i}) log H (C_{1i}, C_{2i}; W)+\ni \\in\n[P(C_{2i}, C_{1i}) log H(C_{2i}, C_{1i}; W)))]$ \nwhere H(c) is the logistic function defined as:\n$H(c_i, c_j; w) = \\frac{1}{1 + exp[U^*(c_j; w) \u2013 U^*(c_i; w)]}$"}, {"title": "3.1.2 Active Learning", "content": "To emphasize the need for active learning, let's consider we have M sentences to summarize, and each sentence has 4\nunique concepts on average. As a result, the number of unique concepts is $4 \\times M$. Therefore, the number of pairwise\npreferences to query the user to have a complete comparison in this setting is equal to $\\binom{4M}{2} = \\frac{4M!}{2!(4M-2)!}$. As an\nexample, if M = 100, this number is equal to 79800, which is impossible. Therefore, active learning aims to find\nthe minimum subset of best samples, in our problem, the best pairs, to query the user to gain the most information.\nTherefore, the number of examples to query user is much lower than the number required in regular supervised learning.\nThere exist different strategies to find the minimum subset of best samples. Examples include [66]:\n\u2022 Balance exploration and exploitation: The exploration and the exploitation of the data space representation\nis the measure to choose samples. In this strategy, the active learning problem is modeled as a contextual\nbandit problem.\n\u2022 Expected model change: The policy behind this model is to select the samples that would most change the\ncurrent model.\n\u2022 Expected error or variance reduction: This strategy selects samples that would most reduce the model's\ngeneralization error or variance.\n\u2022 Uncertainty sampling: The idea is to select samples for which the current model is least certain to the correct\noutput.\n\u2022 Conformal predictors: This method predicts based on the similarity of a sample with previous queried\nsamples.\n\u2022 Query by committee: In this strategy, different models are trained, and the samples that most models, called\nthe \"committee\", disagree have the potential to be queried.\nAs a solution, we propose a heuristic approach, presented in Algorithm 2 for selecting query sample pairs. The proposed\nheuristic approach aims to select the most diverse concepts to compare at first and gradually move to similar ones to\nreduce the search space. For this purpose, we partition the concepts into clusters based on different similarity measures.\nWe use semantic and lexical similarity as the features; a similar measure is proposed by [67] for grouping similar\nconcepts in the process of making a concept map. These features include normalized Levenshtein distance, Jaccard\ncoefficient between stemmed content words, semantic similarity based on Latent Semantic Analysis [68], WordNet [69],\nand word embedding [70]. Then we model the similarity as a binary classification using logistic regression such that a\npositive classification, y = 1, means that concepts are co-referent. The function is defined as:\n$P(y = 1|C_1, C_2, \\theta) = Sigmoid(\\theta^T \\delta(C_1, C_2))$\nwhere $\u03b4(C_1, C_2)$ are the features, $\\theta$ the learned parameters, and the Sigmoid function is defined as:\n$S_{\\theta}(z) = \\frac{1}{1 + \\theta(1-z)}$\nBased on the similarity of two concepts, we use an integer linear programming (ILP) function to find an optimized\npartitioning schema that maximally agrees with the pairwise classifications proposed by [71] and is transitive due to"}, {"title": "3.2 The summarizer", "content": "The user preference extractor's output is the ranking function that estimates each concept's importance based on the\nusers' feedback. The summarizer is responsible for making desired summaries for users based on their given preferences.\nOur summarizer consists of three phases: i) A summary generator, ii) An inverse reinforcement learner for evaluating\nthe generated summaries based on the expert's evaluation history, and iii) A reinforcement learner to learn how to\ngenerate the desired summary for the user, as discussed below."}, {"title": "3.2.1 The Summary Generator", "content": "After learning the importance of concepts for a user, R function, we construct summaries that are more likely the\ndesired summary for the user to reduce the summary search space. Let C(x) be the set of concepts in a given set of\nsource documents x, $pc_i$ the presence of the concept $c_i$ in the resulting summary that $c_i$ belongs to this sentence, $w_i$ the\nconcept's weight (importance), $l_j$ the length of sentence j, $ps_j$ the presence of sentence j in the summary, and L the\nsummary length constraint defined by the user. Based on these definitions, we formulate the following optimization\nfunction using Integer Linear Programming (ILP), which selects sentences with important concepts based on user's\nfeedback, defined as:\n$\\max \\sum w_ip_{ci} where \\forall i \\in [1,.., |C|] and \\forall c_i \\in s_j$\n$El_jps_j < L$ \nj\nWeights are based on the R function learned in the previous part. Then a summary pool is made using the above function.\nTo make a diverse summary pool, among top score summaries, according to Eq. 10, we select summaries where they\nare not redundant. The redundancy is defined as the similarity of sentences inside a summary without considering the\nuser's mentioned concepts divided by the summary length. Document summarization is then formulated as a sequential\ndecision-making problem, solving by a proposed reinforcement learning (RL) algorithm. In the following, we explain\nthe problem definition."}, {"title": "3.2.2 Problem Definition", "content": "We formulate summarization as a discrete optimization problem inspired by the APRIL approach [58]. Let Y indicate\nthe set of all extractive summaries for the document cluster x and $y_x \u2208 Y$ is a potential summary for document cluster\nx. An input can be either a single document or a cluster of documents on the same topic. The summarization task is to\nmap each input x to its best summary in $Y_x$ for the learned preference ranking function. EMDS can be defined as a"}, {"title": "3.2.3 The Reward Learner", "content": "SumRecom is inspired by inverse reinforcement learning [59], where instead of the policy, it first learns the reward\nutilizing an expert demonstrator to present optimal trajectories. The demonstrator is a domain expert who can evaluate\ntwo summaries based on the users' given feedback. The demonstrator can be another RL agent trained to become an\nadvisor for other learner-agents or a human. To approximate the ground-truth reward oracle from \u201cweak supervisions\u201d,\nnumeric scores that indicate the quality of the summary and preferences over summary pairs are used as humans\nreliably provide such judgments [72]. In practice, leveraging preference learning reduces the cognitive load and,\nconsequently, reduces the inevitable noise in evaluating summaries. In this paper, to evaluate SumRecom, it learns from\npreference-based (pairwise) oracles that provide preferences over summary pairs and point-based oracles that provide\npoint-based scores for summaries. In both cases, a summary is selected from the summary-pool created by the summary\ngenerator component. Then, the summaries are queried to the demonstrator for evaluation. In point-based, we draw L\nsample outputs from the summary pool without replacement.\nWe use our previous work strategy [14], ExDos, to evaluate the summaries based on three measures: coverage,\nsalience, and redundancy. ExDoS combines both supervised and unsupervised algorithms in a single framework and an\ninterpretable manner for document summarization purpose. ExDoS iteratively minimizes the error rate of the classifier\nin each cluster with the help of dynamic local feature weighting. Moreover, this approach specifies the contribution\nof features to discriminate each class. Therefore, in addition to summarizing text, ExDoS is also able to measure the\nimportance of each feature in the summarization process. In each iteration, summaries with the most difference with\npreviously selected samples are chosen for being queried. The same approach is also selected for preference-based\nsummaries. Then, we query their score values (V) from the oracle and use a regression algorithm to minimize the\naveraged mean squared error (MSE) between V and the approximate value $V^*$ where the loss function is:\n$L\\newline CMSE = \\sum (V^* \u2013 V)^2$\ni=1\nIn pairwise, we denote the collected preferences by $Ps = \\{p(Y_{11}, Y_{12}), \u2026\u2026\u2026, P(Y_{11}, Y_{21})\\}$ where y denotes the summary\nand I sample pairs are queried. Then the procedure is the same as in Eq. 3 using cross-entropy loss function defined as:\n$\\newline LCE = \\sum[p(Y_{1i}, Y_{2i}) log H (Y_{1i}, Y_{2i}; W)+\\ni \\in L\n[P(Y_{2i}, Y_{1i}) log H (Y_{2i}, Y_{1i}; W)))]$\nwhere\n$\\newline H(Y_{1i}, Y_{2i}; W) = \\frac{1}{1 + exp[V^*(y_j; w) \u2013 V^*(y_i; w)]}$"}, {"title": "3.2.4 The Policy Learner", "content": "The goal of policy learning is to search for optimal solutions in Markov Decision Processes (MDPs). We model the\nsummarization problem as an episodic MDP, meaning that each action's reward is equal to zero if the state is not\nterminate. At each step, the agent can perform either of the two actions: add another sentence to the summary or\nterminate it. The immediate reward function R(s, a) assigns the reward if s is the terminate state. The reward in\nSumRecom is the learned expert's reward, V. In EMDS, a policy \u3160 defines the strategy to add sentences to the draft\nsummary to build the summary for the user. SumRecom defines \u03c0 as the probability of choosing a summary of y among\nall summaries Y denoted as \u03c0(y). Therefore, the optimal policy, $\\pi^*$, is the function that finds the desired summary for a\ngiven input based on the user's feedback. The expected reward of performing proper policy w is defined as:\n$\\newline RRL(\\pi|x) = E_{y\\in Y}R(y) = \\sum \\pi(y)R(y)$\ny\\in Y\nwhere R(y) is the reward for selecting summary y in document cluster x. In our problem, the reward is the ranker\napproximated by the domain expert, V. Therefore, the accumulated reward to be maximized in our problem is equal to:\n$\\newline RRL (\\pi|x) = \\sum \\pi(y)V(y)$\ny\\in Y\nThe goal of MDP is to find the optimal policy $\\pi^*$ that has the highest expected reward:\n$\\newline \\pi^* = argmaxRRL(\\pi|x) = argmax \\sum \\pi(y)V(y)$\ny\\in Y\nWe use the linear Temporal Difference (TD) algorithm to obtain $\\pi^*$. The summarizer algorithm is explained in\nAlgorithm 3."}, {"title": "4 Evaluation", "content": "In this section, we present the experimental setup for implementing and assessing our summarization model's perfor-\nmance. We discuss the datasets, give implementation details, and explain how system output was evaluated."}, {"title": "4.1 Datasets", "content": "We evaluated SumRecom using three commonly employed benchmark datasets from the Document Understanding\nConferences (DUC) 1. The DUC dataset details are described in Table 1. Each dataset contains a set of document\nclusters accompanied by several human-generated summaries used for training and evaluation."}, {"title": "4.2 Evaluation Metric", "content": "We evaluate the quality of summaries using $ROUGE_N$ measure [8] defined as:\n$\\newline ROUGE_N = \\frac{\\sum_{S \\in \\{ReferenceSummaries\\}} \\sum_{gram_n \\in S} Countmatch(gram_n)}{\\sum_{S \\in \\{ReferenceSummaries\\}} \\sum_{gram_n \\in S} Count(gram_n)}$\nThe three variants of ROUGE (ROUGE-1, ROUGE-2, and ROUGE-L) are used. We used the limited length ROUGE\nrecall-only evaluation (75 words) to compare DUC dataset to avoid being biased."}, {"title": "4.2.1 Results and Analysis", "content": "First, we explain the evaluation settings, and then we discuss the results and analysis. SumRecom is evaluated from\ndifferent evaluation aspects, including:\n\u2022 The impact of different features in approximating preference learning algorithms used in this paper, including\nconcept and summary preferences.\n\u2022 The use of different strategies for active learning.\n\u2022 The effect of different concept units: unigram, bigram, and sentence.\n\u2022 The role of the query budget in both concepts and summary preferences.\n\u2022 The quality of produced summaries.\n\u2022 A human study to evaluate SumRecom through users' lens.\n\u2022 The effect of different values for parameters.\n\u2022 An ablation study to evaluate different components of the proposed framework.\nWe have simulated users in all experiments except the one proposed for human evaluation.\nFeature Analysis. 3 Before evaluating the effect of concept preference in summarization, we require to explain\nthe ground-truth concept ranker function (U) and the approximate function (U*). The ground-truth concept ranker\nfunction (U) indicates the importance of each concept. We define a predefined list of preference over concepts and the\nground-truth concept ranker value for ten clusters to simulate the users' preferences. For estimating the approximate\nfunction (U*), we define a linear model $U^*(c) = W^T(c)$ where & are the features. To this end, a set of features\nthat their importance is validated in our previous work [14], including surface-level and linguistic-level features\nare used. Surface-level features include frequency-based features (TF-IDF, RIDF, gain, and word co-occurrence),\nword-based features (uppercase word and signature words), similarity-based features (Word2Vec and Jaccard measure),\nsentence-level features (position, length cut off and length), and Named Entity. Linguistic features are made based on\nthe semantic graph. These features include the average weights of connected edges, the merge status of a sentence as a\nbinary feature, the number of concepts merged with a concept, and the number of concepts connected to the concept.\nWe defined different combinations of features, {2, 5, 8, 10, 12, 15}, starting from the most critical one based on the\nimportance estimated in our previous work [14]. We repeated the experiments for 10 cluster documents. The results of\nROUGE1 and ROUGE 2 are reported in Figure 4.\nAs results show, the performance increased by adding more features; however, the last set of features did not significantly\nimpact ROUGE values. However, it is worth mentioning that adding more features increases complexity. To simulate the"}, {"title": "Active Learning Strategy Analysis.", "content": "To evaluate the impact of the proposed heuristic for active learning, we compared\nSumRecom with six different strategies explained in section 3.1.2. Ten clusters from each dataset is used for experiments.\nROUGE1 and ROUGE2 results for each strategy are reported in Figure 5, proving the supremacy of the proposed\nheuristic approach for our problem. As the pictures reveal, the conformal approach acts approximately similar to the\nproposed heuristic function. Besides, the change model acts better than others in both cases approving that selecting the\nmost different concepts results in better summaries in this problem."}, {"title": "Query Budget Analysis.", "content": "We also measure the effectiveness of the users' query budget size in the process. We selected\nthe query size among the selection of {10, 15, 20, 25, 30, 35}, demonstrating the user's number of feedback. The results\nare reported in Figure"}]}