{"title": "SUMRECOM: A PERSONALIZED SUMMARIZATION APPROACH BY LEARNING FROM USERS' FEEDBACK", "authors": ["Samira Ghodratnama", "Mehrdad Zakershahrak"], "abstract": "Existing multi-document summarization approaches produce a uniform summary for all users without considering individuals' interests, which is highly impractical. Making a user-specific summary is a challenging task as it requires: i) acquiring relevant information about a user; ii) aggregating and integrating the information into a user-model; and iii) utilizing the provided information in making the personalized summary. Therefore, in this paper, we propose a solution to a substantial and challenging problem in summarization, i.e., recommending a summary for a specific user. The proposed approach, called SumRecom, brings the human into the loop and focuses on three aspects: personalization, interaction, and learning user's interest without the need for reference summaries. SumRecom has two steps: i) The user preference extractor to capture users' inclination in choosing essential concepts, and ii) The summarizer to discover the user's best-fitted summary based on the given feedback. Various automatic and human evaluations on the benchmark dataset demonstrate the supremacy SumRecom in generating user-specific summaries.", "sections": [{"title": "1 Introduction", "content": "Document summarization is used to extract the most informative parts of documents as a compressed version for a particular user or task [1, 2]. A good summary should keep the fundamental concepts while helping users to understand large volumes of information quickly. However, it is still hard to produce summaries that are comparable with human-written ones [3, 4]. A significant challenge is a high degree of subjectivity in content selection, i.e., what is considered essential for different users. Optimizing a system towards one best summary that fits all users is highly impractical as it is the current state-of-the-art. Making a user-specific summary for an input document cluster is a challenging task. It requires: i) acquiring relevant information about a user, ii) aggregating and integrating the information into a user-model, and iii) using the provided information in making the personalized summary. We bring the human in the loop and create a personalized summary that better captures the users' needs and their different notions of importance. Our rationale behind this is:\n\u2022 Keeping humans in the loop by giving feedback through interaction. Besides, to reduce users' cognitive burden in giving feedback, we consider two aspects. First, feedback is given in the form of preference. Second, the preference is in the form of concepts, not a complete summary. Moreover, users are allowed to define the detailed properties of produced summaries. It also helps in reducing the search space by leveraging given feedback in making summary space.\n\u2022 Evaluating the quality of a summary based on a domain expert according to the user's feedback. A learner must understand how to generate an optimal summary for a user based on the evaluation metric."}, {"title": "1.1 Personalization", "content": "Existing multi-document summarization approaches produce a uniform summary for all users without considering individuals' interests. Therefore, summaries are not interpretable and personalized. They are also designed to create short summaries and incapable of producing more extended summaries. Therefore, all details are omitted even if the user is interested in more information. Unfortunately, a single summary is unlikely to serve all users in a large population. Therefore, a good summary should reflect users' preferences and interests. As a result, a good summary should change per the preferences of its reader. Therefore, summaries need to reflect users' interests and background in making summaries. For instance, there is various information available on the internet about COVID-19. While one might be interested in symptoms, the other could be looking for the outbreak locations, while others are searching about the death toll."}, {"title": "1.2 Interaction", "content": "In a personalized approach, the system needs to know about the user's background knowledge or interests. When we do not have access to the user's preferences and interests, including profile or background knowledge, the system requires interaction with users to acquire feedback for modeling users' interests. Multiple forms of feedback have been studied for different applications, such as clicks, post-edits, annotations over spans of text, and preferences over pairs of outputs [5, 6, 7]."}, {"title": "1.3 Reference Summaries", "content": "Most existing document summarization techniques require access to reference summaries made by humans to train their systems. Therefore, it is costly and time-consuming. A report shows that 3,000 hours of human efforts were required for a simple evaluation of the summaries for the Document Understanding Conferences (DUC) [8]. Personalized summaries eliminate the need for reference summaries as they make a specific summary for a user instead of optimizing a summary for all users."}, {"title": "1.4 Our contribution", "content": "The proposed method, SumRecom is a preference-based interactive summarization approach that extracts users' interests to generate user-adapted results. SumRecom predicts users' desired summaries by incrementally adapting the underlying model through interacting with users. The proposed approach has two steps: i) The user preference extractor and ii) The summarizer. Our model employs active learning and preference learning to extract users' preference in selecting contents. SumRecom also utilizes integer linear programming (ILP) to maximize user-desired content selection based on the given feedback. It then proposes an inverse reinforcement learning (IRL) algorithm and using the domain expert's knowledge for evaluating the quality of summaries based on the given feedback. The learned reward function is used to learn the optimal policy to produce the user's desired summary using reinforcement learning (RL)."}, {"title": "2 Related Work", "content": "To generate a summary, an agent requires the ability of natural language processing and background knowledge. Thus, this process is complicated even for a domain-expert, yet it can be even more difficult for machines. Early work on document summarization focused on a single document. However, multi-document summarization gains more attention recently due to the massive development of documents [9]. In the following, we discuss multi-document summarization approaches as it is the focus of this paper. We categorize multi-document summarization approaches as traditional approaches, personalized and interactive approaches, and preference-based and reinforcement-learning-based approaches."}, {"title": "2.1 Traditional Approaches", "content": "The main category considers the process and the output type of the summarization algorithm: extractive and abstractive approaches. Abstractive summaries are generated by interpreting the main concepts of documents and then stating those contents in another format [10]. Abstraction techniques are a substitute for the original documents rather than a part of them. Therefore, abstractive approaches require deep natural language processing, such as semantic representation and inference. However, abstractive summaries are challenging to produce [11].\nExtractive multi-document summarization (EMDS) has been widely studied in the past. Since the proposed approach in this paper is extractive, we analyze the extractive methods in more detail. Given a cluster of documents on the same topic as input, an EMDS system needs to extract basic sentences from the input documents to generate a summary complying with a given length requirement that fits the user's needs [11]. Early extractive approaches focused on shallow features, employing graph structure, or extracting the semantically related words [12]. Different machine learning approaches, such as naive-Bayes, decision trees, log-linear, and hidden Markov models are also used for this purpose [13, 14].\nRecently, the focus for both extractive and abstractive approaches is mainly on neural network-based and deep-reinforcement learning methods, which could demonstrate promising results. They employ word embedding [15] to represent words at the input level. Then, feed this information to the network to gain the output summary. These models mainly use a convolutional neural network [16], a recurrent neural network [17, 18] or the combination of these two [19, 20]. JECS [21], PGN [22], and DCA [23] are recent abstractive state-of-the-art approaches. JECS is a neural text-compression-based summarization approach that uses BLSTM as the encoder. It first selects sentences and then prunes the parsing tree to compress chosen sentences. PGN is a pointer generator network that works based on encoder-decoder architecture. DCA (Deep Communicating Agents) works based on the hierarchical attention mechanism.\nIn a recent attempt, they used BERT, a pre-trained transformer to produce extractive summaries, called BERTSUM [24]. BERTSUM has achieved ground-breaking performance on multiple NLP tasks. HIBERT [25], PNBERT[26], Bert-SumExt, and BertSumAbs [27] are also recent state-of-the-art BERT-based approaches."}, {"title": "2.2 Personalized and Interactive Approaches", "content": "There exist few recent attempts on personalized and interactive approaches in various NLP tasks. Unlike non-interactive systems that only present the system output to the end-user, interactive NLP algorithms ask the user to provide certain feedback forms to refine the model and generate higher-quality outputs tailored to the user. Multiple forms of feedback also have been studied including mouse-clicks for information retrieval [28], post-edits and ratings for machine translation [29, 30], error markings for semantic parsing [31], and preferences for translation [32].\nIn the summarization task, most existing computer-assisted summarization tools present important elements of a document or the output of a given automatic summarization system to the user. The output is a summary draft where they ask users to refine the results without further interaction. The refined process include to cut, paste, and reorganize the important elements to formulate a final text [33, 34, 35].\nOther works present automatically derived hierarchically ordered summaries allowing users to drill down from a general overview to detailed information [36, 37]. Therefore, these systems are neither interactive nor consider the user's feedback to update their internal summarization models. Other interactive summarization systems include the iNeATS [38] and IDS [39] systems that allow users to tune several parameters (e.g., size, redundancy, focus) for customizing the produced summaries. Avinesh and Meyer [40] proposed a most recent interactive summarization approach that asks users to label important bigrams within candidate summaries. Their system can achieve near-optimal performance in ten rounds of interaction in simulation experiments, collecting up to 350 critical bigrams. However, labeling important bigrams is an enormous burden on the users, as users have to read through many potentially unimportant bigrams.\nConsidering the interactivity structure of the proposed approach, it can also be comparable with query-focused summarization approaches. In query-based summarization, a good summary is relevant to a specific query. One common technique in this category is to adapt existing summarization approaches for answering a query. In this way, the importance of a sentence should be judged by its significance to the user's query. Using topic signature words or graph-based approaches [41], and sub-modular approaches [42, 43] are examples of this category. Other approaches also exist which are explicitly designed for answering the query [44]. Recent studies include Dual-CES [45], a novel unsupervised approach that uses the Cross-Entropy Summarizer (CES). The focus is to handle the tradeoff between saliency and focus in summarization. QMDS [46] is another study in the abstractive query-focused summarization approach where they tackle two problems. First, generating large-scale, high-quality training datasets using data augmentation techniques. Second, they build abstractive end-to-end neural network models on the combined datasets that yield new state-of-the- art transfer results on DUC datasets.\nEvaluating interactive summarization approaches is another challenge. In a study [47], authors developed an end-to-end evaluation framework, focusing on expansion-based interaction. It considers the accumulating information along a user session."}, {"title": "2.3 Preference-based and Reinforcement-based Approaches", "content": "There is an increasing research interest in using preference-based feedback and reinforcement learning algorithms in summarization. As an example, one approach is to learn a sentence ranker from human preferences on sentence pairs in [48]. The ranker then is used to evaluate the quality of summaries by counting the number of high-ranked sentences included in a summary. Reinforcement-learning-based (RL) approaches are another popular category for both extractive and abstractive summarization in recent years [49, 50, 51]. Most existing RL-based document summarization systems use heuristic functions as the reward function and, therefore, do not rely on reference summaries [49, 52]. Some other approaches use different ROUGE measure variants as the reward function and therefore require reference summaries as the rewards for RL [50, 51, 53]. However, neither ROUGE nor the heuristics-based rewards can precisely reflect real user's preferences on summaries [54]. Therefore, using these imprecise reward models can severely mislead the RL-based summarizer. One challenge in RL-based summarization approaches is defining the reward function [55].\nThe preference-based and reinforcement learning algorithms also have been used in summarization simultaneously. The first approach is SPPI [56, 57], a policy-gradient RL algorithm that receives rewards from the preference-based feedback. The problem is that SPPI suffers heavily from the high sample complexity problem. Another recent preference reinforcement learning approach is APRIL [58], which has two stages. First, the user's ranking over candidate summaries is retrieved, and then a neural reinforcement learning (RL) agent is used to search for the optimal summary. However, preferring one summary over the other one in both approaches puts a considerable burden on users. It is worth mentioning that summarization aims to provide users with a summary, which helps them not read numbers of documents. While asking users to prefer a summary over another in multiple rounds among a summary space that includes all randomly possible combinations of sentences puts an additional cognitive load on them, that is even more than reading the documents."}, {"title": "3 Proposed Method (SumRecom)", "content": "One of the ultimate goals of machine learning is to provide predictability for unseen situations. Therefore, personalizing the summaries is one of the fundamental approaches to construct summaries tailored to the user's demands. This paper proposes a human-in-the-loop approach to create a personalized summary that better captures the users' needs. Making a user-specific summary for an input document cluster is a challenging task. The system should have background knowledge about the user to be able to produce personalized summaries. Since we do not consider any prior knowledge, including user profile or background knowledge, the system requires user interaction to model user interest.\nSumRecom considers the summarization problem as a recommender system where the goal is to suggest a personalized summary to a user based on the given preferences. This novel framework has two components: i) The user preference extractor and ii) The summarizer. The user preference extractor is responsible for querying the user and potentially receiving the feedback using active preference learning. The summarizer aims to generate summaries based on users' feedback and learns how to make a user-desired summary by developing a reinforcement learning algorithm (IRL) [59]."}, {"title": "3.1 The user preference extractor", "content": "Understanding users' interests is the first step towards making personalized summaries. Users' interest can be extracted implicitly based on users' profiles, browsing history, likes or dislikes, or retweeting in social media [60]. Consequently, interaction is an approach to predict user's perspectives in the new circumstances based on the feedback user provided in the past [61, 62]. The user feedback can be in any form, such as mouse-click or post-edits. Further, experiments suggest that preference-based interactive approaches put a lower cognitive burden on human subjects than asking for absolute ratings or categorized labels as it is a binary decision [48, 63]. Besides, preferring one summary over another puts a significant burden on the user, as discussed in section 1. For instance, when collecting feedback about a user's interest, asking the user to compare the concepts \"cancer treatment\" and \"cancer symptoms\" involves a smaller cognitive workload than asking the user to assign a score to each of the concepts. On the other hand, it is challenging for users to decide the usefulness of a summary throughout a scoring scheme. Therefore, in this paper, to reduce users' cognitive load, queries are in the form of concept selection, and the feedback is in the form of preferences.\nConcept selection aims to find the critical information within a given set of source documents as humans can quickly assess the importance of concepts given a topic. Since the notion of importance is specific to a particular topic or user, we query users to ask preference over concepts. Users can better prefer one concept to the other instead of selecting the important concept. However, to collect enough data to make a meaningful conclusion, it is required for users to interact with the system in many rounds to simulate the ideal user feedback. Therefore active learning is also used to reduce the number of interaction rounds. To recap, we use active preference learning (APL) in an interaction loop to maximize the information gained from a small number of preferences, reducing the sample complexity. In the following, the active and preference learning implemented in the proposed method is discussed."}, {"title": "3.1.1 Preference Learning", "content": "Preference learning is a classification method that learns how to rank instances based on observed preference information. In other words, it trains based on a set of pairwise preferred items and obtaining the total ranking of objects [64].\nTo formally define the preference learning in our proposed algorithm, let X be the input space and x a cluster of documents. Let's define C(x) all extracted concepts from document cluster x. Therefore, we have a concept space C(x) = {C1, C2, .., CN} with N concepts. The goal is to query users a set of pairwise preference of concepts {P(C11, C21), P(C12, C22), ..., P(C1n, C2n)} where p(C1i, C2i) is a preference instance showing to user in i th round where:\n\\(P(C_{1i}, C_{2i}) =\\begin{cases} 1, & \\text{if } C_{1i} > C_{2i}.\\\\ 0, & \\text{otherwise.} \\end{cases}\\)\nwhere > indicates the preference of C1i over C2i. Then, the goal of preference learning is to predict the overall ranking of concepts. If we can find a mapping from data to real numbers, ranking the data can be solved by ranking the real numbers. This mapping is called utility function U such that ci > Cj \u2192 U(ci) > U(cj) where U is a function U:C\u2192R.\nIn this problem, the ground-truth utility function (U) measures each concept's importance based on users' attitudes. We assume that no two items in C(x) have the same U value which is a required condition for learning process. Finding the utility function is a regression learning problem that is well-studied in machine learning. In this problem, the ranking function (R) measures the importance of each concept based on users' attitude toward other concepts defined as:\n\\(R(c_i) = \\sum 1\\{U(c_i) > U(c_j)\\}, c_j \\in C(x)\\)\nwhere 1 is the indicator function. Therefore, R gives the rank of c\u2081 among all extracted in x (Cx).\nThe Bradley-Terry model [65] is a probability model widely used in preference learning. Given a pair of individuals Ci and cj drawn from some population, it estimates the probability that the pairwise comparison ci > Cj turns out true. Having n observed preference items, the model approximates the ranking R by computing maximum likelihood estimate:\n\\(J_x(W) = \\sum_{i \\epsilon n} [P(C_{1i}, C_{2i}) log H (C_{1i}, C_{2i}; W)+\n[P(C_{2i}, C_{1i}) log H(C_{2i}, C_{1i}; W)))]\\)\nwhere H(c) is the logistic function defined as:\n\\(H(c_i, c_j; w) = \\frac{1}{1 + exp[U^*(c_j; w) \u2013 U^*(c_i; w)]}\\)"}, {"title": "3.1.2 Active Learning", "content": "To emphasize the need for active learning, let's consider we have M sentences to summarize, and each sentence has 4 unique concepts on average. As a result, the number of unique concepts is 4 \u00d7 M. Therefore, the number of pairwise preferences to query the user to have a complete comparison in this setting is equal to \\(\\binom{4M}{2} = \\frac{4M!}{2!(4M-2)!}\\). As an example, if M = 100, this number is equal to 79800, which is impossible. Therefore, active learning aims to find the minimum subset of best samples, in our problem, the best pairs, to query the user to gain the most information. Therefore, the number of examples to query user is much lower than the number required in regular supervised learning. There exist different strategies to find the minimum subset of best samples. Examples include [66]:\n\u2022 Balance exploration and exploitation: The exploration and the exploitation of the data space representation is the measure to choose samples. In this strategy, the active learning problem is modeled as a contextual bandit problem.\n\u2022 Expected model change: The policy behind this model is to select the samples that would most change the current model.\n\u2022 Expected error or variance reduction: This strategy selects samples that would most reduce the model's generalization error or variance.\n\u2022 Uncertainty sampling: The idea is to select samples for which the current model is least certain to the correct output.\n\u2022 Conformal predictors: This method predicts based on the similarity of a sample with previous queried samples.\n\u2022 Query by committee: In this strategy, different models are trained, and the samples that most models, called the \"committee\", disagree have the potential to be queried.\nAs a solution, we propose a heuristic approach, presented in Algorithm 2 for selecting query sample pairs. The proposed heuristic approach aims to select the most diverse concepts to compare at first and gradually move to similar ones to reduce the search space. For this purpose, we partition the concepts into clusters based on different similarity measures. We use semantic and lexical similarity as the features; a similar measure is proposed by [67] for grouping similar concepts in the process of making a concept map. These features include normalized Levenshtein distance, Jaccard coefficient between stemmed content words, semantic similarity based on Latent Semantic Analysis [68], WordNet [69], and word embedding [70]. Then we model the similarity as a binary classification using logistic regression such that a positive classification, y = 1, means that concepts are co-referent. The function is defined as:\n\\(P(y = 1|C_1, C_2, \\theta) = Sigmoid(\\theta^T \\delta(C_1, C_2))\\)\nwhere \u03b4(C1, C2) are the features, \u03b8 the learned parameters, and the Sigmoid function is defined as:\n\\(S_{\\theta}(z) = \\frac{1}{1 + e^{(1-z)}}\\)\nBased on the similarity of two concepts, we use an integer linear programming (ILP) function to find an optimized partitioning schema that maximally agrees with the pairwise classifications proposed by [71] and is transitive due to"}, {"title": "3.2 The summarizer", "content": "The user preference extractor's output is the ranking function that estimates each concept's importance based on the users' feedback. The summarizer is responsible for making desired summaries for users based on their given preferences. Our summarizer consists of three phases: i) A summary generator, ii) An inverse reinforcement learner for evaluating the generated summaries based on the expert's evaluation history, and iii) A reinforcement learner to learn how to generate the desired summary for the user, as discussed below."}, {"title": "3.2.1 The Summary Generator", "content": "After learning the importance of concepts for a user, R function, we construct summaries that are more likely the desired summary for the user to reduce the summary search space. Let C(x) be the set of concepts in a given set of source documents x, pc\u2081 the presence of the concept ci in the resulting summary that ci belongs to this sentence, wi the concept's weight (importance), lj the length of sentence j, ps; the presence of sentence j in the summary, and L the summary length constraint defined by the user. Based on these definitions, we formulate the following optimization function using Integer Linear Programming (ILP), which selects sentences with important concepts based on user's feedback, defined as:\n\\(\\)max \\sum w_ip_{c_i} \\text{ where } \\forall i\\in [1,.., |C|] \\text{ and } \\forall c_i \\in s_j\\ \\\\sum l_jp_{s_j} <L\\)\nWeights are based on the R function learned in the previous part. Then a summary pool is made using the above function. To make a diverse summary pool, among top score summaries, according to Eq. 10, we select summaries where they are not redundant. The redundancy is defined as the similarity of sentences inside a summary without considering the user's mentioned concepts divided by the summary length. Document summarization is then formulated as a sequential decision-making problem, solving by a proposed reinforcement learning (RL) algorithm. In the following, we explain the problem definition."}, {"title": "3.2.2 Problem Definition", "content": "We formulate summarization as a discrete optimization problem inspired by the APRIL approach [58]. Let Y indicate the set of all extractive summaries for the document cluster x and yx \u2208 Y is a potential summary for document cluster x. An input can be either a single document or a cluster of documents on the same topic. The summarization task is to map each input x to its best summary in Yr for the learned preference ranking function. EMDS can be defined as a sequential decision-making problem, sequentially select sentences from the original documents and add them to a draft summary. Therefore, it can be defined as an episodic MDP (Markov Decision Process) problem described below.\nAn episodic MDP is a tuple (S, A, P, R, T) where S is the set of states, A is the set of actions, P : S \u00d7 A \u00d7 S \u2192 R is the transition function, R(s, a) is the reward action performing an action(a) in a state (s) and T is the set of terminal states. In the EMDS context, as defined in [55], a state is a draft summary and A includes two types of actions: concatenate a new sentence to the current draft summary or terminate the draft summary construction. The reward function R returns an evaluation score of the summary once the action terminates is performed; otherwise, it returns 0 because the summary is still under construction and hence can not be evaluated. A policy \u03c0(s, a) : S \u00d7 A \u2192 R in an MDP defines how actions are selected in state s.\nEpisodic MDP for modeling document summarization has two components: i) reward: what is defined as a good summary, ii) policy: how to select sentences (actions) to maximize the rewards. State-of-the-art summarization approaches are divided into two categories: cross-input paradigm and input-specific [55]. The former employs reinforcement learning algorithms such that the agent interacts with a ground-truth reward oracle over multiple episodes to learn a policy that maximizes the accumulated reward in the episode. The learned policy is used to apply on unseen data at test time for generating summaries. However, learning such a cross-input policy requires considerable time, data, and parameter tuning due to the vast search spaces and delayed rewards. On the other side, learning input-specific RL policies is a more efficient alternative that agent interacts to learn a policy specifically for the given input without requiring parallel data or reward oracle. However, they depend on handcrafted rewards, challenging to design to fit all inputs [55].\nSumRecom takes advantage of two categories of cross-input and input-specific reinforcement learning. First, it learns a cross-input reward oracle at training time and then uses the learned reward to train an input-specific policy for each input at test time, as discussed below."}, {"title": "3.2.3 The Reward Learner", "content": "SumRecom is inspired by inverse reinforcement learning [59], where instead of the policy, it first learns the reward utilizing an expert demonstrator to present optimal trajectories. The demonstrator is a domain expert who can evaluate two summaries based on the users' given feedback. The demonstrator can be another RL agent trained to become an advisor for other learner-agents or a human. To approximate the ground-truth reward oracle from \u201cweak supervisions\u201d, numeric scores that indicate the quality of the summary and preferences over summary pairs are used as humans reliably provide such judgments [72]. In practice, leveraging preference learning reduces the cognitive load and, consequently, reduces the inevitable noise in evaluating summaries. In this paper, to evaluate SumRecom, it learns from preference-based (pairwise) oracles that provide preferences over summary pairs and point-based oracles that provide point-based scores for summaries. In both cases, a summary is selected from the summary-pool created by the summary generator component. Then, the summaries are queried to the demonstrator for evaluation. In point-based, we draw L sample outputs from the summary pool without replacement.\nWe use our previous work strategy [14], ExDos, to evaluate the summaries based on three measures: coverage, salience, and redundancy. ExDoS combines both supervised and unsupervised algorithms in a single framework and an interpretable manner for document summarization purpose. ExDoS iteratively minimizes the error rate of the classifier in each cluster with the help of dynamic local feature weighting. Moreover, this approach specifies the contribution of features to discriminate each class. Therefore, in addition to summarizing text, ExDoS is also able to measure the importance of each feature in the summarization process. In each iteration, summaries with the most difference with previously selected samples are chosen for being queried. The same approach is also selected for preference-based summaries. Then, we query their score values (V) from the oracle and use a regression algorithm to minimize the averaged mean squared error (MSE) between V and the approximate value V* where the loss function is:\n\\(C_{MSE} = \\sum_{i=1}^L (V^* \u2013 V)^2\\)\nIn pairwise, we denote the collected preferences by Ps = {p(Y11, Y12), \u2026\u2026\u2026, P(Y11, Y21)} where y denotes the summary and I sample pairs are queried. Then the procedure is the same as in Eq. 3 using cross-entropy loss function defined as:\n\\(L_{CE} = \\sum_{i \\epsilon L} [p(Y_{1i}, Y_{2i}) log H (Y_{1i}, Y_{2i}; W)+\n[P(Y_{2i}, Y_{1i}) log H (Y_{2i}, Y_{1i}; W)))]\\)\nwhere\n\\(H(Y_{1i}, Y_{2i}; W) = \\frac{1}{1 + exp[V^*(y_j; w) \u2013 V^*(y_i; w)]}\\)"}, {"title": "3.2.4 The Policy Learner", "content": "The goal of policy learning is to search for optimal solutions in Markov Decision Processes (MDPs). We model the summarization problem as an episodic MDP, meaning that each action's reward is equal to zero if the state is not terminate. At each step, the agent can perform either of the two actions: add another sentence to the summary or terminate it. The immediate reward function R(s, a) assigns the reward if s is the terminate state. The reward in SumRecom is the learned expert's reward, V. In EMDS, a policy \u3160 defines the strategy to add sentences to the draft summary to build the summary for the user. SumRecom defines \u03c0 as the probability of choosing a summary of y among all summaries Y denoted as \u03c0(y). Therefore, the optimal policy, \u03c0*, is the function that finds the desired summary for a given input based on the user's feedback. The expected reward of performing proper policy w is defined as:\n\\(RRL(\\pi|x) = E_{y\\epsilon Y}R(y) = \\sum_{Y \\epsilon Y} \\pi(y)R(y)\\)\nwhere R(y) is the reward for selecting summary y in document cluster x. In our problem, the reward is the ranker approximated by the domain expert, V. Therefore, the accumulated reward to be maximized in our problem is equal to:\n\\(RRL (\\pi|x) = \\sum_{Y \\epsilon Y}\\pi(y)V(y)\\)\nThe goal of MDP is to find the optimal policy \u03c0* that has the highest expected reward:\n\\(\\pi^* = argmaxRRL(\\pi|x) = argmax \\sum_{Y \\epsilon Y} \\pi(y)V(y)\\)\nWe use the linear Temporal Difference (TD) algorithm to obtain \u03c0*."}, {"title": "4 Evaluation", "content": "In this section, we present the experimental setup for implementing and assessing our summarization model's performance. We discuss the datasets, give implementation details, and explain how system output was evaluated."}, {"title": "4.1 Datasets", "content": "We evaluated SumRecom using three commonly employed benchmark datasets from the Document Understanding Conferences (DUC)."}, {"title": "4.2 Evaluation Metric", "content": "We evaluate the quality of summaries using ROUGEN measure [8] defined as:\n\\(ROUGEN = \\frac{\\sum_{S\\epsilon \\{Reference Summaries\\}} \\sum_{gram_n \\epsilon S} Count_{match}(gram_n)}{\\sum_{S\\epsilon \\{Reference Summaries\\}} \\sum_{gram_n \\epsilon S} Count(gram_n)}\\)\nThe three variants of ROUGE (ROUGE-1, ROUGE-2, and ROUGE-L) are used. We used the limited length ROUGE recall-only evaluation (75 words) to compare DUC dataset to avoid being biased."}, {"title": "4.2.1 Results and Analysis", "content": "First", "including": "n\u2022 The impact of different features in approximating preference learning algorithms used in this paper", "units": "unigram, bigram, and sentence.\n\u2022 The role of the query budget in both concepts and summary preferences.\n\u2022 The quality of produced summaries.\n\u2022 A human study to evaluate SumRecom through users' lens.\n\u2022 The effect of different values for parameters.\n\u2022 An ablation study to evaluate different components of the proposed framework.\nWe have simulated users in all experiments except the one proposed for human evaluation.\nFeature Analysis. 3 Before evaluating the effect of concept preference in summarization, we require to explain the ground-truth concept ranker function (U) and the approximate function (U*). The ground-truth concept ranker function (U) indicates the importance of each concept. We define a predefined list of preference over concepts and the ground-truth concept ranker value for ten clusters to simulate the users' preferences. For estimating the approximate function (U*), we define a linear model U*(c) = WT(c) where & are the features. To this end, a set of features that their importance is validated in our previous work [14", "14": "."}]}