{"title": "Winning Big with Small Models: Knowledge Distillation vs. Self-Training for Reducing Hallucination in QA Agents", "authors": ["Ashley Lewis", "Michael White", "Jing Liu", "Toshiaki Koike-Akino", "Kieran Parsons", "Ye Wang"], "abstract": "The deployment of Large Language Models (LLMs) in customer support is constrained by hallucination-generating false information and the high cost of proprietary models. To address these challenges, we propose a retrieval-augmented question-answering (QA) pipeline and explore how to balance human input and automation. Using a dataset of questions about a Samsung Smart TV user manual, we demonstrate that synthetic data generated by LLMs outperforms crowdsourced data in reducing hallucination in finetuned models. We also compare self-training (fine-tuning models on their own outputs) and knowledge distillation (fine-tuning on stronger models' outputs, e.g., GPT-40), and find that self-training achieves comparable hallucination reduction. We conjecture that this surprising finding can be attributed to increased exposure bias issues in the knowledge distillation case and support this conjecture with post hoc analysis. We also improve robustness to unanswerable questions and retrieval failures with contextualized \"I don't know\" responses. These findings show that scalable, cost-efficient QA systems can be built using synthetic data and self-training with open-source models, reducing reliance on proprietary tools or costly human annotations.", "sections": [{"title": "Introduction", "content": "While many companies are eager to integrate Large Language Models (LLMs) into customer service and other applications, widespread deployment remains constrained by hallucination, or the generation of false or unsupported information, and the high financial and computational costs of using proprietary models. This issue is particularly critical in customer support, where unreliable responses can mislead users and erode trust.\nWe develop a cost-effective retrieval-augmented question-answering (QA) pipeline and address critical training data questions: what sources of data are most effective for finetuning open source models, and what preprocessing or filtering mechanisms best mitigate hallucination. To do so, we use a dataset from Nandy et al. (2021) comprising crowdsourced questions written by professional annotators about a Samsung Smart TV user manual (but notably lacking human-written responses). In this work, we address the following research questions:\nRQ1: What is the optimal balance between manual and automated methods for data processing and creation? We explore the trade-offs of using automatic and manual methods in two main situations: data processing and data creation.\nWe use Llama-3-8B-Instruct (hereafter Llama-3) (Dubey et al., 2024) to generate answers to the crowdsourced questions, followed by two cleaning methods: manual cleaning performed by the first author and automatic cleaning using LLMs. While many recent studies have shown LLM's ability to iteratively evaluate and refine text to reduce hallucination (Dhuliawala et al., 2024; Wang et al., 2024), these methods are often costly and pose data privacy risks when proprietary models are used at runtime. To address this, we compare the effort of manual cleaning with the effectiveness of closed-source (GPT-40) and open-source (Llama-3) models for data cleaning. We show that while GPT-40 significantly outperforms Llama-3 in cleaning quality, it is comparable to manual efforts, suggesting that manual input may not always be necessary.\nWe also explore a realistic scenario in which no training data is available. Perhaps surprisingly, we demonstrate that LLM-generated synthetic training data leads to lower hallucination rates than crowdsourced data, as measured by FactScore and human evaluation, possibly due to increased variability in human-written questions.\nRQ2: How does self-training compare to model distillation in terms of hallucination rates? We examine the benefits of synthetic data by comparing two training approaches: finetuning models on data generated by the same model (self-training with Llama-3) versus finetuning models on data generated by a stronger model (knowledge distillation using GPT-40). Lewis and White (2023) suggest that knowledge distillation reduces hallucination, but their study only tests on synthetic questions. Meanwhile, Zhang et al. (2024) and Lin et al. (2024) show that self-training can reduce hallucination, though without any human evaluation and with a train/test time mismatch in the case of Lin et al. (2024). To our knowledge, our work is the first apples-to-apples comparison of these two approaches. Surprisingly, we find that self-training of a small model and distillation of a large one achieve comparably low hallucination rates, as measured by FactScore (Min et al., 2023) and human evaluation, when the same data cleaning is used for both methods.\nTo explore this result, we analyze the potential role of exposure bias, which refers to the tendency of a model to perform better in contexts observed during training, leading to errors when faced with unfamiliar contexts during inference. We hypothesize that models trained on their own generated data benefit from greater familiarity with the training examples, compensating for the quality gap between the models. This suggests that self-training can serve as a resource-efficient alternative to model distillation in tasks where minimizing hallucination is critical.\nRQ3: How can retrieval failures and unanswerable questions be anticipated? The dataset includes questions scraped from community forums such as Amazon product QA sections, which are noisier, more diverse, and often unanswerable using the user manual. Such questions are prone to hallucination as the model relies on pretraining rather than the provided document. Since state-of-the-art retrieval models return n-best lists with imperfect accuracy (Gao et al., 2023), it is critical for QA systems to recognize retrieval failures and respond appropriately (e.g., I don't know the answer) while confirming the user's question was understood. While we do not focus on retrieval, we mitigate this issue by inserting negative examples during training, teaching models to provide contextualized \"I don't know\" responses, which also reduces hallucination rates.\nIn light of these questions, this paper makes the following key contributions, with a focus on customer support systems:\n\u2022 We find that manual and automatic data cleaning result in finetuned models with similar factual accuracy, but responses from models based on automatic cleaning are longer.\n\u2022 We demonstrate that LLM-generated synthetic training data can lead to models with lower hallucination rates than using crowdsourced data, as measured by FactScore and human evaluation.\n\u2022 We show that finetuning a model on its own generated answers (e.g., training Llama-3 on Llama-generated data) results in comparable hallucination mitigation to training it on GPT-40-generated answers, despite GPT-40 being a more generally capable model.\n\u2022 We explore exposure bias as a possible explanation for why the self-trained model performs so well. We hypothesize that models perform better when trained on low-perplexity (more familiar) examples. Our FactScore results and perplexity-based analysis provide empirical support for this hypothesis.\n\u2022 We provide a simple, scalable data perturbation strategy and synthesize contextualized I don't know responses to increase model robustness to unanswerable questions and retrieval failures."}, {"title": "Related Work", "content": "Recent studies suggest that finetuning on new, unfamiliar knowledge can lead to hallucination (Gekhman et al., 2024; Lin et al., 2024; Kang et al., 2024). For instance, Lin et al. (2024) propose training on self-generated data to reduce hallucination, but introduce a training-test mismatch where models use grounding documents during training but not testing, potentially causing hallucinations. We maintain consistent setups.\nLike Lin et al. (2024), Zhang et al. (2024) employ self-training to reduce hallucinations. Our approach differs in three ways: first, we use simple supervised finetuning (SFT) instead of techniques like reinforcement learning (RL) and direct preference optimization (DPO), which are promising avenues for future work. Second, we compare selftraining with knowledge distillation, investigating the value of synthetic data from a model's own outputs and from a more performant model. Third, we validate our results with human evaluation in addition to automatic metrics. Other works also focus on iterative self-refinement (Wang et al., 2024; Madaan et al., 2024), though do not specifically focus on the problem of hallucination.\nIn contrast, Lewis and White (2023) employ knowledge distillation to reduce hallucination, using ChatGPT to generate and clean documentgrounded training data. However, their approach is limited in two ways: they finetune a T5-large model (Raffel et al., 2020), which reduces hallucination over GPT-3.5 but limits robustness and fluency, and they evaluate only on synthetic data. Farquhar et al. (2024) detect hallucinations during inference using semantic entropy, which clusters generated outputs based on semantic equivalence and measures uncertainty at the level of meaning. While semantic entropy excels at runtime detection in open-domain settings, the entailmentbased clustering method is very expensive. By contrast, our approach reduces hallucinations at their source by improving training processes for RAG settings."}, {"title": "Data and Experimental Setup", "content": "3.1 Datasets\nThe primary dataset consists of 684 crowdsourced questions paired with retrieved passages from the manual (Nandy et al., 2021). We split the dataset into 534 training, 100 development, and 50 test questions (our \u201cregular test set\u201d). Dataset preprocessing details can be found in Appendix A. We focused on this dataset because many existing QA datasets either lack grounding documents or prioritize open-domain QA, which does not align with the controlled, retrieval-augmented QA setting we aimed to study. This approach also allowed us to conduct a deep-dive analysis into the trade-offs between self-training, knowledge distillation, and synthetic data generation in mitigating hallucinations within a well-defined context.\nAs mentioned, the dataset also contains a collection of 3,000 questions sourced from community forums. We create challenge sets by randomly selecting 100 development and 100 test questions from this set. These questions are noisier and less than half are answerable, which allows us to evaluate how well models handle particularly challenging cases. Examples from both types of questions can be found in Appendix B.\n3.2 Training Data\nRegular Training Data We use the pretrained Llama-3-8B-Instruct (Dubey et al., 2024) to generate answers for the 534 training questions. Three datasets are created: (1) a manually cleaned version where responses were reviewed and corrected by the first author, and (2)\u2013(3) automatically cleaned versions using GPT-40 and Llama-3-70B, respectively. This allows a systematic evaluation of the trade-offs between human effort and automated cleaning. As shown in Table 1, cleaning with Llama-3 was largely unsuccessful. Thus in the remaining experiments GPT-4o was used for the cleaning task. We anticipate that improvements in open-source models like Llama-3 may reduce reliance on proprietary alternatives in the future. Prompts for both data generation and cleaning can be found in Appendix C.\nSynthetic Data In addition to crowdsourced training questions, we generate fully synthetic QA data using LLMs. Specifically, we prompt Llama-3 and GPT-40 to generate new QA pairs based on passages from the Samsung Smart TV manual. To ensure that these datasets have comparable information coverage to the crowdsourced dataset and to prevent retrieval quality from being a confounding factor, we select passages systematically rather than randomly. We identify all 208 unique sections in the manual that are referenced in the crowdsourced training data. From these passages, we generate two synthetic QA pairs per passage, two from Llama-3 and two from GPT-40. This approach ensures that the synthetic datasets are no larger than the crowdsourced dataset and cover similar content while maintaining consistency in passage selection. In a real-world application, this limitation does not exist, as synthetic training data can be generated from any number of passages. Thus, coverage is not inherently a bottleneck when using synthetic data in practical settings.\n3.3 Baseline and Experimental Models\nTo evaluate the impact of data cleaning type and synthetic training data on hallucination reduction, we experiment with both pretrained models and finetuned models trained on different datasets.\nBaseline Models\n\u2022 Pretrained Llama-3-8B-Instruct (Llama-3): An open-source model that serves as a strong starting point for retrieval-augmented generation (RAG) without task-specific adaptation (Dubey et al., 2024). The model is run with few-shot prompting.\n\u2022 GPT-40: A state-of-the-art proprietary model, included as a benchmark to assess how well finetuned open-source models compare to a highly optimized general-purpose system (OpenAI et al., 2024). The model is run with few-shot prompting.\nFinetuned Models We finetune Llama-3 on different variations of training data to analyze the effects of data source, cleaning method, and exposure bias on hallucination rates. Specifically, we train models on the following datasets using the Zheng et al. (2024) finetuning framework and parameters:\n\u2022 Manually Cleaned Training Data: A dataset where the first author reviewed and corrected Llama-3-generated answers to the Nandy et al. (2021) 534 crowdsourced training questions.\n\u2022 Automatically Cleaned Training Data: A version of the training set where errors in Llama-3-generated answers were identified and repaired using GPT-40.\n\u2022 Synthetic Data (Llama vs. GPT): Two datasets where 416 QA pairs were generated by either Llama-3 or GPT-4o based on passages from the Samsung Smart TV manual. All synthetic data was cleaned using GPT-40.\n\u2022 Synth Llama+: Trained on the synthetic Llama data, and augmented with 100 negative examples (see section 4.3 for more details).\n3.4 Metrics for Evaluation\nWe evaluate model performance using two methods: FactScore (Min et al., 2023), an automated metric for factual accuracy, and human evaluation by trained annotators. These complementary approaches measure factual consistency and response quality.\na\nFactScore FactScore evaluates whether a model's response aligns with a reference document. It works by decomposing a response into sentences, breaking each sentence into discrete factual claims, and verifying their alignment with the reference text. FactScore measures the proportion of supported claims while penalizing hallucinated content. However, responses from GPT-40 and SynthGPT, which often use structured formatting (e.g., lists, topic headers), cause FactScore to produce fragmented or nonsensical claims, unfairly penalizing these models. To address this, we removed the sentence-splitting preprocessing and instead generated atomic facts directly from the full response.\nFactScore, which we computed using GPT-40-mini, has been shown to be a reliable proxy for factuality, correlating well with human judgments (Min et al., 2023). However, we find that it is unsuitable for evaluating I don't know responses. Thus, we applied FactScore only to the regular test set (mostly answerable questions), excluding the challenge set (many unanswerable questions). We also used it to evaluate human-written training questions for synthetic models, as they do not see these at training time and it provides a more robust evaluation. Further information in Appendix D.\nHuman Evaluation To obtain a more nuanced assessment of response quality, we conducted a human evaluation with three fluent English speaking, Linguistics PhD students (instructions in Appendix E), who annotated each model-generated response for the regular test set (50 items) and 50 items from the challenge set. They assigned to each response one of the categories listed in Table 2 (examples in Appendix F), which were determined by an author analysis of the dev set. Three-way agreement occurred between annotators 63.14% of the time and two-way agreement occurred 36.43% of the time. Krippendorff's Alpha was \u03b1 = 0.625, indicating substantial agreement.\nEach response was labeled independently by all three annotators. The final assigned label was determined by a majority vote. In the few cases where annotators provided three different labels, the response was assigned the most severe error based on the following predefined ranking: Hallucination > Non-Answer > Partial Answer > IDK - Bad > Disfluent > Other. The purpose of this ranking is to prioritize hallucination and content errors. For example, if a response is labeled as \u201cHallucination,\" \"Good,\u201d and \u201cPartial Answer,\u201d it is assigned the final label of \"Hallucination\" due to its higher severity in the ranking.\nBy combining automated and human evaluation, we ensure a comprehensive analysis of both quality and factual consistency in model-generated responses. The aggregated results can be found in Table 4 and the separate results on the regular and challenge test sets can be found in Appendix G."}, {"title": "Results and Analysis", "content": "4.1 Autocleaning vs. Manual Cleaning\nThe FactScore results on the test set (Table 1) and human evaluation results (Table 4) reveal that models finetuned on autocleaned data perform slightly better in terms of factual accuracy and response quality compared to manually cleaned data, though the gains are small. No models were significantly better than pretrained Llama-3.\nTable 3 shows that responses generated from the model trained on autocleaned data are consistently longer than those from manually cleaned data, suggesting that autocleaning prioritizes including as much information as possible from the retrieved passage, even when it is unnecessary to answer the question. This verbosity, while occasionally useful, does not inherently improve factuality.\nThe response quality of autocleaned and manually cleaned models is similar, as indicated by FactScore and human evaluation results. Both outperform a model trained on uncleaned data but fail to surpass the pretrained Llama-3 baseline. However, hallucination remains a persistent issue across all models, regardless of the cleaning method.\nOne reason for the lack of significant improvements between manual and autocleaned models may be the limited training data (only 534 examples), which likely reduces the relative impact of cleaning strategies. Furthermore, the absence of sufficient negative training examples, such as explicit \"I don't know\" responses, leaves models prone to over-generating information rather than admitting uncertainty\u2014an issue particularly evident in the challenge test set.\n4.2 Human vs. Synthetic Training Data\nA key question in this study is whether crowdsourced training data is necessary for finetuning QA models, or if synthetically generated data can achieve comparable or even superior performance. We compare models trained on crowdsourced answers against those trained on LLM-generated synthetic data (from Llama-3 and GPT-40), evaluating them on both the regular and challenge test sets.\nTable 1 and Table 4 indicate that models trained on synthetic data can outperform those trained on crowdsourced data in terms of factual accuracy and overall response quality. One possible explanation is that crowdsourced data tend to introduce variability and noise, whereas synthetic data is consistently aligned with the retrieved passages and the LLM's internal language patterns, making it easier for the model to learn structured answer generation.\nIn Table 5 we examine diversity using GEM metrics (Gehrmann et al., 2021) and find that crowdsourced questions, while shorter on average, have a larger vocabulary of distinct 1-, 2-, and 3-grams relative to the number of total tokens, suggesting greater diversity. We also calculate BERTScores (Zhang et al., 2020) for every pair of questions within each dataset and find that, on average, the scores for the synthetic data are higher, indicating that the questions are more semantically similar to each other than the questions in the crowdsourced dataset. We also calculate the perplexity of the questions for Llama-3 and find higher perplexity in the human questions, indicating that they are more unfamiliar to the model. While greater diversity can potentially be helpful in finetuning a model, evidently the less diverse and more expected synthetic questions are more consistently helpful in our experiments. Further analysis can be found in Appendix H."}, {"title": "Synth Llama+: Enhancing Synthetic Data for Hallucination Reduction", "content": "To encourage the model to abstain from answering when relevant information is unavailable, as is often the case in the challenge test set, we added negative training examples to the synthetic Llama data by duplicating 100 random training questions.\nThen, instead of generic \"I don't know\" responses, we constructed context-aware refusals by replacing the correct passage with a random one and prompting Llama-3 to generate an answer using these items. This ensured that the model could acknowledge the user's intent while signaling retrieval failure, as shown in the following example:\nQuestion: How do I select Dynamic mode?\nPassage: The compression of video content may cause picture distortions, especially in fast-moving pictures from sports programs and action movies. [...]\nGenerated Response: I'm sorry, I can't find any information about selecting Dynamic mode in the provided section of the user manual.\nUnlike generic refusals, this approach ensures that the model's response acknowledges the intent of the question, making it clear to users that their request was understood but that relevant information is unavailable. We select SynthLlama here because it provides the best balance of low cost and high performance, which is an important consideration for real-world applications.\nThese enhancements led to improvements in both FactScore and human evaluation metrics compared to the base SynthLlama model and comparable performance to GPT-40 on this task. With these improvements, SynthLlama+ achieved a significantly higher proportion of good responses in comparison to pretrained Llama in the human evaluation, as shown in Table 4."}, {"title": "Exposure Bias and Synthetic Data Performance", "content": "One of the key findings in our study is that self-trained models perform comparably to knowledgedistilled ones\u2014that is, models finetuned on synthetic data generated by the same model (e.g., Llama-3 trained on Llama-generated QA pairs) perform about as well as those trained on synthetic data from a more performant model (e.g., Llama-3 trained on GPT-generated QA pairs) when both synthetic datasets use data cleaning. This suggests that exposure bias may influence training stability and factual accuracy, as models appear to be more reliable when finetuned on data that aligns closely with their pretraining distribution. Exposure bias in language models refers to the mismatch between training and inference: during training, the model learns with gold context (\u201cteacher forcing\u201d), but at inference, it generates text based on its own prior predictions, potentially causing errors to accumulate and degrade output quality (Arora et al., 2022).\nTo further investigate this conjecture, we used the pretrained Llama-3 model to compute the perplexity of each QA response, conditioned on the passage. To quantify the relative familiarity of each synthetic example, we calculated the difference in perplexity between the GPT-generated and Llama-generated QA for each passage,\n$\\Delta PP = PP(q_G, a_G | c) \u2013 PP(q_L, a_L | c)$ (1)\nwhere ($q_G$, $a_G$) and ($q_L$, $a_L$) are the question-answer pairs generated by GPT-40 and Llama-3 for passage c, respectively, and $PP(q, a | c)$ represents the perplexity score of a given QA pair under the pretrained Llama-3 model.\nThis measure allows us to rank training examples based on their relative familiarity to the base Llama-3 model. Positive values ($\\Delta PP$ > 0) indicate that the GPT-generated QA pair is more perplexing (i.e., less familiar) to the model than the Llama-generated QA pair, whereas negative values ($\\Delta PP$ < 0) suggest the opposite.\nWe then sorted all passages by their perplexity difference ($\\Delta PP$) and constructed the Best and Worst 50:50 Blends as follows. See Figure 2 for a visual of this process using a toy example.\nBest Blend For each passage, we selected the QA pair where the generating model had a larger perplexity advantage relative to the other model. This means selecting the 50% of GPT-generated QA pairs where $\\Delta PP$ is smallest and the 50% of Llama-generated QA pairs where $\\Delta PP$ is largest.\nWorst Blend For each passage, we selected the QA pair where the generating model had a larger perplexity disadvantage relative to the other model. This means selecting the 50% of GPT-generated QA pairs where $\\Delta PP$ is largest and the 50% of Llama-generated QA pairs where $\\Delta PP$ is smallest.\nEach blend contained an equal mix (50% GPT-generated and 50% Llama-generated), ensuring a direct comparison of training effects when models are finetuned on their most versus least familiar examples relative to each other.\nResults and Analysis Table 6 shows the FactScore results for the regular training set questions. Because these manually-written questions are not used at training time for the synthetic models, they can be repurposed as a larger test set, allowing for significant differences to emerge. The results reveal no significant difference between synthetic GPT and synthetic Llama, suggesting comparable performance. Meanwhile, the Worst Blend model performs significantly worse than the Best Blend model, indicating that the perplexity of the training examples does play a role in the downstream model's propensity to hallucinate. Meanwhile, the Best Blend model has a higher score than both synthetic models, suggesting that perplexitybased selection could be a tool worth exploring further in mitigating hallucination for synthetic data."}, {"title": "Discussion", "content": "Our findings demonstrate that self-training and knowledge distillation can be comparably effective in reducing hallucination, while self-training is much less costly. Models trained on self-generated data consistently performed as well or better than those trained on GPT-generated data, supporting the hypothesis that exposure bias plays a key role in finetuning effectiveness. Additionally, our Best Blend vs. Worst Blend analysis revealed that using high-perplexity examples at training time led to increased hallucination, reinforcing the importance of training on familiar, low-perplexity data. Further improvements were observed with Synth Llama+, where incorporating simple, context-aware negative examples yielded higher factual accuracy, suggesting promising future directions for hallucination mitigation.\nWhile our experiments focus on a single domain, the underlying mechanisms behind exposure bias and synthetic data effectiveness are likely to generalize to other QA tasks. Applying this approach in domains such as medical or legal QA would provide a valuable test of its robustness and effectiveness in higher-stakes applications.\nFuture work should explore scaling synthetic data generation, refining data selection methods based on perplexity differences, and investigating iterative self-training approaches, where models continuously refine their own synthetic data over multiple training cycles. This could further enhance model alignment and factuality while reducing reliance on external supervision."}, {"title": "Conclusion", "content": "In this work, we explore the trade-offs between cost, manual effort, and performance in building a QA agent for customer service, with a focus on mitigating hallucination. We elucidate the components of this process that can be automated and what models are best for that automation. We find that models finetuned on synthetic datasets can outperform ones from crowdsourced datasets, and that self-training with data validation not only matches the performance of knowledge distillation but can rival the original model being distilled (GPT-40). Our findings suggest that using this approach, scalable and cost-effective QA systems can be rapidly developed for customer service applications, delivering performance comparable to or exceeding that of current state-of-the-art models."}, {"title": "Limitations", "content": "Despite these insights, our study has limitations. First, our test set size is relatively small, particularly for human evaluation, where only 50 challenge and 50 regular test items were labeled. We did not want to overwhelm our annotators with too large of a task and judged that this was the maximum we could require. This limits the statistical power of our findings, making it difficult to detect smaller but meaningful performance differences. Expanding the evaluation set and conducting a larger-scale human evaluation in future work could provide a clearer picture of the impact of different training strategies.\nSecond, measuring hallucination remains challenging. FactScore, while useful, is not a perfect proxy for factuality, and human judgments, though more reliable, are limited by annotator agreement and scale. More robust hallucination metrics, particularly those that better capture the subtle ways in which models generate misleading but plausible responses, would enhance future analyses."}, {"title": "Ethics", "content": "8.1 Data Usage and Privacy\nOur research utilizes synthetic data generated by large language models (LLMs) and publicly available and licensed datasets from user manuals for consumer electronics. All data used in this study is devoid of personally identifiable information (PII) and does not infringe upon individual privacy rights. The synthetic data generation process was carefully designed to ensure that no sensitive or identifiable information is included. Our institution's review board reviewed our human evaluation plans and ruled that it does not meet the federal definition of human subjects research requiring review. Our human evaluators were unpaid volunteer colleagues and were informed about how their annotations would be used.\n8.2 Use of Proprietary Models\nOur work leverages GPT-based models in several instances, including as comparison (baseline) models, for synthetic data generation, and in the automatic data cleaning pipeline. While GPT models are not fully reproducible due to their proprietary nature, their use in this work is limited to tasks where their high performance offers meaningful value. Specifically:\n\u2022 GPT is used as a baseline model to benchmark the performance of open-source systems.\n\u2022 GPT-generated synthetic data is provided alongside the Llama-generated data to enable future reproducibility of experiments.\n\u2022 GPT is employed for data cleaning because it demonstrates state-of-the-art performance for this specific task. The study shows that both manual and automated cleaning yield similar outcomes.\n\u2022 To address concerns about reproducibility, all synthetic datasets and cleaned data used in the study will be made publicly available. This ensures that future researchers can reproduce our results even if proprietary models like GPT are unavailable.\nNote also that GPT-40 was used as a writing assistant for this paper in a limited capacity (rephrasings, help with conciseness) and with some coding tasks during research.\n8.3 Potential Risks and Mitigation\nWhile our study focuses on reducing hallucinations and improving factual accuracy in QA systems, we acknowledge potential risks related to synthetic data, which may introduce subtle biases or inaccuracies. Because this domain is specific to a product user manual, we did not feel that this was a relevant issue and we did not see any problematic instances of such biases.\n8.4 Societal Impact\nOur research aims to enhance the accuracy and reliability of QA systems, particularly in retrieving and synthesizing information from structured documents like user manuals. This can improve accessibility and user experience. However, we are aware of the broader implications of deploying such systems in real-world settings, as we demonstrate in this study that these models are still capable of hallucination even in our best-performing settings.\n8.5 Transparency and Reproducibility\nWe are committed to transparency and reproducibility in our research. Despite the use of proprietary GPT-based models, our findings do not hinge on the unique capabilities of GPT. The use of GPT is supplementary and not central to the key contributions of this work. To ensure reproducibility, we will provide all synthetic datasets, cleaned data, and detailed descriptions of our experimental methodologies."}, {"title": "Data Preprocessing", "content": "The dataset used in this study required extensive preprocessing to align the Samsung Smart TV user manual with the accompanying QA pairs and to ensure the data was suitable for a retrieval-augmented QA framework. This process involved converting the manual into a structured format and addressing inconsistencies in the original QA dataset.\nA.1 Unused Components of the Provided Dataset\nThe dataset provided by Nandy et al. (2021) includes several components for QA tasks over electronic device manuals. While we relied heavily on their crowdsourced Samsung Smart TV QA dataset, other components were excluded due to specific limitations, outlined below:\n1. Pretraining Corpus of Product User Manuals\nThis corpus, designed for pretraining, was not used due to: (1) Formatting Issues: It contained significant noise, including garbled characters, mixed languages, and missing elements like images and titles, likely due to automated PDF-to-text conversion. (2) Irrelevance: Pretraining on this noisy data was unnecessary, as this study focused on fine-tuning QA systems and retrieval-augmented methods.\n2. Galaxy S10 User Manual and QA Dataset\nThe Galaxy S10 manual and its associated dataset of 50 crowdsourced questions were excluded because: (1) Subset Issues: The questions were a small subset of a larger, unreleased dataset, raising potential licensing concerns. (2) Scale: With only 50 questions, this dataset lacked the scale required for meaningful experimentation, especially compared to the Samsung Smart TV QA dataset.\nA.2 User Manual Preparation\nThe Samsung Smart TV manual, originally provided as a PDF, presented several challenges for direct use. The JSON format provided was inconsistent, likely due to automatic conversion processes, and the structure of the manual did not align well with the \"Section Hierarchy\" fields used in the QA dataset, which point to the part of the manual from which the passage is retrieved. Unfortunately, an initial search for a reliable PDF conversion tool yielded few satisfactory results. To address these issues, the first author undertook a semi-manual process to convert the manual into a structured JSON format.\nFirst, screenshots of the original manual's table of contents were taken to map its hierarchical structure. Using GPT-40, we generated a nested JSON representation that mirrored this hierarchy, with sections and subsections organized into dictionaries. The text within each section was carefully transcribed into corresponding fields, and images were replaced with placeholders (e.g., [image_X.png]) that referenced a separate folder containing labeled images. To get transcriptions, we first fed each section of the manual to GPT-40 and asked it to fill in the section of the new JSON file. This was a very iterative process, with the first author manually checking the transcriptions and updating as necessary. This approach ensured that the JSON file was both faithful to the manual's structure and practical for passage retrieval tasks. Manual adjustments were made throughout the process to correct formatting errors and inconsistencies, ensuring the final structure was robust and usable.\nA.3 Cleaning the Crowdsourced QA Dataset\nThe QA dataset included human-written questions linked to specific spans of text within the manual. However, the dataset required significant cleaning to align with the newly structured manual. Many questions contained incorrect \u201cSection Hierarchy\u201d fields, which were manually corrected to match the updated JSON structure of the manual.\nAdditionally, we expanded the retrieved passages associated with each question. Instead of limiting retrieval to short spans, we included entire sections from the manual, reflecting a more realistic retrieval scenario for QA systems. These adjustments not only improved the alignment between the questions and the manual but also made the dataset more suitable for the task of mitigating hallucinations."}, {"title": "Constructing the Challenge Dataset", "content": "Included in the Nandy et al. (2021) dataset are a collection of 3,000 real-world user questions sourced from community forums. The questions seem to primarily come from the Amazon product pages of various Samsung Smart TVs. While there is variety in these products (model, size, etc.), they all use the same software and general hardware described in the user manual. There are many questions in this collection that are not answerable by the user manual, however. While the answers from the product pages are included, they are not reliable as (1) there is no guarantee that they are correct, (2) could involve subjective opinions, (3) may not correspond to information available in the user manual, thus we are unable to match the responses to grounding passages. Because of this, we do not rely on the answers as a resource. According to the Nandy et al. (2021) paper, there are annotations for which of these questions are answerable using the manual, but it does not seem that these annotations were publicly available.\nFurther, these questions do not have corresponding retrieved passages, which are necessary for our experiments. However, because these questions are only used at test and validation time and because their usefulness stems from their unanswerability, we could rely on less-than-perfect means of finding corresponding passages. Thus we simply feed the entire user manual JSON to GPT-40 and ask it to identify the most relevant passage for each of the randomly selected 100 questions in the dev and test set (200 total). This proved to be the quickest and easiest way to find passages, but a more reliable and realistic method would have been to use a stateof-the-art retrieval model. In an analysis of the dev set, we found that only 26% of the questions are answerable."}, {"title": "Examples of Questions from the Dataset", "content": "The following are two examples of questions from the crowdsourced dataset:\n1. Question: How do I get better audio quality. What are the connections guidelines for it?\nRetrieved Document:\nFor better audio quality, it is a good idea to use an AV receiver.\nIf you connect an external audio device using an optical cable, the Sound Output setting is automatically changed to the connected device. However, to make this happen, you must turn on the external audio device before connecting the"}]}