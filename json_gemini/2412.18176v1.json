{"title": "Molar: Multimodal LLMs with Collaborative Filtering Alignment for Enhanced Sequential Recommendation", "authors": ["Yucong Luo", "Qitao Qin", "Hao Zhang", "Mingyue Cheng", "Ruiran Yan", "Kefan Wang", "Jie Ouyang"], "abstract": "Sequential recommendation (SR) systems have evolved significantly over the past decade, transitioning from traditional collaborative filtering to deep learning approaches and, more recently, to large language models (LLMs). While the adoption of LLMs has driven substantial advancements, these models inherently lack collaborative filtering information, relying primarily on textual content data neglecting other modalities and thus failing to achieve optimal recommendation performance. To address this limitation, we propose Molar, a Multimodal large language sequential recommendation framework that integrates multiple content modalities with ID information to capture collaborative signals effectively. Molar employs an MLLM to generate unified item representations from both textual and non-textual data, facilitating comprehensive multimodal modeling and enriching item embeddings. Additionally, it incorporates collaborative filtering signals through a post-alignment mechanism, which aligns user representations from content-based and ID-based models, ensuring precise personalization and robust performance. By seamlessly combining multimodal content with collaborative filtering insights, Molar captures both user interests and contextual semantics, leading to superior recommendation accuracy. Extensive experiments validate that Molar significantly outperforms traditional and LLM-based baselines, highlighting its strength in utilizing multimodal data and collaborative signals for sequential recommendation tasks. The source code is available 1.", "sections": [{"title": "1 Introduction", "content": "In the era of information overload, recommender systems have become essential tools for filtering information across various online applications, including e-commerce, advertising, and online video platforms (Resnick and Varian, 1997; Koren, 2008)."}, {"title": "2 Related Work", "content": "LLM-Based Recommendation. The success of LLMs such as GPT4 (OpenAI et al., 2024) and LLaMA (Grattafiori et al., 2024) has sparked extensive exploration of their application in recommendation systems. Firstly, LLMs are used to enhance user or item information, such as aligning semantic spaces for user and item profiling or generating training signals for cold-start items (Xi et al., 2024; Ren et al., 2024; Zhang et al., 2024b). Secondly, some studies convert recommendation data into a conversational format, leveraging LLMs' instruction-following capabilities to predict user behavior (Friedman et al., 2023; Bao et al., 2023; Zhang et al., 2023a). Lastly, LLMs are adapted for recommendation tasks by combining ID-based item embeddings with textual features for hybrid prompting or using them for multi-class classification and regression for rating prediction (Ning et al., 2024; Liao et al., 2023). Although these methods demonstrate the potential of LLMs, improvements over traditional recommendation models remain limited. Prior methods either overlook traditional ID-based models by focusing only on text or introduce ID modalities too early, reducing the effectiveness of collaborative filtering signals during LLM training. Unlike these approaches, we propose a post-alignment mechanism to fuse ID-based collaborative information later in the process, preserving LLMs' world knowledge while retaining essential collaborative information.\nMultimodal Large Language Models. Recent advancements (Peng et al., 2023; Zhang et al., 2024c; Yin et al., 2024) in multimodal pre-training have significantly improved task performance but at the cost of increased computational demands. To address this, researchers are leveraging pre-trained unimodal models, particularly large language models (LLMs) (Kasneci et al., 2023), to develop Multimodal Large Language Models (MLLMs) that integrate language with other modalities. The pri-"}, {"title": "3 Methods", "content": "3.1 Problem Formulation\nWe tackle the task of sequential recommendation, where the goal is to predict the next item $I_{n+1}$ that a user $u \\in U$ is likely to interact with, given their historical interaction sequence $H_u = \\{I_1, I_2, ..., I_n\\}$ arranged in chronological order. Each item $I_i \\in I$ comes with multimodal information, such as titles, textual descriptions, and images. Our approach leverages this multimodal information to improve the prediction accuracy of the next interaction.\n3.2 Molar Overview\nTraditional recommendation systems based on Large Language Models (LLMs) often suffer from inefficiencies when handling extensive user histories, as transforming these histories into lengthy token sequences results in high computational costs and slower inference speeds. To address these challenges, we propose Molar, a decoupled framework that separates the modeling of items and users for more efficient processing. This separation allows tailored-modeling strategies for each component, improving computational efficiency and recommendation accuracy. Our framework is composed of two key modules: the Multimodal Item Representation Model (MIRM) and the Dynamic User Embedding Generator (DUEG). MIRM is designed to compress multimodal features into compact embeddings, mitigating the computational burden of lengthy token sequences. DUEG then utilizes these embeddings to build user representations that capture dynamic user preferences. Together, these modules enable effective multimodal feature modeling and user preference prediction, setting the foundation for robust sequential recommendation.\n3.3 Multimodal Item Representation Model.\nTo effectively extract and encode item features, we introduce the Multimodal Item Representation Model (MIRM), denoted as $f_1$. This encoder leverages a multimodal large language model (MLLM) to combine textual descriptions and images into a unified embedding representation. Although MLLMs excel in understanding text and images, their direct application to feature extraction remains limited. To address this, we append a special identifier, [Cur_Item], to the end of each item's description, guiding the model to focus on extract-ing relevant features.\nThe process begins by merging an item's textual and image attributes into a unified description $L$, augmented with a prompt to enhance model comprehension. This description is tokenized and processed by the MLLM, with [Cur_Item]"}, {"title": "3.4 Dynamic User Embedding Generator", "content": "Building on the item embeddings generated by MIRM, we design the Dynamic User Embedding Generator (DUEG), denoted as $f_u$. This module constructs dynamic user representations based on their historical interactions, effectively capturing evolving preferences. Unlike MIRM, DUEG simplifies the structure by removing the word embedding layer from the MLLM, retaining the pretrained parameters for efficient embedding processing.\nGiven a user's historical interaction sequence $H_u = \\{I_1, I_2,..., I_n\\}$, MIRM transforms each item $I$ into an embedding $emb_i$. These embeddings are then processed by DUEG, which incorporates a special [User] token to represent the user's dynamic preferences. This approach enables DUEG to predict the next likely item $I_{n+1}$ based on past interactions, formalized as:\n$E_u = f_u(emb_1, emb_2, ..., emb_n)$, (2)\nwhere $E_u$ is the user embedding.\nTo optimize both MIRM and DUEG, we introduce two loss functions:\nPoint-wise Recommendation Loss. A binary cross-entropy loss distinguishes between positive and negative samples, encouraging accurate next-item predictions:\n$L_{bce} = - (y \\cdot log(x) + (1 - y) \\cdot log(1 -x))$, (3)\nwhere y is the label vector, and x represents predicted logits for positive and negative samples. Details of this loss can be found in the Appendix B.\nAlignment Loss. To bridge content-based and ID-based representations, we introduce a post-alignment mechanism that uses a contrastive learning objective for both embeddings after DUEG processes multimodal content. This prevents premature integration of collaborative filtering into the LLM, ensuring essential collaborative information is preserved. The contrastive learning objective is defined as follows:\n$L_{align} = \\frac{1}{|U|} \\sum_{u=1}^{U} [log \\frac{exp(s(E_{id}^u, E_{con}^u)/\\tau)}{\\sum_{j \\in K} exp(s(E_{id}^u, E_{con}^j)/\\tau)} + log \\frac{exp(s(E_{con}^u, E_{id}^u)/\\tau)}{\\sum_{j \\in K} exp(s(E_{con}^u, E_{id}^j)/\\tau)}]$, (4)\nwhere $\\tau$ is the temperature parameter, K is the set of comparative instances containing one positive and K-1 negative examples. s(,) denotes the cosine similarity function, and $E_{id}$ and $E_{con}$ are the ID-based user embeddings from a traditional"}, {"title": "4 Experiments", "content": "In this section, we evaluate our proposed framework, Molar, on three real-world datasets and compare it against several baselines, including traditional sequential recommender models and state-of-the-art LLM-based methods. To assess the effectiveness of Molar, we conduct a comprehensive analysis addressing four research questions. Additionally, we investigate the impact of different DUEGs and the various input data modalities on the results.Furthermore, we perform ablation studies to investigate the impact of fine-tuning strategies and post-alignment, as well as evaluate the performance differences between our LLM-based user modeling approach and alternative user representation methods. The following research questions are explored:\n\u2022 RQ1: How does Molar perform compared with traditional sequential recommender models and LLM-based methods?\n\u2022 RQ2: What are the differences in performance between the LLM DUEG and alternative DUEGs for user representation?\n\u2022 RQ3: How do different data modalities impact the performance of Molar?\n\u2022 RQ4: How does the post-alignment model affect the performance of Molar?\n\u2022 RQ5: How does fine-tuning the MIRM combined with post-alignment training influence the overall performance of Molar?\n4.1 Experimental Settings\n4.1.1 Datasets and Evaluation Metrics\n\u2022 Amazon 2: Collected from the Amazon cloth online shopping platform.\n\u2022 PixelRec 3: An image dataset for recommender systems with raw pixels and text."}, {"title": "4.2 Performance Comparison (RQ1)", "content": "In this section, we compare Molar against traditional, content-based, and Ilm-based baselines, taking into metrics of both NDCG and Recall on PixelRec, MovieLens, and Amazon datasets, to showcase the effectiveness and robustness of Molar.\nBaselines. FPMC (Rendle et al., 2010), GRU4Rec (Tan et al., 2016), and SASRec (Kang and McAuley, 2018) are traditional sequential recommendation models based on Markov Chains, RNN, and attention mechanisms, respectively. DuoRec (Qiu et al., 2022) employs contrastive learning to extract discriminative information for"}, {"title": "4.3 Impact of DUEG (RQ2)", "content": "We conducted experiments to evaluate various representation methods for DUEGs, including FPMC, SASRec, GRU4Rec, and our proposed LLM Qwen2vl backbone as the DUEG.\nAs shown in Figure 3, the results indicate that Molar, using an LLM backbone as the DUEG, out-"}, {"title": "4.4 Impact of Input Data Modality (RQ3)", "content": "To gain a thorough understanding of how various data modalities influence the performance of Molar, particularly the contribution of multimodal fusion and the integration of complementary knowledge from various modalities, we conducted an in-depth analysis on PixelRec, showing in Table 3.\nOur findings demonstrate that multimodal fusion enhances recommendation performance. The fusion of multiple modalities significantly enhances recommendation performance. A comparative analysis of Molar on three input types reveals that the combined text with image input yields the best results. This can be attributed to the unique knowledge contributed by each modality, which cannot be captured by the others. MLLM effectively in-"}, {"title": "4.5 Impact of Post-Alignment Models (RQ4)", "content": "In the process of post-alignment contrastive learning, integrating ID information results in varying degrees of improvement across different traditional sequential recommendation models. To verify the performance impact brought by different traditional sequential recommendation models, we conducted the experiments shown in Table 4.\nThe experimental results reveal a clear trend: stronger sequential recommendation models like DuoRec better support post-alignment contrastive learning, enhancing the integration of ID-based collaborative filtering signals into LLMs. This integration significantly boosts recommendation accuracy, coverage, and performance. DuoRec's robust architecture captures richer user-item interaction patterns, enabling the LLM to leverage nuanced ID information for top NDCG and Recall scores. These findings highlight the importance of selecting powerful sequential models for contrastive learning, as they refine the process and ensure coherent ID integration, unlocking the full potential of collaborative filtering for diverse scenarios."}, {"title": "4.6 Ablation Study (RQ5)", "content": "To analyze the contributions of components in the proposed Molar method, particularly the fine-"}, {"title": "5 Conclusion", "content": "In this paper, we introduced Molar, a novel framework for sequential recommendation that bridges the gap between collaborative filtering and multimodal content modeling using large language models (LLMs). While traditional LLM-based approaches excel in semantic understanding, they lack the ability to incorporate collaborative filtering signals, limiting their recommendation performance. To overcome this limitation, Molar integrates multimodal data (textual and non-textual) with ID-based collaborative signals, leveraging an MLLM to generate unified item representations and a post-alignment mechanism to align user embeddings effectively. By combining the strengths of multimodal content modeling and collaborative filtering, Molar captures both user interests and contextual semantics, enabling precise and personalized recommendations. Extensive experimental results demonstrate that Molar consistently outperforms traditional methods and state-of-the-art LLM-based baselines, validating its ability to fully exploit multimodal data and collaborative signals for sequential recommendation tasks.\nLimitation. While Molar effectively integrates multimodal large language models (MLLMs) into sequential recommendation tasks, several limitations remain. First, the framework requires multi-"}, {"title": "A Impact of Different MLLM Backbone", "content": "To evaluate the impact of different MLLM backbones on the performance of Molar, we conduct comparative experiments using MLLMs of various backbones and sizes. Due to computational constraints, we are unable to fine-tune models with 7B parameters or larger in full, so we employ LoRA training for models with 7B parameters and above.\nAs shown in Table 5, for models of the same size, performance variations across different backbones are observed, with Qwen2vl achieving the best results. This suggests that, beyond model size, the choice of backbone plays a crucial role in determining the quality of the recommendations. Interestingly, as the model size increases, there is a consistent improvement in recommendation performance, highlighting the advantages of scaling up model capacity. Even when fine-tuned with LoRA, the 7B Qwen2vl model consistently outperforms the 2B counterpart, indicating that the larger model not only benefits from increased parameters but also capitalizes on the specific architectural strengths of Qwen2vl. These findings suggest that, while model size is an important factor, selecting an appropriate backbone could be equally crucial in optimizing performance, particularly when computational resources are limited."}, {"title": "B Detailed Point-wise Recommendation Loss.", "content": "To enhance the model's accuracy in predicting the next item, we employ a binary cross-entropy (BCE) loss function. In our training process, each positive sample is paired with a negative sample using a 1:1 negative sampling strategy. The target item embedding calculated from MIRM consists of a positive item (pos) and a negative item (neg). For each pair, we define label vector $y = [1,0]$ and generate predicted logits $x = [x_{pos}, x_{neg}]$. The BCE loss is calculated as:\n$L_{bce} = - (y \\cdot log(x) + (1 - y) \\cdot log(1 - x))$, (6)\nMinimizing this loss encourages the model to assign higher probabilities to positive samples and lower probabilities to negative samples, thereby improving its ability to distinguish relevant items for accurate next-item predictions."}]}