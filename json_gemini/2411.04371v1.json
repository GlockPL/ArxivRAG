{"title": "ComFairGNN: Community Fair Graph Neural Network", "authors": ["Yonas Sium", "Qi Li"], "abstract": "Graph Neural Networks (GNNs) have become the leading approach\nfor addressing graph analytical problems in various real-world sce-\nnarios. However, GNNs may produce biased predictions against\ncertain demographic subgroups due to node attributes and neigh-\nbors surrounding a node. Most current research on GNN fairness\nfocuses predominantly on debiasing GNNs using oversimplified\nfairness evaluation metrics, which can give a misleading impression\nof fairness. Understanding the potential evaluation paradoxes due to\nthe complicated nature of the graph structure is crucial for develop-\ning effective GNN debiasing mechanisms. In this paper, we examine\nthe effectiveness of current GNN debiasing methods in terms of\nunfairness evaluation. Specifically, we introduce a community-level\nstrategy to measure bias in GNNs and evaluate debiasing methods at\nthis level. Further, We introduce ComFairGNN, a novel framework\ndesigned to mitigate community-level bias in GNNs. Our approach\nemploys a learnable coreset-based debiasing function that addresses\nbias arising from diverse local neighborhood distributions during\nGNNs neighborhood aggregation. Comprehensive evaluations on\nthree benchmark datasets demonstrate our model's effectiveness\nin both accuracy and fairness metrics.", "sections": [{"title": "1\nINTRODUCTION", "content": "In today's interconnected world, graph learning supports various\nreal-world applications, such as social networks, recommender\nsystems, and knowledge graphs [4, 27]. Graph Neural Networks\n(GNNs) are powerful for graph representation learning and are\nused in tasks like node classification and link prediction by ag-\ngregating neighboring node information. However, GNNs often\noverlook fairness, leading to biased decisions due to structural bias\nand attribute bias like gender, race, and political ideology. These\nbias can cause ethical dilemmas in critical contexts, such as job\ncandidate evaluations, where a candidate might be favored due to\nshared ethnic background or mutual acquaintances. To tackle the\noutlined issue, multiple methods were suggested to evaluate and\nmitigate the fairness of node representation learning on graphs.\nMost of these methods aim to learn node representations that can\nyield statistically fair predictions across subgroups defined based\non sensitive attributes [17, 22, 30]. Choosing the right metric to\nevaluate bias in graphs inherently depends on the specific task at\nhand. The existing methods employed graph-level metrics orig-\ninally designed for other purposes to address fairness concerns.\nThe primary difficulty in addressing fairness within graphs arises\nfrom the frequent correlation between the graph's topology and\nthe sensitive attribute we aim to disregard. However, due to the\ncomplicated nature of graph structures, conducting graph-level\nfairness evaluations and comparisons of different methods is not\nas straightforward as commonly reported in the existing methods.\nRecently works [11, 21, 35, 40] point out that the reported scores\nin prior works might not reflect the true performance in the real-\nworld application because of various evaluation issues and the\nnature of the dataset. For example, Simpson's paradox [3], high-\nlights the context of the graduate admissions bias at UC Berkeley\nin 1973. Historically, it was believed that 44% of the 8,442 male\napplicants were admitted, compared to 35% of the 4,342 female\napplicants, indicating a gender bias. However, a closer look reveals\nthat four out of the six departments had a higher acceptance rate\nfor women, while two departments had a higher acceptance rate for\nmen. A recent example of Simpson's paradox during the COVID-\n19 era illustrates its implications in health policy. In 2020, initial\ndata showed a higher COVID-19 case fatality rate in Italy than in\nChina. However, this analysis was confounded by age distribution\ndifferences. When analyzed by age groups, the case fatality rate\nwas actually higher in China than in Italy within each age group\n[35]. The aforementioned works are not focused on fairness for\nmachine learning on graphs.\nThe majority of real-world graphs are polarized into commu-\nnities due to some sensitive features such as age, gender, race,\nideology, and socio-economic status, as shown in figure 1. A com-\nmunity is a group of nodes that are highly interconnected within\nthe group and have fewer connections to nodes outside the group.\nFor instance, research indicates that the dynamics of interaction\namong communities within United States Twitter networks are\nsignificantly shaped by their political allegiances [13]. Within the\ncountry, the flow of communication between communities in the\nNortheast and West Coast is largely driven by Democratic sup-\nporters, contrasting with the influence of Republican supporters.\nConversely, in Texas, Florida, and Iowa, Republican supporters tend\nto dominate the network flow. This suggests a geographical alterna-\ntion bubble in the dominance of network flows between Democrat\nand Republican communities. Consider a scenario where social me-\ndia users are categorized into those who like basketball and those\nwho like football. The demographics of those subgroups may vary\nacross different regions despite the existence of some individuals\nwho participate in both sports.\nIn this paper, we investigate the important problem of com-\nmunity fairness in graph neural networks in the context of node\nclassification. On the one hand, simply employing node attribute\nand their structure debiasing function may result in misleading\nfairness evaluation and loss of information [39]. Communities are\na manifestation of the local neighborhood structures, which are\nfundamentally different from a sensitive node attribute [29]. Par-\nticularly, in GNNs, each node learns by aggregation of informa-\ntion from the neighboring nodes, thus identical label nodes with\nvarying neighborhood distribution as illustrated in figure 1, giv-\ning rise to structural bias in the learning process. Moreover, the\nbias may be further amplified by multi-layered recursive neighbor-\nhood aggregation in GNNs. Although current debiasing methods\nprimarily address attribute-based node fairness, they fail to tackle\nthe fundamental bias inherent in GNN neighborhood aggregation.\nConsequently, these approaches are unable to effectively mitigate\nlocal community-level biases arising from disparate neighborhood\nstructures.\nTo address community fairness, we introduce the concept of local\nstructural fairness at the community level for all nodes in the graph.\nThis approach recognizes that the broader structural bias affecting\nnodes within a community emerges from the GNN's aggregation\nprocess, which is influenced by the diverse distribution of local\nneighborhoods. We present ComFairGNN, a novel community-\nfair Graph Neural Network compatible with any neighborhood\naggregation-based GNN architecture. Specifically, the coreset-based\ndebiasing function targets the neighborhood aggregation of GNN\noperation, aiming to bring identical label nodes closer in the em-\nbedding space. This approach mitigates structural disparities across\ncommunities, leading to fairer representations for nodes with iden-\ntical labels.\nIn summary, our paper makes three key contributions: (1) We\nmeasure the problem of fairness and potential evaluation paradoxes\nat the community level using different fairness evaluation metrics\nin GNNs. (2) We propose a novel community fairness for GNNS\nnamed ComFairGNN that can balance the structural bias for iden-\ntical label nodes in different communities. ComFairGNN works\nwith any neighborhood aggregation-based GNNs. (3) Comprehen-\nsive empirical evaluations validate our method's effectiveness in\nenhancing both fairness and accuracy."}, {"title": "2\nPRELIMINARIES", "content": "In this section, we introduce the structural-based clustering setting\nto assess our research question: to what extent are the existing\ngraph fairness evaluation metrics prevalent in fair decisions at the\ncommunity level and prevent the corresponding types of fairness\nparadoxes?\nLet's first introduce the notations. A graph $G = (V, E)$ consists\nof n nodes V and edges $E \\subset V \\times V$. The node-wise feature matrix\n$X \\in \\mathbb{R}^{n\\times k}$ has k dimensions for raw node features, with each row\n$x_i$ representing the feature vector for the i-th node. The binary\nadjacency matrix is $A \\in \\{0, 1\\}^{n\\times n}$, and the learned node represen-\ntations are captured in $H \\in \\mathbb{R}^{n\\times d}$, where d is the latent dimension\nsize and $h_i$ is the representation for the i-th node. The sensitive\nattribute $s \\in \\{0, 1\\}^n$ classifies nodes into demographic groups, with\nedges $e_{uv}$ being intragroup if $s_u = s_v$ and intergroup otherwise. The\nfunction $N(u)$ returns the set of neighbors for a node u, providing\na structural node embedding. Clusters based on this embedding are\ndenoted as C, with $C_i \\subset V$ for nodes in the i-th cluster and c as the\ncluster count. Each $C_i$ represents a community and f measures the\nstatistical notion of fairness across different communities in the\ngraph."}, {"title": "2.1\nGraph Neural Network", "content": "Graph Neural Networks (GNNs) utilize a layered architecture for\nneighborhood aggregation, where nodes progressively integrate\ninformation from their immediate neighbors across multiple itera-\ntions. For the l-th layer, the representation of a node v, denoted as\n$h_v^l \\in \\mathbb{R}^{d_l}$, is formulated as follows:\n$h_v^l = \\sigma \\Big(AGGR\\big( \\{h_u^{l-1} | u \\in \\mathcal{N}_v\\} ; w^l \\big)\\Big)$ (1)\nwhere $d_l$ is the dimension of node representations in the l-th\nlayer, $AGGR(\\cdot)$ denotes an aggregation function such as mean-\npooling [19] or self-attention [34], $\\sigma$ is an activation function, $\\mathcal{N}_v$\ndenotes the set of neighbors of v, and $w^l$ denotes the learnable\nparameters in layer 1. The initial layer (layer 0) utilizes the raw\ninput features as node representations, expressed mathematically\nas $h_v^0 = x_v$, where $x_v$ denotes the input attribute vector for node v."}, {"title": "2.2 Node Clustering on Structural Embedding", "content": "By integrating node2vec [12] embeddings with k-means clustering\n[26], we can effectively partition the graph into communities based\non the structural similarities captured by the embeddings.\nGraph embedding using node2vec is a mapping that assigns each\nnode in a graph to a vector in a d-dimensional space, denoted as\n$f : U \\rightarrow \\mathbb{R}^d$, where d is a hyperparameter representing the number\nof dimensions in the vector space. For each node $u \\in U$ in a graph\nG, multiple random walks are performed, as specified by the hyper-\nparameter walk_num. This process generates a list of sequences,\neach comprising node IDs from individual random walks. A single\nrandom walk follows the standard approach in unweighted graphs,\nwhere at each step, the next node is selected uniformly at random\nfrom the current node's neighbors. The hyperparameters walk_num\nand walk_len control the number of walks and the length of each\nwalk, respectively. Next, node2vec uses the generated sequences to\ntrain a neural network and learn embedding vectors. Let us define\na network neighborhood $N_S(u)$ as the set of nodes preceding and\nsucceeding node u in the generated walk sequences. The objective\nfunction is defined as:\n$\\arg \\underset{f} {\\max} \\underset{u\\in U} {\\prod} \\underset{u' \\in N_s(u)} {P(u'|f(u))}$ (2)\n$P(u' | f(u))$ is the conditional probability represented as softmax.\nOnce the node embeddings are learned using node2vec, the\nnext step involves clustering these embeddings to identify groups\nof similar nodes. Our primary focus for clustering is on k-means\nclustering, selected for its efficient linear runtime. k-means aims to\nminimize the total sum of squared errors, also known as inertia or\nwithin-cluster sum of squares [31]. This objective can be formally\nexpressed as:\n$\\arg \\underset{C} {\\min}  \\sum_{i=1}^{k} \\sum_{x \\in C_i} DIST(x, \\mu_i)$. (3)"}, {"title": "2.3\nGroup Fairness Metrics on Graphs", "content": "We specifically target group fairness scenarios where both the node\nlabel and the sensitive attribute are binary. Group fairness is the\nmost prevalent and extensively studied form of fairness. In the\ncontext of group fairness, we consider a model or algorithm to be\nfair when its outputs exhibit no bias against any demographic group\nof sensitive attributes. In this work, we primarily focus on assessing\nthe evaluation of existing graph fairness methods utilizing the two\nwidely used metrics of group fairness, i.e., demographic parity [9]\nand equal opportunity [14].\nLet $\u0177$ denote the prediction output of the classifier $f(\\cdot)$. $y \\in \\{0, 1\\}$\nand $s \\in \\{0, 1\\}$ represent the ground truth and the sensitive attribute,\nrespectively.\nDemographic Parity: Demographic parity, also referred to as sta-\ntistical parity, necessitates that the prediction \u0177 is independent\nof the sensitive attribute s, symbolized as $\u0177 \\bot s$. Formally, demo-\ngraphic parity aims to have:\n$P(\\hat{y} | s = 0) = P(\\hat{y} | s = 1)$ (4)\nEqual Opportunity: Equal opportunity involves an identical true\npositive rate across all demographic groups. This concept can be\nexpressed as:\n$P(\\hat{y} = 1 | y = 1, s = 0) = P(\\hat{y} = 1 | y = 1, s = 1)$ (5)\nDuring experiments, demographic parity and equal opportunity\nare utilized to assess fairness performance as below:\n$\\Delta S P = | P(\\hat{y} = 1| s = 0) - P(\\hat{y} = 1 | s = 1) |$ (6)\n$\\Delta E O = | P(\\hat{y} = 1| y = 1, s = 0) - P(\\hat{y} = 1 | y = 1, s = 1) |$ (7)"}, {"title": "3 PROPOSED METHOD", "content": "The overall framework of the ComFairGNN is illustrated in Figure\n2. We first design a strategy of communities using a clustering\nalgorithm based on structural contrast in Figure 2(c) to evaluate\nthe effectiveness of the existing fairness methods and our proposed\nmethod at the community level. To facilitate the debiasing function,\nwe split nodes in each community into two subgroups based on their\nsensitive feature and sample coreset nodes from each community\nbased on their neighborhood homophily distribution ratio Figure\n2(c). The coreset nodes contrast each other based on their output\nsimilarity to mitigate structural bias at the community level Figure\n2(d). More specifically, the sampled coreset based debiasing function\naims to balance the neighborhood distribution of nodes in different\ncommunities and remove bias by improving the output similarity\nof the nodes that have the same label. Thus, by trying to maximize\nthe similarity of nodes in the coreset with the same label, the GNN\nis able to debias the neighborhood aggregation and balance the\ntwo groups. The GNN is optimized by the task loss and the coreset\nfairness loss Figure 2(e)."}, {"title": "3.1 Community level Structural Contrast", "content": "To debias nodes in a community with varying label neighborhood\ndistribution, we propose a debiasing function based on the core-\nset sample nodes c, which are selected from different communities.\nCoreset selection methods aim to identify crucial subsets of training\nexamples by employing specific heuristic criteria [33]. These sub-\nsets are considered essential for effectively training the model. Our\nstudy proposes a new method of graph coreset selection method\nto debias GNNs, addressing the local structural bias in their aggre-\ngation process. The coreset nodes are selected equally from each\ncommunity for both groups to learn the debiasing function by con-\ntrasting between two groups of nodes, namely, subgroups So and\nS1. Note that GNNs neighborhood aggregation in Eq. 1, can only\naccess its one-hop local contexts in each layer. The majority of\nreal-world networks of nodes in a community tend to be from the\nsame subgroup influenced by the sensitive attribute. Thus, it is a\nnatural choice to select the coreset sample nodes of both groups\nfrom each community to balance the structural imbalance between\ndifferent communities.\n$c = \\bigcup_{k=1}^{K} C_{0}^{k_s} \\bigcup_{k=1}^{K} C_{1}^{k_s}, V$ (8)\nHere c indicates the coreset sample nodes for all $k \\in \\{1, ..., K\\}$.\nK represents the total number of communities, and $C_{0}^{k_s}$, and $C_{1}^{k_s}$\nrepresent selected sample nodes from subgroup $S_0$ and $S_1$ in com-\nmunity k respectively.\nTo minimize the structural bias between the two groups in differ-\nent communities, we contrast the output similarity of nodes with\nthe same label in the coreset. This tends to maximize the similarity\nof nodes of the same label in different communities. This strategy\nenables GNNs to implicitly eliminate the structural bias between\ndifferent communities."}, {"title": "3.2 Coreset Selection for GNNs Debiasing", "content": "To fundamentally eliminate the structural bias of GNNs neighbor-\nhood aggregation, which is the key operation in GNNs, we propose\na neighborhood homophily distribution ratio to select the sample\ncoreset from different communities.\nNode neighborhood homophily distribution ratio: The ho-\nmophily ratio in graphs is typically defined based on the similarity\nbetween connected node pairs. In this context, two nodes are consid-\nered similar if they share the same node label. The formal definition\nof the homophily ratio is derived from this intuition, as follows\n[25].\nDefinition 1 (Node Homophily Ratio). For a graph $G = (V, E)$\nwith node label vector y, we define the edge homophily ratio as the\nproportion of edges linking nodes sharing identical labels. The formal\ndefinition is as follows:\n$h(G, \\{y_i; i \\in V\\}) = \\frac{1}{|V|}  \\sum_{i} \\frac{1}{\\mid{\\mathcal{N}_i}\\mid}\\sum_{j \\in \\mathcal{N}_i}f(y_i = y_k),$ (9)\nwhere $|n|$ is the number of neighboring edges of node i in the graph\nand $f(\\cdot)$ is the indicator function.\nA node in a graph is generally classified as highly homophilous\nwhen its edge homophily ratio $h(\\cdot)$ is high, typically falling within\nthe range 0.5 \u2264 $h(\\cdot)$ \u2264 1, assuming an appropriate label context.\nConversely, a node is considered heterophilous when its edge ho-\nmophily ratio is low. Here, $h(\\cdot)$ denotes the edge homophily ratio\nas previously defined.\nAs shown in figure 3, the neighborhood distribution ratio for\nidentical label nodes in pokec-z dataset varies largely. If we consider\nthat all nodes that share the same label have similar initial node\nfeatures, then cleary, after GNN 1-hop neighborhood aggregation,\nthe identical label nodes will have different output embedding. This\ninability to differentiate the embedding space of identical nodes in\ndifferent communities of the graph can potentially lead the GNN to\nbe biased towards certain subgroups. Considering this, the debiasing\ncoreset nodes are sampled based on their neighborhood distribution\nfrom all communities of the graph.\nFairness-aware Graph Coreset: aims to identify a represen-\ntative subset of training nodes $V_w \\subset V_{train} = [n_t]$, where $|V_w| \\leq\nc < n_t$, along with an associated set of sample weights. This se-\nlection is designed to approximate the training loss $L(\\theta)$ across\nall $\\theta \\in \\Theta$, while maintaining fairness considerations. The graph\ncoreset selection problem is formulated as:\n$\\min_{w \\in W} \\underset{\\theta \\in \\Theta} {\\max} | \\sum_{i \\in [n_t]} w_i l([f_\\theta (A,X)]_i, y_i) - L(\\theta) | \\hspace{1 mm} (10)$\nwhich minimizes the worst-case error for all $\\theta$. Both the graph\nadjacency matrix A and node feature matrix X are still needed to\ncompute the full-graph embeddings $f_\\theta (A, X)$ in the coreset loss.\nSince the goal of the fairness-aware graph, coreset is to debias the\nneighborhood aggregation of GNNs at the community level, we\nconsider the coreset samples of both groups of $S_0$ and $S_1$ from all\ncommunities of the graph."}, {"title": "3.3 Training Objective and Constraints:", "content": "We focus on the task of node classification. We incorporate both\nthe classification loss and the fairness loss associated with the\nclassification to enhance the training process as illustrated in figure\n2(e).\nClassification loss: In node classification tasks, the Graph Neural\nNetwork (GNNs) final layer typically adapts its dimensionality to\nalign with the number of classes. It utilizes a softmax activation\nfunction, wherein the probability of class i is represented by the\nith output dimension. To optimize the model's parameters, we\nminimize the cross-entropy loss, defined as:\n$L_{task} = - \\frac{1}{|Y|}  \\sum_{i} \\sum_{j} Y_{ij} \\log(\\hat{Y}_{i,j})$ (11)\nHere, Y denotes the number of labeled examples (rows in Y),\nwhile $\\hat{Y}_{i,j}$ represents the predicted probability of the ith example\nbelonging to the jth class.\nFairness loss: As the proposed coreset selection aims to maintain\nfair representations across different groups, we introduce a fairness\nloss on the selected coreset samples c. We group the samples in c\nbased on their class label. Let $c_{y=0}$ and $c_{y=1}$ denote the groups of\nsamples in the coreset with labels 0 and 1, respectively for binary\nclassification. We use a similarity-based loss, trying to achieve\nparity in the average pairwise similarities for the two label groups\nin c, as follows:\n$L_{fair} =  \\frac{1}{\\mid C_{y=0}\\mid} \\sum_{i, j \\in C_{y=0}} sim(x_i, x_j) - \\frac{1}{\\mid C_{y=1}\\mid} \\sum_{i, j \\in C_{y=1}} sim(x_i, x_j)$ (12)\nwhere sim(,) is a cosine similarity function between two sam-\nples of same label. This fairness loss drives the coreset selection\ntoward balancing the average intra-label similarities between the\ntwo label groups in c. It aims to constrain the similarity distribution\nof labels to be similar across the two groups, $S_0$ and $S_1$.\nOverall loss: By combining all the above loss terms, we formulate\nthe overall loss as\n$L = L_{task} + L_{fair}$ (13)"}, {"title": "4 EXPERIMENTS", "content": "We aim to address the following research questions in our experi-\nments. RQ1: How reliable is the reported fairness score of graph\ndebiasing methods free from paradox at a community level? RQ2:\nHow efficient are the existing graph fairness evaluation methods\nin estimating the bias of GNNs at a community level? RQ3: How\nefficient is our proposed ComFairGNN method in terms of both\naccuracy and fairness at the community level?"}, {"title": "4.1 Experimental Setup and Datasets", "content": "Datasets. In this context, the downstream task is node classification.\nWe conducted our experiments using three real-world datasets:\nPokec-z, Pokec-n, and Facebook. Specifically, Pokec-z and Pokec-n\ndatasets are derived from Pokec, a well-known social network in\nSlovakia [32]. This dataset encompasses anonymized information\nfrom the entire network. Due to its extensive size, the authors in [5]\nextracted two subsets from the original Pokec dataset, designated\nas Pokec-z and Pokec-n, based on users' provincial affiliations. In\nthe dataset, each user is represented as a node, and each edge\nrepresents a friendship between two users. The sensitive attribute\nin this context is the users' location. The objective is to classify users\nbased on their field of work. The Facebook dataset is derived from\nthe Facebook DataSet [15]. In this dataset, each user is represented\nas a node, and each edge signifies a friendship between two users.\nThe sensitive attribute is the gender information associated with\neach node. We summarize the datasets in Table 6\nBaseline Methods: We investigate the effectiveness of four state-\nof-the-art GNN debiasing methods at a community level, namely\nFairGNN [5], NIFTY (uNIfying Fairness and stabiliTY) [1], and\nUGE [37]. FairGNN utilizes adversarial training to eliminate sensi-\ntive attribute information from node embeddings. NIFTY improves\nfairness by aligning predictions based on both perturbed and unper-\nturbed sensitive attributes. This approach generates graph counter-\nfactuals by inverting the sensitive feature values for all nodes while\nmaintaining all other attributes. The method then incorporates a\nregularization term in the loss function to promote similarity be-\ntween node embeddings derived from the original graph and its\ncounterfactual counterpart. UGE derives node embeddings from\na neutral graph that is free of biases and unaffected by sensitive\nnode attributes, addressing the issue of unbiased graph embedding.\nWe use GCN [19] as the backbone GNN model.\nEvaluation: We use the widely adopted group fairness metrics,\nASP (Statistical Parity), and \u2206\u0395\u039f (Equal Opportunity) to evaluate\nfairness at a community level in Tables 3, 4, 5 and graph level in\nTable 2. As discussed in section 2.3, ASP and \u2206\u0395\u039f fundamentally\nrequire that the model's outcomes remain statistically independent\nof the nodes' sensitive features. In particular, we form two groups\nof nodes from the test set, $S_0$ and $S_1$, containing test nodes with\nsensitive features 0 and 1, respectively. The two groups fairness is\nthe most serious fairness issue in GNN's tasks. As GNNs learn by\nneighborhood aggregation section 2.1, we evaluate fairness within\ndifferent subgraphs or communities within the larger graph. This\ncan help identify and address local disparities more effectively. We\nselect $S_0$ and $S_1$ nodes within the community to evaluate the model's\ndisparity locally, which may present more prominent biases. We\nselect top and bottom 15 sample homophily neighborhood distri-\nbution ratio from each community to formulate the representative\ncoreset nodes from the training set during training. These nodes\nare sufficient to cover the local contextual structures where biases\nare rooted. To further conduct a comprehensive fairness evaluation,\nwe additionally select coreset nodes from both extremes of the\nhomophily neighborhood distribution ratio, specifically choosing\nnodes from the top and bottom 5 and 10 of this distribution. For\nevaluating model performance on the node classification task, we\nuse classification accuracy ACC and the area under the receiver\noperating characteristic curve AUC.\nImplementation Setup: For a fair comparison with other baseline\nmethods, we employ a standard Graph Convolutional Network\n(GCN) [19] as our encoder. This choice emphasizes our focus on the\noverall framework. Consistency is maintained across all baselines,\nwhich also utilize GCN layers. We implement two-layer GCNs for all\ndatasets and incorporate a two-layer Multilayer Perceptron (MLP)\nfor the predictor network. Our model is implemented using PyTorch\nand PyTorch Geometric, leveraging their efficient graph processing\ncapabilities. The baseline implementations were adopted from the\nofficial PyGDebias [8] graph fairness library. Each method was run\nfor three iterations, using the best hyperparameters reported in the\noriginal paper. We Perform for 400 epochs with a learning rate 0.001\non each dataset on the node classification task for three iterations.\nAll of our experiments were conducted on 16 GB NVIDIA V100\nGPUs.\nFairness Paradoxes: To address RQ1, we evaluate the effective-\nness of current debiasing techniques for GNNs at the community\nlevel and compare the consistency of their debiasing performance\nwhen applied to the entire graph. We first measure the group fair-\nness of the GCN model with the debiasing method applied to the\noriginal input graph G. Next, we measure the group fairness of\nthe same GCN model with the debiasing method applied at the\ncommunity level. We use the group fairness metrics ASP and \u2206\u0395\u039f.\nWe observe that the fairness performance across the communities is\nnot as consistent as the fairness score reported for the entire graph\nacross all three real-world datasets for all methods. For example, in\nTable 3, we observe that the unfairness estimation for the Pokec-z\ndataset using ASP and AEO is higher for communities {2, 3, 4, 5}\ncompared to ASP and AEO for the entire dataset when FairGNN\ndebiasing method is used. We also observed Simpson's paradox on\nPokec-z with UGE: the accuracy of Group_0 (66.06) is higher than\nGroup_1 (62.36) on the entire graph, but on the community level,\nfor 4 out of 5 communities, predictions on Group_1 have better\naccuracy.\nOver-Simplification of Fairness Evaluation: It is important to\nnote that both ASP and AEO group fairness metrics use absolute\nvalues for their numerical results. \u0394\u0395\u039f measures whether different\ngroups have equal true positive rates. Using absolute values might\nmask local disparities. For example, in one part of the graph, one\ngroup might have a very high true positive rate, while in another\npart, the same group might have a very low rate. An absolute\nmeasure could average these out, hiding the local unfairness as\nindicated in the results of \u2206\u0395\u039f(w/o) (without absolute value) for\ndifferent communities; some are positive while others are negative.\nSimilarly, ASP checks if different groups have the same likelihood\nof being assigned a positive outcome. the absolute values could hide\nlocal variations in dominance. A group might have equal overall\npositive outcomes when averaged out, but locally, one group might\nbe consistently favored or disfavored as indicated in the results\nwithout absolute value ASP(w/o). Therefore, if one part of the graph\nis very fair and another is very unfair, the overall metric might show\nmoderate fairness. This is misleading because it does not reflect the\nlived experiences of individuals in the unfair part of the graph."}, {"title": "4.2 Model Analysis", "content": "Ablation Study: To Validate the coreset section of ComFairGNN,\nwe consider different variants of selection approaches based on\ntheir homophily neighborhood distribution, section 3.2. (1) Ran-\ndom node selection: we select an equal number of n coreset nodes\nfrom groups $S_0$ and $S_1$ of the entire train set. (2) Top n homophily\nneighborhood ratio distribution: we select the top n highly ho-\nmophily nodes of both groups $S_0$ and $S_1$ from each community. (3)\nwe select n number of coreset nodes of both groups $S_0$ and $S_1$ from\neach community randomly. We report the result in figure 6 and\nmake several observations. Firstly, without selecting the n coreset\nnodes from both extremes of homophily ratio distribution, we often\nget worse fairness performance, which means representative nodes\nfrom both high and low homophily ratio distribution can mitigate\nthe structural bias at the community level. Secondly, by selecting\nthe n coreset nodes randomly from the entire train set, the fairness\nmetric generally becomes worse. Thus, selecting the coreset at the\ncommunity level is effective in driving the debiasing context at the\nlocal subgraph"}, {"title": "5 RELATED WORK", "content": null}, {"title": "5.1 Fairness in Graph", "content": "Fairness in Graph Neural Networks (GNNs) is an emerging area of\nresearch, researchers have performed signifi- cant work to measure\nthe fairness of those systems[10, 16, 20, 36?", "7": "modifies the adjacency matrix\nand node attributes to produce unbiased graph-structured data by\nreducing the Wasserstein distance between different groups. Ad-\nditionally, some studies [5, 38", "5": "incorporates fairness regularization to address bias in\ngraph-based tasks. It aims to ensure equitable treatment of differ-\nent groups in both learned representations and final predictions.\nUGE [37", "1": ".", "2": "nenhances counterfactual fairness and stability in node representa-\ntions through a novel triplet-based objective function. It employs\nlayer-wise weight normalization using the Lipschitz constant to\nimprove performance and reliability. GEAR addresses graph unfair-\nness through counterfactual graph augmentation and adversarial\nlearning for attribute-insensitive embeddings. These combined ap-\nproaches effectively reduce bias in graph-based models. GEAR [24", "23": "aims to ensure fair outcomes for nodes\nwith varying degrees within a group. Despite they introduce at-\ntribute and structural group fairness, overlooked the inherent local"}]}