{"title": "ADAPTIVE LENGTH IMAGE TOKENIZATION VIA RECURRENT ALLOCATION", "authors": ["Shivam Duggal", "Phillip Isola", "Antonio Torralba", "William T. Freeman"], "abstract": "Current vision systems typically assign fixed-length representations to images, regardless of the information content. This contrasts with human intelligence -and even large language models\u2014which allocate varying representational capacities based on entropy, context and familiarity. Inspired by this, we propose an approach to learn variable-length token representations for 2D images. Our encoder-decoder architecture recursively processes 2D image tokens, distilling them into 1D latent tokens over multiple iterations of recurrent rollouts. Each iteration refines the 2D tokens, updates the existing 1D latent tokens, and adaptively increases representational capacity by adding new tokens. This enables compression of images into a variable number of tokens, ranging from 32 to 256. We validate our tokenizer using reconstruction loss and FID metrics, demonstrating that token count aligns with image entropy, familiarity and downstream task requirements. Recurrent token processing with increasing representational capacity in each iteration shows signs of token specialization, revealing potential for object / part discovery. Code available at https://github.com/ShivamDuggal4/adaptive-length-tokenizer.", "sections": [{"title": "1 INTRODUCTION", "content": "Representation learning (Bengio et al., 2013), which involves extracting meaningful and useful information from input observations, is crucial for decision-making. An effective representation should be compact while encoding all relevant information. However, what constitutes \u201crelevant\" information varies based on the specific task; for example, a coarse classification task may require a different latent representation compression factor for satisfactory performance compared to a task demanding perfect pixel-level reconstruction, which necessitates denser representations. This notion of a useful representation aligns closely with aspects of human intelligence (Legg & Hutter, 2007), particularly the concept of adaptive and variable-compressible representations (Hutter, 2006). Similarly, language models can describe content at various levels of abstraction depending on complexity, context (Graves, 2016; Dehghani et al., 2018), and familiarity (Baevski & Auli, 2018). In contrast, most current visual systems, such as VAEs, VQGANs, and ViTs (Kingma & Welling, 2022; Esser et al., 2020; Dosovitskiy et al., 2020), generate fixed-size representations for all images. In this work, we take a step toward learning adaptive and variable-length visual representations, emphasizing that each image requires a different representation capacity (see Sec. 4).\n\nA common framework for learning image embeddings or representations is the encoder-decoder approach, where an encoder compresses input data into a compact latent representation, which can later be decoded and compared with the original image as a learning objective. While there are other encoder-only methods, such as contrastive learning (Chen et al., 2021) and self-distillation (Caron et al., 2021), we focus on encoder-decoder approaches because a reconstruction objective intuitively promotes the learning of adaptive representations by capturing varying level-of-details necessary for better reconstruction. The current state-of-the-art (transformer-based) encoder-decoder approaches (Dosovitskiy et al., 2020) operate in the discrete token space, by encoding images into learned tokens and then decoding them back to image pixels. To generate these tokens, these approaches compress (slightly) at the input patch-level and then maintain the number of tokens (= number of patches) throughout the encoder-decoder network depth. Thus, the representation length for all images is fixed to the number of tokens, equivalent to the fixed patch-size decided by the human-engineer. Moreover, by having number of tokens equal to number of patches, such approaches are tied to the"}, {"title": "2 RELATED WORK", "content": "The goal of image tokenization is to map high-dimensional 2D images into compressed latent repre-sentations. Modern vision systems often use self-supervised learning objectives, such as contrastive learning, self-distillation, in-painting, and generative modeling, to achieve this. Architecturally, these methods typically convert images into 2D feature embeddings using convolutional backbones or transformers after splitting images into patches. However, this approach constrains and tightly binds the processing, compression, and representation capacities to a fixed number of processing units, down-sampling steps or patches, regardless of the input image. Several prior works have explored such issues with different motivations, as discuss below\n\nDynamic Token Processing: Several works (Bolya et al., 2023; Rao et al., 2021; Yin et al., 2022) focus on dynamically processing tokens in the ViT architecture by pruning or merging them across layers. Token Merging (Bolya et al., 2023) accelerates ViT by merging a fixed number of tokens per layer, resulting in a consistent token count for each image. Inspired by ACT (Graves, 2016) and Universal Transformers (Dehghani et al., 2018), DynamicVIT (Rao et al., 2021) and A-ViT (Yin et al., 2022) adaptively prune tokens or dynamically halt processing for different tokens, with a focus on classification tasks. Concurrent work (Jain et al., 2024) extends this by routing 2D image tokens through different experts, rather than pruning or merging, for classification and image retrieval. Tokens in these works remain tightly coupled to image patches. Our approach also involves dynamic token processing but primarily focuses on distilling images into a variable-length compressed 1D"}, {"title": "3 ADAPTIVE LENGTH IMAGE TOKENIZATION", "content": "Tokenization refers to the process of breaking down input data into discrete units or tokens, that are suitable for a specific downstream task. General-purpose tokenizers are usually trained with self-supervised objectives such as auto-encoding, next-token prediction, contrastive learning, or self-distillation. In the visual domain, prominent tokenizers like VAEs (Kingma & Welling, 2022), VQ-GAN (Esser et al., 2020), and ViT (Dosovitskiy et al., 2020) rely heavily on the 2D spatial inductive bias of images, treating 2D patches as tokens. This approach ties the tokenizer's architecture closely"}, {"title": "Latent Distillation of 2D Image Tokens to 1D Tokens", "content": "We want to map an input image to 1D latent tokens. Focusing on the core problem of compressive / adaptive representation learning (and primarily for compute reasons), we first leverage an existing VQGAN image tokenizer to first map an input image to a set of 2D image tokens, $K_{2D}$. Credited to years of research done on quantized 2D auto-encoders, the pre-trained VQGAN model can map a $256 \\times 256$ image to $16 \\times 16$ 2D spatial tokens, without much loss of detail. Each of the $16 \\times 16$ tokens is a pointer to one of the quantized codes in the trained VQGAN codebook. In this section, we distill the $K_{2D}$ (= 256 for 256-dimensional image) spatial tokens to a few $K_{1D}(\u226a 256)$ 1D tokens. For majority of the experiments, we set this atomic (min. token count per image) number, $K_{1D}$ to 32, for ease of experimentation."}, {"title": "2D \u2192 1D \u2192 2D Distillation", "content": "Given $K_{2D}$ spatial image tokens / features, each of dimension $d_{2D}$, we append them with $K_{1D}$ latent tokens along the token axis, and pass then through the latent-distillation-encoder, Enc. $K_{1D}$ are initialized with learned embeddings. The distillation encoder performs joint self-attention on all the tokens and distills $K_{2D} \\rightarrow K_{1D}$. Although previous works (Jaegle et al., 2021b; Jabri et al., 2022) have experimented with cross-attention, we do not focus on this aspect of the architecture for this work and leave it for future analysis. The distilled latent tokens, $K_{1D}$, are then passed to the distillation decoder Dec, which appends them to $M_{2D}$ masked tokens and performs the reverse task of distilling the latent tokens back to the 2D spatial tokens i.e $K_{1D} \\rightarrow M_{2D}$. All inputs to the distillation-encoder and distillation-decoder masked tokens are added with positional encoding (separate ones for 2D image tokens, 1D latent tokens and 2D masked tokens). We factorize and quantize the distilled latent tokens (output of Enc) before passing them to the distillation-decoder, by sampling from a learned 1D codebook via closest codebook logic, following (Yu et al., 2021; 2024). Among other techniques (Zhu et al., 2024; Huh et al., 2023), we found factorization to be most useful for learning quantized 1D codebook.\n\n$\\begin{aligned}\nK_{1D}^{t=1} &= Enc([K_{2D}; K_{1D}^{t=0}]) \\\\\nM_{2D}^{t=1} &= Dec([ M_{2D}^{t=0}; K_{1D}^{t=1}])\n\\end{aligned}$   Latent Distillation 1st Iteration\n\n$K_{1D} \\rightarrow K_{1D}$ denotes an encoder update to map initialized latent embedding to learned distilled embedding. Likewise, $M_{2D} \\rightarrow M_{2D}$ denotes reverse distillation using learned latent tokens ($K_{1D}$)"}, {"title": "Auto-regressive Framework for Variable Tokenization", "content": "In the previous section, we explained the core module for 2D\u21921D distillation module. We now describe the auto-regressive rolling of the encoder-decoder distillation module for learning variable tokens per image, with $K_{t+1}$ as the minimum tokens per image. Multiple works (Dehghani et al., 2018; Graves, 2016) in sequential decision making and natural language processing perform recursive roll-out of the same thinking (Schwarzschild et al., 2021) architecture to provide more computational budget to the input task. In a similar vein, we perform recurrent processing of the input image with the objective of learning variable-length compressed representations. With each roll-out iteration, we not only provide more processing capacity by recursively rolling out the distillation Enc \u2013 Dec architecture, but also provide additional computational memory in terms of new writeable tokens to better distill image tokens into more ID latents. We now dive into the details.\n\nAt each iteration of latent distillation, we concatenate the latent tokens from the previous iteration, $K_{1D}^{t=1}$, with additional newly initialized tokens (initialized with learned embeddings), $K_{1D}$. Optionally, to help the distillation encoder focus on image tokens that were not perfectly distilled in the previous iteration, we apply a masking / dynamic halting operation (Mask) to the processed image tokens from the last iteration, $K_{2D}^{t}$. This mask is determined by the alignment between reconstructed output $M_{2D}^t$ and original image tokens $K_{2D}^{t=0}$. The masked image tokens are then concatenated with the latent tokens and passed through the distillation encoder-decoder, Enc \u2013 Dec. This process is repeated across multiple iterations. As in single-step distillation, the primary training objective is to minimize the reconstruction loss between the new reconstruction, $M_{2D}^{t+1}$, and the original image tokens $K_{2D}^{t=0}$. At each iteration, the distilled latent tokens are factorized and quantized using a shared 1D codebook tokens learned across different iterations belong to the same embedding space.\n\n$\\begin{aligned}\nK_{1D}^{t=T} &= [K_{1D}^{t}; K_{1D}] \\\\\nK_{2D}^{t} &= Mask(K_{2D}^{t=0} | M_{2D}^{t}, K_{2D}^{t=0})\\\\\nK_{1D}^{t+1}, K_{1D}^{t+1} &= Enc([K_{2D}^{t}; K_{1D}^{t}]) \\\\\nM_{2D}^{t+1} &= Dec([ M_{2D}^{t}; K_{1D}^{t+1}])\n\\end{aligned}$    Latent Distillation T + 1th Iteration\n\nIn summary, at each iteration of the recurrent rollout, the latent tokens from the previous iteration receive residual updates, while new computational memory (additional latent tokens) is introduced."}, {"title": "3 ADAPTIVE LENGTH IMAGE TOKENIZATION", "content": "Tokenization refers to the process of breaking down input data into discrete units or tokens, that are suitable for a specific downstream task. General-purpose tokenizers are usually trained with self-supervised objectives such as auto-encoding, next-token prediction, contrastive learning, or self-distillation. In the visual domain, prominent tokenizers like VAEs (Kingma & Welling, 2022), VQ-GAN (Esser et al., 2020), and ViT (Dosovitskiy et al., 2020) rely heavily on the 2D spatial inductive bias of images, treating 2D patches as tokens. This approach ties the tokenizer's architecture closely"}, {"title": "4 NOT ALL IMAGES ARE WORTH THE SAME REPRESENTATION", "content": "Each image is unique and requires a different number of tokens as representation. Additionally, each image or observation can have multiple valid representations, echoing Epicurus' notion of multiple explanations. By mapping an image to various quantized latent spaces, the model learns to sample different tokens from the training set's codebook, optimizing the reconstruction objective at different levels of computational capacity. In this section, we provide experimental insights into how learning adaptive representations can help support and expand upon these concepts.\n\nRepresentation Capacity or Compression Depends on Information Entropy / Complexity: Schmidhuber's Low Complexity Art theory (Schmidhuber, 1996) correlates an image's perceptual complexity with its compressibility\u2014the smallest description length of an image often aligns with its complexity. Given that our approach generates multiple compressed representations for a given image, we evaluate such correlation between human-labeled complexity estimates (ranging from 0 to 100) (Saraee et al., 2018) and the L1 reconstruction loss using our adaptive tokenizer at vary-ing token capacities. Fig.4 (left) illustrates reconstructions of completely out-of-distribution (OOD) images from the PeopleART dataset (Westlake et al., 2016), using between 32 and 256 tokens for images of different complexities. These reconstructions are produced using a model trained on ImageNet-100, which contains no art-related images and is significantly different from PeopleART. As seen, the low-complexity image (top row) is adequately reconstructed with fewer tokens, while the highly complex image (bottom row) requires more tokens for accurate reconstruction. Furthermore, the complexity-reconstruction correlation plot in Fig. 4 (right) perfectly highlights two observations: (a) as image complexity increases, reconstructions with fewer tokens result in higher L1 errors, necessitating a larger memory budget; and (b) at a fixed image complexity, increasing the computational budget (i.e., number of tokens) reduces the loss, demonstrating the efficiency of the adaptive representation model. See Appendix Fig. 11 and Fig. 12 for more such results.\n\nRepresentation Capacity Depends on Familiarity with the Training Set: Similar to how out-of-syllabus questions require more effort, reconstructing OOD images demands more computational tokens. By learning quantized adaptive representations, our model maps test images to adaptive"}, {"title": "6 CONCLUSION", "content": "In this work, we propose a variable-length tokenizer for 2D images that operates by recurrently pro-cessing 2D image tokens, distilling them into 1D latent tokens, and adaptively adding new 1D tokens as computational resources for future iterations. This recurrent processing and adaptive computa-tion enable the learned latent tokens to correspond to semantically meaningful objects or parts in the input image. We demonstrate comparable performance on reconstruction metrics and ImageNet-1K linear-probing experiments. Finally, we utilize per-image learned adaptive representations (and cumulative dataset representations) to highlight alignment of the required image representational ca-pacity with - information entropy, familiarity with train set, knowledge of downstream tasks/models.\n\nFuture Work: We believe that recurrently learning variable-length representations could open up intriguing directions, such as large-scale video representation learning or long-horizon video under-standing, where simply learning fixed-length representations may not even be a feasible solution. Other interesting avenues include exploring adaptive and recurrent nature of the proposed tokenizer as test-time thinking units and training for tasks like vision-language & visual-abstract reasoning (e.g.: ARC (Chollet, 2019)). Variable-token compression could speed up generative models."}]}