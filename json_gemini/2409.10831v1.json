{"title": "PDMX: A Large-Scale Public Domain MusicXML Dataset for Symbolic Music Processing", "authors": ["Phillip Long", "Zachary Novack", "Taylor Berg-Kirkpatrick", "Julian McAuley"], "abstract": "The recent explosion of generative AI-Music systems has raised numerous concerns over data copyright, licensing music from musicians, and the conflict between open-source AI and large prestige companies. Such issues highlight the need for publicly available, copyright-free musical data, in which there is a large shortage, particularly for symbolic music data. To alleviate this issue, we present PDMX: a large-scale open-source dataset of over 250K public domain MusicXML scores collected from the score-sharing forum MuseScore, making it the largest available copyright-free symbolic music dataset to our knowledge. PDMX additionally includes a wealth of both tag and user interaction metadata, allowing us to efficiently analyze the dataset and filter for high quality user-generated scores. Given the additional metadata afforded by our data collection process, we conduct multitrack music generation experiments evaluating how different representative subsets of PDMX lead to different behaviors in downstream models, and how user-rating statistics can be used as an effective measure of data quality. Examples can be found at https://pnlong.github.io/PDMX.demo/.", "sections": [{"title": "I. INTRODUCTION", "content": "There has been a recent explosion in development of generative music systems, both for symbolic- and audio-domain music generation [1]\u2013[6]. As such advances have allowed for significant commercial investment in GenAI products (in music and beyond), debate has increased over the legality of training such systems, and in particular, how such models may replace artists and interact with modern copyright law [7]\u2013[11], including notable high-profile lawsuits against generative music startups [12]. Such concerns over replacing artists with audio-domain text-to-song generators have too renewed interest in symbolic-domain music processing, which allows for greater artist-in-the-loop interaction [5].\nA natural alternative to the intense legal process of licensing music is to train on public domain data. Unfortunately, this approach remains complex, as many symbolic music datasets currently fail to adequately vet for copyrighted music [13]\u2013[17], let alone explicitly filter for works in the public domain [18]\u2013[20]. This results in practically no available public domain large-scale symbolic datasets, to our knowledge. Additionally, heterogeneity in current symbolic music has led to most datasets only supporting MIDI format [21]. While MIDI is useful for simple symbolic music modeling tasks, it omits an abundance of extra information in musical scores w.r.t to notating symbolic music, such as performance directives (e.g. dynamic markings, articulations), time-located text, and section boundaries, which are only present in more structured formats like MusicXML."}, {"title": "II. RELATED WORKS", "content": "There exist a number of publicly available datasets for modeling symbolic music (see Tab. I), with a wide variety of licensing scenarios and sizes. Most datasets, such as the moderate-sized SymphonyNet [13], the large MMD [16] and MetaMidi [17], and the smaller Wikifonia [14] and POP909 [15] datasets offer no clear licensing information, being thus unsafe for copyright issues. While some existing datasets do have explicit licenses, these are either specifically non-commercial, such as for MAESTRO [19] and EMOPIA [20], or have documented licensing violations as in LMD [18]. Additionally, few large datasets contain diverse multitrack music, and fewer still present data in the MusicXML format, thus ommiting the notational information present in MusicXML but unsupported in datatypes like MIDI. Unlike past datasets, PDMX is a large-scale set comprised entirely of CC-0 / Public Domain MusicXML files, containing diverse multitrack symbolic music with song metadata."}, {"title": "III. DATASET", "content": "We created PDMX by scraping MuseScore\u00b9, a score-sharing on-line forum where community members (as well as formal music publishers) can upload their own sheet music arrangements and compositions as MusicXML files, including both licensed and public domain content. Besides the direct MusicXML files, we additionally scraped the metadata for each score in order to provide a cohesive understanding of the dataset, which includes tag-based descriptions"}, {"title": "A. Parsing MuseScore Data", "content": "Unlike MIDI, MusicXML files are meant for rendering sheet music as it should be read by a musician (rather than how it directly sounds), thus containing a wealth of notational information that is common in western music scores. Existing software capable of parsing these files, namely the Python library MusPy [22], is not optimized for the task. Given a piece of symbolic music, the resulting MusPy Music primarily extracts the note value information, as well as limited metadata like time/key signatures and tempo markings. However, MusPy fails to parse many MusicXML-specific score objects, including performance directives (e.g. dynamics, articulations), lyrics, and phrase boundaries, stripping out such content from the notes themselves into a catchall \"annotation\" structure."}, {"title": "B. Data Quality", "content": "A past problem in existing symbolic music datasets is overall dataset quality, as very limited works exist to assess the \"quality\" of symbolic music, and existing high-quality datasets are much smaller [15]. Due to the crowd-sourced nature of the MuseScore platform,"}, {"title": "C. Deduplication", "content": "Deduplication is an important part of dataset construction, as heavy duplication within a dataset may bias the dataset towards particular high frequency data points and thus degrade downstream modeling tasks [26]. However, unlike in more traditional domains like text processing [27], string matching techniques using the textual metadata are insufficient for detecting duplicates in PDMX. This is because many of the same pieces have different titles, such as \"Pachelbel's Canon in D\" and \"Canon by Pachelbel\", despite being the same piece. Additionally, the notion of \"duplicate\" is ill-defined in the context of sheet music, as even two scores of the same song may be arranged differently and for distinct instrumentations.\nTo address these issue, we first use a pre-trained text embedding model, Sentence-BERT [28], to encode song titles as fixed dimension embedding vectors. Formally, we embed a song \"descriptor\" \u2013 a combination of a song's title, subtitle (if applicable), artist, and composer (if different from the artist) \u2013 to combat the scenario where different artists compose works of the same name. We then use cosine similarity to compare embedding vectors and scale those values between zero and one to obtain similarity scores. We set a duplicate threshold of 80%; that is, we cluster a given song with all other songs >80% similar. This threshold was manually verified to capture a reasonable range of duplicate songs. However, deduplicating by song descriptor alone ignores a variety of possible different arrangements, all valuable from a music generation perspective.\nFor each cluster of duplicate songs, we next group by instrumen-tation. For example, solo piano arrangements of \"Pachelbel's Canon\" fall into separate clusters from their string quartet counterparts. However, within a single instrumentation cluster can exist multiple unique arrangements (e.g. a beginner versus advanced version). To address this, we use a simple heuristic of total note count to cluster songs (as note count is an easy proxy for large differences in song arrangement). We set a uniqueness threshold of 5% (i.e. songs with a >5% difference in note count fall into different clusters). Like the duplicate threshold, we obtained this value through trial and error, feeling that 5% well-captured overly similar arrangements. Within each similar note count cluster, we finally select the \"best\" arrange-ment of a song by considering each score's rating and, in the event of a tie, number of notes (the more notes, the better). Therefore, while a score could have tens of duplicates when considering song descriptor alone, within this grouping can exist many unique arrangements.\nUsing this deduplication strategy, we remove 151,442 songs (nearly 60% of the base dataset), leaving 102,635 unique arrangements"}, {"title": "D. Analysis", "content": "As PDMX is a multitrack dataset, we first analyze the density of tracks (i.e. single instrument parts) within a given song. Over 90% of songs in PDMX contain less than five tracks, while over half the dataset consists of solo works. We note that only \u22483% of songs in PDMX have more than five tracks, possibly due to larger multitrack scores (common in orchestral or marching band repertoires) not in the public domain. PDMX encompasses 20 different genres, the most common being classical and folk music (see Fig. 2). More modern genres, like hip-hop and electronic music, are comparatively much fewer in number, likely due to limited public domain content for more recent works. Although genre tags are absent from 67% of songs, only 50% of notes lack a genre, suggesting that genre-labeled works are more dense than unlabelled ones. Additionally, when breaking down the genre distribution by subset (see Fig. 2), we find that the rated (and subsequently rated and deduplicated) subsets of PDMX contain a significantly longer \"tail\" of genres than the full dataset (with \u224840% coming from non classical/folk genres), denoting a large amount of unrated classical music present in PDMX."}, {"title": "E. Additional Features", "content": "While we have focused on highlighting PDMX's note-based con-tent and parts of its metadata, we note that PDMX includes a number of extra features. Regarding metadata, PDMX includes rich rating and user comment information, enabling work on symbolic music recommendation [29], [30] and preference modeling. Notably, as PDMX is a MusicXML dataset, it includes 12M time-aligned performance directives such as tempo text (e.g. adagio), dynamic hairpins (e.g. crescendo), note articulations (e.g. staccato), and section markings, which are parsed by the MusicRender framework and may be used both for discriminative tagging tasks, expressive music rendering, and even as conditions for controllable music generation [5], [31]. PDMX also includes over 10M lyric tokens, opening the door for further research on lyric-to-score and score-to-lyric tasks [32]\u2013[34]. We plan to fully release PDMX as 44.1kHz synthetic audio rendered through Fluidsynth, creating a large corpus for music transcription [35] and audio-domain music generation [6], [31]."}, {"title": "IV. EXPERIMENTS", "content": "The metadata included in PDMX presents an opportunity to ana-lyze how data quality filtering and deduplication affects downstream symbolic music modeling. Specifically, viewing each score in PDMX as a sequence of notes $n = n_1,..., n_\\nu$, our goal is to learn an autoregressive model $p_\\theta(n_i | n_{1:i-1})$ that we can generate symbolic music from. We focus on our four main subsets of PDMX (detailed in Tab. II), as well as a Random subset of songs sampled from the full dataset at the size of the rated and deduplicated subset. We additionally reserve the top 50% (in terms of rating, specifically >4.74 stars) from the rated and deduplicated subset as a potentially \"high quality\" subset for fine-tuning. In this setup, we seek to answer two main questions: (1) How do data quality and deduplication interact to determine symbolic modeling results? and (2) Does fine-tuning on small but high quality data meaningfully change behavior?"}, {"title": "A. Experimental Setup and Metrics", "content": "In this work, we use the tokenization scheme REMI+ [36], an extension to the REMI input representation that allows for multitrack music [37]. Namely, we represent a song as a sequence of notes $n_i$, where each note is represented by a series of five variables: beat, position, pitch, duration, instrument. We employ a metrical timing system (i.e. time is represented in beats rather than seconds).\nFor all experiments, we use a REMI+ style decoder-only trans-former, with 6 layers, 8 attention heads, and a hidden dimension of 512, totaling at \u224820M parameters. We use an absolute positional embedding with a maximum sequence length of 1024. We train each model on a single A6000 GPU for 100K steps with a batch size of 12, learning rate of 5e-4, and the Adam optimizer with default hyperparameters. For fine-tuning, we employ a smaller learning rate of 5e-5, and only train for 5K steps. Following past work [3], [5], [25], we report the PCE, SC, and GC across 1,200 generations per model, which measure how well the model captures the underlying musical patterns of the data."}, {"title": "B. Results", "content": "1) Objective Metrics: In Tab. III, we observe that between the five base models, those trained on the rated subsets display greater harmonic and rhythmic diversity than those not, and have the closest statistics to the fine-tuning subset. However, once all models fine-tune on the 50% best rated scores, this distinction goes away, with all five fine-tuned models showing more similar metrics.\n2) Subjective Listening Test: To measure music quality across our ten models (five base, five fine-tuned), we conducted a listening test with 12 participants. In the questionnaire, each participant listened to 30 different samples randomly chosen from a pool of 10 samples per model. Following past work [38], for each sample, users were asked to rate the generation (from 0 to 100) along three axes:\n\u2022 Correctness: Is the music free of inharmonious notes, unnatural rhythms, and awkward phrasing?\n\u2022 Richness: Is the sample musically / harmonically interesting?\n\u2022 Quality: Subjectively, how much do you like the generation?\nIn Fig. 3, we show the average values (top) and violin plots (bottom) for each model along each rating axis. In comparing different modeling subsets (ignoring fine-tuning), we find that correctness is consistent across all subsets. For richness and quality however, the ROD subset (i.e. our most filtered subset, in red) consistently shows the highest scores, followed by R (green), A (blue), Random (purple), and D (orange), suggesting that our deduplication strategy is primarily useful for rated songs (as we pick the highest rated duplicate) rather than unrated (which is chosen at random).\nRegarding fine-tuning, we find that this process increases richness in all models, and improves quality in three (D, RAD, Random), with a particularly strong effect on the Random (purple) model, converting it into our second-highest rated in quality overall. There is also a noticeable negative effect on correctness in the All (blue) model, suggesting that the model may have overfit to overly simple examples in the full dataset. These results together show the strength of our deduplication and filtering strategies as a way to perform significant dataset distillation (as the ROD model performs best despite seeing only 15% of the data)."}, {"title": "V. CONCLUSION", "content": "We present PDMX: the largest dataset of public domain Mu-sicXML files to our knowledge. Using our proposed MusicRender package for efficient parsing and hand-crafted filtering algorithms, we show promising results for unconditional multitrack generation that indicate improved performance when filtering for high-quality scores. In the future, we hope to investigate ways to use the large dataset as an effective pretraining mechanism for symbolic generation models,"}]}