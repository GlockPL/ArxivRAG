{"title": "Exploratory Visual Analysis for Increasing Data Readiness in Artificial Intelligence Projects", "authors": ["Mattias Tiger", "Daniel Jakobsson", "Anders Ynnerman", "Fredrik Heintz", "Daniel J\u00f6nsson"], "abstract": "We present experiences and lessons learned from increasing data readiness of heterogeneous data for artificial intelligence projects using visual analysis methods. Increasing the data readiness level involves understanding both the data as well as the context in which it is used, which are challenges well suitable to visual analysis. For this purpose, we contribute a mapping between data readiness aspects and visual analysis techniques suitable for different data types. We use the defined mapping to increase data readiness levels in use cases involving time-varying data, including numerical, categorical, and text. In addition to the mapping, we extend the data readiness concept to better take aspects of the task and solution into account and explicitly address distribution shifts during data collection time. We report on our experiences in using the presented visual analysis techniques to aid future artificial intelligence projects in raising the data readiness level.", "sections": [{"title": "INTRODUCTION", "content": "Data is the core component in most of today's artificial intelligence (AI) projects. However, the data can be useless in an AI project unless it has been made accessible and its potential issues have been appropriately resolved. To raise awareness of the challenges involved in getting data ready for use in artificial intelligence projects, Neil D. Lawrence [37] introduced the data readiness level concept. Data readiness can be used to aid in planning the time it takes to make use of data, processing the data itself, and communicating the state of the data. However, the data readiness levels are purposefully vague in order to cover a wide variety of data-driven applications, not limited to specific AI applications but data science in general. Therefore Castelijns et al. [15] contributed with concretely defined levels as a prerequisite for specific kinds of machine learning (ML) projects and thereby lowered the bar for practical usage of the concept. Data readiness is not just important for ML, but for AI as a whole whenever methods are validated on real-world data, or exposed parameters are available to be tuned for a target domain [57].\nWhile having well-defined levels is a great step towards making the data readiness concept usable in practice, we discovered that important aspects are still missing in terms of visualization, time-varying, and textual data. Moreover, the application and its context is essential for determining if the data is relevant to a task. However, application-level aspects are missing in prior work [15,37]. We argue that visualization, which is increasingly used for data analysis, must be better integrated into the data readiness process to both discover and communicate issues in the data. An indication of this is that MLOps practitioners find visualization for data profiling (i.e. assessing data quality) challenging, and better tools and practices are needed [48]. It is not uncommon that discovered data issues require communication with domain experts to identify appropriate actions and solutions. For example, an event might have been classified as a specific class, e.g., a car, which later splits into two or more classes due to a business change, e.g., electric, gas, and diesel cars. Such concept drifts [40] in the data can often be discovered using automatic tools, but assessing their impact and understanding how they should be handled often requires communication with stakeholders outside the ML team, which is where visualization excels.\nIn this work, we aim to bridge the gap between data-driven AI"}, {"title": "RELATED WORK", "content": "This work lies in the intersection between the areas of data-centered AI and visualization. The related works are therefore split into data preparation and visualization recommendations.\nData preparation is an integral part of data-centered AI projects. It involves aspects of both processing and understanding the data. Sacha et al. [56] highlighted this as one of the primary goals of preparatory work when dealing with visual analytics for machine learning. OpenRefine [63] uses various techniques such as faceting and clustering to alleviate data cleaning. ActiveClean [35] takes a machine-learning perspective and minimizes the loss given a cleaning function. However, such an approach requires the data to be readable and does not consider the need to involve the organization generating the data to obtain additional knowledge. The Wrangler tool [28] provides a user interface with contextual interaction for specifying data transformations, which is shown to outperform Excel in data-wrangling tasks [28]. Wrangler is also integrated into the work of Kandel et al. [30], which combines statistical analysis with visualization to aid in detecting and assessing data issues. Wexler et al. [64] presented a tool featuring summary statistics, distribution views, and scatterplots. A drawback is, however, that it does not consider text-based data. Luo et al. [42] focus on the correctness of the visualizations themselves and use it as a way of cleaning data. This way they find means to go from a poor visualization to a cleaned one and find data flaws in this process.\nCompared to the works described above, our work explicitly deals with the concept of data readiness and also provides guidance on what to look for in the corresponding charts.\nVisualization recommendation methods suggest charts and visual encodings based on data and/or tasks. They often focus on core tasks such as comparison or correlation. SeeDB [62] suggests visualiza- tions based on the deviations from variable differences, which can aid in detecting flaws in the data. VizDeck [50] presents a range of chart thumbnails based on a statistical analysis of the data. Tableau introduced the 'Show Me' system with automated chart suggestions based on heuristics for choosing small multiples [43]. Similarly, Voyager [66] also includes perceptual considerations for recommending charts. Hu et al. [24] took a learning approach and trained a model to recommend charts for different tasks. Neither of the above chart recommendation systems includes text beyond categorical variables. There are also efforts that, in line with this work, are more task-focused. Stephen Casner [14] presented an automated system that analyses the task description and suggests suitable charts. In contrast to the above-mentioned visualization recommendation works, we take a higher-level approach that is not dependent on a specific analysis environment and also provides guidance on what to look for in each chart.\nFinally, Bayesian machine learning [46] has a long tradition of implicit data readiness by incorporating known data flaws directly into the ML models and inference techniques, or even inferring such flaws directly as part of the learning processes itself. Being tightly coupled with Bayesian data analysis [36], the application of Bayesian statistics to real-world data, where visualization and data analysis is well explored in this field. For example, the automatic statistician [39] use Bayesian non-parametric ML [67] to discover explanations to data sets and automatically generate detailed figures and natural language text explanations of the data (and inherent potential flaws). We instead mainly focus on the other main branches of ML where data readiness, data analysis, and visualization techniques are disconnected from the methods themselves, and therefore have received less attention."}, {"title": "BACKGROUND", "content": "The process of succeeding with an AI project is riddled with difficulties. The first phase towards deploying machine learning is data management (see Figure 1), where data has to be collected, preprocessed, augmented (e.g. with labels), and analyzed in order to curate and reach high data readiness such that it can be used for model learning of a target task [48]. After model learning follows model verification (making sure that the model does what we want sufficiently well) and finally model deployment, which includes monitoring and model updating to handle distribution drift over time. As the data set is growing it also has to be validated constantly to guard against data errors creeping into the learned model [48]. Apart from applying checks used in data curation steps, this requires the use of visualization to identify new kinds of errors (not yet monitored for).\nThe State of ML 2020 survey [44] identify issues related to data readiness as the biggest problems facing ML practitioners working on deploying machine learning, with messiness of data (B) followed by lack of data (B) and accessibility of data (C) being the top issues. Also, unrealistic expectations (A) is a problem at large, e.g. to prematurely assume that a problem can be solved from a given data set when data readiness is low such that the applicability of the data set is unknown.\nAmong data-driven AI paradigms, the field of Machine Learning (ML) has seen a boom in the last decade, fueled by the advancements in deep learning by applying deep neural networks to unstructured data at scale. The unstructured data categories include sound [2, 59], images [38,54, 55] and text [12, 18,47]. Many of these recent advance- ments have been made possible by large models which are pre-trained on certain general tasks (e.g. using self-supervised learning). How- ever, most organizations rely on structured data with for example a mix of dense numerical measurements and categories entered by hu- man operators. For such structured data, XGBoost [16] has achieved state-of-the-art results on a large number of different machine learning challenges [16]. XGBoost, together with LightGBM [32] and Cat- Boost [53], are gradient boosting algorithms [7] and they remain top contenders for prediction tasks over well-structured data. These meth- ods often outperform deep neural networks on tabular data [27,31,58].\nPolyzotis et al. [51] described experiences from developing data- centric infrastructure for ML at Google. They define three personas rep- resenting common roles in their production machine learning projects: ML expert, software engineer, and site reliability engineer. None of these three personas have clear data analysis roles even though data understanding, validation, and cleaning are pointed out as main steps in their ML data life-cycle [51]. This goes in line with our observations that these three steps are key in AI projects, but that it is not always experienced data analysts that perform this work. In addition, data ana- lysts are often not experts in visualization [29]. Thus, there is a clear need for easy-to-adopt guidelines that can aid in improving data quality in AI projects. Data readiness [37] provides a structured approach to improving data quality, but it has no link to visualization. The focus of this work is therefore on how visual analysis can be leveraged to increase data readiness in Al projects for heterogeneous data. The emphasis is on the data management step (i.e. data understanding and analysis) but it also includes model selection and learning, and model verification. With a starting point in the data readiness concept (sec- tion 5), we analyze where and how different charts can aid in ensuring that the data can be used for ultimately solving a given task (section 6). These guidelines have been formed and utilized based on AI projects from going from low data readiness up to verified ML models for the tasks (section 7) ."}, {"title": "METHODOLOGY", "content": "We started by examining each data readiness aspect discussed by Neil Lawrence [37], and Castelijns et al. [15]. The aspects were grouped by the type of task they relate to, and viewed from the context of the multiple phases of ML projects [48]. In this process, we identified and grouped additional aspects missing (time-varying and text data) based on our experience from the trenches of previous machine learning projects. A summary of the data readiness concept and our extensions are provided in section 5.\nGiven the identified tasks, we searched the visualization literature for recommended data transformations and charts [9, 17, 30, 34, 43, 66] that best supported the tasks. As the data type (continuous, categorical, text) is central to the analysis we further subdivided the tasks to take them into account. To further aid AI practitioners, we finally made illustrations that point out what to look for in each type of chart and how that maps to data readiness. The aim is not to cover all different types of advanced visualizations that could be used as this would be overwhelming for a practitioner, but rather the fundamental techniques. The result is a minimalistic guideline described in section 6, where we have favored simplicity before more advanced methods. Based on our experience in AI projects, we believe that this minimalistic approach has a greater chance of being adopted in AI teams."}, {"title": "DATA READINESS - EXTENDED", "content": "Neil D. Lawrence proposed [37] to categorize the usefulness of avail- able data through three bands denoted by letters A (application context), B (faithfulness and representation), and C (accessibility). Each band has sub-levels denoted by numbers with the most ready being Al and the least ready being e.g. C4. Castelijns et al. [15] on the other hand make some alterations in their specialization and opt for five bands denoted by C (conceive), B (believe), A (analyze), AA (allow analysis) and AAA (data set is clean and self-contained). Similarly to [37] C is the least ready and AAA is the most ready. However, the data readiness in [15] (i.e. AAA) does not relate to any target task. It is consequently solely at the data management phase and does not consider the model learning phase, which [37] does (although vaguely). That is, reaching AAA [15] maps to reaching B [37]. Neither of these two considers temporal aspects of data (i.e. time series) nor the ongoing collection of new data of a deployed Al system. In this work, we extend [37] and [15] to the temporal setting, and propose a concrete mapping to visualization methodology to address the data readiness questions that are raised. Our extended version, depicted in Figure 2, takes its basis from [37] with the three bands A-B-C, and fleshes out the B-band and A-band in accordance with [15] and ML deployment practices [48] as well as with our additional contributions regarding time.\nBand C deals with the accessibility of the data. The lowest level of the band reflects uncertainties around the data even existing as seen in Figure 2. Once its existence has been verified the level increases and questions about for example privacy or digitization must be answered. When the data is ready to be loaded into analysis software it is deemed to fulfill band C. The focus of this work is not on band C as visual data analysis requires data to be available, which in turn requires level C1. Notably, discoveries during visual analysis might move the data readiness to band C if it turns out that for example ethical or privacy issues are found within the data.\nBand B deals with correctness and transformation to a state where it can be used for data-centric AI. Thus, it is not only about detecting flaws such as missing values but also about how such flaws should be handled. Here, visualization can aid in both discovering and understanding flaws as well as presenting them to decision-makers or clients to obtain knowledge about how they should be handled. Our main contribution within band B is to fill this gap by providing an explicit mapping to visualizations that were lacking before. Another aspect that needs to be investigated is how the data collection process affects the data, e.g., did it change over time, or was the data collected randomly or with a bias? This is reflected in question B7, which is our second contribution within band B. In the end, the limitations of the data should be uncovered and one should be confident about what is possible to do with the data.\nBand A deals with the applicability of the information in the data to any desirable downstream task. That is, the use and usefulness of the data for AI applications, or likewise for business insights, business decisions or visualization outcomes. It is common that organizations start their data readiness journey with a target task, despite the apparent necessity of high data readiness before it is possible to assess the fea- sibility of said task. It is not uncommon for organizations to discover that the data is more informative and applicable for other related and valuable tasks than the prematurely picked task in question. Nevertheless, it may still be important for an organization of low data readiness maturity to pick some target tasks to focus resources and organizational efforts towards. High data readiness is valuable for an organization, regardless if the initially pursued task doesn't turn out to be actionable.\nNeil D. Lawrence used the broad question \"Can task X be solved with the data?\" [37] as a way of assessing if data had accomplished band A readiness. While this question covers a wide range of aspects it is also unspecific. Therefore, we have formulated seven new aspects that intend to capture data readiness aspects of both the input variables and the target variable in relation to the task and the considered solution.\nA1 Each model and learning method has associated assumptions. For example, target distribution, data size, uncertainty quantification, prediction type, etc. For a model to be applicable, and likely to be able to perform well, then the data have to match these assumptions.\nA2 To learn a useful model it is necessary that the input variables carry information connected to the target. It is essential to form hy- potheses about such information and how it may give rise to the target. Investigating the input and target variables using visualization allows for both assessing known relations and discovering new ones. This provides insight into what to expect from a solution and from the application of specific models and methods to the data set.\nA3 Which input variables that end up being favored by a model when producing a prediction provide both insights into how the model works and possible risks with deploying the model. If it is understood what impacts model performance there can be ways of improving the data or model. Using this knowledge, risks can also be mitigated by taking additional steps to ensure the quality of high-impact variables.\nA4 It is not always easy to provide a binary decision stating that a task is solved. One solution might have better performance in terms of accuracy, while another one has lower accuracy but higher precision. Thus it is important to understand not only that the task can be solved, but also how well.\nA5 For evaluating if a model is a suitable candidate for solving a task, it is necessary to get it to perform as well as possible to the specific target domain. Tuning the learning method (and model class) to the domain makes the model not just perform better, but the process can also provide insight into how suitable and robust the final model is for the target domain (A5,6).\nA6 A deployed model has to be updated as time progresses, to not degrade in performance due to distribution shift. With new data being collected, it is also important to re-tune the learning method to the evolving target domain. Investigating how hard it is to tune the solution to the domain provides insights into the likely effect new data has when the distribution start to shift. It is also informative to know if a few random tuning tries are expected to find a good tuning fit, when it is very expensive to re-run 100 or more tries even using sample-efficient parameter tuning such as Bayesian optimization.\nA7 Finally, without trust in the data and model then the solution will not be adopted and deployed by the organization. Naturally, the level of trust required varies from task to task, which means that data readiness is also implicitly impacted. Knowing that the data has been verified to have high quality and that the model behaves as expected"}, {"title": "VISUALIZATION FOR DATA READINESS", "content": "We here provide guidelines for which types of charts to use and what to look for when going through the data readiness level process. We purposely use illustrative charts with simplified cases to better convey the main concepts. Furthermore, the guidelines focus on the analysis of a single parameter at a time rather than a holistic visual analytics interface with filtering and linked views. The guidelines are intended to be general enough to be applicable to a wide range of real-world use cases dealing with tabular data. We, therefore, exclude real-world data with application-specific tailoring from the guidelines in favor of presentation clarity.\nAs a practitioner, your first step will be to analyze either one data readiness question or one variable at a time. We, therefore, map each question to a set of chart types with varying visual encodings and aggregations depending on the variable's data type. We limit the data types to the ones found in tabular data, i.e., continuous (also referred to as quantitative or numerical values), ordered categories (also referred to as ordinal), categorical (discrete values without inherent order), and text.\nIn the following, we structure the text according to the data readiness questions and explain our rationale behind the chosen charts and visual encodings, as well as what to look for in the charts. For each of these steps, it is important to verify not only the data itself but also the axes and their ranges as tools often derive them based on the data. Note that we do not describe questions related to C1 - C4 or B3 as they are organizational type of questions.\nIs the data flawed?\nData visualization can aid in detecting flaws in many ways as depicted figures 3 and 4. Building on prior work [30, 64], we suggest analyzing data distributions as they allow for capturing many different types of data flaws. We start with analyzing the distribution with respect to the values themselves as depicted in Figure 4. Note that most analyses require communication with stakeholders depending on the analyst's knowledge of the context around the data.\nThe modes, i.e. how frequently the values appear, can reveal issues with how the data is collected. For example, should there be a normal distribution due to the physical properties of the captured phenomena, or are there relatively few values in a range although the variable should follow a uniform distribution? The detection of such flaws may result in, for example, changing the way data is collected, or how the business makes use of categories. Visual analysis of the modes is particularly useful as what is 'reasonable' or 'unreasonable' is heavily dependent on the source of the data and therefore challenging to automate. Note that analyzing the modes of the parameters includes selecting the appropriate bin size in the bar chart. There are guidelines for selecting bin sizes [10], but a way of visually verifying that the bin size is reasonable is to add a suitable [4] kernel density estimate (KDE) curve and check that it aligns with the bars. As this type of verification can be seen as a second-line analysis, we have excluded KDE curves in the illustrations in favor of clarity.\nThe axis range and categories can reveal errors during data capturing or pre-processing, e.g., a broken sensor that reports incorrect values, a normalization error, or incorrect classification in the case of categories. Data ranges and categories can of course also be analyzed using a data summary table, e.g., min/max, but doing the analysis visually together with the distribution can also aid in for example understanding if the flaw is an outlier or if it is a recurring error.\nText items can be inspected individually, but such analysis does not scale to a large number of items. An orthogonal approach is to analyze the text projected onto a 2D canvas as proposed in the seminal work of Wise et al. [65]. The key here is to choose a good way of projecting the text such that semantically similar text items are close to each other even though the words are different. Good projections can nowadays be acquired using the latent space of a language model as outlined in Figure 3. Thus, much fewer text items need to be inspected as they are representative of the close-by text items. Furthermore, flawed text can here appear as outliers caused by, for example, ill-formatted text. To visually inspect the text items in this high dimensional latent space, we apply a projection technique, e.g., UMAP [45], that enables inspecting the text items using a 2D scatter plot. For detailed text cleaning it can be advisable to use tailored text cleaning tools [30, 63]. Depending on the amount of text available for training the models it might be necessary to reduce noise by transforming individual words (Figure 3). Combining knowledge about which words are important and browsing common words and similar words can be a time-efficient approach to cleaning the most relevant data.\nIs numerical/categorical data flawed?\nAre there distribution shifts during data collection?\nThere are three main types of distribution shifts [68]. Covariate shift means that the distribution of inputs changes over time, which can cause the training set to be substantially different from the deployed input distribution. Label shift means that the target variable changes over time. Concept shift means that the definitions or behavior of the variable change over time [40], e.g., a disease is diagnosed differently due to new knowledge. Therefore, in case data collection takes place over time, it is important to both investigate the previously collected data as well as monitor new data. Here, we focus on the analysis of previously collected data even though the same graphs could be used to monitor new data."}, {"title": "Covariate shifts", "content": "The temporal aspect of data should be investigated from multiple angles to cover changes in for example collection patterns or paradigm shifts in the value. As illustrated in Figure 5, we again separate the analysis based on the data type. Continuous data are aggregated by both a statistic (such as the average value) as well as the count, while categorical data is aggregated by the number of occurrences. Line views are used for the aggregated quantitative-temporal variables to follow best practices [43, 66]. For the count-aggregates, we suggest using bars even though lines make it easier to detect trends. Our reasoning is that bars better visually separate them from the averaged values and are more consistent with how 'count' is represented in the other charts. This visual separation makes it less error-prone when switching between views of the same variable. If there are many categories, we suggest providing an overview by arranging the categories along the vertical axis and color-encoding the number of occurrences. A detailed view of the categories can be obtained by faceting by category, see Figure 5 (right). Text data is more challenging to analyze as the sentences can vary considerably over time. One possibility is to monitor the frequency of important keywords. Another is to observe the change in the distribution of the projected text over time.\nParadigm shifts can be detected by looking for sudden changes that persist over collection time. For continuous values, sudden changes in statistics such as the average value can indicate for example a broken sensor or abrupt changes in the environment in which the data is col- lected. A sudden change in the number of measurements (count) over time can indicate changes in the collection protocol or a behavioral shift in the way data is collected (e.g., seasonal staff). Again note that it is often necessary to communicate with stakeholders to understand if the change is expected, needs to be dealt with by pre-processing the data, the model itself, or if the data collection need to be adjusted.\nTrends can be detected by looking for slower changes over time. This type of analysis can also be supported by trend lines. Compared to paradigm shifts, trends are for example more likely to be caused by deteriorating sensors rather than broken ones, or staff learning effects rather than a change in staffing. For continuous data, we can average the values over collection time to investigate paradigm shifts. We use line plots as the data points can be seen as connected over time. Note that changes in data uncertainty (aleatoric uncertainty [25]) might not be revealed when inspecting isolated statistics over time, such as averages, e.g., the average value is the same but the standard deviation decreases/increases. Inspecting or adding more statistics, e.g. error bars, and looking for trends in their variation over time can allow for spotting such decreasing/growing uncertainties.\nUnexpected collection patterns can be detected by looking for de- creasing/increasing, or an oscillating number of collected values. Non- constant data collection rates might have natural causes, such as weather changes resulting in an event occurring more frequently, but also indi- cators of errors in the data collection process, e.g., a flawed sensor. In many cases, the number of collected items over time should be roughly constant.\nAre there missing values, and how should they be handled?\nFirst, use summary statistics to identify if missing values are present. Missing values can be presented in a tabular format, or possibly inte- grated into the charts as separate visual elements [5]. If the cause for, or how to address, the missing values are not obvious it is necessary to identify patterns behind them. Such patterns can be found using multi- ple views or parallel coordinates [5], highlighting the corresponding values where the data is missing. Showing charts with the identified patterns to stakeholders can aid them in understanding the underlying cause of the missing value as well as the appropriate way of dealing with them. There might be valid ways of dealing with the missing values, e.g., simple replacements such as an empty string or more elab- orate imputation methods. However, missing values might also require changes in the data collection process in case the parameter is crucial to the target variable.\nCan task X be solved using the data?\nThere are many aspects to consider when solving a specific task using the data at hand. What constitutes a valid solution? Are the data assumptions valid for the task? What are the desirable properties of a method that achieves a valid solution? The more complex data and task, the harder it can be to assess the plausibility of solving task X to satisfaction by looking at the data alone. We, therefore, follow our extended scope of data readiness, which includes both a feature perspective and a solution perspective. The feature perspective considers the input and output variables that goes into a model. We break it down to the analysis of the target variable itself, the input variables' independent relationship to the target variable, and their combined impact on the target variable. From the solution perspective, on the other hand, the analysis focus on the properties of the model and its output. We break down the solution perspective to the analysis of how the model adapts to the target domain, the robustness of the solution, and the error distribution with respect to the target variable. By investigating both these perspectives we become more informed about the further needs of data acquisition and data readiness, and gain trust in the resulting deployed models/algorithms.\nFeature perspective:\nThe target variable (A1) needs to behave according to the model's assumptions. Many models that predict numerical values (regression models) assume that the target uncertainty follows a normal distribution. For example, using a square loss for a deterministic model assumes normal distributed aleatoric uncertainty. For this purpose, visualiz- ing the target distribution along with the assumed model distribution, see Figure 6 (upper left), aids in both verifying the assumption and understanding how it should be transformed if it is not the case. For example, the target may be normally distributed with a skew, which can be adjusted using a power transform [11]. Similarly, classification models can perform better with balanced labels, i.e., the number of labels for each class is equal. In this case, the model assumes a uni- form distribution. Label balancing and visualization are related to data readiness but often go beyond tabular data. We here refer the interested"}, {"title": "Needs transformation,", "content": "A single input variable (A2) needs to somehow carry information that is related to the output variable, whether it is by itself or in combination with another variable. A lack of clarity in this relationship may be grounds for revisiting the earlier data readiness levels. It is well known that aggregate correlation metrics such as Pearson/Spearman does not necessarily capture the full story of such a relationship, even though they can provide a good starting point. Therefore, one should also analyze each input's relationship with the target variable. Here, we base our reasoning on [17,34] and suggest using scatter plots for quantitative and ordinal values (violin/swarm if few categories), while violin [23] or swarm plots should be used for categorical parameters as a way to deal with overdraw. The scatter plots in Figure 6 allow for spotting trends, which generally indicate that a variable carries information about the target. Trends may also be unexpected, caused by errors in the data collection process (B5), and require revisiting earlier bands. Scatter plots can also be used for understanding uncertainties caused by outliers or increased spread for a certain interval (that later could relate to A4.6). Such findings could be grounds for re-evaluating the data collection. Automated methods for detecting outliers [22] can be used to further support the visual analysis.\nFor categorical data, a uniform distribution indicates that a category does not by itself contain information related to the target variable, while a narrow distribution indicates that it carries much information (A2). Multiple modes can indicate miss-labeled data (B5) and therefore should be split if possible (band C).\nFor text data, we map the target parameter to a visual attribute in their projection plot, e.g., color, as seen in Figure 3. Patterns revealed by the mapped visual attribute indicate that the text, or a subset of the text, is correlated with the target parameter (A2). Inspecting the patterns, and the areas where the pattern does not appear more closely can reveal both important insights and flaws that can be helpful for stakeholders in improving data collection (A5). Random patterns, on the other hand, indicate that the text cannot be used single-handedly to predict the target parameter. Thus, random patterns might require substantial changes to data collection or grounds for excluding the text as an input parameter to the model. Individual words might have domain-specific meanings for the application at hand. Thus, such words might not be well captured by the natural language processing model. It is therefore important to identify and verify how these specific words are treated and possibly revise the data transformations to better take them into account."}, {"title": "Models", "content": "Multiple input variables (A3) can have complex relationships with the target variable, which are the ones that models need to capture. Taking a starting point from core visualization techniques, we have experimented with scatter plot matrices, parallel coordinates, and mul- tiple views with scatter plots, distribution plots, and correlation metrics together with brushing and linking. While these techniques can cap- ture data readiness aspects, they tend to become custom and complex and we, therefore, recommend using them when a deeper analysis is needed. In our work together with ML practitioners, we found that feature importance techniques [41] (upper right in Figure 6) provided a better trade-off in terms of providing a visual summary while capturing complex relationships. Feature importance techniques estimate the impact (additive contribution) of each variable on the model prediction. Note that the model used for this analysis does not necessarily need to be the same as the one used for the solution and that the feature impor- tance might differ for the solution model in that case. The relationship between the feature importance of the variables can provide valuable insights into if it is possible to solve a task using the data and how the model may operate in a real-world setting. For example, if a few variables dominate the impact on the prediction (Figure 6, right) and one or more of those are unreliable, it might require the organization to improve the resilience of these variables. It is likely that robustness and generalization power is higher if more input features contribute than if only one or a few dominate. Furthermore, if the most important features (for the learned model) pose questions on how and why when presented to domain experts, further investigation is warranted to make sure that the model performance and apparent generalization power are not coincidental.\nSolution perspective:\nNaturally, analysis of the model predictions is also important for un- derstanding how well the task can be solved using the data. This involves the evaluation of errors, accuracy, and bias in concert with the model [56] and the application context. As mentioned in section 5, such analysis connecting the data with the target task is beyond the original definitions of data readiness. Here follow a non-exhaustive list of useful indicators for real-world usefulness. That is, to build trust for practitioners and stakeholders that task X likely can be solved satisfactorily using the data set and selected induction bias [6].\nAdaptation to the target domain is important for useful prediction performance (A4) since most learning methods and models are mediocre or even bad performers unless adapted to the domain. Hyperparame- ter optimization (tuning) of learning parameters allows the model to perform as well as possible in a given domain, using a validation set"}, {"title": "Preprint", "content": "as a surrogate for the true domain. This is achieved by Bayesian opti- mization (BO) [57", "8": "Optuna [1", "3": ".", "13": ".", "landscape": "How much of an improvement was possible to gain from the adaptation? How hard is it to find good configurations? Such queries can be approached using distribution plots as shown in Figure 6 (lower left). More in-depth visualization methods of hyperparameter optimization can be found in the literature [21", "49": "."}]}