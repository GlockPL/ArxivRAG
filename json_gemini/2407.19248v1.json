{"title": "Mamba-UIE: Enhancing Underwater Images with Physical Model Constraint", "authors": ["Song Zhang", "Yuqing Duan", "Dong An", "Daoliang Li", "Ran Zhao"], "abstract": "In underwater image enhancement (UIE), convolutional neural networks (CNN) have inherent limitations in modeling long-range dependencies and are less effective in recovering global features. While Transformers excel at modeling long-range dependencies, their quadratic computational complexity with increasing image resolution presents significant efficiency challenges. Additionally, most supervised learning methods lack effective physical model constraint, which can lead to insufficient realism and overfitting in generated images. To address these issues, we propose a physical model constraint-based underwater image enhancement framework, Mamba-UIE. Specifically, we decompose the input image into four components: underwater scene radiance, direct transmission map, backscatter transmission map, and global background light. These components are reassembled according to the revised underwater image formation model, and the reconstruction consistency constraint is applied between the reconstructed image and the original image, thereby achieving effective physical constraint on the underwater image enhancement process. To tackle the quadratic computational complexity of Transformers when handling long sequences, we introduce the Mamba-UIE network based on linear complexity state space models (SSM). By incorporating the Mamba in Convolution block, long-range dependencies are modeled at both the channel and spatial levels, while the CNN backbone is retained to recover local features and details. Extensive experiments on three public datasets demonstrate that our proposed Mamba-UIE outperforms existing state-of-the-art", "sections": [{"title": "1. Introduction", "content": "With the continuous exploration of marine environments, underwater imaging technology is playing an increasingly important role in visual tasks such as autonomous navigation [1], underwater target detection [2], and environmental monitoring [3]. However, due to the unique underwater environment, light inevitably undergoes attenuation as it propagates underwater, leading to the degradation and distortion of underwater images [4]. This poses significant challenges for vision-based tasks. Improving the recognizability and contrast of targets in underwater images will undoubtedly reduce the burden on downstream visual tasks. Therefore, utilizing underwater image enhancement (UIE) to restore and improve the visual quality of underwater images is crucial for enhancing the performance of downstream tasks.\nUIE is a classic yet challenging task in the field of computer vision. The unavailability of real ground truth for underwater images makes UIE an ill-posed problem. Traditional direct methods [4-7] operate directly on pixels using algorithms to correct colors and enhance contrast. Priors-based methods model the underwater imaging process based on prior knowledge, estimating model parameters to reverse the degradation process and obtain high-quality underwater images [8-10]. Both direct and priors-based methods suffer from low robustness and often exhibit over-enhancement in complex underwater scenarios.\nIn recent years, due to the rapid development of neural networks and their powerful representational capabilities, researchers have introduced convolutional neural network (CNN) to achieve nonlinear mappings from low-quality underwater images to high- quality ones [11-13]. CNN-based UIE has demonstrated impressive performance, as it can automatically learn image features and accomplish end-to-end learning. However, CNNs are limited by their restricted receptive field size, making them ineffective at capturing long-range dependencies. To address this, researchers have proposed Transformer based on self-attention mechanism to effectively extract global features [14-16]. Nonetheless, the quadratic computational complexity exhibited by Transformer when processing long sequences poses challenges for high-resolution image processing. Additionally, most supervised learning methods typically rely on the"}, {"title": "2. Related works", "content": ""}, {"title": "2.1 Transformer-based UIE", "content": "In recent years, CNN-based UIE [12, 13, 23, 24], has developed rapidly. Compared to traditional direct methods [7, 25-27] and priors-based methods [10, 28], CNN can automatically learn effective features from underwater images through end-to-end learning. However, the local receptive field mechanism of CNNs restricts them to focus only on local information in the image, making it difficult to capture global information. Solving this problem by increasing the receptive field leads to a significant increase in computational complexity. These limitations hinder the further development of CNNs in the field of image enhancement.\nTo achieve modeling of global information, Transformer-based UIE has gained attention in recent years [29-32]. Compared to CNNs, the self-attention mechanism in Transformers can capture long-range dependencies in the image, overcoming the limitations of the local receptive field inherent in traditional CNNs. Ren, et al [29]. incorporated the Swin Transformer [14] into U-Net to enhance the ability to capture global dependencies. Considering the insufficiency of Transformers in capturing local attention, they integrated CNN with core attention mechanism to capture more local attention. Similarly, Wang, et al [31]. proposed a feature fusion Transformer for global"}, {"title": "2.2 State space model", "content": "State Space Models (SSM) [17-19], originating from classical control theory, have recently been introduced into deep learning as a competitive backbone for state space transformations. The promising property of linearly scaling with sequence length in long-range dependency modeling has attracted great interest from searchers. The Structured State Space Model (S4) [18] is a pioneering deep state space model in modeling long-range dependencies. Subsequently, the S5 layer [19] was proposed based on S4, introducing multi-input, multi-output (MIMO) SSM and efficient parallel scanning. Mamba [20] is a data-dependent SSM featuring a selective mechanism and efficient hardware design, which scales linearly with input length and offers lower computational complexity compared to Transformer.\nDue to Mamba's outstanding capability in modeling long-range dependencies, it has gradually been applied in the visual tasks, such as medical image segmentation[33, 34], image restoration [35], and image classification [36, 37]. Recently, Mamba has also been applied in the UIE. Guan, et al. [38] proposed an SSM-based UIE method, WaterMamba, to address the quadratic computational complexity problem of Transformers. They embedded spatial-channel coordinate omnidirectional selective scan into U-Net to reconstruct image details and colors. Chen, et al. [39] designed a dynamic interaction-visual SSM to model global dependencies. Lin, et al. [40] proposed PixMamba, which utilizes a dual-layer network to extract both global and local feature information. These methods use Mamba for UIE, establishing global dependencies, which may lead to neglecting the modeling of local relationships. Therefore, this paper proposes combining Mamba with CNN to achieve both global and local dependency modeling."}, {"title": "2.3 Physical model constraint-based UIE", "content": "In the aforementioned deep learning methods, an end-to-end training approach is employed, directly learning the mapping relationship from inputs to labels. The effectiveness of this approach largely depends on how well the loss function constrains the model.\nIn unsupervised learning, physical prior models are fully utilized. Yan, et al. [41] proposed a model-driven cycle-consistency GAN that alleviates underwater image degradation by estimating environmental variables. Yan, et al. [42] employed neural networks to model underwater scene, estimating scene depth, scatter, attenuation and veiling light to compute underwater scene radiance. Huang, et al. [43] designed a color mapping network and a contrast mapping network, using the gray-world assumption and dark channel prior (DCP) as constraints. From the perspective of layer disentanglement, Chai, et al. [44] decomposed underwater images into different components based on physical models and applied constraints to the reconstructed images. Similarly, Khan, et al. [45] decomposed underwater images into reflection and illumination components. Based on the Koschmieder model, Kar et al. [46] found that appropriate degradation of the input image is equivalent to a controlled perturbation describing image formation. They proposed a zero-shot unsupervised training framework. Similarly, Fu, et al. [47] utilized a homology consistency between re- degraded image and input image to apply appropriate constraints to the generated clean images. In supervised learning, high-quality labels are available. By combining physical models, neural network models can be further effectively constrained, generating more natural and high-quality enhanced underwater images."}, {"title": "3. Proposed method", "content": ""}, {"title": "3.1 Preliminaries", "content": ""}, {"title": "3.1.1 Revised underwater image formation model", "content": "Our method is based on the revised underwater image formation model [48], which describes the degradation of underwater images caused by light scattering and absorption. According to the Beer-Lambert law [49], the propagation of light is related to the attenuation factor $e^{-\\beta d(x)}$, where $d$ represents the distance to the light source. $\\beta (>0)$ is the channel extinction coefficient, which increases with the density of scattering-induced particles. According to the Beer-Lambert law, the light scattering effect at a distance $d$ from the light source in a water medium can be expressed as:\n$B(x) = (1-e^{-\\Beta d(x)})A$ (1)"}, {"title": "3.1.2 State space model", "content": "The recent advancements in structured State Space Sequence Models (S4) are largely inspired by continuous Linear Time-Invariant (LTI) systems. These models map a one-dimensional function or sequence $x(t) \\in R$ to $y(t) \\in R$ through an implicit latent state $h(t) \\in R^N$. Formally, this can be represented using linear ordinary differential equation (ODE) as follows:\n$h'(t) = Ah(t)+Bx(t)$,\n$y(t) = Ch(t) + Dx(t)$. (5)\nWhere $A\\in R^{N \\times N}$, $B\\in R^{N \\times 1}$, $C \\in R^{1 \\times N}$, $D\\in R$, with $N$ being the state size.\nA discretization process is typically used to integrate Eq. (5) into practical deep learning tasks. Specifically, a time scale parameter $A$ is used to transform the continuous parameters $A$ and $B$ into discrete parameters $\\mathcal{A}$ and $\\mathcal{B}$. A common"}, {"title": "3.2 Overall architecture", "content": "Our proposed Mamba-UIE utilizes the revised underwater image formation model to effectively constrain the network. In the formation model, considering the impact of light absorption on the attenuation coefficient, the direct transmission map and the backscattering transmission map are distinguished. The backscattering transmission map is one of the main causes of underwater image blurriness. Compared to the Koschmieder light scattering model [50], the revised underwater image formation model performs better (as detailed in the Section 4.5 Ablation Study). As shown in Fig. 2, we decompose the underwater image $I(x)$ into four latent components: global background light $A$, direct transmission map $T_d(x)$, backscattering transmission map $T_B(x)$, and scene radiance $J(x)$ (i.e., the clean underwater image to be recovered). Effective regularization constraints are applied between the scene radiance and the ground truth. Additionally, in accordance with the revised formation model, the components are reorganized to obtain the reconstructed image $I'(x)$:\n$I'(x) = J(x)T_d(x)+(1-T_B(x))A$ (9)"}, {"title": "3.3 Mamba-UIE", "content": "To achieve better global dependency relationships, we designed a Mamba-based scene radiance estimation network, Mamba-UIE. In previous underwater image enhancement efforts, Transformer was often used to extract global features. While effective, Transformer has the drawback of high computational complexity. Mamba effectively addresses this issue. Considering Mamba's advantages in global feature extraction but its shortcomings in detail recovery, we integrated CNN, whose inductive biases favor image detail recovery. Therefore, we introduced Mamba as a parallel module to enhance the network model's capability in modeling long-range dependencies. In our network design, we adopted a hybrid architecture combining Mamba and CNN. This approach ensures effective extraction of global features while"}, {"title": "3.4 Transmission map estimation modules", "content": "The propagation of light underwater is influenced by water quality, depth, and suspended particles, with different wavelengths experiencing varying degrees of attenuation. The direct transmission map reflects the extent of scattering that occurs during light transmission, while the backscattering transmission map indicates the proportion of scattered light returning to the camera. Together, these two maps determine the final image quality. For estimating these transmission maps, we use identical networks, named TD-Net and TB-Net, respectively, as shown in Fig.2. TD- Net and TB-Net each contain six convolutional layers. The first layer uses a 1\u00d71 convolution kernel to quickly increase the number of channels. The last convolutional layer incorporates a SE layer for feature selection along the channel dimension. The output is achieved through a 1\u00d71 convolution followed by a Sigmoid function, with the final values mapped back to the 0-255 range. Notably, TD-Net and TB-Net do not have explicit constraints; they are optimized through the reconstruction loss based on the top layer of the framework."}, {"title": "3.5 Global Background Light Estimation Module", "content": "The global background light exists independently of the image content and reflects the global properties of the image. To improve the accuracy of background light estimation, this paper utilizes the method introduced by [52] for estimating global background light. This method notes that there is a certain correlation between manually annotated global background light and the distribution of actual images in the"}, {"title": "3.6 Loss function", "content": "A Direct loss: Our proposed Mamba-UIE primarily includes direct loss for scene radiance and reconstruction loss. Reconstruction loss emphasizes the consistency between the reconstructed image and the original image, thereby indirectly constraining the scene radiance. Direct loss mainly comprises $l_2$ loss, structural similarity (SSIM) loss $l_{SSIM}$, edge loss $l_{edge}$. $l_2$ loss compares the generated scene radiance $J$ with the reference $J_{label}$ at the pixel level, standardizing the details and textures of the generated image:\n$l_2 =|| J-J_{label} ||_2$ (14)\nWe add SSIM loss to regulate the similarity between the generated scene radiance and the reference.\n$l_{SSIM} =1-SSIM(J, J_{label})$ (15)"}, {"title": "B Reconstruction Loss", "content": "The input image is decomposed into scene radiance, global background light, direct transmission map, and backscattering transmission map. These four components are reassembled according to the formation model to obtain the reconstructed image. Applying appropriate constraints between the reconstructed image and the original image can improve the accuracy of scene radiance estimation. The reconstruction loss mainly consists of two parts: $l_2$ loss and SSIM loss. $l_2$ loss is defined as follows:\n$l_2^R =|| I - I' ||^2$ (18)\nwhere $I$ represents the original image and $I'$ represents the reconstructed image generated through formation model. The superscript $R$ denotes that it belongs to the reconstruction loss. SSIM loss is defined as follows:\n$l_{SSIM}^R =1-SSIM(I,I')$ (19)\nFinally, our total loss function is:\n$l_{total}=l_2+l_{SSIM}+\\lambda l_{edge} + l_{UIQM}+l_2^R+l_{SSIM}^R$ (20)\n$\\lambda$ is set to 0.05 according to [55]."}, {"title": "4 Experiment and analysis", "content": ""}, {"title": "4.1 Setup", "content": ""}, {"title": "4.1.1. Implementation details", "content": "The experiments were conducted on a desktop computer equipped with an Intel Core i9-10850K CPU, an NVIDIA GeForce RTX 3090 GPU with 24GB of VRAM, and 64GB of RAM. The project utilized the PyTorch framework and involved 200 training iterations. The batch size was set to 1. Our model was trained using the ADAM"}, {"title": "4.1.2. Datasets", "content": "We selected three representative datasets, UIEB [23], EUVP [56], and U45 [57], to train and test our model. These datasets are divided into two categories: 1) Full referenced datasets: 890 pairs of images from UIEB and 1,200 pairs of images from EUVP. 2) Non-referenced datasets: 60 challenging images (Challenging 60) from UIEB and 45 images from U45. To facilitate training and testing across different methods, the image sizes in the three datasets were uniformly scaled to 256\u00d7256. The dataset structure and divisions are shown in Table 1."}, {"title": "4.1.3. Compared methods", "content": "Our method was compared with the state-of-the-art methods. The traditional methods include: BTLM [52], UNTV [10], MLLE [7], HLRP [9], PCDE [58], WWPF [27], HFM [59]. Deep learning-based methods: Ucolor [13], PUIE [60], USUIR [47], U-shape [32], UDAformer [30], LiteEnhance [61], DICAM [62]. To ensure a fair comparison, all parameter settings in the comparison models follow the configurations provided in the original papers, except for necessary modifications related to image size."}, {"title": "4.1.4. Evaluation metrics", "content": "The full reference evaluation metrics and the non-reference evaluation metric were used to quantitatively evaluate and analyze the generated enhanced images. Full- reference evaluation metrics include MSE, PSNR [63], and SSIM [53]. MSE compares the differences between the generated image and the reference pixel by pixel. PSNR is a metric based on MSE that represents the difference between the generated image and the GT. SSIM compares images at the block level based on brightness, contrast, and structure. A smaller MSE value indicates better image quality, while higher PSNR and SSIM values indicate better image quality."}, {"title": "4.2 Quantitative evaluation", "content": "Firstly, we conduct comparisons on full referenced datasets. The performance of different methods on the UIEB and EUVP datasets is shown in Table 3. It is worth noting that PUIE requires four types of labels, which are only provided for UIEB. Due to the lack of corresponding labels, we did not implement PUIE on EUVP. On the UIEB dataset, our proposed Mamba-UIE achieved the best results in MSE and PSNR metrics, and the second-best result in SSIM. On the EUVP dataset, Mamba-UIE achieved the second-best result in SSIM and the third-best results in MSE and PSNR. As for the UIQM metric, although Mamba-UIE did not rank in the top three, its performance was still quite good."}, {"title": "4.3 Qualitative evaluation", "content": "Due to the complexity of underwater environments, different degradation scenarios pose various challenges for underwater image enhancement. To validate the reliability and robustness of Mamba-UIE, we conducted qualitative analyses for four common underwater degradation scenarios. These scenarios include color offset scenes (Fig. 4 raw), blurry scene (Fig. 5 raw), scattering scenes (Fig. 6 raw), and extreme degradation scenes (Fig. 7 raw). The enhanced images generated by different methods are shown in Figs. 4-7.\nThe degree of light absorption in water depends on its wavelength. Red light attenuates the fastest, while blue light attenuates the slowest. This attenuation causes color offset in images, leading to green or blue phenomena, which are common degradation scenarios. As shown in Fig. 4, for underwater green scenes, most methods except BTLM, UNTV, HLRP, PCDE, HFM, USUIR, and LiteEnhance can effectively improve the underwater green scene. For underwater blue scenes, methods such as Ucolor, PUIE, U-shape, UDAformer, LiteEnhance, DICAM, and Mamba-UIE can effectively restore the colors of underwater images. However, methods like MLLE, PCDE, and WWPF exhibit excessive enhancement."}, {"title": "4.4 White balance test", "content": "To further demonstrate the performance of the proposed Mamba-UIE in color restoration, we conducted a white balance test. The test was performed on the Color- Checker7 [65] dataset, which consists of images taken by divers holding a standard color card in a swimming pool. The results are shown in Fig. 8. In this test, traditional"}, {"title": "4.5 Ablation study", "content": ""}, {"title": "4.5.1 Effectiveness of physical model constraint", "content": "Our proposed Mamba-UIE framework consists of two main components: the introduced physical model constraint and the Mamba-UIE network. To demonstrate the effectiveness of the introduced physical model constraint, we conducted ablation experiments. As shown in Table 4, even without the Mamba component, the proposed framework still achieves good performance, and the inclusion of Mamba further enhances the performance."}, {"title": "4.5.2 Constraint based on different formation models", "content": "Our proposed Mamba-UIE is constrained by physical formation principles, skillfully combining deep learning methods with prior knowledge. We explored the effectiveness of Mamba-UIE based on different formation model constraints, specifically including the Koschmieder light scattering model [50]:\n$I(x) = J(x)T(x)+(1-T(x))A$ (21)\nIn the Koschmieder light scattering model, direct scattering and backscattering images are not distinguished. A unified transmission map $T(x)$ is used. This represents a simplified underwater image formation model.\nRetinex model [66]:\n$I(x) = R(x)L(x)$ (22)\nThe Retinex model assumes that an underwater image can be represented as the product of illumination $L(x)$ and reflection $R(x)$. and, $R(x)$ is the clean underwater image that we aim to restore.\nJaffe-McGlamery [67, 68] model:\n$I(x) = J(x)T(x)+(1-T(x))A+(J(x)T(x))*g(x)$ (23)\nThe Jaffe-McGlamery model suggests that an underwater image consists of three components: direct projection $J(x)T(x)$, forward scattering $(1-T(x))A$, and background scattering $(J(x)T(x))*g(x)$, where $*$ denotes convolution and $g(x)$ represents the point spread function. We used a trainable 9\u00d79 convolutional kernel as a replacement for $g(x)$. Ablation experiments based on different physical model constraints are shown in Table 5."}, {"title": "5 Conclusion", "content": "We propose a physical model constraint-based underwater image enhancement framework, Mamba-UIE. Specifically, we decompose the input image into four components: underwater scene radiance, direct transmission map, backscatter transmission map, and global background light. These components are reassembled according to the revised underwater image formation model, and a reconstruction consistency constraint is applied between the reconstructed image and the original image, thereby achieving effective physical constraint on the underwater image"}, {"title": "CRediT authorship contribution statement", "content": "Song Zhang: Writing-original draft, Visualization, Validation, Software, Methodology, Conceptualization. Yuqing Duan: Visualization, Validation, Software. Dong An: Writing-review & editing, Methodology. Daoliang Li: Writing-review & editing, Funding acquisition. Ran Zhao: Writing-review & editing, Supervision, Methodology, Funding acquisition, Conceptualization."}, {"title": "Declaration of competing interest", "content": "The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper."}, {"title": "Data availability", "content": "Data will be made available on request."}]}