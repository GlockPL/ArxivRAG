{"title": "Neural Quantum Propagators for Driven-Dissipative Quantum Dynamics", "authors": ["Jiaji Zhang", "Carlos L. Benavides-Riveros", "Lipeng Chen"], "abstract": "Describing the dynamics of strong-laser driven open quantum systems is a very challenging task that requires the solution of highly involved equations of motion. While machine learning techniques are being applied with some success to simulate the time evolution of individual quantum states, their use to approximate time-dependent operators (that can evolve various states) remains largely unexplored. In this work, we develop driven neural quantum propagators (NQP), a universal neural network framework that solves driven-dissipative quantum dynamics by approximating propagators rather than wavefunctions or density matrices. NQP can handle arbitrary initial quantum states, adapt to various external fields, and simulate long-time dynamics, even when trained on far shorter time windows. Furthermore, by appropriately configuring the external fields, our trained NQP can be transferred to systems governed by different Hamiltonians. We demonstrate the effectiveness of our approach by studying the spin-boson and the three-state transition Gamma models.", "sections": [{"title": null, "content": "Thanks to rapid advances in laser technology, researchers can now manipulate quantum pathways by fine-tuning the properties of strong laser fields, which allows for the amplification of specific signals that would otherwise be too weak to detect using conventional methods [1-3]. These developments in strong field spectroscopy provide valuable insights into complex molecular systems, significantly improving the signal-to-noise ratio and leading to more precise observations [4-7]. Numerical methods for solving the corresponding equations of motion (EOM) generally fall into two categories: iterative-based approaches, such as Runge-Kutta, and methods based on the time-dependent variational principle, which minimizes the residual of the EOM solution [8-14]. However, despite varying levels of sophistication, these algorithms are often hindered by the high computational cost of iteratively propagating the dynamics.\nThe rise of machine learning has opened new avenues for theoretical simulations of chemical systems [15-18]. Among these advancements, deep neural networks are now routinely used to approximate force fields in molecular dynamics, neural functionals in density functional theory, or potential energy surfaces in ab initio calculations, offering significantly reduced computational costs while maintaining good accuracy [19-27]. Recently, there has been a growing interest in using operator learning, where neural networks act as surrogate maps for the solution operators of partial differential equations, accommodating a wide range of initial and boundary conditions [28, 29]. A notable example is the Fourier Neural Operator (FNO), originally developed to solve the Navier-Stokes equations [30, 31]. Although FNO has been successfully applied to various classical continuous systems, its potential to directly solve quantum dynamics equations remains largely unexplored, and to the best of our knowledge, it has only been applied to scattering problems [32, 33].\nIn this Letter, we tackle the challenge of constructing neural operators for quantum dynamics and develop a Neural Quantum Propagator (NQP) model for driven-dissipative quantum systems. Originally designed as a solver for Markovian quantum master equations (QME), the NQP has already been applied to model population dynamics and compute various response functions for time-independent (i.e., without external driving forces) Hamiltonians [34]. Here, we aim to overcome this limitation by designing a new architecture for NQP that incorporates external driving fields as additional input. By using the QME as the EOM for the system, our NQP acts as a universal surrogate solver applicable to arbitrary initial states and a wide range of external fields. Unlike conventional FNO, which assumes classical, local interactions, our concept of NQP explicitly focuses on quantum dynamics involving thus non-local quantum effects. Notably, due to its rigorous adherence to the composition property of quantum propagators (an aspect rarely explored in the literature of operator learning [35-37]), NQP can predict long-time dynamics, far beyond the training time window. Moreover, we demonstrate that our model can be easily transferred to various systems governed by different Hamiltonians.\nThe remainder of the paper is organized as follows. First, we present the QME for driven-dissipative quantum systems along with the key technical details of our NQP architecture. We then discuss the training scheme and test the NQP numerical performance by using the spin-boson and the three-state Gamma models. Finally, we offer some conclusions."}, {"title": "Driven-dissipative quantum dynamics.", "content": "\u2014 In this work, we are interested in open quantum systems driven by external fields. The Hamiltonian is written as\n$$H(t) = H_0 + \\sum_{k=1}^K f_k(t) F_k,$$\nwhere the first term is the system Hamiltonian:\n$$H_0 = \\sum_j \\varepsilon_j |j\\rangle \\langle j| + \\sum_{j \\neq j'} \\Delta_{jj'} |j\\rangle \\langle j'|.$$"}, {"title": null, "content": "with $\\varepsilon_j$ being the state energies and $\\Delta_{jj'}$ the interstate couplings. In the second term, $f_k(t)$ is a scalar function and $F_k$, the operator part of the k-th external field. The overall system is further interacted with a set of Markovian heat baths and the EOM is described by the QME [38-40]:\n$$i\\dot{\\rho}(t) = -\\frac{1}{\\hbar} [H(t), \\rho(t)] - \\sum_j \\mathcal{D}_j[\\rho(t)],$$\nwhere $\\rho(t)$ is the density operator of the system, and\n$$\\mathcal{D}_j [\\rho(t)] = \\gamma_j (V_j V_j^\\dagger \\rho(t) + \\rho(t) V_j V_j^\\dagger - 2 V_j^\\dagger \\rho(t) V_j )$$\nwith $\\gamma_j$ and $V_j$ being the coupling strength and the operator function of the j-th heat bath, respectively. In the presence of an external field, the time-dependent propagator can be defined through the equation:\n$$\\rho(t) = \\mathcal{T}_+ \\exp \\left[ \\int_{t_0}^t ds \\mathcal{L}(s) \\right] \\rho(t_0),$$\nwhere $\\mathcal{T}_+$ denotes the time-ordered operator and the super-operator $\\mathcal{L}(s)$ corresponds to the right-hand side of Eq. (3).\nWe now define a discrete time grid with a time step $\\delta t$ and $t_n = n \\delta t$. The k-th external field at $t_n$ is denoted by $f_k^n = f_k(t_n)$. All K external fields from $t_m$ to $t_n$ can thus be represented as a matrix,\n$$f^{n,m} = \\begin{pmatrix}\nf_1^m & f_2^m & \\dots & f_K^m \\\\\nf_1^{m+1} & f_2^{m+1} & \\dots & f_K^{m+1} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nf_1^n & f_2^n & \\dots & f_K^n\n\\end{pmatrix},$$\nwhich is further aligned as a vector and used as the input of the neural network.\nNext, we introduce the collective index $x = (j, j')$ and align the density operator as a vector, $\\hat{\\rho}(t_n) \\sim \\rho_n = {\\rho(x_0, t_n), \\rho(x_1,t_n),\\dots}$, with $\\rho(x,t) = \\langle j |\\rho(t)| j' \\rangle$. Formally this vectorization is identical to using the twin-space or Liouville space representation [41]. The propagator defined in Eq. (5) can be recast into the matrix-vector form as\n$$\\rho_n = \\mathcal{G} \\left[f^{n,m}; \\rho_m \\right].$$\nHere the propagator $\\mathcal{G}$ can be regarded as a functional that projects the given initial state $\\rho_m$ and the external field $f^{n,m}$ to their corresponding final state $\\rho_n$ satisfying Eq. (3). As a consequence of the linearity of quantum mechanics, the composition property holds and reads\n$$\\rho_{n_2} = \\mathcal{G} \\left[f^{n_2, n_1}; \\mathcal{G} \\left[f^{n_1, n_0}; \\rho_{n_0} \\right] \\right].$$\nAlthough our discussion is limited to the Markovian assumption, the same protocol can be easily extended to"}, {"title": "Neural Quantum Propagators with driven fields.", "content": "\u2014 To construct the NQP for driven dynamics, we follow the adaptive FNO transformer [43] and stochastic Fourier Differential Equation [44] architectures. Our model thus takes an arbitrary initial state $\\rho_0$ and external field $f^{n,0}$ as the input, and outputs the target $\\rho_n$, following Eq. (7). The upper time limit of the model is chosen as $n \\leq N_t$ ($N_t = t_{\\text{max}}/\\delta t$). For ease of numerical construction, we limit n to be an integer but higher resolution can be obtained by either reducing the time step or adopting the super-resolution algorithm. Crucially, because of the composition property of quantum propagators, long-time dynamics outside this time interval can be obtained by recursively applying the propagator.\nFig. 1 sketches the architecture of our driven NQP model. In panel (a), $P_{in}$ and $P_{out}$ are linear feedforward neural networks that serve as the projection between the physical space and the latent Fourier space. As shown in panel (b), the l-th Fourier layer performs the following operation:\n$$v_{l+1} = \\sigma \\left(v_l + \\mathcal{F}^{-1} \\left[ \\mathcal{W}_l (\\mathcal{F}[v_l] + P f^{n,0}) \\right] \\right).$$"}, {"title": null, "content": "The model first performs the Fourier transform on $v_l$, which is the output of the previous layer or $P_{in}$. It then passes the external field vector $f^{n,0}$ to another linear feedforward neural network $P_l$. The result of these two routes is then added together and passed to the point-wise convolution $\\mathcal{W}_l$, which serves as learnable parameters on the Fourier space. It is then followed by the"}, {"title": "Physics-informed training algorithm.", "content": "\u2014 To train the NQP model, we adopt the physics-informed training algorithm and define the loss function\n$$\\mathcal{L} = \\alpha \\mathcal{L}_{data} + (1 - \\alpha) \\mathcal{L}_{phys},$$\nwhere $\\mathcal{L}_{data}$ and $\\mathcal{L}_{phys}$ are the data and physics-informed loss functions, and $\\alpha$ is a hyper-parameter that will be dynamically adjusted during the training process.\nThe first term $\\mathcal{L}_{data}$ is evaluated from a prepared training dataset, which contains in total $N_{data}$ samples. The p-th sample of the dataset is composed of $f^{n,0}_{(p)}$, an external field vector, and $\\rho_n^{(p)}$, the corresponding dynamical states. $\\mathcal{L}_{data}$ is thus defined as\n$$\\mathcal{L}_{data} = \\frac{1}{N_{data} N_t} \\sum_{p=1}^{N_{data}} \\sum_{n=0}^{N_t} ||\\hat{\\rho}_n - \\rho_n^{(p)}||_F^2,$$\nwhere $||\\cdot||_F$ denotes the Frobenius norm, $\\hat{\\rho}_n$ is the dynamical state for the p-th sample predicted by the model via the equation\n$$\\hat{\\rho}_n^{(p)} = \\mathcal{G}_{\\Theta} \\left[ f^{n,0}_{(p)}; \\rho_0^{(p)} \\right],$$\nwhere $\\mathcal{G}_{\\Theta}$ is the aforementioned NQP model (with $\\Theta$ being the entire set of learnable parameters) and $\\rho_0^{(p)}$ is the initial state for the p-th sample. This training dataset is prepared by solving the time evolution using some conventional iterative method.\nThe second term $\\mathcal{L}_{phys}$ is defined as the residual of Eq. (3) [45]:\n$$\\mathcal{L}_{phys} = \\frac{1}{N_{phys} N_t} \\sum_{p=1}^{N_{phys}} \\sum_{n=0}^{N_t} || \\partial_t \\hat{\\rho}_n - \\mathcal{L}_n^{(p)} [\\hat{\\rho}_n] ||,$$\nwhere $\\mathcal{L}_n$ represents the right-hand side (time-dependent) operator of Eq. (3). To evaluate such a cost function, we have introduced a physics dataset with in total $N_{phys}$ samples. Interestingly, compared to $\\mathcal{L}_{data}$,"}, {"title": null, "content": "the evaluation of $\\mathcal{L}_{phys}$ requires only the initial states. In fact, the explicit expression of each dynamical state $\\hat{\\rho}_n^{(p)}$ in Eq. (13) can be avoided by using the predicted propagator $\\mathcal{G}_{\\Theta}$ that relates this state with the initial one $\\rho_0 \\sim \\rho(0)$, as indicated in Eq. (12).\nWe now present numerical results for the spin-boson system and three-state transition Gamma system. The accuracy of the model is demonstrated by comparing the time evolution of the density operator with the result from the fourth-order Runge-Kutta method (RK4).\nThe spin-boson system, relevant for quantum control of pulse reverse engineering and controllable dissipative dynamics [9, 46, 47], is governed by the Hamiltonian:\n$$H_0 = \\frac{\\omega_x}{2} (|e\\rangle \\langle e| - |g\\rangle \\langle g|) + \\omega_x (|e\\rangle \\langle g| + |g\\rangle \\langle e|).$$\nWe assume two heat baths: $V_1 = |e\\rangle \\langle g|$ and $V_2 = |g\\rangle \\langle e|$, describing the absorption and emission processes. For the external field, we assume only one field, with $F = |e\\rangle \\langle e|$, and $f(t) = e^{i \\omega_f t}$. In what follows, we choose $\\omega_f = \\omega_0$ and use it as the unit for all the other parameters. The other parameters are chosen as $\\omega_x = 0.5$, $\\gamma_1 = 0.1$ and $\\gamma_2 = 0.2$, respectively. The range of $\\omega_f$ is chosen as $\\omega_f \\in (0.2, 1.0)$. For the time parameters, we let $\\delta t = 0.05$. The time limit $t_{\\text{max}}$, associated with time point $N_t$, plays an important role in the performance of the model and is discussed in the Appendix. We sample the initial states to evaluate $\\mathcal{L}_{phys}$ (13) from the Gaussian Unitary Ensemble as explained in more detail also in the Appendix.\nWe choose $t_{\\text{max}} = 20$ with $N_t = 400$ and the initial state $\\rho(0) = |g\\rangle \\langle g|$. We focus on the time evolution of the density operator up to $t = 80$ by comparing it with the reference result obtained from the RK4 using a time step of 0.05. In Fig. 2, we present the dynamics of populations $p_j(t) = \\langle j|\\rho(t)|j \\rangle$ ($j = g,e$) and coherence $\\rho_{eg}(t) = \\langle e|\\rho(t)|g \\rangle$ at $\\omega_f =$ (a) 0.2, (b) 0.4, (c) 0.6, and (d) 1.0, respectively, representing the typical slow, intermediate, and fast modulation forces. As shown in Fig. 2, the system exhibits different dynamics within $t \\leq 20$ at these four typical $\\omega_f$ cases. Notice that our model can not only predict the time evolution up to $t_{\\text{max}}$ with high accuracy but also produce the correct long-time coherent dynamics for time far beyond $t_{\\text{max}}$."}, {"title": "Universality.", "content": "\u2014 One of the main features of the NQP introduced in this Letter is its universality, which renders it applicable to different Hamiltonians. This is established by assuming the constant force, $f_k(t) = c_k$, with $c_k \\in (c_{k,\\text{min}}, c_{k, \\text{max}})$. The effective system Hamiltonian now becomes $\\hat{H}_c = \\hat{H}_0 + \\sum_k c_k F_k$. Our trained model can be easily employed to predict dynamics for all $\\hat{H}_c$. This significantly improves the transferability over previously developed models, which, being limited to a specific system, have to be re-trained when applied to a different one [34]. To test the performance of our NQP model"}, {"title": "Three-state Gamma system.", "content": "\u2014 This model plays an important role in stimulated Raman adiabatic population transfer [48, 49]. The system Hamiltonian is defined as\n$$H_{c_1, c_3} = \\sum_{j=1}^3 \\omega_j |j \\rangle \\langle j| + c_1 (|1\\rangle \\langle 2| + |2\\rangle \\langle 1|) + c_3 (|2\\rangle \\langle 3| + |3\\rangle \\langle 2|).$$\nWe treat the first term as $\\hat{H}_0$ and the rest as constant external fields. The state $|2 \\rangle$ is the transition state between $|1 \\rangle$ and $|3 \\rangle$. The transition between different states can be tuned by varying the interstate couplings $c_1$ and $c_3$. The system-bath coupling operator is chosen as $V_j = |jj|$ for $j = 1 \\sim 3$, respectively. We set the energies of three states as $\\omega_1 = 0.0$, $\\omega_2 = 0.1$, and $\\omega_3 = 1.0$. For the heat baths, we set $\\gamma_1 = \\gamma_3 = 0.1$, $\\gamma_2 = 0.2$, assuming a stronger dissipation for the transition state. The range of $c_1, c_3$ is chosen as $c_1, c_3 \\in (0.2, 0.8)$.\nWe choose $t_{\\text{max}} = 2$, $\\delta t = 0.05$, and the initial state $\\hat{\\rho}(0) = |1\\rangle \\langle 1|$. In Fig. 3, we show population dynamics up to $t = 40$ for $c_1 = 0.3$ and $c_3 =$ (a) 0.2, (b) 0.4, (c) 0.6, and (d) 0.8, respectively. For all cases, our NQP model predicts accurate dynamics at times far beyond $t_{\\text{max}}$, demonstrating the adaptability of the model on different Hamiltonians. In addition, we found that changing $t_{\\text{max}}$ in the range of 0.5 \\sim 10 does not show any apparent differences in the performance of the model. This"}, {"title": "Conclusion.", "content": "In summary, we developed an NQP model\u2014a universal neural network architecture that treats external fields as additional inputs and can handle arbitrary initial quantum states\u2014for simulating driven-dissipative quantum dynamics. It is striking that the trained NQP can predict quite a long-time dynamics for different external fields far beyond the training time window. We also showed that, by appropriately configuring the external fields, the model can be transferred to systems described by different Hamiltonians. These results highlight the flexibility and expressibility of our NQP model, rendering it a powerful tool for studying quantum dynamical problems.\nOne of the main issues of the NQP model is that the number of learnable parameters scales exponentially with the system size [29]. Similar to other operator-learning frameworks, this scaling problem arises from the fact that the model approximates a functional of inputs with high dimensionality [30]. To reduce computational costs in future applications, one potential solution is to represent the density matrix using tensor network structures, which are known for their polynomial scaling [50-54]. Alternatively, insights can be drawn from exact expressions for steady states [55]. Finally, another promising avenue of research is to reverse the learning protocol, using the learned propagators for quantum control problems, crucial for the development of quantum technologies [56, 57]."}, {"title": "Preparation of the training set.", "content": "\u2014 We briefly introduce the preparation of data and physics set used in our experiments. The initial state $\\rho_0 \\sim \\rho(0)$ is randomly sampled from the Gaussian Unitary Ensemble as Hermitian matrix. A sigmoid scaling is conducted for the diagonal entries to have trace-norm,\n$$\\langle j | \\rho | j \\rangle \\longrightarrow \\frac{\\langle j | \\rho | j \\rangle}{\\left( \\sum_i \\langle i | \\rho | i \\rangle \\right)}.$$\nThe external field vector $f^{n,0}$ is prepared by first selecting a specific function form for f(t), and then randomly choosing the related parameters within chosen intervals. For better understanding, we use periodic forces as $f(t) = \\exp(i\\omega_f t)$. The random sampling is conducted by first choosing $\\omega_f$ as a uniform random number within the range $\\omega_f \\in (\\omega_{\\text{min}}, \\omega_{\\text{max}})$. Finally, the vector $f^{n,0}$ is obtained by embedding f(t) to the time grid $t_n$."}, {"title": "Computational Details.", "content": "\u2014 The hyper-parameters and training setup of the NQP model are chosen as follows. $P_{\\text{in}}$, $P_{\\text{out}}$ and $P_i$ are parameterized as 2-layer neural networks with a hidden channel of size 256. For $\\mathcal{W}_l$, we use 4 Fourier layers, each of which has a hidden channel of size 128. All the Fourier modes are explicitly included in the model. The total number of trainable parameters is around 2 millions. We prepare the training dataset by randomly sampling $N_{\\text{data}} = 2000$ initial states and external field parameters, and integrating the EOM with the RK4. The physics dataset has a size of $N_{\\text{phys}} = 200$, and is regenerated at each epoch. The model is trained by optimizing Eq. 10 (see main text) with $\\alpha = 0.5$ using the Adam optimizer and a learning rate of $10^{-4}$. The training is conducted for $10^4$ epochs on a single Nvidia 4090 GPU card within 5 hour until the loss function reaches $\\sim 10^{-4}$. The trained model can then be applied to any initial states and any $\\omega_f \\in (0.2, 1.0)$."}, {"title": "Performance outside the training window.", "content": "\u2014 We note that the performance of the model strongly depends on the choice of the maximum time $t_{\\text{max}}$, which should be, in principle, large enough. To test the effect of $t_{\\text{max}}$ on the model's performance, we retrained the model using smaller $t_{\\text{max}}$. We show the population dynamics $p_g(t)$ up to $t = 40$ evaluated from the NQP model with $t_{\\text{max}} = 5$ and 10 in Fig. 4. The other settings are the same as the case of $t_{\\text{max}} = 20$ in Fig. 2 (see main text). As shown in Fig. 4, the trained models can always predict the correct dynamics within the training time window $t_{\\text{max}}$. For $t_{\\text{max}} = 5$, the model incorrectly predict dynamics at $t > t_{\\text{max}}$ for all $\\omega_f$. We found that employing a larger model by increasing the number of layers to 15 and channels to 512 merely leads to longer training time, but cannot improve long-time dynamics outside the training window. Increasing $t_{\\text{max}}$ to 10, the performance of the model improves with the increase of $\\omega_f$, yielding almost correct dynamics for the cases of $\\omega_f = 0.6$ and 1.0. It is thus expected that the optimal choice of $t_{\\text{max}}$ is determined by the range of $\\omega_f$, which corresponds to the modulation of the external fields. This hyper-parameter has to be carefully chosen in order to get better performance."}]}