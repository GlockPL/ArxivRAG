{"title": "AdaMixup: A Dynamic Defense Framework for Membership Inference Attack Mitigation", "authors": ["Ying Chen", "Jiajing Chen", "Yijie Weng", "ChiaHua Chang", "Dezhi Yue", "Guanbiao Lin"], "abstract": "Membership inference attacks have emerged as a significant privacy concern in the training of deep learning models, where attackers can infer whether a data point was part of the training set based on the model's outputs. To address this challenge, we propose a novel defense mechanism, AdaMixup. AdaMixup employs adaptive mixup techniques to enhance the model's robustness against membership inference attacks by dynamically adjusting the mixup strategy during training. This method not only improves the model's privacy protection but also maintains high performance. Experimental results across multiple datasets demonstrate that AdaMixup significantly reduces the risk of membership inference attacks while achieving a favorable trade-off between defensive efficiency and model accuracy. This research provides an effective solution for data privacy protection and lays the groundwork for future advancements in mixup training methods.", "sections": [{"title": "1. INTRODUCTION", "content": "As deep learning models become ubiquitous across domains such as healthcare, finance, and autonomous systems, concerns regarding the privacy of training data have intensified. A particularly troubling vulnerability is the susceptibility of these models to membership inference attacks (MIA). In such attacks, an adversary seeks to determine whether a specific data instance was part of the training set by analyzing the model's outputs. This vulnerability is exacerbated by the tendency of deep learning models to overfit to training data, especially when dealing with sensitive information. Consequently, MIAs pose a serious threat to data privacy in machine learning systems.\nThe core issue driving membership inference attacks lies in the disparity between a model's performance on training data versus data unseen during training. Deep neural networks, especially when over-parameterized, often demonstrate high confidence on training samples, which inadvertently provides attackers with exploitable signals to differentiate between members and non-members of the training set. As a result, mitigating overfitting has become a key focus in defending against MIAs. A range of defenses have been proposed to address this challenge, with techniques such as differential privacy, regularization, and Memguard being the most prominent. However, many of these methods come with inherent trade-offs. Differential privacy, for example, introduces noise to protect data privacy, often at the cost of model utility.\nTo overcome limitations, mixup training improves model generalization and reduces overfitting. Traditional mixup uses a fixed ratio, which may not be optimal. We propose AdaMixup, which adapts the mixup ratio based on model performance. AdaMixup balances accuracy and privacy protection, enhancing generalization and MIA defense.\nThe main contributions of our work can be summarized as follows:\n\u2022 We introduce AdaMixup, a novel defense method that dynamically adjusts the mixup ratio during training, enhancing model robustness against membership inference attacks.\n\u2022 We achieve a balance between defending against membership inference attacks and maintaining high model accuracy with AdaMixup."}, {"title": "2. BACKGROUND", "content": "In this section, we introduce various existing types of membership inference attacks. Finally, we summarize various existing state-of-the-art defense methods."}, {"title": "2.1 Existing Membership Inference Attacks", "content": "A membership inference attack aims to determine if a data point was used to train a specific machine learning model, threatening individual privacy. Due to growing concerns, many attack techniques have emerged. This paper categorizes these attacks, which are also key to the defense strategies discussed."}, {"title": "2.1.1 Attacks Based on Confidence Scores.", "content": "An ubiquitous manifestation of Membership Inference Attack (MIA) is the confidence-driven approach, initially unveiled by Shokri et al. in their groundbreaking 2017 research, and then further extended by Salem et al. This assault modality exploits the confidence ratings (equivalent to prediction probabilities) furnished by the model to deduce membership status.\nFormally, Prediction confidence corresponding to training samples F(x)y is typically higher than prediction confidence for testing samples. Therefore, confidence-based attack will only regard the queried sample as a member when the prediction confidence is larger than either a class-dependent threshold \\$\\tau_y\\$ or a class-independent threshold \\$\\tau$\n\nI_{conf} (F(x), y) = {F(x)_y \u2265 \\tau(y)}\t\t\t(1)"}, {"title": "2.1.2 Attacks Based on Labels.", "content": "In 2021, Choquette et al. introduced Label-only Membership Inference Attacks, exploiting model responses to input alterations. They noted models are more consistent in predicting training data labels. Attackers use this predictability to deduce data membership. By slightly perturbing inputs and observing label predictions, they calculate the probability of label consistency. Formally, given an input x and its true label y, the adversary applies a small perturbation d to generate x' = x + d. The model's predicted label \u0177 (x') is observed, and the process is repeated across multiple perturbations. The probability that the label remains unchanged is calculated as:\n\nP (\u0177 (x') ) = y) = \\frac{1}{N} \\sum_{i=1} 1 {\u0177 (x) = y}\n\nWhere N is the number of perturbations, and 1{\u00b7} is the indicator function."}, {"title": "2.2 Existing Defense Methods", "content": "Existing defenses against membership inference attacks include differential privacy, regularization techniques, and mixup methods. However, these defenses have some drawbacks. Differential privacy, while effective in providing privacy guarantees, can often lead to a trade-off with model performance, resulting in reduced accuracy. Regularization techniques, such as L2 regularization, can help mitigate overfitting, but they may not be sufficient to fully protect against sophisticated attacks. Mixup methods, which augment the training data by interpolating between samples, can improve generalization, but they may also suffer from limitations, such as using fixed interpolation ratios that may not be optimal for all datasets and training stages. Therefore, there is a need for continued research and development of more effective and efficient defenses against membership inference attacks."}, {"title": "3. METHODS", "content": "In this section, we introduce AdaMixup, a defense strategy against membership inference attacks. AdaMixup integrates adaptive weight generation and label allocation, dynamically adjusting sample influence during training to boost model robustness. By contrast to traditional Mixup with a fixed \\$\\lambda\\$, AdaMixup adapts \\$\\lambda\\$ over epochs, preventing label distortion and improving classification performance, all while preserving high accuracy."}, {"title": "3.1 Phasel: Adaptive Dynamic Weight Generation", "content": "In standard Mixup, \\$\\lambda\\$ is randomly sampled from a Beta distribution, which results in a uniform and uncontrolled contribution of samples throughout training. This method, while effective in regularization, may lead to suboptimal performance as the training progresses, particularly in later stages where precise classification is more critical.\nIn AdaMixup, \\$\\lambda\\$ is initially large to ensure substantial mixing between samples during early training. As the training process proceeds, \\$\\lambda\\$ is gradually reduced, allowing the model to learn from individual, unaltered samples in later stages. This strategy balances the need for regularization in early epochs with the necessity of fine-grained learning as the model converges. We formalize the decay of \\$\\lambda\\$ over time as follows:\n\n\\lambda_t = \\lambda_{initial} \\left(1 - \\frac{t}{T}\\right)\t\t\t\t\t\t(3)\n\nWhere: \\$\\lambda_t\\$ is the mixing coefficient at epoch t, \\$\\lambda_{initial}\\$ is the initial \\$\\lambda\\$ value (typically close to 1), T is the total number of training epochs.\nThis linear decay ensures that in the early epochs, the model is regularized by strong mixing, which helps to smooth decision boundaries and mitigate overfitting. As training progresses, \\$\\lambda\\$ decreases, allowing the model to focus on learning precise representations from the original data samples. The gradual reduction in mixing intensity prevents over-regularization and ensures the model can refine decision boundaries without excessive distortion from mixed samples."}, {"title": "3.2 Phase2: Adaptive Label Allocation", "content": "A common criticism of Mixup training is the blending of labels, which can introduce ambiguity, particularly when the samples come from significantly different classes. This blending may degrade the model's performance on classification tasks, as the labels may no longer align with the semantic meaning of the original samples. To address this issue, AdaMixup incorporates an adaptive label assignment strategy, ensuring that the label assigned to the mixed sample is consistent with the sample that contributes more significantly to the mixture.\nGiven two samples, x1 and x2, with corresponding one-hot encoded labels, y1 and y2, the mixed sample \\tilde{x} is defined as:\n\n\\tilde{x} = \\lambda_t \u2022 x_1 + (1 - \\lambda_t) \u2022 x_2 \t\t\t\t\t\t\t(4)\n\nHowever, unlike traditional Mixup, where the mixed label \\tilde{y} is a weighted combination of y1 and y2, AdaMixup ensures that the label is determined by the dominant sample in the mixture:\n\n\\tilde{y} = \\begin{cases} y_1, & \\text{if } \\lambda_t \u2265 0.5 \\\\ y_2, & \\text{if } \\lambda_t < 0.5 \\end{cases}\t\t\t\t\t\t\t\t\t(5)\n\nThis approach maintains label consistency, which is critical for classification tasks where the label must reflect the primary content of the input. By adapting the label based on the larger contribution in the mixture, AdaMixup avoids the inaccuracies introduced by label interpolation, thus improving classification performance while still benefiting from the regularization effects of Mixup."}, {"title": "4. EXPERIMENTAL RESULTS", "content": "In this section, we present the experimental validation of our proposed AdaMixup method. We evaluate its effectiveness in defending against membership inference attacks and compare its performance to other established defense mechanisms."}, {"title": "4.1 Datasets and Experimental Details", "content": "In this paper, we conducted experiments on four datasets: MNIST, CIFAR-10, LFW, and STL-10 to verify the superiority of our method. We conducted experiments with consistent settings, using a batch size of 128, a learning rate of 0.001 with Adam, and 100/50 epochs for CIFAR-10/STL-10 and MNIST/LFW. The AdaMixup mixup ratio decayed from 1.0 to 0.1. Each experiment was repeated five times. We focused on defending against two confidence-based attacks (A1, A2) by Salem et al.and one label-based attack (A3) by Choquette et al., providing a comprehensive evaluation."}, {"title": "4.2 Defense Methods for Comparison", "content": "To evaluate AdaMixup, we compared it with Differential Privacy (e=1.0), Dropout (rate=0.5), L1/L2 Regularization, Memguard, and standard Mixup. Each defense has unique strategies to mitigate membership inference attacks."}, {"title": "4.3 Analysis of Results", "content": "We tested AdaMixup on MNIST, CIFAR-10, LFW, and STL-10 datasets. Results in Figure2 and Tablel show AdaMixup significantly reduces attack accuracy compared to no defense. For instance, CIFAR-10's attack accuracy drops from 80.03% to 50.01% with AdaMixup. Similar trends are seen on other datasets, proving AdaMixup's effectiveness against membership inference attacks.\nTable 1 provides a comprehensive comparison of classification accuracy and attack accuracy across different defense strategies. It is clear from the table that AdaMixup not only effectively defends against attacks but also maintains high classification performance. For instance, in the MNIST dataset, AdaMixup achieves a classification accuracy of 98.94%, which is almost identical to the no-defense scenario's accuracy of 98.84%. This indicates that AdaMixup introduces minimal performance degradation while providing strong protection. Similarly, for CIFAR-10, AdaMixup achieves a classification accuracy of 61.21%, which is on par with the baseline (60.95%), while significantly outperforming other defense mechanisms such as Differential Privacy (56.21%) and L1 regularization (60.28%).\nAdditionally, the results from the LFW and STL-10 datasets further confirm the robustness of AdaMixup. For instance, on the LFW dataset, the classification accuracy remains high at 89.84%, with a substantial reduction in attack accuracy from 67.21% (without defense) to 50.07% (with AdaMixup). Similarly, for STL-10, AdaMixup maintains a classification accuracy of 58.99%, while reducing the attack accuracy to 50.06%, compared to 74.43% without any defense.\nIn summary, the results shown in both Figure 2 and Table 1 demonstrate that AdaMixup achieves a favorable balance between model performance and defense effectiveness. Across all four datasets, it consistently reduces attack accuracy while preserving classification accuracy. Compared to traditional defense methods such as Differential Privacy, Dropout, and regularization techniques, AdaMixup shows superior performance, especially in scenarios where preserving both accuracy and privacy is critical. This highlights the potential of AdaMixup as a robust defense mechanism against membership inference attacks."}, {"title": "5. CONCLUSION", "content": "This paper introduces AdaMixup, an innovative adaptive defense mechanism that dynamically adjusts mixup ratios during training to effectively mitigate membership inference attacks while preserving model accuracy. Through rigorous experiments on diverse datasets such as CIFAR-10, MNIST, LFW, and STL-10, AdaMixup has demonstrated significant reductions in attack success rates, outperforming other widely-used defense strategies."}]}