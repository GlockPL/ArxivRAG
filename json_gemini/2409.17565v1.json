{"title": "PIXEL-SPACE POST-TRAINING OF LATENT DIFFUSION MODELS", "authors": ["Christina Zhang", "Simran Motwani", "Matthew Yu", "Ji Hou", "Felix Juefei-Xu", "Sam S. Tsai", "Peter Vajda", "Zijian He", "Jialiang Wang"], "abstract": "Latent diffusion models (LDMs) have made significant advancements in the field of image generation in recent years. One major advantage of LDMs is their ability to operate in a compressed latent space, allowing for more efficient training and deployment. However, despite these advantages, challenges with LDMs still remain. For example, it has been observed that LDMs often generate high-frequency details and complex compositions imperfectly. We hypothesize that one reason for these flaws is due to the fact that all pre- and post-training of LDMs are done in latent space, which is typically 8 \u00d7 8 lower spatial-resolution than the output images. To address this issue, we propose adding pixel-space supervision in the post-training process to better preserve high-frequency details. Experimentally, we show that adding a pixel-space objective significantly improves both supervised quality fine-tuning and preference-based post-training by a large margin on a state-of-the-art DiT transformer and U-Net diffusion models in both visual quality and visual flaw metrics, while maintaining the same text alignment quality.", "sections": [{"title": "INTRODUCTION", "content": "Diffusion models learn to sequentially denoise from random Gaussian noise to sharp images and have revolutionized the field of media generation and editing in recent years. Latent diffusion models represent the most popular type of diffusion model because of their efficiency and simplicity. State-of-the-art LDMs are typically pretrained on webscale data, resulting in \"foundation models\" (Rombach et al., 2022; Podell et al., 2023; Esser et al., 2024; Saharia et al., 2022; Imagen 3 Team, 2024; Dai et al., 2023; Ramesh et al., 2021; 2022; Betker et al., 2023).\nThese foundation models are then post-trained on a smaller, carefully curated dataset to improve quality through either supervised quality fine-tuning (SFT) (Dai et al., 2023) or human-in-the-loop preference modeling (Rafailov et al., 2024; Wallace et al., 2024; Meng et al., 2024). Post-training of image foundation models is also utilized to create new models for a variety of applications, including controllable generation (Zhang et al., 2023), editing (Sheynin et al., 2024), 3D generation (Poole et al., 2022), video generation (Singer et al., 2022; Girdhar et al., 2023), and many others.\nTo achieve efficiency and simplicity, LDMs use a pretrained variational autoencoder (VAE) to compress images into latent representations. For example, in the original LDM paper (Rombach et al., 2022), the authors used a conv-based VAE to compress a 512 \u00d7 512 \u00d7 3 image to 64 x 64 x 4. This representation significantly speeds up training and reduces computational cost as the denoising diffusion model now operates in the 64 \u00d7 64 \u00d7 4 space instead of the original 512 \u00d7 512 \u00d7 3 space (48\u00d7 compression). However, this comes at the cost of lossy compression, which can result in inaccuracies in or loss of high-frequency details."}, {"title": "2 RELATED WORK", "content": "A comprehensive review of diffusion models is out of the scope of this section. Interested readers are referred to Fuest et al. (2024) and Chan (2024). Here we highlight a few works that are closest-related to us."}, {"title": "2.1 TEXT-TO-IMAGE DIFFUSION MODEL", "content": "Researchers have explored a variety of representations to train text-to-image diffusion models, including pixel-diffusion models (Ramesh et al., 2022; Saharia et al., 2022; Balaji et al., 2022), latent diffusion models (Rombach et al., 2022; Dai et al., 2023), and token-based generative transformers (Chang et al., 2023; Sun et al., 2024; Li et al., 2024). Pixel-diffusion models directly generate images in the pixel space, but due to computational constraints, they typically first generate images at a lower resolution (e.g., 64 \u00d7 64) and then upsample them (sometimes multiple times) to achieve the target resolution in a cascade fashion (Saharia et al., 2022).\nLatent Diffusion Models (LDMs), on the other hand, employ a pretrained autoencoder (Rombach et al., 2022) to compress the spatial dimensions of the image to be generated, typically by a factor of 8 \u00d7 8, while moderately increasing the channel dimension from 3 (RGB) to 4. This approach significantly enhances training efficiency compared to pixel diffusion models, thereby facilitating various applications such as high-resolution (Chen et al., 2023) and real-time image generation (Kohler et al., 2024; Wimbauer et al., 2024). Early LDM models use convolutional U-Nets as the backbone diffusion model, such as LDM1.5 (Rombach et al., 2022) and Emu (Dai et al., 2023). Recently, the field has been dominated by diffusion transformers (DiTs), such as SD3 (Esser et al., 2024) and PixArt-a (Chen et al., 2023). PixArt-a incorporates cross-attention modules into DiT and trained the model on high-aesthetic data in its final training stage. However, all of these LDM methods are still trained in latent space, which might suffer from loss of details and artifacts due to low spatial resolution.\nIn this paper, we propose a novel approach to refining image quality in diffusion models by deploying a pixel-space objective function in the post-training stage. Our method does not depend on a particular diffusion model type and works equally well for both U-Nets and DiTs."}, {"title": "2.2 SUPERVISED QUALITY FINE-TUNING (SFT)", "content": "Supervised fine-tuning is crucial to the success of modern LLMs (Zhou et al., 2024; Touvron et al., 2023; Achiam et al., 2023). In image and vision, Dai et al. (2023) proposes using a small set of extremely high-quality images to fine-tune a pretrained LDM model, resulting in significant improvements to the visual quality of generated images without sacrificing text-image alignment. Betker et al. (2023) and Segalis et al. (2023) propose using captions rewritten by vision language models to facilitate better learning, including during SFT. However, none of these proposed methods explored the representation space in which the model was fine-tuned. In this paper, we propose supplementing the regular supervised fine-tuning loss with a pixel-space objective function. We experimented with two different models: a replacement-trained U-Net LDM-1.5 (Rombach et al., 2022) and a DiT model. Our results show that when fine-tuning on a small high-quality dataset, our proposed method can significantly improve generation quality and visual flaws."}, {"title": "2.3 HUMAN PREFERENCE BASED POST TRAINING", "content": "Reinforcement learning represents another popular type of post-training technique. The seminal work of Schulman et al. (2017) makes the policy gradient method practical. Rafailov et al. (2024)"}, {"title": "3 METHOD", "content": "Given an image $x \\in \\mathbb{R}^{H\\times W\\times 3}$ in RGB space, LDMs use an autoencoder $\\mathcal{E}$ that encodes $x$ into a latent representation $z = \\mathcal{E}(x)$. The decoder $\\mathcal{D}$ then reconstructs the image from the latent, giving $\\hat{x} = \\mathcal{D}(z) = \\mathcal{D}(\\mathcal{E}(x))$, where $z \\in \\mathbb{R}^{h\\times w\\times c}$."}, {"title": "3.1 SUPERVISED PIXEL-SPACE FINE-TUNING", "content": "By denoising a normally distributed variable step-by-step, LDMs learn a data distribution $p_\\theta(x)$. Therefore, they can be understood as a series of denoising autoencoders $\\epsilon_\\theta(z_t, t); t = 1 ... T$ that are trained to predict the denoised variant of their input $z_t$ where $z_t$ is the noisy version of latent input $z$ at time $t$, $\\epsilon$ is the original noise added to get $z_t$, and $\\epsilon_\\theta(z_t, t)$ is the predicted noise. Furthermore, the noise added to $z_{t-1}$ to get $z_t$ is Gaussian with variance $\\beta_t$. The standard objective function for LDMs is:\n$L_{latent} := E_{x(x),\\epsilon \\sim N(0,1),t} [||\\epsilon - \\epsilon_\\theta(z_t, t)||^2]$.\nInstead of working only in the latent space $\\mathbb{R}^{h\\times w\\times c}$, we propose a loss function that incorporates the pixel space $\\mathbb{R}^{H\\times W\\times 3}$ in the objective function. This is achieved by adding the noise $\\epsilon$ to the latent image $z$ through the forward diffusion process $x_t = \\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1 - \\bar{\\alpha}_t}\\epsilon$, where $\\alpha_t = 1 - \\beta_t$ and $\\bar{\\alpha}_t = \\Pi_{i=1}^t \\alpha_i$, and decoding it back to the pixel space. The objective function then becomes\n$L_{pixel}^{SFT} := E_{x(x),\\epsilon \\sim N(0,1),t} [||\\mathcal{D}(\\sqrt{\\bar{\\alpha}_t}z + \\sqrt{1 - \\bar{\\alpha}_t}\\epsilon) - \\mathcal{D}(\\sqrt{\\bar{\\alpha}_t}z + \\sqrt{1 - \\bar{\\alpha}_t}\\epsilon_\\theta (z_t, t))||^2]$.\nWe combine the latent objective, Equation 1, with the pixel objective, Equation 2, to obtain an objective that uses both the latent and pixel space, weighted by hyper-parameter $\\lambda$.\n$L_{SFT} := L_{latent}^{SFT} + \\lambda L_{pixel}^{SFT}$"}, {"title": "3.2 PIXEL-SPACE FINE-TUNING USING REWARD MODELING", "content": "Define $x^w$ and $x^l$ to be the \u201cwinning\u201d and \u201closing\u201d samples from human annotations, then $z^w = \\mathcal{E}(x^w)$ and $z^l = \\mathcal{E}(x^l)$ represent the \"winning\" and \"losing\u201d samples in the latent space. Unlike regular supervised fine-tuning, fine-tuning with DPO utilizes a reference distribution $P_{ref}(x)$ and hyperparameter $\\beta$ for regularization. Fine-tuning now aims to learn $p_\\theta$, which is aligned to human preferences, while still remembering $P_{ref}$. The reward modeling objective for fine-tuning takes the form:\n$L_{dpo}^{latent} := E_{\\epsilon(x),\\epsilon \\sim N(0,1),t} log(\\sigma(- \\beta(||\\epsilon^w - \\epsilon_\\theta(z^w, t)||^2 - ||\\epsilon^w - \\epsilon_{ref}(z^w, t)||^2 - (||\\epsilon^l - \\epsilon_\\theta(z^l, t)||^2 - ||\\epsilon^l - \\epsilon_{ref}(z^l, t)||^2))))$.\nInspired by Meng et al. (2024), we remove the reference model and simplify the objective to\n$L_{simpo}^{latent} := -E_{\\epsilon(x),\\epsilon \\sim N(0,1),t} log(\\sigma(- \\beta(||\\epsilon^w - \\epsilon_\\theta(z^w, t)||^2 - (||\\epsilon^l - \\epsilon_\\theta(z^l, t)||^2))))$.\nSimilar to supervised fine-tuning, we also incorporate calculations in the pixel space into our reward modeling. and define the pixel-space objective as\n$L_{simpo}^{pixel} := E_{\\epsilon(x),\\epsilon \\sim N(0,1),t} log \\sigma(-\\beta((||\\mathcal{D}(\\sqrt{\\bar{\\alpha}_t}z^w + \\sqrt{1 - \\bar{\\alpha}_t}\\epsilon^w) - \\mathcal{D}(\\sqrt{\\bar{\\alpha}_t}z^w + \\sqrt{1 - \\bar{\\alpha}_t}\\epsilon_\\theta(z_t^w, t))||^2 -(||\\mathcal{D}(\\sqrt{\\bar{\\alpha}_t}z^l + \\sqrt{1 - \\bar{\\alpha}_t}\\epsilon^l) - \\mathcal{D}(\\sqrt{\\bar{\\alpha}_t}z^l + \\sqrt{1 - \\bar{\\alpha}_t}\\epsilon_\\theta(z_t^l, t))||^2)))$.\nCombining latent and pixel terms and weighted by a constant $\\mu$, we get\n$L_{reward} = L_{simpo}^{latent} + \\mu L_{simpo}^{pixel}$"}, {"title": "4 EXPERIMENT", "content": "We conduct a comprehensive qualitative and quantitative analysis, as well as ablation studies to show that our proposed loss function outperforms the regular latent space loss in both supervised fine-tuning and preference-based post-training."}, {"title": "4.1 HUMAN EVALUATION", "content": "Like many recent studies, we found that a rigorous and independent human evaluation process is the most reliable way to evaluate different models. Commonly used metrics such as the FID score do not correlate well with human preference (Dai et al., 2023; Podell et al., 2023; Kirstain et al., 2023).\nWe contracted a team of paid and independent annotators who do not have contexts of our project to evaluate the generated images. We conducted A/B comparisons on visual flaws and visual appeal, as well as standalone evaluations on text alignment. We use a 600-prompt list in the GenAI MAGIC challenge for the evaluation (Tsai et al., 2024), where each example is annotated by at least 5 people and the majority decision is used.\nVisual flaws. The annotators were presented with two images side-by-side, generated by two different models, without the prompt. The annotators were trained to identify major flaws (e.g., displaced body parts) and minor flaws (distorted eyes), and are asked to choose from \"left wins\", \"tie\" and \"right wins\".\nVisual appeal. Similar to the visual flaws task, but the annotators are asked to compare which image is more aesthetically pleasing. Annotators were instructed to reject any examples where one image was photo-realistic and the other was stylized (e.g., a cartoon).\nText alignment. We predefined a list of binary questions for each prompt and asked annotators to answer yes or no. We calculated the text alignment rate by aggregating the results across all questions. For example, for the prompt \"a cat and a dog\", the annotators are asked \"is there one dog\", \"is there one cat\u201d, and \u201care there other animals present\"."}, {"title": "4.2 EXPERIMENTAL SETUP", "content": "Baseline. We tested our model on three models: 1) A 0.6B parameter DiT model with standard transformer and cross attention blocks and trained with high quality data in an \"annealing\" stage after pretraining to generate high quality images, 2) A 0.86B parameter U-Net with LDM1.5 architecture (Rombach et al., 2022), replacement-trained on 300M Shutterstock data without quality tuning, which thus generates lower-quality images without prompt engineering and 3) A larger U-Net based Emu model (Dai et al., 2023) that has been quality fine-tuned, and generates the highest quality images among the three.\nSupervised fine-tuning. We curated a small, high-quality dataset of 1816 images for fine-tuning, following the practice of Dai et al. (2023). Since Emu is already quality fine-tuned, we focused on replacement-trained LDM1.5 and DiT. For supervised fine-tuning with a small dataset, style consistency is crucial. We found that using a hand-picked set of generated images from a high-quality model is sufficient. Examples of our curated fine-tune data are in Appendix Figure 6.\nPreference-based fine-tuning. We conducted experiments on the higher-quality U-Net Emu and DiT models. For each model, we generate 5 images per prompt and ask annotators to select a positive and negative pair. In instances where visual flaws and visual quality conflicted, we prioritize the image with fewest visual flaws as the positive example.\nImplementation details. We run all experiments at 512 \u00d7 512 resolution for LDM1.5 and DiT, and 768 x 768 for Emu, using Adam optimizer with weight decay of $5e - 6$. For preference-based fine-tuning, we set $\\lambda = \\mu = 8.0$ for DiT and 2.0 for Emu to balance the pixel loss magnitude and latent loss magnitude, running 100 epochs. For supervised fine-tuning, we empirically use 140 epochs. We ablate the hyper-parameters such as learning rate and batch size for each model and choose the best ones. Each experiment took 1-8 hours on 8 H100 GPUs to fine-tune one model. During inference, we use the standard DDIM solver with 50 steps with classifier-free guidance."}, {"title": "4.3 EXPERIMENTAL RESULTS", "content": "Supervised Fine-tuning. After supervised fine-tuning, our proposed loss improved visual flaws win rate from 17.7% to 64.2%, and visual quality win rate from 47.9% to 64.7%, compared to regular fine-tuning against the DiT baseline. When directly comparing the model fine-tuned with our loss to the model fine-tuned with the regular latent space loss, ours showed a 32.8% vs 9.3% win rate on visual flaws and 34.8% vs 16.6% win rate on visual appeal. We also found that supervised fine-tuning did not affect text alignment, with correct alignment rates of 74.0%, 74.3% and 74.0% for baseline, regular latent fine-tuning, and our fine-tuning respectively, (Table 1), all within the margin of error for the annotations. Qualitative examples in Figure 3 demonstrate that our method generates fewer artifacts and much better fine details."}, {"title": "4.4 ABLATION STUDIES", "content": "Latent vs Pixel vs Pixel+Latent Loss. When only using the pixel space loss during supervised fine-tuning, we noticed that the resulting images had very clear details in the main focus of the image, but the background tends to be overly blurred as if they are photographs taken with an extremely narrow depth of field. As shown in Table 5, using pixel space alone also significantly improves"}, {"title": "5 LIMITATIONS", "content": "Limitations of Baseline Models. Fine-tuning improvements are dependent on the quality of the original baseline model, and thus fine-tuning using pixel space loss may not always produce significant improvements. For example, if the original baseline model already has minimal flaws and high visual appeal, fine-tuning may not achieve many improvements. In contrast, if the original baseline foundation model generates significant structural flaws that require the global understanding of the image composition, fine-tuning with our loss alone may not help eliminate them.\nLimitations of Fine-tuning Dataset. Fine-tuning is also dependent on the quality and composition of the dataset used. Our dataset was hand-curated and consisted of images that followed our definition of high quality and style. Changing the composition of this dataset would lead to different results."}, {"title": "Limitations of Human Evaluation", "content": "The images generated by the different models were evaluated by independent annotators. Although the annotators were trained on standards for visual flaws, visual appeal, and text alignment, these results may not fully reflect the real-world use of the models. Human evaluation is also inherently subjective and noisy in terms of aesthetics."}, {"title": "6 CONCLUSIONS", "content": "In this paper, we proposed a novel post-training objective function for latent diffusion models by incorporating a pixel-space loss with the commonly used latent-space fine-tuning loss. The resulting model shows noticeable improvement in visual flaws and visual appeal metrics in both supervised fine-tuning and preference-based post-training through rigorous human evaluations. The proposed objective function is simple and can be easily plugged into existing models such as DiT and U-Net."}]}