{"title": "Can LLMs Identify Gaps and Misconceptions in Students' Code Explanations?", "authors": ["Priti Oli", "Rabin Banjade", "Andrew M. Olney", "Vasile Rus"], "abstract": "This paper investigates various approaches using Large Language Models (LLMs) to identify gaps and misconceptions in students' self-explanations of specific instructional material, in our case explanations of code examples. This research is a part of our larger effort to automate the assessment of students' freely generated responses, focusing specifically on their self-explanations of code examples during activities related to code comprehension. In this work, we experiment with zero-shot prompting, Supervised Fine-Tuning (SFT), and preference alignment of LLMs to identify gaps in students' self-explanation. With simple prompting, GPT-4 consistently outperformed LLaMA3 and Mistral in identifying gaps and misconceptions, as confirmed by human evaluations. Additionally, our results suggest that fine-tuned large language models are more effective at identifying gaps in students' explanations compared to zero-shot and few-shot prompting techniques. Furthermore, our findings show that the preference optimization approach using Odds Ratio Preference Optimization (ORPO) outperforms SFT in identifying gaps and misconceptions in students' code explanations.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs), such as ChatGPT, have captured the attention of many researchers due to their remarkable ability to generate responses to user prompts and their competitive performance on varied tasks such as question answering, summarization, and semantic similarity (Zhao et al. 2023). In the context of education, LLMs offer opportunities to customize learning experiences and adapt instructional material according to the unique needs of each learner (Giannakos et al. 2024). Such AI-powered adaptive learning systems have been explored to analyze student data (Oli et al. 2023a), identify learning gaps (Banjade et al. 2024), and to tailor instructional material resulting in enhanced engagement and improved learning outcomes (Oli et al. 2023b). While LLMs provide immediate and round-the-clock support, it is crucial that they consistently exhibit fairness, accuracy, and reliability (Giannakos et al. 2024; Denny et al. 2024).\nDenny et al. (2024) revealed that students prefer AI Teaching Assistants that offer debugging and code-writing support which fosters their learning autonomy, favoring guidance through problem-solving steps rather than direct solutions. However, the study also highlighted a limitation: current tools like ChatGPT do not account for a student's level of expertise, often resulting in confusing or unhelpful programming patterns. Similarly, Kazemitabaar et al. (2024), noted that LLM-powered tools like ChatGPT provide immediate assistance, but by presenting a direct solution, they might discourage deep conceptual engagement. The authors argue that an AI assistant that is too direct might hinder skill development by bypassing critical learning opportunities, whereas overly indirect responses might overwhelm and discourage students by failing to provide adequate support.\nTo achieve the right balance of support, future AI tools could utilize techniques like the Socratic method or Scaffolding-based approaches to support independent problem-solving(Kazemitabaar et al. 2024). Prior research has shown that interactive, conversational learning rooted in socio-constructivist theories-where students engage with and respond to questions from knowledgeable instructors or digital tutors-is effective in various domains (Oli et al. 2023b; Tamang et al. 2021). In this context, we explore the use of LLMs to further enhance these approaches, leveraging their potential to facilitate deeper learner engagement and adaptive feedback during code-comprehension tasks.\nCode comprehension refers to the ability of an individual to understand and make sense of code written in a programming language (Schulte et al. 2010). Code comprehension involves the process of reading, analyzing, and interpreting code to understand the purpose, functionality, and structure of code. Many researchers advocate for incorporating code reading into programming education, with some suggesting that students should learn to read code before they start writing it. In today's learning environment, code comprehension has become an indispensable skill, especially as learners increasingly encounter code written by experts or generated by LLMs. However, the potential of LLMs to scaffold students' understanding of code has not been studied extensively. A critical component of effective scaffolding is the ability to identify both incomplete and incorrect elements in a student's response. In this work, we investigate how LLMs can be used to identify missing and erroneous parts in students' self-explanations of code examples, aiming to offer targeted support that enhances their understanding.\nThis work is part of a broader initiative to develop educational technology that improves students' understanding of code by providing tailored feedback. The system prompts students to explain their comprehension of each line of code as they read it, fostering deeper engagement. A key component of this system\u2014and the central focus of this paper is the automated identification of gaps and misconceptions in students' code explanations, enabling more tailored and effective feedback. Specifically, this paper explores various approaches that use LLMs to identify gaps or missing elements in learners' self-explanations of specific instructional content, such as code examples. These gaps or misconceptions are evaluated from self-explanation by correctly identifying what makes the explanation incomplete or what type of misconception is prevalent in self-explanation. Identifying such gaps is the major step in providing scaffolding; for instance, if a code explanation is missing a step, a tutor-whether human or AI can offer a hint to help the learner consider and articulate the missing step. Similarly, when a response is incorrect, the tutor needs to provide guidance to address potential misconceptions expressed by the student. Moreover, feedback must be accurate, encouraging, and timely. It should precisely identify issues without causing confusion, and support students in developing their own solutions independently.\nIn this work, we investigate the use of large language models to identify missing and erroneous components in students' self-explanations of code examples. Specifically, we investigate the following research question:\n\"Can LLMs identify gaps and misconceptions in students' code explanations?\"\nWe first experiment with prompting LLMs to identify the gaps and misconceptions in students' code explanations. Although prompting LLMs in zero-shot, few-shot, chain-of-thought settings are shown to be effective across various tasks (Kojima et al. 2022), these methods alone may not be sufficient for handling more complex or specialized tasks. Fine-tuning LLMs for more specific, well-defined tasks such as identifying gaps and misconceptions should lead to better results. We experiment with both approaches.\nThe following section provides a review of prior research on identifying gaps and misconceptions in students' responses, as well as the fine-tuning of LLMs for various applications. We then describe the methods and datasets used in our study and discuss the results."}, {"title": "Related Work", "content": "Identifying Gaps in Student's Response\nNumerous approaches exist for automatically identifying gaps in students' responses or for identifying misconceptions in student's responses. A common method involves using engineered features to detect errors in student responses, followed by a rule-based system to provide relevant feedback or hints (Kochmar et al. 2020; Lan et al. 2015; Botelho et al. 2023). When identifying gaps, such a system compares the learner's solution with either a rule-based or constraint-based model, highlighting where the student diverges from the correct path. Such methods are popular for their interpretability and reliability but require significant human effort to adapt to new questions and may overlook common student mistakes, leading to sub-optimal outcomes in ambiguous domains.\nIn the computer programming domain, researchers have introduced techniques to automatically identify gaps and misconceptions in students' responses, generating hints that offer immediate, relevant feedback to guide novices in correcting mistakes (Al-Hossami et al. 2023). Recent advancements utilize large language models, employing either prompt-based techniques (Al-Hossami et al. 2023; Hunter McNichols et al. 2024; Banjade et al. 2024) or fine-tuning methods (Jia et al. 2022).\nPrior research primarily focused on assisting students with programming exercises by introducing various techniques to help novices correct mistakes and progress through the tasks. In contrast, our work emphasizes enhancing code comprehension by analyzing gaps and misconceptions in learner-generated code explanations. Similar to our approach, Banjade et al. (2024) utilized zero-shot prompting of large language models to identify gaps in students' code explanations. In this work, we extend this by exploring zero-shot prompting, few-shot prompting, and fine-tuning of LLMs to identify both gaps and misconceptions in students' code explanations.\nAdditionally, previous studies have demonstrated that while LLMs can identify gaps and misconceptions and provide feedback to the learners, they have several limitations. For instance, Balse et al. (2023) noted that GPT-3 exhibited significant variability in the quality of generated feedback, occasionally producing incorrect and inconsistent responses. Kiesler, Lohr, and Keuning (2023) found that ChatGPT's performance varied by error type: it was effective at identifying compilation errors but struggled with logic, semantic errors, or cases involving multiple simultaneous errors in student code. Similarly, Hellas et al. (2023) noted that while GPT-3.5 could identify actual issues in student code, it had mixed success in detecting all issues and sometimes falsely identifying nonexistent issues in the code. In this study, we investigate and compare the effectiveness of prompting and fine-tuning various LLMs in accurately identifying gaps and misconceptions in students' code explanations."}, {"title": "Preference Optimization using LLM", "content": "Large Language Models have demonstrated effective instruction-following abilities, allowing them to tackle complex natural language tasks with little to no labeled data (Kojima et al. 2022). This capability stems from their training with instruction-following data (Wei et al. 2021) and reinforcement learning from human feedback (RLHF; Stiennon et al. 2020). Zhou et al. (2024), demonstrated that LLMs can effectively learn to generate specific responses through their assistant model, LIMA achieving competitive performance with just 1,000 high-quality examples. Instruction-tuning trains models to follow task descriptions provided in natural language, allowing them to generalize effectively to tasks they haven't encountered before (Wei et al. 2021; Taori et al. 2023). To further make these models more helpful and harmless, additional training with pairwise preference data is required, using techniques such as reinforcement learning with human feedback (RLHF; Ziegler et al. 2019; Stiennon et al. 2020) or direct preference optimization (DPO; Rafailov et al. 2024). Traditional methods use a reward model based on human preferences to optimize language models for tasks using RL algorithms like PPO (Schulman et al. 2017), but this approach is costly and complex. To address this challenge, Reinforcement Learning with AI Feedback (RLAIF; Lee et al. 2023) adopts rewards sourced from Al systems, such as LLMs, providing a scalable and cost-effective solution. Optimizing LLMs' preferences with RL algorithms like PPO is effective but more complex and time-consuming than traditional supervised learning, involving tuning multiple models and real-time reward sampling.\nA key improvement introduced by DPO is that the reward objective can be expressed using the optimal and reference policies, enabling model training from preference data without needing a separate reward model or policy sampling during learning (Rafailov et al. 2024). Hong et.al proposes Odds Ratio Preference Optimization (ORPO; Hong, Lee, and Thorne 2024), a new method for training LLMs by integrating supervised fine-tuning and preference alignment into a single objective (loss function), achieving state-of-the-art results.\nIn the field of education, Scarlatos et al. (2024) proposed a feedback generation framework that optimizes the correctness and alignment with human feedback using DPO to help students solve mathematics problems. In the programming domain, our focus, Hicke et al. (2023) introduced a DPO-based approach to fine-tune LLama2 for question-answering using a dataset of Piazza\u00b9 posts from an introductory programming course. They created a proxy preference dataset from the edit history of Piazza posts, favoring final versions of answers over earlier iterations. Additionally, Kumar and Lan (2024) proposed a method for generating Socratic questions to provide feedback on programming problems specifically to help a student debug their code. Their approach uses data augmentation techniques to synthetically generate invalid questions, which are then used to fine-tune open-source LLMs. In contrast to these studies, which focus on debugging or code writing, our work emphasizes code comprehension by providing feedback on students' free-response explanations of code examples. We experiment with both prompting and fine-tuning LLMs using SFT and ORPO to identify gaps and misconceptions in students' self-explanations of code examples."}, {"title": "Study Setup and Dataset:", "content": "The Java code examples used in this work are taken from the DeepCode dataset (Rus et al. 2022), which consists of 98 annotated code examples totaling 7,157 lines of code, including comments. We selected DeepCode because it was specifically developed to enhance code comprehension skills among CS1 and CS2 students. The code examples covered the following CS concepts: logical operators, if-else condition, loops, arrays, methods, classes and objects, exception handling, recursion, inheritance in JAVA, binary search, and sorting. These topics were carefully selected to offer a balanced mix of complexity and variety. This dataset also includes expert-annotated code explanations designed for pedagogical purposes, such as assessment, problem-solving, and studying worked-out code examples with explanations. For our analysis, we extracted these expert annotations from the DeepCode dataset to serve as the gold standard. Additionally, we converted the Java code into Python and used GPT-4 to generate code explanations for these Python examples. Prior works have shown that LLMs can be used to generate effective code explanations (Sarsa et al. 2022; Narayanan et al. 2024).\nTo evaluate the ability of large language models to identify gaps in code explanations, we created a dataset that varied across three key dimensions: correct, incorrect, and incomplete explanations. To better simulate real-world scenarios, where correct explanations may include minor errors, we augmented the dataset with correct explanations containing variations such as typos and synonyms. These variations were designed to distort the explanations slightly while preserving the underlying idea and meaning. The details of each process are explained below:\nGenerating Variations of Correct Explanations: We employed various natural language techniques such as random word deletion, synonym replacement, and random character replacement to augment the explanations by introducing typos and other modifications to simulate different quality levels of student explanations. From the original DeepCode explanations, we generated 466 different Java code explanations and 466 Python code explanations using various NLP augmentation techniques. Since the augmentations only include minor variations such as typos, we consider the explanations to be essentially correct. We annotate the gold-standard feedback for such explanations with positive feedback pointing out only minor issues in the explanations. It is important to note that the unaltered original explanations serve as our benchmark.\nExpert Explanation (Code shown in Listing 1):\nThe program creates a person's original travel list, creates a new travel list from the original, and then modifies the new travel list. Make an arrayList travelList for holding the travel list and add destinations(countries) to the travel list. Declare an arrayList travelList whose elements are of type String and add Switzerland, Denmark, India, China, Thailand, and Bhutan. Create a list newTravelList from the original travel list travelList. Create an arrayList newTravelList for copying new travel destinations. The element of the arrayList newTravelList are of type string. Create a for loop that runs from index i= 0 to i= 5.Get elements of arrayList travelList at position i and add the element to arrayList newTravelList at index position i. The array elements Switzerland, Denmark, India, China, Thailand, and Bhutan are added to newTravelList. Remove China from newTravelList and remove element at index 0. The value of newTravelList printed is [Denmark, India, Thailand, Bhutan].\nIncomplete Explanation Generation For incomplete explanation, we created simulated data inspired by the work of Banjade et al. (2024), where we generate the incomplete explanation by deleting two consecutive sentences from the benchmark explanation. We repeated this process to generate all the combinations of the incomplete explanation for a given benchmark explanation. This simulates a scenario where a student has gap in her code explanation, missing or overlooking important aspects of the code. These omitted details become our ground truth, which we use to evaluate whether LLMs can detect them. Our initial observation showed that consecutive pair of sentences from the generated explanation typically corresponded to an expectation, reflecting the understanding of a specific part or concept of the code. We generated a total of 1,296 incomplete code explanations, comprising 338 incomplete Java code explanations and 958 incomplete Python code explanations.\nIncorrect Explanation Generation: To generate incorrect explanations, we simulated incorrect responses in a manner similar to how we simulated incomplete explanations. To simulate incorrect explanations, we first injected logical errors into the code and modified the code explanation to generate the code explanation for the error-injected version of the code. These logical errors injected were based on the misconceptions commonly observed among CS1 and CS2 students (Kaczmarczyk et al. 2010; Ettles, Luxton-Reilly, and Denny 2018; Qian and Lehman 2017).\nSome of the examples of errors injected include: constructing arrays with an off-by-one error, off-by-one error in loop conditions, replacing operator with another similar operator e.g replacing \">\" with \">=\", ignoring the 0-th index of the array, replacing floating-point division with integer division, replacing the assignment operator (=) for the comparison operator (==) and varying formatting in print statements among others. Importantly, all injected errors were logical in nature, introducing subtle variations in the code without causing it to break. The errors injected were assigned as the incorrect explanation to serve as the benchmark feedback when evaluating feedback generated using LLMs. In total, we generated 660 incorrect code explanations, consisting of 330 incorrect Java code explanations and 330 incorrect Python code explanations.\nWe created a dataset\u00b2 of 2,888 code explanations (1,134 in Java and 1,754 in Python) by introducing errors into incorrect explanations and simulating incomplete code examples by removing specific lines. This allowed us to analyze feedback generation using large language models. The data was split into 75% training data, 20% test data, and 5% validation data. Appendix A shows examples of the simulated gaps and misconceptions in code explanations."}, {"title": "Methodology", "content": "We experimented under different settings: zero-shot prompting, few shot prompting, finetuning using supervised fine-tuning (SFT) and SFT with preference optimization. Each of which are explained below:"}, {"title": "Zero-shot Prompting", "content": "As our baseline, we prompt LLMs to identify and generate feedback for incomplete or incorrect explanations in the simulated code explanations. We engaged in an iterative process of prompt selection, which included multiple trials and adjustments, selecting two distinct prompts for our analysis, listed below.\nP1: Given the following code:{code} and the following reference explanation: {reference explanation}, your task is to identify what is incorrect or missing in the following student explanation: {student explanation} of the code. Generate the missing part or incorrect part. If the explanation is complete and correct, aside from minor typos or issues, provide a single line of positive feedback.\nP2: Given the following code:{code}, your task is to identify what is incorrect or missing in the following student explanation:{student explanation} of the code. Generate the missing part or incorrect part. If the explanation is complete and correct, aside from minor typos or issues, provide a single line of positive feedback."}, {"title": "Few-shot Prompting", "content": "Additionally, we also investigate advanced prompting strategies, such as few-shot prompting (in-context learning), where the LLM learns from provided examples or task descriptions (Brown et al. 2020). We only utilized P2 in few-shot prompting to examine how LLMs can generate feedback on students' code explanations without relying on expert explanations, which are costly to produce (Narayanan et al. 2024). In our few-shot setting, prompt P2 was followed by the gold standard feedback. In few-shot learning, we utilized four types of examples: correct and complete, correct and complete with typos, incomplete, and incorrect explanations. These exemplars served as the foundation for the LLMs to learn from."}, {"title": "Supervised Fine Tuning (SFT)", "content": "In this approach, we finetuned GPT4, Llama3 and Mistral with labeled dataset. In our case, our dataset consisted of code:code example, explanation:code explanation and feedback:feedback. For each code and code explanation, we used feedback as our label. To fine-tune OpenAI's model, we utilized the fine-tuning API provided by OpenAI."}, {"title": "SFT with Preference Optimization", "content": "In addition to SFT, preference optimization involves gathering human feedback on the model's outputs and using this feedback as a reward signal to guide the model's behavior. By optimizing the model to produce outputs that reflect human preferences, this fine-tuning helps to reduce undesirable behaviors and ensures that responses are more socially appropriate and beneficial.\nTo prepare the data for the preference optimization we used the benchmark feedback as our accepted sample and the feedback generated from GPT-4 using prompts (P1) and P2 as the rejected sample. We chose the GPT-4 generated feedback as our rejected response because, upon a cursory evaluation, we found it to be irrelevant. The feedback from simple prompting of GPT-4 often focused on grammar, structure, and flow of the student's explanation rather than identifying gaps and misconceptions in the student's explanation, making it irrelevant in our case.\nTo make the feedback more helpful and relevant, we use Odds Ratio Preference Optimization (ORPO) for preference optimization. ORPO integrates an odds ratio-based penalty into the conventional negative log-likelihood loss to differentiate between the generation styles of favored and disfavored responses. Given an input sequence x, the average log-likelihood of generating the output sequence y, consisting of m tokens, is calculated as shown in Equation 1. The odds of generating the output sequence y given the input sequence x are defined in Equation 2.\n$\\log P_{\\theta}(y|x) = \\frac{1}{m} \\sum_{i=1}^{m} \\log p_{\\theta}(y_t|x, y_{<t})$\n$\\text{odds}_{\\theta}(y|x) = \\frac{P_{\\theta}(y|x)}{1 - P_{\\theta}(y|x)}$\nWhen $\\text{odds}_{\\theta}(y|x) = k$, it means that the model @ is k times more inclined to generate the sequence y than to not generate it. Equation 3 defines Odd Ratio, $OR_{\\theta}(Y_w, Y_l)$, showing how much more likely @ is to generate $y_w$ over $y_l$ given input x, where $y_w$ is the preferred response and $y_l$ is the rejected response. The objective function for training for preference optimization has two parts: the supervised fine-tuning loss $L_{SFT}$ and the relative log ratio loss $L_{OR}$ as shown in in Equation 4 where $L_{SFT}$ is the conventional causal language modeling negative log-likelihood loss and \u03bb represents the weighting value for $L_{OR}$, which influences the log probablity ratio of accepted and rejected response. $L_{OR}$ (Equation 5) maximizes the odds ratio between generating the disfavored responses $y_w$ and $y_l$. Further, the log sigmoid function is used to minimize $L_{OR}$ by increasing the log odds ratio.\n$OR_{\\theta}(Y_w, Y_l) = \\frac{\\text{odds}_{\\theta}(Y_w|x)}{\\text{odds}_{\\theta}(y_l|x)}$\n$L_{ORPO} = E_{(x,y_w,y_l)} [L_{SFT} + \\lambda \\cdot L_{OR}]$\n$L_{OR} = - \\log \\sigma \\left( \\frac{\\text{odds}_{\\theta}(Y_w|x)}{\\text{odds}_{\\theta}(y_l|x)} \\right)$"}, {"title": "Experimental Setting", "content": "We used three distinct large language models to assess the feedback generated by LLM to identify gaps in student explanation: gpt-4-0613 (OpenAI 2023), Mistral-7b-Instruct-v0.1 (team 2022), and meta-llama/Meta-Llama-3-8B (Touvron et al. 2023). We accessed the open-source model for prompting and fine-tuning through Hugging Face.3 These LLMs have demonstrated state-of-the-art performance in various tasks, each utilizing different training data and algorithms, although the model size and training data specifics for the OpenAI models are not disclosed. We set the temperature parameter to 0 for all the models to ensure consistency and reproducible results. For fine-tuning we used the 8-bit quantized version of the LLama3 model and used QLora (Dettmers et al. 2024) to fine-tune using the parameter efficient fine-tuning technique (Mangrulkar et al. 2022). For SFT, we set the maximum learning rate to 2e-4, used the AdamW optimizer, and conducted the training over 5 epochs. For fine-tuning ORPO, we set the learning rate to 8e-6, with a batch size of 2, and trained for 3 epochs using AdamW optimizer. We fine-tuned the open-sourced model using NVIDIA A100 GPU and used FlashAttention (Dao et al. 2022) to further speed up the fine-tuning process. The fine-tuned models is publicly available in hugging face4."}, {"title": "Evaluation", "content": "We evaluate the feedback generated by LLMs through prompting and fine-tuning preference optimization both qualitatively and quantitatively.\nQuantitative Analysis\nWe utilize four evaluation metrics to compare explanations generated by different sources: the character-based metric chrF (Popovi\u0107 2015), the word-based metric METEOR (Banerjee and Lavie 2005), and the embedding-based metrics BERTScore (Zhang et al. 2019) and Universal Sentence Encoder (USE; Cer et al. 2018). chrF (character n-gram F-score) measures the character-level matching between the reference text and the machine-generated text by considering both precision and recall. METEOR evaluates the similarity between words and assesses word overlap between the two texts.\nBERTScore is an automated evaluation metric for text generation that assesses the similarity between candidate and reference sentences by comparing the contextual embeddings of individual tokens using cosine similarity. The Universal Sentence Encoder (USE) is a transformer-based model that converts text into high-dimensional vectors, allowing the computation of similarity between two texts based on their vector representations. In their study, Haque et al. (2022) and Roy, Fakhoury, and Arnaoudova (2021) have noted that METEOR, chrF (Popovi\u0107 2015), and USE (Cer et al. 2018) metrics better align with human preferences for code related tasks, as they assign partial credits to words. Additionally, we employ BERTScore to evaluate the generated explanations, primarily because of its extensive use as a reliable measure for assessing the faithfulness of LLMs (Ji et al. 2023).\nQualitative Analysis\nTo analyze the data qualitatively, we selected a stratified sample of 220 code explanations to evaluate feedback generated by various LLMs (GPT-4, LLaMA3, Mistral) across different settings (Prompt P1 & P2, Few-Shot Setting, SFT, ORPO). Our qualitative analysis focused on the rubrics proposed by Scarlatos et al. (2024): correct (feedback that is accurate and relevant to both the current question and the student's response), diagnostic (feedback that identifies errors or misconceptions in the student's answer accurately), and positive (constructive and supportive feedback). The inter-rater agreement between 3 annotators (graduate students) is reflected by very high Cohen's Kappa coefficients, with scores of 0.94 for correctness, 0.95 for diagnostics, and 0.97 for positive metrics."}, {"title": "Results and Discussion", "content": "We present both quantitative and qualitative analyses of our approaches. Our findings indicate that supervised fine-tuning for GPT-4 and fine-tuning of open-source LLMs for preference alignment yield the optimal performance, achieving up to a 15% improvement over the baseline(simple prompting).\nWhen prompting LLMs to identify gaps in students' explanations, we observed frequent occurrences of hallucinations. This issue was particularly evident when the input data was complete and correct. In these cases, the LLMs would sometimes (27%) generate explanations that falsely identified problems in the code explanation or even in code or focused on superficial aspects like formatting or style. Interestingly, when dealing with code explanations that were mostly correct with minor typos or issues, the LLMs performed poorly, fabricating non-existent issues in 34% of our qualitative analysis sample. Our findings indicate that while Large Language Models can identify incorrect and incomplete responses, they often hallucinate when prompted to identify missing gaps in explanations that are complete and correct.\nTable 2 presents the findings from the qualitative analysis of prompting and fine-tuning large language models to identify gaps in students' explanations. In most cases of prompting LLMs, the feedback provided is verbose and deviant, which may not be beneficial to students if delivered directly. For example, much of the response with prompting focused on enhancing explanations, addressing typographical and clarity issues, reducing repetitive and redundant information, or providing background mathematical knowledge of the code implementation. Specifically, with LLama3, prompting the model to identify gaps in explanations mostly resulted in suggestions for fixing or optimizing code, even when it was already error-free. Additionally, LLama3 often provided positive feedback, praising the explanations even when they were incorrect or incomplete. In Table 2, we can see that fine-tuning open-source models such as LLama3 and Mistral to align to human preference significantly (up to 35%) improves the quality of feedback generated by LLMs. Furthermore, we can observe that simple fine-tuning of GPT-4 can enhance the quality of the feedback. Although the LLMs were able to identify explanations as correct or incorrect and complete or incomplete, they struggled to pinpoint the specific missing gaps or incorrect knowledge components in the students' explanations (see Diagnostic metric in Table 2). Appendix B showcases the feedback generated by various LLMs using different methods and demonstrates how fine-tuning and preference optimization improved feedback quality."}, {"title": "Conclusion", "content": "In this work, we explored various methods for using LLMs to identify gaps in students' self-explanations of specific instructional material, such as explanations for code examples. Specifically, we experimented with different techniques including prompting LLMs, Supervised Fine-Tuning (SFT), and preference optimization strategies to detect gaps and misconceptions in students' self-explanations. Our findings indicate that fine-tuning approaches based on preference alignment significantly improve the quality of feedback generated. Our results show promising outcomes on employing LLMs to automatically assess and provide feedback on students' self-explanations."}]}