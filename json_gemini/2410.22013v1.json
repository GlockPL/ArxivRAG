{"title": "Modeling Temporal Positive and Negative Excitation for Sequential Recommendation", "authors": ["Chengkai Huang", "Shoujin Wang", "Xianzhi Wang", "Lina Yao"], "abstract": "Sequential recommendation aims to predict the next item which interests users via modeling their interest in items over time. Most of the existing works on sequential recommendation model users' dynamic interest in specific items while overlooking users' static interest revealed by some static attribute information of items, e.g., category, or brand. Moreover, existing works often only consider the positive excitation of a user's historical interactions on his/her next choice on candidate items while ignoring the commonly existing negative excitation, resulting in insufficient modeling dynamic interest. The overlook of static interest and negative excitation will lead to incomplete interest modeling and thus impede the recommendation performance. To this end, in this paper, we propose modeling both static interest and negative excitation for dynamic interest to further improve the recommendation performance. Accordingly, we design a novel Static-Dynamic Interest Learning (SDIL) framework featured with a novel Temporal Positive and Negative Excitation Modeling (TPNE) module for accurate sequential recommendation. TPNE is specially designed for comprehensively modeling dynamic interest based on temporal positive and negative excitation learning. Extensive experiments on three real-world datasets show that SDIL can effectively capture both static and dynamic interest and outperforms state-of-the-art baselines.", "sections": [{"title": "CCS CONCEPTS", "content": "\u2022 Information systems \u2192 Recommender systems."}, {"title": "KEYWORDS", "content": "Sequential Recommendation, Temporal Information Modeling, Attention Mechanism"}, {"title": "1 INTRODUCTION", "content": "Sequential recommender systems (SRSs) aim to generate sequential recommendations by predicting the next item which interests a given user. SRSs generally model the user's dynamic and timely interest in items from a sequence of historically interacted items for the recommendation. Benefiting from the strength of well-capturing users' interest changes over time, SRSs are able to provide more accurate and timely recommendation results to users [29, 31].\nA variety of SRS methods based on different models including Markov chain models, latent representation learning models, and deep learning models have been proposed in recent years. For instance, Markov chain-based SRSs adopt Markov chain models to model the first-order transitions over user-item interactions within an interaction sequence to predict the probable next item [21, 23]. Distributed representation learning-based methods map each interaction into a latent representation via capturing the contextual information for next-item recommendations [12]. In recent years, advanced models including shallow or deep neural models have been employed in SRSs to improve the recommendation performance. For example, Recurrent Neural Networks (RNN) have been utilized to model the long- and short-term sequential dependencies in a sequence of interactions for next-item recommendations [10]. Convolutional Neural Network (CNN) [37] and self-attention [14] models have been incorporated into SRSs for capturing more complex sequential dependencies (e.g., skip dependencies or collective dependencies) for next-item recommendations. These SRS methods have achieved remarkable recommendation performance.\nAlthough remarkable recommendation performance has been achieved, there are still some significant gaps which prevent the further performance improvement of existing SRS methods. To be specific, most of the existing SRSs only model users' dynamic interest in items while overlooking users' static interest [39] (Gap 1). However, static interest is also an important factor to determine which item to interact with for a given user. In this paper, dynamic interest means users' interest in specific items, which are captured from the ID information of a sequence of items interacted by each user. For instance, a user may be interested in iPhone 12. Item ID often changes frequently in most sequences, leading to frequent fluctuations of users' dynamic interest. Static interest refer to users' interest at a relatively higher level (e.g., item category or brand) and are usually more stable and change less frequently. For instance, a user may be interested in Apple products. Obviously, static interest changes less frequently than dynamic interest.\nMore importantly, most of the existing SRSs cannot thoroughly capture users' dynamic interest since they often only model the positive excitation while overlooking negative one (Gap 2). In SRSs, the positive (resp., negative) excitation refers to one prior interaction has a positive (resp., negative) impact on the next interaction. For instance, as an individual user, Alice may purchase a lens after she has purchased a Canon camera. However, it is unlikely that she will purchase another Nikon camera in a short period. In such a case, the purchase of Canon camera has positive excitation on the purchase of lens and negative excitation on that of Nikon camera. Clearly, negative excitation is an important signal to indicate which item may not be of the user's interest at the next time point and thus is necessary for accurate sequential recommendations.\nAlthough a few works in the literature have tried to model excitation for sequential recommendations, they generally only consider the positive excitation while ignoring the negative one [26] [32]. For example, Wang et al. [26] defined a short-term and life-long positive self-excitation function for next-item recommendations.\nAiming at bridging the aforementioned two significant gaps in exiting SRSs, we propose a novel Static-Dynamic Interest Learning (SDIL) framework. SDIL is able to comprehensively model users' static and dynamic interest in items for generating accurate sequential recommendations. SDIL consists of (1) a Static Interest Modeling (SIM) module, (2) a Dynamic Interest Modeling (DIM) module, and (3) a next-item prediction module. To be specific, given a sequence of items interacted by a user, the corresponding attribute information of these items is input into the SIM module to capture the static interest of the user. Meanwhile, the ID information of these items is imported into the DIM module to comprehensively capture the user's dynamic interest. Then, both types of interest are well integrated as the input of the downstream prediction module for the next-item prediction.\nMore importantly, we devise a novel Temporal Positive and Negative Excitation Modeling (TPNE) framework to quip the DIM module, which constitutes the main contribution of this work. Inspired by [26], TPNE is built on the basis of the temporal point process (TPP) [22], which is a classical model for modeling discrete event sequences in a continuous time period. In TPNE, first, a relation-based temporal module is designed to model the positive or negative excitation of each of a user's historical interactions on his/her next choice on candidate items. Here, the relation mainly refers to the substitute/complementary relations between items, which are extracted from the \"co-click\" (of items), \"co-purchase\" data. Then, a novel time decay kernel function is particularly designed to measure the excitation strength of each historical interaction according to the time interval between it and the next choice. Time interval is a key factor to determine whether the excitation strength is strong or weak. [26, 32]\nThe main contributions of this work can be summarized below:\n\u2022 We propose a novel Static-Dynamic Interest Learning (SDIL) framework for comprehensively modeling users' interest in items. SDIL consists of a static interest modeling (SIM) module, a dynamic interest modeling (DIM) module, and a next-item prediction module.\n\u2022 We devise a novel Temporal Positive and Negative Excitation Modeling (TPNE) framework to quip the DIM module. TPNE is good at modng both the positive and negative excitation as well as the excitation strength.\n\u2022 We evaluate our SDIL on three real-world datasets with different characteristics. Extensive experiments not only show the consistent superiority of SDIL over state-of-the-art baselines but also verify the rationality and effectiveness of our design in SDIL."}, {"title": "2 RELATED WORK", "content": "2.1 Sequential Recommendation\nSequential recommendation intends to recommend items that may interest users by modeling the sequential dependencies across users' historical interactions. A variety of methods have been proposed for the SRS task including rule-based methods [35], KNN-based methods [13], Factorization machine-based methods, and Markov chain-based methods [7, 21]. Nevertheless, these methods fail to model the long user behavior sequences in terms of the limited representation ability of models. Recent years have witnessed the success of deep learning-based methods applying in user modeling [8, 9], various models like CNN-based model [37], RNN-based models [10] [11], self-attention based models [14] [38] were explored for SRS task. We briefly review attention-based SRS methods, which are most relevant to our work. The self-attention mechanism was proposed by Vaswani et al. [24]. Benefiting from its strong ability to model the correlations among context information, many works employed it in SRS by intensifying the correlation between important historical items and target items. Li et al. proposed the NARM [16], which leverages a global and local encoder with an attention module to model both user short- and long-term interest. Wang et al. [30]"}, {"title": "2.2 Time-sensitive Recommendation", "content": "Temporal recommendation considers the temporal evolution of user interest in item transition, represented by methods based on matrix factorization. TimeSVD++ [15] achieved strong results by splitting time into several bins of segments and combining it into the CF framework. Bayesian Probabilistic Tensor Factorization (BPTF) [34] is proposed to include time as an extra dimension for the tensor factorization, which is an extension of traditional MF methods.\nAfter that, a variety of works attempt to model the long- and short-term user interest separately. For instance, Zhu et. al. [41] propose the TimeLSTM to capture users' long and short-term dynamic interest through the specific time gates. Cai et. al. [1] propose the LSHP, combining the short-term and long-term Hawkes Processes based on different segments of user history to predict the type and time of the next action in sequential online interactive behavior modeling. Wang et. al. [26] also leverages the Hawkes Processes to model the user repeat consumption behavior from the perspective of short- and life-long terms. After that, Wang et al. [27] designs the multiple forms of temporal decay functions for item relations in the user interaction history. Recently, Wang et al. [25] have devised relational intensity and frequency domain embeddings to adaptively determine the importance of historical interactions. Besides, Fan et.al. [5] design a new framework TGSRec upon a pre-defined continuous-time bipartite graph, which can capture collaborative signals from both users and items, as well as consider temporal dynamics within sequential patterns. Although these time-sensitive methods well model the user's dynamic interest changes, they do not explicitly model the user's relatively stable interest. Meanwhile, most methods attempt to learn the positive temporal collaborative signals. but ignoring the effectiveness of negative temporal signals."}, {"title": "3 PRELIMINARY", "content": "3.1 Problem Formulation\nLet $U = \\{u_1, u_2, ..., u_{|U|} \\}$ and $V = \\{v_1, v_2, ..., v_{|V|} \\}$ be user set and item set respectively. For each user $u \\in U$, there is a chronological sequence of items interacted by u, denoted as $H_u = \\{v_1, v_2, ..., v_n\\}$, where n is the length of the sequence $H_u$. For each item $v_i \\in H_u$, it is associated with a set of attribute information including item category $c_i$, item brand $b_i$, item price $p_i$ and interaction times-tamp $t_i$. Generally, for a user u, given her/his sequence context $O = \\{v_1, v_2, ..., v_{n-1}\\}$ together with the associated item attribute information, the task of an SRS is to predict the next item (target item) $v_n$ which may interest u."}, {"title": "3.2 Hawkes Processes in Sequential Modeling", "content": "Hawkes Processes is one of the most classic evolutionary processes [3, 6]. The traditional Hawkes Processes is defined by the following conditional intensity function:\n$\\lambda(t) = \\mu(t) + \\sum_{t_i<t} \\varphi(t - t_i),$\\nwhere $\\mu(t)$ represents the base intensity of the model. t and $t_i$ denote the happening time of the target event and that of the last event respectively. Hence, $t - t_i$ denotes the time interval between the target event and the last event. $\\varphi(.)$ denotes the exciting function of a Hawkes Process.\nIn practice, in the sequential recommendation scenario, a user's interest changes over time, and thus the interest evolution process can be formalized as a Hawkes Process. Accordingly, the first term $\\mu(t)$ in Eq (1) can be used to indicate the basic user interest in a target item $v_n$. The second term in Eq (1) means the accumulative impact of all the historical context items bought before time t on the user's interest at time t."}, {"title": "4 METHODOLOGY", "content": "4.1 Overview\nThe framework of our proposed model is shown in Figure 2. Our model is mainly composed of (1) a temporal dynamic user interest modeling module, (2) a static user interest modeling module, and (3) a next-item prediction module. The temporal dynamic user interest modeling module models users' dynamic interest which changes over time by taking the ID information of historically interacted items as the input. Specifically, both the positive and negative excitation of each historical interaction on the user's next choice will be carefully considered. In addition, the temporal information (i.e., the time interval between a historical interaction and the next interaction) is well utilized to measure how positive or negative the excitation is. The static user interest modeling module models users' static interest in items based on static attribute information of items (e.g., item category, item brand) which are interacted by each user. Finally, both the learned dynamic interest and static interest are well integrated into the prediction module for the recommendation of the next item."}, {"title": "4.2 Users' Dynamic Interest Modeling", "content": "Temporal point processes (TPPs) are a flexible and powerful paradigm that is good at modeling discrete event sequences localized in a continuous time period. A sequence of items interacted by a given user in a period well falls into this scenario. Hence, it is natural to take a TPP as the basic structure to model user-item interaction sequences. However, a classical TPP usually considers the positive excitation of each prior event (e.g., user-item interaction) on the following event (e.g., a user's next choice) and cannot model the possible negative excitation. Hence, they cannot be directly utilized to model both positive and/or negative excitation of a user's historical interactions on his/her next choice. To this end, we devise a novel Temporal Positive and Negative Excitation learning (TPNE) module to model both positive and negative excitation of historical interactions to well capture the user's dynamic interest. More importantly, TPNE is able to specify the excitation strength according to the happening time of each historical interaction.\nTo be specific, we take the TPP approach as a base architecture to build TPNE for capturing users' dynamic interest. The most classic model for the TPP is the Hawkes process, which is formulated as:\n$\\lambda_{\\tau}(t) = \\lambda_0 + \\sum_{i:t_i<t_n} \\varphi(t_n - t_i),$\nwhere $\\lambda_0$ denotes the user's basic interest in a candidate item. $\\sum_{i:t_i<t_n} \\varphi(t_n - t_i)$ denotes the historical user-item interaction's excitation on target item $v_n$. For the first term, it can be calculated as:\n$\\lambda_0 = e_n e_{v_n}^T + u_b + i_b,$\nwhere $e_n$ denotes the user historical interest embedding revealed from its historically interacted items and $e_{v_n}$ denotes the target item ID embeddings. $u_b$ and $i_b$ denote the user bias and item bias respectively. To well model the relationship between $e_h$ and $e_{v_n}$, we adopt the self-attention module to train the basic interest value $\\lambda_0$.\nGiven a user u, the input sequence is made up of item IDs sequence $H_u = \\{v_1, v_2, ..., v_n\\}$. To obtain a unique dense embedding for each item ID, we use a linear embedding layer. Then we can get the input item embedding matrix $E \\in R^{|V|xd}$(|V| denotes the number of items and d denotes the dimension of item embeddings). For each item, its embedding $e_v = E(v) \\in R^{1xd}$. Then we can input $H_u$'s item IDs to E and obtain the user u's item embedding sequences and represent it as a matrix $E(H_u)$. Then, following previous works [14, 32, 33], we introduce the position embedding $pos_i$ in each timestamp so as to add the position information of historical items. The input are item ID embeddings' matrix $E(H_u)$ and corresponding position embeddings $POS_i$:\n$A_i = Att((E(H_u)+POS_i)W^Q, (E(H_u)+POS_i)W^K, (E(H_u)+POS_i)W^V),$\n$Att_i = Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V,$\nwhere Q, K, and V denote query, key, and value respectively. $W^Q, W^K$ and $W^V$ denote the linear transformation matrices.\n$E(H_u) = LayerNorm(H_u + Dropout(FFN(A_i)).$\nSimilar to previous works [32, 33], we introduce the LayerNorm, Dropout, and FFN layers to alleviate the over-fitting problem. Finally, we adopt the average pooling operation to get the user historical interest embedding $e_h$:\n$e_h = \\frac{1}{|N|-1}\\sum_{i=1}^{n-1} E(H_u)_i,$"}, {"title": "4.2.1 Positive Excitation Learning", "content": "For seeking the positive correlation between historical items O and target item $v_n$, we introduce four different explicit relations: also_buy ($r_1$), also_view ($r_2$), share_brand ($r_3$) and similar_item ($r_4$) contained in the datasets. For also_buy and share_brand relations, we regard them as complementary relations. While, for also_view and similar_item(two items have a similar price within the same category), we regard them as substitute relations. Then positive excitation learning can be defined as:\n$\\varphi_p(t_n - t_i) = \\sum_i I_{rp}(v_i, v_n)K_1(t_n - t_i),$\nwhere $I_{rp}$ denotes the indicator function, if historical item $v_i$ has a relation $r \\in \\{r_1, r_2, r_3, r_4\\}$ with $v_n$, then the $I_{rp}$=1, otherwise 0. It bridges the excitation between context information and target user interest. $K_1(.)$ denotes the positive temporal kernel function, which is composed of two parts as follow:\n$K^p(\\Delta t_i) = N(\\Delta t_1 | 0, \\sigma_1) + N(\\Delta t_1 | \\mu_2, \\sigma_2).$\nPrevious works [26, 32] commonly model the short-term positive effect of historical items on target item, which only leverages the complementary information($r_1$ and $r_3$) to obtain the positive excitation. We also model this positive complementary relation signal in the first term $N(\\Delta t_1 | 0, \\sigma_1)$, ($\\Delta t_1 = t_n - t_i$ represents the time interval between the target item $v_n$ and the related historical item $v_i$). $\\sigma$ is decided by item ID $v_i$, which is item-specific parameter. We leverage a normal distribution function to simulate the user dynamic interest changes. For example, if a user buys a mobile phone, who may have a high possibility to buy accessories for the mobile phone in a short-term window, such as mobile phone film, matching earphones and so on. However, with the growth of time and the aging of the mobile phone, the interest of users to buy the corresponding supporting products would gradually decrease. Thus, we choose a mean equals to 0 normal distribution to model the user interest decay process. For the second term, which happens because many items purchased by users have corresponding lifespans. When the product life cycle is just approaching, the probability of users purchasing similar substitute products will be a strong positive incentive. For example, if a user buys a mobile phone, and he/she needs to replace a brand new mobile phone one year later, then this will bring a positive excitation for the substitute relationship of similar items. We model this temporal relationship using a normal distribution with mean $\\mu$, which is also related to $v_i$, which assumes the user begins to buy the substitute products after $\\mu$ time interval."}, {"title": "4.2.2 Negative Excitation Learning", "content": "On the other hand, for seeking the negative correlation between historical items O and target item $v_n$, we leverage two different explicit relations: also_view ($r_2$) and similar_item ($r_4$) contained in the datasets. We treat them as substitute relations and define the negative excitation learning as:\n$\\varphi_n(t_n - t_j) = \\sum_j I_{rn}(v_j, v_n)K_2(t_n - t_j),$\nwhere $I_{rn}$ denotes the indicator function, if historical item $v_j$ has a relation $r \\in \\{r_2, r_4\\}$ with $v_n$, then the $I_{rn}$=1, otherwise 0. $K_2(.)$ denotes the negative temporal kernel function:\n$K_2(\\Delta t_2) = -N(\\Delta t_2 | 0, \\sigma_3),$\nwhere $N(\\Delta t_2 | 0, \\sigma_3)$ is a normal distribution of $\\Delta t$($\\Delta t_2 = t_n - t_j$) with 0 mean and $\\sigma_3$ standard deviation. $\\sigma_3$ is also related to $v_j$. From an empirical analysis, users rarely buy a large number of substitutes of the same category in a short period of time. For example, a simple example, a user has just purchased a Mac laptop, so the probability of him/her buying another Lenovo laptop in the short term would be very low. However, with time goes by, the negative effect would decrease gradually, so we use a negative normal distribution with $\\mu = 0$ to fit this process. (Noting that $\\mu, \\sigma_1, \\sigma_2$ and $\\sigma_3$ are all learn-able parameters.)\nIn summary, two different temporal patterns of SRS are modeled positive excitation learning processes and negative excitation learning processes. Since these two important temporal patterns occur frequently across the whole user-item interaction history, they can provide extra-temporal information to help us capture the dynamics of users' interest."}, {"title": "4.3 Users' Static Interest Modeling", "content": "In real-world scenarios, users also have relatively fixed preferences in sequential decision-making, such as high loyalty to a certain brand and the choice of the item price, which are relatively stable in the long term and not easy to change. Therefore, we believe that this part of the more stable user interest should also be included in the modeling to achieve more accurate recommendations.\nTo represent the users' relatively stable interest in their interaction history, we adopt the feature-based self-attention module to model the user's static interest. Because item IDs do not clearly indicate fine-grained attributes that users are interested in, we leverage the feature-based self-attention block to search for users' preferences on static feature-level patterns. Specifically, we project the discrete and heterogeneous attributes of items into low-dimensional dense vectors and fuse these vectors, then use multi-head self-attention to model the user's static interest. Without loss of generality, we choose the category, brand, and price of three different side information to comprehensively represent different aspects of one item. We also use a linear embedding layer to obtain category embedding matrix $C \\in R^{|C|\\times d}$, brand embedding matrix and $B \\in R^{|B|\\times d}$ price embedding matrix $P\\in R^{|P|\\times d}$(|C|,|B|,|P| denotes the number of categories, brands and price bins respectively). Furthermore, the $c_i \\in R^{1xd}$, $b_i \\in R^{1xd}$ and $p_i \\in R^{1xd}$ to denote each item different feature embeddings respectively. Our approach is easily extended to include more features. We adopt a simple yet effective additive feature fusion mode:\n$f_i = c_i + b_i + p_i,$\nwhere $f_i \\in F$ denotes the fused feature-level item $v_i$ representation, F denotes the stacked fusion matrices. Then for each user u, we can get his/her fused feature sequence $\\{f_1, f_2, f_3,...,f_n\\}$. We utilize the multi-head self-attention network [24] to model the user's historic static interest. In the feature-level self-attention module, the query(Q), key(K), and value(V) are the same and equal to F, and we use three different projection transformation matrices to convert them into the same subspace:\n$H_f = Att(FW^Q, FW^K,FW^V).$\nNoting that we do not use the position here because we assume that user static interest is relatively stable and independent of user-item interaction timestamp. Where $W^Q, W^K, and W^V$ are the linear transformation matrices. Then we adopt the multi-head attention mechanism to gain more semantic information from different subspaces.\n$M_f = Multihead(F) = Concat(h_1, h_2, ..., h_{l_f})W^O,$\n$h_i = Att(FW_i^Q, FW_i^K, FW_i^V),$\nwhere $l_f$ denotes the length of different heads, and here we also apply the layer normalization, FFN layers and residual connection module following previous works [39] [33], alleviating the over-fitting problem in the training phase.\n$M_f = LayerNorm(M_f + H_f),$\n$FFN(M_f) = ReLU(M_fW_1 + b_1)W_2 + b_2,$\nwhere $W_1,W_2$ and $b_1, b_2$ denote the learnable parameters, we sum pooling all the $h_i$ in each user sequence to obtain the user static interest representation $e_s$.\n$e_s = \\frac{1}{|N|-1}\\sum_{i=1}^{N-1} H_f,$\n$e_s$ provides more stable user interest on certain attributes (i.e. brand, price in our case), such characteristics may not be easily perceived but are indeed potential consumption habits of users."}, {"title": "4.4 Next-item Prediction", "content": "Since both dynamic user interest and static user interest are vital in SRS task, it is crucial to combine them and as the input of the downstream prediction module for next-item prediction. Hence, inspired by previous works [18, 36], we leverage the gate fusion module to fuse the dynamic embedding $e_h$ and static interest embedding $e_s$ so as to better next-item prediction.\n$g = \\sigma(W_1e_s + W_2e_h + b),$\n$e_f = g \\odot e_s + (1 - g) \\odot e_h,$\nwhere $W_1,W_2,$ and b are the learnable parameters in the gating layer. $\\odot$ denotes the element-wise multiplication and $g \\in R^{1xd}$ denotes the learnable gate. Since there is some redundant information between user dynamic interest and static interest, we adaptively pass the informative messages and restrain the useless ones to obtain the final interest representation $e_f$. In order to learn the parameters of the SDIL framework, we adopt the pairwise ranking loss (BPR loss) [20] to optimize our model:\n$L_r = - \\sum_{u\\in U}\\sum_{i\\in N_u} log \\sigma (\\hat{y}_{ui} - \\hat{y}_{uj}),$\n$\\hat{y}_{ui} = e_u e_i^T + \\lambda_{T,i}, \\hat{y}_{uj} = e_u e_j^T + \\lambda_{T,j}$\nwhere $\\sigma$ represents the sigmoid function, $\\hat{y}_{ui}$ represents the final preference score of user u to positive item i while $\\hat{y}_{uj}$ represents the final preference score of user u to negative item j."}, {"title": "5 EXPERIMENTS", "content": "5.1 Experimental Setting\n5.1.1 Dataset Preparation and Baselines. We conduct experiments on publicly accessible Amazon datasets [19]. We choose three representative sub-datasets from Amazon datasets: Beauty, Cell Phones and Accessories (Cellphones) and Toys and Games (Toys) and keep the '5-core' datasets [40][21][2], which filter out user-item interaction sequences with length less than 5. The statistics of evaluation datasets are shown at Table 2 in Appendix A. Following the previous works[27][26], we take \"also buy\" as complementary relations and \"also view\" as substitute relations between items, i.e., items $v_1$ and $v_2$ are complementary if most users who buy $v_1$ also buy $v_2$. In addition, we introduce two more effective item relationships: (1) items of the same brand have complementary relations (e.g., iPhone and AirPod), and (2) items of the same category with similar prices have substitute relation (e.g., Canon cameras and Nikon cameras). Furthermore, we also take some item attribute information into account, including fine-grained item category, item brand and item price. Item category and brand are categorical features and we use the unique one-hot encoding to represent them. For price information, we bin and cut them into 10 intervals.\nFor reproducibility, we follow the commonly used benchmark setting of ReChorus [26, 27] to set up our experiments. Specifically, for each user, we first discard duplicated interaction sequences and sort the items in each user's sequence chronologically by their timestamp. Furthermore, the maximum length of interaction sequences is set to 20. If there are more than 20 interactions in a sequence, we adopt the latest 20 interactions. If the number of interactions in a sequence is less than 20, we make it up to 20 by padding"}, {"title": "5.2 Performance Comparison with Baselines", "content": "Table 1 reports the comparison results between our method and 10 different baseline methods on three datasets. The best results and the second-best results across all methods are in bold and underlined respectively. We have the following observations:\n(1) First, we can find that all the non-time-sensitive sequential methods such as GRU4Rec [10, 11], Caser [37] surpass the non-sequential method BPR, which indicates the effectiveness of modeling sequential information. Meanwhile, NARM utilizes both global and local encoders to model the long- and short-term interest of users and thus performs better than GRU4Rec and Caser. Moreover, the self attention-based SRS, i.e., SASRec [14], consistently outperforms previous traditional deep learning methods including CNN-based Caser and RNN-based GRU4Rec, proving the effectiveness of using the attention mechanism to encode sequence data.\n(2) Second, more advanced time-sensitive sequential models, such as TiSASRec [17], generally inherit the attention-based encoder while introducing extra-temporal signals to further improve the recommendation performance. Specifically, TiSASRec explicitly models both the absolute positions of items as well as time intervals between them in a sequence, and thus it outperforms SARS w.r.t most evaluation metrics on most datasets. SLRS+ [26] introduces both knowledge-graph information and the multi-excitation of items to further enhance the recommendation performance. Chorus [27] further models the different situations of the temporal effect of user-item interactions on item embedding level and thus outperforms SLRS+. However, due to the limited representation ability of its utilized base model, Chorus shows the over-fitting issue. AHMP [32] models the user temporal category-level consumption excitation via self-attention models, achieving the third-best results on most datasets. KDA [25] designs the virtual relations between items to enhance the relation learning between items, which helps KDA obtain a significant improvement over other baselines.\n(3) Third, our proposed SDIL model consistently outperforms all baselines in all datasets. The average improvement on the Cellphones dataset compared with the best-performing baseline is 10.45% w.r.t MRR. The reason is obvious. Different from most of the existing SRS methods, which ignore modeling either users' relatively stable static interest, or the negative excitation of historical interactions on user's next choice, our proposed SDIL model has been carefully designed to well address these issues. To be specific, SDIL not only well models users' static interest in items via carefully capturing users' interest on item attribute level, but also comprehensively model users' dynamic interest via the well-designed TPNE module. More importantly, TPNE is able to model both positive and negative excitation of users' historical interactions on their next choice, leading to significant performance improvement."}, {"title": "5.3 Ablation Study", "content": "To analyze the rationality and effectiveness of our designed components in the SDIL model, we conduct the ablation study with three variants. SDIL-1: Only uses static interest module to predict the next item; SDIL-2: Only uses dynamic interest module to predict the next item; SDIL-3: Uses both dynamic and static interest module to predict the next item without modeling excitation. Figure 3 presents the performance of these three variants and the full model SDIL on the three experimental datasets. First, compare SDIL-1 and SDIL, we find the performance of SDIL-1 significantly decreases when dropping the dynamic interest modeling module TPNE from SDIL. This proves the extremely importance of our designed TPNE module. Second, the comparison between SDIL-2 and SDIL demonstrates that static interest modeling is also vital for recommendation performance. The performance clearly decreases without it. Third, the comparison between SDIL-3 and SDIL demonstrates the importance of modeling historical interactions' excitation on users' next choice. In summary, the dropping of any of our designed components would lead to a clear reduction in recommendation performance, this effectively verifies the effectiveness and rationality of our design."}, {"title": "5.4 The Effectiveness of Modeling Negative Excitation", "content": "To verify the rationality of incorporating negative excitation and the effectiveness of our proposed TPNE module, we keep the SDIL framework while changing its TPNE module to TPE to obtain another variant SDIL-TPE. TPE models users' dynamic interest by only considering temporal positive excitation. The result is provided in Appendix B. As Table 3 shows, SDIL consistently surpasses SDIL-TPE, indicating that the comprehensive modeling of both positive and negative excitation is more effective compared with the modeling of only positive excitation."}, {"title": "5.5 Parameter Sensitivity Test & Case Study", "content": "We conduct the parameter sensitivity test to see how the key model parameters will affect the performance of our SDIL model. The result is shown in Appendix C to save space. In addition, we conduct case studies to show some insights on how our proposed model can work better in a straightforward way. Due to the limited space, the details are provided in Appendix D."}]}