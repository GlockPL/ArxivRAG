{"title": "Q-PETR: Quant-aware Position Embedding Transformation for Multi-View 3D Object Detection", "authors": ["Jiangyong Yu", "Changyong Shu", "Dawei Yang", "Zichen Yu", "Xing Hu", "Yan Chen"], "abstract": "PETR-based methods have dominated benchmarks in 3D perception and are increasingly becoming a key component in modern autonomous driving systems. However, their quantization performance significantly degrades when INT8 inference is required, with a degradation of 58.2% in mAP and 36.9% in NDS on the NuScenes dataset. To address this issue, we propose a quantization-aware position embedding transformation for multi-view 3D object detection, termed Q-PETR. Q-PETR offers a quantization-friendly and deployment-friendly architecture while preserving the original performance of PETR. It substantially narrows the accuracy gap between INT8 and FP32 inference for PETR-series methods. Without bells and whistles, our approach reduces the mAP and NDS drop to within 1% under standard 8-bit per-tensor post-training quantization. Furthermore, our method exceeds the performance of the original PETR in terms of floating-point precision. Extensive experiments across a variety of PETR-series models demonstrate its broad generalization.", "sections": [{"title": "1. Introduction", "content": "3D perception is crucial for autonomous driving and embodied intelligence [16, 17, 31, 62, 67]. PETR-based methods [29, 30, 43, 49, 50, 52, 56] have gained significant attention due to their effectiveness in this domain. Unlike previous detection methods relying on dense convolutional features [17, 37, 59\u201361] or utilizing 3D-to-2D projections as in DETR3D-based approaches [25, 28, 51], PETR adapts the paradigm of 2D object detection with transformers (DETR) [5] by incorporating 3D positional encoding, achieving seamless end-to-end 3D obstacle detection.\nDespite its advantages, PETR requires substantial storage and computational resources during inference, posing challenges for on-device deployment. To enhance inference efficiency, considerable efforts have been dedicated to efficient model compression [11, 12, 19, 20, 27, 42], with quantization being particularly favored for deploying models on AI chips by representing networks in low-bit formats [7, 32, 39, 54].\nHowever, deploying complex models like PETR on edge devices presents additional challenges. Autonomous vehicles rely on edge AI chips with limited computational capabilities. Nonlinear functions such as softmax, sigmoid, SiLU, and GeLU are commonly used during inference, but most edge AI chips lack sufficient floating-point performance for these operations, exacerbating deployment difficulties.\nMoreover, quantization awareness is often overlooked in neural architecture design, leading to significant performance degradation during deployment. Notably, models like MobileNet [15, 38, 40, 64], EfficientNet [4, 13, 45], and RepVGG [8] face quantization issues requiring additional design efforts or advanced strategies to mitigate deployment challenges. PETR is no exception; when deployed on in-vehicle AI chips, standard 8-bit per-tensor post-training quantization (PTQ) results in a significant performance drop, with a 58.2% decrease in mAP and a 36.9% decrease in NDS.\nIn this paper, we address the quantization issues of PETR to facilitate its deployment on edge AI chips in autonomous vehicles. We conduct an in-depth analysis of the mechanisms behind PETR's quantization failure, revealing that the significantly larger magnitudes of positional encodings compared to image features, and the highly imbalanced scaled dot-products in cross-attention, are the primary causes of quantization collapse. Based on these insights, we propose Q-PETR (Quantization-aware PETR), a model that mitigates quantization collapse and even improves floating-point performance compared to the original PETR."}, {"title": "Our contributions are summarized as follows:", "content": "Our contributions are summarized as follows:\n\u2022 Identifying the root causes of quantization collapse in PETR: We uncover that the imbalanced output of the inverse-sigmoid function and excessively large values in positional encoding, as well as the imbalanced scaled dot-products in cross-attention, are key issues.\n\u2022 Redesigning the positional encoding and quantization strategy: We reformulate the positional encoding and improve the quantization approach for the scaled dot-product in cross-attention, enhancing both floating-point performance and quantization friendliness.\n\u2022 Introducing DuLUT for efficient nonlinear function inference: We propose DuLUT, an improved lookup table [46] approach that reduces the number of table entries without compromising precision, optimizing nonlinear function inference on edge AI chips.\n\u2022 Demonstrating generalizability and deployment readiness: Our method generalizes well across different model scales, achieving exceptional post-quantization performance suitable for deployment on edge devices."}, {"title": "2. Related Work", "content": "3D Object Detection. Surround-view 3D object detection is essential for autonomous driving and is generally categorized into LSS-based [16, 17, 59] and transformer-based [29, 43] approaches. LSS-based methods project multi-camera features onto dense BEV (Bird's Eye View) representations [37], but their high memory consumption hinders efficient long-range perception. Transformer-based methods leverage sparsity to enhance long-distance perception. Among these, the PETR series has gained significant attention. PETR [29] transforms 2D image features into 3D representations using 3D positional encoding. PETRv2 [30] introduces temporal feature indexing, while StreamPETR [49] extends temporal query processing. Some works [9, 48, 52] accelerate processing by incorporating 2D detection priors. CMT [56] fuses vision and LiDAR point clouds. Improvements to PETR's positional encoding have also been explored [14, 43]. Additionally, PETR has been integrated into the Omnidrive framework [50] to enhance 3D perception with large models.\nQuantization. Quantization compresses models by converting weights and activations from floating-point to lower-bit integer representations [3, 6, 7, 65]. Among various methods [1, 2, 32, 39, 53, 54], we focus on uniform symmetric quantization, mapping floating-point values $X_f$ to discrete k-bit integer values $x_q$ as:\n$$x_q = \\text{clamp} (\\lfloor \\frac{X_f}{s} \\rceil, -2^{k-1}, 2^{k-1} - 1),$$(1)\nwhere s is the scaling factor computed as:\n$$s = \\frac{X_f^{\\text{max}} - X_f^{\\text{min}}}{2^k}$$(2)"}, {"title": "3. Quantization and Deployment-Friendly Adaptation of PETR", "content": "In this section, we aim to improve PETR's quantization performance. We begin by introducing the principles of PETR (\u00a73.1), identify its quantization failures (\u00a73.2), and provide strategies to address these challenges (\u00a73.3).\n3.1. Preliminaries\nPETR enhances 2D image features with 3D position-aware properties using camera-ray positional encoding (PE), enabling refined query updates for 3D bounding box prediction. Specifically, surround-view images I pass through a backbone to generate 2D features $f_{2D}$, while camera-ray PE pe is computed using camera intrinsics and extrinsics. The learnable query embeddings q serve as the initial queries Q for the decoder. Here, $f_{2D}$ serves as the values V, and adding pe to $f_{2D}$ element-wise forms the 3D position-aware keys K."}, {"title": "3.2. Quantization Failure of PETR", "content": "We evaluate the performance of several PETR configurations [29] using the official code. Under standard 8-bit symmetric per-tensor post-training quantization (PTQ), PETR suffers significant performance degradation, with an average drop of 58.2% in mAP and 36.9% in NDS on the nuScenes validation dataset\nLayer-wise Quantization Error Analysis. Quantizing a pre-trained network introduces output noise, degrading performance. To identify the root causes of quantization failure, we employ the signal-to-quantization-noise ratio (SQNR), inspired by recent PTQ advancements [35, 36, 57]:\n$$SQNR_{q,b} = 10 \\log_{10} (\\frac{\\sum_{i=1}^{N} E[F_0(x_i)^2]}{\\sum_{i=1}^{N} E[e(x_i)^2]})$$(3)\nHere, N is the number of calibration data points; $F_0$ denotes the full-precision network; the quantization error is $e(x_i) = F_0(x_i) - Q_{q, b}(F_0(x_i))$; and $Q_{q,b}(F_0)$ represents the network output with quantization applied at bit-width b, while other layers remain at full precision.\nSince 8-bit weight quantization incurs minimal information loss, we focus on quantization errors related to operator inputs. Using the PETR configuration from the first row of Table 1, we obtain layer-wise SQNRs, depicted in Fig. 1. From these results, we identify three primary observations contributing to quantization errors:\nObservation 1: The inverse-sigmoid operator disrupts feature distribution balance. As indicated by the red arrow in Fig.1, quantization difficulties stem from PETR's positional encoding module. Analyzing its construction (Fig.5(a)), we find that the inverse-sigmoid operation induces an imbalanced feature distribution. Specifically, Fig. 2 shows that before applying inverse-sigmoid, the feature distribution is balanced and quantization-friendly, whereas afterward, it exhibits significant outliers. Thus, avoiding the inverse-sigmoid operator in the positional encoder is advisable.\nObservation 2: Disparity in magnitude between camera-ray PE and image features leads to information loss after quantization. The issue highlighted by the purple arrows in Fig.1 indicates that applying 8-bit symmetric linear quantization to the 3D position-aware key K significantly degrades performance. Since K is derived by element-wise summing image features and camera-ray PE, we quantize them using K's quantization parameters, as visualized in Fig.3. Clearly, image features occupy only 4 bits, causing severe information loss. Designing a positional encoding with a magnitude distribution similar to image features is crucial to address this quantization issue.\nObservation 3: The scaled dot-product in cross-attention exhibits significant fluctuations along the head dimension. As highlighted by the green arrow in Fig.1, the scaled dot-product in cross-attention presents quantization challenges. Since the subsequent softmax operation is applied along the last dimension, we visualize the maximum values over this dimension in Fig.4. The feature distribution varies significantly across different heads, posing challenges for effective per-tensor quantization."}, {"title": "3.3. Quantization and Deployment Friendly Improvement.", "content": "Based on the analysis in Section 3.2, we identify two key areas for improvement: (1) designing a positional encoding with magnitude and distribution similar to image features, and (2) balancing the scaled dot-products in cross-attention across heads. We propose specific methods to address these challenges.\nPositional Encoding Adaptation. To reduce the magnitude of positional encoding in PETR, we analyze its construction process (Fig.5(a)). PETR samples 64 3D points along each camera ray per pixel, increasing magnitude variance. According to statistical theory, the variance of a sum increases with the number of variables. Inspired by 3DPPE [43], which introduces a lidar-centered lidar-ray PE using only one 3D point (Fig.5(b)), we design a quantization and deployment-friendly (QD-aware) lidar-ray PE (Fig.5(c)).\nOur QD-aware lidar-ray PE avoids both the inverse-sigmoid operator and sinusoidal calculations, making it suitable for quantization and deployment on edge devices. Specifically, we learn three anchor embeddings along each axis a \u2208 x, y, z, denoted $E_{i_a}$ with anchor locations $L_{i_a}$ for i = 1, 2, 3. For each lidar-ray point $(x_j, y_j, z_j)$, we identify the two closest anchors and compute interpolated embeddings:\n$$e_{i_x}^x = \\frac{L_{i_x+1}-x_j}{L_{i_x+1}-L_{i_x}} E_{i_x+1} + \\frac{x_j-L_{i_x}}{L_{i_x+1}-L_{i_x}} E_{i_x}$$,\n$$e_{i_y}^y = \\frac{L_{i_y+1}-y_j}{L_{i_y+1}-L_{i_y}} E_{i_y+1} + \\frac{y_j-L_{i_y}}{L_{i_y+1}-L_{i_y}} E_{i_y}$$,(4)\n$$e_{i_z}^z = \\frac{L_{i_z+1}-z_j}{L_{i_z+1}-L_{i_z}} E_{i_z+1} + \\frac{z_j-L_{i_z}}{L_{i_z+1}-L_{i_z}} E_{i_z}$$.\nThe QD-aware lidar-ray PE is then obtained via $\\text{MLP}([e_{i_x}^x; e_{i_y}^y; e_{i_z}^z])$, where [\u00b7] denotes concatenation along the channel dimension, and the MLP has a conv-relu-conv structure.\nQuantization Strategy for Scaled Dot-Product in Cross-Attention. In softmax operations, numerical stabilization (NS) subtracts the maximum value to prevent overflow. Traditional quantization quantizes before NS, leading to issues in Observation3. We propose quantizing after NS (Fig.6), and adaptively determining the optimal truncation lower bound to minimize softmax error.\nAfter NS, inputs to softmax are non-positive. Values below -20 approach zero after exponentiation, so we define a candidate set of scaling factors $S = {s_1, s_2, ..., s_N}$ with $s_i = 2^{i-1}$ for k-bit quantization. The dequantized input $\\hat{x}$ is:\n$$\\hat{x} = s_i \\cdot \\text{clamp} (\\text{round}(\\frac{x}{s_i}), -2^{k-1}, 2^{k-1} - 1).$$(5)\nensuring $\\hat{x} \\in [-i, 0]$. We compute the softmax distributions $p_f = \\text{softmax}(x_s)$ and $p_{\\hat{s}} = \\text{softmax}(\\hat{x}_{s_i})$, and select"}, {"title": "DuLUT for Non-linear Functions.", "content": "DuLUT for Non-linear Functions. Lookup table (LUT) is commonly used for integer inference of non-linear functions in quantized neural networks. Linear LUT can compute functions losslessly when the input quantization bit width is less than or equal to the number of LUT entries. However, for higher-bit inputs, conventional linear LUT become impractical due to exponentially increasing table sizes-requiring 256 entries for 8-bit inputs and 65,536 entries for 16-bit inputs-leading to significant storage and computational overhead.\nTo improve precision without excessive resource usage, non-linear LUT have been proposed. These methods allocate more entries to regions where the function changes rapidly and fewer entries where it remains flat, thus enhancing quantization precision in critical areas. However, non-linear LUT are complex to implement in hardware, necessitating additional processing to define input regions based on function characteristics, and are not efficiently supported by most hardware architectures. Approximation methods like I-BERT [18] and I-ViT [22] attempt to simplify computations but introduce approximation errors and increased computational steps, offsetting the benefits of quantization.\nTo address these challenges, we propose DuLUT, which uses two small linear tables tailored to the function's properties, enabling accurate non-linear approximation with significantly reduced table sizes. DuLUT splits the input domain into two regions-typically where the function is rapidly changing and where it is relatively flat-and employs a separate small linear LUT for each region. For example, with 8-bit quantization, DuLUT uses two tables of 32 entries each without compromising precision (see Fig.7 and Algorithm 2). We applied DuLUT to common activation functions like softmax, GELU, and SiLU. By utilizing DuLUT, we achieve the same precision as larger single-table lookups while significantly reducing SRAM overhead and maintaining computational efficiency."}, {"title": "4.2. Ablation Study", "content": "On\nProof of Position Encoding Equivalence We conducted experiments to verify whether the proposed QD-aware lidar-ray PE enhances floating-point performance over the original camera-ray PE. As shown in Tab. 2, QD-aware lidar-ray PE provides performance improvements. On PETR, it slightly increases mAP by 0.07 but significantly boosts NDS and mATE by 1.09 and 1.67, respectively. For Stream-PETR, our method yields substantial and balanced enhancements, with increases of 0.94 in mAP, 0.46 in NDS, and 0.22 in MATE.\nEffect of Anchor Embedding Quantity. The QD-aware lidar-ray PE uses three anchor embeddings per axis, obtained through linear interpolation. Experiments (Tab. 3) demonstrate that setting the number of anchor embeddings to 3 achieves the highest NDS and mAP scores. Adjusting this number either up or down results in lower performance, confirming that 3 is the optimal choice.\nQuantization Performance of Different Position En-codings To experimentally demonstrate the superior quantization performance of our proposed QD-aware lidar-ray PE, we focus solely on quantizing the positional encoding, keeping all other modules in floating-point computation. Detailed results are shown in Tab.4. The original camera-ray configuration loses up to 11.97% in mAP and 5.04% in NDS, whereas our QD-aware lidar-ray PE experiences minimal losses of only 1.42% in mAP and 1.15% in NDS. Figure8 further supports this finding; compared to the distribution in Fig. 3, the distribution of our QD-aware"}, {"title": "4.3. Validation on PETR-series Methods", "content": "after stabilization (N=20)\" from Tab. 5 as a baseline and evaluate the performance with different nonlinear function quantization methods applied on top of it. The specific results are shown in Tab. 7. We consider the carefully designed approximation methods I-Bert and I-Vit for different nonlinear functions. Due to the approximation errors introduced by these methods, many points are quantized away. Additionally, we compare with the LUT-based table lookup method and find that 256 entries are required for lossless quantization, while 128 entries lead to severe performance losses of 0.54 NDS and 0.37 mAP. In contrast, our newly proposed DuLUT with 128 entries achieves lossless quantization. Even when the number of entries is further reduced to 64, the quantization only results in a negligible loss of 0.08% NDS and 0.02% mAP, which can be considered negligible. This further demonstrates the superior quantization performance of our proposed DuLUT.\nWe evaluate the effectiveness of our proposed method on various PETR-series models from both floating-point and quantized performance perspectives, specifically considering single-frame PETR and temporal multi-frame Stream-PETR models.\nFirst, we analyze changes in floating-point performance (values in parentheses in Tab. 6). In single-frame PETR models, mAP and NDS generally improve across configurations, except for a slight decrease of 0.06 in NDS when using V2-99's P4 feature with 640\u00d71600 resolution images. mAP increases range from 0.07 to 0.69, while NDS shows significant gains in all cases, ranging from 0.87 to 1.24. For temporal multi-frame StreamPETR models, both mAP and NDS consistently improve, with mAP gains of 0.93 and 0.94, and modest NDS increases of 0.46 and 0.58. Notably, NDS improvements in temporal methods are smaller than in single-frame methods, mainly due to performance degradation in mASE and mAOE, suggesting that our QD-aware lidar-ray PE may not optimally capture scale and orientation information in temporal models. We plan to investigate this further in future work. Overall, QPETR shows significant improvements in most configuration metrics, demonstrating that our method surpasses the original PETR models in floating-point performance.\nNext, we analyze the quantized performance improvements (values in square brackets in Table 6). In single-frame PETR models, mAP and NDS drops are kept below 1% using our QD-aware lidar-ray PE and smoothing techniques. In temporal multi-frame StreamPETR models, mAP and NDS drops remain within 2.5%, likely due to accumulated quantization errors during temporal fusion. Overall, quantized QPETR models maintain high performance with minimal drops in both settings, demonstrating the effectiveness of our quantization strategies in preserv-"}, {"title": "5. Limitations", "content": "Although our method incurs almost no quantization accuracy loss, users need to replace the camera-ray in the original PETR series with our proposed QD-aware lidar-ray. The only drawback is that this requires retraining. However, from the perspective of quantization deployment, this retraining is beneficial, and the floating-point precision can even be improved."}, {"title": "6. Conclusion", "content": "In this paper, we address the significant performance drops of PETR models during quantization by identifying two main issues: the imbalance between positional encoding and image feature magnitudes, and uneven scalar dot-products in cross-attention. To resolve these, we introduce Q-PETR, a quantization-friendly positional encoding transformation that redesigns positional encoding and improves scalar dot-product quantization without sacrificing the original floating-point performance. Our experiments show that Q-PETR limits mAP and NDS drops to below 1% under standard 8-bit post-training quantization and even surpasses the original PETR in floating-point precision. Extensive tests across various PETR models demonstrate the method's strong generalization and deployment suitability. Future work will explore quantization error propagation in multi-frame temporal models and develop advanced quantization strategies to further reduce precision loss in temporal settings."}]}