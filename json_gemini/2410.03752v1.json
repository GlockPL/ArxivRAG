{"title": "Efficient Streaming LLM for Speech Recognition", "authors": ["Junteng Jia", "Gil Keren", "Wei Zhou", "Egor Lakomkin", "Xiaohui Zhang", "Chunyang Wu", "Frank Seide", "Jay Mahadeokar", "Ozlem Kalinli"], "abstract": "Recent works have shown that prompting large language models with audio encodings can unlock speech recognition capabilities. However, existing techniques do not scale efficiently, especially while handling long form streaming audio inputs not only do they extrapolate poorly beyond the audio length seen during training, but they are also computationally inefficient due to the quadratic cost of attention.\nIn this work, we introduce SpeechLLM-XL, a linear scaling decoder-only model for streaming speech recognition. We process audios in configurable chunks using limited attention window for reduced computation, and the text tokens for each audio chunk are generated auto-regressively until an EOS is predicted. During training, the transcript is segmented into chunks, using a CTC forced alignment estimated from encoder output. SpeechLLM-XL with 1.28 seconds chunk size achieves 2.7%/6.7% WER on LibriSpeech test clean/other, and it shows no quality degradation on long form utterances 10x longer than the training utterances.", "sections": [{"title": "I. INTRODUCTION", "content": "Recent studies have shown that a decoder-only large lan-guage model (LLM) pretrained on a large amount of text corpus can be adapted to understand multi-modal input (e.g. image and audio) by simply prompting the LLM with embed-dings of the corresponding modality [1]\u2013[5]. Specifically for automatic speech recognition (ASR), it has been established that a LLM can be finetuned to generate transcription when prompted with speech input [6]\u2013[8]. In short, an audio encoder converts the input audio into a sequence of encodings, then the LLM auto-regressively predict all transcript tokens conditioned on the audio sequence. This class of methods have achieved state-of-the-art (SoTA) accuracy on ASR benchmarks, and will be denoted as SpeechLLMs in the following discussion.\nDespite their success, a practical limitation of SpeechLLMs is that they are unsuitable for processing long form utterances. First, SpeechLLMs have limited length extrapolation abilities, i.e., their performance significantly degrades when the audio length goes beyond the maximum audio length during training. This is because SpeechLLMs are trained to predict an EOS to terminate decoding after all transcript tokens are generated, and they tend to terminate early when an utterance's transcript is longer than all the utterances the models have seen during training. We will discuss this in detail in section IV-D. Second, even if we increase the length of training utterances to improve the accuracy on long form audios, a computational challenge is that per-token inference cost scales linearly with the audio length due to decoder LLM attention, and the overall inference cost scales quadratically with length. Finally, deploying an ASR model to production system typically has a low latency requirement. Since SpeechLLMs are non-streaming models that generate all transcript at once after an entire utterance has been received, the user perceived latency on long utterances would be much higher than short utterances. This problem is further worsen by the quadratic-scaling inference cost.\nIn this work, we tackle these challenges on long form ASR with SpeechLLM-XL (extra long), which is a streaming model that scales linearly with audio length. Our model consists of an audio encoder and a LLM decoder, and its key mechanism is audio chunking. The input audio is segmented into static-length chunks, which implies each chunk could have a variable number of transcript tokens. After, the input audio is processed chunk-by-chunk. In particular, the encoding of the kth audio chunk is used to prompt the LLM decoder, which auto-regressively generate transcript tokens until an EOS is pre-dicted; later, when the LLM is prompted with (k+1)th audio chunk, the previous audio chunks and decoded tokens are used as the LLM context. We introduce two hyper-parameters to trade off accuracy verses latency and computation, 1) audio chunk size controls the model latency, 2) LLM context size controls the computational cost of the attention operations. We will discuss those tradeoffs in detail in sections IV-A and IV-B.\nSpeechLLM-XL is trained on paired audio/text data, and we use the audio/text alignment to segment the text transcript. When the reference alignment is not available, we add an auxiliary CTC loss to the audio encoder and use a CTC forced aligner [9] to align the encoder output and transcript. We will analyze the quality of CTC forced alignment in section IV-C.\nWe conduct a throughout empirical study using LibriSpeech dataset, which shows SpeechLLM-XL successfully addresses all three aforementioned challenges on long form utterances. First, to demonstrate its length extrapolation ability, we use a SpeechLLM-XL model trained on regular LibriSpeech training data to decode a concatenated version of test-clean/other utterances. As discuss in section IV-D, no quality degradation is observed on the concatenated long form utterances. Second, to demonstrate its efficiency, we show in section IV-B that the attention window of the LLM decoder in SpeechLLM-XL can be aggressively reduced without hurting model accuracy. Thus using a small static LLM context would reduce the overall inference cost from quadratic to linear in audio length, and enables efficient inference for long form utterances. Finally, to demonstrate its streaming capability, we show in section IV-A that a SpeechLLM-XL model with a small chunk size of 1.28"}, {"title": "II. BACKGROUND ON SPEECHLLM", "content": "Given an input utterance $X = (X_1,...,X_T)$ where $x_t \\in R^d$ denotes Log-Mel audio features, the goal is to predict the transcription $y = (y_1,..., Y_U)$ where $y_u \\in V$ denotes a text token. SpeechLLM estimates the conditional distribution of the transcription y given the audio X:\n$P(y|X) \\approx P_\\theta(y|X) = \\prod_{u=1}^U P_\\theta(Y_u | Y_{1:u-1}, X) \\cdot P_\\theta(\\$|y, X)$ where $ denotes the EOS token and $P_\\theta$ is parametrized with a neural network. For simplicity, we set $y_{U+1} = \\$$ to extend the transcript $\\tilde{y} = (y_1,..., y_U, \\$)$ and rewrite:\n$P_\\theta(y|X) = P_\\theta(\\tilde{y}|X) = \\prod_{u=1}^{U+1} P_\\theta(Y_u|Y_{1:u-1}, X)$\nDuring training, given a collection of paired audio/text data ${(x^{(n)},y^{(n)})}_{n=1}^N$, the neural network is trained to maximize the conditional probability of the entire training set:\n$\\theta = arg \\max_{\\theta} \\prod_{n=1}^N P_\\theta(\\tilde{y}^{(n)}|x^{(n)})= arg \\min_{\\theta} \\frac{1}{N} \\sum_{n=1}^N \\sum_{u=1}^{U+1} -log P_\\theta(y_u^{(n)}|y_{1:u-1}^{(n)}, X^{(n)})$ which is to minimize the averaged cross-entropy loss across all tokens including EOS. During inference, given an input X, the extended transcript $y$ is generated auto-regressively using beam search, and EOS is removed for the final transcript y."}, {"title": "III. SPEECHLLM-XL", "content": "Given an audio/text pair X, y, we segment X into chunks $(X_1...X_k...X_K)$ with chunk size c. Assuming the audio/-text alignment is known, we further segment y and define: $\\tilde{y} = (\\tilde{y}_1,..., \\tilde{y}_k,\u2026\u2026\\tilde{y}_K) = (y_1, \\$, ..., y_k, \\$, ..., y_K, \\$)$ where $y_k$ denotes the text in the kth chunk. Then, we write the conditional probability of the entire sequence as:\n$P_\\theta(\\tilde{y}|X) = \\prod_{k=1}^K P_e(\\tilde{y}_k|X_k, {\\tilde{y}_{1:k-1}, X_{1:k-1}})$ Note the probability of predicting $\\tilde{y}_k$ given the current chunk $X_k$ depends on audio and transcripts of the previous chunks ${\\tilde{y}_{1:k-1}, X_{1:k-1}}$. To explicitly model this dependency, when computing LLM decoder self-attention on tokens representing the current chunk k, we attend to the cached attention values of the previous b chunks; the total size of the LLM attention window is b \u00d7 c. The entire architecture of our model is illus-trated in fig. 1A. In order to reduce the attention computation on long form utterances, we typically choose a small value b to limit the LLM context. For example, the model in fig. 1B use LLM context b = 1, which only covers the previous chunk, plus the current chunk. We will study the impact of chunk size and LLM context size on accuracy in sections IV-A and IV-B.\nSo far, we have assumed the availability of ground truth alignment between audio and text. When the alignment is not available, we use CTC forced alignment between audio encod-ing and text tokens to determine the token end time, as shown in fig. 1C. Empirically, we found the CTC forced alignment is a good approximation and SpeechLLM-XL trained with CTC forced alignment gives similar accuracy as the reference alignment. We will discuss the CTC forced alignment quality and its impact on overall model accuracy section IV-C.\nDuring inference, our beam search decoding algorithm is similar to the alignment-synchronous search for neural trans-ducers [10], meaning in each decoding step, all hypotheses in the beam have the same number of tokens including the EOS."}, {"title": "IV. EXPERIMENTS", "content": "We benchmark our model performance and conduct ablation studies on the LibriSpeech dataset [11], which consists of 960h training data sourced from English audio-books. Audio features are 80 mel-spaced log filter-bank channels with a sliding window of 25ms at 10ms frame shift. A time reduction layer stacks features with a stride of 4, and the resulting 320-dimensional vectors form the input to the encoder.\nSince the focus of this paper is on the SpeechLLM-XL architecture and its hyper-parameters, we keep the encoder,"}, {"title": "A. Chunk Size", "content": "We vary audio chunk size to explore the trade-off between latency and accuracy. We fix the LLM context to 5.12s and consider a range of audio chunk size options from 2.56s to 0.32s, and we also include a non-streaming SpeechLLM as a baseline for comparison. The results are summarized in table I.\nFirst, compared with the non-streaming SpeechLLM, the streaming model \u2460 with 2.56s chunk size gives better WER on dev/test-clean. Looking into the WER breakdown on the dev-clean dataset, we find that SpeechLLM performs much worse on long utterances and the majorities of the mistakes are deletion errors. In particular, on the 50 longest utterances in dev-clean, SpeechLLM gives a total WER of 16.4 and deletion rate of 13.9, while \u2460 gives a total WER of 2.2 and deletion rate of 0.1. This confirms our hypothesis that SpeechLLM-XL is suitable for long utterances. We will further compare the length extrapolation abilities of SpeechLLM-XL against SpeechLLM in section IV-D.\nSecond, as we decrease the chunk size, WER increase on all evaluation datasets. This is expected since reducing chunk size reduces the model latency. We find a chunk size of 1.28s gives a good trade-off between latency and accuracy, and we will use this chunk size for the remaining of the paper."}, {"title": "B. LLM Context", "content": "Next, we study how the LLM context impact model quality. We fix the audio chunk size to 1.28s and consider a range of"}, {"title": "C. CTC Forced Alignment", "content": "So far, we have used CTC forced alignment during train-ing without examining alignment quality. To this end, we compare the CTC forced alignment from the audio encoder verses the reference alignment from a chenone hybrid acoustic model [12]. We measure the CTC forced alignment quality by the averaged alignment delay and alignment delta measured on LibriSpeech dev-clean/other. The alignment delay is defined as the token end time difference averaged across all tokens $\\frac{1}{U} \\sum_{u=1}^U t(y_u) - t(\\hat{y_u})$, where $t(y_u)$ and $t(\\hat{y_u})$ is the token end time for yu from CTC alignment and hybrid alignment, respectively. The alignment delta is defined as the token end time absolute difference $\\frac{1}{U} \\sum_{u=1}^U|t(y_u) - t(\\hat{y_u})|$. To understand the impact of alignment quality on ASR accuracy, we also replicated \u2461 using the reference hybrid alignment for text segmentation. The results are summarized in table III.\nThe token end time from CTC forced alignment is on-averaged 52ms ahead of the reference hybrid alignment, as indicated by the negative alignment delay. Overall, the CTC"}, {"title": "D. Length Extrapolation", "content": "One major advantage of SpeechLLM-XL over the baseline SpeechLLM is its length extrapolation ability. To demonstrate this advantage, we conduct a length extrapolation experiment by repeating (and concatenating) each test-clean/other ut-terances multiple times. We test the concatenated utterances on the SpeechLLM baseline and SpeechLLM-XL \u2461, and we summarize the results in table IV. Unsurprisingly, SpeechLLM fails to extrapolate beyond the audio length the original model was trained on. The WER more than tripled on concatenated test-clean/other utterances 2x longer than the utterances seen during training. In contrast, SpeechLLM-XL extrapolates perfectly beyond the training length, even yielding a slightly lower WER on 2x/10x concatenated test-other utterances.\nThe length extrapolation ability is important for production deployment because 1) it allows the ASR model to be more robust to edge case input, 2) it allows training on segmented short utterances for better batching and training efficiency, while still maintaining model quality on long form utterances."}, {"title": "V. ADDITIONAL BASELINES AND RELATED WORKS", "content": "We compare SpeechLLM-XL with some additional base-lines. The results are summarized in table V.\nFirst, we consider some classic baselines. LAS with Spec-Augment [14] is widely used non-streaming baseline without"}]}