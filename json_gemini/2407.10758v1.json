{"title": "Continual Deep Learning on the Edge via Stochastic Local Competition among Subnetworks", "authors": ["Theodoros Christophides", "Kyriakos Tolias", "Sotirios Chatzis"], "abstract": "Continual learning on edge devices poses unique challenges due to stringent resource constraints. This paper introduces a novel method that lever- ages stochastic competition principles to pro- mote sparsity, significantly reducing deep net- work memory footprint and computational de- mand. Specifically, we propose deep networks that comprise blocks of units that compete locally to win the representation of each arising new task; competition takes place in a stochastic manner. This type of network organization results in sparse task-specific representations from each network layer; the sparsity pattern is obtained during train- ing and is different among tasks. Crucially, our method sparsifies both the weights and the weight gradients, thus facilitating training on edge de- vices. This is performed on the grounds of win- ning probability for each unit in a block. During inference, the network retains only the winning unit and zeroes-out all weights pertaining to non- winning units for the task at hand. Thus, our ap- proach is specifically tailored for deployment on edge devices, providing an efficient and scalable solution for continual learning in resource-limited environments.", "sections": [{"title": "1. Introduction", "content": "Continual Learning (CL), also referred to as Lifelong Learn- ing (Thrun, 1995), aims to learn sequential tasks and acquire new information while preserving knowledge from previ- ous learned tasks (Thrun & Mitchell, 1995). This paper's focus is on a variant of CL dubbed class-incremental learn- ing (CIL) (Belouadah & Popescu, 2019; Gupta et al., 2020; Deng et al., 2021). The main principle of CIL is a CL sce- nario where on each iteration we are dealing with data from a specific task, and each task contains new classes that must be learnt.\nEdge devices, characterized by their limited computational resources, necessitate efficient machine learning models to perform tasks effectively. Sparsity in neural networks emerges as a critical feature to address these limitations, reducing memory requirements and computational costs. This work introduces a stochastic competition mechanism to induce sparsity, optimizing continual learning processes specifically for such constrained environments.\nRecently, different research groups have drawn inspira- tion from the lottery ticket hypothesis (LTH) (Frankle & Carbin, 2019) to introduce the lifelong tickets (LLT) method (Chen et al., 2021), the Winning SubNetworks (WSN) method (Kang et al., 2022), and, more recently, the Soft-SubNetworks approach (Kang et al., 2023). However, these recent advances are confronted with major limitations: (i) LLT entails an iterative pruning procedure, that requires multiple repetitions of the training algorithm for each task; this is not suitable for edge devices. (ii) The existing al- ternatives do not take into consideration the uncertainty in the used datasets, which would benefit from the subnetwork selection process being stochastic, as opposed to hard unit pruning. In fact, it has been recently shown that stochastic competition mechanisms among locally competing units can offer important generalization capacity benefits for deep networks used in as diverse challenges as adversarial ro- bustness (Panousis et al., 2021), video-to-text translation (Voskou et al., 2021), and model-agnostic meta-learning (Kalais & Chatzis, 2022).\nIn a different vein, SparCL (Wang et al., 2022) has been the first work on CL specifically designed to tackle applications on edge devices, where resource constraints are significant. Apart from weight sparsity, SparCL also considers data ef- ficiency and gradient sparsity to accelerate training while preserving accuracy. SparCL dynamically maintains impor- tant weights for current and previous tasks and adapts the sparsity during transitions between tasks, which helps in mitigating catastrophic forgetting-a common challenge in continual learning. Importantly, in contrast to LTH-based"}, {"title": "2. Proposed Approach", "content": "2.1. Problem Definition\nCIL objective is to learn a unified classifier from a sequential stream of data comprising different tasks that introduce new classes. CIL methods should scale to a large number of tasks without immense computational and memory growth. Let us consider a CIL problem T which consists of a sequence of n tasks, $T = \\{(C^{(1)}, D^{(1)}), (C^{(2)}, D^{(2)}), ..., (C^{(n)}, D^{(n)})\\}$. Each task t contains data $D^{(t)} = (x^{(t)}, y^{(t)})$ and new classes $C^{(t)} = \\{C_{m_{t-1}+1},C_{m_{t-1}+2},..., C_{m_{t}}\\}$, where $m_t$ is the number of presented classes up to task t. We de- note as $x^{(t)}$ the input features, and as $y^{(t)}$ the one-hot label vector corresponding to $x^{(t)}$.\nWhen training for the t-th task, we use the data of the task, $D^{(t)}$. We consider learners-classifiers that are deep net- works parameterized by weights W, and we use $f(x^{(t)}; W)$ to indicate the output Softmax logits for a given input $x^{(t)}$. Facing a new dataset $D^{(t)}$, the model's goal is to learn new classes and maintain performance over old classes.\n2.2. Model formulation\nOur approach integrates a stochastic competition mecha- nism within the training process to promote sparsity. By selectively activating a subset of neurons and pruning less important connections, our method maintains a high level of accuracy while significantly reducing the model's complex- ity. This sparsity not only ensures lower memory usage but also accelerates inference, making it ideal for edge devices.\nLet us denote as $x^{(t)} \\in R^E$ an input representation vector presented to a dense ReLU layer of a traditional deep neural network, with corresponding weights matrix $W \\in R^{E \\times K}$. The layer produces an output vector $y^{(t)} \\in R^K$, which is fed to the subsequent layers.\nIn our approach, a group of J ReLU units is replaced by a group of J competing linear units, organized in one block; each layer contains I blocks of J units. Within each block,"}, {"title": "2.3. A Convolutional Variant", "content": "Further, to accommodate architectures comprising convolu- tional operations, we consider a variant of the TWTA layer, inspired from (Panousis et al., 2019). In the remainder of this work, this will be referred to as the Conv-TWTA layer, while the original TWTA layer will be referred to as the dense variant. The graphical illustration of Conv-TWTA is provided in Fig. 3.\nSpecifically, let us assume an input tensor $X^{(t)} \\in R^{H \\times L \\times C}$ of a layer, where H, L, C are the height, length and chan- nels of the input. We define a set of kernels, each with weights $W_i \\in R^{h \\times l \\times C \\times J}$, where h, l, C, J are the kernel height, length, channels and competing feature maps, and $i = 1,..., I$.\nHere, analogously to the grouping of linear units in a dense TWTA layer of Section 2.2, local competition is performed among feature maps in a kernel. Thus, each kernel is treated as a TWTA block, feature maps in a kernel compete among them, and multiple kernels of competing feature maps con- stitute a Conv-TWTA layer.\nThis way, the output $Y^{(t)} \\in R^{H \\times L \\times (I \\cdot J)}$ of a layer under the proposed convolutional variant is obtained via concate- nation along the last dimension of the subtensors $Y_i^{(t)}$:\n$Y^{(t)} = \\xi_{t,i} (W_i * X^{(t)}) \\in R^{H \\times L \\times J}$ (4)"}, {"title": "2.4. Training", "content": "For each task t, our approach consists in executing a single full training cycle. The performed training cycle targets both the network weights, W, and the posterior hyperparameters $\\pi_{t,i}$ of the winner indicator hidden variables pertaining to the task, $p(\\xi_{t,i}) \\forall i$.\nThe vectors $\\pi_{t,i}$ are initialized at random, while the network weights, W, \"continue\" from the estimator obtained after training on task t \u2212 1. We denote as $W^{(t)}$ the updated weights estimator obtained through the training cycle on the t-th task.\nTo perform training, we resort to minimization of a sim- ple categorical cross-entropy criterion. Let us consider the u-th training iteration on task t, with data batch $D_u^{(t)} = (X^{(t)}, Y^{(t)})$. The training criterion is the categorical cross-entropy $CE(Y^{(t)}, f(x^{(t)}; W^{(t)}, \\xi_t))$ between the data la- bels Y(t) and the class probabilities $f(x^{(t)}; W^{(t)}, \\xi_t)$ gen- erated from the penultimate Softmax layer of the network. In this definition, $\\xi_t = [\\xi_{t,i}]_i$ is a vector concatenation of single Monte-Carlo (MC) samples drawn from the Categori- cal posteriors $p(\\xi_{t,i})$.\nTo ensure low-variance gradients with only one drawn MC sample, we reparameterize these samples by resorting to the Gumbel-Softmax relaxation (Maddison et al., 2017). The Gumbel-Softmax relaxation yields sampled instances $\\xi_{t,i}$ under the following expression:\n$\\xi_{t,i} = Softmax(([log \\pi_{t,i,j} + g_{t,i,j}]_{j=1})/\\tau) \\in R^J, \\forall i$\nwhere: $g_{t,i,j} = - log(- log U_{t,i,j}), U_{t,i,j} \\sim Uniform(0, 1)$ (5)\nand $\u03c4 \u2208 (0, \u221e)$ is a temperature factor that controls how closely the Categorical distribution $p(\\xi_{t,i})$ is approximated by the continuous relaxation. This is similar to (Panousis et al., 2019)."}, {"title": "3. Related Work", "content": "Recent works in (Chen et al., 2021; Kang et al., 2022; 2023) have pursued to build computationally efficient con- tinual learners by drawing inspiration from LTH (Frankle & Carbin, 2019). These works compose sparse subnetworks that achieve comparable or/and even higher predictive per- formance than their initial counterparts. However, our work is substantially different from the existing state-of-the-art, as it specifically accounts for the constraints imposed in the context of execution on an edge device:\n(i) Contrary to (Chen et al., 2021), we do not employ itera- tive pruning, which repeats multiple full cycles of network training and pruning; this would be completely intractable on an edge device. Instead, we perform a single training cycle, at the end of which we retain a (task-specific) subnet- work to perform inference for the task.\n(ii) (Kang et al., 2022) and (Kang et al., 2023) select a sub- network that will be used for the task at hand on the grounds of an optimization criterion for binary masks imposed over"}, {"title": "4. Experiments", "content": "To demonstrate the effectiveness of our method for edge devices, we conducted experiments using a simulated edge computing environment. We benchmarked our sparse model against traditional dense models on various datasets, observ- ing performance in terms of computational efficiency and memory usage.\nWe evaluate on CIFAR-100 (Krizhevsky et al., 2012), Tiny- ImageNet (Le & Yang, 2015), PMNIST (LeCun et al., 1998) and Omniglot Rotation (Lake et al., 2017). Also, we evalu- ate on the 5-Datasets (Saha et al., 2021) benchmark, in order to examine how our method performs in case that cross-task generalization concerns different datasets. We randomly di- vide the classes of each dataset into a fixed number of tasks with a limited number of classes per task. Specifically, in each training iteration, we construct N-way few-shot tasks by randomly picking N classes and sampling few training samples for each class. In Supplementary Section A, we specify further experimental details for our datasets.\nWe adopt the original ResNet18 network (He et al., 2016) for"}, {"title": "4.1. Experimental results", "content": "In Table 1, we show how TWTA-CIL performs in various benchmarks compared to popular alternative methods. We emphasize that the performance of SoftNet and WSN is provided for the configuration reported in the literature that yields the best accuracy, as well as for the reported configu- ration that corresponds to the proportion of retained weights closest to our method. Turning to LLT, we report how the method performs with no pruning and with pruning ratio closest to our method.\nAs we observe, our method outperforms the existing state-of- the-art in every considered benchmark. For instance, WSN performs worse than TWTA-CIL (3.125%), irrespectively of whether WSN retains a greater or a lower proportion of the initial network. Thus, our approach successfully discovers sparse subnetworks (winning tickets) that are pow- erful enough to retain previous knowledge, while gener- alizing well to new unseen tasks. Crucially, our method outperforms all the alternatives, including the related, edge device-oriented SparCL approach, in terms of both obtained accuracy and number of retained parameters at inference time (thus, memory footprint).\nFinally, it is interesting to examine how the winning ticket vectors differentiate across tasks. To this end, we compute the overlap among the $\\xi_t = [\\xi_{t,i}]_i$ vectors, defined in Eq. (3), for all consecutive pairs of tasks, (t \u2212 1, t), and compute average percentages. We observe that average overlap per- centages range from 6.38% to 10.67% across the considered datasets; this implies clear differentiation."}, {"title": "4.1.1. COMPUTATIONAL TIMES FOR TRAINING CIL METHODS", "content": "In Table 2, we report the training FLOPs for our method and its direct competitor, that is SparCL (Wang et al., 2022). It is apparent that our method yields much improved training algorithm computational costs."}, {"title": "4.1.2. REDUCTION OF FORGETTING TENDENCIES", "content": "To examine deeper the obtained improvement in forgetting tendencies, we report the backward-transfer and interfer- ence (BTI) values of the considered methods in Table 3. BTI measures the average change in the accuracy of each task from when it was learnt to the end of the training, that is training on the last task; thus, it is immensely relevant to this empirical analysis. A smaller value of BTI implies lesser forgetting as the network gets trained on additional"}, {"title": "4.2. Effect of block size J", "content": "Finally, we re-evaluate TWTA-CIL with various block size values J (and correspondingly varying number of layer blocks, I). In all cases, we ensure that the total number of feature maps, for a convolutional layer, or units, for a dense layer, which equals I * J, remains the same as in the original architecture of Section 4.1. This is important, as it does not change the total number of trainable param- eters, but only the organization into blocks under the local winner-takes-all rationale. Different selections of J result in different percentages of remaining network weights at inference time, as we can see in Table 4 (datasets Tiny- ImageNet and CIFAR-100). As we observe, the \u201cTWTA- CIL (12.50%)\u201d alternative, with J = 8, is the most accurate configuration of TWTA-CIL. However, the most efficient version, perfectly fit for application to edge devices, that is TWTA-CIL (3.125%), yields only a negligible accuracy drop in all the considered benchmarks."}, {"title": "5. Conclusion", "content": "This paper presented a sparsity-promoting method tailored for continual learning on edge devices. By incorporating stochastic competition, we achieved an approach that is both efficient and effective, suitable for the limited capa- bilities of edge computing. Specifically, the results clearly demonstrated that our sparsity-promoting method signifi- cantly outperforms traditional models on edge devices. We observed a significant reduction in memory usage and an increase in computational speed, confirming the suitabil- ity of our approach for deployment in resource-constrained environments. Future research may explore further opti- mizations to enhance the adaptability of this method across more diverse edge computing scenarios."}, {"title": "A. More details on the used datasets", "content": "Datasets and Task Splittings 5-Datasets is a mixture of 5 different vision datasets: CIFAR-10, MNIST (LeCun et al., 1998), SVHN (Netzer et al., 2011), FashionMNIST (Xiao et al., 2017) and notMNIST (Bui & Chang, 2016). Each dataset consists of 10 classes, and classification on each dataset is treated as a single task. PMNIST is a variant of MNIST, where each task is generated by shuffling the input image pixels by a fixed permutation. In the case of Om- niglot Rotation, we preprocess the raw images of Omniglot dataset by generating rotated versions of (90\u00b0, 180\u00b0, 270\u00b0) as in (Kang et al., 2022). For 5-Datasets, similar to (Kang et al., 2022), we pad 0 values to raw images of MNIST and FashionMNIST, convert them to RGB format to have a dimension of 3*32*32, and finally normalize the raw image data. All datasets used in Section 4 were randomly split into training and testings sets with ratio of 9:1. The number of stored images in the memory buffer - per class - is 5 for Tiny-ImageNet, and 10 for CIFAR-100, PMNIST, Omniglot Rotation and 5-Datasets.\nWe randomly divide the 100 classes of CIFAR-100 into 10 tasks with 10 classes per task; the 200 classes of Tiny- ImageNet into 40 tasks with 5 classes per task; the 200 classes of PMNIST into 20 tasks with 10 classes per task; and in the case of Omniglot Rotation, we divide the avail- able 1200 classes into 100 tasks with 12 classes per task. The N-way few-shot settings for the constructed tasks in each training iteration are: 10-way 10-shot for CIFAR-100, PMNIST and 5-Datasets, 5-way 5-shot for Tiny-ImageNet, and 12-way 10-shot for Omniglot Rotation."}, {"title": "B. Modified Network Architecture details", "content": "B.1. ResNet18\nThe original ResNet18 comprises an initial convolutional layer with 64 3x3 kernels, 4 blocks of 4 convolutional layers each, with 64 3x3 kernels on the layers of the first block, 128 3x3 kernels for the second, 256 3x3 kernels for the third and 512 3x3 kernels for the fourth. These layers are followed by a dense layer of 512 units, a pooling and a final Softmax layer. In our modified ResNet18 architecture, we consider kernel size = 3 and padding = 1; in Table 5, we show the number of used kernels / blocks and competing feature maps / units, J, in each modified layer.\nB.2. AlexNet\nThe 5-layer AlexNet architecture comprises 3 convolutional layers of 64, 128, and 256 filters with 4x4, 3x3, and 2x2 kernel sizes, respectively. These layers are followed by two dense layers of 2048 units, with rectified linear units as acti- vations, and 2x2 max-pooling after the convolutional layers. The final layer is a fully-connected layer with a Softmax out- put. In our modified AlexNet architecture, we replace each dense ReLU layer with a layer of (dense) TWTA blocks, and each convolutional layer with a layer of Conv-TWTA blocks; in Table 6, we show the number of used kernels / blocks and competing feature maps / units, J, in each modified layer.\nB.3. LeNet\nThe LeNet architecture comprises 2 convolutional layers of 20, and 50 feature maps, followed by one feedforward fully connected layer of 500 units, and a final Softmax layer. In our modified LeNet architecture, we replace each of the 2 convolutional layers with one layer of Conv-TWTA blocks; the former retains 2 kernels / blocks of 8 competing feature maps, and the latter 6 kernels / blocks of 8 competing feature maps. The fully connected layer is replaced with a dense TWTA-layer, consisting of 50 blocks of 8 competing units."}]}