{"title": "Continual Deep Learning on the Edge via Stochastic Local Competition among Subnetworks", "authors": ["Theodoros Christophides", "Kyriakos Tolias", "Sotirios Chatzis"], "abstract": "Continual learning on edge devices poses unique challenges due to stringent resource constraints. This paper introduces a novel method that leverages stochastic competition principles to promote sparsity, significantly reducing deep network memory footprint and computational demand. Specifically, we propose deep networks that comprise blocks of units that compete locally to win the representation of each arising new task; competition takes place in a stochastic manner. This type of network organization results in sparse task-specific representations from each network layer; the sparsity pattern is obtained during training and is different among tasks. Crucially, our method sparsifies both the weights and the weight gradients, thus facilitating training on edge devices. This is performed on the grounds of winning probability for each unit in a block. During inference, the network retains only the winning unit and zeroes-out all weights pertaining to non-winning units for the task at hand. Thus, our approach is specifically tailored for deployment on edge devices, providing an efficient and scalable solution for continual learning in resource-limited environments.", "sections": [{"title": "1. Introduction", "content": "Continual Learning (CL), also referred to as Lifelong Learning (Thrun, 1995), aims to learn sequential tasks and acquire new information while preserving knowledge from previous learned tasks (Thrun & Mitchell, 1995). This paper's focus is on a variant of CL dubbed class-incremental learning (CIL) (Belouadah & Popescu, 2019; Gupta et al., 2020; Deng et al., 2021). The main principle of CIL is a CL scenario where on each iteration we are dealing with data from a specific task, and each task contains new classes that must be learnt.\nEdge devices, characterized by their limited computational resources, necessitate efficient machine learning models to perform tasks effectively. Sparsity in neural networks emerges as a critical feature to address these limitations, reducing memory requirements and computational costs. This work introduces a stochastic competition mechanism to induce sparsity, optimizing continual learning processes specifically for such constrained environments.\nRecently, different research groups have drawn inspiration from the lottery ticket hypothesis (LTH) (Frankle & Carbin, 2019) to introduce the lifelong tickets (LLT) method (Chen et al., 2021), the Winning SubNetworks (WSN) method (Kang et al., 2022), and, more recently, the Soft-SubNetworks approach (Kang et al., 2023). However, these recent advances are confronted with major limitations: (i) LLT entails an iterative pruning procedure, that requires multiple repetitions of the training algorithm for each task; this is not suitable for edge devices. (ii) The existing alternatives do not take into consideration the uncertainty in the used datasets, which would benefit from the subnetwork selection process being stochastic, as opposed to hard unit pruning. In fact, it has been recently shown that stochastic competition mechanisms among locally competing units can offer important generalization capacity benefits for deep networks used in as diverse challenges as adversarial robustness (Panousis et al., 2021), video-to-text translation (Voskou et al., 2021), and model-agnostic meta-learning (Kalais & Chatzis, 2022).\nIn a different vein, SparCL (Wang et al., 2022) has been the first work on CL specifically designed to tackle applications on edge devices, where resource constraints are significant. Apart from weight sparsity, SparCL also considers data efficiency and gradient sparsity to accelerate training while preserving accuracy. SparCL dynamically maintains important weights for current and previous tasks and adapts the sparsity during transitions between tasks, which helps in mitigating catastrophic forgetting-a common challenge in continual learning. Importantly, in contrast to LTH-based methods, the foundational characteristics of the approach introduce sparsity into the gradient updates, which reduces the computational cost during backpropagation. This component is critical for efficient training on hardware with limited computational capabilities, such as mobile phones or other edge devices.\nInspired from these facts, this work proposes a radically different regard toward addressing catastrophic forgetting in CIL. Our approach is founded upon the framework of stochastic local competition which is implemented in a task-wise manner. Specifically, our proposed approach relies upon the following novel contributions:\n\u2022 Task-specific sparsity in the learned representations. We propose a novel mechanism that inherently learns to extract sparse task-specific data representations. Specifically, each layer of the network is split into blocks of competing units; local competition is stochastic and it replaces traditional nonlinearities, e.g. ReLU. Being presented with a new task, each block learns a distribution over its units that governs which unit specializes in the presented task. We dub this type of nonlinear units as task winner-takes-all (TWTA). Under this scheme, the network learns a Categorical posterior over the competing block units; this is the winning unit posterior of the block. Only the winning unit of a block generates a non-zero output fed to the next network layer. This renders sparse the generated representations, with the sparsity pattern being task-specific.\n\u2022 Weight gradient pruning driven from the learned stochastic competition posteriors. During training, the network utilizes the learned Categorical posteriors over winning block units to introduce sparsity into the gradient updates. In a sense, the algorithm inherently masks out the gradient updates of the block units with lower winning posteriors. This is immensely important when deep network training is carried out on edge devices.\n\u2022 Winner-based weight pruning at inference time. During inference for a given task, we use the (Categorical) winner posteriors learned for the task to select the winner unit of each block; we zero-out the remainder block units. This forms a task-winning ticket used for inference. This way, the size of the network used at inference time is significantly reduced; pruning depends on the number of competing units per block, since we drop all block units except for the selected winner with maximum winning posterior.\nWe evaluate our approach, dubbed TWTA for CIL (TWTA-CIL), on image classification benchmarks. We show that"}, {"title": "2. Proposed Approach", "content": "2.1. Problem Definition\nCIL objective is to learn a unified classifier from a sequential stream of data comprising different tasks that introduce new classes. CIL methods should scale to a large number of tasks without immense computational and memory growth. Let us consider a CIL problem $\\mathcal{T}$ which consists of a sequence of $n$ tasks, $\\mathcal{T} = \\{(\\mathcal{C}^{(1)}, \\mathcal{D}^{(1)}), (\\mathcal{C}^{(2)}, \\mathcal{D}^{(2)}), ..., (\\mathcal{C}^{(n)}, \\mathcal{D}^{(n)})\\}$. Each task $t$ contains data $\\mathcal{D}^{(t)} = (x^{(t)}, y^{(t)})$ and new classes $\\mathcal{C}^{(t)} = \\{C_{m_{t-1}+1},C_{m_{t-1}+2},..., C_{m_{t}} \\}$, where $m_t$ is the number of presented classes up to task $t$. We denote as $x^{(t)}$ the input features, and as $y^{(t)}$ the one-hot label vector corresponding to $x^{(t)}$.\nWhen training for the t-th task, we use the data of the task, $\\mathcal{D}^{(t)}$. We consider learners-classifiers that are deep networks parameterized by weights $W$, and we use $f(x^{(t)}; W)$ to indicate the output Softmax logits for a given input $x^{(t)}$. Facing a new dataset $\\mathcal{D}^{(t)}$, the model's goal is to learn new classes and maintain performance over old classes.\n2.2. Model formulation\nOur approach integrates a stochastic competition mechanism within the training process to promote sparsity. By selectively activating a subset of neurons and pruning less important connections, our method maintains a high level of accuracy while significantly reducing the model's complexity. This sparsity not only ensures lower memory usage but also accelerates inference, making it ideal for edge devices.\nLet us denote as $x^{(t)} \\in \\mathbb{R}^{E}$ an input representation vector presented to a dense ReLU layer of a traditional deep neural network, with corresponding weights matrix $W \\in \\mathbb{R}^{E \\times K}$. The layer produces an output vector $y^{(t)} \\in \\mathbb{R}^{K}$, which is fed to the subsequent layers.\nIn our approach, a group of $J$ ReLU units is replaced by a group of $J$ competing linear units, organized in one block; each layer contains $I$ blocks of $J$ units. Within each block,"}, {"title": "2.3. A Convolutional Variant", "content": "Further, to accommodate architectures comprising convolutional operations, we consider a variant of the TWTA layer, inspired from (Panousis et al., 2019). In the remainder of this work, this will be referred to as the Conv-TWTA layer, while the original TWTA layer will be referred to as the dense variant. The graphical illustration of Conv-TWTA is provided in Fig. 3.\nSpecifically, let us assume an input tensor $X^{(t)} \\in \\mathbb{R}^{H \\times L \\times C}$ of a layer, where $H, L, C$ are the height, length and channels of the input. We define a set of kernels, each with weights $W_i \\in \\mathbb{R}^{h \\times l \\times C \\times J}$, where $h,l, C, J$ are the kernel height, length, channels and competing feature maps, and $i = 1,..., I$.\nHere, analogously to the grouping of linear units in a dense TWTA layer of Section 2.2, local competition is performed among feature maps in a kernel. Thus, each kernel is treated as a TWTA block, feature maps in a kernel compete among them, and multiple kernels of competing feature maps constitute a Conv-TWTA layer.\nThis way, the output $Y^{(t)} \\in \\mathbb{R}^{H \\times L \\times (I \\cdot J)}$ of a layer under the proposed convolutional variant is obtained via concatenation along the last dimension of the subtensors $Y_i^{(t)}$:\n$\\qquad Y^{(t)} = \\xi_{t,i} (W_i * X^{(t)}) \\in \\mathbb{R}^{H \\times L \\times J}$"}, {"title": "2.4. Training", "content": "For each task $t$, our approach consists in executing a single full training cycle. The performed training cycle targets both the network weights, $W$, and the posterior hyperparameters $\\pi_{t,i}$ of the winner indicator hidden variables pertaining to the task, $p(\\xi_{t,i}) \\forall i$.\nThe vectors $\\pi_{t,i}$ are initialized at random, while the network weights, $W$, \"continue\" from the estimator obtained after training on task $t-1$. We denote as $W^{(t)}$ the updated weights estimator obtained through the training cycle on the t-th task.\nTo perform training, we resort to minimization of a simple categorical cross-entropy criterion. Let us consider the u-th training iteration on task t, with data batch $D_{u}^{(t)} = (X^{(t)}, Y^{(t)})$. The training criterion is the categorical cross-entropy $CE(Y^{(t)}, f(x^{(t)}; W^{(t)}, \\xi_t))$ between the data labels $Y^{(t)}$ and the class probabilities $f(x^{(t)}; W^{(t)}, \\xi_t)$ generated from the penultimate Softmax layer of the network. In this definition, $\\xi_t = [\\xi_{t,i}]_{i=1}^{I}$ is a vector concatenation of single Monte-Carlo (MC) samples drawn from the Categorical posteriors $p(\\xi_{t,i})$.\nTo ensure low-variance gradients with only one drawn MC sample, we reparameterize these samples by resorting to the Gumbel-Softmax relaxation (Maddison et al., 2017). The Gumbel-Softmax relaxation yields sampled instances $\\xi_{t,i}$ under the following expression:\n$\\qquad \\xi_{t,i} = Softmax(([log \\pi_{t,i,j} + g_{t,i,j}]_{j=1}^{J}) / \\tau) \\in \\mathbb{R}^J, \\forall i$\nwhere: $g_{t,i,j} = -log(-log \\mu_{t,i,j}), \\qquad \\mu_{t,i,j} \\sim Uniform(0, 1)$"}, {"title": "3. Related Work", "content": "Recent works in (Chen et al., 2021; Kang et al., 2022; 2023) have pursued to build computationally efficient continual learners by drawing inspiration from LTH (Frankle & Carbin, 2019). These works compose sparse subnetworks that achieve comparable or/and even higher predictive performance than their initial counterparts. However, our work is substantially different from the existing state-of-the-art, as it specifically accounts for the constraints imposed in the context of execution on an edge device:\n(i) Contrary to (Chen et al., 2021), we do not employ iterative pruning, which repeats multiple full cycles of network training and pruning; this would be completely intractable on an edge device. Instead, we perform a single training cycle, at the end of which we retain a (task-specific) subnetwork to perform inference for the task.\n(ii) (Kang et al., 2022) and (Kang et al., 2023) select a subnetwork that will be used for the task at hand on the grounds of an optimization criterion for binary masks imposed over the network weights. Once this subnetwork has been selected, they train randomly selected subsets of the weights of the whole network, to account for the case of a suboptimal subnetwork selection. Similar to (Chen et al., 2021), this is completely intractable on an edge device.\nOn the contrary, our method attempts to encourage different units in a competing block to specialize to different tasks. Training is performed concurrently for the winner unit indicator hidden variables, the posteriors of which regulate weight updates, as well as the network weights themselves. Thus, network pruning comes at the end of weight updating and not beforehand. We posit that this regulated updating scheme, which does not entail a priori hard pruning decisions, facilitates generalization without harming catastrophic forgetting.\nOn the other hand, (Wang et al., 2022) are the first to directly attack the problem of CL on an edge device. To this end, they suggest an weight importance metric and a related weight gradient importance metric, which attempt to retain (1) weights of larger magnitude for output stability, (2) weights important for the current task for learning capacity, and (3) weights important for past data to mitigate catastrophic forgetting. However, the use of these metrics requires the application of intra-task and inter-task adjustment processes, which require extensive heuristic tuning. In addition, the use of two different but related metrics for the weights and their gradients is due to the heuristic thresholds this method requires, which cannot be homogeneous. This is a drawback that makes the method hard to use off-the-shelf in a given scenario. Finally, for the method to"}, {"title": "4. Experiments", "content": "To demonstrate the effectiveness of our method for edge devices, we conducted experiments using a simulated edge computing environment. We benchmarked our sparse model against traditional dense models on various datasets, observing performance in terms of computational efficiency and memory usage.\nWe evaluate on CIFAR-100 (Krizhevsky et al., 2012), Tiny-ImageNet (Le & Yang, 2015), PMNIST (LeCun et al., 1998) and Omniglot Rotation (Lake et al., 2017). Also, we evaluate on the 5-Datasets (Saha et al., 2021) benchmark, in order to examine how our method performs in case that cross-task generalization concerns different datasets. We randomly divide the classes of each dataset into a fixed number of tasks with a limited number of classes per task. Specifically, in each training iteration, we construct N-way few-shot tasks by randomly picking N classes and sampling few training samples for each class. In Supplementary Section A, we specify further experimental details for our datasets.\nWe adopt the original ResNet18 network (He et al., 2016) for Tiny-ImageNet, PMNIST and 5-Datasets; we use a 5-layer AlexNet similar to (Saha et al., 2021) for the experiments on CIFAR-100, and LeNet (LeCun et al., 1998) for Omniglot Rotation. In the case of our approach, we modify those baselines by replacing each ReLU layer with a layer of (dense) TWTA blocks, and each convolutional layer with a layer of Conv-TWTA blocks. See more details in the Supplementary Section B.\nFor both the network weights, $W$, and the log hyperparameters, log $\\pi_{t,i}$, we employ Glorot Normal initialization (Glorot & Bengio, 2010). At the first training iteration of a new task, we initialize the Gumbel-Softmax relaxation temperature $\\tau$ to 0.67; as the training proceeds, we linearly anneal its value to 0.01. We use SGD optimizer (Robbins, 2007) with a learning rate linearly annealed to 0, and initial value of 0.1. We run 100 training epochs per task, with batch size of 40."}, {"title": "5. Conclusion", "content": "This paper presented a sparsity-promoting method tailored for continual learning on edge devices. By incorporating stochastic competition, we achieved an approach that is both efficient and effective, suitable for the limited capabilities of edge computing. Specifically, the results clearly demonstrated that our sparsity-promoting method significantly outperforms traditional models on edge devices. We observed a significant reduction in memory usage and an increase in computational speed, confirming the suitability of our approach for deployment in resource-constrained environments. Future research may explore further optimizations to enhance the adaptability of this method across more diverse edge computing scenarios."}, {"title": "A. More details on the used datasets", "content": "Datasets and Task Splittings 5-Datasets is a mixture of 5 different vision datasets: CIFAR-10, MNIST (LeCun et al., 1998), SVHN (Netzer et al., 2011), FashionMNIST (Xiao et al., 2017) and notMNIST (Bui & Chang, 2016). Each dataset consists of 10 classes, and classification on each dataset is treated as a single task. PMNIST is a variant of MNIST, where each task is generated by shuffling the input image pixels by a fixed permutation. In the case of Omniglot Rotation, we preprocess the raw images of Omniglot dataset by generating rotated versions of (90\u00b0, 180\u00b0, 270\u00b0) as in (Kang et al., 2022). For 5-Datasets, similar to (Kang et al., 2022), we pad 0 values to raw images of MNIST and FashionMNIST, convert them to RGB format to have a dimension of 3*32*32, and finally normalize the raw image data. All datasets used in Section 4 were randomly split into training and testings sets with ratio of 9:1. The number of stored images in the memory buffer - per class - is 5 for Tiny-ImageNet, and 10 for CIFAR-100, PMNIST, Omniglot Rotation and 5-Datasets.\nWe randomly divide the 100 classes of CIFAR-100 into 10 tasks with 10 classes per task; the 200 classes of Tiny-ImageNet into 40 tasks with 5 classes per task; the 200 classes of PMNIST into 20 tasks with 10 classes per task; and in the case of Omniglot Rotation, we divide the available 1200 classes into 100 tasks with 12 classes per task. The N-way few-shot settings for the constructed tasks in each training iteration are: 10-way 10-shot for CIFAR-100, PMNIST and 5-Datasets, 5-way 5-shot for Tiny-ImageNet, and 12-way 10-shot for Omniglot Rotation."}, {"title": "B. Modified Network Architecture details", "content": "B.1. ResNet18\nThe original ResNet18 comprises an initial convolutional layer with 64 3x3 kernels, 4 blocks of 4 convolutional layers each, with 64 3x3 kernels on the layers of the first block, 128 3x3 kernels for the second, 256 3x3 kernels for the third and 512 3x3 kernels for the fourth. These layers are followed by a dense layer of 512 units, a pooling and a final Softmax layer. In our modified ResNet18 architecture, we consider kernel size = 3 and padding = 1; in Table 5, we show the number of used kernels / blocks and competing feature maps / units, J, in each modified layer.\nB.2. AlexNet\nThe 5-layer AlexNet architecture comprises 3 convolutional layers of 64, 128, and 256 filters with 4x4, 3x3, and 2x2 kernel sizes, respectively. These layers are followed by two dense layers of 2048 units, with rectified linear units as activations, and 2x2 max-pooling after the convolutional layers. The final layer is a fully-connected layer with a Softmax output. In our modified AlexNet architecture, we replace each dense ReLU layer with a layer of (dense) TWTA blocks, and each convolutional layer with a layer of Conv-TWTA blocks; in Table 6, we show the number of used kernels / blocks and competing feature maps / units, J, in each modified layer.\nB.3. LeNet\nThe LeNet architecture comprises 2 convolutional layers of 20, and 50 feature maps, followed by one feedforward fully connected layer of 500 units, and a final Softmax layer. In our modified LeNet architecture, we replace each of the 2 convolutional layers with one layer of Conv-TWTA blocks; the former retains 2 kernels / blocks of 8 competing feature maps, and the latter 6 kernels / blocks of 8 competing feature maps. The fully connected layer is replaced with a dense TWTA-layer, consisting of 50 blocks of 8 competing units."}]}