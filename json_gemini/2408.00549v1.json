{"title": "Learning to Embed Distributions via Maximum Kernel Entropy", "authors": ["Oleksii Kachaiev", "Stefano Recanatesi"], "abstract": "Empirical data can often be considered as samples from a set of probability distributions. Kernel methods have emerged as a natural approach for learning to classify these distributions. Although numerous kernels between distributions have been proposed, applying kernel methods to distribution regression tasks remains challenging, primarily because selecting a suitable kernel is not straightforward. Surprisingly, the question of learning a data-dependent distribution kernel has received little attention. In this paper, we propose a novel objective for the unsupervised learning of data-dependent distribution kernel, based on the principle of entropy maximization in the space of probability measure embeddings. We examine the theoretical properties of the latent embedding space induced by our objective, demonstrating that its geometric structure is well-suited for solving downstream discriminative tasks. Finally, we demonstrate the performance of the learned kernel across different modalities.", "sections": [{"title": "1 Introduction", "content": "Most discriminative learning methods conventionally assume that each data point is represented as a real-valued vector. In practical scenarios, however, data points often manifest as a 'set' of features or a 'group' of objects. A quintessential example is the task of predicting a health indicator based on multiple blood measurements. In this case, the single data point of a patient has multiple, or a distribution of, measurements. One approach to accommodate such cases involves representing each input point as a probability distribution. Beyond mere convenience, it is more appropriate to model input points as distributions when dealing with missing data or measurement uncertainty, as often encountered when facing the abundance of data, which commonly presents a challenge for data-rich fields such as genetics, neuroscience, meteorology, astrophysics, or economics.\n\nThe task of regressing a mapping of probability distributions to a real-valued response is known as distribution regression. Distribution regression has been successfully applied in various fields, such as voting behavior prediction [13], dark matter halo mass learning [41], human cancer cells detection [42], brain-age prediction [6], among others [36, 31, 61]. The versatility and effectiveness of this framework underscore its power in solving complex problems [62, 27]. Kernel methods have become a widely used approach for solving distribution regression tasks by exploiting a kernel between distributions referred to as a distribution kernel. Despite the multitude of proposed kernels, the practical application of kernel methods remains challenging due to the nontrivial choice of the appropriate kernel. While some efforts focus on identifying kernels with broad applicability and favorable statistical properties [52], others aim to tailor kernels to the geometric characteristics of specific input spaces [6]. Remarkably, the question of learning data-dependent kernels has received limited attention. This study is thus driven by a fundamental question: What are the"}, {"title": "2 Preliminaries", "content": "We first introduce the main concepts necessary to formalize our learning framework: kernel mean embeddings and covariance operator embeddings."}, {"title": "2.1 Kernel Embeddings of Distributions", "content": "Consider an input space X and a positive-definite (p.d.) kernel $k : X \\times X \\rightarrow \\mathbb{R}$. Let H be the corresponding reproducible kernel Hilbert space (RKHS) induced by such kernel. Consider a probability distribution $P\\in \\mathcal{P}(X)$. The kernel mean embedding map embeds the distribution P as a function in Hilbert space:\n\n$\\mu_P = \\mu(P) := \\int_X k(x,.) dP(x) = \\int_X \\varphi(x) dP(x)$,\n\nwhere $\\varphi : X \\rightarrow \\mathcal{H}$ is a feature map such that $\\varphi(x) = k(x,\\cdot)$.\n\nImportantly, if the kernel k is characteristic [48], the mapping $\\mu : \\mathcal{P}(X) \\rightarrow \\mathcal{H}$ is injective, implying that all information about the original distribution is preserved in H. This last property underscores much of power under recent applications of kernel mean embeddings [39, 46, 14]. The natural empirical estimator for the kernel mean embedding approximates the true distribution with a finite sum of Dirac delta functions:\n\n$\\hat{\\mu}_P := \\frac{1}{N} \\sum_{i=1}^N \\varphi(x_i) \\in \\mathcal{H}$"}, {"title": "2.2 Covariance Operators and Entropy", "content": "A second way of mapping a probability distribution to a Hilbert space can be defined by means of a covariance operators. For a given feature map $\\varphi(x) = k(x, \\cdot) : X \\rightarrow \\mathcal{H}^1$, and a given probability distribution $P\\in \\mathcal{P}(X)$, the covariance operator embedding is defined as:\n\n$\\Sigma_P := \\int_X \\varphi(x) \\otimes \\varphi(x) dP(x)$\n\nwhere $\\otimes$ is a tensor product. $\\Sigma_P$ is a self-adjoint positive semi-definite (p.s.d.) operator acting on H. Such operator can be seen as a mean embedding w.r.t. the feature map $x \\rightarrow \\varphi(x) \\otimes \\varphi(x)$ and therefore, for a universal kernel k, the map $P\\rightarrow \\Sigma_P$ is injective (see Bach [1]).\n\nSimilarly to Eq. (2), the natural empirical estimator is:\n\n$\\hat{\\Sigma}_P = \\frac{1}{N} \\sum_{i=1}^N \\varphi(x_i) \\otimes \\varphi(x_i)$\n\nFor a translation-invariant kernel $k(x,x') = k(x - x')$ normalized such that $k(x,x) = 1$, the covariance operator $\\Sigma_P$ is a density operator [1]. Henceforth, entropy measures can be applied to it, and the quantum R\u00e9nyi entropy of the order $\\alpha$ can be defined as:\n\n$\\delta_\\alpha(\\Sigma_P) := \\frac{1}{1-\\alpha} \\log \\text{tr} [(\\Sigma_P)^\\alpha] = \\frac{1}{1-\\alpha} \\log \\sum_i \\lambda_i^\\alpha$\n\nwhere ${\\lambda_i}_i$ are the eigenvalues of $\\Sigma_P$. The Von Neumann entropy can be seen as a special case of R\u00e9nyi entropy in the limit $\\alpha \\rightarrow 1$. However, in our work, we focus primarily on the second-order case of R\u00e9nyi entropy, i.e. $\\alpha = 2$ (see Carlen [9], M\u00fcller-Lennert et al. [40], Wilde [58], Giraldo et al. [15] for an in-depth overview of the properties and theory of quantum entropies)."}, {"title": "3 Unsupervised Distribution Kernel Learning", "content": ""}, {"title": "3.1 Distribution Regression", "content": "In this section, we discuss the key topics in distribution regression, including problem setup, the notion of a 2-stage sampling process, and the common solutions to regression employing kernel methods.\n\nDistribution regression extends the common regression framework to the setup where covariates are given as probability distributions available only through samples. Formally, consider the task of finding a regressor $f : \\mathcal{P}(X) \\rightarrow \\mathcal{Y}$ from the dataset of samples $\\Gamma_M = \\{(P_i,y_i)\\}_{i=1}^M$, where $P_i \\in \\mathcal{P}(X)$ are distributions provided as a set of i.i.d. empirical samples $x_1, ..., x_N \\sim P_i$ (see Poczos et al. [43], Szab\u00f3 et al. [51, 52] for a comprehensive analysis). A viable approach to solving this problem is to define a kernel $K : \\mathcal{P}(X) \\times \\mathcal{P}(X) \\rightarrow \\mathbb{R}$ that is universal in $\\mathcal{P}(X)$. By setting up such a kernel K, we can utilize kernel-based regression techniques, with SVM often being the preferred method for classification tasks [37] or Kernel Ridge Regression (KRR) when the space $\\mathcal{Y}$ is continuous [35] 2. To this end, several kernels have been proposed over time (see details in Sec. 4)."}, {"title": "3.2 Dataset Embedding", "content": "Instead of using standard kernels designed to encapsulate the geometry of the input space, we consider learning a data-dependent kernel, tailored to the specific properties of the dataset. In a similar vein, Yoshikawa et al. [61] proposed learning an optimal kernel (or equivalently, a feature map) jointly with the classifier to address the text modality. In this work, we focus on an unsupervised problem, aiming to learn a data-dependent kernel between probability distributions without access to classification labels.\n\nWe first introduce proper parametrization to ensure both expressivity and robustness followed by the definition of the optimization objective. Leveraging the idea of 2-level kernel setup, we define the embedding kernel as\n\n$k_{\\theta} : X \\times X \\rightarrow \\mathbb{R} = k_{\\text{emb}}(f_{\\theta}(x), f_{\\theta}(x'))$.\n\nwhere $f_{\\theta}$ is a trainable encoder function $f_{\\theta} : X \\rightarrow \\mathcal{Z}$, $\\mathcal{Z}$ is a latent encoding space, and $k_{\\text{emb}}$ is a kernel defined on the latent space $k_{\\text{emb}} : \\mathcal{Z} \\times \\mathcal{Z} \\rightarrow \\mathbb{R}$. The encoder function $f_{\\theta}$ transforms every input probability distribution $P\\in \\mathcal{P}(X)$ into a distribution over the latent space $P_{\\theta} \\in \\mathcal{P}(\\mathcal{Z})$ 3\n\nBy the definition of the encoding process, $P_{\\theta}$ is a push-forward measure. For an empirical probability distribution $q = \\sum_i \\delta(x_i) \\in \\mathcal{P}(X)$ and a measurable map $f : X \\rightarrow \\mathcal{Z}$, the push-forward measure $f_{\\#}q \\in \\mathcal{P}(\\mathcal{Z})$ is defined as $\\sum_i \\delta(f(x_i))$."}, {"title": "3.3 Unsupervised Optimization Objective", "content": "This dataset level representation depends on the choice of first and second level kernels k, K and, in turn, on the trainable function $f_\\theta$ parameterized by the set of parameters $\\theta$. In this work, we propose learning the parameters $\\theta$ to maximize quantum entropy of the dataset embedding, i.e.,\n\n$\\theta = \\text{arg} \\max_\\theta \\{ S_2(\\Sigma_D) := - \\log \\text{tr} [(\\frac{1}{M}\\Sigma_D)^2] \\}.$\n\nAs we will describe in brief, optimizing this target has clear benefits inherited from the underlying geometry of the setup. But, first, we show how to empirically compute $S_2(\\Sigma_D)$. Building upon previous work [1]4, we exploit the following property of the covariance estimator:\n\n$\\text{tr} [(\\Sigma_D)^2] = \\frac{1}{M} \\text{tr} [K_D^2]$\n\nwhere $K_D \\in \\mathbb{R}^{M\\times M}$ is the distribution kernel matrix, with $[K_D]_{ij} = K_{\\text{distr}}(\\mu_{P_i}, \\mu_{P_j})$. This equation follows directly from the fact that $\\Sigma_D$ and $\\frac{1}{M}K_D$ share the same set of eigenvalues. Leveraging this relationship, we can define tractable unsupervised training loss, which we term Maximum Distribution Kernel Entropy (MDKE), with respect to the parameters of the encoder $f_\\theta$:\n\n$\\mathcal{L}_{\\text{MDKE}}(\\theta) := - S_2(\\Sigma_D) = \\log \\text{tr} [(\\frac{1}{M}K_D)^2] = \\log \\sum_i (\\frac{1}{M} \\lambda_i)^2 = \\log \\|\\frac{1}{M}K_D \\|_F^2$\n\nwhere the latter relies on the fact that the Frobenius norm $\\|A\\|_F^2 = \\sum_i \\lambda_i^2(A)$, where $\\lambda_i(A)$ are eigenvalues A.\n\nThe MDKE objective is differentiable w.r.t. $\\theta$ for commonly used kernels, provided that the encoder $f_\\theta$ is differentiable as well. While the entropy estimator $S_2(\\Sigma_D)$ is convex in the kernel matrix $K_D$,"}, {"title": "3.4 Geometrical Interpretation", "content": "The optimization objective is specifically designed to minimize the variance within each distribution (inner-distribution variance) while simultaneously maximizing the spread of distributions over the compact latent space $\\mathcal{Z} = \\mathcal{S}^{d-1}$. This shaping of the distributions embeddings in the latent space facilitates easier separation in downstream tasks. In this section we show that the geometry of the optimal (w.r.t. the MDKE loss) configuration of mean embeddings in the RKHS attains describe properties. For doing so we leverage the notion of distributional variance $\\mathcal{V}_\\mathcal{H}$ (Definition 1 in Muandet et al. [38]).\n\nDefinition 3.2. For a set of M probability distributions $\\mathcal{D}_M$, distributional variance $\\mathcal{V}_\\mathcal{H}(\\mathcal{D}_M)$ of the mean embeddings in the RKHS $\\mathcal{H}$ is given by\n\n$\\mathcal{V}_\\mathcal{H}(\\mathcal{D}_M) := \\frac{1}{M} \\text{tr} [G] - \\frac{1}{M^2} \\sum_{i=1}^M \\sum_{j=1}^M G_{ij}$\n\nwhere G is the M \u00d7 M Gram matrix of mean embeddings in $\\mathcal{H}$, i.e. $G_{ij} = \\langle \\mu_{P_i}, \\mu_{P_j} \\rangle_\\mathcal{H}$ [38].\n\nHere we show that the distributional variance $\\mathcal{V}_\\mathcal{H}$ can be equally reformulated into two separate contributions:\n\n$\\mathcal{V}_\\mathcal{H}(\\mathcal{D}_M) = \\frac{1}{M} \\sum_{i=1}^M \\|\\mu_{P_i}\\|_\\mathcal{H}^2 - \\|\\mu_{\\bar{P}}\\|_\\mathcal{H}^2$\n\nwhere $\\bar{P}$ denotes mixture distribution with elements of $\\mathcal{D}_M$ being uniformly weighted mixture components (see proof in the Appendix A.1).\n\nThe relevance of distributional variance for MDKE objective is established by the following result.\n\nProposition 3.3. For a set of M probability distributions $\\mathcal{D}_M$, the second-order R\u00e9nyi entropy $S_2$ of the empirical covariance operator embedding $\\Sigma_D$ induced by the choice of Gaussian distribution kernel $K_{\\text{RBF}}$ over points in the RKHS $\\mathcal{H}_{\\text{emb}}$, as defined in Eq. (8), - is upper bounded by the distributional variance $\\mathcal{V}_{\\mathcal{H}_{\\text{emb}}}(\\mathcal{D}_M)$, i.e.,\n\n$2^{-S_2(\\Sigma_D)} \\leq \\mathcal{V}_{\\mathcal{H}_{\\text{emb}}}(\\mathcal{D}_M)$\n\nwhere $\\gamma$ is the bandwidth of the distribution kernel $K_{\\text{RBF}}$.\n\nThe proof of this proposition is provided in Appendix A.3. This result formalizes the fact that our objective increases distributional variance, pushing up the average squared norm of mean embedding of input distributions while minimizing squared norm of the mean embedding of the mixture. We further explore the geometrical implications of such optimization by formalizing connection between the variance of the distribution and the squared norm of the its mean embedding in RKHS.\n\nProposition 3.4. Under Assumption 3.1, the maximum norm of kernel mean embedding is attained by Dirac distributions {$\\delta_z\\}_{z\\in \\mathcal{Z}}$.\n\nThis result is trivial due to the fact that the set of mean embeddings is contained in the convex hull of {$k_{\\text{emb}}(z,\\cdot)$}\\}_{z\\in \\mathcal{Z}}, and, under Assumption 3.1, $\\forall z \\in \\mathcal{Z} : \\|k_{\\text{emb}}(z,\\cdot)\\|_{{\\mathcal{H}_{\\text{emb}}}} = k_{\\text{emb}}(z, z) = 1$.\n\nProposition 3.5. Under Assumption 3.1, uniform distribution $U(\\mathcal{Z})$ is a unique solution of\n\n$\\underset{P \\in \\mathcal{P}(\\mathcal{Z})}{\\text{argin} \\min } \\| \\mu_{\\text{emb}}(P)\\|_\\mathcal{H}_{\\text{emb}}^2 = \\int_{\\mathcal{Z}\\times \\mathcal{Z}} k_{\\text{emb}}(z,z') dP(z)dP(z')$."}, {"title": "3.5 An Illustrative Example", "content": "We use a simple example to illustrate the connection between geometrical configurations of embedded distributions and distribution kernel entropy $S_2(\\Sigma_D)$ (see Fig. 2). We sample a number of points from 6 different Gaussian distributions and project on a sphere $\\mathcal{S}^2$ varying their projected variance $\\gamma$. As $\\gamma$ decreases, the distributional variance of the overall distribution of Gaussians increases (Fig. 2a). For very small $\\gamma$ each distribution converges to a point (a Dirac distribution). This results in the entropy interpolating between lower and upper bounds, demonstrating how entropy behaves in response to changes in distribution variance. Fig. 2b showcases the behavior of two terms comprising distributional variance (Eq. (16)): the average kernel norm of the distributions alongside the kernel norm of the mixture. The increase in entropy and variance corresponds to a 'flattening' effect on the spectrum of the distribution kernel matrix. This example provides a simplified picture of how input distributions configurations influence kernel entropy."}, {"title": "3.6 Limitations", "content": "Runtime complexity. The applicability of a data-dependent distribution kernel to solving discriminative tasks relies on the structure of the dataset being well-suited for distribution regression modeling. The model performs best when the number of input distributions is relatively small (e.g., thousands rather than millions), while the number of samples per distribution is large. It is crucial to note that the computational complexity of the proposed method, which is a common concern in practical applications, is most favorable for the tasks described. A detailed analysis of runtime complexity can be found in Appendix B.3.\n\nBroader impact. We wish to emphasize that the distribution regression framework has emerged as a powerful tool for analysis and predictive modeling, especially in domains where traditional methods face challenges, including social science, economics, and medical studies. We urge researchers and practitioners applying distribution regression in these areas to give special consideration to issues such as bias, fairness, data quality, and interpretability, - aspects that are currently under-researched in the context of distributional regression, largely due to the relative novelty."}, {"title": "4 Related Work", "content": ""}, {"title": "4.1 Distribution Regression", "content": "Distribution regression was introduced in Poczos et al. [43], while the seminal work of Szab\u00f3 et al. [52] provides a comprehensive theoretical analysis of this regression technique. A natural approach"}, {"title": "4.2 Matrix Information Theory", "content": "Quantum entropy, including R\u00e9nyi entropy, is a powerful metric to describe information in a unique way (see M\u00fcller-Lennert et al. [40] for foundational insights). Giraldo et al. [15] designed the measure of entropy using operators in RKHS to mimic R\u00e9nyi entropy's behavior, offering the advantage of direct estimation from data. Bach [1] applied von Neumann entropy of the density matrix to the covariance operator embedding of probability distributions, thereby defining an information-theoretic framework utilizing kernel methods. In machine learning, especially within self-supervised learning (SSL) setups, entropy concepts have recently found novel applications. Our study builds on most recent developments [47, 21, 53] by applying quantum R\u00e9nyi entropy to the covariance operator in RKHS."}, {"title": "5 Experiments", "content": "We here demonstrate that our proposed method successfully performs unsupervised learning of data-dependent distribution kernel across different modalities. The experimental setup is divided into two phases: unsupervised pre-training and downstream regression classification using the learned kernel.\n\nFor each dataset, we select a hold-out validation subset with balanced classes, while the remainder of the dataset is utilized for unsupervised pre-training. We use mini-batch ADAM [22] with a static learning rate of 0.0005. We report mini-batch based (instead of epoch based) training dynamics as our tasks do not require cycling over the entire dataset to converge to the optimal loss value. All experiments use Gaussian kernel both as an embedding kernel and distribution kernel, the hyperparameter selection is performed as described in Appendix B.1.\n\nOnce the samples encoder $f_\\theta$ is learned, we employ it to compute distribution kernel Gram matrix, used as an input to the Support Vector Machine (SVM) for solving downstream classification tasks. A grid search with 5 splits (70/30) is conducted to optimize the strength of the squared $l_2$ regularization penalty C, exploring 50 values over the log-spaced range {$10^{-7}, ..., 10^{5}$}. The best estimator is then applied to evaluate classification accuracy on the validation subset, which we report.\n\nAdditional experiments exploring the application of data-dependent distribution kernels in domains where distribution regression models are less common, such as image and text, are presented in Appendix D."}, {"title": "5.1 Flow Cytometry", "content": "Flow Cytometry (FC) is a widely used technique for measuring chemical characteristics of mixed cell population. Because population-level properties are described through (randomized) sampling of cells, FC is used as a canonical setup of distribution regression. For this study we used a dataset [54] where more than 100.000 cells are measured per each patient (sub- ject). For each cell a total of ten parameters are reported, hence, we treated each subject as an empirical distribution over $\\mathbb{R}^{10}$. We considered downstream classification tasks on two different sets of labels. The first ('Tissue' classification) contains peripheral blood (pB) and bone marrow (BM) samples from N = 44 subjects. The second ('Leukemia' classification) presents healthy and leukemia BM cell samples, N = 50. Classes were balanced in both cases."}, {"title": "6 Conclusion", "content": "In this work, we presented an unsupervised way of learning data-dependent distribution kernel. While previous studies in distribution regression predominantly relied on hand-crafted kernels, our work, in contrast, demonstrates that entropy maximization can serve as a powerful guiding principle for learning adaptable, data-dependent kernel in the space of distributions. Our empirical findings show that this technique can not only serve as a pre-training step to enhance the performance of downstream distribution regression tasks, but also facilitate complex analyses of the input space. The interpretation of the learning dynamics induced by the proposed objective relies on a theoretical link between the quantum entropy of the dataset embedding and distributional variance. This theoretical link, which we have proven, enables us to approach the optimization from a geometrical perspective, providing crucial insights into the flexibility of the learned latent space encoding.\n\nWe hope that theoretically grounded way of learning data-dependent kernel for distribution regression tasks will become a strong alternative to the common practice of hand-picking kernels. More broadly, our results present a methodology for leveraging the distributional nature of input data along side the novel perspective on the encoding of complex input spaces. This highlights the potential to extend the application of more advanced learning methods, embracing the ever-increasing complexity of data by going beyond more conventional vector-based representations."}, {"title": "A Proofs", "content": "In this section, we present the proofs for the propositions outlined in our study. To ensure clarity, we will first restate the setup and introduce necessary concepts.\n\nWe define the input space as X, and $\\mathcal{P}(X)$ represents the space of probability distributions over X. Consider a dataset of M probability distributions, denoted as $\\mathcal{D}_M = \\{P_i \\in \\mathcal{P}(X)\\}_{i=1}^M$. With a p.d. characteristic embedding kernel $k : X \\times X \\rightarrow \\mathbb{R}$, the corresponding RKHS H, and the feature map $\\varphi : X \\rightarrow \\mathcal{H}$, we define the mean embedding map $\\mu : \\mathcal{P}(X) \\rightarrow \\mathcal{H}$ such that $\\mu_P = \\mu(P) := \\int_X \\varphi(x) dP(x)$. A p.d. translation-invariant characteristic distribution kernel $K : \\mathcal{P}(X) \\times \\mathcal{P}(X) \\rightarrow \\mathbb{R}$ is defined using the mean embeddings of corresponding distributions. For simplicity, $\\mu_i$ denotes $\\mu(P_i)$.\n\nAdditional concepts essential for our proofs include the Gram matrix of mean embeddings $G \\in \\mathbb{R}^{M\\times M}$, representing the inner products of mean embeddings in the dataset, i.e., $G := [\\langle \\mu_i, \\mu_j \\rangle_\\mathcal{H}]_{ij}$. The kernel matrix $K_D \\in \\mathbb{R}^{M\\times M}$ with respect to the distribution kernel K is denoted as $K_D := [K(P_i, P_j)]_{ij}$. We also recall the definition of distributional variance $\\mathcal{V}_\\mathcal{H}$ (see Eq. (15)):\n\n$\\mathcal{V}_\\mathcal{H}(\\mathcal{D}_M) := \\frac{1}{M} \\text{tr} [G] - \\frac{1}{M^2} \\sum_{i=1}^M \\sum_{j=1}^M G_{ij}$\n\nThese definitions and notations will be referenced throughout the proofs."}, {"title": "A.1 Kernel Norms Gap and Distributional Variance", "content": "For both cases of input distributions being empirical probability distributions or continuous densities, we define mixture distribution $\\bar{P}$, with a slight abuse of notation:\n\n$\\bar{P}(x) := \\frac{1}{M} \\sum_{i=1}^M P_i(x)$\n\nLemma A.1. For the mixture distribution $\\bar{P}$ and the Gram matrix G, the following relationship holds:\n\n$\\|\\mu_{\\bar{P}}\\|_\\mathcal{H}^2 = \\frac{1}{M^2} \\sum_{i=1}^M \\sum_{j=1}^M G_{ij}$\n\nProof. We begin by recalling that the inner product between mean embeddings $\\mu_i$ and $\\mu_j$ in H is given by:\n\n$\\langle \\mu_i, \\mu_j \\rangle_\\mathcal{H} = \\int_{X\\times X} k(x, x') dP_i(x) dP_j(x')$\n\nSubstituting this into the expression for the Gram matrix, we have:\n\n$\\frac{1}{M^2} \\sum_{i=1}^M \\sum_{j=1}^M G_{ij} = \\frac{1}{M^2} \\sum_{i=1}^M \\sum_{j=1}^M \\int_{X\\times X} k(x, x') dP_i(x) dP_j(x')$\n\n$= \\int_{X\\times X} k(x, x') \\frac{1}{M} \\sum_{i=1}^M dP_i(x) \\frac{1}{M} \\sum_{j=1}^M dP_j(x') \\ = \\int_{X\\times X} k(x, x') d\\bar{P}(x) d\\bar{P}(x') = \\|\\mu_{\\bar{P}}\\|_\\mathcal{H}^2$\n\nThis completes the proof of Lemma A.1.\n\nBy incorporating Eq. (19) into the definition of distributional variance (see Eq. (15)), and noting that the trace of G is the sum of squared norms of input distributions (i.e., $\\text{tr}[G] = \\sum_{i=1}^M \\|\\mu_{P_i}\\|^2$), we obtain:"}, {"title": "A.2 Pairwise Distance and Distributional Variance", "content": "Definition A.2. For a dataset $\\mathcal{D}_M$ of M probability distributions, we define the average pairwise distance between kernel mean embeddings in H as $\\mathcal{J}_\\mathcal{H}(\\mathcal{D}_M)$, given by:\n\n$\\mathcal{J}_\\mathcal{H}(\\mathcal{D}_M) := \\frac{1}{M^2} \\sum_{i=1}^M \\sum_{j=1}^M \\|\\mu_{P_i} - \\mu_{P_j}\\|_\\mathcal{H}^2$\n\nLemma A.3. For the distributional variance $\\mathcal{V}_\\mathcal{H}$ of the dataset $\\mathcal{D}_M$, the following relationship holds:\n\n$\\mathcal{V}_\\mathcal{H}(\\mathcal{D}_M) = \\frac{1}{2} \\mathcal{J}_\\mathcal{H}(\\mathcal{D}_M)$\n\nProof. Starting with the definition of $\\mathcal{J}_\\mathcal{H}(\\mathcal{D}_M)$:\n\n$\\mathcal{J}_\\mathcal{H}(\\mathcal{D}_M) = \\frac{1}{M^2} \\sum_{i=1}^M \\sum_{j=1}^M \\|\\mu_{P_i} - \\mu_{P_j}\\|_\\mathcal{H}^2$\n\n$= \\frac{1}{M^2} \\sum_{i=1}^M \\sum_{j=1}^M (\\mu_{P_i} - \\mu_{P_j}, \\mu_{P_i} - \\mu_{P_j})_\\mathcal{H}$\n\n$= \\frac{1}{M^2} \\sum_{i=1}^M \\sum_{j=1}^M (\\langle \\mu_{P_i}, \\mu_{P_i} \\rangle_\\mathcal{H} + \\langle \\mu_{P_j}, \\mu_{P_j} \\rangle_\\mathcal{H} - 2 \\langle \\mu_{P_i}, \\mu_{P_j} \\rangle_\\mathcal{H})$\n\n$= \\frac{1}{M^2} \\bigg( 2M \\sum_{i=1}^M \\langle \\mu_{P_i}, \\mu_{P_i} \\rangle_\\mathcal{H} - 2 \\sum_{i=1}^M \\sum_{j=1}^M \\langle \\mu_{P_i}, \\mu_{P_j} \\rangle_\\mathcal{H} \\bigg)$\n\n$= 2 \\bigg( \\frac{1}{M} \\sum_{i=1}^M \\langle \\mu_{P_i}, \\mu_{P_i} \\rangle_\\mathcal{H} - \\frac{1}{M^2} \\sum_{i=1}^M \\sum_{j=1}^M \\langle \\mu_{P_i}, \\mu_{P_j} \\rangle_\\mathcal{H} \\bigg)$\n\nRecognizing that $\\sum_{i=1}^M \\langle \\mu_{P_i}, \\mu_{P_i} \\rangle_\\mathcal{H} = \\text{tr}[G]$, we can equate expression in the brackets to $\\mathcal{V}_\\mathcal{H}(\\mathcal{D}_M)$, leading to:\n\n$\\mathcal{V}_\\mathcal{H}(\\mathcal{D}_M) = \\frac{1}{2} \\mathcal{J}_\\mathcal{H}(\\mathcal{D}_M)$\n\nLemma A.3 will be instrumental in further proofs, establishing a crucial link between distributional variance in H and quantum entropy of the covariance operator embedding $\\Sigma_D$."}, {"title": "A.3 Distribution Kernel Entropy Upper-Bound", "content": "In this section, we provide the proof for the key theoretical result stated in Proposition 3.3.\n\nConsider a dataset D consisting of probability distributions {P \u2208 $\\mathcal{P}(X)$}i sampled i.i.d. from an unknown meta-distribution D. We assert that the second-order R\u00e9nyi entropy $S_2$ of the empirical covariance operator embedding $\\Sigma_D$, induced by the choice of Gaussian distribution kernel $K_{\\text{RBF}}$ over points in the RKHS H, is upper-bounded by the distributional variance $\\mathcal{V}_\\mathcal{H}(D)$:\n\n$\\frac{1}{2}S_2(\\Sigma_D) \\leq \\gamma \\mathcal{V}_\\mathcal{H}(D)$\n\nwhere $\\gamma$ is the bandwidth of the distribution kernel $K_{\\text{RBF}}$."}, {"title": "A.4 Generalized Variance in RKHS", "content": "In this section we prove the connection between generalized variance and norm of the kernel mean embedding.\n\nSo far we established the squared norm of mean embedding is maximized by Dirac points (zero variance) and is minimized by uniform distribution (max variance). We now formalize the intuition that larger squared norm in embedding kernel RHKS corresponds to smaller variance in the latent space. We first note that under Assumption 3.1", "56": "Let X be a random variable which takes values in a Fr\u00e9chet space F equipped with seminorm $\\| \u00b7 \\|_a$. And suppose that X is square-integratable", "following\n\n$\\text{Var}[X": "E\\|X - \\mu\\|^2$.\n\nNote that for a random variable in RKHS defined as a push-forward of a probability distribution $P \\in \\mathcal{P}(\\mathcal{Z})$ over the latent space, i.e. $X = z \\sim f_{\\#}P$ satisfies conditions of Definition A.4 with $F = H, \\| \\|_a = \\| \\| _\\mathcal{H}_{\\text{emb}}$, and a Pettis integral being a kernel mean embedding. Denote the described variance as $\\text{Var}_{k_{\\text{emb}"}]}