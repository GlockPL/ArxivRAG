{"title": "HandDGP: Camera-Space Hand Mesh Prediction\nwith Differentiable Global Positioning", "authors": ["Eugene Valassakis", "Guillermo Garcia-Hernando"], "abstract": "Predicting camera-space hand meshes from single RGB im-\nages is crucial for enabling realistic hand interactions in 3D virtual and\naugmented worlds. Previous work typically divided the task into two\nstages: given a cropped image of the hand, predict meshes in relative\ncoordinates, followed by lifting these predictions into camera space in\na separate and independent stage, often resulting in the loss of valu-\nable contextual and scale information. To prevent the loss of these cues,\nwe propose unifying these two stages into an end-to-end solution that\naddresses the 2D-3D correspondence problem. This solution enables back-\npropagation from camera space outputs to the rest of the network through\na new differentiable global positioning module. We also introduce an im-\nage rectification step that harmonizes both the training dataset and the\ninput image as if they were acquired with the same camera, helping to\nalleviate the inherent scale-depth ambiguity of the problem. We validate\nthe effectiveness of our framework in evaluations against several baselines\nand state-of-the-art approaches across three public benchmarks.", "sections": [{"title": "1 Introduction", "content": "Predicting 3D hand meshes from single-view RGB images has become an increas-\ningly popular research area due to its potential in augmented and virtual reality\napplications, such as virtual try-on experiences [49], human digitization [16], gam-\ning [22], and teleoperation [1,18]. Despite recent progress, challenges remain [3,52]\ndue to the hand's articulated structure, self-occlusions, annotation difficulty, and\n2D-to-3D scale and depth ambiguity.\nBecause of these challenges, most previous work have focused on predicting\nquality root-relative hand meshes, i.e. 3D hand meshes in coordinates relative\nto a pre-defined root joint, such as the wrist [14], as opposed to predicting in\nthe global camera space. Root-relative predictions with a camera projection\nmodel [4, 8, 29,54] can be sufficient in applications that end up displayed on 2D\nimages, such as virtual try-on experiences. However, camera-space predictions are\ncritical for interactions in 3D virtual and augmented worlds, e.g. in applications\nsuch as gaming, and office work when using mixed-reality headsets [2,37]."}, {"title": "2 Related Work", "content": "Camera-Space 3D Hand Mesh Prediction. Most previous works in monocular\nRGB-based camera-space 3D hand mesh and pose estimation follow a two-\nstage approach: (1) a first stage for predicting hand mesh/pose in root-relative\ncoordinates, and (2) a lifting stage to recover camera-space coordinates from\nroot-relative ones. For monocular 3D hand pose estimation, Iqbal et al. [28]\npredicts in 2.5D root-relative space, and then lifts those predictions to 3D camera\nspace predictions using an analytical solution. This result is up to a scaling\nfactor, however, and to resolve the scale ambiguity an extra scale parameter\nis required, which is assumed to be provided [48] or globally estimated from\ndata. I2L-MeshNet [40] proposes a regression approach to recover root-relative\n2.5D meshes and subsequently lifts them to camera-space using a separate\nnetwork RootNet [39] which uses prior anthropometric knowledge to reduce\nscale ambiguity [34]. NFV [26] proposes a neural voting approach with a 3D\nimplicit function that directly regresses 3D hand poses in camera-space from\nfull images. NFV uses a Marching Cubes post processing to predict meshes,\nwhich degrades efficiency and is at the cost of not having semantic mappings\nfor their predicted vertices, which are crucial for some applications. Hasson et\nal. [24] predicts both object and hand camera-space translations using both hand\nand object cues, but makes assumptions on the geometry of the object which\nfacilitate the scale recovery. Closest to our work, CMR [15], MobRecon [14] and\nHandOccNet [41] first predict both 2D keypoints and 3D root-relative meshes,\nwhile 3D camera-space coordinates are obtained with a test-time registration\nfunction that estimates the root position. This function typically aims to find the"}, {"title": "3 Proposed Framework", "content": "Overview. Shown in Figure 2, the core idea behind our approach is to exploit\nthe geometry of the problem to integrate hand root finding in a differentiable\npipeline that can predict a hand mesh directly in 3D camera-space coordinates.\nPredicting root-relative hand meshes. Starting from one RGB image\n$I \\in \\mathbb{R}^{H \\times W \\times 3}$, we first predict a set of 2D keypoints $K^{2D} = \\{k_i^{2D}\\}_{i=1}^{i=N_K}$ that can be\njoints or other landmarks, a set of root-relative 3D vertices $V^{rel} = \\{v_i^{rel}\\}_{i=1}^{N_V}$, and\na set of weights $W = \\{w_i\\}_{i=1}^{N_K}$, that represent the confidence in the predictions\nof each keypoint. We then obtain $K^{3D} = \\{k_i^{3D}\\}_{i=1}^{N_K}$, a set of root-relative 3D\nkeypoints on the hand model that correspond to the 2D keypoints $K^{2D}$. To obtain\n$K^{3D}$, we assume having access to a 3D keypoint regressor $J_{reg} : V^{rel} \\rightarrow K^{3D}$.\n$J_{reg}$ usually comes in the form of a matrix, which defines keypoints on the hand\nas a linear combination of mesh vertices, and is typically provided with popular\nmesh models such as MANO [46] for hands and SMPL [42] for full body meshes.\nFinding the hand root with HandDGP. Our key innovation is our differen-\ntiable global positioning (DGP or positioning module, for brevity), which uses\nthe root-relative 3D keypoints $K^{3D}$, the 2D keypoints $K^{2D}$ and the weights $W\nin order to predict a global translation $t \\in \\mathbb{R}^{3}$ in camera-space which we can use\nto obtain the camera-space vertex predictions $V^{cs} = \\{v_i^{cs}\\}_{i=1}^{N_V}$, with\n$v_i^{cs} = v_i^{rel} + t$.\n(1)\nThe camera-space vertex predictions can finally be used to project the mesh\ninto 2D using a pinhole camera perspective projection. This pipeline allows us to"}, {"title": "3.1 Differentiable Global Positioning", "content": "At the core of our method lies the differentiable global positioning module, which\ntakes in $K^{3D}, K^{2D}, J_{reg}$, and the camera intrinsics $A$ as input, and outputs the\nglobal camera-space translation $t$ in a differentiable manner. Although our full\napproach also considers the keypoint confidences $W$, for clarity, in this section\nwe describe how we obtain the global translation assuming equally confident\nkeypoints. We explain how we incorporate keypoint confidences in Section 3.2.\nTo obtain the global translation $t = [T_x, T_y, T_z]^T$ in a differentiable way, we\nderive a solution based on the Direct Linear Transform (DLT) [23,44], adapted\nto our specific problem. Firstly, by design $K^{3D}$ and $K^{2D}$ give us a set of 2D-\n3D correspondences $M = \\{(k_i^{3D}, k_i^{2D})\\}_{i=1}^{N_K}$, with $k_i^{3D} = [x_i, y_i, z_i]^T$ and $k_i^{2D} =$\n$[u_i, v_i]^T$. Additionally, it is important to note that the 3D keypoints $K^{3D}$ are\nexpressed in a frame that shares the same orientation as the camera frame, with\nonly the global root translation missing to map root-relative keypoint coordinates\nto camera-space coordinates. Assuming a pinhole camera model with intrinsic\nparameters $A$, we can express the projection equation as:\n$d_i \\begin{bmatrix} u_i \\\\ v_i \\\\ 1 \\end{bmatrix} = A \\begin{bmatrix} 1 & 0 & 0 & T_x \\\\ 0 & 1 & 0 & T_y \\\\ 0 & 0 & 1 & T_z \\end{bmatrix} \\begin{bmatrix} x_i \\\\ y_i \\\\ z_i \\\\ 1 \\end{bmatrix}$,\n(2)\nwith $d_i$ the depth value of keypoint $i$. Expanding and re-arranging, this gives a\nsystem of linear equations that can be written in the following form:\n$\\begin{bmatrix} -1 & 0 & u_i' \\\\ 0 & -1 & v_i' \\end{bmatrix} \\begin{bmatrix} T_x \\\\ T_y \\\\ T_z \\end{bmatrix} = \\begin{bmatrix} x_i - z_i u_i' \\\\ y_i - z_i v_i' \\end{bmatrix}$,\n(3)\nwhere\n$\\begin{bmatrix} u_i' \\\\ v_i' \\\\ 1 \\end{bmatrix} = A^{-1} \\begin{bmatrix} u_i \\\\ v_i \\\\ 1 \\end{bmatrix}$.\n(4)\nSince Equation 3 is obtained considering a single keypoint correspondence, and\nsince we have three unknowns, it is under-constrained. Using all the keypoints in"}, {"title": "3.2 Keypoint Selection", "content": "While the solution presented in Section 3.1 allows us to incorporate root finding\ninto an end-to-end differentiable pipeline, it does not provide for any outlier\nfiltering or keypoint selection mechanism that could help filter out more uncertain\ncorrespondences. This can be problematic in cases such as occlusion or self-\nocclusion of parts of the hand. In those instances, occluded parts would presumably\nresult in more uncertain keypoint placements, they would be considered equally\nto visible keypoints when computing the global translation. To address this\nissue, we consider a weighted variant to our approach. Assuming each keypoint\ncorrespondence has a confidence score $w_i$ associated with it it in practice\nobtained from our weight decoder we first construct a weight matrix by\nduplicating each weight once and placing them in a diagonal matrix $W =$\ndiag($[w_1, w_1, w_2, w_2...w_{N_K}, w_{N_K}]$). Then, we consider a weighted least-squares\nminimisation t* = arg mint ||W(At \u2013 B)||2, with closed-form solution:\nt* = (ATA)-1ATB.\n(6)\nThe DGP module first uses the network outputs to build the matrices A and B,\nand then uses the linear least-squares solution to solve for the translation. Since\nall the operations involved are differentiable, we can use this translation to obtain\nand backpropagate through camera-space vertex predictions, fully incorporating\nthe root-finding task in an end-to-end training pipeline.\n$t^* = (A^T W^2 A)^{-1}A^T W^2B$.\n(7)"}, {"title": "3.3 Input Image Rectification", "content": "Inspired by the recent work on monocular 3D geometry estimation from Yin et\nal. [51], the main idea is to establish a canonical camera space and transform all\nthe training data to that space. During inference, the image is rectified just before\nentering the network, and the predictions are then mapped back to the original\ncamera space. The original set of camera parameters is defined by \\{f, u_o, v_o\\}\nwhere $f$ represents the focal length (we assume $f_x = f_y = f$) and $u_o$ and $v_o$\nbe the principal points. We resize the input image $I$ with the ratio $w_r = \\frac{f}{f_c}$\nwhich converts the camera parameters to \\{f_c, w_r u_o, w_r v_o\\}. Different to [51] and"}, {"title": "3.4 Architecture and Training Details", "content": "Network Overview. In practice, our network first takes an image $I \\in \\mathbb{R}^{H \\times W \\times 3}$\nas an input to a convolutional encoder to produce a feature map $F \\in \\mathbb{R}^{H/32 \\times W/32 \\times C}$.\nThe feature map $F$ is then input to three separate decoder heads: a 2D decoder\noutputting a set of $N_K$ 2D keypoints $K^{2D}$, a 3D decoder outputting the root-\nrelative vertices $V^{rel}$, and a weights decoder outputting a set of confidence weights\n$W$. Using $J_{reg}$, we obtain the root-relative 3D keypoints $K^{3D}$, forming a set of\n2D-3D correspondences $M = \\{(u_i, v_i, x_i, y_i, z_i, z_i)\\}$. Using $M$, we then construct\nthe matrices $A$ and $B$, and obtain $t$ using Equations 6 and 7. Finally, we use\n$t, V^{rel}$ and $K^{3D}$ to obtain the camera-space vertices $V^{cs}$ and camera-space\nkeypoints, following Equation 1, and use all of the mentioned network outputs to\nconstruct our training losses.\nWeights decoder. While our 2D and\n3D decoders follow MobRecon [14], our\nweights decoder is illustrated in Fig. 3.\nStarting from the feature map $F \\in$\n$\\mathbb{R}^{H/32 \\times W/32 \\times C}$, we perform a series of\n1 \u00d7 1 convolutions to obtain a new fea-\nture map $F_w \\in \\mathbb{R}^{H/32 \\times W/32 \\times D}$. We\nthen use the 2D positions provided by\n$K^{2D}$ in order to grid sample a set of\n$N_K, D$-dimentional features, which we\nconcatenate in $D \\times N_K$ dimensional\nlatent vector $Z_w$ which is then pro-\ncessed through a set of dense layers\nwith leaky ReLU activations, and the\nfinal output is processed through a sig-\nmoid function, forcing the confidence\nweights to be in [0, 1]."}, {"title": "4 Experiments", "content": "Datasets. We report our experiments on the following datasets:\nFreiHAND [56]. We follow the evaluation protocol by [15, 26] using the\ndataset for our experiments. The dataset consists of images, 3D hand poses\nand MANO [46] fittings. It provides 130,240 training and 3,960 test images.\nHO3D-v2 [20]. This dataset comprises real images capturing 3D hand-object\ninteractions, with 66,034 images in the training set and 11,524 in the test set\nwith MANO [46] model hand mesh annotations. Hands in this dataset suffer\nfrom severe occlusions caused by the manipulated object. The test set is not\npublicly available, and the evaluation is conducted on a public server. The\nserver provides results in camera-space, root-relative, and aligned formats.\nHowever, participants are given ground truth camera-space hand translation\nvalues, and previous work typically reports results using this ground truth.\nHuman3.6M [27] This dataset is a large-scale 3D body pose benchmark\ncontaining 3.6 million frames with annotations of 3D joint coordinates and\nSMPL [42] meshes. We follow existing evaluation protocols [15], but do not\nuse common aligned metrics and measure errors in camera-space. We adapt\nour framework to predict body meshes by just swapping MANO by SMPL.\nMetrics. We report the following metrics:\nCS-MJE / CS-MVE: Measures the error, in terms of Euclidean distance,\nbetween the predicted joints (MJE) / vertices (MVE) and the ground truth\nin camera-space (CS) coordinates. Both are average errors over the test set\nin mm. In some experiments, we compute the Area-Under-the-Curve (AUC)\nof Percentage of Correct Keypoints (PCK) vs. error thresholds.\nRS-MJE / RS-MVE: This measures the error between Procrustes-aligned\npredicted and ground truth joints and vertices. This metric serves as a\nmeasure of root-relative reconstruction quality."}, {"title": "4.1 Baselines and Method Ablations: Descriptions", "content": "Baselines. Our root-relative module is combined with different methods for\npredicting camera-space root translation. Methods labeled as \u2018Baseline + \\{root\nprediction method\\}' incorporate the root-relative module (see Fig. 2) without any\nglobal positioning system during training. Therefore, only the relative-space losses\nare applied and the positioning of the hand mesh in camera space occurs only at"}, {"title": "4.2 Baselines and Method Ablations: Results Discussion", "content": "Learning the 3D Global Positioning Function. In this evaluation, we assess\nthe effect of back-propagating gradients from the DGP to the network, as pro-"}, {"title": "4.3 Comparison with State of the Art", "content": "FreiHAND: In Table 3 and Fig. 5 (a) we\ncompare our proposed framework with state-\nof-the-art camera-space methods. Notably, our\nmethod achieves the lowest camera-space er-\nrors among all methods. We achieve a 2.6 mm\nerror improvement over CMR [15], despite also\nusing segmentation masks in their root finding.\nWe also compare with the ResNet50 variant\nof MobRecon [14] which is the same as our\nbaseline B4 from the previous section. For Mo-only be evaluated for keypoints.\nbRecon, we report our results based on our implementation, which closely aligns\nwith the one provided by the authors, with slight changes in the data processing\nthat are also shared with our method. A similar trend is observed in Figure 5\nwhere our method achieves the highest AUC of vertex PCK's among all methods\nclosely followed by both CMR and MobRecon-RN-50. We also compare favorably\nwith NVF [26], the only available method that, similar to us, predicts directly in\ncamera space. Note that NVF does not directly predict hand meshes, making\nthem not easily comparable due to the loss of the MANO topology as their meshes\nare generated using Marching Cubes. We observe that the root-relative + 2D-3D\nglobal positioning paradigm (ourselves, CMR and MobRecon) performs signifi-\ncantly better than other methods, followed by I2L-MeshNet [40] that uses [39]\nfor root positioning, similar to our baseline B3.\nHO3D-v2: In Table 4 we present quan-\ntitative results of our method compared to\nstate-of-the-art methods in camera-space coor-\ndinates using the public submission server. It is\nimportant to note that previous work typically\nreports their results in relative coordinates af-\nter an aligning step and often uses provided\nground-truth root values. We do not have a way to know which participants on\nthe leaderboard used this ground-truth; because of this, we had to recompute\nthe scores. We report results of the available subset of methods that: i) have\npublicly available code, ii) provide a trained model, and iii) include a global\ncoordinate prediction stage. This dataset is more challenging than FreiHAND"}, {"title": "5 Conclusion", "content": "We presented a framework for camera space hand mesh prediction, enabling\nlearning directly in camera space. Our baseline and ablation studies validated\nour design choices, showing our method surpasses state-of-the-art approaches\nthat predict hand meshes in camera-space coordinates. Estimating absolute 3D\ngeometry from a single RGB image is inherently ill-posed. Rectifying images and\npredicting in camera space help reduce errors.\nOur experiments show that while root-relative error is in the low single-digit\nmillimeters, likely lower than annotation error, camera space error is 6 to 7 times\nlarger. This is visually illustrated in Fig. 5 (b) and (c), suggesting a significant\nportion of total errors stems from 2D-to-3D depth ambiguity. We conjecture\nthat isolating the hand from its context will soon reach a performance ceiling.\nFurther research in new datasets and context-aware approaches, such as using\nscene geometry or objects, is needed to advance camera-space mesh inference."}]}