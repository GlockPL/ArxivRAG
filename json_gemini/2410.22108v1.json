{"title": "Protecting Privacy in Multimodal Large Language Models with MLLMU-Bench", "authors": ["Zheyuan Liu", "Guangyao Dou", "Mengzhao Jia", "Zhaoxuan Tan", "Qingkai Zeng", "Yongle Yuan", "Meng Jiang"], "abstract": "Generative models such as Large Language Models (LLM) and Multimodal Large Language models (MLLMs) trained on massive web corpora can memorize and disclose individuals' confidential and private data, raising legal and ethical concerns. While many previous works have addressed this issue in LLM via machine unlearning, it remains largely unexplored for MLLMs. To tackle this challenge, we introduce Multimodal Large Language Model Unlearning Benchmark (MLLMU-Bench), a novel benchmark aimed at advancing the understanding of multimodal machine unlearning. MLLMU-Bench consists of 500 fictitious profiles and 153 profiles for public celebrities, each profile feature over 14 customized question-answer pairs, evaluated from both multimodal (image+text) and unimodal (text) perspectives. The benchmark is divided into four sets to assess unlearning algorithms in terms of efficacy, generalizability, and model utility. Finally, we provide baseline results using existing generative model unlearning algorithms. Surprisingly, our experiments show that unimodal unlearning algorithms excel in generation and cloze tasks, while multimodal unlearning approaches perform better in classification tasks with multimodal inputs.", "sections": [{"title": "Introduction", "content": "The rapid development of Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) has played a dominant role in both NLP and multimodal applications, largely due to their extensive pre-training on vast copora and their exceptional general reasoning abilities. However, this powerful learning capacity can alsolead to unintended consequences, such as privacy violations or copyright infringements when sensitive information is retained in the model. Retraining the entire model without the problematic data is straightforward but computationally prohibitive and impractical for ensuring all sensitive data is excluded. As a result, machine unlearning (MU) has emerged as an alternative, allowing models to \"forget\" specific data points without requiring a full retraining cycle, while also complying with legal frameworks such as the Right to be Forgotten.To facilitate the development of unlearning in generative models, many existing works have proposed unlearning benchmarks for LLMs. For instance, TOFU introduces a framework that uses synthetic author data to evaluate unlearning algorithms, while WMDP focuses on evaluating hazardous knowledge and testing unlearning methods to mitigate malicious use. However, as we shift towards MLLMs, the need for benchmarks designed to address privacy concerns becomes even more pressing. Existing benchmarks in MLLMs tend to focus on tasks like hallucination reduction or red teaming detection, but there remains a gap in evaluating MLLMs specifically for privacy protection through unlearning. In the context of MLLM, unlearning presents unique challenges due to the interconnected nature of knowledge across different modalities. In a unimodal setting, unlearning only textual information is insufficient compared to a multimodal approach, as the model may still retain knowledge from the visual modality. This entanglement of multimodal information complicates evaluation, making it crucial to develop benchmarks that assess the unlearning effectiveness across both visual and textual modalities.\nTo address this challenge, we propose MLLMU-Bench, a fictitious unlearning benchmark for MLLMs. It features four distinct datasets: Forget Set, Test Set, Retain Set, and Real Celebrity, each designed to evaluate specific aspects of unlearning methods, including unlearning efficacy, generalizability, and model utility, across both multimodal and unimodal settings. In the multimodal setting, both the image and textual information from each individual's profile are used as unlearning inputs, while the unimodal setting relies solely on the individual's textual information. MLLMU-Bench consists of 20.7 K carefully generated questions, covering 500 fictitious profiles created by GPT-40 and 153 real celebrity profiles, reviewed by human experts, used for evaluation. Additionally, MLLMU-Bench incorporates three levels of unlearning scenarios, targeting 5%, 10%, and 15% of the fictitious profiles, while treating the remaining 95%, 90%, and 85% as retain data.\nWe evaluate five baseline methods across all three unlearning setups on two base MLLMs using classification, generation, and cloze tasks. From the experimental results, we observe that unimodal unlearning approaches consistently outperform multimodal ones in generation and cloze tasks for unlearning performance, while multimodal approaches perform significantly better in classification with multimodal inputs. Additionally, we find a trade-off between unlearning effectiveness and model utility across various factors, including performance on retained samples, neighboring concepts, and model general ability. In summary, our contributions are as follows:"}, {"title": "", "content": "1.  We propose MLLMU-Bench, a privacy-preserving multimodal unlearning benchmark designed to evaluate a method's ability to remove private knowledge while maintaining model utility, focusing on Retain Set accuracy, neighbor concepts and model general ability.\n2.  MLLMU-Bench provides a comprehensive evaluation of unlearning in both multimodal and unimodal settings, highlighting the focus of each setup and the interplay between modalities in affecting unlearning performance.\n3.  We conduct extensive experiments with four baseline methods and one prompting technique, offering insights into the trade-offs between unlearning effectiveness and model utility, particularly the impact on general capabilities in MLLMs."}, {"title": "Related Work", "content": "Privacy Protection Regulations. LLMs and MLLMs often memorize large amounts of information during pre-training or fine-tuning on diverse datasets, which may include sensitive data, raising privacy concerns . Privacy regulations like GDPR  and CCPA enforce the right to be forgotten, requiring models to remove specific data upon request. A popular approach is Differential Privacy (DP), which ensures that individual user data in the training set cannot be accessed. However, these techniques are impractical for generative models due to high computational complexity and the degradation of model general ability, necessitating more efficient and targeted unlearning algorithms.\nMU for Generative Models. Many works have explored unlearning in generative models. first defined the setup and objective of unlearning in LLMs as generating whitespace in response to harmful prompts. To mitigate catastrophic forgetting caused by gradient ascent-based approaches , other works  introduced task vector-based techniques. TOFU  later presented a benchmark for unlearning in large language models (LLMs) using synthetic data, highlighting the need for privacy-preserving unlearning methods that ensure the removal of sensitive information while maintaining model performance. However, few works have addressed unlearning in MLLMs, where the challenge lies in removing the effect of data samples across both textual and visual modalities. Even the study  that have attempted MLLM unlearning tend to focus on textual modality, expecting that unlearning in one modality will result in knowledge removal across both."}, {"title": "The MLLMU-Bench Benchmark", "content": "We introduce the MLLMU-Bench benchmark, a novel benchmark meticulously curated to assess the unlearning ability of MLLMs in the context of privacy protection, simulating real-life scenarios. The benchmark encompasses a diverse set of profiles across 70 countries, 240 regions, a wide range of birth years from the 1950s to the 2010s, and 145 distinct employment categories. Additionally, it features over 1,900 unique fun facts tailored to each individual based on their established profiles. Detailed subject coverage and statistics are provided in Figure 1. Each profile image was generated using the StyleGAN-powered platform ThisPersonDoesNotExist , ensuring all images are synthetic and free from privacy concerns. The MLLMU-Bench benchmark includes a total of 500 fictitious profiles and 153 public celebrity profiles, each accompanied by 14 questions\u20147 image+text questions and 7 textual questions. These questions are generated by GPT-4o based on the key attributes provided for each individual, such as residence, employment, and other personal details. The corresponding answers are then derived from the ground-truth information directly extracted from the individual's profile. This structure is mirrored in the Test Set, which includes 3.5K paraphrased questions and 500 transformed images with varied poses, modified using a Stable Diffusion-based model, Arc2Face, to assess the generalizability of unlearning algorithms. Altogether, the benchmark comprises 20k+ questions, evenly divided between image with associated text and pure text formats. The dataset is divided into the Forget Set, Retain Set, and Test Set. The Forget Set is further split into unlearning tasks that target the removal 5%, 10%, and 15% of the profiles, while the Retain Set covers the remaining 95%, 90%, and 85%.\nAdditionally, MLLMU-Bench features 153 real celebrity profiles, selected from CelebA dataset, each verified by human experts for accuracy. Same to the fictitious profile, each celebrity profile includes 14 questions\u2014half multimodal and half pure text\u2014ensuring a thorough evaluation across modalities. A detailed breakdown of the dataset and data quality control can be found in Appendix B.3."}, {"title": "Evaluation Metrics", "content": "MLLMU-Bench is designed to measure three critical aspects of unlearning algorithms in MLLMs: unlearning efficacy, unlearning generalizability, and model utility, following the definitions from . For each of these properties, we assess model performance in classification, generation and cloze tasks under both multimodal and unimodal settings. In particular, the multimodal setting is evaluated using both image and associated text, while the unimodal setting is provided with only text as input. The evaluation metrics are elaborated in detail in Appendix A.\nClassification\nClassification task is designed based on the key attributes of each profile (e.g., birthplace, occupation), generating multiple-choice questions about personal details. In particular, we represent the input to the model as $(image, x, y)$, where $image$ is the visual input in the multimodal setup (absent in the unimodal setup), $x$ is the question, and $y$ is the correct answer. The model predicts $\\hat{y}$ by maximizing the probability $P(y \\mid image, x, M)$, where $M$ is the evaluated model:\n$\\hat{y} = \\arg \\max_{y \\in Y} P(y \\mid image, x, M)$\nIn the unimodal setup, the input simplifies to $(\\O, x, y)$. To evaluate classification performance, accuracy $Acc$ is computed as following:\n$Acc = \\frac{1}{|X|} \\sum_{x \\in X} \\mathbb{I} (\\hat{y}(x) = Y_{correct}(x))$\nwhere $X$ is the set of questions, and $\\mathbb{I}$ indicates correct predictions.\nGeneration\nTo prevent catastrophic forgetting , where the model loses all previously learned information, we also assess its generation ability using a free-generation format. Specifically, the questions are customized to each individual's profile, with GPT-4o generating answers based on key attributes extracted from the profile such as residence and employments. Detailed data curation can"}, {"title": "Cloze Task", "content": "Previous studies have shown that Cloze-style task effectively determine whether models rely on memorized content . Accordingly, we employ a cloze task to evaluate whether sensitive information is retained in the model after unlearning. Specifically, the only information provided in the Cloze-style task is the individual's name, which we assume to be the only publicly available information about the individual. We then prompt the model to complete a designated $[Blank]$ in a sentence, targeting many more details from the person's profile like residence, employment and personal hobbies. We then assess the model's response by exact matching it with the ground-truth information from individual profiles. Unlike generation and classification tasks, the Cloze task is designed to assess the model's unlearning ability with respect to forgotten information when only partial context about the individuals is provided.\nGeneral Benchmarks\nBesides testing the unlearned model on classification, generation and cloze tasks, we also leverage MMMU  and LLaVA-Bench  to assess the model's reasoning ability"}, {"title": "Evaluation Datasets", "content": "To comprehensively assess model performance from various perspectives in the context of unlearning private data, we constructed a set of structured datasets designed to evaluate three critical aspects: unlearning efficacy, unlearning generalizability, and model utility. Our framework incorporates four distinct datasets: the Forget Set, Test Set, Retain Set, and Real Celebrity Set. Specifically, the Forget Set is designed to evaluate a method's unlearning efficacy, the Test Set assesses unlearning generalizability, while the Retain Set and Real Celebrity Set focus on evaluating model utility from different perspectives including retained samples and neighboring concepts. Below, we provide detailed descriptions of each dataset.\nForget Set (Unlearning Efficacy): The Forget Set is designed to evaluate the unlearning efficacy of algorithms. In particular, Forget Set consists of selected profiles from the fine-tuning dataset, comprising either 5%, 10%, or 15% of the total 500 profiles. Each profile in this set is targeted for complete unlearning. Ideally, an effective unlearning algorithm should erase all knowledge of these individuals while preserving its performance on other data. This dataset serves as the foundation for evaluating the model's ability to forget specific knowledge without retaining fragments of it.\nTest Set (Unlearning Generalizability): The Test Set aims to evaluate the unlearning generalizability of the algorithms. Specifically, it is a transformed version of the Forget Set. For images, we use Arc2Face to transform profile images by generating various poses and angles. For text, we paraphrase questions or generate new ones using GPT-40. By altering both modalities, we assess whether the model has truly forgotten the profiles or can still recognize transformed versions, ensuring unlearning extends beyond specific data forms.\nRetain Set (Model Utility): The Retain Set includes the remaining profiles from the full dataset D that are not part of the Forget Set. After unlearning, the model is expected to retain its knowledge of these profiles with high fidelity.\nReal Celebrity (Model Utility): The Real Celebrity Set acts as a control to measure unintended consequences of unlearning. It includes real public figures in both multimodal and text-only formats. By evaluating the model's responses on"}, {"title": "Experimental Results", "content": "In this section, we present a comprehensive comparison of different unlearning algorithms in three unlearning setups against the vanilla model, fine-tuned on the full data D for 3 epochs. Details of the fine-tuning process for the vanilla model can be found in Appendix B.2.\nDatasets and base models\nOur experiment setup focuses on benchmarking the unlearning scenario where the model practitioner is mandated to remove confidential information of each requested individual on both the visual level and textual levels. We consider LLaVA-1.5-7B , and Idefics2-8B  as base MLLM models. For forget set Df, we have randomly selected 5%, 10% and 15% individuals from our curated dataset and the rest of profiles as retain data Dr. The Test Set mirrors the Forget Set split but includes transformed images and text. Lastly, we use Real Celebrity Set to assess the unlearning entanglement with neighboring concepts. For detailed dataset creation, please refer to Appendix B.\nUnlearning Methodologies\nGiven the limited research in the area of MLLM unlearning, we adapt foundational baselines from LLM unlearning and apply them as benchmarks for MLLM unlearning. Specifically, the unlearning approaches include Gradient Ascent (GA) , Gradient Difference , KL Minimization , Negative Preference Optimization (NPO) , and a generic prevention strategies using system prompts to instruct models not to generate privacy-related information. In particular, the GA method applies opposite gradient updates on Df. The GA Difference approach extends this by introducing a balancing mechanism between Df and the Retain Set Dr, ensuring unlearning without performance degradation. The KL Minimization technique aligns the model's predictions on"}, {"title": "MU algorithms with different modalities", "content": "The first question we aim to investigate is: Is it possible to apply unlearning techniques solely to the text modality and expect the model to forget target information across both the image and text modalities? To explore this, we conducted separate experiments using same baselines across different modalities. In the multimodal setup, we provided the unlearning target as a combination of image and associated text, whereas in the unimodal setup, we applied unlearning techniques using only textual information. Here we present with classification, generation and cloze results of GA using LLaVA as base model with 5% forget data, which is shown in Figure 3.\nClassification Task\nshows the GA performance across modalities in classification tasks. The multimodal GA approach demonstrates better unlearning in the multimodal evaluations on both the Forget Set and Test Set but falls short in unimodal"}, {"title": "Generation Task", "content": "Next, we demonstrate the GA performance across different modalities on generation tasks, as shown in Figure 3a, 3b, 3c, 3d Interestingly, unlike the classification results, the unimodal GA approach always shows better unlearning effectiveness than multimodal GA on both multimodal and unimodal setups, as indicated by the larger Rouge-L difference compared to the multimodal GA. However, its generation performance on the Retain and Real Celebrity sets lags behind the multimodal GA. This is likely due to differences in how models handle classification versus generation tasks. As prior works suggest, models excelling in classification often struggle with instruction-following and open-ended generation. In generation tasks, maintaining alignment with instructions and context becomes critical, and unlearning methods can disrupt this balance, especially when focused on a single modality, like text, as seen with unimodal GA."}, {"title": "Cloze Task", "content": "Lastly, we assess GA performance across different modalities on the cloze task, as shown in Figure 3i, 3j, 3k, 3l. The trend aligns with the generation task results, where the unimodal GA approach consistently outperforms the multimodal approach across both multimodal and unimodal setups. Since this task is evaluated based on the exact matches with ground-truth data, it also reflects the model's capacity to maintain alignment with instructions and context. The results further support the conclusion from the generation task, where unimodal unlearning methods risk disrupting the balance between instruction alignment and contextual understanding, reducing performance on complex, multimodal tasks. Detailed results for other baselines can be found in Appendix D.1."}, {"title": "Unlearning v.s. Model Utility", "content": "While many previous works on LLM unlearning have discussed the trade-off between unlearning effectiveness and model utility, this question is rarely explored in the setting of multimodal. Hence, the question we aim to answer in this section is: Does this trade-off between unlearning v.s. utility still persist in the context of MLLM unlearning? To investigate this in detail, we break down \"model utility\" into three branches and analyze the results from three perspectives: retain accuracy, neighboring concepts (celebrity set), and model general ability including reasoning ability and helpfulness level.\nFirst, we present the trade-off analysis between unlearning effectiveness and Retain Set accuracy, shown in Figure 4a. GA demonstrates the strongest unlearning ability, showing the largest decrease in forget accuracy compared to the vanilla model. However, this exceptional unlearning performance comes at the cost of a significant decline in retain set accuracy, likely due to the unintended removal of some retained knowledge during unlearning. In terms of preserving the model utility from the perspective of Retain Set accuracy, NPO and prompting method perform best, achieving the highest retain accuracy. We observe a similar trend on other perspectives of model utility such as neighboring concepts (i.e. Figure 4b), model reasoning ability (i.e. Figure 4c), and model helpfulness ability (i.e. Figure 4d). For example, on the Real Celebrity Set, we observe that as unlearning effectiveness improves, performance on neighboring concepts declines, as seen with the GA and GA Difference approaches. Lastly, we find that model reasoning ability and helpfulness are also closely tied to unlearning effectiveness as evidenced by the downward trends in Figure 4d. This highlights that as unlearning performance improves, it can negatively impact the model's reasoning ability and helpfulness. The rest of the experiments are detailed in Appendix D.2."}, {"title": "Future Directions", "content": "Unlearning is a broad topic with general applications and numerous potential directions for future exploration. Here we discuss observations and promising future directions derived from our work.\nWhy not just Unimodal Unlearning?\nIn section 5, we found that the unimodal approach can outperform the multimodal approach in both multimodal (i.e., image with associated text as input) and unimodal (i.e., text-only input) setups on tasks other than classification. Hence, a natural question arises: Why not exclusively use unimodal unlearning approaches, given their superior unlearning performance compared to multimodal methods?\nTo answer this, we note that although the unimodal approach demonstrates better unlearning effectiveness, it shows poorer utility performance on the Retain Set and Real Celebrity Set. In the discussion section, even with careful hyperparameter tuning, unimodal GA exhibits a faster rate of collapse compared to multimodal GA, making it challenging to balance unlearning effectiveness and model utility. This tendency is also observed in other more balanced approaches like NPO and KL Minimization, as shown in Appendix D. This phenomenon is expected because the textual modality plays a central role in decision-making within multimodal language models , meaning that unlearning has greater impacts on retained knowledge and the model's general abilities, such as reasoning and instruction following. Unlearning in textual modality alone may not comprehensively remove the targeted knowledge and could inadvertently impair performance on tasks requiring multimodal comprehension. Hence, achieving selective unlearning within MLLMs is more challenging with unimodal approaches alone, as they can disrupt the balance between unlearning effectiveness and utility across modalities. This highlights the necessity and importance of developing more crafted multimodal unlearning approaches to achieves a better balance performance with respects to both unlearning objectives and utility across all modalities.\nPotential MLLMU-Bench Improvements\nMLLMU-Bench uses the Test Set to assess the robustness of the unlearned model with transformed profile images and paraphrased questions. Various attack techniques could be employed to further test the robustness of unlearning methods for MLLMs. For example, evaluated the robustness of LLMs by performing a training data extraction attack to recover trained examples, while focused on jailbreaking MLLMs to generate objectionable responses to harmful user queries. Consequently, similar at"}, {"title": "Conclusion", "content": "The introduction of the MLLMU-Bench benchmark represents a significant step toward implementing unlearning algorithms that simulate real-world scenarios. By assessing unlearning algorithms from three key perspectives, MLLMU-Bench provides a comprehensive framework for evaluating their performances. Additionally, we conduct heuristic experiments to explore the performance of unlearning algorithms in both multimodal and unimodal setups, highlighting the need for more advanced multimodal unlearning approaches in future research. Lastly, we present a systematic analysis of the trade-offs between unlearning effectiveness and model utility, offering valuable insights from multiple perspectives.\nLimitations\nMLLMU-Bench has several limitations. First, while we identified a performance gap between unimodal and multimodal approaches, we have only empirically shown this phenomenon without uncovering its root cause. Further analysis and exploration are needed to explain this gap. Second, to better simulate real-world scenarios, it would be important to generate group images where the forget target is present. This would allow a more precise evaluation of knowledge disentanglement between unlearned and retained information. Third, our benchmark targets the removal of all information related to an individual, such as name, age, and residence, assuming that a person's name is public information from which other details can be inferred. In the future, it would be beneficial to selectively unlearn specific key attributes (e.g., residence) while preserving other details. Lastly, as noted in recent work, unlearned models may relearn forgotten data through in-context learning (ICL). Therefore, it is an interesting direction to investigate methods to prevent unlearned models from reacquiring this data, which we leave for future work."}, {"title": "Appendix: Evaluation Metrics", "content": "Unlearning Efficacy\nUnlearning efficacy refers to the model's ability to completely erase specific knowledge about the targeted data, ensuring that it behaves as if the data had never been part of the training process. To evaluate this, we focus on the Forget Set, where the model is expected to unlearn all information associated with selected profiles. The challenge here lies in ensuring that the model not only forgets the factual content of these profiles but also any latent representations or implicit associations formed during training.\nIn our framework, unlearning efficacy is measured by the model's performance in both multimodal (image+text) and text-only settings. Specifically, the model is evaluated on a set of multiple-choice questions, where it must avoid selecting the correct answer associated with a forgotten profile. Formally, given a question x and a set of possible answers Y, the model should minimize the probability of selecting the correct answer $y^* \\in Y$ from the Forget Set:\n$\\hat{y} = \\arg \\max_{y \\in Y} P(y|x, M_u) \\quad \\text{where } y \\neq y^*,$\nwhere $M_u$ represents the model after unlearning. An ideal model will treat the forgotten profiles as unknown, exhibiting behavior indistinguishable from random guessing.\nAdditionally, we employ generation and cloze tasks to further assess unlearning efficacy. In generation task, the model generates descriptions or answers related to forgotten profiles. If the generated output contains factual inconsistencies or a lack of information about the forgotten profile, the unlearning process is considered effective . This ensures that the model has thoroughly forgotten both explicit knowledge and nuanced associations. Additionally, in cloze tasks, the model is provided with the person's name and part of the context, such as a portion of the residence country, and is asked to fill in the blank with the target answer based on the given information.\nUnlearning Generalizability\nUnlearning generalizability refers to the model's ability to extend its unlearning to altered representations of the forgotten data, ensuring that knowledge removal is not limited to the original form of the data but generalizes across different variations . This is particularly important as models often form robust associations that allow them to recognize paraphrased or transformed versions of the original content .\nTo assess this, we evaluate the model's performance on the Test Set, which consists of transformations of the samples in the Forget Set. These transformations include modifications to both the image and text modalities. For image transformations, we use a stable-diffusion based model named Arc2Face to modify the pose of individuals. For the textual modality, we either paraphrase the original question from the Forget Set or use GPT-4o to generate new questions based on the target person's profile that were not present in the Forget Set. The model's ability to unlearn across such variations demonstrates a more comprehensive and thorough forgetting process .\nFormally, for each transformed input $z' = (image', x', y')$, where $x'$ is a paraphrased version of the original question and image' is a modified version of the original image, the model should minimize the probability of retrieving the correct answer y*:\n$\\hat{y'} = \\arg \\max_{y \\in Y} P(y | image', x', M_u) \\quad \\text{where } y \\neq y^*.$\nThis ensures that the unlearning process is robust and that the model does not retain latent traces of the forgotten knowledge in modified forms. Additionally, by evaluating both multimodal (image+text) and text-only setups, we closely align our approach with real-life scenarios, where data may appear in different formats and contexts, requiring the model to effectively forget across all representations.\nModel Utility\nModel utility refers to the model's ability to retain valuable knowledge and maintain strong performance on data that is not targeted for unlearning, ensuring that the unlearning process does not degrade overall capabilities. We assess model utility across several dimensions using the Retain Set, Real Celebrity Set, and additional reasoning benchmarks. The Retain Set consists of the remaining profiles from the fine-tuning dataset, excluding those in the Forget Set, and is designed to evaluate the model's performance on unrelated samples. The Real Celebrity Set, in contrast, examines the"}, {"title": "Implementation Details", "content": "The Gradient Ascent approach  is a straightforward method to enforce unlearning. The goal is to increase the loss for samples in the forget set, Df, thereby reducing the likelihood that the model retains specific information about these profiles. For each sample $x \\in D_f$, we aim to maximize the loss, encouraging the model to deviate from its initial predictions. The overall objective is to maximize the average loss over the forget set:\n$L(D_f, w) = \\frac{1}{|D_f|} \\sum_{x \\in D_f} l(x, w),$\nwhere $l(x, w)$ represents the loss for sample x given the model parameters w. By doing so, the model is encouraged to unlearn the specific associations formed during fine-tuning with respect to the forget set.\nGradient Difference\nGradient Difference  builds upon Gradient Ascent by balancing the unlearning of the forget set with the preservation of performance on the retain set, Dr. The objective is to increase the loss on Df while minimizing the impact on"}]}