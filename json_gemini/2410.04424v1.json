{"title": "DADEE: Unsupervised Domain Adaptation in Early Exit PLMs", "authors": ["Divya Jyoti Bajpai", "Manjesh Kumar Hanawal"], "abstract": "Pre-trained Language Models (PLMs) exhibit good accuracy and generalization ability across various tasks using self-supervision, but their large size results in high inference latency. Early Exit (EE) strategies handle the issue by allowing the samples to exit from classifiers attached to the intermediary layers, but they do not generalize well, as exit classifiers can be sensitive to domain changes. To address this, we propose Unsupervised Domain Adaptation in EE framework (DADEE) that employs multi-level adaptation using knowledge distillation. DADEE utilizes GAN-based adversarial adaptation at each layer to achieve domain-invariant representations, reducing the domain gap between the source and target domain across all layers. The attached exits not only speed up inference but also enhance domain adaptation by reducing catastrophic forgetting and mode collapse, making it more suitable for real-world scenarios. Experiments on tasks such as sentiment analysis, entailment classification, and natural language inference demonstrate that DADEE consistently outperforms not only early exit methods but also various domain adaptation methods under domain shift scenarios. The anonymized source code is available at https://github.com/Div290/DADEE.", "sections": [{"title": "1 Introduction", "content": "Pre-trained Language Models (PLMs) such as BERT (Devlin et al., 2018), GPT (Radford et al., 2019), XLNet (Yang et al., 2019), and RoBERTa (Liu et al., 2019) have experienced substantial growth in size to attain state-of-the-art performances across a wide range of natural language processing tasks. Despite their remarkable efficacy, PLMs suffer from inference latencies, limiting their utility in industrial applications requiring faster inference. Prior research (Zhou et al., 2020; Zhu, 2021) has also highlighted overthinking issues in PLMs. More specifically, shallow representations in the initial layers may suffice for correct inference of \u2018easy' samples, while final layer representations may overfit or be influenced by irrelevant features, resulting in poor generalization and computational inefficiency.\nTo circumvent this, several methods such as pruning (Michel et al., 2019), quantization (Kim et al., 2021) and knowledge distillation (Jiao et al., 2019) have been proposed. Among them, Early Exit methods (Zhou et al., 2020; Zhu, 2021) have gained significant attention, where inference can be made at classifiers attached at intermediary layer based on the hardness of the input samples. They perform adaptive inference strategies to address both efficiency and overthinking concerns.\nNevertheless, all these methods do not generalize well to new domains. Also, as EE introduces more parameters in the exit classifers, it requires high-quality labeled training data to learn the exit weights. The cost of creating labeled training data is often prohibitively expensive. The question then arises: How can we adapt early exit PLMs to diverse domains in an unsupervised setup?\nDomain adaptation is a vital technique in machine learning to ensure models perform well on data from a target domain, even if trained on a different source domain. Unsupervised domain adaptation methods primarily focus on aligning source and target data in a shared feature space. This alignment is achieved by optimizing the model's representation to minimize domain discrepancies, such as maximum mean discrepancy (Tzeng et al., 2014) or correlation instances (Sun and Saenko, 2016). Previous approaches, like adversarial training methods (Ajakan et al., 2014; Tzeng et al., 2017; Ryu et al., 2022), have utilized adversarial objectives to bridge domain gaps by producing domain-invariant features at the final layer. However, since the Early Exit PLMs (EEPLMs) have added exits, domain adaptation methods cannot be directly used as they only focus on adapting to the final layer performance. This necessitates domain adaptation at every layer such that domain invariant features are not only available at the final layer but across all the layers of PLM.\nTo tackle this problem, we present a novel strategy that integrates adversarial domain adaptation techniques in early exits that adapt EEPLMs to diverse domains. Our approach, named Unsupervised Domain Adaptation in EE PLMs (DADEE), emphasizes achieving domain invariant representations across all the layers such that exit classifiers trained on a source domain can be directly utilized for the target domain. This allows for the exit classifiers trained on the source domain to be directly utilized in the target domain without requiring labels.\nOur method not only speeds up inference but also allows for fast and robust bridging of domain gaps compared to traditional methods that rely solely on final layer representations in adversarial setups, which is insufficient for such large and complex models. During adversarial training, the exits aid the adaptive process by mitigating the risk of catastrophic forgetting and mode collapse using knowledge distillation between source and target representations across all the layers.\nBy leveraging multi-level domain feature adaptation, DADEE enhances overall effectiveness across real-world applications, addressing the challenges of inference speed and domain adaptation in PLMs. Also, in scenarios where the size of the source dataset is limited, early exit models can adapt more readily as these models do not depend on the prediction of just the final layer but multiple layers, improving generalization. Our method achieves improved performance metrics on sentiment analysis, entailment classification, and natural language inference (NLI) tasks.\nOur main contributions are as follows:\n\u2022 We propose DADEE for unsupervised domain adaptation to bridge the gaps between the source and target domains in EEPLMs.\n\u2022 DADEE uses a GAN-based multi-level adaptation to bridge the domain gaps, i.e., we perform layer-by-layer adaptation.\n\u2022 We utilize EE strategies not only for faster inference but also for the adaptation process. Our method gets the best of both early exit and domain adaptation methods to simultaneously increase both performance and speed.\n\u2022 Through extensive experiments on sentiment analysis, entailment classification and natural language inference (NLI) tasks, we show that DADEE achieved an average improvement of 2.9% in accuracy and 1.61\u00d7 average inference speed up as compared to previous vanilla PLM inference."}, {"title": "2 Related works", "content": "In this section, we discuss studies relevant to our work in domain adaptation and early exiting.\nDomain Adaptation: The aim is to learn domain-invariant representations for labeled source and unlabeled target domains. Key methods include Deep Domain Classification (Tzeng et al., 2014), which minimizes maximum mean discrepancy with classification loss, and Deep Adaptation Network (Long et al., 2015), employing multiple kernels across layers. DCA (Sun and Saenko, 2016) reduces the disparity in second-order statistics. Adversarial methods like DANN (Ajakan et al., 2014) use a domain classifier with a gradient reversal layer for domain confusion. Similarly Domain Separation Networks (DSN) (Bousmalis et al., 2016) have a notion of private subspaces for every domain and separate the information for each domain.\nGenerative approaches, such as CyCADA (Hoffman et al., 2018), enforce cycle and semantic consistency. DAAT (Du et al., 2020) enhances domain awareness through post-training, while ADDA (Tzeng et al., 2017) introduces GAN-based loss, further improved by AAD (Ryu et al., 2022) with knowledge distillation. Pivot-based methods (Blitzer et al., 2007; Yu and Jiang, 2016; Ziser and Reichart, 2018; Peng et al., 2018; Zhang et al., 2019) induce shared low-dimensional features based on pivot co-occurrence. Multi-level domain adaptation methods (Malik et al., 2023) utilize all layers to bridge domain gaps.\nEarly Exits: are input adaptive inference methods. For image classification, BranchyNet (Teerapittayanon et al., 2016) uses classification entropy for early inference, while MSDNet (Huang et al., 2017) selects thresholds based on confidence distribution. DeeCAP (Fei et al., 2022) and MuE (Tang et al., 2023) extend it to image captioning. Early exiting has also been applied to PLMs for various NLP tasks (Bapna et al., 2020; Elbayad"}, {"title": "3 Methodology", "content": "In this section, we detail our method. We start with a PLM such as BERT/ROBERTa with L layers. We attach exit classifiers to each layer and train them using the source dataset. We then perform multi-level adversarial training on target data."}, {"title": "3.1 Training the source backbone", "content": "For any source sample $(x_s,y_s)$, the loss at exit classifier i is computed as:\n\n$L_i(\\theta) = L_{CE}(f_i(x_s,\\theta),y_s), \\tag{1}$\n\nwhere $f_i(x_s, \\theta)$ is the output of the classifier attached at the ith layer, $\\theta$ is the set of learnable parameters of the source backbone, and $L_{CE}$ is the cross-entropy loss. We learn the parameters for all the exit-classifiers simultaneously following the approach of Shallow-Deep (Kaya et al., 2019), with the loss function defined as $L = \\frac{\\sum_{i=1}^{L} i \\cdot L_i}{\\sum_{i=1}^{L} i}$ over all the L layers. This weighted average considers the relative inference cost of each internal classifier. This is shown as the step Supervised learning in Fig 1. After this training, the weights of the source encoder and exit classifiers at each layer are frozen. We note that as a part of the training, a function $C_i$ that maps the source encoder's output to class probabilities at ith layer is also learned and frozen."}, {"title": "3.2 Adversarial adaptation on target", "content": "We initialize the target encoder weights with what is learned for the source encoder. We assume that the label space of the target dataset is the same as the source label space C. We denote outputs of the ith layer of the source and target encoder as $E(x)$ and $E(x')$, respectively. Let $D_i$ denote a discriminator function that maps the output of ith layer of either source encoder or target encoder to domain probabilities, i.e., target or source domain. We train the target encoder and the discriminator alternately as the ADDA framework (Tzeng et al., 2017). This can be formulated as an unconstrained optimization problem and is represented as step 2(a): Adversarial Framework in Fig. 1. Let $D_s$ and $D_t$ denote the target distributions. For $x_s \\sim D_s$ and $x_t \\sim D_t$, loss for ith discriminator can be formulated as\n$L^{dis}_i(x_s, x_t) = -logD_i(E(x_s)) \\\\ - log(1 - D_i(E(x_t))), \\tag{2}$\nand the overall discriminator loss across all the layers can be as $E^{dis} = \\frac{\\sum_{i=1}^{L} i \\cdot L^{dis}_i}{\\sum_{i=1}^{L} i} = L^{dis}$. The generator loss for ith layer is:\n\n$L^{gen}_i(x_t) = -logD_i(E(x_t)) \\tag{3}$\n\nGiven that the weights of the target encoder are untied from those of the source encoder, the target encoder has more flexibility in learning the specific domain features. However, this formulation is prone to catastrophic forgetting (Ryu et al., 2022), leading to erratic classification performance, as it lacks access to class labels and can diverge from the original task.\nTo address this challenge, we employ knowledge distillation. This involves introducing distillation loss alongside generator loss after each layer to mitigate the risk of catastrophic forgetting and mode collapse across all layers. We ensure robustness throughout the model by leveraging classifiers (exits) appended after each layer to distil the knowledge between source and target. The formulation for knowledge distillation loss is expressed as follows:\n\n$L^{KD}_i = KL(C_i(E(x_s)), C_i(E(x_s))) \\tag{4}$\n\nwhere KL is the KL-divergence loss. The total loss of generator of the ith classifier then becomes $L_i = L^{gen}_i + L^{KD}_i$, and the overall generator loss is taken as $L = \\frac{\\sum_{i=1}^{L} i \\cdot L^{gen}_i}{\\sum_{i=1}^{L} i} = L$. We provide lower weights to the discriminator and generator at initial layers since these layers are responsible for learning the general features which should not be changed much while deeper domain-specific rich representations lie in deeper layers justifying higher weights to deeper layers.\nAfter this step, the target backbone is ready for inference as discussed next."}, {"title": "3.3 Inference on target dataset", "content": "For any $x_t \\sim D_t$, let $p_i(c)$ denote the probability assigned at layer i that a sample belongs to class $c \\in C$. Let $S_i := max_{c \\in c}p_i(c)$. It denotes the confidence in prediction at the i layer. The decision to exit early is made based on this confidence score exceeding a fixed threshold a, i.e., a sample exit from layer i if $S_i > a$, else it is processed to the next layer. When the sample exits at layer i, it is assigned a label $argmax_{c \\in c} p_i(c)$. If the sample's confidence is below a for all the intermediary layers, the sample is inferred at the final layer.\nWe set the value of a using the validation split of the source dataset. We use the same threshold for the target dataset as after adversarial training as all the layers provide domain invariant feature representation resulting in a similar accuracy-efficiency trade-off in the target domain as learned on the source domain."}, {"title": "3.4 Analysis", "content": "This section provides a theoretical justification of DADEE. Initially, we delve into the existing theoretical framework, subsequently elucidating the advantageous aspects of early exits and adversarial training from a theoretical standpoint. We follow the method pioneered in (Ben-David et al., 2010) to upper bound expected error of a hypothesis on the target domain.\nLet $h$ and $h'$ denote the hypotheses that assign ground-truth labels for the source and target domain, respectively. We define the disagreement function for any hypothesis $h_1$ and $h_2$ as :\n\n$\\epsilon(h_1, h_2) = E[|h_1(x) - h_2(x)|]. \\tag{5}$\n\nFor any hypothesis $h$, define $\\epsilon_s(h) = \\epsilon_s(h,h^*)$ and $\\epsilon_t(h) = \\epsilon_t(h, h^*)$ as the expected error on the source and target domain respectively. The error $\\epsilon_t (h)$ of a hypothesis h on the target domain can be bounded using three terms: (a) expected error of h on the source domain, $\\epsilon_s(h)$; (b) $H\\Delta H$-distance $d_{H\\Delta H}(D_s, D_t)$ measuring domain shift as the discrepancy between the disagreement of the two hypotheses h, h' $\\in H$ which is defined as\n$d_{H\\Delta H}(D_s, D_t) = 2 \\sup_{h, h' \\in H} |\\epsilon_s(h,h') - \\epsilon_t(h,h')| \\tag{6}$\nand (c) the error $\\lambda$ of the ideal joint hypothesis $h^*$ on both source and target domains. The upper bound is:\n$\\epsilon_t(h) \\leq \\epsilon_s(h) + \\frac{1}{2} d_{H\\Delta H} + \\lambda \\tag{7}$\nUsually, $\\lambda$ is considered negligible and discarded. Therefore, we can focus on making the first and second terms small to keep the target error small.\nFor the first term, the error rate of the source domain is minimized by training it on the labeled training data. For the second term, it is required that the PLM generate similar features for the source as well as the target dataset. By imposing multi-level adaptation, we reduce the value of the second term in the upper bound. In our method, all the exits simultaneously bridge the domain gap across all the layers instead of just the final one. This helps in layer-by-layer adaptation to the target domain, producing similar feature representations for the target and source datasets across all the layers instead of just the final layer. This helps significantly reduce the second term. We demonstrate it in figure 2a by calculating the A-distance. Note that if knowledge distillation is not applied at every layer, it could increase the value of the first term because of mode collapse or catastrophic forgetting. This aspect was not considered in previous methods like (Ryu et al., 2022), where knowledge distillation is applied only at the last layer."}, {"title": "4 Experiments", "content": "In this section, we elaborate on all the experimental details of our work."}, {"title": "4.1 Datasets", "content": "We conduct experiments on well-established benchmark datasets sourced from Amazon reviews (Blitzer et al., 2007). These datasets cover reviews from four distinct domains: Books (B), DVDs (D), Electronics (E), and Kitchen appliances (K). Additionally, we utilize the Airline review dataset (A) (Nguyen, 2015) and the IMDB dataset (I) (Maas et al., 2011). In total, our study encompasses 30 domain adaptation tasks for sentiment analysis. For NLI and entailment classification, we use the datasets available in GLUE (Wang et al., 2019) and ELUE (Liu et al., 2021) tasks.\nFor each domain, we use the train split of the source (with labels) and the target domain (without labels) to train and adapt the backbone. The validation split of the source dataset is used for development and then finally the model is tested on the target test set."}, {"title": "4.2 Implementation details", "content": "Training: Initially, we train the backbone on the source dataset. We add a linear classifier layer after each intermediate layer of the pre-trained BERT/ROBERTa model, running the model for three epochs. The training uses a batch size of 16 and a learning rate of le-5 with the ADAM optimizer (Kingma and Ba, 2014). We apply early stopping and select the best-performing model on the development set. Subsequently, we freeze the source encoder and initialize the target encoder with the source encoder's weights. The discriminator is an MLP with two hidden layers (hidden size 3072) and LeakyReLU activation. The domain adaptation step runs for five epochs with the same hyperparameters. The experiments are conducted on a single NVIDIA RTX 2070 GPU with an average runtime of less than 10 minutes."}, {"title": "4.3 Inference", "content": "During inference, we use a batch size of 1 and $\\alpha$ is chosen as the best-performing threshold on the source dataset's validation split based on accuracy. The search space for $\\alpha$ is $S_{\\alpha} = {0.8, 0.85, 0.9, 0.95,1.0}$. We apply the same threshold as learned on the source dataset since all layers produce domain-invariant features, allowing the threshold to work effectively for the target domain as well.\nSpeedup metric: To maintain consistency with previous methods, we use the speedup ratio as the metric to assess our model which could be written as: $\\frac{\\sum_{i=1}^{L}i \\times n_i}{\\sum_{i=1}^{L} L \\times n_i}$ where $n_i$ are the number of samples exiting from the ith layer and L represents the number of layers. This metric could be interpreted as the increase in speed of the model as compared to the naive BERT/ROBERTa models."}, {"title": "4.4 Baselines", "content": "We compare our method against state-of-the-art early exiting and domain adaptation techniques, categorizing the baselines into four groups:\n1) Backbone: We use vanilla BERT/ROBERTa as the primary baseline, in this we fine-tune the model on source domain data and infer on complete target data without adaptation.\n2) Domain Adaptation: This category includes methods focused solely on domain adaptation. DANN (Ajakan et al., 2014) employs adversarial training on deep neural networks, adapting the representation encoded in a 5000-dimensional feature vector of frequent unigrams and bigrams. IATN (Zhang et al., 2019) features two attention networks: one identifies common features between domains through domain classification, and the other extracts information from aspects using these common features. DAAT (Du et al., 2020) initializes with BERT's pre-trained weights and adapts it using novel self-supervised pre-training tasks followed by adversarial training. This method is specific to BERT and not easily extendable to other PLMs. AAD (Ryu et al., 2022) uses adversarial training with distillation from the source to reduce overfitting. UDApter (Malik et al., 2023) performs unsupervised multi-level domain adaptation.\n3) Early Exit Methods: PABEE (Zhou et al., 2020) is a state-of-the-art early exit method that exits based on prediction consistency. This is chosen as a baseline to give an interpretation of how vanilla early exiting performs without adaptation.\n4) Domain Adaptation with Early Exit: DDA (Li et al., 2021) performs multi-level adaptation for image datasets to enable faster inference, though it can suffer from catastrophic forgetting. CeeBERT (Bajpai and Hanawal, 2024) uses multi-armed bandits to adapt thresholds, focusing on inference efficiency without directly addressing the domain gap. In the result tables, \u2018Final' represents the performance of our method when inference is conducted solely at the final layer after adaptation, without utilizing early exits."}, {"title": "4.5 Experimental Results", "content": "In Tables 1 and 2, we present comprehensive results using the BERT/ROBERTa backbone across various domain pairs for sentiment analysis, while Table 3 showcases results for NLI and entailment classification tasks on BERT. Each experiment is performed five times with different seeds, and the average results are reported. Our method consistently outperforms existing baselines.\nWhile PABEE, an early exiting method without adaptation, shows comparable performance to BERT, both lack domain adaptation techniques. Previous methods like DANN and IATN rely on word2vec or GloVe embeddings, which fail to capture the nuanced word characteristics that BERT's contextual embeddings can. By leveraging BERT embeddings, we enhance these methods to achieve comparable performance to more advanced baselines. AAD, which uses BERT but focuses only on the final layer for adaptation, fails to enforce domain-invariant representations across all layers, resulting in suboptimal performance. DAAT improves on prior baselines by incorporating an additional post-training step that makes it domain-aware before performing adversarial training. However, DANN's method is specific to BERT and not easily extensible.\nThe performance of DDA suffers from overfitting to the source domain data and does not achieve better speedup or performance compared to our approach due to poor generalization as explained in (Ryu et al., 2022). Additionally, DDA only has a feature classifier to reduce the domain gap which might not be sufficient to effectively minimize the domain discrepancy.\nOur method surpasses previous baselines by adapting to the target domain across all network layers, creating domain-invariant hidden representations throughout rather than relying solely on the final layer. DADEE effectively reduces the issues of catastrophic forgetting using knowledge distillation which further makes adaptation robust. As shown in the last two columns of Table 1, we observe performance improvements even with early predictions, mitigating the issue of overthinking. Moreover, we achieve an average speedup of 1.61x, with the highest at 2.11x and the lowest at 1.00x, demonstrating our method's efficiency in both domain adaptation and faster inference, making the model both quick and robust."}, {"title": "4.6 A-distance", "content": "From Equation 6, we note the significance of domain divergence, a pivotal metric in assessing the efficacy of domain adaptation methods. To quantify this, we calculate the A-distance, commonly utilized for measuring domain dissimilarity. Defined as $d_A = 2(1 - 2\\epsilon)$, where $\\epsilon$ denotes the generalization error of a classifier trained to discern source from target domain samples. Following existing methods, we divide the data into equal subsets (both source and target), train a linear SVM on one subset to classify domain origin, and evaluate error rates on the other subset, deriving the A-distance accordingly.\nComparing the A-distance across BERT, DAAT, DDA, and our method as these are the top best-performing methods, BERT consistently exhibits the highest A-distance across dataset pairs. DDA and DAAT demonstrate lower A-distance, attributed to their application of adversarial training, which mitigates domain dissimilarity. Our method achieves the lowest A-distance, owing to adversarial training and reduction of catastrophic forgetting across all layers."}, {"title": "4.7 Feature visualization", "content": "For an intuitive understanding of the effects of multi-level domain adaptation on BERT, we further perform visualization of feature representations of our method and the DDA method. We perform the feature representation visualization on the test split of the source and target domain for the D \u2192 E task. In figure 2b, 2c, we apply t-SNE (t-distributed Stochastic Neighbor Embedding) on the set of all representations of source and target data points. Every sample is mapped into a 768-dimensional feature space by BERT and projected back to the two-dimensional plane by the t-SNE.\nFrom figure 2b, we can observe that there is a lack of distinction between source positive and source negative samples in DDA and many points overlap when in target positive and target negatives. Still, it manages to distinguish between the larger portion of data from the target domain due to adaptive inference. For our method, the target domain positives and negatives are well-separated with few overlaps showing that the multi-level domain adaptation in an adversarial setup can benefit in bridging the domain gap more efficiently. Also, in our method, there is a better overlap between source positives, target positives, and source negatives, as well as target negatives, which further shows that our method can give better domain invariant features."}, {"title": "4.8 Ablation study", "content": "In this section, we show the robustness of our method to source dataset size and its stability. The effect of changing the hyperparameter a is given in the Appendix A.2.\nIn Table 4, we demonstrate the scalability of our method by varying the size of the source dataset used for training. We use different fractions of samples as training data for the source backbone. Our findings highlight a key strength of our approach: robustness to variations in the size of the source dataset. As shown in Table 4, reducing the size of the labeled set significantly impacts other models' performance, whereas our method experiences only a slight performance decline. This robustness is attributed to the multi-level domain adaptation integrated into our framework through early exits, effectively utilizing limited labeled data and incorporating valuable representations from the training dataset. The distillation loss applied at every layer also enhances robustness, as previous methods often only attach knowledge distillation to the final layer, potentially leading to catastrophic forgetting or mode collapse. Our approach addresses this by mitigating noise accumulation across all layers.\nMoreover, early exit models reduce the chances of overthinking which further helps in better results when the labeled dataset size varies as reflected in our results of table 4.\nWe also assess the stability of our method by calculating the standard deviation across five random runs with different seeds. Early exiting models have shown good generalization capabilities in previous works (Zhu, 2021; Zhou et al., 2020), further verifying the scalability and stability of our method. The stability performance of other methods is provided in Table 5 in the Appendix."}, {"title": "5 Conclusion", "content": "We present a new method DADEE for multi-level domain adaptation in PLMs using the early exit approach and adversarial training. The exits attached lower the chances of catastrophic forgetting by incorporating knowledge distillation across all layers. Also, the attached exits decrease the inference time by inferring the easier samples early. t-SNE plots and A-distance demonstrate the effectiveness of our method in domain adaptation. Extensive experiments demonstrate that our method not only bridges the domain gap in a cross-domain setup but also provides faster inference making it suitable for real-world applications."}, {"title": "6 Limitations", "content": "Our model uses knowledge distillation at every layer which is a sensitive part of the method if we remove the knowledge distillation loss from a few layers, it might lead to noise accumulation from those layers as each layer is forced to give similar representations as the target domain. If a few layers face the mode collapse, it might lead to degraded performance.\nAlso, after adapting to the target domain, we can adapt to the threshold values based on the confidence values given by the exits at each layer so that the inference at each layer becomes more effective."}, {"title": "A Appendix", "content": "A.1 Stability\nIn table 5, we have shown the stability as well as scalability results of all the state-of-the-art baselines, we can observe that the stability of our method is close but slightly better than the previous methods. The reason for better stability is the better generalization achieved using the early exits."}]}