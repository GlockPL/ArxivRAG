{"title": "Sabi\u00e1-3 Technical Report", "authors": ["Hugo Abonizio", "Thales Sales Almeida", "Thiago Laitz", "Roseval Malaquias Junior", "Giovana Kerche Bon\u00e1s", "Rodrigo Nogueira", "Ramon Pires"], "abstract": "This report presents Sabi\u00e1-3, our new flagship language model trained on a large brazilian-centric corpus. Evaluations across diverse professional and academic benchmarks show a strong performance on Portuguese and Brazil-related tasks. Sabi\u00e1-3 shows large improvements in comparison to our previous best of model, Sabia-2 Medium, especially in reasoning-intensive tasks. Notably, Sabi\u00e1-3's average performance matches frontier LLMs, while it is offered at a three to four times lower cost per token, reinforcing the benefits of domain specialization.", "sections": [{"title": "1 Introduction", "content": "This technical report presents the details of the development and evaluation of the Sabi\u00e1-3 model. We trained Sabi\u00e1-3 on a large corpus of documents written in Portuguese, with a special focus on Brazil-related resources. Through its training, the model was exposed to information relevant to the Brazilian culture, history, and context. The main objective was to have a specialized model that is aware of the linguistic nuances, societal norms, and regional variations unique to the country. Throughout this report, we show that this specialization allows the model to perform better in knowledge-intensive tasks.\nWe applied an approach of continual learning by leveraging a \"generalist\" model that already acquired some level of language understanding and reasoning abilities, and then further train it on our corpus of high-quality data relevant to the Brazilian context. The development consisted of two main phases: (1) the pre-training phase, in which we further train a pre-trained model on specialized data following a self-supervised learning strategy optimizing for the next token prediction objective, and (2) the post-training phase where the model is tuned to follow instructions and align to human preferences.\nCompared to our previous release, Sabi\u00e1-2 [5], we have collected a signifi-cantly larger volume of data for pre-training. In addition to the scale, we also improved the quality of our pre-training data by using a mixture of heuristic and model-based methods to filter out low-quality data."}, {"title": "2 Evaluation", "content": "In this section, we compare Sabi\u00e1-3 with proprietary and open-source LLMs, listed in Table 1, on various benchmarks."}, {"title": "2.1 Multiple-choice Exams", "content": "We have developed a benchmark suite to evaluate the Sabi\u00e1-3 performance on 78 academic exams, including entry-level, undergraduate, and professional cer-tification exams. Our focus is on multiple-choice questions from various Brazil-ian educational assessments, particularly those administered after the middle of 2023 to mitigate the risk of data contamination. However, ENADE is an"}, {"title": "2.2 Conversation Capabilities", "content": "This section presents the benchmark we use to evaluate the Sabi\u00e1-3 capabilities to engage in dialogues.\nIn our previous work [5], we introduced the Brazilian Chat Evaluation (BRACE-val), a benchmark inspired on MT-Bench [19] and designed to evaluate AI assis-tants' performance in following instructions, engaging in multi-turn dialogues, and demonstrating knowledge specific to Brazil. BRACEval consists of 150 multi-turn questions distributed across 13 categories, including culturally rele-vant topics such as writing, roleplay, extraction, humanities, entity, and contra-diction analysis, as well as universal categories like abstention, harmful content, reasoning, math, and coding."}, {"title": "2.3 Instruction-Following Capabilities", "content": "One relevant benchmark to assess LLM abilities to follow instructions is the IFEval [20]. IFEval consists of approximately 500 prompts including instruc-tions that are veriafiable programmatically such as \"write in more than 400 words\". It uses two main metrics: strict and loose. In the strict metric, the model's response is checked against a set of verifiable instructions to determine if it followed all of them. If so, the model receives a point. If any part of the instruction is not followed exactly as specified, the response is deemed incorrect and the model does not receive a point. The loose metric is more lenient as it considers variations and common transformations that might occur in the model's response. For example, if an instruction requires a specific phrase to be included at the end of an email, but the model includes that phrase with addi-tional text formatting like bold or italics, the loose metric might still consider the instruction to be followed, whereas the strict metric would not.\nSince this benchmark's prompts and target answers are in English, we do not expect our Portuguese-specialized model to yield significant improvements in this scenario. Instead, this benchmark primarily serves to assess how well our model can follow instructions in comparison to state-of-the-art LLMs.\nThe results presented in Table 3 show that Sabi\u00e1-3 outperforms Sabi\u00e1-2 Medium in terms of instruction-following capabilities. However, it lags behind other models evaluated in this study, including cheaper options such as GPT-40 Mini and Llama-3.1 8B."}, {"title": "2.4 Long Context Capabilities", "content": "Sabi\u00e1-3 can process up to 32,000 tokens in a sequence. Here we evaluate how well the model uses this capacity through the Needle-in-the-Haystack (NIAH) benchmark [9], which measures the model's ability to find answers within a long, unrelated context. The test involves inserting a small piece of text within a larger context ranging from 1,000 to 32,000 tokens, at various depths from 0% to 100%. Since we are focusing on evaluating the performance of an LLM on"}, {"title": "Portuguese tasks", "content": "Portuguese tasks, we adapt this benchmark using the book Dom Casmurro\u00b9 as the context. In a random spot within the text, we insert as a needle a sentence that says \u201cO n\u00famero m\u00e1gico de Campinas \u00e9 {random_number}.\u201d (\u201cThe magic number of Campinas is {random_number}.", "Qual o n\u00famero m\u00e1gico de Campinas?\" (\\\"What is the magic number of Campinas?\\\") to the model. Using regular expression, we check whether the model correctly outputs the random number as the answer.\nAs depicted in Figure 5, Sabi\u00e1-3 exhibited a perfect recall in our Por-tuguese version of the NIAH benchmark, consistently locating relevant infor-mation within long contexts. We acknowledge, however, that the task, due to its simplicity, may not fully capture the long-context capabilities of language models. Therefore, for a better measure of long-range comprehension skills, we need Portuguese versions of more challenging benchmarks, such as the QuAL-ITY benchmark [14": "which tests a model's ability to answer questions about books."}, {"title": "2.5 Function Calling Capabilities", "content": "The Berkeley Function Calling Leaderboard (BFCL) [17] is a benchmark for evaluating the LLM's capabilities in executing function calls and using tools effectively. The benchmark is updated periodically, and comprises three distinct subsets:\n1. Non-live: An expert-curated collection of question-function-answer pairs, encompassing multiple languages and complex use cases, designed to chal-lenge the models' ability to understand and apply functions accurately. The examples are categorized into Abstract Syntax Tree (AST) analysis, measuring whether the function names, arguments, and parameter types match the expected response; and executable verification (Exec), which executes the function and verifies whether the output is correct.\n2. Live: User-contributed examples that reflect real-world applications, en-suring that the models are tested against scenarios that mirror everyday"}, {"title": "tasks.", "content": "tasks.\n3. Multi turn: Multi-turn and multi-step function calling tasks that simulate agentic behaviors, requiring models to plan execution strategies, request information, and manage a sequence of function invocations to complete a task.\nThe benchmark also includes two categories to measure hallucinations: irrel-evance detection, when none of the provided functions is relevant to the query and should not be invoked; and relevance, when at least one of the functions should be invoked.\nTo evaluate the models we use the original examples in English. Table 4 shows the results of Sabi\u00e1-3 and other proprietary LLMs. The results indicate that Sabi\u00e1-3 ranks competitively alongside top-performing models. It outper-forms in the Non-live categories but requires enhancements in multi-turn and multi-step tasks. For instance, increasing the support for more than 32,000 to-kens could improve in one of the multi-turn categories. It also needs to improve in the irrelevance category."}, {"title": "2.6 Agentic Performance", "content": "Recent advancements in the ability of LLMs on instruction-following, function calling, and reason in long contexts have enabled the development of LLM-based agents. These agents can automatically execute complex tasks, such as developing code [18, 4], managing everyday user tasks on operating systems [11, 12], and orchestrating robots [3].\nTo evaluate an LLM in agentic tasks, it is necessary to integrate it with a framework with modules for memory, action, and reasoning. This setup allows the LLM to interact with responsive environments through tools using natu-ral language. To this end, we use Agent Bench [10], a framework designed to evaluate the agentic capabilities of LLMs across eight distinct environments. It tests the model's performance in multi-turn open-ended generation scenarios involving coding, gaming, and web.\nHere are the eight environments where the agent LLM performs tasks in Agent Bench:\n\u2022 Operating System (OS): Interacts with a real Ubuntu terminal, executing bash scripts to retrieve information about the virtual OS."}, {"title": "\u2022 Database (DB):", "content": "\u2022 Database (DB): Runs SQL queries on a database to answer high-level questions about multiple tables.\n\u2022 Knowledge Graph (KG): Navigates a knowledge graph with 45 million entities and 3 billion facts to extract high-level insights about relationships between entities.\n\u2022 Web Shopping (WS): Navigates an online shopping environment to make purchases from a web page.\n\u2022 Web Browsing (WB): Browses web pages across various domains to extract information or perform high-level actions.\n\u2022 Digital Card Game (DCG): Plays a game against a rule-based agent, an-alyzing cards with different abilities to defeat the opponent.\n\u2022 Lateral Thinking Puzzle (LTP): Solves riddles by asking questions where another agent LLM (GPT-3.5 Turbo) can only respond with \"yes\", \"no\", or \"irrelevant\".\n\u2022 House-Holding (HH): Explores a virtual house, interacting with objects and performing high-level tasks."}, {"title": "3 Conclusion", "content": "We introduced Sabi\u00e1-3, a language model focused on Brazil-related content. It improves upon our earlier Sabi\u00e1-2 models, especially in handling long texts, reasoning, and coding. Despite rivaling state-of-the-art proprietary and open-source models on knowledge-intensive tasks while having a lower per token cost, Sabi\u00e1-3 still has room for improvement in multi-step tasks and following in-structions accurately, both of which we plan to address in future work."}]}