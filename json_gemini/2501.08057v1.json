{"title": "Optimizing Speech Multi-View Feature Fusion through Conditional Computation", "authors": ["Weiqiao Shan", "Yuhao Zhang", "Yuchen Han", "Bei Li", "Xiaofeng Zhao", "Yuang Li", "Min Zhang", "Hao Yang", "Tong Xiao", "Jingbo Zhu"], "abstract": "Recent advancements have highlighted the efficacy of self-supervised learning (SSL) features in various speech-related tasks, providing lightweight and versatile multi-view speech representations. However, our study reveals that while SSL features expedite model convergence, they conflict with traditional spectral features like FBanks in terms of update directions. In response, we propose a novel generalized feature fusion framework grounded in conditional computation, featuring a gradient-sensitive gating network and a multi-stage dropout strategy. This framework mitigates feature conflicts and bolsters model robustness to multi-view input features. By integrating SSL and spectral features, our approach accelerates convergence and maintains performance on par with spectral models across multiple speech translation tasks on the MUSTC dataset.", "sections": [{"title": "I. INTRODUCTION", "content": "Conventional speech processing methods rely on spectral features (e.g., MFCC, FBanks) [1]\u2013[3], while recently, features learned through self-supervised learning (SSL) methods have garnered the attention of researchers [4]\u2013[6]. The SSL-based features (S-feature) alleviate the variability of signals in a fixed representation space using the vector quantization (VQ) method [7]. This type of feature is easy to learn and operate, and offers multi-view representations for speech processing tasks [8], [9]. Furthermore, the S-feature demonstrates a strong capacity for integrating contextual information, making it particularly effective for downstream tasks [7], [10], even beneficial for building speech large language models [11].\nAlthough the S-feature incorporates more in-context information learned from the pre-training stage, it suffers from information loss during the VQ process. This makes it difficult to achieve the strongest performance compared to spectral features in complicated tasks. Inspired by the work that fuses multi-view features to enhance the performance of downstream tasks [12]\u2013[14], we naturally raised the question: can we leverage the strengths of the two views of feature to improve complex speech-to-text tasks?\nThough multi-view feature fusion has been achieved for low-resource cases and emotion recognition in separate studies [15], [16], there remains a lack of discussion on how to accomplish this under more general conditions. Our primary analysis reveals a significant divergence in the update directions between FBanks and S-feature as shown in Fig. 1(b). This gradient conflict explains why simple fusion methods fail to effectively integrate multi-view features (Tab. III). To overcome this limitation, we introduce conditional computation which dynamically activates parts of the model parameters based on the inputs through a gating network [17]. This dynamic activation property allows the model to better adapt to the input characteristics, similar to the widely discussed early exit models [18], [19] and the mixture of experts used in multitask learning [20], [21].\nSpecifically, we propose a gradient-sensitive gating network (GSGN) that implements conditional computation for the gradient of parameters, enhancing the adaptability of our approach by dynamically computing the correlation between heterogeneous features and adjusting the weights of different feature branches to achieve optimal fusion. We carry on the experiences in MuST-C for three languages (En-De, En-Es, and En-Fr) in ST tasks. Our proposed method effectively resolves the conflict between the two features. For models trained from scratch, our method can provide comparable performance and average 1.24 times training speedup compared to the model with the FBanks feature. As opposed to the pre-trained ST model, our approach is also able to provide a corresponding training acceleration effect when the S-feature is sufficiently effective and guarantees the performance of the model."}, {"title": "II. METHOD", "content": "We adopt an end-to-end ST system following previous work [22], [23]. The system consists of an acoustic encoder (A-enc), a textual encoder (T-enc), and a decoder (dec). The forward process of the network can be denoted as follows:\n$h_A = A\\text{-enc}(x)$\n$h_T = T\\text{-enc}(h_A)$\n$h_{out} = \\text{dec}(h_T)$                                                                                                         (1)\nDuring the training phase, the model generates the final prediction based on the decoder's output and obtains the final loss via the subsequent cross-entropy function:\n$\\mathcal{L} = \\text{CrossEntropy}(w_{out}h_{out}, y)$                                                                                                              (2)\nwhere \u0177 is the target text. Conventional ST tasks typically use the FBanks feature as input, denoted as x = Xfbank. In this paper, we introduce a representation with general and contextual information as a multi-view speech representation, the S-feature feature, which is represented as xunit, we incorporate new modules that dynamically fuse the two features through an additional fusion layer as shown in Fig. 2.\nWe first present the specific formulation of our gradient-sensitive gating network. It accepts two inputs with embedding dimension size D, Xfbank, Xunit \u2208 R1\u00d7D. Each input undergoes a linear mapping, resulting in the gating vector g \u2208 R1\u00d7D as follows:\n$g[.] = \\text{Sigmoid}(\\text{Linear}[.](x_{fbank}) + \\text{Linear}[.](x_{unit}) + \\text{bias}[.])$                                                                        (3)\nFor input Xfbank and xunit, we compute the gating vector gfbank and gunit as the weight separately. Finally we fuse the two features by Hadamard product:\n$x_{fusion} = g_{fbank} \\odot x_{fbank} + g_{unit} \\odot x_{unit}$                                                                                                   (4)\nGiven that the network structure in use is a residual network [24], we express the transfer of each layer using the following equation:\n$h_{i+1} = F(h_i) + h_i \\approx w_i h_i + b_i + h_i$\nFor simplicity, we assume a linear model for F(.), omitting nonlinear operations and layer normalization.\nTaking the input x = xfusion as an example, the gradient for the parameter wi in layer i can be expressed as:\n$\\frac{\\partial L}{\\partial w_i} = \\sum_{j=0, j\\neq i}^{T}(\\frac{\\partial L}{\\partial h_{out}} \\times \\frac{\\partial h_{out}}{\\partial w_i}) = \\sum_{j=0, j\\neq i}^{T}( (g_{fbank} \\odot x_{fbank} + g_{unit} \\odot x_{unit}) \\times \\frac{\\partial F(h_i)}{\\partial w_i} \\times \\prod_{k=j+1}^{T} \\frac{\\partial h_k}{\\partial h_{k-1}})$                                                                                                  (6)\nWe can derive the gradient $\\frac{\\partial L}{\\partial w_i}^{fbank}$ when we only use the FBanks feature xfbank to replace the input-dependent term in Eq. (6). Similarly, we use $\\frac{\\partial L}{\\partial w_i}^{unit}$ to denotes the gradient when xunit is the only input. Given that the Hadamard product scales the elements directly, Eq. (6) can be transformed into a weighted summation of two gradients when different features are input separately under frozen parameters.\n$\\frac{\\partial L}{\\partial w_i} = g_{fbank} \\frac{\\partial L}{\\partial w_i}^{fbank} + g_{unit} \\frac{\\partial L}{\\partial w_i}^{unit}$                                                                                   (7)\nAs illustrated in Fig. 1, the two gradients $\\frac{\\partial L}{\\partial w_i}^{fbank}$ and $\\frac{\\partial L}{\\partial w_i}^{unit}$ contain conflicting components when cos(\u03b8) < 0. Such conflicts in gradients are commonly observed in multitask learning and can lead to the seesaw phenomenon [25]. To mitigate this issue, an efficient strategy is to eliminate the components of one gradient b, that conflict with the gradient of the main feature \u00e0 [26].\n$\\text{Deconflict}(\\mathbf{a}, \\mathbf{b}) = \\mathbf{b} - \\frac{\\mathbf{a}^T \\mathbf{b}}{\\|\\mathbf{a}\\|^2} \\mathbf{a}$                                                                                                 (8)\nTherefore the gradient of the model should be corrected to the summation of the two gradients without conflicting components.\n$\\left\\{\n        \\begin{array}{lr}\n         \\mathbf{a} + \\mathbf{b}, & \\text{if } cos(\u03b8) >= 0, \\\\\n         \\mathbf{a} + \\text{Deconflict}(\\mathbf{a}, \\mathbf{b}) & \\text{else}\n        \\end{array}\n       \\right.$                                                                                               (9)\nBased on the Eq. (8), the final gradient of the model is as follows:\n$\\left\\{\n        \\begin{array}{lr}\n         \\mathbf{a} + \\mathbf{b}, & \\text{if } cos(\u03b8) >= 0, \\\\\n         (1-\\frac{\\|\\mathbf{b}\\|}{\\|\\mathbf{a}\\|} cos(\u03b8))^{-1} \\frac{\\|\\mathbf{b}\\|}{\\|\\mathbf{a}\\|} cos(\u03b8) \\mathbf{a} + \\mathbf{b} & \\text{else}\n        \\end{array}\n       \\right.$                                                                                             (10)\nBuilding on the previous description, we observe that when we set \u00e0 = $\\frac{\\partial L}{\\partial w_i}^{fbank}$ and b= $\\frac{\\partial L}{\\partial w_i}^{unit}$, we only need to adjust g[.] in Eq. (7) to satisfy Eq. (10), and in turn eliminate the conflicting components of the gradients produced by the two features. Consequently, we can derive the ideal gradient by constraining gunit = 1 and introducing an additional loss term for gfbank.\n$L_{gate} = \\left\\{\n        \\begin{array}{lr}\n         MSE(g_{fbank}, 1) & \\text{if } cos(\u03b8) >= 0, \\\\\n         MSE(g_{fbank}, (1 - \\frac{\\|\\frac{\\partial L}{\\partial w_i}^{unit}\\|}{\\|\\frac{\\partial L}{\\partial w_i}^{fbank}\\|} cos(\u03b8))) & \\text{else}\n        \\end{array}\n       \\right.$                                                                                               (11)\nSo the final loss of the model is:\n$L_{final} = L + L_{gate}$                                                                                                             (12)\nFor the instability gradient shown in Fig. 1, the cos(\u03b8) between the two gradients from each feature is not constant during training. The variation in gradient directions highlights the inconsistency in the roles of different features during the training process. Initially, the roles of these features may be highly similar. However, these updating directions change significantly during the training process, suggesting that the branches adapt to focus on different aspects of the data at various stages.\nTo enhance the model's robustness to each input feature and ensure that all features can be fully utilized, we dynamically sample from three features during the training stage and input them into the next layer. Specifically, we sample the FBanks, S-feature, and fusion features with two fixed threshold \u03b4fbank, \u03b4unit and a random probability p\u2208 (0,1).\n$x = \\left\\{\n        \\begin{array}{lr}\n         x_{fbank}, & \\text{when } p < \\delta_{fbank} \\\\\n         x_{unit} & \\text{when } \\delta_{fbank} <= p < \\delta_{fbank} + \\delta_{unit} \\\\\n         x_{fusion} & \\text{when } \\delta_{fbank} + \\delta_{unit} <= p\n        \\end{array}\n       \\right.$                                                                                             (13)"}, {"title": "III. EXPERIMENTS", "content": "We mainly experimented on the MuST-C dataset (MuSTC) including three benchmarks [23], [27], English-German (En-De), English-French (En-Fr) and English-Spanish (En-Es), and we report results on the test sets for all models. For the experiment based on the pre-trained model, we obtain the pre-trained ASR model on the LibriSpeech [28] dataset to initialize the acoustic encoder. For pre-training textual encoder and decoder, pre-training data include WMT16, WMT14, and WMT13 for En-De, En-Fr, and En-Es three tasks respectively. The detailed training setup throughout the pre-training phase follows [29]. To obtain the S-feature, we use the pre-trained and fine-tuned ASR Hubert models [7], and setting 500 for k-means cluster center based on the release model\u00b9.\nBased on the Transformer model [30], our model consists of an acoustic encoder with 12 layers, and a textual encoder and decoder with 6 layers. We set the model hidden size to 512 and the feedforward size to 2048, with 8 head attention. We applied dropout with a value of 0.1 to attention weights and 0.2 to residual connections, respectively. We also used label smoothing of 0.1 to handle overfitting. The Adam (\u03b2\u2081 = 0.9, \u03b22 = 0.98) optimizer with a warmup step of 4K was adopted. Additionally, we use the early stopping strategy set the patience = 10.\nFor GSGN, we follow the setting in Eq. 4 to compute a weight matrix g[.]. The matrix g[.] has the same dimension with each input feature, where each g[.] is given by Linear[.]: R1\u00d7D \u2192 R1\u00d7D, and we set D to 512 following the model hidden size.\nFor multi-stage dropout, we empirically divide the training pro-cess into three stages, when the current epoch < 10, we set the \u03b4fbank = 0.3, \u03b4unit = 0 for better performance of gradient descent. During the next stage, when 10 < epoch < 25 we increase the \u03b4fbank = 0.5, \u03b4unit = 0.3 to ensure the model fully leverages the different features. Finally, for 25 < epoch and we revert the thresholds to \u03b4fbank = 0.3, \u03b4unit = 0, allowing the fusion branch to achieve better performance during the inference stage.\nDuring the inference stage, we average the best 10 checkpoints on the valid set for evaluation with beam size = 5 for beam search, and length penalty = 1.0, we provide the last epoch at the average 10 checkpoints to indicate the convergence speed. We do a down-sample for the S-feature for the same length as the FBanks feature, and we use the same setting as the third stage in multi-stage dropout to sample from three branches. We evaluate translation quality with sacreBLEU [31]."}, {"title": "IV. ANALYSIS", "content": "To demonstrate the effectiveness of the S-feature, we incorporated random noise into the model in the EN-DE direction. The random noise sampling from a uniform distribution has the same maximum and minimum values as the S-feature across the entire training set, the results are shown in Tab. II. We employed two noise addition methods 1)Sum: adding noise directly to the current input, and 2)Replace: replacing the S-feature with noise. While the model with random noise achieves better results, it requires more training epochs. This suggests that even adding noise directly to the baseline model with FBanks can enhance performance by improving robustness and mitigating overfitting. When replacing the S-feature with random noise in the S2T-GSGN model, we observed better performance but slower convergence, indicating that the S-feature indeed provides a fast convergence capability for the ST model, rather than behaving like inaccurate noise. Furthermore, in the S2T-GSGN+Drop model, the results suggest that the multi-stage dropout method enhances the model's ability to leverage different features, rather than solely serving as a robust training method.\nWe also investigated the effectiveness of GSGN by comparing a simple concatenation-based gating network on En-De direction based on the model trained from scratch. The new gating concat two features xconcat [Xfbank; Xunit] \u2208 R1\u00d72D and maps them back to original dimensions by Linear(xconcat) : R1\u00d72D \u2192 R1\u00d7D. We find that GSGN makes more efficient use of the S-feature and achieves faster and better convergence as shown in Tab. III. This suggests that the simple concatenation-based gating network fails to resolve the conflicting gradient problem between two features, thereby limiting its ability to fuse them effectively.\nWe conducted the same experiment on the ST model initialized with a pre-trained model, as shown in Tab. IV. The results indicate that the GSGN is more effective on the pre-trained models than on the models trained from scratch. We find that this difference is due to the varying frequencies of gradient conflicts and instability in the trained from scratch and pre-trained models (Fig. 3). In pre-trained models, gradient conflict is particularly common at the beginning of training, and GSGN effectively mitigates this issue, leading to improved results. In contrast, gradient instability is more prominent in models trained from scratch, making the multi-stage dropout method more beneficial.\nAdditionally, we analyzed the weights assigned by the GSGN to the FBanks features in the S2T-GSGN+Drop model. As shown in Fig. 4, for the model trained from scratch, we observed that the GSGN pays more attention to the S-feature in the initial phase of the training to leverage the rapid convergence properties. During training, GSGN gradually fuses more FBanks to achieve a more stable and effective descent process.\nFor the pre-trained model, we observe that GSGN assigns a higher weight to gfbank. This is because, when gradient conflicts occur, the cos(\u03b8) is less than 0, and the term (1 \u2013 $\\frac{\\|\\frac{\\partial L}{\\partial w_i}^{unit}\\|}{\\|\\frac{\\partial L}{\\partial w_i}^{fbank}\\|} cos(\u03b8)) in Eq. 10, or equivalently the mean of gfbank in Eq. 4 would be greater than 1. We find that 89.53% of the elements in gfbank exceed 1 in the pre-trained model. This indicates that the conflict gradient in the pre-trained model is severe, which aligns with our observation in Fig. 3 (right).\nIt is important to emphasize that both instability gradient and conflict gradient co-exist in models trained from scratch and pre-trained models. Although the conflict gradient is particularly common in the pre-trained models, there are still 10.46% of the elements in gfbank that are less than 1. Conversely, in models trained from scratch, about 8.63% of the elements in gfbank exceed 1. This demonstrates that GSGN, in combination with the multi-stage dropout method, effectively addresses the gradient-related issues in both cases. After correctly handling the gradient problem, we find that our method achieves faster convergence speedup compared to the model using only FBanks, and superior performance compared to the model using only the S-feature. Our fusion method successfully leverages the strengths of both features, as shown in Fig. 5."}, {"title": "V. CONCLUSION AND FUTURE WORK", "content": "Recent studies have demonstrated the effectiveness of S-features across various speech-related tasks, offering lightweight and general-purpose multi-view speech representations. However, we observed that while S-features exhibit rapid convergence, they fail to outperform FBanks features due to conflicts in their update directions. To address this, we propose a conditional computation method with a specialized gradient-sensitive gating network, which dynamically computes the correlation between heterogeneous features and adjusts the weights of different feature branches. Our method effectively mitigates conflicts between the two features while exploiting their complementary aspects. As a result, we achieve comparable performance and average 1.24 times training speedup compared to models using only FBanks features, for both models trained from scratch and pre-trained models in the MuST-C En-De, En-Es, and En-Fr speech translation tasks. In the future, we aim to explore more effective fusion approaches by incorporating varied unit representations from more pre-trained models, as well as exploiting the pre-trained models directly."}]}