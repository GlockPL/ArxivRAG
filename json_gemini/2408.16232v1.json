{"title": "Enhancing Conditional Image Generation with Explainable Latent Space Manipulation", "authors": ["Kshitij Pathania"], "abstract": "In the realm of image synthesis, achieving fidelity to a reference image while adhering to conditional prompts remains a significant challenge. This paper proposes a novel approach that integrates a diffusion model with latent space manipulation and gradient-based selective attention mechanisms to address this issue. Leveraging Grad-SAM (Gradient-based Selective Attention Manipulation), we analyze the cross attention maps of the cross attention layers and gradients for the denoised latent vector, deriving importance scores of elements of denoised latent vector related to the subject of interest. Using this information, we create masks at specific timesteps during denoising to preserve subjects while seamlessly integrating the reference image features. This approach ensures the faithful formation of subjects based on conditional prompts, while concurrently refining the background for a more coherent composition. Our experiments on places365 dataset demonstrate promising results, with our proposed model achieving the lowest mean and median Frechet Inception Distance (FID) scores compared to baseline models, indicating superior fidelity preservation. Furthermore, our model exhibits competitive performance in aligning the generated images with provided textual descriptions, as evidenced by high CLIP scores. These results highlight the effectiveness of our approach in both fidelity preservation and textual context preservation, offering a significant advancement in text-to-image synthesis tasks.", "sections": [{"title": "Introduction", "content": "Image synthesis, particularly in the context of adhering to both reference images and conditional prompts, poses a significant challenge in contemporary research. While numerous approaches exist, achieving fidelity to reference images while integrating specified conditional prompts remains elusive. This paper presents a novel methodology aimed at addressing this challenge through the integration of a diffusion model with latent space manipulation and gradient-based selective attention mechanisms.\nIn recent years, diffusion models have emerged as powerful tools for image generation, allowing for the creation of high-quality images by iteratively denoising a latent representation. However, preserving the fidelity of a reference image while adhering to conditional prompts requires further refinement. State-of-the-art models like Stable Diffusion have demonstrated remarkable capabilities in generating images based on a conditional reference image. However, they face challenges when adhering to both the reference image and an additional text condition simultaneously. These models typically operate by controlling the level of noise added to the latent representation, with lesser noise injection required to retain properties from the reference image. This approach often leads to a compromise in faithfully rendering the subjects specified in the text condition, as the reduced number of denoising timesteps limits the model's ability to accurately generate these subjects.\nTo address this limitation, we propose the integration of Absolute Gradient-based Selective Attention Manipulation for images (Abs-Grad-SAM) motivated from Grad-SAM [1], a technique that analyzes cross-attention maps and gradients within the denoised latent space to generate importance scores. By leveraging Abs-Grad-SAM, we derive importance scores for elements of the denoised latent vector related to the subject of interest. Since we are dealing with images and the Grad-SAM [1] technique is originally designed for language models, here, instead of using ReLU, we utilize the absolute values of gradients. This adaptation is crucial as attention can impact pixel values both positively and negatively, requiring a more comprehensive consideration of gradient magnitudes.\nThis enables the creation of masks at specific timesteps during denoising, facilitating the preservation of subjects derived from the text condition while seamlessly integrating background elements from the reference image. Consequently, our approach ensures faithful adherence to both the text prompt and the reference image, overcoming the trade-off between subject integrity and background preservation faced by existing methods.\nIn this paper, we present preliminary findings. Through comprehensive experiments conducted on the Places 365 dataset [2], we demonstrate the superiority of our proposed method over baseline models. Specifically, our method yields improved Frechet Inception Distance (FID) scores and CLIP scores, indicative of higher-quality image synthesis. Furthermore, our approach generates images that are not only of"}, {"title": "Related Works", "content": "Denoising Diffusion Probabilistic Models: DDPMs [6] have emerged as a powerful framework for high-quality image synthesis. These models operate by gradually removing noise from an initial noisy latent representation, guided by a neural network trained to predict the noise distribution. [Ho et al., 2020] [6] introduced DDPMs and demonstrated their effectiveness in generating high-quality images on datasets like CIFAR-10 and LSUN. [Nichol and Dhariwal, 2021] [11] extended DDPMs to enable class-conditional image generation, while [Rombach et al., 2022] [13] proposed Latent Diffusion Models (LDMs) that apply diffusion models in the latent space of pre-trained autoencoders, enabling high-resolution synthesis with reduced computational requirements.\nText-to-Image Diffusion Models: Building upon the success of DDPMs, researchers have explored techniques to generate images conditioned on text prompts. [Rombach et al., 2022] [13] introduced cross-attention layers into the diffusion model architecture, enabling text-to-image synthesis. The Stable Diffusion model [Rombach et al., 2022] [13] demonstrated state-of-the-art performance in this domain. However, these models often struggle with faithfully rendering all aspects of the text prompt, leading to the phenomenon of \"catastrophic neglect\" [Chefer et al., 2023] [3]. To control the trade-off between sample quality and diversity, classifier guidance [Nichol and Dhariwal, 2021] [11] was introduced, which combines the score estimate of the diffusion model with the estimates of a separately trained image classifier. Alternatively, classifier-free guidance [Ho and Salimans, 2022] [7] jointly trains a conditional and an unconditional diffusion model, and combines their respective score estimates to achieve a similar trade-off without requiring a separate classifier. Nichol et al. (2022) [12] introduced GLIDE, a text-guided diffusion model for photorealistic image generation and editing. They compare two guidance strategies-CLIP guidance and classifier-free guidance-and find that the latter is preferred by human evaluators for both photorealism and caption similarity, often producing photorealistic samples. Additionally, they demonstrate the capability of their models for image inpainting, enabling powerful text-driven image editing.\nImproving Diffusion Model Performance: Several works have aimed to enhance the performance of diffusion models, particularly in the context of text-to-image generation. [Chefer et al., 2023] [3] proposed the Attend-and-Excite technique, which guides the model to attend to all subject tokens in the text prompt and strengthen their activations, improving the faithfulness of the generated images. [Hoogeboom and Salimans, 2022] [8] showed that blurring can be equivalently defined through a Gaussian diffusion process with non-isotropic noise, bridging the gap between inverse heat dissipation and denoising diffusion, and proposed Blurring Diffusion Models that offer the benefits of both standard Gaussian denoising diffusion and inverse heat dissipation. [Lee et al., 2022] [10] introduced a progressive deblurring approach for diffusion models, enabling coarse-to-fine image synthesis by diffusing and deblurring different frequency components of an image at different speeds.. [Ji et al., 2023] [9] proposed Self-Attention Control for Diffusion Models Training (SAT), leveraging attention maps to refine intermediate samples during training. [Ruiz et al. 2023] [15] presented DreamBooth, a method for fine-tuning text-to-image diffusion models to generate personalized images of subjects based on a few reference images. Saharia et al. (2022) [16]"}, {"title": "Methodology", "content": "In our approach, we extend upon the breakthroughs achieved by the state-of-the-art Stable Diffusion model (SD) introduced by Rombach et al. [13] in 2022. Unlike traditional image-based methods, SD operates within the latent space of an autoencoder architecture. Initially, an encoder (E) is trained to transform input images (x \u2208 X) into spatial latent vectors (z = E(x)). We then utilize a denoising diffusion probabilistic model (DDPM) [6] to first add noise to the latent vector through a forward diffusion process, and subsequently denoise the noisy latent vector through a reverse diffusion process over the learned latent space. This process generates a denoised version of the input latent vector z at each timestep and ultimately we get the final denoised latent vector $z_d$. Throughout the denoising procedure, the diffusion model is conditioned on an additional input condition. Following denoising, a decoder (D) reconstructs the input image, ensuring that the output of the decoder (D($z_d$)) closely resembles the desired image (x). Leveraging this latent space framework allows for efficient manipulation and enhancement of image representations, laying the foundation for our integration of importance scores based on Abs-Grad-SAM [1] and latent space manipulation techniques."}, {"title": "Mask Creation", "content": "The denoising diffusion probabilistic model (DDPM) utilizes a UNet architecture to predict noise at each timestamp. Conditional guidance is provided through the use of cross-attention [17] modules within the UNet. These cross-attention maps within the UNet has varying spatial dimensions and comprises a query, derived from the transformation of the input latent vector, and a key, derived from the input text embedding. Specifically, we define a nth cross attention map weights as $W_{A,t,n}$ as a tensor in $R^{H\u00d7(P\u00d7P)\u00d7N}$ where H is the number of attention heads, P is the spatial dimension of the latent vector, and N is the input text size.\nFor each subject within the input text processed by the cross-attention layer, we compute the Jacobian of gradients of the predicted noise $z_t$ with respect to the corresponding attention weights. This calculation is mathematically represented as:\n$\\frac{d z_t}{J_{W_{A, t, n}}}=\\frac{d z_t\\left(c_i, x_i, y_i\\right)}{d W_{A, t, n}\\left(h_j, s_k, v_l\\right)}$\nwhere $c_i$ indexes the channel, $h_j$ represent the attention-head, $(x_i, y_i)$ represent the spatial coordinates, $s_k$ indexes the token, and $v_l$ indexes the variable in the attention map weight vector.\nNow, based on the Jacobian calculated and the attention scores for a particular subject $s_m$, we calculate the importance score of predicted noise elements corresponding to that subject based on the findings from Grad-SAM [1]. Mathematically, importance score $I_{score}$ is given by:\n$I_{score} (s_m, c_i, t, x_i, t, y_i, t) = W_{A,t} (s_m) \\odot Abs(G_{s_m})$\nwhere $G_{s_m} := \\frac{d z_t}{d W_{A,t} (s_m)}$ represents the gradients of the particular subject $s_m$ in the Jacobian of gradients, and $\\odot$ denotes the Hadamard product.\nBased on the calculated importance scores, we create a mask M(x, y) based on a dynamic threshold $\\theta$ as described:\n$M_{s_m,n}(c,x,y)=\\begin{cases}1, & \\text{if } I_{score}(s_m, c_i, x_i, y_i) \\geq \\theta, \\\\0, & \\text{otherwise}.\\end{cases}$\nAdditionally, to smooth these masks, we apply a Gaussian filter and morphological dilation on them. Similarly, we compute the importance score for all the subjects in the input text and generate a mask for each of them. Subsequently, we take union of these masks to create the corresponding mask for the nth cross-attention layer, which selectively masks all the relevant subjects in the latent vector. Mathematically, we denote this final mask as $M_n(x, y)$ is given by,\n$M_n(c,x,y) = \\bigcup_m M_{s_m,n}(c,x,y),$\nFinally, we compute the intersection of these binary masks from all cross-attention layers to derive the ultimate mask $M_{final}(x, y)$, serving as the composite representation.:\n$M_{final}(c,x,y) = \\bigcap_n M_n(c,x,y)$"}, {"title": "Latent Space Manipulation", "content": "In latent space manipulation, we manipulate the final denoised latent vector $z_{denoised}$ for the particular timestamp t. We utilize the previously formed mask $M_{final}$ to retain the formation of subjects in the image while altering the background from the reference image by replacing the unmasked elements with the corresponding elements from the latent vector $z_{ref}$ obtained by denoising the reference image up to timestep t. Mathematically, the manipulated latent vector $z_{manipulated}$ is given by:\n$z_{manipulated}(c,x,y) =\\begin{cases}z_{denoised}(c,x,y), & \\text{if } M_{final}(c,x,y) = 1 \\\\z_{ref}(c,x,y), & \\text{if } M_{final}(c,x,y) = 0\\end{cases}$        (1)\nWe perform this latent space manipulation for selected timesteps, which we identify by fine-tuning on a validation set. The choice of these timesteps plays a crucial role in determining the quality of the final output image, as manipulating the latent vector at different timesteps can lead to variations in the preservation of subject details and the incorporation of background elements from the reference image."}, {"title": "Mask Refinement and Smoothing", "content": "While the initial masks generated from the importance scores capture the desired regions, their boundaries often exhibit abrupt transitions, which can lead to visual artifacts in the synthesized images. To mitigate this issue and ensure smooth transitions between the preserved subjects and the integrated background elements, we apply two post-processing operations on the importance scores: Gaussian blurring and morphological dilation.\nMathematically, the Gaussian blurring operation can be expressed as:\n$G(x,y) = \\frac{1}{2 \\pi \\sigma^2} exp \\left(-\\frac{x^2 + y^2}{2\\sigma^2}\\right)$\n$I_{blurred}(x,y) = I(x,y) * G(x,y)$\nwhere I(x, y) represents the importance score, G(x, y) is the Gaussian kernel with standard deviation \u03c3, and * denotes the convolution operation. By applying Gaussian blurring to the generated importance score and use them for generating masks, we effectively smooth the boundaries, reducing abrupt transitions and promoting gradual changes between the preserved and integrated regions.\nMorphological dilation is another image processing technique which involves convolving the input mask with a structuring element, typically a small binary kernel. The dilation operation can be mathematically expressed as:\n$I_{dilated}(x,y) = \\max_{(s,t) \\in S} \\{I(x-s, y-t) + K(s,t)\\}$\nwhere I(x, y) is the importance score, K(s, t) is the structuring element (e.g., a disk or square kernel), and S is the domain of the structuring element. By applying dilation to the the scores regions, we slightly expand their boundaries, ensuring a smoother transition between the preserved subjects and the integrated background.\nThe combination of Gaussian blurring and morphological dilation on the importance scores helps generated masks to mitigate visual artifacts and promote seamless integration of the preserved subjects with the background elements from the reference image. The specific parameters, such as the standard deviation for Gaussian blurring and the structuring element for dilation, are fine-tuned empirically to achieve the desired level of smoothness and boundary preservation."}, {"title": "Experiments and Results", "content": "In our experiments, we utilized the Stable Diffusion v1-5 [14] model for text-to-image generation tasks. We downloaded the necessary tokenizer files from the Hugging Face model hub and for text embeddings, we employed the CLIP model. This setup allowed us to leverage the capabilities of Stable Diffusion v1-5 for generating high-quality images based on text prompts, while utilizing CLIP for text embeddings to facilitate the generation process.\nAdditionally, our experiments involved utilizing the Place365 dataset, a subset of which was obtained from kaggle [2]. This dataset consisted of images resized to 256x256 pixels for data compression purposes. We carefully curated the dataset by selecting and identifying specific photos from various categories. The final conditional images dataset had a structured organization, including both artificial and natural scenes each containing several subcategories. Under the artificial category, we have athletic field, industrial area, residential area, downtown, and park. Under the natural category, we have arid area, forest, inland water body, mountain, and ocean. For each subcategory, specific prompts were provided to guide the image generation process.\nTo assess the quality of the generated images, we relied on two key metrics: Frechet Inception Distance (FID) [5] scores and CLIP scores [4]. FID scores measure the similarity between the distribution of real and generated images in feature space [3], while CLIP scores evaluate the alignment between generated images and corresponding textual prompts [4]."}, {"title": "Results Analysis", "content": "In this section, we present the results of our experiments on text-to-image generation tasks. Initially, we employed the Stable Diffusion v1-5 model [14] as our baseline model, coupled with CLIP for text embeddings. To enhance the fidelity of generated images and ensure alignment with the provided conditional image, we fine-tuned the baseline model, identifying optimal hyperparameters. This fine-tuned model is referred to as SD tuned. Subsequently, leveraging the dynamic masking technique outlined in the paper, we developed our model. We utilized the Place365 dataset [2] and evaluated the generated images using two key metrics: Frechet Inception Distance (FID) [5] and CLIP scores [4]."}, {"title": "Frechet Inception Distance (FID) Analysis.", "content": "In the FID scores comparison plot, each bar represents the FID score obtained by a specific model configuration, distinguished by different colors. The x-axis enumerates various scenes and subcategories, providing insights into how each model performs across different visual contexts."}, {"title": "CLIP Score Analysis.", "content": "The CLIP scores comparison plot follows a similar format as FID scores, with each bar illustrating the CLIP score achieved by a particular model."}, {"title": "Aggregated Results.", "content": "Our model demonstrates notable performance in both fidelity preservation, as indicated by the Frechet Inception Distance (FID) scores, and textual context preservation, as demonstrated by the CLIP scores. FID scores provide crucial insights into the similarity between the generated images and the real images from the dataset. Lower FID scores imply better fidelity, suggesting that the generated images closely resemble the real images in terms of visual features. Our model achieved the lowest mean and median FID scores compared to the SD Tuned and SD (Baseline) models. With a mean FID score of 6.89 and a median FID score of 5.32, our model demonstrates a high degree of fidelity, maintaining consistency across various scenes and categories. These results indicate our model's effectiveness in capturing the visual characteristics of the scenes from the Place365 dataset, producing images that closely align with real images in terms of visual quality and features. CLIP scores offer insights into the alignment between the generated images and the textual descriptions provided as input. Higher CLIP scores suggest better alignment, indicating that the generated images effectively capture the intended textual context. Despite primarily focusing on fidelity, our model excels in preserving textual context as well.\nWith a mean CLIP score of 27.98 and a median CLIP score of 28.24, our model achieves competitive performance in aligning the generated images with the provided textual descriptions. This highlights our model's capability to not only produce visually appealing images but also ensure that these images accurately reflect the intended textual context. Overall, our model's balanced performance in both fidelity preservation and textual context preservation demonstrates its effectiveness in text-to-image generation tasks, providing high-quality image generation while maintaining fidelity to the provided textual descriptions."}, {"title": "Qualitative Results", "content": "Based on qualitative analysis of the images presented in comparative analysis, it is apparent that our model exhibits a notably higher adherence to the specified condition in comparison to the tuned model. While the baseline model strictly adheres to the condition, it falls short in integrating essential features and contextual cues from the reference image, primarily attributable to the increased noise present in its latent vector.\nConversely, the tuned model displays adeptness in capturing features from the reference image, facilitated by a latent vector having reduced noise levels. However, this diminished noise level contributes to a compromised fidelity in incorporating conditional elements into the image. Our proposed model effectively preserves the contextual integrity of the image while concurrently upholding fidelity to the prescribed condition."}, {"title": "Conclusion", "content": "In this study, we explored text-to-image generation tasks using the Stable Diffusion model coupled with Abs-Grad-SAM technique for latent space manipulation. Through experimentation, we fine-tuned the baseline model and developed our model by leveraging the dynamic masking technique. We evaluated the generated images using Frechet Inception Distance (FID) scores and CLIP scores, which provided insights into fidelity preservation and textual context alignment, respectively.\nOur model exhibited superior performance compared to both the baseline SD model and the fine-tuned SD model (SD Tuned). With lower FID scores indicating better fidelity and competitive CLIP scores suggesting effective alignment with textual prompts, our model demonstrated balanced performance in both visual quality and textual context preservation.\nFurthermore, the incorporation of the Abs-Grad-SAM method enabled us to manipulate latent space vectors in an explainable manner, potentially enhancing image precision and control over the generation process. While our experiments utilized importance scores from a single cross-attention layer, future research could explore leveraging all cross-attention layers for even more precise image generation.\nOverall, our findings underscore the effectiveness of our model in text-to-image generation tasks, offering high-quality image generation while maintaining fidelity to textual descriptions. This research contributes to advancing the capabilities of generative models and opens avenues for further exploration in controlled image generation and explainable latent space manipulation."}]}